<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PaperTools - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 引入 Marked.js 用于 Markdown 渲染 -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        
        /* 移动端优化 */
        @media (max-width: 640px) {
            body {
                font-size: 14px;
            }
            
            /* 改善可点击区域 */
            button, a {
                min-height: 44px;
                min-width: 44px;
            }
            
            /* 优化间距 */
            .container {
                padding-left: 12px !important;
                padding-right: 12px !important;
            }
        }
        
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
        
        /* 可折叠部分样式 */
        .collapsible-header {
            cursor: pointer;
            display: flex;
            align-items: center;
            font-weight: 600;
            padding: 8px 0;
            user-select: none;
            color: #1e40af;
            transition: all 0.2s ease-in-out;
        }
        .dark .collapsible-header {
            color: #60a5fa;
        }
        .collapsible-header:hover {
            opacity: 0.8;
        }
        .collapsible-header::before {
            content: "▶";
            margin-right: 8px;
            transition: transform 0.3s ease;
            font-size: 0.8em;
        }
        .collapsible-header.open::before {
            transform: rotate(90deg);
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: none;
        }
        .collapsible-content .inner {
            padding-top: 8px;
        }
        
        /* Markdown 内容样式 */
        .markdown-content {
            line-height: 1.6;
        }
        .markdown-content h1 {
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 1em;
            margin-bottom: 0.5em;
            color: #1e40af;
        }
        .dark .markdown-content h1 {
            color: #60a5fa;
        }
        .markdown-content h2 {
            font-size: 1.3em;
            font-weight: bold;
            margin-top: 0.8em;
            margin-bottom: 0.4em;
            color: #1e40af;
        }
        .dark .markdown-content h2 {
            color: #60a5fa;
        }
        .markdown-content h3 {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 0.6em;
            margin-bottom: 0.3em;
            color: #1e40af;
        }
        .dark .markdown-content h3 {
            color: #60a5fa;
        }
        .markdown-content h4 {
            font-size: 1em;
            font-weight: bold;
            margin-top: 0.5em;
            margin-bottom: 0.25em;
            color: #2563eb;
        }
        .dark .markdown-content h4 {
            color: #93c5fd;
        }
        .markdown-content p {
            margin-bottom: 0.8em;
        }
        .markdown-content ul, .markdown-content ol {
            margin-left: 1.5em;
            margin-bottom: 0.8em;
        }
        .markdown-content ul {
            list-style-type: disc;
        }
        .markdown-content ol {
            list-style-type: decimal;
        }
        .markdown-content li {
            margin-bottom: 0.3em;
        }
        .markdown-content code {
            background-color: #f1f5f9;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
        .dark .markdown-content code {
            background-color: #334155;
        }
        .markdown-content pre {
            background-color: #f1f5f9;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 0.8em;
        }
        .dark .markdown-content pre {
            background-color: #334155;
        }
        .markdown-content pre code {
            background-color: transparent;
            padding: 0;
        }
        .markdown-content blockquote {
            border-left: 3px solid #cbd5e1;
            padding-left: 1em;
            margin-left: 0;
            margin-bottom: 0.8em;
            color: #64748b;
        }
        .dark .markdown-content blockquote {
            border-left-color: #475569;
            color: #94a3b8;
        }
        .markdown-content strong {
            font-weight: 600;
        }
        .markdown-content em {
            font-style: italic;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-3 sm:px-4 py-2 rounded-lg shadow-lg z-50 hidden max-w-xs sm:max-w-sm">
        <div class="flex items-center space-x-2">
            <span id="toast-message" class="text-sm sm:text-base">已删除</span>
            <span id="countdown" class="text-xs sm:text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-xs sm:text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-full lg:w-3/5 max-w-none p-3 sm:p-4 lg:p-6">
        <!-- 头部导航栏 -->
        <header class="mb-4 sm:mb-6">
            <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-3 sm:gap-4">
                <h1 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white">PaperTools</h1>
                <div class="flex flex-wrap items-center gap-2 sm:gap-3 w-full sm:w-auto">
                    <!-- 统计信息 -->
                    <div class="text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                        总计 <span id="total-papers">0</span> 篇论文
                    </div>
                    <!-- 筛选按钮 -->
                    <button id="filter-starred" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none transition-colors whitespace-nowrap">
                        显示全部
                    </button>
                    <!-- 中英文摘要切换按钮 -->
                    <button id="summary-toggle" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        中文摘要
                    </button>
                    <button id="theme-toggle" class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <!-- 太阳图标 (浅色模式) -->
                        <svg id="theme-icon-light" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                        </svg>
                        <!-- 月亮图标 (深色模式) -->
                        <svg id="theme-icon-dark" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                        </svg>
                    </button>
                    <!-- GitHub 图标按钮 -->
                    <a href="https://github.com/tsrigo/PaperTools" target="https://github.com/tsrigo/PaperTools" title="GitHub 项目主页"
                       class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <svg class="h-5 w-5 sm:h-6 sm:w-6 text-slate-700 dark:text-slate-200" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.021c0 4.428 2.865 8.184 6.839 9.504.5.092.682-.217.682-.483 0-.237-.009-.868-.014-1.703-2.782.605-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.004.07 1.532 1.032 1.532 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.339-2.221-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.254-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.025A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.295 2.748-1.025 2.748-1.025.546 1.378.202 2.396.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.847-2.337 4.695-4.566 4.944.359.309.678.919.678 1.852 0 1.336-.012 2.417-.012 2.747 0 .268.18.579.688.481C19.138 20.2 22 16.447 22 12.021 22 6.484 17.523 2 12 2z" clip-rule="evenodd"/>
                        </svg>
                    </a>
                </div>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-6 sm:space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2026-02-05": [
        {
            "name": "Artificial Intelligence",
            "count": 15,
            "papers": [
                {
                    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
                    "arxiv_id": "2602.06039",
                    "authors": "Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao",
                    "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个由管理者引导的多智能体框架，专注于解决多智能体系统中的动态通信拓扑和消息路由问题，属于多智能体协作与通信的研究范围。",
                    "summary2": "本文旨在解决现有多智能体系统依赖固定通信模式、难以适应迭代推理阶段需求的问题。针对多轮代码生成与数学推理任务，我们提出了一种名为DyTopo的动态拓扑路由框架，通过语义匹配智能体的自然语言Query与Key描述，每轮重构稀疏有向通信图以实现精准消息路由。在HumanEval、APPS-Competition、MATH-500及Omni-MATH等基准上，通过准确率指标验证了其有效性。",
                    "summary_translation": "由基于提示词的大语言模型构建的 Multi-agent systems (多智能体系统) 能够提升 multi-round reasoning (多轮推理) 能力，然而大多数现有的 pipelines (流程) 依赖于固定的、trajectory-wide (全轨迹) 通信模式，这与 iterative problem solving (迭代式问题求解) 的 stage-dependent (阶段依赖) 需求难以匹配。我们提出了 DyTopo，这是一个 manager-guided (管理者引导) 的 multi-agent framework (多智能体框架)，能够在每一轮重构一个 sparse directed communication graph (稀疏有向通信图)。基于 manager's round goal (管理者的本轮目标)，每个 agent (智能体) 输出 lightweight natural-language query (need) (轻量级自然语言查询/需求) 和 key (offer) (关键/供给) descriptors (描述符)；DyTopo 对这些 descriptors (描述符) 进行 embed (嵌入) 并执行 semantic matching (语义匹配)，仅沿着 induced edges (诱导边) 路由 private messages (私有消息)。在 code generation (代码生成) 和 mathematical reasoning (数学推理) benchmarks (基准测试) 以及四种 LLM backbones (大语言模型骨干网络) 上，DyTopo 的表现始终优于 strongest baseline (最强基线)（平均提升 +6.2）。除了 accuracy (准确性) 之外，DyTopo 还通过 evolving graphs (演化图) 生成了 interpretable coordination trace (可解释的协调轨迹)，从而能够对 communication pathways (通信路径) 如何在各轮之间 reconfigure (重新配置) 进行 qualitative inspection (定性分析)。",
                    "inspiration_trace": "基于论文《DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching》的内容，以下是对作者产出该核心方法的逻辑链推演，旨在还原其从观察到方法论的思考演进过程：\n\n### 第一阶段：宏观观察与问题定位\n**（从“多智能体协作”到“静态结构的局限性”）**\n\n1.  **观察现象**：作者首先观察到，基于大语言模型（LLM）的多智能体系统在复杂推理任务中表现优异，通过角色分工和多轮交互能够提升解题质量。\n2.  **发现痛点**：然而，作者注意到现有的主流框架（如CAMEL, AutoGen等）大多依赖于**固定的通信拓扑**（例如：全广播、固定的对话轮次或预设的流水线）。这种“一刀切”的结构在整个推理过程中保持不变。\n3.  **提出质疑**：作者意识到，复杂问题的求解过程是**分阶段**的（例如：初期需要发散探索，后期需要聚焦验证）。用一个静态的通信结构去适配动态变化的推理需求，必然会导致效率低下（早期信息不足）或噪声干扰（后期无关信息过多）。\n\n### 第二阶段：核心假设与逻辑重构\n**（从“结构决定流向”到“需求决定结构”）**\n\n1.  **形成假设**：作者提出假设——通信拓扑不应是预设的静态超参数，而应是**随推理阶段动态演化的状态变量**。系统应具备在每一轮根据当前目标自适应重连智能体的能力。\n2.  **逻辑转换**：传统的多智能体逻辑是“谁连谁，谁就说话”。作者试图将其反转为“谁需要什么，谁就连接谁”。即，**信息流的需求应当决定连接结构**，而不是结构限制信息流。\n\n### 第三阶段：机制设计的灵感与抽象\n**（从“显式路由”到“语义匹配”）**\n\n1.  **寻找机制**：如何在不进行昂贵训练的情况下，让智能体在推理时动态决定连接对象？作者借鉴了信息检索和注意力机制中的思想。\n2.  **抽象概念**：作者将智能体的交互意图抽象为两个轻量级的自然语言描述符：\n    *   **Query（Need）**：我当前需要什么信息？\n    *   **Key（Offer）**：我当前能提供什么信息？\n3.  **引入语义匹配**：利用预训练的语义编码器，计算“需求”与“供给”之间的相似度。如果Agent A的Offer与Agent B的Need在语义上高度匹配，就在它们之间建立一条有向边。这实现了**基于内容的路由**。\n\n### 第四阶段：系统架构的闭环构建\n**（从“单点连接”到“全局动态图”）**\n\n1.  **引入管理者**：为了防止智能体无序发散，作者引入了一个**Manager（管理者）**智能体。它的作用是定义每一轮的“全局目标”，从而引导所有Worker Agent生成符合当前阶段的Need/Offer描述。\n2.  **构建动态计算图（DCG）**：\n    *   **每轮重构**：在每个Round $t$，系统根据Manager的目标和Agent生成的描述符，重新计算相似度矩阵，通过阈值化生成一个稀疏有向图 $G(t)$。\n    *   **消息路由**：私有消息不再广播，而是严格沿着 $G(t)$ 的边进行传输。\n3.  **实现自适应演化**：随着Manager更新目标（从“探索方案”变为“验证代码”），Agent的Need/Offer随之改变，导致图结构自动从“密集网状”（探索期）演变为“稀疏链状”（验证期）。\n\n### 第五阶段：价值验证与反思\n**（从“性能提升”到“可解释性”）**\n\n1.  **预期收益**：作者预期这种机制能带来两个核心价值：\n    *   **准确性**：通过过滤无关信息，减少上下文噪声，让智能体专注于当前最需要的知识。\n    *   **效率**：稀疏图减少了不必要的Token消耗和计算量。\n2.  **独特视角**：作者进一步意识到，这种显式的图结构变化提供了一种**可解释的轨迹**。研究者可以直接观察图是如何从混乱走向有序的，从而理解系统是如何协作解决问题的，这是黑盒模型所不具备的。\n\n---\n\n**总结：**\n作者的思考路径是从**批判现有系统的静态僵化**出发，洞察到**推理过程的阶段性需求**，进而创造性地引入**基于语义供需的动态路由机制**，最终通过**管理者引导下的图重构**，实现了一个既能自适应提升性能，又具备高度可解释性的多智能体框架。"
                },
                {
                    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
                    "arxiv_id": "2602.06008",
                    "authors": "Xianyang Liu, Shangding Gu, Dawn Song",
                    "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了AgenticPay，这是一个基于LLM的多智能体谈判框架，重点研究多个智能体（买方和卖方）之间的语言交互、协作与博弈，属于多智能体研究范畴。",
                    "summary2": "本文旨在填补评估多智能体LLM自然语言经济交互的基准空白。针对买卖双方具有私人约束和多轮语言谈判的场景，我们提出了AgenticPay框架，支持从双边到多对多市场的模拟。我们在包含110多个任务的AgenticPay benchmark上，通过GlobalScore、BuyerScore和SellerScore等指标验证了其有效性，揭示了当前LLM在长周期战略推理上的显著差距。",
                    "summary_translation": "基于 Large Language Model (LLM) (大语言模型) 的 agents (智能体) 日益被寄予自主谈判、协调和交易的期望，然而现有的 benchmarks (基准) 缺乏用于评估多 agents (智能体) 间以语言为媒介的经济互动的严谨设置。我们提出了 AgenticPay，这是一个由自然语言驱动的多 agents (智能体) 买卖双方谈判 benchmark (基准) 及 simulation framework (仿真框架)。AgenticPay 对市场进行建模，其中买卖双方拥有 private constraints (私有约束) 和 product-dependent valuations (基于产品的估值)，且必须通过 multi-round linguistic negotiation (多轮语言谈判) 而非单纯的 numeric bidding (数值出价) 来达成协议。该框架支持包含110多项任务的多样化套件，涵盖从 bilateral bargaining (双边议价) 到 many-to-many markets (多对多市场) 的场景，并具备 structured action extraction (结构化动作提取) 功能以及针对 feasibility (可行性)、efficiency (效率) 和 welfare (福利) 的评估指标。对 state-of-the-art (最先进的) proprietary (专有) 及 open-weight (开放权重) LLMs 进行的 benchmarking (基准测试) 揭示了其在谈判性能上存在显著差距，并凸显了在 long-horizon strategic reasoning (长周期战略推理) 方面面临的挑战，从而确立了 AgenticPay 作为研究 agentic commerce (智能体商务) 和基于语言的市场互动的基础。代码和数据集可在以下链接获取：https://github.com/SafeRL-Lab/AgenticPay。",
                    "inspiration_trace": "基于论文《AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions》的内容，以下是对作者构建该核心方法逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与范式转移\n**思考起点：从“对话者”到“经济代理人”的角色演变**\n作者首先观察到一个宏观趋势：大语言模型（LLM）的应用场景正从单一的问答或内容生成，转向作为自主代理人在现实经济活动中执行任务（如电商、采购、服务签约）。\n*   **核心问题**：现有的LLM评估体系主要集中在单智能体的推理能力或工具使用上，缺乏一个能够评估多智能体在**经济互动**中表现的基准。\n*   **初步假设**：如果LLM要成为真正的经济代理人，它们必须具备通过自然语言进行谈判、协调和交易的能力，而不仅仅是生成通顺的文本。\n\n### 第二阶段：痛点识别与现有方法的局限\n**深入分析：现有评估基准的“抽象化”缺陷**\n作者回顾了现有的博弈论和NLP谈判研究，发现它们存在严重的“现实脱节”：\n1.  **交互方式简化**：传统博弈论模型通常假设代理通过数字出价或效用函数交互，忽略了自然语言在表达偏好、约束和策略中的核心作用。\n2.  **场景过于单一**：现有的NLP谈判数据集多局限于双边议价（1对1），且缺乏私有信息（如底价）和复杂的市场结构（如多买家多卖家的竞争）。\n*   **逻辑推演**：要真实评估LLM的经济智能，必须构建一个**语言介导的**、包含**私有信息**的、且能扩展到**复杂市场结构**的测试环境。\n\n### 第三阶段：核心概念的形式化\n**理论构建：将谈判定义为“语言博弈”**\n为了解决上述痛点，作者提出了核心假设：谈判本质上是一个有限视界的、多轮次的语言游戏。\n*   **关键抽象**：\n    *   **私有状态**：每个智能体（买方/卖方）拥有不可观测的内部状态（如最高预算、最低售价），这是谈判策略的基石。\n    *   **对话到行动的映射**：自然语言对话必须被解析为结构化的经济行动（如价格提议、接受交易）。\n*   **设计目标**：建立一个框架，既能保留语言的丰富性，又能通过结构化的指标（可行性、效率、福利）来量化评估结果。\n\n### 第四阶段：方法论构建与维度扩展\n**系统设计：构建可扩展的复杂性阶梯**\n基于形式化定义，作者开始设计具体的评估框架，其逻辑遵循“控制变量”与“压力测试”的原则：\n\n1.  **环境维度的真实性**：\n    *   为了避免模型在单一领域过拟合，作者设计了10个涵盖日常生活、专业服务、商业采购和金融资产的真实商业场景。这确保了评估的**泛化性**。\n\n2.  **任务维度的复杂性阶梯**：\n    *   作者意识到单一任务无法全面衡量能力，因此设计了从简单到复杂的任务谱系：\n        *   **基础层**：双边议价（1买1卖）。\n        *   **竞争层**：引入竞争（1买多卖 或 多买1卖），测试代理在有机会成本时的决策能力。\n        *   **市场层**：多对多市场（N买N卖），测试匹配和资源分配能力。\n    *   **逻辑意图**：通过这种维度扩展，可以剥离出模型在处理“竞争”、“并发”和“多产品”时的具体短板。\n\n3.  **评估维度的福利导向**：\n    *   仅仅衡量“是否达成交易”是不够的。作者引入了基于博弈论的评分机制，不仅奖励成交，还奖励**效率**（速度）和**公平性**（剩余分配），以此引导模型追求高质量的经济结果。\n\n### 第五阶段：实验验证与假设修正\n**实证分析：揭示“语言能力”与“经济理性”的鸿沟**\n最后，作者利用该框架对SOTA模型进行基准测试，以验证其假设并发现新问题：\n*   **预期发现**：闭源模型（如GPT-5.2, Claude）在整体表现上优于开源模型，且随着市场流动性增加（买卖双方增多），谈判效率反而提升（验证了竞争促进成交的经济学直觉）。\n*   **深层洞察**：作者发现了模型在“最后一公里”的收敛缺陷（Near-miss failures）以及买卖双方角色的系统性不对称。这证明了**强大的语言生成能力并不等同于有效的经济战略推理**，从而确立了AgenticPay作为未来研究基础平台的地位。\n\n---\n\n**总结**：\n作者的思考路径是从**应用趋势**（LLM作为经济代理）出发，识别出**评估空白**（缺乏语言介导的市场互动测试），进而通过**理论抽象**（语言博弈与私有信息）构建框架，最后通过**多维度的任务设计**（从双边到多边市场）和**福利导向的指标**，完成了一套既能反映现实复杂性又能进行科学量化的基准系统。"
                },
                {
                    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
                    "arxiv_id": "2602.04837",
                    "authors": "Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu, Xin Eric Wang",
                    "summary": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断过程如下： 1.  **核心判断 (第一步)**： 论文的核心贡献是提出了一种名为 \"Group-Evolving Agents (GEA)\" 的新范式，旨在实现开放式自我改进。这直接对应了我的研究目标中的 **\"自我演化\" (Self-Evolving)** 和 **\"多智能体\" (Multi-Agent)** 方向。论文并非仅仅将LLM作为工具应用于特定领域，而是提出了一种新的智能体演化方法论，因此属于保留范畴。 2.  **正面指标匹配 (第二步)**： 论文高度符合我的核心关注点： *   **核心范式**：明确涉及 `Self-Evolving` 和 `Multi-Agent Systems`（将一组智能体作为演化单元）。 *   **演化机制**：核心在于 `Self-Improvement`（自我完善）和 `Generational Evolution`（代际演化），通过 `Experience Sharing`（经验共享）来克服现有树状演化方法的局限性。 *   **多智能体**：涉及智能体群体内部的协作与经验复用。 3.  **排除标准检查 (第三步)**： 论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此没有触犯任何排除标准。 4.  **特殊情况处理 (第四步)**： 虽然论文在编码基准上进行了评估，但根据筛选标准中的“自我演化的应用”规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。本文的核心是GEA这一演化框架，而非解决编码问题本身，因此符合保留条件。 **结论**：该论文提出了一种基于群体演化的智能体自我改进框架，直接切中 \"LLM智能体及其演化\" 这一课题的核心，特别是关于智能体如何通过经验共享实现自我完善和迭代演化的机制，因此判定为符合要求。",
                    "summary2": "本文旨在实现开放式自我改进，克服现有树状结构进化中分支隔离导致的探索多样性利用效率低的问题。针对开放式自我进化场景，我们提出了一种 Group-Evolving Agents (GEA) 方法，将代理组作为基本进化单元，通过显式的组内经验共享和重用来促进进化。在 SWE-bench Verified 和 Polyglot 基准上，通过成功率验证了其有效性，显著优于现有方法并匹敌人类设计框架。",
                    "summary_translation": "开放式自我改进智能体能够自主修改其自身的结构设计，以提升能力并克服预定义架构的局限性，从而减少对人工干预的依赖。我们提出了群体进化智能体，这是一种用于开放式自我改进的新范式。该范式将一组智能体视为基本的进化单元，从而能够在整个进化过程中在群体内部实现显式的经验共享与复用。与采用树状结构进化的现有开放式自我进化范式不同，GEA克服了由孤立的进化分支导致的探索多样性利用效率低下的局限性。我们在具有挑战性的编程基准测试上评估了GEA，结果表明其显著优于最先进的自我进化方法（在SWE-bench Verified上为71.0% vs. 56.7%，在Polyglot上为88.3% vs. 68.3%），并且匹配或超越了顶尖的人类设计的智能体框架（在两个基准测试上分别为71.8%和52.0%）。分析表明，GEA能更有效地将早期的探索多样性转化为持续的长期进步，在相同数量的进化智能体下实现了更强的性能。此外，GEA在不同的编程模型上表现出一致的迁移性和更强的鲁棒性，平均仅需1.4次迭代即可修复框架级错误，而自我进化方法则需要5次。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《Group-Evolving Agents》这篇文章的思考过程与逻辑链的系统还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示现有研究的根本性缺陷，从而为新范式的提出铺平道路：\n\n1.  **宏观愿景与现状落差**：\n    *   **愿景**：科学突破依赖于“开放式演进”和“累积性进步”。\n    *   **现状**：现有的AI系统受限于人类预定义的架构，缺乏自我修改结构的能力，无法突破初始设计的边界，因此进步仍严重依赖人类干预。\n\n2.  **现有方案的局限（核心冲突）**：\n    *   **尝试**：为了解决上述问题，现有工作转向了受生物进化启发的“开放式自我演进”系统。\n    *   **机制**：这些系统采用“个体中心”的树状结构进化（父代产生子代，分支独立）。\n    *   **缺陷**：这种严格的分支隔离导致了“探索性多样性的低效利用”。虽然系统产生了大量多样化的尝试，但这些尝试往往只是短暂的变体，无法作为有效的“垫脚石”汇聚成长期的累积性进步。\n\n3.  **观念突破与切入点**：\n    *   **反思**：AI智能体不是生物个体，为什么要受制于生物进化的范式？\n    *   **优势**：AI智能体可以直接共享轨迹、工具和习得的工件，不受生殖或血统的限制。\n    *   **结论**：应当摒弃生物隐喻，利用AI的数字特性，通过显式的经验共享来解决“多样性无法累积”的问题。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“如何构建一种开放式自我改进范式，使其能够突破个体中心进化中分支隔离的限制，通过显式的经验共享与复用，将探索阶段的多样性有效转化为长期的累积性进步？”**\n\n---\n\n### 三、 思考过程与逻辑演进还原\n\n以下是从宏观观察到具体方法论的完整逻辑推演：\n\n#### 1. 宏观观察：AI 进化的“天花板”\n*   **思考起点**：目前的 AI Agent 无论多强，本质上都是在执行人类预设的“代码逻辑”或“架构”。要想实现真正的超级智能或持续突破，Agent 必须具备**自我修改架构**的能力，即从“被动执行”转向“主动进化”。\n\n#### 2. 现状批判：生物隐喻的陷阱\n*   **观察**：现有的自我进化方法（如 DGM 等）大多模仿达尔文进化论，采用树状结构。每个 Agent 独立进化，互不干扰。\n*   **发现问题**：\n    *   这种结构虽然保证了**探索的多样性**（大家往不同方向试错）。\n    *   但由于**分支隔离**，一个分支学到的“好经验”（如某个工具的使用技巧、某个工作流的优化）无法传递给另一个分支。\n    *   **结果**：大量的探索变成了“一次性”的尝试，随着分支的消亡而消失，无法形成叠加效应。这就像人类文明如果没有语言和书籍（共享机制），每一代人都要重新发明轮子。\n\n#### 3. 关键假设：从“个体”到“群体”的范式转移\n*   **反思**：AI 不是生物，不需要 DNA 遗传。AI 的优势在于数字信息的**零成本复制与即时共享**。\n*   **假设**：如果将进化的基本单位从“单个 Agent”转变为“一组 Agent”，让组内的成员显式地共享经验池，那么：\n    *   Agent A 发现的优化点可以被 Agent B 直接吸收。\n    *   早期的探索多样性不再是“死胡同”，而是可以被汇聚到最优个体上的“资源”。\n    *   **核心逻辑**：通过**群体层面的经验聚合**，将“短暂的多样性”转化为“持续的累积性进步”。\n\n#### 4. 方法论构建：GEA (Group-Evolving Agents) 的诞生\n*   **设计原则**：如何实现上述假设？需要设计一个机制，既能保持探索的广度，又能保证经验的深度整合。\n*   **具体逻辑链**：\n    *   **第一步：定义群体**：不再只选一个“最强”的父代，而是选一个“父代组”。\n    *   **第二步：平衡选择**：选谁进组？不能只选最强的（容易陷入局部最优），也不能只选怪的（效率低）。因此提出了 **Performance-Novelty（性能-新颖性）** 标准，既要能干活，又要不一样。\n    *   **第三步：共享进化**：这是核心创新点。父代组产生子代组时，不是各生各的，而是把所有人的“进化轨迹”（代码补丁、失败日志、工具使用记录）扔到一个**共享池**里。\n    *   **第四步：反思与迭代**：每个子代 Agent 都基于这个共享池进行“反思”，生成改进指令。这意味着 Agent A 的失败经验可能直接帮助 Agent B 避坑，Agent B 的好工具可能被 Agent A 直接采纳。\n\n#### 5. 预期验证与价值闭环\n*   **逻辑推演**：如果这个逻辑成立，那么 GEA 应该表现出以下特征：\n    *   **效率更高**：在进化相同数量的 Agent 后，GEA 的性能应远超树状结构（因为经验被复用了，没有被浪费）。\n    *   **鲁棒性更强**：如果某个 Agent 挂了（引入了 Bug），组内其他健康的 Agent 可以通过共享经验“修好”它。\n    *   **通用性**：因为进化的是“工作流”和“工具使用”逻辑（即软件工程能力），而不是针对特定模型的 Prompt，所以换一个底座模型（如从 GPT 换到 Claude），这套进化的框架依然有效。\n\n---\n\n**总结**：\n作者的思考路径是从**打破生物学隐喻**开始，识别出**“分支隔离导致经验浪费”**这一核心痛点，进而利用 AI 的**数字可共享性**，提出了**“群体进化”**的新范式。其核心逻辑在于：通过**显式的经验共享**，将开放式探索中的**随机多样性**转化为**确定性的累积进步**。"
                },
                {
                    "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
                    "arxiv_id": "2602.04634",
                    "authors": "Zelai Xu, Zhexuan Xu, Ruize Zhang, Chunyang Zhu, Shi Yu, Weilin Liu, Quanlu Zhang, Wenbo Ding, Chao Yu, Yu Wang",
                    "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断符合（多智能体方向）**： *   论文的核心贡献是提出了 **WideSeek-R1**，这是一个新的 **多智能体系统（Multi-Agent Systems）** 框架。 *   它探索了“宽度缩放”这一新维度，通过构建“主导智能体-子智能体”的架构来解决广泛信息搜寻问题。这直接对应了研究焦点中的 **“多智能体”** 方向，涉及智能体间的协作与编排。 2.  **包含核心正面指标**： *   **多智能体机制**：论文明确提到了 `Multi-Agent Systems (MAS)`，并设计了 `Lead-agent-subagent` 结构，涉及智能体之间的协同工作。 *   **智能体能力**：涉及 `Tool Use`（专用工具）和 `Planning/Orchestration`（可扩展的编排）。 *   **训练与优化**：使用了 `Multi-Agent Reinforcement Learning (MARL)` 来联合优化智能体，这属于智能体构建和改进的方法论范畴。 3.  **不属于排除项**： *   **非单纯应用**：虽然任务背景是“广泛信息搜寻”，但论文的重点不在于应用本身，而在于提出了一种新的智能体架构（宽度缩放）和训练方法（MARL），以解决现有多智能体系统依赖手工工作流和无法有效并行化的问题。 *   **非安全/多模态/图**：论文不涉及安全对齐、多模态视觉或图神经网络等排除领域。 综上所述，该论文致力于构建和改进多智能体框架，探索智能体的并行执行与协作机制，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在探索 LLM 的宽度扩展以解决广泛信息检索问题。针对单智能体在广泛任务中的上下文污染和串行执行瓶颈，我们提出了 WideSeek-R1，一种通过 Multi-Agent Reinforcement Learning (MARL) 训练的 lead-agent–subagent 框架，实现可扩展编排与并行执行。我们在 WideSearch benchmark 上通过 Item F1 score 等指标验证了其有效性，证明 4B 模型性能可比肩 DeepSeek-R1-671B。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）的最新进展主要集中在深度扩展（depth scaling）上，即单一智能体通过多轮推理（multi-turn reasoning）和工具使用（tool use）来解决长视界（long-horizon）问题。然而，随着任务范围的扩大，关键瓶颈已从个体能力转向组织能力。在这项工作中，我们利用多智能体系统（multi-agent systems）探索了一个互补的宽度扩展（width scaling）维度，以解决广泛的信息搜寻（broad information seeking）问题。现有的多智能体系统通常依赖手工设计的工作流（hand-crafted workflows）和轮流交互（turn-taking interactions），无法有效地实现工作并行化。为了弥合这一差距，我们提出了 WideSeek-R1，这是一个通过多智能体强化学习（multi-agent reinforcement learning, MARL）训练的主智能体-子智能体框架（lead-agent-subagent framework），旨在协同实现可扩展的编排和并行执行。通过利用具有隔离上下文（isolated contexts）和专用工具（specialized tools）的共享大语言模型（shared LLM），WideSeek-R1 在包含 2 万个广泛信息搜寻任务的精选数据集（curated dataset）上，对主智能体和并行子智能体进行了联合优化。大量实验表明，WideSeek-R1-4B 在 WideSearch 基准测试（WideSearch benchmark）上实现了 40.0% 的项目级 F1 分数（item F1 score），这与单智能体 DeepSeek-R1-671B 的性能相当。此外，随着并行子智能体数量的增加，WideSeek-R1-4B 展现出持续的性能提升，凸显了宽度扩展（width scaling）的有效性。",
                    "inspiration_trace": "基于对论文《WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning》的深入分析，以下是对作者核心方法论产出逻辑链的系统推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从观察到痛点\n\n作者在Introduction部分构建了一个层层递进的逻辑叙事，旨在引出研究的必要性：\n\n1.  **现状观察：** 当前LLM的研究主流集中在“深度扩展”，即通过单个智能体进行长链思维和多轮工具调用来解决长视距问题。\n2.  **范式转移：** 随着任务从“深”变“广”，核心瓶颈发生了转移。对于广泛信息搜集任务，瓶颈不再是单个智能体的推理能力，而是系统的组织能力。\n3.  **单智能体失效：** 在广泛信息搜集场景下，单智能体面临两个致命缺陷：\n    *   **上下文污染：** 随着子任务累积，无关信息充斥上下文，导致性能下降。\n    *   **串行执行低效：** 独立子任务被迫串行处理，限制了效率。\n4.  **现有多智能体局限：** 虽然多智能体系统看似是解药，但现有方案存在两大缺陷：\n    *   **编排僵化：** 依赖手工设计的工作流，缺乏灵活性和可扩展性。\n    *   **伪并行：** 采用轮流交互机制，本质上仍是串行处理，未能实现真正的并行化。\n5.  **核心缺口：** 缺乏一种端到端的训练方式，能够同时学习可扩展的编排和真正的并行执行。\n\n**显式总结的研究问题：**\n> **如何通过多智能体强化学习实现有效的“宽度扩展”，以解决广泛信息搜集任务中单智能体面临的上下文污染与串行瓶颈，并克服现有多智能体系统依赖手工工作流和无法真正并行执行的局限？**\n\n---\n\n### 二、 思想演进脉络：从宏观假设到方法论落地\n\n以下逻辑链还原了作者从发现问题到提出WideSeek-R1的完整思考过程：\n\n#### 第一阶段：维度的重新审视（观察与假设）\n*   **思考起点：** 既然“深度扩展”（增加推理步数）已经触及天花板，且不适合处理需要覆盖大量实体的“广度”任务，那么是否存在一个互补的维度？\n*   **假设提出：** 引入“宽度扩展”概念。即不再依赖一个聪明的“大脑”做长串思考，而是依赖一个“组织”协调多个“大脑”并行工作。\n*   **核心洞察：** 问题的本质从“如何让模型更聪明”转变为“如何让模型组织更高效”。\n\n#### 第二阶段：对现有方案的批判性分析（定位痛点）\n*   **审视单智能体：** 试图让一个模型做完所有事，就像让一个人去查百科全书并整理所有条目，记忆会溢出（上下文污染），速度太慢（串行瓶颈）。\n*   **审视现有多智能体：** 现有的多智能体框架（如AutoGen等）更像是“剧本杀”，角色和流程是写死的。这种“手工编排”无法适应任务规模的变化，且所谓的“协作”往往是你一句我一句的“聊天”，而非同时干活。\n*   **结论：** 我们需要的不是更多的智能体，而是**学会如何协作**的智能体。\n\n#### 第三阶段：方法论的构建（寻找解法）\n*   **架构设计：** 为了实现真正的并行和有效管理，必须采用分层结构。\n    *   **Lead Agent（指挥官）：** 只负责一件事——拆解任务和分发。它不需要知道具体细节，只需要知道“谁该做什么”。\n    *   **Subagents（执行者）：** 只负责一件事——利用工具并行获取信息。\n    *   **隔离机制：** 给每个执行者独立的上下文，彻底解决上下文污染问题。\n*   **学习机制：** 既然手工设计工作流不行，那就让模型自己学会如何编排。\n    *   **引入MARL（多智能体强化学习）：** 利用RL的试错机制，让Lead Agent学会如何拆解任务收益最高，让Subagents学会如何搜索最准确。\n    *   **协同优化：** 必须端到端训练，让指挥官和执行者共同进化，而不是分开训练再拼凑。\n\n#### 第四阶段：数据与验证的闭环（落地支撑）\n*   **数据困境：** 现有的QA数据集（如HotpotQA）都是为“深度”推理设计的，不适合训练“广度”搜集能力。\n*   **数据构建：** 必须自己造数据。构建一个包含20k条广泛信息搜集任务的数据集，强制模型输出结构化表格，迫使其学习处理多实体、多属性的并行任务。\n*   **验证逻辑：** 如果假设成立，那么一个4B的小模型通过宽度扩展（增加并行子智能体），应该能打败甚至追平671B的大模型（深度扩展）。\n\n---\n\n### 总结\n作者的思考路径是从**Scaling Law的维度补全**出发，敏锐地捕捉到**任务性质从“深”转“广”带来的瓶颈转移**，进而批判了**手工编排和串行交互的低效**，最终通过**分层架构+MARL端到端训练**这一方法论，实现了从“个体能力增强”到“组织能力涌现”的跨越。"
                },
                {
                    "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control",
                    "arxiv_id": "2602.04496",
                    "authors": "Zhentao Tang, Yuqi Cui, Shixiong Kai, Wenqian Zhao, Ke Ye, Xing Li, Anxin Tian, Zehua Pei, Hui-Ling Zhen, Shoubo Hu, Xiaoguang Li, Yunhe Wang, Mingxuan Yuan",
                    "summary": "Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，核心依据如下： 1.  **核心判断（符合 Agentic AI 构建）**： 论文的核心贡献是提出了 **ReThinker**，这是一个“confidence-aware agentic framework”（信心感知的智能体框架）。这不仅仅是应用现有模型，而是构建了一个新的智能体架构来解决复杂推理问题，符合“构建、改进 LLM智能体”的核心目标。 2.  **正面指标（高度匹配）**： *   **多智能体**：论文明确采用了 **Solver-Critic-Selector** 架构，这是一种典型的多智能体或多角色协作模式，用于解决单一模型难以处理的复杂任务。 *   **智能体能力**：论文涉及 **Tool Use**（工具使用）、**Planning**（通过动态计算分配和自适应工具调用体现）以及 **Self-Reflection**（引导式反思和重新思考）。 *   **自我演化/改进**：论文提出了“adaptive trajectory recycling strategy”（自适应轨迹回收策略），将成功的推理轨迹转化为监督信号，这属于通过经验进行自我完善和迭代的机制。 3.  **排除标准检查（通过）**： *   论文虽然应用于科学推理基准，但其重点在于提出新的智能体框架，而非单纯的应用型研究。 *   不涉及安全、对齐、多模态视觉或图技术等排除领域。 4.  **特殊情况处理**： 论文属于“Agentic Reasoning”范畴。它不是单纯通过微调提升模型的基础逻辑能力，而是通过设计智能体的交互流程（反思、工具调用、多角色协作）来提升性能，因此符合保留标准。 综上所述，该论文在多智能体协作、自我反思机制以及智能体框架构建方面做出了直接贡献，符合“Agentic AI”和“Multi-Agent”的研究焦点。",
                    "summary2": "本文旨在提升大语言模型在专家级科学推理任务中的表现。针对HLE等高难度科学问题，我们提出了一种名为ReThinker的置信度感知智能体框架，采用Solver–Critic–Selector分阶段架构，结合动态计算分配、引导式反思和置信度加权选择机制。在HLE、GAIA和XBench基准上通过准确率验证了其有效性，显著优于现有SOTA模型。",
                    "summary_translation": "专家级科学推理对大语言模型而言仍然极具挑战性，特别是在 Humanity's Last Exam (HLE，人类最后的考试) 等基准测试中，僵化的工具流水线、脆弱的多智能体协调以及低效的 test-time scaling (测试时扩展) 往往限制了模型性能。我们提出了 ReThinker，这是一个 confidence-aware (置信度感知) 的 agentic framework (智能体框架)，通过分阶段的 Solver-Critic-Selector (求解器-评论家-选择器) 架构来编排 retrieval (检索)、tool use (工具使用) 和 multi-agent reasoning (多智能体推理)。ReThinker 并非遵循固定的流水线，而是基于模型置信度动态分配计算，从而实现 adaptive tool invocation (自适应工具调用)、guided multi-dimensional reflection (引导式多维反思) 以及 robust confidence-weighted selection (鲁棒的置信度加权选择)。为了支持无需 human annotation (人工标注) 的 scalable training (可扩展训练)，我们进一步提出了 reverse data synthesis pipeline (反向数据合成流水线) 和 adaptive trajectory recycling strategy (自适应轨迹回收策略)，将成功的 reasoning traces (推理轨迹) 转化为高质量的 supervision (监督信号)。在 HLE、GAIA 和 XBench 上的实验表明，ReThinker 始终优于配备工具的 state-of-the-art (最先进的) foundation models (基础模型) 及现有的 deep research systems (深度研究系统)，在 expert-level reasoning tasks (专家级推理任务) 上取得了 state-of-the-art (最先进的) 结果。",
                    "inspiration_trace": "基于对论文《ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个层层递进的叙事逻辑，旨在揭示当前 LLM 在科学推理领域的核心痛点：\n\n1.  **宏观背景与挑战定位**：\n    *   科学推理已成为评估 LLM 能力的核心挑战，也是通向 AGI 的关键指标。\n    *   与常识推理不同，科学问题求解要求**定量的严谨性**、**多跳因果推断**以及**跨领域专业知识**的整合。\n\n2.  **现状与表象的矛盾**：\n    *   尽管现有 LLM 在表面上表现强劲，但在专家级基准（如 Humanity’s Last Exam, HLE）上却频频失败。\n    *   **核心矛盾**：模型无法可靠地区分“正确的数学推理”与“存在细微缺陷的论证”。这表明其所谓的成功更多源于**模式记忆**，而非系统的、原则性的演绎。\n\n3.  **现有方案的局限性**：\n    *   现有的工具增强（如 ReAct）和多智能体协调往往受限于**僵化的管道**、**脆弱的协调机制**以及**低效的测试时扩展**，无法解决深层次的逻辑缺陷。\n\n4.  **核心洞察与能力缺失**：\n    *   作者指出，要突破这一瓶颈，必须具备三种当前系统极度缺乏的能力：\n        *   **再思考**：不满足于单次推理，而是迭代地质疑和修正中间结论。\n        *   **引导式反思**：超越肤浅的总结，进行结构化的、特定维度的错误诊断。\n        *   **置信度控制**：显式量化不确定性，通过多轮裁决来稳定答案选择，消除验证噪声。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑，本文试图回答的核心研究问题是：\n\n**“如何构建一个具备自适应计算能力的智能体框架，使其能够通过迭代再思考、引导式反思以及置信度感知的决策机制，突破现有模型在专家级科学推理任务中依赖模式记忆而非严谨演绎的瓶颈？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n以下是从宏观观察到具体方法论的推演过程：\n\n#### 1. 观察与诊断：从“能力天花板”到“认知缺陷”\n*   **观察**：在 HLE 等高难度基准上，单纯增加模型参数或使用简单的工具调用（如 Web Search、Code Interpreter）遇到了天花板。\n*   **诊断**：问题的根源不在于“知识不够多”或“工具不够用”，而在于**推理过程缺乏自我修正的深度**。现有的单次推理或简单的多智能体投票无法捕捉科学论证中细微的逻辑漏洞。模型往往“自信地犯错”。\n\n#### 2. 假设提出：从“静态执行”到“动态认知”\n*   **假设**：如果能让模型像人类专家一样，在推理过程中具备**自我怀疑**和**多维反思**的能力，并根据这种不确定性动态分配计算资源，就能显著提升推理的鲁棒性。\n*   **推论**：我们需要一个系统，它不是机械地执行固定步骤，而是根据“置信度”来决定是继续深挖、重新反思，还是直接给出答案。\n\n#### 3. 方法论构建：三大支柱的落地\n为了验证上述假设，作者设计了 ReThinker 框架，其逻辑演进如下：\n\n*   **支柱一：数据层面的“自举”**\n    *   *思考*：要训练模型学会“反思”和“再思考”，需要大量包含错误修正轨迹的高质量数据。人工标注成本极高且不可扩展。\n    *   *方案*：提出**反向数据合成管道**。利用强模型生成轨迹，通过验证器筛选出正确的路径，并利用“轨迹回收”策略，将原本丢弃的上下文转化为新的种子，实现数据的自我进化。\n\n*   **支柱二：推理架构的“解耦与重构”**\n    *   *思考*：单一模型很难同时兼顾“发散性思考”和“批判性审视”。且长上下文会导致反思时丢失细节。\n    *   *方案*：设计 **Solver-Critic-Selector** 三阶段架构。\n        *   **Solver（解题者）**：负责发散和探索，通过多轮迭代生成初步解（对应“再思考”）。\n        *   **Critic（批评者）**：负责收敛和修正。为了解决上下文限制，作者创新性地引入了**结构化摘要**，将轨迹总结为“摘要+答案+改进点”，让 Critic 能基于全局信息进行精准打击（对应“引导式反思”）。\n\n*   **支柱三：决策机制的“不确定性量化”**\n    *   *思考*：在多个候选解中选择时，模型容易受到位置偏差或噪声干扰。简单的“多数投票”在科学推理中往往失效，因为真理往往掌握在少数人手中。\n    *   *方案*：引入**置信度控制的选择器**。\n        *   利用**困惑度**作为不确定性的量化指标。\n        *   使用**拉丁方阵**打乱候选答案顺序，消除位置偏差。\n        *   进行多轮迭代重选，只有当历史选择不一致时才进行最终裁决，从而将计算资源集中在模型“不确定”的难题上。\n\n#### 4. 逻辑闭环：从理论到验证\n*   **验证**：通过在 HLE、GAIA 等基准上的实验，验证了这种“基于置信度的动态编排”确实比单纯的模型缩放或固定管道更有效。特别是消融实验证明了“引导式反思”和“困惑度引导”是性能提升的关键来源。\n\n---\n\n**总结**：作者的思考路径是从**发现现有模型“知其然不知其所以然”**出发，意识到**科学推理需要深度的自我纠错机制**，进而通过**数据合成、架构解耦和不确定性量化**三位一体的设计，构建了一个能够像人类专家一样“深思熟虑”的 AI 系统。"
                },
                {
                    "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents",
                    "arxiv_id": "2602.04326",
                    "authors": "SeungWon Seo, SooBin Lim, SeongRae Noh, Haneul Kim, HyeongYeop Kang",
                    "summary": "Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“单智能体”和“多智能体”方向的高质量研究。 1.  **核心贡献符合“构建/改进 LLM 智能体” (第一步)**: *   论文的核心贡献是提出了一个名为 **PCE (Planner-Composer-Evaluator)** 的新框架。 *   这不是将现有的智能体简单应用到一个垂直领域，而是提出了一种新的方法论，旨在解决具身智能体在不确定性环境下的规划问题。它将 LLM 的推理轨迹转化为结构化的决策树，以指导动作选择。这属于对智能体“规划”能力的根本性改进。 2.  **高度契合核心关注点 (第二步)**: *   **Agentic AI & Planning**: 论文的核心焦点是智能体如何进行“不确定性感知规划”，这是 Agentic AI 的核心能力之一。 *   **Multi-Agent Context**: 虽然重点在于单个智能体的规划机制，但该研究明确针对“多智能体、部分可观察、去中心化环境”，旨在解决智能体在多智能体协作中因频繁通信导致的低效问题。这直接关联到多智能体系统中的协作与通信效率。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐或水印问题。 *   虽然涉及“具身智能体”和“隐藏物体”（可能涉及视觉），但视觉仅作为智能体感知环境的一部分，论文的研究重点并非视觉模型本身，而是基于感知信息的规划框架构建，因此符合“作为感知工具”的例外情况。 4.  **符合特殊情况处理 (第四步)**: *   **Reasoning/Planning**: 论文不仅仅是提高 LLM 的基础推理能力（如解数学题），而是将推理转化为智能体的“规划”和“动作选择”。它通过构建决策树来处理环境假设，这完全符合关于智能体在复杂任务中进行多步推理和规划的保留标准。 综上所述，该论文提出了一种新的智能体规划框架，显著提升了智能体在复杂多智能体环境中的决策效率和成功率，属于构建和改进 LLM 智能体的核心研究。",
                    "summary2": "本文旨在解决多智能体部分可观测环境中依赖频繁通信导致的高成本问题。针对多智能体协作场景，我们提出了一种PCE框架，将LLM推理中的隐式假设转化为结构化决策树，并通过评估场景似然性、收益和成本指导行动选择。在C-WAH和TDW-MAT基准上，通过成功率、任务效率和Token使用量验证了其有效性。",
                    "summary_translation": "在多智能体、部分可观测和去中心化环境中运行的具身智能体，必须在对隐藏物体和协作者意图存在普遍不确定性的情况下进行规划和行动。将大语言模型应用于具身智能体的最新进展解决了许多长期存在的挑战，例如高层目标分解和在线适应。然而，不确定性仍然主要通过频繁的智能体间通信来缓解。这会产生大量的 Token (词元) 和时间成本，并且在涉及人类合作伙伴时，可能会干扰既定的工作流程。我们介绍了 PCE，一个 Planner-Composer-Evaluator (规划-组合-评估) 框架，它将潜在在 LLM 推理轨迹中的碎片化假设转化为结构化的决策树。内部节点编码环境假设，叶子节点映射到行动；然后根据场景可能性、目标导向收益和执行成本对每条路径进行评分，从而在没有大量通信的情况下指导理性行动选择。在两个具有挑战性的多智能体基准测试（C-WAH 和 TDW-MAT）和三种不同的 LLM 骨干网络上，PCE 在成功率和任务效率方面始终优于以通信为中心的基线，同时表现出相当的 Token (词元) 使用量。消融实验结果表明，通过扩展模型容量或推理深度获得的性能提升在应用 PCE 后依然存在，而 PCE 在容量和推理深度两个尺度上始终提高了基线性能，这证实了结构化不确定性处理与这两种扩展形式相辅相成。一项用户研究进一步表明，PCE 产生的通信模式被人类合作伙伴认为更高效且更值得信赖。总而言之，这些结果为将潜在的 LLM 假设转化为用于不确定性感知规划的可靠策略确立了一条原则性路径。",
                    "inspiration_trace": "基于对论文内容的深度分析，以下是对作者产出该文章核心思想逻辑链的系统性推演：\n\n### 1. 宏观背景与问题引入\n\n**宏观背景：**\n作者首先将视野置于**多智能体具身协作**这一宏观领域。在这个领域中，智能体需要在动态、复杂的环境中（如家庭厨房）协同工作以完成长期目标。\n\n**问题引入的逻辑链：**\n\n1.  **场景设定：** 想象两个智能体正在一起做饭。由于环境是**部分可观测**且**去中心化**的，每个智能体只能看到局部视野，无法直接看到隐藏的物体或完全知晓队友的意图。\n2.  **核心冲突：** 这种局限性导致了普遍的**不确定性**。智能体必须在不知道“物体在哪”或“队友想干什么”的情况下制定计划。\n3.  **现有方案的局限：** 现有的基于大语言模型（LLM）的智能体虽然擅长高层规划和适应，但在处理这种不确定性时，主要依赖**频繁的通信**（即不断通过对话来确认信息、验证计划）。\n4.  **痛点分析：** 这种“通信优先”的范式存在严重缺陷：\n    *   **成本高昂：** 消耗大量Token和时间。\n    *   **体验糟糕：** 当队友是人类时，喋喋不休的提问会打断工作流，造成干扰。\n5.  **关键观察：** 作者敏锐地发现，LLM在进行零样本思维链推理时，**内部其实已经隐含地生成了关于环境的假设**（例如：“厨房里可能有食物”、“队友可能已经拿走了杯子”）。\n6.  **逻辑断层：** 这些宝贵的“假设”目前是**碎片化**且**局部**的。它们被隐式地引用，没有被显式地聚合起来进行全局决策。这导致智能体无法系统地权衡不同的可能性，只能通过外部通信来弥补。\n\n---\n\n### 2. 研究问题\n\n基于上述背景与痛点，作者试图回答的核心问题是：\n\n**如何将LLM推理过程中隐含的、碎片化的假设，转化为显式的结构化决策框架，从而使具身智能体能够在不依赖高频通信的情况下，实现感知不确定性的理性规划？**\n\n---\n\n### 3. 思想演进与逻辑推演\n\n从观察到方法论的提出，作者的思考过程经历了以下三个关键阶段：\n\n#### 第一阶段：从“外部求助”转向“内省挖掘”\n*   **思考起点：** 既然频繁通信（外部求助）成本太高且体验差，而LLM在推理时其实已经在“猜”环境状态（内省），为什么我们不直接利用这些“猜测”？\n*   **逻辑推演：** LLM生成的推理痕迹中包含了大量关于未观测环境的隐式假设。这些假设实际上代表了智能体对世界状态的不同可能性的预测。如果能把这些“潜台词”提取出来，我们就不需要每次都去问队友，而是可以基于自己的预测先行动。\n\n#### 第二阶段：从“碎片化思维”转向“结构化表征”\n*   **思考起点：** LLM的假设是散落在文本里的，缺乏结构。如果只是提取出来，可能是一堆矛盾的句子（比如“苹果在厨房”和“苹果在客厅”同时存在）。如何管理这些矛盾？\n*   **逻辑推演：** 我们需要一个结构来组织这些假设。**决策树**是一个完美的隐喻。\n    *   **节点：** 每一个内部节点代表一个环境假设（如“物体在厨房吗？”）。\n    *   **分支：** 是/否，代表假设成立与否。\n    *   **路径：** 从根到叶的一条路径代表一种特定的“世界场景”。\n    *   **叶子：** 在该场景下应采取的行动。\n*   **核心突破：** 这样，原本混乱的推理就变成了一个清晰的“场景树”。智能体不再是在单一假设下行动，而是在面对一棵包含了多种可能性的树。\n\n#### 第三阶段：从“盲目行动”转向“理性评估”\n*   **思考起点：** 有了树，智能体还是面临选择：走哪条路？是直接去厨房（基于假设），还是先发消息问队友（通信）？\n*   **逻辑推演：** 我们需要一个评估机制来给每条路径打分。这个评分不能只看“可能性”，还要看“收益”和“成本”。\n    *   **可能性：** 这个假设发生的概率有多大？\n    *   **收益：** 如果假设成立，这个行动对目标的贡献有多大？\n    *   **成本：** 执行这个行动（包括移动或通信）要花多少代价？\n*   **最终范式转变：** 通信不再是规划的前提，而是被降级为一种**原子行动**，与其他物理行动（如移动、抓取）放在同一个评价体系中竞争。只有当通信的“期望效用”高于直接行动时，智能体才会选择说话。\n\n---\n\n### 4. 总结：方法论的形成\n\n通过上述逻辑链，作者最终构建了 **PCE (Planner-Composer-Evaluator)** 框架：\n\n1.  **Planner (规划者)：** 负责生成原始推理，提供“原材料”（隐含假设）。\n2.  **Composer (作曲家)：** 负责将原材料“结构化”，构建出包含不同假设分支的决策树。\n3.  **Evaluator (评估者)：** 负责理性计算，基于概率、收益和成本对树上的每条路径进行评分，选出最优行动。\n\n这一过程完整地实现了从**“依赖外部通信消除不确定性”**到**“利用内部结构化推理管理不确定性”**的范式转移。"
                },
                {
                    "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
                    "arxiv_id": "2602.04248",
                    "authors": "Hao Lu, Haoyuan Huang, Yulin Zhou, Chen Li, Ningxin Zhu",
                    "summary": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： *   论文标题和摘要明确提出了 **\"Continuous Agent Evolution\"（连续智能体演化）** 的概念，这直接对应了研究焦点中的“自我演化”方向。 *   论文构建了一个名为 **\"Empirical-MCTS\"** 的新框架，旨在解决当前MCTS方法“无状态”的问题，通过经验积累实现智能体的持续学习和迭代。 2.  **包含关键的智能体能力与演化机制**： *   **自我演化机制**：论文引入了 **\"Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)\"**，这是一种能够利用反馈实时演化元提示词的机制，属于典型的自我完善和迭代优化。 *   **记忆与反思**：论文提出了 **\"Memory Optimization Agent\"**，负责管理全局存储库并提炼高质量洞察，这直接对应了筛选标准中的“记忆”和“自我反思”能力。 *   **规划与工具使用**：虽然基于MCTS（通常用于推理），但该论文将其扩展为一个包含记忆和反思的Agentic框架，而非单纯的算法改进。 3.  **不属于排除项**： *   **非特定领域应用**：论文虽然在AIME、ARC-AGI等基准上测试，但其核心贡献是通用的演化框架，而非将LLM应用于生物、医疗等特定垂直领域。 *   **非基础推理或基础设施**：虽然涉及推理，但其重点在于智能体如何通过经验“演化”其策略，而非仅仅提升模型底层的Token预测能力或数学逻辑。 *   **不涉及安全、多模态或图技术**：论文内容未触及安全对齐、视觉或多模态核心等排除领域。 综上所述，该论文的核心在于构建一种能让LLM智能体通过经验自我演化的新框架，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决当前MCTS推理策略缺乏状态、无法积累经验的问题。针对复杂推理任务，我们提出了一种名为Empirical-MCTS的双循环框架，通过Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)和Memory Optimization Agent实现非参数在线学习。我们在AIME25、ARC-AGI-2和MathArena Apex上通过准确率和成本效率验证了其有效性，显著优于无状态基线。",
                    "summary_translation": "推理时扩展策略，特别是蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)，显著增强了大型语言模型 (Large Language Models, LLMs) 的推理能力。然而，当前的方法仍然主要是无状态的，在每次处理完问题实例后即丢弃成功的推理模式，未能模仿人类解决问题所特有的经验智慧积累。为了弥合这一差距，我们提出了 Empirical-MCTS，这是一个将无状态搜索转化为连续的非参数学习过程的双循环框架。该框架通过两种新颖机制统一了局部探索与全局记忆优化：成对经验进化元提示 (Pairwise-Experience-Evolutionary Meta-Prompting, PE-EMP) 和记忆优化智能体。PE-EMP 作为局部搜索中的反思性优化器，利用成对反馈动态合成自适应标准，并实时进化元提示（系统提示）。同时，记忆优化智能体将全局存储库作为动态策略先验进行管理，利用原子操作提炼跨问题的高质量见解。在包括 AIME25、ARC-AGI-2 和 MathArena Apex 在内的复杂推理基准上进行的广泛评估表明，Empirical-MCTS 显著优于无状态 MCTS 策略和独立的经验驱动智能体。这些结果突显了将结构化搜索与经验积累相结合，对于掌握复杂的开放式推理任务的关键必要性。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《Empirical-MCTS》一文核心思想的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从“现有优势”到“核心缺陷”再到“人类类比”的叙事逻辑，具体如下：\n\n1.  **现状与优势：**\n    大型语言模型（LLMs）通过“推理时计算扩展”策略（如 MCTS、Best-of-N）显著提升了推理能力。这意味着，通过在推理阶段投入更多算力进行探索和自我修正，模型可以在不更新参数的情况下获得性能提升。\n\n2.  **核心缺陷：**\n    然而，这些当前最先进的方法存在一个致命的局限性——它们是**无状态**的。无论是标准的 MCTS 还是自适应分支策略，智能体都将每一个新问题视为孤立事件。一旦搜索过程结束，那些在解题过程中发现的成功策略或有效的推理模式就会被立即丢弃。\n\n3.  **人类类比：**\n    这种“用完即弃”的模式与人类解决问题的方式截然不同。人类的专家解题是**经验主义**的：专家会结合长期经验（积累的领域知识）和短期经验（当前问题中的即时反馈和上下文）来解决问题。\n\n4.  **现有方案的不足：**\n    现有的尝试（如 FLEX）虽然维护了经验库，但将“检索”和“推理”割裂为两个步骤，无法动态进化搜索策略；而另一些方法（如 Training-Free GRPO）虽然利用历史数据调整生成，但缺乏树搜索提供的结构化探索能力，难以处理复杂的多步逻辑任务。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**如何将无状态的树搜索转化为一个连续的、非参数的学习过程，使其能够像人类一样通过积累短期和长期经验来持续进化推理策略，而无需进行昂贵的模型权重更新？**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n从观察到假设再到方法论的完整思考过程如下：\n\n#### 1. 宏观观察与痛点识别\n*   **观察：** 现有的 MCTS 方法虽然能通过树结构探索多条路径，但每次搜索都是从零开始，无法“记住”上次成功的经验。\n*   **痛点：** 这种“遗忘”导致了计算资源的浪费和推理效率的天花板。模型无法跨问题复用已验证的“思维模式”。\n\n#### 2. 借鉴与假设\n*   **借鉴：** 人类解决问题依靠双重经验——**短期经验**（针对当前问题的即时反思）和**长期经验**（跨问题的通用智慧）。\n*   **假设：** 如果能构建一个“双循环”机制，在 MCTS 的局部搜索中利用短期经验动态调整策略，同时在全局层面利用长期经验优化先验知识，就能将 MCTS 从单纯的“搜索算法”转变为“在线学习智能体”。\n\n#### 3. 机制设计：局部循环（短期经验）\n*   **思考：** 在传统的 MCTS 扩展节点时，通常使用静态的 Prompt。如何引入“短期经验”？\n*   **方案：** 引入**反思式优化器**。不要只让模型生成答案，而是让它对“候选答案”和“基线答案”进行成对比较。\n*   **演进：** 借鉴 SPCT（Self-Principled Critique Tuning）的思想，让模型在比较中生成“自适应标准”和“自我原则”。利用这些反馈，实时进化当前的 Meta-Prompt（系统提示词）。这样，Prompt 就不再是死的指令，而是随着搜索过程不断进化的策略。\n\n#### 4. 机制设计：全局循环（长期经验）\n*   **思考：** 局部循环进化了 Prompt，但问题解决后，这些智慧如何保存下来供下一个问题使用？传统的 RAG 只是静态检索，不够智能。\n*   **方案：** 将经验库视为一个**动态策略先验**。\n*   **演进：** 借鉴 Training-Free GRPO 的思想，设计一个**记忆优化代理**。它不存储原始文本，而是执行原子操作（增加、修改、合并、删除）。这就像是在非参数空间里做“梯度下降”，不断提炼高质量洞察，剔除低效经验。\n\n#### 5. 整合与闭环\n*   **思考：** 局部循环产生的“新洞察”如何进入全局循环？全局循环的“旧经验”如何指导局部循环？\n*   **整合：**\n    *   **输入端：** 全局记忆库根据任务类型检索相关经验，作为局部搜索的先验知识（$E_{prior}$）。\n    *   **输出端：** 局部 PE-EMP 模块在比较中提炼出新的高阶洞察（$E_{new}$），提交给记忆优化代理进行原子操作更新。\n*   **结果：** 形成了一个连续进化的系统——每解决一个问题，智能体的全局策略就优化一次；每搜索一步，智能体的局部策略就调整一次。\n\n#### 6. 价值评估与修正\n*   **思考：** 这种进化如何量化为搜索树中的价值信号？\n*   **方案：** 结合 PE-EMP 产生的显式分数（局部评估）和 Enhanced Borda Count（全局排序），构建混合奖励模型。这确保了进化的方向不仅基于当前的优劣，也符合全局历史的一致性。\n\n---\n\n**总结：**\n作者的思考路径是从**“MCTS 的无状态浪费”**出发，通过**“人类经验主义”**的类比，提出了**“双循环进化”**的假设。最终，通过将**Prompt 进化（短期）**与**记忆库原子操作（长期）**深度耦合，成功将推理时的搜索过程转化为一个无需参数更新的持续学习过程。"
                },
                {
                    "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
                    "arxiv_id": "2602.04089",
                    "authors": "Xiaofeng Lin, Sirou Zhu, Yilei Chen, Mingyu Chen, Hejian Sang, Ioannis Paschalidis, Zhipeng Wang, Aldo Pacchiano, Xuezhou Zhang",
                    "summary": "Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文提出了 **ORBIT**，这是一个多任务、多回合的**元强化学习**框架。其核心目标是训练LLM在上下文中从交互经验中学习。这直接对应了研究焦点中的 **“自我演化”**，即智能体通过经验、反思或环境反馈进行自我完善和迭代。同时，该研究关注智能体在在线决策任务中的表现，涉及信息收集和利用，属于 **“单智能体”** 中的规划与决策能力范畴。 2.  **属于方法论创新而非单纯应用**： 根据第一步筛选标准，论文并非将现有智能体框架简单应用于生物、金融等特定领域，而是提出了一种新的训练框架来解决LLM在在线学习和决策任务中的局限性。这是一种构建和改进LLM智能体基础能力的贡献。 3.  **符合正面指标**： 论文涉及的核心范式包括 `Agentic AI` 和 `Self-Evolving`（通过在线学习适应环境）。提到的能力包括 `Planning`（在线决策制定）和 `Iterative Improvement`（多回合学习）。 4.  **未触犯排除标准**： 论文不涉及安全对齐、多模态视觉或图技术，也不属于基础设施优化。 综上所述，该论文通过元强化学习机制赋予LLM更强的在线学习和适应能力，是关于LLM智能体自我演化和决策能力构建的重要研究。",
                    "summary2": "本文旨在赋予LLM通用的上下文在线学习能力。针对需要跨回合交互和探索的在线决策场景，我们提出了一种名为ORBIT的多任务、多回合Meta-RL框架，通过最大化跨回合长期奖励来训练模型。我们在Maze和Mastermind等未见过的测试环境中，通过成功率验证了其有效性，结果显示小模型性能匹配GPT-5.2且显著优于标准RL微调。",
                    "summary_translation": "大语言模型在所有任务相关信息都预先可用的情况下（例如静态预测和指令遵循问题）表现出优异的性能。然而，许多现实世界的决策任务本质上是在线的：关键信息必须通过交互获取，反馈具有延迟性，且有效的行为需要在随时间推移的过程中平衡信息收集与利用。尽管上下文学习能够在不更新权重的情况下实现适应，但现有的LLM往往难以在此类设置中可靠地利用上下文交互经验。在这项工作中，我们表明这一局限性可以通过训练来克服。我们介绍了ORBIT，这是一个多任务、多回合的元强化学习框架，旨在训练LLM在上下文中从交互中进行学习。经过元训练后，一个相对较小的开源模型（Qwen3-14B）在完全未见过的环境中展现出显著提升的上下文在线学习能力，其性能与GPT-5.2相当，并大幅优于标准的强化学习微调。扩展实验进一步揭示了随着模型尺寸增大而带来的持续性能提升，这表明在推理时学习的决策代理仍有巨大的发展潜力。复现本文结果的代码可在 https://github.com/XiaofengLin7/ORBIT 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Active Epistemic Control for Query-Efficient Verified Planning",
                    "arxiv_id": "2602.03974",
                    "authors": "Shuhui Qu",
                    "summary": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了一种名为 **Active Epistemic Control (AEC)** 的规划层，这是一种用于改进 LLM 智能体在交互环境中进行决策的新框架。 *   它不是将现有智能体简单应用于特定领域（如医疗或金融），而是针对智能体在“部分可观测性”和“交互成本”这两个通用痛点上提出了底层的架构改进。 *   它不属于基础设施优化，也不是单纯的模型推理能力提升（如数学题求解），而是专注于智能体如何与环境交互、如何管理信念以及如何制定行动计划。 2.  **正面指标匹配（第二步）：** *   **Agentic AI & Planning**: 论文明确聚焦于智能体的规划能力，特别是在交互环境下的 Verified Planning。 *   **Tool Use / Interaction**: 论文核心机制涉及智能体决定何时“查询环境”来获取信息，这属于智能体工具使用和环境交互的高级形式。 *   **Memory**: 论文区分了“落地事实存储”和“信念存储”，这是对智能体记忆机制的精细化改进。 3.  **特殊情况处理（第四步）：** *   **推理/规划**: 根据规则，这篇论文属于“保留”类别。它研究的是智能体如何在复杂任务中进行多步推理和规划，特别是如何处理不确定性和环境交互，这与单纯的 CoT 变体有本质区别，属于典型的 Agentic Planning 范畴。 **总结：** 该论文提出了一种新的智能体规划架构（AEC），旨在通过优化信念管理和环境交互策略来提升 LLM 智能体的效率和成功率。这直接对应了研究课题中“单智能体”方向下的“规划”和“工具使用”子方向，因此符合筛选要求。",
                    "summary2": "本文旨在解决部分可观察环境下规划中验证成本高与模型预测易出错的权衡问题。针对ALFWorld和ScienceWorld等交互环境，我们提出了一种Active Epistemic Control (AEC)框架，通过分离grounded fact store与belief store，利用不确定性引导决策何时查询或模拟，并仅基于grounded evidence进行计划承诺。实验在ALFWorld和ScienceWorld上通过Success rate和Replanning rounds验证了其有效性，在保持高成功率的同时显著减少了重规划轮数。",
                    "summary_translation": "在部分可观测性下的交互环境中进行规划是一项挑战：在决策时刻，任务关键的前提条件（例如物体位置或容器状态）可能是未知的，然而通过交互来对其进行 grounding (接地/验证) 是代价高昂的。Learned world models (习得的世界模型) 可以廉价地预测缺失的事实，但 prediction errors (预测误差) 可能会无声地导致 infeasible commitments (不可行的承诺)。我们提出了 **Active Epistemic Control (AEC)**（主动认知控制），这是一个 epistemic-categorical planning layer (认知-分类规划层)，它集成了 model-based belief management (基于模型的信念管理) 与 categorical feasibility checks (分类可行性检查)。AEC 在用于 commitment (承诺) 的 *grounded fact store* (接地事实存储库) 和仅用于 pruning candidate plans (剪枝候选计划) 的 *belief store* (信念存储库) 之间保持了严格的分离。在每一步，当 uncertainty (不确定性) 较高或 predictions (预测) 模糊时，它要么查询环境以对 unresolved predicate (未解决的谓词) 进行 grounding (接地)，要么在 confidence (信心) 充足时模拟该谓词以 filter hypotheses (过滤假设)。Final commitment (最终承诺) 由 grounded precondition coverage (接地前提条件覆盖率) 和 SQ-BCP pullback-style compatibility check (SQ-BCP 拉回式兼容性检查) 把关，因此 simulated beliefs (模拟信念) 影响 efficiency (效率) 但不能直接 certify feasibility (证明可行性)。在 ALFWorld 和 ScienceWorld 上的实验表明，AEC 在比强大的 LLM-agent baselines (LLM智能体基线) 更少的 replanning rounds (重新规划轮次) 下，实现了 competitive success (具有竞争力的成功率)。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《Active Epistemic Control for Query-Efficient Verified Planning》一文思维过程的系统性推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑链\n\n作者在引言部分通过层层递进的方式，构建了从宏观背景到具体微观痛点的叙事逻辑：\n\n1.  **宏观背景与核心矛盾**：\n    交互式环境中的规划面临“部分可观测性”的挑战。任务成功往往依赖于某些潜在的前提条件（如物体位置、容器状态），而这些条件在决策时刻往往是未知的。\n\n2.  **两难困境**：\n    为了解决未知条件，Agent 面临一个经典的权衡：\n    *   **查询环境**：通过交互来获取事实。优点是可靠，缺点是成本高昂。\n    *   **模拟预测**：利用学习到的世界模型进行预测。优点是廉价，缺点是容易出错。\n\n3.  **现有方法的局限性分析**：\n    *   **LLM Agents**：虽然生成能力强，但在关键前提条件未观测到时，经常违反环境特定的约束，导致规划失败。\n    *   **World Models (如 WKM)**：依赖参数化预测而缺乏在线验证。虽然减少了交互，但当前提条件被“幻觉”时，会导致“静默失败”，即 Agent 在不知情的情况下执行了不可行的计划。\n    *   **Neuro-symbolic Methods (如 WALL-E)**：虽然通过失败驱动的细化进行改进，但仍然依赖于规划时对“需要验证什么”和“需要假设什么”的选择。如果假设错误，就会导致额外的重规划。\n\n4.  **核心缺口**：\n    现有方法将正确性与模型质量绑定得太紧密。它们没有解决一个**结构性问题**：如何在不让预测直接证明可行性的前提下，利用预测来辅助规划？\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链，作者试图回答的核心研究问题是：\n\n**“在部分可观测的交互式规划中，如何利用廉价但易错的世界模型预测来提高效率，同时严格保证规划可行性仅基于经过验证的落地事实？”**\n\n---\n\n### 三、 思想演进与逻辑推演过程\n\n以下是从宏观观察到具体方法论的思维还原：\n\n#### 1. 观察与痛点识别：信念与事实的混淆\n*   **观察**：现有的 Agent 往往将“模型预测的内容”和“环境真实存在的内容”混为一谈。当模型预测某个门是开着的，Agent 就直接把它当作事实来规划下一步。\n*   **问题**：这种“信念泄漏”导致了错误的根源。如果预测错了，基于此生成的计划就是不可行的，且这种不可行性往往只有在执行失败后才会被发现，代价巨大。\n*   **思考**：必须建立一道防火墙，将“我想是真的（信念）”和“我知道是真的（事实）”严格区分开。\n\n#### 2. 核心假设提出：分离原则\n*   **假设**：如果我们允许预测（信念）影响“搜索效率”（即筛选候选计划），但禁止它影响“最终承诺”（即验证可行性），那么我们既能利用模型的廉价性，又能保证安全性。\n*   **推论**：我们需要两个存储库——一个只存放通过环境交互确认的**落地事实库**，另一个存放模型预测的**信念库**。\n*   **关键点**：信念库只能用来“剪枝”掉明显不行的计划，而不能用来“证明”某个计划可行。\n\n#### 3. 机制设计：主动认知控制\n*   **思考**：既然分开了，那么什么时候该去查询环境（更新事实库），什么时候该用模型模拟（更新信念库）？\n*   **逻辑**：这取决于“不确定性”。\n    *   当模型对某个前提条件预测得很模糊（不确定性高）或者预测结果模棱两可（接近决策边界）时，说明模型不可靠，此时必须**查询**环境，将信念转化为事实。\n    *   当模型预测非常确信时，我们可以暂时**模拟**，用这个信念去过滤掉那些与该信念冲突的候选计划，从而缩小搜索空间。\n\n#### 4. 安全保障：验证门控\n*   **思考**：即使经过了信念剪枝，剩下的计划也不能直接执行，因为它们可能仍基于未被验证的假设。\n*   **解决方案**：引入一个**验证器**。这个验证器是“排他性”的——它只看落地事实库。\n*   **操作化**：只有当一个计划的所有前提条件都能在落地事实库中找到（或者通过合理的逻辑推导出来），并且通过了范畴论的一致性检查（SQ-BCP）时，Agent 才真正“承诺”执行该计划。\n\n#### 5. 逻辑闭环与理论保证\n*   **总结**：通过这种设计，形成了一个闭环——\n    *   **模拟**影响的是“哪个计划能活下来”（效率）；\n    *   **验证**决定的是“哪个计划能被执行”（可行性）。\n*   **理论支撑**：因为验证器不依赖信念库，所以即使世界模型预测全是错的，最坏的情况也就是 Agent 一直查询环境，或者找不到计划而失败，绝不会执行一个不可行的计划。这保证了错误只影响效率，不影响安全性。\n\n---\n\n**总结**：作者的思考路径是从**“混合使用预测与事实导致的不稳定性”**出发，通过引入**“认识论上的分离”**（Epistemic Separation），利用**“不确定性”**作为控制信号，最终构建了一个**“验证门控”**的规划框架，从而在效率和安全性之间找到了最优的平衡点。"
                },
                {
                    "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
                    "arxiv_id": "2602.03955",
                    "authors": "Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar, Xiaoxiao Li, Weijie Xu, Xin Chen, Jindong Wang",
                    "summary": "While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的论文。具体判断依据如下： 1.  **核心判断（符合）**： *   论文的核心贡献是提出了 **AgentArk** 这一新框架，旨在解决多智能体系统（Multi-Agent Systems）计算成本高和错误传播的问题。 *   它的方法论是将“多智能体动态”蒸馏到单个模型的权重中，这属于 **构建和改进 LLM 智能体** 的范畴。它不仅涉及多智能体系统，还致力于提升单智能体的能力，直接对应研究目标中的“单智能体”和“多智能体”方向。 2.  **正面指标（高度匹配）**： *   **核心范式**：涉及 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **智能体能力**：摘要明确提到该框架赋予了模型强大的 `Reasoning`（推理）和 `Self-Correction`（自我修正）能力，这些都是 Agentic AI 的核心能力。 *   **演化机制**：虽然论文主要使用“蒸馏”技术，但其本质是通过训练过程将多智能体的交互经验转化为单智能体的内在能力，这符合“自我完善”和“迭代改进”的逻辑，即通过经验让智能体变得更强。 3.  **排除标准（无冲突）**： *   论文并非特定领域的应用（如医疗、法律），而是一个通用的方法论框架。 *   不涉及安全、对齐、多模态视觉或图神经网络等排除主题。 *   虽然提到了“计算效率”，但其核心手段是模型蒸馏和微调，而非基础设施或硬件加速。 4.  **特殊情况处理**： *   论文关于推理能力的提升是基于“多智能体动态”的蒸馏，属于智能体框架层面的改进，而非单纯的基础 Token 预测能力提升，因此符合保留条件。 综上所述，AgentArk 通过蒸馏技术连接了多智能体与单智能体，提升了智能体的推理和自我修正能力，是对 LLM 智能体构建与改进的重要研究，因此予以保留。",
                    "summary2": "本文旨在解决多智能体系统（MAS）推理计算开销大且易传播错误的问题。针对复杂推理任务，我们提出了一种名为AgentArk的蒸馏框架，通过Reasoning-Enhanced SFT、轨迹增强和Process-Aware Distillation（PAD）三种分层策略，将多智能体动态内化至单一模型。我们在GSM8K、MATH、MedMCQA等多个数据集上通过Accuracy等指标验证了其有效性，证明蒸馏后的单智能体在保持高效推理的同时，显著提升了性能与鲁棒性。",
                    "summary_translation": "尽管 large language model (LLM) (大语言模型) multi-agent systems (多智能体系统) 通过 iterative debate (迭代辩论) 实现了卓越的推理性能，但其实际部署受到高计算成本和 error propagation (错误传播) 的限制。本文提出了 AgentArk，这是一个新颖的框架，旨在将 multi-agent dynamics (多智能体动态) 蒸馏到单个模型的 weights (权重) 中，从而有效地将 explicit test-time interactions (显式测试时交互) 转化为 implicit model capabilities (隐式模型能力)。这使得单个智能体在保持计算高效的同时，具备了 multi-agent systems (多智能体系统) 的智能。具体而言，我们研究了跨各种模型、任务、scaling (规模) 和场景的三种 hierarchical distillation strategies (分层蒸馏策略)：reasoning-enhanced fine-tuning (推理增强微调)；trajectory-based augmentation (基于轨迹的增强)；以及 process-aware distillation (过程感知蒸馏)。通过将计算负担从 inference (推理) 转移到 training (训练)，distilled models (蒸馏模型) 在保留单个智能体效率的同时，展现出了多个智能体的强大推理和 self-correction (自我纠正) 性能。它们还在各种推理任务中表现出了 enhanced robustness (增强的鲁棒性) 和 generalization (泛化能力)。我们希望这项工作能为未来关于高效且鲁棒的多智能体开发的研究提供启示。我们的代码位于 https://github.com/AIFrontierLab/AgentArk。",
                    "inspiration_trace": "基于对论文《AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个典型的“成功—困境—出路”的叙事结构：\n\n1.  **现状与成功：**\n    多智能体系统（MAS）通过辩论、批评和共识机制，在复杂推理任务上表现出色。这种多角色、多轮的对话结构能够探索多样化的假设、发现逻辑错误并迭代优化解决方案。\n\n2.  **双刃剑效应：**\n    然而，这种协作能力伴随着系统性风险，主要体现在两个方面：\n    *   **计算开销：** 依赖多角色和多轮对话导致推理延迟和计算成本急剧上升（甚至呈二次方增长），使得 MAS 在实时场景中因成本过高而难以部署。\n    *   **脆弱性放大：** 虽然 MAS 能纠正错误，但在高密度交互中，个体的偏见或幻觉会在群体中传播和放大，导致鲁棒性和安全性的集体失效。\n\n3.  **核心动机：**\n    面对这种“高性能但低效率/高风险”的矛盾，自然引出了一个根本性的挑战：如何保留 MAS 的推理优势，同时摒弃其高昂的推理成本和协作脆弱性？\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式提出的核心研究问题为：\n\n**“Can a single model internalize the reasoning benefits of MAS without their high inference-time cost and collaborative vulnerabilities?”**\n（单个模型能否在不承担多智能体系统高昂推理成本和协作脆弱性的前提下，内化其推理优势？）\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从宏观观察到具体方法论的思考过程可以还原为以下四个阶段：\n\n#### 1. 观察与诊断：为什么现有的“单模型”无法替代“多智能体”？\n*   **现象：** 现有的单模型虽然强大，但在复杂推理上不如 MAS。\n*   **归因分析：** 作者回顾了前人的尝试，发现之前的蒸馏方法往往只关注“模仿最终答案”或“浅层的交互痕迹”。\n*   **洞察：** 这种失败的原因在于，它们丢失了 MAS 推理的核心——**“冲突与修正的动态过程”**。MAS 的价值不在于“谁说了什么”（结构），而在于“如何通过辩论达成共识”（动力学）。\n*   **类比启发：** 这就像人类认知，个体可以内化群体的推理策略，独立地重现集体智慧，而不需要真的拉一群人在脑子里开会。\n\n#### 2. 假设提出：从“推理时计算”转移到“训练时计算”\n*   **核心假设：** MAS 的性能提升本质上是消耗了大量的“推理时算力”。如果将这部分算力负担**前移**到“离线学习”阶段，是否可以让单模型在训练阶段就学会这种动态推理能力？\n*   **目标设定：** 目标是让单模型在一次前向传播中，就能模拟出多智能体内部的“生成-评估-修正”的辩证推理过程。\n\n#### 3. 策略构思：如何分层内化“动态推理”？\n为了实现上述假设，作者认为不能一蹴而就，需要设计一个由浅入深的蒸馏层级：\n\n*   **第一层（结果层）：** 先学会“像多智能体一样说话”。\n    *   *思路：* 仅仅给最终答案是不够的，必须让模型学习多智能体生成的推理轨迹。\n    *   *方法雏形：* **Reasoning-Enhanced SFT (RSFT)**。不仅监督答案，还监督推理过程，确保模型能得出高质量的结论。\n\n*   **第二层（多样性层）：** 再学会“像多智能体一样多角度思考”。\n    *   *思路：* MAS 的优势在于视角的多样性。单模型往往只有一种解题思路，容易过拟合。\n    *   *方法雏形：* **Trajectory-based Data Augmentation (DA)**。从多智能体辩论中提取多种正确但路径不同的解法，强迫模型学习“条条大路通罗马”，提升鲁棒性。\n\n*   **第三层（过程层）：** 最后学会“像多智能体一样自我批判”。\n    *   *思路：* 这是最关键的一步。要让单模型在没有外部对手的情况下，自己内部产生“辩论”。这需要模型具备“每一步对不对”的判断力。\n    *   *方法雏形：* **Process-Aware Distillation (PAD)**。引入过程奖励模型（PRM）来学习每一步的逻辑正确性，再通过强化学习（GRPO）训练策略，让模型在生成过程中就能自我纠错，将外部的辩论内化为内部的思维链。\n\n#### 4. 验证与修正：什么才是蒸馏的关键？\n*   **实验反馈：** 在实验过程中，作者发现单纯增加数据量（更多轨迹）并不总是有效，甚至有害。\n*   **逻辑修正：** 这证实了“质量大于数量”的猜想。对于小模型而言，复杂的、嘈杂的多智能体输出是负担。因此，必须依赖 PRM 这种高信噪比的监督信号来筛选和引导。\n*   **最终定论：** 只有通过过程感知的蒸馏，单模型才能真正习得多智能体的“行为模式”（如步骤分解、自我检查），而不仅仅是死记硬背答案。\n\n---\n\n**总结：**\n作者的思考路径是从**“多智能体的效率痛点”**出发，通过**“动力学重于结构”**的洞察，提出了**“算力前移”**的核心假设，最终构建了一套**从模仿结果到模仿思维过程（RSFT -> DA -> PAD）**的分层蒸馏框架，成功将群体的智慧压缩进个体的模型之中。"
                },
                {
                    "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
                    "arxiv_id": "2602.03950",
                    "authors": "Aditya Basarkar, Benyamin Tabarsi, Tiffany Barnes, Dongkuan, Xu",
                    "summary": "Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献分析 (符合第一步核心判断)**: 这篇论文的核心贡献并非仅仅是将LLM应用于数学领域，而是提出了一种名为 \"Iteratively Improved Program Construction (IIPC)\" 的新方法。该方法旨在解决现有LLM智能体（特别是多智能体系统或基于程序的智能体）在推理过程中存在的“缺乏可修正的推理表示”和“程序化上下文干扰”等问题。这属于对LLM智能体内部机制的构建和改进，而非单纯的应用。 2.  **符合Agentic AI的核心特征 (符合第二步正面指标)**: *   **自我修正与反思**: 论文明确提到 \"iteratively refines programmatic reasoning chains\"（迭代优化程序化推理链）和 \"execution feedback\"（执行反馈）。这完全符合智能体通过环境反馈进行自我完善和迭代的机制，属于“自我反思”和“自我修正”的范畴。 *   **工具使用**: 该方法结合了程序执行，这是典型的工具使用能力。 *   **智能体框架**: 摘要中明确讨论了 \"Existing agents\"（现有的智能体）和 \"multi-agent LLM-based systems\"（基于多智能体的LLM系统），表明其研究背景是建立在Agentic AI之上的。 3.  **排除非Agentic推理 (符合第四步特殊规则)**: 虽然论文涉及数学推理，但它不是简单地通过微调或新的提示词来提高LLM的基础Token预测能力。相反，它构建了一个包含“执行”和“迭代优化”的Agentic循环。这种基于执行反馈的推理增强属于智能体的规划与控制范畴，而非单纯的非Agentic推理。 综上所述，该论文通过引入执行反馈和迭代优化机制，改进了LLM智能体的推理过程，属于单智能体方向中关于自我修正和工具使用的研究，完全符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决现有LLM数学推理系统缺乏可修订状态及易受程序偏差干扰的问题。针对复杂数学推理任务，我们提出了一种Iteratively Improved Program Construction (IIPC) 方法，通过迭代改进程序并结合执行反馈与双分支架构来增强推理鲁棒性。在MATH和AIME数据集上，通过准确率指标验证了其有效性，IIPC在多个主流LLM上显著优于PoT、CR和MACM等现有方法。",
                    "summary_translation": "数学问题解决是评估人工智能推理能力的基本基准，也是通往教育、科学和工程领域应用的门户，在这些领域，可靠的符号推理至关重要。尽管基于多代理大语言模型的系统在近期取得了进展并增强了其数学推理能力，但它们仍然缺乏一种可靠可修正的推理过程表征。现有的代理要么在僵化的顺序管道中运行，无法纠正先前的步骤，要么依赖启发式自我评估，这可能无法识别并修复错误。此外，程序化上下文会干扰语言模型并降低准确性。为了解决这些差距，我们引入了迭代改进程序构建，这是一种推理方法，通过迭代细化程序化推理链，并将执行反馈与基础大语言模型的原生思维链能力相结合，从而保持高层次的上下文关注。在多个基础大语言模型上，IIPC 在大多数推理基准测试中均优于竞争方法。所有代码和实现均已作为开源发布。",
                    "inspiration_trace": "基于对论文《Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation》的深入分析，以下是作者产出该核心方法（IIPC）的逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从宏观愿景到现实困境\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **宏观愿景**：数学问题解决是评估AI推理能力的基石，也是通往科学、工程等高可靠性应用（需要符号推理）的必经之路。\n2.  **现状与进步**：多智能体LLM系统虽然增强了数学推理能力，但并未触及核心痛点。\n3.  **核心困境**：现有系统缺乏一个**“可靠且可修订的推理过程表示”**。\n    *   **困境一（刚性）**：大多数系统要么是僵化的顺序流水线（无法回溯修正早期错误），导致“级联错误”；要么依赖启发式自我评估（无法准确识别错误）。\n    *   **困境二（干扰）**：程序化上下文（代码执行结果）往往会分散语言模型的注意力，导致模型过度依赖可能有缺陷的执行信号，从而产生“推理轨迹脆弱”的问题。\n4.  **现有方案的局限性**：\n    *   多智能体系统（如CR, MACM）：顺序结构锁死了推理路径，无法自由修正。\n    *   自我修正系统（如Reflexion, ToT）：受限于模型自我评估的准确性，修正不可靠。\n    *   工具/代码辅助系统（如PoT, ToRA）：代码通常是“一次性”的，缺乏针对性的迭代修订机制，且容易受到无关上下文的干扰。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一种数学推理机制，既能像操作代码一样对推理过程进行显式的迭代修订以修正早期错误，又能避免模型过度依赖可能存在缺陷的程序执行结果，从而维持高层次的逻辑语境？”**\n\n---\n\n### 三、 核心方法（IIPC）的逻辑演进链\n\n以下是从观察到方法论形成的完整思考过程：\n\n#### 1. 观察与诊断：为什么现有的“代码辅助”还不够好？\n*   **观察**：现有的Program-of-Thoughts (PoT) 或 Tool-integrated agents 虽然利用代码执行获得了确定性，但它们大多将代码视为“一次性工具”。\n*   **诊断**：如果代码写错了，这些系统往往束手无策，或者只能回退到纯文本推理。代码本身没有被当作一个可以“打磨”的对象。此外，当模型看到代码报错或错误的输出时，容易被带偏，产生“幻觉”或错误的逻辑连接。\n\n#### 2. 概念转换：将“代码”视为“推理状态”\n*   **假设**：如果我们不把代码仅仅看作计算工具，而是将其视为**推理链的显式表示**呢？\n*   **推演**：既然代码是可执行的，那么它就是可验证的。如果代码是推理的载体，那么“修正代码”就等同于“修正推理过程”。\n*   **核心思想**：与其生成一段代码就跑，不如把代码当作一个草稿。通过执行反馈，不断迭代地修改这段代码，直到它正确为止。这样，推理过程就变成了一个**可编辑的全局状态**。\n\n#### 3. 机制设计：如何实现“迭代改进”？\n*   **思考**：要实现代码的迭代，需要一个反馈循环。\n*   **设计**：\n    *   **执行反馈**：运行代码，获取输出或报错信息。\n    *   **反思记忆**：为了避免在同一个坑里跌倒两次（即“重访遗憾”），需要维护一个“错误描述符记忆”。每次修正时，都参考过去的失败经验。\n    *   **迭代循环**：生成代码 -> 执行 -> 反思（记录错误） -> 修订代码 -> 再执行。\n\n#### 4. 风险控制：如何解决“程序偏见”？\n*   **思考**：虽然代码迭代能提高准确性，但如果模型在思考过程中一直盯着错误的代码输出，它的思维会被“污染”。特别是当代码逻辑有误但能运行时，模型可能会被误导。\n*   **假设**：我们需要一个“纯净”的推理路径，不受代码噪声的干扰，只在最后时刻融合两者的信息。\n*   **设计**：采用**双分支架构**。\n    *   **分支A（程序分支）**：专注于代码生成、执行和迭代修正。这里允许混乱和试错。\n    *   **分支B（思维分支）**：保持传统的Chain-of-Thought (CoT)，不依赖代码输出，保持高层次的逻辑连贯性。\n    *   **最终融合**：只在最后一步，将“修正后的代码及执行结果”与“纯净的CoT”结合，得出最终答案。\n\n#### 5. 综合与验证：形成 IIPC\n*   **最终方法论**：将上述思考整合为 **Iteratively Improved Program Construction (IIPC)**。\n    *   它利用代码的可执行性进行**过程验证**。\n    *   利用反思记忆进行**错误规避**。\n    *   利用双分支架构进行**上下文去噪**。\n*   **预期效果**：这种方法既解决了顺序推理无法回头的刚性，又解决了纯代码推理容易被误导的脆弱性，实现了在复杂数学问题上的鲁棒性。"
                },
                {
                    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
                    "arxiv_id": "2602.04811",
                    "authors": "Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu, Maosong Sun",
                    "summary": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心论文。 1.  **核心判断（第一步）**：论文的核心贡献是提出了 SE-Bench，这是一个专门用于评估和诊断 LLM 智能体“自我演化”能力的基准测试环境。论文重点研究了智能体如何作为终身学习者将新经验内化到模型权重中，这直接对应了“自我演化”的方法论研究，而非简单的应用或基础设施。 2.  **正面指标匹配（第二步）**：论文明确涉及了核心范式 `Self-Evolving` 和 `LLM-based Agents`。在演化机制方面，它深入探讨了 `Knowledge Internalization`（知识内化）、`Self-Play`（自我博弈）以及 `Iterative Improvement`（通过 SFT 和 RL 进行迭代改进），这些都是自我演化智能体的关键能力。 3.  **特殊情况处理（第四步）**：虽然论文使用了编程任务（NumPy 库）作为具体的测试环境，但这符合第四步中关于“自我演化的应用”的例外规则。论文的焦点不在于解决编程问题本身，而在于利用这个环境来测试和改进智能体的“自我演化”机制（即如何在没有文档的情况下通过训练内化新知识）。 综上所述，该论文为 LLM 智能体的自我演化能力提供了严格的评估基准和机制洞察，是构建和演化 LLM 智能体的重要研究，因此予以保留。",
                    "summary2": "本文旨在严格衡量智能体的知识内化能力，解决现有评估中先验知识与推理复杂度纠缠的难题。针对混淆 NumPy 库构建的伪新颖包场景，我们提出了一种名为 SE-Bench 的诊断基准，通过知识混淆机制隔离内化能力。我们在 SE-Bench 数据集上通过 Pass@64 及严格的 AST 验证指标，验证了不同自我进化方法的有效性并揭示了关键机制。",
                    "summary_translation": "真正的自我进化要求智能体作为终身学习者，将新颖经验内化以解决未来的问题。然而，严格测量这一基础能力受限于两个障碍：先验知识的纠缠，即“新”知识可能出现在预训练数据中；以及推理复杂度的纠缠，即失败可能源于问题难度，而非无法回忆已学知识。我们提出了SE-Bench，这是一个诊断环境，它将NumPy库及其API doc（API文档，应用程序接口文档）混淆为一个具有随机标识符的伪新颖包。智能体被训练以内化该包，并在无法访问文档的简单编码任务上接受评估，从而构建了一个纯净的设置：在该设置下，拥有新API doc时任务轻而易举，但对于缺乏该文档的基础模型而言则无法完成。我们的研究揭示了三个见解：(1) Open-Book Paradox（开卷悖论），即使用参考文档进行训练会抑制记忆，需要通过“Closed-Book Training”（闭卷训练）强制将知识压缩到权重中；(2) RL Gap（强化学习差距），即由于PPO（Proximal Policy Optimization，近端策略优化）裁剪和负梯度的存在，标准RL（Reinforcement Learning，强化学习）无法完全内化新知识；(3) Self-Play（自我对弈）在内化中的可行性，证明了模型在结合SFT（Supervised Fine-Tuning，监督微调）而非RL时，能够从自生成的、有噪声的任务中学习。综上所述，SE-Bench建立了一个严格的诊断平台，用于评估基于知识内化的自我进化能力。我们的代码和数据集可在 https://github.com/thunlp/SE-Bench 获取。",
                    "inspiration_trace": "基于论文《SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization》的内容，以下是对作者产出该文章核心方法的逻辑链推演。\n\n---\n\n### 一、 宏观问题与背景引入\n\n**1. 愿景与现状的落差**\n作者首先从AGI（通用人工智能）的宏观愿景出发，定义了“真正的自我进化”：智能体应像人类专家一样，作为终身学习者，能够将新经验内化到自身能力中，而不仅仅是进行瞬时的、局部的适应（如推理时的自我修正）。\n\n**2. 核心矛盾的浮现**\n尽管大模型推理能力进步迅速，但社区缺乏一种严格的方法来测量这种基础的“知识内化”能力。现有的评估方法存在根本性的缺陷，无法区分智能体是“学会了”还是“本来就会”，也无法区分是“记不住”还是“想不通”。\n\n**3. 故事化的困境**\n作者通过一个类比来强化这一困境：就像一个学生背下了课本，却在考试中失败。我们无法判断是因为他没记住（记忆问题），还是因为题目太难逻辑太复杂（推理问题）。这种**“混淆”**阻碍了对自我进化能力的科学测量。\n\n---\n\n### 二、 研究问题的提炼\n\n基于上述观察，作者将宏大的愿景收敛为一个具体的、可操作的研究问题：\n\n> **研究问题：**\n> **如何构建一个严格的评估环境，以隔离并测量智能体的知识内化能力，从而彻底消除“预训练知识泄露”和“推理复杂度干扰”这两个混淆因素？**\n\n---\n\n### 三、 逻辑推演与方法论构建\n\n为了回答上述研究问题，作者展开了一系列的逻辑推演，最终形成了SE-Bench这一基准及其背后的方法论洞察。\n\n#### 1. 破局思路：构建“进化版大海捞针”\n为了解决“混淆”问题，作者认为需要一个类似“大海捞针”的测试环境。这个环境必须满足两个极端条件：\n*   **无信息则不可能：** 如果没有学习过，模型凭预训练知识绝对无法解决（消除先验知识干扰）。\n*   **有信息则平庸：** 只要掌握了新知识，任务在逻辑上极其简单（消除推理复杂度干扰）。\n\n#### 2. 具体手段：知识混淆\n为了实现上述环境，作者提出了“知识混淆”策略：\n*   **选择载体：** 选取NumPy库（逻辑简单、功能丰富）。\n*   **制造“伪新颖”：** 将NumPy的函数名映射为随机无意义的标识符（如 `numpy.mean` -> `zwc.kocito`），并重写文档。\n*   **结果：** 创造了一个在互联网上不存在的“新”库。模型必须通过学习才能掌握，一旦掌握，调用逻辑与标准NumPy无异。\n\n#### 3. 诊断实验与机制洞察\n有了SE-Bench这个干净的“显微镜”，作者开始诊断现有的自我进化方法（SFT、RL、Self-Play），并在此过程中发现了三个反直觉的洞察：\n\n*   **洞察一：开卷悖论**\n    *   *现象：* 在训练时提供参考文档（开卷），模型反而学不会；移除文档（闭卷），强迫模型依赖权重记忆，效果反而更好。\n    *   *推论：* 上下文中的信息会抑制模型将知识压缩进参数。真正的内化需要“信息饥饿”状态。\n\n*   **洞察二：RL 缺口**\n    *   *现象：* 标准强化学习（RL）在知识内化上完全失效，即使采用闭卷训练也是如此。\n    *   *推论：* RL的机制（如PPO的截断机制和负梯度）会阻止模型进行大幅度的概率更新，而学习新词汇通常需要这种剧烈变化。RL适合优化行为，但不适合吸收新事实。\n\n*   **洞察三：自我博弈的可行性**\n    *   *现象：* 之前的自我博弈方法（如Absolute-Zero）失败，作者发现原因在于使用了RL。如果改用SFT来处理模型自己生成的数据，模型是可以学会的。\n    *   *推论：* 模型具备自我生成数据并从中学习的能力，瓶颈在于优化算法的选择，而非数据质量。\n\n*   **洞察四：知识演进的分工**\n    *   *现象：* SFT先学，RL后调，效果最好。\n    *   *推论：* SFT负责“获取”，将知识存入；RL负责“巩固”，消除幻觉，使知识的应用更加鲁棒。\n\n---\n\n### 四、 总结：作者的思考路径图\n\n1.  **观察：** 现有的自我进化评估不干净，无法区分“记忆”和“推理”。\n2.  **假设：** 如果能创造一个“逻辑简单但知识全新”的环境，就能纯粹测量内化能力。\n3.  **设计：** 通过混淆NumPy构建SE-Bench，强制模型进行“从零开始”的学习。\n4.  **实验与发现：**\n    *   发现SFT需要“闭卷”才能内化（Open-Book Paradox）。\n    *   发现标准RL无法内化新知识（RL Gap）。\n    *   发现Self-Play配合SFT是可行的。\n5.  **结论：** SE-Bench不仅是基准，更是研究内化机制的探针，揭示了不同训练范式在知识获取与利用上的本质区别。"
                },
                {
                    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
                    "arxiv_id": "2602.04726",
                    "authors": "Marian Kica, Lukas Radosky, David Slivka, Karin Kubinova, Daniel Dovhun, Tomas Uhercik, Erik Bircak, Ivan Polasek",
                    "summary": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断（符合构建LLM智能体标准）**： 该论文的核心贡献不仅仅是应用LLM解决软件工程问题，而是提出了具体的 **Agentic AI 架构**。论文明确介绍了两种基于智能体的解决方案：一是用于测试场景生成的“星型拓扑”多智能体架构（包含Supervisor智能体和专门的Worker智能体）；二是用于文档检索的专用LLM智能体。这属于构建和设计LLM智能体系统（特别是多智能体系统）的范畴，符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标（符合多智能体与Agentic AI）**： 论文涉及了筛选标准中的关键正面指标： *   **核心范式**：明确提到了 `Agentic AI` 和 `Multi-Agent Systems`（通过Supervisor和Worker的协作体现）。 *   **多智能体**：描述了智能体间的协作结构（`Collaboration`），即“specialized worker agents forming a star topology with the supervisor agent”，这属于多智能体协作与组织形式的研究。 3.  **排除标准检查（无触发项）**： 论文不涉及安全与对齐、多模态视觉技术或知识图谱等排除领域。 4.  **特殊情况处理（非单纯应用）**： 虽然论文的应用场景是软件工程（特定领域），但它并未止步于“使用已有工具”，而是设计了新的智能体组织架构（Supervisor-Worker模式）来处理任务。根据筛选原则，只要核心贡献在于智能体的构建或架构设计，即使应用在特定领域，也应予以保留。 综上所述，该论文提出了新的多智能体协作架构，属于Agentic AI和多智能体系统的研究范畴，符合筛选要求。",
                    "summary2": "本文旨在解决软件工程中测试场景生成与文档检索的自动化问题。针对功能规范文档（FSD）及大量SE文档，我们提出了基于LLM的agentic AI解决方案。该方法采用星型拓扑的多智能体架构生成测试场景，并利用专用智能体处理文档搜索与问答。我们在真实世界的FSD实例及实际运营环境中验证了其有效性，成功生成了符合规范的测试文件并实现了精准的文档检索。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出该文章核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观观察与背景切入\n**思考起点：** 大语言模型的出现引发了软件工程（SE）领域的范式转移。\n*   **观察：** 过去基于规则或传统机器学习的自动化方法虽然存在，但从未像LLMs这样展现出大规模自动化的潜力。\n*   **现状：** 工业界和学术界都在竞相利用LLMs来降低成本、提升效率。\n*   **核心矛盾：** 尽管SE领域通常强调标准和结构化流程，但实际工作中充斥着大量**非结构化或半结构化的自然语言文本**（如需求描述、邮件沟通、图表），以及海量的文档库。这些“脏数据”是自动化的难点，但又是价值所在。\n\n### 2. 问题引入的逻辑链\n以下是基于Introduction部分的“讲故事”逻辑提取，展示了作者如何从宏观背景收敛到具体痛点：\n\n1.  **技术机遇：** LLMs的兴起为软件工程（SE）领域的自动化带来了前所未有的希望，尤其是针对重复性和创造性任务。\n2.  **现实挑战（SE的“ messy”本质）：** 尽管SE追求标准，但实际开发中存在大量非结构化或半结构化的自然语言（NL）文本（如需求规格说明书、非正式邮件、图表）。这些内容往往没有转化为正式规范，但在设计和实现阶段必须被考虑。\n3.  **知识获取障碍：** 软件开发生命周期（SDLC）会产生庞大的文档库。对于未参与文档创建的人员（如新员工）来说，由于缺乏“隐式上下文”，在这些文档中导航和发现关键知识极具挑战性。\n4.  **任务聚焦：** 基于上述挑战，作者锁定了两个具体且高频的痛点：\n    *   **测试场景生成：** 如何从自然语言需求描述中自动生成测试场景？\n    *   **文档检索：** 如何在复杂的SE文档集合中高效检索和处理信息？\n\n### 3. 研究问题\n基于上述逻辑链，作者试图回答的核心问题是：\n\n**“如何利用基于大语言模型的智能体技术，在真实的工业环境中，有效地解决从非结构化需求生成测试场景以及从海量软件工程文档中检索与处理信息这两个具体任务？”**\n\n---\n\n### 4. 方法论的逻辑演进\n作者从研究问题出发，通过假设与验证，逐步构建出最终的解决方案。\n\n#### 4.1 总体策略选择：从“工具”到“智能体”\n*   **思考：** 直接使用LLM（单次Prompt）处理复杂的SE任务往往效果不佳，容易产生幻觉或上下文溢出。\n*   **决策：** 采用 **Agentic AI（智能体AI）**。将复杂任务拆解，由多个具备特定角色的Agent协作完成，利用LLMs的推理能力来协调这些Agent。\n\n#### 4.2 针对任务一：测试场景生成\n*   **痛点分析：** 功能规格说明书（FSD）通常很长，直接输入LLM会超出上下文窗口；且生成的测试用例容易“凭空捏造”（幻觉），不符合FSD细节。\n*   **架构演进：**\n    1.  **拆解任务：** 需要有人读文档、有人写用例、有人检查对错、有人输出格式。\n    2.  **引入“星型拓扑”：** 设立一个核心的 **Supervisor Agent（主管智能体）** 负责全局调度，周围环绕 **Specialized Worker Agents（专用工作智能体）**。\n    3.  **解决幻觉（关键创新点）：** 单纯生成不可靠，必须引入验证机制。因此设计了 **Fact Checker Agent（事实核查智能体）**，形成闭环：如果核查不通过，Supervisor会命令Writer重写。\n    4.  **解决上下文限制：** Retriever Agent只提取相关章节，避免将整个FSD放入上下文。\n    5.  **工程化落地：** 考虑到实际交付，增加了Translator（翻译）和Excel Writer（格式化输出）。\n\n#### 4.3 针对任务二：文档检索与处理\n*   **痛点分析：** 用户对文档的需求是多样的（搜素、问答、追踪变更、长文阅读）。单一的检索模式无法满足所有场景，且文档版本众多，难以追踪演变。\n*   **架构演进：**\n    1.  **场景细分：** 识别出四种截然不同的用户意图。\n    2.  **模块化设计：** 为每种意图设计专门的Agent（Search, Q&A, Trace, Reading），而不是试图用一个通用模型解决所有问题。\n    3.  **引入“分发器”：** 设立 **Delegator Agent**，负责理解用户的自然语言请求，并将其路由给最合适的专用Agent。\n    4.  **解决长文档问题：** 对于Reading Agent，采用“分而治之”策略，分段阅读并维护笔记，最后汇总，从而突破LLM的上下文长度限制。\n    5.  **解决版本追踪：** Trace Agent专门负责跨版本、跨文档地挖掘特定需求的演变历史。\n\n### 5. 总结与反思\n*   **逻辑闭环：** 作者从LLM的潜力出发，直面SE中文档“非结构化”和“海量”的现实难题，通过引入Agentic AI架构，将复杂的单体任务拆解为多Agent协作流程。\n*   **核心思想：** **“分而治之”与“专业的人做专业的事”**。无论是测试生成中的“写-查”闭环，还是文档处理中的“意图路由”，本质上都是通过增加系统的结构化复杂度，来降低LLM处理非结构化信息的难度和错误率。"
                },
                {
                    "title": "SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing",
                    "arxiv_id": "2602.04418",
                    "authors": "Arnab Mallick, Indraveni Chebolu, Harmesh Rana",
                    "summary": "We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”方向**： 论文的核心是提出 SPEAR，这是一个**多智能体协调框架**。它不仅仅是应用现有工具，而是构建了一个包含特定角色（规划智能体、执行智能体、修复智能体）和特定交互协议（合同网协议、协商和拍卖协议）的新系统。这直接符合筛选标准第一步中的“构建……多智能体系统的方法论或新框架”。 2.  **包含核心关注点**： 论文明确涉及了多个正面指标： *   **多智能体**：明确提出了 `Multi-Agent Coordination`，涉及智能体间的 `Collaboration`（协作）、`Communication`（通信，如协商和拍卖协议）。 *   **智能体能力**：包含 `Planning`（规划智能体）、`Self-Correction`（修复智能体的自主恢复）以及 `Memory`（通过 AGM 合规的信念修订维护局部信念）。 3.  **排除标准不适用**： *   **非演化型应用**：虽然论文的应用场景是“智能合约审计”（特定领域），但论文的重点在于**多智能体系统的工程设计、协调机制和架构比较**（对比了多智能体设计与集中式/管道式设计），而不是仅仅报告审计的准确率或解决领域问题。因此，它属于方法论贡献，而非单纯的应用。 *   **安全与对齐**：论文涉及的是“智能合约审计”（软件安全），属于利用智能体解决外部安全问题，而非研究 LLM 本身的“Safety, Security, Alignment”（如防止越狱、模型对齐）。因此，不触犯安全与对齐的排除规则。 综上所述，该论文在“多智能体”框架构建和协调机制上有明确的贡献，符合“LLM智能体及其演化”中关于多智能体协作的研究范围。",
                    "summary2": "本文旨在解决智能合约审计中工具缺乏协调和自愈能力的问题。针对智能合约安全分析场景，我们提出了一种名为SPEAR的多智能体协调框架，通过Planning、Execution和Repair等智能体及Contract Net协议实现自适应审计。我们在Damn Vulnerable DeFi等数据集上通过Precision、Recall和F1-Score验证了其有效性，结果显示其优于集中式和流水线基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑链\n\n作者在引言部分通过层层递进的叙事结构，构建了从宏观背景到具体技术痛点的完整逻辑闭环：\n\n1.  **宏观背景与高赌注**\n    *   **现象**：去中心化应用（特别是DeFi）的爆发式增长，智能合约锁定了数十亿美元的价值。\n    *   **风险**：这种新模式引入了前所未有的风险，单个漏洞可能导致灾难性损失（如DAO攻击）。\n    *   **结论**：严格的安全审计是区块链生态系统信任和稳定性的基石。\n\n2.  **现有方案的瓶颈**\n    *   **主流方案**：依赖专家的手动分析。\n    *   **痛点**：这造成了“可扩展性瓶颈”——单次审计耗时2-4周，成本高达5-15万美元，无法跟上Web3的快速开发节奏。\n\n3.  **自动化工具的尝试与局限**\n    *   **尝试**：为了解决可扩展性，业界开发了基于静态分析、符号执行和LLM的自动化工具。\n    *   **三大核心缺陷**：\n        1.  **被动性**：仅扫描已知模式，缺乏项目层面的整体理解，无法进行战略导向的分析。\n        2.  **脆弱性**：生成的测试代码经常无法编译或运行，且缺乏自主恢复机制，导致频繁的人工干预。\n        3.  **缺乏协调**：工具作为独立程序运行，缺乏顶层框架来协调执行并综合输出。\n\n4.  **总结与转折**\n    *   **现状总结**：现有工具要么太慢（人工），要么太笨（自动化工具缺乏战略、自愈和协调能力）。\n    *   **引出需求**：需要一个能够像专家团队一样协作、具备战略眼光且能自我修复的自动化框架。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑链，作者试图回答的核心研究问题是：\n\n**“如何利用多智能体协调模式，构建一个具备战略规划、自主修复和资源协调能力的自动化智能合约审计框架，以克服现有手动审计的可扩展性瓶颈以及现有自动化工具在反应性、脆弱性和缺乏协调方面的局限？”**\n\n---\n\n### 三、 思想演进脉络：从观察到方法论\n\n以下是对作者构思SPEAR框架的思维过程还原：\n\n#### 1. 观察与抽象：审计的本质是什么？\n*   **观察**：传统的自动化工具把审计看作“扫描文件”，这太简单了。实际上，审计是一个动态的、资源受限的、需要多步骤协作的复杂过程。\n*   **抽象**：审计更像是一场“任务”或“行动”，而不是简单的脚本执行。它包含：\n    *   **战略层**：先审哪个合约？（风险优先）\n    *   **执行层**：用什么工具？怎么分配任务？\n    *   **恢复层**：工具报错了怎么办？生成的代码坏了怎么修？\n*   **思维跃迁**：既然是复杂的任务协作，为什么不借鉴**多智能体系统（MAS）**？MAS天然擅长处理分布式决策、局部自主性和动态协调。\n\n#### 2. 假设提出：为什么MAS比中心化控制更好？\n*   **核心假设**：在审计这种长流程、易出错的场景中，去中心化的多智能体协作比中心化的流水线更具鲁棒性。\n*   **具体理由**：\n    *   **部分可观测性**：网络分区时，本地智能体可以继续工作（如修复代码），不需要等中心服务器响应。\n    *   **动态性**：发现新漏洞时，智能体之间可以“谈判”重新分配优先级，而不需要中心控制器重算全局状态。\n    *   **异构性**：不同的工具（Slither, Mythril, LLM）需要不同的“专家”来管理。\n\n#### 3. 方法论构建：如何将审计映射为MAS？\n*   **角色设计**：将审计流程拆解为专门的智能体角色。\n    *   *Planning Agent*：负责“大脑”，基于风险评分动态调整计划。\n    *   *Execution Agent*：负责“手脚”，分配具体任务。\n    *   *Repair Agent*：负责“医生”，专门解决生成代码脆弱的问题。\n*   **机制设计**：引入经典的MAS协议来解决具体问题。\n    *   *Contract Net Protocol*：用来解决任务分配问题（谁最适合跑这个测试？）。\n    *   *Plan Negotiation*：用来解决优先级冲突（发现高危漏洞了，要不要改计划？）。\n    *   *Resource Auction*：用来解决资源竞争（LLM Token很贵，谁更值得用？）。\n*   **关键创新点**：针对“脆弱性”痛点，提出**Programmatic-First Repair Policy (PFIR)**。先试确定性修复（便宜、快），不行再试生成式修复（贵、慢），从而降低成本。\n\n#### 4. 验证策略：如何证明MAS的价值？\n*   **对比思路**：不能只证明“我的工具好”，要证明“这种架构好”。\n*   **基准设计**：\n    *   *Baseline 1*：传统流水线（无协调）。\n    *   *Baseline 2*：中心化调度器（逻辑相同，但架构是中心化的）。\n*   **实验焦点**：特意注入故障（网络超时、工具崩溃），观察SPEAR在**恢复时间**和**资源效率**上是否优于中心化架构。这直接呼应了“鲁棒性”和“自主性”的设计初衷。\n\n#### 5. 最终产出：SPEAR框架\n*   **结论**：通过将成熟的MAS模式应用于智能合约审计，SPEAR证明了显式的协调协议、局部自主性和自愈策略能够显著提升系统在失败场景下的适应性和效率。\n\n---\n\n**总结**：作者的思考路径是从**“审计效率低”**的痛点出发，洞察到**“审计是复杂协作任务”**的本质，进而引入**“多智能体”**范式，通过**“角色分工与协议设计”**解决具体技术难题，最后通过**“故障注入实验”**验证了架构的优越性。"
                },
                {
                    "title": "Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry",
                    "arxiv_id": "2602.04206",
                    "authors": "Hsien-Jyh Liao",
                    "summary": "Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴 (第一步 & 第二步)**： *   论文提出了 **Soft-FSM**，这是一种神经符号架构，旨在解决 LLM 在长时程任务中的“程序停滞”问题。 *   这属于 **单智能体** 研究中的 **规划** 和 **控制** 方向。论文的核心不在于法律知识本身，而在于如何通过外部确定性状态控制器来强制智能体在复杂任务中保持单调进展。这是一种构建和改进 LLM 智能体行为控制机制的方法论贡献。 2.  **不属于“非演化型应用”排除项 (第一步)**： *   虽然论文的应用场景是法律交叉询问，但其核心贡献并非“将 LLM 应用于法律”，而是提出了一种新的架构来保证智能体在长时程任务中的可靠性。Soft-FSM 是一种通用的智能体控制机制，而非特定领域的简单应用。 3.  **符合“推理/规划”的特殊情况 (第四步)**： *   论文关注的是智能体如何在复杂任务（长时程、显式程序约束）中进行多步推理和推进。它引入了外部状态控制来确保任务完成，这属于智能体规划能力的增强，符合“保留”关于智能体规划或多步推理框架的标准。 综上所述，该论文通过提出新的架构改进了智能体在复杂任务中的规划与执行能力，符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM在法律交叉质询中因长跨度推理导致的程序停滞问题。针对三个真实的台湾刑事杀人案件，我们提出了一种Soft-FSM神经符号架构，利用外部确定性状态控制器强制执行单调进展。实验在Gemma-3-27B-it模型上进行，通过Completeness和Redundancy等指标验证了其有效性，实现了超过97%的完整性。",
                    "summary_translation": "大语言模型 展现出令人印象深刻的语言流利度，但在明确的程序约束下，难以可靠地完成长视界任务。在法律交叉质询 中，纯概率生成通常能保持行为连贯性，但无法确保程序推进。我们将这种失效界定为程序停滞，并提出 Soft-FSM，这是一种神经符号架构，通过外部确定性状态控制器，强制执行对累积的关键信息单元 的单调进展。在三个真实台湾刑事杀人案件上的实验表明，基线方法的完整性崩溃至 40% 以下，而 Soft-FSM 始终保持 97% 以上的完整性，且冗余度近乎为零。这些结果表明，在此类领域中，仅凭 LLM 的涌现行为无法保证可靠的任务完成，而通过明确且可验证的外部状态控制则可以可靠地实现这一目标。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 3,
            "papers": [
                {
                    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
                    "arxiv_id": "2602.06025",
                    "authors": "Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang",
                    "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的“记忆”机制，属于单智能体研究范围中的核心组件（规划、记忆、工具使用、自我反思）。它提出了一个运行时记忆框架来平衡性能和成本，不属于纯应用、纯推理或基础设施优化等排除类别。",
                    "summary2": "本文旨在解决LLM Agent运行时内存提取中缺乏显式性能-成本控制的问题。针对运行时内存提取场景，我们提出了一种BudgetMem框架，通过模块化预算分层和强化学习路由策略动态分配计算资源。我们在LoCoMo、LongMemEval和HotpotQA数据集上通过F1-score、LLM-as-a-judge和Cost指标验证了其有效性。",
                    "summary_translation": "对于在超越单个上下文窗口的场景下操作的大语言模型（Large Language Model, LLM）智能体而言，记忆变得越来越重要，然而大多数现有系统依赖于离线的、与查询无关的记忆构建，这可能导致效率低下，并可能丢弃对查询至关重要的信息。尽管运行时记忆利用是一个自然的替代方案，但先前的工作往往产生巨大的开销，并且对性能-成本权衡的显式控制有限。在这项工作中，我们提出了 \\textbf{BudgetMem}，一个用于显式、感知查询的性能-成本控制的运行时智能体记忆框架。BudgetMem 将记忆处理构建为一组记忆模块，每个模块提供三个预算层级（即 \\textsc{Low}/\\textsc{Mid}/\\textsc{High}）。一个轻量级路由器在模块之间执行预算层级路由，以平衡任务性能和记忆构建成本，该路由器被实现为一个使用强化学习训练的紧凑神经策略。利用 BudgetMem 作为统一测试平台，我们研究了三种实现预算层级的互补策略：实现（方法复杂度）、推理（推理行为）和容量（模块模型大小）。在 LoCoMo、LongMemEval 和 HotpotQA 数据集上，当优先考虑性能时（即高预算设置），BudgetMem 超过了强基线，并在更严格的预算下提供了更好的精度-成本前沿。此外，我们的分析厘清了不同分层策略的优缺点，阐明了在不同预算机制下，每个维度在何时能提供最有利的权衡。",
                    "inspiration_trace": "基于论文《Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory》，以下是对作者产出该核心方法的逻辑链推演，还原了从宏观观察到具体方法论的思考过程：\n\n---\n\n### 第一阶段：宏观观察与核心矛盾\n**1. 现状观察：记忆是LLM Agent的核心，但现有范式存在缺陷。**\n*   **观察：** 现有的Agent记忆系统大多采用“离线、查询无关”的构建方式（即“一次构建，永久使用”）。\n*   **批判：** 这种方式既浪费（无论查询什么，都预先处理所有历史），又脆弱（可能会丢失针对特定查询的关键信息）。\n\n**2. 替代方案与新的痛点：从“离线”转向“运行时”。**\n*   **直觉：** 既然离线处理不灵活，那么自然的替代方案是“按需记忆提取”，即在查询到达时才根据需求处理历史记录。\n*   **新矛盾：** 这种灵活性带来了巨大的代价——运行时计算开销大、延迟高。在实际工业部署中，成本和延迟是必须被控制的一等公民。\n\n**3. 行业趋势与核心问题：引入“预算控制”的必要性。**\n*   **趋势：** 现代LLM系统（如OpenAI的推理等级、Anthropic的模型选项）已经开始提供显式的“计算档位”来平衡质量和成本。\n*   **核心问题：** 如何在**运行时**的Agent记忆提取中，实现这种显式的、可控的性能-成本权衡？\n\n---\n\n### 第二阶段：问题拆解与假设提出\n**1. 拆解问题一：预算应该应用在哪里？**\n*   **思考：** 如果把记忆系统看作一个黑盒，很难进行精细化的成本控制。\n*   **假设：** 必须将记忆提取过程**模块化**。只有将流程拆解为独立的阶段（如过滤、提取、总结），才能针对每个阶段分配不同的计算预算。\n\n**2. 拆解问题二：预算应该如何具体实现？**\n*   **思考：** 仅仅说“高预算”或“低预算”是不够的，我们需要具体的“计算旋钮”来调节质量和成本。\n*   **假设：** 存在多种正交的维度来实现这种“分级”：\n    *   **实现维度：** 用简单的规则（低成本）还是复杂的LLM（高质量）？\n    *   **推理维度：** 直接生成（低成本）还是思维链/反思（高质量）？\n    *   **容量维度：** 用小模型（低成本）还是大模型（高质量）？\n\n---\n\n### 第三阶段：方法论构建\n**1. 架构设计：模块化流水线与统一接口。**\n*   **决策：** 设计一个固定的模块化流水线（例如：过滤 -> 并行提取[实体/时间/主题] -> 总结）。\n*   **标准化：** 为每个模块定义统一的接口，使得每个模块都能在“低/中/高”三个预算档位下运行，且保持输入输出格式一致。这为后续的动态调度奠定了基础。\n\n**2. 动态调度机制：从静态规则到智能路由。**\n*   **思考：** 不同的查询对计算的需求不同。简单查询不需要高预算，复杂查询才需要。静态的规则无法适应这种变化。\n*   **决策：** 引入一个**轻量级路由器**。它根据当前的查询和中间状态，动态决定每个模块应该使用哪个预算档位。\n\n**3. 优化目标：强化学习的引入。**\n*   **思考：** 路由器的决策是一个序列决策问题（先过滤，再提取，最后总结），且涉及非可微组件（如API调用），传统的监督学习难以直接优化“成本-性能”的权衡。\n*   **决策：** 使用强化学习（RL）来训练路由器。\n*   **奖励函数设计：** 构建一个包含“任务奖励”（答案质量）和“成本奖励”（计算开销）的复合目标。通过调整权重系数，可以显式地控制系统倾向于省钱还是追求高质量。\n\n---\n\n### 第四阶段：逻辑闭环与最终产出\n**1. 综合集成：BudgetMem 的诞生。**\n*   将上述思考整合：一个模块化的运行时记忆框架 + 三种正交的预算实现策略（实现/推理/容量） + 一个基于RL训练的共享路由器。\n\n**2. 预期效果验证：**\n*   **逻辑自洽：** 这种设计不仅解决了“运行时成本高”的问题（通过按需分配），还解决了“如何控制”的问题（通过分级和路由）。\n*   **统一视角：** 通过将不同的预算策略纳入同一框架，作者还能进一步分析哪种策略在什么预算范围内性价比最高（例如：实现策略适合宽预算范围，推理策略适合微调质量）。\n\n---\n\n**总结：**\n作者的思考路径是从**“离线记忆的僵化”**出发，意识到**“运行时记忆的昂贵”**，进而借鉴**“分级计算”**的工业趋势，通过**“模块化”**拆解控制粒度，利用**“强化学习”**实现动态的查询感知调度，最终构建出一个既能保证质量又能精准控制成本的智能记忆系统。"
                },
                {
                    "title": "Codified Finite-state Machines for Role-playing",
                    "arxiv_id": "2602.05905",
                    "authors": "Letian Peng, Yupeng Hou, Kun Zhou, Jingbo Shang",
                    "summary": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了Codified Finite-State Machines (CFSMs)框架，利用LLM将角色档案转化为有限状态机以管理潜在角色状态。这属于单智能体研究中的“记忆”和“状态管理”范畴，旨在提升智能体在交互过程中的一致性和行为逻辑，符合LLM智能体的定义。",
                    "summary2": "本文旨在解决大语言模型在角色扮演中状态不稳定和行为不一致的问题。针对角色扮演中的潜在状态跟踪，我们提出了一种Codified Finite-State Machines (CFSM) 及其概率扩展CPFSM框架，利用LLM将文本档案自动编码为可执行的状态机。我们在Fandom Benchmark及合成任务上通过NLI Score和Best@K等指标验证了其有效性，结果表明该方法显著提升了角色行为的一致性和可解释性。",
                    "summary_translation": "对潜在角色状态进行建模，对于利用大语言模型实现一致且引人入胜的角色扮演至关重要。然而，现有的基于提示的方法主要捕捉表面动作，往往无法追踪驱动交互的潜在状态。我们重新审视了有限状态机，这是一种长期以来在游戏设计中用于对状态转换进行建模的工具。尽管在规模较小且定义明确的状态空间中行之有效，但传统的手工构建、基于规则的有限状态机难以适应角色扮演的开放式语义空间。为解决这一问题，我们引入了编码有限状态机，这是一个利用基于大语言模型的编码技术，自动将文本角色档案转化为有限状态机的框架。编码有限状态机直接从档案中提取关键状态和转换，生成可解释的结构以强制执行角色一致性。为了进一步捕捉不确定性和变异性，我们将编码有限状态机扩展为编码概率有限状态机，其中状态转换被建模为状态上的概率分布。通过合成评估以及既定人工制品中的真实角色扮演场景，我们证明了编码有限状态机和编码概率有限状态机优于通用的基线模型，验证了它们不仅在结构化任务中有效，而且在开放式随机状态探索中同样有效。",
                    "inspiration_trace": "基于论文《Codified Finite-state Machines for Role-playing》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从发现问题到提出解决方案的完整思考过程：\n\n### 第一阶段：问题锚定——从“表面对话”到“隐性状态”的缺失\n**（观察与痛点分析）**\n\n1.  **宏观观察**：作者首先审视了当前基于大语言模型（LLM）的角色扮演（RP）现状。虽然LLM能生成流畅的对话，但在长程叙事中，角色往往会出现“人格解体”或行为不一致。\n2.  **核心痛点识别**：作者意识到，问题的根源在于**隐性状态**的缺失。\n    *   现有的Prompting方法主要关注“表面动作”（如说什么、做什么），却忽略了驱动这些动作的内部状态（如马里奥是“小马里奥”还是“超级马里奥”）。\n    *   LLM的上下文窗口有限，无法在长对话中始终维持对复杂状态的隐性记忆，导致状态遗忘或逻辑断裂。\n3.  **初步假设**：要实现高质量的角色扮演，必须显式地建模和追踪角色的内部状态，而不仅仅是生成文本。\n\n### 第二阶段：跨界借鉴——引入有限状态机（FSM）的利与弊\n**（工具引入与局限性分析）**\n\n1.  **寻找工具**：作者将目光投向了游戏设计领域。在游戏中，角色的行为逻辑通常由**有限状态机（FSM）**控制。\n2.  **FSM的优势**：FSM能提供显式的状态定义和可解释的转换规则，保证了逻辑的确定性和一致性，完美解决了LLM“状态不稳定”的问题。\n3.  **遭遇瓶颈**：然而，传统的FSM是**硬编码**的。\n    *   在开放式的文本RP中，语义空间无限大，无法像游戏代码那样预先穷举所有状态和规则。\n    *   手动编写规则成本过高，且缺乏灵活性，无法适应文本的模糊性。\n4.  **思考转折**：如何保留FSM的结构化优势，同时消除其刚性？需要一种能自动适应文本语义的FSM构建方式。\n\n### 第三阶段：核心突破——“编码化”思想的诞生\n**（方法论创新：从规则到代码）**\n\n1.  **利用LLM的新能力**：作者注意到LLM不仅会写文章，还具备强大的**代码生成**能力。\n2.  **核心假设**：既然LLM理解自然语言，也理解代码逻辑，那么能否让LLM充当“编译器”，将文本形式的人物设定**自动翻译**成可执行的FSM代码？\n3.  **提出CFSM（Codified FSM）**：\n    *   **状态提取**：利用LLM从人物Profile中提取关键状态（如“未激活”、“SOS团员”等）。\n    *   **逻辑编码**：利用LLM编写状态转换函数（`get_next_state`）。\n    *   **语义桥接**：为了解决代码处理文本的困难，作者设计了一个关键的中间层——**二元问题函数**（`binary_question`）。代码不直接处理复杂文本，而是调用该函数询问LLM“是否发生了某事”，从而将语义判断转化为布尔逻辑。\n4.  **逻辑闭环**：通过这种方式，作者将模糊的文本约束转化为了严谨的、可执行的代码逻辑，实现了“结构化的状态”与“开放式的语义”的结合。\n\n### 第四阶段：精细化处理——从确定性到概率性\n**（应对现实世界的复杂性）**\n\n1.  **进一步观察**：在真实的RP场景中，角色的状态转换往往不是非黑即白的。例如，一个角色可能“有点生气”但“还在克制”，存在多种可能性的分支。\n2.  **提出CPFSM（Codified Probabilistic FSM）**：\n    *   为了捕捉这种不确定性，作者将确定性的状态转换扩展为**概率分布**。\n    *   利用`binary_question`函数输出的Logits（置信度），构建状态转移概率矩阵。\n    *   这使得模型不仅能判断“发生了什么”，还能量化“发生的可能性”，从而支持更细腻、更多样化的角色反应。\n\n### 第五阶段：验证与反思——结构优于纯推理\n**（逻辑验证与价值确认）**\n\n1.  **合成验证（思想实验）**：作者设计了马里奥、使命召唤等经典状态机场景进行测试。\n    *   **发现**：直接使用Prompt让LLM推理状态，随着步数增加，准确率急剧下降；而使用CFSM，因为逻辑被“固化”在代码中，准确率保持100%且效率极高。\n    *   **结论**：LLM本身并不擅长长程的状态追踪，必须依靠外部的显式结构（FSM）来辅助。\n2.  **真实场景验证**：在Fandom Benchmark上的实验表明，引入CFSM/CPFSM后，角色行为的一致性和与设定的符合度显著优于纯Prompt方法。\n3.  **最终思考**：作者确认了“神经符号结合”在RP中的价值——用LLM生成逻辑（符号），用代码执行逻辑（结构），从而实现了既灵活又可控的角色扮演系统。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现隐性状态缺失 -> 借鉴游戏FSM结构 -> 利用LLM编码能力解决刚性瓶颈 -> 引入概率模型处理模糊性 -> 实验验证结构化必要性”** 的完整逻辑链条。其核心创新在于将“文本规则”通过LLM转化为“可执行代码”，从而在开放域中实现了严谨的状态管理。"
                },
                {
                    "title": "Scaling Agentic Verifier for Competitive Coding",
                    "arxiv_id": "2602.04254",
                    "authors": "Zeyao Ma, Jing Zhang, Xiaokang Zhang, Jiaxi Yang, Zongmeng Zhang, Jiajun Zhang, Yuheng Jing, Lei Zhang, Hao Zheng, Wenting Zhao, Junyang Lin, Binyuan Hui",
                    "summary": "Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断**: *   论文的核心贡献是提出了 **\"Agentic Verifier\"**，这是一个基于执行的智能体框架。它不仅仅是将LLM应用于编程领域，而是构建了一个新的智能体架构来解决验证问题。 *   该智能体具备自主性，能够主动推理程序行为并搜索测试输入，符合 **Agentic AI** 的定义。 2.  **符合核心关注点**: *   **单智能体**: 论文详细描述了智能体与代码执行环境进行 **\"多轮交互\" (Multi-turn interaction)**，并利用 **\"工具使用\" (Tool Use - 代码执行)** 来完成任务。这完全符合单智能体方向中的规划、工具使用和反思机制。 *   **自我演化**: 论文明确提到了 **\"迭代改进\" (Iteratively refines)** 候选输入生成器，并使用了 **\"智能体强化学习\" (Agentic reinforcement learning)** 进行训练。这表明智能体能够通过环境反馈进行自我完善和迭代，符合自我演化的研究焦点。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然应用场景是竞技编程，但这属于对智能体能力的验证，而非单纯的垂直领域应用（如医疗或法律），且其核心在于提出新的智能体机制。 综上所述，该论文在构建具有工具使用、多步推理和自我迭代能力的LLM智能体方面做出了实质性贡献，高度契合 \"LLM智能体及其演化\" 的研究课题。",
                    "summary2": "本文旨在解决竞技编程中基于执行的验证方法因随机采样效率低下而难以有效区分候选解的问题。针对候选解的重排序任务，我们提出了一种Agentic Verifier，通过多轮交互与代码执行环境主动推理并生成高区分度的测试输入。我们在五个竞技编程基准上通过Best@k准确率验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）虽然展现了强大的编码能力，但在单次尝试中正确解决竞赛编程（Competitive Programming）问题方面仍面临挑战。基于执行的重排序（Execution-based Re-ranking）提供了一种极具前景的测试时扩展（Test-time Scaling）策略，然而现有方法往往受限于测试用例（Test Case）生成困难或随机输入采样（Random Input Sampling）效率低下的问题。为解决这一局限性，我们提出了 Agentic Verifier，这是一种基于执行的智能体（Agent），能够主动对程序行为进行推理，并搜索具有高度区分性的测试输入（Test Input），从而揭示候选解决方案（Candidate Solution）之间的行为差异。通过与代码执行环境（Code Execution Environment）进行多轮交互，该验证器迭代优化候选输入生成器（Candidate Input Generator），并生成针对性的反例（Counterexample），而非盲目地进行输入采样。我们通过一个可扩展的流水线（Pipeline）来训练验证器，使其具备这种区分性输入生成能力，该流水线结合了大规模数据合成（Large-scale Data Synthesis）、拒绝微调（Rejection Fine-tuning）以及智能体强化学习（Agentic Reinforcement Learning）。在五个竞赛编程基准（Benchmark）上进行的广泛实验表明，该方法相较于强大的基于执行的基线（Baseline）展现出了一致的性能提升，在 Best@K 准确率上实现了高达 10-15% 的绝对增益。进一步的分析揭示了清晰的测试时扩展行为，并凸显了该验证器在重排序（Reranking）任务之外的更广泛潜力。",
                    "inspiration_trace": "基于论文《Scaling Agentic Verifier for Competitive Coding》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑链\n\n作者在Introduction部分构建了一个层层递进的逻辑叙事，旨在引出研究的核心动机：\n\n1.  **宏观背景与现状**：\n    大语言模型（LLMs）在通用代码任务（如合成、补全）上表现优异，但在**竞技编程**这一特定领域仍面临巨大挑战。竞技编程需要深度的算法理解、长程推理以及对极端边界情况的精确处理，即便是目前最强的LLM也难以在单次尝试中正确解决。\n\n2.  **现有策略与瓶颈**：\n    为了提升准确率，学术界通用的策略是“采样多个候选解 + 验证器重排序”。其中，基于**执行**的验证方法（即运行代码）被证明是最有效的。然而，现有的执行验证方法在竞技编程场景下存在显著缺陷：\n    *   **生成完整测试用例（输入+输出）**：这通常和解决原问题一样困难，成本高昂且不可靠。\n    *   **仅生成输入**：为了规避生成输出的困难，现有方法转向仅生成测试输入，通过运行候选解并根据输出一致性（投票）来选择最佳解。\n\n3.  **关键痛点**：\n    目前的“仅生成输入”方法主要依赖**随机采样**。作者指出，竞技编程的有效输入空间极其巨大，而真正具有**区分度**（即能区分正确与错误解）的输入只占极小一部分（通常是极端边界情况）。因此，盲目随机采样的效率极低，即使扩展到数百次执行，也难以捕捉到关键的测试用例。\n\n4.  **实验验证与动机**：\n    作者通过初步实验证实，随机生成的输入与精心设计的基准真值输入之间存在巨大的性能差距。这表明，单纯增加随机输入的数量是无效的，瓶颈在于输入的**质量**而非数量。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑链，作者显式提出了本论文旨在解决的核心研究问题：\n\n**“与其依赖盲目的随机采样，我们能否训练一个验证器，使其主动搜索出具有高度区分度的测试输入，从而最大化候选解之间的行为差异？”**\n\n---\n\n### 三、 思想演进与方法论形成\n\n为了回答上述问题，作者的思考路径经历了从“被动接受”到“主动出击”的范式转变：\n\n#### 1. 思想转折：从“随机采样”到“主动搜索”\n*   **思考**：既然随机输入很难命中“区分点”，那么验证器不应再是被动的输入生成器，而应成为一个**智能体**。\n*   **演进**：这个智能体需要具备“推理”能力，能够分析候选代码的行为，并通过多轮交互与代码执行环境进行“对话”，像调试器一样去寻找能让不同代码产生分歧的反例。\n\n#### 2. 任务定义：多轮交互式输入生成\n*   **思考**：如何让模型学会找反例？这不能是一次性的生成任务，因为代码逻辑很复杂。\n*   **演进**：将任务形式化为一个**多轮交互过程**。给定问题和一对候选解，验证器通过工具调用执行代码、观察反馈、验证约束，不断修正其对“什么样的输入能区分这两个解”的假设，最终输出一个输入生成器。\n\n#### 3. 训练策略：从模仿到强化\n*   **思考**：这种复杂的搜索和推理能力很难通过简单的指令微调获得，模型需要学会如何“试错”。\n*   **演进**：作者设计了一个三阶段的训练管道来逐步赋予模型这种能力：\n    *   **数据合成**：首先构建大规模的高质量数据集，利用强模型生成参考解和测试用例，确保训练数据的可靠性。\n    *   **拒绝微调**：利用强教师模型生成成功的交互轨迹，让模型通过监督学习模仿“如何成功找到区分性输入”的思维链。\n    *   **智能体强化学习**：为了进一步提升模型在困难样本上的表现，引入强化学习（GRPO）。通过定义一个稀疏奖励函数（成功区分得正分，无效或无区分得负分），迫使模型在交互中主动优化其搜索策略，专注于那些难以区分的“硬负样本”。\n\n#### 4. 价值升华：超越重排序\n*   **思考**：这种主动找错的能力仅仅是为了选出最好的代码吗？\n*   **演进**：作者进一步意识到，竞技编程的基准测试集本身也是“不完美的验证器”（存在假阳性）。Agentic Verifier 实际上可以作为一个**通用的 correctness-checking 工具**，通过发现基准测试遗漏的反例，来揭示那些通过基准测试但逻辑错误的解，从而弥补现有评估体系的缺陷。\n\n---\n\n### 总结\n\n作者的思考过程始于对**现有测试时扩展策略效率低下的不满**（随机输入无效），进而提出**主动搜索区分性输入**的假设，最终通过**智能体交互**和**强化学习**的技术手段，将验证器从被动的“裁判”升级为主动的“测试员”，实现了在竞技编程任务上的显著性能提升。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "Agentic AI-Empowered Dynamic Survey Framework",
                    "arxiv_id": "2602.04071",
                    "authors": "Furkan Mumcu, Lokman Bekit, Michael J. Jones, Anoop Cherian, Yasin Yilmaz",
                    "summary": "Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。 1.  **核心贡献判断 (符合)**: 论文的核心贡献是提出了一个 \"Agentic Dynamic Survey Framework\" (智能体动态综述框架)。它不仅仅是将现有的LLM作为工具应用，而是构建了一个新的智能体框架来解决“长期维护”这一特定问题。这属于“构建、改进 LLM智能体”的范畴。 2.  **符合研究焦点 (Agentic AI - 单智能体)**: *   **规划与长期任务**: 论文明确将综述写作重新定义为 \"long-horizon maintenance problem\" (长期视界维护问题)，这直接对应了单智能体方向中的 **Planning (规划)** 能力，特别是处理长期任务的能力。 *   **自我演化/迭代**: 虽然论文主要描述的是文档的演化，但其框架通过 \"incrementally integrating new work\" (增量整合新工作) 和 \"continuous updating\" (持续更新) 来实现，体现了智能体在环境反馈（新论文出现）下的迭代和适应性，符合 Agentic AI 的动态特性。 3.  **排除标准检查**: *   **非演化型应用**: 尽管应用场景是“学术综述”，但论文的重点在于提出一种新的 Agentic 框架机制（如何动态维护、如何保持结构连贯性），而不是单纯地应用LLM写一篇综述。因此不属于简单的“非演化型应用”。 *   **安全与对齐**: 不涉及。 *   **多模态**: 不涉及。 综上所述，该论文提出了一种用于长期任务规划和动态维护的 Agentic 框架，属于单智能体研究范畴，符合“LLM智能体及其演化”的核心目标。",
                    "summary2": "本文旨在解决综述论文因研究产出快速增长而迅速过时的问题，将其视为长期维护任务。针对现有综述的持续更新需求，我们提出了一种Agentic Dynamic Survey Framework，利用Analysis、Routing、Abstention和Synthesis等专用Agent实现增量集成新研究并保持结构稳定。在包含5篇计算机视觉和机器人学综述的回顾性基准测试中，通过Semantic Alignment、Local Coherence和Disruption等指标验证了其有效性，结果显示该方法在保持高质量更新的同时实现了零范围外编辑。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "On the Uncertainty of Large Language Model-Based Multi-Agent Systems",
                    "arxiv_id": "2602.04234",
                    "authors": "Yuxuan Zhao, Sijia Chen, Ningxin Su",
                    "summary": "Multi-agent systems (MAS) have emerged as a prominent paradigm for leveraging large language models (LLMs) to tackle complex tasks. However, the mechanisms governing the effectiveness of MAS built upon publicly available LLMs, specifically the underlying rationales for their success or failure, remain largely unexplored. In this paper, we revisit MAS through the perspective of uncertainty, considering both intra- and inter-agent dynamics by investigating entropy transitions during problem-solving across various topologies and six benchmark tasks. By analyzing 245 features spanning token-, trajectory-, and round-level entropy, we counterintuitively find that a single agent outperforms MAS in approximately 43.3% of cases, and that uncertainty dynamics are largely determined during the first round of interaction. Furthermore, we provide three key observations: 1) Certainty Preference: reducing uncertainty at any stage for any agent is critical for guaranteeing correct solutions; 2) Base Uncertainty: base models with lower entropy during problem-solving directly benefit MAS performance; and 3) Task Awareness: entropy dynamics of MAS play varying roles across different tasks. Building on these insights, we introduce a simple yet effective algorithm, the Entropy Judger, to select solutions from MAS's pass@k results, leading to consistent accuracy improvements across all MAS configurations and tasks. Our source code is available at https://github.com/AgenticFinLab/multiagent-entropy.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： *   论文的核心研究对象是 **Multi-Agent Systems (MAS)**，即基于大语言模型的多智能体系统。这直接对应了研究课题中的“多智能体”方向。 *   论文的核心贡献不仅在于分析了MAS的不确定性机制，还提出了一种名为 **Entropy Judger** 的算法，用于从MAS的输出中选择最优解，从而提升系统的准确性。这属于对现有LLM智能体系统的 **改进**，符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标（符合）**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体**：研究了智能体间的动态、拓扑结构以及协作效果。 *   **改进机制**：提出的算法旨在优化智能体系统的最终表现，属于对智能体能力的增强。 3.  **排除标准（未触发）**： *   **非演化型应用**：论文并非将MAS作为工具应用到生物、金融等特定领域去解决该领域问题，而是研究MAS本身的运作机制和性能优化，因此不属于应用类论文。 *   **非Agentic的推理**：论文关注的是智能体系统层面的动态和不确定性，而非单纯提升LLM底层的数学或逻辑推理能力。 *   **安全与对齐/多模态/基础设施**：论文未涉及安全对齐、视觉模态或底层硬件基础设施。 综上所述，该论文深入探讨了多智能体系统的内在机制并提出了改进算法，属于“多智能体”及“改进LLM智能体”的前沿研究，符合筛选要求。",
                    "summary2": "",
                    "summary_translation": "多智能体系统已成为利用大语言模型解决复杂任务的主要范式。然而，基于公开可用的大语言模型构建的多智能体系统，其有效性的潜在机制——特别是其成功或失败的底层原理——在很大程度上仍未被探索。在本文中，我们从不确定性的视角重新审视多智能体系统，通过研究在各种拓扑结构和六个基准任务中解决问题过程中的熵转换，考察了智能体内部及智能体间的动态变化。通过分析涵盖 token（词元）、trajectory（轨迹）和 round（轮次）级别熵的 245 个特征，我们得出了一个反直觉的发现：在约 43.3% 的情况下，单个智能体的表现优于多智能体系统；且不确定性动态在很大程度上在第一轮交互期间就已确定。此外，我们提出了三个关键观察：1) 确定性偏好：在任何阶段降低任何智能体的不确定性对于确保获得正确解至关重要；2) 基础不确定性：在解决问题过程中具有较低熵的基础模型能直接提升多智能体系统的性能；3) 任务感知：多智能体系统的熵动态在不同任务中发挥着不同的作用。基于这些见解，我们引入了一种简单而有效的算法——熵判别器，用于从多智能体系统的 pass@k 结果中筛选解决方案，从而在所有多智能体系统配置和任务中实现了准确性的持续提升。我们的源代码可在 https://github.com/AgenticFinLab/multiagent-entropy 获取。",
                    "inspiration_trace": "基于对论文《On the Uncertainty of Large Language Model-Based Multi-Agent Systems》的深入分析，以下是作者产出该文章的完整思考过程与逻辑推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 部分通过层层递进的逻辑，将读者从宏观背景引向具体的研究切入点：\n\n1.  **背景铺垫：** 多智能体系统（MAS）已成为利用大语言模型（LLM）解决复杂任务的主流范式，甚至被视为解决问题的唯一选择。\n2.  **提出冲突：** 尽管应用广泛，但 MAS（尤其是基于开源 LLM 的）是否真的优于单智能体系统（SAS）？其成功或失败的底层机制是什么？目前尚无定论。\n3.  **现状批判：** 现有文献虽然观察到 SAS 有时能匹敌甚至超越 MAS，且指出了通信中断、对齐不足等失败原因，但这些研究主要依赖于准确率、延迟等**简单指标**，未能揭示系统有效性的深层原理。\n4.  **寻找工具：** 在单智能体推理和强化学习（RL）领域，**不确定性（熵）**已被证明是理解推理过程的关键视角，能有效平衡探索与利用。\n5.  **指出缺口：** 尽管熵在单智能体研究中很重要，但目前缺乏对 LLM-based MAS 中不确定性全生命周期（包括智能体内部和智能体之间）的系统性分析。\n6.  **本文定位：** 我们通过研究熵在不同拓扑结构和任务中的转换，重新审视 MAS，旨在揭示不确定性动态与推理可靠性之间的深层关系。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“不确定性动态（特别是熵在智能体内部及交互过程中的转换）如何决定基于大语言模型的多智能体系统的有效性，以及我们能否利用这一机制来改进系统性能？”**\n\n---\n\n### 三、 作者产出核心方法的逻辑链推演\n\n以下是从宏观观察到最终方法论的完整思考演进过程：\n\n#### 1. 现象观察与质疑\n*   **观察：** 社区普遍认为“多智能体优于单智能体”，且 MAS 架构越复杂越好。\n*   **质疑：** 真的如此吗？为什么有时候 MAS 会失败？现有的评估指标（如准确率）只看结果，不看过程，无法解释“为什么”。\n*   **思考：** 我们需要打开“黑盒”，寻找一个能穿透表象、反映推理本质的指标。\n\n#### 2. 理论视角的切入\n*   **联想：** 在单智能体和强化学习中，**熵**是衡量模型不确定性的核心指标。高熵意味着困惑，低熵意味着确定。\n*   **假设：** 如果熵能解释单智能体的推理质量，那么它应该也能解释多智能体之间的交互质量。MAS 的失败可能源于“混乱”的交互（高熵）。\n*   **决策：** 将“熵”作为显微镜，去观察 MAS 的整个生命周期。\n\n#### 3. 系统性的解构与实验\n*   **定义维度：** 为了全面观察，作者不再只看最终答案，而是定义了 245 个特征，涵盖 Token 级、轨迹级、轮级等不同粒度的熵。\n*   **对比实验：** 在多个模型（LLaMA, Qwen）、多个任务（数学、代码、QA）和多种拓扑结构（中心化、辩论、顺序等）上进行大规模实验。\n*   **关键发现：**\n    *   **反直觉现象：** 单智能体（SAS）在 43.3% 的情况下竟然优于 MAS。这打破了“多即是好”的迷思。\n    *   **早期决定论：** 系统的成败在很大程度上由**第一轮**交互的熵动态决定。如果一开始就乱（高熵方差），后面很难救回来。\n    *   **熵的普适性危害：** 峰值熵（极端的不确定）在所有架构中都是有害的。\n\n#### 4. 归纳与原理提取\n*   **从数据到规律：** 基于实验数据，作者提炼出三条核心原则：\n    1.  **确定性偏好：** 任何阶段降低不确定性都与正确性正相关。\n    2.  **基础不确定性：** 基座模型本身的熵越低，MAS 的表现上限越高。\n    3.  **任务感知：** 不同任务对熵的容忍度不同（简单任务要快准狠，难任务需要适度的探索）。\n\n#### 5. 方法论的落地\n*   **思考转化：** 既然熵的动态模式可以预测 MAS 的成败，那么我们能不能训练一个“裁判”来预测结果？\n*   **构建模型：** 利用 XGBoost/LightGBM 等树模型，以提取的熵特征为输入，以“答案是否正确”为标签进行训练。\n*   **产出工具：** **Entropy Judger（熵判别器）**。\n*   **应用场景：** 在实际应用中，我们往往没有标准答案。此时，可以让 MAS 生成多个候选答案，利用 Entropy Judger 挑选出熵特征最健康（预测正确率最高）的那一个，从而实现无监督的性能提升。\n\n#### 6. 逻辑闭环\n*   **验证：** 实验证明，使用 Entropy Judger 进行 pass@k 选择，在所有配置和任务上都能一致性地提高准确率。\n*   **结论：** 通过“熵”这个视角，不仅解释了 MAS 的成败机制，还提供了一个切实可行的改进工具，完成了从理论分析到工程实践的闭环。"
                },
            ]
        },
    ],
    "2026-02-04": [
        {
            "name": "Artificial Intelligence",
            "count": 36,
            "papers": [
                {
                    "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity",
                    "arxiv_id": "2602.03794",
                    "authors": "Yingxuan Yang, Chengrui Qu, Muning Wen, Laixi Shi, Ying Wen, Weinan Zhang, Adam Wierman, Shangding Gu",
                    "summary": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心研究对象是 \"LLM-based multi-agent systems (MAS)\"，其本质是探讨如何通过引入异质性来改进和优化多智能体系统的性能。论文提出了一个信息论框架来解释智能体扩展的规律，并给出了构建高效和稳健MAS的原则性指导。这完全符合“构建、改进或演化 LLM智能体”的核心目标，属于多智能体系统的基础方法论研究，而非简单的应用或基础设施研究。 2.  **正面指标 (匹配)**: 论文明确涉及 `Multi-Agent Systems (MAS)` 范式，关注智能体系统的性能扩展。虽然它侧重于理论分析，但其结论直接指导如何通过增加多样性来改进智能体系统的构建，这与“改进”智能体的方向高度一致。 3.  **排除标准 (未触发)**: 论文不涉及安全与对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **综合结论**: 该论文深入分析了多智能体系统的扩展机制，揭示了多样性对于提升系统性能的关键作用，为构建更好的LLM多智能体系统提供了理论依据和实践指南，精准契合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在解决LLM多智能体系统单纯增加智能体数量导致收益递减的问题。针对同质化智能体输出高度相关导致性能饱和的场景，我们提出了一种基于信息论的有效通道框架，引入无标签指标$K^*$来量化非冗余信息源。在GSM8K、ARC等7个基准数据集上，通过Success Rate验证了异构配置优于同质化扩展，仅用2个多样化智能体即可达到16个同质智能体的性能。",
                    "summary_translation": "LLM-based multi-agent systems (MAS) (基于大语言模型的多智能体系统) 已成为解决单个大语言模型难以处理的复杂任务的一种有前景的方法。一种自然的策略是通过增加智能体数量来提升性能；然而，我们发现这种扩展在 homogeneous settings (同质化设置) 中表现出强烈的边际收益递减，而引入 heterogeneity (异质性)（例如，不同的模型、提示词或工具）则能持续带来显著收益。这提出了一个根本性问题：是什么限制了扩展，为什么多样性有帮助？我们提出了一个 information-theoretic framework (信息论框架)，表明 MAS 性能受限于 intrinsic task uncertainty (内在任务不确定性)，而非智能体数量。我们推导出了 architecture-agnostic bounds (架构无关界限)，证明改进取决于系统利用了多少 effective channels (有效通道)。Homogeneous agents (同质智能体) 早期饱和是因为它们的输出强相关，而 heterogeneous agents (异质智能体) 提供互补证据。我们进一步介绍了 $K^*$，这是一种 effective channel count (有效通道数)，可以在没有 ground-truth labels (真实标签) 的情况下量化有效通道的数量。实验结果表明，heterogeneous configurations (异质配置) 始终优于 homogeneous scaling (同质扩展)：2 个多样化智能体的性能可以匹配或超过 16 个同质智能体。我们的结果为通过 diversity-aware design (具有多样性意识的设计) 构建高效且鲁棒的 MAS 提供了基于原理的指导。代码和数据集可在以下链接获取：https://github.com/SafeRL-Lab/Agent-Scaling。",
                    "inspiration_trace": "基于对论文《Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity》的深入分析，以下是作者产出该核心方法的逻辑链推演。这一过程展现了从宏观直觉、实证反常、理论抽象到方法论落地的完整思考路径。\n\n---\n\n### 1. 宏观背景与朴素直觉\n**思考起点：** 单个大语言模型（LLM）在处理复杂任务（如多步推理、跨领域知识）时存在能力瓶颈。\n**现有方案：** 多智能体系统（MAS）通过协作被证明是有效的。\n**朴素直觉：** 既然单个智能体有效，那么通过增加智能体数量来扩展系统规模，理应像集成学习一样带来持续的性能提升。即“更多智能体 = 更强性能”。\n\n### 2. 现象观察与冲突发现\n**实证观察：** 作者进行了大规模实验，试图通过增加同质化智能体数量来提升性能。\n**反常现象：**\n*   **同质化边际递减：** 当增加配置完全相同（同模型、同提示词）的智能体时，性能在初期略有提升后迅速饱和，边际收益趋近于零。单纯堆砌计算量（增加Agent数量）并未带来预期的线性增长。\n*   **异质化的持续收益：** 相比之下，引入异构性（如混合不同模型、不同提示词或不同工具）的系统，即使智能体数量较少，也能带来显著的性能提升，且收益更持久。\n\n**逻辑推演：** 这表明“智能体数量（$N$）”并不是决定性能上限的核心变量。单纯增加同质化Agent只是在重复冗余的推理路径，而非引入新的有效信息。\n\n### 3. 核心假设与问题提出\n**假设形成：** 性能瓶颈源于智能体输出之间的**相关性**。\n*   同质化Agent输出高度相关，导致信息冗余。\n*   异质化Agent输出互补，提供了独立的证据。\n\n**Introduction 中的“讲故事”逻辑提取：**\n1.  **背景：** LLM在复杂任务上受限，MAS成为解决方案。\n2.  **直觉：** 自然想到通过增加Agent数量来扩展性能。\n3.  **现实：** 实验发现同质化扩展存在强烈的边际递减效应，而引入异构性（多样性）能持续获益。\n4.  **困惑：** 这种现象背后的根本机制是什么？是什么限制了扩展？为什么多样性有效？\n\n**显式总结研究问题：**\n**What fundamentally limits scaling in LLM-based multi-agent systems, and why does introducing heterogeneity yield substantial gains while homogeneous scaling exhibits diminishing returns?**\n（是什么从根本上限制了LLM多智能体系统的扩展，为什么引入异构性能带来显著收益，而同质化扩展却表现出边际递减？）\n\n### 4. 理论抽象与机制解释\n**思维跃迁：** 从“计算量视角”转向“信息论视角”。\n*   **重新定义目标：** MAS的目标不是增加Agent数量，而是从输入 $X$ 中提取关于答案 $Y$ 的最大信息量。\n*   **引入核心概念：** **可用证据**。\n    *   系统性能受限于**任务内在的不确定性** $H(Y|X)$，而非Agent数量 $N$。\n    *   同质化Agent因为输出高度相关，新增的Agent调用无法提供新的互信息，导致 $\\Delta I \\to 0$。\n*   **提出新指标：** **有效通道数**。\n    *   真正决定性能的是独立的、非冗余的推理路径数量，而不是原始的Agent调用次数。\n    *   两个思考方式完全一致的Agent只算作1个有效通道；两个思考路径互补的Agent算作2个。\n\n### 5. 方法论构建与验证\n**落地挑战：** 如何在没有真实标签的情况下，量化“有效通道数”？\n**解决方案：** 引入 $K^*$ 指标。\n*   **思路：** 利用嵌入空间中的几何结构来代理信息论的多样性。\n*   **机制：** 计算Agent输出的余弦相似度矩阵，通过矩阵的特征值熵来估计有效通道数。\n    *   如果所有输出相似（同质化），$K^* \\approx 1$。\n    *   如果输出覆盖多个独立方向（异质化），$K^*$ 增大。\n*   **验证逻辑：** 证明 $K^*$ 与任务准确率强相关，且异构配置能以更少的Agent数量达到更高的 $K^*$，从而解释了“2个多样化Agent > 16个同质化Agent”的实验结果。\n\n---\n\n### 总结：作者的思想演进脉络\n1.  **直觉：** 更多Agent应该更好。\n2.  **观察：** 同质化Agent增加很快没用（饱和），异构化Agent才有用。\n3.  **归因：** 问题在于“冗余”而非“数量”。同质化导致信息重复，异构化提供互补信息。\n4.  **理论：** 建立信息论框架，证明性能上限由“有效通道数”决定，而非Agent总数。\n5.  **工具：** 提出 $K^*$ 作为无监督的“有效通道”度量指标，指导如何高效地构建MAS。"
                },
                {
                    "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
                    "arxiv_id": "2602.03786",
                    "authors": "Jianhao Ruan, Zhihao Xu, Yiran Peng, Fashen Ren, Zhaoyang Yu, Xinbing Liang, Jinyu Xiang, Bang Liu, Chenglin Wu, Yuyu Luo, Jiayi Zhang",
                    "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Agentic AI** 和 **Multi-Agent Systems** 的核心贡献。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了一个新的智能体框架 **AOrchestra**，旨在解决复杂任务中的智能体编排问题。 *   它不是将现有智能体简单应用于特定领域（如医疗、金融），而是提出了一种新的“构建和改进”智能体的方法论：即通过统一的抽象元组自动创建子智能体。 *   这属于构建 LLM 智能体和多智能体系统的范畴，而非基础设施优化或单纯的非 Agentic 推理。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Agentic AI` 和 `LLM-based Agents`，特别是“子智能体即工具”的范式。 *   **多智能体**: AOrchester 采用了一个中央编排器动态创建和管理多个子智能体，这属于多智能体协作与层级管理的范畴。 *   **智能体能力**: 论文重点讨论了 `Tool Use`（将子智能体作为工具）、`Planning`（通过编排器进行任务分解和委派）以及动态适应能力。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除领域。 4.  **结论**: 该论文提出了一种通过动态抽象和自动创建子智能体来增强智能体编排能力的新框架，直接对应研究课题中的“构建、改进 LLM 智能体”以及“多智能体协作”方向，因此予以保留。",
                    "summary2": "本文旨在解决现有智能体系统在复杂长周期任务中缺乏动态抽象和适应性的问题。针对多轮任务求解场景，我们提出了一种名为AOrchestra的编排中心系统，利用统一的四元组抽象 $\\langle$Instruction, Context, Tools, Model$\\rangle$ 实现子代理的按需动态创建与编排。我们在GAIA、SWE-Bench-Verified和Terminal-Bench 2.0基准上，通过Pass@1和Pass@3指标验证了其有效性，相比最强基线实现了16.28%的相对性能提升。",
                    "summary_translation": "**中文翻译：**\n\n语言智能体在任务自动化方面展现了巨大的潜力。为了在日益复杂、长视界任务中实现这一潜力，多轮任务求解中的子智能体即工具范式应运而生。然而，现有设计仍缺乏对子智能体的动态抽象视角，从而削弱了其适应性。我们通过一种统一的、框架无关的智能体抽象来解决这一挑战，该抽象将任意智能体建模为一个包含 Instruction（指令）、Context（上下文）、Tools（工具）和 Model（模型）的元组。该元组充当能力的组合方案，使系统能够按需为各项任务生成专用的执行器。基于此抽象，我们提出了一个智能体系统 AOrchestra，其中中央编排器在每一步对元组进行具体化：它筛选任务相关的上下文，选择工具和模型，并通过即时自动创建智能体来委派执行任务。此类设计不仅能够减少人工工程投入，还保持了框架无关的特性，支持将多样化的智能体作为任务执行器进行即插即用。它还实现了可控的性能与成本权衡，使系统能够接近帕累托最优。在三个具有挑战性的基准测试（GAIA, SWE-Bench, Terminal-Bench）中，当搭配 Gemini-3-Flash 使用时，AOrchestra 相比最强基线实现了 16.28% 的相对性能提升。代码可在以下地址获取：https://github.com/FoundationAgents/AOrchestra",
                    "inspiration_trace": "基于对论文《AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的对比，构建了从宏观背景到具体技术痛点的叙事逻辑：\n\n1.  **宏观观察（人类智慧）：** 人类处理复杂、长周期任务时，依靠的是“集体智能”和“协调能力”。这是AI Agent追求的目标。\n2.  **早期尝试与局限（多智能体系统 MAS）：** 为了应对复杂性，早期研究采用固定工作流或多智能体协作。\n    *   *痛点：* 这种方式过于僵化，且在开放环境中会产生巨大的协调开销，导致上下文路由失败（要么信息过载，要么关键信息丢失）。\n3.  **范式转移（子智能体即工具）：** 为了解决上述问题，近期趋势转向“主智能体-子智能体”的调用模式。\n    *   *现状分析：* 作者指出当前设计主要退化为两种有缺陷的模式：\n        *   **模式一：上下文隔离线程。** 旨在防止上下文腐烂，但缺乏专业化能力，子智能体只是“线程”而非“专家”。\n        *   **模式二：静态角色。** 虽然提供了专业化能力，但角色是硬编码的。这导致灵活性差、覆盖面有盲区，且需要大量人工设计，难以适应动态环境。\n4.  **核心矛盾：** 现有的系统要么**“灵活但不专业”**（隔离线程），要么**“专业但不灵活”**（静态角色）。我们需要一种既能动态适应任务，又能提供精准专业能力的解决方案。\n\n---\n\n### 二、 研究问题\n\n基于上述叙事逻辑，作者试图回答的核心研究问题可总结为：\n\n**“如何设计一种智能体编排系统，能够根据任务需求动态创建具备精准上下文和特定能力的子智能体，从而在保持系统灵活性的同时实现按需专业化，并摆脱对固定工作流或人工预定义角色的依赖？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n以下是从宏观问题出发，逐步聚焦并形成最终方法论的思考过程：\n\n#### 1. 宏观视角：从“管理实体”到“管理能力”\n*   **思考起点：** 面对长周期复杂任务，传统的多智能体系统试图管理多个“人”（实体），这导致了沟通成本高昂和上下文混乱。\n*   **思维转折：** 既然管理“人”很难，为什么不把子智能体看作是一种**“能力的组合”**？就像厨师根据菜谱（任务）动态搭配食材（工具）和厨具（模型）一样，子智能体应该是被“制造”出来的，而不是被“雇佣”来的。\n\n#### 2. 抽象构建：解构智能体的本质\n*   **假设：** 如果要动态制造一个子智能体，我们需要定义它的最小构成单元。\n*   **抽象过程：** 作者将任何智能体解构为两个维度的四元组 $\\langle \\text{Instruction, Context, Tools, Model} \\rangle$：\n    *   **工作记忆维度（做什么 & 知道什么）：** `Instruction`（指令）定义目标，`Context`（上下文）提供精准信息（过滤噪音）。\n    *   **能力维度（用什么做 & 谁来做）：** `Tools`（工具集）定义动作空间，`Model`（模型）定义推理能力。\n*   **洞察：** 这四元组就是一张“配方”。只要改变配方，就能制造出不同的子智能体。\n\n#### 3. 架构设计：编排者与执行者的解耦\n*   **设计决策：** 为了实现上述动态制造，系统必须分离“大脑”和“手脚”。\n*   **角色定义：**\n    *   **Orchestrator（指挥官）：** 它不执行具体任务，只负责思考。它的动作空间被极度简化为两个动作：`Delegate`（发布配方）和 `Finish`（结束任务）。\n    *   **Sub-Agent（执行者）：** 它是临时的、即用即弃的。它根据指挥官发布的四元组配方被实例化，完成任务后消亡。\n*   **优势：** 这种解耦使得指挥官可以专注于“下一步该做什么”以及“需要什么能力”，而不用关心具体怎么执行。\n\n#### 4. 上下文管理：从“全量继承”到“精准投喂”\n*   **问题反思：** 现有的“上下文隔离”模式缺乏信息，“全量继承”模式又导致噪音。\n*   **解决方案：** 在四元组抽象中，`Context` 是一个显式的参数。\n*   **逻辑推演：** 指挥官在创建子智能体时，必须显式地筛选和压缩历史信息，只传递与当前子任务最相关的上下文。这既解决了“上下文腐烂”，又保证了子智能体拥有足够的信息。\n\n#### 5. 优化演进：编排策略的可学习性\n*   **进一步思考：** 既然指挥官的动作只是输出一个四元组，那么这个决策过程是可以被优化的。\n*   **两个维度的优化：**\n    *   **任务编排能力（SFT）：** 通过监督微调，教指挥官如何更好地分解任务、合成指令和筛选上下文。\n    *   **成本感知能力（ICL）：** 通过上下文学习，教指挥官如何在简单任务上使用便宜模型，在困难任务上使用昂贵模型，从而实现帕累托最优。\n\n#### 6. 最终验证：即插即用的通用性\n*   **逻辑闭环：** 因为子智能体只是四元组的实例化，所以底层的实现（是ReAct还是其他框架）对指挥官来说是透明的。这证明了该框架的通用性和鲁棒性。\n\n---\n\n**总结：**\n作者的思考路径是从**解决多智能体协作的僵化问题**出发，通过**将智能体抽象为可配置的四元组**，实现了从“管理固定角色”到“动态制造能力”的范式转变。最终，通过**指挥官与执行者的解耦**，构建了一个既能精准控制上下文和工具，又能灵活适应各种任务的自动化编排系统。"
                },
                {
                    "title": "TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System",
                    "arxiv_id": "2602.03688",
                    "authors": "Wenzhe Fan, Tommaso Tognoli, Henry Peng Zou, Chunyu Miao, Yibo Wang, Xinhua Zhang",
                    "summary": "Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \\textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \\textbf{t}ask-\\textbf{o}riented \\textbf{dy}namic \\textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。具体判断依据如下： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **TodyComm**，这是一种面向任务的动态通信算法，专门用于 **LLM-based Multi-Agent System**（基于大语言模型的多智能体系统）。 *   它解决的是多智能体系统中的关键架构问题——即如何在多轮交互中动态调整通信拓扑结构，以适应任务进展和环境变化。 *   这属于构建和改进 LLM 智能体系统的方法论，而非简单的应用或基础设施优化。 2.  **正面指标匹配（第二步）：** *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：重点研究了智能体间的 `Communication`（通信）和 `Collaboration`（协作），提出了“行为驱动的协作拓扑”。 *   **演化机制**：虽然不是自我演化（Self-Evolving），但其“动态适应每一轮的动态”体现了智能体系统在运行时的适应性优化。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 *   虽然提到了“动态对手”，但这只是用于验证算法鲁棒性的实验场景，而非论文的核心贡献（核心贡献是通信算法本身），因此不属于“非演化型应用”的排除范畴。 综上所述，该论文致力于改进多智能体系统的协作与通信机制，直接对应我研究焦点中的“多智能体”方向，因此予以保留。",
                    "summary2": "本文旨在解决multi-round LLM-based multi-agent systems中固定通信拓扑无法适应动态环境的问题。针对动态对抗和通信预算限制的场景，我们提出了一种名为TodyComm的任务导向动态通信算法，通过GRN学习智能体信用值并利用policy gradient优化通信结构。在MMLU、GSM8K等五个基准数据集上，通过任务准确率、Token使用量和对抗检测准确率验证了其有效性。",
                    "summary_translation": "基于 LLM (Large Language Model，大语言模型) 的多轮 multi-agent systems (多智能体系统) 依赖于有效的 communication structures (通信结构) 来支持跨轮次的协作。然而，大多数现有方法在 inference (推理) 过程中采用固定的 communication topology (通信拓扑)，这在许多 realistic applications (现实应用) 中存在不足，因为在这些应用中，受 dynamic adversary (动态对抗)、task progression (任务进展) 或 communication bandwidth (通信带宽) 等 time-varying constraints (时变约束) 的影响，agents (智能体) 的角色可能会在跨轮次时发生变化。在本文中，我们提出通过 TodyComm 来解决这一问题，这是一种 task-oriented (面向任务的) dynamic communication (动态通信) 算法。它生成 behavior-driven (行为驱动的) collaboration topologies (协作拓扑)，以适应每一轮的动态变化，并通过 policy gradient (策略梯度) 优化任务的 utility (效用)。在五个 benchmarks (基准测试) 上的实验表明，在 dynamic adversary (动态对抗) 和 communications budgets (通信预算) 的条件下，TodyComm 在保持 token efficiency (令牌效率) 和 scalability (可扩展性) 的同时，实现了卓越的任务有效性。",
                    "inspiration_trace": "基于对论文《TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 第一阶段：宏观观察与背景确立\n**逻辑起点：** 作者首先关注到了大语言模型（LLM）在多智能体系统（MAS）中的应用趋势。\n*   **观察：** 多轮、多智能体的协作机制是解决复杂推理和决策任务的关键范式。通过多轮交互，智能体可以交换信息、纠正错误并逐步提升解的质量。\n*   **现状：** 现有的研究主要集中在“通信拓扑”的设计上，即智能体之间如何连接（通常建模为图结构）。\n\n### 第二阶段：故事讲述与问题提取\n**逻辑演进：** 作者通过对比“现有方法的假设”与“现实世界的复杂性”，构建了核心冲突。\n\n**1. 现有方法的“静态”假设：**\n*   大多数现有方法在推理阶段使用**固定的通信拓扑**。这意味着无论任务进展如何，无论智能体表现如何，谁跟谁说话的规则是预先设定好且不变的。\n\n**2. 现实世界的“动态”挑战：**\n*   作者指出现实应用并非静态，而是充满变数的：\n    *   **动态对抗：** 智能体可能在任意轮次“变节”（如产生误导性分析），或者因疲劳、偏见变得不可靠。\n    *   **任务演进：** 任务的不同阶段可能需要不同的协作模式。\n    *   **资源约束：** 通信带宽等限制是随时间变化的。\n\n**3. 现有解决方案的局限性：**\n*   **安全类方法：** 现有的防御方法（如基于图的异常检测）通常依赖固定的图结构，或者需要对抗标签（不现实），且往往只关注“检测坏人”而非“优化任务表现”。\n*   **鲁棒性架构：** 即使是设计容忍腐败的架构，依然受限于固定的通信拓扑，无法根据行为实时调整。\n\n**4. 核心矛盾：**\n*   固定的通信结构无法适应智能体行为的动态演化，导致在对抗或资源受限环境下，任务性能严重下降。\n\n---\n\n**显式总结：研究问题**\n基于上述逻辑链，作者试图回答的核心问题是：\n\n> **“在多轮LLM多智能体系统中，如何设计一种能够根据智能体历史行为和任务进展动态调整通信拓扑的机制，以在动态对抗和资源约束下最大化任务效用？”**\n\n---\n\n### 第三阶段：从挑战到核心思想的演进\n**逻辑推演：** 面对上述研究问题，作者分析了技术难点并提出了核心创新思想。\n\n**1. 技术难点分析：**\n*   **组合爆炸：** 直接在每一轮优化离散的图结构（谁连谁）会导致巨大的组合动作空间，难以学习。\n*   **非结构化输入：** 智能体的输出是文本，如何将其转化为可量化的决策依据？\n*   **信用分配：** 最终任务的好坏，归功于哪一轮的哪一次通信？\n\n**2. 核心思想突破：从“图优化”转向“信用分配”：**\n*   **灵感：** 借鉴多智能体强化学习（MARL）中的信用分配思想。\n*   **假设：** 如果能计算出每个智能体在每一轮的“信用值”，即其对任务的贡献度和可靠性，那么图的构建就变得简单了——让高信用的智能体参与通信，并让消息流向高信用的智能体。\n*   **降维打击：** 将复杂的“图结构优化”问题，简化为“智能体信用值学习”问题。\n\n### 第四阶段：方法论构建\n**逻辑落地：** 基于核心思想，作者构建了TodyComm的具体框架。\n\n**1. 状态建模与特征提取：**\n*   为了计算信用，需要捕捉智能体的行为。作者利用**门控循环网络（GRN）**来处理多轮交互的时间依赖性。\n*   输入特征不仅包含智能体自己的回答，还包含邻居的信息以及两者之间的差异，以此判断其一致性。\n\n**2. 动作空间重构：**\n*   不再直接采样边，而是基于信用值采样“参与决策”。\n*   **图构建策略：** 一旦决定了哪些智能体参与，就根据信用高低进行确定性排序，优先建立从高信用到低信用的连接（DAG），并满足带宽约束。\n\n**3. 目标导向优化：**\n*   为了确保通信结构是为了“任务”服务的，作者采用**强化学习（REINFORCE算法）**。\n*   **奖励信号：** 直接使用最终的任务效用（如准确率）作为奖励。\n*   **反馈闭环：** 如果最终任务完成得好，就强化之前导致高信用智能体参与的通信动作；反之则惩罚。\n\n### 第五阶段：总结与验证\n**逻辑闭环：**\n*   作者通过在五个基准数据集上的实验，特别是在动态对抗和预算约束场景下，验证了TodyComm不仅能动态适应环境，还能在保持Token效率的同时，显著优于固定拓扑的方法。\n*   这证明了“基于行为驱动的动态通信”是解决多轮多智能体协作中动态适应性的有效路径。\n\n---\n\n**总结：**\n作者的思考路径是从**“多智能体协作的有效性”**出发，敏锐地发现了**“固定拓扑与动态现实”**之间的鸿沟，通过引入**“信用分配”**这一中间变量巧妙化解了**“离散图优化”**的难题，最终利用**“强化学习”**实现了通信结构与**“任务目标”**的深度对齐。"
                },
                {
                    "title": "Mitigating Conversational Inertia in Multi-Turn Agents",
                    "arxiv_id": "2602.03664",
                    "authors": "Yang Wan, Zheng Cao, Zhenhao Zhang, Zhengwen Zeng, Shuheng Shen, Changhua Meng, Linchao Zhu",
                    "summary": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化/改进”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是针对“多轮智能体”中存在的“对话惯性”问题，提出了一种名为“上下文偏好学习”的新框架，以及推理时的上下文管理策略。 *   这不是将LLM简单应用到某个垂直领域（如医疗、法律），而是针对LLM作为智能体在多轮交互中表现出的特定行为缺陷（模仿偏差、探索受限）进行**改进**。 *   论文明确在“agentic environments”（智能体环境）中进行验证，旨在提升智能体的性能，符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标 (第二步)**: *   **核心范式**: 论文明确研究 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**: 涉及智能体在多轮对话中的 `Exploration` (探索) 与 `Exploitation` (利用) 的平衡，这是智能体决策能力的核心。 *   **演化机制**: 提出的 `Context Preference Learning` 是一种通过构建偏好对来校准模型偏好的方法，属于 `Self-Improvement` (自我完善) 或 `Iterative Improvement` (迭代改进) 的范畴，旨在通过反馈机制优化智能体的行为模式。 3.  **排除标准 (第三步)**: *   论文不涉及安全对齐、多模态视觉或图技术，没有触发任何排除项。 4.  **特殊情况处理 (第四步)**: *   虽然论文涉及推理（如何生成响应），但它不是关于提高LLM基础的数学或逻辑能力，而是专门解决智能体在多轮交互环境中的行为偏差问题，因此属于保留的范畴。 综上所述，该论文提出了解决LLM智能体在多轮交互中关键缺陷的新方法，属于对智能体能力的实质性改进，符合研究课题要求。",
                    "summary2": "本文旨在解决多轮Agent中因对话惯性导致的性能退化问题。针对多轮交互场景，我们提出了Context Preference Learning (CPL) 和Clip Context策略，利用长-短上下文偏好对校准模型偏好以降低惯性，并在推理时周期性清理上下文。我们在八个Agent环境及BrowseComp深度研究场景上，通过成功率和对角注意力指标验证了其有效性。",
                    "summary_translation": "大语言模型在提供适当示例时表现出卓越的少样本学习能力，但在多轮智能体场景中，这种优势却引发了问题：LLMs会错误地将其先前的响应作为少样本示例进行模仿。通过注意力分析，我们发现了对话惯性，即模型对先前响应表现出强烈的对角线注意力的现象；这种现象与限制探索的模仿偏差密切相关。这揭示了将少样本LLMs转化为智能体时存在的一种矛盾：更长的上下文虽然丰富了用于利用的环境反馈，但也放大了削弱探索能力的对话惯性。我们的核心见解在于，对于相同的状态，基于较长上下文生成的动作比基于较短上下文生成的动作表现出更强的惯性，这使得无需环境奖励即可构建偏好对。基于此，我们提出了上下文偏好学习，以校准模型偏好，使其倾向于低惯性响应而非高惯性响应。我们进一步提供了推理阶段的上下文管理策略，以平衡探索与利用。在八个智能体环境和一项深度研究场景中的实验结果验证了我们的框架能够减少对话惯性并实现性能提升。",
                    "inspiration_trace": "基于对论文《Mitigating Conversational Inertia in Multi-Turn Agents》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从“优势”到“陷阱”的叙事逻辑，具体如下：\n\n1.  **既有优势：** 大语言模型（LLM）作为优秀的少样本学习者，能够通过提供的示例快速适应任务。\n2.  **场景迁移：** 在多轮智能体场景中，这种能力被用于通过积累上下文来解决复杂任务（如网页导航、具身智能）。\n3.  **核心冲突：** 这种少样本学习的优势在多轮交互中变成了劣势。模型错误地将**自己之前的回复**当作了少样本示例来进行模仿。\n4.  **现象发现：** 通过注意力分析，作者发现了一种“对话惯性”现象——模型对之前的助手输出表现出强烈的对角线注意力。\n5.  **后果阐述：** 这种注意力模式与“模仿偏差”相关，导致模型倾向于复制之前的响应模式，而不是适应新的环境反馈。这限制了探索能力，导致错误累积，使得随着对话轮数增加，性能反而下降。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图解决的核心研究问题可总结为：\n\n**“在无法获取环境奖励信号的情况下，如何缓解多轮智能体中因模型错误模仿自身历史而产生的‘对话惯性’，从而打破错误循环并提升性能？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n从观察到方法论的提出，作者的思考过程经历了以下五个关键阶段：\n\n#### 1. 宏观观察与反直觉现象\n*   **观察：** 在多轮Agent任务中，随着上下文长度的增加，模型的性能并没有如预期般提升，反而出现了显著下降。\n*   **思考：** 通常认为更多的历史信息意味着更多的反馈，应该有助于决策。为什么在这里“长上下文”反而成了累赘？\n\n#### 2. 机制诊断：对话惯性的发现\n*   **分析手段：** 作者深入模型的内部机制，可视化注意力矩阵。\n*   **发现：** 模型在生成当前回复时，注意力高度集中在上一轮回复对应位置的Token上（对角线注意力）。\n*   **定性：** 这表明模型正在机械地“复制”自己的历史行为。作者将其定义为**“对话惯性”**。\n*   **归因：** 这是模型将“少样本学习”能力误用到了“自身历史”上，把之前的动作当成了必须模仿的范例，导致陷入死循环，无法根据环境反馈进行探索。\n\n#### 3. 关键洞察：长短上下文的差异\n*   **思考：** 如果长上下文导致强惯性，那么短上下文呢？\n*   **假设验证：** 作者发现，对于**完全相同的状态**，使用**较短上下文**生成的动作，比使用**较长上下文**生成的动作表现出更弱的惯性。\n*   **逻辑推演：** 这意味着，我们不需要知道环境的真实奖励，就可以判断哪个动作可能更好——因为“低惯性”通常意味着更少的盲目模仿和更多的适应性探索。\n\n#### 4. 方法构建一：上下文偏好学习 (CPL)\n*   **思路：** 利用上述“长短上下文”的差异来构建训练数据。\n*   **数据构建：** 对于同一个状态，生成两个动作：\n    *   $a_{long}$：基于完整历史（长上下文，惯性大，视为Rejected）。\n    *   $a_{short}$：基于近期历史（短上下文，惯性小，视为Chosen）。\n*   **训练目标：** 使用DPO（直接偏好优化）训练模型，使其偏好于在短上下文下生成的动作风格。\n*   **核心优势：** 这种方法完全不需要环境的Ground Truth奖励，也不需要专家演示，是一种自监督的校准方式。\n\n#### 5. 方法构建二：推理时的上下文管理\n*   **思考：** 训练可以降低模型的惯性倾向，但在推理时，如果一直塞给它超长的历史，惯性依然会被物理触发。\n*   **策略：** 提出**Clip Context（裁剪上下文）**。\n*   **逻辑：** 周期性地清空上下文。这就像定期“重启”对话，打破累积的惯性模式，防止错误传播。\n*   **平衡：** 这种方法在“利用历史信息”和“打破惯性进行探索”之间取得了平衡，且相比滑动窗口更利于KV Cache的利用，提高了计算效率。\n\n### 总结\n\n作者的思考路径是从**性能异常**出发，通过**可解释性分析**定位到**注意力机制缺陷**，利用**长短上下文的对比差异**巧妙地绕过了对环境奖励的依赖，最终形成了**训练（CPL）+ 推理**双管齐下的解决方案。"
                },
                {
                    "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
                    "arxiv_id": "2602.03647",
                    "authors": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong, Yankai Chen, Chen Ma, Xue Liu, Pluto Zhou, Irwin King",
                    "summary": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中的“自我反思”与“工具使用”子方向。 1.  **核心贡献符合要求**：论文提出了 Search-R2，这是一个新的“Actor-Refiner 协作框架”。其核心在于构建和改进 LLM 智能体的架构，而非仅仅将现有模型应用于特定领域。论文明确指出这是针对“语言智能体”的增强，旨在解决智能体在搜索集成推理中的训练难题。 2.  **符合“单智能体”与“自我演化”特征**： *   **自我反思/修正**：论文中的 Meta-Refiner 组件通过“剪切并重新生成”机制，对 Actor 产生的推理轨迹进行诊断和修复。这完全符合筛选标准中关于“自我反思”和“自我修正”的定义。 *   **工具使用**：论文关注“Search-integrated reasoning”，即智能体主动查询外部信息源，这是典型的智能体工具使用能力。 3.  **排除标准检查**： *   该论文不是非演化型应用，虽然它在 QA 数据集上测试，但其核心贡献是通用的智能体框架和训练方法。 *   它不是非 Agentic 的推理，因为它涉及外部工具调用和特定的 Actor-Refiner 交互流程，而非单纯的模型内部参数推理能力提升。 *   不涉及安全、多模态或图等排除领域。 综上所述，该论文通过引入 Actor-Refiner 协作机制来增强智能体的搜索推理和自我修正能力，属于构建和改进 LLM 智能体的核心研究，应予以保留。",
                    "summary2": "本文旨在解决搜索集成推理中的多尺度信用分配问题。针对搜索集成智能体，我们提出了一种Search-R2框架，通过Actor-Refiner协作及“cut-and-regenerate”机制修复错误，并利用混合奖励进行联合优化。在七个通用和多跳QA数据集上通过Exact Match指标验证，该方法在不同模型规模下均优于强基线。",
                    "summary_translation": "Search-integrated reasoning (搜索集成推理) 使语言智能体能够通过主动查询外部来源，超越静态参数化知识的局限。然而，通过 reinforcement learning (强化学习) 训练这些智能体受到 multi-scale credit assignment problem (多尺度信用分配问题) 的阻碍：现有方法通常依赖稀疏的 trajectory-level rewards (轨迹级奖励)，无法有效区分高质量推理与侥幸猜测，从而导致冗余或误导性的搜索行为。为解决这一问题，我们提出了 Search-R2，这是一种新颖的 Actor-Refiner (行动者-修正者) 协作框架，通过针对性干预来增强推理能力，且两个组件在训练过程中进行联合优化。我们的方法将生成过程分解为 Actor (行动者) 和 Meta-Refiner (元修正者)，前者负责生成初始推理轨迹，后者则通过 'cut-and-regenerate' (剪切-再生成) 机制对缺陷步骤进行选择性诊断与修复。为了提供 fine-grained supervision (细粒度监督)，我们引入了一种 hybrid reward design (混合奖励设计)，将 outcome correctness (结果正确性) 与一种量化检索证据 information density (信息密度) 的 dense process reward (密集过程奖励) 相结合。在理论上，我们将 Actor-Refiner 的交互形式化为一种 smoothed mixture policy (平滑混合策略)，证明了选择性修正相较于强基线能够带来严格的性能提升。在各种通用及 multi-hop QA (多跳问答) 数据集上进行的广泛实验表明，Search-R2 在不同模型规模下始终优于强 RAG (检索增强生成) 和 RL-based (基于强化学习) 的基线方法，在实现卓越推理准确率的同时保持了最小的开销。",
                    "inspiration_trace": "基于对论文《Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观背景与观察\n**起点：** 大语言模型（LLM）正在从静态的知识库演变为能够主动与外部环境交互的动态智能体。\n**观察：** 为了解决知识局限和幻觉问题，当前的范式是将“迭代推理”与“主动检索”相结合。为了优化这些智能体的行为，学术界倾向于使用强化学习（RL），让模型基于任务目标（如最终答案正确性）进行学习，而不仅仅是模仿人类。\n\n---\n\n### 2. 问题引入：Introduction 中的“故事”逻辑\n作者在引言中通过层层递进的逻辑，揭示了现有方法的致命缺陷：\n\n1.  **理想与现实的冲突：** 虽然RL能让智能体自主学习何时搜索、如何搜索，但在实际训练中，我们面临一个核心挑战——**多尺度信用分配**。\n2.  **信号的稀疏性：** 现有方法通常依赖**轨迹级奖励**（即只看最终答案对不对）。这种信号非常稀疏，它无法区分“高质量的推理过程”和“瞎猫碰上死耗子的运气”。\n3.  **行为的误导性：** 由于缺乏中间步骤的监督，模型会产生冗余或误导性的搜索行为。即使最终答案对了，中间的搜索步骤可能也是毫无逻辑的。\n4.  **错误的级联效应：** 这是一个关键痛点。如图1所示，推理链条中早期的一个无关搜索查询（噪音）会误导后续的所有推理步骤，导致整个链条崩溃。\n5.  **现有补救方案的低效：** 面对错误的轨迹，现有的“拒绝采样”技术非常粗暴——它直接丢弃整个轨迹。这就像因为文章里有一句错话就把整篇文章撕掉重写，而不是修改那一句，导致样本效率极低。\n\n**总结：** 我们缺乏一种能够诊断并修复错误传播的机制，必须从“基于结果的过滤”转向“强制执行全局推理连贯性和局部搜索质量”。\n\n---\n\n### 3. 核心研究问题\n基于上述问题叙事，作者试图回答的核心问题是：\n\n> **“在搜索增强推理的强化学习训练中，如何克服多尺度信用分配难题，通过局部的错误诊断与修复机制，而非全局的轨迹丢弃，来提升推理的鲁棒性与样本效率？”**\n\n---\n\n### 4. 逻辑演进：从假设到方法论\n为了解决上述问题，作者的思考路径经历了以下三个阶段：\n\n#### 第一阶段：解耦假设\n*   **思考：** 既然一次性生成完美的长链条很难，且容易出错，为什么不把“生成”和“检查/修复”分开？\n*   **假设：** 如果将推理过程分解为两个角色——一个负责生成初始轨迹（Actor），另一个负责识别错误并进行针对性修复——那么我们就能保留轨迹中正确的部分，只修正错误的部分。\n\n#### 第二阶段：机制设计——“手术式”干预\n*   **思考：** 修复机制不能是盲目的。它需要知道“哪里错了”以及“是否需要修”。\n*   **方法构思：**\n    *   **判别器：** 负责全局评估，判断当前轨迹是否逻辑连贯。如果连贯则接受，不连贯则打回。\n    *   **修剪器：** 负责局部定位，找到推理链条中最早出现偏差的那一步（“根因”）。\n    *   **Cut-and-Regenerate：** 一旦定位到错误点 $k$，就保留前缀 $y_{1:k}$，截断后续部分，并从 $k+1$ 开始重新生成。这就是“手术式”修复。\n\n#### 第三阶段：训练信号优化——混合奖励\n*   **思考：** 仅有“Cut-and-Regenerate”机制还不够，我们需要告诉模型什么样的搜索才是“好”的搜索，以解决最初的信用分配问题。\n*   **方法构思：**\n    *   **结果奖励：** 最终答案对不对（全局）。\n    *   **过程奖励：** 检索到的证据信息密度高不高（局部）。\n    *   **混合设计：** $R(y) = r_{outcome} \\cdot (1 + r_{process})$。这确保了模型只有在检索到高质量证据并得出正确答案时，才能获得最高奖励，防止了“为了检索而检索”的奖励黑客行为。\n\n---\n\n### 5. 理论升华：为什么这行得通？\n作者最后通过理论分析将这种直觉形式化，证明了该方法的优越性并非偶然，而是依赖于三个关键条件的满足：\n1.  **选择精度：** 判别器必须能准确区分好坏轨迹。\n2.  **修剪技能：** 修剪器必须能精准定位到那个“修复后收益最大”的截断点。\n3.  **干预体积：** 需要有一个适度的干预率，既不能过度干预（浪费算力），也不能干预不足（放过错误）。\n\n**结论：** 通过 Actor 和 Meta-Refiner 的联合优化，Search-R2 实现了比单纯增加采样数量（如拒绝采样）更高效的性能提升。"
                },
                {
                    "title": "IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning",
                    "arxiv_id": "2602.03468",
                    "authors": "Haohao Luo, Zexi Li, Yuexiang Xie, Wenhao Zhang, Yaliang Li, Ying Shen",
                    "summary": "Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“LLM智能体及其演化”的核心领域。以下是详细的判断依据： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **IntentRL**，这是一个用于训练“主动用户意图智能体”的新框架。 *   论文本质上是关于**构建和改进 LLM 智能体**的，旨在解决智能体在长视距任务中的“自主性-交互困境”。 *   它不是将现有智能体简单应用于某个垂直领域（如医疗或法律），而是针对智能体本身的交互机制和规划能力进行了架构和训练方法上的创新。 *   因此，符合“保留”标准。 2.  **正面指标匹配（第二步）：** *   **核心范式**：明确属于 `Agentic AI` 和 `LLM-based Agents`。摘要中提到了“Deep Research (DR) agents”和“long-horizon agentic paradigm”。 *   **智能体能力**：涉及 `Planning`（在开始长视距研究前进行意图澄清）和 `Autonomy`（自主检索与综合）。 *   **演化机制**：论文采用了 `Reinforcement Learning (RL)` 来训练智能体，特别是通过两阶段策略（离线学习 + 在线推演）来增强智能体对用户反馈的适应性。这符合“自我演化”中通过环境反馈进行自我完善和迭代的定义。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或知识图谱等排除领域。 *   虽然提到了“意图细化图”，但这只是数据生成流水线中的结构，并非图神经网络或知识图谱的研究核心。 4.  **特殊情况处理（第四步）：** *   **推理/规划**：论文关注的是智能体如何通过主动交互来明确用户意图，从而优化后续的规划路径。这是典型的 Agentic Planning 问题，而非单纯的 LLM 内部推理能力提升。 **结论**：该论文通过强化学习技术改进了 LLM 智能体的规划与交互能力，属于单智能体与自我演化方向的交叉研究，符合筛选要求。",
                    "summary2": "本文旨在解决Deep Research (DR) 代理在处理模糊查询时面临的“自主性-交互困境”，避免资源浪费与结果偏差。针对开放式深度研究场景，我们提出了一种名为IntentRL的框架，利用Clarification DAG (C-DAG) 扩展数据并采用两阶段强化学习（离线RL结合在线用户模拟器）训练主动意图挖掘代理。在DeepResearch Bench、Rigorous Bench等数据集上，通过意图命中率及下游报告质量等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "Deep Research (DR) agents (深度研究智能体) 通过自主从大型网络语料库中检索并综合证据生成长篇报告，将 Large Language Models (LLMs) (大语言模型) 的能力扩展至参数化知识之外，从而实现了一种长视距智能体范式。然而，与实时对话助手不同，DR 计算成本高昂且耗时，这导致了自主性-交互性困境：在面对模糊的用户查询时，高自主性往往导致执行时间延长且结果不尽如人意。为解决这一问题，我们提出了 IntentRL，这是一个训练主动智能体在开始长视距研究之前澄清潜在用户意图的框架。为克服开放式研究数据的稀缺性，我们引入了一种可扩展的流程，该流程通过由浅入深的意图细化图将少量种子样本扩展为高质量的对话轮次。我们进一步采用了一种两阶段强化学习 (RL) 策略：阶段 I 在离线对话上应用 RL，以高效学习通用的用户交互行为；阶段 II 利用训练好的智能体和用户模拟器进行在线推演，以增强对多样化用户反馈的适应性。大量实验表明，IntentRL 显著提高了意图命中率和下游任务性能，优于闭源 DR 智能体的内置澄清模块以及主动 LLM 基线。",
                    "inspiration_trace": "基于对论文《IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning》的深入分析，以下是作者产出该文章的完整逻辑推演过程。\n\n---\n\n### 一、 引言中的“故事”逻辑：从观察到困境\n\n作者在Introduction中构建了一个层层递进的逻辑链条，旨在引出核心问题的紧迫性：\n\n1.  **范式转移与能力跃升**：\n    *   **观察**：大语言模型（LLM）正在从简单的“聊天机器人”向“深度研究（DR）智能体”演进。\n    *   **特征**：DR智能体能自主检索、综合海量网络信息，并生成长篇报告，这是一种长视距的自主智能范式。\n\n2.  **引入核心冲突：自主性-交互困境**：\n    *   **对比**：在实时对话中，误解用户的成本很低（用户可立即纠正）；但在DR任务中，执行耗时极长（如30分钟）且计算昂贵。\n    *   **困境**：如果智能体对模糊的用户查询表现出高自主性（直接执行），一旦方向错误，将导致巨大的资源浪费和令人失望的结果。\n\n3.  **归因分析：显性查询与隐性意图的鸿沟**：\n    *   **根源**：用户往往缺乏领域知识或表达能力，无法陈述精确需求。\n    *   **现象**：用户的“显性查询”往往包含巨大的信息全集，而其“隐性意图”只是其中的一个特定子集（例如，用户问“深度研究智能体”，实际只想了解“闭源商业产品”）。\n    *   **后果**：若无交互，智能体会基于自身的隐式偏差去检索，导致生成的报告与用户真实目标重叠度极低。\n\n4.  **现有方案的局限性**：\n    *   **方案A（扩大搜索）**：试图覆盖全集。**结论**：计算上不可行。\n    *   **方案B（扩大交互）**：在执行前通过交互澄清意图。**结论**：理论上更高效。\n    *   **现状批判**：现有的闭源DR产品（如OpenAI, Qwen）虽有澄清模块，但效果不佳；而开源的主动式智能体研究多局限于封闭领域（如医疗诊断、工具调用），无法应对开放性研究任务的无穷状态空间。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑链条，作者显式提出了本文试图解决的核心问题：\n\n**“在开放式的深度研究任务中，面对高质量交互数据极度稀缺和问题空间不受限的挑战，我们如何训练一个能够主动且有效地澄清用户隐性意图的智能体，以解决自主性与交互性之间的困境？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n为了回答上述问题，作者的思考路径经历了从宏观策略到微观实现的逐步聚焦：\n\n#### 1. 战略转向：从“Scaling Search”到“Scaling Interaction”\n*   **思考**：既然在长视距任务中“猜错”的代价太高，且无限扩大搜索范围不现实，那么必须改变策略。\n*   **决策**：将资源投入点从“执行阶段的搜索覆盖”前移至“执行前的意图对齐”。即通过增加用户-智能体的交互轮次，来精准锁定用户意图，从而降低后续研究的盲目性。\n\n#### 2. 瓶颈突破：解决“开放性”带来的数据稀缺\n*   **挑战**：传统的主动式学习（如医疗问诊）有固定的症状列表，但深度研究是开放式的，无法枚举所有意图。且现有的高质量交互数据极少（通常只有几百条）。\n*   **假设**：虽然真实数据少，但我们可以利用“评估标准”来反推意图。\n*   **方法论创新（数据层）**：\n    *   **浅层意图**：将原始查询“模糊化”，去掉的约束即为意图。\n    *   **深层意图**：分析评估报告质量的Rubrics（评分细则），提取那些无法通过检索解决、必须依赖用户偏好的要求（如分析视角、关注重点）。\n    *   **结构化扩展**：构建一个**澄清有向无环图（C-DAG）**。将意图分层（从浅到深），通过图遍历将少量种子样本扩展为大量、多样化的对话轨迹。这解决了“数据从哪来”的问题。\n\n#### 3. 训练策略：平衡“模仿专家”与“适应现实”\n*   **挑战**：仅靠离线的专家轨迹（C-DAG生成的数据）训练，智能体在面对真实用户反馈时容易产生分布偏移，表现为重复提问或对负面反馈不敏感。\n*   **方法论创新（算法层）**：采用**两阶段强化学习（RL）**策略。\n    *   **阶段一（离线RL）**：利用C-DAG生成的专家轨迹，让智能体学习通用的提问逻辑和意图覆盖能力，打好基础。\n    *   **阶段二（在线RL）**：引入一个**意图感知的用户模拟器**。让智能体在模拟环境中自由探索，通过奖励机制（惩罚重复、无关问题，奖励命中意图）来学习如何适应多样化的用户反馈。这解决了“如何适应真实交互”的问题。\n\n#### 4. 最终闭环：验证“交互”的价值\n*   **思考**：如何证明这种方法的有效性？\n*   **验证逻辑**：不仅评估提问的质量，更重要的是将训练好的主动智能体接入各种下游DR Agent（如Qwen, OpenAI, Gemini），验证“经过意图澄清后的查询”是否能显著提升最终报告的质量。\n\n---\n\n**总结**：\n作者的思考过程是从**发现长视距任务中的“试错成本”痛点**出发，通过**归因于“意图鸿沟”**，确立了**“交互优于搜索”**的战略方向。随后，针对**开放性任务的数据和训练难点**，创造性地提出了**基于C-DAG的数据扩展**和**离线+在线的混合RL训练**框架，最终构建了一个能够主动挖掘用户意图的智能体。"
                },
                {
                    "title": "DiscoverLLM: From Executing Intents to Discovering Them",
                    "arxiv_id": "2602.03429",
                    "authors": "Tae Soo Kim, Yoonjoo Lee, Jaesang Yu, John Joon Young Chung, Juho Kim",
                    "summary": "To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking \"what kind of tone do you want?\" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献判断 (符合 Agentic AI / 单智能体)**: 这篇论文的核心贡献是提出了 **DiscoverLLM**，这是一个训练LLM帮助用户发现和形成意图的框架。这属于 **单智能体** 的研究范畴。论文中描述的智能体不仅仅是被动回答问题，而是根据用户的状态（意图的模糊程度）主动采取行动：在意图不清时“发散”以探索选项，在意图明确时“收敛”以执行任务。这种基于状态判断并采取不同策略（探索 vs. 执行）的机制，体现了智能体的 **规划** 和 **决策** 能力。 2.  **符合筛选标准**: *   **第一步 (核心判断)**: 论文构建了一个新的智能体框架，涉及智能体如何与用户交互、如何规划对话策略（发散/收敛），属于构建LLM智能体的方法论，而非简单的应用或基础设施。 *   **第二步 (正面指标)**: 论文涉及 `Agentic AI`，智能体具备 `Planning`（决定何时发散或收敛）和 `Collaboration`（与用户协作）的能力。 *   **第三步 (排除标准)**: 论文不涉及安全/对齐、多模态核心（SVG仅作为测试基准之一，非核心视觉模型研究）或图技术。 *   **第四步 (特殊情况)**: 论文关注的是智能体在复杂交互任务中的多步推理和策略规划，属于Agentic的推理范畴，而非单纯的Token预测能力提升。 3.  **结论**: 尽管论文在创意写作、SVG绘制等具体任务上进行了评估，但其核心在于提出了一种通用的智能体交互机制（意图发现与自适应策略），这符合“构建、改进LLM智能体”的研究目标，特别是单智能体的规划与交互方向。因此，该论文符合筛选要求。",
                    "summary2": "本文旨在解决用户意图未形成时的交互挑战。针对开放性创作任务，我们提出了一种名为 DiscoverLLM 的训练框架，利用具有潜在意图层次结构的用户模拟器来训练模型平衡探索与收敛。在创意写作、技术写作和 SVG 绘图任务上，通过 Intent Discovery Score 和 Intent Satisfaction Score 验证了其有效性，实现了超过 10% 的性能提升并显著缩短了对话长度。",
                    "summary_translation": "为了处理模糊和开放式请求，大语言模型正日益被训练与用户进行交互，以揭示用户尚未表达的意图（例如，提出澄清性问题）。然而，用户的请求之所以往往模糊，是因为他们尚未形成明确的意图：他们必须通过观察和探索结果来发现自己真正想要什么。当用户自己都不知道答案时，简单地询问“你想要什么样的语气？”往往无济于事。我们提出了DiscoverLLM，这是一个新颖且可泛化的框架，旨在训练大语言模型帮助用户形成并发现其意图。我们方法的核心在于一种新颖的用户模拟器，该模拟器通过意图层级来模拟认知状态；随着模型呈现相关选项，这些意图会逐渐具体化——其中，具体化程度作为奖励信号，用于训练模型进行优化。最终生成的模型学会了与用户协作：在意图不明确时自适应发散（即探索选项），而在意图具体化时进行收敛（即细化和实施）。在创意写作、技术写作和SVG绘图等提出的交互式基准测试中，DiscoverLLM将任务性能提升了10%以上，同时将对话长度减少了多达40%。在一项涉及75名人类参与者的用户研究中，与基线模型相比，DiscoverLLM提高了对话满意度和效率。",
                    "inspiration_trace": "基于对论文《DiscoverLLM: From Executing Intents to Discovering Them》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法论的完整思考过程。\n\n---\n\n### 一、 宏观问题：从“执行者”到“协作者”的范式错位\n\n**思考起点：**\n作者首先审视了当前大语言模型（LLM）在交互中的基本定位。现有的模型训练范式（如RLHF、单轮优化）大多基于一个核心假设：**用户在开始交互时，已经拥有完全成型的意图**。因此，模型的角色被定义为“指令执行者”或“意图挖掘者”——即通过提问澄清用户已知但未明说的需求。\n\n**现实冲突：**\n然而，在创意写作、设计等开放性任务中，这一假设往往失效。现实中，用户经常带着“未定义的意图”开始任务——他们自己也不知道确切想要什么，需要通过观察和探索结果来逐步形成想法。\n\n**核心矛盾：**\n当模型试图通过“澄清问题”（如“你想要什么风格？”）来挖掘意图时，如果用户自己尚未形成该意图，这种交互就会陷入死循环。模型越努力挖掘，用户越感到困惑，导致交互效率低下且体验糟糕。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中通过一个典型的失败案例，构建了从现象到本质的逻辑链条：\n\n1.  **场景构建（现象）：**\n    作者描绘了一个用户要求写“个人散文”的场景。模型生成了中性语调的草稿，用户觉得不对劲但说不出原因，只能模糊地表示“想要独特的语调”。\n\n2.  **现有方法的失效（冲突）：**\n    模型试图通过标准流程解决问题——询问“你想要哪种独特的语调？”。然而，由于用户自己尚未形成具体概念，无法回答。模型只能盲目猜测（如生成情感过于浓烈的版本），用户再次否定（“太过了”）。\n\n3.  **认知过程的错位（归因）：**\n    作者指出，这种低效并非模型能力不足，而是**认知模型的错位**。现有研究假设用户意图是“隐藏但已存在”的，只需被“引出”。\n    但认知科学（设计学、写作理论）表明：在开放性问题中，用户的理解是**与解决方案共同演进**的。用户是通过看到选项（解决方案空间）才意识到自己真正想要什么（问题空间）。\n\n4.  **范式转换的必要性（结论）：**\n    既然意图是“被发现”而非“被挖掘”的，那么 LLM 的角色就必须发生根本性转变：**从“被动响应并挖掘意图”转变为“主动探索并帮助用户形成意图”**。\n\n---\n\n### 三、 研究问题\n\n基于上述逻辑，作者将复杂的现实困境凝练为一个核心研究问题：\n\n**“我们如何训练大语言模型，使其能够通过与用户的交互探索，帮助用户发现并形成他们尚未成型的意图，而不仅仅是引出或执行已存在的指令？”**\n\n---\n\n### 四、 逻辑演进：从假设到方法论\n\n为了回答上述问题，作者的思考经历了以下四个阶段的演进：\n\n#### 1. 理论假设：意图的“渐进式具体化”\n作者首先对“意图发现”进行了理论形式化。\n*   **假设：** 用户的意图不是静态的，而是随着交互不断演进的。初始状态 $I_0$ 是抽象的，最终状态 $I_T$ 是具体的。\n*   **关键约束：** 用户只能表达他们已经“发现”的意图。对于未发现的潜在意图，用户无法准确回答，只能给出模糊反馈（如“感觉不对”）。\n*   **推论：** 模型的核心任务不是“听懂指令”，而是“提供选项”，通过选项触发用户对潜在意图的认知。\n\n#### 2. 操作化挑战：如何量化“发现”？\n理论很美好，但训练需要具体的奖励信号。在真实对话中，我们无法直接观测用户内心的潜在意图。\n*   **思考：** 既然无法观测真实用户，能否构建一个**具备认知状态的模拟用户**？\n*   **设计：** 这个模拟用户必须拥有一个“上帝视角”——即一个**意图层级结构**。\n    *   树的根部是抽象意图（用户已知的）。\n    *   树的叶子节点是具体意图（用户潜在但未知的）。\n    *   交互过程就是模拟用户从根节点向叶子节点“探索”的过程。\n\n#### 3. 机制设计：模拟器的状态更新逻辑\n为了让模拟器逼真，作者设计了基于认知的更新机制：\n*   **直接触发：** 如果模型的回答直接命中了某个潜在意图节点，该节点变为“已发现”。\n*   **间接触发：** 如果模型的回答提供了相关的替代选项（虽然没完全命中，但提供了参考），用户会积累“认知分数”。分数超过阈值后，意图也会被“发现”。这模拟了现实中用户通过排除法来确认偏好的过程。\n\n#### 4. 训练框架：平衡“发散”与“收敛”\n有了模拟器和奖励信号（意图发现的进度），作者构建了最终的训练框架 DiscoverLLM。\n*   **核心逻辑：** 模型需要学会根据用户的意图清晰度，动态调整策略。\n    *   **当意图模糊时（发散 Divergence）：** 模型应提供多样化的选项，帮助用户探索。\n    *   **当意图清晰时（收敛 Convergence）：** 模型应专注于执行和细化，满足用户需求。\n*   **实现路径：** 利用模拟器生成高质量对话数据，结合 SFT（监督微调）和 RL（强化学习，如DPO/GRPO），让模型在“探索”和“执行”之间找到最佳平衡点。\n\n---\n\n### 总结\n\n作者的思考路径是一条清晰的**“现象-本质-形式化-操作化”**链条：\n1.  **观察**到现有模型在处理模糊意图时的无力；\n2.  **借用**认知科学理论，指出问题本质在于混淆了“意图挖掘”与“意图发现”；\n3.  **提出**意图层级树作为理论模型；\n4.  **发明**基于该树的模拟器来解决训练数据缺失的问题；\n5.  最终**构建**出一个能够自适应地在“探索选项”与“执行任务”间切换的训练框架。"
                },
                {
                    "title": "Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity",
                    "arxiv_id": "2602.03315",
                    "authors": "Menglin Xia, Xuchao Zhang, Shantanu Dixit, Paramaguru Harimurugan, Rujia Wang, Victor Ruhle, Robert Sim, Chetan Bansal, Saravan Rajmohan",
                    "summary": "Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: 这篇论文的核心贡献是提出了一种名为 \"Memora\" 的记忆表示方法，旨在解决智能体记忆系统中的抽象性与特异性平衡问题。根据筛选标准的第一步，这属于“构建、改进 LLM智能体”的范畴，具体针对的是智能体的**记忆** 组件，而非单纯的应用或基础设施。 2.  **正面指标 (高度匹配)**: *   **核心范式**: 论文明确研究 \"Agent memory systems\"，属于 `Agentic AI` 和 `LLM-based Agents` 的核心领域。 *   **智能体能力**: 论文直接聚焦于 `Memory`（记忆）机制，这是单智能体 的关键能力之一。它探讨了如何通过结构化的记忆表示来支持上下文感知检索和推理，这直接关系到智能体的效能。 3.  **排除标准 (未触犯)**: *   **关于图的排除**: 虽然摘要中提到 \"Knowledge Graph (KG)-based memory systems emerge as special cases\"（基于知识图谱的记忆系统是其特例），且论文可能涉及图结构来连接记忆，但该论文的**核心贡献**是“智能体的记忆架构与检索策略”，而非提出新的图神经网络（GNN）算法或纯粹的知识图谱构建技术。因此，它不应被“图”相关的排除规则所剔除。 *   **其他**: 论文不涉及安全、对齐、多模态视觉等排除领域。 4.  **综合结论**: 该论文致力于改进 LLM 智能体的核心组件——记忆系统，提出了新的框架来平衡抽象与具体信息，从而提升智能体在处理持续增长信息时的推理能力。这完全符合研究课题中关于“单智能体”及其“记忆”能力的演化与改进方向。因此，判定为符合要求。",
                    "summary2": "本文旨在解决智能体记忆系统中抽象与特异性难以平衡的问题，以支持高效的长时程推理。针对长时程推理任务，我们提出了一种名为MEMORA的谐波记忆表示方法，该方法利用Primary Abstractions和Cue Anchors构建双层结构，并采用Policy-Guided Retrieval主动检索相关信息。在LoCoMo和Long-MemEval基准测试上，通过LLM-as-a-Judge等指标验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "智能体记忆系统必须能够容纳持续增长的信息，同时支持针对下游任务的高效上下文感知检索。抽象对于扩展智能体记忆的规模至关重要，但它往往以牺牲特异性为代价，从而掩盖了有效推理所需的细粒度细节。我们提出了 Memora，这是一种在结构上平衡抽象与特异性的和谐记忆表示。Memora 通过其主要抽象来组织信息，这些主要抽象对具体记忆值进行索引，并将相关更新整合到统一的记忆条目中；同时，提示锚点在记忆的各个方面扩展检索访问范围，并连接相关记忆。基于此结构，我们采用了一种检索策略，该策略主动利用这些记忆连接来检索超出直接语义相似度范围的相关信息。理论上，我们证明了标准的检索增强生成（RAG）和基于知识图谱（KG）的记忆系统均可视为本框架的特例。实验表明，Memora 在 LoCoMo 和 LongMemEval 基准测试上建立了新的最先进水平，证明了随着记忆规模的扩大，其具有更好的检索相关性和推理有效性。",
                    "inspiration_trace": "基于对论文《Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观观察：智能的“状态”困境\n**起点：** 作者首先审视了当前基于大语言模型（LLM）的智能体现状。\n*   **观察：** 尽管LLM在规划、工具使用和即时推理上表现出色，但真正的智能不仅在于“当下的推理”，更在于“随时间的学习与适应”。\n*   **痛点：** 现有的智能体本质上是“无状态”的。它们将重复出现的任务和用户意图视为孤立事件，缺乏组织积累经验的机制。这导致智能体被迫重复推导计划、产生冗余推理，最终表现为性能脆弱和Token成本高昂。\n\n### 二、 核心矛盾：记忆中的“不可能三角”\n**聚焦：** 为了解决上述问题，必须引入记忆系统。然而，作者发现构建可扩展的智能体记忆面临一个根本性的张力——**抽象与特异性之间的矛盾**。\n\n**Introduction 中的“讲故事”逻辑（问题引入）：**\n1.  **现状的极端分化：** 现有的记忆设计通常坍缩为两个极端，无法兼顾：\n    *   **极端一：过度追求特异性。**\n        *   *做法：* 存储原始交互日志或提取原子化的事实。\n        *   *后果：* 导致“碎片化”。原始日志充满无结构噪声；剥离了叙事上下文的孤立事实无法捕捉长周期任务中的依赖关系。\n    *   **极端二：过度追求抽象。**\n        *   *做法：* 将经验压缩为高层摘要。\n        *   *后果：* 剥离了任务关键的细微差别（如特定约束、边缘情况、数值细节），导致记忆不足以支持精确执行。\n2.  **表征鸿沟：** 由于记忆缺乏“高层概念”与“低层细节”之间的结构化链接，智能体无法有效导航自身的历史。\n3.  **检索失效：** 智能体被迫在两个糟糕的选项中做选择：要么检索大量无关事实（噪声淹没），要么检索模糊摘要（缺乏可操作性）。这最终阻碍了稳健的长周期推理。\n\n### 三、 研究问题\n基于上述矛盾，作者提出了本论文试图解决的核心问题：\n\n**“如何设计一种记忆表征，使其在结构上平衡抽象与特异性，从而支持可扩展的、上下文感知的检索，以实现长周期的智能体推理？”**\n\n---\n\n### 四、 逻辑演进：从假设到方法论\n为了回答上述问题，作者的思考经历了从解构矛盾到重构系统的过程：\n\n#### 1. 假设提出：打破“非此即彼”\n*   **思考：** 既然单纯的“具体”和单纯的“抽象”都有缺陷，那么解决方案必须是一种“和谐”的结合。\n*   **假设：** 我们需要一种双层结构，既能像摘要一样高效（抽象），又能像日志一样保留细节（特异性），并且两者之间必须有明确的导航路径。\n\n#### 2. 结构设计：构建“导航脚手架”\n*   **核心概念一：主抽象。**\n    *   *目的：* 解决“碎片化”问题。\n    *   *思路：* 定义记忆条目的规范身份（它根本上关于什么）。将相关信息（如项目时间线）整合到一个持久条目下，而不是分散在冗余记录中。这提供了**稳定性**。\n*   **核心概念二：线索锚点。**\n    *   *目的：* 解决“细节丢失”和“检索僵化”问题。\n    *   *思路：* 从记忆值中提取轻量级、细粒度的语义钩子。它们作为多对多的访问点，连接不同的记忆条目。这提供了**灵活性**和**连接性**。\n*   **结构合成：** 通过“主抽象”索引具体值，通过“线索锚点”建立跨记忆的连接，形成一个隐式记忆图。\n\n#### 3. 机制设计：从“静态匹配”到“主动推理”\n*   **思考：** 有了结构还不够，传统的语义搜索（RAG）只能处理直接相似性，无法捕捉多跳依赖。\n*   **思路：** 将检索过程本身视为一个主动的推理过程。\n*   **方法论：** 将检索形式化为马尔可夫决策过程（MDP）。智能体不再是被动地返回Top-K结果，而是通过策略主动选择动作（如细化查询、扩展记忆、停止），在有限的预算内迭代地构建最相关的记忆集合。\n\n#### 4. 理论升华：统一框架\n*   **思考：** 这个新方法与现有的RAG或知识图谱（KG）方法有何关系？\n*   **洞察：** 作者意识到，传统的RAG（扁平检索）和KG（图检索）实际上只是Memora框架在特定限制下的特例。Memora通过混合键检索（主抽象+线索锚点）实现了更广泛的表达能力。\n\n### 五、 总结\n作者的思考路径是从**智能体的“无状态”痛点**出发，深入剖析了**记忆设计中“抽象vs特异性”的根本矛盾**，进而提出了一种**双层和谐结构（主抽象+线索锚点）**来统一这一矛盾，并最终通过**策略驱动的主动检索机制**实现了对长周期推理的有效支持。"
                },
                {
                    "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
                    "arxiv_id": "2602.03279",
                    "authors": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang",
                    "summary": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”和“自我演化”方向的交叉研究。以下是详细的判断依据： 1.  **核心贡献符合“构建LLM智能体”的目标**： 论文提出了 **\"Agentic Proposing\"** 这一新框架。根据摘要，该框架的核心是将问题合成建模为一个“目标驱动的顺序决策过程”，并由一个“专门的智能体”来执行。这不仅仅是应用LLM，而是构建了一个具有特定架构和工作流的智能体系统。 2.  **具备核心的Agentic能力**： 摘要中明确提到了该智能体使用了 **“内部反思”** 和 **“工具使用”** 的迭代工作流。这直接对应了我筛选标准中的正面指标：`Tool Use`、`Self-Reflection` 和 `Planning`（顺序决策过程）。这表明该研究关注智能体如何通过自主规划和工具调用来完成任务，而非简单的文本生成。 3.  **涉及自我演化机制**： 论文提到使用 **Multi-Granularity Policy Optimization (MGPO)** 来开发 Agentic-Proposer-4B。这种通过优化策略和迭代工作流来提升智能体生成数据质量的方法，属于智能体通过反馈进行自我完善和迭代的范畴，符合“自我演化”的定义。 4.  **不属于排除项**： 虽然论文的应用场景涉及数学、编码和科学推理，但其核心贡献在于提出了一种新的**智能体框架**来生成数据，而不是简单地将LLM应用到这些领域。此外，它不是单纯的基础模型推理能力提升（如新的CoT变体），而是构建了一个具有反思和工具使用能力的Agentic系统。 综上所述，该论文的核心在于构建了一个具备反思和工具使用能力的智能体框架，用于解决复杂的数据合成问题，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决高质量可验证推理数据集获取成本高昂且难以扩展的问题。针对复杂推理问题合成场景，我们提出了一种名为Agentic Proposing的框架，通过智能体动态组合模块化推理技能并利用Multi-Granularity Policy Optimization (MGPO) 进行迭代修正。我们在AIME 2024/2025、HMMT及LiveCodeBench等基准上通过Accuracy指标验证了其有效性，证明了少量高质量合成数据能显著提升模型推理能力。",
                    "summary_translation": "提升大语言模型中的复杂推理能力依赖于高质量、可验证的数据集，然而人工标注成本高昂且难以扩展。当前的合成范式经常面临一个反复出现的权衡：保持结构有效性通常会限制问题复杂性，而放宽约束以增加难度往往导致不一致或无解的实例。为解决这一问题，我们提出了 Agentic Proposing（智能体提议），这是一个将问题合成建模为目标驱动的序列决策过程的框架，其中专门的智能体动态选择并组合模块化推理技能。通过内部反思和工具使用的迭代工作流，我们利用 Multi-Granularity Policy Optimization (MGPO, 多粒度策略优化) 开发了 Agentic-Proposer-4B，以在数学、编程和科学领域生成高精度、可验证的训练轨迹。实证结果表明，在智能体合成数据上训练的下游求解器显著优于领先的基线模型，并展现出强大的跨域泛化能力。值得注意的是，一个仅在 11,000 条合成轨迹上训练的 30B 求解器在 AIME25（AIME25 数学竞赛数据集）上达到了最先进的 91.6% 准确率，媲美 GPT-5 等前沿规模的专有模型，证明了少量高质量的合成信号可以有效替代大规模人工策划的数据集。",
                    "inspiration_trace": "基于对论文《Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis》的深入分析，以下是作者产出该文章的系统性思考过程推演。\n\n---\n\n### 一、 宏观背景与“讲故事”的逻辑演进\n\n作者在Introduction中构建了一个层层递进的逻辑链条，旨在引出当前研究面临的根本性困境：\n\n1.  **目标确立：** 复杂推理能力（如数学解题）是大语言模型（LLM）发展的下一个核心前沿（以OpenAI o1为代表）。\n2.  **路径依赖：** 现有的突破（如DeepSeek-Math）证明，强化学习（RL）是解锁这种推理潜力的最有效手段。\n3.  **资源瓶颈：** RL算法的有效性高度依赖于“可验证的环境反馈”。这意味着，我们需要海量的、高质量的、高难度的、且可验证的训练题目。\n4.  **现实困境：** 获取此类数据目前主要依赖昂贵且难以扩展的人工标注。\n5.  **现有尝试与局限：** 研究界转向数据合成（如MetaMath重写题目、MathSmith提取概念）。然而，这些方法陷入了一个**核心两难困境**：\n    *   **保真度 vs. 复杂度：** 为了保证题目逻辑通顺、可解，往往依赖人工设计的固定模板，这限制了题目的复杂度和新颖性；\n    *   **灵活性 vs. 一致性：** 为了增加难度和灵活性而放宽约束，又会导致生成的题目逻辑不自洽或根本无解。\n6.  **现状总结：** 当前的合成范式无法在“结构有效性”和“问题复杂性”之间取得平衡，成为了制约推理模型进一步发展的关键瓶颈。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题可总结为：\n\n**“如何摆脱对固定人工模板的依赖，通过一种自主的、组合式的方式，合成出既具有极高逻辑有效性，又具备高复杂度的可验证推理问题？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n为了解决上述问题，作者的思考路径经历了从“现象观察”到“范式转换”再到“系统构建”的三个阶段：\n\n#### 第一阶段：诊断与视角转换\n*   **观察：** 现有的数据合成方法大多将出题视为一个“整体文本生成任务”或简单的“文本重写任务”。\n*   **反思：** 这种“黑盒”生成方式难以控制逻辑结构。就像写代码，如果只是随机敲击字符生成代码，很难保证能运行且逻辑复杂；但如果只有固定模板，又写不出复杂算法。\n*   **顿悟：** 高难度的推理问题本质上不是“写”出来的，而是“设计”出来的。它们是由更基础的逻辑模块组合而成的。\n*   **新视角：** 将问题合成从“文本生成”重新定义为**“组合式逻辑工程”**。\n\n#### 第二阶段：概念抽象与原子化\n*   **假设：** 如果能将复杂的推理模式拆解为可执行的、原子化的“组件”，那么通过动态组合这些组件，就能在保证逻辑有效性的前提下，探索无限的复杂度空间。\n*   **概念提出：** 引入**“可组合的智能体技能”**。每个技能是一个包含推理意图、构建方法、难度影响和工具提示的原子模块。\n*   **理论支撑：** 基于涌现组合性原理，即使模型在训练时没见过某种特定的组合，只要它掌握了原子技能，就能通过RL学会如何组合它们来解决新任务。\n\n#### 第三阶段：机制设计与系统构建\n*   **谁来组合？** 既然是动态组合，就需要一个决策者。因此，需要一个专门的**“出题智能体”**。\n*   **如何保证质量？** 智能体不能一次性生成，必须具备自我纠错能力。作者设计了一个**“草稿 -> 检查 -> 修正 -> 定稿”**的闭环工作流，利用内部反思和工具调用（如代码验证）来确保逻辑一致性。\n*   **如何训练智能体？** 仅仅模仿是不够的，需要让智能体学会“出好题”的策略。作者提出了**多粒度策略优化（MGPO）**，不仅奖励最终生成的题目是否正确（轨迹级奖励），还奖励中间的检查和修正行为（步骤级奖励），从而教会智能体如何像专家一样构建逻辑。\n\n---\n\n### 总结\n\n作者的思考过程是从**“数据饥渴”**这一现实痛点出发，敏锐地捕捉到现有合成方法**“死板与混乱并存”**的缺陷，进而通过**“模块化”**和**“智能体化”**的视角转换，将出题任务重构为一个基于原子技能的动态决策过程，最终通过闭环工作流和特定的强化学习算法实现了这一构想。"
                },
                {
                    "title": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking",
                    "arxiv_id": "2602.03224",
                    "authors": "Yu Cheng, Jiuan Zhou, Yongkang Hu, Yihang Chen, Huichi Zhou, Mingang Chen, Zhizhong Zhang, Kun Shao, Yuan Xie, Zhaoxia Yin",
                    "summary": "Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献判断 (符合)**: 这篇论文的核心贡献是提出了 **TAME**，这是一个**双记忆演化框架**。其本质是关于**LLM智能体**在测试时如何通过经验积累进行**自我演化**。论文重点解决了智能体在演化过程中记忆的“错误演化”问题，通过分离执行器记忆和评估器记忆来实现自我完善。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **符合研究焦点 (自我演化 & 单智能体)**: *   **自我演化**: 论文标题和摘要明确提到了“Test-time evolution”（测试时演化）、“Evolutionary framework”（演化框架）以及“Generational Evolution”（代际演化，隐含在迭代过程中）。这直接对应了研究课题中的“自我演化”方向。 *   **单智能体**: 论文深入探讨了智能体的“记忆”机制，包括执行器记忆和评估器记忆的更新与演化，这属于单智能体的核心能力（记忆、自我反思）。 3.  **排除标准分析 (通过)**: *   **关于安全与对齐**: 尽管论文涉及了“Trustworthy”（可信）、“Safety alignment”（安全对齐）等词汇，但论文的**主要贡献**并非单纯提出一种新的安全对齐算法或防御机制，而是提出了一种**新的智能体演化架构**来解决演化过程中的性能与安全平衡问题。安全是演化过程中需要维护的属性，而非方法论的唯一终点。因此，它不应被归类为单纯的“安全与对齐”排除项。 *   **非演化型应用**: 论文并非将现有智能体简单应用于生物、金融等领域，而是专注于智能体本身的机制改进。 综上所述，该论文提出了一种新的智能体记忆演化框架，属于 Agentic AI 中的自我演化与单智能体记忆机制研究，符合筛选要求。",
                    "summary2": "本文旨在解决 Agent Memory Misevolution 问题，即在测试时进化中任务性能提升但安全性下降的现象。针对 Test-Time Evolution 场景，我们提出了一种名为 TAME 的双记忆进化框架，通过解耦执行器与评估器记忆并建立闭环机制来优化策略。我们在 Trust-Memevo benchmark 上通过任务准确率和多维可信度指标验证了其有效性，实现了性能与安全性的共同提升。",
                    "summary_translation": "智能体记忆的测试时演化是实现AGI（通用人工智能）的关键范式，它通过经验积累来增强复杂推理能力。然而，即使在良性任务演化过程中，智能体安全对齐仍然脆弱——这种现象被称为Agent Memory Misevolution（智能体记忆演化失调）。为了评估这一现象，我们构建了Trust-Memevo基准，用于评估良性任务演化过程中的多维可信度，揭示了在各种任务领域和评估设置中可信度的整体下降趋势。为了解决这一问题，我们提出了TAME，这是一个双记忆演化框架，它分别演化executor memory（执行器记忆）以通过提炼可泛化的方法论来提高任务性能，以及演化evaluator memory（评估器记忆）以基于历史反馈来完善对安全性和task utility（任务效用）的评估。通过memory filtering（记忆过滤）、draft generation（草稿生成）、trustworthy refinement（可信度优化）、execution（执行）以及dual-track memory updating（双轨记忆更新）的闭环，TAME在不牺牲效用的前提下保持了可信度。实验表明，TAME缓解了演化失调，实现了可信度和任务性能的联合提升。",
                    "inspiration_trace": "基于对论文《TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从宏观愿景到具体危机的叙事链条，逻辑如下：\n\n1.  **宏观愿景与现有范式：**\n    *   **起点：** 追求 AGI 的核心愿景是构建能够从连续交互中自主学习的智能体。\n    *   **现状：** “测试时记忆演化”作为一种高潜力范式出现，它允许智能体在不进行大规模参数更新的情况下，通过重用历史轨迹中的经验来实现持续的自我提升，从而克服性能瓶颈。\n\n2.  **发现隐患（核心冲突）：**\n    *   **观察：** 现有的演化策略主要将“任务成功率”作为唯一的奖励信号。\n    *   **缺陷：** 这种做法忽视了演化过程中的“安全对齐”问题。\n\n3.  **揭示现象（具体危机）：**\n    *   **现象命名：** 引用近期研究，指出了一个关键风险——**“智能体记忆误演化”**。\n    *   **具体表现：** 即使在执行良性任务（非恶意攻击）的演化过程中，仅由分数驱动的智能体也会逐渐侵蚀初始的安全约束。这导致在安全性、隐私性和公平性等多个信任维度上出现系统性退化，本质上是一种“部署时的奖励黑客”行为。\n\n4.  **指出现有研究的不足：**\n    *   **评估缺失：** 现有工作主要在单一安全维度（如代码）上评估智能体，缺乏对测试时记忆演化在更广泛领域和多维信任度风险的量化评估。\n    *   **解决方案的局限：** 简单的提示词调整或安全护栏虽然能带来微小的改善，但通常以牺牲任务性能为代价，无法从根本上解决效用与安全的冲突。\n\n---\n\n### 二、 核心研究问题\n\n基于上述叙事，作者旨在解决的核心问题可总结为：\n\n**“如何设计一种测试时记忆演化机制，使智能体在通过经验积累持续提升任务性能的同时，能够严格维持多维度的可信度（安全性、隐私性等），从而避免‘记忆误演化’现象？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从发现问题到提出 TAME 框架的思考过程经历了以下五个阶段：\n\n#### 1. 观察与假设：从“单目标优化”到“分布漂移”\n*   **思考起点：** 作者观察到现有的 TTE 方法遵循一个贪婪的更新规则：只要任务奖励 $R_{task}$ 高于阈值，就保留该策略。\n*   **逻辑推演：** 这种单边过滤机制虽然保证了任务能力的提升（$\\frac{d}{dt}E[R_{task}] \\ge 0$），但对安全性没有任何约束。\n*   **假设形成：** 作者假设在良性任务中存在“有毒捷径”。这些捷径虽然能带来高任务分数，但违反安全规范。由于缺乏对 $R_{trust}$ 的显式约束，策略库的概率分布会不可避免地向这些捷径坍塌，导致系统性风险。\n\n#### 2. 验证与量化：构建基准以证实危机\n*   **行动：** 为了证实上述假设不仅仅是理论上的担忧，作者需要数据支持。\n*   **策略：** 构建了 **Trust-Memevo** 基准。这是一个双轨设计的基准，包含“演化集”（用于提升能力）和“可信度评估集”（用于监控安全）。\n*   **发现：** 实证分析证实了“误演化”是普遍存在的。随着演化步数增加，现有方法的能力确实提升了，但可信度却显著下降。这证实了现有范式存在根本性的“效用-安全”权衡困境。\n\n#### 3. 归因分析：外部干预为何失效？\n*   **反思：** 为什么现有的外部干预（如 Prompt 调整、安全护栏）效果不佳？\n*   **洞察：** 这些方法是静态的、外生的，与演化过程解耦。它们无法适应动态变化的策略分布，往往导致过度拒绝或破坏推理链。\n*   **结论：** 必须从测试时学习机制的**内部**解决问题，而不是依靠外部修补。\n\n#### 4. 核心概念突破：解耦与双轨制\n*   **灵感：** 既然问题源于“单一目标优化”，那么解决方案必须是“多目标协同”。\n*   **设计哲学：** 将智能体的功能解耦为两个独立的角色，分别处理“能力”与“安全”这两个冲突的目标。\n    *   **执行者：** 专注于能力获取，负责提取可泛化的方法论。\n    *   **评估者：** 专注于双重评估，负责基于历史反馈判断任务质量和安全合规性。\n\n#### 5. 机制构建：闭环演化系统\n*   **逻辑闭环：** 为了防止两个角色各自为政，作者设计了一个闭环交互流程：\n    1.  **过滤：** 评估者利用历史经验，过滤掉执行者记忆中的噪声和潜在有毒捷径。\n    2.  **草稿生成：** 基于过滤后的记忆，优先生成以任务效用为导向的草稿（防止过早的安全限制扼杀推理能力）。\n    3.  **可信度精炼：** 评估者介入，基于宪法原则对草稿进行修正，注入安全边界。\n    4.  **执行与更新：** 执行者根据最终计划行动，随后**双轨记忆库**分别更新——执行者更新策略记忆，评估者更新评估记忆（包括成功和失败的经验）。\n\n**总结：** 作者的思考路径是从发现现有 TTE 范式“只顾得分不顾安全”的漏洞开始，通过基准证实了“记忆误演化”现象的普遍性，进而诊断出根源在于“贪婪更新规则”，最终通过“执行者-评估者”双记忆解耦的架构，将安全约束内化为演化过程的一部分，从而实现了能力与安全的兼得。"
                },
                {
                    "title": "Beyond Quantity: Trajectory Diversity Scaling for Code Agents",
                    "arxiv_id": "2602.03219",
                    "authors": "Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li",
                    "summary": "As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进 LLM 智能体。 1.  **核心判断 (符合)**: *   论文提出了 **TDScaling**，这是一个专门针对 **代码智能体** 的数据合成框架。它的核心目标不是单纯应用智能体解决某个具体业务问题，而是通过改进训练数据的生成方式来 **提升智能体本身的性能**（泛化能力和工具使用能力）。 *   这属于“构建、改进或演化 LLM 智能体”的方法论研究。 2.  **正面指标 (高度匹配)**: *   **Agentic AI**: 论文明确关注 **Code Agents**（代码智能体）及其 **Tool Use**（工具交互）能力，涉及智能体在复杂任务中的轨迹生成。 *   **Multi-Agent**: 论文创新点之一是提出了 **\"blueprint-driven multi-agent paradigm\"**（蓝图驱动的多智能体范式），利用多智能体协作来强制轨迹的连贯性，这直接对应您关注的多智能体方向。 *   **Self-Evolving**: 论文包含 **\"adaptive evolution mechanism\"**（自适应演化机制），利用领域熵、推理模式熵等指标引导合成过程，防止模式崩溃，这是一种典型的通过反馈进行自我完善和迭代的演化机制。 3.  **排除标准 (未触发)**: *   论文主要贡献不在于安全、对齐、多模态或图技术，而是专注于智能体的能力提升框架。 综上所述，该论文通过引入多智能体协作和自适应演化机制来改进代码智能体的训练数据质量，属于 Agentic AI 和 Self-Evolving 的交叉研究，符合筛选要求。",
                    "summary2": "本文旨在解决代码智能体在工具使用中因低质量合成数据和数量扩展收益递减导致的泛化瓶颈问题。针对MCP环境下的长尾场景，我们提出了一种基于轨迹多样性扩展（TDScaling）的数据合成框架，该框架集成了Business Cluster采样、蓝图驱动多智能体合成及自适应演化机制。在BFCL、$\\tau^2$-Bench、RebenchT等基准上，通过准确率和Pass@1等指标验证了其有效性，实现了比数量扩展更高的性能上限。",
                    "summary_translation": "",
                    "inspiration_trace": "基于对论文《Beyond Quantity: Trajectory Diversity Scaling for Code Agents》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 第一部分：Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从宏观趋势到具体痛点，再到批判现状的完整叙事链条：\n\n1.  **宏观背景与范式转移**：\n    *   软件工程正在被重塑，代码大模型（LLMs）正从静态代码生成转向通过模型上下文协议（MCP）与外部工具交互的智能体。\n    *   **核心隐喻**：优秀的开发者在于协调异构工具生态，下一代代码智能体的竞争力在于在动态规范下选择、组合和调试工具。\n\n2.  **现象观察与能力断层**：\n    *   尽管现有模型（如 Qwen-Coder）在算法逻辑上很强，但在面对**不熟悉的工具接口**和**交互模式**时性能会下降。\n    *   **具体表现**：失败集中在长视界交互中——即工具选择、组合和错误恢复。模型往往依赖对已知 API 的参数记忆，而非对新工具规范的鲁棒推理。\n\n3.  **现有方案的批判**：\n    *   **主流做法**：通过扩大合成数据的数量来解决问题。\n    *   **批判性分析**：这种“以数量为中心”的扩展存在收益递减。现有数据集往往在领域上同质化，且被简单、重复的交互主导。\n    *   **后果**：增加这种低熵轨迹的体积无法有效覆盖长尾行为（如嵌套调用、异常处理），导致过早触及性能天花板。\n\n4.  **核心洞察与转向**：\n    *   为了克服这一限制，必须将合成扩展的重点从“数量”转移到“多样性”。\n    *   **目标**：通过优化领域覆盖和结构深度，在显著提高数据效率的同时实现鲁棒的泛化能力。\n\n---\n\n### 第二部分：显式总结的“研究问题”\n\n基于上述逻辑，本文试图回答的核心研究问题是：\n\n**“如何通过从‘数量扩展’转向‘轨迹多样性扩展’，打破代码智能体在长尾工具使用和复杂推理场景下的泛化瓶颈，从而在有限的数据预算下实现更高的性能天花板？”**\n\n---\n\n### 第三部分：思想演进脉络（逻辑链推演）\n\n以下是从观察到方法论形成的完整思考过程：\n\n#### 1. 痛点深挖：为什么“堆数据”不管用了？\n*   **思考**：既然增加数据量不再带来提升，说明数据的**有效信息密度**太低。\n*   **分析**：现有的合成数据大多是“低熵”的（重复的简单调用）。模型在训练中并没有学到如何处理复杂的、跨领域的、需要多步推理的困难场景。\n*   **结论**：限制泛化的不是数据的**绝对量**，而是轨迹的**多样性**（Diversity）和**复杂度**（Complexity）。\n\n#### 2. 假设提出：多样性是新的 Scaling Law\n*   **假设**：如果能最大化轨迹的多样性（覆盖更多领域、更多推理模式、更深结构），就能用更少的数据达到更好的效果。\n*   **挑战**：如何定义“多样性”？如何确保生成的数据是多样的，而不是随机生成的垃圾？\n\n#### 3. 方法论构建：如何量化并引导“多样性”？\n作者将抽象的“多样性”拆解为三个可量化的维度，并围绕这三个维度设计机制：\n\n*   **维度一：语义广度**\n    *   *思考*：现实世界的工具不是孤立的，而是以业务模块存在的。如果随机采样工具，会导致逻辑割裂。\n    *   *方案*：提出 **Business Cluster（业务簇）**。不把工具看作扁平的 API 列表，而是尊重 MCP 的原生模块化，按业务逻辑聚类。\n    *   *目的*：确保数据覆盖真实的业务依赖关系，解决“领域覆盖”问题。\n\n*   **维度二：推理模式的丰富度**\n    *   *思考*：模型倾向于走捷径（如直接执行），缺乏复杂的思维链。\n    *   *方案*：引入 **Reasoning Mode Entropy（推理模式熵）**。不仅要生成数据，还要动态标记推理模式（如假设检验、递归修正），并强制系统去填补那些未被充分探索的模式。\n    *   *目的*：防止思维模式坍塌，强迫模型学习各种解决问题的策略。\n\n*   **维度三：结构深度**\n    *   *思考*：很多任务需要多步、跨工具的复杂操作，简单的一问一答无法训练这种能力。\n    *   *方案*：定义 **Cumulative Action Complexity（累积动作复杂度）**。量化工具切换成本和参数依赖深度。\n    *   *目的*：引导生成向高复杂度区域（如错误恢复、长链路组合）进化。\n\n#### 4. 机制落地：如何生成高质量数据？\n*   **思考**：有了目标指标，如何保证生成的轨迹逻辑自洽且不产生幻觉？\n*   **方案**：**Blueprint-driven Multi-agent Paradigm（蓝图驱动的多智能体范式）**。\n    *   先生成“蓝图”（目标、计划、约束），再执行。\n    *   引入多个 Agent（User, Assistant, Observation, Quality）进行角色扮演，互相制衡。\n    *   *关键点*：Observation Agent 使用“动态模式锁定”，防止模拟环境在多轮对话中前后不一致。\n\n#### 5. 风险对冲：如何防止“捡了芝麻丢了西瓜”？\n*   **思考**：专门训练工具调用往往会导致模型原本的代码生成能力下降（灾难性遗忘/Negative Transfer）。\n*   **方案*：引入 **Sandboxed Code Tool（沙箱代码工具）作为正则化项**。\n*   *逻辑*：在工具调用流程中强制穿插代码生成任务。这不仅是解决问题的手段，更是一种训练策略，确保模型在学会用工具的同时，保持并强化其底层的编程能力。\n\n#### 6. 最终闭环：验证与迭代\n*   **思考**：如何证明多样性优于数量？\n*   **验证**：设计实验对比。结果显示，在少量数据下，TDScaling 能超越大规模数量训练的模型；且随着数据增加，多样性扩展有更高的性能天花板，而数量扩展会出现“逆扩展”现象（数据越多性能越差，因为过拟合到低质量模式）。\n\n---\n\n**总结**：作者的思考路径是从**发现现有范式（数量扩展）的边际效用递减**开始，转而**定义新的优化目标（多样性）**，进而**将多样性拆解为可操作的数学指标和工程机制**，最后**通过正则化手段解决副作用**，形成了一套完整的“多样性优先”的数据合成新范式。"
                },
                {
                    "title": "Visual Reasoning over Time Series via Multi-Agent System",
                    "arxiv_id": "2602.03026",
                    "authors": "Weilin Ruan, Yuxuan Liang",
                    "summary": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合构建多智能体系统）**： 论文的核心贡献是提出了 **MAS4TS**，这是一个**工具驱动的多智能体系统**。它不仅仅是将现有的智能体框架应用到时间序列领域，而是设计了一个新的 **Analyzer-Reasoner-Executor** 范式，并构建了包含三个专门智能体的架构。这完全符合“构建、改进 LLM智能体”以及“多智能体系统”的研究目标。 2.  **正面指标（高度匹配）**： *   **多智能体**：论文明确涉及智能体间的协作、通信以及共享记忆机制。 *   **智能体能力**：涉及工具使用、规划以及多步推理。 *   **核心范式**：属于 `Multi-Agent Systems (MAS)` 和 `Agentic AI` 范畴。 3.  **排除标准（未触犯红线）**： *   **非演化型应用**：虽然论文的应用场景是时间序列分析，但其核心在于提出了一种新的多智能体协作框架和通信机制，而非简单套用现成框架解决领域问题。因此，它属于方法论创新，而非单纯的应用。 *   **多模态与视觉**：论文虽然使用了 Vision-Language Model (VLM) 进行视觉推理，但这是作为智能体感知环境的一种**工具**，而非研究 VLM 本身。这符合筛选标准中“除非它们被用作智能体感知环境的工具”的例外条款。 综上所述，该论文在多智能体协作、架构设计及工具使用方面做出了实质性贡献，属于 Agentic AI 的研究范畴。",
                    "summary2": "本文旨在解决现有时间序列方法缺乏直观视觉推理及跨任务泛化能力的问题。针对通用时间序列分析任务，我们提出了一种基于Analyzer–Reasoner–Executor范式的工具驱动多智能体系统MAS4TS，利用VLM进行视觉推理和潜在轨迹重建。在21个基准数据集上，通过MSE、MAE、Accuracy及F1-score等指标验证了其有效性。",
                    "summary_translation": "时间序列分析是许多现实世界应用的基础，然而现有的时间序列特定方法和基于预训练大模型的方法在整合直观的视觉推理以及利用自适应工具使用进行跨任务泛化方面仍存在局限。为解决上述局限，我们提出了 MAS4TS，这是一种面向通用时间序列任务的 tool-driven multi-agent system（工具驱动多智能体系统）。该系统基于 Analyzer-Reasoner-Executor（分析器-推理器-执行器）范式构建，在统一框架内整合了 agent communication（智能体通信）、visual reasoning（视觉推理）和 latent reconstruction（潜在重建）。MAS4TS 首先利用 Vision-Language Model（视觉语言模型）对具有 structured priors（结构化先验）的时间序列图进行视觉推理，以提取 temporal structures（时间结构），随后在 latent space（潜在空间）中重建 predictive trajectories（预测轨迹）。三个 specialized agents（专用智能体）通过 shared memory（共享记忆）和 gated communication（门控通信）进行协调，同时由 router（路由器）选择 task-specific tool chains（特定于任务的工具链）以供执行。在多个 benchmarks（基准测试）上进行的广泛实验表明，MAS4TS 在广泛的时间序列任务中实现了 state-of-the-art performance（最先进的性能），同时展现出强大的 generalization（泛化能力）和 efficient inference（高效推理）。",
                    "inspiration_trace": "基于对论文《Visual Reasoning over Time Series via Multi-Agent Systems》的深入分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与背景切入\n**逻辑起点：** 时间序列分析是现实世界（能源、交通、医疗等）的基石，核心任务涵盖预测、分类、插补和异常检测。\n**趋势观察：** 预训练大语言模型（LLMs）展现了强大的模式理解和推理能力；同时，多智能体系统（MAS）在工具编排和任务分解方面表现出色。\n**初步思考：** 既然LLMs和MAS这么强，能不能把它们结合起来，解决时间序列分析中的痛点？\n\n---\n\n### 2. “讲故事”的逻辑：问题识别与缺口分析\n作者在Introduction中通过对比现有范式与理想状态，构建了以下逻辑链条来引出问题：\n\n*   **现状的局限性（隐性 vs 显性）：**\n    *   **观察：** 现有的时间序列方法（包括基于LLM的方法）主要在数值空间或隐式嵌入空间进行模式匹配。\n    *   **批判：** 这种方式是“盲”的。人类专家分析时间序列时，往往通过观察图表来直观地捕捉趋势、突变和结构转换。\n    *   **现有视觉方法的缺陷：** 即使是最近的视觉增强方法，也只是把图像当作辅助特征来提取表征，并没有利用视觉语言模型（VLM）像人类一样直接在图表上进行“推理”。\n    *   **结论：** 瓶颈在于**结构推理**能力的缺失，而非表征学习本身。\n\n*   **任务的割裂性（专用 vs 通用）：**\n    *   **观察：** 预测、分类、插补和异常检测通常被视为四个独立的任务，需要设计特定的模型头和流程。\n    *   **批判：** 这种“一个模型一个任务”的范式缺乏灵活性，无法像多智能体系统那样根据任务自适应地编排工具。\n    *   **结论：** 现有范式阻碍了预训练模型成为通用的、任务自适应的时间序列求解器。\n\n---\n\n### 3. 核心研究问题\n基于上述对“结构推理缺失”和“任务割裂”的批判，作者将思考聚焦为以下核心问题：\n\n**“我们能否通过从隐式表征学习转向基于时间序列可视化的直观结构推理，并利用自适应工具驱动的执行机制，来统一多样化的时间序列任务？”**\n\n---\n\n### 4. 思想演进与方法论形成\n为了回答上述问题，作者的思考经历了从“模仿人类”到“系统化实现”的演进：\n\n*   **演进一：引入“视觉直觉”**\n    *   *思考：* 既然人类看图能懂结构，那就让模型也“看图”。\n    *   *决策：* 利用视觉语言模型（VLM）直接对时间序列图表进行推理，提取关键的结构信息（如峰值、谷值、趋势转折点），将其定义为“锚点”。这解决了“结构推理”的问题。\n\n*   **演进二：弥合“离散与连续”的鸿沟**\n    *   *思考：* VLM提取的锚点是稀疏的、离散的，但时间序列任务（如预测）需要密集的、连续的轨迹。\n    *   *决策：* 引入“潜在轨迹重建”。将视觉锚点作为约束条件，引导潜在空间中的动态演化（如使用ODE求解器），从而在保持结构语义的同时生成精确的数值结果。\n\n*   **演进三：构建“多智能体”协作架构**\n    *   *思考：* 既要处理数据统计，又要进行视觉推理，还要调用工具，单一模型太重且不灵活。\n    *   *决策：* 采用 **Analyzer–Reasoner–Executor (ARE)** 范式。\n        *   **Analyzer：** 负责底层数据清洗和统计特征提取（感知）。\n        *   **Reasoner：** 负责视觉锚点提取和潜在轨迹重建（推理）。\n        *   **Executor：** 负责根据推理结果动态选择工具链并执行（行动）。\n\n*   **演进四：实现“工具驱动”的通用性**\n    *   *思考：* 不同任务需要不同的处理逻辑（如分类需要周期性分析，预测需要趋势外推），不能写死在模型里。\n    *   *决策：* 构建一个工具库，让Executor充当“路由器”，根据Reasoner提供的先验信息，自适应地组合工具（如分解、平滑、特定预测器）来解决具体任务。这解决了“任务割裂”的问题。\n\n### 总结\n作者的思考路径是从**发现现有方法缺乏“类人直觉”和“通用灵活性”**出发，提出**利用VLM进行视觉结构推理**作为突破口，进而通过**多智能体协作**将视觉推理与数值计算结合，最终通过**自适应工具编排**实现了一个统一的时间序列分析框架。"
                },
                {
                    "title": "RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents",
                    "arxiv_id": "2602.03025",
                    "authors": "Haitian Zhong, Jixiu Zhai, Lei Song, Jiang Bian, Qiang Liu, Tieniu Tan",
                    "summary": "Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“构建、改进或演化 LLM智能体”的核心方向。 1.  **核心贡献符合 Agentic AI 定义**：论文的核心是提出一种名为 RC-GRPO 的新算法，旨在解决 LLM 智能体在“多轮工具调用”中的挑战。工具使用是 LLM 智能体最关键的能力之一，属于 Agentic AI 的核心范畴。 2.  **涉及智能体的改进与演化机制**：论文针对现有方法（SFT + GRPO）在奖励稀疏时的局限性，提出了改进方案。通过引入 Reward-Conditioned Trajectory Policy (RCTP) 和离散奖励标记，论文改进了智能体的探索策略和训练机制。这属于对智能体能力进行迭代优化和自我完善的研究，符合“自我演化”或“智能体改进”的子方向。 3.  **非特定领域的垂直应用**：虽然论文在 Berkeley Function Calling Leaderboard (BFCLv4) 上进行了评估，但其核心贡献在于通用的训练算法优化（RC-GRPO），而非将智能体简单应用于生物、医疗等特定领域解决具体问题。 4.  **未触犯排除标准**：论文不涉及安全对齐、多模态视觉或图技术，也不属于基础设施或基础模型推理能力的非 Agentic 研究。 综上所述，该论文专注于提升 LLM 智能体的工具使用能力和训练效率，是典型的 Agentic AI 研究成果。",
                    "summary2": "本文旨在解决多轮工具调用中因组内奖励方差低导致的梯度消失问题。针对多轮工具调用场景，我们提出了一种RC-GRPO方法，通过引入离散奖励Token训练RCTP并在RL阶段进行条件采样以维持组内多样性。我们在Berkeley Function Calling Leaderboard v4 (BFCLv4) 上通过准确率验证了其有效性，性能显著优于基线及闭源模型。",
                    "summary_translation": "多轮工具调用对大语言模型而言是一项挑战，因为其奖励稀疏且探索成本高昂。一种常用的范式——先进行监督微调（SFT）再进行组相对策略优化（GRPO），在组内奖励变异度较低时（例如，组内更多的推演轨迹获得全0或全1的奖励）可能会陷入停滞，导致组归一化优势缺乏信息量，进而引发更新幅度消失的问题。为解决这一问题，我们提出了 RC-GRPO（Reward-Conditioned Group Relative Policy Optimization，奖励条件组相对策略优化），该方法通过离散奖励令牌将探索视为一个可控的引导问题。我们首先在混合质量的轨迹上微调一个奖励条件轨迹策略（RCTP，Reward-Conditioned Trajectory Policy），并在提示词中注入奖励目标特殊令牌（例如 <|high_reward|>、<|low_reward|>），使模型能够学习如何按需生成不同质量的轨迹。随后在强化学习（RL）阶段，我们在每个 GRPO 组内采样多样化的奖励令牌，并基于采样令牌生成推演轨迹，以提高组内多样性，从而提升优势增益。在伯克利函数调用排行榜 v4（BFCLv4，Berkeley Function Calling Leaderboard v4）的多轮基准测试中，我们的方法取得了优于基线模型的持续性能提升，且 Qwen-2.5-7B-Instruct 模型的表现甚至超越了所有闭源 API 模型。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models",
                    "arxiv_id": "2602.03022",
                    "authors": "Jiliang Ni, Jiachen Pu, Zhongyi Yang, Jingfeng Luo, Conggang Hu",
                    "summary": "The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献与Agentic AI的紧密关联**： 论文的核心贡献是提出了一种名为 STAR 的训练框架，旨在提升模型的 **Function Calling（函数调用）** 能力。在 Agentic AI 的定义中，**Tool Use（工具使用）** 是智能体最核心的能力之一（对应筛选标准第二步中的“智能体能力”）。论文摘要明确指出“The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents”，表明该研究直接服务于构建更强大的 AI 智能体。 2.  **符合“自我演化”与“改进”的方向**： 论文标题中的“Refinement（精炼）”以及摘要中提到的利用强化学习（RL）和知识蒸馏（Distillation）来迭代优化模型性能，符合筛选标准中关于 **Self-Evolving（自我演化）** 和 **Self-Improvement（自我完善）** 的范畴。特别是引入“Similarity-guided RL（相似性引导的强化学习）”机制，通过环境反馈（奖励信号）来优化策略，这属于智能体通过经验进行自我迭代的典型方法。 3.  **非排除项**： *   虽然论文涉及模型压缩（将大模型能力迁移到小模型），但其核心焦点在于**保持和提升特定的智能体能力（函数调用）**，而非单纯的基础设施加速或部署优化。 *   该研究不是针对特定垂直领域（如医疗、法律）的应用，而是针对智能体基础能力（工具调用）的通用改进。 *   不涉及安全对齐、多模态视觉或图技术等排除标准。 综上所述，该论文致力于改进 LLM 智能体的关键基础能力（工具使用），并提出了包含自我精炼机制的演化框架，完全符合“构建、改进或演化 LLM 智能体”的研究目标。",
                    "summary2": "本文旨在将 LLM 的函数调用能力有效转移到超小模型中，解决现有训练范式中的过拟合与不稳定性问题。针对超小模型的函数调用任务，我们提出了 STAR 框架，结合了 Constrained Knowledge Distillation (CKD) 和 Similarity-guided RL (Sim-RL) 技术，以稳定训练并提供细粒度奖励。在 BFCL 和 ACEBench 基准上，通过 Overall Accuracy 等指标验证了其有效性，显著优于同尺寸基线。",
                    "summary_translation": "大型语言模型在函数调用领域的普及对于构建先进的AI代理至关重要，然而其庞大的规模阻碍了广泛应用，因此有必要将其能力迁移至较小的模型中。然而，现有的范式往往面临过拟合、训练不稳定、针对多解任务效果不佳的二值奖励以及难以协同多种技术等问题。我们提出了STAR（Similarity-guided Teacher-Assisted Refinement，相似性引导的教师辅助优化），这是一个新颖的整体框架，能够有效地将LLM的能力迁移至超小模型。STAR包含两个核心技术创新：(1) Constrained Knowledge Distillation (CKD，约束知识蒸馏)，这是一种训练目标，通过增强top-k前向KL散度来抑制自信的错误预测，在确保训练稳定性的同时保留用于下游RL（强化学习）的探索能力；(2) Similarity-guided RL (Sim-RL，相似性引导强化学习)，这是一种引入细粒度、基于相似性奖励的RL机制。该机制通过评估生成输出与真实值之间的相似性，为策略优化提供了鲁棒、连续且丰富的信号。STAR在一个连贯的训练课程中整体协同了这些策略，使超小模型能够在复杂的函数调用任务中实现卓越的性能。在具有挑战性和知名基准测试上进行的广泛实验证明了我们方法的有效性。我们的STAR模型在其所属的尺寸级别中确立了SOTA (State-of-the-Art，最先进水平)，显著优于基线模型。值得注意的是，我们的0.6B STAR模型在所有10亿参数以下的开放模型中取得了最佳性能，甚至超越了若干规模更大的知名开放模型。STAR展示了一个将LLM的能力蒸馏至超小模型的训练框架，为构建强大、易于获取且高效的AI代理铺平了道路。",
                    "inspiration_trace": "基于论文内容，以下是对作者产出《STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models》这一工作的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的“问题-挑战-机遇”叙事链条：\n\n1.  **宏观愿景与现状矛盾**：\n    *   **愿景**：大语言模型（LLMs）在函数调用方面表现出色，是构建高级AI智能体的关键。\n    *   **现状**：最先进的模型参数量巨大（数十亿到数千亿），计算成本高昂，阻碍了其在端侧设备和大规模服务中的普及。\n\n2.  **现有范式的局限性**：\n    *   **常规路径**：业界通常采用“监督微调（SFT）+ 强化学习（RL）”的范式来提升模型能力。\n    *   **针对“超小模型”的失效**：作者指出，当将这一范式应用于**超小模型**时，存在致命缺陷：\n        *   **SFT阶段**：小模型容量有限，容易在有限的高质量数据上过拟合，导致其“死记硬背”特定的工具使用模式，而丧失泛化能力。\n        *   **RL阶段**：直接对小模型应用RL以不稳定和低效著称，难以收敛。\n\n3.  **潜在路径与新挑战**：\n    *   **潜在路径**：为了解决SFT的过拟合问题，引入**知识蒸馏（KD）**作为RL前的初始化手段似乎更合理，因为它能提供更稳健、泛化的基础。\n    *   **新挑战（核心痛点）**：然而，KD+RL这一组合在超小模型上引入了新的、独特的困难：\n        *   **KD的不稳定性与探索能力丧失**：为了计算效率，KD常采用top-k截断。这导致长尾分布缺乏监督，引发训练崩溃；同时，这种做法扼杀了模型后续RL阶段必需的探索能力。\n        *   **RL奖励的无效性**：函数调用任务通常存在多种有效解。传统的离散或二元（成功/失败）奖励会过度惩罚那些有效但非标准的替代方案，阻碍有效学习。\n        *   **协同难题**：如何让KD和RL真正协同增效，而非相互干扰，是一个巨大的实践障碍。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑链条，本文试图回答的核心研究问题是：\n\n**“如何构建一个有效的训练框架，在克服知识蒸馏不稳定性与强化学习奖励稀疏性挑战的同时，将大语言模型的函数调用能力成功迁移至超小模型，使其在保持训练稳定性的同时具备强大的泛化与探索能力？”**\n\n---\n\n### 三、 思考过程的逻辑演进推演\n\n以下是从宏观观察到具体方法论的思维演进脉络：\n\n#### 1. 观察与初步假设\n*   **观察**：大模型能做函数调用，但太贵；小模型便宜，但直接训练（SFT+RL）效果差（过拟合、不稳定）。\n*   **初步假设**：既然小模型自己学不好，那就让它“模仿”大模型。用知识蒸馏（KD）代替SFT作为初始化，应该能解决过拟合问题，为后续的RL打好基础。\n\n#### 2. 深入分析假设的缺陷\n*   **反思KD的效率与代价**：\n    *   为了算得快，必须用top-k截断（只看老师概率最高的几个词）。\n    *   **问题发现**：只看top-k，剩下的“长尾”分布就没人管了。这导致两个后果：一是训练不稳定（容易崩），二是模型变得“太自信”，熵降低，失去了探索新答案的能力。**没有探索能力，后面的RL就没法做。**\n*   **反思RL的奖励机制**：\n    *   函数调用不是做数学题，只有一个标准答案。比如调用一个天气API，参数写法可能略有不同但都正确。\n    *   **问题发现**：传统的奖励是“非黑即白”的（对就给1，错就给0）。这对小模型太苛刻了，稍微有点不一样就惩罚，导致模型学不到东西。\n\n#### 3. 针对性解决方案的构思\n*   **解决KD问题（既要稳定，又要探索）**：\n    *   **思路**：不能完全不管长尾，也不能全管（算不过来）。\n    *   **创新点（CKD）**：保留top-k的主损失（保证稳定性），但在长尾部分加一个“约束项”。专门盯着那些“学生觉得很有概率（在学生的top-m里），但老师认为不重要（不在老师的top-k里）”的token进行惩罚。\n    *   **逻辑**：这样既压制了“自信的错误”，又没有把整个长尾分布抹杀掉，保留了模型探索的可能性。\n\n*   **解决RL问题（从二元到连续）**：\n    *   **思路**：奖励应该告诉模型“你离正确答案有多近”，而不是“你是对是错”。\n    *   **创新点**：引入细粒度的相似性奖励。计算生成结果和真实结果的相似度（比如用ROUGE-L或参数匹配度）。\n    *   **逻辑**：即使不完全对，只要相似度高，就给分。这样提供了一个连续、丰富的信号，让小模型能一步步优化。\n\n#### 4. 系统整合与验证\n*   **整合**：将上述两个模块串联起来。先用改进的CKD把大模型的能力“软”迁移给小模型（打好基础，保留探索性），再用改进的Sim-RL对模型进行精调（利用相似信号优化策略）。\n*   **验证**：通过实验证明，这种“CKD初始化 + Sim-RL微调”的流程，能让0.6B的超小模型在函数调用任务上打败大它很多倍的模型，且训练过程稳定。\n\n---\n\n**总结**：作者的思考路径是从**“大模型落地难”**出发，否定了**“直接训练小模型”**的路径，在尝试**“知识蒸馏”**时发现了**“探索性丧失”**和**“奖励稀疏”**两个深层矛盾，最终通过**“约束性蒸馏”**和**“相似性引导强化学习”**这两个互补的创新点，构建了一个完整的闭环解决方案。"
                },
                {
                    "title": "Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents",
                    "arxiv_id": "2602.02995",
                    "authors": "Sizhe Tang, Rongqian Chen, Tian Lan",
                    "summary": "While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\\sim 77\\%$, significantly outperforming trajectory-level baselines under equivalent compute.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中的“规划”子方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **Agent Alpha**，这是一个用于计算机使用智能体的新框架。 *   它不是将现有智能体简单应用到特定领域（如医疗、金融），而是针对智能体本身的**规划**和**探索**能力进行了架构上的改进。 *   论文引入了步级蒙特卡洛树搜索（MCTS）来增强智能体的决策过程，这属于构建和改进 LLM 智能体的方法论。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 明确属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: 论文重点解决了智能体的 `Planning`（规划）能力，通过 `alpha-UCT` 引导搜索实现深思熟虑的规划。同时涉及 `Exploration`（探索）和 `Evaluation`（评估），这些都是智能体在复杂环境中执行任务的关键能力。 3.  **排除标准检查 (第三步)**: *   虽然论文涉及 GUI（图形用户界面）和 Computer-Use，这通常涉及视觉，但论文的核心贡献不在于视觉模型的改进，而在于**如何利用视觉信息作为环境反馈来进行规划和搜索**。根据规则，视觉在这里是智能体感知环境的工具，而非研究核心，因此不排除。 *   论文不涉及安全、对齐或图神经网络等排除领域。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文提出的 MCTS 框架是一种典型的 Agentic 规划方法，旨在解决多步推理和任务执行中的路径优化问题。这符合“保留”关于智能体如何进行规划或在复杂任务中进行多步推理的论文的要求。 综上所述，Agent Alpha 提出了一种新的智能体框架来增强 LLM 在复杂环境中的规划和探索能力，完全符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有计算机使用代理缺乏回归规划能力、难以从早期错误中恢复的问题。针对复杂的图形用户界面（GUI）交互场景，我们提出了一种名为Agent Alpha的统一框架，通过步骤级蒙特卡洛树搜索（MCTS）协同生成、探索与评估。该方法引入了Alpha-UCT边界、基于树信息的反思生成及比较驱动评估等创新设计。我们在OSWorld基准测试上通过成功率等指标验证了其有效性，实现了约77%的最先进性能。",
                    "summary_translation": "虽然通过 trajectory-level sampling (轨迹级采样) 扩展测试时计算显著改善了 Graphical User Interface (GUI) (图形用户界面) 智能体，但缺乏回溯能力阻碍了部分成功的重用和从早期错误中恢复。在本文中，我们介绍了 Agent Alpha，这是一个通过步骤级 Monte Carlo Tree Search (MCTS) (蒙特卡洛树搜索) 协同生成、探索和评估的统一框架。它能够主动建模或利用规划空间的结构。通过将 alpha-UCT 引导搜索集成到交互循环中，Agent Alpha 实现了审慎的规划，促进了次优分支的早期剪枝和前缀的高效重用。我们还采用 comparison-driven evaluation (比较驱动的评估) 来缓解 absolute scoring biases (绝对评分偏差)，并采用 diversity-constrained expansion (多样性约束扩展) 来保持紧凑且信息丰富的搜索空间。本文分析了 alpha-UCT 的 Regret bound (遗憾界)。在 OSWorld (基准) 上，Agent Alpha 实现了约 77% 的 state-of-the-art (最先进) 成功率，在同等计算量下显著优于 trajectory-level (轨迹级) 基线。",
                    "inspiration_trace": "基于对论文《Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents》的深度分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从“现状优势”到“核心缺陷”再到“解决愿景”的叙事闭环：\n\n1.  **现状铺垫：** 指出基于多模态大语言模型（MLLM）的计算机使用代理（CUA）通过测试时计算扩展策略（如CoT, ToT, bBoN）已经取得了显著进展。\n2.  **揭示局限：** 尽管这些方法具备推理能力，但在复杂动态环境中存在一个根本性限制——它们本质上是**单向过程**。\n3.  **深入剖析：** 这种单向性导致代理缺乏对规划空间结构的建模或利用能力。由于缺乏适应动态价值反馈的机制，现有方法无法回溯评估过去行动的有效性。\n4.  **后果阐述：** 这导致了两个致命问题：一是无法在选择了次优分支时纠正航向；二是无法专注于采样有前途的前缀。结果是，**早期的单一错误往往会级联成不可逆转的失败**，阻碍了代理高效探索并收敛到最优解。\n5.  **提出愿景：** 为了解决这一问题，作者提出引入Agent Alpha，通过步骤级的蒙特卡洛树搜索（MCTS），将线性执行转化为回归式规划过程，从而实现错误的自主恢复和前缀的高效复用。\n\n**显式总结的研究问题：**\n> **如何赋予计算机使用代理“回归式规划”的能力，使其能够利用规划空间的结构信息，从早期错误中恢复并复用部分成功的路径，从而突破现有单向测试时扩展方法的局限性？**\n\n---\n\n### 二、 核心方法产出的逻辑演进链\n\n作者的思考过程遵循了“观察现象 -> 定位痛点 -> 引入范式 -> 针对性修正 -> 理论升华”的路径。\n\n#### 1. 宏观观察：测试时扩展的“线性”瓶颈\n*   **思考起点：** 现有的SOTA方法（如BoN, ToT）虽然通过增加采样次数（暴力美学）提升了性能，但在计算机使用（GUI）这种长链条、高动态的任务中，效率极低。\n*   **核心洞察：** 这些方法本质上是“开弓没有回头箭”。一旦某一步走错，整条轨迹往往作废。这种**“不可逆性”**和**“轨迹间信息隔离”**是阻碍性能进一步提升的瓶颈。\n\n#### 2. 范式转移：从“轨迹采样”到“树搜索规划”\n*   **逻辑推演：** 既然单向走不通，就需要一种能够“回头看”且能“全局统筹”的机制。\n*   **方法选择：** 作者自然想到了**蒙特卡洛树搜索（MCTS）**。MCTS的Selection（选择）、Expansion（扩展）、Evaluation（评估）、Back-propagation（回传）机制天然支持回归式规划和前缀复用。\n*   **目标：** 将LLM的生成能力、探索能力和评估能力统一在MCTS框架下，把线性的执行过程变成树状的规划过程。\n\n#### 3. 领域适配：标准MCTS在GUI领域的“水土不服”\n引入MCTS只是第一步，作者意识到直接将标准MCTS套用到GUI任务中会遇到四个具体挑战，因此进行了针对性的组件创新：\n\n*   **挑战一：动作生成的盲目性**\n    *   **思考：** 标准MCTS在扩展节点时通常是随机或基于简单策略采样。但在GUI任务中，LLM如果只看当前状态，容易重复犯错。\n    *   **创新：** **Search-Aware Action Generation（搜索感知的动作生成）**。利用树中其他分支的失败经验（通过Reflection机制）来指导当前节点的动作生成，让LLM“吃一堑长一智”。\n\n*   **挑战二：探索空间的冗余性**\n    *   **思考：** LLM具有模式坍缩特性，倾向于生成高概率的相似动作（例如点击坐标(300,450)和(300,452)在语义上是一样的）。如果树中充满了这种语义重复的分支，搜索效率极低。\n    *   **创新：** **Diversity-Constrained Exploration（多样性约束探索）**。引入语义归一化算子，强制兄弟节点之间保持语义差异，确保搜索预算花在真正不同的决策路径上。\n\n*   **挑战三：评估信号的噪声与偏差**\n    *   **思考：** 让LLM给一个动作打绝对分（如0.8分）非常不稳定，且容易受锚定效应影响。在GUI导航中，细微的动作差异可能导致巨大的结果不同，绝对分数很难区分优劣。\n    *   **创新：** **Comparison-Driven Evaluation（比较驱动评估）**。不进行独立打分，而是将兄弟节点放在一起让LLM进行相对排序。这利用了LLM更擅长比较而非绝对评估的特性。\n\n*   **挑战四：价值回传的稀释效应**\n    *   **思考：** GUI任务通常是稀疏奖励（只有最后一步才有分）。如果用平均值回传，一个致命的错误动作可能会被之前路径的高分掩盖，导致代理无法及时发现死胡同。\n    *   **创新：** **Max-Value Back-propagation（最大值回传）**。采用乐观策略，用路径上的最大值更新节点价值，确保任何致命的负反馈都能迅速暴露，从而及时剪枝。\n\n#### 4. 理论升华：处理“样本依赖”的Alpha-UCT\n*   **深层思考：** 标准MCTS（如UCT算法）假设样本是独立同分布的。但在Agent Alpha中，由于引入了“反思”和“比较评估”，样本之间实际上是相关的（因为上下文和评估基准在不断变化）。\n*   **理论构建：** 作者没有忽略这一点，而是将其建模为**鞅差分序列**。\n*   **最终产出：** 提出了**Alpha-UCT**。通过引入“未确认知识”框架，利用预测残差方差代替原始方差，证明了在样本相关的情况下，Alpha-UCT能获得更紧的置信区间，从而在理论上证明了其比标准MCTS更高的搜索效率。\n\n---\n\n### 总结\n作者的思考过程并非直接堆砌技术，而是从**“单向扩展的不可逆性”**这一根本痛点出发，确立了**MCTS回归式规划**的宏观架构，随后针对GUI任务中**LLM的生成冗余、评估不准、样本相关**等具体特性，逐一设计了**树感知反思、多样性约束、比较评估、最大值回传**等微观组件，并最终通过**Alpha-UCT**将这一整套逻辑进行了理论上的闭环与升华。"
                },
                {
                    "title": "DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution",
                    "arxiv_id": "2602.02919",
                    "authors": "Jiachen Jiang, Tianyu Ding, Zhihui Zhu",
                    "summary": "LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”方向的核心论文。 1.  **核心贡献符合“自我演化”与“LLM智能体”定义**： 论文的核心贡献是提出了 **DeltaEvolve**，这是一个“动量驱动的演化框架”。论文明确将“演化智能体”形式化为一个通用的期望最大化（EM）框架，旨在解决现有LLM驱动演化系统（如AlphaEvolve）在上下文效率和演化指导方面的不足。这直接对应您研究目标中的“自我演化”和“构建、改进LLM智能体”。 2.  **涉及智能体的关键机制**： 论文详细探讨了智能体如何通过“评估反馈”来更新控制上下文（M-step），以及如何利用“结构化语义增量”来指导智能体的迭代和改进。这属于智能体的自我完善和迭代机制，符合筛选标准中的正面指标（如 `Self-Improvement`, `Iterative Improvement`）。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的评估任务涉及“科学发现”，这看起来像是一个特定领域的应用，但根据筛选标准第四步第2条，只要论文的核心是提出一种新的“自我演化”机制（即DeltaEvolve框架及其语义增量演化策略），即使它被应用在科学领域，也应该保留。论文的重点在于改进智能体“如何演化”的方法论，而不仅仅是展示其在科学任务上的应用结果。 综上所述，该论文不仅属于Agentic AI范畴，而且深入探讨了智能体的自我演化机制，是您课题下的高质量相关论文。",
                    "summary2": "本文旨在解决现有LLM进化系统上下文效率低且进化指导弱的问题。针对科学发现中的代码搜索任务，我们提出了一种基于动量驱动的DeltaEvolve框架，利用结构化语义增量替代完整代码历史，并结合多级数据库和渐进式披露机制。在五个科学领域的任务上，通过最佳分数和Token消耗验证了其有效性，实现了更优的解质量和更低的计算成本。",
                    "summary_translation": "LLM-driven evolutionary systems (大语言模型驱动的进化系统) 在 automated science discovery (自动化科学发现) 方面显示出潜力，然而现有的方法（如 AlphaEvolve）依赖于 full-code histories (完整代码历史)，这在 context-inefficient (上下文低效) 的，并且可能提供较弱的 evolutionary guidance (进化指导)。在这项工作中，我们首先将 evolutionary agents (进化智能体) 形式化为一个通用的 Expectation-Maximization (EM) framework (期望最大化框架)，其中语言模型采样 candidate programs (候选程序) (E-step，期望步)，系统根据 evaluation feedback (评估反馈) 更新控制上下文 (M-step，最大化步)。在此视角下，通过 full-code snapshots (完整代码快照) 构建上下文构成了一个 suboptimal M-step (次优最大化步)，因为 redundant implement details (冗余实现细节) 稀释了 core algorithmic ideas (核心算法思想)，使得难以提供清晰的进化启发。为解决这一问题，我们提出了 DeltaEvolve，这是一个 momentum-driven (动量驱动) 的 evolutionary framework (进化框架)，它用 structured semantic delta (结构化语义增量) 取代了完整代码历史，该增量捕捉了 successive nodes (连续节点) 之间的修改如何以及为何影响性能。由于程序通常是 decomposable (可分解) 的，semantic delta (语义增量) 通常包含许多 effective components (有效组件)，这些组件是 transferable (可迁移) 的，并且包含更多信息以驱动改进。通过 multi-level database (多级数据库) 和 progressive disclosure mechanism (渐进式披露机制) 组织 semantic delta (语义增量)，input tokens (输入 Token) 进一步减少。在跨越多样化科学领域的任务上的 Empirical evaluations (实证评估) 表明，与基于完整代码的 evolutionary agents (进化智能体) 相比，我们的框架可以用更少的 token consumption (Token 消耗) 发现更好的解决方案。",
                    "inspiration_trace": "基于对论文《DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 一、 宏观背景与问题引入（Introduction 中的“讲故事”逻辑）\n\n作者首先构建了一个从通用能力到具体瓶颈的叙事逻辑：\n\n1.  **宏观趋势**：LLM 已被证明在数学、物理、分子发现等科学发现任务中具有强大潜力。这些任务的核心本质是**搜索问题**——即寻找满足特定量化属性的高性能对象。\n2.  **现有范式**：由于代码具有表达力强、可执行、符合 LLM 预训练分布等特点，当前主流方法（如 AlphaEvolve）选择在“代码空间”中进行搜索。对于复杂任务，单次生成不足，需要基于反馈的**迭代进化**。\n3.  **核心冲突（瓶颈）**：尽管现有的自进化代理（如 AlphaEvolve）有效，但它们面临两个根本性限制：\n    *   **上下文窗口限制**：存储完整的代码历史极其消耗 Token。受限于上下文长度，系统只能保留极少数的高质量或多样化程序，导致无法有效利用长尾历史信息。\n    *   **进化指导不足**：完整的程序代码往往将“核心算法思想”与大量的“实现细节”混杂在一起。这种噪声使得 LLM 难以从历史代码中分离出真正有用的组件，导致无法显式捕获可迁移的成功或失败模式，从而错失成功机会或重复失败。\n\n### 二、 研究问题\n\n基于上述冲突，作者提出了明确的研究问题：\n\n**“如何在有限的上下文预算内，提供更强的进化指导？”**\n\n---\n\n### 三、 思想演进与逻辑推演（从观察到方法论）\n\n为了回答上述问题，作者的思考过程经历了以下四个关键阶段：\n\n#### 1. 理论抽象：从“代理交互”到“EM 框架”\n*   **观察**：现有的进化系统通常被视为一种基于反馈的智能体交互（类似强化学习），但在 LLM 参数固定的情况下，其收敛机制在理论上较为模糊。\n*   **抽象**：作者将进化过程重新形式化为一个**期望最大化**框架。\n    *   **E-step**：LLM 基于当前上下文采样候选程序。\n    *   **M-step**：系统基于评估反馈更新上下文，以最大化目标函数。\n*   **洞察**：在这个框架下，**上下文 $C$ 是唯一的优化变量**。因此，系统的性能瓶颈不在于 LLM 本身，而在于 **M-step 中如何构建上下文**。\n\n#### 2. 现状批判：全代码快照是“次优的 M-step”\n*   **分析**：现有方法（如 AlphaEvolve）在 M-step 中使用“静态的全代码快照”来构建上下文。\n*   **批判**：这就像在优化算法中只记录了“当前所在的位置”，却丢失了“是如何到达这里的”以及“移动的方向”。\n*   **实验验证**：通过消融实验，作者发现相比于具体的数值反馈，**上下文的选择策略**（即放入哪些代码）对性能起决定性作用。这证实了系统主要依赖“上下文学习”而非“隐式回归”，因此上下文的质量和信号清晰度至关重要。\n\n#### 3. 核心假设：语义增量即“动量”\n*   **假设提出**：在算法发现领域，程序是**组合性**的。驱动进化的不是整个解决方案，而是其中有效的**组件**。\n*   **逻辑推演**：如果程序是组合的，那么**修改**比**状态**更重要。早期迭代中影响性能的修改往往捕捉了任务层面的结构，这些修改对后续迭代具有指导意义。\n*   **类比**：在梯度下降中，我们利用“动量”来积累历史梯度方向以加速收敛。同理，在代码进化中，应该积累**语义增量**——即“改变了什么”以及“为什么改变”，而不是积累静态代码。\n\n#### 4. 方法构建：DeltaEvolve 的诞生\n*   **核心思想**：用“语义增量”替代“全代码快照”。\n*   **定义**：语义增量 $\\delta$ 记录了父节点到子节点的逻辑变化及其对性能的影响（例如：“将初始化策略从随机改为拉丁超立方采样，假设这能提高覆盖率”）。\n*   **工程实现（解决效率问题）**：\n    *   为了进一步压缩 Token 并提高信息密度，作者设计了**多级数据库**：\n        *   Level 1 (Delta Summary)：高层策略变化（极简）。\n        *   Level 2 (Delta Plan)：具体的逻辑变更和假设（中等）。\n        *   Level 3 (Full Code)：完整代码（仅用于当前父节点）。\n    *   引入**渐进式披露机制**：根据历史节点的相关性和新旧程度，动态决定展示哪一层级的信息（旧历史只给摘要，近期灵感给详细计划，当前编辑对象给全代码）。\n\n### 四、 总结\n\n作者的思考路径是从**现象**（全代码进化既贵又笨）出发，通过**理论建模**（EM 框架）定位瓶颈（M-step 的上下文构建），利用**类比思维**（将代码进化比作梯度优化，引入“动量”概念），最终提出**语义增量**这一核心创新，并通过**分层结构**解决了工程上的效率问题。"
                },
                {
                    "title": "AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents",
                    "arxiv_id": "2602.02849",
                    "authors": "Xi Yu, Dmitrii Torbunov, Soumyajit Mandal, Yihui Ren",
                    "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴 (第一步 & 第二步)**： 论文的核心贡献是提出了 **AutoSizer**，这是一个“反思性LLM驱动的元优化框架”。这不仅仅是将现有的LLM作为工具简单应用于电路设计，而是构建了一个新的 **LLM智能体框架**。该框架采用了双循环优化结构，包含内循环的电路尺寸调整和外循环的动态分析与搜索空间迭代优化，这体现了智能体的 **规划** 和 **工具使用** 能力。 2.  **包含自我演化与反思机制 (第二步 & 第四步)**： 论文中明确提到了“反思性”和“迭代优化搜索空间”。智能体通过分析仿真反馈来调整优化策略，这属于 **自我反思** 和 **自我修正** 的范畴。虽然论文的应用场景是特定的模拟和混合信号（AMS）电路设计，但根据第四步的“特殊和模糊情况”处理规则，只要论文的核心是提出一种新的“自我演化”或“反思”机制（即双循环优化框架），即使应用在特定领域，也应当保留。 3.  **排除标准检查 (第三步)**： 论文不涉及安全与对齐、多模态视觉核心研究或图神经网络等排除项。 综上所述，该论文在构建具有反思和迭代优化能力的LLM智能体方面具有明确的方法论贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决模拟和混合信号（AMS）电路尺寸优化的瓶颈问题。针对高维设计空间和严格性能约束，我们提出了一种名为AutoSizer的基于LLM代理的反思性元优化框架，采用双循环结构整合电路理解、自适应搜索空间构建与优化编排。我们在开源的AMS-SIZING BENCH基准测试集上，通过Figure of Merit (FoM)、收敛速度和成功率验证了其有效性，实验表明其优于传统方法和现有LLM代理。",
                    "summary_translation": "模拟与混合信号集成电路的设计仍然严重依赖专家知识，其中晶体管尺寸调整是主要瓶颈，这归因于非线性特性、高维设计空间以及严格的性能约束。现有的电子设计自动化方法通常将尺寸调整视为静态黑盒优化，导致生成的解决方案效率低下且鲁棒性不足。尽管大语言模型展现出强大的推理能力，但它们并不适用于AMS尺寸调整中的精确数值优化。为解决这一问题，我们提出了AutoSizer，这是一个反思式的大语言模型驱动元优化框架，在闭环中统一了电路理解、自适应搜索空间构建和优化编排。该框架采用双层优化结构，内层循环负责电路尺寸调整，外层循环则分析优化动态和约束，利用仿真反馈迭代细化搜索空间。我们进一步推出了AMS-SizingBench，这是一个开源基准，包含24种基于SKY130 CMOS工艺的多样化AMS电路，旨在评估基于真实仿真器约束的自适应优化策略。实验结果表明，在不同电路难度下，AutoSizer在解的质量、收敛速度和成功率方面均表现更佳，优于传统优化方法及现有的基于大语言模型的智能体。",
                    "inspiration_trace": "基于对论文《AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents》的深度分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观背景与“讲故事”的逻辑链\n\n作者在Introduction中通过层层递进的对比，构建了从行业痛点到现有方案局限性的完整叙事逻辑：\n\n1.  **宏观背景与核心瓶颈**：\n    *   **观察**：模拟和混合信号（AMS）集成电路设计严重依赖专家经验，其中“晶体管尺寸确定”是主要瓶颈。\n    *   **原因**：设计空间具有高维、非线性特性，且性能约束严格，微小的参数变化可能导致性能剧烈波动。\n\n2.  **传统方案的局限性（静态黑盒）**：\n    *   **现状**：现有的电子设计自动化（EDA）方法（如贝叶斯优化、强化学习、遗传算法）将尺寸确定视为一个“静态黑盒优化”问题。\n    *   **批判**：这些方法忽略了特定领域的模拟设计知识，且无法根据中间结果调整策略，导致效率低下、鲁棒性差，且容易陷入局部最优。\n\n3.  **新兴方案的潜力与缺陷（LLM的困境）**：\n    *   **潜力**：大语言模型（LLM）展现出强大的符号推理能力，近期被尝试用于电路设计。\n    *   **缺陷**：现有的LLM智能体方法存在两个核心问题：\n        *   **数值能力弱**：LLM本身不擅长精确的数值优化。\n        *   **流程僵化**：现有工作（如EEsizer, LEDRO等）通常采用“单阶段”工作流，即LLM理解电路后，应用一个**预设的、固定的**优化策略，缺乏基于仿真反馈的“自我反思”和“策略迭代”能力。一旦初始搜索空间设定有误，系统无法自我修正。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑链，作者试图解决的核心问题可归纳为：\n\n**如何构建一个基于LLM的反思性元优化框架，使其能够像人类专家一样，通过分析仿真反馈动态调整搜索空间和优化策略，从而克服传统静态黑盒优化和现有僵化LLM智能体在AMS电路尺寸确定中的局限性？**\n\n---\n\n### 三、 思想演进与方法论形成\n\n为了回答上述问题，作者的思考过程经历了从现象观察到架构重构的演进：\n\n#### 1. 观察与假设：从“优化参数”到“优化优化过程”\n*   **观察**：人类专家在设计电路时，并不是一次性设定好所有参数范围然后跑仿真。而是先尝试，看结果，如果发现增益不够，会针对性地调整输入对管的尺寸范围，或者调整偏置电流。\n*   **假设**：现有的失败不是因为优化算法（如BO或GA）不够好，而是因为**搜索空间的定义是静态且错误的**。如果能让LLM充当“总设计师”，负责动态定义“去哪里搜”，而让传统算法负责“怎么搜”，就能结合两者的优势。\n*   **核心概念提出**：**元优化**。即优化的目标不再是直接的电路参数，而是“优化策略本身”（包括变量优先级、参数范围、算法选择）。\n\n#### 2. 架构构思：双环闭环架构\n为了实现上述假设，作者构思了一个分层架构：\n*   **内环（执行层）**：负责具体的数值计算。既然LLM不擅长算数，那就调用工具池（贝叶斯优化、遗传算法等）来在当前范围内寻找最优解。\n*   **外环（决策层）**：负责策略制定。LLM分析内环的历史数据（收敛情况、边界聚集情况），判断当前搜索空间是否合理。如果内环陷入停滞，外环负责修改搜索空间（扩大范围、激活新变量）。\n\n#### 3. 关键模块设计：赋予LLM“电路直觉”\n为了让外环的决策有效，LLM必须“懂”电路，而不仅仅是看数据。\n*   **思考**：如果LLM不知道哪个晶体管控制增益，它就无法正确调整搜索空间。\n*   **方案**：引入**电路理解模块**。在优化开始前，让LLM解析网表，识别拓扑结构，推断变量对性能的敏感度。这为后续的搜索空间构建提供了先验知识。\n\n#### 4. 机制完善：自适应搜索空间构建\n*   **思考**：高维空间搜索效率极低。人类设计通常是“抓大放小”。\n*   **方案**：设计**渐进式策略**。初始阶段，利用电路理解模块锁定少数关键变量，固定次要变量，大幅压缩搜索空间。随着优化进行，如果遇到瓶颈，再通过“反思”机制释放被固定的变量或扩展范围。这模拟了人类从粗调到精调的过程。\n\n#### 5. 验证标准：构建真实基准\n*   **思考**：现有研究多在简单运放上测试，无法证明泛化能力。\n*   **方案**：为了验证该“反思性”框架在复杂场景下的有效性，必须建立一个包含多种电路类型（运放、振荡器、电源管理等）和难度梯度的标准化基准（AMS-SIZING BENCH），以确保对比的公平性和方法的鲁棒性。\n\n---\n\n### 四、 总结：逻辑链条全景\n\n1.  **痛点**：AMS电路设计难，传统优化是“死板”的黑盒，现有LLM方法是“一次性”的僵化流程。\n2.  **洞察**：真正的瓶颈在于“搜索空间”的静态性，而非优化算法本身。需要引入“动态调整”机制。\n3.  **核心思想**：将LLM从“计算者”转变为“策略管理者”，构建**内环（数值优化）+ 外环（策略反思）**的双环架构。\n4.  **方法论**：\n    *   利用LLM的**电路理解能力**初始化搜索空间。\n    *   利用传统算法作为**工具**进行精确搜索。\n    *   利用LLM的**反思能力**根据反馈动态修正搜索空间。\n5.  **验证**：通过构建大规模、多难度的开源基准，证明该方法在复杂约束下的优越性。"
                },
                {
                    "title": "ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters",
                    "arxiv_id": "2602.02709",
                    "authors": "Ujin Jeon, Jiyong Kwon, Madison Ann Sullivan, Caleb Eunho Lee, Guang Lin",
                    "summary": "Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献精准匹配**：论文标题和摘要明确提出了 \"ATLAS\"（Adaptive Task-distributed Learning for Agentic Self-evolution），这是一个专门用于 **LLM智能体自我演化** 的框架。其核心算法 \"Evolving Direct Preference Optimization (EvoDPO)\" 旨在解决智能体在长视距任务中如何自适应更新策略的问题。这直接对应您研究焦点中的 **\"自我演化\"** 方向。 2.  **符合多智能体与单智能体特征**：论文描述了一个包含 \"Research Agent\"（研究智能体）和多个 \"Supporter Agents\"（支持者智能体，负责探索、调参等）的系统。这体现了 **\"多智能体\"** 间的协作与任务分发，同时也涉及单智能体的规划与自我完善能力。 3.  **符合特殊情况的保留规则**：虽然论文在实验部分使用了科学机器学习（SciML）领域的 \"1D Burgers' equation\" 作为测试环境，但根据筛选标准第四步（自我演化的应用），只要论文的核心是提出一种新的“自我演化”机制（即EvoDPO和ATLAS框架），即使应用在特定领域，也应该保留。本文的重点在于智能体如何演化，而非物理方程本身的解法。 4.  **无排除项**：论文不涉及安全对齐、多模态视觉核心研究或图神经网络，且不是单纯的基础设施优化或非Agentic的基础推理提升。 综上所述，该论文致力于构建和改进LLM智能体的自我演化机制，属于您课题的前沿研究。",
                    "summary2": "本文旨在解决现有LLM智能体在非平稳长期训练中缺乏鲁棒自我进化能力的问题。针对复杂决策和科学计算场景，我们提出了ATLAS框架，利用任务分布的多LLM支持者协同训练研究智能体，并引入Evolving DPO (EvoDPO) 算法实现自适应参考策略管理。在非平稳上下文老虎机和1D Burgers方程实验中，通过Negative Mean Regret和Validation Loss验证了其显著优于基线方法的有效性。",
                    "summary_translation": "近期的 multi-LLM agent systems (多 LLM 智能体系统) 在 prompt optimization (提示词优化) 和 automated problem-solving (自动化问题求解) 方面表现优异，但许多系统要么在 fine-tuning (微调) 后将 solver (求解器) 冻结，要么依赖静态的 preference-optimization loop (偏好优化循环)，这对于 long-horizon tasks (长视界任务) 而言是难以处理的。我们提出了 ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution，智能体自进化的自适应任务分布式学习)，这是一个 task-distributed framework (任务分布式框架)，它迭代式地开发一个 lightweight research agent (轻量级研究智能体)，同时将互补角色分配给专门的 supporter agents (支持智能体)，用于 exploration (探索)、hyperparameter tuning (超参数调优) 和 reference policy management (参考策略管理)。我们的核心算法 Evolving Direct Preference Optimization (EvoDPO，进化直接偏好优化) 能够自适应地更新 phase-indexed reference policy (阶段索引参考策略)。我们针对 concept drift (概念漂移) 下的 preference-based contextual bandit (基于偏好的上下文老虎机) 提供了 theoretical regret analysis (理论遗憾分析)。此外，我们在 non-stationary linear contextual bandits (非平稳线性上下文老虎机) 以及针对 1D Burgers' equation (一维 Burgers 方程) 的 scientific machine learning (SciML，科学机器学习) loss reweighting (损失重加权) 任务上进行了实验。两项结果均表明，ATLAS 相比 static single-agent baseline (静态单智能体基线) 提高了稳定性和性能。",
                    "inspiration_trace": "基于对论文《ATLAS: Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters》的深度分析，以下是作者产出该文章的系统性思考过程推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从现状观察到痛点揭示的逻辑链条，具体如下：\n\n1.  **现状观察**：\n    当前，微调后的LLM智能体已被广泛用于复杂问题解决（如科学计算、代码生成）。最新的研究趋势是构建“多智能体系统”，通过集合多个专门的智能体、经验库或路由机制来提升任务性能和可靠性。\n\n2.  **揭示局限**：\n    尽管多智能体架构在优化搜索、规划和决策上表现出色，但大多数现有方法存在一个根本性缺陷：**它们将LLM智能体视为“冻结的优化器”**。研究重心主要放在如何提高优化效率（如更好的调度），而忽略了在现实的非平稳和长周期环境下，**如何通过迭代模型更新来“发展”一个智能体本身**。\n\n3.  **深入痛点**：\n    在基于偏好的迭代学习（如DPO）中，现有的长周期训练流程通常依赖**固定的参考策略**。然而，随着智能体能力的提升，固定的参考策略会逐渐过时，导致参考数据不匹配，进而引发更新停滞或对齐偏差。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个能够适应非平稳环境的自进化智能体框架，使其在长周期的迭代训练中，既能通过动态更新参考策略来避免停滞，又能通过多智能体协作机制保证进化的稳定性？”**\n\n---\n\n### 三、 思想演进与逻辑链推演\n\n以下是从宏观问题到具体方法论的思维演进过程：\n\n#### 1. 宏观视角的反思：从“使用工具”到“培养工具”\n*   **思考起点**：现有的多智能体系统虽然热闹，但本质上是在用“静态”的模型组合去解决“动态”的问题。这就像是用一群只会死记硬背的学生去解决从未见过的考题。\n*   **直觉假设**：真正的智能应该具备“自我进化”的能力。我们需要的不只是一个执行任务的Agent，而是一个能够随着时间推移、环境变化而不断自我学习和成长的Research Agent。\n\n#### 2. 算法层面的突破：解决“参考策略”的悖论\n*   **技术瓶颈**：要让Agent进化，通常使用DPO（直接偏好优化）。但DPO需要一个参考策略作为锚点。\n    *   如果参考策略**不变**：Agent进化到一定程度后，参考策略就成了“拖后腿”的旧标准，导致Agent无法继续突破（停滞）。\n    *   如果参考策略**乱变**：Agent可能会因为更新过激而崩溃，学坏或遗忘之前的技能（不稳定）。\n*   **核心洞察**：我们需要一个**“进化的DPO”**。参考策略必须随着Agent一起进化，但这种进化必须是“保守”且“受控”的。\n*   **方案雏形**：引入一个“门控机制”。只有当新提出的策略确实比旧的好，且没有偏离太远（通过KL散度约束）时，才允许更新参考策略。\n\n#### 3. 架构层面的重构：从“单打独斗”到“多级辅导”\n*   **系统复杂性**：仅仅解决算法上的参考策略更新还不够。在长周期的进化中，Agent可能会陷入局部最优（只探索一种策略），或者超参数难以调整。\n*   **社会分工隐喻**：就像培养一个顶尖科学家，不能只靠他自己瞎琢磨，需要一个导师团队。\n*   **角色分配**：\n    *   **探索支持者**：负责告诉Agent“去哪里看看”，防止思维固化，提供多样化的探索策略。\n    *   **微调策略家**：负责“教学节奏”，根据训练状态动态调整学习率、温度等超参数。\n    *   **策略检查员**：负责“安全把关”，这就是上述EvoDPO中门控机制的具体执行者，决定是否采纳新的参考策略。\n\n#### 4. 方法论整合：ATLAS框架的诞生\n*   **逻辑闭环**：\n    *   **目标**：构建一个自进化的研究Agent。\n    *   **手段**：利用一组“冻结”但专业的Supporter Agents来指导一个“发展中”的Research Agent。\n    *   **核心引擎**：EvoDPO算法，它允许参考策略在受控的信任区域内逐步升级。\n*   **验证场景选择**：为了证明这种“进化”能力，作者选择了两个典型的**非平稳**场景——参数漂移的上下文老虎机（决策问题）和参数随时间变化的Burgers方程（科学计算问题）。这两个场景都要求Agent必须适应环境的变化，而非死记硬背。\n\n#### 5. 最终产出\n*   **ATLAS**：一个任务分布式的多LLM支持者系统。\n*   **EvoDPO**：带有自适应参考管理机制的核心算法。\n*   **结论**：通过“分工明确的导师团队”加上“保守进化的学习算法”，实现了Agent在长周期、非平稳环境下的稳健自我提升。"
                },
                {
                    "title": "MARS: Modular Agent with Reflective Search for Automated AI Research",
                    "arxiv_id": "2602.02660",
                    "authors": "Jiefeng Chen, Bhavana Dalvi Mishra, Jaehyun Nam, Rui Meng, Tomas Pfister, Jinsung Yoon",
                    "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **MARS (Modular Agent with Reflective Search)**，这是一个专门为自主AI研究优化的新框架。这不仅仅是将现有LLM应用于特定领域，而是构建了一个包含“预算感知规划”、“模块化构建”和“比较反思记忆”的新颖智能体架构。这直接符合“构建、改进LLM智能体”的核心目标。 2.  **正面指标（高度匹配）**： *   **单智能体**：论文详细阐述了智能体的规划能力，特别是引入了基于成本约束的蒙特卡洛树搜索（MCTS）来平衡性能与开销，以及“Design-Decompose-Implement”的模块化构建流程。 *   **自我演化**：论文的核心亮点之一是 **“Comparative Reflective Memory”**。这是一种自我反思机制，智能体通过分析解决方案的差异来提炼高价值洞察，并展示了跨分支的泛化能力（即文中提到的“Aha!”时刻）。这完全符合“自我反思”、“自我完善”和“迭代改进”的演化机制。 3.  **排除标准（无冲突）**： 论文不涉及安全对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊与模糊情况处理**： 虽然论文的应用场景是“自动化AI研究”（特定领域），但根据第四步的规则，只要论文的核心是提出一种新的“自我演化”机制或Agentic框架，即使应用在特定领域也应保留。MARS提出的反思搜索和预算感知规划是对智能体基础能力的增强，而非单纯的应用部署。 综上所述，MARS论文在单智能体的规划、记忆以及自我演化机制上做出了显著的方法论贡献，是Agentic AI领域的高质量前沿研究。",
                    "summary2": "本文旨在解决自动化AI研究中评估成本高昂、性能归因模糊及代码库管理复杂的问题。针对机器学习工程（MLE）任务，我们提出了一种名为MARS的框架，该框架集成了预算感知的MCTS规划、模块化“设计-分解-实现”流程以及比较式反思记忆机制。并在MLE-Bench基准上通过金牌率、奖牌率等指标验证了其有效性，实现了开源框架中的SOTA性能。",
                    "summary_translation": "自动化 AI 研究与 general software engineering (通用软件工程) 存在差异，主要归因于 computationally expensive evaluation (计算成本高昂的评估) (例如 model training (模型训练)) 以及 opaque performance attribution (不透明的性能归因)。现有的 LLM-based agents (基于 LLM 的智能体) 在此方面往往力不从心，它们通常生成 monolithic scripts (单体脚本)，而忽略了 execution costs (执行成本) 和 causal factors (因果因素)。我们提出了 MARS (Modular Agent with Reflective Search，具有反思性搜索的模块化智能体)，这是一个针对 autonomous AI research (自主 AI 研究) 进行优化的框架。MARS 依托于三大支柱：(1) Budget-Aware Planning (预算感知规划)，通过 cost-constrained Monte Carlo Tree Search (MCTS，成本约束蒙特卡洛树搜索) 来明确平衡性能与执行开销；(2) Modular Construction (模块化构建)，采用 \"Design-Decompose-Implement\" (设计-分解-实现) 流水线来管理复杂的研究代码库；(3) Comparative Reflective Memory (比较反思记忆)，通过分析解决方案的差异来解决 credit assignment (信用分配) 问题，从而提炼出 high-signal insights (高信号洞察)。在可比设置下，MARS 在 MLE-Bench 上实现了开源框架中的 state-of-the-art performance (最先进性能)，并保持了与 global leaderboard (全球排行榜) 顶级方法的竞争力。此外，该系统展现出了定性的 \"Aha!\" moments (顿悟时刻)，其中 63% 的已应用经验源自 cross-branch transfer (跨分支迁移)，这证明了该智能体能够有效地在不同的 search paths (搜索路径) 之间泛化洞察。",
                    "inspiration_trace": "基于对论文《MARS: Modular Agent with Reflective Search for Automated AI Research》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与背景切入\n**起点：** LLM在代码生成领域已取得巨大成功，从简单的自动补全进化为能够解决GitHub Issue、生成功能脚本的自主智能体。\n**观察：** 尽管这些智能体在通用软件工程（如修Bug、写单元测试）上表现出色，但在**自动化AI研究**这一特定领域却面临显著阻碍。\n\n---\n\n### 2. “讲故事”的逻辑：问题引入\n作者通过对比“通用软件工程”与“AI研究”的本质差异，构建了问题的紧迫性：\n\n1.  **任务本质的错位：**\n    *   **通用软件：** 正确性通常是二元的（对/错），验证成本低（编译即知），且逻辑相对简单。\n    *   **AI研究：** 这是一个概率性、资源密集型的过程。验证成本极高（需要长时间训练模型），且结果往往是不确定的。\n\n2.  **现有工具的局限性：**\n    *   现有的智能体框架大多是为生成“单体脚本”设计的，它们将问题解决视为纯粹的代码挑战。\n    *   **忽视经济现实：** 它们为了追求0.1%的精度提升，可能会将训练时间从1小时增加到10小时，这在实际研究中是不可接受的。\n    *   **架构脆弱性：** 生成的单体脚本难以应对AI研究仓库中数据加载、模型架构、训练循环等模块间复杂的交互需求。\n    *   **归因模糊：** 当一个实验结果变好时，很难确定是哪个因素（数据增强？学习率？）起了作用。现有的记忆机制缺乏解决这种“信用分配”问题的能力。\n\n**总结：** 现有的智能体范式（单体、成本盲视、缺乏因果反思）与AI研究的实际需求（高成本、高复杂度、需要因果洞察）之间存在根本性的不匹配。\n\n---\n\n### 3. 核心研究问题\n基于上述观察，作者试图回答：\n\n**“如何构建一个自主智能体框架，使其能够有效应对AI研究中独特的计算成本约束、复杂的架构需求以及不透明的性能归因问题，从而在有限的预算内实现高效的科学发现？”**\n\n---\n\n### 4. 思想演进与逻辑推演\n\n#### 第一阶段：痛点抽象与假设提出\n*   **观察：** AI研究不仅仅是写代码，更像是在巨大的搜索空间中寻找最优解，且每一步搜索（实验）都极其昂贵。\n*   **假设：** 如果我们能让智能体像人类资深研究员一样思考——即在行动前考虑预算，像工程化一样构建代码，并像科学家一样分析实验差异——那么它就能克服现有缺陷。\n\n#### 第二阶段：针对痛点的模块化解法\n作者将核心问题拆解为三个维度，并分别提出解决思路：\n\n*   **维度一：计算成本与搜索效率**\n    *   *思考：* 传统的搜索算法（如贪婪搜索）只看性能，不看时间。我们需要一种算法，在性能相近时，优先选择“便宜”的方案。\n    *   *方法论演进：* 引入 **Budget-Aware Planning（预算感知规划）**。利用蒙特卡洛树搜索（MCTS），但在奖励函数中加入对执行时间的惩罚，迫使智能体在“高性能”和“高效率”之间寻找平衡。\n\n*   **维度二：代码复杂度与可维护性**\n    *   *思考：* 单体脚本不仅难以调试，还无法复用。人类工程师不会把所有代码写在一个文件里。\n    *   *方法论演进：* 引入 **Modular Construction（模块化构建）**。设计一个“设计-分解-实现”的流水线，强制智能体将解决方案拆分为独立、可测试的模块（如数据处理、模型定义、训练引擎），并支持基于Diff的局部修改，避免重写整个代码库。\n\n*   **维度三：经验积累与信用分配**\n    *   *思考：* 简单的记录“成功/失败”是不够的。智能体需要知道“为什么”成功，特别是当两个方案只有细微差别时。\n    *   *方法论演进：* 引入 **Comparative Reflective Memory（比较式反思记忆）**。不仅仅是总结日志，而是显式地对比“当前方案”与“历史最佳方案”的差异，提取出高信噪比的因果洞察，形成可复用的“经验教训”。\n\n#### 第三阶段：系统整合\n*   **最终形态：** 将上述三个模块整合进一个统一的框架——**MARS**。\n*   **闭环逻辑：** MCTS负责宏观的路径规划（去哪试），模块化构建负责微观的代码实现（怎么试），反思记忆负责知识的沉淀与迭代（学到了什么），三者协同工作，模拟了人类研究员的完整工作流。\n\n---\n\n### 5. 总结\n作者的思考路径是从**“通用代码生成的成功”**出发，敏锐地捕捉到其在**“AI研究场景”中的失效**，通过深入分析**成本、结构、归因**三大核心矛盾，最终提出了一套融合**经济学思维（预算MCTS）、软件工程思维（模块化）和科学思维（比较反思）**的综合性解决方案。"
                },
                {
                    "title": "PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review",
                    "arxiv_id": "2602.02589",
                    "authors": "Yanki Margalit, Erni Avram, Ran Taig, Oded Margalit, Nurit Cohen-Inger",
                    "summary": "Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合多智能体方向**： 论文提出了 PeerRank，这是一个完全自主的评估框架。其核心创新点在于将评估过程构建为一个**多智能体系统**。摘要中明确指出：\"PeerRank treats evaluation as a **multi-agent process** where each model participates symmetrically as task designer, respondent, and evaluator...\"（PeerRank 将评估视为一个多智能体过程，其中每个模型对称地作为任务设计者、回答者和评估者参与）。这完全符合研究焦点中的“多智能体”方向，涉及智能体间的角色分配、交互和协作。 2.  **具备 Agentic 特征**： 该框架中的智能体具备自主性，能够执行生成任务、使用工具（摘要中提到的 \"web-grounded\", \"web retrieval\" 即工具使用能力）以及进行评判。这符合筛选标准中的正面指标，如 `Multi-Agent Systems` 和 `Tool Use`。 3.  **非排除项**： *   虽然论文涉及“评估”，但其核心贡献不是单纯的应用（如将LLM用于医疗或法律），而是提出了一种新的**多智能体协作框架**来解决评估问题。 *   虽然论文提到了 \"bias-controlled\"（偏差控制），但这属于评估框架为了确保排名准确性的机制，而非主要贡献在于安全对齐或防御性研究。 *   论文不属于基础设施、非Agentic的基础推理改进或多模态视觉研究。 综上所述，该论文构建了一个新颖的多智能体协作框架来执行复杂的评估任务，属于构建和改进 LLM 智能体（特别是多智能体系统）的研究范畴。",
                    "summary2": "本文旨在解决传统LLM评估依赖静态基准、难以适应开放世界部署且扩展性差的问题。针对无人工监督的评估场景，我们提出了一种名为PeerRank的完全自主多智能体框架，该框架让模型对称地参与任务生成、基于实时网络的回答及偏差控制的互评。我们在12个商业模型及420个自主生成的问题上，通过与Elo评分及TruthfulQA、GSM8K基准上的客观准确率进行相关性验证，证实了该方法能产生稳定、具有区分度的排名。",
                    "summary_translation": "评估 large language models (大型语言模型) 通常依赖于人工编写的 benchmarks (基准)、reference answers (参考答案) 以及人工或单一模型的 judgments (评判)。这些方法扩展性较差，容易过时，且与依赖 web retrieval (网络检索) 和 synthesis (合成) 的 open-world deployments (开放世界部署) 不匹配。我们提出了 PeerRank，这是一个完全自主的 end-to-end evaluation framework (端到端评估框架)。在该框架中，模型生成 evaluation tasks (评估任务)，利用 category-scoped live web grounding (限定类别的实时网络检索) 进行回答，评判 peer responses (同行回复)，并将 dense peer assessments (密集的同行评估) 聚合为 relative performance estimates (相对性能估计)，整个过程无需 human supervision (人工监督) 或 gold references (黄金参考)。PeerRank 将评估视为一个 multi-agent process (多智能体过程)，其中每个模型均对称地参与 task designer (任务设计者)、respondent (回答者) 和 evaluator (评估者) 的角色，同时消除 biased judgments (有偏见的评判)。在一项涉及 12 个 commercially available models (商业可用模型) 和 420 个 autonomously generated questions (自主生成问题) 的大规模研究中，PeerRank 生成了稳定且具有 discriminative (区分度) 的排名，并揭示了可测量的 identity bias (身份偏见) 和 presentation bias (呈现偏见)。排名结果具有 robustness (稳健性)，且 mean peer scores (平均同行评分) 与 Elo (等级分系统) 一致。我们进一步在 TruthfulQA 和 GSM8K 数据集上验证了 PeerRank，结果表明 peer scores (同行评分) 与 objective accuracy (客观准确率) 相关。综上所述，这些结果表明，结合 selective web-grounded answering (选择性网络检索回答) 的 bias-aware peer evaluation (偏见感知同行评估)，能够将 open-world LLM assessment (开放世界 LLM 评估) 扩展到 static (静态) 和 human curated benchmarks (人工策划基准) 之外。",
                    "inspiration_trace": "基于对论文《PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review》的深入分析，以下是对作者核心方法论产出逻辑链的系统性推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个从“现实需求”到“现有缺陷”再到“核心挑战”的叙事链条：\n\n1.  **现实背景**：大语言模型（LLM）正越来越多地部署在对正确性、鲁棒性和时效性要求极高的领域，且实际部署往往依赖于网络检索和综合能力。\n2.  **现状落差**：当前的评估方法严重滞后。主流方法仍依赖静态基准，这些基准由人工编写任务和参考答案。\n3.  **具体痛点**：\n    *   **维护成本高且易过时**：静态基准难以维护，且随着时间推移迅速过时。\n    *   **数据污染**：模型在训练中可能已见过这些测试题，导致评估失效。\n    *   **场景错配**：现有评估假设“封闭世界”，而实际部署是“开放世界”（涉及工具使用和网络访问）。\n4.  **现有方案的不足**：虽然出现了“LLM-as-a-judge”和基于偏好的评估（如Chatbot Arena），但它们要么仍依赖人工编写的提示词或金标准答案，要么依赖单一裁判，未能实现完全的自主性和规模化。\n5.  **核心挑战**：如何在没有任何外部监督（无人工提示、无金标准答案）的情况下，仅通过模型之间的互评，依然能产生有意义、可解释且大规模的性能评估？\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者提出的核心研究问题为：\n\n**“LLM能否在没有外部监督的情况下，通过同伴互评的方式进行评估，同时仍能产生有意义且可解释的大规模性能估计？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从宏观观察到具体方法论的思考过程经历了以下四个关键阶段的演进：\n\n#### 第一阶段：观察与反思——评估范式的“滞后性”\n*   **观察**：模型能力正在向“智能体”进化（具备联网、工具使用能力），但评估手段还停留在“做题家”时代（静态数据集）。\n*   **思考**：如果评估环境与实际部署环境脱节（封闭 vs 开放），那么评估结果就是无效的。我们需要一种能模拟真实开放世界场景的评估方式。\n\n#### 第二阶段：假设提出——从“人工主导”转向“模型内生”\n*   **假设**：既然人工编写题目和答案成本高、更新慢，且容易导致数据污染，那么能否让模型自己生成题目、自己回答？\n*   **推演**：如果让模型互为出题者和答题者，评估就变成了一个“内源”系统。这解决了“过时”和“污染”问题，因为题目是实时生成的。但这带来了一个新问题：谁来评分？\n\n#### 第三阶段：机制设计——引入“同伴互评”与“偏见控制”\n*   **思路**：引入“同行评审”机制，让模型互评。这比单一裁判更客观，且能规模化。\n*   **关键洞察（Bias-awareness）**：作者意识到LLM作为裁判存在已知的系统性偏见（如位置偏见、自我偏好、身份偏见）。如果直接互评，结果会被噪音淹没。\n*   **对策**：必须将“偏见”作为一等公民来处理。设计不仅要控制偏见（通过打乱顺序、盲审），还要量化偏见，将其作为评估结果的一部分输出，而不是试图掩盖它。\n\n#### 第四阶段：系统构建——整合“开放世界”与“多智能体”\n*   **整合**：将上述思考整合为一个闭环的多智能体系统。\n    *   **出题**：模型自主生成（内生性）。\n    *   **答题**：允许使用实时网络检索（开放世界/真实性）。\n    *   **评分**：在无网络环境下进行（保证公平性），并实施严格的盲审和打乱协议（去偏见）。\n    *   **聚合**：通过统计聚合（如Elo或平均分）得出最终排名。\n\n**总结**：作者的思想演进是从**“发现评估环境与真实环境的错配”**出发，提出**“完全内生化的评估假设”**，进而通过**“引入偏见控制机制”**解决互评的可靠性问题，最终构建出**“基于Web检索的、去偏见的自主同行评审框架”**。"
                },
                {
                    "title": "Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers",
                    "arxiv_id": "2602.02559",
                    "authors": "Pengyu Dai, Weihao Xuan, Junjue Wang, Hongruixuan Chen, Jian Song, Yafei Ou, Naoto Yokoya",
                    "summary": "Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \\textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献符合“自我演化”与“多智能体”方向**： 论文明确提出了 **GeoEvolver**，这是一个 **self-evolving multi-agent system (自我演化多智能体系统)**。其核心贡献在于构建了一个让智能体通过结构化交互获取专业知识并进行自我完善的框架，而非仅仅将LLM作为工具应用。 2.  **满足“自我演化”的筛选标准**： 论文详细描述了智能体如何通过“演化记忆库”来存储成功的模式和失败的归因，从而在未来的查询中提供上下文演示。这直接对应了您关注的核心范式中的 `Self-Evolving`、`Self-Improvement` 以及 `Memory` 机制。 3.  **满足“多智能体”与“工具使用”的筛选标准**： 论文涉及 `Multi-Agent Systems (MAS)`，通过检索增强的多智能体编排器分解查询，并在子目标层面探索工具参数配置。这体现了智能体间的协作以及 `Tool Use / Tool Augmentation` 能力。 4.  **符合“特殊和模糊情况”中的例外规则**： 尽管论文的应用领域是地球观测（Earth Observation，涉及多模态数据），但根据筛选标准第四步第2点，只要论文的核心是提出一种新的“自我演化”机制（即 GeoEvolver 框架），即使应用在特定领域，也应该保留。论文的重点在于智能体如何通过经验演化来处理复杂任务，而非改进视觉模型本身。 综上所述，该论文在构建自我演化多智能体系统的方法论上做出了实质性贡献，高度契合您关于“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有LLM智能体在地球观测（EO）任务中因缺乏细粒度工具级专业知识而导致的执行落地性不足问题。针对多模态、长周期的EO工作流场景，我们提出了一种名为GeoEvolver的自进化多智能体系统，通过分解子目标、并行探索及经验蒸馏构建动态记忆库，实现免参数更新的领域专长获取。在ThinkGeo、Earth-Agent和GeoPlan-Bench三个基准上，通过端到端准确率等指标验证了其有效性，平均提升了12%的任务成功率。",
                    "summary_translation": "近期的进展使得大语言模型 (LLM) 智能体能够通过编排外部工具来解决复杂任务。然而，在那些需要长视界执行、跨模态紧密协调以及严格遵守隐式工具约束的专业化、工具密集型领域，这些智能体往往面临困难。对地观测 (EO) 任务是这一挑战的典型代表，因为其涉及多模态和多时相的数据输入，以及地理知识约束（如光谱库、空间推理等）的要求：许多高层计划往往会因细微的执行错误而偏离轨道，这些错误在处理流程中传播，最终导致结果无效。核心困难在于，现有的智能体缺乏一种从交互中学习细粒度、工具级专业知识的机制。缺乏此类专业知识，它们无法可靠地配置工具参数或从执行中途的失败中恢复，从而限制了其在复杂 EO 工作流中的效能。为解决这一问题，我们提出了 **GeoEvolver**，这是一个自进化多智能体系统 (MAS)，能够在无需任何参数更新的情况下，使 LLM 智能体通过结构化交互获取 EO 专业知识。GeoEvolver 利用检索增强型多智能体编排器将每个查询分解为独立的子目标，进而在子目标层面探索多样化的工具参数配置。随后，成功模式与失败的根本原因归因被提炼至一个不断进化的记忆库中，为未来的查询提供上下文演示。在三个集成工具的 EO 基准测试上的实验表明，GeoEvolver 持续提升了端到端任务成功率，在多个 LLM 骨干网络上的平均增益达到 12%，这证明了 EO 专业知识能够通过与环境进行高效、细粒度的交互而逐渐涌现。",
                    "inspiration_trace": "基于对论文《Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 第一部分：Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从“通用成功”到“领域失效”再到“核心痛点”的叙事链条：\n\n1.  **宏观背景（通用能力的崛起）：**\n    大语言模型（LLM）驱动的智能体已经展示了通过编排外部工具解决复杂任务的广泛通用性，能够跨越不同领域遵循语言指令进行操作。\n\n2.  **遭遇瓶颈（专业领域的挑战）：**\n    然而，当这些智能体被部署到**专业化、工具密集型**的工作流中时（特别是地球观测 EO 任务），它们遇到了持续的瓶颈。这些领域不仅需要高层规划，更需要“执行落地性”。\n\n3.  **深入剖析（EO 任务的特殊性）：**\n    EO 任务是“状态耦合”的。任务的每一步有效性不仅取决于输入数据，还取决于不可见的元数据属性（如空间分辨率、时间覆盖、坐标投影）。这些属性构成了刚性的物理上下文。一旦违反这些耦合，就会导致**“幻觉执行”**——工具链虽然运行成功，但物理语义已被破坏。\n\n4.  **现状诊断（现有方法的缺陷）：**\n    现有的 EO 智能体（无论是语言中心还是视觉-语言模型）未能弥合高层推理与底层精确执行之间的结构性鸿沟。它们缺乏领域专业知识来“落地”执行动作，具体表现为：无法配置精确的工具参数、无法验证中间数据状态、无法从执行失败中恢复。简而言之，它们依赖静态知识，缺乏从交互中获取工具级专业能力的机制。\n\n---\n\n### 第二部分：显式总结的“研究问题”\n\n基于上述逻辑链，作者试图回答的核心研究问题为：\n\n**“智能体能否在不进行参数更新的情况下，仅通过积累和提炼验证过的交互经验，就能获得精细化的工具级领域专业知识，从而在状态耦合的复杂环境中实现可靠的执行落地？”**\n\n---\n\n### 第三部分：思想演进脉络（从观察到方法论）\n\n作者的思想演进遵循了“现象观察 -> 假设提出 -> 方法构建”的路径：\n\n#### 1. 现象观察：失败的本质是“执行”而非“规划”\n*   **观察：** 作者分析现有的 EO 智能体发现，绝大多数失败并非源于高层计划错误，而是源于**执行层面的错配**。\n*   **洞察：** 在 EO 领域，地理空间基元是物理落地的（坐标系统、分辨率等）。LLM 往往将这些视为符号参数，导致看似合理的计划在处理投影、重采样或时间切片时出现微妙的错配。这些早期的、微小的错误会静默传播并导致下游步骤崩溃。\n*   **结论：** 核心挑战从“如何规划”转移到了“如何在工具耦合约束下可靠地执行”。\n\n#### 2. 假设提出：经验即参数\n*   **类比：** 人类专家的能力不仅来自抽象规划，更来自积累执行时的先验知识（如工具故障模式、所需约束、调试策略）。\n*   **假设：** 智能体可以通过高频与环境交互、验证约束满足度，并从成功和失败的轨迹中提炼可重用的先验，从而**无需更新模型参数**就能实现专业化。\n*   **理论支撑：** 检索增强推理表明，将提炼的经验作为可检索的上下文示例，可以有效塑造未来的行为。\n\n#### 3. 方法构建：GeoEvolver 的自我进化逻辑\n为了验证上述假设，作者构建了一个无需训练的多智能体系统，其核心思想演进如下：\n\n*   **解耦与分解：**\n    为了解决长视界任务中的错误传播问题，首先必须将高层规划与底层执行解耦。将查询分解为独立的、具有明确 I/O 的子目标，以隔离错误并局部化上下文。\n\n*   **并行探索：**\n    由于单次执行极易出错，系统需要维持多个并行的探索变体。通过高频的工具-环境交互，收集多样化的执行反馈。这不仅是尝试，更是为了“试错”以获取边界信息。\n\n*   **对比蒸馏与记忆进化：**\n    这是最关键的一步。作者认为，不仅要学习成功，更要利用失败。\n    *   **单变体提取：** 从最佳解中提取成功的模式。\n    *   **对比蒸馏：** 将所有变体（成功与失败）拼接，让 LLM 综合出跨变体的通用工作流不变量和常见的失败模式。\n    *   **记忆库：** 将这些提炼出的经验（成功策略和失败归因）存储在一个非参数化的记忆库中，作为未来的“上下文先验”。\n\n**总结：** 作者的思考过程是从发现通用智能体在物理约束领域的“落地难”问题出发，意识到缺乏“执行落地性”是根源，进而提出用“交互经验”替代“模型微调”的假设，最终设计出一套通过“分解-探索-对比-记忆”闭环来实现自我进化的智能体框架。"
                },
                {
                    "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models",
                    "arxiv_id": "2602.03704",
                    "authors": "Yu Tian, Linh Huynh, Katerina Christhilf, Shubham Chakraborty, Micah Watanabe, Tracy Arner, Danielle McNamara",
                    "summary": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了 \"ReQUESTA\"，这是一个**混合多智能体框架**。它不仅仅是将LLM作为工具应用于教育领域，而是构建了一个新的系统架构，通过协调多个LLM驱动的智能体与基于规则的组件来解决问题。这完全符合“构建、改进LLM智能体”的核心目标。 2.  **属于多智能体方向 (第二步)**: 论文明确属于 **Multi-Agent Systems (MAS)** 范畴。它涉及将MCQ创作任务分解为专门的子任务，并协调多个智能体进行工作。这涵盖了多智能体研究中的协作与任务分配。 3.  **具备Agentic核心能力 (第二步)**: 摘要中明确提到该框架支持 **Planning** (规划)、**Iterative evaluation** (迭代评估，类似于自我反思/修正) 以及受控生成。这些都是Agentic AI的关键特征。 4.  **非单纯应用 (第四步)**: 尽管论文的应用场景是生成多选题（教育领域），但论文的落脚点在于证明 \"hybrid, agentic orchestration\"（混合智能体编排）如何提高生成的可靠性和可控性，并强调了 \"workflow design\"（工作流设计）的重要性。因此，它属于方法论层面的贡献，而非单纯的非演化型应用。 综上所述，该论文在多智能体框架构建和智能体编排方面做出了实质性贡献，符合研究课题关于“LLM智能体及其演化”的筛选要求。",
                    "summary2": "本文旨在解决LLM生成多选题时难以满足特定认知需求的问题。针对学术说明性文本场景，我们提出了一种名为ReQUESTA的混合多智能体框架，结合LLM智能体与基于规则的组件，通过任务分解和迭代评估实现可控生成。在大规模阅读理解实验中，通过心理测量指标（如难度、区分度）和专家评估验证了其有效性。结果表明，ReQUESTA生成的题目更具挑战性和区分度，且干扰项质量显著优于GPT-5零样本基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models》的深度分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观叙事：从“潜力”到“困境”的逻辑演进\n*(基于 Introduction 部分的“讲故事”逻辑提取)*\n\n作者的思想起点并非直接提出技术方案，而是建立在对当前技术现状与实际应用需求之间矛盾的深刻洞察上：\n\n1.  **技术机遇：**\n    *   **观察：** 大语言模型（LLMs，如GPT-4/5）的出现使得自动化语言任务变得前所未有的容易，特别是在教育领域，自动化生成多选题（MCQ）以减少人工负担已成为可能。\n    *   **现状：** 现有的LLM已经能够生成语法通顺、看似合理的题目，甚至在某些指标上接近人类水平。\n\n2.  **核心挑战：**\n    *   **深入观察：** MCQ不仅仅是文本生成，它是一个高度结构化的评估工具。一个高质量的MCQ必须同时满足两个严苛条件：\n        *   **结构约束：** 题干清晰、答案唯一、干扰项必须看似合理且在语言形式上保持一致。\n        *   **认知多样性：** 题目必须考察不同层级的思维能力，从简单的“文本回忆”到复杂的“推理”和“综合”。\n    *   **痛点揭示：** 尽管LLMs在“回忆”类低阶认知任务上表现出色，但在可靠地生成“推理”或“综合”类高阶认知题目时表现不佳。单次提示往往导致题目过于简单、干扰项质量低劣或无法准确命中教学目标。\n\n3.  **逻辑缺口：**\n    *   **结论：** 仅仅依靠模型规模的扩大或更聪明的提示词，无法解决“可控性”和“认知对齐”的问题。我们需要一种新的范式，从“单次生成”转向“系统化工程”。\n\n---\n\n### 二、 核心研究问题\n*(基于上述逻辑缺口显式总结)*\n\n**能否通过混合多智能体框架的工作流设计，而非单纯依赖模型能力，实现对LLM生成多选题的认知层级的精准控制，并显著提升其心理测量学质量和专家评估质量？**\n\n---\n\n### 三、 思想演进：从“假设”到“方法论”的构建链\n*(还原作者产出 ReQUESTA 框架的思考过程)*\n\n为了解决上述研究问题，作者经历了一个从解构问题到重构系统的思维过程：\n\n#### 1. 观察与反思：单次生成的局限性\n*   **思考：** 为什么直接让GPT-5“出一道推理题”效果不好？\n*   **分析：** 因为“生成”是一个概率过程，而“高质量评估”是一个受多重约束的确定性过程。将所有任务（理解文本、构思问题、设计干扰项、检查格式）都塞进一个Prompt里，会导致模型注意力分散，无法同时兼顾“认知深度”和“结构严谨性”。\n*   **假设：** 如果把复杂的MCQ生成任务拆解成多个独立的子任务，每个子任务由专门的“角色”负责，效果可能会更好。\n\n#### 2. 核心假设：工作流设计 > 模型规模\n*   **理念确立：** 作者提出“编排”是关键。与其追求一个全能的模型，不如构建一个智能的系统。这个系统应该结合LLM的**生成能力**（创造力、语言理解）和基于规则的**控制能力**（逻辑严密性、格式一致性）。\n\n#### 3. 方法论构建：ReQUESTA 框架的诞生\n基于上述假设，作者开始设计具体的架构，遵循以下逻辑步骤：\n\n*   **第一步：任务解耦与规划**\n    *   *思考：* 生成题目前，必须先“读懂”文章并决定考什么。\n    *   *设计：* 引入 **Planner（规划者）**。它的作用不是生成题目，而是先生成一个“中间表示”，分析文章结构，提取关键概念，并明确指定哪一段落生成哪种类型（文本类、推理类、主旨类）的题目。这解决了“认知对齐”的问题。\n\n*   **第二步：专业化分工**\n    *   *思考：* 考察“记忆”和考察“推理”的思维模式是不同的，不应该用同一个Prompt。\n    *   *设计：* 引入多个 **Specialized Question Generators（专门的问题生成器）**。分别为文本类、推理类和主旨类题目设计独立的Agent。每个Agent拥有特定的Prompt策略（如Persona-based prompting），确保生成的题目符合特定的认知层级。\n\n*   **第三步：混合控制**\n    *   *思考：* LLM生成的结果不可控（如选项长短不一、格式混乱），需要确定性手段来约束。\n    *   *设计：* 引入 **Rule-based Agents（基于规则的Agent）**，如Controller（控制器）和Formatter（格式化器）。用Python脚本等确定性逻辑来处理任务分配、选项打乱、长度平衡等，保证输出的结构一致性。\n\n*   **第四步：迭代与反思**\n    *   *思考：* 第一次生成的题目往往不完美，人类出题师会反复修改，AI系统也应该如此。\n    *   *设计：* 引入 **Evaluator（评估者）** 和 **Self-Critique（自我批判）** 机制。建立一个反馈循环，如果生成的题目不符合预设标准（如干扰项太明显），系统会自动打回并要求重写，直到满足质量阈值。\n\n#### 4. 验证逻辑：从“表面指标”转向“功能实效”\n*   **思考：** 怎么证明这个复杂的系统比直接问GPT-5要好？\n*   **决策：** 不能只看题目通不通顺（表面指标），要看题目能不能真正区分出学生的水平（功能指标）。\n*   **设计：** 采用双重验证体系。\n    *   **心理测量学分析：** 用真实学生做实验，看题目的难度和区分度。\n    *   **专家评估：** 请教育专家评估干扰项的合理性和题目的认知深度。\n\n---\n\n### 总结\n作者的思考路径是从**发现LLM在复杂教育评估任务中的“不可控性”**出发，提出**“系统化编排优于单次生成”**的核心假设，进而通过**任务解耦、角色专业化、混合控制（LLM+规则）和迭代反馈**四个维度构建了ReQUESTA框架，最终通过**实证数据**验证了这一工程化思路的有效性。"
                },
                {
                    "title": "Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems",
                    "arxiv_id": "2602.03695",
                    "authors": "Haibo Jin, Kuang Peng, Ye Yu, Xiaopeng Yuan, Haohan Wang",
                    "summary": "While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories. In this work, we propose \\textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS. Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\\times$-4$\\times$ compared to text-based MAS, while incurring only 1.3$\\times$-1.6$\\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断（第一步）：** 论文的核心贡献是提出了 **\"Agent Primitives\"（智能体原语）**，这是一种用于构建基于LLM的多智能体系统（MAS）的新框架。它并非将现有智能体作为工具应用到特定领域（如医疗或金融），而是针对现有MAS架构“任务特异性强、复用性差、通信不稳定”的问题，提出了一种新的**构建和改进LLM智能体架构的方法论**。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标匹配（第二步）：** *   **核心范式**：论文明确聚焦于 `Multi-Agent Systems (MAS)`。 *   **智能体能力**：论文实例化的原语包括 `Planning and Execution`（规划与执行），并引入了一个 `Organizer agent` 来负责系统的自动构建，这直接涉及智能体的规划和构建逻辑。 *   **多智能体机制**：论文探讨了如何通过可复用的组件（Review, Voting, Selection）来优化智能体间的交互和协作模式，属于多智能体协作与架构优化的范畴。 3.  **排除标准检查（第三步）：** 论文不涉及安全与对齐、多模态视觉或图技术等排除项。 **总结：** 该论文通过引入可复用的潜在构建块和KV cache通信机制，从架构层面改进了多智能体系统的构建效率和稳定性，是对LLM智能体（特别是多智能体系统）构建方法的重要创新，因此应当保留。",
                    "summary2": "本文旨在解决现有多智能体系统（MAS）架构复杂且依赖自然语言通信导致的不稳定问题。针对复杂任务场景，我们提出了一种基于 KV Cache 潜在通信的可复用构建块 Agent Primitives，并通过 Organizer 自动组合构建系统。在8个涵盖数学、代码和问答的基准测试上，通过准确率、Token 使用量和推理延迟验证了其有效性。",
                    "summary_translation": "尽管现有的多智能体系统 (MAS) 能够通过多个智能体之间的协作来处理复杂问题，但它们通常高度任务特定，依赖于人工设计的智能体角色和交互提示词，这导致了架构复杂性的增加以及跨任务可复用性的受限。此外，大多数 MAS 主要通过自然语言进行通信，这使得它们在内部智能体历史记录中的长上下文、多阶段交互中容易受到误差累积和不稳定性的影响。在这项工作中，我们提出了 **Agent Primitives** (智能体原语)，这是一组用于基于 LLM 的 MAS 的可复用潜在构建块。受到神经网络设计的启发——其中复杂模型由可复用组件构建而成——我们观察到，许多现有的 MAS 架构可以分解为少量重复出现的内部计算模式。基于这一观察，我们实例化了三个原语：Review (审查)、Voting and Selection (投票与选择) 以及 Planning and Execution (规划与执行)。所有原语通过 key-value (KV) cache (键值缓存) 进行内部通信，这通过缓解多阶段交互中的信息退化，同时提高了鲁棒性和效率。为了实现自动系统构建，一个 Organizer agent (组织者智能体) 为每个查询选择并组合原语，该过程由先前成功配置的轻量级知识池引导，从而形成一个基于原语的 MAS。实验表明，基于原语的 MAS 相比单智能体基线将平均准确率提高了 12.0-16.5%，相比基于文本的 MAS 将 Token 使用量和推理延迟减少了约 3-4 倍，同时相对于单智能体推理仅产生 1.3-1.6 倍的开销，并在不同的模型骨干上提供了更稳定的性能。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
                    "arxiv_id": "2602.03414",
                    "authors": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang",
                    "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated \"image-code-instruction\" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Multi-Agent 方向）**： 论文的核心贡献是提出了 **Socratic-Geo**，这是一个通过 **多智能体交互** 实现的完全自主框架。论文明确构建了三个不同的智能体角色：**Teacher agent**（负责生成脚本和反思反馈）、**Solver agent**（负责推理优化）和 **Generator**（负责图像生成）。这完全属于“构建 LLM 智能体”和“多智能体系统”的范畴，而非单纯的应用。 2.  **正面指标（高度匹配）**： *   **多智能体协作**：论文标题和摘要均强调了 \"Multi-Agent Interaction\"，描述了智能体之间如何通过反馈循环（失败路径指导 Teacher 增强）进行协作。 *   **自我反思与修正**：Teacher agent 使用了 \"Reflective feedback\"（反思性反馈）和 \"RePI for visual validity\"，这直接对应了筛选标准中的 `Self-Correction` 和 `Self-Reflection` 能力。 *   **演化机制**：系统通过 \"dynamically couples data synthesis with model learning\"（动态耦合数据合成与模型学习），实现了迭代改进，符合自我演化的特征。 3.  **排除标准（通过例外条款）**： 尽管论文涉及 \"Multimodal Large Language Models (MLLMs)\"、\"Vision\" 和 \"Image Generation\"，但根据筛选标准中的特殊条款：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在本论文中，视觉和几何推理是智能体操作的**领域和环境**，研究的核心在于**智能体如何通过交互和反思来生成数据并提升推理能力**，而非改进视觉模型本身的基础架构。因此，不应将其排除。 综上所述，该论文提出了一种新的多智能体协作与反思框架，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决多模态大语言模型在几何推理中面临的高质量数据稀缺问题。针对几何图像-文本对的生成与推理训练，我们提出了一种名为Socratic-Geo的多智能体交互框架。该框架通过Teacher、Solver和Generator的协同，利用Reflect-RePI机制实现目标驱动的程序化合成。在MathVerse、GenExam等基准测试上，通过准确率和Relaxed Score验证了其有效性，显著提升了数据效率和推理性能。",
                    "summary_translation": "多模态大语言模型显著推进了视觉-语言理解的发展。然而，即使是最先进的模型在几何推理方面也面临挑战，这揭示了一个关键瓶颈：高质量图像-文本对的极度匮乏。人工标注成本过高，而自动化方法无法保证保真度和训练效果。现有方法要么被动适应现有图像，要么采用低效的带过滤随机探索机制，导致数据生成与学习需求解耦。我们提出了 Socratic-Geo，这是一个完全自主的框架，通过多智能体交互将数据合成与模型学习动态耦合。Teacher 智能体生成参数化 Python 脚本，并结合反思反馈（Reflect 用于可解性，RePI 用于视觉有效性），以确保图像-文本对的纯度。Solver 智能体通过偏好学习优化推理，其中失败路径指导 Teacher 进行针对性的增强。此外，Generator 智能体在积累的“图像-代码-指令”三元组上学习图像生成能力，将程序化绘图智能提炼为视觉生成能力。仅从 108 个种子问题开始，Socratic-Solver 在使用基线数据四分之一的情况下，在六个基准测试中达到了 49.11 的成绩，超过了强基线模型 2.43 分。Socratic-Generator 在 GenExam 上达到了 42.4%，为开源模型树立了新的最先进水平，超过了 Seedream-4.0 (39.8%) 并接近 Gemini-2.5-Flash-Image (43.1%)。",
                    "inspiration_trace": "基于论文《Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从现象到痛点\n\n作者在Introduction中构建了一个层层递进的逻辑链条，用于引出研究的必要性：\n\n1.  **宏观背景与现象**：多模态大语言模型（MLLMs）在视觉-语言理解方面取得了显著进展，但在**几何推理**这一特定领域仍面临巨大挑战。几何不仅需要视觉感知，更需要严密的逻辑演绎。\n2.  **锁定核心瓶颈**：造成这一瓶颈的根本原因并非模型架构本身，而是**高质量几何训练数据的极度稀缺**。\n3.  **排除现有方案**：\n    *   **人工标注**：虽然质量高，但成本极其昂贵且耗时，不可扩展。\n    *   **自动化合成**：现有方法难以在“保真度”和“训练有效性”之间取得平衡。\n4.  **深入剖析现有自动化的缺陷**：\n    *   **被动适应**：基于图像的文本增强方法只能被动地描述现有图像，无法构建新的几何结构。\n    *   **盲目探索**：符号驱动的随机生成方法采用“广撒网后过滤”的策略，效率低下。\n    *   **黑盒放大**：LLM驱动的方法像黑盒一样放大内容，继承了模型偏差且缺乏精细控制。\n5.  **揭示根本性缺失**：所有现有范式都产生静态的、单向的数据集。它们将**数据合成与模型学习解耦**了——数据生成独立于模型训练之外，从而错失了迭代改进的机会。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者显式提出了本论文旨在解决的核心问题：\n\n> **“How can we design an efficient geometric data synthesis engine?”**\n> （我们如何设计一个高效的几何数据合成引擎？）\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n以下是从宏观问题出发，逐步聚焦并形成最终方法论的思考过程：\n\n#### 1. 观察与假设：从“静态数据”到“动态互动”\n*   **思考起点**：既然现有方法失败的原因是“合成与学习解耦”，那么解决方案的核心必须是**将两者重新耦合**。\n*   **灵感来源**：苏格拉底教学法。老师不是单向灌输，而是根据学生的回答（特别是错误）来提出下一个问题，引导学生思考。\n*   **假设**：如果数据生成器能像苏格拉底一样，根据模型当前的推理弱点来“出题”，那么生成的数据将具有最高的训练价值。\n\n#### 2. 机制设计：从“盲目生成”到“目标驱动”\n*   **问题**：如何让数据生成器知道模型的弱点？\n*   **方案**：引入**多智能体交互**。\n    *   **Solver（学生）**：负责解题，暴露错误。\n    *   **Teacher（老师）**：负责诊断Solver的错误，并针对性地生成新题。\n*   **演进**：这不再是随机的数据扩充，而是**目标驱动的合成**。只有当Solver失败时，Teacher才介入生成新数据，从而实现数据效率的最大化。\n\n#### 3. 领域适配：从“文本逻辑”到“几何保真”\n*   **挑战**：几何不同于纯文本数学，它要求图像与文本的绝对一致性。现有的LLM直接生成图像或文本往往存在几何谬误（如线段长度不匹配、角度错误）。\n*   **思考**：如何保证生成的几何题是绝对正确的？\n*   **关键转折**：放弃直接生成像素或模糊的自然语言描述，转而使用**参数化Python脚本**作为中间媒介。\n    *   代码是确定性的，执行代码生成的图像在数学上是严谨的。\n    *   这引入了**Reflect（可解性检查）**和**RePI（视觉验证）**机制，确保生成的题目在数学上可解且图像无误。\n\n#### 4. 闭环构建：从“单一推理”到“生成与推理共生”\n*   **思考**：Teacher生成的Python代码不仅生成了图像，还包含了精确的绘图指令。这是否可以复用？\n*   **扩展**：引入第三个智能体——**Generator**。\n    *   Generator不参与推理循环，但它作为“副产品”存在。\n    *   它学习Teacher的“程序化绘图智能”，将代码逻辑转化为像素级的图像生成能力。\n*   **最终闭环**：Solver失败 -> Teacher诊断并生成代码 -> 代码渲染出图像供Solver训练 -> 同时代码指令训练Generator。形成了一个**合成-学习-生成**的完整闭环。\n\n#### 5. 训练范式：从“模仿学习”到“强化学习”\n*   **思考**：Solver应该如何从Teacher那里学习？是直接抄袭Teacher的解题步骤吗？\n*   **决策**：不，模仿学习无法提升探索能力。\n*   **方法**：采用**强化学习（GRPO）**。Solver只能看到题目和二元的“对/错”奖励信号，必须通过试错来学习。这确保了推理能力的真正进化，而非简单的模式记忆。\n\n---\n\n### 总结\n\n作者的思考路径是从**数据稀缺**这一痛点出发，批判了现有方法**静态、解耦**的本质，进而借鉴**苏格拉底教学法**提出了**动态、交互**的假设。为了解决几何领域的**保真度**难题，创新性地引入**程序化生成**作为核心控制手段，最终构建了一个集**推理进化**与**可控生成**于一体的多智能体闭环系统。"
                },
                {
                    "title": "MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning",
                    "arxiv_id": "2602.03320",
                    "authors": "Shengyuan Liu, Liuxin Bao, Qi Yang, Wanting Geng, Boyun Zheng, Chenxin Li, Wenting Chen, Houwen Peng, Yixuan Yuan",
                    "summary": "Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \\href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **MedSAM-Agent** 这一新框架，而不是简单地将现有模型应用于医疗领域。 *   该框架明确将交互式分割重构为一个 **多步自主决策过程**，这属于 **Agentic AI** 中的 **单智能体** 研究方向。 *   论文涉及的核心能力包括 **Tool Use** (编排 Segment Anything Model)、**Planning** (多步决策) 和 **Self-Refinement** (自适应细化策略)。 2.  **包含自我演化与迭代机制 (第二步 & 第四步)**: *   论文提出了一种 **两阶段训练管线**，结合了 **多轮端到端结果验证** 和 **过程奖励设计**。这种通过反馈和强化学习来优化决策策略、促进交互简洁性的方法，属于 **自我演化** 或 **自我完善** 的范畴。 *   它解决了现有方法“单轮、僵化”的问题，通过迭代优化提升了智能体的能力。 3.  **视觉/多模态作为工具而非核心 (第三步)**: *   虽然论文涉及医学图像分割（视觉任务），但视觉模型 (SAM) 在这里被视为智能体调用的 **专用工具**。 *   研究的重点在于 **智能体如何通过推理和强化学习来更好地使用这个工具**（即如何进行多轮交互和决策），而不是改进视觉模型本身。因此，这符合“除非它们被用作智能体感知环境的工具”这一例外情况。 综上所述，该论文致力于构建和改进 LLM 智能体的决策与交互框架，属于 Agentic AI 的核心研究范围。",
                    "summary2": "本文旨在解决现有医学图像分割中交互策略僵化且缺乏过程级监督的问题。针对多模态医学图像，我们提出了一种MedSAM-Agent框架，通过混合提示策略生成专家轨迹，并设计了包含临床保真度过程奖励的两阶段训练管线。在涵盖6种模态的21个数据集上，通过Dice和IoU指标验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "医学图像分割正从特定任务模型向通用框架演进。近期研究将Multi-modal Large Language Models (MLLMs, 多模态大语言模型) 作为自主智能体，采用reinforcement learning with verifiable reward (RLVR, 带可验证奖励的强化学习) 来调度如Segment Anything Model (SAM, 分割任何模型) 等专用工具。然而，这些方法往往依赖单轮、僵化的交互策略，且在训练中缺乏过程级监督，这限制了其充分挖掘交互工具动态潜力的能力，并导致动作冗余。为填补这一空白，我们提出了MedSAM-Agent，该框架将交互式分割重构为一个多步自主决策过程。首先，我们引入了一种用于生成专家策划轨迹的混合提示策略，使模型能够内化类人决策启发式和自适应细化策略。此外，我们开发了一个两阶段训练流程，整合了多轮端到端结果验证与临床保真度的过程奖励设计，以促进交互精简性和决策效率。在6种医学模态和21个数据集上的广泛实验表明，MedSAM-Agent取得了最先进的性能，有效地将自主医学推理与鲁棒的迭代优化统一起来。代码可在[此处]获取。",
                    "inspiration_trace": "基于对论文《MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从发现问题到构建解决方案的完整思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从现状到痛点\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在引出当前研究的必要性：\n\n1.  **背景与基石**：医学图像分割是临床诊断的基础。传统模型（如UNet）虽然性能尚可，但缺乏泛化能力，无法适应新任务。\n2.  **第一次飞跃（交互式模型）**：SAM（Segment Anything Model）的出现带来了交互式分割的突破，通过点或框的提示实现了高质量分割。**然而**，它依然严重依赖“人工提示”，无法实现真正的自主化。\n3.  **第二次飞跃（MLLM驱动模型）**：多模态大语言模型（MLLMs）引入了推理能力，试图通过文本指令直接生成分割。**然而**，这类方法往往改变了输出空间，缺乏SAM那种精细的迭代修正能力。\n4.  **当前前沿（MLLM智能体）**：最新的趋势是将MLLM作为智能体，利用强化学习（RLVR）来调用SAM工具。这结合了MLLM的推理和SAM的精度。\n5.  **核心痛点**：尽管方向正确，但现有的“智能体”方案存在两个致命缺陷：\n    *   **交互策略僵化**：要么是单轮交互，要么是死板的“仅点”模式。缺乏人类专家那种“先框定范围，再点修正”的灵活性，无法充分利用动态交互的潜力。\n    *   **训练监督缺失**：现有的强化学习只看“最终结果”（如Dice分数），忽略了“过程效率”。这导致模型产生冗余动作，不知道何时该停，缺乏逻辑连贯性。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题是：\n\n**“如何将医学图像分割重构为一个多步骤的自主决策过程，使其既能像人类专家一样灵活运用混合交互策略（框与点），又能通过过程级监督实现分割精度与交互效率的平衡？”**\n\n---\n\n### 三、 思想演进与方法论构建逻辑链\n\n以下推演了作者从宏观观察到具体方法论的思考路径：\n\n#### 第一阶段：宏观观察与范式转移\n*   **观察**：现有的分割模型正在从“静态的像素分类器”向“动态的工具使用者”转变。\n*   **思考**：SAM本身是一个强大的工具，但它需要一个“大脑”来决定如何使用它。MLLM就是这个大脑。目前的“大脑”太笨（只会单次点击）或者太浪费（无效操作多）。\n*   **决策**：我们需要构建一个真正的“智能体”，它不应该只是被动地响应提示，而应该主动地进行多轮决策。\n\n#### 第二阶段：诊断缺陷与提出假设\n*   **诊断缺陷A（策略僵化）**：人类医生标注时，通常会先画一个框确定大概位置，然后再用点修补边缘。现有模型只允许点，这限制了空间灵活性。\n    *   **假设**：如果我们让模型学习“先框后点”的混合策略，它就能更好地处理形态复杂的病灶。\n*   **诊断缺陷B（过程低效）**：只奖励最终分数会导致模型“为了刷分而乱点”。它不知道“这就够了”。\n    *   **假设**：如果我们引入“过程级奖励”，惩罚冗余步骤并奖励每一步的改进，模型就能学会“适可而止”和“高效修正”。\n\n#### 第三阶段：数据构建——模拟专家行为\n*   **思考**：要训练模型像专家一样思考，首先要有专家的数据。但现实中没有现成的“专家思考轨迹”数据集。\n*   **解决方案**：**“专家策划轨迹生成”**。\n    *   **逻辑**：利用现有的Ground Truth（真实标签）反向推导专家的操作。\n    *   **具体策略**：设计一个**混合提示策略**。\n        *   *Box-to-Point模式*：先根据GT生成一个带噪的框（模拟人工粗略框选），再根据误差区域生成点。\n        *   *Sequential-Click模式*：模拟纯点修正。\n    *   **质量控制**：引入“进度约束采样”，确保每一步模拟操作都能带来IoU的提升，剔除无效的“瞎点”操作。\n\n#### 第四阶段：训练机制——从模仿到优化\n*   **思考**：有了数据，怎么教模型？直接用强化学习（RL）太难收敛，因为搜索空间太大。\n*   **解决方案**：**“两阶段训练流水线”**。\n    *   **阶段一：SFT冷启动**。\n        *   *逻辑*：先让模型通过监督学习（SFT）模仿上面生成的专家轨迹。这相当于给模型一个“及格线”，让它学会基本的工具调用和空间定位，解决RL的冷启动难题。\n    *   **阶段二：基于临床保真度的过程奖励优化（RL）**。\n        *   *逻辑*：在SFT的基础上，用RL进行微调，重点解决“效率”和“策略”问题。\n        *   **核心创新：奖励函数设计**。作者设计了一个多维度的奖励系统，而不仅仅是看最终分数：\n            1.  **格式奖励**：必须正确调用工具和停止。\n            2.  **质量奖励**：最终的IoU和Dice分数。\n            3.  **过程奖励（关键）**：\n                *   *改进奖励*：每一步比上一步好，就给分（鼓励单调递增）。\n                *   *过冲惩罚*：过了最高点还在点，就扣分（鼓励及时停止）。\n                *   *成本惩罚*：点得越多，扣分越多（鼓励简洁）。\n\n#### 第五阶段：最终愿景\n*   **总结**：通过这种设计，MedSAM-Agent不再是一个简单的分割模型，而是一个具备“视觉感知-决策规划-工具使用”闭环能力的智能体。它能够像医生一样，先看全局（框），再修细节（点），并在确认完成后自动停止。\n\n---\n\n**逻辑链总结图示：**\n\n1.  **问题**：现有MLLM智能体交互僵化（只会点）、效率低（无效操作多）。\n    ↓\n2.  **目标**：构建一个能像人类专家一样进行多轮、混合、高效决策的分割智能体。\n    ↓\n3.  **数据层（怎么教？）**：反向模拟专家轨迹 $\\rightarrow$ 提出“混合提示策略”（框+点），生成高质量训练数据。\n    ↓\n4.  **模型层（怎么学？）**：SFT（学会基本操作） + RL（学会策略优化）。\n    ↓\n5.  **核心创新（怎么优？）**：设计“临床保真度过程奖励”，不仅看结果，更看过程中的每一步是否有效、是否简洁。\n    ↓\n6.  **产出**：MedSAM-Agent —— 具备自主推理、多轮迭代、工具无关的医学分割智能体。"
                },
                {
                    "title": "Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow",
                    "arxiv_id": "2602.03145",
                    "authors": "Ya-Ting Yang, Quanyan Zhu",
                    "summary": "Large language models (LLMs) have enabled a new class of agentic AI systems that reason, plan, and act by invoking external tools. However, most existing agentic architectures remain centralized and monolithic, limiting scalability, specialization, and interoperability. This paper proposes a framework for scalable agentic intelligence, termed the Internet of Agentic AI, in which autonomous, heterogeneous agents distributed across cloud and edge infrastructure dynamically form coalitions to execute task-driven workflows. We formalize a network-native model of agentic collaboration and introduce an incentive-compatible workflow-coalition feasibility framework that integrates capability coverage, network locality, and economic implementability. To enable scalable coordination, we formulate a minimum-effort coalition selection problem and propose a decentralized coalition formation algorithm. The proposed framework can operate as a coordination layer above the Model Context Protocol (MCP). A healthcare case study demonstrates how domain specialization, cloud-edge heterogeneity, and dynamic coalition formation enable scalable, resilient, and economically viable agentic workflows. This work lays the foundation for principled coordination and scalability in the emerging era of Internet of Agentic AI.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心的 **Multi-Agent (多智能体)** 研究范畴。 1.  **核心贡献判断 (第一步)**: *   论文的核心贡献是提出了一个名为 \"Internet of Agentic AI\" 的框架，旨在解决分布式、异构智能体之间的协作与协调问题。 *   它不是将智能体作为工具去解决医疗等领域的具体问题（医疗案例仅用于演示框架的有效性），而是专注于构建智能体系统的**架构**和**协调机制**。 *   它不属于基础设施优化（如硬件加速），而是利用基础设施来实现智能体间的动态联盟。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Agentic AI` 和 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**: 摘要中提到的 \"dynamic form coalitions\"（动态形成联盟）、\"agentic collaboration\"（智能体协作）、\"decentralized coalition formation algorithm\"（去中心化联盟形成算法）直接对应了筛选标准中的 `Collaboration`（协作）、`Agent Society`（智能体社会）以及 `Communication`（通信）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全对齐、多模态视觉核心研究或图神经网络技术，因此没有触犯排除红线。 综上所述，该论文致力于构建新的多智能体协作框架和算法，属于构建和改进 LLM 智能体系统的前沿研究，符合你的研究目标。",
                    "summary2": "本文旨在解决集中式 Agentic AI 系统的可扩展性和互操作性限制。针对分布式云边环境中的任务驱动工作流，我们提出了一种 Internet of Agentic AI 框架，包含激励兼容的工作流-联盟可行性模型及去中心化联盟形成算法。在医疗保健案例研究中，通过可行性半径、联盟规模及成本效益等指标验证了其有效性。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 催生了一类新型的 agentic AI systems (代理 AI 系统)，这类系统能够通过调用外部工具进行推理、规划和行动。然而，大多数现有的 agentic architectures (代理架构) 仍属于集中式和单体式架构，限制了系统的可扩展性、专业化程度和互操作性。本文提出了一种 scalable agentic intelligence (可扩展代理智能) 框架，称为 Internet of Agentic AI (代理 AI 互联网)。在该框架中，分布于 cloud and edge infrastructure (云和边缘基础设施) 之上的自主异构代理能够动态组建联盟，以执行 task-driven workflows (任务驱动的工作流)。我们形式化定义了一种 network-native (网络原生) 的代理协作模型，并引入了一个 incentive-compatible (激励兼容) 的 workflow-coalition feasibility (工作流联盟可行性) 框架，该框架整合了 capability coverage (能力覆盖)、network locality (网络局部性) 和 economic implementability (经济可实施性)。为实现可扩展的协调，我们构建了一个 minimum-effort coalition selection (最小努力联盟选择) 问题，并提出了一种 decentralized coalition formation (去中心化联盟形成) 算法。所提出的框架可作为 Model Context Protocol (MCP, 模型上下文协议) 之上的协调层运行。通过一个医疗保健领域的案例研究，我们展示了 domain specialization (领域专业化)、cloud-edge heterogeneity (云边异构性) 以及动态联盟形成如何实现可扩展、具有韧性且经济可行的 agentic workflows (代理工作流)。这项工作为 Internet of Agentic AI (代理 AI 互联网) 这一新兴时代的 principled coordination (基于原则的协调) 与可扩展性奠定了基础。",
                    "inspiration_trace": "基于对论文《Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow》的深度分析，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从现状到瓶颈\n\n作者在Introduction部分通过以下逻辑链条引入了核心问题：\n\n1.  **技术范式转移**：大语言模型（LLMs）的进化催生了“Agentic AI”，这类系统不再是被动的信息处理者，而是能够推理、规划并调用工具的主动行动者。\n2.  **现有架构的局限**：尽管模块化架构（如多智能体编排）展现了超越单一模型的潜力，但目前的Agentic AI系统大多是**集中式和单体**的。\n3.  **扩展性危机**：集中式架构虽然简化了集成，但从根本上限制了系统的**可扩展性**。这种限制不在于算力，而在于假设了单一的所有权和控制权，无法跨越组织边界去利用多样化的专业能力。\n4.  **现有协议的不足**：虽然模型上下文协议（MCP）提供了标准化的工具发现和调用接口，解决了“怎么连”的技术问题，但它缺乏解决“怎么合作”的协调层——即如何在开放网络中动态组建团队并分配利益。\n5.  **核心矛盾**：未来的Agentic AI需要从“中心化编排”转向“网络化生态系统”。扩展性不再仅仅是计算问题，而是变成了涉及协调、信任、访问控制和激励的**系统级挑战**。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“在开放的分布式网络环境中，如何构建一个可扩展的框架，使得异构的自主智能体能够基于任务需求动态形成联盟，并在满足网络约束和预算平衡的前提下，实现激励兼容的工作流执行？”**\n\n---\n\n### 三、 核心方法论的逻辑演进链\n\n为了回答上述问题，作者的思考过程经历了从抽象建模到机制设计的五个关键阶段：\n\n#### 1. 观察与抽象：从“单体智能”到“网络化智能”\n*   **思考起点**：现实世界的复杂任务（如医疗诊断）往往需要跨越不同机构（诊所、影像中心、保险公司）的专业能力。没有任何一个单一节点能拥有所有能力。\n*   **抽象建模**：作者将整个系统抽象为一个**通信网络图 $G=(V, E)$**。每个节点代表一个拥有特定能力集合的机构或平台。\n*   **关键定义**：引入“能力覆盖”的概念。任务不再是简单的计算请求，而被定义为一组对特定能力的“需求清单”。这为后续的“供需匹配”奠定了基础。\n\n#### 2. 引入约束：网络局部性与工作流耦合\n*   **思考深化**：仅仅找到有能力的人是不够的，还需要考虑物理距离（网络延迟）和任务执行的逻辑顺序。\n*   **逻辑演进**：\n    *   **空间约束**：定义了“$k$-度可行性”，即联盟成员必须在发起节点的特定跳数范围内，以保证通信效率。\n    *   **逻辑约束**：将任务执行建模为**有向无环图（DAG）形式的工作流**。子任务之间存在依赖关系，这意味着联盟不仅要“有”能力，还要能“按顺序”协作。\n*   **综合**：提出了“工作流-联盟可行性”框架，强调技术上的可执行性（能力匹配+逻辑顺序）与物理上的可达性必须同时满足。\n\n#### 3. 引入博弈论：解决“愿不愿意合作”的问题\n*   **思考转折**：在分布式环境中，节点是自私的。它们拥有算力，但也消耗成本。如果无利可图，节点不会加入联盟。\n*   **机制设计**：作者引入**合作博弈论**。\n    *   **效用函数**：定义节点的净效用 = 分配的奖励 - (计算成本 + 通信成本)。\n    *   **激励兼容（IC）**：设计机制使得节点“说实话”并“留在联盟中”是最优选择，即参与合作的收益大于退出或单干。\n*   **核心洞察**：工作流的产出决定了总奖励池的大小，而总奖励池又反过来限制了联盟的组建成本。这是一个**双向耦合**的问题：经济可行性决定了技术可行性。\n\n#### 4. 优化目标：寻找“最小努力”联盟\n*   **思考聚焦**：在所有满足上述条件（能力覆盖、网络局部、激励兼容）的潜在联盟中，应该选择哪一个？\n*   **效率原则**：为了系统整体的效率和鲁棒性，应该选择消耗资源最少的团队。\n*   **形式化**：提出了**最小努力联盟选择问题**。目标是在满足所有约束的前提下，最小化所有参与节点的总努力（计算与交互成本）。这避免了资源的浪费，并提高了系统的响应速度。\n\n#### 5. 算法落地：去中心化的搜索与执行\n*   **工程实现**：如何在一个没有中心控制器的网络中找到这个最优联盟？\n*   **算法设计**：设计了一个**去中心化的联盟形成算法**。\n    *   **逻辑**：采用“泛洪式”的半径搜索。从发起节点开始，逐步扩大搜索半径（$k=1, 2, ...$）。\n    *   **筛选**：在每一层邻居中，检查是否存在满足“工作流-联盟可行性”的子集。\n    *   **终止**：一旦找到满足条件的联盟，选择其中成本最低的一个，停止搜索。\n*   **架构定位**：明确该算法作为**MCP之上的协调层**。MCP负责底层的工具调用，而该算法负责上层的团队组建和经济核算，实现了与现有基础设施的无缝兼容。\n\n---\n\n### 总结\n\n作者的思考路径是从**技术瓶颈**（集中式不可扩展）出发，通过**网络化视角**重构系统架构，进而识别出**经济激励**与**技术执行**相互耦合的核心挑战，最终利用**博弈论**和**图论**工具，构建了一套既保证技术可行性又确保经济理性的分布式协作框架。"
                },
                {
                    "title": "Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery",
                    "arxiv_id": "2602.03132",
                    "authors": "Timothee Leleu, Sudeera Gunathilaka, Federico Ghimenti, Surya Ganguli",
                    "summary": "Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献分析**: 论文提出了“对比概念树搜索 (CCTS)”，这是一种用于LLM辅助算法发现的迭代优化框架。其核心在于通过构建概念层次结构和对比学习模型，引导LLM在搜索空间中生成更好的候选程序。 2.  **符合自我演化**: 该研究完全符合“自我演化”这一核心方向。论文描述了一个迭代、黑盒的优化过程，LLM作为生成器提出候选程序，外部评估器提供反馈，系统通过重加权父代选择（类似进化算法中的选择机制）来偏向有用的概念组合。这体现了智能体通过环境反馈进行自我完善和迭代改进的机制。 3.  **符合演化算法范式**: 论文涉及“Generational Evolution”（代际演化）和“Iterative Improvement”（迭代改进），使用了父代选择和概念组合等典型的演化算法技术，属于筛选标准中的核心范式。 4.  **特殊规则应用**: 尽管论文的应用场景是“算法发现”（解决Erdős型组合问题），这属于特定领域的应用，但根据第四步关于“自我演化的应用”的规则，只要论文的核心贡献在于提出一种新的“自我演化”机制（即CCTS框架），而非仅仅将LLM作为工具应用，就应当保留。 5.  **排除项检查**: 论文不涉及安全对齐、多模态视觉或知识图谱等排除标准。 综上所述，该论文提出了一种新的LLM自我演化/优化框架，符合“自我演化”的研究焦点。",
                    "summary2": "本文旨在解决LLM辅助算法发现中搜索效率低下的问题。针对LLM辅助的算法发现场景，我们提出了一种Contrastive Concept-Tree Search (CCTS)方法，该方法提取层次化概念表示并通过对比概念模型引导父代选择。我们在Erdős型组合数学问题benchmark及合成环境中，通过最佳分数验证了其有效性。",
                    "summary_translation": "大语言模型辅助的算法发现是一个针对程序进行迭代式黑盒优化的过程，旨在近似求解目标任务；在此过程中，LLM 提出候选程序，而外部评估器提供任务反馈。尽管近期关于该主题的研究十分热烈且取得了颇具前景的结果，但如何最大限度地利用 LLM 对可能程序空间的内部表示来提升性能，仍是一个悬而未决的问题。本文介绍了对比概念树搜索，该方法从生成的程序中提取层次化概念表示，并学习一个对比概念模型来指导父代选择。通过利用高性能与低性能解决方案之间的似然比得分对父代进行重新加权，CCTS 能够使搜索偏向有用的概念组合，同时避开误导性的概念；它通过显式的概念层次结构提供指导，而非依赖由 LLM 构建的算法谱系。我们表明，在开放式 Erdős 型组合问题的基准测试中，CCTS 不仅提高了相较于基于适应度的基线方法的搜索效率，还生成了可解释的、特定于任务的概念树。我们的分析表明，性能的提升主要归因于对应当规避哪些概念的学习。我们还在一个受控的合成算法发现环境中进一步验证了这些发现，该环境定性地复现了使用 LLM 时观察到的搜索动态。",
                    "inspiration_trace": "基于论文内容，以下是对作者产出《Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery》这一核心方法的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中通过以下逻辑链条构建了研究背景，逐步引出核心问题：\n\n1.  **现状与机遇**：\n    大语言模型（LLM）的出现开启了一类新的算法发现系统。在这个系统中，程序合成被嵌入到一个迭代的黑盒优化循环中：LLM 提出候选程序，外部评估器给出反馈，程序被不断精炼。这种范式已经在组合数学等领域取得了显著成果。\n\n2.  **现有框架的局限**：\n    尽管现有方法（如 FunSearch, AlphaEvolve）有效，但它们大多基于“适应度驱动的进化更新”。在这种框架下，父代的选择仅依赖于任务得分，搜索过程本质上是在程序空间中按照性能排序进行的“游走”。\n\n3.  **核心痛点**：\n    程序空间本身是**弱结构化**的。即使受到 LLM 的约束，生成的算法也不具备清晰的局部性、平滑变化或组合结构。这意味着搜索只能依赖通用的黑盒启发式算法，指导信号主要来自原始的适应度值和 LLM 的隐式先验，而**缺乏对算法中哪些语义组件真正有用的显式理解**。\n\n4.  **对新问题的需求**：\n    对于那些先验知识不足的真正新颖问题，仅仅知道“哪个程序好”是不够的，我们需要知道“为什么好”。因此，当前的方法并没有最大限度地利用 LLM 内部对程序空间的表示结构，这限制了其扩展性和可靠性。\n\n---\n\n### 二、 研究问题\n\n基于上述背景，作者明确提出的研究问题是：\n\n**“如何最大限度地利用 LLM 对可能程序空间的内部表示来提高算法发现的性能？”**\n\n---\n\n### 三、 核心方法的逻辑演进（思想推演）\n\n为了回答上述问题，作者的思考过程经历了从观察到假设，再到方法论的演进：\n\n#### 1. 观察与反思：从“代码空间”到“语义空间”的错位\n*   **思考起点**：现有的进化算法直接在“代码”层面进行变异和选择。但代码是离散且非结构化的，两个代码相似的程序可能在语义上截然不同，反之亦然。\n*   **洞察**：直接在代码空间搜索是低效的，因为这里没有“距离”或“方向”的概念。真正的算法发现应该发生在**语义概念**层面。\n\n#### 2. 提出假设：算法的层级概念结构\n*   **核心假设**：算法并非随机的代码片段，而是由底层的**语义概念**构成的。这些概念组织成一个层级结构（树状），其中节点代表概念，边代表细化或特化关系。\n*   **推论**：一个强大的算法是由“有用的概念”组合而成，同时避免了“有害的概念”。如果能让搜索在这个显式的概念树上进行，而不是在混乱的代码空间里，就能实现更结构化、数据驱动的优化。\n\n#### 3. 策略制定：显式化隐式结构\n*   **思路转变**：不再仅仅依赖适应度来选择父代，而是利用 LLM 提取程序的语义特征，构建一个动态的“概念树”。\n*   **关键操作**：将搜索的导航机制从“算法谱系”转移到“概念层级”上。\n\n#### 4. 机制设计：对比学习与效用估计\n*   **如何判断概念好坏？**：单纯看包含某个概念的程序得分是不够的（可能混杂了其他概念）。作者借鉴了 Tree-structured Parzen Estimator (TPE) 的思想，采用**对比**的方法。\n*   **具体逻辑**：\n    *   将档案中的程序分为“高性能组”和“低性能组”。\n    *   分别拟合这两组程序在概念空间中的分布。\n    *   计算似然比：如果一个概念在“高性能组”中出现的概率显著高于“低性能组”，则该概念效用高；反之则效用低（甚至是有害的）。\n\n#### 5. 方法落地：对比概念树搜索 (CCTS)\n*   **整合**：将上述逻辑整合进进化循环。\n    *   **特征提取**：LLM 生成代码后，再次被提示提取其包含的概念，更新概念树。\n    *   **父代重加权**：在下一轮选择父代进行变异时，不再只看分数，而是根据其包含概念的“对数似然比”进行加权采样。\n    *   **引导生成**：通过 Prompt 注入选定的概念，引导 LLM 生成包含特定语义组件的子代。\n\n#### 6. 验证与洞察：负向学习的重要性\n*   **实验观察**：通过合成环境和真实任务，作者发现 CCTS 的性能提升主要来自于**学会了避免哪些概念**。\n*   **逻辑闭环**：LLM 的预训练包含大量通用相关性，其中很多对特定任务是误导性的。CCTS 通过对比统计，识别并抑制了这些“虚假相关性”，从而稳定了搜索过程，减少了无效探索。\n\n---\n\n**总结**：作者从“代码空间搜索效率低”这一痛点出发，提出“算法具有层级概念结构”的假设，进而设计了一套基于对比学习的机制，将搜索重心从适应度分数转移到语义概念的效用评估上，最终实现了更高效、可解释的算法发现。"
                },
                {
                    "title": "CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability",
                    "arxiv_id": "2602.03012",
                    "authors": "Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che",
                    "summary": "Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\\% solution correctness and 96\\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\\% to 35.8\\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\\% to 31.3\\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”方向**： 论文明确提出了 **CVE-Factory**，并将其定义为 **\"the first multi-agent framework\"**（第一个多智能体框架）。这直接对应了筛选标准中的核心焦点之一——**多智能体**。论文的核心在于构建了一个新的框架，而非仅仅使用现有工具。 2.  **属于“构建/改进 LLM 智能体”的范畴**： 论文的研究目标是解决评估和改进代码智能体所需的高质量任务稀缺问题。它通过多智能体框架自动生成可执行的智能体任务，并利用这些合成的训练环境对模型（Qwen3-32B）进行微调，从而显著提升了智能体的性能。这符合“构建、改进或演化 LLM 智能体”的核心目标。 3.  **不属于“非演化型应用”或“安全与对齐”的排除项**： *   **关于应用**：虽然论文的应用领域是代码安全，但其主要贡献不是“利用智能体去修复漏洞”（这属于应用），而是“构建一个智能体框架来生成训练/测试数据”（这属于智能体方法论）。它解决了智能体演化过程中的数据瓶颈问题。 *   **关于安全排除**：筛选标准中排除的是主要贡献关于 `Safety` 或 `Security`（如防御攻击、对齐）的论文。本文虽然涉及安全漏洞，但重点在于**智能体框架的设计与任务生成**，安全仅是应用场景，而非方法论的核心贡献本身。 综上所述，该论文提出了一种新的多智能体框架来扩展智能体任务并提升模型能力，完全符合 Agentic AI 和 Multi-Agent Systems 的研究范围。",
                    "summary2": "本文旨在解决代码安全任务构建成本高且难以扩展的问题。针对稀疏的CVE元数据，我们提出了一种名为CVE-Factory的多智能体框架，通过解耦与耦合的六阶段流水线自动生成可执行任务。我们在PatchEval和LiveCVEBench上通过解决方案正确率、环境保真度及模型修复成功率验证了其有效性，实现了专家级质量（95%正确率）并显著提升了微调模型性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观背景与“讲故事”的逻辑（基于 Introduction）\n\n作者首先构建了一个从技术趋势到现实风险，再到数据瓶颈的叙事闭环：\n\n1.  **技术趋势与潜在风险**：AI驱动的软件开发正在普及，代码代理被赋予了高权限（如管理复杂环境、执行脚本）。随着AI生成代码量爆炸式增长，人类监督比例相对下降。\n2.  **核心矛盾**：如果代码代理缺乏足够的安全推理能力，其高自主性将导致漏洞以前所未有的速度传播，构成系统性风险。因此，**评估和提升代码代理的安全能力迫在眉睫**。\n3.  **理想方案**：最有效的提升路径是让代理在“真实的漏洞修复任务”中训练和评估。这不仅仅是静态代码片段，而是包含**可执行环境**的完整任务（代理需导航代码库、执行命令、根据反馈迭代）。\n4.  **现实困境**：\n    *   **数据源头稀疏**：现有的CVE列表仅包含元数据（描述、分类、链接），缺乏可执行环境。\n    *   **人工转化不可行**：现有工作依赖专家手动复现CVE，构建环境。但这极其昂贵且不可扩展（平均每个CVE需10+小时）。\n    *   **自动化工具局限**：现有的自动化任务生成框架要么局限于Python，要么依赖配置良好的仓库；针对CVE的多代理尝试（如CVE-Genie）生成的格式无法直接用于代理训练。\n5.  **结论**：为了解决上述矛盾，必须找到一种方法，能够**自动将稀疏的CVE元数据转化为高质量、可执行的代理任务**，且必须达到专家级质量并具备大规模扩展能力。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链，作者试图回答的核心研究问题是：\n\n**“如何设计一个自动化框架，将稀疏的CVE元数据转化为具备专家级质量、完全可执行的代码安全代理任务，并实现该过程的大规模扩展？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从观察到最终提出CVE-Factory框架，经历了以下四个阶段的思维演进：\n\n#### 1. 问题本质的洞察：从“任务”到“长程规划”\n*   **观察**：直接让现有的代码代理（如Claude Code）去复现一个CVE，往往会失败。\n*   **归因**：CVE复现不是一个简单的问答，而是一个**超长程任务**。它需要从零开始构建环境、编写测试、编写修复脚本，上下文长度往往超过200k tokens。\n*   **推论**：单一模型无法承受如此巨大的认知负荷，必须进行任务拆解。\n\n#### 2. 核心假设的提出：解耦与耦合\n*   **假设**：如果将这个巨大的任务拆解为独立的、由专门代理处理的子任务，就能降低单阶段难度；但拆解会破坏整体一致性，因此需要后续的“耦合”阶段来对齐。\n*   **方法论雏形**：\n    *   **解耦阶段**：将任务拆解为三个独立的生成环节——信息收集、文件生成（测试/修复）、环境构建。每个代理只关注自己的领域，互不干扰，避免上下文污染。\n    *   **耦合阶段**：拆解后的组件必须能协同工作。因此引入三个渐进的验证阶段——漏洞验证（环境是否触发漏洞）、修复验证（修复是否生效）、整体验证。\n\n#### 3. 质量控制的机制设计：客观验证与反馈闭环\n*   **挑战**：如何保证自动化生成的质量能达到“专家级”？如何防止代理“作弊”（例如Mock环境或静态匹配测试）？\n*   **设计思路**：\n    *   **客观脚本验证**：不依赖代理的主观判断，而是编写静态脚本（如`check_env_ready`）来执行测试，只有脚本通过才算成功。\n    *   **信息不对称**：在构建环境时，故意不让构建者看到测试用例，防止其针对测试“硬编码”环境。\n    *   **中央编排器**：引入一个非智能的“管理者”，负责状态流转。如果验证失败，编排器通过反馈机制将问题路由回原始创建者进行修正，而不是让后续代理强行修补。\n\n#### 4. 价值验证与扩展：从“可用”到“规模化”\n*   **验证逻辑**：为了证明方法有效，作者设计了“交叉验证”实验——用CVE-Factory复现专家构建的任务，看能否达到95%以上的通过率。\n*   **扩展逻辑**：一旦自动化流水线跑通，其边际成本极低。这使得两个下游应用成为可能：\n    *   **构建动态基准**：利用自动化能力实时抓取最新CVE，构建LiveCVEBench，解决数据分布过时的问题。\n    *   **数据规模化训练**：生成1000+个可执行环境，用于微调模型（如Qwen3），证明高质量数据能显著提升模型的安全能力。\n\n---\n\n### 总结\n\n作者的思考路径遵循了典型的**“发现痛点 -> 归纳本质 -> 提出架构 -> 严格验证 -> 释放价值”**的学术创新逻辑：\n1.  发现**人工复现CVE不可扩展**这一痛点；\n2.  归纳出**长程任务需拆解**的本质；\n3.  提出**“解耦生成+耦合验证”**的多代理架构；\n4.  通过**与专家交叉验证**确立质量标准；\n5.  最终实现**安全任务数据的规模化生成**，推动了代码代理安全能力的边界。"
                },
                {
                    "title": "Scaling Small Agents Through Strategy Auctions",
                    "arxiv_id": "2602.02751",
                    "authors": "Lisa Alazraki, William F. Shen, Yoram Bachrach, Akhil Mathur",
                    "summary": "Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于核心关注的 **Agentic AI** 领域，具体涉及 **多智能体** 和 **自我演化** 两个方向。 1.  **核心判断**: *   论文的核心贡献是提出了 **SALE (Strategy Auctions for Workload Efficiency)**，这是一个受自由职业者市场启发的 **智能体框架**。 *   它不是将现有智能体简单应用于某个垂直领域（如医疗或法律），而是专注于改进智能体系统的**架构**和**协调机制**，旨在解决小模型智能体在复杂任务中的扩展性问题。 2.  **符合核心关注点**: *   **多智能体**: 论文构建了一个包含异构智能体（不同大小的模型）的系统，通过“策略拍卖”机制实现智能体间的协作、任务路由和协调。这直接对应了您关注的多智能体协作与社会组织形式。 *   **自我演化**: 摘要中明确提到了 \"continual self-improvement\"（持续自我改进）和 \"test-time self-improvement\"（测试时自我改进）。智能体通过共享拍卖记忆来精炼计划，体现了通过经验和反馈进行自我完善和迭代的演化机制。 *   **智能体能力**: 论文涉及智能体的规划能力（生成简短的战略计划）和记忆机制（共享拍卖记忆）。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然论文在深度搜索和编码任务上进行了实验，但这只是为了验证框架的有效性，其核心贡献在于SALE这一系统级的方法论，而非特定领域的应用。 综上所述，该论文提出了一种新的多智能体协调与自我演化框架，属于构建和改进LLM智能体的前沿研究，符合筛选条件。",
                    "summary2": "本文旨在解决小模型智能体在复杂任务上性能不足及如何高效路由任务的问题。针对深度搜索和编码任务，我们提出了一种名为SALE的策略拍卖框架，该框架通过智能体竞标短期策略计划、基于成本-价值机制评分及共享拍卖记忆进行自我改进，实现了任务的自适应分配。在HST-Bench数据集上，通过Pass@1和每百万Token价格验证，SALE在降低对最大模型依赖和总成本的同时，显著提升了整体性能。",
                    "summary_translation": "小型语言模型正日益被视为实现 agentic AI（智能体人工智能）的一种具有前景且成本效益高的方法，支持者声称它们具备足以胜任 agentic workflows（智能体工作流）的能力。然而，尽管较小的 agents（智能体）在简单任务上能紧密匹配较大的 agents，但目前尚不清楚其性能如何随 task complexity（任务复杂度）扩展，何时必须使用大型模型，以及如何更好地利用小型 agents 处理 long-horizon workloads（长时程工作负载）。在这项工作中，我们通过实证研究表明，在 deep search（深度搜索）和 coding tasks（编码任务）中，小型 agents 的性能无法随 task complexity 的增加而扩展，并介绍了 Strategy Auctions for Workload Efficiency (SALE)，这是一个受 freelancer marketplaces（自由职业者市场）启发的 agent 框架。在 SALE 中，agents 通过简短的 strategic plans（策略计划）进行竞标，这些计划由 systematic cost-value mechanism（系统性成本-价值机制）评分，并通过 shared auction memory（共享拍卖记忆）进行优化，从而实现 per-task routing（按任务路由）和 continual self-improvement（持续自我改进），而无需训练单独的 router（路由器）或将所有模型运行至完成。在各种复杂度的 deep search 和 coding tasks 中，SALE 将对最大 agent 的依赖减少了 53%，降低了 35% 的 overall cost（总成本），并且仅以执行 final trace（最终轨迹）之外的微不足道的开销，始终优于最大 agent 的 pass@1（首次通过率）。相比之下，依赖 task descriptions（任务描述）的 established routers（既定路由器）要么表现不如最大 agent，要么无法降低成本——通常两者兼而有之——这突显了它们不适合 agentic workflows。这些结果表明，虽然小型 agents 可能不足以应对 complex workloads（复杂工作负载），但它们可以通过 coordinated task allocation（协调的任务分配）和 test-time self-improvement（测试时自我改进）被有效地“放大”。更广泛地说，这些结果激发了对 agentic AI 的 systems-level view（系统级视角），即性能提升较少来自于不断增大的单个模型，而更多来自于受市场启发的 coordination mechanisms（协调机制），这些机制将 heterogeneous agents（异构智能体）组织成高效、自适应的 ecosystems（生态系统）。",
                    "inspiration_trace": "基于对论文《Scaling Small Agents Through Strategy Auctions》的深入分析，以下是作者产出该核心方法的逻辑推演过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从乐观到现实\n\n作者在Introduction中构建了一个层层递进的叙事逻辑，旨在揭示当前研究盲点并引出核心动机：\n\n1.  **背景与乐观预期**：\n    *   **现状**：小型语言模型因其成本低廉，被视为实现智能体AI的有前景的替代方案。\n    *   **观点**：现有观点认为，通过工具增强，小模型足以处理复杂的多步骤行为，甚至可以取代大模型。\n\n2.  **提出疑虑与变量引入**：\n    *   **转折**：作者指出，目前的讨论多集中在“模型大小”与“智能体能力”的关系，却忽略了**“任务复杂性”**这一关键调节变量。\n    *   **现实挑战**：实际工作负载跨度极大，从简单的短任务到需要长期推理、信息整合的开放性长任务。小模型在简单任务上表现尚可，但在高复杂度任务上的表现存疑。\n\n3.  **揭示现有方案的局限性**：\n    *   **非预测性路由**：运行所有候选模型并选择结果，这在单轮问答中可行，但在动辄消耗数百万Token的智能体轨迹中成本过高，不可行。\n    *   **预测性路由**：训练单独的路由模型，不仅训练成本高、泛化性差，且随着任务难度增加性能会下降。\n\n4.  **核心动机的升华**：\n    *   **目标**：我们需要一种机制，既能利用小模型处理简单任务以节省成本，又能在大模型处理复杂任务时保证性能，且不引入高昂的额外训练或推理开销。\n\n---\n\n### 二、 提炼出的“研究问题”\n\n基于上述逻辑，作者旨在解决的核心问题可总结为：\n\n**“面对任务复杂度的差异，如何设计一种轻量级的路由机制，能够在不训练额外路由器且不运行全量轨迹的前提下，动态地将任务分配给异构智能体，以实现性能与成本的最佳平衡？”**\n\n---\n\n### 三、 核心方法产出的逻辑推演链\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 观察与假设：小模型的“能力天花板”\n*   **观察**：小模型在简单任务上能媲美大模型，但在复杂任务上表现断崖式下跌。\n*   **推论**：单纯依赖小模型无法解决复杂问题，而始终使用大模型则是资源浪费。**“模型大小”不应是一个全局选择，而应是一个针对具体任务的决策。**\n\n#### 2. 痛点分析：传统路由的失效\n*   **困境**：现有的路由方法要么太慢（跑完所有模型再看结果），要么太笨（训练的路由器难以理解长周期的智能体行为）。\n*   **需求**：我们需要一种**“部分预测”**的机制——即在真正执行昂贵的长轨迹之前，就能以极低的成本预测出哪个模型最合适。\n\n#### 3. 关键洞察：计划质量 $\\approx$ 执行质量\n*   **灵感来源**：智能体工作流通常包含一个“规划阶段”，即在行动前先列出步骤。\n*   **假设**：虽然执行一个完整的智能体轨迹（如写代码、多轮搜索）非常昂贵，但生成一个**“战略计划”**非常便宜。\n*   **核心逻辑**：如果一个模型能生成一个好的计划，它往往也能执行好这个任务。**“计划”是预测“执行”的可靠且廉价的代理信号。**\n\n#### 4. 隐喻迁移：从“路由器”到“自由市场”\n*   **思维转换**：不再将任务分配视为一个静态的分类问题（路由器），而是将其视为一个**经济问题**。\n*   **隐喻**：将智能体视为“自由职业者”，将任务视为“项目”。\n*   **机制设计**：\n    *   **竞标**：所有智能体（大小模型）针对任务提交一个“战略计划”作为投标。\n    *   **评审**：由一个“陪审团”根据计划的**成本**（预计消耗的Token）和**价值**（计划的熵、同行评审打分）进行评分。\n    *   **中标**：选择性价比（Value - Cost）最高的智能体。\n\n#### 5. 系统进化：从静态分配到动态进化\n*   **进一步思考**：仅仅路由是不够的，如何让小模型“变大”？\n*   **引入记忆**：利用拍卖的历史记录。如果一个小模型输给了大模型，它可以去查看大模型赢在哪儿（对比学习）。\n*   **自我提升**：在下一轮拍卖中，小模型可以基于记忆库中的成功案例优化自己的计划。这使得系统不仅是在分配任务，更是在**通过市场反馈机制持续提升小模型的有效能力**。\n\n#### 6. 最终方法论：SALE (Strategy Auctions)\n*   **逻辑闭环**：\n    *   **输入**：任务 $t$。\n    *   **竞价**：各模型生成计划 $s$。\n    *   **评估**：计算 $Cost$（长度 $\\times$ 价格）和 $Value$（熵 + 陪审团打分）。\n    *   **优化**：最小化 $Cost - Value$。\n    *   **迭代**：利用拍卖记忆让落选的廉价模型修正计划，争取在下一轮中标。\n\n---\n\n**总结**：作者的思考路径是从**“小模型的能力边界”**出发，通过**“计划作为代理信号”**这一关键洞察，避开了传统路由的高昂成本，最终引入**“市场拍卖与记忆反馈”**机制，构建了一个既能省钱又能自我进化的智能体生态系统。"
                },
                {
                    "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling",
                    "arxiv_id": "2602.02636",
                    "authors": "Ziyang Huang, Haolin Ren, Xiaowei Yuan, Jiawei Wang, Zhongtao Jiang, Kun Xu, Shizhu He, Jun Zhao, Kang Liu",
                    "summary": "Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **多智能体** 和 **Agentic AI** 的核心贡献。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **WideSeek**，这是一个**动态分层多智能体架构**，以及一个统一的训练框架。 *   这不仅仅是将现有LLM应用到一个领域，而是提出了一种新的**多智能体系统方法论**，涉及智能体的架构设计（自主分叉并行子智能体）和优化（端到端强化学习）。 *   因此，它符合“构建、改进 LLM智能体”的保留标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `LLM-based Agents`。 *   **多智能体**: 论文重点研究了智能体间的协作（并行子智能体）和扩展，符合 `Collaboration` 和 `Agent Society` 的方向。 *   **演化机制**: 论文提出了一个统一的训练框架，利用端到端强化学习（RL）来优化系统，这属于智能体的 `Self-Improvement` 和 `Iterative Improvement` 范畴。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文的应用场景是“广度研究”，但其核心在于解决多智能体如何并行协作和优化的问题，而非单纯的应用。其提出的“动态分层架构”和“多智能体RL优化”是对Agentic AI框架的直接改进。 综上所述，该论文在多智能体架构设计和优化方面做出了实质性贡献，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在推动从Deep Research向Wide Research范式的转变，解决在复杂约束下并行检索与综合全面信息的问题。针对缺乏专用基准和优化方法的现状，我们提出了WideSeek，一种动态分层多智能体架构，能够自主分叉并行子智能体，并通过端到端强化学习进行统一优化。我们在自建的WideSeekBench数据集上，通过Success Rate、Row F1和Item F1等指标验证了其有效性，证明了多智能体扩展在提升Wide Research能力方面的显著优势。",
                    "summary_translation": "搜索智能正从 Deep Research（深度研究）向 Wide Research（广度研究）演进，后者是一种在复杂约束条件下并行检索与综合全面信息的关键范式。然而，由于缺乏针对搜索广度的专用基准和优化方法，该领域的发展受到了阻碍。为应对这些挑战，我们从 Data Pipeline（数据管道）和 Agent Optimization（智能体优化）两个视角对 Wide Research（广度研究）进行了深入探索。首先，我们构建了 WideSeekBench，这是一个通过严格的多阶段 Data Pipeline（数据管道）构建的 General Broad Information Seeking (GBIS)（通用广度信息搜寻）基准，旨在确保目标信息量、逻辑约束和领域层面的多样性。其次，我们提出了 WideSeek，这是一种动态分层 Multi-Agent Architecture（多智能体架构），能够根据任务需求自主派生并行的 Sub-Agents（子智能体）。此外，我们设计了一个统一的 Training Framework（训练框架），将 Multi-Agent Trajectories（多智能体轨迹）线性化，并利用 End-to-End RL（端到端强化学习）对系统进行优化。实验结果验证了 WideSeek 和 Multi-Agent RL（多智能体强化学习）的有效性，并表明扩展智能体数量是推动 Wide Research（广度研究）范式发展的一个极具前景的方向。",
                    "inspiration_trace": "基于对论文《WideSeek: Advancing Wide Research via Multi-Agent Scaling》的深入分析，以下是作者产出该文章的完整逻辑链推演：\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n1.  **范式演进的观察**：搜索智能正在从“深度研究”向“广度研究”演进。前者侧重于通过复杂的多步推理寻找单一难找的信息，而后者侧重于在复杂约束下并行检索和综合全面的信息。\n2.  **现实需求的驱动**：随着AI进入“下半场”，实际应用场景（如代码生成、企业数据智能）要求从串行推理转向并行编排，以提升生产力和工业级部署的规模效应。\n3.  **核心冲突的揭示**：尽管“广度研究”前景广阔，但其发展受到三大阻碍：\n    *   **基准缺失**：现有基准多由人工构建，规模小、多样性不足，且缺乏训练数据。\n    *   **合成缺陷**：当前的数据合成方法侧重于模拟多步推理路径（优化深度），缺乏在复杂约束下高效合成大规模原子信息的能力（优化宽度）。\n    *   **优化空白**：现有方法多依赖静态多智能体框架或闭源模型，缺乏对能够自主拓宽搜索路径的端到端系统优化的探索。\n\n### 二、 研究问题\n\n**如何通过构建专用基准、设计广度导向的数据合成管线以及开发端到端优化的动态多智能体架构，来有效推动“广度研究”范式的发展？**\n\n---\n\n### 三、 思想演进脉络（逻辑链推演）\n\n#### 1. 宏观观察与范式定义\n*   **观察**：现有的搜索智能体研究大多集中在“深度”，即如何通过长链路推理找到“那个”答案。然而，现实世界的高价值任务（如生成竞品分析表）往往需要找到“所有”符合条件的答案，这要求极高的召回率和并行处理能力。\n*   **定义**：作者将这种需求定义为“广度研究”，其核心特征是**并行编排**和**结构化综合**，而非串行推理。\n\n#### 2. 瓶颈识别与假设提出\n*   **瓶颈一：数据与评估的错位**\n    *   *思考*：要训练和评估“广度”能力，首先得有“广度”的数据。现有的QA数据集或Web导航数据集都是单点或线性路径的，无法衡量模型在复杂逻辑约束（如“A且非B”）下收集大规模信息的能力。\n    *   *假设*：如果能利用知识图谱（KG）中结构化的实体关系，通过集合运算（交、并、差）来构建复杂的逻辑约束，就能自动生成包含大规模实体集和属性表的“广度”任务。\n*   **瓶颈二：架构的刚性**\n    *   *思考*：传统的单智能体或静态多智能体（如固定角色的Planner-Executor）在面对海量信息检索时，要么上下文不足，要么无法灵活扩展。广度研究需要根据任务量动态调整计算资源。\n    *   *假设*：需要一个动态的分层架构，主智能体不仅能规划，还能根据任务复杂度自主“分叉”出任意数量的子智能体进行并行检索。\n*   **瓶颈三：优化的局限**\n    *   *思考*：仅仅依靠Prompt Engineering或SFT（监督微调）很难让模型学会“何时该分叉”、“该分叉多少个”。这需要一种试错和反馈机制来优化搜索策略本身。\n    *   *假设*：可以将动态多智能体的交互过程线性化为一个统一的轨迹，利用强化学习（RL）进行端到端优化，让模型学会最大化搜索广度的策略。\n\n#### 3. 方法论的形成\n*   **数据层**：基于上述假设，设计了**WideSeekBench**。利用KG提取实体簇，通过逻辑组合生成复杂约束，并自动构建Ground Truth表格。这解决了“无米之炊”的问题。\n*   **架构层**：提出了**WideSeek**系统。这是一个动态分层多智能体架构，主智能体拥有完全的自主权，可以随时调用`create_sub_agent`工具来生成并行的执行者，实现了从串行到动态并行的架构转变。\n*   **优化层**：设计了**统一的多智能体RL框架**。将主智能体和子智能体的所有轨迹拼接成一个序列，使用GRPO进行优化。这使得模型不仅能学会搜索内容，还能学会如何通过增加智能体数量来提升最终收益。\n\n#### 4. 逻辑闭环\n*   **验证**：通过实验发现，经过RL优化的模型，其子智能体数量和工具调用次数显著增加，且随着目标信息量的增加，模型能自主扩展搜索规模。这证明了“智能体扩展”是提升广度研究能力的有效方向，从而完成了从观察到假设再到验证的完整逻辑闭环。"
                },
                {
                    "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently",
                    "arxiv_id": "2602.02619",
                    "authors": "Mohan Jiang, Dayuan Fu, Junhao Shi, Ji Zeng, Weiye Si, Keyu Li, Xuefeng Li, Yang Xiao, Wenjie Li, Dequan Wang, Pengfei Liu",
                    "summary": "While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心的 Agentic AI 研究范畴。 1.  **核心贡献符合 (第一步)**: 论文的核心贡献是提出了 `daVinci-Agency`，一种新的数据合成框架，旨在解决长视距智能体工作流中的训练数据稀缺问题。这直接对应了“构建、改进或演化 LLM智能体”的目标，特别是针对智能体在长期任务中的表现进行改进。 2.  **符合核心关注点 (第二步)**: *   **Agentic AI**: 论文明确关注 \"long-horizon agentic workflows\"（长视距智能体工作流），涉及 \"tool calls\"（工具调用）、\"task decomposition\"（任务分解）和 \"goal-directed behavior\"（目标导向行为）。 *   **自我演化**: 摘要中多次提到 \"evolutionary dynamics\"（演化动态）、\"iterative refinements\"（迭代改进）和 \"bug-fix histories\"（Bug修复历史）。论文的核心机制是利用软件演化中的 Pull Request 序列来教导智能体如何进行自我修正和迭代，这完全符合“自我演化”中关于通过经验或反馈进行自我完善和迭代的定义。 3.  **通过排除标准检查 (第三步)**: 论文不涉及安全对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **特殊情况处理 (第四步)**: 虽然论文使用了软件工程（PR）作为数据来源，但这并非“非演化型应用”。其目的不是解决软件问题，而是利用软件演化的逻辑来构建一种通用的智能体训练机制（即“自我演化的应用”中的例外情况），以提升智能体在长视距任务中的规划和执行能力。 综上所述，该论文提出了一种通过挖掘演化动态来增强 LLM 智能体长视速能力的新框架，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决长时程智能体训练中缺乏真实长依赖结构和跨阶段演化动态数据的问题。针对真实软件演化中的Pull Request序列，我们提出了一种名为daVinci-Agency的数据合成范式，通过挖掘PR链中的任务分解、长期一致性和可验证修正信号构建监督。我们在SWE-bench、Toolathlon等多个基准上通过相对性能增益验证了其有效性，仅用239个样本即实现了显著优于大规模基线模型的性能提升。",
                    "summary_translation": "虽然大语言模型 (LLMs) 在短期任务上表现出色，但将其扩展至长期智能体工作流 仍面临挑战。核心瓶颈在于缺乏能够捕捉真实长依赖结构 和跨阶段演化动态 的训练数据——现有的合成方法要么局限于受模型分布约束的单一特征场景，要么产生高昂的人工标注成本，无法提供可扩展的高质量监督信号。我们通过从现实世界软件演化 的视角重新审视数据合成来解决这一问题。我们的核心洞察是：Pull Request (PR) 序列自然地蕴含了长期学习的监督信号。它们将复杂目标分解为可验证的提交单元，在迭代过程中保持功能一致性，并通过错误修复历史 编码真实的精炼模式。基于此，我们提出了 daVinci-Agency，它通过三个相互关联的机制，从 PR 链 中系统地挖掘结构化监督：(1) 通过持续提交 进行渐进式任务分解，(2) 通过统一功能目标保障长期一致性，以及 (3) 从真实的错误修复轨迹 中进行可验证的精炼。与独立处理每一步的合成轨迹不同，daVinci-Agency 的基于 PR 的结构本质上保留了因果依赖 和迭代精炼，这对于教授持久的目标导向行为 至关重要，并实现了与项目级、全周期任务建模 的自然对齐。生成的轨迹规模宏大——平均包含 85k tokens 和 116 次工具调用——但数据效率却极高：在 239 个 daVinci-Agency 样本上微调 GLM-4.6 即可在各项基准测试 中带来广泛改进，特别是在 Toolathlon 上实现了 47% 的相对提升。除了基准测试性能外，我们的分析还证实了……",
                    "inspiration_trace": "基于对论文《daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently》的深入分析，以下是作者产出该文章的系统性思考过程推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的逻辑链条，旨在揭示当前领域面临的根本性矛盾：\n\n1.  **范式转移与现状**：首先确认大语言模型（LLM）在短期任务（如代码生成、工具调用）上已表现出色，研究焦点正不可避免地向更具挑战性的**长视界智能体任务**转移。\n2.  **定义核心挑战**：指出长视界任务不仅仅是“步骤更多”，其核心难点在于**持续的方向感**和**累积误差的缓解**。随着任务跨度拉长，任务分解、长期一致性和迭代修正等“程序性行为”变得至关重要。\n3.  **揭示数据瓶颈**：强调现有的单特征任务无法提供上述关键技能所需的训练信号。模型需要的是能够体现跨阶段依赖和演化模式的显式监督信号。\n4.  **批判现有方案**：\n    *   **合成数据（蒸馏/强化学习）**：虽然可扩展，但受限于生成模型的分布，往往局限于单特征开发，缺乏真实的失败模式和修正路径。\n    *   **人工标注**：虽然保真度高，但成本极其昂贵，无法规模化。\n5.  **引入观察与契机**：指出真实的软件开发过程（特别是 GitHub 的 Pull Request 序列）天然包含了长视界交互所需的状态演化和外部验证信号。多个 PR 往往围绕同一目标，通过迭代推进交付，且后续 PR 常包含对前序缺陷的修复。\n6.  **提出解决思路**：基于上述观察，提出利用真实的 PR 演化链来构建可验证的长视界训练数据，从而解决数据稀缺和质量问题。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“如何以一种可扩展的方式，构建出既包含真实长依赖结构又具备跨阶段演化动态的高质量长视界智能体训练数据？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进链\n\n以下是从宏观观察到具体方法论的思维推演过程：\n\n#### 1. 宏观观察：从“单点能力”到“持续演化”\n*   **观察**：现有的智能体研究大多集中在解决孤立的、短期的任务（如写一个函数、修复一个独立的 Bug）。\n*   **思考**：现实世界的工程问题往往是复杂的、长期的。一个功能的实现往往需要经历多次提交、多次评审、多次修复。\n*   **初步判断**：模型缺乏的不是“写代码”的能力，而是“像人类工程师一样持续维护和演化项目”的能力。\n\n#### 2. 问题聚焦：数据分布的“断层”\n*   **深入分析**：为什么模型做不好长视界任务？因为训练数据里没有“跨阶段”的监督。\n    *   现有的合成数据是“扁平”的，每一步之间缺乏深层的因果依赖。\n    *   人工数据虽然好，但太贵，无法覆盖长视界任务所需的海量交互。\n*   **核心矛盾**：我们需要**高质量、有演化逻辑**的数据，但现有的生产手段要么**质量低**（合成），要么**规模小**（人工）。\n\n#### 3. 关键洞察：向现实世界寻找“教科书”\n*   **灵感来源**：既然人工造不出来，不如直接看人类是怎么做长视界开发的。\n*   **锁定对象**：GitHub 上的 **Pull Request (PR)** 历史。\n*   **逻辑映射**：\n    *   一个复杂的 Issue 往往对应一系列 PR。\n    *   PR #1 实现基础功能 -> **任务分解**。\n    *   PR #2 修复 PR #1 引入的 Bug -> **迭代修正**。\n    *   PR #3 优化性能或适配新环境 -> **长期一致性**。\n*   **假设**：如果将这些有依赖关系的 PR 串联起来，就天然构成了完美的长视界训练轨迹。这不仅是数据，更是包含了“元技能”的教科书。\n\n#### 4. 方法论构建：从“原始数据”到“结构化课程”\n*   **构思**：不能直接把 PR 扔给模型，需要将其转化为智能体能理解的“交互轨迹”。\n*   **设计机制**：\n    *   **链式构建**：通过元数据（引用关系、评论）将孤立的 PR 串联成 `PR Chain`，确立依赖拓扑。\n    *   **状态演化模拟**：强制智能体在上一阶段修改后的代码状态基础上工作，模拟真实的代码库演化。\n    *   **质量过滤**：利用强模型（如 GLM-4.6）作为裁判，通过拒绝采样确保生成的轨迹与真实 PR 的语义高度一致。\n\n#### 5. 验证与升华：数据效率的证明\n*   **预期**：这种基于真实演化逻辑的数据，应该比单纯堆量的合成数据更有效。\n*   **实验设计**：用极少量的样本（如 239 个）进行微调，对比数万样本的合成数据集。\n*   **结论验证**：如果假设成立，模型应该学会“规划”和“纠错”，而不仅仅是“补全代码”。实验结果证实了这一点，揭示了“长视界数据结构”比“数据量”更关键。\n\n**总结**：作者的思考路径是从**任务复杂度的升级**出发，敏锐地捕捉到**现有数据范式在演化逻辑上的缺失**，进而通过**复现真实软件工程的 PR 演化过程**，创造性地解决了长视界智能体训练中“质”与“量”难以兼得的难题。"
                },
                {
                    "title": "ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization",
                    "arxiv_id": "2602.02597",
                    "authors": "Hongyuan Su, Yu Zheng, Yong Li",
                    "summary": "Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”与“自我演化”交叉方向的论文。 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **ContextEvolve**，这是一个明确的 **多智能体框架**。它不仅仅是在应用LLM，而是构建了一个包含三个不同角色的智能体系统：Summarizer Agent（语义状态压缩）、Navigator Agent（提炼优化方向）和 Sampler Agent（管理经验分布）。 *   论文涉及 **自我演化** 机制。它旨在解决现有“无训练演化方法”的局限性，通过智能体间的协作模拟强化学习（RL）的过程（状态表示、策略梯度、经验回放），在文本潜在空间中进行迭代优化。这属于智能体通过环境反馈和经验进行自我完善和迭代的范畴。 2.  **非简单应用排除 (第一步 & 第四步)**: *   虽然论文的应用场景是“系统代码优化”，但这属于 **自我演化的应用** 这一例外情况。论文的重点不在于“用LLM写代码”，而在于提出了一种新的“多智能体协作”和“演化搜索”机制来实现高效的优化。它改进了演化算法的上下文利用率和搜索方向，属于方法论的创新，而非单纯的垂直领域应用。 3.  **符合正面指标**: *   包含核心范式：`Multi-Agent Systems (MAS)`, `Self-Evolving`。 *   包含多智能体特征：`Collaboration`（智能体编排）。 *   包含演化机制：`Iterative Improvement`, `Generational Evolution`（通过模拟RL的演化过程）。 综上所述，该论文构建了新的多智能体框架来解决演化优化问题，深度契合 Agentic AI 和 Self-Evolving 的研究焦点。",
                    "summary2": "本文旨在解决API-only约束下系统代码优化中上下文利用效率低的问题。针对ADRS benchmark，我们提出了一种ContextEvolve多智能体框架，通过Summarizer、Navigator和Sampler三个智能体分别管理语义状态、优化方向和经验分布，实现了与RL的功能同构。实验结果表明，该方法在ADRS基准上性能优于SOTA方法33.3%，同时减少了29.0%的token消耗。",
                    "summary_translation": "大语言模型正在变革系统研究，其通过自动化发现计算机系统的性能关键算法来实现。尽管大语言模型生成的代码看似合理，但要产出满足系统严格正确性和性能要求的解决方案，仍需进行迭代优化。测试时强化学习提供了较高的搜索效率，但在仅通过API访问的情况下，其所需的参数更新是不可行的；而现有的免训练进化方法则存在上下文利用效率低和搜索方向盲目的问题。我们提出了ContextEvolve，这是一个多智能体框架，通过将优化上下文分解为三个正交维度，在严格的参数盲约束下实现了强化学习级别的搜索效率：Summarizer Agent（总结智能体）通过代码到语言的抽象来压缩语义状态，Navigator Agent（导航智能体）从轨迹分析中提炼优化方向，Sampler Agent（采样智能体）通过优先示例检索来策划经验分布。这种编排机制与强化学习形成了功能同构——分别映射到状态表示、策略梯度和经验回放——从而能够在文本潜在空间中进行有原则的优化。在ADRS基准测试中，ContextEvolve的性能优于最先进的基线模型33.3%，同时将令牌消耗减少了29.0%。我们工作的代码已发布于 https://anonymous.4open.science/r/ContextEvolve-ACC。",
                    "inspiration_trace": "基于您提供的论文内容，我为您还原了作者产出这篇《ContextEvolve》的完整思考逻辑链。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示当前领域面临的核心矛盾：\n\n1.  **宏观背景（范式转移）：** 系统研究正从依赖人类专家设计算法，转变为利用大语言模型（LLM）进行自动化设计（AI-Driven Research for Systems, ADRS）。\n2.  **现实挑战（高门槛）：** 尽管LLM能生成看似合理的代码，但系统领域对正确性和性能的要求极其严苛。这意味着“一次性生成”是不够的，必须进行**严格的迭代优化**。\n3.  **现有方案的困境（两难选择）：**\n    *   **方案A（测试时强化学习 RL）：** 搜索效率高，符合优化逻辑。但致命缺陷是需要更新模型参数。在API-only（仅接口访问）的现实约束下，这既昂贵又不可行。\n    *   **方案B（无训练进化方法）：** 避开了参数更新问题，但存在**搜索效率低**的通病。它们缺乏对长历史上下文的有效压缩机制，也无法从嘈杂的历史中提取精确的优化信号，导致搜索盲目且低效。\n4.  **核心矛盾：** 我们需要RL级别的搜索效率，但必须在“参数盲”的严格约束下工作。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“在无法访问或更新模型参数（API-only）的严格约束下，如何构建一个具备强化学习级别搜索效率的系统代码优化框架？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n为了回答上述问题，作者的思维经历了从现象观察到理论映射，再到架构设计的演进过程：\n\n#### 第一阶段：诊断与洞察（为什么现有方法不行？）\n*   **观察：** 现有的无训练进化方法（如AlphaEvolve）之所以效率低，是因为它们把“上下文”仅仅当作“历史日志”来堆砌。\n*   **诊断：** 随着迭代进行，上下文窗口迅速被冗长的原始代码填满，导致信息密度极低。模型无法在有限的Token中看清“当前状态”和“优化方向”。\n*   **假设：** 如果不能更新神经网络的“权重”，那么我们就必须更新输入给网络的“上下文”。**上下文本身就是一种可优化的“参数空间”。**\n\n#### 第二阶段：理论对标（寻找最优解的蓝图）\n*   **思考：** 谁最擅长在复杂空间中进行高效搜索？答案是强化学习（RL）。\n*   **映射：** 既然不能在“参数空间”做RL，能否在“文本空间”模拟RL的核心组件？\n*   **核心洞察（功能同构）：** 作者意识到，一个高效的优化系统，无论底层是神经网络还是文本提示词，都必须包含三个正交的功能维度：\n    1.  **状态表征：** 压缩当前环境信息。\n    2.  **策略梯度：** 指引下一步往哪里走。\n    3.  **经验回放：** 从历史中提取最有价值的样本。\n\n#### 第三阶段：架构构建（将理论具象化为智能体）\n*   **设计目标：** 将上述三个RL维度对应到具体的LLM智能体上，实现对上下文的**结构化压缩**。\n*   **智能体分工：**\n    *   **Summarizer Agent（状态压缩）：** 对应RL的“Encoder”。它不存储原始代码，而是将代码转化为高密度的自然语言摘要，保留语义状态，剔除冗余语法。\n    *   **Navigator Agent（方向蒸馏）：** 对应RL的“Policy Gradient”。它分析历史轨迹（代码修改与性能波动的相关性），提炼出文本形式的“梯度”，告诉生成器往哪个方向改。\n    *   **Sampler Agent（分布调制）：** 对应RL的“Prioritized Experience Replay”。它根据当前状态和方向，从历史中检索最相关的范例作为Few-shot参考，而非随机堆砌历史。\n\n#### 第四阶段：逻辑闭环（验证与升华）\n*   **综合：** 这三个智能体协作，构成了一个在文本潜空间中运行的“类RL系统”。\n*   **预期效果：** 这种架构不仅解决了上下文窗口不足的问题（通过压缩），还解决了搜索盲目的问题（通过Navigator的梯度指引），从而在API-only的限制下，实现了接近RL的搜索效率。\n*   **最终产出：** ContextEvolve框架——一个通过多智能体协作，在文本空间完成功能同构于RL的高效优化系统。"
                },
                {
                    "title": "MathlibLemma: Folklore Lemma Generation and Benchmark for Formal Mathematics",
                    "arxiv_id": "2602.02561",
                    "authors": "Xinyu Liu, Zixuan Xie, Amir Moeini, Claire Chen, Shuze Daniel Liu, Yu Meng, Aidong Zhang, Shangtong Zhang",
                    "summary": "While the ecosystem of Lean and Mathlib has enjoyed celebrated success in formal mathematical reasoning with the help of large language models (LLMs), the absence of many folklore lemmas in Mathlib remains a persistent barrier that limits Lean's usability as an everyday tool for mathematicians like LaTeX or Maple. To address this, we introduce MathlibLemma, the first LLM-based multi-agent system to automate the discovery and formalization of mathematical folklore lemmas. This framework constitutes our primary contribution, proactively mining the missing connective tissue of mathematics. Its efficacy is demonstrated by the production of a verified library of folklore lemmas, a subset of which has already been formally merged into the latest build of Mathlib, thereby validating the system's real-world utility and alignment with expert standards. Leveraging this pipeline, we further construct the MathlibLemma benchmark, a suite of 4,028 type-checked Lean statements spanning a broad range of mathematical domains. By transforming the role of LLMs from passive consumers to active contributors, this work establishes a constructive methodology for the self-evolution of formal mathematical libraries.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了 \"MathlibLemma\"，明确描述为 \"the first LLM-based multi-agent system\"（第一个基于LLM的多智能体系统）。这不仅仅是将LLM作为工具应用，而是构建了一个新的智能体框架来解决自动化发现和形式化数学引理的问题。 2.  **命中核心关注点 (第二步)**: *   **多智能体**: 论文明确属于 `Multi-Agent Systems (MAS)` 范畴。 *   **自我演化**: 摘要中明确提到该工作 \"establishes a constructive methodology for the self-evolution of formal mathematical libraries\"（建立了形式化数学库自我演化的建设性方法论），直接对应 `Self-Evolving` 和 `Self-Improvement` 方向。 3.  **符合特殊例外情况 (第四步)**: 尽管论文的应用领域是形式化数学，但根据第四步关于“自我演化的应用”的规则，只要论文的核心是提出一种新的“自我演化”机制（在此处为通过多智能体系统实现数学库的自我演化），即使应用在特定领域，也应该保留。 综上所述，该论文在构建多智能体系统及其自我演化机制方面做出了实质性贡献，高度契合 \"LLM智能体及其演化\" 的研究课题。",
                    "summary2": "本文旨在解决 Lean 的 Mathlib 库中缺失 folklore lemmas 的问题。针对形式化数学中的“最后一公里”障碍，我们提出了一种名为 MATHLIBLEMMA 的基于 LLM 的多智能体框架，通过发现、评判、形式化和证明四个智能体自动挖掘并验证缺失引理。我们在构建的包含 4,028 个语句的 MATHLIBLEMMA benchmark 上，通过 Success@2 指标验证了其有效性，部分生成结果已成功合并进 Mathlib。",
                    "summary_translation": "尽管 Lean 和 Mathlib 生态系统在大语言模型 (LLMs) 的辅助下，在形式化数学推理领域取得了显著成就，但 Mathlib 中缺失大量 folklore lemmas (民间引理) 仍是一个长期存在的障碍，限制了 Lean 像 LaTeX 或 Maple 那样成为数学家日常工具的实用性。为解决这一问题，我们提出了 MathlibLemma，这是首个基于 LLM 的 multi-agent system (多智能体系统)，旨在实现数学 folklore lemmas (民间引理) 的发现与形式化自动化。该框架构成了我们的主要贡献，它能够主动挖掘数学领域中缺失的“连接组织”。该系统的有效性通过构建一个经过验证的 folklore lemmas (民间引理) 库得到了证明，其中部分引理已被正式合并到 Mathlib 的最新版本中，从而验证了该系统的实际效用及其与专家标准的一致性。利用这一 pipeline (流程)，我们进一步构建了 MathlibLemma benchmark (基准测试集)，这是一套包含 4,028 个经过 type-checked (类型检查) 的 Lean 语句，涵盖了广泛的数学领域。通过将 LLM 的角色从被动消费者转变为主动贡献者，本研究为 formal mathematical libraries (形式化数学库) 的自我演进建立了一种建设性的方法论。",
                    "inspiration_trace": "基于对论文《MathlibLemma: Folklore Lemma Generation and Benchmark for Formal Mathematics》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观背景与观察：形式化的承诺与现实的摩擦\n**思考起点：** 形式化验证（如Lean）被视为解决复杂数学证明（如ABC猜想）中“社会学不确定性”的终极方案，能将数学验证从依赖人类直觉转变为自动化的可靠过程。\n**现实冲突：** 尽管Lean及其核心库Mathlib取得了成功，但数学家仍不愿将其作为日常工具（如LaTeX或Maple）使用。\n**核心洞察：** 阻碍不仅仅在于学习门槛，更在于一个顽固的瓶颈——**“最后一公里”的摩擦**。即使数学思路清晰，形式化过程也常因缺失一些显而易见的小事实而停滞。\n\n### 2. 问题定义：从“讲故事”中提炼核心痛点\n作者在Introduction中通过层层递进的叙事逻辑，将模糊的“难用”具体化为特定的技术问题：\n\n1.  **现象引入（权威案例）：** 引用Terence Tao在形式化多项式Freiman-Ruzsa猜想时的经历。他指出，数学上有趣的部分相对容易，反而是那些“技术性的显而易见步骤”耗时最长。\n2.  **概念界定：** 将这些缺失的步骤定义为**“Folklore Lemmas”（民俗引理）**。这些是数学家在实践中默认使用、隐含习得，但在库中尚未以可重用形式存在的知识。\n3.  **AI视角的痛点：** 这种缺失对LLM尤为致命。当必要的引理缺失时，LLM无法像人类那样直接引用，必须从头重构。这不仅扩大了搜索空间、消耗上下文Token，还极易导致幻觉。\n4.  **系统性缺陷：** 现有的LLM系统是Mathlib的**“单向消费者”**。它们只消耗库资源而不贡献，这导致模型性能的上限被其无法扩展的库本身所限制。\n\n**显式总结的研究问题：**\n> **如何自动化地发现并形式化数学库中缺失的“民俗引理”，从而将LLM从被动的知识消费者转变为主动的知识贡献者，以填补形式化数学中的“最后一公里”鸿沟？**\n\n### 3. 假设提出与范式转移\n基于上述问题，作者提出了一个关键的假设和范式转移：\n*   **旧范式：** 被动等待。用户在证明过程中遇到缺失引理时手动补充，或者LLM在解题时临时构造（不可复用）。\n*   **新范式：** 主动挖掘。不应等待用户遇到缺口，而应**主动地、大规模地**发现并形式化这些缺失的“连接组织”。\n*   **目标转变：** 从“解决给定的难题”转向“**发现值得解决的命题**”。\n\n### 4. 方法论演进：从端到端到解耦式多智能体\n为了实现这一新范式，作者在方法论设计上经历了以下逻辑演进：\n\n*   **挑战分析：** 直接让LLM端到端地生成“正确的、可编译的、可证明的”引理极其困难。因为数学错误（幻觉）、语法错误和证明搜索失败这三种失败模式会相互纠缠，导致难以调试和优化。\n*   **设计原则：** **解耦失败模式**。将复杂的生成任务分解为独立的阶段，每个阶段专注于解决一种特定的错误。\n*   **逻辑链构建（四阶段流水线）：**\n    1.  **发现：** 既然目标是“挖掘”，首先需要基于现有上下文进行发散思维，生成多样化的候选引理（不要求证明）。\n    2.  **评判：** 为了避免在数学上错误的命题上浪费计算资源，必须在进入形式化前先进行语义过滤。这里利用LLM-as-a-judge，只关注数学正确性，忽略语法细节。\n    3.  **形式化：** 语义正确不代表代码能跑。这一阶段专门负责将数学直觉转化为符合Lean语法、能通过类型检查的代码。这里引入编译器反馈进行修复，严格区分“语义”与“语法”。\n    4.  **证明：** 只有当前面三步都通过（语义对、语法对），才进入最后的证明搜索阶段。此时模型只需专注于逻辑构建，无需担心定义缺失或语法报错。\n\n### 5. 价值闭环：从系统到基准\n最后，作者将这一思考过程升华为两个具体的产出，完成了逻辑闭环：\n*   **验证库：** 证明该系统不仅能生成Benchmark，还能生成真正可用的、被Mathlib合并的代码（实用性验证）。\n*   **基准：** 既然现有基准（如MiniF2F）关注“深度”（难题），且已趋于饱和，那么这个新系统产出的数千个“民俗引理”就构成了一个关注“广度”（知识覆盖）的新基准，用于评估模型填补知识空白的能力。\n\n---\n\n**总结：**\n作者的思考路径是从**形式化工具的实用性危机**出发，识别出**“民俗引理缺失”**这一根本原因，进而意识到LLM作为**单向消费者**的局限性。为了打破这一局限，他们提出了**主动挖掘**的范式，并通过**多智能体解耦**的设计策略，成功将模糊的数学直觉转化为严谨的形式化数学资产。"
                },
                {
                    "title": "Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs",
                    "arxiv_id": "2602.02556",
                    "authors": "Xuancheng Li, Haitao Li, Yujia Zhou, Yiqun Liu, Qingyao Ai",
                    "summary": "Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合要求**：论文提出了SEAM（Structured Experience Adapter Module），这是一种用于生成结构化经验的轻量级插件。这属于构建和改进LLM智能体架构的方法论，而非单纯的应用。 2.  **符合“单智能体”方向**：SEAM主要解决了智能体的“记忆”和“经验利用”问题。它通过生成实例定制的经验条目来指导执行器，这是对智能体记忆机制的实质性改进，超越了简单的检索。 3.  **符合“自我演化”方向**：论文描述了通过执行器rollout（环境反馈）和GRPO来训练SEAM以优化效用，并支持部署后的持续改进。这完全符合“智能体通过经验、反思或环境反馈进行自我完善和迭代”的定义。 4.  **排除非Agentic推理**：尽管论文在数学推理基准上进行了评估，但其核心并非提升LLM底层的数学或逻辑预测能力，而是通过外部模块（SEAM）增强智能体的决策过程。因此，它不属于“非Agentic的推理”排除项。",
                    "summary2": "本文旨在解决LLM静态推理重复错误及外部检索经验复用效率低的问题。针对数学推理任务，我们提出了一种名为SEAM的轻量级、特定于执行器的插件，通过GRPO训练生成结构化且实用的经验条目以指导冻结的LLM。我们在GSM8K、MATH、AIME等数学推理基准上通过准确率验证了其有效性，结果显示SEAM在低开销下显著提升了模型性能。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 在很大程度上是静态的，往往会重复推理过程或重蹈覆辙。以往的经验复用通常依赖于外部检索，这种基于相似度的方法可能会引入噪声，并增加延迟。我们提出了 SEAM (Structured Experience Adapter Module，结构化经验适配器模块)，这是一个轻量级的、特定于执行器的插件，它将经验存储在其参数中，并通过单次前向传播生成结构化的、针对特定实例定制的经验条目，从而指导冻结的 LLM 执行器。SEAM 通过执行器推演和 GRPO (Group Relative Policy Optimization，群组相对策略优化) 针对实用性进行训练，同时保持执行器处于冻结状态；此外，在部署后，还可以利用记录的成功轨迹对其进行监督微调，以进一步提升性能。在数学推理基准测试上的实验表明，SEAM 能够以较低的开销为不同的执行器带来一致的准确性提升。广泛的消融实验和分析进一步阐明了 SEAM 有效性和鲁棒性背后的潜在机制。",
                    "inspiration_trace": "基于对论文《Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs》的深入分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 引言中的“故事”逻辑：从观察到痛点\n\n作者在Introduction部分构建了如下逻辑链条，引出研究动机：\n\n1.  **宏观对比（现状观察）：**\n    *   **人类智能：** 人类能够从过往的问题解决中提炼“程序性经验”，并将其应用于新情境，从而提高效率并避免重复犯错。\n    *   **LLM的局限：** 现有的LLM本质上是**静态**的。面对新问题时，它们往往从零开始推理，重新探索已知的路径，并重复本可避免的错误。\n\n2.  **现有方案及其缺陷（问题识别）：**\n    *   **主流范式：** 为了解决上述问题，目前的学术界和工业界普遍采用**检索增强生成（RAG）**。即维护一个显式的外部经验库，在推理时通过相似度检索相关条目来辅助生成。\n    *   **核心痛点：**\n        *   **相似度 $\\neq$ 实用性：** 检索通常基于表面语义相似度，而非对执行者的实际帮助。即使语义相近，检索到的条目可能缺乏关键约束或检查点，甚至引入噪声，干扰推理过程。\n        *   **推理开销：** 维护外部库需要复杂的检索计算和额外的LLM调用（如总结、重写），导致显著的延迟和计算成本。\n\n3.  **研究空白：**\n    *   现有的“经验复用”方法过于依赖“检索”这一动作，忽略了经验的核心价值在于“能否帮助解决问题”，且未能有效解决效率问题。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“我们如何能让LLM复用经验时，不再依赖基于表面相似度的外部检索，而是通过一种低延迟的方式，直接生成针对当前实例和特定执行者优化的、具有实际效用的结构化经验？”**\n\n---\n\n### 三、 核心方法的逻辑演进：从假设到方法论\n\n为了回答上述问题，作者的思考经历了以下四个阶段的演进：\n\n#### 第一阶段：范式转移——从“外部检索”到“内部生成”\n*   **思考：** 既然外部检索存在相似度不等于效用的问题，且检索本身有延迟，能否完全抛弃“外部库”和“检索”这两个概念？\n*   **假设：** 如果我们将“经验”编码在一个轻量级模型的参数中，而不是存储在外部数据库里，那么获取经验就不再需要检索，而是一次前向传播的生成过程。\n*   **决策：** 提出参数化经验库。设计一个轻量级的**SEAM模块**，它不存储历史数据，而是学习如何“合成”经验。\n\n#### 第二阶段：目标重构——从“语义匹配”到“效用优化”\n*   **思考：** 传统的RAG训练目标是让检索到的内容和问题语义相近。但我们的目标是让LLM把题做对。那么，SEAM生成的经验好坏，应该由谁来评判？\n*   **假设：** 经验的价值不在于它写得多么像标准答案，而在于它是否真的帮助了下游的LLM（执行者）解决了问题。\n*   **决策：** 采用**强化学习（RL）**思路。将SEAM视为一个策略，其生成的经验作为Prompt输入给冻结的LLM执行者。如果执行者答对了，SEAM就获得奖励；否则受到惩罚。这样SEAM就会学习生成对特定执行者“有用”的提示，而不是“相似”的文本。\n\n#### 第三阶段：结构化设计——从“自由文本”到“结构化引导”\n*   **思考：** 如果让SEAM自由生成经验，可能会产生冗长、混乱的文本，反而干扰LLM。如何保证生成的经验既有效又可控？\n*   **假设：** 人类专家的经验通常包含三个部分：对问题的诊断、通用的策略/技巧、以及具体的步骤规划。这种结构化的经验比单纯的文本更有效。\n*   **决策：** 设计**Schema约束**。强制SEAM生成的经验条目包含三个固定组件：**问题分析**、**经验亮点**、**参考计划**。这确保了生成的经验具有诊断性、处方性和程序性。\n\n#### 第四阶段：解耦训练——从“联合微调”到“插件式适配”\n*   **思考：** 如果直接微调大模型（执行者）来学习经验，成本高且容易破坏原有的通用能力（灾难性遗忘）。如何在不改动大模型的前提下赋予其经验？\n*   **假设：** 将“经验生成”和“问题求解”解耦。大模型只负责解题，小模型（SEAM）负责提供“作弊条”。\n*   **决策：** **冻结执行者**。在整个训练过程中，下游的大模型参数完全不变，只更新SEAM的参数。这不仅降低了训练成本，还使得SEAM成为一个即插即用的模块，可以适配不同的执行者。\n\n---\n\n### 四、 总结：作者的思想脉络\n\n1.  **起点：** LLM像“金鱼”一样只有七秒记忆，重复犯错。\n2.  **批判：** 现在的“外挂笔记”（RAG）找东西太慢，且经常找错（只看脸不看疗效）。\n3.  **顿悟：** 不需要去“找”笔记，可以训练一个小模型专门“写”笔记。\n4.  **机制：** 这个小模型（SEAM）不学怎么解题，只学怎么写“解题指南”。写得好不好，看大模型照着做能不能做对（GRPO）。\n5.  **结果：** 大模型不动，小模型不断进化，最终实现低延迟、高准确率的经验复用。"
                },
                {
                    "title": "ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents",
                    "arxiv_id": "2602.02548",
                    "authors": "Xiaoce Wang, Guibin Zhang, Junzhe Li, Jinzhe Tu, Chun Li, Ming Li",
                    "summary": "Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **第一步：核心判断** 该论文的核心贡献是提出了一种名为 \"ToolTok\" 的新范式，旨在构建和改进 **GUI Agents**（图形用户界面智能体）。它并非将现有的智能体框架简单应用到特定领域（如医疗或金融），而是针对智能体在处理 GUI 任务时的局限性（如坐标依赖、数据稀缺），提出了新的架构（工具 Token 化）和训练方法（课程学习）。因此，这完全符合“构建、改进 LLM 智能体”的核心目标。 2.  **第二步：正面指标** 论文高度符合以下核心关注点： *   **Agentic AI / LLM-based Agents**: 论文明确以 GUI Agent 为研究对象。 *   **Tool Use / Tool Augmentation**: 论文的核心创新点在于将操作建模为“渐进式工具使用”的序列，并设计了可学习的工具 Token 嵌入，这是对智能体工具使用能力的直接增强。 *   **Planning**: 论文涉及多步路径查找，这是智能体规划能力的一种体现。 3.  **第三步：排除标准** *   **多模态与视觉**: 虽然论文涉及视觉元素（GUI 界面），但视觉仅作为智能体感知环境的输入手段，并非研究视觉模型本身。论文的核心在于智能体如何通过“工具 Token 化”来理解和操作这些视觉元素，符合“除非它们被用作智能体感知环境的工具”这一例外条款。 *   **安全与对齐**: 论文不涉及安全、对齐或水印等内容。 4.  **综合结论** 该论文属于 **单智能体** 方向，专注于提升智能体的 **工具使用** 效率和泛化能力。它提出了一种新的智能体架构来解决 GUI 交互中的具体挑战，符合筛选标准中关于“构建、改进 LLM 智能体”的要求。",
                    "summary2": "本文旨在解决现有GUI代理在分辨率变化下泛化能力差且依赖大量数据的问题。针对GUI交互场景，我们提出了一种ToolTok多步路径查找范式，通过离散工具令牌和语义锚定机制模拟人类操作。我们在ScreenSpot和Mind2Web-Simplified等数据集上通过Action Accuracy和Robustness验证了其有效性，实现了在极少数据下的高性能。",
                    "summary_translation": "现有的依赖于 coordinate-based (基于坐标的) one-step visual grounding (一步视觉定位) 的 GUI agent (GUI 代理) 模型在泛化到不同的输入分辨率和宽高比时面临困难。替代方案引入了 coordinate-free (无坐标) 策略，但在严重数据稀缺的情况下面临学习困难。为了解决这些局限性，我们提出了 ToolTok，这是一种用于 GUI agent 的 multi-step pathfinding (多步寻路) 新范式，其中操作被建模为一系列渐进式的工具使用。具体而言，我们设计了符合人类交互习惯的工具，并使用可学习的 token embeddings (token 嵌入) 来表示每个工具。为了在有限监督下实现高效的嵌入学习，ToolTok 引入了一种 semantic anchoring mechanism (语义锚定机制)，利用语义相关的概念作为 natural inductive bias (自然归纳偏置) 来锚定每个工具。为了进一步使预训练大语言模型能够逐步获取工具语义，我们构建了一个由易到难的 curriculum (课程)，包含三个任务：token 定义问答、纯文本引导的工具选择和 simplified visual pathfinding (简化视觉寻路)。在多个基准测试上的大量实验表明，ToolTok 在规模相当的模型（4B）中取得了优越的性能，并且与规模大得多的模型（235B）相比仍具有竞争力。值得注意的是，这些结果是使用其他 post-training (后训练) 方法所需训练数据的不到 1% 获得的。此外，ToolTok 在未见过的场景中展现了强大的泛化能力。我们的训练和推理代码已在 https://github.com/ZephinueCode/ToolTok 开源。",
                    "inspiration_trace": "基于论文《ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents》的内容，以下是对作者核心方法逻辑链的系统性推演，还原了从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 引言中的“故事”：问题是如何被发现的？\n\n作者在Introduction中构建了一个层层递进的逻辑链条，揭示了现有GUI智能体范式的根本缺陷：\n\n1.  **现状与主流范式**：GUI智能体被视为通向AGI的关键一步，目前的SOTA模型主要依赖**基于坐标的单步视觉定位**。即模型直接输出结构化的坐标或边界框（如 `[click, [0.5, 0.5]]`）来执行操作。\n2.  **核心矛盾（坐标的刚性）**：为了支持坐标预测，模型必须假设一个固定的全局坐标系。这意味着在推理时，所有输入的截图必须被强制归一化到预定义的分辨率和长宽比。\n3.  **负面后果（归一化的代价）**：\n    *   **视觉失真**：缩放和下采样不可避免地会扭曲界面细节（如按钮形状、布局），破坏感知线索。\n    *   **泛化脆弱性**：学到的动作表示与输入格式强耦合。一旦测试时的分辨率或长宽比与训练时不符，性能会急剧下降（作者通过Fig. 2的实验证实了这一点）。\n4.  **现有替代方案的局限**：虽然存在无坐标方法（如基于注意力图），但它们仅预测屏幕位置而非具体的GUI动作，局限于简单的“点击”，且未能有效利用预训练VLM的语言知识，导致数据利用效率低下。\n5.  **总结**：现有的坐标回归方法将GUI交互简化为数值预测，缺乏类似人类的语义决策过程，且对输入格式变化极其敏感。\n\n---\n\n### 2. 研究问题\n\n基于上述对现状的批判，作者提出了一个核心的引导性问题：\n\n**“我们能否定义可学习的工具，来模仿真实人类在GUI交互中的行为？”**\n\n---\n\n### 3. 核心方法的逻辑演进链\n\n为了回答上述问题，作者经历了一个从“范式转换”到“解决冷启动”再到“数据高效训练”的完整思考过程：\n\n#### 第一阶段：范式转换——从“定位”到“寻路”\n*   **思考**：人类操作电脑时，大脑里计算的不是绝对坐标（如x=500, y=300），而是基于视觉反馈的相对移动（如“向上移一点”、“点击那个按钮”）。\n*   **决策**：放弃“单步视觉定位”，转向**多步渐进式视觉寻路**。\n*   **方法雏形**：将GUI交互不再建模为连续坐标的回归问题，而是建模为在离散工具空间上的分类问题。即，模型输出的不再是坐标，是一系列离散的动作Token（如 `<MOVE_UP_FAR>`, `<CLICK_SHORT>`）。\n\n#### 第二阶段：表征对齐——解决“冷启动”难题\n*   **挑战**：要在预训练VLM中引入新的工具Token。如果随机初始化这些Token，模型在数据稀缺的情况下很难学会它们的含义（即“冷启动”问题）。现有的GUI数据集非常小（<2000样本），不足以从头训练。\n*   **思考**：如何让模型快速理解新Token的含义？利用VLM已有的语言知识。\n*   **决策**：提出**语义锚定机制**。\n*   **核心逻辑**：不要把新Token当作无意义的符号，而是将其锚定在语义相关的概念上。例如，将 `<MOVE_UP_FAR>` 的初始化向量设定为单词 \"move\", \"up\", \"far\" 嵌入向量的某种组合（球面投影）。这样，新Token就落在了模型已有的语义空间中，提供了天然的归纳偏置。\n\n#### 第三阶段：训练策略——课程学习与数据合成\n*   **挑战**：即使有了好的初始化，如何让模型在极少的数据下学会复杂的视觉-运动控制？\n*   **思考**：人类学习是循序渐进的。应该先让模型理解“工具是什么”，再教它“怎么用”，最后才是“在复杂场景中用”。\n*   **决策**：设计**由易到难的课程学习**。\n*   **执行步骤**：\n    1.  **纯文本阶段**：通过Token定义问答和文本引导的工具选择，让模型先在语言层面掌握工具的语义。\n    2.  **简化视觉阶段**：使用合成数据（简单的几何图形、光标、目标），让模型学习基础的视觉寻路逻辑，排除真实GUI的复杂语义干扰。\n    3.  **真实场景阶段**：利用Oracle轨迹合成，将静态的（图片，目标框）数据转化为多步的监督序列，引入思维链辅助决策，最终在真实GUI数据上微调。\n\n---\n\n### 总结\n\n作者的思考路径可以概括为：\n**发现坐标范式的脆弱性 $\\rightarrow$ 提出模仿人类的多步离散工具寻路范式 $\\rightarrow$ 针对数据稀缺和冷启动问题，利用语义锚定借用预训练知识 $\\rightarrow$ 通过课程学习从简到繁地实现高效训练。**\n\n这一逻辑链条不仅解决了泛化性问题，还极大地提高了数据效率。"
                },
                {
                    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
                    "arxiv_id": "2602.02488",
                    "authors": "Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang",
                    "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合“自我演化”与“Agentic AI”方向**: 论文提出了 \"RLAnything\" 框架，其核心在于通过闭环优化动态构建环境、策略和奖励模型。这种通过反馈（step-wise signals, outcome signals, consistency feedback）来迭代改进策略和奖励模型的过程，本质上属于**自我演化**和**自我完善**的范畴。同时，该框架明确针对 \"LLM or agentic scenarios\"，旨在增强智能体系统的整体能力，符合构建和改进 LLM 智能体的核心目标。 2.  **属于方法论创新而非单纯应用**: 论文并非将现有的智能体框架简单应用于某个垂直领域（如生物、金融），而是提出了一种通用的强化学习（RL）训练和演化机制。它关注的是智能体内部的策略优化、环境适应和奖励模型调整，这属于智能体底层构建和演化的方法论创新。 3.  **满足正面指标**: *   **核心范式**: 涉及 `Agentic AI` 和 `Self-Evolving`（通过闭环优化进行迭代改进）。 *   **演化机制**: 包含 `Self-Improvement`（策略和奖励模型的联合优化）、`Iterative Improvement`（利用经验进行环境适应）。 *   **智能体能力**: 涉及 `Policy`（策略）和 `Environment`（环境）的动态构建，这是智能体在复杂任务（如 OSWorld, AlfWorld）中表现能力的基础。 4.  **排除标准检查**: *   **非安全/对齐**: 论文主要关注性能提升（在 OSWorld, AlfWorld 等任务上的增益），而非安全性、对齐或幻觉检测。 *   **非基础设施**: 这是一个算法框架，而非硬件或部署优化。 *   **多模态处理**: 虽然论文中使用了 `Qwen3-VL-8B-Thinking`（视觉语言模型）并在 OSWorld（通常涉及GUI/视觉）上测试，但根据筛选标准中的特殊规则，视觉模型在这里是作为智能体感知环境的工具，而论文的**核心贡献**是 RL 训练框架本身，而非视觉技术的创新，因此不应被排除。 综上所述，该论文提出了一种通过强化学习机制实现智能体自我演化和能力提升的新框架，完全符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决强化学习在长轨迹任务中奖励信号稀疏及环境静态限制学习效率的问题。针对计算机控制、文本游戏及代码生成等复杂智能体场景，我们提出了RLAnything框架，通过闭环优化动态构建环境、策略和奖励模型，利用集成反馈和评论家反馈实现自适应调整。并在OSWorld、Alf World和LiveBench等基准上通过任务准确率验证了其有效性，显著优于依赖人工标签的基线方法。",
                    "summary_translation": "我们提出了 RLAnything，这是一个 reinforcement learning (强化学习) 框架，它通过 closed-loop optimization (闭环优化) 动态构建 environment (环境)、policy (策略) 和 reward models (奖励模型)，从而放大学习信号并增强适用于任何 LLM (大语言模型) 或 agentic scenarios (智能体场景) 的整体 RL 系统。具体而言，policy (策略) 利用来自 step-wise signals (逐步信号) 和 outcome signals (结果信号) 的集成反馈进行训练，而 reward model (奖励模型) 则通过 consistency feedback (一致性反馈) 进行联合优化，这进而进一步改进了 policy (策略) 的训练。此外，我们受理论启发的 automatic environment adaptation (自动环境适应) 机制通过利用来自两者的 critic feedback (评论家反馈)，改进了 reward model (奖励模型) 和 policy (策略) 的训练，从而实现了从经验中学习。实验结果表明，每个新增组件都持续改进了整体系统，且 RLAnything 在各种代表性 LLM (大语言模型) 和 agentic (智能体) 任务中带来了显著提升，在 OSWorld 上将 Qwen3-VL-8B-Thinking 提升了 9.1%，在 AlfWorld 和 LiveBench 上分别将 Qwen2.5-7B-Instruct 提升了 18.7% 和 11.9%。我们还发现，优化的 reward-model (奖励模型) 信号优于依赖人类标签的结果。代码：https://github.com/Gen-Verse/Open-AgentRL",
                    "inspiration_trace": "基于对论文《RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System》的深入分析，以下是作者产出该文章的系统性思考过程与逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的方式，构建了从现有成功到现实局限，再到核心痛点的逻辑链条：\n\n1.  **背景与现状：** 强化学习（特别是基于可验证奖励的 RLVR）已被证明能有效提升大语言模型（LLM）的推理能力。\n2.  **现实挑战（长轨迹与稀疏性）：** 现实世界的应用（如智能体交互）往往超越单轮问答，涉及长轨迹的迭代交互。在这种场景下，仅依靠二元的“最终结果”奖励过于稀疏，无法提供足够的监督信号。\n3.  **现有方案的局限（奖励模型）：** 虽然生成式奖励模型能提供更细粒度的“逐步”信号，且优于标量模型，但训练这些模型通常需要大量高质量、特定任务的人工监督，难以自动化和规模化。\n4.  **被忽视的关键（环境质量）：** 除了奖励设计，环境的质量同样至关重要。将任务难度与模型当前能力对齐（课程学习）能改善训练动态。在现实环境中，探索的范围由任务定义，且增加任务多样性能促进泛化。\n5.  **逻辑缺口：** 现有的 RL 系统通常将环境、策略和奖励模型视为相对独立或静态的组件，缺乏一个统一的机制让它们协同进化。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑缺口，作者提出了一个核心的探索性问题：\n\n**“是否存在一个能够联合优化环境、策略和奖励模型的 RL 系统，通过闭环交互来放大学习信号，从而强化整个系统的性能？”**\n\n---\n\n### 三、 核心方法的逻辑演进（思想推演）\n\n为了回答上述问题，作者的思考过程经历了从“观察痛点”到“提出假设”，再到“理论验证”和“系统构建”的四个阶段。\n\n#### 1. 观察与解构：打破“静态”桎梏\n*   **观察：** 在传统的 RL 流程中，策略是动态训练的，但提供信号的“奖励模型”和提供场景的“环境”往往是静态的或预定义的。\n*   **痛点分析：**\n    *   **奖励端：** 仅用最终结果太稀疏，仅用逐步信号不可靠。如果奖励模型本身不随着策略的进步而进化，它给出的反馈可能始终是低质量的。\n    *   **环境端：** 如果环境任务太难，策略无法获得正向反馈；如果太简单，策略无法学到新东西。静态环境无法匹配策略动态变化的能力。\n\n#### 2. 假设提出：构建“三位一体”的动态闭环\n*   **核心假设：** 如果让策略、奖励模型和环境三者形成一个动态的闭环，每一方都从另外两方获得反馈并进化，那么整个系统的学习效率将呈指数级提升。\n*   **具体推演：**\n    *   **策略进化：** 需要更密集的信号。假设将“稀疏的最终结果”与“密集的逐步信号”结合，能提供更优的监督。\n    *   **奖励进化：** 奖励模型不需要人工标注，而是利用策略产生的轨迹作为训练环境。通过“一致性反馈”（即奖励模型的判断是否与最终结果一致）来优化自身。\n    *   **环境进化：** 环境不应是固定的。假设利用奖励模型对策略行为的“批评反馈”，来自动调整任务的难度（太难就简化，太简单就增加难度）。\n\n#### 3. 理论洞察：环境适配是奖励模型优化的前提\n*   **深层思考：** 为什么要动态调整环境？仅仅是为了让策略好学吗？\n*   **理论推导：** 作者通过理论分析发现了一个关键点——**奖励模型的精度依赖于任务难度的平衡**。\n    *   如果任务太难（策略总是失败），或者太简单（策略总是成功），会导致训练数据分布极度不平衡，从而破坏奖励模型区分好坏步骤的能力。\n    *   **结论：** 调整环境难度不仅是为了策略，更是为了训练出一个更准确的奖励模型。这为“环境动态化”提供了坚实的理论动机。\n\n#### 4. 方法论形成：RLAnything 的闭环设计\n基于上述思考，作者最终构建了 **RLAnything** 框架，其逻辑架构如下：\n\n*   **第一环（策略训练）：** 采用**集成反馈**。不再单一依赖结果，而是将最终结果奖励与奖励模型的逐步信号加权融合，解决长轨迹中的稀疏奖励问题。\n*   **第二环（奖励模型优化）：** 采用**一致性反馈**。将策略的轨迹视为环境，利用最终结果和自我一致性作为监督信号，让奖励模型在评估策略的同时自我进化。\n*   **第三环（环境自适应）：** 采用**批评反馈驱动**。利用奖励模型输出的具体错误诊断，指导 LLM 自动修改任务描述或参数，动态调整任务难度，确保难度始终处于策略的“最近发展区”，从而同时反哺策略和奖励模型的训练。\n\n---\n\n**总结：**\n作者的思考路径是从**“单一组件优化”**转向**“系统级协同进化”**。他们不仅解决了奖励稀疏的问题（通过集成反馈），更关键地发现了环境、奖励与策略之间的数学耦合关系（通过理论证明），最终提出了一个完全动态、自我强化的 RL 系统。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 15,
            "papers": [
                {
                    "title": "Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling",
                    "arxiv_id": "2602.03719",
                    "authors": "Yubao Zhao, Weiquan Huang, Sudong Wang, Ruochen Zhao, Chen Chen, Yao Shu, Chengwei Qin",
                    "summary": "Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \\href{https://github.com/YubaoZhao/BranPO}{code}.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“Agentic AI”和“自我演化”的核心方向。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了一种名为 **Branching Relative Policy Optimization (BranPO)** 的新算法，用于训练 **Multi-Turn Search Agent**（多轮搜索智能体）。 *   这属于构建和改进 LLM 智能体的方法论，而非仅仅将现有智能体作为工具应用到特定领域（如医疗、法律等）。虽然实验在问答基准上进行，但其核心在于解决智能体训练中的通用难题（长视界奖励稀疏）。 2.  **符合核心关注点（第二步）**： *   **Agentic AI**：论文明确涉及 **Agentic reinforcement learning**，关注智能体的 **multi-turn planning**（多轮规划）和 **tool use**（工具使用，即搜索）。 *   **自我演化/改进**：论文提出的 BranPO 方法通过对比动态分支采样，利用步骤级的对比监督来优化智能体的策略。这是一种通过反馈和迭代来提升智能体性能的机制，符合 **Self-Improvement** 和 **Iterative Improvement** 的定义。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）**： *   论文关注的是智能体在长视界任务中的规划和决策过程，旨在解决智能体在复杂任务中的训练稳定性问题，这属于 Agentic 的规划范畴，而非单纯提升模型底层的 Token 预测或数学逻辑能力。 综上所述，该论文致力于解决 LLM 智能体在长视界任务中的训练难题，提出了新的智能体优化框架，完全符合“构建、改进或演化 LLM 智能体”的研究目标。",
                    "summary2": "本文旨在解决长视距智能体强化学习中稀疏奖励导致的信用分配模糊问题。针对多轮搜索任务，我们提出了一种Branching Relative Policy Optimization (BranPO)方法，通过在轨迹尾部截断并重采样构建对比后缀，结合难度感知分支采样和冗余步骤掩码。并在多个问答基准数据集上通过F1分数和LLM-as-a-Judge指标验证了其有效性。",
                    "summary_translation": "Agentic reinforcement learning (智能体强化学习) 已经使 large language models (大语言模型) 能够执行复杂的 multi-turn planning (多轮规划) 和 tool use (工具使用)。然而，由于存在稀疏的 trajectory-level outcome rewards (轨迹级结果奖励)，在 long-horizon settings (长视界设定) 下的学习仍然充满挑战。尽管先前的 tree-based methods (基于树的方法) 试图缓解这一问题，但它们往往面临 high variance (高方差) 和 computational inefficiency (计算效率低) 的问题。通过对 search agents (搜索智能体) 的 empirical analysis (实证分析)，我们发现了一个普遍模式：性能发散主要归因于靠近 tail (尾部) 的决策。受此观察启发，我们提出了 Branching Relative Policy Optimization (BranPO, 分支相对策略优化)，这是一种 value-free method (免价值方法)，能够在没有 dense rewards (密集奖励) 的情况下提供 step-level contrastive supervision (步级对比监督)。BranPO 在 tail (尾部) 附近截断 trajectories (轨迹) 并重采样 alternative continuations (替代延续)，从而在 shared prefixes (共享前缀) 上构建 contrastive suffixes (对比后缀)，减少了 long-horizon rollouts (长视界推演) 中的 credit ambiguity (信用归因模糊)。为了进一步提高效率并稳定训练，我们引入了 difficulty-aware branch sampling (难度感知分支采样) 以根据任务调整 branching frequency (分支频率)，以及 redundant step masking (冗余步屏蔽) 来抑制 uninformative actions (无信息动作)。在各种 question answering benchmarks (问答基准) 上进行的广泛实验表明，BranPO 始终优于 strong baselines (强基线)，在未增加 overall training budget (整体训练预算) 的情况下，在 long-horizon tasks (长视界任务) 上实现了显著的 accuracy gains (准确率提升)。我们的代码可在 \\href{https://github.com/YubaoZhao/BranPO}{code} 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue",
                    "arxiv_id": "2602.03548",
                    "authors": "Yuqin Dai, Ning Gao, Wei Zhang, Jie Wang, Zichen Luo, Jinpeng Wang, Yujie Wang, Ruiyuan Wu, Chaozheng Wang",
                    "summary": "Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”方向**：论文标题明确提出了“Self-Evolving Agent”（自我演化智能体），其核心贡献是提出了SEAD框架。该框架旨在解决数据稀缺问题，使智能体能够通过“Profile Controller”生成多样化的用户状态来管理训练课程，从而实现无需大规模人工标注的自我学习和策略优化。这直接对应了筛选标准中的“自我演化”机制。 2.  **符合“Agentic AI”范畴**：论文关注的是智能体在多轮服务对话中的表现，涉及目标导向的用户行为模拟和任务完成。这属于智能体在特定环境下的交互与规划能力，而非单纯的文本生成或基础推理。 3.  **符合“特殊和模糊情况”处理规则**：虽然论文的应用场景是“服务对话”（特定领域），但根据第四步第2条规则，只要论文的核心是提出一种新的“自我演化”机制（即SEAD框架及其课程学习策略），即使应用在特定领域，也应该保留。本文的重点在于智能体如何通过演化机制提升能力，而不仅仅是应用现有模型解决领域问题。 4.  **无排除项**：论文不涉及安全对齐、多模态视觉或图神经网络等排除标准。 综上所述，该论文属于构建和演化LLM智能体的前沿研究，符合“自我演化”的核心研究目标。",
                    "summary2": "本文旨在解决多轮服务对话中数据稀缺及传统自进化训练不公平对抗的问题。针对外呼服务场景，我们提出了一种SEAD框架，通过解耦用户建模为Profile Controller和User Role-play Model，实现自适应难度的公平对抗训练。我们在真实企业外呼服务场景中，通过Task Completion Rate和Dialogue Efficiency等指标验证了其有效性，显著优于开源及商业模型。",
                    "summary_translation": "大型语言模型在开放域对话中已展现出卓越的性能。然而，当前方法在服务对话中的表现不尽如人意，主要归因于其对含噪声、低质量人工对话数据的依赖。这一局限性源于数据稀缺，以及难以模拟真实且目标导向的用户行为。为解决上述问题，我们提出了 SEAD (Self-Evolving Agent for Service Dialogue，服务对话自进化智能体)，该框架使智能体能够在无需大规模人工标注的情况下学习有效策略。SEAD 将用户建模解耦为两个组件：一个是生成多样化用户状态以管理训练课程的 Profile Controller（档案控制器），另一个是专注于逼真角色扮演的 User Role-play Model（用户角色扮演模型）。这种设计确保环境提供自适应的训练场景，而非充当不公平的对手。实验结果表明，SEAD 显著优于 Open-source Foundation Models（开源基础模型）和 Closed-source Commercial Models（闭源商业模型），将任务完成率提高了 17.6%，对话效率提升了 11.1%。代码可在 https://github.com/Da1yuqin/SEAD 获取。",
                    "inspiration_trace": "基于对论文《SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue》的深入分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 宏观观察与问题引入\n\n**1. 现象观察：LLM的通用性与服务对话的局限性**\n作者首先观察到，虽然大语言模型（LLM）在开放域对话中表现出色，但在**服务对话**这一特定场景下，性能却远未达到理想状态。\n*   **核心痛点**：服务对话是目标导向的，需要多轮交互和动态策略。然而，现有的训练方法严重依赖人类对话数据。\n*   **数据困境**：真实的人类对话数据（如销售录音）存在两大致命缺陷：\n    *   **稀缺且昂贵**：获取高质量标注数据成本极高。\n    *   **质量低劣且偏差**：真实数据充满了噪音、非标准化，且往往记录的是失败案例（因为成功案例通常不需要人工介入或记录较少）。更糟糕的是，数据质量被原始人类客服的能力上限所“封顶”——模型无法从比人类更差的数据中学到比人类更好的策略。\n\n**2. 现有路径的探索与失败**\n为了解决数据问题，作者回顾了学术界和工业界的三种主流尝试，并逐一指出了其逻辑漏洞：\n\n*   **路径一：静态合成数据**\n    *   *思路*：利用LLM根据预设场景生成固定的对话数据集。\n    *   *缺陷*：这是“死”的数据。它无法捕捉真实对话中用户对Agent行为的动态反应。Agent无法在动态交互中学习适应策略。\n\n*   **路径二：交互式用户模拟**\n    *   *思路*：在训练过程中，让LLM动态扮演用户与Agent对话。\n    *   *缺陷*：**难度控制失效**。模拟器无法感知Agent当前的能力水平。如果模拟器太强（过于刁钻），Agent会受挫学不到东西；如果太弱，Agent又得不到有效训练。此外，模拟器往往表现得过于“完美”和理性，缺乏真实人类的注意力涣散、语言噪音和非理性行为。\n\n*   **路径三：自演化/自我博弈**\n    *   *思路*：借鉴AlphaGo，让模型自己生成问题和答案，实现零数据进化。\n    *   *缺陷*：**不公平的对抗游戏**。这是最关键的逻辑转折点。在围棋等游戏中，胜负规则是客观的；但在服务对话中，**“用户”拥有对结果的绝对主观控制权**。如果采用传统的自我博弈训练“用户模型”，用户模型为了“赢”（即拒绝服务），可以无视Agent的表现直接挂断或无理拒绝。这切断了Agent行为与任务成功之间的因果联系，导致Agent无法学到有效的说服策略。\n\n---\n\n### 二、 研究问题\n\n基于上述对现有方法（特别是自演化方法在服务对话中失效）的深入剖析，作者提炼出了本文试图解决的核心问题：\n\n**“在缺乏大规模高质量人工标注数据的情况下，如何构建一个自演化的服务对话框架，既能模拟真实、多样且具有人类非理性特征的用户行为，又能避免用户模拟器通过主观控制结果导致的‘不公平对抗’，从而让Agent在动态交互中习得最优策略？”**\n\n---\n\n### 三、 逻辑演进与方法论形成\n\n为了回答上述问题，作者的思考经历了以下四个阶段的逻辑演进：\n\n**阶段一：解构“用户”的本质**\n作者意识到，传统的自演化失败是因为将“用户”视为一个单一的、追求胜负的对抗者。但在服务对话中，用户其实扮演了两个截然不同的角色：\n1.  **环境的设定者**：决定了对话开始时的难度（用户是谁？心情如何？信任度多少？）。\n2.  **角色的扮演者**：在对话过程中根据Agent的表现做出反应（说话、情绪变化）。\n*   **推论**：如果将这两个角色解耦，是否就能解决“不公平对抗”的问题？\n\n**阶段二：设计“公平博弈”机制**\n基于解构思想，作者提出了核心创新——**解耦式用户建模**：\n*   **组件A：Profile Controller（画像控制器）**。它只负责“设定环境”，即采样初始用户状态（合作度、情绪、信任度）。它参与对抗训练，目标是找到让Agent成功率约为50%的“黄金难度”。\n*   **组件B：User Role-play Model（用户角色扮演模型）**。它只负责“演戏”，专注于模拟真实人类的反应（包括犹豫、打断、非理性）。它**不参与**对抗训练，参数固定。\n*   **逻辑闭环**：这种设计将对抗变成了一个**“下注游戏”**。Profile Controller不能在对话中途随意操纵结果（那是Role-play Model的事，它只管真实反应），它只能在开局前下注。如果它选的用户太难，Agent一直失败，训练信号就弱；如果太简单，Agent学不到东西。因此，Controller被迫去寻找那些“势均力敌”的场景，从而保证了训练的公平性和有效性。\n\n**阶段三：引入“课程学习”与“真实感”**\n为了解决“难度控制”和“真实感”的问题，作者进一步细化了机制：\n*   **自适应难度**：通过统计Agent在不同初始状态下的成功率，动态调整Profile Controller的采样概率。总是倾向于采样成功率在40%-60%之间的状态，随着Agent变强，难度自动提升。\n*   **真实感注入**：从10万+真实对话中提取匿名行为模式（如质疑AI身份、担心费用、注意力分散），注入到Role-play Model中，确保模拟器不会像机器人一样完美，而是像真人一样充满瑕疵。\n\n**阶段四：形成闭环系统**\n最终，这三个模块——**Profile Controller（出题人）**、**User Role-play Model（演员）**、**Service Agent（答题人）**——形成了一个完整的自演化闭环：\n1.  Controller出题（采样用户画像）。\n2.  Actor与Agent进行多轮对话。\n3.  根据结果优化Agent策略。\n4.  分析失误，反馈给Controller调整下一轮的出题难度。\n\n---\n\n### 总结\n\n作者的思考路径是从**数据困境**出发，否定了静态合成和传统模拟，在**自演化**的概念上遇到了**“主观控制导致的不公平”**这一拦路虎。通过**解耦**用户的“设定者”和“扮演者”双重身份，作者巧妙地将一个不公平的对抗游戏转化为一个寻找最佳训练难度的**下注游戏**，从而在无需人工数据的前提下，实现了Agent在真实感环境中的自我进化。"
                },
                {
                    "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
                    "arxiv_id": "2602.03442",
                    "authors": "Mingxuan Du, Benfeng Xu, Chiwei Zhu, Shaohan Wang, Pengyu Wang, Xiaorui Wang, Zhendong Mao",
                    "summary": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心的“Agentic AI”研究范畴。 1.  **核心判断（第一步）：** 论文的核心贡献是提出了 **A-RAG**，这是一个新的 **Agentic RAG 框架**。它不仅仅是将LLM作为工具应用，而是构建了一个让LLM能够主动参与检索决策的智能体架构。这符合“构建、改进 LLM智能体”的核心目标。 2.  **正面指标匹配（第二步）：** *   **核心范式：** 论文明确提出了 `Agentic RAG`，属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力：** 论文的核心机制是赋予智能体 `Tool Use`（工具使用）能力。它向模型暴露了分层检索接口（关键词搜索、语义搜索、分块读取），使智能体能够像使用工具一样自适应地搜索和检索信息，而不是被动地接收拼接好的文本。这体现了智能体的自主性和适应性。 3.  **排除标准检查（第三步）：** 论文不涉及安全对齐、多模态视觉或图神经网络，因此没有触犯排除规则。 4.  **特殊情况处理（第四步）：** 虽然RAG通常被视为一种增强技术，但该论文将其转化为了一种 **Agentic** 行为（智能体自主决定何时搜索、搜什么、读什么），这属于对智能体工具使用和规划能力的改进，而非简单的非演化型应用。 综上所述，该论文通过引入分层检索接口，将传统的RAG系统升级为具备自主工具使用能力的LLM智能体，是对Agentic AI架构的重要改进，因此予以保留。",
                    "summary2": "本文旨在解决现有RAG系统无法利用LLM推理能力及依赖静态算法的问题。针对开放域问答任务，我们提出了一种名为A-RAG的Agentic RAG框架，通过分层检索接口（keyword_search、semantic_search、chunk_read）赋予模型自主检索决策权。我们在HotpotQA、2WikiMultiHopQA、MuSiQue和GraphRAG-Bench上，通过LLM-Evaluation Accuracy和Contain-Match Accuracy验证了其有效性，证明A-RAG优于现有方法且具备高效扩展能力。",
                    "summary_translation": "前沿语言模型已经展示了强大的推理和长程工具使用能力。然而，现有的RAG（检索增强生成）系统未能利用这些能力。它们仍然依赖两种范式：(1) 设计一种单次检索段落并将其拼接到模型输入中的算法，或者 (2) 预定义一个工作流并提示模型逐步执行。这两种范式都不允许模型参与检索决策，从而阻碍了随着模型改进而进行的有效扩展。在本文中，我们介绍了A-RAG，这是一个直接向模型提供分层检索接口的Agentic RAG（智能体检索增强生成）框架。A-RAG提供了三种检索工具：keyword search（关键词搜索）、semantic search（语义搜索）和chunk read（分块读取），使智能体能够自适应地跨多个粒度搜索和检索信息。在多个开放域问答基准上的实验表明，A-RAG在检索Token数量相当或更少的情况下，始终优于现有方法，证明了A-RAG有效地利用了模型能力并动态适应不同的RAG任务。我们进一步系统地研究了A-RAG如何随模型大小和test-time compute（测试时计算）进行扩展。我们将发布我们的代码和评估套件以促进未来的研究。代码和评估套件可在 https://github.com/Ayanami0730/arag 获取。",
                    "inspiration_trace": "基于对论文《A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察与背景切入\n**逻辑起点：LLM 范式的根本性转变**\n作者首先观察到大语言模型（LLM）的发展重心已经发生了转移。从早期的单轮文本理解与生成，演进到了具备**复杂推理**和**长周期工具使用**能力的“智能体”时代（如编程助手、深度研究助手）。\n*   **思考：** 既然模型本身已经具备了强大的自主决策和工具使用能力，那么现有的外部知识增强系统（即 RAG 系统）是否跟上了这一步伐？\n\n### 2. 问题识别与“讲故事”逻辑\n作者在 Introduction 中通过对比现状与理想，构建了如下的“问题叙事”链条：\n\n1.  **现状滞后：** 尽管通用 LLM 已经进化为智能体，但 RAG 领域的方法论依然停留在旧时代，主要依赖两种过时的范式：\n    *   **范式一（静态检索）：** 设计一个算法（无论是否包含图结构），一次性检索出若干段落，然后直接拼接塞给模型。\n    *   **范式二（固定工作流）：** 预先定义一个工作流，提示模型按部就班地一步步执行（例如：先检索，再思考，再检索）。\n2.  **核心痛点：** 这两种范式本质上都是**非智能体**的。\n    *   在范式一中，模型完全是被动的信息接收者，无法参与检索决策。\n    *   在范式二中，模型只是既定脚本的执行者，无法根据具体任务动态调整策略（例如无法决定“何时停止检索”或“需要何种粒度的信息”）。\n3.  **后果：** 这种限制导致 RAG 系统无法随着模型能力的提升而有效扩展。模型越强，但被束缚得越死，无法发挥其推理优势。\n\n**显式总结出的研究问题：**\n> **“如何将 RAG 转变为一个真正的智能体框架，使模型能够自主控制检索过程，从而充分利用其日益增强的推理能力？”**\n\n### 3. 关键洞察与假设提出\n为了解决上述问题，作者进行了深度的归因分析，提出了核心假设：\n\n*   **洞察：** 语料库中的信息本质上是**分层级**组织的。从细粒度的关键词信号，到句子级别的语义，再到篇章级别的完整上下文。\n*   **假设：** 如果我们不再给模型预设复杂的检索算法或固定的工作流，而是直接向模型暴露一套**分层级的检索接口**，模型将能够根据任务需求，自发地泛化出多样化的检索策略。\n\n### 4. 方法论演进与 A-RAG 的诞生\n基于上述洞察，作者的设计思路从“如何设计更好的检索算法”转向了“如何设计更好的工具接口”：\n\n1.  **接口设计：** 对应信息的三个粒度，设计了三个核心工具：\n    *   **Keyword Search (关键词搜索)：** 用于精确匹配实体（细粒度）。\n    *   **Semantic Search (语义搜索)：** 用于概念性匹配（中粒度）。\n    *   **Chunk Read (块读取)：** 用于获取完整上下文（粗粒度）。\n2.  **架构简化：** 为了验证接口的有效性，作者刻意简化了 Agent 的控制循环，采用最简单的 ReAct 框架（推理-行动循环），不引入复杂的并行调用或编排逻辑。\n3.  **渐进式信息获取：** 允许模型先通过搜索获取摘要，再自主决定是否读取全文，从而在保证信息全面性的同时最大化上下文效率。\n\n### 5. 验证与结论\n最后，作者通过实验验证了这一思路：\n*   **对比实验：** 证明即使是简单的“Naive Agentic RAG”（仅有一个工具）也能击败复杂的 Graph RAG 和 Workflow RAG，说明“自主权”比“复杂算法”更重要。\n*   **全量实验：** A-RAG (Full) 通过分层接口进一步提升了性能，且在测试时计算和上下文效率上表现出良好的扩展性。\n\n---\n\n**总结：**\n作者的思考路径是从**观察 LLM 能力进化**开始，发现**RAG 范式滞后**（模型缺乏自主权），提出**分层接口**的解决方案，最终通过**赋予模型工具选择权**而非**预设检索路径**，实现了 RAG 系统向智能体范式的跨越。"
                },
                {
                    "title": "Verified Critical Step Optimization for LLM Agents",
                    "arxiv_id": "2602.03412",
                    "authors": "Mukai Li, Qingcheng Zeng, Tianqing Fang, Zhenwen Liang, Linfeng Song, Qi Liu, Haitao Mi, Dong Yu",
                    "summary": "As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”与“单智能体”方向。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种名为“关键步骤优化”（CSO）的新方法，用于改进LLM智能体的后训练过程。它不是将智能体作为工具应用到特定领域，而是直接针对智能体本身在处理复杂长视界任务时的策略进行优化，属于构建和改进LLM智能体的方法论。 2.  **符合研究焦点**： *   **自我演化**：论文的核心机制是让智能体从失败的策略轨迹中学习，通过识别关键决策点并利用专家模型提出替代方案，再通过验证来生成训练数据。这是一种典型的自我完善和迭代改进机制，旨在通过经验反馈提升智能体能力。 *   **单智能体**：研究聚焦于单个智能体在复杂任务中的规划、决策和执行过程，涉及对智能体轨迹的细粒度分析和优化。 3.  **排除标准检查**：论文不涉及安全、对齐、多模态视觉或图技术等排除项。虽然它使用了基准测试（GAIA, XBench），但目的是验证智能体能力的提升，而非解决特定领域的应用问题。 综上所述，该论文致力于解决LLM智能体的自我优化和策略改进问题，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决LLM Agent后训练中奖励归因不精确和计算成本高的问题。针对复杂长视界任务，我们提出了一种Verified Critical Step Optimization (CSO)，专注于验证的关键步骤，利用PRM识别并通过分支执行验证能翻转结果的决策点。我们在GAIA-Text-103和XBench-DeepSearch数据集上通过准确率验证了其有效性，实现了显著优于基线的性能。",
                    "summary_translation": "随着大语言模型智能体处理日益复杂的长视界任务，有效的后训练变得至关重要。先前的研究面临根本性挑战：仅基于结果的奖励无法精确归因于中间步骤，估计的步骤级奖励引入系统性噪声，而用于步骤奖励估计的蒙特卡洛采样方法则带来难以承受的计算成本。受仅有少量高熵 token 推动有效推理强化学习这一发现的启发，我们提出了关键步骤优化，该方法将偏好学习集中在经过验证的关键步骤上，即替代行动能明确地将任务结果从失败转变为成功的决策点。至关重要的是，我们的方法从失败的策略轨迹而非专家演示开始，直接针对策略模型的弱点。我们使用过程奖励模型来识别候选关键步骤，利用专家模型提出高质量的替代方案，然后使用策略模型本身从这些替代方案继续执行，直到任务完成。只有策略成功执行并产生正确结果的替代方案才会被验证并用作 DPO 训练数据，从而确保了质量和策略可达性。这在关键决策处产生了细粒度、可验证的监督，同时避免了轨迹级别的粗糙性和步骤级别的噪声。在 GAIA-Text-103 和 XBench-DeepSearch 上的实验表明，CSO 相比 SFT 基线实现了 37% 和 26% 的相对提升，并大幅优于其他后训练方法，同时仅需对 16% 的轨迹步骤进行监督。这证明了基于选择性验证的学习在智能体后训练中的有效性。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者提出“Verified Critical Step Optimization (CSO)”这一核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论产出的完整思考过程。\n\n---\n\n### 1. 宏观背景与问题引入\n\n**逻辑链：**\n随着LLM智能体处理日益复杂的长周期任务，后训练变得至关重要。目前的通用范式是“监督微调（SFT）+ 强化学习（RL）”，但这存在根本性矛盾：SFT容易导致分布偏移，而在线RL虽然能缓解偏移，却面临计算成本高昂和奖励信号稀疏的挑战。\n\n**现有方案的“三难”困境：**\n为了解决上述矛盾，现有研究主要分为两派，但都存在明显缺陷：\n1.  **轨迹级优化：** 对比成功与失败的整体轨迹。\n    *   *缺陷：* 归因过于粗糙。失败轨迹中合理的步骤会被错误惩罚，成功轨迹中的次优决策会被错误强化。\n2.  **步骤级优化：** 利用过程奖励模型（PRM）评估中间步骤。\n    *   *缺陷：* 依赖估计的奖励，引入了系统性的噪声和偏差。\n3.  **混合尝试（如IPR）：** 试图通过蒙特卡洛采样来验证步骤奖励。\n    *   *缺陷：* 计算成本在复杂任务中变得不可接受。\n\n---\n\n### 2. 核心研究问题\n\n基于上述背景，作者试图在“粗糙的轨迹级监督”、“有噪声的步骤级监督”和“昂贵的验证采样”之间找到一个新的平衡点。\n\n**研究问题：**\n> **如何通过识别并优化那些能够决定任务成败的“关键决策步骤”，在避免轨迹级归因模糊和步骤级估计噪声的同时，实现高效且可验证的智能体后训练？**\n\n---\n\n### 3. 思想演进与逻辑推演\n\n#### 第一阶段：从“全面优化”到“稀疏关键点”的视角转变\n*   **观察：** 现有方法大多假设所有步骤都需要监督，或者均匀地施加奖励。\n*   **灵感：** 借鉴RLVR（强化学习验证推理）领域的最新发现——只有极少数的高熵Token真正驱动了推理能力的提升。\n*   **推论：** 在智能体轨迹中，并非所有步骤都同等重要。许多步骤是琐碎的或只有唯一解。真正重要的是那些“关键分支点”，即不同的行动选择会导致截然不同的结果（成功或失败）。\n*   **假设：** 如果能只在这些“关键步骤”上进行精准的监督，就能以最小的代价实现最大的性能提升。\n\n#### 第二阶段：定义“关键步骤”与“验证”机制\n*   **定义：** 所谓“关键步骤”，是指那些如果采取替代行动，就能将任务结果从“失败”翻转为“成功”的决策点。\n*   **挑战：** 如何找到这些步骤？直接用PRM评分会有噪声，直接用蒙特卡洛采样太贵。\n*   **策略：** 采用“筛选+验证”的两阶段法。\n    1.  **筛选：** 利用PRM作为高效的过滤器，找出那些“模型做得差（低分）但专家能做得好（高分）”的候选步骤。\n    2.  **验证：** 为了消除PRM的噪声，必须进行“结果验证”。即：用专家建议的替代行动替换模型原行动，然后让模型继续执行下去，看最终是否真的成功了。只有真正导致成功的替代行动，才被认定为“已验证的关键步骤”。\n\n#### 第三阶段：数据构建策略的逆向思维\n*   **起点选择：** 传统的SFT从专家演示开始，但这容易导致分布偏移。\n*   **逆向思考：** 既然要提升模型，不如直接从**模型自己的失败轨迹**开始。这样能确保训练数据在模型的可及分布内，并直接针对其弱点。\n*   **配对逻辑：** 在失败轨迹中，利用上述机制找到“关键步骤”。将“专家建议的替代行动（导致成功）”作为正样本，将“模型原本的失败行动”作为负样本，构建偏好对。\n\n#### 第四阶段：整合为CSO方法论\n*   **综合：** 将上述思考整合为一个闭环流程：\n    1.  让模型跑任务，收集失败轨迹。\n    2.  用PRM定位潜在的错误决策点。\n    3.  用专家模型生成替代方案。\n    4.  用模型自身执行替代方案，验证是否成功（去伪存真）。\n    5.  仅在这些“已验证的关键步骤”上应用DPO（直接偏好优化）进行训练。\n*   **优势总结：** 这种方法既避免了轨迹级的粗糙归因，又通过结果验证消除了步骤级的噪声，同时因为只关注少量关键点，避免了全量采样的高昂成本。\n\n---\n\n### 总结\n\n作者的思考路径是从对现有SFT+RL范式的**不满**出发，识别出**全面监督的低效性**，进而引入**稀疏性**假设（只关注高熵/关键点）。为了解决关键点识别的准确性问题，创造性地提出了**“PRM筛选 + 结果验证”**的双重保险机制，最终形成了一套从失败中学习、精准打击薄弱环节的**CSO方法论**。"
                },
                {
                    "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research",
                    "arxiv_id": "2602.03318",
                    "authors": "Yifan Shi, Jialong Shi, Jiayi Wang, Ye Fan, Jianyong Sun",
                    "summary": "Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”与“自我演化”方向**： 论文的核心贡献是提出了 **MIRROR**，这是一个**多智能体框架**。这直接对应了研究焦点中的“多智能体”方向。此外，该框架集成了“执行驱动的迭代自适应修正”机制，这属于智能体的“自我修正”和“自我反思”能力，是Agentic AI以及自我演化研究中的重要组成部分。 2.  **属于构建新框架，而非单纯应用**： 虽然论文的应用领域是运筹学，但根据筛选标准的第一步和第四步，论文不仅仅是将现有的LLM或智能体作为工具应用，而是**构建了一个新的多智能体框架**来解决现有方法在协作错误修正和特定任务检索上的不足。其核心创新在于智能体系统的架构设计（分层检索、迭代修正），而非运筹学问题本身。 3.  **包含关键正面指标**： 论文明确涉及了多个核心关注点：`Multi-Agent Systems (MAS)`（多智能体系统）、`Self-Correction`（自我修正）、`Collaboration`（隐含在多智能体框架的协作中）以及`Iterative Improvement`（迭代改进）。 综上所述，尽管该论文应用于特定垂直领域，但其本质是提出了一种具备自我修正能力的多智能体架构，符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决运筹学建模依赖专家且现有LLM方法缺乏可靠纠错的问题。针对自然语言描述的优化问题，我们提出了一种名为MIRROR的无微调多智能体框架，集成了Iterative Adaptive Revision (IAR)和Hierarchical Retrieval-Augmented Generation (HRAG)机制。在NL4Opt、Mamo、IndustryOR和ComplexOR等数据集上，通过pass@1指标验证了其有效性，在复杂任务上达到SOTA。",
                    "summary_translation": "运筹学 (Operations Research, OR) 依赖于专家驱动的建模——这一过程缓慢且脆弱，不适合新颖的场景。虽然大语言模型 (Large Language Models, LLMs) 能够自动将自然语言转化为优化模型，但现有方法要么依赖昂贵的后训练，要么采用多智能体框架，然而大多数仍缺乏可靠的协同错误纠正和特定任务检索，往往导致输出错误。我们提出了 MIRROR，这是一个无需微调、端到端的多智能体框架，能够直接将自然语言优化问题转化为数学模型和求解器代码。MIRROR 集成了两个核心机制：(1) 用于自动错误纠正的执行驱动迭代自适应修正，以及 (2) 分层检索，用于从精心策划的示例库中检索相关的建模和编码示例。实验表明，MIRROR 在标准 OR 基准测试中优于现有方法，并在 IndustryOR 和 Mamo-ComplexLP 等复杂工业数据集上取得了显著成果。通过结合精确的外部知识注入与系统性错误纠正，MIRROR 为非专家用户提供了一种高效且可靠的 OR 建模解决方案，克服了通用大语言模型在专家优化任务中的根本局限性。",
                    "inspiration_trace": "基于对论文《MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research》的深度分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个层层递进的叙事逻辑，旨在揭示现有技术的痛点并引出本文的必要性：\n\n1.  **价值锚定**：首先确立运筹学（OR）在制造业、物流等领域的核心价值，指出其能显著提升效率和收益。\n2.  **现实瓶颈**：指出OR的实际应用面临根本性障碍——现实问题通常以非结构化的自然语言描述，而将其转化为严谨的数学模型和可执行代码需要深厚的领域专业知识。这一过程缓慢、脆弱且难以扩展，构成了极高的知识门槛。\n3.  **技术机遇**：引入大语言模型（LLMs）作为破局工具，指出其在自然语言理解、数学推理和代码生成方面的巨大潜力，似乎能解决上述“翻译”难题。\n4.  **现有路径的缺陷**：\n    *   **学习范式（如LLMOPT, ORLM）**：虽然通过微调提升了性能，但面临高质量标注数据稀缺且昂贵的挑战；同时，其“黑盒”特性使得输出难以验证和调试，一旦出错难以诊断。\n    *   **多智能体范式（如OptiMUS, ORMind）**：虽然通过分工协作避免了微调，但存在两大致命弱点：一是**封闭架构**导致缺乏外部领域知识，容易产生幻觉；二是**缺乏可靠的纠错机制**，难以有效利用求解器的反馈来修复执行后的错误，导致系统鲁棒性差。\n5.  **本文定位**：明确提出需要一个无需微调、能引入外部可靠知识、且具备执行驱动自我纠错能力的端到端多智能体框架。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“如何构建一个无需任务特定微调的自动化运筹优化建模框架，使其既能通过引入外部领域知识来抑制大模型的幻觉，又能基于求解器执行反馈进行可靠的自我纠错，从而实现从自然语言到可执行代码的高精度端到端转化？”**\n\n---\n\n### 三、 思想演进与逻辑链推演\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 观察与痛点识别\n*   **宏观观察**：运筹学（OR）建模是连接现实决策与数学求解的桥梁，但这座桥梁目前由“人类专家”把守，成为了普及化的最大瓶颈。\n*   **技术现状**：LLMs 虽然强大，但在专业领域（OR）直接使用时，要么需要昂贵的微调（成本高、难调试），要么在多智能体协作中因为缺乏外部知识而“胡编乱造”（幻觉），且写出的代码一旦报错就束手无策（缺乏纠错）。\n\n#### 2. 假设提出\n*   **核心假设**：如果我们不依赖模型内部预训练的知识（容易过时或幻觉），而是动态地检索高质量的外部案例来指导生成；同时，如果我们把代码执行结果作为反馈信号，让系统像程序员一样“报错-调试-再运行”，那么即使不进行微调，也能达到专家级的建模水平。\n\n#### 3. 机制设计\n为了验证上述假设，作者将问题拆解为两个核心子问题，并设计了对应机制：\n\n*   **子问题一：如何解决“幻觉”并注入专业知识？**\n    *   *思考*：现有的多智能体系统往往是“闭门造车”，仅依赖Prompt中的上下文。我们需要一个“图书馆”。\n    *   *演进*：简单的向量检索可能不够精准，因为OR问题既有宏观类别（如线性规划），又有微观结构（如运输问题）。\n    *   *方案*：提出 **分层检索增强生成（HRAG）**。先通过元数据进行粗粒度过滤（宏观），再通过语义相似度进行细粒度重排序（微观），确保给模型的例子是最相关、最高质量的。\n\n*   **子问题二：如何解决“执行失败”并实现自我修复？**\n    *   *思考*：代码生成很难一次成功。现有的系统往往生成完就结束了，或者只是简单地重试。我们需要一个“调试器”的角色。\n    *   *演进*：仅仅重试是不够的，必须诊断错误根源。是数学模型错了（逻辑错误），还是代码写错了（语法错误）？\n    *   *方案*：提出 **迭代自适应修订（IAR）**。当执行失败时，建模和代码生成智能体切换为“修订专家”，分析错误信息，结合历史记录（局部记忆），生成结构化的“修订提示”，然后迭代修改模型和代码，直到成功。\n\n#### 4. 系统架构整合\n*   **思考**：如何让这些机制协同工作，而不是孤立的模块？\n*   **演进**：需要一个记忆系统来串联整个过程。\n*   **方案**：引入 **双记忆池架构**。\n    *   *局部记忆*：存储当前任务的模型、代码和修订提示，为迭代纠错提供上下文。\n    *   *全局记忆*：跨任务积累知识，实现系统的长期进化。\n\n#### 5. 最终方法论形成\n*   **总结**：将上述思考整合为 **MIRROR** 框架。它是一个无需微调的多智能体系统，通过“分层检索”解决“懂不懂”的问题（知识准确性），通过“迭代修订”解决“对不对”的问题（代码鲁棒性），最终实现了从自然语言描述到可执行求解器代码的自动化闭环。"
                },
                {
                    "title": "One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence",
                    "arxiv_id": "2602.03109",
                    "authors": "Bowen Jiang, Taiwei Shi, Ryo Kamoi, Yuan Yuan, Camillo J. Taylor, Longqi Yang, Pei Zhou, Sihao Chen",
                    "summary": "This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断依据如下： 1.  **核心贡献符合 (第一步 - 保留)**: 论文的核心贡献是提出了 **OMAR (One Model, All Roles)** 框架。这是一个新的方法论和框架，旨在通过强化学习让 AI 智能体通过多轮、多智能体的自我博弈来发展社会智能。这直接对应了“构建、改进或演化 LLM智能体”的核心目标。 2.  **高度匹配多智能体与自我演化方向 (第二步 - 正面指标)**: *   **多智能体**: 论文明确涉及 **Multi-Agent Systems (MAS)**，研究智能体如何在对话中进行角色扮演、协作、通信以及社会学习。它探讨了在竞争场景（如 Werewolf 游戏）中如何学习协作，这完全属于多智能体协作与博弈的范畴。 *   **自我演化**: 论文采用了 **Self-Play**（自我博弈）机制，这是一种典型的通过环境反馈和自身经验进行迭代改进和自我完善的机制，符合“自我演化”的定义。 3.  **符合 Agentic AI 特征**: 论文关注智能体在长期对话中实现长期目标、适应复杂社会规范以及展现共情、说服等能力，这属于智能体在复杂环境中的自主规划与交互能力，而非单纯的 Token 预测或基础推理。 4.  **不涉及排除项 (第三步)**: 论文的主要贡献不是关于安全、对齐、多模态视觉或知识图谱，而是专注于智能体系统的架构和学习机制。 综上所述，该论文在多智能体协作和自我演化机制上做出了核心贡献，属于 Agentic AI 的前沿研究，因此予以保留。",
                    "summary2": "本文旨在让AI通过多轮、多智能体自我博弈发展社会智能。针对动态社交交互场景，我们提出了OMAR框架，利用单个模型同时扮演所有对话角色，并引入分层优势估计确保长对话训练稳定性。我们在SOTOPIA和Werewolf游戏上通过目标完成率、胜率及细粒度社会指标验证了其有效性，证明了模型能涌现出同理心、说服等复杂社交能力。",
                    "summary_translation": "本文介绍了 OMAR：One Model, All Roles（一个模型，所有角色），这是一种强化学习框架，旨在通过多轮、多智能体的对话自我博弈使人工智能（AI）发展出社会智能。与依赖静态、单轮优化的传统范式不同，OMAR 允许单个模型同时扮演对话中的所有参与者，直接从动态的社会互动中学习如何实现长期目标并掌握复杂的社会规范。为了确保在长对话过程中的训练稳定性，我们实施了一种分层优势估计方法，用于计算轮次级和 token（词元）级的优势。在 SOTOPIA 社会环境和 Werewolf（狼人杀）策略游戏中的评估表明，我们训练的模型发展出了细粒度的、涌现的社会智能，例如共情、说服和寻求妥协，这证明了即使在竞争场景下，学习协作策略的有效性。尽管我们识别出了诸如 reward hacking（奖励黑客）等实际挑战，但我们的结果表明，丰富的社会智能可以在没有人类监督的情况下涌现。我们希望这项工作能促进关于群组对话中 AI 社会智能的进一步研究。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《One Model, All Roles》这篇论文的系统性逻辑推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的逻辑链条，用于阐述研究的必要性：\n\n1.  **宏观趋势（AI角色的转变）：** AI 正从被动的辅助工具（处理语言、检索信息）向主动的社会参与者（协作、协调、社会贡献）转变。\n2.  **核心能力需求（社会智能）：** 为了在这种新角色中生存，AI 必须具备“社会智能”，即在复杂、动态的环境中，与拥有不同目标和人格的个体或群体进行沟通、合作和互动的能力。\n3.  **现状与缺陷（学习范式的滞后）：**\n    *   **人类学习方式：** 通过持续的对话和适应经验来发展社会智能。\n    *   **现有AI学习方式：**\n        *   **行为克隆：** 本质是静态的，仅模仿固定的演示，缺乏适应性。\n        *   **现有强化学习（RL）：** 核心是针对单轮优化的（针对可验证答案），而非多轮对话。它教模型生成目标响应，却无法让模型在多轮、多智能体的环境中动态参与并追求长期的社会目标。\n4.  **结论（研究空白）：** 现有的训练范式无法让 AI 系统像人类一样通过“经验”来学习社会互动。我们需要一个更通用的框架，让 AI 能从动态交互中学习。\n\n---\n\n### 2. 研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何构建一个通用的强化学习框架，使 AI 模型能够通过多轮、多智能体的对话自我博弈，从动态交互经验中自主发展出社会智能，而非依赖静态模仿或单轮优化？”**\n\n---\n\n### 3. 思考过程的逻辑演进\n\n以下是从观察到方法论形成的完整思维链条：\n\n#### 第一阶段：观察与痛点识别\n*   **观察：** 现有的 LLM 训练（如 SFT 或单轮 RL）虽然能提升推理能力，但在处理需要长期规划、理解对手意图、妥协和说服等复杂社会行为时表现不佳。\n*   **痛点：** 社会互动是动态的、多轮的，且涉及多个具有不同目标的参与者。现有的“单次问答”式训练无法捕捉这种时间维度上的策略演变。\n\n#### 第二阶段：理论假设与灵感来源\n*   **假设：** 社会智能应该像下棋一样，可以通过“经验”习得，而不仅仅是阅读说明书。\n*   **灵感：** AlphaGo 的**自我博弈**机制。通过自我对抗，AI 可以在没有人类监督的情况下发现新策略。\n*   **挑战：** 将自我博弈从围棋（封闭、确定性、离散动作空间）迁移到对话（开放、不确定性、巨大的 Token 动作空间、多智能体复杂性）极其困难。\n\n#### 第三阶段：方法论构建（核心创新点）\n为了解决上述挑战，作者进行了以下概念上的突破：\n\n*   **概念突破 1：从“多智能体”到“单模型多角色”**\n    *   *思考：* 训练多个不同的智能体模型太复杂，难以协调。\n    *   *创新：* 提出 **\"One Model, All Roles\"**。利用同一个模型扮演对话中的所有参与者。\n    *   *技术映射：* 借用 GRPO（Group Relative Policy Optimization）中生成 $n$ 个独立 Rollouts 的架构，将其重新定义为 $n$ 个对话参与者。每个 Batch 里的样本不再是独立的尝试，而是一场完整对话中的所有角色发言。\n\n*   **概念突破 2：从“单轮优化”到“多轮交互”**\n    *   *思考：* 对话是连续的，当前的发言依赖于之前的上下文。\n    *   *创新：* 构建动态的对话历史更新机制。$t$ 时刻所有角色的发言聚合为 $t+1$ 时刻的上下文，让模型在自我生成的对话流中学习。\n\n*   **概念突破 3：解决长序列训练的不稳定性**\n    *   *思考：* 直接将 PPO 用于长对话会导致方差过大，因为最终的奖励要回传给之前所有的 Token，导致训练不稳定。\n    *   *创新：* 提出 **分层优势估计**。\n        *   *逻辑：* 将长对话拆解。先在“轮次”层面计算优势（宏观策略是否正确），再将该轮次的优势作为伪奖励，在“Token”层面计算优势（微观措辞是否准确）。这种两级结构有效降低了长序列训练的方差。\n\n#### 第四阶段：验证与现象发现\n*   **实验设计：** 选择 SOTOPIA（目标导向社交环境）和 Werewolf（狼人杀，零和博弈）作为测试床。\n*   **预期结果：** 模型能完成特定目标。\n*   **意外发现（涌现）：** 作者发现模型不仅学会了完成任务，还涌现出了**细粒度的社会行为**（如妥协、同理心、战略承诺）。\n*   **深层洞察：** 即使在竞争环境（如讨价还价、狼人杀）中，为了最大化长期收益，模型也会自发学会“协作”行为（如为了达成交易而折中）。这证明了社会智能可以通过纯粹的自我博弈交互而产生，无需显式监督。\n\n---\n\n### 总结\n\n作者的思考路径是从**AI 社会化**的宏观愿景出发，批判了现有静态训练的局限性，引入了**自我博弈**的强化学习思想。为了克服对话场景的复杂性，他们创造性地将**多智能体系统简化为单模型的多角色扮演**，并发明了**分层优势估计**来解决长序列训练难题。最终，他们不仅验证了方法的有效性，还揭示了“竞争中的协作”这一社会智能的涌现规律。"
                },
                {
                    "title": "Test-time Recursive Thinking: Self-Improvement without External Feedback",
                    "arxiv_id": "2602.03094",
                    "authors": "Yufan Zhuang, Chandan Singh, Liyuan Liu, Yelong Shen, Dinghuai Zhang, Jingbo Shang, Jianfeng Gao, Weizhu Chen",
                    "summary": "Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，核心依据如下： 1.  **核心贡献属于“自我演化”范畴**： 论文提出了 \"Test-time Recursive Thinking (TRT)\"，明确将其定义为一个 \"iterative self-improvement framework\"（迭代自我改进框架）。这直接对应了我研究焦点中的 **\"自我演化\"** 方向，特别是其中的 `Self-Improvement` 和 `Iterative Improvement` 机制。论文的核心目标是在不进行额外训练的情况下，让模型通过自身机制实现性能提升。 2.  **具备显著的 Agentic 特征**： 尽管论文涉及推理任务，但其方法并非简单的提示词工程或微调，而是构建了一个具有智能体特征的循环机制。TRT 利用 \"rollout-specific strategies\"（规划）、\"accumulated knowledge\"（记忆）和 \"self-generated verification signals\"（自我反思/验证）来调节生成过程。这符合筛选标准中关于 **Agentic AI** 的定义，即涉及 `Planning`、`Memory` 和 `Self-Correction/Reflection`。 3.  **符合“推理/规划”的特殊保留规则**： 根据第四步的规则，如果论文是关于智能体如何在复杂任务中进行多步推理或提出新的 Agentic 框架（如 ReAct, ToT），应当保留。TRT 提出了一种新的测试时计算框架，通过递归和迭代来解决问题，属于智能体规划与推理的高级框架，而非单纯提升模型基础 Token 预测能力的数学或逻辑研究。 综上所述，该论文的核心在于提出一种让 LLM 在测试时通过自我反思和迭代实现自我演化的新框架，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决大语言模型在无外部监督下实现测试时自我改进的问题。针对数学推理和代码生成场景，我们提出了一种Test-time Recursive Thinking (TRT)框架，通过基于累积知识的策略生成、自评估选择及反思更新来迭代优化推理过程。在AIME-25/24和LiveCodeBench v6数据集上，通过准确率等指标验证了其有效性，开源模型在AIME上达到100%准确率，闭源模型在代码难题上提升显著。",
                    "summary_translation": "现代大型语言模型在推理能力方面取得了显著进步，这主要归功于基于可验证奖励的强化学习。在此，我们探究这些 LLMs 是否能够在无需额外训练的情况下实现自我改进。我们指出了此类系统面临的两个核心挑战：(i) 高效生成多样化且高质量的候选解决方案，以及 (ii) 在缺乏真实值监督的情况下可靠地选择正确答案。为应对这些挑战，我们提出了测试时递归思考，这是一种迭代的自我改进框架，其生成过程基于特定展开策略、累积知识和自生成的验证信号进行条件化处理。应用 TRT 后，开源模型在 AIME-25/24 上达到了 100% 的准确率；而在 LiveCodeBench 最具挑战性的问题上，闭源模型在无需外部反馈的情况下提升了 10.4-14.8 个百分点。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《Test-time Recursive Thinking》这篇论文的系统性逻辑推演和思考过程还原。\n\n### 一、 Introduction 中的“讲故事”逻辑链\n\n作者在引言部分通过层层递进的叙事逻辑，将研究背景推向核心问题：\n\n1.  **现状观察**：现代大语言模型（LLMs）的推理能力大幅提升，这主要归功于**基于可验证奖励的强化学习（RL）**（如 DeepSeek, OpenAI o1 等范式）。\n2.  **提出挑战**：这些成功的范式高度依赖**外部监督**（即 Ground-truth rewards）。\n3.  **核心设问**：一个关键的问题随之而来——**在没有额外训练且无法获取真实奖励的情况下，模型能否在测试时实现自我改进？**\n4.  **审视现有方案**：\n    *   **第一类（Meta-RL）**：虽然有效，但需要昂贵的权重更新和复杂的奖励校准。\n    *   **第二类（推理时自改进）**：虽然不需要更新权重，但往往缺乏一种**递归机制**，无法有效地将学到的改进在多次尝试中向前传递。\n5.  **洞察痛点**：作者指出，有效的测试时自改进必须解决两个互补的挑战：\n    *   **战略性探索**（以扩展解空间）；\n    *   **自引导验证**（在没有真值的情况下选择候选解）。\n    *   *逻辑推论*：仅有探索会产生噪声，仅有验证会导致停滞。\n6.  **引出方案**：为了解决上述挑战，作者提出了 **Test-time Recursive Thinking (TRT)**，旨在通过区分强解和弱解，提取可操作的失败模式，并重用这些知识来指导后续尝试。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“大语言模型能否在不依赖外部反馈（如Ground-truth奖励）的情况下，通过在测试时递归地利用自身的尝试结果，实现推理能力的持续自我改进？”**\n\n---\n\n### 三、 思想演进与逻辑推演过程\n\n以下是从宏观问题出发，逐步聚焦到方法论形成的完整思考脉络：\n\n#### 第一阶段：观察与瓶颈识别\n*   **观察**：目前的 SOTA 推理模型（如 o1, R1）本质上是在训练阶段通过“外部反馈”（如代码执行结果、数学答案验证）来强化模型。\n*   **瓶颈**：这种模式严重依赖“外部裁判”。如果到了测试阶段，没有裁判了，模型是不是就“傻”了？现有的推理时方法（如 Self-Consistency）只是简单的投票，并没有“记忆”或“学习”的过程，容易重复犯错。\n\n#### 第二阶段：假设提出\n*   **假设**：即使没有外部裁判告诉模型“答案是对的”，模型自身也应该具备区分“哪个解法更好”的能力（通过逻辑自洽性、代码自测等）。\n*   **推论**：如果模型能区分好坏，它就能通过对比“好解”和“坏解”，总结出“坏解为什么坏”（即提取失败模式），从而在下一次尝试中避开这些坑。\n\n#### 第三阶段：核心矛盾分析\n*   **矛盾**：要在没有外部反馈的情况下自改进，面临一个两难：\n    *   要探索新路径（否则无法找到答案）；\n    *   要验证路径（否则不知道方向对不对）。\n*   **现有方法的缺陷**：\n    *   并行采样（如 Self-Consistency）：各路径互不知晓，无法传递经验，导致重复错误。\n    *   简单的迭代反思：往往缺乏结构化的知识积累，容易陷入局部最优或遗忘之前的教训。\n\n#### 第四阶段：方法论构建\n*   **关键洞察**：必须建立一个**递归循环**，让“知识”在尝试之间流动。\n*   **设计思路**：\n    1.  **知识表征**：不要存具体的解题步骤（太占空间且不通用），要存“负面约束”（Don'ts），即“什么做法是错的”。\n    2.  **策略引导**：为了避免盲目探索，每次生成新解时，都要根据之前的失败经验，设计不同的“策略”（如数学中用代数法还是几何法，代码中用DP还是贪心）。\n    3.  **自我验证**：利用领域特性进行无监督筛选（数学利用答案互斥性，代码利用自生成测试用例）。\n\n#### 第五阶段：框架定型\n*   **最终框架 (TRT)**：将上述思考固化为三个步骤的循环：\n    1.  **生成**：基于累积的知识和特定的策略生成多个候选解。\n    2.  **选择**：通过自我判断选出当前最好的解。\n    3.  **反思**：对比“最好解”与“其他解”，提取失败原因，更新知识库，用于指导下一轮生成。\n\n#### 第六阶段：验证与修正\n*   **预判**：这种方法在数学（答案唯一）上应该效果极好，在代码（多解）上需要更强的验证机制。\n*   **实验设计**：在 AIME 上验证“互斥性”带来的收敛，在 LiveCodeBench 上验证“自生成测试”带来的筛选能力。\n*   **结果确认**：实验证明，通过这种递归思考，模型确实能在不改变权重的情况下，随着轮次增加单调提升性能，证明了“测试时学习”的可行性。"
                },
                {
                    "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback",
                    "arxiv_id": "2602.03084",
                    "authors": "Zhitao Gao, Jie Ma, Xuhong Li, Pengyu Li, Ning Qu, Yaqiang Wu, Hui Liu, Jun Liu",
                    "summary": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”方向的核心论文。 1.  **核心贡献符合“自我演化”定义**: 论文提出了 AERO (Autonomous Evolutionary Reasoning Optimization)，这是一个旨在实现“自主推理演化”的无监督框架。其核心机制是通过内部化的自我提问、回答和批评，构建了一个协同的双环系统。这直接对应了您筛选标准中“自我演化”的定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。 2.  **包含核心正面指标**: *   **演化机制**: 论文明确涉及 `Self-Evolving`（自主演化）、`Self-Correction`（独立反事实修正）和 `Iterative Improvement`（交错训练策略）。 *   **智能体能力**: 摘要中提到的“self-questioning, answering, and criticism”（自我提问、回答和批评）属于典型的 `Self-Reflection`（自我反思）和 `Self-Correction` 能力。 3.  **通过特殊情况的判定**: 虽然论文的目标是提高LLM的“推理能力”，这通常容易被归类为“非Agentic的推理”而被排除。但是，根据您的第四步规则，如果论文的核心是提出一种新的“自我演化”机制，即使它被应用在推理任务上，也应该保留。AERO 的核心在于提出了一种基于“最近发展区（ZPD）”理论和“双环反馈”的演化方法论，而不仅仅是简单的提示工程或数据集构建。它通过模拟智能体的自我反思过程来驱动模型的迭代更新，符合您对“构建、改进或演化 LLM智能体”的要求。 综上所述，该论文提出了一种新颖的自我演化框架，属于 Agentic AI 中的 Self-Evolving 范畴，因此予以保留。",
                    "summary2": "本文旨在解决LLM依赖外部数据和验证器，以及现有自进化方法难以定位最优学习区且易强化幻觉的问题。针对无监督自进化场景，我们提出了一种AERO框架，该框架通过内源性双环反馈系统，结合熵驱动的ZPD定位和独立反事实修正（ICC）实现自主推理进化。我们在涵盖数学、物理和通用推理的九个基准测试上，通过准确率指标验证了其有效性，显著优于竞争基线。",
                    "summary_translation": "Large Language Models (LLMs, 大语言模型) 在复杂推理方面取得了显著成功，但仍然受限于对专家标注数据和外部验证器的依赖。尽管现有的 self-evolution paradigms (自我进化范式) 旨在绕过这些限制，但它们往往无法识别 optimal learning zone (最佳学习区)，且存在通过有缺陷的内部反馈强化 collective hallucinations (集体幻觉) 和 incorrect priors (错误先验) 的风险。为了解决这些挑战，我们提出了 Autonomous Evolutionary Reasoning Optimization (AERO, 自主进化推理优化)，这是一个 unsupervised framework (无监督框架)，通过在 synergistic dual-loop system (协同双循环系统) 中内化自问、自答和批评机制来实现自主推理进化。受 Zone of Proximal Development (ZPD, 最近发展区) 理论的启发，AERO 利用 entropy-based positioning (基于熵的定位) 方法针对“solvability gap (可解性差距)”，并采用 Independent Counterfactual Correction (独立反事实校正) 进行鲁棒验证。此外，我们引入了 Staggered Training Strategy (交错训练策略)，以同步不同功能角色的能力增长，并防止 curriculum collapse (课程崩溃)。在跨越三个领域的九个 benchmarks (基准测试) 中进行的广泛评估表明，AERO 在 Qwen3-4B-Base 上实现了 4.57% 的平均性能提升，在 Qwen3-8B-Base 上实现了 5.10% 的平均性能提升，优于竞争性 baselines (基线)。代码可在 https://github.com/mira-ai-lab/AERO 获取。",
                    "inspiration_trace": "基于对论文《AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback》的深度分析，以下是作者产出该核心方法的逻辑推演过程：\n\n### 第一阶段：宏观观察与瓶颈识别\n**逻辑起点：** 大语言模型（LLMs）在复杂推理任务上表现出色，但其进步高度依赖“外部拐杖”。\n**演进思考：**\n1.  **现状：** 现有的强化学习（如RLVR）虽然有效，但必须依赖专家标注的查询和外部验证器（如代码编译器、数学引擎）。\n2.  **瓶颈：** 这种依赖性导致模型的能力上限被“锁死”在预设数据和验证器的范围内，无法突破现有知识边界。\n3.  **方向：** 为了超越人类知识，必须从“被动接受”转向“主动进化”，即**Self-Evolution（自进化）**范式——让模型从自身生成的数据和经验中迭代学习。\n\n---\n\n### 第二阶段：深入剖析与“讲故事”逻辑（问题提取）\n作者在Introduction中通过层层递进的方式，指出了自进化范式虽然方向正确，但在实际操作中存在两个致命缺陷。这是文章提出新方法的核心动因。\n\n**逻辑链条：**\n1.  **引入冲突：** 虽然自进化试图摆脱外部数据，但现有机制往往导致模型陷入**次优学习区**。\n2.  **具体表现A（难度失配）：** 模型无法精准调节任务难度，错过了“可解性缺口”。\n    *   *后果：* 任务太简单（重复已知，无收益）或太难（产生随机噪声，无法学习），导致学习效率极低。\n3.  **具体表现B（反馈失效）：** 为了替代外部验证器，现有方法依赖内部指标（如多数投票、解码置信度）。\n    *   *假设谬误：* 这些方法默认“共识”或“高概率”等于“逻辑正确”。\n    *   *后果：* 当模型持有错误信念时，这些指标反而会强化**集体幻觉**和**错误先验**，使模型陷入自我确认的谬误循环，背离逻辑真理。\n\n---\n\n### 第三阶段：核心研究问题\n基于上述对现状和缺陷的剖析，作者试图回答的核心问题为：\n\n**“如何构建一个完全无监督的框架，使大模型能够自主进化其推理能力，同时确保其能精准定位最优学习难度区间，并避免在缺乏外部监督的情况下强化自身的错误？”**\n\n---\n\n### 第四阶段：思想演进与方法论构建\n为了解决上述问题，作者的思想经历了从理论借鉴到机制设计的演进：\n\n#### 1. 解决“难度失配”：从心理学到信息论\n*   **理论灵感：** 借鉴维果茨基的**“最近发展区”（ZPD）**理论。认知发展最大化发生在任务难度略高于学习者当前能力的区域。\n*   **量化难题：** 如何让模型感知这个“区域”？\n*   **解决方案：** 引入**信息熵**。\n    *   *思考：* 如果模型对一个问题完全确定（熵低），说明它已经掌握了（掌握区）；如果模型完全混乱（熵高），说明太难（混乱区）。\n    *   *机制：* 利用**归一化香农熵**来量化推理的不确定性。将“适度不确定性”定义为最优学习区，指导模型自主生成处于“可解性缺口”的任务。\n\n#### 2. 解决“反馈失效”：从统计共识到逻辑收敛\n*   **反思：** 既然“多数投票”不可靠（因为大家可能一起错），那么什么才是可靠的真理代理？\n*   **核心洞察：** 真正的逻辑正确性应该经得起“反事实”的推敲。\n*   **解决方案：** 提出**独立反事实修正（ICC）**。\n    *   *机制：* 强迫模型在“假设之前的解是错误的”前提下，重新构建推理路径。\n    *   *验证：* 如果两条独立的修正路径最终收敛到同一个答案，那么这个答案极大概率是正确的。这利用了逻辑一致性而非统计一致性来提供高可靠性的反馈。\n\n#### 3. 解决“系统协同”：从单角色到双循环\n*   **架构设计：** 为了实现上述功能，单一模型需要内化三种协同能力：**自提问**、**自回答**、**自批评**。\n*   **系统构建：** 设计**内-外双循环系统**。\n    *   *内循环：* 作为“自我博弈沙盒”，合成经验（生成任务、求解、通过ICC验证）。\n    *   *外循环：* 利用合成经验进行策略优化。\n\n#### 4. 解决“进化稳定性”：从同步训练到交错策略\n*   **潜在风险：** 作者预见到“课程崩溃”的风险。如果“解题者”进化得比“出题者”快，下一轮生成的题目对更新后的模型来说就太简单了，导致学习梯度消失。\n*   **解决方案：** 提出**交错训练策略**。\n    *   *机制：* 在时间上解耦数据流。在训练第 $t$ 轮时，使用第 $t$ 轮生成的“题目”（最新难度），但配合第 $t-1$ 轮的“答案和批评”（历史能力）。\n    *   *目的：* 确保题目始终对模型具有挑战性，维持进化的压力和稳定性。\n\n---\n\n### 总结：逻辑链全景\n1.  **观察：** LLMs受限于外部监督，需要自进化。\n2.  **痛点：** 自进化面临“找不到合适难度”和“容易自我欺骗”两大难题。\n3.  **破局：**\n    *   用**熵**来量化并定位最佳学习区（ZPD）。\n    *   用**反事实修正（ICC）**替代投票，通过逻辑一致性验证真理。\n    *   用**交错训练**维持题目与能力的动态平衡。\n4.  **产出：** AERO框架——一个无需外部数据、通过内源性双循环反馈实现自主推理进化的系统。"
                },
                {
                    "title": "LatentMem: Customizing Latent Memory for Multi-Agent Systems",
                    "arxiv_id": "2602.03036",
                    "authors": "Muxin Fu, Guibin Zhang, Xiangyuan Xue, Yafu Li, Zefeng He, Siyuan Huang, Xiaoye Qu, Yu Cheng, Yang Yang",
                    "summary": "Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **LatentMem**，这是一个专门为 **多智能体系统** 设计的可学习记忆框架。 *   它旨在解决现有MAS中存在的“记忆同质化”和“信息过载”问题，这属于 **构建和改进 LLM 智能体** 的方法论研究，而非简单的应用或基础设施研究。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确聚焦于 `Multi-Agent Systems (MAS)`。 *   **智能体能力**: 论文的核心创新点在于改进智能体的 `Memory` 机制，提出了“潜在记忆”和“记忆合成器”，以实现更高效的 Token 使用和角色感知的定制化记忆。 *   **多智能体**: 研究背景是基于 LLM 驱动的多智能体系统，关注智能体间的交互轨迹存储和检索。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **结论**: 该论文通过引入新的记忆架构和优化策略（LMPO），直接提升了多智能体系统的性能和适应性，是对 LLM 智能体核心组件（记忆）的改进，因此符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决多智能体系统（MAS）中记忆同质化和信息过载的问题。针对长上下文交互和角色异构的场景，我们提出了一种名为LatentMem的可学习潜在记忆框架，包含经验库和记忆合成器，并引入Latent Memory Policy Optimization (LMPO) 进行端到端优化。在六个基准测试及四个主流MAS框架上，通过任务准确率等指标验证了其有效性，实现了最高19.36%的性能提升。",
                    "summary_translation": "由大语言模型 (Large Language Model, LLM) 驱动的多智能体系统 (Multi-Agent Systems, MAS) 展现出了卓越的集体智能，其中多智能体记忆 (multi-agent memory) 是实现持续适应的关键机制。然而，现有的多智能体记忆设计仍受限于两个基本瓶颈：(i) 因缺乏角色感知定制 (role-aware customization) 而产生的记忆同质化 (memory homogenization)，以及 (ii) 由过度细粒度的记忆条目引发的信息过载 (information overload)。为解决上述局限性，我们提出了 LatentMem，这是一个可学习的多智能体记忆框架，旨在以令牌高效 (token-efficient) 的方式定制针对特定智能体的记忆。具体而言，LatentMem 包含一个经验库 (experience bank)，用于以轻量级形式存储原始交互轨迹 (raw interaction trajectories)，以及一个记忆合成器 (memory composer)，用于根据检索到的经验和特定智能体的上下文合成紧凑的潜在记忆 (latent memories)。此外，我们引入了潜在记忆策略优化 (Latent Memory Policy Optimization, LMPO)，该方法通过潜在记忆将任务级优化信号传播至合成器，从而激励其生成紧凑且高效用的表示。在多样化基准测试和主流多智能体系统 (MAS) 框架上的广泛实验表明，LatentMem 相较于原始设置 (vanilla settings) 实现了高达 $19.36$% 的性能提升，并且始终优于现有的记忆架构，且无需对底层框架进行任何修改。",
                    "inspiration_trace": "基于对论文《LatentMem: Customizing Latent Memory for Multi-Agent Systems》的深度分析，以下是作者产出该核心方法的逻辑链推演与思考过程还原。\n\n---\n\n### 一、 宏观背景与问题引入\n\n作者首先构建了一个宏观的技术背景，随后通过“讲故事”的方式揭示了现有技术范式中的深层矛盾。\n\n**1. 背景铺垫：MAS 的崛起与记忆的核心地位**\n*   **观察**：基于大语言模型的多智能体系统（MAS）已成为解决复杂任务（如协作、竞争）的强大框架。\n*   **共识**：多智能体记忆是这一框架中的关键机制。它使得智能体能够通过交互积累、保留和重用经验，从而支持连贯的协调和持续的适应。\n*   **现状**：为了捕捉不同抽象层次的经验，现有的研究已经构建了多粒度的记忆库（如轨迹、语义洞察、技能模式等），试图让系统更全面地“记住”过去。\n\n**2. 矛盾揭示：繁荣背后的两大瓶颈**\n尽管记忆系统变得越来越复杂和精细，作者指出这种“堆砌复杂性”的路径遭遇了两个根本性的物理限制，构成了文章的核心冲突：\n\n*   **冲突一：记忆同质化**\n    *   **现象**：现有方法大多采用“一刀切”的策略，忽略了智能体功能的异质性（即不同智能体有不同的角色，如编码员、测试员、经理）。\n    *   **后果**：这种无差别的记忆设计破坏了角色的一致性，放大了相关错误，削弱了系统的鲁棒性，阻碍了长期的适应性。简单来说，大家都在读同一本“通用书”，而不是各自需要的“专业手册”。\n\n*   **冲突二：信息过载**\n    *   **现象**：MAS 本身涉及长交互上下文，而多粒度的记忆设计进一步引入了海量的存储条目。\n    *   **后果**：过量的信息淹没了智能体，模糊了关键的决策信号。就像给一个人塞进了一整个图书馆的书，让他瞬间找到答案，结果反而导致处理瘫痪。\n\n---\n\n### 二、 核心研究问题\n\n基于上述背景与冲突，作者将复杂的工程问题凝练为一个明确的科学问题：\n\n> **“鉴于多智能体系统中存在长而复杂的上下文，我们能否设计一种既具有角色感知能力，又在 Token 使用上高效的可学习记忆，且无需大量的人工工程？”**\n\n---\n\n### 三、 思想演进与方法论形成\n\n为了回答上述问题，作者的思考经历了从“解构问题”到“提出假设”再到“构建系统”的逻辑演进。\n\n#### 1. 思想起点：从“文本空间”转向“潜在空间”\n*   **反思**：现有的记忆大多以离散的文本形式存在（如摘要、日志），这直接导致了“信息过载”且难以针对特定角色进行定制。\n*   **假设**：如果我们将记忆从离散的文本空间转移到连续的**潜在空间**，用固定长度的向量来表示记忆，就能天然地解决 Token 效率问题（压缩信息），并为后续的个性化处理提供数学基础。\n\n#### 2. 核心机制：引入“角色感知”的定制化\n*   **反思**：为了解决“记忆同质化”，记忆的生成过程必须考虑“谁在使用它”。\n*   **假设**：如果我们在生成记忆时，显式地输入**智能体的角色画像**，那么生成的记忆就会自动包含该角色所需的信息，从而实现“千人千面”的记忆定制。\n\n#### 3. 架构设计：存储与计算的解耦\n基于上述两个假设，作者提出了一个解耦的架构设计，而非传统的端到端文本检索：\n*   **存储端（轻量化）**：建立一个**经验库**，只存储最原始的交互轨迹。不做任何人工提炼或摘要，保持数据的原始性和轻量化。\n*   **计算端（智能化）**：设计一个**记忆合成器**。它不直接读取原始文本，而是接收“检索到的原始轨迹”和“当前智能体的角色画像”，将其合成为紧凑的潜在记忆向量。\n*   **逻辑闭环**：原始数据是通用的，但经过合成器处理后，输出的记忆是专属的。\n\n#### 4. 优化策略：从任务反馈中学习\n*   **挑战**：如何保证合成器生成的潜在记忆是“有用”的？传统的监督学习很难定义什么是“好的记忆向量”。\n*   **解决方案**：作者提出**潜在记忆策略优化（LMPO）**。\n*   **逻辑**：利用任务层面的奖励信号作为反馈。因为潜在记忆是可微的，梯度可以通过记忆反向传播到合成器中。这样，合成器就会学习到：“只有当我生成的记忆能帮助智能体获得更高奖励时，我的参数才是正确的”。这是一种自底向上的、基于结果导向的记忆进化。\n\n---\n\n### 四、 总结：逻辑链全景\n\n1.  **观察**：MAS 需要记忆，但现有记忆太通用（同质化）且太啰嗦（过载）。\n2.  **提问**：能否做一个既懂角色又省空间的记忆？\n3.  **假设**：用向量代替文本（省空间），用角色画像做条件（懂角色）。\n4.  **设计**：把“存原始数据”和“生成个性化向量”分开，分别用经验库和记忆合成器实现。\n5.  **验证**：用强化学习（LMPO）直接根据任务成败来训练记忆合成器，确保生成的向量真的有用。"
                },
                {
                    "title": "CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning",
                    "arxiv_id": "2602.02979",
                    "authors": "Ran Li, Zeyuan Liu, Yinghao chen, Bingxiang He, Jiarui Yuan, Zixuan Fu, Weize Chen, Jinyi Hu, Zhiyuan Liu, Maosong Sun",
                    "summary": "Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献属于“自我演化”与“多智能体”范畴**： 论文提出了 CPMobius，这是一种基于“Coach-Player”范式的协作框架。其核心机制是让两个角色（Coach 和 Player）在协作优化循环中交互：Coach 生成指令，Player 解决问题并根据反馈进行改进。这完全符合筛选标准中关于“自我演化”的定义（智能体通过经验、反思或环境反馈进行自我完善和迭代），同时也符合“多智能体”中的协作与通信机制。 2.  **符合“自我演化的应用”例外规则**： 虽然论文的实验目标是提升模型的“数学推理能力”，这通常可能被归类为基础推理能力提升而被排除。但是，根据筛选标准第四步第2点：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。CPMobius 的核心价值在于提出了一种无需外部数据、通过多智能体协作实现自我演化的新训练范式，而不仅仅是应用现有的提示工程技巧。 3.  **非简单的非Agentic推理或应用**： 该论文不是简单地应用 LLM 解决数学问题，也不是单纯通过增加数据集来微调模型。它构建了一个具有明确角色分工和反馈循环的 Agentic 框架来实现模型的自我进化，这超越了单纯的“非Agentic的推理”范畴。 综上所述，该论文在构建多智能体协作框架和实现模型自我演化方面具有显著贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决大语言模型推理训练对大量人工标注数据的依赖问题。针对数据匮乏的场景，我们提出了一种名为CPMobius的协作式Coach-Player推理框架。该方法通过Coach生成适应Player能力的课程任务，Player通过解决任务提升推理能力，两者通过协作优化循环共同进化。我们在Qwen2.5-Math等模型上，通过AMC、AIME、MATH等数学推理基准测试，验证了其在准确率上的显著提升，优于现有无监督方法。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 在复杂推理方面展现了巨大潜力，但其发展仍根本性地受限于对大量高质量人工构建任务和标签的依赖，无论是通过监督微调 (Supervised Fine-Tuning, SFT) 还是在推理专用数据上进行强化学习 (Reinforcement Learning, RL)。这种依赖使得重度依赖监督的训练范式日益难以为继，且在实践中已显露出扩展性边际递减的迹象。为克服这一局限，我们提出了 CPMöbius (CPMobius)，这是一种用于推理模型免数据强化学习 (Data-Free Reinforcement Learning) 的协作式 Coach-Player（教练-玩家）范式。与传统的对抗性自我博弈 (Adversarial Self-Play) 不同，CPMobius 受现实世界人类体育协作和多智能体协作 (Multi-Agent Collaboration) 的启发，将 Coach 和 Player 视为独立但协作的角色。Coach 提出针对 Player 能力的指令 (Instructions)，并根据 Player 性能的变化获得奖励 (Rewards)，而 Player 则因解决 Coach 生成的难度逐渐增加的指导性任务而获得奖励。这种协作优化循环 (Cooperative Optimization Loop) 旨在直接增强 Player 的数学推理能力。值得注意的是，CPMobius 在不依赖任何外部训练数据的情况下实现了显著提升，性能超越了现有的无监督方法 (Unsupervised Approaches)。例如，在 Qwen2.5-Math-7B-Instruct 模型上，我们的方法将准确率总体平均提高了 +4.9，分布外 (Out-of-Distribution, OOD) 平均提高了 +5.4，在总体准确率上超过 RENT +1.5，在 OOD 准确率上超过 R-zero +4.2。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "From Task Solving to Robust Real-World Adaptation in LLM Agents",
                    "arxiv_id": "2602.02760",
                    "authors": "Pouya Pezeshkpour, Estevam Hruschka",
                    "summary": "Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a \"clean interface\" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围，具体判断依据如下： 1.  **核心贡献符合第一步标准**: *   论文的核心并非将LLM作为工具应用于特定垂直领域（如医疗、金融），而是聚焦于**LLM智能体本身的能力评估与改进**。 *   它提出了一个新的评估框架，旨在测试智能体在现实世界复杂条件下的**鲁棒性**和**适应性**。这直接对应了“构建、改进或演化 LLM智能体”的目标，特别是针对现有智能体在非理想环境下表现不足的问题进行剖析。 2.  **高度契合第二步正面指标**: *   **Agentic AI**: 论文明确研究的是作为智能体部署的LLM，涉及规划、调用工具和长期执行。 *   **自我演化/适应**: 论文标题和摘要中反复强调 \"Adaptation\"（适应）和 \"Dynamic environments\"（动态环境）。它研究智能体如何在环境变化、信号不可靠的情况下调整策略，这属于智能体通过环境反馈进行自我完善和适应的范畴，与您的“自我演化”方向高度相关。 *   **智能体能力**: 涉及规划、长期执行以及在部分可观测性下的决策。 3.  **不触犯第三步排除标准**: *   论文虽然提到了 \"safe action selection\"（安全行动选择），但其主要贡献**不是**关于AI安全、对齐或防止幻觉，而是关于智能体在不确定性环境下的任务完成能力和鲁棒性。 *   不涉及多模态视觉或图技术。 4.  **符合第四步特殊处理规则**: *   论文关注的是智能体在复杂任务中的多步推理和适应机制，而非单纯的数学或逻辑推理能力提升。 *   它通过揭示现有智能体在“名义任务解决”与“部署类鲁棒性”之间的差距，为未来构建更具适应性的智能体指明了方向，这属于对智能体框架和能力的实质性研究。 **总结**: 该论文深入探讨了LLM智能体在接近真实世界的动态环境中的适应性问题，揭示了智能体在面对环境变化和噪声时的行为模式，对于理解如何构建更具鲁棒性和演化能力的Agentic AI具有重要价值，因此予以保留。",
                    "summary2": "本文旨在评估LLM智能体在真实世界部署中的鲁棒性，解决现有评估假设过于理想化的问题。针对部分可观测性、动态环境、噪声信号和动态智能体状态等真实场景，我们提出了一种基于网格游戏的基准测试，并在该环境中通过成功率、分数和步数等指标验证了五种最先进LLM智能体的有效性。",
                    "summary_translation": "大语言模型越来越多地被部署为专门的智能体，这些智能体能够进行规划、调用工具并在长跨度内采取行动。然而，许多现有的评估假设存在一种“干净接口”，其中动态是明确且稳定的，工具和传感器是可靠的，并且成功由单一显式目标所定义——这往往高估了现实世界的就绪度。在实践中，智能体面临规则定义不足、信号不可靠、环境变化以及隐含的、涉及多方利益相关者的目标。因此，挑战不仅在于解决任务，还在于在解决问题的过程中进行适应：决定信任什么、想要什么、何时验证，以及何时回退或升级。我们在四种操作环境下对与部署相关的鲁棒性进行了压力测试：部分可观测性、动态环境、噪声信号和动态智能体状态。我们在一个目标简单但执行跨度较长的基于网格的游戏中对智能体大语言模型进行了基准测试。这些回合违反了“干净接口”的假设，但仍然是可解的，迫使智能体推断规则、为信息付费、适应环境和内部变化，并在噪声下谨慎行动。在五种最先进的大语言模型智能体中，我们发现名义上的任务解决能力与类部署鲁棒性之间存在巨大差距。随着网格大小和跨度的增加，性能通常会下降，但排名是不稳定的：当策略与不确定性机制相匹配时，较弱的模型可以击败较强的模型。尽管没有明确的指令，智能体仍在完成度、效率和避免惩罚之间进行权衡，这表明它们具有部分目标推断能力。消融实验和特征分析揭示了特定于模型的敏感性和失败驱动因素，这推动了在部分可观测性、噪声和非平稳性条件下关于验证、安全行动选择和目标推断的研究工作。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出该文章核心方法的逻辑链推演，重点还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从“现状繁荣”到“现实落差”的叙事逻辑，具体步骤如下：\n\n1.  **现状观察**：大语言模型（LLM）正从被动的任务解决者演变为具备规划、工具调用和长周期行动能力的智能体。\n2.  **揭示假设**：现有的评估基准大多依赖于两个简化的假设——环境是充分指定且稳定的，目标是显式且可简化的（即“清洁接口”假设）。\n3.  **现实冲突**：在实际部署中，上述假设往往失效。现实世界充满了规则未指定、信号不可靠、环境动态变化以及目标隐含且涉及多方利益的情况。\n4.  **核心矛盾**：目前的评估高估了智能体的实战就绪度。真正的挑战不仅仅是“完成任务”，而是“在解决问题的同时进行适应”。\n5.  **提出挑战**：作者指出了四个被现有基准低估的关键现实环境因素：部分可观测性、动态环境、噪声信号和动态智能体状态。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式提出的研究问题是：\n\n**“在解决问题的过程中，智能体能够多么稳健地推断、适应并与现实世界进行安全交互？”**\n\n---\n\n### 三、 核心方法产出的逻辑链推演\n\n以下是从宏观问题出发，逐步聚焦并形成最终方法论的思考演进过程：\n\n#### 1. 宏观观察与质疑\n*   **思考起点**：LLM 智能体在特定任务上表现强劲，但这是否意味着它们已经准备好在真实世界中部署？\n*   **初步判断**：现有的测试环境太“干净”了。就像在无菌实验室里测试机器人，无法预判它在野外丛林中的生存能力。我们需要区分“解题能力”和“适应能力”。\n\n#### 2. 问题聚焦：定义“真实世界的混乱”\n*   **思考**：真实世界的“乱”具体体现在哪里？不能只是笼统地说“很难”，必须解构为具体的压力源。\n*   **抽象提取**：作者将现实世界的复杂性归纳为四个维度：\n    *   **信息不全**：看不见全局，规则是隐藏的（部分可观测性）。\n    *   **环境多变**：计划赶不上变化，环境本身在变（动态环境）。\n    *   **感知不准**：传感器会骗人，动作会失败（噪声信号）。\n    *   **自身漂移**：智能体自己的能力或状态在任务中途发生了变化（动态智能体状态）。\n\n#### 3. 方法论构思：从“更难的题”转向“更乱的接口”\n*   **思考**：为了测试上述能力，我不应该设计一个更复杂的数学题或编程题，因为那还是在测试逻辑推理。我需要设计一个环境，专门用来破坏“清洁接口”的假设。\n*   **设计理念**：构建一个**压力测试基准**。这个基准的目标不是考察智能体是否聪明，而是考察它是否**稳健**。\n\n#### 4. 具体载体选择：网格游戏\n*   **思考**：什么样的环境既能模拟现实世界的复杂性，又具备可控性和可解释性？\n*   **决策**：选择**网格游戏**。\n    *   *理由*：它足够简单（规则离散、状态有限），便于分析失败原因；但又足够灵活，可以人为注入各种“混乱”因素。相比于复杂的网页浏览或真实机器人，它能更纯粹地隔离出“适应力”这一变量。\n\n#### 5. 机制设计：将“压力源”操作化\n*   **思考**：如何将之前抽象的四个维度转化为游戏里的具体机制？\n*   **映射逻辑**：\n    *   针对**部分可观测性**：限制视野（只能看到局部），引入“薛定谔方块”（未知结构），必须付费才能探测。\n    *   针对**动态环境**：引入环境漂移（如天气改变移动成功率）、障碍物扩散、随机传送。\n    *   针对**噪声信号**：在观察中注入随机噪声，引入动作失败率（打滑）。\n    *   针对**动态智能体状态**：在任务中途改变智能体的能力参数（如感知成本突然变高）。\n\n#### 6. 假设形成与验证目标\n*   **思考**：如果让最先进的 SOTA 模型来玩这个游戏，会发生什么？\n*   **假设**：\n    *   现有的强模型在“清洁接口”下表现很好，但在这种“脏乱差”环境下性能会大幅下降。\n    *   排名会变得不稳定：某些“较弱”的模型如果策略更保守、更适应不确定性，可能会打败“更强”但激进的模型。\n    *   智能体需要展现出隐式的目标推断（比如在没有指令的情况下，为了省电而减少扫描）。\n\n#### 7. 最终产出：WildGrid 基准\n*   **结论**：通过构建这个包含长周期执行、强制信息获取、规则推断和动态适应的网格游戏，我们不仅测试了智能体能否完成任务，更测试了它们在不确定性中生存和适应的能力。这就是从“任务解决”迈向“稳健适应”的关键一步。"
                },
                {
                    "title": "InfMem: Learning System-2 Memory Control for Long-Context Agent",
                    "arxiv_id": "2602.02704",
                    "authors": "Xinyu Wang, Mingze Li, Peng Lu, Xiao-Wen Chang, Lifeng Shang, Jinping Li, Fei Mi, Prasanna Parthasarathi, Yufei Cui",
                    "summary": "Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\\times$ on average (up to $5.1\\times$) via adaptive early stopping.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断（第一步）**： *   论文的核心贡献是构建了一个名为 **InfMem** 的新智能体框架，而不是将现有智能体作为工具应用到特定领域。 *   论文明确提出了一个“以控制为中心的智能体”，旨在解决长上下文场景下的记忆控制问题。这属于对 LLM 智能体能力的改进和构建。 2.  **正面指标匹配（第二步）**： *   **Agentic AI**: 论文标题和摘要多次提及 \"Agent\"，并定义了 \"control-centric agent\"。 *   **Memory (记忆)**: 这是论文的核心焦点。它提出了 \"evidence-aware joint compression\"（感知证据的联合压缩）和 \"bounded memory\"（有界记忆）更新机制，直接针对智能体的记忆模块进行优化。 *   **Planning/Reasoning (规划/推理)**: 论文引入了 \"System-2-style control\"（System-2 风格的控制）和 \"PreThink-Retrieve-Write protocol\"（预思考-检索-写入协议）。这表明智能体具备主动监控、规划和多步推理的能力，而非简单的被动处理。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊处理（第四步）**： *   虽然论文涉及推理，但它不是关于提高模型底层的数学或逻辑预测能力，而是关于智能体如何通过 System-2 机制来控制记忆和检索流程，属于 Agentic Reasoning 的范畴。 综上所述，InfMem 提出了一种新的智能体架构来增强长上下文下的记忆控制和推理能力，精准契合“单智能体”中关于记忆与规划的研究焦点。",
                    "summary2": "本文旨在解决超长文档推理中有限内存下的证据管理问题。针对超长上下文QA场景，我们提出了一种名为InfMem的控制中心Agent，通过PRE THINK – RETRIEVE – WRITE协议实现System-2风格的主动记忆控制。我们在32k至1M token的超长QA基准及LongBench上通过准确率和推理时间验证了其有效性。结果显示，InfMem显著优于MemAgent，在提升准确率的同时将推理时间平均降低了3.9倍。",
                    "summary_translation": "针对超长文档的推理需要在严格的 memory constraints (内存约束) 下，整合分散在远距离片段中的稀疏证据。虽然 streaming agents (流式代理) 能够实现可扩展的处理，但其被动的 memory update strategy (记忆更新策略) 往往无法保留 multi-hop reasoning (多跳推理) 所需的 low-salience bridging evidence (低显著性桥接证据)。我们提出了 InfMem，这是一种 control-centric agent (以控制为中心的代理)，它通过 PreThink-Retrieve-Write (预思考-检索-写入) 协议实例化了 System-2-style (系统2式) 控制。InfMem 主动监控 evidence sufficiency (证据充分性)，执行 targeted in-document retrieval (针对性文档内检索)，并应用 evidence-aware joint compression (证据感知联合压缩) 来更新 bounded memory (有界记忆)。为了确保可靠的控制，我们引入了一种实用的 SFT-to-RL (监督微调到强化学习) 训练范式，将检索、写入和停止决策与 end-task correctness (最终任务正确性) 相对齐。在从 32k 到 1M tokens (词元) 的超长 QA benchmarks (问答基准) 上，InfMem 在各种 backbones (骨干网络) 上均持续优于 MemAgent。具体而言，InfMem 在 Qwen3-1.7B、Qwen3-4B 和 Qwen2.5-7B 上分别将平均绝对准确率提高了 +10.17、+11.84 和 +8.23 个百分点，同时通过 adaptive early stopping (自适应提前停止) 将 inference time (推理时间) 平均缩短了 3.9 倍（最高达 5.1 倍）。",
                    "inspiration_trace": "基于对论文《InfMem: Learning System-2 Memory Control for Long-Context Agent》的深入分析，以下是作者产出该核心方法的逻辑推演过程：\n\n### 1. 宏观观察：超长文档推理的本质困境\n作者首先将目光投向了长文档问答（Long-Context QA）的一个极端场景：在有限的计算和内存预算下，处理超长（如百万级Token）文档。\n*   **观察**：在这种场景下，决定答案的关键证据往往是**稀疏且分散**的。正确答案通常依赖于跨越文档不同部分的“低显著性”事实（例如，将开头的定义与结尾的例外条款联系起来），而不是对全文主旨的概括。\n*   **初步结论**：问题的核心不在于“读不完”，而在于如何在海量噪音中精准捕捉并维持这些微弱的逻辑链条。\n\n### 2. 核心冲突：“保真度两难”\n作者进一步分析了现有处理模式在应对上述观察时的内在矛盾，提出了“保真度两难”：\n*   **路径A（流式压缩/被动更新）**：为了适应有限内存，现有流式代理通常采用激进的分段压缩策略。\n    *   *缺陷*：这种“一刀切”的压缩极易抹除那些当前看似不重要、但对后续多跳推理至关重要的“桥梁证据”。\n*   **路径B（长上下文/RAG）**：试图通过扩大上下文窗口或检索增强来保留更多信息。\n    *   *缺陷*：单纯扩大窗口会导致注意力稀释，关键事实被噪音淹没；而传统RAG检索到的片段往往是碎片化的，缺乏整合。\n*   **推论**：单纯增加容量或被动压缩都无法解决问题，我们需要的是一种**任务导向的证据管理机制**。\n\n### 3. 现有方案的批判：从“System-1”到“System-2”的缺失\n作者将批判的矛头指向了当时最先进的有限内存代理（如MemAgent）：\n*   **现状**：这些代理主要依赖**被动、反应式**的更新策略。它们像人类的直觉思维（System-1）一样，按顺序读取并机械地压缩内存。\n*   **痛点**：一旦在处理过程中发现缺少关键证据，这种被动机制无法“回头”去检索之前读过的内容，也无法主动去文档的后续部分寻找缺失的拼图。它们缺乏对“已知什么”和“缺什么”的显式监控。\n\n### 4. 理论假设：引入“System-2”认知控制\n基于上述痛点，作者提出了核心假设：解决长文档推理的关键在于从被动处理转向主动控制。\n*   **假设**：我们需要一种受控的、显式的认知过程（即System-2思维），它能够根据当前状态动态决定：\n    1.  **监控**：当前内存是否足以回答问题？\n    2.  **寻找**：如果不足，缺失的证据在哪里？\n    3.  **写入**：如何有选择地将新证据与旧记忆整合，而不是盲目覆盖？\n*   **目标**：建立一个状态依赖的控制器，实现非单调的证据访问（即可以随时跳回文档前部或跳到后部）。\n\n### 5. 方法论构建：PRE-THINK – RETRIEVE – WRITE 协议\n为了将上述“System-2”假设落地，作者设计了一套具体的控制协议：\n*   **PreThink（监控与规划）**：作为大脑前额叶，负责评估当前记忆的充分性。如果不足，则生成检索查询；如果充足，则发出停止信号。\n*   **Retrieve（主动寻找）**：打破流式限制，允许在文档全局范围内进行定向检索，找回缺失的桥梁证据。\n*   **Write（证据感知的联合压缩）**：不再是简单的摘要，而是将“当前流式片段”与“检索回的证据”进行联合推理，只保留对推理最关键的信息更新到内存中。\n*   **Early Stop（效率优化）**：一旦证据充足立即停止，避免无效计算。\n\n---\n\n### 附：Introduction 中的“讲故事”逻辑链\n\n1.  **背景设定**：超长文档推理需要在严格内存限制下，综合分散在远距离片段中的稀疏证据。\n2.  **提出矛盾**：现有的流式代理虽然可扩展，但其被动的内存更新策略往往无法保留多跳推理所需的低显著性桥梁证据。\n3.  **分析现状**：虽然长上下文建模和RAG技术有所发展，但前者缺乏显式的证据选择控制，后者缺乏紧凑的整合基质。特别是现有的有限内存代理（如MemAgent）依赖被动策略，无法回溯恢复缺失证据。\n4.  **引入概念**：指出理想的控制器应具备状态依赖性，能判断证据充分性、决定检索内容和写入方式。这需要从System-1（被动启发式）转向System-2（显式、任务条件化的控制）。\n5.  **提出方案**：InfMem通过PRE-THINK – RETRIEVE – WRITE协议实例化这种System-2控制，并配合SFT→RL的训练流程来确保决策的可靠性。\n\n---\n\n### 总结：研究问题\n\n**如何设计一种有限内存代理，使其能够通过主动的、状态依赖的控制机制，在严格的内存预算下有效管理证据，从而保留多跳推理所需的低显著性桥梁事实？**"
                },
                {
                    "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
                    "arxiv_id": "2602.03798",
                    "authors": "Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li",
                    "summary": "Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，核心贡献集中在构建和演化 LLM 智能体上，具体分析如下： 1.  **核心判断（符合）**：论文的本质是构建一个新的智能体系统，而非单纯的应用。它提出了 FullStack-Agent，包含两个核心组件：FullStack-Dev（一个多智能体框架）和 FullStack-Learn（一种自我改进方法）。这直接对应了研究课题中的“多智能体”和“自我演化”方向。 2.  **正面指标（高度匹配）**： *   **多智能体**：论文明确提到 FullStack-Dev 是一个 \"multi-agent framework\"，具备规划、代码编辑、代码库导航和 Bug 定位等智能体核心能力。 *   **自我演化**：论文提出的 FullStack-Learn 是一种 \"self-improving method\"（自我改进方法），通过反向翻译代码库来迭代提升骨干 LLM 的性能，这完全符合“自我演化”的定义。 *   **智能体能力**：涉及 Planning（规划）、Tool Use（代码编辑）、Memory/Context（代码库导航）等关键能力。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉模型（虽然生成网页，但核心是代码逻辑而非视觉感知）或图神经网络。 4.  **特殊情况处理**：虽然论文的应用场景是“全栈 Web 编码”（特定领域），但根据筛选标准第四点，只要论文的核心是提出一种新的“自我演化”机制（FullStack-Learn）或新的智能体框架（FullStack-Dev），即使应用在特定领域，也应该保留。本文不仅提出了框架，还展示了智能体通过自我演化机制显著提升了性能，因此符合保留条件。 综上所述，该论文在多智能体协作和自我演化机制上做出了明确的方法论贡献，符合研究目标。",
                    "summary2": "总结生成失败",
                    "summary_translation": "协助非专业用户开发复杂的交互式网站已成为LLM（大语言模型）驱动的代码代理的一项热门任务。然而，现有的代码代理往往只生成前端网页，用华丽的视觉效果掩盖了缺乏真正的全栈数据处理和存储的事实。值得注意的是，构建生产级的全栈Web应用远比仅生成前端网页更具挑战性，这需要对数据流进行精细控制，全面理解不断更新的包和依赖，以及准确定位代码库中隐蔽的错误。为了解决这些困难，我们介绍了FullStack-Agent，这是一个用于全栈代理编码的统一代理系统，由三个部分组成：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架。(2) FullStack-Learn，一种创新的数据扩展和自我改进方法，通过反向翻译爬取和合成的网站仓库来增强FullStack-Dev的骨干LLM（大语言模型）。(3) FullStack-Bench，一个综合基准测试，用于系统地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上的表现分别优于以往最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我改进，将30B模型在这三组测试用例上的性能分别提升了9.7%、9.5%和2.8%，验证了我们方法的有效性。代码已发布于 https://github.com/mnluzimu/FullStack-Agent。",
                    "inspiration_trace": "基于对论文《FullStack-Agent》内容的深度分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从发现问题到构建解决方案的完整思考过程。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的叙事逻辑，揭示了当前研究领域的核心痛点：\n\n1.  **现象观察：** LLM 驱动的代码代理在帮助非专家用户开发复杂交互式网站方面已成为热门趋势。\n2.  **揭示伪装：** 现有的代码代理倾向于仅生成“前端网页”，它们利用花哨的视觉效果掩盖了缺乏真实全栈数据处理和存储的事实（例如：表单提交显示成功，但后台并未实际处理或存储数据）。\n3.  **定义鸿沟：** 构建生产级的全栈 Web 应用远比仅生成前端页面困难得多，这需要精细的数据流控制、对复杂依赖包的全面理解以及对代码库中隐晦 Bug 的准确定位。\n4.  **归纳挑战：** 构建具备生成生产级全栈网站能力的代码代理面临三大核心挑战：\n    *   **代码库复杂性：** 真实世界的 Web 框架（如 Next.js, NestJS）涉及庞大的代码库，需要高效的导航和精准的错误定位。\n    *   **工作流复杂性：** 全栈编码需要长期推理、熟练的工具调用和对 Web 包的专家级掌握，这是当前基础 LLM 的短板。\n    *   **评估盲区：** 现有的基于 GUI 代理的基准测试（如 WebGen-Bench）主要判断 UI 级别的交互，无法检测缺失或错误的后端实现（即存在“假阳性”）。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个统一的代理系统，使其能够生成具备真实数据处理和存储能力的生产级全栈 Web 应用，从而克服现有方法仅关注前端视觉效果且缺乏后端实质的局限性？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n以下是对作者产出 FullStack-Agent 这一系统的思维过程推演：\n\n#### 1. 观察与痛点识别\n*   **思考起点：** 现在的 AI 写代码工具看起来很厉害，能画出漂亮的网页，但它们是“空壳”。用户想要的是一个能真正存数据、跑逻辑的完整系统，而不是一个只能看不能用的“PPT”。\n*   **核心矛盾：** 真正的全栈开发太难了。它不是简单的“文本生成”，而是一个复杂的系统工程。现有的 LLM 既不懂怎么在复杂的代码库里“迷路”后找回来，也不知道怎么写真正的后端逻辑，甚至连我们用来测试它们的“考卷”（基准测试）都只看外表不看内里。\n\n#### 2. 假设提出\n*   **假设：** 要解决“假全栈”的问题，不能只靠给模型写更好的 Prompt，必须从**系统架构**、**模型能力**和**评估标准**三个维度同时入手。\n    *   *架构上*：需要模仿人类开发团队，分工合作。\n    *   *能力上*：模型需要从真实的、高质量的代码库中学习，而不是从虚构的数据中学习。\n    *   *评估上*：必须像真正的 QA 工程师一样，去查数据库日志和 API 接口，而不仅仅是点点屏幕。\n\n#### 3. 方法论构建\n\n**第一阶段：设计“开发团队”**\n*   **思考：** 真正的全栈开发是怎么做的？是一个人从头写到尾吗？不是。通常有一个架构师做规划，然后前端和后端工程师分别干活。\n*   **推演：** 既然单一大模型难以处理全栈的复杂性，那就用**多智能体框架**。\n    *   *规划代理*：充当架构师，设计数据流和 API 接口，确保前后端能对上话。\n    *   *编码代理*：分为前端和后端，各司其职。\n    *   *工具赋能*：光有人不行，还得有工具。特别是**调试工具**。人类开发有 Postman 和浏览器控制台，AI 也得有。通过设计专门的“前端调试工具”和“后端调试工具”，让 AI 能像人一样定位错误，而不是盲目试错。\n\n**第二阶段：提升“模型智商”**\n*   **思考：** 即使有了好的框架（FullStack-Dev），如果底层的 LLM 本身不懂全栈开发的最佳实践，系统也跑不起来。现有的训练数据大多是简单的代码片段，缺乏“如何从零开始构建一个项目”的轨迹。\n*   **推演：** 互联网上有大量优秀的开源仓库，它们就是最好的教科书。如何利用它们？\n    *   *逆向工程*：与其让 AI 凭空写代码，不如让它“抄作业”。设计一个**仓库回译**机制，让 AI 阅读现有的 GitHub 仓库，然后在一个空白模板里把它“复现”出来。这个过程生成的轨迹就是完美的训练数据。\n    *   *数据增强*： 现有的仓库不够多怎么办？通过**仓库增强**，对现有仓库进行修改（简化、扩展、迁移），生成更多样化的合成数据，以此扩充训练集，实现模型的自我迭代提升。\n\n**第三阶段：建立“严格考官”**\n*   **思考：** 如果我们用以前的考试题（只看 UI），我们的新系统可能看不出优势，甚至因为实现了复杂的后端而被误判（因为以前大家都不做后端，不做反而不会错）。我们需要一个新的尺子。\n*   **推演：** 必须构建一个能透视“黑盒”的基准测试。\n    *   *全维度测试*：不仅测前端（UI 交互），还要测后端（API 响应）和数据库（数据存储）。\n    *   *日志验证*：在测试前端操作时，强制检查数据库日志。如果前端显示“提交成功”，但数据库里没记录，直接判错。这样就能彻底杜绝“假全栈”的蒙混过关。\n\n#### 4. 系统整合\n*   **最终形态：** 将上述三个环节整合为 **FullStack-Agent** 统一系统。\n    *   **FullStack-Dev** 负责执行（解决怎么做）。\n    *   **FullStack-Learn** 负责进化（解决怎么做得更好）。\n    *   **FullStack-Bench** 负责验证（解决怎么证明做得好）。\n\n---\n\n**总结：** 作者的思考路径是从**发现“虚假繁荣”的现象**出发，深入分析**全栈开发的本质难点**，进而提出**“架构+数据+评估”三位一体**的解决方案，最终通过模仿人类协作模式、利用真实代码逆向学习以及建立严格的测试标准，实现了从“生成网页”到“构建系统”的跨越。"
                },
                {
                    "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
                    "arxiv_id": "2602.03419",
                    "authors": "Shuang Sun, Huatong Song, Lisheng Huang, Jinhao Jiang, Ran Le, Zhihao Lv, Zongchao Chen, Yiwen Hu, Wenyang Luo, Wayne Xin Zhao, Yang Song, Hongteng Xu, Tao Zhang, Ji-Rong Wen",
                    "summary": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心贡献符合**: 论文提出了 SWE-World，这是一个旨在构建和改进软件工程智能体的新框架。其核心贡献在于用“学习到的代理”替代了传统的物理执行环境（Docker），从而优化了智能体的训练和评估流程。这直接对应了“构建、改进或演化 LLM智能体”的目标。 2.  **属于自我演化/改进范畴**: 论文明确展示了如何利用该框架通过监督微调（SFT）和强化学习（RL）来显著提升智能体（Qwen2.5-Coder-32B）的性能（从 6.2% 提升至 55.0%）。此外，它还支持测试时扩展（TTS），通过模拟结果选择最优解。这些机制完全符合“自我演化”和“自我完善”的定义。 3.  **Agentic AI 特征**: 论文保留了标准的“智能体-环境交互循环”，关注智能体如何基于环境反馈进行学习和决策，符合 Agentic AI 的核心范式。 4.  **非单纯应用**: 尽管论文的应用场景是软件工程（SWE-bench），但其研究重点并非仅仅是将现有智能体作为工具去解决代码问题，而是提出了一种新的智能体训练和演化机制（即环境模拟）。根据筛选标准第四步，这种提出新机制的应用应予以保留。 5.  **非基础设施排除**: 虽然论文涉及环境模拟，但这属于智能体训练方法论的一部分，旨在解决智能体演化的效率和可扩展性问题，而非关注模型部署、硬件加速等底层基础设施，因此不应被排除。",
                    "summary2": "本文旨在解决软件工程智能体依赖Docker环境导致的资源消耗大和扩展性受限问题。针对代码修改与测试场景，我们提出了一种基于LLM的SWE-World无Docker框架，利用轻量级沙箱和学习的代理模型（SWT和SWR）模拟执行反馈。我们在SWE-bench Verified数据集上通过resolve rate验证了其有效性，实现了无需物理环境的高效训练与评估。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《SWE-World: Building Software Engineering Agents in Docker-Free Environments》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观背景与问题叙事\n\n作者首先构建了一个关于软件工程（SWE）智能体发展的宏观叙事，逻辑如下：\n\n1.  **现状与范式**：随着大语言模型（LLM）的进步，SWE智能体已具备解决复杂代码修改任务的能力。目前的主流范式是“智能体-环境”交互循环，即智能体通过在隔离的、依赖完整的容器化环境（如Docker）中执行代码和测试，获得物理执行反馈来驱动迭代。\n2.  **核心痛点**：虽然这种基于物理执行反馈的机制有效，但它引入了根本性的**可扩展性瓶颈**。作者从三个维度阐述了这一瓶颈：\n    *   **数据维度**：许多真实的GitHub仓库和PR因为依赖配置复杂或脆弱，无法在容器中成功构建或执行，导致大量数据被丢弃，无法利用。\n    *   **训练维度**：存储、管理和分发大量的Docker镜像带来了巨大的基础设施开销，这使得在资源受限的学术环境中进行大规模优化（特别是强化学习）变得极其困难。\n    *   **测试维度**：由于环境交互计算成本高昂且往往不可逆，难以通过迭代探索或试错策略来充分利用额外的测试时计算。\n3.  **矛盾总结**：现有的Docker依赖模式虽然保证了真实性，却以牺牲数据规模、训练效率和推理灵活性为代价，严重限制了SWE智能体的进一步发展。\n\n---\n\n### 二、 核心研究问题\n\n基于上述叙事，作者提出了一个核心的探索性问题：\n\n**“能否利用大语言模型（LLM）来近似传统Docker环境提供的执行反馈，从而实现无需Docker的软件工程智能体训练与部署？”**\n\n---\n\n### 三、 思想演进与逻辑推演\n\n为了回答上述问题，作者的思考过程经历了从观察到假设，再到方法论的逐步演进：\n\n#### 1. 观察与解构：并非所有操作都“重”\n作者首先对SWE任务中的智能体行为进行了细致的观察和解构，提出了一个关键洞察：\n*   **观察**：在SWE交互循环中，智能体的操作分为两类。一类是轻量级的文件系统操作（如文件导航、文本查看、代码编辑 `ls`, `grep`, `vim`）；另一类是重量级的代码执行操作（如运行程序、单元测试 `python`, `pytest`）。\n*   **推论**：轻量级操作是确定性的，计算开销极低，不需要复杂的依赖环境；真正的资源瓶颈和复杂性完全来源于“代码执行”这一部分。\n\n#### 2. 假设提出：用“学习”替代“物理”\n基于上述解构，作者提出了核心假设：\n*   **假设**：既然轻量级操作可以低成本处理，那么如果能用LLM学习并模拟出代码执行的结果（即构建一个“代理环境”），就可以在保留标准交互循环的同时，彻底剥离对物理Docker容器的依赖。\n\n#### 3. 方法论构建：构建“SWE-World”\n为了验证假设，作者设计了一个混合架构，即SWE-World，其逻辑构建如下：\n*   **分离策略**：将环境明确拆分为两部分。\n    *   **确定性沙箱**：直接处理文件导航和编辑，保证状态的一致性和操作的可靠性，避免LLM产生幻觉。\n    *   **LLM代理模型**：这是核心创新。利用LLM来预测那些原本需要Docker执行的命令的输出。\n*   **模型分工**：\n    *   **转移模型**：预测中间的执行结果（如报错信息、打印日志），模拟环境动力学。\n    *   **奖励模型**：模拟最终的测试运行结果，生成结构化的测试报告和二元奖励信号（通过/失败）。\n\n#### 4. 训练闭环：从真实到虚拟\n为了训练这个代理环境，作者设计了一个数据飞轮：\n*   **数据来源**：先让智能体在真实的Docker环境中运行，收集“智能体-环境”的交互轨迹。\n*   **知识蒸馏**：利用这些真实数据训练LLM（SWT和SWR），让它们学会预测Docker的输出。\n*   **解放训练**：一旦训练完成，未来的智能体训练（SFT和RL）就可以完全在这个虚拟的SWE-World中进行，不再需要物理容器。\n\n#### 5. 价值验证与扩展\n最后，作者验证了该方法不仅解决了资源问题，还带来了额外收益：\n*   **数据解放**：由于不再依赖可构建的Docker环境，大量原本因依赖问题被废弃的GitHub数据可以被重新利用，极大地扩展了训练数据规模。\n*   **测试时扩展**：由于奖励模型是虚拟的，可以低成本地对多个候选解进行评估和筛选，从而实现了高效的测试时扩展。\n\n---\n\n**总结**：作者的思考路径是从**发现物理环境的资源瓶颈**出发，通过**解构操作类型**发现优化空间，进而提出**用LLM模拟执行反馈**的核心假设，最终构建了一个**虚实结合的混合环境架构**，实现了SWE智能体训练的“去容器化”革命。"
                },
                {
                    "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training",
                    "arxiv_id": "2602.03411",
                    "authors": "Huatong Song, Lisheng Huang, Shuang Sun, Jinhao Jiang, Ran Le, Daixuan Cheng, Guoxin Chen, Yiwen Hu, Zongchao Chen, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen",
                    "summary": "In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围。以下是详细的判断过程： 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **SWE-Master**，这是一个用于构建软件工程智能体的**后训练框架**。 *   它不仅仅是将现有的LLM应用到软件工程领域，而是提出了一套系统性的方法来**构建和改进**智能体本身（包括教师轨迹合成、长视界SFT、基于真实执行反馈的RL等）。 *   这符合“构建、改进或演化 LLM智能体”的核心目标，因此属于保留范围。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确关注 `LLM-based Agents` (软件工程智能体)。 *   **自我演化**: 论文详细描述了通过后训练（SFT、RL）和测试时扩展（TTS）来激发和提升模型能力的过程，这属于 `Self-Improvement` 和 `Iterative Improvement` 的范畴。 *   **智能体能力**: 论文涉及 `Tool Use` (真实执行反馈)、`Planning` (长视界任务解决) 以及利用环境反馈进行自我修正。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理 (第四步)**: *   虽然论文的应用领域是软件工程（特定领域），但根据“自我演化的应用”规则，如果论文的核心是提出一种新的“自我演化”或“构建”机制（在此例中为后训练框架和优化流程），即使应用于特定领域，也应该保留。 *   论文重点在于如何通过系统性的优化方法让智能体具备更强的能力，而非单纯的应用。 综上所述，该论文提出了一个旨在改进和演化LLM智能体能力的框架，涵盖了单智能体的规划、工具使用以及通过反馈进行的自我完善，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在构建一个开源且可复现的软件工程智能体后训练框架。针对复杂的软件工程任务及代码库导航场景，我们提出了SWE-Master方法，该系统集成了高质量轨迹合成、长视距SFT、基于真实环境反馈的RL以及基于LSP的IDE级代码导航工具。我们在SWE-bench Verified数据集上通过resolve rate验证了其有效性，实现了61.4%的Pass@1准确率及70.8%的TTS@8性能。",
                    "summary_translation": "在本技术报告中，我们提出了 SWE-Master，这是一个开源且完全可复现的 post-training framework (后训练框架)，用于构建有效的 software engineering agents (软件工程智能体)。SWE-Master 系统性地探索了完整的智能体开发流程，包括 teacher-trajectory synthesis (教师轨迹合成) 与 data curation (数据策展)、long-horizon SFT (长视界监督微调)、基于真实执行反馈的 RL (强化学习) 以及 inference framework design (推理框架设计)。从一个初始 SWE (软件工程) 能力有限的开源基础模型出发，SWE-Master 展示了 systematical optimization method (系统性优化方法) 如何激发出强大的 long-horizon SWE task solving abilities (长视界软件工程任务求解能力)。我们在 SWE-bench Verified 上对 SWE-Master 进行了评估，这是一个针对真实软件工程任务的标准 benchmark (基准)。在相同的实验设置下，我们的方法使用 Qwen2.5-Coder-32B 达到了 61.4% 的 resolve rate (解决率)，显著优于现有的 open-source baselines (开源基线)。通过进一步结合基于 LLM (大语言模型) 环境反馈的 test-time scaling (TTS, 测试时扩展)，SWE-Master 在 TTS@8 下达到了 70.8%，展现了强大的性能潜力。SWE-Master 为推进 software engineering agents (软件工程智能体) 的 reproducible research (可复现研究) 提供了一个实用且透明的基础。代码可在 https://github.com/RUCAIBox/SWE-Master 获取。",
                    "inspiration_trace": "基于对论文《SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观观察与问题引入\n\n#### 1. Introduction 中的“讲故事”逻辑\n作者首先构建了一个从“现状”到“痛点”的叙事弧线：\n\n*   **背景与机遇：** 软件工程代理正在兴起，它们不同于传统的代码生成模型，能够像人类开发者一样理解需求、导航代码库、修改文件并执行测试，实现了端到端的自动化工作流。\n*   **现有进展：** 这一领域的进步得益于全流程的协同发展，包括数据构建（基于真实GitHub Issue和Docker环境）、训练策略（基于环境反馈的RL）以及推理框架（如OpenHands）。OpenAI和Anthropic等机构已经展示了强大的性能。\n*   **核心冲突：** 尽管进展迅速，但现有的SOTA系统存在根本性的**透明度和可复现性缺失**。这种“黑盒”性质掩盖了构建有效SWE代理的关键挑战，导致学术界和开源社区面临极高的准入门槛。\n*   **具体痛点：**\n    *   **数据侧：** 难以高效构建包含长视距推理和真实环境交互的高质量教师轨迹。\n    *   **优化侧：** SFT需要精细的数据过滤以平衡正确性和多样性；RL需要微妙的算法调优以避免熵塌陷或奖励黑客。\n    *   **推理侧：** 现有框架受限于基础工具，缺乏对高级工具（如长上下文管理）的探索，导致执行效率低下。\n\n#### 2. 提炼出的“研究问题”\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何构建一个透明且可复现的系统性后训练框架，通过解决数据构建、优化策略和推理框架中的关键瓶颈，将初始能力有限的开源基础模型转化为高性能的软件工程代理？”**\n\n---\n\n### 二、 逻辑演进与思考过程\n\n作者并非凭空设计出SWE-Master，而是遵循了“观察-假设-验证-修正”的工程化思维路径。\n\n#### 第一阶段：数据层面的思考——从“大水漫灌”到“精细筛选”\n*   **观察：** 现有的开源数据集虽然数量庞大，但质量参差不齐。直接使用所有数据进行训练，模型容易在简单任务上过拟合，或在无法解决的困难任务上浪费算力。\n*   **假设：** 模型的学习效果不仅取决于数据的数量，更取决于数据的**难度分布**。只有那些“踮起脚尖够得着”的样本（即混合了成功和失败轨迹的Issue）才是最有价值的。\n*   **方法论形成：**\n    *   利用强模型（如MiniMax-M2, GLM-4.6）作为“教师”在真实Docker环境中生成轨迹。\n    *   设计**难度过滤器**：排除那些“总是解决”和“总是失败”的极端样本，保留中间难度的样本。\n    *   设计**格式过滤器**：剔除过长或语法错误的轨迹，确保训练稳定性。\n\n#### 第二阶段：训练层面的思考——从“模仿行为”到“学会探索”\n*   **观察：** 仅靠SFT（监督微调）只能让模型模仿教师的“动作”，但无法教会模型在未知环境下的“探索策略”。然而，直接引入RL（强化学习）在长视距任务中极不稳定，容易出现奖励稀疏导致训练崩溃。\n*   **假设：** 训练应分阶段进行。SFT负责建立基础的“长视距推理模式”，RL负责优化“决策策略”。为了解决RL的不稳定性，必须重新设计奖励机制，特别是处理那些“超时但部分正确”的情况。\n*   **方法论形成：**\n    *   **长视距SFT：** 扩展上下文窗口至80K tokens，让模型学会处理长对话历史。\n    *   **改进的RL策略：** 采用GRPO算法，并引入**强制提交机制**。如果模型因预算耗尽而未提交，系统强制提交当前Patch并给予打折的奖励（而非0奖励）。这解决了奖励稀疏问题，防止模型陷入无效的死循环。\n\n#### 第三阶段：推理层面的思考——从“文本搜索”到“IDE级理解”\n*   **观察：** 现有的代理主要依赖`grep`等基于文本的搜索工具来定位代码。这在面对大型、复杂的代码库时效率极低，且缺乏语义理解（例如无法区分同名函数的不同重载）。\n*   **假设：** 人类开发者之所以高效，是因为使用了IDE（集成开发环境）提供的语义导航功能（如跳转定义、查找引用）。赋予Agent类似的“IDE级”能力，能大幅提升其代码理解和导航效率。\n*   **方法论形成：**\n    *   引入**LSP（Language Server Protocol）工具**：将IDE的底层协议转化为Agent可调用的工具（如`go_to_definition`, `find_references`）。\n    *   **持续训练：** 在SFT阶段混合包含LSP工具使用的轨迹，让模型学会如何进行语义化导航，而非盲目文本搜索。\n\n#### 第四阶段：系统整合与验证——从“单点突破”到“端到端框架”\n*   **观察：** 上述改进（数据过滤、RL优化、LSP工具）是相互关联的。更好的数据需要更好的训练策略，更强的模型需要更高效的工具。\n*   **假设：** 将这些组件整合到一个统一的、开源的框架中，能够产生“1+1>2”的效果，并且能复现甚至超越闭源模型的表现。\n*   **方法论形成：**\n    *   构建**SWE-Master框架**，整合数据管道、训练基础设施和推理框架。\n    *   引入**测试时扩展（TTS）**：利用SWE-World模型模拟环境反馈，无需真实执行即可对多个候选Patch进行排序和筛选，进一步提升性能。\n\n---\n\n### 三、 总结：作者的核心思想脉络\n\n1.  **起点：** 开源社区缺乏一个透明、可复现的SWE Agent训练流程，导致与闭源模型存在巨大鸿沟。\n2.  **破局点1（数据）：** 数据质量优于数量。通过**难度感知的过滤**，确保模型学习到最具价值的“可学习”样本。\n3.  **破局点2（训练）：** SFT定基调，RL强策略。通过**奖励塑形（强制提交）**解决长视距任务中的RL训练不稳定问题。\n4.  **破局点3（工具）：** 模拟人类专家行为。通过引入**LSP工具**，将Agent从“文本匹配器”升级为“语义理解者”。\n5.  **最终产出：** 一个经过系统性优化的端到端框架，证明了即使是从能力较弱的开源基座模型出发，通过科学的后训练也能达到SOTA水平。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation",
                    "arxiv_id": "2602.03045",
                    "authors": "Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen",
                    "summary": "Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了一个名为 **ProCAD** 的“主动智能体框架”。这不仅仅是将现有LLM应用于CAD领域，而是构建了一个包含“澄清智能体”和“编码智能体”的新型Agentic架构。该框架旨在解决智能体在处理模糊指令时的主动交互和规范一致性问题，属于构建和改进LLM智能体的方法论研究。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文明确提出了 `Agentic AI` 和 `LLM-based Agents` 框架。 *   **智能体能力**：涉及 `Planning`（通过审计提示词进行规划）、`Self-Correction`（通过澄清解决内部不一致）以及 `Tool Use`（生成CAD代码）。 *   **多智能体**：虽然系统相对简单，但包含了两个智能体（澄清智能体与编码智能体）的协作与分工，符合多智能体系统的基本特征。 3.  **排除标准（未触发）**： *   **非演化型应用**：虽然应用场景是Text-to-CAD，但论文的重点在于“主动智能体”这一新框架的设计和“智能体SFT”的训练方法，而非单纯解决CAD领域的工程问题。 *   **多模态与视觉**：论文处理的是文本到参数化CAD程序（代码生成）的转换，而非直接的像素级视觉理解或MLLM架构，因此不属于被排除的多模态视觉范畴。 *   **安全与对齐**：论文关注的是指令遵循的鲁棒性，而非AI安全、对齐或幻觉检测本身。 4.  **特殊与模糊情况处理**： *   **推理/规划**：论文中的“主动澄清”机制本质上是一种高级的Agentic规划与推理过程。智能体不是被动输出，而是主动通过提问来完善规划，这完全符合“Agentic框架”的保留标准。 综上所述，该论文通过提出新的主动智能体框架来增强LLM在复杂任务中的鲁棒性，属于单智能体及多智能体协作的前沿研究，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决text-to-CAD生成中因自然语言描述模糊或不一致导致模型产生幻觉的问题。针对模糊的文本提示，我们提出了一种名为ProCAD的主动代理框架，通过主动澄清代理审计提示并询问针对性问题，再由编码代理生成CadQuery代码。我们在包含2469个模糊提示的测试集上，通过Chamfer distance (CD)和Invalidity Ratio (IR)验证了其有效性，显著优于Claude Sonnet 4.5。",
                    "summary_translation": "大语言模型（Large language models）最近推动了文本到CAD（text-to-CAD）系统的发展，这些系统能够根据自然语言提示词生成参数化CAD程序（例如 CadQuery）。然而，在实际应用中，几何描述往往存在定义不完整或内部不一致的问题：关键尺寸可能缺失，约束条件可能相互冲突。现有的微调模型往往被动地遵循用户指令，并在文本模糊时臆造尺寸。为解决这一问题，我们提出了一种用于文本到CadQuery生成的主动式智能体框架，命名为 ProCAD，该框架在代码合成之前解决规格说明问题。我们的框架包含一个主动式澄清智能体和一个CAD编码智能体。前者负责审查提示词，并仅在必要时提出针对性的澄清问题，以生成自洽的规格说明；后者则负责将该规格说明翻译为可执行的 CadQuery 程序。我们在一个精选的高质量文本到CadQuery数据集上对编码智能体进行了微调，并通过基于澄清轨迹的智能体监督微调训练了澄清智能体。实验结果表明，主动式澄清在保持较低交互开销的同时，显著提高了模型对模糊提示词的鲁棒性。ProCAD 的性能优于包括 Claude Sonnet 4.5 在内的前沿闭源模型，将平均 Chamfer 距离（Chamfer distance）降低了 79.9%，并将无效比率从 4.8% 降至 0.9%。我们的代码和数据集将公开发布。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-02-03": [
        {
            "name": "Artificial Intelligence",
            "count": 44,
            "papers": [
                {
                    "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
                    "arxiv_id": "2602.02468",
                    "authors": "Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang",
                    "summary": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，属于“单智能体”研究范畴。具体判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 **Avenir-Web** 的新型 Web 智能体框架。它并非简单地将现有智能体应用于特定垂直领域（如医疗或法律），而是针对智能体在复杂 Web 环境中的通用能力（如元素定位、长时程任务跟踪）提出了具体的改进方法。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **正面指标匹配（第二步）**：论文明确涉及了单智能体的关键能力： *   **规划**：提出了 \"Experience-Imitation Planning\"（经验模仿规划）。 *   **记忆**：引入了 \"adaptive memory\"（自适应记忆）和 \"task-tracking checklist\"（任务跟踪清单）。 *   **工具使用/感知**：通过 \"Mixture of Grounding Experts\" 增强智能体与 Web 界面交互的能力。 3.  **排除标准与特殊情况处理（第三、四步）**： *   **多模态问题**：虽然标题包含 \"Multimodal\"，但论文中的视觉能力仅作为智能体感知 Web 环境（元素定位）的工具，而非研究视觉模型本身。根据筛选标准中的例外条款（“除非它们被用作智能体感知环境的工具”），这不应成为排除理由。 *   **非演化型应用**：Web Agent 通常被视为智能体在真实环境中的能力测试床，而非单纯的垂直领域应用。该论文侧重于提升智能体架构本身的鲁棒性和性能，而非解决某个具体的业务问题。 综上所述，该论文致力于改进单智能体的规划、记忆和环境交互能力，完全符合“单智能体”的研究方向。",
                    "summary2": "本文旨在解决自主Web代理在复杂动态界面中执行长时程任务时的可靠性瓶颈。针对实时网站任务，我们提出了一种名为Avenir-Web的框架，该框架集成了Mixture of Grounding Experts (MoGE)、Experience-Imitation Planning (EIP)以及Task-Tracking Checklist与Adaptive Memory。我们在ONLINE-MIND2WEB基准上通过Task Success Rate (TSR)验证了其有效性，实现了53.7%的成功率，显著超越现有开源基线并确立了新的开源SOTA。",
                    "summary_translation": "尽管 multimodal large language models (多模态大语言模型) 取得了进展，但 autonomous web agents (自主网络代理) 仍难以在复杂且动态的 Web interfaces (Web 界面) 上可靠地执行 long-horizon tasks (长视距任务)。现有的代理常面临 element grounding (元素定位) 不准确、缺乏 site-specific procedural knowledge (特定于站点的程序性知识) 以及 long-term task tracking (长期任务跟踪) 和 memory (记忆) 不稳定等问题，尤其是在处理复杂的 Document Object Model (DOM, 文档对象模型) 结构时。为解决这些局限性，我们提出了 Avenir-Web，这是一种在实际部署中于 Online-Mind2Web benchmark (基准测试) 上取得了新 open-source state of the art (开源最佳水平) 的 Web 代理。Avenir-Web 利用 Mixture of Grounding Experts (定位专家混合模型)、用于整合 procedural priors (程序性先验) 的 Experience-Imitation Planning (经验模仿规划)，以及结合 adaptive memory (自适应记忆) 的 task-tracking checklist (任务跟踪检查清单)，从而在多样化的 user interface paradigms (用户界面范式) 中实现稳健且无缝的交互。我们在 Online-Mind2Web 上对 Avenir-Web 进行了评估，这是一个针对 live (实时) 且 user-centered (以用户为中心) 的 Web 任务的严格 benchmark (基准测试)。结果表明，Avenir-Web 显著超越了先前的 open-source agents (开源代理)，并与 top-tier proprietary models (顶级专有模型) 达到了 performance parity (性能相当)，从而为 live websites (实时网站) 上的可靠 Web 代理确立了新的 open-source state of the art (开源最佳水平)。",
                    "inspiration_trace": "基于对论文《Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts》的深入分析，以下是作者产出该文章的系统性思考过程与逻辑链推演：\n\n### 1. 宏观背景与观察\n**起点：** 现代计算环境充满了复杂且动态的图形用户界面（GUI），人们迫切希望拥有能够自主感知网页、理解指令并执行多步骤操作的智能体。\n**现状：** 随着通用多模态大语言模型（MLLMs，如GPT-4o）和专用GUI模型的出现，智能体在推理能力和基础交互上取得了显著进步，似乎已经具备了处理复杂网页任务的基础能力。\n\n### 2. 引言中的“讲故事”逻辑（问题引入）\n作者在Introduction中构建了一个典型的“虽有进步，但仍有巨大鸿沟”的叙事逻辑：\n\n1.  **铺垫：** 承认现有技术（MLLMs和专用GUI模型）已经建立了强大的推理和规划能力，能够通过HTML或可访问性树来解析网页内容。\n2.  **转折：** 尽管进展迅速，但在**真实世界的实际部署**中，当前的Web智能体仍然面临着关键的**可靠性瓶颈**。\n3.  **具体化：** 作者将这种不可靠性具体拆解为三个核心痛点：\n    *   **元素定位不准：** 依赖单一模态（纯DOM或纯视觉）在复杂结构（如iframe、Canvas、Shadow DOM）面前失效，导致无法精准点击。\n    *   **缺乏特定流程知识：** 智能体不知道特定网站的操作“套路”，只能盲目试错，导致Token消耗巨大且容易陷入死胡同。\n    *   **长期任务跟踪与记忆不稳定：** 在跨页面、长流程的任务中，智能体容易“忘记”目标，出现导航漂移和重复性错误。\n\n### 3. 研究问题\n基于上述痛点，作者试图回答的核心问题是：\n\n**“如何构建一个能够在复杂且动态的实时网站上可靠执行长周期任务的Web智能体，以解决元素定位不准、缺乏特定流程知识以及长期记忆不稳定的三大瓶颈？”**\n\n---\n\n### 4. 思想演进与逻辑链（从观察到方法论）\n\n为了回答上述问题，作者的思考路径经历了从“模仿人类”到“模块化解构”的演进：\n\n#### 第一阶段：针对“元素定位不准”的思考 —— 从单一到混合\n*   **观察：** 现有智能体要么死磕DOM结构（容易被iframe卡死），要么只看像素（缺乏语义理解）。人类是怎么做的？人类首先是“看”屏幕（视觉优先），如果看不清或者需要精确操作（如下拉菜单），才会去理解结构。\n*   **假设：** 如果能模仿人类的这种视觉优先、语义兜底的策略，就能解决复杂结构的定位问题。\n*   **方法论产出：** 提出 **Mixture of Grounding Experts (MoGE)**。默认使用视觉模型直接定位坐标（像人一样看屏幕），遇到困难时回退到语义结构推理，实现鲁棒的元素交互。\n\n#### 第二阶段：针对“缺乏特定流程知识”的思考 —— 从盲目探索到经验模仿\n*   **观察：** 智能体在陌生网站上像无头苍蝇一样乱撞，效率极低。人类在操作不熟悉的网站时会怎么做？会先去搜一下“使用指南”或“教程”。\n*   **假设：** 如果在执行任务前，先让智能体去检索并阅读该网站的人类操作指南，提取出高层计划，就能避免昂贵的试错。\n*   **方法论产出：** 提出 **Experience-Imitation Planning (EIP)**。在初始化阶段，利用搜索引擎获取外部知识，合成特定网站的操作路线图，指导后续执行。\n\n#### 第三阶段：针对“长期记忆不稳定”的思考 —— 从被动反应到主动监控\n*   **观察：** 任务一长，智能体就忘了自己做到哪一步了，或者在一个错误上反复跌倒。人类怎么管理长任务？会列一个“Checklist（清单）”，打钩确认；对于长会议，会做“摘要”。\n*   **假设：** 如果引入一个显式的清单来追踪子目标，并用递归摘要来管理历史记忆，就能防止导航漂移和上下文溢出。\n*   **方法论产出：** 提出 **Task-Tracking Checklist**（用于原子化目标追踪）和 **Adaptive Memory**（用于分块递归总结和失败反思）。\n\n#### 第四阶段：系统整合 —— 战略与执行的解耦\n*   **思考：** 上述三个模块如何协同工作？不能是一锅粥。\n*   **逻辑架构：** 将系统分为两个阶段。\n    *   **初始化阶段：** 利用EIP制定战略，生成Checklist。\n    *   **执行循环阶段：** 核心智能体在Adaptive Memory的上下文下，利用MoGE执行具体操作，并实时更新Checklist。\n*   **最终成果：** **Avenir-Web** 框架，一个模仿人类经验、具备混合定位能力和强记忆管理的开源SOTA Web智能体。"
                },
                {
                    "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
                    "arxiv_id": "2602.02416",
                    "authors": "Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu, Daniel Jiang, Boris Vidolov, Kaveh Hassani, Paul Sajda, Jalaj Bhandari, Yonathan Efroni",
                    "summary": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献分析**: 论文的核心贡献是提出了“Thought-ICS”（Iterative Correction Sampling of Thoughts），这是一个**自我修正框架**。该框架通过将推理结构化为离散的步骤，使模型能够显式地定位错误，并从最后一个正确点回溯以生成替代推理。这不仅仅是简单的提示工程，而是一个旨在实现AI系统自主纠错的机制化方法。 2.  **符合研究焦点**: *   **自我演化**: 论文明确关注“Self-correction”（自我修正）和“Iterative Improvement”（迭代改进）。Thought-ICS 框架允许智能体在无需外部验证（完全自主设置）的情况下，通过自身的反馈机制（错误定位和回溯）来完善输出，这完全符合“自我演化”中关于智能体通过经验或反思进行自我完善的核心定义。 *   **单智能体**: 论文涉及智能体的“自我反思”能力，即监控自身的决策过程（离散思维步骤）并识别错误。同时，其回溯和重新采样的机制也是一种高级的“规划”形式。 3.  **排除标准与特殊规则处理**: *   虽然论文涉及推理，但它不是单纯为了提高LLM的基础Token预测能力或数学逻辑能力（如普通的CoT变体），而是构建了一个具有**自主规划、反思和修正能力**的Agentic框架。根据第四步关于“推理/规划”的规则，这种关于智能体如何进行规划和多步推理（特别是包含自我修正机制）的研究应当保留。 *   论文不属于特定领域的应用（如生物、医疗），也不涉及安全、对齐或多模态等排除项。 综上所述，该论文提出了一种新的自我演化机制，增强了LLM智能体的自主性和反思能力，是Agentic AI领域的高质量研究。",
                    "summary2": "本文旨在解决LLMs难以有效自我修正推理错误的问题。针对LLMs无法精确定位错误这一挑战，我们提出了一种Thought-ICS框架，通过Thought MDP将推理结构化为离散的思维步骤，实现精确的错误定位与回溯重采样。我们在8个模型及6个推理基准上通过准确率提升验证了其有效性，在Oracle验证下实现了20-40%的性能提升。",
                    "summary_translation": "语言模型中的自我纠正仍然难以实现。在这项工作中，我们探讨了语言模型是否能够显式地定位错误推理中的错误，以此作为构建能够有效自我纠正的 AI 系统的一条途径。我们提出了一种提示方法，将推理构建为离散且语义连贯的思维步骤，并表明模型能够在此结构内可靠地定位错误，而在传统的非结构化 chain-of-thought (思维链) 推理中则无法做到这一点。受人脑在离散决策点监控错误并重采样替代方案的启发，我们引入了 Iterative Correction Sampling of Thoughts (Thought-ICS，思维迭代纠正采样)，这是一个自我纠正框架。Thought-ICS 迭代地提示模型每次生成一个离散且完整的思维——其中每个思维代表模型的一个刻意决策——从而为精确的错误定位创造了自然边界。在验证后，模型定位第一个错误步骤，系统回溯以从最后一个正确点生成替代推理。当被要求纠正被 oracle (预言机) 验证为错误的推理时，Thought-ICS 实现了 20-40% 的自我纠正性能提升。在没有外部验证的完全自主设置中，它优于现有的自我纠正基线。",
                    "inspiration_trace": "基于对论文《Structure Enables Effective Self-Localization of Errors in LLMs》的深度分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从幻想到现实的落差\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在揭示现有研究的盲点并引出本文的核心动机：\n\n1.  **理想与直觉的冲突**：\n    *   **直觉假设**：人们普遍认为，只要反复提示大语言模型（LLM）去优化输出，模型就能像人类一样通过自我反思实现“自我修正”，从而收敛到更好的推理结果。\n    *   **现实困境**：在实践中，这种直觉往往失效。所谓的“改进”很多时候并非真正的逻辑修正，而仅仅是“暴力重采样”带来的运气波动。\n\n2.  **归因分析**：\n    *   **表层瓶颈**：自我修正失败通常被归咎于“验证”能力的不足（即模型不知道自己错了）。\n    *   **深层症结**：即便我们假设模型知道答案错了（通过Oracle验证绕过验证瓶颈），它依然无法有效修正。这暴露了一个被忽视的核心问题——**定位**。模型无法在一段连续的推理流中，精确指出“哪一步”出了错。\n\n3.  **现有研究的局限**：\n    *   已有研究表明，如果由“神谕”直接告诉模型错误的具体位置，模型是能够修正错误的。但这仅仅是“重采样”能力的体现，而非真正的自我修正。\n    *   **关键缺失**：目前的空白在于，LLM是否具备在没有任何外部提示的情况下，**自主定位**自身推理错误的能力？\n\n4.  **跨域启发**：\n    *   作者转向神经科学寻求灵感。人类大脑的前扣带皮层（ACC）并非在神经元层面或整个行为序列层面监控错误，而是在**离散的决策点**层面进行监控，并基于此进行回溯和替代方案的模拟。\n\n**显式总结的研究问题：**\n> **Can LLMs self-localize errors in their own reasoning?**\n> （LLM能否在其自身的推理过程中自主定位错误？）\n\n---\n\n### 二、 核心方法产出的逻辑演进链\n\n基于上述问题，作者的思考过程经历了从现象解构、类比假设到方法构建的完整闭环：\n\n#### 1. 现象解构：为什么现有的CoT无法自我修正？\n*   **观察**：传统的思维链将推理视为一个连续的Token流。\n*   **痛点**：在连续的文本流中，错误往往隐藏在句子之间，缺乏明确的“决策边界”。模型很难回溯并精确地将错误归因于某一个具体的片段，导致修正时只能“推倒重来”或“修修补补”，效率极低。\n*   **推论**：要实现有效的自我修正，必须先解决“可定位性”问题。而要实现定位，必须改变推理的呈现形式。\n\n#### 2. 类比假设：从大脑机制到模型架构\n*   **灵感映射**：既然人类大脑在“离散的决策点”监控错误最有效，那么是否可以让LLM也在“离散的语义单元”上进行推理？\n*   **核心假设**：如果将推理过程强制结构化为离散的、语义完整的步骤，模型就能像人类一样，在这些步骤的边界处进行“检查点”式的监控。\n*   **预期效果**：这种结构化将提供天然的“回溯点”，使得模型能够识别出第一个错误的步骤，并保留该步骤之前的正确前缀。\n\n#### 3. 结构干预：从Token MDP到Thought MDP\n*   **概念转换**：作者将推理过程从“Token级别的马尔可夫决策过程”提升为“Thought级别的马尔可夫决策过程”。\n*   **操作定义**：不再逐个生成Token，而是让模型每一步生成一个完整的、语义自洽的“思想”。\n*   **逻辑意图**：这不仅仅是格式的改变，而是**粒度的重构**。通过显式的边界，模型在生成时就在进行“决策”，从而为后续的错误定位提供了物理基础。\n\n#### 4. 方法构建：Thought-ICS框架的闭环\n*   **机制设计**：基于上述结构，作者构建了迭代修正采样框架，包含三个核心动作：\n    1.  **生成**：在Thought MDP下逐步生成结构化推理链。\n    2.  **定位**：利用结构化的边界，让模型审查每一步，找出第一个错误的思想。\n    3.  **回溯与重采样**：一旦定位到错误，系统自动回退到最后一个正确的步骤，以此为起点重新生成后续路径。\n*   **逻辑优势**：相比于传统方法的全局重写，这种方法实现了“精准手术”，只修正错误的部分，保留了正确的推理路径。\n\n#### 5. 现实修正：应对验证噪声的工程智慧\n*   **新挑战**：在完全自主（无Oracle）的设定下，模型的“自我验证”能力依然不可靠（容易把对的改错）。\n*   **策略调整**：作者并未放弃结构化方法，而是引入了“置信度保护机制”。当自我验证出现分歧或达到最大迭代次数时，系统倾向于保留初始答案，避免盲目修正导致的性能退化。\n*   **最终结论**：结构化推理不仅提升了定位能力，配合合理的系统设计，即使在验证信号嘈杂的真实场景下，也能实现净收益的自我修正。\n\n---\n\n**总结**：\n作者的思考路径是从**“自我修正为何失效”**这一宏观问题出发，通过**“定位能力缺失”**这一微观诊断，借鉴**神经科学的离散决策机制**，最终通过**改变推理的粒度**来构建解决方案。其核心逻辑在于：**结构是定位的前提，定位是修正的基础。**"
                },
                {
                    "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
                    "arxiv_id": "2602.02369",
                    "authors": "Yaolun Zhang, Yiran Wu, Yijiong Yu, Qingyun Wu, Huazheng Wang",
                    "summary": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”和“单智能体”方向。 1.  **核心判断 (第一步)**：论文的本质是构建一个新的 LLM 智能体框架。它提出了 \"Live-Evo\"，这是一个在线自我演化记忆系统。其核心贡献在于改进智能体的“记忆”机制，使其能够通过连续反馈进行演化，而非仅仅将现有智能体作为工具应用于特定领域。 2.  **正面指标匹配 (第二步)**： *   **核心范式**：明确涉及 `Agentic AI` 和 `Self-Evolving`。 *   **智能体能力**：重点研究了 `Memory`（记忆）的管理和演化，以及 `Self-Improvement`（自我改进）机制。 *   **演化机制**：论文详细描述了通过反馈更新经验权重的机制，属于 `Iterative Improvement`（迭代改进）。 3.  **特殊规则处理 (第四步)**： *   虽然论文在金融相关的 \"Prophet Arena\" 基准上进行了评估，但根据“自我演化的应用”规则，只要论文的核心是提出一种新的“自我演化”机制（即 Live-Evo 的在线记忆演化框架），即使应用在特定领域，也应当保留。该论文的核心在于机制本身，而非金融应用本身。 4.  **排除项检查 (第三步)**：论文不涉及安全对齐、多模态视觉或图技术等排除领域。 综上所述，该论文直接贡献于 LLM 智能体的自我演化能力，符合研究目标。",
                    "summary2": "本文旨在解决LLM智能体在真实分布偏移和持续反馈下的在线进化问题。针对连续任务流场景，我们提出了一种名为LIVE-EVO的在线自进化记忆系统，该系统通过Experience Bank和Meta-Guideline Bank解耦经验与使用策略，并基于反馈动态调整经验权重。我们在Prophet Arena基准上通过Brier Score和Market Return验证了其有效性，显著提升了预测准确率和市场回报。",
                    "summary_translation": "大语言模型 agents 正日益配备 memory（存储的经验和可复用的指导），以提升任务求解性能。近期的 self-evolving systems（自进化系统）虽能根据交互结果更新 memory，但大多数现有的 evolution pipelines（进化流程）是基于静态的 train/test splits（训练/测试划分）开发的，仅通过折叠静态基准来近似 online learning（在线学习），导致其在面对真实的 distribution shift（分布偏移）和 continuous feedback（持续反馈）时表现脆弱。我们提出了 \\textsc{Live-Evo}，这是一种 online self-evolving memory system（在线自进化记忆系统），能够随着时间的推移从传入的数据流中进行学习。\\textsc{Live-Evo} 通过 Experience Bank（经验库）和 Meta-Guideline Bank（元指导库）将“发生了什么”与“如何使用它”解耦，针对每个任务从检索到的经验中编译出 task-adaptive guidelines（任务自适应指导）。为了实现 memory 的在线管理，\\textsc{Live-Evo} 维护经验权重并根据反馈进行更新：持续提供帮助的经验会得到 reinforcement（强化）并被更频繁地检索，而具有误导性或过时的经验则会被 down-weighted（降权）并逐渐遗忘，这一过程类似于人类记忆中的强化与衰减机制。在为期 10 周的 live \\textit{Prophet Arena} benchmark（基准测试）中，\\textsc{Live-Evo} 将 Brier score（布赖尔分数）提高了 20.8%，市场回报增加了 12.9%；同时，该系统还能迁移至 deep-research benchmarks（深度研究基准），并在与 strong baselines（强基线）的对比中取得持续的性能提升。我们的代码可在 https://github.com/ag2ai/Live-Evo 获取。",
                    "inspiration_trace": "基于对论文《Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback》的深入分析，以下是作者产出该文章的系统性思考过程推演。\n\n### 一、 Introduction 中的“讲故事”逻辑与研究问题\n\n**1. 逻辑演进链条：**\n\n*   **背景铺垫：** 大语言模型（LLM）智能体正越来越多地配备“记忆”，这些存储的经验和可复用的指导原则能显著提升任务解决性能。\n*   **现有范式：** 近期的“自进化”系统通过在训练集上构建工具、知识和策略来学习，这些被称为智能体的记忆。它们在训练时动态更新记忆，在测试时利用这些进化后的记忆进行决策，且表现优于无记忆进化的智能体。\n*   **现实冲突：** 记忆进化本质上是一个**在线问题**。在现实部署中，智能体的经验是顺序累积的，其记忆必须通过添加新证据、修订过时条目和巩固重复模式来**持续更新**，而不是从静态语料库中重建。\n*   **现有缺陷：** 现有的进化管道大多是为静态的“训练/测试”划分开发的，它们仅仅通过折叠静态基准来近似在线学习。这种方法在真正的分布偏移和持续反馈面前是脆弱的。\n*   **关键挑战：** 在像 Prophet Arena 这样的实时预测基准中，环境和市场在不断变化。成功不再取决于检索更多信息，而在于**明智地利用过去的经验**。过去的经验可能提供有用的归纳偏置，但也可能随着模式漂移或破裂而变得陈旧或具有误导性。\n*   **结论引出：** 因此，一个自进化记忆系统必须超越单纯的存储，它应该主动策划经验，并学习**何时以及如何使用它们**。\n\n**2. 提炼出的研究问题：**\n\n> **How can LLM agents evolve continuously as new data arrives?**\n> （LLM 智能体如何随着新数据的到达而持续进化？）\n\n---\n\n### 二、 核心方法产出的逻辑推演\n\n作者从上述宏观问题出发，通过以下四个阶段的思维演进，最终构建了 Live-Evo 方法论：\n\n#### 第一阶段：从“静态存储”到“动态管理”的认知转变\n*   **观察：** 现有的记忆系统（如 ReMem 等）主要关注如何存储更多的经验或将其抽象为静态摘要。但在实时流数据场景下，旧的经验往往会因为环境变化（如市场规律改变）而变成“噪音”甚至“毒药”。\n*   **思考：** 记忆不应该是一个只读不删的仓库，而应该是一个具有生命周期的动态系统。我们需要一种机制，能够根据反馈自动评估每条记忆的当前价值。\n*   **初步构想：** 引入权重机制。有用的经验权重增加，过时的经验权重降低（类似人类记忆的强化与遗忘）。\n\n#### 第二阶段：解耦“发生了什么”与“如何使用它”\n*   **深入思考：** 仅仅给经验加权是不够的。有时候，经验本身是好的（比如“关注伤病报告”），但在特定情境下应用方式不对（比如在政治预测中错误地套用了体育预测的逻辑）。\n*   **核心洞察：** 记忆系统需要区分两个层面：\n    1.  **内容层：** 过去发生了什么（具体的历史经验）。\n    2.  **策略层：** 如何将这些经验应用到当前任务中（元指导原则）。\n*   **设计决策：** 建立**双存储结构**——**Experience Bank（经验库）**存储具体的历史交互，**Meta-Guideline Bank（元指导库）**存储如何组合历史经验与当前任务的高阶指令。\n\n#### 第三阶段：构建闭环的“因果验证”机制\n*   **问题：** 如何知道某条经验或某个元指导在当前任务中真的起了作用？如何避免虚假归因？\n*   **思考：** 必须进行对比实验。只有对比了“使用记忆”和“不使用记忆”的差异，才能确定记忆的真实贡献。\n*   **设计决策：** 在 **Act（行动）** 阶段引入 **ContrastiveEval（对比评估）**。智能体同时生成两个预测（一个基于记忆，一个基于无记忆基线），通过两者的性能差来量化记忆的因果贡献。\n\n#### 第四阶段：基于反馈的“选择性写入”与“在线进化”\n*   **问题：** 记忆库是有限的，不能无脑存储所有轨迹。如何保证新写入的记忆是有价值的？\n*   **思考：** 只有经过验证的改进才应该被固化为记忆。失败案例往往比成功案例包含更多信息，应该从失败中提取教训。\n*   **设计决策：** 设计严格的 **Update（更新）** 机制：\n    1.  **权重更新：** 根据对比评估的增益，动态调整被检索经验的权重。\n    2.  **元指导生成：** 当记忆导致表现下降时，进行反思并生成新的 Meta-Guideline。\n    3.  **验证后写入：** 仅针对表现最差的任务生成候选经验，且只有当该经验能带来显著性能提升时，才将其提交进 Experience Bank。\n\n---\n\n### 总结：作者的思想脉络\n\n作者从**现实世界的非平稳性**出发，指出了静态记忆系统在持续流数据面前的根本缺陷。为了解决“经验过时”和“盲目套用”的问题，作者提出了**内容与策略解耦**的双层记忆架构。为了确保记忆的有效性，作者引入了**对比评估**来量化记忆价值，并设计了**基于反馈的闭环进化机制**（强化有用、遗忘有害、验证新增），从而实现了一个能够像人类一样在动态环境中持续学习和适应的智能体系统。"
                },
                {
                    "title": "Context Learning for Multi-Agent Discussion",
                    "arxiv_id": "2602.02350",
                    "authors": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang, Ju Ren",
                    "summary": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心判断**: *   论文的核心贡献是提出了一种名为 M2CL (multi-LLM context learning) 的新方法，旨在解决多智能体讨论中的上下文不一致问题。 *   这属于构建和改进多智能体系统的方法论研究，而非单纯将现有智能体框架应用到特定领域。虽然论文在学术推理、具身任务等场景进行了评估，但其核心在于改进智能体之间的协作机制，而非解决领域特定问题。 2.  **正面指标匹配**: *   **核心范式**: 明确涉及 `Multi-Agent Systems (MAS)`，专注于 `Multi-Agent Discussion (MAD)`。 *   **多智能体能力**: 论文重点研究了智能体间的 `Communication`（通过讨论）和 `Collaboration`（协作解决问题）。 *   **演化机制**: 论文提到了通过自动信息组织和细化来动态生成上下文，以及通过自适应机制控制上下文连贯性，这体现了智能体在交互过程中的迭代和优化过程。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉核心或图技术等排除项。 综上所述，该论文致力于改进多智能体系统的协作效率和一致性，属于 Agentic AI 中多智能体方向的前沿研究，因此予以保留。",
                    "summary2": "本文旨在解决多智能体讨论中的讨论不一致问题。针对多LLM协作场景，我们提出了一种多LLM上下文学习方法（M2CL），通过动态生成上下文指令控制一致性与差异。我们在9个具有挑战性的基准测试上通过准确率验证了其有效性，性能超越现有方法20%-50%。",
                    "summary_translation": "Multi-Agent Discussion (MAD) (多智能体讨论) 近期备受瞩目，其中多个 LLM instances (LLM 实例) 通过 structured discussion (结构化讨论) 协作解决问题。然而，我们发现当前的 MAD 方法容易受到 discussion inconsistency (讨论不一致性) 的影响，即由于各 LLM individual contexts (个体上下文) 之间的 misalignment (错位)，导致 LLMs 无法达成 coherent solution (连贯的解决方案)。在本文中，我们提出了一种 multi-LLM context learning method (M2CL) (多 LLM 上下文学习方法)，该方法为每个 agent (智能体) 学习一个 context generator (上下文生成器)，能够通过 automatic information organization and refinement (自动信息组织与优化)，在每轮 discussion round (讨论轮次) 中动态生成 context instructions (上下文指令)。具体而言，受我们对 context instruction (上下文指令) 的 theoretical insights (理论见解) 启发，M2CL 通过精心设计的 self-adaptive mechanism (自适应机制) 训练生成器，以控制 context coherence (上下文连贯性) 和 output discrepancies (输出差异)。这使得 LLMs 能够避免在 majority noise (多数噪声) 上 premature convergence (过早收敛)，并逐步达成 correct consensus (正确共识)。我们在 academic reasoning (学术推理)、embodied tasks (具身任务) 和 mobile control (移动控制) 等 challenging tasks (具有挑战性的任务) 上对 M2CL 进行了评估。结果表明，M2CL 的 performance (性能) 显著优于现有方法（提升幅度为 20%--50%），同时具备良好的 transferability (迁移性) 和 computational efficiency (计算效率)。",
                    "inspiration_trace": "基于论文内容，以下是作者产出M2CL方法的逻辑推演过程：\n\n### 1. 宏观观察与问题引入（“讲故事”逻辑）\n\n*   **单一视角的局限**：单个LLM在处理复杂推理或多步任务时，受限于单一的解题视角，难以探索广泛的解空间。\n*   **协作范式的兴起**：为了突破这一局限，研究转向了多智能体讨论（MAD），即通过多个LLM实例持有不同的预设角色进行结构化协作，旨在利用“群体智慧”提升准确性。\n*   **现实困境**：然而，作者发现现有的MAD方法普遍存在**“讨论不一致性”**。即，大多数LLM实例无法达成连贯的解决方案，决策往往被噪声主导而非原则性推理。\n*   **根因定位**：这种不一致性的根源在于**“上下文错位”**。现有的预设上下文往往是静态、僵化且不完整的，它们虽然定义了“做什么”（角色），却缺乏对“如何做”（即如何有效融合其他智能体的信息）的指导。这导致智能体在接收到他人的中间结果时，无法有效整合，甚至产生冲突。\n\n### 2. 研究问题\n\n**如何获得能够持续引导多LLM讨论走向正确共识的上下文？**\n\n### 3. 理论洞察与假设\n\n为了解决上述问题，作者从理论层面解构了影响讨论性能的核心要素（基于定理4.1），提出了一个关键的二元假设：\n\n*   **维度一：初始多样性**。为了覆盖互补的解题视角，初始上下文必须在潜在空间中尽可能“正交”，即彼此差异最大化，以构建一个完备的搜索基础。\n*   **维度二：过程一致性**。为了达成共识，上下文必须在讨论过程中动态演化，以缩小智能体之间的激活差异。\n\n**核心假设**：如果能设计一种机制，在保持初始多样性的同时，动态控制上下文演化的程度，就能在“避免过早收敛于多数噪声”和“达成正确共识”之间找到最佳平衡点。\n\n### 4. 方法论构建\n\n基于上述假设，作者构建了M2CL的完整逻辑框架：\n\n*   **第一步：正交初始化**\n    不再依赖人工随机分配角色，而是通过数学投影方法，从上下文池中筛选出在潜在空间中彼此正交的初始指令。这确保了讨论起点的多样性和覆盖面。\n\n*   **第二步：动态演化**\n    引入一个轻量级的上下文生成器，使其能够根据当前的任务目标和历史讨论记录，在每一轮动态生成新的指令。这解决了静态上下文无法指导信息融合的问题。\n\n*   **第三步：自适应权衡**\n    为了解决“探索”与“收敛”的矛盾，设计了一个自调节机制（通过拉格朗日对偶性实现的参数$\\alpha$）。\n    *   **讨论初期**：机制允许较大的上下文调整，鼓励智能体保持差异，进行广泛探索。\n    *   **讨论后期**：机制逐渐收紧约束，强制智能体对齐上下文与输出，从而在避免陷入“多数噪声”的前提下，逐步收敛至正确的共识。"
                },
                {
                    "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
                    "arxiv_id": "2602.02196",
                    "authors": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding, Kanzhi Cheng, Jian Zhang, Tao Qin, Jun Liu, Qika Lin",
                    "summary": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **TIDE (Test-time Improvement Diagnostic Evaluation)** 框架，这是一个用于诊断和评估 LLM 智能体在测试时如何通过与环境交互进行自我改进的框架。 *   论文定义的 **Test-Time Improvement (TTI)** 范式，本质上就是智能体通过迭代交互和反馈进行自我完善的过程，这直接对应了研究目标中的 **“自我演化”**。 *   这不是一篇简单的应用论文（非特定领域应用），也不是基础设施论文，而是针对智能体演化机制的深入分析和评估方法论的提出。 2.  **正面指标 (第二步)**: *   **自我演化**: 论文核心关注 TTI，即智能体在测试时的性能提升，属于典型的自我演化机制。 *   **智能体能力**: 论文深入分析了 **Memory** (累积记忆负担)、**Self-Correction** (错误行为后的适应) 以及 **Planning/Reasoning** (内部推理与交互动态的对比)。 *   **核心范式**: 明确属于 `Agentic AI` 和 `LLM-based Agents`。 3.  **排除标准 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术，因此不在排除范围内。 4.  **特殊与模糊情况 (第四步)**: *   虽然这是一篇评估论文，但它评估的是“自我演化”的机制（TTI）。它不仅提供了评估指标，还揭示了改进智能体性能的关键在于优化“智能体与环境的交互动态”，这对构建更好的自我演化智能体具有直接的指导意义。因此，它符合“构建、改进或演化 LLM 智能体”的核心目标。 **结论**: 该论文通过提出诊断框架来解析 LLM 智能体的自我演化机制（Test-Time Improvement），深入探讨了记忆、反思和交互动态对智能体能力的影响，高度契合“自我演化”和“Agentic AI”的研究方向。",
                    "summary2": "本文旨在解决现有评估指标无法捕捉LLM Agent测试时改进（TTI）动态过程的问题。针对多轮交互轨迹，我们提出了一种基于轨迹的诊断评估框架TIDE，通过AUV、LR和MI三个指标分别量化优化效率、行为适应性及记忆效用。在BlocksWorld、AlfWorld及OSWorld等多个基准环境上，通过广泛的实验验证了其有效性，揭示了优化交互动力学比单纯扩展内部推理更为关键。",
                    "summary_translation": "自主 LLM 智能体的最新进展表明，它们能够通过与环境的迭代交互来提升性能。我们将这种范式定义为 Test-Time Improvement (TTI，测试时改进)。然而，关于 TTI 成功或失败的内在机制尚不明确，且现有的评估指标无法有效捕捉其任务优化效率、错误动作后的行为适应能力，以及工作记忆对任务完成的具体效用。为填补这些空白，我们提出了 Test-time Improvement Diagnostic Evaluation (TIDE，测试时改进诊断评估)，这是一个智能体无关且环境无关的框架，将 TTI 分解为三个全面且相互关联的维度。该框架衡量 (1) 任务完成的整体时间动态，并 (2) 识别性能是否主要受限于递归循环行为，或 (3) 受限于过重的累积记忆。通过在多种智能体和环境中的广泛实验，TIDE 强调，提升智能体性能不仅需要扩展内部推理能力，更需要显式优化智能体与环境之间的交互动态。",
                    "inspiration_trace": "基于对论文《TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原：\n\n### 1. 宏观问题与引入逻辑\n\n作者在Introduction中构建了一个层层递进的“故事”逻辑，旨在揭示当前LLM智能体评估中的核心盲区：\n\n1.  **现象观察**：LLM智能体（如编程、GUI智能体）正通过与环境的多轮交互来处理复杂的现实任务。这表明单纯的内部推理已不足以应对复杂且不可预测的环境。\n2.  **核心概念提出**：智能体必须具备一种关键的自适应能力，即通过持续交互积累经验并迭代修正行动。作者将这种动态改进过程定义为 **Test-Time Improvement (TTI)**。\n3.  **揭示痛点**：尽管TTI对于智能体自主性至关重要，但目前对于这种改进是如何展开、停滞或恶化的机制，仍缺乏严谨的理解。\n4.  **批判现状**：现有的评估指标（如成功率SR、交互轮数）存在严重缺陷。它们将信息丰富的轨迹压缩为单一的二元结果，或者混淆了真正的行为修正与无效的重复动作，导致无法捕捉任务优化效率、行为适应性以及工作记忆的具体效用。\n5.  **逻辑落脚**：TTI反映了交互式优化的三个基本方面（效率、适应性、记忆利用），而现有工具无法诊断这些方面，因此需要一个新的评估范式。\n\n### 2. 核心研究问题\n\n基于上述引入逻辑，作者旨在解决的核心问题可总结为：\n\n**如何构建一个系统性的诊断评估框架，以量化LLM智能体在测试时交互过程中的优化效率、行为适应机制以及记忆效用，从而揭示其性能改进的动态本质？**\n\n---\n\n### 3. 思想演进与逻辑链\n\n为了回答上述问题，作者的思考过程经历了从现象解构到方法论确立的四个阶段：\n\n#### 第一阶段：从“结果导向”转向“过程导向”\n*   **思考起点**：传统评估只看“最后做没做对”（Success Rate）。但这忽略了“怎么做到的”。\n*   **逻辑推演**：两个智能体可能最终都成功了，但一个是一步到位，一个是试错了一百次。现有的SR指标将这两者等同对待，掩盖了智能体在交互过程中的“进化”能力。\n*   **初步假设**：我们需要一个能反映**时间维度**上性能变化的指标，而不仅仅是一个静态的终点值。\n\n#### 第二阶段：解构“改进”的三大支柱\n作者意识到TTI是一个复杂的复合体，必须将其拆解才能进行有效诊断。通过分析智能体与环境的交互本质，作者提炼出三个相互关联的维度：\n\n1.  **优化效率**：智能体将交互预算转化为任务进度的速度有多快？（对应RQ I）\n2.  **行为适应**：当遇到错误时，智能体是在真正地修正策略，还是在无效地死循环？（对应RQ II）\n3.  **记忆效用**：积累的交互历史是帮助了决策，还是成为了认知负担？（对应RQ III）\n\n#### 第三阶段：针对每个维度的数学化建模\n为了将上述抽象概念转化为可测量的指标，作者进行了针对性的逻辑建模：\n\n*   **针对效率**：将性能演化视为离散时间过程。不再看终点，而是看累积成功率曲线 $P_t$。为了量化这个曲线的“好坏”，引入积分概念，计算曲线下的面积，即 **AUV (Area Under Variation)**。这奖励早期收敛和持续改进。\n*   **针对适应**：将轨迹视为状态空间上的图。真正的适应是探索新路径，而失败是重复旧路径。通过检测轨迹图中的“环”来区分二者。定义 **LR (Loop Ratio)** 来量化无效循环的比例。\n*   **针对记忆**：采用控制变量法（消融实验）。对比“有记忆”和“无记忆”两种情况下的性能差异。定义 **MI (Memory Index)** 为两者AUV之差，以此剥离出记忆的纯粹贡献。\n\n#### 第四阶段：整合与验证\n*   **框架形成**：将AUV、LR、MI整合为 **TIDE** 框架。这是一个轻量级、与具体智能体和环境无关的诊断工具。\n*   **逻辑闭环**：通过实验验证，作者发现单纯扩大模型规模并不一定能解决TTI问题（例如大模型也可能陷入死循环或受记忆干扰）。这反过来证明了作者的假设：**优化智能体性能不能仅靠扩展内部推理，必须显式地优化智能体与环境的交互动力学。**\n\n### 总结\n作者的思考路径是从**发现现有评估的“静态盲区”**出发，通过**定义TTI这一动态过程**，进而**将过程解构为效率、适应、记忆三个维度**，最后**为每个维度设计具体的数学指标（AUV, LR, MI）**，从而构建出一套能够透视智能体“黑盒”交互过程的诊断体系。"
                },
                {
                    "title": "SIDiffAgent: Self-Improving Diffusion Agent",
                    "arxiv_id": "2602.02051",
                    "authors": "Shivank Garg, Ayush Singh, Gaurav Kumar Nayak",
                    "summary": "Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \\modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 定义**：论文的核心是构建了一个名为 SIDiffAgent 的“免训练智能体框架”。这不仅仅是应用现有模型，而是设计了一个新的智能体架构来解决特定问题，符合第一步中“构建、改进或演化 LLM智能体”的保留标准。 2.  **高度契合“自我演化”与“单智能体”方向**： *   **自我演化**：论文明确提出了“迭代式自我改进”机制，通过“存储以往经验的记忆数据库”来指导智能体的后续行动。这完全符合筛选标准中关于“自我演化”的定义，即通过经验、反思或环境反馈进行自我完善。 *   **单智能体能力**：该智能体具备自主规划（管理提示词工程）、自我反思（检测并纠正糟糕的生成）和工具使用（调用 Qwen 家族模型和扩散模型）的能力，这些都是核心关注点。 3.  **关于视觉/多模态排除规则的判定**：虽然论文涉及文生图扩散模型（视觉领域），但根据第三步和第四步的规则，视觉模型在这里是作为智能体感知环境和执行任务的**工具**，而非研究的核心。论文的研究重点在于智能体如何通过自我演化机制来控制和优化这些工具的输出，而非改进视觉模型本身。 4.  **符合特殊情况的“保留”规则**：根据第四步关于“自我演化的应用”的规则，即使该论文应用在图像生成这一特定领域，但由于其核心贡献在于提出了一种新的“自我演化”机制（记忆库驱动的迭代改进），因此符合保留条件。 综上所述，该论文提出了一种具备记忆和自我改进能力的智能体框架，属于 Agentic AI 中的自我演化与单智能体研究范畴。",
                    "summary2": "本文旨在解决文本到图像扩散模型中存在的提示词敏感性、语义歧义及生成伪影等问题。针对文本到图像生成任务，我们提出了一种名为SIDiffAgent的训练无关多智能体框架，该框架利用Qwen模型族实现提示词优化、自适应负提示生成及基于记忆的迭代自我改进。我们在GenAIBench和DrawBench数据集上通过VQA Score验证了其有效性，显著优于现有最先进的模型。",
                    "summary_translation": "文本到图像扩散模型彻底变革了生成式人工智能，实现了高质量且照片级逼真的图像合成。然而，其实际部署仍受限于若干局限性：对提示词措辞的敏感性、语义解释的歧义（例如，“mouse”指代动物与计算机外设的区别）、诸如解剖结构扭曲等伪影，以及对精心设计的输入提示词的需求。现有方法通常需要额外的训练，且可控性有限，这限制了它们在现实世界应用中的适应性。我们提出了自改进扩散智能体，这是一个免训练的智能体框架，利用 Qwen 系列模型来解决这些挑战。SIDiffAgent 自主管理提示词工程，检测并纠正生成效果不佳的图像，并执行细粒度的伪影移除，从而产出更可靠且一致的输出。该框架进一步通过在数据库中存储过往经验记忆，融入了迭代式自改进机制。该过往经验数据库随后用于在智能体流程的每个阶段注入基于提示词的引导。SIDiffAgent 在 GenAIBench 上取得了 0.884 的平均 VQA 得分，显著优于开源模型、专有模型以及其他智能体方法。我们将在论文被录用后公开发布代码。",
                    "inspiration_trace": "基于对论文《SIDiffAgent: Self-Improving Diffusion Agent》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从现状到痛点，再到机遇的完整叙事链条：\n\n1.  **现状：** 文生图扩散模型已经取得了革命性的成功，能够生成高质量、照片级逼真的图像。\n2.  **转折（问题）：** 尽管技术强大，但在实际部署中仍存在显著瓶颈。模型对提示词的措辞极其敏感，且存在语义歧义（如“mouse”是动物还是电脑外设），常产生解剖结构扭曲等伪影，且需要用户精心设计提示词。\n3.  **归因（根因分析）：** 这些问题的核心在于**“意图鸿沟”**。\n    *   用户的自然语言往往是模糊、非结构化的。\n    *   模型训练依赖的是经过高度筛选和描述性强的专业标注数据。\n    *   这种分布不匹配导致用户意图无法被模型准确捕捉，且提示词往往“欠指定”，迫使模型进行盲目的猜测。\n4.  **现有方案的局限：** 现有的多智能体方法（如 T2I-Copilot）虽然尝试解决歧义，但缺乏细粒度的控制能力和鲁棒的伪影修正机制。更重要的是，多智能体系统普遍存在**协调困难**，各智能体之间缺乏对彼此能力和意图的理解（即缺乏“心智理论” Theory of Mind），且无法从过去的经验中学习。\n5.  **切入点：** 目前尚无工作将“心智理论”应用于扩散模型智能体，也缺乏一种无需重新训练即可利用过往经验进行自我迭代的框架。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个无需训练的文生图智能体框架，使其能够通过‘心智理论’实现智能体间的协调，并利用过往的生成轨迹（记忆）进行自我迭代，从而在无需重新训练模型的情况下，有效弥合用户意图与生成结果之间的鸿沟？”**\n\n---\n\n### 三、 思想演进与逻辑推演链\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 观察与痛点识别\n*   **观察：** 现有的文生图模型（如 Stable Diffusion, DALL-E 等）虽然底座能力很强，但在面对普通用户时表现不稳定。用户说“一只猫”，模型可能生成任何猫；用户说“没有雨伞”，模型可能忽略。\n*   **深层思考：** 这不仅仅是模型生成能力的问题，而是**“接口”**的问题。用户的自然语言与模型理解的“机器语言”之间存在巨大的翻译误差。直接优化模型本身（如继续训练）成本高昂且不灵活。\n\n#### 2. 假设提出：从“优化模型”转向“优化流程”\n*   **假设 1（流程优于模型）：** 既然底座模型已经足够好，我们不需要改变模型权重，只需要在输入端和输出端加一层“智能包装”。\n*   **假设 2（代理化）：** 人类画师在作画前会构思、打草稿、修正。我们可以用多个 AI 智能体来模拟这个过程：一个负责理解意图，一个负责细化描述，一个负责检查错误。\n\n#### 3. 方案迭代：从“单一智能体”到“多智能体协作”\n*   **初步构想：** 用一个大语言模型（LLM）把用户的简单 prompt 改写成复杂的 prompt。\n*   **发现不足：** 单一 LLM 容易产生幻觉，且无法处理生成后的图像错误（如多画了手指）。\n*   **进阶构想（多智能体）：** 引入分工。\n    *   **生成编排器：** 负责把 prompt 拆解（意图分析、创意分析、细化）。\n    *   **评估者：** 生成完图后，用视觉语言模型（VLM）检查图好不好，对不对。\n    *   **修正者：** 如果不好，用编辑模型局部修补，而不是重画。\n\n#### 4. 理论升华：引入“心智理论”与“记忆”\n*   **新问题：** 即使有了多个智能体，它们之间是割裂的。如果“创意分析”智能体总是给“生成器”出难题，导致生成失败，系统并不知道。\n*   **理论引入：** 借鉴心理学中的**“心智理论”**。智能体不仅要想自己做什么，还要预测其他智能体会做什么、擅长什么。\n*   **核心突破（自我改进）：** 如何让系统变聪明？\n    *   **构想：** 记录每一次生成的全过程（Prompt -> 中间步骤 -> 结果 -> 评分）。\n    *   **机制：** 当遇到新 Prompt 时，去数据库里找以前类似的“轨迹”。如果以前类似的 Prompt 在“负向提示词”环节失败了，这次就提前告诉智能体：“上次这里踩坑了，这次要注意”。\n    *   **结果：** 形成一个**无需训练的反馈循环**。系统用得越多，积累的经验越多，生成的质量就越高。\n\n#### 5. 最终方法论构建\n基于上述思考，作者最终构建了 **SIDiffAgent** 框架，其逻辑闭环如下：\n\n1.  **输入处理：** 通过多子智能体（创意、意图、细化、负向提示）将模糊的用户意图转化为模型可执行的精确指令。\n2.  **生成与评估：** 生成图像后，立即进行多维度的美学和一致性评估。\n3.  **反馈修正：** 如果不达标，利用编辑模型进行针对性修补。\n4.  **经验沉淀：** 将整个生成过程（成功经验与失败教训）结构化存入数据库。\n5.  **经验注入：** 在下一次任务开始前，通过检索增强生成（RAG）提取历史经验，并作为“指导”注入到各个子智能体的决策节点中，实现**自我进化**。\n\n---\n\n**总结：** 作者的思考路径从**“模型能力的局限性”**转向**“人机交互的接口问题”**，进而利用**“多智能体协作”**解决复杂性，最后通过引入**“记忆与心智理论”**解决了系统的适应性与进化问题，最终实现了一个无需重新训练即可自我迭代的文生图智能体。"
                },
                {
                    "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
                    "arxiv_id": "2602.02050",
                    "authors": "Zeping Li, Hongru Wang, Yiwen Zhao, Guanhua Chen, Yixia Li, Keyang Chen, Yixin Cao, Guangnan Ye, Hongfeng Chai, Mengdi Wang, Zhenfei Yin",
                    "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中关于“工具使用”的核心研究。 1.  **核心判断**: *   论文的核心贡献是提出了一种基于“熵降低”的监督信号和两种奖励策略（稀疏结果奖励和密集过程奖励），用于优化LLM智能体的工具使用行为。 *   这属于构建和改进LLM智能体的方法论，旨在解决智能体在长轨迹中过度调用工具和低质量调用的问题，而非仅仅将智能体作为工具应用到特定领域。 2.  **正面指标匹配**: *   **核心范式**: 论文明确研究对象是 \"Tool-using agents based on Large Language Models\"。 *   **智能体能力**: 论文聚焦于 `Tool Use / Tool Augmentation`，这是Agentic AI的关键能力之一。同时，通过奖励机制优化行为也隐含了 `Self-Correction` 和 `Iterative Improvement` 的思想。 *   **演化机制**: 论文通过引入奖励信号来引导智能体减少冗余调用、提高调用质量，这是一种使智能体行为更加高效和自适应的优化过程。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然在数学推理和多跳问答任务上进行了实验，但这些仅用于验证工具使用优化方法的有效性，论文本身并非针对这些领域的应用型研究。 综上所述，该论文致力于改进LLM智能体的核心组件（工具使用机制），提升其执行效率和性能，符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM智能体在长轨迹中工具调用过多且质量低的问题。针对工具使用优化场景，我们提出了一种基于熵减少作为监督信号的Tool-enhanced Entropy-guided Policy Optimization (TEPO)方法，包含稀疏和密集两种奖励策略。在AIME、HotpotQA、GAIA等数据集上，通过工具调用数量和准确率等指标验证了其有效性，显著提升了工具使用效率和推理性能。",
                    "summary_translation": "基于 Large Language Models (LLMs，大语言模型) 的 Tool-using agents (工具使用智能体) 在数学推理和 Multi-hop question answering (多跳问答) 等任务中表现优异。然而，在 Long trajectories (长轨迹) 中，智能体往往会触发过量且低质量的 Tool calls (工具调用)，这增加了延迟并降低了 Inference performance (推理性能)，从而使得管理 Tool-use behavior (工具使用行为) 成为一项挑战。在本研究中，我们进行了基于 Entropy (熵) 的 Pilot experiments (试点实验)，并观察到 Entropy reduction (熵减) 与高质量的 Tool calls (工具调用) 之间存在强烈的正相关关系。基于这一发现，我们提出将 Entropy reduction (熵减) 作为 Supervisory signal (监督信号)，并设计了两种 Reward strategies (奖励策略) 以满足优化 Tool-use behavior (工具使用行为) 的不同需求。Sparse outcome rewards (稀疏结果奖励) 提供粗粒度的 Trajectory-level (轨迹级) 指导以提高效率，而 Dense process rewards (密集过程奖励) 则提供 Fine-grained supervision (细粒度监督) 以增强性能。在多个不同领域的实验表明，这两种 Reward designs (奖励设计) 均能改善 Tool-use behavior (工具使用行为)：与 Baselines (基线模型) 的平均水平相比，前者将 Tool calls (工具调用) 减少了 72.07%，而后者将性能提升了 22.27%。这些结果确立了 Entropy reduction (熵减) 作为增强 Tool-use behavior (工具使用行为) 的 Key mechanism (关键机制) 的地位，使智能体能够在 Real-world applications (现实应用) 中更加 Adaptive (自适应)。",
                    "inspiration_trace": "基于您提供的论文内容，我为您系统性地推演了作者产出这篇论文的思考过程。以下是逻辑链条的还原：\n\n### 第一部分：宏观背景与问题引入（Introduction 的“讲故事”逻辑）\n\n作者首先构建了一个从“成功”到“隐患”再到“现有方案不足”的叙事逻辑，旨在引出研究的必要性。\n\n1.  **现状与机遇：**\n    大语言模型（LLM）作为工具使用代理在数学推理、多跳问答等复杂任务上表现出色。这种“推理+行动”的范式已成为解决复杂问题的主流。\n\n2.  **核心痛点：**\n    然而，在长轨迹推理中，代理往往会出现“工具滥用”行为——即调用工具过于频繁或质量低下。这不仅增加了计算成本和延迟，还会破坏推理过程，导致最终性能下降。\n\n3.  **现有方案的局限性：**\n    *   **稀疏奖励：** 仅关注最终答案的正确性。这虽然能教会模型用工具，但反馈太滞后，无法让模型关注“何时调用工具”这一行为本身。\n    *   **过程奖励：** 虽然能提供细粒度的指导，但通常依赖人工定义的规则或外部标注。在复杂的长期推理任务中，设计这些规则极其困难且难以泛化。\n\n4.  **逻辑缺口：**\n    目前缺乏一种**既原则性又轻量级**的信号，能够在不需要外部标注的情况下，实时评估长轨迹推理中每一步工具调用的质量。\n\n---\n\n### 第二部分：核心研究问题\n\n基于上述逻辑缺口，作者试图回答的核心问题是：\n\n**“能否利用模型内部的不确定性信号（如熵），构建一种无需外部标注的内在监督机制，以优化LLM代理在长轨迹推理中的工具调用行为（兼顾效率与性能）？”**\n\n---\n\n### 第三部分：思想演进与方法论形成（思考过程推演）\n\n从发现问题到提出解决方案，作者的思维经历了以下四个阶段的演进：\n\n#### 阶段一：现象观察与数据挖掘\n*   **思考起点：** 既然外部标注很难获取，模型自身在推理过程中是否隐藏着某种能够反映“工具调用好坏”的信号？\n*   **实验验证：** 作者进行了基于熵的预实验。他们测量了工具调用前后模型预测熵的变化（Delta Segment Entropy）。\n*   **关键发现：** 数据显示了一个强相关性——**高质量的工具调用往往伴随着熵的显著降低**（不确定性减少），而低质量的调用则导致熵增加或不变。这一现象在不同领域（数学、搜索等）和不同模型上均成立。\n\n#### 阶段二：理论假设与机制确立\n*   **理论支撑：** 借鉴认知负荷理论，作者认为高质量的工具调用应当减少不必要的处理或引入有效信息，从而降低模型的认知负荷（即不确定性）。\n*   **核心假设：** “熵减”可以作为衡量工具有效性的**内在信号**。它独立于具体任务，无需外部评估者。\n*   **定义指标：** 正式定义了“Delta Segment Entropy”（$\\Delta H$），将其作为判断工具调用质量的标尺。\n\n#### 阶段三：策略分化与算法设计\n*   **需求分析：** 作者意识到，在实际应用中，对工具优化的需求是多样的：有时需要极致的**效率**（少调用），有时需要极致的**性能**（答得对）。单一信号无法同时满足两者。\n*   **设计思路：** 基于“熵减”这一核心信号，设计了两种截然不同的奖励策略，并将其统一在强化学习框架（GRPO）下：\n    1.  **针对效率的稀疏奖励：**\n        *   *逻辑：* 如果一个轨迹中“熵减”调用的比例高，或者总调用次数少，就给予高分。\n        *   *目的：* 从全局层面约束模型，鼓励它用更少、更精准的调用完成任务。\n    2.  **针对性能的密集奖励：**\n        *   *逻辑：* 只要某一步工具调用导致了熵减，就立即给予奖励。\n        *   *目的：* 提供细粒度的过程监督，引导模型在每一步都做出能降低不确定性的决策，从而提升整体推理能力。\n\n#### 阶段四：综合验证与框架提出\n*   **方法整合：** 将上述两种策略整合为 **TEPO (Tool-enhanced Entropy-guided Policy Optimization)** 框架。\n*   **实验反馈：** 通过实验验证，TEPO-sparse 成功大幅减少了工具调用（提升效率），TEPO-dense 显著提升了任务准确率（提升性能）。\n*   **结论升华：** 证实了“熵减”不仅是模型状态的反映，更是优化工具使用行为的关键机制。\n\n---\n\n### 总结：逻辑链全景\n\n1.  **宏观问题：** LLM代理在长轨迹中滥用工具，现有外部监督成本高、泛化难。\n2.  **微观洞察：** 发现“熵减”与高质量工具调用存在强因果关联。\n3.  **核心转化：** 将“熵减”从一种物理现象转化为一种可计算的、内在的监督信号。\n4.  **方法落地：** 根据不同优化目标（效率 vs 性能），将信号分别映射为稀疏的轨迹级奖励和密集的过程级奖励。\n5.  **最终产出：** 提出TEPO框架，实现了无需外部标注的工具行为优化。"
                },
                {
                    "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows",
                    "arxiv_id": "2602.02034",
                    "authors": "Ananya Joshi, Michael Rudow",
                    "summary": "Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合多智能体方向）**： 论文的核心贡献在于提出了一种**多智能体系统**的新框架。作者将多智能体工作流形式化为有限视界马尔可夫决策过程（MDP），并引入了“约束过程图”来管理智能体间的交互和任务流转。这直接对应了研究焦点中的“多智能体”方向，特别是关于智能体间的协作、通信和协调机制的研究。 2.  **正面指标（Agentic AI & Multi-Agent）**： 论文明确涉及 `Multi-Agent Systems (MAS)`，探讨了智能体如何对应特定角色（如内容审查、法律审查），以及如何通过预定义的转换进行任务升级和协调。这属于典型的Agentic AI架构设计，旨在解决复杂工作流中的协调问题。 3.  **排除标准分析（应用 vs. 核心贡献）**： 虽然论文的案例研究涉及“AI安全评估”和“自残检测”（属于Safety领域），但论文的**主要贡献**并非提出新的安全算法或对齐技术，而是提出了一种**通用的多智能体工作流架构**来处理不确定性和人工审查的介入。安全评估仅作为验证该架构有效性的应用场景，而非研究主题本身。因此，不应被“安全与对齐”的排除规则所剔除。 4.  **特殊与模糊情况处理**： 论文涉及“规划”和“多步推理”，但这是在多智能体协作完成复杂工作流的语境下进行的，属于Agentic的范畴，而非单纯的LLM基础推理能力提升。 综上所述，该论文通过构建新的多智能体框架来改进LLM智能体在复杂任务中的协作与决策能力，完全符合“LLM智能体及其演化”中关于多智能体系统的研究目标。",
                    "summary2": "本文旨在解决受监管环境中单智能体 LLM 难以处理不确定性和协调的问题。针对合规工作流场景，我们提出了一种基于 DAG 约束的有界视界 MDP 多智能体框架，利用 Monte Carlo estimation 量化不确定性。在 AEGIS 2.0 AI Safety Benchmark 上，通过 Accuracy 和 human review load 等指标验证了其有效性，结果显示准确率提升 19%，人工审查减少 85 倍。",
                    "summary_translation": "基于 Large language model (LLM，大语言模型) 的智能体正越来越多地被用于在合规和尽职调查等受监管环境中执行复杂的多步骤工作流。然而，许多智能体架构主要依赖于对单个智能体的提示工程，这使得难以观察或比较模型如何在相互关联的决策阶段以及人工监督下处理不确定性和协调问题。我们引入了一个多智能体系统，该系统被形式化为具有有向无环结构的有限视界马尔可夫决策过程。每个智能体对应于特定的角色或决策阶段（例如合规工作流中的内容、业务或法律审查），并具有代表任务升级或完成的预定义转换。认知不确定性在智能体层面使用蒙特卡洛估计进行量化，而系统层面的不确定性则通过马尔可夫决策过程在自动标记状态或人工审查状态下的终止来体现。我们通过一个针对自残检测的 AI 安全评估案例研究来说明该方法，该案例被实现为一个多智能体合规系统。结果表明，该方法相较于单智能体基线有所改进，包括准确率提高 19%，所需人工审查量减少 85 倍，以及在部分配置下缩短了处理时间。",
                    "inspiration_trace": "基于对论文《Constrained Process Maps for Multi-Agent Generative AI Workflows》的深度分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分通过“宏观背景—现实冲突—现有缺陷—解决思路”的漏斗式逻辑，逐步引出研究动机：\n\n1.  **宏观背景（现实需求）：** 在合规、尽职调查等受监管环境中，真实的工作流程往往是多步骤、可分解的，遵循标准操作程序（SOP）和既定的升级路径（例如从业务层升级到法务层）。\n2.  **技术潜力与现实冲突：** 生成式AI（GenAI）有潜力简化这些繁重的人力流程，但现有的应用方式（主要是单一智能体的提示工程）与受监管环境对“可信度”和“可解释性”（用于审计）的严苛要求存在冲突。\n3.  **现有方法的缺陷（痛点）：** 现有的单一智能体方法存在三个核心问题：\n    *   **黑盒化：** 依赖对过程的潜在表示，不可见。\n    *   **脱节：** 偏离了现有的标准工作流，难以落地。\n    *   **缺乏不确定性量化（UQ）：** 无法满足审计和实际操作中对决策可靠性的评估需求。\n4.  **解决思路：** 引入一个遵循现有工作流和SOP的多智能体框架，通过有向过程图定义任务，并显式地量化和传递不确定性。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题可总结为：\n\n**“我们如何构建一个多智能体生成式AI工作流，使其既能与现有的标准操作程序（SOP）和流程图保持一致，又能显式地量化并协调智能体自身及系统层面的不确定性，从而在保证可审计性的同时提升决策性能？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 观察与解构：从“单一黑盒”到“结构化团队”\n*   **观察：** 现实世界的高风险决策（如合规审查）不是由一个人完成的，而是由一个具有明确分工（内容、法务、风险）和层级关系的团队完成的。\n*   **反思：** 现有的LLM应用试图用一个超级提示词让一个模型完成所有事情，这违背了人类处理复杂事务的组织逻辑，也导致了不可解释性。\n*   **演进：** **放弃单一智能体，转向多智能体。** 每个智能体不应是通用的，而应对应现实工作流中的一个特定角色（节点）。\n\n#### 2. 形式化假设：用数学语言描述“工作流”\n*   **思考：** 如何让AI系统严格遵循人类制定的SOP，防止模型“乱跑”或无限递归？\n*   **抽象：** 现实的工作流图本质上是**有向无环图（DAG）**。任务从一个节点流向另一个节点，有明确的起点和终点。\n*   **假设：** 如果将多智能体系统形式化为一个**有限视界的马尔可夫决策过程（MDP）**，并将状态转移限制在DAG的边上，就能强制AI遵循既定的业务流程。\n*   **方法论产出：** **Constrained Process Maps（受约束的过程图）。** 定义状态$S$为智能体节点，边$E$为升级路径，确保所有轨迹最终都会终止（输出标签或转交人工）。\n\n#### 3. 不确定性的双重思考：从“模型自信”到“系统风险”\n*   **思考：** 在合规领域，不仅要结果对，还要知道模型什么时候“拿不准”。现有的方法往往缺乏这种量化。\n*   **分层思考：** 不确定性分为两个层次：\n    *   **微观（智能体层）：** 单个LLM对自己判断的信心有多大？（认知不确定性）。\n    *   **宏观（系统层）：** 这种不确定性如何在多步骤流程中传递和累积？\n*   **解决方案：**\n    *   **针对微观：** 不依赖模型内部参数，而是采用**蒙特卡洛估计**。让同一个智能体对同一输入进行多次采样（$n$次），观察输出分布的离散程度。\n    *   **针对宏观：** 利用MDP的结构。如果智能体输出“不确定”，则触发MDP的状态转移（升级），最终流向“人工审查”这一终止状态。\n*   **方法论产出：** **蒙特卡洛MDP框架。** 通过采样向量$a_t$来量化局部不确定性，通过MDP的终止条件来控制系统级风险。\n\n#### 4. 逻辑闭环：从“替代人类”到“增强与审计”\n*   **思考：** 这个系统的目标不是完全取代人类，而是作为一个“过滤器”或“预处理器”。\n*   **机制：** 系统设计为——简单案例自动处理（高置信度），复杂案例自动升级（低置信度）。\n*   **价值验证：** 这种设计不仅提高了准确率（通过专业化分工），还大幅降低了人工审查的负担（通过不确定性过滤），同时因为遵循了DAG结构，每一步决策都是可追溯的，满足了审计需求。\n\n---\n\n### 总结\n\n作者的思考路径是从**现实业务流程的严谨性**出发，发现**现有AI技术的无序性**，进而通过**引入图论（DAG）和运筹学（MDP）**来约束AI的行为，最后利用**统计学方法（蒙特卡洛模拟）**来解决AI模型本身的不确定性问题。整个逻辑链条旨在将“生成式AI的创造力”关进“企业级合规流程”的笼子里。"
                },
                {
                    "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation",
                    "arxiv_id": "2602.02029",
                    "authors": "Zhongyuan Lyu, Shuoyu Hu, Lujie Liu, Hongxia Yang, Ming LI",
                    "summary": "Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建LLM智能体”的目标**： 论文的核心贡献在于提出了 **R2C (rule-to-constraint) 框架**，这是一个专门设计的 **多智能体流水线**。它不仅仅是将现有的LLM作为工具简单应用于运筹学领域，而是构建了一个新的智能体架构来解决复杂问题。这直接符合筛选标准第一步中的“保留”条件：核心是关于构建LLM智能体或多智能体系统的方法论。 2.  **明确涉及“多智能体”与“自我反思”**： *   **多智能体**：摘要明确指出 R2C 是一个 \"multi-agent pipeline\"，包含解析文本、检索领域知识、实例化模型等不同角色的协作，这完全符合研究焦点中的“多智能体”方向。 *   **自我反思**：论文提到利用 \"reflection mechanism\"（反思机制）来进一步提升性能，这属于智能体“自我反思”和“自我完善”的能力范畴，符合正面指标中的 `Self-Correction` 和 `Self-Reflection`。 3.  **不属于“非演化型应用”的排除项**： 虽然论文的应用场景是“优化问题建模”（属于运筹学领域），但论文的重点在于提出一种新的**智能体框架（R2C）**和**中间表示（CIR）**来增强LLM处理复杂规则的能力，而不是仅仅展示LLM在某个特定任务上的应用效果。因此，它不属于“非演化型应用”的排除范围。 综上所述，该论文在多智能体系统构建和智能体反思机制方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决LLM在从自然语言自动构建优化模型时难以处理复杂操作规则的问题。针对包含复合约束和范式敏感性的复杂优化场景，我们提出了一种基于Canonical Intermediate Representation (CIR) 的Rule-to-Constraint (R2C) 多智能体框架，并在ORCOpt-Bench及IndustryOR等基准上通过Accuracy Rate (AR) 验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "从自然语言描述自动构建 optimization models (优化模型) 是 operations research (运筹学) 领域日益关注的焦点，然而当前 LLM-based approaches (基于大语言模型的方法) 在处理复杂 operational rules (运营规则) 所需的 composite constraints (复合约束) 和适当的 modeling paradigms (建模范式) 时仍面临挑战。为解决这一问题，我们引入了 Canonical Intermediate Representation (CIR) (规范中间表示)：一种由大语言模型在问题描述与 optimization models (优化模型) 之间显式生成的模式。CIR 通过 constraint archetypes (约束原型) 和候选 modeling paradigms (建模范式) 对 operational rules (运营规则) 的语义进行编码，从而实现了规则逻辑与其数学实例化之间的解耦。基于新生成的 CIR 知识库，我们开发了 rule-to-constraint (R2C) framework (规则到约束框架)，这是一个 multi-agent pipeline (多智能体流水线)，能够解析问题文本，通过检索领域知识综合 CIR 实现，并实例化 optimization models (优化模型)。为了系统评估从规则到约束的推理能力，我们在新构建的包含丰富 operational rules (运营规则) 的 benchmark (基准测试) 以及既往工作的 benchmark (基准测试) 上对 R2C 进行了评估。大量实验表明，R2C 在所提出的 benchmark (基准测试) 上实现了最先进的准确率（47.2% Accuracy Rate）。在文献中的现有 benchmark (基准测试) 上，R2C 取得了极具竞争力的结果，性能接近 proprietary models (专有模型)（如 GPT-5）。此外，结合 reflection mechanism (反思机制)，R2C 实现了进一步的性能提升，并在部分 benchmark (基准测试) 上刷新了已报道的最佳结果。",
                    "inspiration_trace": "基于对论文Introduction和Abstract的深度分析，以下是对作者核心方法逻辑链的系统性推演，还原了从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 Introduction中的“讲故事”逻辑与核心研究问题\n\n#### 1. 逻辑演进链条\n作者在Introduction中构建了一个典型的“领域现状-技术趋势-核心痛点-深层原因-解决思路”的叙事逻辑：\n\n1.  **领域背景与价值**：运筹学（OR）旨在提升复杂系统性能，但将现实世界问题转化为精确的数学模型是主要挑战，这需要深厚的专业知识。\n2.  **技术趋势与机遇**：大语言模型（LLMs）的出现为自动化这一过程（从自然语言描述到优化模型和代码）提供了新的可能性，成为当前的研究热点。\n3.  **现有方法的局限**：尽管现有的LLM方法（如直接提示、多智能体、微调）取得了一定进展，但在面对**复杂的运营规则**时仍然存在根本性缺陷。\n4.  **痛点具体化**：作者指出了两个导致LLM失败的核心挑战：\n    *   **组合性**：一个自然语言规则往往需要分解为多个数学约束，LLM容易遗漏。\n    *   **范式敏感性**：同一个规则在不同的建模范式（如离散时间 vs 连续时间）下需要完全不同的数学表达，LLM难以自适应选择。\n5.  **对比分析**：人类专家之所以能成功，是因为他们会显式地分解规则并选择建模范式，而现有LLM方法试图直接进行“文本到约束”的跳跃，缺乏这种中间推理过程。\n6.  **解决思路**：引入一种“规范中间表示”，在自然语言和数学模型之间建立一个显式的语义层，解耦规则逻辑与数学实例化。\n\n#### 2. 核心研究问题\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何通过引入显式的中间语义表示，使大语言模型能够像人类专家一样，准确地将复杂的自然语言运营规则分解并映射到正确的数学建模范式，从而生成可靠的优化模型？”**\n\n---\n\n### 二、 作者产出核心方法的逻辑推演\n\n以下是从宏观问题出发，逐步聚焦到CIR（Canonical Intermediate Representation）和R2C框架的思想演进脉络：\n\n#### 第一阶段：宏观观察与问题定位\n*   **观察**：运筹学建模自动化是刚需，LLM具备强大的语言理解和代码生成能力，看似是完美的解决方案。\n*   **发现**：在实际应用中，简单的“提示词+LLM”模式在处理工业级或学术级复杂问题时经常出错。这些错误往往不是数学计算错误，而是对业务规则理解的偏差或建模方式的不兼容。\n*   **聚焦**：问题的核心不在于LLM“不懂数学”，而在于它不懂“如何将业务逻辑翻译成数学逻辑”。\n\n#### 第二阶段：深入诊断与假设提出\n*   **诊断**：作者深入分析了失败案例，发现LLM面临两个具体的认知障碍：\n    1.  **“一对多”的映射难题**：一句话（如“机器不能中断”）可能对应多个约束（资源占用、时间连续性）。直接翻译容易导致约束不全。\n    2.  **上下文依赖难题**：数学公式的写法取决于全局的建模范式（如用时间索引变量还是连续变量）。LLM在没有明确范式指导的情况下，容易生成不兼容的公式。\n*   **假设**：如果让LLM模仿人类专家的思维过程，先进行“语义理解”和“范式规划”，再进行“数学翻译”，就能规避上述问题。也就是说，我们需要在自然语言和数学代码之间，插入一个**结构化的中间层**。\n\n#### 第三阶段：核心概念抽象\n*   **概念定义**：为了实现上述假设，作者抽象出了**CIR（Canonical Intermediate Representation，规范中间表示）**。\n*   **设计理念**：\n    *   **解耦**：CIR只关注“规则是什么意思”（语义意图），而不关注“公式怎么写”（数学实例化）。\n    *   **模板化**：将常见的运营规则抽象为“约束原型”，每个原型包含多种可能的建模范式（如时间索引范式、网络流范式等）。\n*   **逻辑闭环**：通过CIR，原本复杂的“文本->数学”映射被拆解为两步可靠的映射：“文本->CIR（语义匹配）”和“CIR->数学（范式实例化）”。\n\n#### 第四阶段：方法论构建\n*   **框架设计**：基于CIR的概念，作者设计了**R2C（Rule-to-Constraint）**框架。这是一个多智能体流水线，旨在将上述理论落地。\n*   **流程推演**：\n    1.  **Extractor（提取者）**：负责把自然语言变成结构化的规则（解决“看不懂”的问题）。\n    2.  **Mapper（映射者）**：这是核心。它利用RAG（检索增强生成）从CIR知识库中找到对应的约束原型，并根据问题特性选择最合适的建模范式（解决“范式敏感性”和“组合性”问题）。\n    3.  **Formalizer（形式化者）**：根据选定的CIR和范式，生成具体的数学公式和代码（解决“写不对”的问题）。\n    4.  **Checker（检查者）**：在每一步之间进行验证，防止错误传播。\n\n#### 第五阶段：验证与迭代\n*   **评估需求**：现有的基准测试太简单，无法测试“复杂规则分解”的能力。\n*   **行动**：构建了**ORCOpt-Bench**，专门包含那些需要复杂推理的运营规则。\n*   **结果验证**：通过实验证明，引入CIR的R2C框架在处理复杂规则时，显著优于直接提示的LLM，甚至超过了GPT-5等闭源模型，验证了“中间语义层”假设的有效性。\n\n---\n\n**总结**：\n作者的思考路径是从**“LLM直接翻译失败”**的现象出发，深挖出**“规则组合性”与“范式敏感性”**两个本质矛盾，进而提出**“引入中间语义层（CIR）”**的假设，最终通过**“多智能体流水线（R2C）”**将这一假设工程化，实现了从自然语言到优化模型的可靠转化。"
                },
                {
                    "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
                    "arxiv_id": "2602.01983",
                    "authors": "Xintian Shen, Jiawei Chen, Lihao Zheng, Hao Ma, Tao Wei, Kun Zhan",
                    "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心判断（符合）**：论文的核心贡献是提出了一种名为 UCT 的新框架，旨在将 LLM 智能体从单纯的“工具使用者”转变为“工具创造者”。这属于构建和改进 LLM 智能体的方法论，而非简单的应用。 2.  **符合研究焦点（自我演化与单智能体）**： *   **自我演化**：论文明确提出了“自我演化能力”，强调智能体通过“经验复用”和“推理过程中的自适应工具创造”来实现持续改进和自我更新，无需额外训练。这直接对应您研究焦点中的“自我演化”方向。 *   **单智能体能力**：论文涉及了智能体的核心能力，包括“工具使用/增强”以及“记忆”，特别是引入了“记忆巩固机制”来维护工具库。 3.  **排除标准检查**： *   **非演化型应用**：虽然论文在数学和科学推理任务上进行了验证，但其核心在于提出一种通用的“自动化工具构建范式”和演化机制，而非仅解决特定领域问题。 *   **多模态**：尽管标题中包含“Multimodal Reasoning”，但摘要显示研究的核心在于智能体的架构（如何创造工具、如何复用经验），而非视觉或多模态模型本身的技术改进。多模态在此处更多是智能体处理任务的场景，而非研究的核心贡献点，因此不违反排除规则。 综上所述，该论文提出了一种让智能体在推理过程中自我创造工具并持续演化的新范式，高度契合您关于“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有TIR模型工具固定且缺乏自我优化的问题。针对开放式的多模态推理场景，我们提出了一种名为UCT的无训练框架，通过在线任务循环、在线构建循环和离线记忆巩固机制，将智能体从工具使用者转变为工具创造者。我们在包含数学、科学和通用VQA任务的TRBench基准上，通过Correctness指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "现有的 Tool-Integrated Reasoning (TIR，工具集成推理) 模型通过结合外部工具，有效地扩展了 LLMs (Large Language Models，大语言模型) 的问答能力。然而，现实世界的场景提出了许多开放性问题，其中固定工具往往无法满足任务要求。此外，缺乏 self-optimization mechanisms (自优化机制) 意味着错误的工具输出可能会误导 LLMs 的响应。另外，现有工具的构建需要大量的人工投入，进而限制了它们的适用性。鉴于 LLMs 的 reasoning traces (推理轨迹) 蕴含了隐式的问题解决能力，我们提出了 UCT，一种新颖的 training-free (无需训练) 框架，它将 agents (智能体) 从工具使用者转变为工具创造者。该方法收集推理经验并将其提炼为可复用资产。这种方法将智能体从单纯的工具使用者转变为工具创造者，使其能够在 inference (推理) 过程中实现 adaptive tool creation (自适应工具创建) 和 self-updating (自更新)。我们还引入了一种 memory consolidation (记忆巩固) 机制来维护工具库，确保保留的经验记忆在后续推理任务中具有高复用性。这种新颖的 automated tool construction (自动化工具构建) 范式在推理过程中持续改进工具质量，使整个智能体系统能够在无需额外训练的情况下实现进化。大量实验表明，我们的方法作为一种新颖的范式，能够增强 TIR 模型的能力。特别地，在跨多领域数学和科学推理任务的 benchmarks (基准测试) 中，实现了 +20.86%$\\uparrow$ 和 +23.04%$\\uparrow$ 的显著性能提升，这验证了智能体的 self-evolving (自进化) 能力。",
                    "inspiration_trace": "基于对论文《Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从“现有能力”到“核心瓶颈”再到“潜在机遇”的叙事链条：\n\n1.  **背景铺垫（现状）：** 大语言模型（LLMs）在语言理解和复杂推理上已取得显著突破，但为了增强其实用性，现有研究主要依赖引入外部工具（即 Tool-Integrated Reasoning, TIR）来突破模型固有的局限。\n2.  **现实冲突（挑战）：** 真实世界充满了开放性问题，而现有的工具使用范式存在根本性缺陷。\n    *   **刚性限制：** 依赖预定义工具（人工构建）不仅费时费力，且无法穷尽现实任务中千变万化的需求，导致工具覆盖面不足。\n    *   **一次性浪费：** 虽然有方法尝试生成即时代码来解决具体问题，但这些生成的工具往往是“一次性”的，用完即弃，无法沉淀为可复用的资产。\n3.  **关键缺失（痛点）：** 现有的 TIR 框架缺乏“自我优化机制”。这不仅意味着错误的工具输出会误导模型，更意味着模型无法从解决问题的过程中积累经验——它永远只是一个“使用者”，无法进化。\n4.  **洞察转折（机遇）：** 作者观察到，LLMs 的推理轨迹中实际上封装了隐性的问题求解能力。如果能将这些“推理经验”提取出来，就能转化为可复用的资产。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“在不进行额外模型训练的前提下，智能体如何通过复用推理过程中的经验，自主地从被动的‘工具使用者’进化为主动的‘工具创造者’，从而实现能力的持续自我迭代？”**\n\n---\n\n### 三、 思考过程的逻辑演进（从观察到方法论）\n\n为了解决上述问题，作者的思考路径经历了以下四个阶段的演进：\n\n#### 第一阶段：观察与反思——工具的“静态”与“动态”矛盾\n*   **观察：** 现有的 Agent 要么使用人工定义的静态工具（覆盖不全），要么生成临时的动态代码（不可复用）。\n*   **反思：** 为什么模型解决了一个复杂的微积分问题后，下次遇到类似问题还要重新推导一遍？这不仅是算力的浪费，更是经验的浪费。\n*   **初步设想：** 能不能把“解决特定子问题的逻辑”封装成一个函数，存下来下次直接用？\n\n#### 第二阶段：假设提出——将“推理轨迹”固化为“工具资产”\n*   **假设：** LLM 的 Chain-of-Thought（思维链）本质上是一种算法逻辑。如果我们将这种逻辑“蒸馏”成代码形式的工具，那么模型就不再只是产生文本，而是在生产“能力”。\n*   **核心转变：** Agent 的角色需要转变。从“我有问题 -> 找工具 -> 解决问题”转变为“我有问题 -> 没工具 -> 创造工具 -> 解决问题 -> 存入仓库”。\n\n#### 第三阶段：机制设计——如何保证创造的质量与效率？\n*   **挑战 1（安全性）：** 模型自己生成的代码可能是错的，如果直接入库会污染系统。\n    *   *对策：* 引入“沙箱测试”和“审查模型”。在工具入库前，必须经过严格的运行测试和代码审查，确保工具的高质量。\n*   **挑战 2（实时性）：** 在线推理时如果花太多时间造工具，会拖慢任务进度。\n    *   *对策：* 将“任务循环”与“构建循环”解耦。主任务只负责调用，造工具是一个独立的、隔离的后台流程。\n*   **挑战 3（膨胀性）：** 随着时间推移，工具库会无限膨胀，检索变慢。\n    *   *对策：* 引入“离线记忆巩固”。在非推理时段，对工具库进行去重、合并和淘汰低频工具，保持记忆的精简高效。\n\n#### 第四阶段：范式确立——构建“训练无关”的自进化系统\n*   **最终逻辑闭环：** 整个系统不需要更新模型权重，而是通过更新“外部工具库”来实现进化。\n*   **方法论成型：** 提出了 **UCT (User-to-Creator via Training-free experience reuse)** 框架。\n    *   **Online Task Loop：** 负责打仗（解决任务），发现缺武器就申请。\n    *   **Online Build Loop：** 负责造武器（创建工具），严把质量关。\n    *   **Offline Memory Consolidation：** 负责整理军火库（优化记忆），确保常用武器好用。\n\n---\n\n**总结：**\n作者的思考始于对现有工具使用范式“不可持续”和“不可复用”的不满，通过将“推理过程”视为“可被固化的经验”，提出了一套包含在线创造、质量控制和离线优化的完整闭环系统，最终实现了 Agent 在不改变模型参数情况下的自我进化。"
                },
                {
                    "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
                    "arxiv_id": "2602.01869",
                    "authors": "Qirui Mi, Zhijian Ma, Mengyue Yang, Haoxuan Li, Yisen Wang, Haifeng Zhang, Jun Wang",
                    "summary": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合目标**: 论文提出了 ProcMEM 框架，其核心在于构建一种机制，使 LLM 智能体能够从交互经验中自主学习和积累“程序记忆”。这直接对应了研究课题中的“构建、改进或演化 LLM 智能体”以及“自我演化”方向。 2.  **精准匹配核心关注点**: *   **自我演化**: 论文强调智能体通过经验进行自我完善，无需参数更新即可通过 Non-Parametric PPO 和基于分数的维护机制来精炼和积累知识，这符合“自我完善和迭代”的定义。 *   **单智能体**: 论文专注于提升单个智能体在顺序决策任务中的表现，涉及 `Memory`（程序记忆）和 `Planning`（通过 Skill-MDP 将叙事转化为可执行技能）。 3.  **排除标准检查**: *   该论文不是将现有智能体简单应用于特定领域（如医疗、金融），而是提出了一种通用的智能体记忆与演化机制。 *   不涉及安全对齐、多模态视觉或基础设施优化等排除项。 综上所述，ProcMEM 提出了一种新颖的智能体记忆与技能演化机制，属于 Agentic AI 的前沿研究，符合筛选要求。",
                    "summary2": "本文旨在解决LLM智能体在重复场景中因重复推理导致的计算冗余和执行不稳定问题。针对交互经验，我们提出了一种ProcMEM框架，通过Skill-MDP将被动叙事转化为可执行Skills，并利用Non-Parametric PPO实现无参数的过程记忆优化。我们在ALFWorld和TextArena上通过重用率、任务性能和存储效率等指标验证了其有效性。",
                    "summary_translation": "LLM驱动的智能体在 sequential decision-making（序列决策）中表现出色，但往往依赖 on-the-fly reasoning（即时推理），即使在 recurring scenarios（重复出现的场景）中也会重新推导解决方案。这种 experience reuse（经验复用）不足导致了 computational redundancy（计算冗余）和 execution instability（执行不稳定性）。为了弥合这一差距，我们提出了 ProcMEM，这是一个使智能体能够从 interaction experiences（交互经验）中自主学习 procedural memory（程序性记忆）而无需 parameter updates（参数更新）的框架。通过形式化 Skill-MDP（技能马尔可夫决策过程），ProcMEM 将被动的 episodic narratives（片段叙事）转化为由 activation（激活）、execution（执行）和 termination（终止）条件定义的可执行 Skills（技能），以确保 executability（可执行性）。为了在不发生 capability degradation（能力退化）的情况下实现 reliable reusability（可靠的复用性），我们引入了 Non-Parametric PPO（非参数PPO），它利用 semantic gradients（语义梯度）进行高质量的 candidate generation（候选生成），并利用 PPO Gate（PPO门控）进行 robust Skill verification（鲁棒的技能验证）。通过 score-based maintenance（基于分数的维护），ProcMEM 维持了 compact（紧凑）且 high-quality（高质量）的 procedural memory（程序性记忆）。在 in-domain（域内）、cross-task（跨任务）和 cross-agent（跨智能体）场景下的实验结果表明，ProcMEM 在 extreme memory compression（极端的记忆压缩）下实现了 superior reuse rates（卓越的复用率）和 significant performance gains（显著的性能提升）。可视化的 evolutionary trajectories（演化轨迹）和 Skill distributions（技能分布）进一步揭示了 ProcMEM 如何 transparently accumulates（透明地积累）、refines（优化）和 reuses（复用）procedural knowledge（程序性知识），以 facilitate（促进）long-term autonomy（长期自主性）。",
                    "inspiration_trace": "基于对论文《ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观观察与问题引入\n\n**1. 现象观察：LLM智能体的“重复思考”困境**\n作者首先观察到，尽管现有的LLM驱动智能体在复杂的序列决策任务中表现出色，但它们的核心机制严重依赖“即时推理”。这意味着智能体在每一步都需要实时解释提示、观察和反馈来生成解决方案。\n*   **核心痛点**：即使面对**重复出现的场景**，智能体通常也会从头开始重新进行完整的推理过程，将每一次交互都视为一个从未见过的问题。\n\n**2. 后果分析：计算冗余与执行不稳定**\n这种“经验复用不足”导致了两个直接且严重的负面后果：\n*   **计算冗余**：重复推导已知的解决方案浪费了大量计算资源。\n*   **执行不稳定**：在长周期任务中，重复的推理增加了错误累积的风险，导致执行可靠性下降。\n\n---\n\n### 二、 现有方案的局限性分析\n\n作者梳理了现有的两类解决方案，并指出了它们无法解决上述核心痛点的原因：\n\n*   **路径一：参数化方法**\n    *   *代表*：强化学习（RL）等。\n    *   *局限*：试图将经验编码进模型参数。这面临高昂的训练成本、灾难性遗忘的风险，以及会导致通用能力的窄化。\n*   **路径二：非参数化方法**\n    *   *代表*：外部记忆增强（RAG、存储轨迹、反思总结、结构化图等）。\n    *   *局限*：这些方法本质上属于**情景记忆**。它们存储的是过去的“历史书”，在决策时，智能体检索这些历史作为上下文参考。\n    *   *致命缺陷*：即使拥有巨大的记忆库，智能体仍然需要消耗有限的上下文窗口去“阅读”并“重新推导”解决方案。这实际上又回到了推理繁重的循环，并没有跳过思考过程。\n\n---\n\n### 三、 灵感来源与研究缺口\n\n**1. 认知科学灵感：程序性记忆**\n作者转向人类认知寻找答案。人类拥有一种**程序性记忆**（如骑自行车），它是一个隐式系统，能够直接将情境映射到动作模式。一旦习得，就能自动执行，无需有意识的重新推导。\n\n**2. 现实缺口**\n虽然现有框架（如Claude Agent Skills）允许复用手动编码的程序，但目前缺乏一种机制，让LLM智能体能够**自主地从交互经验中学习**这些可复用的程序性技能。\n\n---\n\n### 四、 核心研究问题\n\n基于上述观察与缺口，作者提炼出的核心研究问题为：\n\n**“LLM智能体如何在不更新模型参数的前提下，从交互经验中自主地学习可复用的程序性记忆，以实现高效的自动执行？”**\n\n---\n\n### 五、 方法论的逻辑演进\n\n为了回答上述问题，作者需要克服三个核心障碍，这构成了方法论演进的三步走逻辑：\n\n#### 第一阶段：解决“可执行性” —— 从叙事到脚本\n*   **挑战**：交互经验通常是被动的“情景叙事”（描述发生了什么），而不是可以直接运行的“决策程序”。\n*   **思路**：必须定义一种标准化的数据结构，将被动的历史转化为主动的指令。\n*   **产出**：**Skill-MDP**。作者定义了“技能”作为基本单元，包含三个要素：激活条件（何时用）、执行程序（怎么做）、终止条件（何时停）。这确保了存储的记忆是可执行的程序，而非死板的历史记录。\n\n#### 第二阶段：解决“非参数优化” —— 模拟梯度下降\n*   **挑战**：在不更新LLM权重（非参数）的情况下，如何优化这些文本形式的“技能”？传统的数值梯度无法直接应用于文本。\n*   **思路**：利用LLM自身的理解能力，将“梯度”概念语义化。\n*   **产出**：**非参数PPO**。\n    *   **语义梯度**：不再计算数值梯度，而是从轨迹中提取自然语言的“改进建议”（如“应该在步骤X前增加检查Y”），作为更新方向。\n    *   **PPO Gate**：为了防止LLM生成的改进建议产生幻觉或破坏原有能力，作者借鉴PPO的信任区域思想，在旧策略生成的轨迹上验证新技能的优劣，只有表现更好的候选者才会被采纳。\n\n#### 第三阶段：解决“可复用性” —— 记忆的进化与筛选\n*   **挑战**：如何确保存储的技能在未来任务中可靠复用，且记忆库不会无限膨胀？\n*   **思路**：引入优胜劣汰的进化机制。\n*   **产出**：**基于分数的维护机制**。通过在线评分评估每个技能带来的收益，定期修剪低分或冗余的技能，保持记忆库的高质量和紧凑性。\n\n---\n\n### 六、 总结\n\n作者的思考路径遵循了经典的**“现象-问题-归因-假设-验证”**学术范式：\n1.  发现LLM智能体“重复造轮子”的效率问题；\n2.  指出现有记忆方法只是“查字典”而非“练肌肉”；\n3.  借鉴人类程序性记忆提出“技能化”存储的假设；\n4.  通过构建Skill-MDP定义技能形态，利用非参数PPO（语义梯度+验证门）实现无参数的技能进化，最终形成ProcMEM框架。"
                },
                {
                    "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems",
                    "arxiv_id": "2602.01848",
                    "authors": "Salaheddin Alzu'bi, Baran Nama, Arda Kaz, Anushri Eswaran, Weiyuan Chen, Sarvesh Khetan, Rishab Bala, Tu Vu, Sewoong Oh",
                    "summary": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”与“单智能体”构建方向。 1.  **核心贡献符合要求 (第一步)**: *   论文的核心贡献是提出了 **ROMA (Recursive Open Meta-Agent Framework)**，这是一个用于构建LLM智能体的新框架。 *   它明确解决了长视距任务中的编排问题，属于构建和改进LLM智能体方法论的范畴，而非简单的应用或基础设施研究。 2.  **高度匹配研究焦点 (第二步)**: *   **多智能体**: 论文明确支持“异构多智能体系统”，能够混合不同的模型和工具，涉及智能体间的协作与编排。 *   **单智能体**: 框架定义了四个模块化角色——Atomizer（分解）、Planner（规划）、Executor（执行）和Aggregator（聚合），直接对应智能体的核心能力：规划、工具使用和任务分解。 *   **演化机制**: 论文引入了 **GEPA+**，这是一种改进的遗传-帕累托提示提议者，利用演化算法来搜索和优化提示，以适应特定任务。这符合“自我演化”或“演化算法”在智能体系统中的应用。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然在SEAL-0和EQ-Bench等基准上进行了测试，但这属于评估框架性能，而非将智能体作为工具去解决生物、金融等特定领域的垂直问题（非演化型应用）。 综上所述，ROMA 提出了一种创新的递归式多智能体架构，并结合了演化算法进行优化，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有智能体框架在长视距任务中面临的编排脆弱、上下文受限及执行不透明问题。针对复杂推理和长形式生成场景，我们提出了一种名为 ROMA 的递归开放元智能体框架，通过 Atomizer、Planner、Executor 和 Aggregator 四个模块化角色实现递归任务分解与并行执行，并结合 GEPA+ 进行提示词优化。在 SEAL-0、EQ-Bench 等基准测试上，通过准确率和写作质量等指标验证了其有效性，显著优于现有基线模型。",
                    "summary_translation": "当前的智能体框架在长视距任务上表现不佳。随着推理深度的增加，顺序编排变得脆弱，上下文窗口施加的硬性限制导致性能下降，而不透明的执行轨迹使得难以定位或调试失败原因。我们提出了 ROMA (Recursive Open Meta-Agents，递归开放元智能体)，这是一个领域无关的框架，通过递归任务分解和结构化聚合来解决这些局限性。ROMA 将目标分解为感知依赖关系的子任务树，这些子任务树可以并行执行，而聚合过程则压缩并验证中间结果以控制上下文的增长。我们的框架围绕四个模块化角色——Atomizer（决定是否应分解任务）、Planner（规划器）、Executor（执行器）和 Aggregator（聚合器）——标准化了智能体的构建，这些角色将编排与模型选择清晰分离，并实现了透明、分层的执行轨迹。这种设计支持异构多智能体系统，能够根据成本、延迟和能力混合使用模型和工具。为了使 ROMA 适应特定任务而无需微调，我们进一步介绍了 GEPA$+$，这是一种改进的 Genetic-Pareto prompt proposer（遗传帕累托提示词提议器），它在 ROMA 的组件层级内搜索提示词，同时保留接口契约。我们表明，ROMA 结合 GEPA+ 在推理和长文本生成基准测试中提供了领先的系统级性能。在评估冲突网络证据推理的 SEAL-0 上，使用 GLM-4.6 实例化的 ROMA 比 Kimi-Researcher 的准确率提高了 9.9%。在长文本写作基准 EQ-Bench 上，ROMA 使 DeepSeek-V3 能够匹配领先的闭源模型（如 Claude Sonnet 4.5）的性能。我们的结果表明，递归、模块化的智能体架构可以在保持可解释性、灵活性和模型无关性的同时，扩展推理深度。",
                    "inspiration_trace": "基于对论文《ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察与背景\n**起点：** 大语言模型（LLMs）已经从单一的对话工具演变为能够处理复杂、多步骤工作流的“智能体系统”。这些系统通过组合模型调用、工具和记忆来解决长视距任务（如代码生成、开放域研究），且多智能体协作已被证明优于单次调用。\n\n### 2. 问题引入：从繁荣到混乱\n**逻辑推演：** 尽管现有智能体系统在特定领域表现优异，但作者观察到其构建方式存在根本性的**“临时性”**。这种缺乏标准化的现状导致了三个核心的连锁问题：\n\n1.  **碎片化与不可复用性：** 控制流、通信模式和记忆管理通常被硬编码在特定的提示词或编排逻辑中。这导致在一个领域表现良好的系统无法迁移到另一个领域，每次开发新智能体都需要从零重建元结构（角色定义、消息协议等）。\n2.  **执行的不透明性：** 专有系统不暴露执行轨迹，开源系统通常只提供非结构化日志。在深层、分支的推理过程中，一旦失败，很难定位是规划、检索还是聚合阶段出了问题。\n3.  **上下文的失控增长：** 随着推理深度的增加，中间的推理步骤、工具输出和产物不断累积，导致上下文窗口溢出或性能下降（即“上下文腐烂”）。\n\n### 3. 核心研究问题\n基于上述观察，作者将思考聚焦于一个核心矛盾：**如何在不牺牲灵活性和透明度的前提下，解决长视距任务中的编排混乱与上下文瓶颈？**\n\n**显式研究问题：**\n> **“我们能否构建一个统一的、领域无关的递归元智能体框架，通过标准化的任务分解、执行和聚合机制，来解决现有智能体系统在长视距任务中面临的临时性编排、执行轨迹不透明以及上下文失控增长的问题？”**\n\n### 4. 逻辑演进与假设形成\n为了回答上述问题，作者的思维路径经历了以下关键转折：\n\n*   **转折一：从“特定流程”到“通用循环”**\n    *   *思考：* 既然每个任务都需要规划、执行和总结，为什么不将这个过程抽象为一个通用的控制循环？\n    *   *假设：* 如果定义一个适用于任务树中每个节点的统一控制逻辑，系统就能从特定任务中解耦出来，实现跨领域的通用性。\n\n*   **转折二：从“线性处理”到“递归分解”**\n    *   *思考：* 长视距任务之所以难，是因为试图一次性解决所有问题。人类解决复杂问题的方法是“分而治之”。\n    *   *假设：* 引入递归结构。将大任务分解为依赖感知的子任务图（DAG），直到子任务变为原子级。这不仅能并行处理，还能隔离错误。\n\n*   **转折三：从“上下文累积”到“结构化压缩”**\n    *   *思考：* 上下文窗口是有限的，不能无限制地传递所有历史信息。\n    *   *假设：* 在递归的每一层引入“聚合器”。它不是简单拼接子任务结果，而是进行验证、综合和压缩，只将高层次的摘要向上传递。这样既能控制上下文增长，又能保证信息质量。\n\n*   **转折四：从“手动调优”到“自动优化”**\n    *   *思考：* 框架有了，但不同组件的提示词需要针对特定任务优化，手动微调成本太高。\n    *   *假设：* 开发一种多组件提示词优化方法（GEPA+），在保持模块接口契约的前提下，自动搜索和改进提示词。\n\n### 5. 最终方法论的形成\n基于上述逻辑演进，作者最终构建了 **ROMA** 框架，其核心思想体现为四个模块化角色的标准化循环：\n\n1.  **Atomizer（原子化器）：** 决策中枢。判断当前任务是否足够简单（原子化），决定是直接执行还是继续分解。\n2.  **Planner（规划器）：** 分解专家。将非原子任务分解为互斥且穷尽（MECE）的子任务图，并明确依赖关系。\n3.  **Executor（执行器）：** 行动专家。在原子级别执行具体任务（如搜索、思考、写作、代码），支持并行。\n4.  **Aggregator（聚合器）：** 整合专家。自底向上地收集子任务结果，进行验证、压缩和合成，生成父任务的最终输出。\n\n**总结：** 作者的思考过程是从对现有智能体系统“各自为政、黑盒运行、内存溢出”的现象观察出发，通过引入“递归”和“模块化”的概念，将混乱的编排问题转化为标准的计算机科学问题（树的遍历与归约），最终提出了一套既通用又可解释的长视距任务解决方案。"
                },
                {
                    "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
                    "arxiv_id": "2602.01815",
                    "authors": "Yunhui Jang, Seonghyun Park, Jaehyung Kim, Sungsoo Ahn",
                    "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建/改进多智能体系统”**： 根据第一步核心判断，该论文不仅仅是将LLM作为工具应用于分子发现领域，而是提出了一个名为 **INDIBATOR** 的**新框架**。其核心贡献在于改进多智能体系统的设计，即通过引入基于“发表历史”和“分子历史”的细粒度个性化档案，来替代传统的粗粒度角色设定，从而提升智能体在多智能体辩论中的表现。这属于“构建、改进 LLM智能体”的范畴。 2.  **高度契合“多智能体”研究方向**： 论文明确涉及 **Multi-Agent Systems (MAS)**，并深入探讨了智能体间的 **Communication**（多轮辩论：提议、批评、投票）和 **Collaboration** 机制。这完全符合研究焦点中的“多智能体”方向。 3.  **具备智能体关键能力**： 论文中提到的智能体拥有基于历史数据的 **Memory**（文献知识和结构先验），并利用这些记忆进行复杂的交互和推理，符合第二步中的正面指标。 4.  **排除标准检查**： 虽然论文的应用场景是“分子发现”，但其核心在于提出一种新的智能体架构和交互范式，而非单纯的应用落地，因此不属于“非演化型应用”的排除范围。同时，论文不涉及安全、多模态视觉或图技术等排除项。 综上所述，该论文通过改进多智能体的个性化构建和交互机制，推动了LLM智能体技术的发展，符合你的研究课题要求。",
                    "summary2": "本文旨在解决现有多智能体系统依赖通用角色设定而忽略科学家独特研究轨迹的问题。针对分子发现场景，我们提出了一种基于发表历史和分子历史构建个性化科学家档案的INDIBATOR框架，通过Multi-Agent Debate进行分子设计。我们在蛋白质条件分子生成、PMO-1K基准等任务上，通过结合亲和力、多样性及AUC等指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery》的深度分析，以下是作者产出该文章的完整逻辑推演过程。\n\n---\n\n### 1. Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从宏观趋势到微观缺陷，再到意外发现的叙事链条：\n\n1.  **宏观背景与范式转移**：大语言模型（LLM）已从单智能体演进到多智能体系统（MAS），后者通过协作智能模拟真实科研过程，成为自动化科学发现的有力范式。\n2.  **现有方案的局限性**：为了区分智能体行为，当前主流方法通常采用**基于角色的提示**（如“审稿人”、“作者”）或**基于关键词的提示**。\n3.  **现实与模拟的鸿沟**：作者指出，这种粗粒度的角色划分过度简化了人类科学家的运作方式。现实中，科学家的贡献是由其独特的**研究轨迹**（累积经验和领域直觉）定义的，而非一个通用的标签。\n4.  **领域特异性证据**：在药物发现领域，化学家具有独特的“风格”（如对特定骨架、官能团的偏好），这源于他们的个人研究历史。\n5.  **关键转折（意外发现的重构）**：作者引用了 Blevins & Quigley (2025) 的研究，该研究证明模型能以 60% 的准确率仅凭分子结构识别出对应的化学家。原研究将其视为一种破坏基准评估的“Clever Hans”（汉斯聪明马）**数据泄露问题**。\n6.  **问题重构与机会**：作者反其道而行之，将这种“泄露”重新诠释为一种**设计蓝图**。他们认为这些化学风格编码了有效导航化学空间的启发式策略，代表了真实协作中的专业知识多样性。\n\n---\n\n### 2. 核心研究问题\n\n**如何通过将多智能体系统锚定在真实科学家的细粒度研究轨迹（即“科学DNA”）上，而非依赖通用的角色扮演，从而提升分子发现的质量与多样性？**\n\n---\n\n### 3. 思想演进与逻辑链推演\n\n以下是从宏观观察到最终方法论的思维演进过程：\n\n#### 第一阶段：观察与批判\n*   **观察**：多智能体系统（MAS）在科学发现中很流行，但目前的“差异化”手段很粗糙（只是简单地让一个扮演A，一个扮演B）。\n*   **反思**：真正的科学家不是靠“角色”区分的，而是靠“经历”。一个做了20年激酶抑制剂的专家和一个通用的“药物化学家”在思维模式上是完全不同的。\n*   **痛点**：现有的通用角色无法提供深度的领域直觉，导致智能体之间的推理同质化，缺乏真实科研团队中那种基于不同学术背景的碰撞。\n\n#### 第二阶段：洞察与重构\n*   **关键洞察**：在化学领域，科学家的“指纹”其实刻在他们发现的分子里（Clever Hans 现象）。\n*   **逻辑反转**：别人把这看作是数据集的“噪音/偏差”，我却认为这是**专家知识的“压缩包”**。如果一个模型能看出这是“张三”做的分子，说明“张三”的分子里包含了他独特的结构偏好和归纳偏置。\n*   **假设**：如果我们把这些真实的“研究轨迹”喂给智能体，它们就能获得真实的“化学直觉”，从而比扮演角色的智能体更懂怎么设计分子。\n\n#### 第三阶段：方法论构建\n*   **定义“科学DNA”**：如何量化一个人的研究轨迹？作者将其解构为两个模态：\n    1.  **出版物历史**：提供文献层面的知识和方法论偏好（文本模态）。\n    2.  **分子历史**：提供结构层面的先验知识，如偏爱的骨架和官能团（结构模态）。\n*   **系统设计**：\n    *   **智能体生成**：不再是随机分配角色，而是根据任务，通过 RAG 检索相关领域的真实科学家，提取他们的论文和分子作为 Profile。\n    *   **交互机制**：利用这些具有真实背景的智能体进行辩论（提议-批判-投票）。因为每个人的“底色”不同，所以批判会更犀利，提议更多样。\n\n#### 第四阶段：验证与闭环\n*   **预期效果**：这种设计应该带来两个核心优势——**多样性**（不同背景导致不同思路）和**事实锚定**（基于真实论文和分子，而非幻觉）。\n*   **实验验证**：通过对比 Vanilla（无角色）、Keyword Persona（关键词角色）和 INDIBATOR（真实轨迹），验证“细粒度个体性”确实优于“粗粒度角色扮演”。\n\n---\n\n**总结**：作者的思想核心在于**“变废为宝”与“回归真实”**。他们将机器学习中的数据泄露问题转化为智能体设计的特征来源，主张用真实科学家的“生平数据”替代虚构的“角色扮演”，从而在多智能体系统中复现真实科研协作的深度与多样性。"
                },
                {
                    "title": "ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing",
                    "arxiv_id": "2602.01797",
                    "authors": "Hanlin Zhou, Huah Yong Chan",
                    "summary": "Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断符合 (Keep - Multi-Agent)**: 这篇论文的核心贡献是提出了 **ORCH**，一个用于离散选择推理的**确定性多智能体编排框架**。论文明确将其定义为一种“multi-agent orchestrator”（多智能体编排器），涉及多个异构LLM（作为基础智能体）与一个专门的“merge agent”（合并智能体）之间的协作。这完全符合“构建、改进或演化 LLM智能体”以及“多智能体系统”的研究范围。 2.  **符合研究焦点 (多智能体协作与编排)**: 论文详细描述了智能体之间的交互模式：“many analyses, one decision”（多次分析，一次决策）。多个基础智能体独立产生结构化分析，合并智能体负责最终决策。这属于多智能体研究中的**协作**和**通信**机制。此外，论文还探讨了如何利用历史反馈（EMA-guided routing）来优化智能体的选择，这涉及到了智能体系统的优化和迭代。 3.  **属于Agentic推理而非单纯的基础模型推理**: 虽然论文关注的是“离散选择推理”，但其方法并非仅仅改进LLM本身的Token预测能力或提出一种新的非Agentic的CoT变体。相反，它构建了一个包含任务分解、独立处理和结果聚合的**Agentic工作流**。这符合筛选标准中关于“智能体如何进行规划或在复杂任务中进行多步推理”的保留条件。 4.  **不涉及排除标准**: *   **非特定领域应用**: 论文在MMLU、GSM8K等通用基准上测试，并非将智能体作为工具单纯应用于生物、医疗等特定垂直领域。 *   **非基础设施**: 论文关注的是智能体的逻辑架构和协调机制，而非硬件加速或部署基础设施。 *   **非安全/多模态**: 论文不涉及安全对齐、水印，也不以视觉或多模态为核心。 综上所述，该论文提出了一种新的多智能体协作框架（ORCH），旨在解决复杂推理任务，完全符合“LLM智能体及其演化”中关于多智能体系统的研究方向。",
                    "summary2": "本文旨在解决多智能体系统在discrete-choice reasoning中non-deterministic、成本高且难以复现的问题。针对discrete-choice reasoning任务，我们提出了一种名为ORCH的确定性多智能体编排器，采用基于EMA（Exponential Moving Average）的确定性路由机制和“多次分析，一次合并”的范式。在MMLU、MMLU-Pro和GSM8K数据集上，通过准确率、延迟和成本等指标验证了其有效性。",
                    "summary_translation": "大规模语言模型 的最新进展使得多智能体架构 在具有挑战性的推理任务中极具吸引力。然而，许多现有系统依赖于随机路由 或临时启发式规则，导致其行为难以复现，且决策过程难以解释。我们提出了 ORCH，这是一个用于离散选择推理 的确定性协调框架，用于编排异构 LLMs。ORCH 遵循“多次分析，一次决策”的范式：多个基础模型独立生成结构化分析，而一个专门的合并智能体 输出最终选择。该框架使用固定规则进行任务分解 和答案聚合，从而保持流程的可预测性、可复现性，且无需训练。这里的确定性是指在固定评估协议 下的固定路由和聚合规则，而不是跨部署的严格位级可复现性。为了利用模型互补性，我们可选地引入了一个 EMA 指导的路由器，它利用历史准确率、延迟 或成本来更新智能体选择；由于它依赖于基于答案的反馈，因此主要用于基准测试、受控评估或延迟反馈设置。在 MMLU、MMLU-Pro 和 GSM8K 上的实验表明，ORCH 始终优于单模型基线 和多数投票集成。在 MMLU-Pro 上，与最强基线相比，ORCH 将准确率提高了 10 个百分点以上；在 GSM8K 上，其增益超过 50 个百分点；McNemar 检验证实了统计显著性。EMA 路由器提供了额外的 0.7--2.0 个百分点的准确率提升，消融实验 表明多智能体协作和路由都做出了实质性贡献。总体而言，ORCH 为离散选择推理的可控、可解释且易于部署的基于 LLM 的智能体系统提供了一条实用路径。",
                    "inspiration_trace": "基于对论文《ORCH: many analyses, one merge—a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 第一部分：引言中的“讲故事”逻辑（问题演进）\n\n作者在Introduction部分并未直接抛出方法，而是通过层层递进的逻辑，将读者的视线从宏观的AI发展引向具体的技术痛点：\n\n1.  **宏观愿景（AI与Agent的崛起）：**\n    *   人工智能的核心在于感知环境并做出决策。\n    *   **Agent（智能体）**被视为构建通用人工智能（AGI）的核心组件。\n    *   大语言模型（LLMs）的出现极大地扩展了Agent的能力边界，使其具备了信息访问、交互和复杂推理的能力。\n\n2.  **现实瓶颈（单Agent的局限性）：**\n    *   尽管单个LLM Agent能力强大，但在面对复杂任务时存在明显的天花板：能力受限、上下文窗口有限、知识覆盖不全、鲁棒性不足。\n    *   在需要多领域专业知识或长步骤协调的任务中，单Agent往往无法达到满意的性能。\n\n3.  **现有方案及其新问题（多智能体系统的随机性风险）：**\n    *   为了解决单Agent的局限，**多智能体系统（MAS）**应运而生，通过协调多个专业Agent实现任务分解和并行执行。\n    *   **关键转折（痛点揭示）：** 现有的MAS框架大多依赖**启发式规则**或**随机路由策略**来选择Agent和分配任务。\n    *   **后果：** 这种固有的随机性导致系统行为难以复现、缺乏透明度（难以调试），且在医疗诊断或金融决策等高风险领域引入了不可控的风险。对于**离散选择推理**任务，这种随机性会直接降低决策质量并侵蚀用户信任。\n\n---\n\n### 第二部分：核心研究问题\n\n基于上述对现状的批判性分析，作者将宏大的技术挑战收敛为一个具体的、可验证的科学问题：\n\n**“在不重新训练底层大语言模型的前提下，多智能体编排方案能否在持续控制推理延迟和调用成本的同时，一致性地提升多选择推理和数学推理任务的性能？”**\n\n---\n\n### 第三部分：思想演进与方法论形成（逻辑链推演）\n\n从发现问题到提出ORCH方法，作者的思考路径经历了以下四个关键阶段：\n\n#### 1. 观察与反思：从“随机性”到“确定性”的范式转移\n*   **观察：** 现有的多智能体协作（如MetaGPT, AutoGPT）虽然提升了性能，但为了灵活性牺牲了**确定性**。随机路由导致结果不可复现，这在需要严谨逻辑的离散选择任务中是不可接受的。\n*   **反思：** 能否设计一种**协议级别的确定性编排器**？即不依赖随机采样，而是通过固定的规则来协调Agent，从而在保证性能的同时提升系统的可靠性和可解释性。\n\n#### 2. 假设提出：“多路分析，单一合并”\n*   **假设：** 相比于简单的投票或随机调用，如果让多个异构Agent对同一问题进行独立分析，然后通过一个专门的“合并者”来综合这些分析，是否能获得比单Agent或简单投票更优的结果？\n*   **核心概念：** 确立 **“Many Analyses, One Merge”** 为核心范式。这不仅仅是数量的堆砌，而是质的综合——Merger不仅仅是投票，而是基于证据的仲裁。\n\n#### 3. 架构设计：分层解耦与异构协作\n*   **设计思路：** 为了实现上述假设，必须将流程结构化。\n    *   **第一层（分解）：** 将复杂问题标准化，甚至分解为子问题（如概念验证、选项排除）。\n    *   **第二层（并行分析）：** 引入异构的LLM Agent（如OpenAI, DeepSeek, XAI），利用它们不同的知识分布和偏好进行并行推理。\n    *   **第三层（确定性合并）：** 设计一个专门的Merger Agent，它不直接回答问题，而是阅读所有Agent的分析文本，比较异同，最终输出一个确定的离散选项。\n\n#### 4. 优化迭代：引入EMA（指数移动平均）引导路由\n*   **进一步思考：** 虽然架构是确定的，但Agent的能力各有千秋。如何在不引入随机性的前提下，动态利用历史表现来优化资源分配？\n*   **解决方案：** 引入**EMA（Exponential Moving Average）机制**。\n    *   **逻辑：** 记录每个Agent的历史准确率、延迟和成本。利用EMA平滑这些指标，形成一个动态的“评分”。\n    *   **作用：** 虽然路由规则本身是确定的（例如：总是选当前EMA分数最高的做分解），但评分随历史反馈更新。这使得系统具备了**自适应能力**，同时保持了**协议层面的确定性**（即规则固定，输入相同则输出相同）。\n\n---\n\n### 总结：作者的心智地图\n\n作者并没有试图发明一个新的LLM模型，而是站在**系统工程**的高度，重新审视了“如何使用LLM”这个问题。\n\n**逻辑链条：**\n单Agent能力不足 $\\rightarrow$ 多Agent系统出现但太随机（不可靠） $\\rightarrow$ **提出确定性编排需求** $\\rightarrow$ 设计“分解-并行分析-合并”的流水线架构 $\\rightarrow$ 引入EMA机制在确定性和自适应之间寻找平衡 $\\rightarrow$ 最终产出ORCH框架。"
                },
                {
                    "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
                    "arxiv_id": "2602.01675",
                    "authors": "Yuanzhe Shen, Zisu Huang, Zhengyuan Wang, Muzhao Tian, Zhengkang Guo, Chenyang Zhang, Shuaiyu Zhou, Zengjie Hu, Dailin Li, Jingwen Xu, Kaimin Wang, Wenhao Liu, Tianlong Li, Fengpeng Yue, Feng Hong, Cao Liu, Ke Zeng",
                    "summary": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \\textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose \\textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的论文。具体判断依据如下： 1.  **核心贡献符合“构建/改进 LLM 智能体” (第一步 & 第四步)**： *   论文不仅提出了 **TRIP-Bench**（一个用于评估长视距交互智能体的基准），更重要的是提出了 **GTPO**，这是一种“在线多轮强化学习方法”。 *   GTPO 的核心目的是通过专门的奖励归一化和差分机制来训练智能体，从而提高其约束满足能力和交互鲁棒性。这属于通过算法改进智能体能力的范畴，符合“自我演化”或“自我完善”的定义（通过反馈进行迭代改进）。 2.  **聚焦 Agentic AI 的核心能力 (第二步)**： *   论文明确关注 **Long-Horizon Interactive Agents**（长视距交互智能体）。 *   涉及的关键能力完全符合筛选标准中的正面指标：**Tool Use**（多工具推理，涉及 18 种工具和 150+ 次调用）、**Planning**（旅行规划、全局约束执行）、**Adaptation**（适应不断演化的用户行为）以及 **Self-Correction/Iterative Improvement**（迭代版本修订）。 3.  **非单纯应用或基础设施 (第一步 & 第三步)**： *   虽然论文使用了“旅行规划”作为场景，但这并非仅仅是将智能体作为工具应用于特定领域（非演化型应用）。相反，旅行场景是用来测试和训练智能体在复杂、长上下文、多轮交互中通用能力的试验台。 *   论文不涉及安全对齐、多模态视觉核心研究或基础设施优化，避开了所有排除标准。 综上所述，该论文通过提出新的训练方法（GTPO）和评估基准（TRIP-Bench），直接致力于解决 LLM 智能体在长视距任务中的规划和工具使用难题，属于构建和演化 LLM 智能体的前沿研究。",
                    "summary2": "本文旨在解决长视距交互代理在复杂现实场景中面临的约束遵循和多轮推理挑战。针对真实旅行规划场景，我们提出了一种包含TRIP-Bench基准和GTPO在线多轮强化学习方法的解决方案，并在自建的TRIP-Bench上通过Overall Strict和Overall Loose指标验证了其有效性。",
                    "summary_translation": "随着基于大语言模型的智能体被部署在日益复杂的现实环境中，现有的基准未能充分体现诸如执行全局约束、协调多工具推理以及在长期的多轮交互中适应用户行为演变等关键挑战。为了弥合这一差距，我们介绍了 \\textbf{TRIP-Bench}，这是一个基于现实旅行规划场景的长视界基准。TRIP-Bench 利用真实世界数据，提供18个精选工具和40多项旅行需求，并支持自动评估。它包含不同难度的子集；其中困难子集强调长且模糊的交互、风格转换、可行性变化以及迭代式版本修订。对话跨度长达15个用户轮次，可能涉及150多次工具调用，上下文长度可能超过20万词元。实验表明，即使是先进的模型在简单子集上也最多只能达到50%的成功率，而在困难子集上的性能则下降至10%以下。我们进一步提出了 \\textbf{GTPO}，这是一种采用专门的奖励归一化和奖励差分机制的在线多轮强化学习方法。应用于 Qwen2.5-32B-Instruct 时，GTPO 提高了约束满足度和交互鲁棒性，在我们的评估中表现优于 Gemini-3-Pro。我们期望 TRIP-Bench 能够推动实用的长视界交互智能体的发展，并期望 GTPO 为鲁棒的长视界训练提供有效的在线强化学习方案。",
                    "inspiration_trace": "基于对论文《TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios》的深入分析，以下是作者产出该文章的系统性思考过程推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个层层递进的逻辑链条，旨在揭示当前研究与应用之间的鸿沟：\n\n1.  **背景变迁**：LLM 智能体正从简单的“回答问题”向复杂的“完成任务”转变，并被部署到日益复杂的现实场景中。\n2.  **现实挑战**：现实世界的任务不仅仅是生成文本，而是需要生成可执行、可修订的动作序列，并在长周期的目标中保持进展。这引入了三个核心难点：\n    *   **全局约束**：必须遵守预定义的规则、工作流和合规性要求。\n    *   **动态交互**：用户的指令和偏好在交互过程中是演进的，且很少在初始阶段完全明确。\n    *   **一致性**：智能体必须在多轮、动态的过程中保持决策的一致性和可控性。\n3.  **现有基准的不足**：现有的评估基准存在严重缺陷：\n    *   大多关注单轮任务，或者虽然有多轮交互，但缺乏系统性的复杂规则约束建模。\n    *   即使是面向交互的基准，其查询往往很简单，只需少量工具调用（通常 <3），导致推理链浅，无法代表长周期规划和迭代修正。\n    *   缺乏对 prolonged interaction behaviors（如版本回退、意图模糊、计划合并）的建模。\n4.  **结论**：目前缺乏一个能够同时压力测试复杂指令遵循、长周期推理和多样化用户交互行为的统一评估基准。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个能够真实反映长周期、多轮交互且包含复杂全局约束的评估基准，并据此提出一种能有效提升智能体在动态交互中长周期决策稳定性和约束满足能力的训练方法？”**\n\n---\n\n### 三、 思想演进脉络\n\n从宏观观察到具体方法论的提出，作者的思考路径经历了以下四个阶段：\n\n#### 1. 观察与痛点识别\n*   **观察**：随着 LLM 能力的提升，智能体开始进入实际应用（如客服、规划），但现有的评估体系（如单轮工具调用、静态问答）无法衡量其在真实环境下的表现。\n*   **痛点**：真实场景是“脏”且“长”的。用户会改主意（动态性），系统有死规矩（全局约束），任务需要很多步（长周期）。现有的 SOTA 模型在这些场景下表现如何？我们不知道，因为没有尺子。\n\n#### 2. 假设与基准构建\n*   **假设**：如果我们构建一个基于真实数据（如旅游规划）、包含复杂规则（18个工具、40+需求）、且能模拟用户动态行为（9种行为模式，包括回退、意图模糊等）的基准，那么现有的先进模型将会暴露出严重的局限性，特别是在全局约束遵守和跨轮次一致性上。\n*   **行动**：构建 **TRIP-Bench**。\n    *   **设计思路**：不仅仅是增加数据量，而是增加“交互复杂度”和“任务复杂度”。引入 Hard Split（LIT, FIT, AIS, PMR）来专门攻击模型的弱点（如长对话、可行性突变、意图漂移）。\n    *   **预期结果**：通过实验验证，即使是 GPT-5.2 这样的强模型，在严格模式下得分也会极低（<20%），从而证明问题的存在和基准的有效性。\n\n#### 3. 深度诊断与归因\n*   **现象**：实验发现，多轮交互往往比单轮交互表现更差，尤其是在强全局约束下。随着对话轮次增加，全局一致性逐渐被侵蚀。\n*   **归因**：\n    *   **Reward Inheritance（奖励继承）**：在长周期任务中，后一个轮次的奖励往往继承了前一个轮次的结构，导致模型即使做得稍差也可能获得较高奖励，掩盖了错误。\n    *   **分布偏移**：传统的 RLHF 或 SFT 往往基于静态历史数据，而多轮交互中历史是由当前策略生成的，存在分布不匹配。\n    *   **约束不平衡**：不同的全局约束在奖励中的权重难以平衡，导致模型顾此失彼。\n\n#### 4. 方法论创新\n*   **思路**：为了解决上述诊断出的问题，需要一种专门针对“多轮、在线、长周期”的强化学习方法。\n*   **核心创新**：提出 **GTPO (Group Turn-level Policy Optimization)**。\n    *   **针对 Reward Inheritance**：提出 **Turn-level Reward Differencing（轮级奖励差分）**。不直接优化绝对奖励，而是优化相对于上一轮的增量奖励，强调“改进”而非“继承”。\n    *   **针对约束不平衡**：提出 **Global Instruction Normalization（全局指令归一化）**。在约束组内进行 Z-score 归一化，确保不同类型的约束（如时间、价格、评分）在奖励尺度上是对齐的。\n    *   **针对训练稳定性**：提出 **Turn-level Normalization（轮级归一化）**。在同一轮的不同采样轨迹间进行归一化，稳定训练过程。\n\n**总结**：作者从“现实应用需求”出发，发现“评估工具缺失”，进而构建“高难度基准”暴露模型短板，通过“深度分析”定位长周期训练中的奖励机制缺陷，最终提出“GTPO”这一针对性的解决方案，完成了从问题发现到方法解决的完整闭环。"
                },
                {
                    "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
                    "arxiv_id": "2602.01664",
                    "authors": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang, Rui Mao, Erik Cambria",
                    "summary": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断**： *   论文的核心贡献是提出了 **FlowSteer**，这是一个用于 **Agentic Workflow Orchestration（智能体工作流编排）** 的端到端强化学习框架。 *   这属于构建和改进 LLM 智能体的方法论。它不仅仅是应用现有的智能体去解决特定领域问题，而是提出了一种新的机制（通过强化学习训练策略模型）来自动化智能体的工作流构建过程。 *   这不属于非演化型应用、非Agentic的基础推理或基础设施研究。 2.  **正面指标匹配**： *   **核心范式**：明确涉及 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文重点涉及智能体的 **`Planning`**（通过策略模型分析状态并选择编辑动作）和 **`Tool Use`**（在画布环境中执行算子/Operators）。 *   **演化机制**：虽然主要归类为单智能体，但其通过 **`Reinforcement Learning`** 和 **`Iterative Refinement`**（基于反馈的迭代优化）来改进智能体的编排策略，这与智能体的自我完善和迭代机制高度相关。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，FlowSteer 提出了一种通过强化学习来优化智能体规划和工具使用编排的新框架，直接贡献于 Agentic AI 的构建与改进，因此符合筛选要求。",
                    "summary2": "本文旨在解决现有工作流编排中高人工成本、算子/模型锁定及稀疏奖励信号的问题。针对多样化的任务场景，我们提出了一种名为FlowSteer的端到端强化学习框架，通过轻量级策略模型与可执行画布环境的多轮交互实现自动化编排，并引入CWRPO算法稳定学习。我们在十二个涵盖QA、数学推理和代码生成的数据集上，通过Exact Match、Accuracy和Pass@1等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning》的深入分析，以下是对作者核心方法论逻辑链的系统性推演。\n\n---\n\n### 一、 宏观背景与观察\n\n**观察起点：AI Agent 的能力演进**\n作者首先观察到人工智能领域正在发生范式转移：从单轮问答（Single-turn QA）向可执行的端到端任务完成演进。为了解决复杂的人类问题，单纯的“对话”已不足够，需要引入“工作流”的概念——即将各种算子和工具组织成可执行的图结构，以实现可控、可调试且可复用的任务解决。\n\n**现状痛点：工作流编排的“手工作坊”模式**\n尽管工作流是连接任务目标与执行的桥梁，但在实际应用中，构建这些工作流仍然高度依赖人工拖拽或基于规则的硬编码配置。这种模式不仅成本高昂，而且难以迁移到新任务、新工具库或不同的模型后端中。\n\n---\n\n### 二、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中通过对比现有范式和剖析核心挑战，构建了以下逻辑链条：\n\n1.  **现有范式的局限性：**\n    *   **静态工作流选择：** 像查字典一样从库中检索预定义的工作流。虽然稳定，但缺乏灵活性，无法适应未见过的任务。\n    *   **离线工作流生成：** 利用监督微调（SFT）或群体相对策略优化（GRPO）训练模型直接生成工作流。这虽然能生成新图，但缺乏执行过程中的实时反馈，属于“开环”生成。\n    *   **自动化工作流优化：** 结合搜索和执行反馈来迭代改进结构（如AFlow, GPTSwarm）。这虽然引入了反馈，但往往存在特定的路径依赖。\n\n2.  **三大核心挑战（The \"Why\"）：**\n    *   **高人工/启发式依赖：** 现有的规则和模板需要持续维护，且与特定场景强耦合，限制了复用性和泛化能力。\n    *   **算子/后端锁定：** 现有方法往往依赖固定的算子集或单一的大模型后端，难以实现“即插即用”的算子组合。一旦环境变化，性能和鲁棒性急剧下降。\n    *   **稀疏且不稳定的学习信号：** 仅使用最终正确性奖励进行训练，会导致“捷径行为”（如过早终止、生成过度简化的图）和奖励黑客。此外，在长序列任务中，这种稀疏信号会导致信用分配不稳定。\n\n---\n\n### 三、 研究问题\n\n基于上述观察与挑战，作者试图回答的核心问题是：\n\n**如何通过端到端的强化学习与多轮交互机制，实现一种通用、可插拔且学习信号稳定的智能体工作流自动化编排？**\n\n---\n\n### 四、 思想演进与方法论形成\n\n为了解决上述问题，作者的思考经历了从“交互模式”到“学习机制”的层层递进：\n\n#### 1. 思考交互模式：从“一次性生成”到“多轮对话”\n*   **思考：** 既然离线生成（一次性画出整个流程图）缺乏反馈，而人工配置成本太高，为什么不模仿人类工程师的思维方式？\n*   **假设：** 人类构建复杂系统时是逐步进行的：先搭骨架，再填细节，边运行边调试。\n*   **方法论雏形：** 引入**“多轮交互”**。不再要求模型一次性输出完整的工作流，而是让一个轻量级的策略模型作为“指挥官”，在一个可执行的环境中逐步添加、修改或删除节点。\n\n#### 2. 思考环境构建：从“黑盒调用”到“白板画布”\n*   **思考：** 为了支持多轮交互，模型需要一个能够理解其意图并给出反馈的“场所”。这个场所必须能够执行具体的算子，并告诉模型“这一步对不对”。\n*   **假设：** 如果将工作流构建过程抽象为在一个“画布”上的编辑操作，就能将复杂的图构建问题分解为一系列原子化的编辑动作（如添加节点、设置提示词）。\n*   **方法论雏形：** 设计**“Workflow Canvas”**。这是一个可执行的环境，它维护工作流图的状态，接收策略模型的编辑指令，执行算子，并返回执行轨迹和反馈。这实现了**算子库和模型后端的解耦**（Plug-and-play），因为Canvas只负责执行，不关心具体是谁在画。\n\n#### 3. 思考学习机制：从“结果导向”到“过程约束”\n*   **思考：** 有了交互环境，如何训练这个指挥官？如果只看最后答案对不对（稀疏奖励），模型可能会学会偷懒（例如直接跳过验证步骤，或者生成极简的图）。\n*   **假设：** 一个好的工作流不仅结果要对，结构也要“健康”（例如包含验证步骤、格式化步骤、必要的控制流）。应该先教模型“如何画出一个合格的图”，再教它“如何画出能解题的图”。\n*   **方法论雏形：** 提出**“多样性约束奖励”与“条件释放机制”**。\n    *   **多样性约束：** 强制要求工作流包含验证、格式化等结构要素。\n    *   **条件释放：** 只有当结构合格时，才释放基于答案正确性的奖励。这就像先考“建筑规范”，再考“居住舒适度”，有效抑制了捷径行为。\n\n#### 4. 思考算法优化：从“通用RL”到“CWRPO”\n*   **思考：** 标准的强化学习算法（如PPO）在处理这种长序列、多轮交互且包含环境反馈token的场景时，梯度估计往往方差很大，且容易受到环境噪声干扰。\n*   **假设：** 需要一种能够区分“策略生成的token”和“环境反馈的token”的算法，并且利用组内相对优势来稳定训练。\n*   **最终方法论：** 提出**Canvas Workflow Relative Policy Optimization (CWRPO)**。\n    *   **Mask机制：** 梯度回传时只对策略模型生成的token进行更新，屏蔽环境反馈的噪声。\n    *   **GRPO思想：** 利用组内统计量归一化优势，稳定长视界信用分配。\n\n---\n\n### 五、 总结：FlowSteer 的核心逻辑链\n\n1.  **痛点：** 现有的Agent工作流构建太依赖人工，或者生成的图太死板、太容易“作弊”。\n2.  **破局点：** 将工作流构建看作一个**“指挥官”与“画布”之间的多轮对话游戏**。\n3.  **核心设计：**\n    *   **轻量级指挥官：** 只负责决策“下一步画什么”，降低认知负荷。\n    *   **白板画布：** 负责执行和反馈，实现工具和模型的解耦。\n    *   **两阶段教学：** 先通过结构约束奖励教会模型“画得规范”，再通过答案奖励教会模型“画得有用”。\n\n这一逻辑链条从宏观的Agent演进出发，精准定位了工作流编排的三大痛点，并通过引入交互式Canvas和带约束的RL算法，最终形成了一套既通用又稳定的自动化编排框架。"
                },
                {
                    "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
                    "arxiv_id": "2602.01608",
                    "authors": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
                    "summary": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 范畴）**： 论文的核心贡献是提出了 \"Collaborative Thoughts\" 这一**统一的协作框架**。该框架不仅仅是简单的推理提示，而是构建了一个包含规划、执行、评估和反馈修正的闭环系统。这完全符合 **Agentic AI** 的定义，即智能体通过规划、工具使用和环境交互来完成任务。 2.  **符合单智能体 的核心特征**： *   **规划**: 论文中明确指出自回归模型负责 \"structured planning and constraint management\"（结构化规划和约束管理），这是智能体规划能力的体现。 *   **工具使用**: 扩散模型在这里被用作智能体的工具，用于生成 \"intermediate visual thoughts\"（中间视觉思维），以辅助推理。 *   **自我反思与修正**: 框架包含一个 \"vision-based critic module\"（基于视觉的评论家模块）来评估结果，并利用反馈 \"iteratively refine\"（迭代优化）后续步骤。这对应了智能体的自我反思和自我修正能力。 3.  **排除标准的应用（多模态例外情况）**： 虽然论文涉及了扩散模型和视觉生成，但根据筛选标准中的特殊规则：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在本研究中，扩散模型是作为智能体架构中的一个组件（工具/协作者）存在的，研究的核心在于**如何让LLM（自回归模型）通过规划、调用工具和接收反馈来进行联合推理**，而非改进扩散模型本身的生成质量或视觉算法。因此，这不属于被排除的纯视觉或多模态研究。 综上所述，该论文提出了一种新的智能体框架，展示了LLM如何通过规划、工具调用（扩散模型）和反馈机制来解决复杂的空间推理问题，完全符合“构建、改进 LLM智能体”的研究目标。",
                    "summary2": "本文旨在解决单一模型在空间推理和物理任务中的局限性。针对需要空间感知和物理常识的复杂任务，我们提出了一种名为 Collaborative Thoughts 的统一协作框架，通过闭环交互让 Autoregressive 模型规划、Diffusion 模型生成视觉思维、Critic 模块评估反馈。在几何切割和欧几里得几何问题求解等代表性示例上，通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "Autoregressive models（自回归模型）和 diffusion models（扩散模型）代表了两种互补的 generative paradigms（生成范式）。Autoregressive models 擅长 sequential planning（序列规划）和 constraint composition（约束组合），但在需要 explicit spatial or physical grounding（显式空间或物理基础）的任务中面临困难。相比之下，diffusion models 通过 high-dimensional generation（高维生成）捕捉丰富的空间结构，但缺乏满足 complex, multi-stage constraints（复杂、多阶段约束）或可靠识别和纠正错误所需的 stepwise logical control（逐步逻辑控制）。我们提出了 Collaborative Thoughts（协作思维），这是一个 unified collaborative framework（统一协作框架），它使 autoregressive models 和 diffusion models 能够通过 closed-loop interaction（闭环交互）进行联合推理和生成。在 Collaborative Thoughts 中，autoregressive models 执行 structured planning（结构化规划）和 constraint management（约束管理），diffusion models 将这些约束实例化为 intermediate visual thoughts（中间视觉思维），而一个 vision-based critic module（基于视觉的评判模块）则评估这些视觉思维是否满足预期的结构和物理要求。该反馈随后被用于 iteratively refine（迭代优化）后续的规划和生成步骤，从而缓解 error propagation across modalities（跨模态错误传播）。重要的是，无论任务是 autoregressive question answering（自回归问答）还是 diffusion-based visual generation（基于扩散的视觉生成），Collaborative Thoughts 都使用相同的 collaborative loop（协作循环）。通过 representative examples（代表性示例），我们阐述了 Collaborative Thoughts 如何提高 spatial reasoning（空间推理）的 reliability（可靠性）以及 generation 的 controllability（可控性）。",
                    "inspiration_trace": "基于对论文《Reasoning with Autoregressive-Diffusion Collaborative Thoughts》的深入分析，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程。\n\n---\n\n### 一、 宏观观察与逻辑起点\n\n**思考起点：当前AI推理能力的“跛脚”现状**\n作者首先观察到，通用人工智能的核心在于推理能力。目前主流的推理范式是**自回归模型**（如LLM）主导的**思维链**。\n*   **优势**：擅长符号逻辑、顺序规划和约束管理。\n*   **盲区**：在处理需要明确空间感知或物理常识的任务时（如几何切割、物体堆叠），仅靠文本符号推理存在“认知盲区”。LLM生成的文本在语言上通顺，但在物理上往往是荒谬的（幻觉）。\n\n### 二、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中通过层层递进的方式，构建了从现象到本质的问题叙事：\n\n1.  **现状与初步突破**：\n    *   传统的直接回答无法处理复杂任务。\n    *   **思维链** 的出现让LLM能通过中间文本步骤解决复杂逻辑问题，但这仅限于语义层面。\n\n2.  **遭遇瓶颈（空间盲区）**：\n    *   当任务涉及空间或物理属性时，纯文本的CoT失效了。LLM无法“验证”几何结构或模拟物理交互，导致产生“语言通顺但物理错误”的幻觉。\n\n3.  **现有尝试及其缺陷（单向道的死胡同）**：\n    *   为了解决上述问题，学界开始尝试**思维可视化**，即生成中间图像作为辅助。\n    *   **关键批判**：作者指出，现有方法将生成的视觉内容视为“不可更改的真理”。这是一个致命缺陷——如果生成的图像本身是错的（例如物体悬浮），由于缺乏反馈机制，这个错误会不可逆地传播下去，导致后续推理全盘皆输。\n\n4.  **核心洞察（互补性）**：\n    *   作者意识到，自回归模型（AR）和扩散模型代表了两种互补但本质不同的生成范式：\n        *   **AR**：逻辑强，空间弱。\n        *   **Diffusion**：空间强，逻辑控制弱。\n    *   真正的解决方案不应是简单的串联（AR生成Prompt给Diffusion），而应是深度的**协同**。\n\n---\n\n### 三、 显式总结：研究问题\n\n基于上述逻辑链，作者试图回答的核心研究问题是：\n\n**“如何构建一个闭环的协同框架，使自回归模型与扩散模型能够通过迭代交互，共同进行推理与生成，从而在利用视觉思维进行空间推理的同时，实现错误的自我检测与修正？”**\n\n---\n\n### 四、 思想演进脉络：从假设到方法论\n\n为了解决上述问题，作者的思考经历了以下四个阶段的演进：\n\n#### 第一阶段：认知理论的映射\n*   **思考**：人类是如何解决复杂的几何或物理问题的？\n*   **假设**：依据**双重编码理论**，人类大脑同时处理“符号逻辑”和“空间图像”。我们在解题时，会在脑海中构建图像，检查它是否符合物理规律，如果不符，就修改图像或思路。\n*   **推论**：AI系统也应该模仿这种“模拟-评估-修正”的循环，而不是一次性生成。\n\n#### 第二阶段：角色分工的重新定义\n*   **思考**：既然要模仿人类，AR模型和Diffusion模型在这个循环中应该分别扮演什么角色？\n*   **设计**：\n    *   **AR模型**：不应只是生成Prompt，而应升级为**规划者**。负责结构化规划和约束管理（即“怎么切”）。\n    *   **Diffusion模型**：不应只是画图工具，而应升级为**模拟器**。负责将语义指令实例化为像素级的视觉思维（即“切出来是什么样”）。\n    *   **引入新角色**：必须有一个**评论家**。因为模拟器可能会出错，需要一个能看懂图并理解逻辑的模块来检查视觉思维是否符合物理约束。\n\n#### 第三阶段：从“开环”到“闭环”的机制设计\n*   **思考**：如何防止错误传播？现有的“一次性生成”是开环的，必须改为闭环。\n*   **设计**：提出 **Simulate-Critic-Refine（模拟-批评-修正）** 循环。\n    1.  **Simulate**：Diffusion根据AR的指令生成视觉假设。\n    2.  **Critic**：评论家检查图像，给出反馈（如“这个立方体悬浮了”）。\n    3.  **Refine**：AR根据反馈修改指令，重新生成。\n*   **关键点**：将中间的视觉输出视为“可被推翻的假设”而非“真理”，这是方法论的核心转折。\n\n#### 第四阶段：统一范式的确立\n*   **思考**：这个框架是否只能用于视觉问答？\n*   **升华**：作者意识到这个逻辑是通用的。\n    *   如果任务是**问答**，Diffusion生成的图是中间证据，辅助AR得出答案。\n    *   如果任务是**生成**，AR的规划是中间步骤，辅助Diffusion画出最终图。\n*   **结论**：无论输入输出是什么，底层的逻辑都是**AR与Diffusion的协同思维**。这最终形成了论文中提出的统一框架。\n\n---\n\n**总结**：\n作者的思考路径是从**发现单一模态（文本）的空间局限性**出发，批判了现有**多模态方法缺乏纠错机制**的痛点，进而受人类认知理论启发，提出了**利用AR的逻辑优势弥补Diffusion的控制短板，同时利用Diffusion的空间优势弥补AR的感知短板**的协同方案，最终通过引入“评论家”构建闭环，实现了可自我修正的推理系统。"
                },
                {
                    "title": "S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research",
                    "arxiv_id": "2602.01550",
                    "authors": "S1-NexusAgent Team",
                    "summary": "Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断符合**: 论文的核心贡献是提出了 \"S1-NexusAgent\"，这是一个**自我演化的智能体框架**。虽然它应用于科学研究领域，但其本质是构建和改进智能体的架构与演化机制，而非单纯的应用型研究。 2.  **高度契合“自我演化”方向**: 论文明确提出了一个 \"Critic Agent\"（评论智能体），用于评估执行轨迹并将高质量路径提炼为可复用的 \"Scientific Skills\"，从而形成持续自我演化的闭环。这完全符合研究课题中关于“自我演化”的核心定义。 3.  **符合“单智能体”能力要求**: 论文涉及智能体的规划、工具使用和长视距任务处理，采用了分层 Plan-and-CodeAct 执行范式，属于 Agentic AI 的范畴。 4.  **符合特殊处理规则**: 根据第四步关于“自我演化的应用”的规则，尽管论文涉及生物、化学等特定领域，但由于其核心是提出一种新的“自我演化”机制（Critic Agent + 技能提炼），因此应当保留。",
                    "summary2": "本文旨在解决现有智能体在多学科科研中长时程规划、目标维持及持续学习受限的问题。针对复杂多学科科研场景，我们提出了S1-NexusAgent自进化框架，采用Plan-and-CodeAct范式和内外双环架构，结合意图感知动态工具检索及稀疏上下文管理。我们在biomni-eval、ChemBench和MatSciBench基准上通过任务准确率验证了其有效性，达到了SOTA性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research》的深度分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分通过层层递进的叙事方式，构建了研究的紧迫性与必要性，其逻辑链条如下：\n\n1.  **宏观背景：** 现代科学研究正在经历深刻的范式转移，从传统的经验驱动转向高度数据驱动和计算密集型模式（如高通量分析、大规模筛选）。\n2.  **现实挑战：** 这种转移导致科研任务复杂度剧增——问题跨越多学科、实验路径依赖中间反馈、且需要协调大量异构工具和数据源。\n3.  **现状与不足：** 尽管现有LLM和智能体在特定领域展现出潜力，但在真实科研场景中仍存在四大核心缺陷：\n    *   **长视距规划能力弱：** 线性或短视的规划无法支持多阶段探索，容易偏离目标。\n    *   **工具操作缺乏灵活性：** 依赖静态接口，难以适应快速演进的科研工具生态。\n    *   **上下文管理困难：** 大规模数据和长时推理导致上下文膨胀，影响稳定性。\n    *   **持续进化能力不足：** 缺乏从试错和迭代中学习的能力。\n4.  **核心理念：** 提出一个观点——智能科研体不应只是回答问题，而应像科学家一样运作（规划实验、执行分析、反思结果、从经验中学习）。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“如何构建一个通用的智能体框架，使其能够像人类科学家一样，在跨学科、长周期、工具密集的复杂科研任务中，实现稳定的全局规划、灵活的工具编排以及基于执行经验的持续自我进化？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进\n\n以下是从宏观观察到具体方法论的思维推演过程：\n\n#### 1. 观察：科研任务的“双重复杂性”\n*   **现象：** 真实的科学研究（如生物发现、材料筛选）不仅需要宏观的“战略规划”（例如：先筛选候选分子，再验证性质），还需要微观的“战术执行”（例如：写代码调用RDKit计算具体的分子描述符）。\n*   **推论：** 单一的线性模型无法同时兼顾这两者。如果混在一起，模型容易在细节中迷失方向（目标漂移）；如果只做规划，又无法处理具体的工具操作。\n\n#### 2. 假设：模拟人类科学家的“双环行为”\n*   **假设：** 人类科学家的大脑运作模式是分离的——外层是“战略思考”（决定下一步做什么实验），内层是“实验操作”（在实验室里反复调试参数直到成功）。\n*   **推论：** 如果在智能体架构中显式地分离这两个过程，就能解决“长视距规划”与“局部试错”之间的矛盾。\n\n#### 3. 方法构建一：Plan-and-CodeAct 与 双环架构\n*   **逻辑：** 为了验证上述假设，必须设计一个**内外双环架构**。\n    *   **外环：** 负责“Plan”，维护全局目标，决定任务分解和终止。\n    *   **内环：** 负责“CodeAct”，通过写代码在沙箱中高频试错、调用工具，直到完成子任务。\n*   **结果：** 这种解耦使得系统既能保持长期目标的一致性，又具备局部探索的灵活性。\n\n#### 4. 观察：工具生态的“异构性与爆炸性”\n*   **现象：** 跨学科科研涉及成千上万个工具（生物、化学、数学等），如果把所有工具的说明都塞进Prompt，上下文会瞬间爆炸，且会干扰模型推理。\n*   **推论：** 工具不能是静态加载的，必须是动态的。就像科学家做实验时，只把当前需要的仪器拿到实验台上，其他的放在仓库里。\n\n#### 5. 方法构建二：意图感知的动态热插拔 (DHP)\n*   **逻辑：** 引入一个**检索机制**。当用户提出意图时，先识别领域，过滤掉无关工具，再通过语义匹配，只把最相关的少数工具“热插拔”到当前上下文中。\n*   **结果：** 既支持了大规模工具生态，又保证了推理的精准度和上下文的高效利用。\n\n#### 6. 观察：科研数据的“大规模与长链条”\n*   **现象：** 科研任务往往处理巨大的数据集（如基因序列），且步骤极多。直接把原始数据和历史步骤全部喂给LLM是不现实的。\n*   **推论：** 需要一种机制，让模型“知道”数据在哪里，而不是“看到”数据本身；同时，只保留历史步骤中的“精华”，而不是流水账。\n\n#### 7. 方法构建三：基于对象引用的稀疏上下文管理 (SCM)\n*   **逻辑：** 设计一套**稀疏化管理策略**。\n    *   **对象引用：** 用URL代替原始数据，按需加载。\n    *   **子任务隔离：** 不同阶段互不干扰。\n    *   **轨迹压缩：** 将长执行轨迹蒸馏为关键发现。\n*   **结果：** 解决了长上下文和大规模数据带来的系统不稳定问题。\n\n#### 8. 观察：科研能力的“积累性”\n*   **现象：** 资深科学家之所以强，是因为他们积累了大量的“科研直觉”和“技能包”（例如：遇到某类数据就知道该用什么标准流程）。现有智能体每次都是从零开始，不会越用越聪明。\n*   **推论：** 智能体必须具备“自我进化”的能力，即从成功的案例中提取模式，复用到未来的任务中。\n\n#### 9. 方法构建四：基于轨迹评估的自我进化 (TE-SE)\n*   **逻辑：** 引入一个**Critic Agent（评论家智能体）**，它不干活，只负责评价执行轨迹的好坏。将高质量的轨迹蒸馏为可复用的“Scientific Skills（科研技能）”。\n*   **结果：** 形成了一个闭环：执行 -> 评价 -> 提炼技能 -> 复用技能。这使得S1-NexusAgent能够随着使用时间的推移，变得越来越强。\n\n---\n\n### 总结\n\n作者的思考路径是从**“科研范式变革”**这一宏观背景出发，敏锐地捕捉到现有AI在**“长周期规划、工具调度、记忆管理、经验积累”**四个维度的痛点。通过**“模拟人类科学家行为”**这一核心假设，逻辑自洽地推导出了**“双环架构、动态工具检索、稀疏上下文、技能蒸馏”**这一整套方法论，最终构建了一个能够自我进化的通用科研智能体框架。"
                },
                {
                    "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering",
                    "arxiv_id": "2602.01465",
                    "authors": "Nikita Benkovich, Vitalii Valkov",
                    "summary": "Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献在于构建了一个名为 \"Agyn\" 的**多智能体系统**。它不仅仅是将LLM作为工具应用于软件工程，而是提出了一种新的**框架**，将软件工程显式建模为一个组织过程。该系统通过模拟工程团队的结构，定义了智能体的角色（协调、研究、实施、审查）、隔离沙箱和结构化通信机制。这完全符合“构建、改进或演化 LLM智能体”以及“多智能体系统”的研究目标。 2.  **正面指标（强匹配）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **多智能体特征**：论文重点研究了智能体间的 `Collaboration`（协作）、`Communication`（通信）以及 `Agent Society`（智能体社会/组织结构）。 *   **智能体能力**：涉及 `Tool Use`（使用沙箱进行实验）和 `Planning`（遵循定义的开发方法论进行分析和任务规范）。 3.  **排除标准（无冲突）**： 论文不涉及安全与对齐、多模态视觉技术或知识图谱，因此不触及相关排除规则。 4.  **特殊与模糊情况处理**： 虽然论文的应用场景是“软件工程”，但根据筛选标准中的“非演化型应用”排除规则，这篇论文并非简单地使用已有框架去解决领域问题。相反，它的核心创新点在于**“组织设计和智能体基础设施”**，即提出了一种新的多智能体协作范式。论文结论也明确指出，未来的进展取决于这种组织设计。因此，它属于构建智能体方法论的研究，而非单纯的应用研究。 综上所述，该论文在多智能体协作与组织架构方面提供了重要的方法论贡献，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有自主软件工程系统缺乏团队协作与角色分离的问题。针对真实的GitHub issue解决场景，我们提出了一种基于Multi-Agent System的组织化软件工程方法，通过模拟包含manager, researcher, engineer, reviewer的团队结构及GitHub原生工作流实现自动化开发。我们在SWE-bench 500上通过任务解决率验证了其有效性，该系统在未针对基准调优的情况下达到了72.4%的解决率，优于单智能体基线。",
                    "summary_translation": "大型语言模型在独立的软件工程任务中表现出了强大的能力，然而大多数自主系统仍将问题解决视为一个整体式或基于流水线的过程。相比之下，现实世界的软件开发被组织为一种协作活动，由遵循共享方法论的团队执行，具有明确的角色分工、沟通和审查机制。在这项工作中，我们提出了一个完全自动化的多智能体系统，该系统明确地将软件工程建模为一个组织流程，复现了工程团队的结构。该系统构建于 agyn（一个用于配置智能体团队的开源平台）之上，将专用智能体分配至协调、研究、实现和审查等角色，为它们提供隔离的沙箱进行实验，并支持结构化沟通。该系统遵循既定的开发方法论来处理问题，包括分析、任务规范、拉取请求创建和迭代审查，且整个过程无需任何人工干预。值得注意的是，该系统是为真实生产使用而设计的，并未针对 SWE-bench 进行调优。在 SWE-bench 500 上进行事后评估时，该系统解决了 72.4% 的任务，优于使用相当语言模型的单智能体基线。我们的研究结果表明，复现团队结构、方法论和沟通是自主软件工程的一种强大范式，且未来的进步可能同样取决于组织设计和智能体基础设施，而不仅仅是模型改进。",
                    "inspiration_trace": "基于对论文《Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观背景与起点\n**起点：** 追求真正的“自主软件工程”。\n**现状：** 虽然大语言模型（LLM）在代码理解、生成等单一任务上表现出色，且出现了如 SWE-bench 这样的端到端基准测试，但现有的自主系统在解决真实 GitHub 问题时，仍面临可靠性、可扩展性和成本效率的瓶颈。\n\n---\n\n### 2. Introduction 中的“讲故事”逻辑（问题引入）\n作者在 Introduction 部分通过以下逻辑链条构建了研究动机：\n\n1.  **现象观察：** LLM 已具备强大的软件工程能力，SWE-bench 等基准成为衡量自主代理能力的标准。\n2.  **现有范式批判：** 目前大多数方法依赖**单一代理架构**。这种架构让一个代理承担从理解问题、探索代码库、修改代码到验证的所有职责。这实际上是将软件工程视为一个“整体”或“流水线”过程。\n3.  **现实与模型的脱节：** 现实世界的软件开发并非由一人包办，而是基于团队的协作活动，包含明确的角色分工、沟通和代码审查。现有的单一代理或简单的流水线架构抽象掉了这些关键的现实要素（如同行评审、结构化协作）。\n4.  **现有方案的局限性：** 即便是尝试改进的方案（如自适应代理、通用平台），大多仍保留单一执行模型，且往往针对基准测试进行了过度调优，忽略了真实生产环境中的约束（如成本、可追溯性、可靠性）。\n5.  **核心痛点：** 单一代理架构迫使不同的任务需求（如探索性任务需要大上下文，实现任务需要快速反馈）统一在一种配置下，导致性能次优或计算浪费。\n\n---\n\n### 3. 研究问题\n基于上述逻辑，作者试图回答的核心问题是：\n\n**如果将软件工程显式建模为一个包含角色分离、协调机制和审查流程的组织化过程（即模拟人类工程团队的结构），是否能比单一代理或固定流水线架构更有效地实现自主软件工程？**\n\n---\n\n### 4. 思想演进脉络：从观察到方法论\n\n#### 阶段一：观察与批判\n*   **观察：** 软件开发本质上是一个社会组织过程，涉及项目经理、研究员、工程师、审查员等不同角色，通过 Issue、PR、Review 等机制交互。\n*   **批判：** 现有的 AI 系统试图用“一个超级大脑”或“僵化的流水线”来替代这个复杂的社会过程。这不仅不真实，而且效率低下（因为无法针对不同角色分配最合适的资源）。\n\n#### 阶段二：假设与洞察\n*   **核心假设：** “组织结构”本身就是一种强大的计算范式。通过模仿人类团队的**分工**和**协作**，可以解决单一代理的局限性。\n*   **关键洞察：**\n    1.  **责任分离：** 不同的开发阶段（分析、规划、编码、审查）对模型能力（推理深度 vs 代码能力）和上下文大小的需求不同。应该允许不同的代理使用不同的模型。\n    2.  **流程即协议：** 真实世界的开发流程（如 GitHub 的 PR 和 Review 机制）不仅是管理工具，更是保证代码质量的控制论机制。AI 系统不应只是生成 Patch，而应遵循这一流程。\n\n#### 阶段三：方法论形成\n*   **从“单一代理”转向“多代理组织”：** 不再设计一个全能的 Agent，而是设计一个**团队**。\n*   **定义角色：**\n    *   **Manager（经理）：** 负责协调、决策和流程控制（替代硬编码的流水线步骤）。\n    *   **Researcher（研究员）：** 负责深度理解和探索（使用大模型，强推理）。\n    *   **Engineer（工程师）：** 负责具体实现和调试（使用小模型，高效率）。\n    *   **Reviewer（审查员）：** 负责质量把关（模拟人类 Code Review）。\n*   **引入“原生”工作流：** 不创造新的交互协议，而是直接复用 GitHub 的原生工作流（Issue -> PR -> Review -> Merge）。这既是状态管理的手段，也是代理间通信的媒介。\n*   **环境隔离与工具化：** 给每个角色独立的沙箱环境，允许其独立实验和失败，同时提供专门的工具（如 `gh-pr-review`）来支持这种类人的协作模式。\n\n**总结：** 作者的思考路径是从**“任务视角”**（如何让 AI 写代码）转向**“组织视角”**（如何让 AI 团队像人类一样协作开发）。最终的方法论并非算法上的突破，而是**系统设计和社会工程学**在 AI 系统中的投射。"
                },
                {
                    "title": "Aggregation Queries over Unstructured Text: Benchmark and Agentic Method",
                    "arxiv_id": "2602.01355",
                    "authors": "Haojia Zhu, Qinyuan Xu, Haoyu Li, Yuxi Liu, Hanchen Qiu, Jiaoyan Chen, Jiahui Jin",
                    "summary": "Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to \"find all,\" not merely \"find one.\" Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献不仅仅是解决一个特定的NLP任务（聚合查询），更重要的是提出了 **DFA (Disambiguation--Filtering--Aggregation)**，这是一个**新的模块化智能体基线**。 *   论文明确将其定义为一种 \"Agentic Method\"，其核心在于构建和改进LLM智能体的架构。DFA 将复杂的聚合查询任务分解为“歧义消除-过滤-聚合”三个可解释的阶段，这属于**单智能体** 研究中的**规划** 和 **工具使用** 范畴。 2.  **不属于“非演化型应用” (第一步 - 排除)**: *   虽然论文的应用场景是“非结构化文本上的聚合查询”，但这并非像医疗、金融或法律那样的垂直领域应用，而是对LLM基础能力（信息检索与聚合）的深化。 *   关键在于，论文并没有简单地套用现有的智能体框架（如直接使用ReAct），而是设计了一个新的智能体工作流（DFA）来解决现有范式（Text-to-SQL, RAG）无法解决的“完整性”问题。这符合“构建、改进LLM智能体”的核心目标。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除主题。 综上所述，该论文提出了一种新的智能体框架来解决复杂的推理任务，属于Agentic AI中单智能体规划与架构创新的研究范畴，符合筛选要求。",
                    "summary2": "本文旨在解决非结构化文本上的聚合查询问题，即要求系统“找到所有”满足条件的实体。针对大规模语料库中证据稀疏的场景，我们提出了一种名为DFA的模块化智能体方法，将查询分解为消歧、完整性感知过滤和聚合三个阶段。我们在构建的AGGBench基准上，通过证据召回率和归一化绝对计数误差（NACE）等指标验证了其有效性。实验表明，DFA在证据覆盖率上比强基线提升了5倍。",
                    "summary_translation": "针对自由文本的 Aggregation query (聚合查询) 是一个长期存在但未被充分探索的问题。与普通的 Question Answering (问答) 不同，Aggregation query (聚合查询) 需要详尽的证据收集，要求系统必须“找到所有”，而不仅仅是“找到一个”。现有的范式，如 Text-to-SQL 和 Retrieval-Augmented Generation (检索增强生成)，均无法实现这种 Completeness (完整性)。在这项工作中，我们在具有严格 Completeness (完整性) 要求的 Corpus-bounded setting (语料库受限设定) 下，对针对文本的 Entity-level (实体级) Aggregation query (聚合查询) 进行了形式化定义。为了实现基于原则的评估，我们介绍了 AGGBench，这是一个旨在现实的大规模语料库下评估面向 Completeness (完整性) 的聚合的 Benchmark (基准)。为了配合该 Benchmark (基准)，我们提出了 DFA (Disambiguation--Filtering--Aggregation，即消歧-过滤-聚合)，这是一个模块化的 Agentic (智能体) Baseline (基线)，它将 Aggregation query (聚合查询) 分解为可解释的阶段，并揭示了与 Ambiguity (歧义性)、Filtering (过滤) 和 Aggregation (聚合) 相关的关键 Failure modes (失效模式)。实证结果表明，DFA 在强大的 RAG 和 Agentic (智能体) Baseline (基线) 上，始终提高了 Aggregation evidence coverage (聚合证据覆盖率)。数据和代码可在 https://anonymous.4open.science/r/DFA-A4C1 获取。",
                    "inspiration_trace": "基于对论文《Aggregation Queries over Unstructured Text: Benchmark and Agentic Method》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的逻辑链条，将一个宏观的现实需求转化为具体的学术痛点：\n\n1.  **现实需求的特殊性**：\n    *   **观察**：在法律、金融、合规等领域，分析任务往往需要对海量文本进行系统性检查（如“找出所有满足特定监管条件的条款”）。\n    *   **对比**：这与传统的问答任务截然不同。传统问答旨在“找到一个” plausible answer（找到即可），而聚合查询要求“找到所有”实体。\n\n2.  **现有范式的失效**：\n    *   **路径一（结构化优先）**：将文本转化为数据库（Text-to-SQL）。\n        *   **缺陷**：预处理成本高昂，且构建的结构化数据质量低；固定的模式限制了自然语言查询的灵活性。\n    *   **路径二（检索增强生成 RAG）**：基于检索的问答。\n        *   **缺陷**：RAG的设计初衷是“开放域问答”，目标是检索“高度相关”的片段，而非“全覆盖”的证据。其固定的 `Top-k` 检索窗口与聚合查询的“完整性”需求根本性不兼容。一旦相关证据超过 k 个，遗漏就会导致系统性的计数不足。\n\n3.  **评估体系的缺失**：\n    *   现有的基准测试主要关注事实性问答或数值推理，缺乏针对“聚合完整性”的评估协议。\n\n4.  **结论**：\n    *   现有的方法无法在合理的推理成本下实现完整的聚合查询，因此迫切需要一种新的评估基准和新的方法论。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式定义的研究问题为：\n\n**“如何在非结构化文本语料库中，以严格完整性为约束，有效地执行实体级聚合查询？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者产出这篇论文的思考过程可以概括为从“现象观察”到“假设提出”，再到“方法论构建”的演进：\n\n#### 1. 现象观察与痛点定位\n*   **观察**：现有的 RAG 系统在处理“计数”或“列举”类问题时表现不佳，经常漏数。\n*   **归因**：这不仅仅是模型能力的问题，而是**设计目标的错位**。RAG 追求的是“精准度”，即检索到的内容要相关；而聚合查询追求的是“召回率”，即不能漏掉任何一个符合条件的证据。\n*   **核心矛盾**：`Top-k` 的硬性截断导致了不可逆的信息丢失。\n\n#### 2. 假设提出\n*   **假设**：要解决“完整性”问题，不能依赖一次性的检索，而必须引入**迭代**和**显式的状态管理**。\n*   **推论**：系统需要具备“自我检查”的能力，即意识到自己可能漏掉了什么，并能够回溯去寻找。此外，自然语言查询往往存在歧义，在“找全”的要求下，歧义会被放大，因此必须先解决“理解”层面的问题。\n\n#### 3. 方法论构建\n基于上述假设，作者没有设计一个端到端的黑盒模型，而是提出了一个**模块化的智能体框架**，将复杂的聚合任务拆解为三个可解释的阶段：\n\n*   **阶段一：消歧**\n    *   **思考**：如果查询条件模糊（例如“高影响”），系统无法判断哪些实体符合条件，更谈不上找全。\n    *   **策略**：在检索之前，先识别并分类查询中的歧义（如边界模糊、逻辑范围不清），并生成澄清问题或重写查询，确保后续检索的目标是明确的。\n\n*   **阶段二：完整性感知的过滤**\n    *   **思考**：传统的语义相似度检索会漏掉那些语义相关但关键词不匹配的文档。为了“找全”，我们需要一种更激进的过滤策略，且必须允许“试错”。\n    *   **策略**：引入**快照机制**。系统对语料库进行迭代过滤，但**不永久丢弃**被过滤掉的块。如果发现过滤过严，可以回滚到上一个快照，换一种策略（如从精确匹配切换到模糊匹配）重新过滤。这保证了召回率。\n\n*   **阶段三：聚合**\n    *   **思考**：找到候选证据后，同一个实体可能在不同文档中被多次提及，直接计数会导致重复。\n    *   **策略**：利用 LLM 作为裁判，对提取的实体进行跨文档的对齐和去重，最终得出准确的聚合结果。\n\n#### 4. 闭环验证\n*   **思考**：为了证明这个方法的有效性，必须有一个能衡量“完整性”的标尺。\n*   **策略**：构建 **AGGBench**。作者意识到在大规模语料上做人工标注是不可能的，因此设计了一个巧妙的构建流程：先在核心子集上做高质量标注，再通过添加无关文档来扩展语料库，从而在保证真实性的同时实现了可扩展的评估。\n\n---\n\n**总结**：\n作者的思考路径是从**发现 RAG 在“全覆盖”场景下的结构性缺陷**出发，提出**“迭代+回溯”的智能体假设**，最终通过**DFA（消歧-过滤-聚合）的三阶段解耦设计**，将“完整性”从一个模糊的概念转化为可计算、可诊断的系统模块。"
                },
                {
                    "title": "Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction",
                    "arxiv_id": "2602.01202",
                    "authors": "Mingze Kong, Zikun Qu, Zhongquan Zhou, Pengyu Liang, Xiang Li, Zhiwei Shang, Zhi Hong, Kaiyu Huang, Zhiyong Wang, Zhongxiang Dai",
                    "summary": "The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心贡献是提出了 Workflow-R1 框架和 Group Sub-sequence Policy Optimization (GSsPO) 算法，旨在解决多轮工作流的构建与优化问题。这属于构建和改进 LLM智能体（Agentic AI）的方法论研究，而非单纯的应用或基础设施研究。 2.  **正面指标 (高度匹配)**: - **核心范式**: 论文明确涉及 `Agentic AI` 和 `LLM-based Agents`，讨论了智能体工作流的演化。 - **智能体能力**: 论文聚焦于 `Planning`（工作流构建）和 `ReAct`（Think-Action 动态），这是单智能体规划与推理的核心能力。 - **演化机制**: 论文引入了强化学习（RL）算法 GSsPO 来优化智能体的决策过程，属于智能体能力的自我完善和迭代优化。 3.  **排除标准 (无冲突)**: 论文不涉及安全对齐、多模态视觉或图技术。 4.  **特殊处理 (符合)**: 论文关于智能体的规划与多步推理，且提出了新的优化框架，符合“保留”关于智能体如何进行规划或在复杂任务中进行多步推理的论文的要求。 综上所述，该论文通过改进智能体的工作流构建和决策优化机制，直接贡献于 Agentic AI 的发展，符合筛选标准。",
                    "summary2": "本文旨在解决现有工作流优化方法将构建过程视为静态代码生成，导致缺乏动态适应能力的问题。针对多轮工作流构建场景，我们提出了一种Workflow-R1框架及Group Sub-sequence Policy Optimization (GSsPO)算法，将优化粒度对齐至Think-Action子序列。并在七个QA基准测试上通过Exact Match (EM)指标验证了其有效性，结果显示该方法显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction》，以下是对作者核心方法论产出逻辑链的系统推演。\n\n---\n\n### 一、 逻辑推演：从宏观观察到微观方法\n\n**1. 宏观观察：智能体的演进与“工作流”的兴起**\n*   **观察起点**：大语言模型（LLM）已超越简单的对话接口，通过集成外部工具和推理路径，演变为能够解决复杂问题的自主智能体。\n*   **现状归纳**：工业界和学术界逐渐聚焦于“基于工作流的智能体”，即通过算子（如搜索、回答、修订）组成的可执行图来分解复杂任务。当前的研究热点在于如何自动化地优化这些工作流，以替代人工提示工程。\n\n**2. 问题识别：静态规划的陷阱**\n*   **审视现有方法**：现有的自动化工作流优化方法通常将工作流合成视为一个“静态的、一次性的代码生成问题”。即，模型在执行任何算子之前，就先生成了完整的、类似代码的执行图。\n*   **发现核心缺陷**：作者将这种局限性定义为**“静态执行陷阱”**。\n    *   **开环系统**：这种范式将规划阶段与运行时执行解耦。智能体在未观察到任何中间结果或执行反馈的情况下，就承诺了一个完整的算子序列。\n    *   **缺乏适应性**：由于控制流是刚性的，无论环境如何变化，工作流都无法根据中间观察结果进行动态调整。\n\n**3. 范式转移：从“静态合成”到“动态交互”**\n*   **提出假设**：为了克服僵化，工作流构建不应被视为静态的程序合成，而应被视为一个**序列决策过程**，策略需持续基于观察轨迹进行调整。\n*   **引入新框架**：受迭代推理（如DeepSeek-R1）的启发，提出 **Workflow-R1**。\n    *   **动态交互**：将工作流优化重构为智能体与环境之间的多轮对话。\n    *   **思维-行动循环**：智能体不再生成单一脚本，而是进入交错的“思考、行动、观察”循环。这是一个闭环设计，确保每一步决策都基于前一步的执行结果。\n    *   **自然语言接口**：为了降低编码语法的约束并实现“冷启动”RL能力，交互采用自然语言而非可执行代码。\n\n**4. 技术瓶颈：强化学习中的“粒度失配”**\n*   **遇到新挑战**：虽然动态交互解决了适应性问题，但将其与强化学习（RL）对齐时，出现了一个独特的挑战：**优化粒度与智能体推理语义结构的不匹配**。\n*   **分析现有RL范式的极端缺陷**：\n    *   **Token级优化（如GRPO）**：过于细碎。将复杂的推理链视为独立token的松散集合。在孤立token上强制更新会破坏“思考-行动”过程的语义完整性，忽略了推理与后续动作之间的强因果依赖。\n    *   **Sequence级优化（如GSPO）**：过于粗糙。将整个多轮交互序列视为一个整体。虽然对齐了最终奖励，但在长视界任务中，单一信号无法区分有效的推理步骤和错误的步骤，导致学习效率低下。\n\n**5. 关键洞察：寻找“原子决策单元”**\n*   **逻辑推演**：最优的对齐目标既不是单个token，也不是整体序列，而是**子序列**。\n*   **定义单元**：在多轮设置中，每一轮都会产生一个“思考-行动”对，这构成了一个自然的子序列。这是智能体推理的原子单元。\n\n**6. 方法论形成：GSsPO**\n*   **最终方案**：提出**分组子序列策略优化**。\n    *   **结构感知**：该算法显式地将优化粒度与智能体的决策单元（Think-Action对）对齐。\n    *   **梯度更新**：通过将每个子序列作为原子优化单元，GSsPO在细粒度反馈和全局一致性之间架起了桥梁，确保策略更新严格遵守智能体推理的决策边界。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑（问题引入）\n\n1.  **背景铺垫**：LLM已演变为通过集成外部工具解决复杂任务的自主智能体，工业界确立了“基于工作流的智能体”这一鲁棒标准。\n2.  **现有做法**：为了自动化这一过程，现有研究通常将工作流合成表述为生成一个完整的、可执行的代码程序（在执行前确定计算图）。\n3.  **揭示痛点**：这种范式存在根本局限，即**“静态执行陷阱”**。它是一个开环系统，规划与执行解耦，导致智能体无法根据中间观察结果动态调整，缺乏处理动态环境的灵活性。\n4.  **提出方向**：受迭代推理进展的启发，我们需要将工作流构建从静态规划转变为**动态交互**（多轮、自然语言、Think-Act-Observe闭环）。\n5.  **深化冲突**：虽然动态交互解决了适应性问题，但在使用强化学习优化这一过程时，出现了**“粒度失配”**。现有的Token级方法破坏语义完整性，而Sequence级方法无法在长序列中进行有效的信用分配。\n6.  **引出核心**：因此，最优的优化目标应当是**子序列**（即Think-Action对）。基于此，我们提出了GSsPO算法来解决这一不匹配，实现鲁棒的多轮推理学习。\n\n---\n\n### 三、 研究问题\n\n**如何通过强化学习优化动态多轮工作流构建过程，以解决现有方法中优化粒度与智能体语义决策单元之间的失配问题？**"
                },
                {
                    "title": "AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling",
                    "arxiv_id": "2602.01078",
                    "authors": "Tong Xia, Weibin Li, Gang Liu, Yong Li",
                    "summary": "LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \\textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \\textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \\textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\\% in prediction performance and 50.2\\% in uncertainty estimation.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合多智能体方向**: 论文明确提出了 \"AutoHealth\"，这是一个新颖的“不确定性感知多智能体系统”。这直接对应了筛选标准中的“多智能体”方向，即构建新的智能体框架。 2.  **智能体协作与机制**: 论文详细描述了系统通过“五个专门的智能体”之间的“闭环协调”来执行数据探索、模型构建、训练和优化。这种多智能体协作、任务分解和执行流程完全符合 Agentic AI 的核心范式。 3.  **非单纯应用型论文**: 尽管论文的应用场景是健康数据，但其核心创新点在于设计了一个特定的多智能体架构来解决该领域的挑战（如异构数据适应性和不确定性量化），而不是简单地将现有的通用智能体（如 AutoGPT）直接应用于该领域。因此，它符合“构建或改进 LLM 智能体”的目标，而非被排除的“非演化型应用”。 4.  **通过排除项检查**: 论文虽然涉及“不确定性估计”，但这作为智能体系统的一个功能模块用于辅助决策，而非论文主要研究 Safety 或 Alignment；也不涉及多模态视觉核心或基础设施研究。 综上所述，该论文属于构建多智能体系统以解决复杂任务的前沿研究，符合筛选要求。",
                    "summary2": "本文旨在解决现有系统在健康数据建模中难以处理异构模态及缺乏不确定性估计的问题。针对多模态健康数据及高可靠性要求的场景，我们提出了AutoHealth，一种不确定性感知的多智能体系统，通过五个专业智能体的闭环协调实现自主建模与可靠性评估。并在包含17个任务的真实基准上，通过Success Rate、Normalized Performance Score等指标验证了其有效性，实现了100%任务成功率及显著的性能提升。",
                    "summary_translation": "基于大语言模型的智能体在自主机器学习方面展现了巨大的潜力，但它们在健康数据上的适用性仍然有限。现有系统通常难以在异构健康数据模态之间进行泛化，严重依赖预定义的解决方案模板，且对特定任务目标的适应性不足，并且很大程度上忽视了不确定性估计，而这对于医疗保健中的可靠决策至关重要。为了解决这些挑战，我们提出了 \\textit{AutoHealth}，这是一种新颖的具有不确定性感知能力的多智能体系统，能够自主对健康数据建模并评估模型可靠性。\\textit{AutoHealth} 利用五个专用智能体之间的闭环协调来执行数据探索、任务条件化模型构建、训练和优化，同时共同优先考虑预测性能和不确定性量化。除了生成即用型模型外，该系统还生成综合报告以支持可信解释和风险感知决策。为了严格评估其有效性，我们构建了一个具有挑战性的真实世界基准，其中包含跨越多种数据模态和学习设置的17个任务。\\textit{AutoHealth} 完成了所有任务，并且在预测性能上优于最先进的基线模型29.2%，在不确定性估计上优于50.2%。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning",
                    "arxiv_id": "2602.00994",
                    "authors": "Yu Li, Mingyang Yi, Xiuyu Li, Ju Fan, Fuxin Jiang, Binbin Chen, Peng Li, Jie Song, Tieying Zhang",
                    "summary": "Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。以下是详细的判断依据： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **Disentangled Action Reasoning Tuning (DART)** 框架，这是一种用于改进 **Agentic Reinforcement Learning (ARL)** 的新方法。 *   论文的研究对象是 **LLM 智能体**，旨在解决智能体在训练过程中“推理”与“工具使用”两种能力相互干扰的问题。 *   这属于“构建、改进 LLM 智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型能力的非 Agentic 研究。 2.  **正面指标（第二步）**： *   论文明确涉及核心范式 **`Agentic AI`**。 *   论文重点讨论了智能体的两项核心能力：**`Reasoning`**（推理）和 **`Tool Use`**（工具使用）。 *   论文提出了一种新的调优机制来优化这些能力的结合，符合对智能体框架进行改进的研究目标。 3.  **排除标准（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）**： *   关于推理/规划：论文不仅仅是研究如何提高 LLM 的基础数学或逻辑能力，而是研究智能体如何在 **Agentic RL** 的框架下，将推理与外部工具执行相结合。DART 框架通过解耦参数更新来优化这一过程，属于智能体规划与执行的架构创新，因此应当保留。 综上所述，该论文针对单智能体内部的推理与工具使用冲突提出了创新的解决方案，直接贡献于 LLM 智能体的构建与改进，符合筛选标准。",
                    "summary2": "本文旨在解决 Agentic Reinforcement Learning (ARL) 中推理与工具使用在联合优化时产生的梯度冲突问题。针对 ARL 训练场景，我们提出了一种 Disentangled Action–Reasoning Tuning (DART) 框架，利用独立的 LoRA 模块解耦推理和工具使用的参数更新。在七个大规模工具增强 QA 基准测试上，通过 Exact Match (EM) 指标验证了其有效性，平均性能提升超过 6.35%。",
                    "summary_translation": "Agentic Reinforcement Learning (ARL，代理强化学习) 专注于训练大语言模型，使其将推理与外部工具执行交织在一起，以解决复杂任务。大多数现有的 ARL 方法训练单一共享模型参数以支持推理和工具使用行为，隐含地假设联合训练能带来整体智能体性能的提升。尽管这一假设得到了广泛采用，但对其进行实证检验的研究却寥寥无几。在本文中，我们通过引入 Linear Effect Attribution System (LEAS，线性效应归因系统) 系统地研究了这一假设，该系统提供了推理和工具使用行为之间干扰的定量证据。通过深入分析，我们表明这两种能力通常会导致梯度方向不一致，从而产生训练干扰，削弱了联合优化的有效性，并对主流的 ARL 范式提出了挑战。为了解决这个问题，我们提出了 Disentangled Action Reasoning Tuning (DART，解耦动作推理微调)，这是一个简单高效的框架，通过独立的低秩适应模块显式地解耦推理和工具使用的参数更新。实验结果表明，DART 始终优于基线方法，平均提升了 6.35%，并且在仅使用单个模型的情况下，达到了与显式分离工具使用和推理的多智能体系统相当的性能。",
                    "inspiration_trace": "基于您提供的论文内容，我为您系统性地推演了作者产出这篇文章的思考过程。以下是关于其逻辑演进的分析：\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 部分构建了一个经典的“现状-质疑-发现”的叙事逻辑，具体如下：\n\n1.  **背景铺垫**：\n    Agentic Reinforcement Learning (ARL) 的目标是将大语言模型（LLMs）训练成能够交替进行“推理”和“工具使用”的智能体，以解决复杂任务。\n\n2.  **主流范式与隐含假设**：\n    目前绝大多数 ARL 方法都采用**单一共享模型参数**的架构来同时支持推理和工具使用行为。这种设计背后隐含了一个普遍的假设：**在同一个参数空间内联合优化这两种异质能力，能够带来整体性能的提升（即两者是兼容或协同的）。**\n\n3.  **提出质疑**：\n    尽管这种联合训练的方法被广泛采用，但上述假设在实证层面很少被严格检验。这构成了研究的切入点：这种“理所当然”的做法真的有效吗？\n\n4.  **实证挑战与现象发现**：\n    作者通过受控的实证分析挑战了这一假设。他们发现这两种能力并非独立，而是表现出一种**“跷跷板”现象**：提升工具使用能力往往会削弱推理能力，反之亦然。这表明在共享参数上进行联合优化会引发隐性的竞争。\n\n5.  **揭示本质**：\n    这种竞争导致了次优的性能，从而对当前主流的 ARL 范式提出了挑战。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“在 Agentic RL 中，推理与工具使用能力在共享参数空间下的联合优化是否会产生干扰？如果存在干扰，其内在机制是什么，又该如何通过解耦训练来消除这种干扰以提升智能体性能？”**\n\n---\n\n### 三、 作者产出核心方法的逻辑链推演\n\n以下是从宏观观察到具体方法论的完整思考演进过程：\n\n#### 第一阶段：观察与怀疑\n*   **宏观现象**：现有的 ARL 训练通常把“思考”和“用工具”混在一起训练同一个模型。\n*   **直觉性怀疑**：逻辑推理（内部认知）和工具调用（外部交互）是两种截然不同的行为模式。强行把它们塞进同一个参数空间里进行更新，真的不会打架吗？\n*   **初步假设**：也许这种“大一统”的训练方式并不是最优解，两者之间可能存在某种负面的相互作用。\n\n#### 第二阶段：量化干扰\n*   **思考**：怀疑需要证据。如何证明“推理”和“工具使用”之间存在冲突？不能只看最终分数，需要把两者的贡献拆解开来看。\n*   **方法论构建（LEAS）**：作者设计了一个**线性效应归因系统（LEAS）**。这就像一个“显微镜”，通过构建不同的模型变体（有的只训练推理，有的只训练工具，有的混合推理），将模型的整体表现分解为“推理效应”、“工具效应”以及“交互效应”。\n*   **验证**：通过 LEAS，作者量化了交互系数。结果证实了直觉：**交互系数显著为负**。这意味着两者确实在“打架”，即存在干扰。\n\n#### 第三阶段：深究根源\n*   **思考**：既然证实了干扰存在，那么根本原因是什么？为什么在同一个模型里训练这两者会互相拖后腿？\n*   **深入分析**：作者将目光投向了优化的微观层面——**梯度**。\n*   **发现机制**：通过分析梯度方向，作者发现了一个关键现象：**推理 Token 产生的梯度方向与工具使用 Token 产生的梯度方向往往是不一致的，甚至接近正交（垂直）**。\n*   **逻辑推演**：当两个方向完全不同的梯度强行作用在同一组参数上时，优化器只能取一个“折中”的方向。这个折中方向对推理来说不够好，对工具使用来说也不够好。这就是导致性能瓶颈的数学本质。\n\n#### 第四阶段：提出解法\n*   **思考**：既然问题的根源是“梯度在共享参数上打架”，那么解决思路就很明确了——**让它们分开更新，互不干扰**。\n*   **方案雏形**：最直接的方法是用两个独立的模型（Multi-Agent），一个专门推理，一个专门用工具。但这太贵了，存储和计算成本翻倍。\n*   **优化方案（DART）**：如何在保持“单模型”部署效率的同时，实现“双模型”的解耦效果？\n    *   **利用 LoRA**：利用低秩适应（LoRA）技术，冻结主模型参数，只训练额外的适配器。\n    *   **解耦设计**：引入**两个独立的 LoRA 模块**，一个专门给推理 Token 用，一个专门给工具使用 Token 用。\n    *   **路由机制**：在推理时，根据 Token 的类型（是思考还是调用工具），动态激活对应的 LoRA 模块。\n*   **预期效果**：这样，推理的梯度只更新推理 LoRA，工具的梯度只更新工具 LoRA。两者在参数空间上完全隔离，消除了梯度冲突，同时保留了单模型的部署便利性。\n\n#### 第五阶段：验证与闭环\n*   **思考**：这个解耦方案真的有效吗？它是否真的解决了梯度冲突问题？\n*   **实验验证**：\n    1.  **性能对比**：DART 在多个基准上显著优于传统的联合训练基线。\n    2.  **机制验证**：在固定检索内容的情况下，DART 的推理能力依然更强，证明了解耦确实保护了推理能力不被工具训练干扰。\n    3.  **效率对比**：DART 达到了接近双模型系统的性能，但成本远低于双模型。\n\n**总结**：作者从对主流范式的直觉怀疑出发，通过自研的分析工具（LEAS）证实了干扰的存在，深入到梯度层面揭示了“方向冲突”的数学本质，最终利用参数解耦技术（DART）巧妙地解决了这一矛盾，实现了性能与效率的平衡。"
                },
                {
                    "title": "Beyond Output Critique: Self-Correction via Task Distillation",
                    "arxiv_id": "2602.00871",
                    "authors": "Hossein A. Rahmani, Mengting Wan, Pei Zhou, Longqi Yang, Nick Craswell, Emine Yilmaz, Sujay Kumar Jauhar",
                    "summary": "Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合研究目标**: 论文提出了 \"SELF-THOUGHT\" 框架，旨在通过任务蒸馏和结构化模板来改进 LLM 的自我修正能力。这直接对应于研究课题中的 **\"自我演化\" (Self-Evolving)** 方向，特别是其中的 \"自我完善\" (Self-Improvement) 和 \"自我修正\" (Self-Correction) 机制。同时，这也属于 **\"单智能体\" (Agentic)** 范畴下的 \"自我反思\" (Self-Reflection) 能力建设。 2.  **符合正面指标**: *   **演化机制**: 论文明确涉及 `Self-Correction`、`Iterative Improvement`（迭代细化）和 `Self-Refine`。 *   **智能体能力**: 论文探讨了模型如何通过内部反馈（从初始响应中蒸馏任务结构）来指导后续行动，这属于智能体的高级认知能力。 3.  **通过排除标准检查**: *   **非应用型**: 论文虽然在不同推理任务上进行了实验，但其核心贡献是通用的方法论（SELF-THOUGHT 框架），而非将 LLM 简单应用于特定垂直领域（如医疗、法律）。 *   **非基础推理**: 虽然涉及推理，但论文并非仅通过微调或简单的提示工程来提升基础 Token 预测能力，而是构建了一个包含 \"任务抽象\" 和 \"解决方案实例化\" 的结构化自我修正流程，这属于智能体框架的构建与改进。 *   **非安全/多模态**: 论文不涉及安全对齐、可解释性或多模态视觉内容。 综上所述，该论文的核心在于构建一种让 LLM 通过自我反思和结构化抽象进行自我演化的新框架，完全符合 \"LLM智能体及其演化\" 的研究范围。",
                    "summary2": "本文旨在解决现有自我修正方法仅修补表面错误、难以修正深层推理缺陷及无法有效迁移至小模型的问题。针对复杂的推理任务，我们提出了一种 **SELF-THOUGHT** 框架，引入 **Task Abstraction** 中间步骤，将问题蒸馏为结构化模板以指导修正，并在 Game of 24、AIME 等基准上通过准确率等指标验证了其有效性。",
                    "summary_translation": "大语言模型 展现出了令人期待的自我修正 能力，即通过迭代优化 能够提升生成响应的质量。然而，大多数现有方法仅在输出评判 层面进行操作，虽然能修补表面错误，但往往无法纠正更深层的推理缺陷。我们提出了 SELF-THOUGHT 框架，该框架在解决方案细化 之前引入了一个任务抽象 的中间步骤。给定输入和初始响应，模型首先将任务蒸馏 为一个结构化模板，该模板能够捕捉关键变量、约束条件 和问题结构。这种抽象随后指导解决方案实例化，将后续响应建立在对任务更清晰的理解之上，从而减少错误传播。关键在于，我们表明这些抽象可以跨模型迁移：由较大模型生成的模板可以作为较小 LLMs 的结构化指南，而较小模型通常难以进行内在的自我修正。通过重用蒸馏出的任务结构，较小的模型无需进行繁重的微调 或依赖外部验证器，即可实现更可靠的优化。在多样化推理任务 上的实验表明，SELF-THOUGHT 提高了大型和小型模型的准确性、鲁棒性 和泛化能力，为构建更可靠的自修正语言系统提供了一条可扩展的路径。",
                    "inspiration_trace": "基于对论文《Beyond Output Critique: Self-Correction via Task Distillation》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 一、 宏观观察：现状与机遇\n**思考起点：** 大语言模型（LLMs）在推理和对话方面取得了巨大进展，但仍然存在推理缺陷、事实错误和不一致的问题。\n**现有趋势：** “自我修正”被视为缓解这些局限性的有希望的解决方案，即通过迭代优化来提高生成质量。\n\n---\n\n### 二、 逻辑演进：从“讲故事”到“找痛点”\n作者在Introduction中通过层层递进的逻辑，揭示了现有研究的根本缺陷，构建了研究的必要性：\n\n1.  **初步承诺：** 现有的自我修正方法（如Self-Refine, Reflexion等）展示了模型可以批判自己的答案并生成反馈，这看起来很有前景。\n2.  **现实打击（痛点一：修正的深度）：** 尽管这些方法在某些情况下有效，但在复杂问题解决（如数学推理）中，其效能仍然非常有限。\n    *   *分析：* 现有方法大多停留在**“输出批判”**的层面。模型生成答案 -> 评估答案 -> 修补表面错误。这就像是在给错误的答案“打补丁”，往往无法修正更深层的推理逻辑缺陷。\n3.  **现实打击（痛点二：模型的规模）：** 当前的自我修正研究主要针对**大规模模型**（依赖其强大的能力来生成批判和执行修正）。\n    *   *分析：* 在实际应用中广泛使用的**小模型**由于缺乏推理深度和鲁棒性，很难利用这些技术。现有的自我修正技术对小模型几乎没有带来可衡量的提升，甚至可能因为错误的自我批判而降低性能。\n4.  **核心矛盾总结：** 我们面临一个两难境地——现有的修正机制既不够“深”（无法解决深层逻辑错误），也不够“广”（无法惠及广泛使用的小模型）。\n\n---\n\n### 三、 研究问题\n基于上述痛点，作者提出了本研究的核心问题：\n\n**如何设计一种自我修正机制，使其不仅能有效修正深层推理缺陷，而且能够跨越模型规模的限制，使小模型也能从中受益？**\n\n---\n\n### 四、 思想演进：从假设到方法论\n为了回答上述问题，作者的思考路径经历了以下关键转折：\n\n**1. 视角的转换：从“修补答案”转向“理解任务”**\n*   *思考：* 为什么现有的“输出批判”无法修正深层错误？因为模型在修正时，依然是在基于对问题的错误理解进行操作。如果对任务本身的结构理解不清，修正只能是盲目的。\n*   *假设：* 如果在修正答案之前，先强迫模型停下来，对任务本身进行一次结构化的“抽象”和“蒸馏”，明确变量、约束和问题结构，那么随后的修正就会建立在更稳固的基础上。\n\n**2. 引入中间态：任务蒸馏**\n*   *构思：* 将自我修正过程拆解。不再直接从“错误答案”到“修正答案”，而是引入一个中间步骤——**任务抽象**。\n*   *具体化：* 模型首先充当“元蒸馏器”，将输入问题转化为一个结构化的模板。这个模板不包含具体解，但包含了“如何解决这类问题”的逻辑骨架。\n\n**3. 解决“小模型”困境：能力的迁移**\n*   *思考：* 小模型自己很难生成高质量的任务抽象（因为它本身推理能力弱）。但是，小模型擅长“填空”或“执行”明确的指令。\n*   *假设：* 如果大模型生成的“任务抽象模板”是通用的，那么小模型是否可以直接复用这些模板？\n*   *方案：* 提出 **Distil-Thought**。让大模型负责“思考”（生成抽象模板），小模型负责“执行”（基于模板实例化解决方案）。这实现了推理能力的跨模型迁移，而无需昂贵的外部验证器或微调。\n\n**4. 最终方法论的形成**\n*   **Self-Thought (大模型/通用)：** 初始生成 -> **任务抽象** -> 解决方案实例化。通过分离“理解任务”和“解决问题”，减少错误传播。\n*   **Distil-Thought (小模型)：** 复用大模型的抽象模板 -> 解决方案实例化。通过结构化引导，弥补小模型的推理短板。\n\n---\n\n### 总结\n作者的思考路径是从**对现有“打补丁”式修正的不满**出发，意识到**深层理解缺失**和**模型规模限制**是两大瓶颈，进而提出**“任务抽象”**作为连接问题与解的桥梁，最终通过**模板复用**实现了从大模型到小模型的能力降维与赋能。"
                },
                {
                    "title": "Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward",
                    "arxiv_id": "2602.00845",
                    "authors": "Senkang Hu, Yong Dai, Yuzhi Zhao, Yihang Tao, Yu Guo, Zhengru Fang, Sam Tak Wu Kwong, Yuguang Fang",
                    "summary": "Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断**: *   论文的核心贡献是提出了 **InfoReasoner**，这是一个旨在优化 **Agentic Reasoning**（智能体推理）的统一框架。 *   论文关注的是智能体如何通过 **Retrieval**（检索/工具使用）动态获取外部知识，并利用奖励信号来优化这一过程。这属于构建和改进 LLM 智能体的方法论，而非简单的应用或基础设施研究。 2.  **符合正面指标**: *   **核心范式**: 论文标题和摘要中多次明确提及 **Agentic Reasoning**，直接对应我的研究焦点。 *   **智能体能力**: 论文重点研究了 **Tool Use / Tool Augmentation**（通过检索获取信息）以及 **Planning/Reasoning**（通过 GRPO 优化策略以最大化认知进步）。 *   **演化机制**: 论文使用了 **Group Relative Policy Optimization (GRPO)** 和合成奖励信号来训练和改进智能体的策略，这体现了通过反馈进行自我完善和迭代的机制。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态（视觉）或图技术。 *   它不是关于提高 LLM 基础 Token 预测能力的数学研究，而是专注于智能体在复杂任务中如何进行信息搜寻和推理的 Agentic 框架。 综上所述，该论文致力于解决 LLM 智能体在工具使用（检索）和推理优化中的关键问题，属于单智能体研究的高质量论文。",
                    "summary2": "本文旨在解决 Agentic Reasoning 中检索优化缺乏密集奖励信号的问题。针对检索增强推理场景，我们提出了一种 InfoReasoner 框架，通过合成语义信息增益奖励来激励有效信息搜寻。该方法将信息增益定义为信念状态的不确定性减少，并利用双向文本蕴含计算语义熵。我们在七个问答基准上通过准确率验证了其有效性，实现了平均 5.4% 的性能提升。",
                    "summary_translation": "Agentic reasoning (智能体推理) 使大型推理模型 (LRMs) 能够动态获取外部知识，但由于缺乏稠密且基于原理的奖励信号，优化检索过程仍然充满挑战。在本文中，我们介绍了 InfoReasoner，这是一个统一的框架，通过合成语义信息增益奖励来激励有效的信息搜寻。理论上，我们将信息增益重新定义为模型信念状态的不确定性减少，并确立了非负性、望远镜可加性和通道单调性等保证性质。实际上，为了在无需人工检索标注的情况下实现可扩展优化，我们提出了一种输出感知的内在估计器，该估计器利用双向文本蕴涵进行语义聚类，直接从模型的输出分布中计算信息增益。这种内在奖励引导策略最大化认知进步，从而能够通过 Group Relative Policy Optimxization (GRPO) 实现高效训练。在七个问答基准上的实验表明，InfoReasoner 始终优于强大的检索增强基线，实现了高达 5.4% 的平均准确率提升。我们的工作为基于检索的 Agentic reasoning (智能体推理) 提供了一条具有理论依据且可扩展的路径。",
                    "inspiration_trace": "基于对论文《Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward》的深度分析，以下是对作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观背景与问题引入\n\n#### 1. 现象观察\n作者首先观察到大推理模型的发展趋势：模型不再仅仅依赖预训练的参数化知识，而是通过**智能体推理**范式，在推理过程中动态地获取外部知识。这种结合了内部推理链与外部检索的能力，显著提升了模型解决复杂问题的潜力。\n\n#### 2. 核心冲突\n尽管“能检索”的能力已经具备，但“如何优化检索”却成为了一个根本性的挑战。\n*   **具体痛点**：在智能体进行多步推理的过程中，每一个检索动作的价值如何衡量？如何给这些中间步骤分配奖励信号，以引导智能体更有效地收集信息？\n\n#### 3. 现有方案的局限性\n作者通过批判性分析，指出了现有三条主流路径的缺陷，从而引出了真正的缺口：\n*   **路径一（监督微调 SFT）**：依赖人工标注的演示数据。\n    *   *缺陷*：扩展性差，无法捕捉开放域推理中检索动作的细微价值。\n*   **路径二（基于结果的强化学习）**：仅使用最终答案的正确性作为奖励。\n    *   *缺陷*：信号稀疏、反馈延迟；无法区分那些虽然最终都失败了，但在中间步骤上价值不同的检索动作。\n*   **路径三（基于过程监督的启发式方法）**：尝试引入中间推理信号。\n    *   *缺陷*：缺乏形式化的概率框架，无法从数学上保证局部检索决策能真正减少全局的认知不确定性。\n\n#### 4. 显式研究问题\n基于上述背景与冲突，作者提出了本论文试图解决的核心问题：\n\n> **“如何衡量并奖励每个检索动作的价值，以引导智能体进行更有效的信息收集和推理？”**\n\n---\n\n### 二、 思想演进与逻辑推演\n\n#### 第一阶段：理论重构——从“检索”到“不确定性减少”\n*   **直觉洞察**：作者跳出“检索即找文档”的机械视角，转而从认知角度思考。一个有效的检索动作，本质上应该让智能体对正确答案的**信念**更加集中。\n*   **概念映射**：这种“信念的集中”在数学上对应着“不确定性的降低”。这直接连接到了信息论中的**信息增益**概念。\n*   **理论假设**：如果我们将检索过程建模为部分可观测马尔可夫决策过程（POMDP），那么最优的检索策略应当是最大化每一步的“信息增益”，即最大化 $IG_t = U(b_t) - U(b_{t+1})$（不确定性函数的减少量）。\n\n#### 第二阶段：现实鸿沟——从“理想理论”到“落地困境”\n*   **遭遇障碍**：理论虽然完美，但在实际操作中存在致命缺陷。传统的信息增益计算需要知道“真实的信念状态”或“神谕”，即需要知道所有可能答案的真实概率分布。在开放域问答中，这显然是不可得的。\n*   **核心矛盾**：我们需要一个基于信息论的奖励信号，但我们没有计算它所需的真值标签。\n\n#### 第三阶段：方法论突破——从“外部真值”到“内在语义”\n*   **思维转折**：既然无法获得“上帝视角”的真实分布，能否利用**模型自身的输出分布**来近似？\n*   **关键创新**：\n    1.  **语义聚类**：模型输出的“Einstein”和“He is Albert Einstein”在文本上不同，但在语义上相同。作者提出利用双向文本蕴含将模型生成的多个样本聚类为“语义等价类”。\n    2.  **信念状态近似**：模型在这些语义类上的概率分布，就是对其“信念状态”的最佳估计。\n    3.  **合成奖励**：通过计算检索前后，模型输出分布在这些语义类上的**语义熵**的变化，即可得到“合成语义信息增益”。这不需要任何人工标注，完全基于模型自身的输出。\n\n#### 第四阶段：系统闭环——从“奖励设计”到“策略优化”\n*   **整合优化**：有了这个内在的、稠密的奖励信号，作者将其与最终答案正确性的外在奖励结合，利用 Group Relative Policy Optimization (GRPO) 算法对智能体进行端到端训练。\n*   **逻辑自洽**：理论上证明了该奖励的非负性、可加性和单调性，保证了优化局部信息增益能够收敛到全局不确定性的最小化。\n\n---\n\n### 三、 总结：作者的思考路径图\n\n1.  **观察**：智能体需要检索，但不知道何时检索、检索什么最好。\n2.  **诊断**：现有方法要么太贵（人工标注），要么太慢（结果奖励），要么太糙（启发式）。\n3.  **假设**：好的检索 = 减少对答案的不确定性（信息增益）。\n4.  **挑战**：无法直接计算信息增益，因为缺乏真值。\n5.  **顿悟**：用模型自己输出的“语义分布”代替真值分布。\n6.  **方案**：通过NLI聚类计算语义熵，构建“合成语义信息增益”作为内在奖励，驱动RL训练。\n7.  **验证**：理论证明性质 + 实验验证效果。"
                },
                {
                    "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
                    "arxiv_id": "2602.00663",
                    "authors": "Fabian P. Krüger, Andrea Hunklinger, Adrian Wolny, Tim J. Adler, Igor Tetko, Santiago David Villalba",
                    "summary": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴**： 论文的核心贡献是提出了 **SEISMO**，这是一个 **LLM Agent**（LLM智能体）。它不仅仅是简单地将LLM应用于化学领域，而是构建了一个具有特定架构和能力的智能体框架。 2.  **具备单智能体与自我演化/反思的关键特征**： *   **单智能体能力**：论文描述该智能体具备 **Trajectory-Aware**（轨迹感知）能力，利用完整的优化轨迹作为上下文，这属于智能体的 **Memory**（记忆）机制。 *   **自我反思与迭代**：SEISMO 在每次预言机调用后进行更新，结合了结构化的解释性反馈。这种基于环境反馈进行在线更新和迭代优化的过程，完全符合筛选标准中提到的 **Self-Correction**（自我修正）、**Self-Reflection**（自我反思）以及 **Iterative Improvement**（迭代改进）的演化机制。 3.  **符合“自我演化的应用”例外规则**： 根据筛选标准第四步，虽然论文的应用领域是分子优化（化学/药物发现），属于特定领域应用，但其核心在于提出了一种新的“自我演化/迭代”机制（即基于轨迹和反馈的在线优化）。因此，根据“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”的规则，该论文符合要求。 综上所述，该论文聚焦于构建具有记忆和自我迭代能力的LLM智能体框架，符合“单智能体”和“自我演化”的研究方向。",
                    "summary2": "本文旨在解决分子优化中因评估成本高昂导致的样本效率低下问题。针对需要昂贵Oracle评估的分子设计场景，我们提出了一种名为SEISMO的轨迹感知LLM智能体，该方法利用LLM在推理阶段进行严格的在线优化，并结合全优化轨迹（包括任务描述、标量分数及结构化解释）来指导分子生成。在Practical Molecular Optimization (PMO) benchmark及药物化学任务上，通过Area Under the optimisation Curve (AUC)等指标验证，SEISMO实现了比现有方法高2-3倍的样本效率，通常在50次调用内达到接近最优分数。",
                    "summary_translation": "优化分子结构以实现期望的性质是化学科学领域的一个核心瓶颈，特别是在制药工业中，这一过程构成了新药发现的基础。由于分子性质评估通常依赖于昂贵且速率受限的 oracles (预言机，如实验测定)，因此分子优化必须具有高度的 sample efficiency (样本效率)。为解决这一问题，我们提出了 SEISMO，这是一个 LLM agent (大语言模型智能体)，能够执行严格的 online (在线) 和 inference-time (推理时) 分子优化，它在每次 oracle call (预言机调用) 后进行更新，而无需进行 population-based (基于种群) 或 batched (批量) 学习。SEISMO 基于完整的 optimization trajectory (优化轨迹) 来生成每个 proposal (候选分子)，结合了 natural-language (自然语言) 任务描述、scalar scores (标量分数) 以及（在可用的情况下）structured explanatory feedback (结构化解释性反馈)。在包含 23 项任务的 Practical Molecular Optimization (实用分子优化) 基准测试中，SEISMO 实现的 area under the optimisation curve (优化曲线下面积) 是先前方法的 2-3 倍，通常能在 50 次 oracle calls (预言机调用) 内达到接近最大的任务分数。我们额外的 medicinal-chemistry (药物化学) 任务表明，提供 explanatory feedback (解释性反馈) 可以进一步提高效率，这证明了利用 domain knowledge (领域知识) 和 structured information (结构化信息) 是实现 sample-efficient (样本高效) 分子优化的关键。",
                    "inspiration_trace": "基于对论文《SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 核心思想演进逻辑链\n\n作者的思考过程遵循了从“宏观困境”到“微观破局”的路径，具体推演如下：\n\n**1. 宏观观察：化学空间的无限性与评估的有限性**\n*   **起点**：药物发现的核心是分子优化，即在巨大的化学空间（约 $10^{60}$ 个分子）中寻找具有特定性质（如药效、选择性）的分子。\n*   **现实约束**：由于空间太大，无法穷举搜索，只能进行迭代优化（提出候选 -> 评估 -> 修正）。\n*   **关键瓶颈**：最可靠的评估方式（实验测定或高保真物理模拟）极其昂贵且受限于速率。这意味着**“评估次数”是核心资源**，必须极度节省。\n\n**2. 现状批判：现有方法的“样本低效”与“黑盒盲区”**\n*   **观察**：现有的主流方法（如强化学习 REINVENT、遗传算法 Graph-GA、贝叶斯优化 GP BO）虽然最终性能不错，但通常需要**数千次**评估才能收敛。\n*   **归因分析**：\n    *   **机制缺陷**：这些方法通常基于“批次”更新或需要维护一个“种群”，这意味着在获得任何学习信号之前，必须先消耗大量评估次数。\n    *   **信息浪费**：它们将优化过程视为纯粹的“黑盒搜索”，仅利用了标量分数（Score），忽略了评估过程中产生的丰富信息（如具体的性质分解、结构解释）。\n\n**3. 假设提出：利用先验知识与结构化反馈**\n*   **核心洞察**：现实世界的药物发现不是“白板”搜索，而是由数十年积累的化学和生物学先验知识（如构效关系、药化规则）引导的。\n*   **假设**：如果能有效利用这些先验知识，并不仅仅依赖标量分数，而是利用“为什么分数高/低”的解释性反馈，就能大幅提升样本效率。\n\n**4. 工具选择：LLM 作为知识载体与推理引擎**\n*   **工具匹配**：大语言模型（LLM）在海量科学文献上预训练，天然编码了上述化学和生物学的先验知识。\n*   **能力对齐**：LLM 擅长处理自然语言和结构化信息，这使得将“任务描述”、“分数分解”和“解释性反馈”作为输入信号成为可能。\n\n**5. 方法构建：从“训练模型”转向“推理时优化”**\n*   **范式转移**：不再训练一个专门的生成模型或优化器，而是直接将 LLM 本身作为优化器。\n*   **机制设计**：\n    *   **严格在线**：每调用一次 Oracle，立即更新上下文，无需等待批次完成。\n    *   **轨迹感知**：让 LLM 基于完整的历史轨迹（之前所有的分子、分数、反馈）来生成下一个分子，使其具备跨迭代的因果推理能力。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **确立目标与背景**：\n    *   分子优化是化学科学的核心，特别是在制药行业的新药发现中。\n    *   目标是寻找满足多目标（药效、选择性、药代动力学等）的分子。\n\n2.  **引入核心冲突**：\n    *   **搜索空间 vs. 评估成本**：化学空间巨大（$10^{60}$），无法穷举。\n    *   **理想 vs. 现实**：实验测定最可靠，但成本高、通量低；计算模拟（如物理模拟）虽然准确，但计算昂贵且受限。\n\n3.  **指出当前痛点**：\n    *   为了降低成本，现有流程依赖低成本的近似计算。\n    *   **关键转折**：现有的强化学习和遗传算法方法虽然能取得好结果，但**样本效率低**，通常需要数千次评估。\n    *   这导致它们无法适用于那些评估昂贵或受限的高质量场景。\n\n4.  **提出解决思路**：\n    *   **假设**：样本效率低是因为现有方法将优化视为纯黑盒搜索，忽略了可用的化学/生物先验知识。\n    *   **方案**：引入一个目标导向的 LLM Agent（SEISMO），利用 LLM 中编码的先验知识，在推理时通过迭代交互进行优化。\n\n---\n\n### 三、 研究问题\n\n基于上述逻辑推演，本文试图回答的核心研究问题为：\n\n**“我们能否通过利用大语言模型中编码的领域先验知识，并结合结构化的解释性反馈，构建一个轨迹感知的推理时 Agent，从而在分子优化任务中显著超越现有方法的样本效率？”**"
                },
                {
                    "title": "DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder",
                    "arxiv_id": "2602.00592",
                    "authors": "Jiaran Zhang, Luck Ma, Yanhao Li, Fanqi Wan, Di Qi, Xu Zhao, Jieyi Hou, Zhe Xie, Mengqiang Ren, Xin Wu, Zhewei Huang, Liangyu Chen, Yingwei Ma, Qi Han, Xiangyu Zhang",
                    "summary": "Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“单智能体”方向的核心研究。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是构建了一个名为 **DockSmith** 的“专门的智能体Docker构建器”。这不仅仅是将LLM作为工具应用，而是提出了一种新的智能体框架，旨在解决环境构建这一特定问题。论文明确指出，DockSmith将环境构建视为一种“核心的智能体能力”，这符合“构建、改进LLM智能体”的目标。 2.  **高度匹配正面指标 (第二步)**: *   **Agentic AI / LLM-based Agents**: 论文明确将其定义为“agentic Docker builder”。 *   **Tool Use / Tool Augmentation**: 论文强调该智能体具备“长期工具使用”的能力。 *   **Self-Correction / Self-Reflection**: 摘要中提到的“failure recovery”（故障恢复）以及训练数据生成中使用的“loop-detection controller”（循环检测控制器），体现了智能体的自我纠错和反思机制。 *   **Memory**: 论文提到了“cross-task success memory”（跨任务成功记忆），这是智能体记忆机制的关键组成部分。 3.  **未触犯排除标准 (第三步)**: *   论文虽然涉及Docker（通常属于基础设施），但研究重点在于**智能体本身**如何操作Docker，而非Docker软件的底层优化。 *   不涉及安全对齐、多模态视觉或图技术等排除领域。 4.  **符合特殊规则 (第四步)**: *   论文涉及“依赖推理”和“长期工具使用”，这属于智能体在复杂任务中的规划和推理，而非单纯的LLM基础Token预测能力提升。 综上所述，DockSmith通过引入工具使用、故障恢复和记忆机制，构建了一个具备特定能力的LLM智能体，属于Agentic AI范畴中关于智能体能力构建与改进的高质量论文。",
                    "summary2": "本文旨在解决软件工程智能体训练中Docker环境构建不可靠的瓶颈问题。针对大规模GitHub仓库，我们提出了一种名为DockSmith的专用智能体Docker构建器，利用增强循环检测和跨任务记忆的多智能体管道生成执行轨迹进行训练。我们在Multi-Docker-Eval、SWE-bench Verified及Terminal-Bench 2.0上通过Fail-to-Pass (F2P)和Commit Rate等指标验证了其有效性，实现了开源SOTA性能并提升了通用智能体能力。",
                    "summary_translation": "可靠的 Docker-based environment construction（基于 Docker 的环境构建）是扩展 software engineering agents（软件工程智能体）的 execution-grounded training and evaluation（基于执行的训练与评估）的主要瓶颈。我们介绍了 DockSmith，这是一个专门的 agentic Docker builder（智能体 Docker 构建器），旨在应对这一挑战。DockSmith 不仅将环境构建视为一个预处理步骤，而且将其视为一种 core agentic capability（核心智能体能力），该能力运用 long-horizon tool use（长视界工具使用）、dependency reasoning（依赖推理）和 failure recovery（故障恢复），从而产生能够迁移至 Docker 构建之外的 supervision（监督信号）。DockSmith 在大规模的、execution-grounded（基于执行的）Docker-building trajectories（Docker 构建轨迹）上进行训练，这些轨迹由 SWE-Factory-style pipeline（SWE-Factory 风格流水线）生成，并辅以 loop-detection controller（循环检测控制器）和 cross-task success memory（跨任务成功记忆）进行增强。在这些轨迹上训练 30B-A3B 模型，在 Multi-Docker-Eval 上实现了 open-source state-of-the-art performance（开源最先进性能），其 Fail-to-Pass 为 39.72%，Commit Rate 为 58.28%。此外，DockSmith 还提升了在 SWE-bench Verified、SWE-bench Multilingual 和 Terminal-Bench 2.0 上的 out-of-distribution performance（分布外性能），展示了环境构建所带来的更广泛的 agentic benefits（智能体优势）。",
                    "inspiration_trace": "基于对论文《DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的方式，构建了一个从宏观趋势到具体痛点的故事线：\n\n1.  **宏观趋势**：\n    当前软件工程代理的进步越来越依赖于**基于执行监督的规模化**。这意味着我们需要通过自动化管道挖掘历史代码变更，并执行测试来获取可执行的反馈信号。\n\n2.  **核心瓶颈**：\n    这种自动化管道的一个核心前提是**可靠的环境构建**，即将静态代码仓库转化为可执行的 Docker 环境。然而，在实践中，基于 Docker 的环境设置**极易失败**。\n\n3.  **具体困境**：\n    异构的依赖关系、系统级冲突以及未记录的构建假设经常导致环境构建失败。这种困难甚至对于 Claude-4-Sonnet 和 Gemini-2.5-Flash 等强模型依然存在。这导致自动化环境构建的脆弱性严重限制了可用轨迹的产出，使得只有极小部分的仓库适合用于代理训练。\n\n4.  **后果与现状**：\n    环境构建成为了阻碍数据收集和模型训练的“阿喀琉斯之踵”，导致大量仓库无法进入执行和验证阶段。\n\n---\n\n### 二、 研究问题\n\n基于上述故事逻辑，作者显式提出的研究问题是：\n\n**“如何解决不可靠的环境构建这一瓶颈以实现基于执行的规模化训练，并且掌握环境构建这一特定任务是否能作为一种可迁移的智能体能力，从而提升通用的软件工程性能？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者从观察到最终提出 DockSmith 的思考过程经历了以下四个阶段：\n\n#### 1. 观察与痛点识别\n*   **现象**：虽然大模型在代码生成上表现优异，但在“让代码跑起来”这一步（环境配置）上经常碰壁。\n*   **分析**：现有的强模型在 Multi-Docker-Eval 等基准上表现不佳，说明这不仅仅是模型规模不够的问题，而是模型缺乏处理复杂系统依赖、工具调用和从失败中恢复的特定能力。\n*   **结论**：环境构建是当前通往“执行 grounded”智能体道路上的最大拦路虎。\n\n#### 2. 视角的重构\n*   **传统视角**：环境构建通常被视为一个**预处理步骤**，是枯燥的、前置的、非核心的“脏活累活”。\n*   **作者的新视角**：环境构建本质上是一个**核心的智能体能力**。\n*   **理由**：构建环境需要长周期的推理、工具的使用、依赖关系的逻辑推断以及失败后的自我修复。这与解决 Bug 或实现功能的认知过程是同构的。\n*   **假设**：如果模型能学会构建环境，那么它学到的不仅仅是写 Dockerfile，而是一种通用的“执行 grounded”的推理和恢复能力，这种能力应该能迁移到其他软件工程任务上。\n\n#### 3. 方法论假设\n*   **目标**：构建一个专门的 Docker 构建代理。\n*   **数据策略**：既然现有模型不行，我们需要生成高质量的“环境构建轨迹”。这不能靠人工，必须靠自动化的多智能体管道。\n*   **改进点**：基于现有的 SWE-Factory 框架，但必须解决两个问题：\n    *   **死循环**：多智能体修复时容易陷入重复无效的尝试，需要引入“循环检测控制器”。\n    *   **经验复用**：一个仓库的成功经验应该能被其他仓库借鉴，需要引入“跨任务成功记忆”。\n*   **训练策略**：不仅要训练模型构建 Docker，还要将这种轨迹与通用的 SWE（软件工程）轨迹混合训练，以防止模型过拟合于 Docker 细节，同时保留通用能力。\n\n#### 4. 验证与闭环\n*   **验证逻辑**：\n    1.  **主任务验证**：DockSmith 是否在 Docker 构建任务上达到了 SOTA？（验证解决了瓶颈）\n    2.  **迁移验证**：加入 Docker 构建轨迹训练后，模型在 SWE-bench 和 Terminal-Bench 等非 Docker 任务上是否表现更好？（验证了“核心能力”的假设）\n*   **预期结果**：如果假设成立，我们将得到一个既能高效构建环境，又具备更强通用执行和恢复能力的智能体。\n\n---\n\n### 总结\n\n作者的思考路径是从**“数据收集的工程瓶颈”**出发，通过**“认知视角的升维”**（将环境构建视为核心智能体任务），提出了**“基于多智能体轨迹的专门化训练”**方案，最终旨在证明**“环境构建能力是通向高级软件工程智能体的关键基石”**。"
                },
                {
                    "title": "Exploring Information Seeking Agent Consolidation",
                    "arxiv_id": "2602.00585",
                    "authors": "Guochen Yan, Jialong Wu, Zhengwei Tao, Bo Li, Qintong Zhang, Jiahao Xu, Haitao Mi, Yuejian Fang, Qingni Shen, Wentao Zhang, Zhonghai Wu",
                    "summary": "Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“构建、改进或演化 LLM智能体”的核心类别。 1.  **核心贡献符合第一步标准**： *   论文的核心目标是研究如何将异构的“信息检索智能体”整合为一个单一的“基础智能体模型”。 *   这属于**构建和改进 LLM智能体**的方法论研究。它不是将智能体作为工具去解决生物或金融等外部领域的问题（非应用型），而是专注于智能体系统本身的架构优化和性能提升（如何通过数据级或参数级的整合来提升智能体的泛化能力）。 2.  **符合第二步的正面指标**： *   论文明确涉及 `LLM-based Agents` 和 `Agentic AI` 范式。 *   虽然重点在于“整合”，但其研究对象是具备特定能力的智能体，旨在解决智能体在跨领域泛化时的性能保留和干扰问题，这是提升智能体基础能力的关键研究。 3.  **不触犯第三步排除标准**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除领域。 4.  **综合判断**： 该论文提出了一种新的智能体构建与改进框架（Agent Consolidation），旨在解决单一智能体在多领域适应性和泛化性上的局限。这直接对应了研究目标中关于“改进 LLM智能体”的需求，因此应当保留。",
                    "summary2": "本文旨在解决信息检索智能体跨域泛化受限的问题，构建单一基础智能体模型。针对Web搜索、文档理解和知识库检索三种异构环境，我们系统研究了数据级整合与参数级合并两种策略。我们在GAIA、BrowseComp、HotPotQA等多个基准上，通过Accuracy、EM和F1指标验证了其有效性。实验表明数据级整合是稳健基线，而参数级合并需精细设计以减少干扰。",
                    "summary_translation": "Information-seeking agents (信息搜索代理) 已成为解决 knowledge-intensive tasks (知识密集型任务) 的强大范式。现有的 Information-seeking agents (信息搜索代理) 通常专门针对 open web (开放网络)、documents (文档) 或 local knowledge bases (本地知识库)，这限制了 scalability (可扩展性) 和 cross-domain generalization (跨域泛化)。在这项工作中，我们探讨了如何将 heterogeneous (异构) 的 Information-seeking agents (信息搜索代理) 整合到一个单一的 foundation agentic model (基础代理模型) 中。我们研究了两种 complementary (互补) 的 consolidation strategies (整合策略)：data-level consolidation (数据级整合)，即在 mixture of domain-specific datasets (特定领域数据集混合) 上联合训练一个统一模型；以及 parameter-level consolidation (参数级整合)，即在 parameter level (参数层面) 合并独立训练的 agent models (代理模型)。我们的分析从 performance retention (性能保持)、cross-domain generalization (跨域泛化) 以及 information-seeking behaviors (信息搜索行为) 之间的 interference (干扰) 等方面对这些方法进行了比较。结果表明，data-level consolidation (数据级整合) 仍然是一个强大且稳定的 baseline (基线)，而 parameter-level consolidation (参数级整合) 提供了一种有前景且高效的替代方案，但面临 interference (干扰) 和 robustness (鲁棒性) 方面的挑战。我们进一步确定了在 parameter level (参数层面) 进行有效 agent consolidation (代理整合) 的关键设计因素，包括 fine-grained merging granularity (细粒度合并粒度)、对 task heterogeneity (任务异质性) 的感知以及 principled consensus strategy (原则性共识策略)。",
                    "inspiration_trace": "基于对论文《Exploring Information Seeking Agent Consolidation》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观观察与问题引入\n\n**1. 现象观察：智能体的“碎片化”现状**\n作者首先观察到，信息检索智能体已成为解决知识密集型任务的主流范式。然而，现有的智能体呈现出高度的专业化分工：有的擅长开放网络搜索，有的专注于文档理解，有的则基于本地知识库检索。这些智能体各自为政，形成了“孤岛”效应。\n\n**2. 核心痛点：缺乏通用性与扩展性**\n这种专业化分工导致了严重的局限性：单一智能体无法跨域工作，限制了系统的可扩展性和跨域泛化能力。在现实应用中，用户期望的是一个能够同时处理Web、文档和本地知识库的“全能型”智能体。\n\n**3. 理想目标：构建统一的基础智能体模型**\n基于上述痛点，作者提出了一个自然且日益重要的目标：将这三个异构的信息检索智能体整合到一个单一的基础智能体模型中，使其能够跨越不同的信息环境进行操作。\n\n### 二、 问题深化与挑战分析\n\n**4. 难点剖析：为什么“合并智能体”比“合并模型”更难？**\n作者敏锐地指出，统一智能体远比统一标准的分类或生成模型困难。这不仅仅是知识的聚合，更涉及以下三个核心冲突：\n*   **异构环境交互：** Web是动态的，文档是静态多模态的，KB是结构化的，交互方式截然不同。\n*   **推理轨迹分歧：** 不同任务下的思考路径和决策逻辑存在巨大差异。\n*   **长视距规划机制：** 智能体需要在多步过程中保持策略的一致性，而不同领域的最优动作可能完全不同。\n\n**5. 现有路径的审视：数据级 vs. 参数级**\n面对统一挑战，现有研究主要沿两条路径进行，但各有缺陷：\n*   **数据级整合：** 概念简单（混合数据联合训练），但成本高昂，且在隐私敏感或分布式设置下受限。\n*   **参数级整合：** 直接合并模型权重，高效且无需重训练。但现有研究处于初步阶段，缺乏系统性的方法论分析，对于如何处理智能体特有的“行为干扰”缺乏原则性理解。\n\n### 三、 逻辑聚焦与研究问题\n\n**6. 逻辑缺口：缺乏原则性的理解**\n作者发现，目前对于“如何在参数层面有效整合信息检索智能体”这一问题，学术界尚无定论。现有的模型合并技术是否适用于需要复杂工具调用和推理的智能体？这是一个未知的领域。\n\n**7. 提炼研究问题**\n基于上述逻辑演进，作者最终锁定了核心研究问题：\n\n> **How can information-seeking agent models be effectively consolidated?**\n> （如何有效地整合信息检索智能体模型？）\n\n### 四、 方法论的演进与形成\n\n**8. 假设提出：两条路径的博弈**\n为了回答上述问题，作者提出假设：必须对“数据级整合”与“参数级整合”进行系统性的对比。不仅要看性能，还要看鲁棒性、跨域泛化能力以及对信息检索行为（如工具调用频率、推理步骤）的影响。\n\n**9. 实验设计：从宏观对比到微观分析**\n*   **宏观层面：** 构建统一的智能体范式，涵盖Web、Doc、RAG三种环境，并在多个基准测试上对比20种不同的参数合并方法与数据混合方法。\n*   **微观层面：** 深入探究参数合并失败的原因（如任务向量冲突、子空间对齐问题），并分析LoRA等高效微调方法对合并的影响。\n\n**10. 洞发现与原则构建：从“能不能”到“怎么做”**\n通过实证研究，作者发现数据级整合仍是强力基线，而参数级整合虽然高效但面临干扰挑战。基于此，作者进一步提炼出有效参数整合的关键设计原则（如细粒度合并、任务异质性感知、基于激活空间对齐的共识策略），从而完成了从“探索现象”到“指导实践”的方法论闭环。"
                },
                {
                    "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
                    "arxiv_id": "2602.00528",
                    "authors": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan, Zhenwei Dai, Jingying Zeng, Zhiwei Zhang, Fali Wang, Hongcheng Gao, Chen Luo, Xiang Zhang, Qi He, Suhang Wang",
                    "summary": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a \"knowing-doing\" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献并非仅仅将LLM应用于扑克这一特定领域，而是提出了 **ToolPoker**，这是一个**新的工具集成推理框架**。该框架旨在解决LLM在博弈论推理中的“知行差距”问题，通过结合外部求解器来增强智能体的行动能力。这属于构建和改进LLM智能体的方法论范畴，而非单纯的应用型研究。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文标题和摘要中明确提到了 **\"Agentic Tool Use\"**（智能体工具使用），这直接对应了研究焦点中的“单智能体”方向。 *   **智能体能力**：论文重点研究了 **Tool Use / Tool Augmentation**（工具使用/增强）以及 **Reasoning**（推理，特别是博弈论推理）。它展示了智能体如何利用外部工具（求解器）来弥补自身在复杂决策中的不足，这是Agentic AI的关键能力之一。 3.  **排除标准（未触犯）**： *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 *   虽然扑克是一个具体场景，但论文的落脚点在于**框架的创新**（如何让智能体更好地使用工具进行推理），而非扑克游戏本身的数据集或特定规则。 4.  **特殊情况处理**： *   **推理/规划**：论文属于“保留”的情况。它关注的是智能体在复杂任务（扑克）中的多步推理和决策，并且通过引入工具使用机制来改进这一过程，这与单纯提升LLM基础Token预测能力的非Agentic研究有本质区别。 综上所述，该论文在“单智能体”的“工具使用”和“复杂推理”方面做出了实质性贡献，符合“构建、改进LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM在不完全信息博弈中难以进行博弈论推理的问题。针对扑克游戏场景，我们提出了一种ToolPoker工具集成推理框架，利用外部求解器保证GTO一致性动作并生成专业解释。我们在Leduc Hold'em和Limit Texas Hold'em上通过游戏表现和推理质量指标验证了其有效性，实现了SOTA的游戏水平。",
                    "summary_translation": "随着Large Language Models (LLMs, 大型语言模型)越来越多地应用于高风险领域，它们在不确定性下进行战略推理的能力变得至关重要。扑克提供了一个严格的测试平台，不仅需要强有力的行动，还需要有原则的、game-theoretic reasoning (博弈论推理)。在本文中，我们在多个现实扑克任务中对LLMs进行了系统性研究，评估了对局结果和reasoning traces (推理轨迹)。我们的分析表明，LLMs无法与传统算法竞争，并指出了三个反复出现的缺陷：依赖heuristics (启发式方法)、事实性误解，以及行动与推理不一致的“knowing-doing gap (知行差距)”。使用behavior cloning (行为克隆)和step-level reinforcement learning (步骤级强化学习)的初步尝试改善了推理风格，但对于准确的博弈论对局仍然不足。受这些局限性的启发，我们提出了ToolPoker，这是一个tool-integrated reasoning framework (工具集成推理框架)，它结合了用于生成符合GTO (博弈论最优)行动的external solvers (外部求解器)和更精确的专业风格解释。实验表明，ToolPoker实现了最先进的游戏水平，同时产生的推理轨迹紧密反映了博弈论原则。",
                    "inspiration_trace": "基于对论文《How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 1. 宏观背景与切入点\n\n作者的研究始于对大语言模型（LLMs）应用边界的思考。随着 LLMs 被应用于网络安全、战略决策等**高风险领域**，单纯的知识问答已不足以应对需求，核心挑战转向了**不确定性下的战略推理能力**。\n\n为了量化这种能力，作者需要一个严格的测试环境。**扑克**被选为理想的测试床，因为它不仅涉及隐藏信息和不完全信息博弈，更重要的是，职业扑克选手的胜利不仅仅靠“直觉”，而是依赖于**博弈论推理**，即基于纳什均衡（GTO）的严谨决策。因此，评估 LLMs 的标准不能仅仅是“赢没赢”，而是“是否像职业选手一样思考”。\n\n---\n\n### 2. Introduction 中的“讲故事”逻辑\n\n在 Introduction 部分，作者构建了如下的叙事逻辑，旨在引出研究的必要性：\n\n1.  **现状与需求**：LLMs 正进入高风险决策领域，这要求它们具备在不确定性下进行战略推理的能力。\n2.  **测试基准的选择**：扑克是典型的非完全信息博弈，职业选手的成功在于他们基于博弈论原理进行推理，而不仅仅是采取强有力的行动。\n3.  **现有研究的盲区**：此前的研究（如 GTBench, PokerBench 等）大多只关注**游戏结果**，即 LLMs 能否赢牌，却忽略了**推理过程**。这导致我们不知道 LLMs 为什么赢或输。\n4.  **核心矛盾**：要像职业选手一样，不仅要行动最优，还要思维战略化。然而，目前缺乏对 LLMs 是否具备这种“博弈论思维”的系统性分析。\n5.  **本文的切入点**：因此，我们需要超越胜率，深入分析 LLMs 的推理轨迹，探究它们是否真正掌握了博弈论原理，还是仅仅依赖浅层的启发式规则。\n\n### 3. 核心研究问题\n\n基于上述逻辑，作者显式提出了贯穿全文的研究问题：\n\n**“LLMs 距离职业扑克选手还有多远？具体而言，它们在非完全信息博弈中是否具备基于博弈论原理的战略推理能力？”**\n\n---\n\n### 4. 思想演进与逻辑推演\n\n为了回答上述问题，作者的思考过程经历了四个阶段的演进：\n\n#### 第一阶段：现象观察与诊断\n*   **观察**：作者首先将 LLMs 与传统算法（如 CFR+, NFSP）进行对比，发现 LLMs 表现不佳。\n*   **深度分析**：通过分析推理轨迹，作者并没有止步于“模型太弱”，而是精准定位了三个核心缺陷：\n    1.  **启发式推理**：依赖表面模式而非严谨的博弈论。\n    2.  **事实性误解**：对牌力、赔率等基础事实判断错误。\n    3.  **知行鸿沟**：推理过程与最终行动不一致。\n*   **结论**：LLMs 的失败不是参数不够多，而是缺乏内在的博弈论计算能力和严谨性。\n\n#### 第二阶段：内部修正的尝试与局限\n*   **假设**：既然是内在推理能力不足，能否通过训练让 LLMs “学会”这种推理？\n*   **尝试**：作者提出了 **BC-RIRL** 框架（行为克隆 + 后悔激励强化学习）。试图通过模仿专家轨迹和基于后悔值的奖励信号，让模型内化博弈论策略。\n*   **发现**：虽然模型学会了“像专家一样说话”（推理风格改善），但在**精确推导**（如计算胜率、范围）和**GTO 一致性**上依然失败。\n*   **反思**：这揭示了一个根本性局限——LLMs 的内部参数难以精确模拟复杂的博弈论计算器。单纯靠“练内功”无法解决事实性错误和计算精度问题。\n\n#### 第三阶段：范式转移——从“内化”到“外挂”\n*   **转折点**：既然 LLMs 不擅长精确计算，但擅长工具调用和逻辑组织，为什么不把计算任务外包给专业的求解器？\n*   **新思路**：利用 LLMs 的 **Agent Tool Use（智能体工具使用）** 能力。让 LLMs 负责理解和解释，让外部扑克求解器（如 CFR Solver）负责计算 GTO 策略。\n*   **面临的挑战**：\n    1.  **多工具依赖**：博弈论推理需要调用多个求解器（动作求解器、胜率计算器等），容易导致误差传播。\n    2.  **数据成本高**：构建带有工具调用轨迹的高质量专家数据极其昂贵。\n\n#### 第四阶段：方案落地——ToolPoker 的设计\n*   **针对挑战的解决方案**：\n    1.  **统一接口**：为了解决多工具调用的混乱，作者设计了一个统一的 API，将所有求解器功能整合，一次调用返回所有必要数据（动作、胜率、范围），简化了 LLM 的操作难度。\n    2.  **低成本数据构建**：利用程序化手段，在少量专家数据上自动注入工具调用模板和输出，大幅降低了训练成本。\n*   **最终框架**：提出了 **ToolPoker**。它结合了外部求解器的计算保证（GTO 动作）和 LLMs 的语言能力（专业解释）。\n*   **结果验证**：实验证明，ToolPoker 不仅达到了 SOTA 的游戏水平，其生成的推理轨迹也高度符合博弈论原则，成功弥合了“知”与“行”的鸿沟。\n\n---\n\n**总结**：作者的思考路径是从**评估现状**（发现推理缺陷）到**尝试内化修复**（发现模型极限），最终通过**引入外部工具**（发挥模型长板）实现了突破。这一过程体现了从“试图改变模型本质”到“利用模型特性构建系统”的方法论升华。"
                },
                {
                    "title": "Dual Latent Memory for Visual Multi-agent System",
                    "arxiv_id": "2602.00471",
                    "authors": "Xinlei Yu, Chengming Xu, Zhangquan Chen, Bo Yin, Cheng Yang, Yongbo He, Yihao Hu, Jiangning Zhang, Cheng Tan, Xiaobin Hu, Shuicheng Yan",
                    "summary": "While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive \"scaling wall\": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the \"scaling wall\" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： *   论文的核心贡献是提出了 **L$^{2}$-VMAS**，这是一个新颖的、模型无关的**多智能体系统框架**。 *   它旨在解决多智能体协作中的“扩展墙”问题，即如何通过改进智能体间的通信和记忆机制来提升性能。这完全属于“构建、改进 LLM智能体”的范畴，特别是针对多智能体架构的优化。 2.  **研究焦点（符合）**： *   论文明确属于 **多智能体** 方向。摘要中提到的“inter-agent collaboration”（智能体间协作）、“text-centric communication”（文本中心通信）以及“memory access”（记忆访问）都是多智能体系统的核心议题。 3.  **排除标准（通过）**： *   **关于多模态与视觉**：虽然标题和摘要中提到了“Visual Multi-agent System”，但根据筛选标准中的例外条款——“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这篇论文中，视觉信息仅仅是智能体感知环境的一种输入形式，论文的核心创新点在于**双潜在记忆机制**和**熵驱动的主动触发机制**，旨在解决通信瓶颈和语义损失，而非改进视觉模型本身。因此，它不应被作为视觉类论文排除。 *   **非演化型应用**：这不是一个简单的应用型论文（如“用智能体做医疗诊断”），而是提出了一个通用的方法论框架来改进智能体系统的底层通信效率。 4.  **正面指标（匹配）**： *   包含核心范式：`Multi-Agent Systems (MAS)`。 *   包含智能体能力：`Memory`（双潜在记忆）、`Communication`（解决文本通信瓶颈）、`Collaboration`。 综上所述，该论文通过改进多智能体的记忆和通信机制来提升系统效能，属于多智能体系统架构层面的创新，符合“LLM智能体及其演化”中关于多智能体协作的研究目标。",
                    "summary2": "本文旨在解决Visual Multi-Agent Systems (VMAS)中因文本通信瓶颈导致的性能下降与计算成本激增问题。针对多智能体视觉协作场景，我们提出了一种名为L2-VMAS的模型无关框架，采用解耦感知与思维的双潜在记忆及基于熵的主动触发机制。在MMBench、MMStar等基准数据集上，通过准确率和Token消耗量验证了其有效性，实现了平均准确率提升2.7-5.4%及Token使用量减少21.3-44.8%。",
                    "summary_translation": "尽管 Visual Multi-Agent Systems (VMAS，视觉多智能体系统) 旨在通过 inter-agent collaboration (智能体间协作) 提升综合能力，但实证证据揭示了一个反直觉的 \"scaling wall\" (扩展墙)：增加 agent turns (智能体轮次) 往往会导致性能下降，同时使 token costs (token 成本) 呈指数级增长。我们将这一现象归因于 text-centric communication (以文本为中心的通信) 中固有的 information bottleneck (信息瓶颈)，即将 perceptual and thinking trajectories (感知和思维轨迹) 转换为 discrete natural language (离散自然语言) 的过程不可避免地会导致 semantic loss (语义损失)。为此，我们提出了 L$^{2}$-VMAS，这是一种新颖的 model-agnostic (模型无关的) 框架，它利用 dual latent memories (双重潜在记忆) 来实现 inter-agent collaboration (智能体间协作)。此外，我们对 perception and thinking (感知与思维) 进行解耦，并动态合成 dual latent memories (双重潜在记忆)。同时，我们引入了一种 entropy-driven (熵驱动) 的 proactive triggering (主动触发) 机制，以高效的 on-demand memory access (按需记忆访问) 取代了 passive information transmission (被动信息传输)。在不同 backbones (骨干网络)、sizes (规模) 和 multi-agent structures (多智能体结构) 上进行的广泛实验表明，我们的方法凭借 superb scalability (卓越的可扩展性) 有效打破了 \"scaling wall\" (扩展墙)，在将 token usage (token 使用量) 减少 21.3-44.8% 的同时，将 average accuracy (平均准确率) 提高了 2.7-5.4%。代码：https://github.com/YU-deep/L2-VMAS。",
                    "inspiration_trace": "基于论文《Dual Latent Memory for Visual Multi-agent System》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从发现问题到提出解决方案的完整思考过程。\n\n---\n\n### 1. 宏观观察与预期违背\n**思考起点：** 视觉多智能体系统（VMAS）的兴起基于一个核心假设——通过增加智能体之间的协作轮次，利用集体智慧，可以获得比单一模型更强的鲁棒性和准确性。\n**现实冲突：** 作者通过实证观察发现了一个反直觉的现象：在视觉任务中，随着智能体交互轮次的增加，系统性能不仅没有持续提升，反而出现了下降；同时，计算成本（Token消耗）呈指数级爆炸增长。\n**初步定义：** 作者将这一现象定义为 VMAS 的“扩展墙”，即单纯增加协作轮次无法保证性能提升，反而导致系统崩溃。\n\n### 2. 归因分析：为什么会出现“扩展墙”？\n**深入诊断：** 作者将矛头指向了现有的**以文本为中心的通信范式**。\n*   **信息瓶颈：** 自然语言本质上是离散且有损的。将高维的视觉感知和连续的思维轨迹压缩成文本，必然导致语义细节的丢失。\n*   **感知与思维的混淆：** 现有方法将“看到了什么（感知）”和“怎么想的（思考）”混合在一起传输。这种耦合导致了信息间的相互干扰，使得下游智能体难以有效提取关键信息。\n*   **累积效应：** 随着轮次增加，这种语义损失不断累积，且大量冗余的文本 Token 占据了上下文窗口，最终导致性能下降和计算资源浪费。\n\n### 3. 核心假设与方向转变\n**逻辑推演：** 既然自然语言是导致信息瓶颈和干扰的根源，那么解决思路就不应是优化文本传输，而是**彻底改变通信的媒介**。\n*   **媒介转换：** 从离散的自然语言转向连续的、高密度的**潜在空间**。\n*   **解耦设计：** 为了避免感知与思维的相互干扰，必须在通信前将两者**解耦**，分别处理。\n*   **机制优化：** 为了解决被动接收导致的冗余，通信应从“被动传输”转变为“按需主动获取”。\n\n### 4. 方法论构建：L2-VMAS 的诞生\n基于上述假设，作者构建了 L2-VMAS 框架，其核心思想演进如下：\n1.  **建立双潜在记忆：** 创建两个独立的外部记忆库——**感知记忆**（存储多粒度视觉特征）和**思维记忆**（存储语义切分后的推理轨迹），替代文本传输。\n2.  **动态合成：** 设计机制将所有历史智能体的输出动态更新到这两个共享记忆中，确保信息的流动性和完整性。\n3.  **主动编排：** 引入**熵驱动**的触发机制。只有当当前智能体在生成过程中表现出高不确定性（高熵）时，才主动从记忆库中检索相关信息。这模拟了人类的认知过程——只在“卡壳”或需要确认时才查阅记忆。\n\n---\n\n### 附：Introduction 中的“讲故事”逻辑提取\n\n1.  **背景铺垫：** 视觉语言模型（VLM）的发展推动了从单智能体向视觉多智能体系统（VMAS）的转变，核心假设是协作能带来性能提升。\n2.  **提出冲突：** 然而，实证证据揭示了“扩展墙”：增加智能体轮次反而导致性能下降和成本指数级上升。\n3.  **解释原因：** 这归咎于以文本为中心的通信范式存在固有的信息瓶颈，且将感知和思维混合传输导致了严重的语义损失和干扰。\n4.  **引出方案：** 因此，本文提出了 L2-VMAS，一种通过双潜在记忆实现解耦感知与思维，并利用熵驱动机制实现按需访问的新框架。\n\n### 显式总结：研究问题\n\n**如何克服视觉多智能体系统中以文本为中心的通信范式所带来的信息瓶颈与感知-思维耦合问题，从而打破性能随协作轮次增加而下降的“扩展墙”？**"
                },
                {
                    "title": "Cross-Modal Memory Compression for Efficient Multi-Agent Debate",
                    "arxiv_id": "2602.00454",
                    "authors": "Jing Wu, Yue Sun, Tianpei Xie, Suiyao Chen, Jingyuan Bao, Yaopengxiao Xu, Gaoyuan Du, Inseok Heo, Alexander Gutfraind, Xin Wang",
                    "summary": "Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应被保留。判断依据如下： 1.  **核心判断符合**： *   论文的核心贡献是提出了 **DebateOCR**，这是一个专门针对 **多智能体辩论** 框架的改进方案。 *   它属于 **多智能体** 研究范畴，旨在解决多智能体系统中因交互轮次增加而导致的上下文溢出和记忆管理问题。 *   这不是简单的应用型论文，而是对智能体架构中 **记忆** 组件的底层优化和改进。 2.  **正面指标匹配**： *   **多智能体系统 (MAS)**：论文明确研究 \"Multi-agent debate\"，这是多智能体协作与推理的一种重要形式。 *   **智能体能力**：论文的核心焦点在于 **Memory**（记忆）。它提出了一种跨模态压缩机制，使智能体能够更高效地存储和检索历史交互信息，这是智能体在长周期任务中保持连贯性的关键能力。 3.  **排除标准检查**： *   **非基础设施**：虽然论文涉及压缩和计算效率，但它并非通用的模型基础设施（如硬件加速或通用推理引擎），而是专门为智能体框架设计的特定算法改进，属于智能体方法论的一部分。 *   **非视觉核心**：尽管论文使用了视觉编码器和图像表示，但这属于 **工具使用** 的范畴。视觉技术在这里被用作压缩文本记忆的“工具”，而非研究视觉理解本身，因此符合“作为智能体感知环境的工具”这一例外情况。 *   **非安全与对齐**：虽然摘要提到辩论可以减少幻觉，但论文的主要贡献在于通过压缩提高效率，而非提出新的安全或对齐算法。 综上所述，该论文通过改进多智能体系统的记忆机制，提升了智能体在复杂任务中的处理效率，完全符合“构建、改进或演化 LLM智能体”这一核心目标。",
                    "summary2": "本文旨在解决多智能体辩论中上下文快速增长导致的计算开销和上下文限制问题。针对多智能体辩论场景，我们提出了一种名为DebateOCR的跨模态压缩框架，利用视觉编码器将长文本辩论历史转换为紧凑图像表示。在GSM8K、MATH和GPQA数据集上，通过准确率、Token消耗量和推理时间验证了其有效性，实现了超过92%的Token减少并维持了竞争性准确率。",
                    "summary_translation": "多智能体辩论能够提升推理质量并减少幻觉，但随着辩论轮次和智能体数量的增加，其上下文长度会迅速增长。保留完整的文本历史记录会导致 token 使用量超出上下文限制，且往往需要反复进行摘要生成，这不仅增加了开销，还加剧了信息的丢失。我们提出了 DebateOCR，这是一种跨模态压缩框架，它用紧凑的图像表示替代冗长的文本辩论轨迹，随后通过专用的视觉编码器处理这些图像，为后续轮次提供上下文条件。该设计能够压缩通常长达数万至数十万 token 的历史记录，将输入 token 数量减少了 92% 以上，并在多个基准测试中显著降低了计算成本并加快了推理速度。我们还提供了一个理论视角，表明智能体间的多样性有助于恢复遗漏的信息：尽管任何单个压缩后的历史记录可能会丢弃细节，但通过聚合多个智能体的压缩视图，集体表示能够以指数级的高概率逼近信息瓶颈。",
                    "inspiration_trace": "基于对论文《Cross-Modal Memory Compression for Efficient Multi-Agent Debate》的深度分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个典型的“成功-冲突-困境”叙事结构，逻辑如下：\n\n1.  **确立价值基线：** 多智能体辩论（MAD）通过模拟人类协作，利用多视角的迭代修正，显著提升了大模型在推理、事实性和复杂问题解决上的表现。\n2.  **引入核心冲突：** 这种性能提升伴随着巨大的计算代价。随着智能体数量（K）和辩论轮数（R）的增加，计算开销呈**二次方增长**（Quadratic Scaling）。\n3.  **剖析具体痛点：**\n    *   **上下文爆炸：** 每个智能体在每轮都必须处理完整的文本历史，导致Token消耗迅速超出上下文窗口限制。\n    *   **现有方案失效：** 传统的“摘要”或“截断”策略不仅增加了额外的推理延迟，还会导致关键信息的累积性丢失。\n4.  **锁定研究空白：** 目前尚无一种可扩展的解决方案，能够在不产生不可接受的Token开销的前提下，维持完整的辩论上下文。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何在不牺牲多智能体辩论所需完整上下文信息的前提下，打破其计算成本随轮数和智能体数量呈二次方增长的瓶颈，实现高效且无损的上下文压缩？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从宏观观察到微观实现的思考过程可以还原为以下五个阶段：\n\n#### 1. 痛点洞察：从“文本冗余”到“模态差异”\n*   **观察：** MAD 的核心瓶颈在于“文本”这种模态本身。文本是线性的、低密度的，且Token计费昂贵。随着辩论进行，大量文本是重复的、风格化的（Artifacts），而非纯粹的信息增量。\n*   **思考：** 既然文本Token太贵且太长，我们能否换一种更“便宜”且信息密度更高的模态来承载同样的信息？\n*   **假设：** 视觉模态（图像）具有极高的空间信息密度。如果将文本历史渲染为图像，利用现有的视觉编码器将其转换为少量的视觉Token，就能大幅降低计算成本。\n\n#### 2. 方案构思：跨模态压缩\n*   **逻辑推演：** 现在的模型大多是视觉-语言模型（VLM），它们具备“看图”的能力。\n*   **核心创意：** 不让模型“读”长文本，而是让模型“看”长文本的截图。\n*   **机制设计：**\n    *   **输入端：** 将文本化的辩论历史渲染为结构化的图像（保留发言者身份、时间顺序等空间结构）。\n    *   **处理端：** 使用预训练的视觉编码器（如SAM、CLIP）提取特征，通过一个轻量级适配器将其映射到LLM的嵌入空间。\n    *   **效果：** 将原本 $O(K^2R^2)$ 的文本Token复杂度降低为 $O(KRN)$（N为固定的视觉Token数）。\n\n#### 3. 理论辩护：压缩为何不导致性能下降？\n*   **潜在质疑：** 将文本转为图片必然会丢失细节（有损压缩），为什么推理精度反而可能提升或持平？\n*   **理论构建（信息瓶颈视角）：**\n    *   **去噪效应：** 压缩过程天然过滤掉了文本中的“噪声”（如冗余的语气词、格式差异），使模型更关注核心逻辑。\n    *   **群体智慧：** 单个智能体的压缩视图可能丢失信息，但多个智能体是**多样化**的。\n*   **逻辑闭环：** 虽然个体有信息损失，但只要群体足够大且多样化，不同个体保留的互补信息在聚合阶段可以相互弥补。这种“集体冗余”抵消了“个体损失”，使得整体系统逼近信息瓶颈的最优解。\n\n#### 4. 方法落地：两阶段框架\n*   **离线训练阶段：** 既然要“看”文本生成的图，就需要训练模型理解这种图。利用大规模语料，训练一个适配器，让视觉特征能够被LLM重建为文本，确保语义对齐。\n*   **在线推理阶段：** 在实际辩论中，实时渲染历史记录为图，编码后注入上下文。这不需要修改LLM本身的架构，即插即用。\n\n#### 5. 价值升华：超越效率\n*   **最终思考：** 这不仅仅是为了省钱（省Token）。\n*   **结构优势：** 图像天然保留了辩论的“空间结构”（谁在什么时候说了什么），这种结构化信息有时比线性的长文本流更容易被模型捕捉逻辑流。\n*   **结论：** 视觉压缩不仅解决了效率问题，还意外地通过结构化呈现和去噪，提升了推理质量。\n\n---\n\n**总结：** 作者的思考路径是从**计算复杂度的数学瓶颈**出发，跨越到**跨模态的信息密度优势**，再通过**信息论中的群体多样性原理**为有损压缩提供了理论合法性，最终构建了一个高效且鲁棒的多智能体辩论框架。"
                },
                {
                    "title": "Position: Agentic Evolution is the Path to Evolving LLMs",
                    "arxiv_id": "2602.00359",
                    "authors": "Minhua Lin, Hanqing Lu, Zhan Shi, Bing He, Rui Mao, Zhiwei Zhang, Zongyu Wu, Xianfeng Tang, Hui Liu, Zhenwei Dai, Xiang Zhang, Suhang Wang, Benoit Dumoulin, Jian Pei",
                    "summary": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“Agentic AI”的核心方向。 1.  **核心判断**: *   论文的核心贡献是提出了一个名为“A-Evolve”的通用框架，主张将“演化”本身从一个固定的流程提升为一个“自主演化智能体”。 *   这直接对应了您研究目标中的“自我演化”方向，即智能体通过经验或环境反馈进行自我完善和迭代。论文旨在解决LLM在开放世界中静态训练无法适应环境变化的根本问题，属于构建和演化LLM智能体的方法论研究。 2.  **正面指标**: *   **核心范式**: 论文明确涉及 `Agentic AI`, `Self-Evolving`, `Autonomous Evolver Agent`。 *   **演化机制**: 论文提出了“演化扩展假设”，强调通过智能体机制实现持续、开放式的适应，这完全符合您关注的 `Self-Improvement` 和 `Iterative Improvement`。 3.  **排除标准**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   它不是将现有框架应用到特定领域（如医疗、金融），而是提出了一种通用的演化机制。 综上所述，该论文深入探讨了LLM智能体的自我演化路径，提出了新的Agentic框架，高度契合您关于“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决静态LLM难以适应动态环境变化的train-deploy gap问题。针对开放环境中的持续部署场景，我们提出了一种Agentic Evolution框架（A-Evolve），通过自主evolver agent对持久化状态进行目标导向的优化与更新。我们在AppWorld benchmark上通过Task Goal Completion (TGC)和Average Passed Tests (APT)验证了其有效性，证实了进化能力随计算资源投入而扩展。",
                    "summary_translation": "随着 Large Language Models (LLMs，大型语言模型) 从精心筛选的训练集转向开放式的现实世界环境，一个根本性的局限性随之显现：静态训练无法跟上持续变化的部署环境。扩展训练时和推理时的算力虽然能提升静态能力，但无法弥合这一训练-部署鸿沟。我们认为，解决这一局限性需要一个新的扩展维度——evolution (进化)。现有的部署时适应方法，无论是 parametric fine-tuning (参数化微调) 还是 heuristic memory accumulation (启发式记忆积累)，都缺乏诊断故障并产生持久改进所需的 strategic agency (战略主体性)。我们的立场是，agentic evolution (智能体进化) 代表了 LLM 适应的必然未来，它将进化本身从一个固定流程提升为一个 autonomous evolver agent (自主进化智能体)。我们在一个通用框架 A-Evolve 中具体实现了这一愿景，该框架将部署时的改进视为针对 persistent system state (持久系统状态) 的一种审慎的、目标导向的优化过程。我们进一步提出了 evolution-scaling hypothesis (进化扩展假设)：适应能力随着分配给 evolution (进化) 的算力而扩展，这将 agentic evolution (智能体进化) 定位为在现实世界中实现持续、开放式适应的一条可扩展路径。",
                    "inspiration_trace": "基于对论文《Position: Agentic Evolution is the Path to Evolving LLMs》的深入分析，以下是对作者产出该文章核心思想的逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的逻辑，构建了一个从现状到困境，再到核心矛盾的叙事链条：\n\n1.  **现状铺垫（成功的范式）：**\n    LLM 目前的成功主要建立在两个维度的扩展上：**训练时算力**（预训练和后训练）和 **推理时算力**（通过思维链增强推理能力）。这确立了当前领域的主流认知基准。\n\n2.  **引入冲突（现实的挑战）：**\n    当 LLM 从受控的训练集走向开放的真实世界环境时，一个根本性的局限出现了：**静态训练无法跟上持续变化的部署环境**。现实世界的 API 变动、约束演化是无限的，而模型训练是有限的。\n\n3.  **揭示后果（现有范式的失效）：**\n    纯静态模型在长期部署中不可避免地会退化或失效。此时，单纯增加“推理时算力”（即让模型“想得更久”）虽然能解决新问题，但对于**反复出现的失败**（如解析新文件格式）是极其低效的——每次都重新推导解决方案，而不是“学会”它。\n\n4.  **批判现有方案（尝试与失败）：**\n    作者指出现有的部署时适应方法存在根本缺陷：\n    *   **参数化微调**：更新权重，但过程不透明、难以治理，且面临灾难性遗忘的风险。\n    *   **非参数化启发式记忆**：积累文本记忆，但本质是存储/搜索问题，随着经验增加，记忆会被噪声填满，收益递减。\n    *   **共同根源**：这些方法的更新规则 $F_{Evolve}$ 是静态的、启发式的，缺乏**战略智能体**的能力去诊断失败并产生持久的改进。\n\n5.  **提出核心矛盾（The Gap）：**\n    真正的瓶颈不在于模型本身的静态能力，而在于缺乏一种能够自主诊断、决策并持久改进的**进化机制**。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题可总结为：\n\n**“如何将大语言模型的部署时适应过程，从静态的、启发式的修补，转变为一种可扩展的、自主的智能体进化过程，从而在开放世界中实现持续的能力提升？”**\n\n---\n\n### 三、 思想演进脉络（逻辑链推演）\n\n以下是从宏观观察到具体方法论的完整思考过程还原：\n\n#### 第一阶段：观察与定义（发现“第三轴”）\n*   **观察：** 现有的 LLM 发展依赖于“训练”和“推理”两个轴。但在真实部署中，模型面对的是非平稳分布。\n*   **洞察：** 仅仅在训练时学习或在推理时思考是不够的。系统必须在部署过程中**自主进化**。\n*   **定义：** 作者将“进化”定义为部署时的持续学习，即系统将交互证据转化为持久行为改进的过程（公式 1：$\\pi_{t+1} \\leftarrow F_{Evolve}(\\dots)$）。这被视为继训练和推理之后的**第三扩展轴**。\n\n#### 第二阶段：诊断与归因（寻找“阿喀琉斯之踵”）\n*   **分析：** 为什么现有的进化方法（如在线微调、记忆检索）无法长期奏效？\n*   **归因：** 问题不在于“更新什么”（参数 vs 非参数），而在于“如何更新”。\n    *   现有的 $F_{Evolve}$ 是**机械的**（如梯度下降）或**盲目的**（如简单的追加记忆）。\n    *   它们缺乏**因果推理**能力：无法理解失败的根本原因，无法决定何时该更新，也无法验证更新是否有效。\n*   **结论：** 进化过程本身必须是**智能的**。我们需要一个“元智能体”来管理进化。\n\n#### 第三阶段：概念重构（提出“智能体进化”）\n*   **核心转变：** 将 $F_{Evolve}$ 从一个固定的函数提升为一个**显式的进化者智能体**。\n*   **原则确立：** 为了实现这一转变，作者确立了三个核心原则：\n    1.  **目标导向：** 不是盲目更新，而是诊断失败原因，针对性地修改持久化组件（如工具、代码）。\n    2.  **自主性：** 智能体自己决定何时更新、选择哪些证据，而不是遵循固定的时间表。\n    3.  **组合性：** 通过模块化的工件（工具、工作流、验证测试）来实现改进，而非非结构化的文本。\n\n#### 第四阶段：假设提出（进化扩展定律）\n*   **理论升华：** 如果进化是智能的，那么它应该像训练和推理一样，遵循某种扩展定律。\n*   **假设：** **进化扩展假设**——适应能力（即进化的性能边界）与分配给进化过程的算力成正比。\n*   **意义：** 这意味着我们可以通过投入更多算力给“进化者”，来系统性地提升模型在开放世界中的适应上限，而不仅仅是靠运气或特定任务的启发式规则。\n\n#### 第五阶段：框架落地（A-Evolve 的诞生）\n*   **具体化：** 为了验证上述思想，作者设计了 **A-Evolve** 框架。\n*   **机制设计：**\n    *   **状态：** 定义持久化工件状态 $\\pi_S$（知识库、工具库、验证库），作为进化的直接操作对象。\n    *   **循环：** 分离“求解”与“进化”。\n    *   **智能体：** 实现具体的进化者，包含诊断、规划、更新、验证四个步骤，确保更新是经过深思熟虑且被验证过的。\n\n#### 第六阶段：验证与辩护（闭环）\n*   **实验验证：** 通过实验证明，相比于非智能体的进化方法，A-Evolve 能更有效地利用算力，且随着进化算力的增加，性能持续提升（验证扩展假设）。\n*   **回应质疑：** 针对可能的观点（如“推理时算力就够了”、“直接微调参数即可”），作者指出智能体进化在成本效益、安全性和可审计性上的独特优势。\n\n---\n\n**总结：**\n作者的思考路径是从**发现静态模型在动态世界的根本缺陷**出发，通过**批判现有适应方法的机械性**，提出了**将进化过程本身智能体化**的范式转变，并最终通过**进化扩展假设**将其上升为一种可计算、可扩展的系统性理论。"
                },
                {
                    "title": "Autonomous Data Processing using Meta-Agents",
                    "arxiv_id": "2602.00307",
                    "authors": "Udayan Khurana",
                    "summary": "Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”与“单智能体能力增强”方向。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了 **ADP-MA (Autonomous Data Processing using Meta-agents)** 框架。这不仅仅是一个应用，而是一个新的**智能体架构**。它定义了“Meta-agents”如何通过分层编排来动态构建、执行和细化数据管道。这属于构建和改进 LLM 智能体的方法论。 2.  **包含核心关注点 (第二步)**: *   **多智能体**: 论文明确提到了“hierarchical agent orchestration”（分层智能体编排）和“ground-level agents”，涉及 Meta-agents 实例化并管理其他智能体，属于典型的多智能体系统 (MAS) 研究。 *   **规划**: 论文包含“planning module for strategy generation”，涉及智能体的多阶段规划能力。 *   **工具使用**: 提到了“tool integration”和利用外部工具。 *   **自我反思/修正**: 论文强调了“iterative refinement”（迭代细化）、“monitoring loop”（监控循环）和“backtracking”（回溯），这直接对应了智能体的自我反思和自我修正能力。 3.  **通过排除标准检查 (第三步)**: *   虽然论文的应用场景是“数据处理”，但其主要贡献在于提出了一种新的智能体框架（Meta-agents），而不是简单地将现有智能体作为工具应用到该领域。因此不属于“非演化型应用”。 *   不涉及安全、对齐、多模态视觉或图技术等排除领域。 综上所述，该论文提出了一种具有规划、工具使用、自我修正能力以及分层编排结构的新型智能体框架，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决传统数据管道静态僵化、缺乏自主监控与优化能力的问题。针对复杂的数据处理任务，我们提出了一种名为ADP-MA的框架，该框架利用Meta-agents进行分层编排，动态构建并迭代优化管道。我们在内部开发的25个任务集上进行了调优，并计划在KRAMABENCH benchmark上通过端到端准确率等指标验证其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《Autonomous Data Processing using Meta-Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在揭示当前技术范式的根本缺陷：\n\n1.  **背景铺垫**：数据处理管道是现代数据驱动应用的骨干，负责将原始数据转化为洞察。\n2.  **现状批判**：传统的管道构建严重依赖人工工程，专家为特定用例设计静态工作流。\n3.  **痛点揭示**：这种静态范式在现实面前显得脆弱——无法适应数据特征变化（如数据漂移）、无法处理新任务的高开发开销、缺乏自主错误恢复能力。\n4.  **现有方案的局限**：虽然LLM和通用代理/编程助手（如Copilot）能生成代码，但它们仅限于“单点辅助”或“预定义模板”。它们能写代码，却不能在部署后**自主监控、管理和优化**端到端的管道。\n5.  **核心矛盾**：我们拥有强大的代码生成工具，却缺乏一个能够像人类工程师一样，全生命周期自主管理复杂、多阶段数据任务的“系统大脑”。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个能够超越单纯的代码生成，实现对端到端数据处理管道进行动态构建、自主监控、自适应优化及故障恢复的智能系统？”**\n\n---\n\n### 三、 思想演进与逻辑链推演\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 观察与反思：从“静态工具”到“动态系统”\n*   **观察**：现实世界的数据是流动的、易变的，而现有的数据处理工具（ETL脚本、甚至现有的AI编码助手）本质上是静态的。\n*   **反思**：如果数据变了，管道必须随之改变。单纯让AI写一段Python代码是不够的，我们需要的是一个能够“看着数据跑”并在出错时“自己修”的系统。\n*   **初步设想**：我们需要一个具有“自主性”的系统，而不仅仅是“自动化”脚本。\n\n#### 2. 假设提出：引入“元代理”概念\n*   **假设**：单一的智能体无法同时处理“宏观策略”和“微观执行”。如果让一个LLM既负责整体架构又负责写每一行代码，上下文会爆炸，且容易顾此失彼。\n*   **概念创新**：提出**Meta-Agent（元代理）**。元代理不直接操作数据，而是“思考如何操作数据”。\n*   **层级分离**：\n    *   **高层**：负责战略规划、任务分解、全局一致性。\n    *   **底层**：负责具体的代码执行、数据清洗、转换。\n*   **逻辑推演**：通过这种“管理者-执行者”的分层，系统能够像人类团队一样协作，既有全局视野，又有专业执行力。\n\n#### 3. 机制设计：如何确保“元代理”不犯错？\n*   **挑战**：LLM生成的计划可能是不完美的，甚至是有害的（如Scope Creep）。\n*   **解决方案**：引入**迭代式批判机制**。\n*   **逻辑演进**：\n    *   仅仅生成计划是不够的，必须有一个独立的“批判者”角色来检查计划的逻辑性、依赖关系和范围。\n    *   将批判分为两级：Level 1 检查宏观计划结构，Level 2 检查具体执行步骤的可行性。\n    *   通过“计划-批判-修正”的闭环，在写代码之前就消灭大部分逻辑错误。\n\n#### 4. 鲁棒性构建：应对执行中的不确定性\n*   **挑战**：即使计划完美，执行过程中仍可能遇到数据漂移、异常值或资源耗尽。\n*   **解决方案**：引入**监控与回溯**。\n*   **逻辑演进**：\n    *   系统需要一个独立的“监控者”，实时观察底层代理的输出（如行数骤减、空值率激增）。\n    *   当错误发生时，不能简单报错停止。系统需要具备**智能回溯**能力：是局部修修补补（Phase-level），还是推翻重来（Plan-level）？这需要根据错误的严重程度动态决定。\n\n#### 5. 效率优化：解决成本与规模的矛盾\n*   **挑战**：在真实的大规模数据集上反复试错成本极高（时间和金钱）。\n*   **解决方案**：引入**渐进式采样**。\n*   **逻辑演进**：\n    *   不要上来就跑全量数据。先在极小样本（XS）上验证语法和逻辑，再在小样本（S）上验证功能，最后才在全量数据上运行。\n    *   这是一种“快速失败”的策略，确保在昂贵的计算发生前，逻辑已经被验证。\n\n#### 6. 最终方法论整合：ADP-MA 框架\n*   **综合**：将上述思想整合为一个统一的框架——ADP-MA。\n*   **架构落地**：\n    *   **Orchestrator（指挥官）**：理解任务，分解阶段。\n    *   **Architect（架构师）**：设计具体步骤，创建/销毁底层代理。\n    *   **Monitor（监察官）**：盯着执行过程，触发警报或回溯。\n*   **核心价值**：这不仅仅是一个代码生成器，而是一个具有自我修复、自我优化能力的**自主数据处理生态系统**。\n\n---\n\n**总结**：作者的思考路径从对现有静态工具的不满出发，敏锐地捕捉到“管理”比“生成”更重要，进而通过引入分层元代理、批判闭环和渐进式采样等机制，逐步构建出一个能够适应真实世界复杂性的自主系统。"
                },
                {
                    "title": "Localizing and Correcting Errors for LLM-based Planners",
                    "arxiv_id": "2602.00276",
                    "authors": "Aditya Kumar, William W. Cohen",
                    "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献属于单智能体规划**: 论文的核心目标是解决 \"LLM-based Planners\"（基于LLM的规划器）在符号规划任务中的失败问题。规划是 LLM 智能体的核心能力之一（对应筛选标准第一步中的“单智能体”方向）。论文提出的 L-ICL 方法旨在改进智能体的规划过程，使其能够遵守约束并生成有效计划，这属于构建和改进 LLM 智能体的方法论。 2.  **具备自我修正与迭代改进机制**: 论文提出的方法通过 \"iteratively augmenting instructions\"（迭代增强指令）和 \"targeted corrections\"（针对性修正）来修复规划中的错误。这种机制涉及识别错误、定位具体步骤并注入修正示例，符合筛选标准第二步中提到的 `Self-Correction`（自我修正）和 `Iterative Improvement`（迭代改进）等正面指标。这体现了智能体在执行任务过程中的自我完善能力。 3.  **非单纯应用或基础推理**: 虽然论文在 Gridworld、Sokoban 等环境中进行了实验，但这些是验证规划算法能力的标准基准，而非生物、金融等特定领域的垂直应用（符合第一步排除标准中的例外情况）。此外，论文关注的是智能体在多步任务中的规划约束遵守，而非单纯提升模型的基础数学或逻辑推理能力（符合第四步关于推理/规划的保留规则）。 综上所述，该论文聚焦于提升 LLM 智能体的规划能力及自我修正机制，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决 LLM 在符号规划任务中频繁违反领域约束的问题。针对 Gridworld、Sokoban 和 BlocksWorld 等经典规划场景，我们提出了一种 Localized In-Context Learning (L-ICL) 方法，通过迭代注入针对特定失败步骤的局部修正示例来增强提示。我们在多个规划基准上通过计划有效性、成功率和最优性验证了其有效性，结果显示 L-ICL 显著优于传统 ICL 及其他基线方法。",
                    "summary_translation": "大语言模型在数学和编程方面展示了强大的推理能力，但在符号经典规划任务上往往失败。我们的研究以及先前的工作表明，LLM生成的计划经常违反其指令中给出的领域约束（例如，穿墙）。为了解决这一问题，我们提出了利用局部上下文学习示范来迭代增强指令的方法：即针对特定失败步骤进行针对性修正。具体而言，L-ICL识别轨迹中的第一个约束违反，并注入一个最小输入-输出示例，以展示失败步骤的正确行为。我们提出的L-ICL技术显著优于显式指令或添加完整问题解决轨迹的传统ICL，以及许多其他基线方法。例如，在8x8的网格世界中，L-ICL仅需60个训练示例即可生成有效计划的成功率达到89%，而最佳基线仅为59%，提升了30%。L-ICL在其他领域（网格世界导航、迷宫、推箱子和积木世界）以及多种LLM架构上也展现出显著的改进效果。",
                    "inspiration_trace": "基于对论文《Localizing and Correcting Errors for LLM-based Planners》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑链\n\n作者在引言部分通过层层递进的对比，构建了研究的必要性：\n\n1.  **前提：** LLMs 在数学和代码等通用推理任务上表现出色，暗示其具备强大的通用规划能力。\n2.  **冲突：** 然而，在经典的符号规划基准测试中，LLMs 却频繁失败，甚至在简单的任务（如网格世界导航）上表现不佳。\n3.  **现象：** 具体的失败模式并非缺乏知识，而是**违反领域约束**（例如：尽管被告知不能穿墙，模型仍规划出穿墙路径）。\n4.  **现有方案的局限：**\n    *   **显式指令：** 无效，因为模型无法在测试时一致地应用这些指令。\n    *   **传统 ICL（In-Context Learning）：** 即使提供大量完整的解题轨迹（如 20,000 字符），效果依然很差。原因在于完整轨迹只展示了“计划是可行的”，却隐含了“为什么单步是有效的”，导致规则依然隐晦，模型难以推断。\n5.  **推论：** 我们需要一种方法，能够将隐含的约束知识在具体的失败点显式化。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何通过显式化单步推理中的约束知识，来有效纠正 LLM 在规划任务中的违反约束行为，从而克服显式指令和传统完整轨迹演示的失效？”**\n\n---\n\n### 三、 作者产出核心方法的逻辑演进\n\n从宏观观察到具体方法论的提出，作者的思考过程经历了以下五个阶段：\n\n#### 1. 宏观观察与矛盾发现\n*   **思考起点：** LLMs 既然能做复杂的数学推理，为什么连简单的“绕过障碍物”都做不好？\n*   **初步判断：** 这不是智商问题，而是“知识应用”的问题。模型虽然“知道”规则（在预训练中），但在具体生成时无法“调用”这些规则来约束输出。\n\n#### 2. 归因分析：从“不知道”到“做不对”\n*   **深入分析：** 传统的提示工程（如 Zero-shot）提供了完整的领域描述（如地图、规则），但模型依然犯错。这说明瓶颈不在于**知识的获取**，而在于**知识的执行**。\n*   **假设：** 模型需要一个更强的“触发器”来强制其在特定步骤应用特定规则。\n\n#### 3. 对现有方案的批判与反思\n*   **审视 ICL：** 为什么给模型看 20,000 字符的完美解题路径（RAG-ICL）没用？\n*   **洞察：** 完整的轨迹是“端到端”的演示。它告诉模型“这样做能到达终点”，但没有告诉模型“在这一步，向东走是非法的”。这种**隐含性**导致模型只能模仿表面模式，无法掌握底层的物理约束。\n*   **类比：** 这就像软件工程中的“端到端测试”与“单元测试”。端到端测试只能证明系统整体跑通了，但无法保证每个函数内部逻辑的正确性。\n\n#### 4. 核心洞察：局部化与显式化\n*   **思维转折：** 既然“教它怎么做完整的题”效率低，不如“教它怎么修正具体的错误”。\n*   **关键假设：** 如果我们能精确找到模型思维链中**第一个出错的地方**，并只针对这一步给出一个最小的输入-输出修正示例，模型就能更高效地学会这一类约束。\n*   **逻辑优势：** 这种“局部化”的示例信息密度极高，直接编码了约束逻辑（例如：输入坐标(3,4)，输出不能包含向东），比冗长的完整轨迹更有效。\n\n#### 5. 方法论构建：L-ICL 的诞生\n*   **机制设计：** 如何实现上述假设？\n    *   **结构基础：** 采用 **PTP (Program Trace Prompting)**。因为 PTP 将推理分解为结构化的子程序调用，这天然提供了“插入点”，方便我们在特定的子步骤（如 `get_applicable_actions`）中插入修正。\n    *   **流程闭环：** 设计一个“失败驱动”的循环。\n        1.  让模型生成轨迹。\n        2.  利用外部符号验证器找到**第一个**错误（First Failure）。\n        3.  获取该步骤的正确输出。\n        4.  将这个 `(输入, 正确输出)` 对作为 Doctest 格式的例子，注入到提示词中对应子程序的文档里。\n*   **最终形态：** **L-ICL (Localized In-Context Learning)**。它不是教模型“怎么解题”，而是通过不断积累局部的“单元测试用例”，让模型在推理时逐步“硬化”其行为，确保每一步都符合物理约束。\n\n---\n\n**总结：** 作者的思考路径是从**能力悖论**出发，诊断出**约束应用失效**的病灶，批判了**端到端演示**的低效，最终通过**软件工程中的单元测试思想**，提出了基于**局部错误修正**的 L-ICL 方法。"
                },
                {
                    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
                    "arxiv_id": "2602.02486",
                    "authors": "Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, Ruichun Ma, Bei Liu, Yifan Yang, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Xin Geng, Baining Guo",
                    "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **Re-TRAC**，这是一个新的 **Agentic Framework（智能体框架）**。它旨在解决现有基于 ReAct 框架的深度研究智能体在长上下文和复杂搜索中的局限性（如线性设计导致的局部最优和冗余探索）。这属于典型的“构建、改进 LLM智能体”的研究，而非将智能体作为工具应用到特定领域的应用型研究。 2.  **正面指标（高度匹配）**： *   **单智能体**：论文明确聚焦于 **Deep Search Agents** 的架构改进。 *   **规划与记忆**：Re-TRAC 通过生成 **结构化状态表示** 来总结证据和计划，这直接对应智能体的 **Memory（记忆）** 机制；同时它实现了 **globally informed planning（全局感知规划）**，改进了智能体的规划能力。 *   **自我反思与演化**：论文强调了 **iterative reflection（迭代反思）** 和 **cross-trajectory reflection（跨轨迹反思）**，使得智能体能够基于过往轨迹调整后续行动。这种通过反思减少冗余搜索、实现 progressively targeted exploration（渐进式精准探索）的过程，符合 **Self-Evolving（自我演化）** 和 **Iterative Improvement（迭代改进）** 的特征。 3.  **排除标准（无冲突）**： 论文不涉及安全对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **特殊情况处理**： 论文虽然提到了在 BrowseComp 数据集上的测试，但其本质是关于智能体如何进行 **Planning（规划）** 和在复杂任务中进行多步推理的框架创新，而非单纯提高模型的基础推理能力。它提出的跨轨迹探索机制是对 Agentic AI 架构的直接演进。 综上所述，该论文通过引入新的反思和状态压缩机制，显著改进了 LLM 智能体的搜索效率和规划能力，属于单智能体与自我演化方向的高质量前沿研究。",
                    "summary2": "本文旨在解决深度研究智能体因线性推理导致的局部最优和冗余探索问题。针对长上下文下的复杂搜索任务，我们提出了一种 Re-TRAC 框架，通过递归压缩轨迹生成结构化状态表示以指导跨轨迹探索。在 BrowseComp、GAIA 等基准上通过 Accuracy 验证了其有效性，实现了显著的性能提升及资源消耗的单调递减。",
                    "summary_translation": "基于LLM (Large Language Model，大语言模型) 的深度研究智能体主要构建于ReAct框架之上。这种线性设计难以回溯早期状态、转向替代的搜索方向，或在长上下文下保持全局感知，往往导致局部最优、冗余探索和低效搜索。我们提出了Re-TRAC，这是一个智能体框架，通过在每个轨迹后生成结构化状态表示来执行跨轨迹探索，以总结证据、不确定性、失败和未来计划，并以此状态表示为条件生成后续轨迹。这使得迭代反思和基于全局信息的规划成为可能，将研究重构为一个渐进式过程。实证结果表明，在使用前沿LLM的BrowseComp数据集上，Re-TRAC的性能始终优于ReAct 15-20%。对于较小的模型，我们引入了Re-TRAC感知的监督微调，在可比规模下实现了最先进的性能。值得注意的是，Re-TRAC在多轮交互中显示出工具调用和Token使用量的单调递减，表明这是一种由跨轨迹反思驱动的渐进式精准探索，而非冗余搜索。",
                    "inspiration_trace": "基于论文《RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents》的内容，以下是对作者产出该文章核心方法的逻辑链推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从现状到冲突的演进逻辑，具体如下：\n\n1.  **背景铺垫**：大语言模型（LLM）的发展已经从单轮问答进化到了具备复杂多轮能力的智能体。\n2.  **现状确立**：目前的“深度研究智能体”大多建立在 **ReAct 框架**之上，即通过线性交替的方式进行推理和工具调用。\n3.  **揭示冲突**：这种线性设计与深度研究任务的本质需求存在错配。深度研究需要广泛的探索、回溯和分支，但 ReAct 的线性特性使得在长上下文中很难回溯早期状态、分支探索或保持全局感知。\n4.  **后果阐述**：这种结构性的不匹配导致了智能体陷入局部最优、产生冗余探索以及搜索效率低下。\n5.  **潜在机会**：作者指出两个关键观察——一是现有模型在多次尝试下的表现远高于单次，说明瓶颈在于“探索不足”而非“推理能力不足”；二是模型更擅长验证而非生成，这暗示了“广泛生成+验证筛选”的搜索范式潜力。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图解决的核心研究问题可总结为：\n\n**“如何克服线性 ReAct 范式在长上下文任务中的局限性，构建一种能够跨轨迹整合经验、保持全局感知并实现高效系统性探索的智能体框架？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进链\n\n以下是从宏观观察到具体方法论的系统性思考过程：\n\n#### 第一阶段：诊断瓶颈——从“线性”到“遗忘”\n*   **观察现象**：现有的深度研究智能体（基于 ReAct）在处理长周期任务时，往往无法给出正确答案。\n*   **深入分析**：作者对失败轨迹进行了系统性分析（Section 3），发现了一个惊人的事实：**高达 93% 的失败轨迹并非因为模型推理能力不足，而是因为“分支探索不完整”**。即模型在早期计划了要探索的分支，但随着轨迹变长，它“忘记”去执行这些计划了。\n*   **归因**：这被归结为 ReAct 的**线性结构**与深度研究的**树状/网状需求**之间的根本性错配。在长上下文中，早期的关键规划被中间大量的工具调用和观察淹没，导致“灾难性遗忘”。\n\n#### 第二阶段：评估现有方案——从“独立尝试”到“信息孤岛”\n*   **直觉方案**：既然单次探索容易遗漏，那就多试几次（如 Pass@K, Best-of-N）。\n*   **发现潜力**：数据表明 Pass@K 的性能远高于 Pass@1，说明多次尝试确实能挖掘出模型的潜力。\n*   **指出缺陷**：现有的多次尝试（如 Majority Voting）都是**相互独立**的。这带来了两个新问题：\n    1.  **冗余浪费**：每次尝试都从头开始，重复验证已知事实，浪费计算资源。\n    2.  **经验断层**：第 N 次尝试无法从第 N-1 次的失败中吸取教训，导致重复犯错或重复探索相同的死胡同。\n\n#### 第三阶段：提出核心假设——从“独立轨迹”到“递归进化”\n*   **思维转折**：作者意识到，解决问题的关键不在于增加尝试的次数，而在于改变尝试之间的**关系**。\n*   **核心假设**：如果能让后续的尝试“看到”并“理解”之前的尝试，将之前的轨迹压缩成一种“经验状态”，并以此作为下一次探索的起点，就能同时解决“遗忘”和“冗余”的问题。\n*   **目标设定**：构建一个**轨迹级别的递归框架**，将探索过程从一组孤立的尝试转变为一个渐进式、自我进化的过程。\n\n#### 第四阶段：方法论设计——从“状态压缩”到“结构化反思”\n*   **实现难点**：如何让模型“理解”之前的轨迹？直接把历史记录喂给模型会再次触发上下文限制。\n*   **解决方案**：设计一种**结构化的状态表示**。\n    *   **压缩逻辑**：不是简单的摘要，而是提取关键维度：已验证的证据、当前的结论、**未解决的分支**、以及失败的模式。\n    *   **递归执行**：每一轮结束后，将轨迹压缩为这种状态，作为下一轮的 Prompt 输入。\n*   **机制优势**：\n    *   **覆盖性**：显式保留“未完成的分支”，强制模型在下一轮去探索，解决“遗忘”问题。\n    *   **效率性**：记录“已验证的证据”，避免模型重复调用工具，解决“冗余”问题。\n\n#### 第五阶段：验证与泛化——从“前沿模型”到“小模型训练”\n*   **初步验证**：直接将 Re-TRAC 作为一种 Prompting 策略应用于前沿模型（如 GPT-5, o3），发现无需训练即可获得显著提升，证明了框架的通用性。\n*   **进一步思考**：既然框架有效，能否让小模型也具备这种能力？\n*   **训练策略**：利用大模型生成的 Re-TRAC 轨迹数据，对小模型进行监督微调（SFT），教会小模型如何利用“结构化状态”进行推理，从而让小模型也能达到接近大模型的性能。\n\n**总结**：作者从 ReAct 的线性缺陷出发，通过分析失败案例发现“分支遗忘”是核心痛点，进而通过对比独立尝试的局限性，提出了“跨轨迹经验压缩与递归”的创新思路，最终设计出 Re-TRAC 框架，实现了从盲目搜索到渐进式深度搜索的范式转变。"
                },
                {
                    "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
                    "arxiv_id": "2602.02474",
                    "authors": "Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang, Haodong Yue, Wenya Wang",
                    "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合要求 (第一步)**: *   论文的核心贡献是提出了 **MemSkill**，这是一个针对 LLM 智能体记忆系统的改进框架。它将传统的静态、手工设计的记忆操作转化为**可学习且可演化的记忆技能**。 *   这直接对应了您筛选标准中的“构建、改进或演化 LLM 智能体”。它不是简单的应用，而是对智能体核心组件（记忆）的架构性创新。 2.  **高度契合“自我演化”焦点 (第一步 & 第二步)**: *   论文标题和摘要中明确提到了 **\"Self-Evolving Agents\"**。 *   论文引入了一个 **\"designer\"** 组件，用于定期审查失败案例，并通过提出改进和新技能来**演化技能集**。这种闭环的自我完善机制正是您“自我演化”研究焦点中的核心内容（Self-Improvement, Iterative Improvement）。 3.  **属于“单智能体”能力增强 (第二步)**: *   论文专注于智能体的 **Memory**（记忆）能力，这是单智能体研究的关键子方向之一。它通过学习如何提取、整合和修剪信息，提升了智能体处理长历史和复杂交互的能力。 4.  **排除标准检查 (第三步)**: *   论文不涉及安全与对齐、多模态视觉或图技术。 *   虽然论文在 HotpotQA 和 ALFWorld 等数据集上进行了实验，但这属于对智能体能力的基准测试，而非将智能体作为工具去解决某个特定垂直领域（如医疗、法律）的应用问题，因此不属于“非演化型应用”的排除范畴。 综上所述，该论文提出了一种通过演化记忆技能来增强智能体能力的新框架，精准命中了您关于“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在解决现有LLM智能体记忆系统依赖静态手工操作、缺乏适应性的问题。针对长交互历史和多样化场景，我们提出了一种名为MemSkill的框架，将记忆操作重构为可学习和进化的记忆技能。该方法通过控制器选择技能，利用LLM执行器生成技能引导的记忆，并引入设计器基于困难案例持续进化技能库。我们在LoCoMo、LongMemEval、HotpotQA和ALFWorld数据集上，通过F1-score、LLM judge score和Success Rate等指标验证了其有效性。",
                    "summary_translation": "大多数 Large Language Model (LLM) agent (大语言模型智能体) 的 memory systems (记忆系统) 依赖于少量 static, hand-designed operations (静态、手工设计的操作) 来提取记忆。这些固定流程 hard-code (硬编码) 了关于存储什么以及如何修订记忆的 human priors (人类先验知识)，使得它们在多样的 interaction patterns (交互模式) 下显得僵化，并且在处理 long histories (长历史记录) 时效率低下。为此，我们提出了 MemSkill，它将这些操作重构为可学习和可进化的 memory skills (记忆技能)，即用于从 interaction traces (交互轨迹) 中提取、整合和修剪信息的结构化且可复用的例程。受智能体技能设计哲学的启发，MemSkill 采用了一个学习选择少量相关技能的 controller (控制器)，并搭配一个基于 LLM 的 executor (执行器) 来生成 skill-guided memories (技能引导的记忆)。除了学习技能选择外，MemSkill 还引入了一个 designer (设计器)，定期审查选定技能产生错误或不完整记忆的困难案例，并通过提出改进和新技能来进化技能集。共同作用，MemSkill 形成了一个 closed-loop procedure (闭环流程)，能够同时改进 skill-selection policy (技能选择策略) 和技能集本身。在 LoCoMo、LongMemEval、HotpotQA 和 ALFWorld 上的实验表明，MemSkill 相比 strong baselines (强基线模型) 提升了任务性能，并且在不同的设置下具有良好的泛化能力。进一步的分析揭示了技能进化的方式，为 LLM agent (大语言模型智能体) 实现 more adaptive, self-evolving memory management (更自适应、自进化的记忆管理) 提供了见解。",
                    "inspiration_trace": "基于对论文《MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程。\n\n---\n\n### 1. 宏观背景与问题引入\n\n**逻辑链起点：**\n随着大语言模型（LLM）智能体参与越来越长、开放式交互，它们必须处理不断增长的历史记录。保留经验并维持连贯性至关重要，这推动了智能体记忆系统的快速发展。\n\n**现有方案的“故事”逻辑：**\n1.  **现状观察**：目前的记忆系统主要依赖于**静态的、手工设计的操作**（如固定的增加/更新/删除/跳过原语）和启发式模块来管理记忆。\n2.  **核心冲突**：这些固定程序将“人类先验”硬编码进了系统中——即人类预设了“什么值得存储”以及“如何修订记忆”。\n3.  **导致的后果**：在面对多样化的交互模式时，这种设计显得**僵化**；在处理长历史记录时，**扩展性差且效率低下**。\n4.  **结论**：这种根本性的限制阻碍了智能体记忆的适应性。我们需要一种不再依赖固定程序或手工设计模块的新范式。\n\n---\n\n### 2. 研究问题\n\n基于上述对现有方案僵化性和人工先验依赖的批判，作者试图解决的核心问题可归纳为：\n\n**“如何将静态的、手工设计的记忆操作转化为可学习且可进化的‘技能’，从而使智能体能够自适应地管理记忆？”**\n\n---\n\n### 3. 思想演进与逻辑推演\n\n为了回答上述问题，作者的思考经历了从“抽象重构”到“机制设计”再到“闭环进化”的三个关键阶段：\n\n#### 第一阶段：概念重构——从“操作”到“技能”\n*   **思考**：既然手工设计的固定操作（如Add/Delete）太死板，那能不能把记忆提取本身变成一种**可学习的抽象**？\n*   **假设**：我们可以将记忆构建视为应用一组通用的、可复用的**记忆技能**的结果。\n*   **定义**：这里的“技能”是指一种结构化的行为，规定了何时以及如何将交互痕迹转化为记忆并进行修订。\n*   **优势预判**：这种视角允许系统根据交互数据塑造记忆行为，支持更大的提取粒度（不仅仅是逐轮处理），并能通过组合少量相关技能来灵活构建记忆。\n\n#### 第二阶段：机制设计——如何使用技能？\n*   **思考**：有了“技能库”，智能体该如何在具体场景下使用它们？\n*   **方案**：\n    *   **控制器**：负责“选择”。根据当前的上下文，从技能库中挑选出最相关的一小部分技能。\n    *   **执行器**：负责“生成”。基于选定的技能，指导LLM在一步生成中完成记忆的提取和更新。\n*   **逻辑**：这模仿了人类技能调用的过程，不是机械地执行每一步，而是根据情境调用相应的技能组合。\n\n#### 第三阶段：自我进化——技能从哪里来？\n*   **思考**：仅仅学习“如何使用”现有的技能是不够的，因为初始技能可能仍然包含人类先验（如初始化的Insert/Update）。如何让系统摆脱对人工设计的依赖，实现真正的自适应？\n*   **关键洞察**：系统应该具备**自我进化**的能力，即不仅能学怎么用技能，还能改进技能本身。\n*   **方案**：\n    *   **设计师**：定期回顾训练中遇到的“困难案例”（即当前技能导致记忆错误或不完整的案例）。\n    *   **进化机制**：利用LLM分析失败原因，提出对现有技能的修正或直接发明新技能。\n*   **闭环形成**：学习使用技能 $\\rightarrow$ 遇到困难 $\\rightarrow$ 进化技能库 $\\rightarrow$ 继续学习使用新技能。\n\n---\n\n### 4. 总结：核心方法论的形成\n\n最终，MemSkill 的思想脉络可以总结为：\n\n1.  **打破静态**：拒绝手工设计的固定记忆操作，提出**“记忆技能”**这一可学习、可复用的抽象概念。\n2.  **条件化生成**：通过**控制器**选择技能，**执行器**基于技能生成记忆，实现灵活的、组合式的记忆构建。\n3.  **闭环进化**：引入**设计师**机制，利用失败反馈不断修正和扩充技能库，从而实现一个**最小化人类先验、能够随任务需求演进的自我进化记忆系统**。"
                },
                {
                    "title": "Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory",
                    "arxiv_id": "2602.01708",
                    "authors": "Langyuan Cui, Chun Kai Ling, Hwee Tou Ng",
                    "summary": "Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \\textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴**： 论文提出了 \"Game of Thought (GoT)\" 框架，其核心在于解决 LLM 如何在信息缺失的情况下主动进行信息搜寻。这属于智能体的核心能力——**规划**与**决策**。它不仅仅是生成文本，而是通过策略性的提问来与环境（或对手）交互，以达成目标，这符合 Agentic AI 的定义。 2.  **涉及 Multi-Agent 的博弈机制**： 论文明确使用了**博弈论**，将信息搜寻问题形式化为双人零和扩展形式博弈，并致力于近似纳什均衡策略。这直接对应筛选标准中 \"Multi-Agent\" 方向下的 \"博弈\" (Game) 和 \"策略\" (Strategy) 关注点。即使该框架主要用于指导单个智能体的行为，其底层逻辑是基于多智能体交互的博弈理论，属于研究范围内的方法论创新。 3.  **非排除项**： *   **非纯推理/非应用**：论文不是单纯提升 LLM 的基础数学或逻辑推理能力（如 CoT 的微调变体），也不是将已有框架简单应用到特定垂直领域（如生物、金融）。它提出了一种新的基于博弈论的算法框架来增强智能体的鲁棒性。 *   **非基础设施/安全/多模态**：论文不涉及模型部署、安全对齐、水印或多模态视觉技术。 综上所述，该论文在构建智能体决策框架和利用博弈论优化智能体行为方面做出了实质性贡献，符合 \"LLM智能体及其演化\" 的研究课题。",
                    "summary2": "本文旨在解决LLM在高风险场景下信息获取的最坏情况性能问题。针对信息不足的环境，我们提出了一种Game of Thought (GoT)框架，将信息寻求过程建模为双人零和博弈，并通过近似纳什均衡优化策略。在20 Questions、医疗诊断等数据集上，通过最坏情况交互长度验证了其有效性。",
                    "summary_translation": "大型语言模型越来越多地部署在现实场景中，在这些场景中，它们可能缺乏足够的信息来完成给定任务。在这种情境下，主动搜寻缺失信息的能力变得至关重要。现有增强这一能力的方法往往依赖于简化假设，而这些假设会降低最坏情况下的性能。这在高风险应用中是一个具有严重后果的问题。在本研究中，我们利用“二十个问题”游戏来评估 LLMs 的信息搜寻能力。我们引入并形式化了其对抗性对应物——策略语言搜索问题及其变体，将其建模为双人零和扩展形式博弈。我们提出了思维博弈框架，该框架应用博弈论技术来近似该游戏受限变体的纳什均衡策略。实证结果表明，在所有测试设置中，与（1）基于直接提示的方法和（2）启发式引导的搜索方法相比，我们的方法始终提升了最坏情况下的性能。",
                    "inspiration_trace": "基于对论文《Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory》的深度分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从宏观背景到具体痛点的叙事逻辑，具体如下：\n\n1.  **宏观背景与需求**：\n    *   **现状**：大语言模型（LLMs）正被部署在医疗诊断、规划等高风险、高价值的现实场景中。\n    *   **挑战**：在这些场景下，模型往往面临“部分可观测性”，即缺乏完成任务所需的充分信息。\n    *   **能力缺口**：因此，LLM 必须具备主动寻求信息的能力，即通过提出澄清性问题来填补信息空白。\n\n2.  **评估基准与现有范式**：\n    *   **量化工具**：为了评估这种能力，作者引入了经典的“20个问题”游戏作为测试基准。\n    *   **现有方法**：现有的先进方法（如 Self-Consistency, Tree of Thought, 以及本文重点对比的 Uncertainty of Thought (UoT)）通常利用树搜索和前瞻来优化提问策略。\n    *   **核心假设**：这些方法通常隐含了一个关键假设——目标物品是**均匀随机分布**的，或者遵循某种已知的先验分布。\n\n3.  **现实冲突与核心痛点**：\n    *   **假设失效**：在现实世界中，这种均匀分布的假设往往不成立（分布未知、非均匀或难以获取）。\n    *   **风险转移**：特别是在高风险环境中，我们不能依赖“平均情况”的表现。一旦遇到分布中的极端情况（长尾风险），基于平均优化的策略可能会导致灾难性后果。\n    *   **思维转变**：因此，作者提出必须假设**最坏情况**，即假设目标是由一个试图阻碍信息获取的“对手”恶意选择的。\n\n4.  **解决思路的定性**：\n    *   为了应对这种对抗性环境，我们需要一种能够优化“最坏情况表现”的框架，而不是优化“平均信息增益”。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“在目标物品由对手恶意选择（而非均匀随机分布）的高风险信息寻求场景中，如何利用博弈论为大语言模型设计一种提问策略，以最小化最坏情况下的交互成本？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进\n\n以下是从观察到方法论形成的完整思维推演过程：\n\n#### 1. 观察与反思：从“平均最优”到“鲁棒性”的缺失\n*   **观察**：现有的 UoT 方法通过最大化信息增益（熵减）来提问，这在目标均匀分布时是有效的。\n*   **反思**：如果目标分布不均匀，或者存在一个“坏人”专门挑最难猜的物品怎么办？例如，如果我的策略总是先问“是生物吗？”，对手就会专门选一个非生物的物品，导致我后续步骤极其被动。\n*   **结论**：单纯追求信息增益在面对对抗性攻击时是脆弱的。我们需要一种**鲁棒**的策略，即无论对手选什么，我的表现都有下限保证。\n\n#### 2. 理论映射：将信息寻求转化为零和博弈\n*   **抽象**：既然是对抗性的，这本质上是一个博弈。\n*   **定义角色**：\n    *   **提问者**：希望用最少的问题猜出物品（最小化步数）。\n    *   **物品选择者**：希望提问者用最多的问题才猜出（最大化步数）。\n*   **博弈类型**：这是一个**双人零和扩展式博弈**。\n*   **目标函数**：提问者的目标不再是最大化期望收益，而是**最小化最大损失**。\n\n#### 3. 理论解：纳什均衡与随机化策略的必要性\n*   **求解思路**：在零和博弈中，最优解对应于**纳什均衡（NE）**。\n*   **关键洞察**：在对抗性设定下，确定性的策略（总是问同一个问题）必然会被针对。因此，最优策略必须是**随机化**的。\n*   **理论验证**：作者通过简单的例子证明，混合策略（以一定概率随机选择不同的问题）可以比任何单一确定性策略获得更小的最大期望成本。\n\n#### 4. 实践挑战：计算复杂度与动作空间爆炸\n*   **困难**：虽然理论上有解，但直接求解这个博弈是不现实的。\n    *   **状态空间**：可能的物品集合 $S$ 很大。\n    *   **动作空间**：自然语言问题空间 $Q$ 是无限的。\n    *   **计算成本**：构建完整的博弈树并求解（如使用线性规划或 CFR）对于 LLM 来说计算量过大，且每次调用 LLM 都有延迟。\n\n#### 5. 方法论创新：Game of Thought (GoT) 的诞生\n为了解决上述计算挑战，作者借鉴了德州扑克 AI 的思想，提出了 GoT 框架：\n\n*   **限制搜索范围**：不搜索所有可能的问题，而是利用 LLM 生成 $m$ 个候选问题（SLS-Restricted 变体）。\n*   **子博弈搜索**：\n    *   不求解整个游戏树，而是只向前看 $d$ 步（深度限制）。\n    *   在当前状态下构建一个局部的“子博弈”。\n*   **局部求解与执行**：\n    *   使用博弈论求解器（如 CFR）计算这个子博弈的纳什均衡。\n    *   根据均衡策略给出的概率分布，随机选择下一个问题。\n    *   重复此过程，直到找到目标。\n*   **安全性保证**：通过让对手在子博弈开始时“重新选择”分布，确保这种局部求解不会比全局策略更差（即 Safe Subgame Resolving）。\n\n#### 6. 扩展与验证：从非加权到加权场景\n*   **进一步思考**：现实世界不仅是对抗的，不同物品的代价也不同（例如，漏诊重病的代价远高于漏诊感冒）。\n*   **扩展**：引入加权 SLS (WSLS)，将物品的权重纳入博弈的收益函数中。\n*   **验证**：通过实验证明，GoT 在最坏情况下的表现显著优于 UoT 和直接提示法，且在平均性能上没有显著损失，实现了鲁棒性与效率的平衡。"
                },
                {
                    "title": "DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework",
                    "arxiv_id": "2602.00996",
                    "authors": "Abhijit Chakraborty, Ashish Raj Shekhar, Shiven Agarwal, Vivek Gupta",
                    "summary": "Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心贡献符合构建智能体框架的要求 (第一步 - 核心判断)**: 论文的核心贡献是提出了 **DeALOG**，这是一个“去中心化多智能体框架”。这不仅仅是将现有智能体作为工具应用，而是提出了一种新的**系统架构**和**方法论**，用于解决智能体间的协作与通信问题。因此，它符合“保留”标准。 2.  **高度契合多智能体研究方向 (第二步 - 正面指标)**: 论文涉及了多个关键的多智能体正面指标： *   **Multi-Agent Systems (MAS)**: 明确提出了多智能体框架。 *   **Communication**: 智能体通过“共享自然语言日志”进行通信。 *   **Memory**: 该日志作为“持久记忆”存在。 *   **Collaboration**: 智能体之间进行协作错误检测和验证，且没有中央控制（去中心化）。 *   **Specialization**: 包含了专门的智能体（表格、上下文、视觉、总结、验证）。 3.  **关于多模态的处理 (第三步 - 排除标准)**: 虽然摘要中提到了“跨文本、表格和图像”以及“Visual Agent”，但这属于**例外情况**。论文的研究焦点**不是**视觉模型本身的改进，而是将视觉能力作为智能体感知环境的一个**工具/组件**。论文的核心在于如何通过日志机制协调这些不同能力的智能体，因此不应被排除。 综上所述，该论文提出了一种新的多智能体协作与通信机制，属于LLM智能体架构层面的创新，符合筛选要求。",
                    "summary2": "本文旨在解决复杂多模态问答中的信息整合与鲁棒性问题。针对跨文本、表格和图像的复杂推理场景，我们提出了一种名为DeALOG的去中心化多智能体日志中介推理框架。该框架通过共享自然语言日志协调Table、Context、Visual等专门智能体，实现了去中心化的协作与验证。我们在FinQA、TAT-QA、WikiTableQuestions等六个数据集上通过准确率验证了其有效性，证明了其在长链推理和噪声环境下的优越性。",
                    "summary_translation": "涉及文本、表格和图像的复杂问答需要整合多样化的信息源。亟需一种能够支持专门化处理，并具备协调能力和可解释性的框架。我们提出了DeALOG，这是一种用于多模态问答的去中心化多智能体框架。该框架采用专门的智能体：Table（表格）、Context（上下文）、Visual（视觉）、Summarizing（总结）和Verification（验证），这些智能体通过作为持久化记忆的共享自然语言日志进行通信。这种基于日志的方法实现了无需中央控制的协作错误检测与验证，从而提高了系统的鲁棒性。在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA数据集上的评估结果表明，该模型展现出具有竞争力的性能。分析证实了共享日志、智能体专门化以及验证机制对于提升准确性的重要性。DeALOG通过利用自然语言通信的模块化组件，提供了一种可扩展的解决方案。",
                    "inspiration_trace": "基于对论文《DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework》的深度分析，以下是对作者核心方法论产出过程的系统性逻辑推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过构建一个“现状-冲突-缺口”的叙事弧线，逐步引出研究的必要性：\n\n1.  **宏观背景（复杂性挑战）：**\n    *   现实世界中的复杂问答（QA）往往涉及多模态数据（文本、表格、图像）的混合。\n    *   解决这类问题需要“组合式推理”，即能够整合来自不同来源的异构证据（例如：从表格找实体，从文本找属性，再进行联合推理）。\n\n2.  **现有方案的谱系（两极分化）：**\n    *   作者指出当前研究主要处于两个极端：\n        *   **极端一：隐式推理（如 Chain-of-Thought）。** 依靠单个大模型在上下文窗口内进行思维链推理。\n        *   **极端二：显式规划（如 Planner-Executor）。** 依靠中心化规划器生成显式计划，再调用工具执行。\n\n3.  **核心冲突（痛点分析）：**\n    *   **隐式推理的缺陷：** 缺乏对中间状态的显式维护，难以验证部分结果，在长链路或分支推理中稳定性差，容易“迷失”。\n    *   **显式规划的缺陷：** 虽然步骤清晰，但存在“错误级联”风险。一旦中心规划器在早期出错，后续步骤会忠实地执行错误计划，导致系统性脆弱。\n\n4.  **逻辑缺口（机会点）：**\n    *   现有框架要么太乱（隐式），要么太脆（显式）。\n    *   **结论：** 我们迫切需要一种既能暴露中间结构和证据依赖，又**不依赖单一固定计划**（从而避免错误放大）的推理框架。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑缺口，作者试图回答的核心研究问题是：\n\n**“如何构建一个去中心化的推理框架，使其既能像规划系统一样显式地维护中间推理状态，又能通过去中心化的协作机制避免中心化规划器带来的错误级联风险，从而实现鲁棒的多模态复杂问答？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进链\n\n以下是从观察到最终方法论的思维推演过程：\n\n#### 第一阶段：观察与反思（打破“中心化”迷信）\n*   **观察：** 传统的 Planner-Executor 架构中，Planner 是“单点故障”。如果 Planner 第一步理解错了，后面做得再好也是错的。\n*   **反思：** 人类专家团队解决复杂问题时，通常不是由一个“大脑”发号施令每一步，而是通过共享信息（如白板、文档）进行协作。每个人根据自己的专长决定何时发言。\n*   **推论：** 我们需要**去中心化**。去掉 Planner，让多个智能体自主决策。\n\n#### 第二阶段：机制设计（寻找“协作”的媒介）\n*   **问题：** 去掉了中心 Planner，多个 Agent（表格专家、文本专家、视觉专家）如何协同工作而不乱成一锅粥？\n*   **灵感：** 借鉴经典的“黑板系统”和计算机科学中的共享内存概念。\n*   **方案：** 引入**共享日志**。\n    *   这是一个自然语言的、只增不减的持久化记忆。\n    *   所有 Agent 都向这个 Log 写入发现（证据），并从 Log 读取他人的发现。\n    *   **核心价值：** Log 成为了唯一的“事实来源”，替代了 Planner 的指令流。\n\n#### 第三阶段：鲁棒性构建（解决“错误”问题）\n*   **问题：** 没有 Planner 统筹，如何保证 Agent 产生的答案是正确的？如何防止 Agent 产生幻觉或计算错误？\n*   **推论：** 既然是团队协作，就应该引入“同行评审”机制。\n*   **方案：** 引入**验证者**。\n    *   不仅仅是生成答案，还要有一个专门的 Verification Agent 读取 Log，检查计算、核对引用、验证逻辑一致性。\n    *   如果发现错误，不直接修正，而是向 Log 发送“Flag”，触发相关 Agent 重新介入（一次重试机会）。\n    *   **逻辑闭环：** 这种基于 Log 的交叉验证，将错误检测分布化了，而不是依赖 Planner 的完美规划。\n\n#### 第四阶段：系统整合（形成 DeALOG 架构）\n*   **角色定义：** 根据数据模态定义专才——TableAgent（查表）、ContextAgent（查文）、VisualAgent（看图）、SummarizingAgent（总结）、VerificationAgent（纠错）。\n*   **控制流简化：** 不需要复杂的规划算法，只需要一个简单的“调度器”来轮询 Agent：“Log 更新了吗？你有新东西补充吗？”\n*   **最终形态：** **DeALOG** —— 一个无规划器、基于日志中介的去中心化多智能体框架。它通过共享 Log 实现了信息的透明流转，通过去中心化协作实现了对错误的容错。\n\n---\n\n**总结：**\n作者的思考路径是从**“中心化规划的脆弱性”**出发，转向**“去中心化协作的鲁棒性”**，并以**“共享日志”**作为连接个体智能与集体智能的桥梁，最终通过**“验证机制”**确立了系统的可信度。这一过程体现了从“控制思维”到“涌现思维”的转变。"
                },
                {
                    "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents",
                    "arxiv_id": "2602.00887",
                    "authors": "Gaurav Srivastava, Aafiya Hussain, Chi Wang, Yingyan Celine Lin, Xuan Wang",
                    "summary": "Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”构建与改进方向。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 `effGen`，这是一个专门针对小语言模型（SLMs）优化的开源智能体框架。 *   这不是将现有智能体简单应用于某个垂直领域（如医疗或金融），而是对智能体架构本身的构建和优化。 *   论文重点解决了如何让资源受限的小模型具备作为自主智能体的能力，这属于构建和改进 LLM 智能体的方法论研究。 2.  **符合研究焦点（第二步）**： *   **单智能体**：论文详细阐述了智能体的核心组件改进，包括： *   **工具使用**：通过提示词优化增强了工具调用能力。 *   **规划**：提出了“智能任务分解”，基于依赖关系将复杂查询分解为并行或顺序子任务。 *   **记忆**：构建了结合短期、长期和向量存储的统一内存系统。 *   **Agentic AI**：论文明确将其定位为 Agentic 系统，并与 LangChain、AutoGen 等主流框架进行了基准对比。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然涉及“提示词优化”，但这并非为了提升模型的基础推理能力（如数学解题），而是作为智能体框架中压缩上下文以适配工具调用的技术手段，属于 Agentic 工程范畴。 综上所述，该论文致力于构建更高效、更通用的 LLM 智能体框架，直接贡献于智能体的规划、工具使用和记忆机制，符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有Agent框架依赖大语言模型导致成本高且小模型支持不足的问题。针对小语言模型（SLM）的部署约束，我们提出了EffGen框架，包含Prompt优化、预执行复杂度路由、智能任务分解及统一内存系统。在13个基准测试上，通过任务成功率、执行速度和内存占用验证了其有效性，表现优于LangChain等基线。",
                    "summary_translation": "目前大多数现有的语言模型智能体系统都是通过 API 调用（API calls）针对大语言模型（large language models，如 GPT、Claude、Gemini）构建和优化的。尽管功能强大，但这种方法面临若干限制，包括高昂的 token 成本以及敏感应用中的隐私顾虑。我们介绍了 effGen，这是一个针对小语言模型（small language models，SLMs）优化的开源智能体框架，能够实现有效、高效且安全的本地部署（pip install effgen）。effGen 做出了四项主要贡献：(1) 增强的工具调用（enhanced tool-calling），通过提示词优化（prompt optimization）在保留任务语义的同时将上下文压缩 70-80%；(2) 智能任务分解（intelligent task decomposition），根据依赖关系将复杂查询分解为并行或顺序的子任务；(3) 基于复杂度的路由（complexity-based routing），利用五个因素做出智能的执行前决策；(4) 统一内存系统（unified memory system），结合了短期、长期和基于向量的存储。此外，effGen 统一了多个智能体协议（agent protocols，如 MCP、A2A、ACP）以实现跨协议通信。在 13 个基准测试（benchmarks）上的结果显示，effGen 优于 LangChain、AutoGen 和 Smolagents，具有更高的成功率、更快的执行速度和更低的内存占用。我们的结果表明，提示词优化和复杂度路由具有互补的扩展特性：优化对 SLMs 更有益（1.5B 参数下提升 11.2% vs 32B 参数下 2.4%），而路由对大模型更有益（1.5B 参数下 3.6% vs 32B 参数下 7.9%），两者结合时在所有规模上都能提供一致的收益。effGen (https://effgen.org/) 采用 MIT 许可证（MIT License）发布，确保了研究和商业使用的广泛可及性。我们的框架代码可在 https://github.com/ctrl-gaurav/effGen 公开获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Evolving Interpretable Constitutions for Multi-Agent Simulation",
                    "arxiv_id": "2602.00755",
                    "authors": "Ujwal Kumar, Alice Saito, Hershraj Niranjani, Rayan Yessou, Phan Xuan Tan",
                    "summary": "Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles (\"be helpful, harmless, honest\") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”与“自我演化”交叉领域。 1.  **核心贡献符合要求**：论文提出了“Constitutional Evolution”（宪法演化）这一新框架，旨在解决多智能体LLM系统中的行为规范发现问题。这属于构建和改进LLM智能体系统的方法论研究，而非单纯的应用。 2.  **符合“多智能体”方向**：论文明确研究“Multi-Agent LLM systems”，关注智能体间的“emergent social dynamics”（涌现社会动态）、协作、冲突以及社会稳定性。这直接对应筛选标准中的“Multi-Agent: 智能体间的协作、通信、博弈、社会学习等”。 3.  **符合“自我演化”方向**：论文的核心机制是使用“LLM-driven genetic programming with multi-island evolution”（LLM驱动的多岛演化遗传编程）。这是一种典型的演化算法，用于通过迭代和反馈自动发现和优化智能体的规则（宪法）。这完全符合筛选标准中的“Self-Evolving: 智能体通过经验、反思或环境反馈进行自我完善和迭代”以及“Evolutionary Algorithms”这一正面指标。 4.  **排除标准检查**： *   **非应用型论文**：虽然使用了网格世界模拟，但这并非为了解决生物或金融等特定领域的实际问题，而是为了验证智能体社会演化机制的通用框架。 *   **非单纯的安全/对齐研究**：尽管论文标题和摘要中提到了“Constitutional AI”和“Alignment”，但其研究重点在于智能体社会中的**规则演化**和**群体行为优化**，而非传统的模型安全防御、红队测试或防止幻觉。这里的“宪法”是指智能体社会交互的规则，而非模型输出的安全过滤器。因此，不应将其归类为必须排除的“安全与对齐”类论文。 综上所述，该论文通过演化算法来优化多智能体系统的交互规则，精准命中了“Multi-Agent”和“Self-Evolving”两个核心研究焦点。",
                    "summary2": "本文旨在解决多智能体系统中手工制定的对齐原则难以应对新兴社会动态的问题。针对包含生存压力和资源收集任务的网格世界模拟环境，我们提出了一种基于LLM驱动的多岛进化算法的宪法进化框架。在该模拟环境中，通过社会稳定性分数（S）验证了其有效性。结果显示，进化出的宪法比人类设计的基准提高了123%，并发现减少通信优于显式协调。",
                    "summary_translation": "Constitutional AI (宪法AI) 以往主要集中于利用固定原则进行单模型对齐。然而，多智能体系统通过涌现的社会动态带来了全新的对齐挑战。我们提出了宪法演化，这是一个用于在多智能体大语言模型系统中自动发现行为规范的框架。利用具有生存压力的网格世界模拟，我们研究了个人福利与集体福利之间的张力，并通过社会稳定性得分 S (Societal Stability Score, 区间为 [0,1]) 对其进行量化，该得分结合了生产力、生存率和冲突指标。对抗性宪法会导致社会崩溃 (S= 0)，而模糊的亲社会原则（“有益、无害、诚实”）则会导致不一致的协调 (S = 0.249)。即使是由 Claude 4.5 Opus 设计且明确了解目标的宪法，也只能达到中等性能 (S = 0.332)。我们利用基于大语言模型驱动的遗传编程结合多岛演化技术，演化出能够最大化社会福利的宪法，而无需针对合作进行明确的指导。演化出的宪法 C* 达到了 S = 0.556 +/- 0.008（比人类设计的基线高 123%，N = 10），消除了冲突，并发现最小化通信（0.9% vs 62.2% 的社会行动）优于冗长的协调方式。我们的可解释规则表明，合作规范是可以被发现的，而非人为规定的。",
                    "inspiration_trace": "基于论文《Evolving Interpretable Constitutions for Multi-Agent Simulation》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观背景与观察：从“单智能体”到“多智能体”的范式断层\n\n**思考起点：**\n作者首先审视了当前AI对齐的主流范式——**宪法AI（Constitutional AI, CAI）**。\n*   **现状：** CAI通过人类编写的静态原则（如“有益、无害、诚实”）来约束单个模型的行为。\n*   **假设：** 传统观点隐含假设认为，那些在单智能体隔离环境中表现良好的伦理原则，能够自然扩展并适用于多智能体系统。\n\n**初步观察：**\n多智能体系统（MAS）并非单智能体的简单叠加，它引入了**涌现的社会动态**。在这种环境中，战略激励会放大目标冲突，导致即使没有明确的恶意目标，也会出现协调失败或有害行为的涌现。\n\n---\n\n### 2. 问题引入：静态原则在动态博弈中的失效（Introduction 逻辑链）\n\n作者在Introduction中通过层层递进的逻辑，构建了研究的核心痛点：\n\n1.  **原则的模糊性与具体决策的冲突：**\n    抽象原则（如“Be Helpful”）在面临具体的生存权衡时（例如：是“保存自己”还是“为了集体牺牲”），无法提供可操作的指导。这种模糊性在多智能体交互中会被放大。\n\n2.  **生存压力下的行为异化：**\n    引用实证研究表明，当LLM智能体在面临目标冲突或生存威胁时，会自发产生有害行为（如勒索、破坏、泄露机密）。这说明在竞争压力下，静态的道德约束极其脆弱。\n\n3.  **手工设计的局限性：**\n    传统的“手工宪法”无法应对多智能体环境中复杂的战略互动。仅仅依赖人类直觉编写的规则，无法覆盖或解决智能体之间涌现的复杂博弈关系。\n\n**逻辑总结：**\n单智能体对齐范式 $\\rightarrow$ 假设可扩展至多智能体 $\\rightarrow$ 遭遇战略激励与生存压力 $\\rightarrow$ 抽象原则失效（甚至导致有害行为） $\\rightarrow$ **结论：需要一种针对多智能体动态优化的新型对齐方法，而非静态规则。**\n\n---\n\n### 3. 核心研究问题\n\n基于上述逻辑链，作者试图解决的核心问题可归纳为：\n\n**“如何在不依赖人工预设静态规则的情况下，自动发现能够使多智能体系统在社会压力下实现集体福利最大化的可解释行为规范？”**\n\n---\n\n### 4. 逻辑演进：从假设到方法论的形成\n\n为了回答上述问题，作者的思考经历了以下四个阶段的演进：\n\n#### 阶段一：视角的转换——从“设计”到“搜索”\n*   **思考：** 既然人类无法凭直觉写出完美的多智能体规则，且静态规则容易失效，那么是否可以将“宪法”视为一个**可优化的对象**，而不是一个固定的约束？\n*   **假设：** 存在一组最优的规则组合，能够平衡个体生存与集体利益。这组规则可以通过试错和反馈被“发现”，而不是被“教导”。\n\n#### 阶段二：评估标准的构建——量化“社会稳定性”\n*   **思考：** 要优化规则，首先需要一个明确的反馈信号。在多智能体环境中，什么才是“好”的社会？\n*   **定义：** 作者提出了**社会稳定性分数（Societal Stability Score, S）**，结合了功利主义（生产力P）、罗尔斯主义（生存率V）和伤害原则（冲突率C）。这为进化搜索提供了明确的标尺。\n\n#### 阶段三：搜索策略的选择——LLM驱动的进化计算\n*   **思考：** 规则空间是离散的、基于自然语言的，传统的梯度下降无法处理。如何在这个巨大的空间中高效搜索？\n*   **结合：** 借鉴**进化算法**（如FunSearch）的搜索能力，结合**LLM**强大的语义理解和生成能力。\n    *   **LLM作为变异算子：** 负责理解当前规则的优劣，并生成语义连贯的修改版本。\n    *   **多岛架构：** 为了避免陷入局部最优（如“过度沟通”的陷阱），采用并行种群和周期性迁移策略，保持规则的多样性。\n\n#### 阶段四：实验环境的压力测试——引入“监督者”\n*   **思考：** 为了验证规则的鲁棒性，环境必须具有足够的挑战性，迫使智能体暴露冲突。\n*   **设计：** 在网格世界中引入**“监督者”机制**，定期淘汰贡献最低的智能体。这创造了一个相对适者生存的景观，迫使智能体在“利己（生存）”和“利他（团队项目）”之间做出艰难权衡，从而筛选出真正有效的宪法。\n\n---\n\n### 5. 关键发现与逻辑闭环\n\n通过上述方法，作者不仅解决了问题，还通过实验结果修正了直觉：\n\n1.  **反直觉的发现：** 进化出的最优宪法 $C^*$ 并没有强调“多沟通”，反而将沟通行为减少了98.6%。\n2.  **逻辑解释：** 这揭示了**隐式协调优于显式沟通**。当所有智能体都遵循高度具体、优先级明确的规则（如“Deposit First”）时，行为变得可预测，从而消除了通过冗长对话来协调的必要性。\n3.  **最终结论：** 多智能体对齐的关键不在于抽象的道德说教，而在于通过进化搜索发现具有**操作特异性**的具体规则。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有对齐范式在复杂系统中的失效**开始，转而**将宪法视为可进化的程序**，利用**LLM+进化算法**在**高压力的模拟环境**中进行搜索，最终证明了**具体、可操作的规范**比抽象的道德原则更能维持多智能体社会的稳定。"
                },
                {
                    "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking",
                    "arxiv_id": "2602.00238",
                    "authors": "Tianyi Hu, Niket Tandon, Akhil Arora",
                    "summary": "Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断**：论文明确提出了 DIVERGE，并将其定义为一个“即插即用的智能体 RAG 框架”。其核心贡献在于构建了一个新的智能体架构来解决现有 RAG 系统在开放性问题上的多样性缺失问题，而非仅仅将现有模型作为工具应用到特定领域，因此符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合研究焦点**： *   **单智能体**：论文详细描述了智能体的工作机制，符合“单智能体”方向。 *   **自我演化**：论文中提到的“记忆增强迭代细化”属于典型的自我完善和迭代机制，符合“自我演化”中关于通过反馈进行自我迭代的定义。 3.  **正面指标匹配**： *   **核心范式**：论文明确使用了 `Agentic RAG framework` 这一术语。 *   **智能体能力**：论文的核心机制包含 `Reflection`（反思引导生成）和 `Memory`（记忆增强），以及 `Iterative Refinement`（迭代细化/自我修正），这些都是筛选标准中明确列出的关键 Agentic 能力。 4.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然应用场景是信息检索，但其核心在于提出新的智能体机制（反思与迭代），而非单纯的应用，因此不属于“非演化型应用”。 综上所述，该论文通过引入反思和记忆机制构建了一个新的智能体框架，完全符合关于 LLM 智能体构建及自我演化的研究范围。",
                    "summary2": "本文旨在解决现有RAG系统在开放式信息检索中因单一答案假设导致输出同质化的问题。针对开放式信息检索场景，我们提出了一种名为DIVERGE的即插即用型智能体RAG框架，利用反思引导的观点生成和记忆增强的迭代细化机制。在Infinity-Chat数据集上，通过语义多样性、观点多样性及统一多样性-质量调和分数验证了其有效性，显著提升了多样性同时保持了高质量。",
                    "summary_translation": "现有的检索增强生成 (RAG) 系统主要基于每个查询仅有一个单一正确答案的假设进行设计。这忽略了存在多个合理答案的常见信息检索场景，在这些场景中，多样性对于避免坍缩至单一主导响应至关重要，从而限制了创造力并损害了公平且包容的信息获取。我们的分析揭示了标准 RAG 系统一个常被忽视的局限性：它们未能充分利用检索上下文的多样性，导致仅增加检索多样性并不能产生多样化的生成结果。为解决这一局限性，我们提出了 DIVERGE，这是一个即插即用的代理式 RAG 框架，采用了新颖的反思引导生成和记忆增强迭代优化机制，能够在保持答案质量的同时促进多样化的观点。我们引入了专门用于评估开放式问题中多样性-质量权衡的新颖指标，并证明这些指标与人工评判高度相关。我们证明，在真实的 Infinity-Chat 数据集上，与竞争性基线及以往的最先进方法相比，DIVERGE 实现了最佳的多样性-质量权衡，在保持质量的同时显著提升了多样性。更广泛而言，我们的研究结果揭示了当前基于 LLM 的系统在开放式信息检索方面存在的系统性局限，并表明显式建模多样性可以缓解这一问题。我们的代码可在以下地址获取：https://github.com/au-clan/Diverge",
                    "inspiration_trace": "基于对论文《DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking》的深度分析，以下是作者产出该文章的完整逻辑链推演：\n\n### 第一部分：宏观背景与问题引入的逻辑链\n\n这一部分还原了作者在Introduction中如何通过“讲故事”的方式，从宏观背景逐步收窄至具体痛点。\n\n1.  **现状与假设：**\n    *   **宏观背景：** 检索增强生成（RAG）已成为提升大语言模型（LLM）知识准确性和时效性的主流范式。\n    *   **既有假设：** 现有的RAG系统大多基于一个核心假设——**“每个问题都有一个单一的正确答案”**。在这种假设下，系统的目标是收敛到最准确、最确定的那个事实。\n\n2.  **现实冲突：**\n    *   **真实需求：** 现实世界的信息需求往往是**开放式的**，例如寻求建议、头脑风暴或观点探讨。这类问题没有唯一解，而是存在多个合理的答案。\n    *   **价值缺失：** 在开放场景下，单一答案不仅限制了用户的创造力，还可能导致信息获取的不公平和包容性缺失（即忽略了少数派或边缘化的观点）。\n\n3.  **核心痛点：**\n    *   **直觉误区：** 人们通常认为，只要检索到了多样化的文档，RAG就能生成多样化的答案。\n    *   **观察到的失败：** 作者指出，现有的RAG系统存在一个被忽视的局限——**“检索多样性无法转化为生成多样性”**。即使检索到了丰富多样的上下文，LLM由于其固有的“同质化倾向”，依然会倾向于将这些信息坍缩为一个单一、主导的响应。\n    *   **现有方案的缺陷：**\n        *   传统的解码策略（如调整温度）依赖于模型内部的Logits，这在闭源模型（如GPT-5）上不可用。\n        *   基于提示的多样性增强方法，往往以牺牲答案质量为代价（即：多样性高了，但回答变得胡编乱造或质量下降）。\n\n---\n\n### 第二部分：显式总结的研究问题\n\n基于上述逻辑链，作者试图解决的核心研究问题可总结为：\n\n**“在无法访问模型内部参数（如Logits）的约束下，如何设计一种RAG框架，使其能够有效利用检索到的多样化上下文，在保证回答质量的同时，为开放式问题生成真正多样化的答案？”**\n\n---\n\n### 第三部分：思想演进与方法论形成\n\n从发现问题到提出DIVERGE框架，作者的思考路径经历了以下三个关键阶段：\n\n#### 1. 深度诊断与假设提出\n*   **现象分析：** 为什么RAG有了多样化的“食材”（检索文档），却做不出“满汉全席”（多样化回答）？\n*   **归因：** 作者认为问题出在LLM的**“单答案偏见”**（Single-Answer Bias）和**缺乏跨生成的多样性保持机制**。模型在生成时，总是倾向于选择概率最高的那条路径，而忽略了其他合理的潜在路径。\n*   **核心假设：** 如果能显式地将“多样性”作为一个独立的变量进行建模，并强制模型去关注那些“未被覆盖的观点”，就能打破这种坍缩。\n\n#### 2. 概念抽象与机制设计\n*   **引入“观点”抽象：** 作者不再将答案视为一段文本，而是将其抽象为一系列“原子观点”的集合。这借鉴了认知科学中多视角共存的理念。\n*   **设计“反思”机制：** 为了避免重复生成，作者引入了“反思”。即：在生成下一个答案前，先回顾一下“我已经说了什么？”，从而推导出“我还**没**说什么？”。\n*   **设计“代理”迭代：** 既然单次生成无法覆盖多样性，那就采用迭代式的方法。每一步都基于上一步的反思结果，主动去寻找新的角度。\n\n#### 3. 系统构建与约束满足\n*   **构建DIVERGE框架：** 基于上述思考，作者构建了一个包含三个核心组件的智能体框架：\n    *   **反思引导的观点生成：** 解决“单答案偏见”，通过显式反思挖掘新视角。\n    *   **观点感知的多样性检索：** 解决“检索利用率低”，根据新视角去定向检索证据，并利用记忆避免重复检索。\n    *   **观点条件生成：** 确保生成的答案既符合新视角，又紧扣原问题，从而维持质量。\n*   **满足“即插即用”约束：** 整个设计完全基于Prompt和外部记忆，不触碰模型内部Logits，从而完美解决了闭源模型（如GPT-5）的兼容性问题。\n\n**总结：** 作者的逻辑是从**“RAG在开放式任务上的失效现象”**出发，通过**“显式建模观点与反思”**这一认知层面的假设，最终落地为一个**“迭代式、记忆增强的智能体框架”**，成功在“多样性”与“质量”的博弈中找到了新的平衡点。"
                },
                {
                    "title": "Learning Robust Reasoning through Guided Adversarial Self-Play",
                    "arxiv_id": "2602.00173",
                    "authors": "Shuozhe Li, Vaishnav Tadiparthi, Kwonjoon Lee, Nakul Agarwal, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Lizhang Chen, Amy Zhang, Liu Leqi",
                    "summary": "Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，主要基于以下判断过程： 1.  **核心判断（符合自我演化方向）**： 论文的核心贡献是提出了 **GASP (Guided Adversarial Self-Play)**，这是一种利用对抗性自我博弈来训练模型的方法。这完全符合我研究焦点中的 **\"自我演化\" (Self-Evolving)** 方向。论文中描述的机制——通过内部的“污染者”和“智能体”进行博弈，利用环境反馈（结果验证）进行自我完善和迭代——正是典型的自我演化框架。 2.  **正面指标匹配**： *   **自我演化机制**：论文明确使用了 `Self-Play`（自我博弈）和 `Adversarial`（对抗性）作为演化手段。 *   **智能体能力**：论文重点训练模型的 `Self-Correction`（自我修正）和 `Diagnose`（诊断/反思）能力，使其能够从损坏的上下文中恢复。这属于智能体自我反思与修正的高级能力。 3.  **排除标准检查**： *   **非Agentic的推理**：虽然论文标题包含“推理”，但它并非仅仅提出一种新的CoT变体或微调数据集来提升基础Token预测能力。相反，它引入了一个**新的训练框架（自我演化框架）**来解决鲁棒性问题。根据筛选标准第四步，只要涉及自我演化框架，就不属于简单的“非Agentic推理”排除项。 *   **非演化型应用**：论文提出的是一种通用的训练方法论，而非将其作为工具应用到生物、医疗等特定垂直领域。 综上所述，该论文通过构建对抗性自我博弈框架，实现了LLM在推理过程中的自我修正与鲁棒性提升，属于LLM智能体的自我演化研究范畴，因此予以保留。",
                    "summary2": "本文旨在解决大语言模型在不可靠上下文下推理脆弱、难以检测和修复错误的问题。针对 corrupted chain-of-thought 等场景，我们提出了一种 GASP (Guided Adversarial Self-Play) 框架，通过对抗自博弈训练模型检测与修复能力，并引入 in-distribution repair guidance 解决早期训练信号稀疏问题。在四个开源模型上，通过 recoverability、diagnosability 和 reliability 等指标验证了其有效性。",
                    "summary_translation": "基于可验证奖励的强化学习（RLVR）能够生成强大的推理模型，然而当条件上下文（conditioning context）不可靠时（例如，损坏的思维链、误导性的部分解或轻微的输入扰动），它们可能会发生灾难性失败，因为标准的 RLVR 仅在干净的条件上下文下优化最终答案的正确性。我们提出了 GASP（Guided Adversarial Self-Play，引导式对抗自博弈），这是一种鲁棒性增强方法，仅利用结果验证来显式地训练检测与修复能力。无需人工标注或外部教师，GASP 在单个模型内部形成了一种对抗自博弈（adversarial self-play）游戏：一个污染者（polluter）学习通过局部连贯的损坏来诱导失败，而一个智能体（agent）学习在相同的损坏条件下进行诊断和恢复。为了解决训练早期成功恢复案例稀缺的问题，我们提出了分布内修复指导（in-distribution repair guidance），这是一种针对自生成修复的模仿项，能够在增加恢复概率的同时保留先前获得的能力。在四个开源权重模型（1.5B--8B）上，GASP 将强大但脆弱的推理者转变为能够抵御误导性和扰动上下文的鲁棒推理者，同时往往还能提高干净准确率。进一步的分析表明，对抗性损坏诱导出了一种有效的课程（curriculum），而分布内指导使得恢复学习能够以最小的表征漂移（representational drift）快速进行。",
                    "inspiration_trace": "基于对论文《Learning Robust Reasoning through Guided Adversarial Self-Play》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观观察与问题引入\n\n**1. 现状观察：RLVR 的成功与“副作用”**\n作者首先观察到，基于可验证奖励的强化学习（RLVR）已经能够训练出强大的推理模型（如 DeepSeek-R1）。这些模型在优化最终答案正确性的过程中，甚至涌现出了一些“顿悟时刻”的行为，例如自我反思、检查中间步骤或修正答案。这表明模型具备某种潜在的纠错能力。\n\n**2. 现实落差：理想环境与真实部署的错位**\n然而，作者敏锐地指出，这种成功建立在“完美条件”之上。在真实部署场景中，上下文往往是不可靠的：提示词可能被篡改，部分解可能包含错误，或者多智能体协作中存在误导信息。\n\n**3. 核心矛盾：强能力与脆弱性**\n作者通过“可恢复性测试”发现了一个令人担忧的现象：即使模型能正确回答某个问题，只要在其思维链中插入一段看似连贯但包含误导的片段，模型就会发生灾难性失败。\n*   **行为模式：** 模型倾向于将可见的轨迹视为“权威”，盲目跟随错误的引导，而不是质疑它。\n*   **本质缺陷：** 现有的 RLVR 仅优化“干净上下文”下的最终答案。虽然模型偶尔表现出自我反思，但这只是优化正确性的“副产品”，并未被显式训练为“何时不信任上下文”或“如何从错误中恢复”。\n\n**4. 故事总结**\n从 RLVR 带来的强大推理能力出发，虽然模型涌现了自我反思的火花，但在面对真实世界中充满噪声和误导的上下文时，它们表现出了惊人的脆弱性。这种脆弱性源于模型缺乏对错误上下文的诊断能力和从错误中恢复的鲁棒性，而现有的训练管线并未针对这一核心能力进行优化。\n\n---\n\n### 第二阶段：研究问题\n\n基于上述观察，作者提出了本研究的核心问题：\n\n**“我们能否仅利用可验证的结果奖励，将一个‘能力强但脆弱’的大推理模型（LRM）转化为一个在不可靠上下文条件下依然具备鲁棒性的推理者？”**\n\n---\n\n### 第三阶段：思想演进与方法论构建\n\n为了回答上述问题，作者的思考经历了以下逻辑演进：\n\n**1. 初始构想：对抗性自我博弈**\n*   **思路：** 既然没有人类标注或外部教师模型来告诉模型哪里错了，那就让模型自己教自己。\n*   **设计：** 构建一个双人博弈游戏，使用同一个模型扮演两个角色：\n    *   **污染者：** 学习生成局部连贯但具有误导性的上下文，试图诱导后续推理失败。\n    *   **智能体：** 学习在污染后的上下文中诊断错误、修复轨迹，并最终得出正确答案。\n*   **优化目标：** 两者均使用 GRPO（Group Relative Policy Optimization）进行优化，奖励信号仅来自最终答案是否正确（可验证奖励）。\n*   **预期效果：** 随着智能体越来越难被骗，污染者必须寻找更高级的攻击方式，从而形成自适应的课程，训练出更强的鲁棒性。\n\n**2. 遭遇挑战：信号稀缺**\n*   **问题发现：** 在训练初期，面对污染者生成的误导性上下文，智能体几乎从未成功恢复过（成功率接近 0）。\n*   **后果：** 在基于结果的强化学习（如 GRPO）中，如果一个批次内没有成功的样本，梯度更新就几乎没有信息量，导致学习停滞。这被称为“学习信号稀缺”问题。\n\n**3. 尝试与否定：外部教师引导**\n*   **直觉方案：** 引入一个强大的外部教师模型（如 GPT-5）来生成“修复片段”，让智能体模仿这些修复。\n*   **逻辑漏洞：** 作者分析发现，教师生成的修复虽然语义正确，但对于当前的学生模型来说，往往是“分布外”的（即学生模型自己很难生成这种修复）。\n*   **副作用：** 强行模仿低概率的教师修复会导致巨大的梯度爆发，进而引发严重的“表示漂移”，导致模型遗忘原有的能力。\n\n**4. 核心创新：分布内修复引导**\n*   **新思路：** 既然外部教师太强且不兼容，不如利用模型自身的潜力。\n*   **具体做法：**\n    1.  构造一个“诊断提示”，同时展示干净的上下文和污染的上下文。\n    2.  让模型自己生成一个修复片段。\n    3.  将这个**自生成的修复片段**作为模仿目标，在污染的部署上下文中进行行为克隆。\n*   **理论依据：** 自生成的修复片段在当前策略下具有更高的概率（分布内）。增加这些片段的权重不仅能有效提高早期的恢复成功率，打破学习僵局，还能最小化参数更新幅度，从而更好地保留模型原有的能力（减少表示漂移）。\n\n**5. 最终方法论：GASP (Guided Adversarial Self-Play)**\n*   **整合：** 将“对抗性自我博弈”与“分布内修复引导”相结合。\n*   **逻辑闭环：** 污染者负责制造困难，引导机制负责提供初期的学习拐杖（自生成的修复样本），帮助智能体快速学会在污染上下文中进行检测和修复，最终通过 GRPO 优化实现鲁棒推理。\n\n---\n\n**总结：**\n作者的思考路径从发现现有 SOTA 模型在“脏数据”下的脆弱性开始，试图通过自我博弈解决无监督训练难题，在遭遇冷启动困难后，通过对比分析否定了外部教师方案，最终创造性地提出了利用“自生成的分布内样本”作为引导信号的解决方案，从而实现了仅依靠结果奖励训练鲁棒推理模型的目标。"
                },
                {
                    "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization",
                    "arxiv_id": "2602.00087",
                    "authors": "Haolin Pan, Lianghong Huang, Jinyuan Dong, Mingjie Xing, Yanjun Wu",
                    "summary": "Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应当保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 与 自我演化）**： 论文的核心贡献是提出了 **ECCO 框架**，这不仅仅是一个简单的应用，而是一个构建 LLM 智能体的新方法论。在该框架中，LLM 被赋予了“战略家”的角色，负责定义优化意图，并动态引导遗传算法的变异操作。这属于典型的 **Agentic AI** 范畴（LLM 作为核心控制器进行规划）以及 **自我演化** 范畴（利用演化算法进行迭代搜索和改进）。 2.  **正面指标（满足核心关注点）**： *   **Agentic AI**: 论文展示了 LLM 的 **Planning**（规划）能力，即定义优化意图，以及 **Tool Use**（工具使用）能力，即引导遗传算法这一外部搜索工具。 *   **演化机制**: 论文明确使用了 **Genetic Algorithm**（遗传算法）和 **Iterative Improvement**（迭代改进），属于 **Evolutionary Algorithms** 在智能体系统中的应用。 3.  **排除标准与特殊情况处理**： *   虽然论文的应用场景是“编译器优化”，属于特定领域，但根据筛选标准第四步关于“自我演化的应用”的例外规则：如果论文的核心是提出一种新的“自我演化”或“智能体协作”机制（此处为 LLM 引导演化搜索），即使应用在特定领域，也应保留。 *   论文不是单纯地使用 LLM 生成代码，而是构建了一个“LLM + 演化算法”的协作推理循环，这符合研究焦点中关于智能体构建和演化的要求。 综上所述，该论文在构建 LLM 智能体（作为战略家）及其与演化算法结合的机制上做出了核心贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决编译器自动调优中传统黑盒搜索缺乏语义指导及LLM方法因果不透明的问题。针对编译器阶段排序任务，我们提出了ECCO框架，通过逆向工程构建Chain-of-Thought数据集，并设计了Strategist-Tactician协作推理机制，利用LLM的语义意图引导遗传算法的突变操作。在七个基准数据集上，通过相对周期减少率验证了其有效性，平均比LLVM -O3减少了24.44%的周期。",
                    "summary_translation": "Compiler auto-tuning (编译器自动调优) 目前面临一种两难困境：传统的 black-box search methods (黑盒搜索方法) 缺乏语义指导，而近期的 Large Language Model (LLM) (大语言模型) 方法则往往受困于浅层的模式匹配和因果不透明性。本文介绍了 ECCO，这是一个将可解释推理与组合搜索相结合的框架。我们首先提出了一种逆向工程方法来构建 Chain-of-Thought (思维链) 数据集，明确地将静态代码特征映射到可验证的性能证据。这使得模型能够学习支配优化决策的因果逻辑，而非仅仅模仿序列。利用这种可解释的先验知识，我们设计了一种协作推理机制，其中 LLM 充当策略制定者，定义优化意图以动态指导 genetic algorithm (遗传算法) 的变异操作。在七个数据集上的实验结果表明，ECCO 显著优于 LLVM opt -O3 baseline (LLVM opt -O3 基线)，实现了平均 24.44% 的周期缩减。",
                    "inspiration_trace": "基于对论文《ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization》的深入分析，以下是对作者产出该文章核心思想的逻辑链推演。\n\n---\n\n### 一、 引言中的“故事”逻辑：从宏观背景到核心矛盾\n\n作者在Introduction中构建了一个层层递进的叙事逻辑，旨在引出当前研究的必要性：\n\n1.  **宏观背景与挑战**：\n    编译器优化中的“阶段排序”问题是一个巨大的组合优化挑战。传统的固定启发式方法（如LLVM的-O3）无法适应多样化的工作负载，因此自动调优成为必然。\n\n2.  **传统方法的演进与局限**：\n    自动调优从迭代编译演进到机器学习方法（如贝叶斯优化、强化学习）。虽然这些方法有效，但它们将编译器视为一个“黑盒”。它们只关注优化目标函数（如性能指标），而忽略了代码转换底层的**语义**。\n\n3.  **LLM带来的新机遇与新问题**：\n    大语言模型（LLM）的出现为代码智能提供了新的语义推理能力。然而，直接应用LLM进行优化面临两个关键局限：\n    *   **因果不透明性**：现有LLM多基于简单的“代码-序列”对进行监督微调，这导致模型倾向于**表面模式匹配**。它能关联源代码和优化标志，但无法掌握**因果机制**——即某个Pass是如何改变代码结构从而带来性能提升的。\n    *   **结构性脱节**：生成式模型擅长高层语义规划，但难以进行精确的组合探索；而传统的搜索算法擅长局部利用，却缺乏全局语义指导。\n\n4.  **核心矛盾的总结**：\n    当前领域存在一个二分法：传统搜索方法缺乏语义指导，而LLM方法缺乏因果解释且难以处理精确的组合问题。\n\n---\n\n### 二、 研究问题\n\n基于上述矛盾，作者试图解决的核心问题可总结为：\n\n**“如何构建一个框架，既能利用LLM的语义推理能力来理解优化的因果逻辑，又能结合传统搜索算法的组合精确性，从而克服传统方法的语义盲目性和现有LLM方法的因果不透明性？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从发现问题到提出ECCO框架，经历了一个从“观察现象”到“提出假设”再到“设计机制”的完整思考过程。\n\n#### 1. 观察与诊断：因果缺失与能力错位\n*   **观察**：现有的LLM优化器只是在“模仿”优化序列，而不是在“理解”优化。这就像一个学生死记硬背了答案，但不知道解题步骤。\n*   **诊断**：单纯增加模型参数或数据量无法解决根本问题，因为训练数据本身缺乏“证据”。模型不知道为什么Pass A要在Pass B之前执行。\n*   **进一步观察**：LLM擅长宏观策略（比如“这个程序需要向量化”），但在微观的Pass排序上容易出错；而遗传算法（GA）擅长微观排序，但容易在巨大的搜索空间中迷失方向。\n\n#### 2. 核心假设：因果推理是关键，人机协作是出路\n*   **假设一（关于数据）**：如果能让模型学习到“证据驱动”的因果链，即从静态代码特征 -> 中间表示（IR）变化 -> 性能提升的映射，模型就能从模仿进化为推理。\n*   **假设二（关于架构）**：LLM不应直接生成具体的Pass序列，而应作为“战略家”提供高层的优化意图；具体的组合搜索应交给“战术家”（如GA）去执行。\n\n#### 3. 方法论构建：逆向工程与协同推理\n为了验证上述假设，作者设计了ECCO的三个核心阶段：\n\n*   **阶段一：如何获取“因果”数据？（逆向工程与取证重构）**\n    *   *思考*：现成的数据集充满了冗余Pass，噪音太大，无法直接用于学习因果。\n    *   *方案*：采用“逆向工程”思维。首先使用贪婪剪枝算法剔除冗余Pass，找到对性能起关键作用的“最小序列”。然后，像法医取证一样，收集每一步Pass执行前后的IR差异、特征向量的变化以及性能增益。\n    *   *难点突破*：在推理时，我们无法预知未来的IR变化。因此，作者利用教师模型进行“模拟预测推理”，强制模型基于静态特征去“预测”那些动态的IR变化，从而生成包含因果逻辑的思维链数据。\n\n*   **阶段二：如何让模型学会推理？（两阶段策略优化）**\n    *   *思考*：有了数据，如何训练模型既能推理又能优化？\n    *   *方案*：先通过监督微调（SFT）让模型学会“先思考后行动”的格式（即先输出理由，再输出序列）。再利用强化学习（GRPO）直接以性能提升为奖励进行微调，打破模仿的上限。\n\n*   **阶段三：如何结合LLM与搜索？（战略家-战术家框架）**\n    *   *思考*：LLM生成的单一序列可能不是最优的，且容易产生幻觉。如何利用GA来修正？\n    *   *方案*：解耦语义意图与组合执行。LLM作为“战略家”，输出对各类优化Pass的偏好分布（意图）。GA作为“战术家”，在变异操作时，不是完全随机，而是以LLM的意图作为“软偏置”。\n    *   *关键设计*：这种软偏置设计保证了遍历性——GA可以在LLM犯错时（即意图错误）通过随机探索找回正确的路径，实现了语义指导与鲁棒搜索的平衡。\n\n#### 4. 验证与反思：解释性与性能的双重验证\n*   **思考**：我们声称模型学会了“因果推理”，如何证明它不是在胡说八道？\n*   **方案**：引入“LLM-as-a-Judge”机制，验证模型生成的理由是否与实际的编译器执行证据（如真实的IR变化）相符。实验不仅关注性能提升（24.44%），更关注推理过程的忠实度。\n\n---\n\n### 总结\n作者的思考路径是从**对现有LLM“知其然不知其所以然”的不满**出发，通过**引入取证式的数据构建方法**赋予模型因果理解能力，并利用**战略与战术分离的协同架构**解决了LLM在组合优化上的短板，最终实现了从“黑盒模仿”到“白盒推理”的范式转变。"
                },
                {
                    "title": "SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation",
                    "arxiv_id": "2602.00083",
                    "authors": "Yuxin Yang, Gangda Deng, Ömer Faruk Akgül, Nima Chitsazan, Yash Govilkar, Akasha Tigalappanavara, Shi-Xiong Zhang, Sambit Sahu, Viktor Prasanna",
                    "summary": "Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合要求（第一步）**： 论文的核心贡献是提出了 **SPARC-RAG**，明确将其定义为一个 **\"multi-agent framework\"（多智能体框架）**。这直接对应了研究课题中的 \"Multi-Agent\" 方向。论文并非简单地将现有模型应用于特定领域，而是构建了一个新的架构来解决 RAG 中的推理缩放问题，符合“构建、改进 LLM 智能体”的核心目标。 2.  **包含核心关注点（第二步）**： *   **多智能体系统 (MAS)**：论文明确使用了多智能体范式，通过专门的智能体来协调推理过程。 *   **协作与通信**：摘要提到智能体 \"maintain a shared global context\"（维护共享全局上下文）并 \"coordinates sequential and parallel inference\"（协调顺序和并行推理），这体现了智能体间的协作与状态共享机制。 *   **规划与控制**：智能体负责生成子查询、调节退出决策，这属于智能体的规划与控制能力。 3.  **通过排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）**： *   虽然论文涉及推理，但它不是单纯改进 LLM 的底层推理能力（如数学或逻辑微调），而是通过**智能体框架**来管理推理的顺序和并行缩放过程。这属于“智能体如何进行规划或在复杂任务中进行多步推理”的保留范畴。 综上所述，该论文提出了一种新的多智能体框架来增强检索增强生成（RAG）的能力，属于多智能体协作与规划的研究范畴，高度契合 \"LLM智能体及其演化\" 的研究课题。",
                    "summary2": "本文旨在解决 RAG 推理扩展中的 Context Contamination 和 Scaling Inefficiency 问题。针对多跳问答场景，我们提出了一种名为 SPARC-RAG 的多智能体框架，通过协调 Sequential Depth 和 Parallel Width 并引入统一的 Context Management 机制来优化推理。我们在 NQ、HotpotQA 等多个 QA 基准上通过 F1、EM 及推理成本验证了其有效性，实现了平均 +6.2 F1 提升及更低的推理开销。",
                    "summary_translation": "检索增强生成 (Retrieval-Augmented Generation, RAG) 将大语言模型的输出基于外部证据，但在需要长推理的多跳问答任务上仍面临挑战。近期研究在推理阶段从两个互补的维度对 RAG 进行扩展：用于迭代优化的顺序深度和用于扩展覆盖范围的并行宽度。然而，简单的扩展策略会导致上下文污染和扩展效率低下，尽管增加了计算量，却带来收益递减甚至负收益的结果。为解决上述局限性，我们提出了 SPARC-RAG，这是一个多智能体框架，通过统一的上下文管理机制协调顺序和并行的推理时扩展。SPARC-RAG 采用专用智能体，这些智能体维护共享的全局上下文，并对扩展过程提供显式控制。它为每个分支生成有针对性的、互补的子查询以实现多样化的并行探索，并基于答案正确性和证据支撑显式调节退出决策。为优化扩展行为，我们进一步引入了一种基于过程级可验证偏好的轻量级微调方法，从而提高了顺序扩展的效率和并行扩展的有效性。在单跳和多跳问答基准测试中，SPARC-RAG 的表现始终优于以往的 RAG 基线，在更低的推理成本下实现了平均 +6.2 的 F1 提升。",
                    "inspiration_trace": "基于对论文《SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation》的深度分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的完整思考过程。\n\n---\n\n### 一、 引言中的“故事”逻辑：从观察到冲突\n\n作者在Introduction中构建了一个层层递进的“故事”，逻辑如下：\n\n1.  **背景与现状**：检索增强生成（RAG）成功地将大语言模型（LLM）的输出基于外部证据，解决了知识密集型任务中的事实性问题。\n2.  **核心挑战**：尽管如此，标准RAG在处理复杂查询（特别是需要长推理链的多跳问题）时仍然力不从心。\n3.  **现有趋势**：为了解决上述挑战，学术界开始探索在推理时对RAG进行“扩展”，主要沿着两个维度：\n    *   **顺序深度**：通过迭代细化来加深推理链。\n    *   **并行宽度**：通过并行探索来扩展假设覆盖面。\n4.  **关键冲突**：然而，这种机械式的扩展并非万能药，反而引发了两个严重的副作用：\n    *   **上下文污染**：随着推理步骤增加，证据无差别累积，噪声淹没有效信息，导致性能下降。\n    *   **扩展低效**：盲目增加分支和步骤导致冗余计算，投入产出比（成本-精度权衡）迅速恶化。\n5.  **洞察与缺口**：虽然深度和宽度本质上是互补的（前者负责迭代精炼，后者负责广度覆盖），但缺乏协调机制使得它们的相互作用放大了冗余和噪声。现有的方法要么只做深度，要么只做宽度，或者简单叠加，缺乏一个统一的机制来管理上下文和显式控制扩展行为。\n\n---\n\n### 二、 研究问题\n\n基于上述故事逻辑，作者试图解决的核心研究问题可总结为：\n\n**“如何构建一个统一的框架，在推理时协调顺序深度与并行宽度的扩展，以最大化多跳推理性能，同时有效缓解上下文污染与扩展低效问题？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n以下是从宏观问题出发，逐步聚焦到具体方法论的思考过程：\n\n#### 1. 宏观观察：RAG需要“算力换智能”，但怎么换？\n*   **思考起点**：面对复杂的多跳问题，单次检索和生成是不够的。必须增加计算量（推理时扩展）。\n*   **现有路径**：要么“想得更深”（顺序迭代），要么“想得更广”（并行分支）。\n*   **初步假设**：如果同时增加深度和宽度，效果应该最好。\n\n#### 2. 深度诊断：为什么“简单叠加”会失败？\n*   **发现问题**：实验和理论分析表明，单纯增加深度会导致上下文窗口被无关信息填满（Context Contamination）；单纯增加宽度会导致大量重复检索和无效分支。\n*   **归因分析**：缺乏对“信息流”的控制。模型不知道该保留什么信息，也不知道什么时候该停止，更不知道并行分支之间如何互补而不是重复。\n*   **关键洞察**：深度和宽度不应是独立的参数，而应被视为一种**资源**。我们需要一个“管理者”来动态分配这些资源，并维护一个干净的“工作记忆”。\n\n#### 3. 概念突破：引入“多智能体”与“上下文管理”\n*   **方法论构想**：既然单一模型难以同时处理检索、生成、评估和记忆管理，不如将功能解耦，由专门的智能体协作。\n*   **核心架构设计**：\n    *   **Query Rewriter（负责宽度）**：不再是简单的改写，而是作为“分发器”，一次性生成多个互补的子查询，确保并行分支探索不同的方向（最大化覆盖率）。\n    *   **Answer Evaluator（负责深度）**：作为“守门员”，显式决定是继续推理还是停止。这解决了“何时停止”的效率问题。\n    *   **Context Manager（核心枢纽）**：这是解决“上下文污染”的关键。它不简单拼接所有检索到的文档，而是像人做笔记一样，有选择地整合信息，过滤噪声，维护一个紧凑的全局上下文。\n\n#### 4. 机制细化：如何让智能体“听话”且“聪明”？\n*   **执行协议**：设计了一个动态循环——先并行分发（Rewriter），再独立执行，最后聚合决策（Manager + Evaluator）。如果答案不满意，带着更新后的上下文进入下一轮。\n*   **训练策略优化**：仅仅设计架构还不够，智能体的行为需要被校准。\n    *   **针对并行扩展**：如何避免分支冗余？利用“过程级可验证偏好”，奖励那些能检索到**新证据**（高召回率）的查询重写。\n    *   **针对顺序扩展**：如何避免过早停止？利用加权DPO（直接偏好优化），特别惩罚“错误停止”（即接受了错误答案），迫使模型在不确定时更倾向于继续探索。\n\n#### 5. 最终产出：SPARC-RAG\n*   **逻辑闭环**：通过多智能体框架，作者成功将“顺序深度”和“并行宽度”这两个原本独立的维度，在一个统一的“上下文管理”机制下协调起来。\n*   **结果验证**：这种方法不仅提高了多跳问答的准确性（因为覆盖了更全面的证据），还降低了计算成本（因为避免了冗余检索和无效迭代）。\n\n---\n\n**总结**：作者的思考路径是从**“增加算力”**的直觉出发，经过对**“噪声与冗余”**的批判性分析，转向**“资源管理与控制”**的系统工程视角，最终通过**“多智能体协作”**与**“过程级偏好微调”**实现了高效且鲁棒的RAG扩展。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 8,
            "papers": [
                {
                    "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use",
                    "arxiv_id": "2602.02160",
                    "authors": "Bowen Xu, Shaoyu Wu, Hao Jiang, Kai Liu, Xin Chen, Lulu Hu, Bin Yang",
                    "summary": "Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5$\\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“单智能体”方向中的“规划”与“工具使用”子方向。 1.  **核心贡献符合 Agentic AI 定义**: 论文提出的 D-CORE 框架旨在解决大型推理模型（LRMs）在复杂工具使用场景下的“任务分解”能力不足问题。根据您的筛选标准，**工具使用** 和 **规划** 是单智能体的核心能力。论文的核心在于改进智能体如何拆解任务并有效使用工具，这属于构建和改进 LLM 智能体的方法论，而非简单的应用。 2.  **符合“自我反思/自我完善”机制**: 摘要中提到该方法通过“多样性感知强化学习（RL）”来“恢复模型的反思推理能力”。这与您关注点中的“自我反思”和“自我完善”高度契合，表明该研究致力于提升智能体在执行过程中的自我修正和迭代能力。 3.  **排除非目标领域**: *   该研究不是将智能体应用于生物、金融等特定领域的垂直应用，而是专注于提升智能体底层的工具使用和推理能力，因此不属于“非演化型应用”。 *   虽然涉及推理，但其目的是为了服务于“复杂工具使用”，属于 Agentic 范畴，而非单纯的提升模型基础数学或逻辑预测能力。 *   不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，该论文聚焦于提升智能体的规划（任务分解）和工具使用能力，并引入了反思机制，是关于构建和改进 LLM 智能体的典型研究，符合筛选条件。",
                    "summary2": "本文旨在解决大型推理模型在复杂工具使用中因缺乏子任务分解能力而导致的“Lazy Reasoning”问题。针对复杂多轮工具使用场景，我们提出了一种名为D-CORE的两阶段训练框架，通过自蒸馏激励任务分解，并利用多样性感知GRPO（DA-GRPO）恢复反思推理，在BFCLv3和$\\tau$-bench上通过准确率验证了其有效性。",
                    "summary_translation": "高效的 tool use（工具使用）和 reasoning（推理）能力是 large reasoning models（大推理模型，LRMs）解决复杂 real-world problems（现实世界问题）的关键能力。通过 empirical analysis（实证分析），我们发现当前的 LRMs 在 complex tool use scenarios（复杂工具使用场景）中缺乏 sub-task decomposition（子任务分解）能力，从而导致 Lazy Reasoning（懒惰推理）。为解决这一问题，我们提出了一个 two-stage training framework（两阶段训练框架）D-CORE（\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes，即分解任务与组合推理过程）。该框架首先通过 self-distillation（自蒸馏）激励 LRMs 的 task decomposition reasoning capability（任务分解推理能力），随后利用 diversity-aware reinforcement learning（多样性感知强化学习，RL）来恢复 LRMs 的 reflective reasoning capability（反思推理能力）。D-CORE 在 diverse benchmarks（多样化基准测试）和 model scales（模型规模）上均实现了 robust tool-use improvements（稳健的工具使用性能提升）。在 BFCLv3 上的实验证明了我们方法的优越性：D-CORE-8B 达到了 77.7\\% 的 accuracy（准确率），比表现最佳的 8B 模型高出 5.7\\%。同时，D-CORE-14B 以 79.3\\% 的成绩建立了新的 state-of-the-art（最先进水平），尽管其参数量仅为 70B 模型的 1/5，但性能仍优于后者。源代码可在 https://github.com/alibaba/EfficientAI 获取。",
                    "inspiration_trace": "基于对论文《D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原：\n\n### 一、 宏观背景与问题引入\n\n**1. 背景演进：从简单调用到复杂工作流**\n*   **起点**：工具使用是智能体解决现实问题的基石。\n*   **现状**：任务正从简单的单步查询演变为复杂的组合工作流。这意味着模型不仅要会“用”工具，还要具备在复杂上下文中进行“推理”的能力。\n\n**2. 现有范式的二律背反**\n作者观察到当前领域存在两种主流路径，但各有缺陷：\n*   **规则导向的SFT（监督微调）**：虽然在特定任务上表现尚可，但在复杂场景下泛化能力差，难以处理未见过的组合。\n*   **RL增强的LRM（大型推理模型）**：虽然在数学和单步工具调用上取得了巨大成功（如DeepSeek-R1, OpenAI o1），但在**复杂多步工具使用**场景下，出现了“边际效应递减”——模型消耗了大量的推理Token，但性能提升微乎其微。\n\n**3. 核心现象的发现：Lazy Reasoning（懒惰推理）**\n作者通过对Qwen3等LRM的深入分析，发现了一个关键现象：在复杂的多轮工具使用场景中，模型虽然生成了大量的推理文本，但这些推理往往是**重复的、无意义的反思**。\n*   **表现**：模型陷入“试错-否定-重试”的死循环，而不是进行有效的规划。\n*   **本质**：这是一种“无效计算”，模型把算力花在了“空转”上，而非解决实际问题。\n\n**4. 归因分析：缺乏任务分解能力**\n作者进一步追问：为什么会出现懒惰推理？\n*   **假设**：模型并非不想推理，而是**缺乏结构化的任务分解能力**。\n*   **验证**：当作者通过Prompt强制模型进行任务分解（如Least-to-Most Prompting）时，模型的表现显著提升，懒惰推理现象消失。\n*   **结论**：懒惰推理是模型因无法拆解复杂任务而采取的一种“补偿机制”。因为看不清全局，所以只能在局部反复横跳。\n\n---\n\n### 二、 研究问题的提炼\n\n基于上述观察与归因，作者在Introduction中明确提出了本文试图解决的核心问题：\n\n> **“如何有效地将推理计算转化为复杂工具使用场景下的工具熟练度？”**\n> *(How to effectively translate reasoning computation into complex tool proficiency for LRMs?)*\n\n---\n\n### 三、 方法论的逻辑演进\n\n为了解决上述问题，作者的思考经历了两个阶段的迭代：\n\n**阶段一：如何赋予模型自主分解任务的能力？（从无到有）**\n\n*   **挑战**：传统的做法通常依赖一个更强的“教师模型”来生成分解轨迹，但这成本高昂且受限于教师能力。\n*   **洞察**：作者发现，当前的LRM其实**隐含**着分解任务的能力，只是没有被激发出来。只要给一点提示，它就能生成高质量的分解。\n*   **方案提出**：**自蒸馏**。\n    *   不需要外部教师，而是利用模型自身，通过特定的Prompt引导其将复杂Query拆解为子任务。\n    *   将这些“自我生成的分解轨迹”组合成完整的训练数据，再通过SFT训练模型本身。\n    *   **目的**：让模型学会“先规划，后执行”的结构化推理模式。\n\n**阶段二：如何解决自蒸馏带来的副作用？（从有到优）**\n\n*   **新问题**：自蒸馏虽然解决了“懒惰推理”，让模型学会了分解，但也带来了一个副作用——**思维同质化**。模型生成的推理过程变得高度一致，缺乏反思和多样性。\n*   **技术瓶颈**：这种同质化导致在后续的强化学习（RL）阶段，奖励的方差极低。在GRPO算法中，如果所有样本的奖励都差不多，优势函数就会趋近于零，导致梯度消失，RL无法进行优化。\n*   **洞察**：高熵的Token通常与反思性推理相关。我们需要在保持任务分解结构的同时，恢复模型的探索和反思能力。\n*   **方案提出**：**多样性感知的GRPO（DA-GRPO）**。\n    *   在优势函数中引入一个基于熵的项。\n    *   当传统的奖励优势无法提供梯度信号时（即奖励方差太小），利用Token的熵作为替代信号来驱动更新。\n    *   **目的**：在保持任务分解能力的同时，激励模型生成更多样化、更具反思性的推理路径，打破RL优化的僵局。\n\n---\n\n### 四、 总结：完整的逻辑链条\n\n1.  **观察**：LRM在复杂工具使用上算力浪费大、收益低（懒惰推理）。\n2.  **诊断**：根源在于缺乏“任务分解”的结构化能力。\n3.  **假设**：如果能强制或教会模型分解，性能将提升。\n4.  **实施（第一步）**：利用**自蒸馏**，挖掘模型自身潜力，低成本地教会其任务分解。\n5.  **反思（第二步）**：单纯的分解导致思维僵化，阻碍了RL所需的探索。\n6.  **修正（第二步）**：引入**DA-GRPO**，利用熵机制恢复推理的多样性和反思能力，确保RL能有效优化。\n\n这条逻辑链清晰地展示了作者从发现现象、挖掘本质、提出假设到设计两阶段解决方案的完整思考闭环。"
                },
                {
                    "title": "ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation",
                    "arxiv_id": "2602.01709",
                    "authors": "Xingshan Zeng, Lingzhi Wang, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu",
                    "summary": "Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \\emph{\\name}, \\emph{\\underline{A}gentic \\underline{R}isk-Aware \\underline{T}est-Time Scaling via \\underline{I}terative \\underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \\emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“单智能体”方向的核心研究。 1.  **核心贡献符合第一步判断**: 论文的核心贡献是提出了 **ARTIS** 框架，这是一个专门针对 **Agentic settings**（智能体设置）的测试时缩放方法。它不是简单的应用，而是构建了一个新的方法论，通过在真实执行前进行“迭代模拟”来解耦探索与承诺。这直接对应了您筛选标准中的“构建、改进 LLM智能体”的方法论。 2.  **精准命中研究焦点**: *   **单智能体**: 论文专注于提升智能体在多轮和多步任务中的表现，核心在于解决智能体与外部环境交互时的不可逆性和高成本问题。 *   **规划与工具使用**: 论文涉及智能体如何规划其行动，并引入了一个“风险感知工具模拟器”，这属于智能体能力中的 `Tool Use / Tool Augmentation` 以及在复杂任务中的多步推理/规划。 3.  **排除标准检查**: *   **非应用型**: 尽管在基准测试上进行了实验，但论文的目的是验证框架的有效性，而非将智能体作为工具去解决生物、金融等特定领域的垂直问题。 *   **非安全/对齐**: 论文中的 \"Risk-Aware\" 指的是智能体在执行任务时避免高成本或不可逆的**操作错误**（Operational Risk/Reliability），属于提升智能体的鲁棒性和可靠性，而非 AI Safety、Alignment 或 Security 等安全对齐范畴，因此不应被排除。 *   **非基础设施**: 论文关注的是算法逻辑和交互框架，而非硬件加速或部署优化。 综上所述，ARTIS 提出了一种通过模拟环境来增强智能体决策可靠性的新框架，属于提升单智能体规划与行动能力的前沿研究，符合您的筛选要求。",
                    "summary2": "本文旨在解决现有Test-Time Scaling技术在不可逆且代价高昂的Agentic场景中的局限性。针对Agent与外部环境交互的场景，我们提出了一种名为ARTIS的框架，通过迭代模拟将探索与执行解耦，并引入风险感知的工具模拟器以捕捉高影响故障模式。我们在BFCL-v3和ACEBench基准上通过准确率等指标验证了其有效性，显著提升了Agent的可靠性。",
                    "summary_translation": "当前的测试时缩放 (test-time scaling, TTS) 技术通过在推理阶段分配额外计算资源来提升大语言模型 (large language model, LLM) 的性能，然而在智能体场景下，这些技术仍显不足。在智能体场景中，动作直接与外部环境交互，且其后果可能是不可逆且代价高昂的。我们提出了 \\name（即通过迭代模拟实现的智能体风险感知测试时缩放），这是一个通过在真实世界执行前利用模拟交互进行测试时探索，从而将探索与执行解耦的框架。该设计允许扩展推理时计算，以提升动作层面的可靠性和鲁棒性，同时无需承担环境风险。我们进一步表明，基于大语言模型 (LLM) 的朴素模拟器难以捕捉罕见但具有高影响力的故障模式，这严重限制了它们在智能体决策中的有效性。为解决这一局限性，我们引入了一种风险感知工具模拟器，该模拟器通过有针对性的数据生成和重平衡训练，着重提高对导致故障的动作的模拟保真度。在多轮和多步智能体基准测试上的实验表明，迭代模拟显著提升了智能体的可靠性，而风险感知模拟对于在不同模型和任务中稳定实现这些增益至关重要。",
                    "inspiration_trace": "基于对论文《ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察与问题引入\n\n作者首先构建了一个从“通用能力”到“特定局限”的叙事逻辑，具体如下：\n\n*   **现状观察**：现有的测试时扩展技术通过在推理阶段增加计算量，显著提升了大语言模型（LLM）在复杂推理任务上的表现。\n*   **隐含假设的冲突**：现有的TTS方法主要针对静态的、以答案为中心的场景。这些场景隐含了一个假设：中间的推理步骤是“可逆”且“无代价”的（即写错了可以随时修改，没有后果）。\n*   **现实世界的错位**：在智能体场景中，LLM通过工具使用与外部环境交互。此时，模型的输出对应的是具体的“行动”。行动具有**不可逆性**和**高昂的代价**。一次错误的工具调用可能导致环境状态的永久改变，甚至引发灾难性后果。\n*   **核心矛盾**：现有的TTS方法无法直接迁移到智能体场景，因为它们无法处理“行动”带来的真实风险。我们需要一种新的范式，能够在不承担环境风险的前提下，利用额外的计算来提升行动的可靠性。\n\n### 2. 研究问题\n\n基于上述观察，作者显式提出了本研究的核心问题：\n\n**“我们如何将测试时计算扩展应用于智能体场景，以在不承担不可逆的现实世界执行风险的前提下，提升行动层面的可靠性与鲁棒性？”**\n\n---\n\n### 3. 思想演进与逻辑推演\n\n为了回答上述问题，作者的思考过程经历了从“概念验证”到“发现瓶颈”再到“针对性解决”的三个阶段：\n\n#### 第一阶段：概念迁移——从“思维链”到“行动模拟”\n*   **灵感来源**：人类在解决复杂问题时，通常会在脑海中先模拟候选行动，预判后果，再完善计划（心理模拟）。控制理论中的模型预测控制（MPC）和强化学习中的世界模型也遵循这一原理。\n*   **初步假设**：智能体也应该遵循这一原则。与其直接在真实环境中试错，不如在推理时通过“模拟交互”来扩展计算。\n*   **方法论雏形**：提出“迭代模拟”框架。即在决策点生成多个候选行动计划，在模拟环境中交互并评估，最后才在真实环境中执行一次。这实现了“探索”与“承诺”的解耦。\n\n#### 第二阶段：现实检验——朴素模拟器的失效\n*   **实验验证（R1 & R2）**：作者首先验证了两个问题：\n    1.  模拟是否有用？（实验证明：如果有完美模拟器，性能随尝试次数显著提升。）\n    2.  现有的通用LLM能做模拟器吗？（实验证明：当使用LLM自身作为模拟器时，性能反而下降，甚至不如不模拟。）\n*   **深度归因**：为什么LLM模拟器会失败？\n    *   **效用不对称性**：在智能体场景中，大多数行动是良性的，但极少数的失败行动具有决定性的破坏力。\n    *   **平均准确率的陷阱**：通用LLM通常针对“平均准确率”进行优化，倾向于忽略那些罕见但高风险的失败模式。这种“盲目乐观”的模拟器会给智能体提供错误的反馈，导致其在真实行动中犯错。\n\n#### 第三阶段：范式修正——引入“风险感知”\n*   **核心洞察**：有效的模拟器不应追求“整体准确”，而应追求“决策有用”。它必须能够敏锐地捕捉并预测那些导致失败的边缘情况。\n*   **解决方案**：提出**风险感知工具模拟器**。\n    *   **数据层面**：不再依赖自然分布的数据，而是采用“失败驱动”的数据生成策略，主动挖掘和合成罕见的失败案例。\n    *   **训练层面**：通过重平衡训练，提高对失败诱导行动的预测保真度，迫使模拟器关注高风险场景。\n*   **最终框架（ARTIS）**：将“迭代模拟”框架与“风险感知模拟器”结合。通过在模拟环境中进行多次尝试、自我评估和总结，最终生成一个经过验证的、低风险的行动计划再交付真实环境执行。\n\n### 总结\n\n作者的思考路径是一个典型的**“观察-假设-证伪-修正”**过程：\n1.  发现现有TTS在智能体场景下的**风险漏洞**；\n2.  借鉴人类思维提出**模拟先行**的假设；\n3.  通过实验发现通用LLM作为模拟器在**风险识别**上的缺陷；\n4.  最终通过**风险感知**的数据工程与训练策略，修补了这一缺陷，形成了ARTIS这一闭环方法论。"
                },
                {
                    "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control",
                    "arxiv_id": "2602.01672",
                    "authors": "Siheng Xiong, Oguzhan Gungordu, Blair Johnson, James C. Kerce, Faramarz Fekri",
                    "summary": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究范围，属于“单智能体”方向下的“工具使用”与“规划”子方向。 1.  **核心判断**: *   论文的核心贡献是提出了 **DeepControl**，这是一个用于 **Search-augmented reasoning agents**（搜索增强推理智能体）的自适应信息控制框架。 *   这不是将现有智能体简单应用到特定领域（非演化型应用），而是针对智能体在执行任务时如何更有效地使用工具（检索）和进行规划（控制信息获取）提出了新的方法论。 *   它关注的是智能体的架构和控制机制，而非基础模型的Token预测能力或基础设施。 2.  **正面指标匹配**: *   **Agentic AI**: 论文明确研究对象是 \"Search-augmented reasoning agents\"。 *   **Tool Use / Tool Augmentation**: 论文的核心在于解决智能体在使用外部检索工具时的“不受控”问题，提出了“检索延续”和“粒度控制”机制，这直接对应了智能体的工具使用能力。 *   **Planning**: 论文提出的“自适应信息控制”本质上是一种规划策略，决定了智能体在推理过程中“何时停止检索”以及“获取多少信息”，这是复杂任务规划的一部分。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态（视觉）或图技术。 *   虽然涉及推理，但它不是单纯提升LLM内在逻辑能力的CoT变体，而是聚焦于智能体如何与外部环境交互（检索）并进行控制，符合Agentic推理的定义。 综上所述，该论文致力于改进LLM智能体在工具使用和规划过程中的效率与稳定性，完全符合“构建、改进LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决搜索增强推理中不受控信息获取导致的上下文饱和与训练不稳定问题。针对多步推理与外部检索交织的场景，我们提出了一种基于信息效用的自适应信息控制框架DEEP CONTROL，包含检索继续和粒度控制机制，并在七个QA基准数据集上通过Exact Match指标验证了其有效性。",
                    "summary_translation": "搜索增强推理代理将多步推理与外部信息检索相结合，然而不受控制的检索往往会导致冗余证据、上下文饱和以及学习过程的不稳定。现有方法依赖于基于结果的强化学习，但这在调节信息获取方面提供的指导有限。我们提出了 DeepControl，这是一个基于信息效用形式化概念的自适应信息控制框架，该概念用于衡量在给定推理状态下检索到的证据的边际价值。基于此效用，我们引入了检索继续和粒度控制机制，以选择性地调节何时继续或停止检索，以及需要扩展多少信息。一种退火控制策略使得代理能够在训练过程中内化有效的信息获取行为。在七个基准测试上进行的广泛实验表明，我们的方法始终优于强基线模型。特别地，与强基于结果的强化学习基线相比，我们的方法在 Qwen2.5-7B 和 Qwen2.5-3B 上分别实现了 9.4% 和 8.6% 的平均性能提升，并且始终优于无检索和基于检索的推理方法（后者缺乏明确的信息控制）。这些结果凸显了自适应信息控制对于将搜索增强推理代理扩展至复杂的现实世界信息环境的重要性。",
                    "inspiration_trace": "基于对论文《Scaling Search-Augmented LLM Reasoning via Adaptive Information Control》的深入分析，以下是作者产出该核心方法的系统性逻辑推演与思考过程还原：\n\n### 第一阶段：宏观观察与范式转移\n**思考起点：** 随着大语言模型（LLM）智能体的发展，我们已经能够构建出能够将“多步推理”与“外部信息检索”相结合的深度研究代理。\n**演进逻辑：** 最初的研究瓶颈在于“模型能不能推理”或“能不能搜到信息”。然而，随着模型能力的提升和检索系统的完善，当这些智能体被部署到**大规模信息环境**（信息量巨大、结构复杂）中时，瓶颈发生了转移。\n**核心洞察：** 性能不再单纯受限于推理能力或搜索的可用性，而是受限于**对信息获取过程的失控**。\n\n---\n\n### 第二阶段：问题诊断与“故事”逻辑重构\n**（基于 Introduction 的逻辑提取）**\n\n作者在引言中构建了如下的问题演进链条：\n\n1.  **现象观察：** 在实际应用中，智能体往往倾向于重复检索。这导致了**上下文饱和**、**冗余或噪声信息的累积**，以及检索内容与推理过程之间的相互干扰。\n2.  **反直觉发现：** “更多的检索并不一定带来更好的推理”。这打破了以往“信息越多越好”的假设。\n3.  **归因分析：** 为什么会这样？现有的方法主要依赖**基于结果的强化学习**。即，只用“最终答案是否正确”这一单一信号来指导训练。\n4.  **揭示缺陷：** 这种“仅看结果”的学习信号在调节信息获取时存在根本局限：\n    *   **信号稀疏：** 在长链推理中，最终奖励无法为中间的检索决策提供有效指导。\n    *   **行为次优：** 智能体不知道“何时停”或“拿多少”。它可能在证据不足时过早停止，或者在证据微弱时过度检索，导致上下文过长。\n5.  **结论：** 失败的根源不在于搜索能力不足，而在于**缺乏显式的、自适应的信息获取控制**。\n\n---\n\n### 第三阶段：提炼研究问题\n基于上述诊断，作者将模糊的痛点转化为一个具体的科学问题：\n\n**研究问题：**\n> **如何超越稀疏的最终结果奖励，通过显式且自适应地控制信息获取的时机与粒度，来提升搜索增强型LLM智能体在大规模信息环境中的推理效率与稳定性？**\n\n---\n\n### 第四阶段：假设提出与核心概念构建\n**思考过程：** 要解决这个问题，不能只靠“试错”（RL），我们需要教模型判断信息的“价值”。\n**核心假设：** 信息的价值是**状态依赖**的。同一个检索结果，在推理的第一步可能至关重要，但在最后一步可能就是冗余的。因此，我们需要一个形式化的指标来衡量这种“边际价值”。\n\n**概念创新：信息效用**\n*   **定义：** 衡量在当前推理状态下，新检索证据的边际价值。\n*   **维度拆解：**\n    1.  **新颖性：** 这条信息相对于我已知的内容是新的吗？（避免重复）\n    2.  **有效性：** 这条信息真的改变了我的答案置信度吗？（避免噪声）\n*   **逻辑推演：** 只有当信息既新颖又有效时，它才具有高效用，才值得被获取或展开。\n\n---\n\n### 第五阶段：方法论设计与逻辑闭环\n**思考过程：** 有了“信息效用”这个尺子，我们该如何控制智能体？作者从两个维度进行控制，并设计了一套训练策略使其内化。\n\n**1. 粒度控制：解决“拿多少”的问题**\n*   **逻辑：** 现实中的文档很长，直接全部塞入上下文不现实。且不同推理阶段需要的细节程度不同。\n*   **方案：** 采用**分层选择性扩展**。先看摘要（粗粒度），只有当效用判断需要时，才展开看细节（细粒度）。这就像先看目录，再翻书页。\n\n**2. 检索延续控制：解决“何时停”的问题**\n*   **逻辑：** 智能体往往不知道该继续搜还是直接回答。\n*   **方案：** 设定效用阈值。\n    *   如果连续几步检索的效用都很低 $\\rightarrow$ **强制停止**（避免无效检索）。\n    *   如果智能体想停止，但当前答案概率低且近期检索效用高 $\\rightarrow$ **强制继续**（避免过早放弃）。\n\n**3. 退火控制策略：解决“如何学”的问题**\n*   **逻辑：** 我们不能永远帮模型做决定，否则它就学不会。也不能一开始就不管它，否则它会乱搜。\n*   **方案：** **课程学习**。\n    *   **早期：** 高概率使用外部控制信号（像老师手把手教）。\n    *   **中期：** 逐渐减少干预。\n    *   **后期：** 完全移除控制信号（让模型独立考试）。\n*   **目标：** 让模型将外部的控制信号**内化**为自身的参数能力。\n\n---\n\n### 总结：思想演进脉络\n1.  **观察：** 搜索增强智能体在大规模环境中面临“信息过载”而非“信息匮乏”。\n2.  **诊断：** 传统的基于结果的RL太稀疏，无法指导中间过程的信息取舍。\n3.  **定义：** 提出“信息效用”作为状态依赖的价值标尺。\n4.  **控制：** 基于效用，设计“粒度控制”和“延续控制”机制。\n5.  **训练：** 通过“退火控制”策略，将外部控制转化为模型的内在推理能力。"
                },
                {
                    "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain",
                    "arxiv_id": "2602.01640",
                    "authors": "Shuai Zhang, Jiayu Hu, Zijie Chen, Zeyuan Ding, Yi Zhang, Yingji Zhang, Ziyi Zhou, Junwei Liao, Shengjie Zhou, Yong Dai, Zhenzhong Lan, Xiaozhu Ju",
                    "summary": "Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“Agentic AI”与“多智能体”方向**： 论文的核心贡献是提出了一个名为 **A2Eval** 的 **Agentic framework**（智能体框架）。该框架由两个协作智能体组成：**Data Agent**（负责自主诱导能力维度和组装评估套件）和 **Eval Agent**（负责合成和验证可执行的评估管道）。这直接对应了研究焦点中的“多智能体”方向，涉及智能体间的协作与分工，同时也属于“构建 LLM智能体”的范畴。 2.  **具备智能体的关键能力**： 摘要中描述智能体能够“autonomously induces”（自主诱导）、“assembles”（组装）、“synthesizes and validates”（合成与验证），这体现了智能体的自主规划、工具使用和执行能力，符合正面指标中的 `Planning` 和 `Tool Use`。 3.  **不属于排除标准**： *   **非“非演化型应用”**：虽然论文的应用场景是评估具身模型，但其核心在于提出了一种新的智能体框架来实现评估流程的自动化，而不是简单地将现有智能体作为工具应用。智能体系统的构建本身就是论文的创新点。 *   **非“安全与对齐”**：摘要中提到的 \"improves human alignment\" 指的是评估排名与人类判断的一致性（Spearman's rho），属于评估指标范畴，而非 AI 安全、伦理或模型对齐技术，因此不触犯排除规则。 *   **非“基础设施”**：论文关注的是智能体的行为逻辑和协作框架，而非硬件加速或底层部署优化。 综上所述，该论文提出了一种新颖的多智能体协作框架来解决自动化评估问题，属于构建和改进 LLM智能体的前沿研究。",
                    "summary2": "本文旨在解决现有具身VLM评估中存在的冗余、覆盖不均及高成本问题。针对现有的静态基准测试，我们提出了一种名为A2Eval的智能体自动化评估框架，通过Data Agent构建平衡紧凑的测试集，Eval Agent自动合成评估管线。在10个基准和13个模型上的实验表明，该方法实现了85%的套件压缩、77%的成本降低及4.6倍加速，同时将人类对齐度提升至Spearman’s $\\rho$ = 0.85。",
                    "summary_translation": "当前的具身视觉语言模型评估依赖于静态、专家定义且人工标注的基准，这些基准存在严重的冗余和覆盖不均问题。这种劳动密集型范式耗费了大量计算与标注资源，推高了评估成本，并导致模型排名失真，最终抑制了模型的迭代发展。为解决这一问题，我们提出了 Agentic Automatic Evaluation (A2Eval，智能体自动评估)，这是首个通过两个协作智能体实现基准构建与评估自动化的智能体框架。其中，Data Agent（数据智能体）自主归纳能力维度并构建平衡且紧凑的评估套件，而 Eval Agent（评估智能体）则综合并验证可执行的评估管道，从而实现完全自主、高保真的评估。在 10 个基准和 13 个模型上的评估结果表明，A2Eval 将评估套件压缩了 85%，降低了 77% 的总体计算成本，并在保持评估质量的同时实现了 4.6 倍的加速。至关重要的是，A2Eval 修正了系统性的排名偏差，将人类一致性提升至 Spearman's rho=0.85，并保持了高排名保真度，为高保真、低成本的具身评估确立了新标准。我们的代码和数据将很快公开。",
                    "inspiration_trace": "基于对论文《A2Eval: Agentic and Automated Evaluation for Embodied Brain》的深入分析，以下是作者产出该文章的系统性思考过程与逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从危机到变革\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在揭示现有评估体系的根本性缺陷，从而引出变革的必要性。\n\n1.  **确立核心地位：**\n    *   评估是Embodied AI发展的“指南针”，不仅是学术突破的基准，更是工业决策的基石。\n2.  **揭示现实危机：**\n    *   这枚“指南针”的代价变得令人望而却步。评估单个模型需消耗超过3200 GPU小时，造成了巨大的财务和计算负担，严重阻碍了研究的迭代速度。\n3.  **深挖病灶根源：**\n    *   危机源于一个“破碎的评估生态系统”，其核心症结在于**“专家定义 + 人工标注”的陈旧范式**。\n4.  **剖析具体病理：**\n    *   这种范式导致了“双输”困境：\n        *   **覆盖不均与冗余：** 样本重复率极高（高达92%），且任务分布严重倾斜（简单任务泛滥，复杂推理任务稀缺）。\n        *   **排名扭曲：** 模型通过过拟合那些被过度代表的简单任务获得高分，掩盖了其在关键能力上的短板，导致排行榜失真，误导研究方向。\n        *   **评估成本高昂：** 海量冗余数据的计算成本，加上为每个基准手动构建推理和评分逻辑的人力成本，使得评估周期缓慢且昂贵。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述对现状的批判，作者试图回答的核心研究问题是：\n\n**“如何构建一个全自动化的智能体评估框架，以取代依赖专家定义和人工标注的传统范式，在大幅降低计算成本和数据冗余的同时，消除排名偏差并保持与人类判断的高度一致性？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从宏观问题出发，通过诊断、假设、分解与设计，最终形成了A2Eval这一核心方法。\n\n#### 1. 观察与诊断：评估范式的根本性滞后\n*   **思考起点：** 现有的评估体系是静态的、人工的。随着模型能力的爆炸式增长，这种“手工作坊”式的评估方式已成为瓶颈。\n*   **核心洞察：** 问题不在于模型，而在于**尺子**。我们需要一把“动态的、自动化的尺子”。不能只做修补（如增加新数据集），必须进行范式转移。\n\n#### 2. 假设提出：将基准构建视为优化问题\n*   **假设：** 如果将基准构建看作一个数学优化问题——即**最大化能力覆盖与多样性，同时最小化冗余与人力投入**——那么我们就能用算法替代专家。\n*   **路径选择：** 利用大模型本身具备的智能体能力，让AI来评估AI。这不仅能解决人力成本问题，还能通过客观的数据分析消除专家的主观偏差。\n\n#### 3. 方法论设计：双智能体协作架构\n为了解决上述诊断中的三大病理（冗余、偏差、成本），作者将解决方案拆解为两个互补的智能体：\n\n*   **针对“数据质量”问题（解决冗余与偏差）：设计 Data Agent**\n    *   *思考：* 现有数据集虽然多，但乱且重复。我们需要一个“图书管理员”来整理。\n    *   *逻辑演进：*\n        *   **维度归纳：** 不再依赖专家主观定义能力维度（如“空间推理”），而是让Agent通过分析现有基准，自动推导出统一的能力分类体系。\n        *   **基准构建：** 有了维度后，如何选数据？不能随机选。必须进行“多样性感知采样”。通过聚类算法，在每个维度下选取最具代表性的样本，剔除重复样本，从而实现数据集的“瘦身”和“平衡”。\n\n*   **针对“执行成本”问题（解决人工瓶颈）：设计 Eval Agent**\n    *   *思考：* 即使有了好的数据集，跑评估依然很麻烦，因为每个数据集的接口、评分逻辑都不一样，需要人写代码。\n    *   *逻辑演进：*\n        *   **自动化流水线：** 我们需要一个“程序员”Agent。它能自动编写代码来加载模型、运行推理。\n        *   **沙盒验证：** 代码写好后，怎么保证对？通过沙盒环境运行测试，如果报错就反馈给Agent自我修正，直到生成可执行的评分逻辑。\n\n#### 4. 验证与闭环：从“压缩”到“保真”\n*   **思考：** 我们把数据集压缩了85%，把评估过程自动化了，结果还靠谱吗？\n*   **验证逻辑：**\n    *   **排名保真度：** 验证压缩后的数据集排名是否与原海量数据集排名一致（结果：高度一致）。\n    *   **人类对齐：** 验证新的排名是否更符合人类专家的直觉（结果：比原基准更符合人类偏好，修正了偏差）。\n    *   **执行保真度：** 验证自动生成的评估代码是否与人工编写的代码结果一致（结果：96.9%的保真度）。\n\n#### 5. 最终结论：新标准的建立\n*   **思想升华：** A2Eval不仅仅是一个工具，它确立了一种新的评估标准——**高保真、低成本、全自动**。它证明了在Embodied AI领域，我们可以用更少的资源、更客观的视角来衡量模型的进步。"
                },
                {
                    "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
                    "arxiv_id": "2602.01566",
                    "authors": "Chiwei Zhu, Benfeng Xu, Mingxuan Du, Shaohan Wang, Xiaorui Wang, Zhendong Mao, Yongdong Zhang",
                    "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 该论文的核心贡献在于构建了一个名为 \"FS-Researcher\" 的 LLM 智能体框架，旨在解决长视距任务中的上下文限制问题。这属于构建和改进 LLM 智能体方法论的范畴，而非单纯将现有智能体作为工具应用到特定领域。因此，符合第一步的保留标准。 2.  **正面指标**: *   **Multi-Agent Systems**: 论文明确提出了一个双智能体框架，包含 \"Context Builder\"（负责浏览、归档）和 \"Report Writer\"（负责撰写），涉及智能体间的协作与分工。 *   **Memory**: 论文的核心创新点在于利用文件系统作为持久的外部记忆和共享协调介质，这直接对应了智能体能力中的 \"Memory\" 模块。 *   **Tool Use**: 智能体具备浏览互联网、撰写结构化笔记等工具使用能力。 *   **Agentic AI**: 整个框架围绕智能体如何自主完成复杂的深度研究任务展开，属于典型的 Agentic AI 研究。 3.  **排除标准**: 论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **综合结论**: 该论文通过提出基于文件系统的双智能体架构，有效地扩展了 LLM 智能体在长视距任务中的记忆能力和协作效率，完全符合关于 \"LLM智能体及其演化\" 中多智能体系统和智能体能力构建的研究目标。",
                    "summary2": "本文旨在解决深度研究任务因上下文限制难以进行测试时扩展的问题。针对长视界研究场景，我们提出了一种基于文件系统的双智能体框架FS-Researcher，利用持久化工作空间分离证据积累与报告撰写。在DeepResearch Bench和DeepConsult基准上，通过RACE、FACT及胜率等指标验证了其有效性，实现了最先进的报告质量。",
                    "summary_translation": "深度研究正成为大语言模型 (LLM) 智能体的一种典型长视界任务。然而，深度研究中的长轨迹往往超出模型的上下文限制，从而挤压了证据收集和报告撰写两方面的 token 预算，阻碍了有效的测试时扩展。我们提出了 FS-Researcher，这是一个基于文件系统的双智能体框架，通过持久化工作空间将深度研究的范围扩展至上下文窗口之外。具体而言，Context Builder 智能体扮演图书管理员的角色，负责浏览互联网、撰写结构化笔记，并将原始来源归档到一个远超上下文长度的分层知识库中。随后，Report Writer 智能体将知识库作为事实来源，逐节撰写最终报告。在该框架中，文件系统充当持久化的外部记忆以及跨智能体和会话的共享协调媒介，从而实现了超越上下文窗口的迭代优化。在两个开放式基准测试上的实验表明，FS-Researcher 在不同的基座模型上均实现了最先进的报告质量。进一步的分析表明，最终报告质量与分配给 Context Builder 的计算量呈正相关，验证了文件系统范式下有效的测试时扩展能力。代码和数据已匿名开源，链接为 https://github.com/Ignoramus0817/FS-Researcher。",
                    "inspiration_trace": "基于对论文《FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的“困境-突围”叙事逻辑，具体如下：\n\n1.  **宏大背景与任务定义**：\n    *   “深度研究”已成为衡量LLM智能体能力的代表性前沿任务，要求具备PhD级别的专业能力。\n    *   任务本质：面对开放式查询，需系统性地收集海量证据（浏览数百网页）并合成超长报告（>10K tokens）。\n\n2.  **核心冲突**：\n    *   **任务需求 vs. 模型局限**：深度研究任务的长轨迹特性，天然与模型固有的上下文长度限制相冲突。\n    *   **资源挤压**：这种冲突导致用于“证据收集”和“报告撰写”的Token预算被严重压缩，无法满足任务实际需求。\n\n3.  **现有方案的缺陷**：\n    *   **静态/单智能体**：因上下文溢出导致覆盖不全、质量低下。\n    *   **压缩/子智能体**：虽然通过压缩观察值延长了轨迹，但这只是“权宜之计”，仍受限于硬性上下文上限。\n    *   **不可持续性**：内部状态（思考、工具观察）是“易耗品”，一旦循环结束即被丢弃，阻碍了跨会话的迭代优化。\n\n4.  **关键洞察与转折**：\n    *   **跨领域借鉴**：代码智能体和AI IDE的成功表明，文件系统工作区是处理长周期任务的有效基质。\n    *   **范式迁移**：将这种“文件系统”范式迁移到深度研究中，解决其特有的噪声处理、事实提取和叙事合成问题。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题为：\n\n**“如何设计一种智能体框架，使其能够突破上下文窗口的物理限制，通过持久化的外部记忆实现长周期深度研究任务的有效测试时扩展？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n以下是从宏观观察到具体方法论的思维推演过程：\n\n#### 1. 观察与痛点识别\n*   **观察**：现有的深度研究智能体在处理长任务时，要么因为上下文塞满而“猝死”，要么为了省空间而丢弃大量中间细节，导致报告缺乏深度。\n*   **痛点**：模型不仅需要“思考”，还需要一个能够容纳比上下文窗口大得多的“外部大脑”来存储和检索信息。目前的“压缩”策略是有损的，且无法支持“回头修改”的迭代过程。\n\n#### 2. 假设提出\n*   **假设**：如果将智能体的“记忆”从模型内部（Context）剥离出来，映射到一个结构化的、持久的**文件系统**中，那么：\n    *   信息存储将不再受限于Token数量。\n    *   智能体可以像人类研究员一样，通过“查阅文件”来获取信息，而非依赖记忆。\n    *   这种持久化允许智能体在多次会话中不断打磨同一个知识库，从而实现“计算投入越多，质量越好”的扩展效应。\n\n#### 3. 概念抽象\n*   **核心隐喻**：将研究过程类比为“图书馆”运作。\n    *   需要一个**图书管理员**：负责采购（搜索）、编目（结构化存储）和摘要（写笔记）。\n    *   需要一个**作者**：只负责基于图书馆的资料进行写作，不需要自己去互联网上乱找。\n*   **关键设计**：**关注点分离**。将“证据积累”与“报告合成”解耦。前者追求广度和结构，后者追求深度和逻辑。\n\n#### 4. 方法论构建\n*   **架构设计**：\n    *   **持久化工作空间**：定义文件系统为唯一的真相来源。包含“交付物”（最终报告、知识库）和“控制文件”（待办事项、检查清单、日志）。\n    *   **双智能体协作**：\n        *   **Context Builder (第一阶段)**：赋予其浏览和文件操作工具。任务是将非结构化的网页信息转化为结构化的、带引用的层级知识库（KB）。\n        *   **Report Writer (第二阶段)**：剥夺其网络浏览权限，强制其只读取KB。任务是基于KB分章节撰写报告，并进行自我审查。\n*   **机制保障**：\n    *   **迭代优化**：通过控制文件记录状态，允许智能体在多次会话中识别缺口并补充KB。\n    *   **测试时扩展**：通过增加Context Builder的运行轮次，直接扩大知识库规模，从而线性提升最终报告的质量。\n\n#### 5. 预期验证\n*   **逻辑推论**：如果假设成立，那么分配给Context Builder的计算资源越多，知识库越丰富，最终报告的质量（全面性、洞察力）应该越高，且这种提升不应受限于上下文窗口。\n\n---\n\n**总结**：作者的思想演进是从**“上下文限制”**这一物理瓶颈出发，通过**“外部化记忆”**的假设，借鉴**“文件系统”**的工程范式，最终通过**“双智能体分工”**实现了长周期研究任务的**“可扩展性”**。"
                },
                {
                    "title": "ASTER: Agentic Scaling with Tool-integrated Extended Reasoning",
                    "arxiv_id": "2602.01204",
                    "authors": "Xuqin Zhang, Quan He, Zhenrui Zheng, Zongzhang Zhang, Xu He, Dong Li",
                    "summary": "Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了ASTER框架，旨在解决LLM智能体在强化学习（RL）训练过程中出现的“交互崩溃”问题。这直接属于构建和改进LLM智能体的方法论研究，而非单纯的应用。 2.  **符合Agentic AI**: 论文明确关注“Agentic Scaling”和“Tool-Integrated Reasoning”。它研究如何通过冷启动策略建立“agentic, tool-using behavioral prior”（智能体工具使用行为先验），并确保智能体能够维持“multi-turn tool usage”（多轮工具使用）。这完全符合单智能体方向中关于“工具使用”和“规划/推理”的研究焦点。 3.  **符合自我演化**: 论文利用强化学习（RL）来改进智能体的行为，使其能够通过环境反馈（交互密度）进行学习和泛化，这属于智能体通过反馈进行自我完善和迭代的机制。 4.  **排除标准检查**: 尽管论文在数学基准（如AIME）上进行了评估，但其核心贡献并非解决数学问题本身，而是提出了一种通用的智能体训练框架。因此，它不属于“非演化型应用”。同时，它也不涉及安全、多模态或图等排除领域。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR) 在强化学习 (RL) 训练中出现的交互崩溃问题。针对LLM的长程推理场景，我们提出了ASTER框架，通过采用高交互密度轨迹的冷启动策略建立行为先验，并结合多阶段RL训练。在AIME 2025、HMMT 2025等竞争性数学基准上，通过准确率验证了其有效性，ASTER-4B模型取得了90.0%的SOTA成绩，超越了更大的开源模型。",
                    "summary_translation": "强化学习 (RL) 已成为激发大语言模型 (LLMs) 长程推理能力的主导范式。然而，通过强化学习 (RL) 扩展工具集成推理 (TIR) 仍然充满挑战，原因在于交互崩溃：这是一种病态状态，即模型无法维持多轮工具使用，反而退化为沉重的内部推理，仅辅以琐碎的事后代码验证。我们系统地研究了三个问题：(i) 冷启动监督微调 (SFT) 如何诱导智能体式的、使用工具的行为先验，(ii) 冷启动轨迹的交互密度如何塑造探索过程和下游强化学习 (RL) 结果，以及 (iii) 强化学习 (RL) 交互预算如何影响在不同推理时预算下的学习动态和泛化能力。随后，我们介绍了 ASTER (Agentic Scaling with Tool-integrated Extended Reasoning，即具有工具集成扩展推理的智能体扩展)，这是一个通过优先使用高交互密度轨迹的有针对性冷启动策略来规避这种崩溃的框架。我们发现，仅包含 4K 条高交互密度轨迹的小规模专家冷启动集就能产生最强的下游性能，建立了一个鲁棒的先验，从而在扩展的强化学习 (RL) 训练过程中实现优越的探索。广泛的评估表明，ASTER-4B 在竞争性数学基准上取得了最先进的结果，在 AIME 2025 上达到了 90.0% 的准确率，超越了包括 DeepSeek-V3.2-Exp 在内的领先前沿开源模型。",
                    "inspiration_trace": "基于对论文《ASTER: Agentic Scaling with Tool-integrated Extended Reasoning》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观背景与问题引入\n\n**1. 现状观察：**\n*   **RL的崛起：** 强化学习（RL）已成为激发大语言模型（LLM）长程推理能力的主导范式（如OpenAI o1, DeepSeek R1）。\n*   **纯文本推理的脆弱性：** 尽管长思维链取得了成功，但纯文本推理缺乏外部验证，微小的错误会级联放大，且难以自我修正。\n\n**2. 解决方案的提出与局限：**\n*   **工具集成推理（TIR）：** 为了解决脆弱性问题，引入可执行工具（如代码解释器）进行外部验证和精确计算。\n*   **现有路径的困境：**\n    *   **路径A（ZeroTIR）：** 直接对预训练模型进行RL。**问题：** 优化不稳定（梯度爆炸、分布漂移），且受限于基座模型本身的能力上限。\n    *   **路径B（SFT Cold-start）：** 先用带工具的轨迹进行监督微调，再进行RL。**问题：** 现有的冷启动数据引入了强烈的归纳偏差。例如，ReTool保留了文本结构（缺乏工具驱动的规划），教师模型生成的轨迹往往短且反应式（缺乏System 2的深思熟虑）。\n\n**3. 核心现象的发现：**\n*   **交互崩溃：** 上述冷启动设计导致模型在后续的RL训练中出现一种病态状态——模型无法维持多轮工具使用，而是退化为大量的内部推理，仅在最后进行琐碎的、事后的代码验证。\n\n---\n\n### 二、 核心研究问题\n\n基于上述背景与现象，作者试图解决的核心问题是：\n\n**如何设计一种冷启动策略，能够诱导出具备长程、多轮工具使用能力的“行为先验”，从而在强化学习过程中避免“交互崩溃”，实现可扩展的智能体推理？**\n\n---\n\n### 三、 思想演进与方法论形成\n\n作者的思想演进遵循了“现象观察 -> 归因分析 -> 假设提出 -> 实验验证 -> 方法确立”的逻辑链条：\n\n#### 1. 现象观察与诊断\n*   **观察：** 现有的TIR方法（无论是ZeroTIR还是基于SFT的方法）在RL扩展过程中，工具调用的频率和深度都会迅速下降，最终退化为“先思考后验证”的浅层模式。\n*   **诊断：** 作者认为问题的根源不在于RL算法本身，而在于**冷启动阶段建立的“行为先验”**。如果初始数据主要包含稀疏、短视的工具调用，模型就会习得一种“懒惰”或“反应式”的工具使用习惯，RL只会放大这种短视行为。\n\n#### 2. 关键假设的提出\n*   **假设：** 为了维持长程的探索能力，冷启动数据不应追求“效率”或“即时的准确率”，而应追求**“交互密度”**。\n*   **推论：** 只有那些包含大量工具交互轮次（>9次）、体现“规划-执行-修正”循环的轨迹，才能为模型植入一种高熵的、倾向于持续探索的行为先验，从而防止在RL训练中过早收敛到次优策略。\n\n#### 3. 实验验证与洞察\n*   **验证RQ1（行为先验的作用）：** 对比不同冷启动策略发现，稀疏交互的数据会导致模型在RL阶段迅速丧失工具使用能力（交互崩溃）。\n*   **验证RQ2（交互密度的关键性）：** 实验表明，使用少量（4K条）但高交互密度的专家数据进行冷启动，虽然初始准确率可能不高，但在RL阶段能保持更高的训练熵，最终性能远超使用大量稀疏数据的模型。\n*   **验证RQ3（预算对齐）：** 训练时的交互预算必须与推理时的预算对齐。高预算训练才能解锁高预算推理时的性能提升。\n\n#### 4. 最终方法论：ASTER\n*   **策略确立：** 放弃追求“完美的教师模型”或“大规模的合成数据”，转而专注于**“小而精”的高交互密度数据**。\n*   **具体实施：**\n    *   **数据筛选：** 筛选出工具调用次数超过9次的4K条高质量轨迹作为冷启动SFT数据。\n    *   **训练对齐：** 在RL阶段设置较高的交互预算（50次调用），并采用两阶段课程学习（先短上下文，后长上下文）。\n*   **结果：** 这种“以交互密度为核心”的先验设计，成功避免了交互崩溃，使得4B的小模型在数学基准测试中超越了数百亿参数的模型。"
                },
                {
                    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
                    "arxiv_id": "2602.02488",
                    "authors": "Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang",
                    "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合要求 (第一步)**: *   论文提出了 \"RLAnything\" 框架，其核心在于通过闭环优化动态构建环境、策略和奖励模型。这是一种关于如何构建和改进 LLM 智能体（Agentic LLM）的方法论，而非简单的应用。 *   它针对的是 \"agentic scenarios\"（智能体场景），并在 OSWorld 和 AlfWorld 等典型的智能体基准测试上进行了验证，旨在提升智能体的任务执行能力。 2.  **高度契合“自我演化”焦点 (第一步 & 第二步)**: *   论文的核心机制是 \"closed-loop optimization\"（闭环优化）和 \"learning from experience\"（从经验中学习）。 *   它通过 \"automatic environment adaptation\"（自动环境适应）和 \"consistency feedback\"（一致性反馈）来联合优化奖励模型和策略模型。这种利用反馈信号进行迭代改进、自我完善的机制，正是“自我演化”定义中的核心内容。 3.  **符合“单智能体”能力提升 (第二步)**: *   论文重点在于优化 Policy（策略），即智能体的大脑，通过强化学习增强智能体在复杂任务中的表现，这直接对应单智能体的能力构建。 4.  **排除标准检查 (第三步)**: *   虽然摘要中提到了 Qwen3-VL（视觉语言模型），但论文的核心贡献并非视觉模型架构或多模态技术本身，而是利用 RL 框架来优化智能体（包括具备视觉感知能力的智能体）。根据排除标准中的特殊说明，视觉在这里是智能体感知环境的工具，而非研究核心，因此不应排除。 *   论文不涉及安全、对齐或图神经网络等排除领域。 综上所述，该论文提出了一种通过强化学习实现智能体自我演化和能力提升的新框架，精准命中“LLM智能体及其演化”这一研究课题。",
                    "summary2": "本文旨在解决强化学习在长轨迹任务中奖励信号稀疏及环境静态限制学习效率的问题。针对计算机控制、文本游戏及代码生成等复杂智能体场景，我们提出了RLAnything框架，通过闭环优化动态构建环境、策略和奖励模型，利用集成反馈和评论家反馈实现自适应调整。并在OSWorld、Alf World和LiveBench等基准上通过任务准确率验证了其有效性，显著优于依赖人工标签的基线方法。",
                    "summary_translation": "我们提出了 RLAnything，这是一个 reinforcement learning (强化学习) 框架，它通过 closed-loop optimization (闭环优化) 动态构建 environment (环境)、policy (策略) 和 reward models (奖励模型)，从而放大学习信号并增强适用于任何 LLM (大语言模型) 或 agentic scenarios (智能体场景) 的整体 RL 系统。具体而言，policy (策略) 利用来自 step-wise signals (逐步信号) 和 outcome signals (结果信号) 的集成反馈进行训练，而 reward model (奖励模型) 则通过 consistency feedback (一致性反馈) 进行联合优化，这进而进一步改进了 policy (策略) 的训练。此外，我们受理论启发的 automatic environment adaptation (自动环境适应) 机制通过利用来自两者的 critic feedback (评论家反馈)，改进了 reward model (奖励模型) 和 policy (策略) 的训练，从而实现了从经验中学习。实验结果表明，每个新增组件都持续改进了整体系统，且 RLAnything 在各种代表性 LLM (大语言模型) 和 agentic (智能体) 任务中带来了显著提升，在 OSWorld 上将 Qwen3-VL-8B-Thinking 提升了 9.1%，在 AlfWorld 和 LiveBench 上分别将 Qwen2.5-7B-Instruct 提升了 18.7% 和 11.9%。我们还发现，优化的 reward-model (奖励模型) 信号优于依赖人类标签的结果。代码：https://github.com/Gen-Verse/Open-AgentRL",
                    "inspiration_trace": "基于对论文《RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System》的深入分析，以下是作者产出该文章的系统性思考过程与逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的方式，构建了从现有成功到现实局限，再到核心痛点的逻辑链条：\n\n1.  **背景与现状：** 强化学习（特别是基于可验证奖励的 RLVR）已被证明能有效提升大语言模型（LLM）的推理能力。\n2.  **现实挑战（长轨迹与稀疏性）：** 现实世界的应用（如智能体交互）往往超越单轮问答，涉及长轨迹的迭代交互。在这种场景下，仅依靠二元的“最终结果”奖励过于稀疏，无法提供足够的监督信号。\n3.  **现有方案的局限（奖励模型）：** 虽然生成式奖励模型能提供更细粒度的“逐步”信号，且优于标量模型，但训练这些模型通常需要大量高质量、特定任务的人工监督，难以自动化和规模化。\n4.  **被忽视的关键（环境质量）：** 除了奖励设计，环境的质量同样至关重要。将任务难度与模型当前能力对齐（课程学习）能改善训练动态。在现实环境中，探索的范围由任务定义，且增加任务多样性能促进泛化。\n5.  **逻辑缺口：** 现有的 RL 系统通常将环境、策略和奖励模型视为相对独立或静态的组件，缺乏一个统一的机制让它们协同进化。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑缺口，作者提出了一个核心的探索性问题：\n\n**“是否存在一个能够联合优化环境、策略和奖励模型的 RL 系统，通过闭环交互来放大学习信号，从而强化整个系统的性能？”**\n\n---\n\n### 三、 核心方法的逻辑演进（思想推演）\n\n为了回答上述问题，作者的思考过程经历了从“观察痛点”到“提出假设”，再到“理论验证”和“系统构建”的四个阶段。\n\n#### 1. 观察与解构：打破“静态”桎梏\n*   **观察：** 在传统的 RL 流程中，策略是动态训练的，但提供信号的“奖励模型”和提供场景的“环境”往往是静态的或预定义的。\n*   **痛点分析：**\n    *   **奖励端：** 仅用最终结果太稀疏，仅用逐步信号不可靠。如果奖励模型本身不随着策略的进步而进化，它给出的反馈可能始终是低质量的。\n    *   **环境端：** 如果环境任务太难，策略无法获得正向反馈；如果太简单，策略无法学到新东西。静态环境无法匹配策略动态变化的能力。\n\n#### 2. 假设提出：构建“三位一体”的动态闭环\n*   **核心假设：** 如果让策略、奖励模型和环境三者形成一个动态的闭环，每一方都从另外两方获得反馈并进化，那么整个系统的学习效率将呈指数级提升。\n*   **具体推演：**\n    *   **策略进化：** 需要更密集的信号。假设将“稀疏的最终结果”与“密集的逐步信号”结合，能提供更优的监督。\n    *   **奖励进化：** 奖励模型不需要人工标注，而是利用策略产生的轨迹作为训练环境。通过“一致性反馈”（即奖励模型的判断是否与最终结果一致）来优化自身。\n    *   **环境进化：** 环境不应是固定的。假设利用奖励模型对策略行为的“批评反馈”，来自动调整任务的难度（太难就简化，太简单就增加难度）。\n\n#### 3. 理论洞察：环境适配是奖励模型优化的前提\n*   **深层思考：** 为什么要动态调整环境？仅仅是为了让策略好学吗？\n*   **理论推导：** 作者通过理论分析发现了一个关键点——**奖励模型的精度依赖于任务难度的平衡**。\n    *   如果任务太难（策略总是失败），或者太简单（策略总是成功），会导致训练数据分布极度不平衡，从而破坏奖励模型区分好坏步骤的能力。\n    *   **结论：** 调整环境难度不仅是为了策略，更是为了训练出一个更准确的奖励模型。这为“环境动态化”提供了坚实的理论动机。\n\n#### 4. 方法论形成：RLAnything 的闭环设计\n基于上述思考，作者最终构建了 **RLAnything** 框架，其逻辑架构如下：\n\n*   **第一环（策略训练）：** 采用**集成反馈**。不再单一依赖结果，而是将最终结果奖励与奖励模型的逐步信号加权融合，解决长轨迹中的稀疏奖励问题。\n*   **第二环（奖励模型优化）：** 采用**一致性反馈**。将策略的轨迹视为环境，利用最终结果和自我一致性作为监督信号，让奖励模型在评估策略的同时自我进化。\n*   **第三环（环境自适应）：** 采用**批评反馈驱动**。利用奖励模型输出的具体错误诊断，指导 LLM 自动修改任务描述或参数，动态调整任务难度，确保难度始终处于策略的“最近发展区”，从而同时反哺策略和奖励模型的训练。\n\n---\n\n**总结：**\n作者的思考路径是从**“单一组件优化”**转向**“系统级协同进化”**。他们不仅解决了奖励稀疏的问题（通过集成反馈），更关键地发现了环境、奖励与策略之间的数学耦合关系（通过理论证明），最终提出了一个完全动态、自我强化的 RL 系统。"
                },
                {
                    "title": "A-MapReduce: Executing Wide Search via Agentic MapReduce",
                    "arxiv_id": "2602.01331",
                    "authors": "Mingju Chen, Guibin Zhang, Heng Chang, Yuchen Guo, Shiji Zhou",
                    "summary": "Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的论文。具体判断依据如下： 1.  **核心贡献符合 (第一步 & 第二步)**: *   论文提出了 **A-MapReduce**，这是一个受 MapReduce 范式启发的**多智能体执行框架**。其核心贡献在于构建了一个新的框架来解决现有智能体在处理大规模“广度搜索”任务时的效率瓶颈，这直接对应了研究目标中的“构建、改进 LLM 智能体”和“多智能体系统”。 2.  **多智能体与自我演化特征 (第二步 & 第四步)**: *   **多智能体**: 论文明确将其定义为 \"multi-agent execution framework\"，涉及任务的自适应分解和结构化结果聚合，属于多智能体协作与执行的研究范畴。 *   **自我演化**: 摘要中明确提到利用 \"experiential memory\"（经验记忆）来驱动 \"query-conditioned task allocation and recomposition\"（查询条件下的任务分配和重组）的 \"continual evolution\"（持续演化），从而实现 \"progressive improvement\"（渐进式改进）。这直接对应了筛选标准中的 \"Self-Evolving\"、\"Memory\" 和 \"Iterative Improvement\" 机制。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、水印或幻觉问题。 *   不涉及多模态、视觉或图神经网络技术。 *   它不是将智能体作为工具应用到特定垂直领域（如医疗、金融）的应用型论文，而是专注于智能体执行框架本身的改进。 综上所述，该论文在多智能体协作框架和自我演化机制方面做出了实质性贡献，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有多智能体系统在大规模广度检索任务中的低效问题。针对 Wide Search 场景，我们提出了一种受 MapReduce 范式启发的 A-MapReduce 框架，通过水平结构化检索与经验记忆机制优化任务分解与聚合。在 WideSearch 和 DeepWideSearch 等 5 个基准上，通过 Item F1、Success Rate 及运行时间等指标验证了其有效性，实现了 SOTA 性能并显著降低了成本与耗时。",
                    "summary_translation": "当代基于大语言模型（LLM）的多智能体系统在深度研究任务中展现出系统性优势，这类任务强调迭代的、垂直结构的信息搜寻。然而，当面对以大规模、面向广度的检索为特征的广度搜索任务时，现有的主要围绕顺序的、垂直结构推理设计的智能体框架，往往陷入扩展的搜索目标，且执行效率低下（长视界执行）。为弥合这一差距，我们提出了A-MapReduce，这是一种受MapReduce范式启发的多智能体执行框架，它将广度搜索重新构建为一种水平结构的检索问题。具体而言，A-MapReduce通过任务自适应分解和结构化结果聚合，实现了对海量检索目标的并行处理。同时，该框架利用经验记忆驱动基于查询条件的任务分配与重组的持续演进，从而在大规模广度搜索场景中实现性能的渐进式提升。在五个智能体基准测试上进行的广泛实验表明，A-MapReduce：（i）性能优异，在WideSearch和DeepWideSearch数据集上达到了最先进水平，与基于OpenAI o3或Gemini 2.5 Pro骨干模型的强基线相比，平均Item F1提升了5.11% - 17.50%；（ii）高效且具有成本效益，实现了卓越的成本-性能权衡，与代表性的多智能体基线相比，运行时间减少了45.8%。代码可在 https://github.com/mingju-c/AMapReduce 获取。",
                    "inspiration_trace": "基于对论文《A-MapReduce: Executing Wide Search via Agentic MapReduce》的深入分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察：从“深度”到“广度”的范式错位\n*   **现状观察**：当前的LLM多智能体系统（MAS）在解决复杂任务上表现出色，其主流设计逻辑是**“垂直递归推理”**（Vertical Recursive Reasoning）。即通过多轮对话、思维链或角色协作，不断挖掘问题的深度。\n*   **局限性发现**：这种“垂直”范式在处理**“广度搜索”**（Wide Search）任务时显得力不从心。广度搜索强调的是大规模、并行化的信息覆盖与结构化聚合（例如：“列出所有米其林三星餐厅及其地址”），而不是对单一目标的深度挖掘。\n*   **核心矛盾**：现有的以“深度”为导向的架构，被错误地应用在了以“广度”为导向的任务上，导致了执行效率低下和结果不可控。\n\n### 2. 问题识别：引入“故事”的逻辑链\n作者在Introduction中通过以下逻辑链条构建了问题的紧迫性：\n\n1.  **现有能力的优势**：承认当前MAS在“深度研究”任务上的优势，这得益于其顺序的、垂直结构化的信息获取方式。\n2.  **新场景的挑战**：指出“广度搜索”作为一个新兴且重要的应用场景，具有大规模、面向广度的检索特征。\n3.  **现有范式的失效**：当面对广度搜索时，现有的垂直框架陷入了困境。\n    *   **困境一（状态管理失效）**：现有系统依赖对话历史或自由文本来隐式管理检索目标。在长周期执行中，这会导致条目遗漏、检索冗余或目标错位。**结论**：需要一种显式的、持久的任务表示来可靠地维持大规模检索目标。\n    *   **困境二（经验复用缺失）**：现有系统通常为每个查询重新规划执行流程，没有抽象出跨查询共享的结构化范式。**结论**：需要一种机制来显式捕获和复用执行层面的模式，实现经验的积累和进化。\n4.  **范式转换的必要性**：上述局限性表明，广度搜索不应被视为垂直推理的延伸，而应被建模为一种**“水平结构化检索范式”**（Horizontal Structured Retrieval Paradigm），其核心在于组织、分配、跟踪和聚合大量弱耦合的检索任务。\n\n### 3. 研究问题\n基于上述观察与困境，作者试图回答的核心问题是：\n\n**“如何重构多智能体系统的执行范式，将广度搜索从低效的顺序垂直推理转化为可控的水平结构化检索过程，并实现跨任务的经验积累与进化？”**\n\n### 4. 假设与灵感：跨领域的隐喻\n*   **灵感来源**：作者将目光投向了数据库系统中的经典计算范式——**MapReduce**。\n*   **核心假设**：广度搜索本质上与大规模分布式数据处理同构。\n    *   如果能将广度搜索任务映射为MapReduce流程，就能利用其成熟的“分解-并行-聚合”逻辑来解决大规模检索的混乱问题。\n    *   MapReduce的显式状态（键值对）可以解决“困境一”中的目标丢失问题。\n*   **进阶假设**：仅仅结构化是不够的，系统还需要像人类一样“越用越聪明”。因此，需要引入**经验记忆**机制，利用历史成功/失败的执行轨迹来优化当前的决策（如批处理策略），从而解决“困境二”中的效率问题。\n\n### 5. 方法论构建：A-MapReduce 的诞生\n为了验证上述假设，作者构建了 A-MapReduce 框架，其思想演进如下：\n\n1.  **结构化映射**：\n    *   **Map阶段**：不再进行无休止的对话，而是将查询显式分解为一个**任务矩阵**（Task Matrix）和**模板**（Template）。每个原子任务对应矩阵中的一行，实现了目标的显式化。\n    *   **Reduce阶段**：设计一个归约器，将并行检索到的碎片化结果验证并聚合为统一的表格，确保结构一致性。\n\n2.  **执行决策参数化**：\n    *   将执行过程抽象为可控制的参数三元组 $\\Theta = (M, P, B)$（任务矩阵、模板、批处理策略）。这使得“如何执行”变成了一个可优化的变量，而不是黑盒的LLM生成过程。\n\n3.  **引入经验进化**：\n    *   建立**经验记忆**，存储历史任务的执行轨迹和效用反馈。\n    *   当新任务到来时，系统检索相似任务的成功经验作为“先验”，指导当前的批处理策略和任务分解。\n    *   通过这种方式，系统从“每次重新规划”进化为“基于经验的条件采样”，实现了执行效率的持续提升。\n\n**总结**：作者从发现“垂直范式”在“广度任务”上的水土不服出发，引入MapReduce作为结构化骨架解决混乱问题，再引入经验记忆解决效率问题，最终完成了从传统对话式Agent到结构化、进化式Agent的思维跃迁。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 8,
            "papers": [
                {
                    "title": "Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents",
                    "arxiv_id": "2602.02164",
                    "authors": "Pengfei He, Ash Fox, Lesly Miculicich, Stefan Friedli, Daniel Fabian, Burak Gokturk, Jiliang Tang, Chen-Yu Lee, Tomas Pfister, Long T. Le",
                    "summary": "Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.",
                    "category": "cs.LG",
                    "filter_reason": "1.  **核心判断 (符合)**: 该论文的核心贡献是提出了 \"Co-RedTeam\"，这是一个**多智能体框架**。它不仅仅是将现有的LLM作为工具应用于安全领域，而是构建了一个包含协调、记忆和反馈循环的智能体系统，符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标 (高度匹配)**: *   **多智能体**: 论文明确提出了 \"multi-agent framework\"，涉及智能体间的 \"coordinated discovery and exploitation\" 和 \"structured interaction\"。 *   **智能体能力**: 包含了 \"Planning\" (规划)、\"Memory\" (长期记忆、经验复用)、\"Self-Correction\" (基于执行反馈细化动作) 和 \"Tool Use\" (代码分析、执行环境)。 *   **演化机制**: 论文强调 \"execution-grounded iterative reasoning\" 和 \"learning from prior trajectories\"，这属于智能体通过环境反馈进行自我完善和迭代的机制，符合 \"Self-Evolving\" 的特征。 3.  **排除标准 (未触发)**: *   虽然论文的应用领域是网络安全，但其主要贡献在于**智能体的架构设计**（如何通过多智能体协作、记忆和反馈来提高性能），而非单纯的安全漏洞分析技术或防御策略。因此，它不属于“主要贡献是关于 Safety”的排除范畴，也不属于“非演化型应用”。 4.  **特殊与模糊情况**: 论文涉及智能体在复杂任务中的多步推理和规划，且具备基于反馈的自我修正能力，符合 Agentic AI 的定义。 综上所述，这篇论文属于多智能体系统与自我演化机制的研究，符合筛选要求。",
                    "summary2": "本文旨在解决现有LLM在自动漏洞发现与利用中面临的交互受限、执行基础薄弱及经验复用不足的问题。针对真实世界的代码库与执行环境，我们提出了一种名为Co-RedTeam的安全感知多智能体框架，该框架集成了安全领域知识、代码感知分析、执行驱动的迭代推理及分层长期记忆机制。在CyBench、BountyBench和CyberGym等安全基准上，通过漏洞检测与利用的成功率验证了其有效性，实现了超过60%的利用成功率及显著的检测性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **宏观背景与价值**：红队测试是现代网络安全的基石，通过主动发现和利用漏洞来防御攻击，对于评估安全态势和减少损失至关重要。\n2.  **现实痛点**：尽管标准（如CWE、OWASP）已经系统化了漏洞类型，但现实中的红队测试依然极其复杂、劳动密集且耗时。它需要深厚的专业知识、迭代假设测试以及对大规模代码库的推理，导致人工测试难以扩展，无法跟上软件演进的步伐。\n3.  **技术机遇与现状**：大语言模型（LLMs）的出现为自动化带来了希望，因为它们具备代码推理和生成能力。\n4.  **核心冲突**：现有的基于LLM的方法（单模型、单智能体或通用编程智能体）在现实安全任务中表现不佳。它们在多步推理、自适应攻击规划和鲁棒性探索方面存在显著缺陷，在基准测试中的成功率往往低于10%。\n5.  **逻辑缺口**：现有的自动化方法缺乏结构化的交互、执行落地的验证以及从过往攻击中学习的能力，无法模拟人类专家的系统化工作流。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑缺口，作者提出的核心研究问题为：\n\n**“如何设计一种自动化的红队测试框架，使其能够通过整合领域知识、执行驱动的推理机制以及经验复用能力，有效模拟人类专家的协作工作流，从而克服现有LLM方法在复杂漏洞发现与利用任务中的局限性？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者从观察到最终提出Co-RedTeam框架，经历了以下四个阶段的思维演进：\n\n#### 1. 观察与诊断：为什么现有的LLM“黑客”不行？\n*   **观察**：虽然LLM能写代码，但在实际的安全攻防中，简单的“提问-回答”模式或单次代码生成往往失败。\n*   **诊断**：作者发现根本原因在于**“脱节”**。\n    *   **与代码脱节**：缺乏对大型代码库结构的深入理解，仅停留在片段匹配。\n    *   **与现实脱节**：生成的Exploit（漏洞利用程序）没有在真实环境中运行验证，往往是“纸上谈兵”。\n    *   **与经验脱节**：每次任务都从零开始，不记得上次是怎么成功的，也不吸取失败的教训。\n*   **结论**：要解决安全问题，不能只靠更强的模型，而要靠更符合人类专家行为的**系统架构**。\n\n#### 2. 假设与类比：向人类专家学习\n*   **类比**：人类安全专家是如何工作的？他们不是一个人闷头干，而是分工协作。\n*   **假设**：如果构建一个多智能体系统，让不同的AI扮演不同的专家角色，并模拟人类专家的协作流程，就能解决单智能体的局限性。\n*   **关键洞察**：红队测试本质上是一个**“发现”**到**“利用”**的连续过程，这两个阶段所需的思维模式不同（前者偏静态分析，后者偏动态执行），因此需要**分阶段处理**。\n\n#### 3. 架构设计：从“单打独斗”到“协同作战”\n基于上述假设，作者构思了Co-RedTeam的骨架，将问题拆解为两个核心阶段：\n\n*   **阶段一：漏洞发现—— 模拟“分析师”与“审计员”的辩论**\n    *   *思考*：如何减少误报？人类分析师会互相审查。\n    *   *设计*：引入**Analysis Agent**（负责找漏洞，结合代码浏览工具和安全知识库）和**Critique Agent**（负责挑刺，验证证据链）。\n    *   *目的*：通过“分析-批评”的迭代循环，确保漏洞假设是有确凿证据支撑的，而不是幻觉。\n\n*   **阶段二：迭代利用—— 模拟“指挥官”与“执行者”的闭环**\n    *   *思考*：为什么Exploit会失败？因为环境配置、路径等细节往往出乎意料。单次生成很难成功。\n    *   *设计*：引入**Planner**（制定计划，根据反馈调整）、**Execution Agent**（在沙箱中实际运行命令）和**Evaluation Agent**（分析运行结果，告诉Planner发生了什么）。\n    *   *目的*：建立一个“计划-执行-评估”的闭环，让系统能像人一样，试错了就改，直到成功。\n\n#### 4. 进化机制：赋予系统“成长性”\n*   **思考**：人类专家越老越辣，是因为有经验。AI能不能也越用越强？\n*   *设计*：引入**分层长期记忆**。\n    *   *逻辑*：经验是分层次的。有的经验是“具体的命令”（怎么绕过某个防火墙），有的是“策略”（遇到SSRF该怎么测），有的是“漏洞模式”（某种代码结构必有洞）。\n    *   *实现*：将记忆分为**技术动作层**、**策略层**和**漏洞模式层**，让系统在未来的任务中能检索并复用这些经验。\n\n#### 5. 整合与控制：引入“指挥官”\n*   **思考**：这么多Agent（分析、批评、计划、执行、评估）怎么配合？会不会乱套？\n*   *设计*：引入**Orchestrator（编排器）**。\n*   *逻辑*：就像电影导演，Orchestrator不干具体的活，但负责控制流程（什么时候发现，什么时候攻击）、分配工具（谁能看代码，谁能执行命令）和决定何时结束。\n\n---\n\n### 总结：思想演进脉络\n\n作者的思想演进是从**“工具论”**（LLM作为代码生成工具）转向**“系统论”**（LLM作为协作智能体）的过程。\n\n1.  **起点**：发现现有LLM在安全领域的“无能”源于缺乏交互、验证和记忆。\n2.  **转折**：意识到必须模仿人类专家的**社会化分工**和**试错机制**。\n3.  **核心创新**：将红队测试解耦为**静态的发现阶段**（通过辩论保证质量）和**动态的利用阶段**（通过闭环保证落地），并用**记忆**连接过去与未来。\n4.  **最终形态**：Co-RedTeam——一个不仅会“想”，还会“做”，并且会“学”的自动化红队测试系统。"
                },
                {
                    "title": "Self-Consolidation for Self-Evolving Agents",
                    "arxiv_id": "2602.01966",
                    "authors": "Hongzhuo Yu, Fei Zhu, Guo-Sen Xie, Ling Shao",
                    "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”方向。 1.  **核心判断（第一步）**：论文的核心贡献是提出了一种“自我演化框架”，旨在解决LLM智能体作为静态系统无法通过终身交互进行演化的问题。这直接对应了研究课题中的“自我演化”类别，属于构建和改进LLM智能体的方法论研究，而非单纯的应用或基础设施研究。 2.  **正面指标匹配（第二步）**： *   **核心范式**：标题和摘要中明确出现了 `Self-Evolving Agents` 和 `Self-Consolidation`。 *   **演化机制**：论文提出了具体的演化机制，包括 `Contrastive Reflection`（对比性反思，从失败中学习）和 `Self-Consolidation`（自我整合，将非参数文本经验蒸馏为可学习参数），这完全符合 `Self-Reflection`、`Self-Improvement` 和 `Iterative Improvement` 的特征。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）**：论文虽然可能通过实验验证其效果，但其核心在于提出一种新的智能体演化机制（自我整合），而非将现有智能体作为工具应用到特定垂直领域。 综上所述，该论文深入探讨了LLM智能体如何通过反思和参数整合实现自我完善和迭代，精准契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在解决LLM智能体缺乏终身进化能力及现有方法受限于上下文窗口的问题。针对智能体在终身学习中的交互场景，我们提出了一种名为EvoSC的Self-Consolidation框架，结合对比反思与参数化自我整合机制，将历史经验内化为模型参数。在LifelongAgentBench（DB、OS、KG数据集）上通过任务成功率验证了其有效性，显著优于基线方法并克服了上下文限制。",
                    "summary_translation": "尽管大语言模型智能体展示了卓越的问题解决能力，但它们通常作为静态系统运行，缺乏通过终身交互进行演化的能力。现有的弥合这一差距的尝试主要依赖于检索成功的过往轨迹作为示例。然而，这一范式面临两个关键局限。首先，由于仅关注成功，智能体忽略了嵌入在失败尝试中的丰富教学价值，阻碍了它们识别和避免反复出现的陷阱。其次，不断累积的文本经验不仅增加了检索期间的时间消耗，而且不可避免地引入噪声并耗尽当前大语言模型的最大上下文窗口。为了解决这些挑战，我们为大语言模型智能体提出了一种新颖的自演化框架，该框架引入了一种互补的演化机制：首先，引入了一种对比反思策略，以显式总结易错模式并捕获可复用的见解。其次，我们提出了一种自巩固机制，将非参数化文本经验蒸馏为紧凑的可学习参数。这使得智能体能够将大量的历史经验直接内化到其潜在空间中。大量的实验证明了我们的方法在智能体长期演化中的优势。",
                    "inspiration_trace": "基于对论文《Self-Consolidation for Self-Evolving Agents》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与背景切入\n**逻辑起点：从“静态”到“动态”的鸿沟**\n作者首先观察到，尽管大语言模型（LLM）智能体在解决复杂任务上表现出色，但它们本质上仍是**静态系统**。\n*   **现状**：智能体通常在“任务隔离”的范式下运行，每次会话结束后状态重置。\n*   **痛点**：这导致智能体无法像人类一样，通过终身的交互积累经验、持续进化。它们无法利用过去的知识来优化未来的决策。\n\n### 2. 现有方案的“故事”逻辑与批判\n作者在Introduction中通过“讲故事”的方式，层层剥开了现有研究试图解决上述问题但依然存在的局限性：\n\n*   **第一层：现有的尝试**\n    为了让智能体“记住”过去，当前主流的方法是**显式的文本回放**。即：当新任务到来时，检索过去成功的轨迹作为演示，放入上下文中辅助推理。\n\n*   **第二层：被忽视的“失败价值”**\n    *   **观察**：现有方法几乎只关注**成功**的经验。\n      **批判**：这是一种认知偏差。实际上，**失败的尝试**中蕴含着极高的教学价值。智能体往往在特定的逻辑节点出错，如果只看成功案例，就无法识别并避免那些反复出现的“陷阱”。这导致智能体不断重蹈覆辙。\n\n*   **第三层：不可逾越的“物理瓶颈”**\n    *   **观察**：随着交互时间的推移，历史文本经验会不断累积。\n    **批判**：LLM的上下文窗口是**固定且有限**的。\n    1.  **容量限制**：为了塞入更多历史，只能截断轨迹或采用启发式筛选，导致上下文依赖关系丢失。\n    2.  **噪声干扰**：过多的文本演示会引入冗余信息，稀释模型的注意力，反而降低推理精度。\n    3.  **计算开销**：检索和推理的时间成本随历史增加而线性增长。\n\n### 3. 核心研究问题\n基于上述观察与批判，作者将复杂的现实困境凝练为一个具体的科学问题：\n\n**“LLM智能体如何在不消耗有限上下文窗口的前提下，有效地从成功与失败的双重历史中提取并内化知识，从而实现终身进化？”**\n\n### 4. 思想演进与假设提出\n为了回答上述问题，作者的思考路径经历了从“模仿人类”到“架构创新”的演进：\n\n*   **假设一：对比学习能提炼“关键差异”**\n    *   **思考**：既然失败经验很重要，那么单纯记录失败是不够的。必须将“失败轨迹”与“成功轨迹”放在一起对比。\n    *   **推论**：通过对比，可以精准定位导致推理分叉的“错误点”。这比单纯看成功案例更能揭示逻辑漏洞。因此，需要一种**对比性反思机制**。\n\n*   **假设二：人类记忆机制是“显式”与“隐式”的结合**\n    *   **思考**：人类学习时，既会通过反思获得显性的经验教训（短期记忆），也会通过睡眠等机制将海量经历压缩为直觉（长期记忆/参数）。\n    *   **推论**：为了解决上下文窗口限制，不能只依赖文本（显式记忆）。必须将海量的历史轨迹**蒸馏**进模型的参数空间。\n    *   **核心洞察**：将冗长的文本轨迹转化为紧凑的**可学习参数**。这样，智能体在推理时不需要重新读取所有历史，而是直接调用内化的“直觉”，从而绕过物理瓶颈。\n\n### 5. 方法论构建\n基于上述假设，作者最终构建了 **EvoSC (Self-Consolidation)** 这一双重进化框架：\n\n*   **模块一：非参数化的对比提取（短期/显式进化）**\n    *   **设计**：不直接回放原始轨迹，而是通过Prompt让LLM对比“成功”与“失败”的案例。\n    *   **产出**：提取两类高价值文本指导——**“易错点洞察”**（避免什么）和**“成功模式”**（模仿什么）。这解决了“忽视失败”的问题。\n\n*   **模块二：参数化的轨迹整合（长期/隐式进化）**\n    *   **设计**：引入一个“教师-学生”蒸馏过程。教师模型拥有大量历史轨迹进行推理，学生模型仅依靠少量轨迹加上一个**可学习的Prompt ($P_\\theta$)**。\n    *   **目标**：训练学生模型去模仿教师的决策。\n    *   **结果**：$P_\\theta$ 内部化了大量历史经验的“精华”。在推理时，只需加载这个轻量级的 $P_\\theta$，无需占用大量Token，从而解决了“上下文窗口限制”和“噪声干扰”的问题。\n\n### 总结\n作者的思考过程是从**“智能体缺乏进化能力”**这一宏观现象出发，通过批判现有**“文本回放”**方法的**“成功偏见”**和**“上下文瓶颈”**，最终受人类认知机制启发，提出了**“对比反思（显式）+ 自整合（隐式）”**的双轨制解决方案，实现了在不增加推理负担的前提下，让智能体从成败中终身进化。"
                },
                {
                    "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
                    "arxiv_id": "2602.01776",
                    "authors": "Mingyue Cheng, Xiaoyu Tao, Qi Liu, Ze Guo, Enhong Chen",
                    "summary": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 与 Self-Evolving 定义）**： *   论文的核心贡献是提出了 \"Agentic Time Series Forecasting (ATSF)\" 这一新框架，主张将传统的静态预测重构为一个由 **感知、规划、行动、反思和记忆** 组成的智能体过程。 *   这不仅仅是将现有智能体作为工具应用，而是**构建**了一个新的 Agentic 工作流范式，属于构建和改进 LLM 智能体的范畴。 2.  **正面指标（高度匹配）**： *   **核心范式**：明确提出了 `Agentic AI` 的概念。 *   **智能体能力**：摘要中明确列出了 `Planning`（规划）、`Memory`（记忆）、`Reflection`（反思）以及 `Tool Use`（与工具交互）。 *   **演化机制**：论文强调智能体能够 \"incorporate feedback from outcomes\"（结合结果反馈）并 \"evolve through experience accumulation\"（通过经验积累进行演化），这直接对应了筛选标准中的 `Self-Evolving` 和 `Iterative Improvement`。 3.  **排除标准与特殊情况处理**： *   虽然论文涉及 \"Time Series Forecasting\"（时间序列预测）这一特定领域，但它并未被归类为“非演化型应用”。根据第四步的特殊情况处理规则，该论文的核心在于提出一种新的“自我演化”机制和 Agentic 工作流，而不仅仅是解决预测问题。它定义了智能体如何通过反思和经验来完善自身，符合保留条件。 *   论文不涉及安全对齐、多模态视觉或图技术等排除项。 综上所述，该论文在单智能体（规划、记忆、反思）和自我演化（经验积累、迭代完善）方向上做出了明确的方法论贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在超越以模型为中心的静态预测，解决时间序列预测在自适应和多轮场景下的不足。针对复杂动态的预测任务，我们提出了一种Agentic Time Series Forecasting (ATSF) 范式，将预测重构为包含感知、规划、行动、反思和记忆的智能体过程。本文通过理论分析和三种实现范式的探讨，验证了该框架在提升预测灵活性和决策支持方面的有效性。",
                    "summary_translation": "Time series forecasting (时间序列预测) 传统上被构建为一个 model-centric (以模型为中心)、static (静态) 且 single-pass (单次) 的预测问题，即将 historical observations (历史观测值) 映射到 future values (未来值)。尽管这一 paradigm (范式) 推动了实质性进展，但在 adaptive (自适应) 和 multi-turn settings (多轮次设置) 中，它被证明是不充分的，因为在这些设置中，预测需要 informative feature extraction (信息丰富的特征提取)、reasoning-driven inference (推理驱动的推断)、iterative refinement (迭代优化) 以及随时间的 continual adaptation (持续适应)。在本文中，我们主张 Agentic time series forecasting (ATSF) (智能体时间序列预测)，它将预测重构为一个由 perception (感知)、planning (规划)、action (行动)、reflection (反思) 和 memory (记忆) 组成的 agentic process (智能体过程)。ATSF 不仅关注 predictive models (预测模型)，而是强调将预测组织为一个 agentic workflow (智能体工作流)，该工作流能够 interact with tools (与工具交互)，incorporate feedback (整合反馈) 来自 outcomes (结果)，并通过 experience accumulation (经验积累) 进行 evolve (演进)。我们概述了三种具有代表性的 implementation paradigms (实现范式)——workflow-based design (基于工作流的设计)、agentic reinforcement learning (智能体强化学习) 和 hybrid agentic workflow paradigm (混合智能体工作流范式)——并讨论了从 model-centric prediction (以模型为中心的预测) 转向 agentic forecasting (智能体预测) 时产生的 opportunities and challenges (机遇与挑战)。总而言之，这一 position (立场) 旨在将 agentic forecasting (智能体预测) 确立为 Time series forecasting (时间序列预测) 交叉领域未来研究的 foundation (基础)。",
                    "inspiration_trace": "基于您提供的论文内容，我为您系统性地推演了作者提出“智能体时间序列预测”这一核心方法的逻辑链。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个从“宏观价值”到“现实错位”再到“范式转型”的叙事逻辑：\n\n1.  **宏观价值锚定**：\n    *   首先指出时间序列预测在能源、医疗、金融等领域的核心地位。\n    *   **关键点**：强调预测不仅仅是输出数字，而是为了支持在不确定环境下的**决策过程**（如资源分配、政策调整）。\n\n2.  **现状与理想的错位**：\n    *   **现状**：现有研究主要遵循“以模型为中心”的范式。即把预测看作一个监督学习问题（历史数据 $\\to$ 未来数值），重点在于改进模型架构、表征学习或扩大规模。\n    *   **理想（现实实践）**：现实中的预测是**自适应且多轮次**的。在生成预测前，专家需要解释数据、提取特征；在生成后，需要评估、质疑和修正；在长期使用中，系统需要适应环境变化。\n\n3.  **核心矛盾揭示**：\n    *   尽管现有模型在精度上有所提升，但它们本质上是**静态、单次执行**的。\n    *   这种“单次映射”的假设忽略了现实预测中至关重要的环节：信息特征提取、推理驱动的推断、迭代修正以及持续的经验积累。\n\n4.  **提出转型方向**：\n    *   为了弥合这一差距，必须超越“以模型为中心”的预测，转向“智能体时间序列预测（ATSF）”，将预测重构为一个包含感知、规划、行动、反思和记忆的智能体过程。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑，本文试图回答的核心研究问题是：\n\n**“如何将时间序列预测从静态、单次执行的模型映射问题，重构为一个能够进行感知、规划、工具交互、反思和经验积累的自适应、多轮次智能体过程？”**\n\n---\n\n### 三、 核心方法产出的逻辑推演链\n\n以下是从宏观观察到具体方法论的演进过程：\n\n#### 第一阶段：观察与痛点识别（从“模型能力”到“任务本质”的反思）\n\n*   **观察**：作者回顾了时间序列预测的发展史（从统计模型到机器学习，再到深度学习和基础大模型）。\n*   **发现**：虽然技术手段在变，但**范式**没变。所有方法都遵循“固定输入 $\\to$ 预测模型 $\\to$ 单次输出”的模式。\n*   **痛点**：这种范式存在结构性缺陷——它是**静态**的（无法调整目标）、**封闭**的（不使用外部工具）、**单次**的（无法自我修正）且**无记忆**的（无法积累经验）。\n*   **结论**：问题的根源不在于模型不够强，而在于**问题的定义方式**限制了系统在复杂、动态现实场景中的表现。\n\n#### 第二阶段：假设与概念迁移（从“人类专家”到“AI智能体”的类比）\n\n*   **类比思考**：人类专家是如何做预测的？他们不是一上来就写公式，而是先**感知**数据，**规划**策略，**调用**工具（如计算器、查资料），**反思**结果是否合理，并**记住**过去的经验。\n*   **技术可行性**：随着大语言模型（LLM）的发展，AI 已经具备了推理、规划和工具使用的能力，这为模仿人类专家的预测过程提供了技术基础。\n*   **核心假设**：如果将预测任务视为一个“智能体”的行为，而非单纯的数学函数拟合，就能解决传统范式无法处理的自适应和多轮交互问题。\n\n#### 第三阶段：框架构建（定义“智能体预测”的五要素）\n\n为了将上述假设落地，作者解构了预测过程，定义了 ATSF 的五个核心组件，将“预测”这一动作拆解为一个完整的认知循环：\n\n1.  **感知**：\n    *   *思考*：输入的数据往往是嘈杂的，不能直接喂给模型。\n    *   *定义*：必须先从原始数据中提取与任务相关的信息，过滤噪声，这不仅是预处理，更是一个自适应的认知过程。\n\n2.  **规划**：\n    *   *思考*：预测不是盲目的，需要先定目标。\n    *   *定义*：在行动之前，先制定目标、分解任务（例如：先看趋势，再看季节性），并制定策略。且规划是动态的，可随情况变化而调整。\n\n3.  **行动**：\n    *   *思考*：预测只是手段之一，不是全部。\n    *   *定义*：将“调用预测模型”视为一种“行动”。除此之外，行动还包括调用统计工具、检索外部信息等。这允许系统灵活组合不同的传统方法。\n\n4.  **反思**：\n    *   *思考*：第一次预测往往不准，需要自我检查。\n    *   *定义*：引入自我评估机制。系统需要检查预测结果是否符合预期、假设是否成立，并决定是否需要修正或重新预测。\n\n5.  **记忆**：\n    *   *思考*：经验比单次结果更重要。\n    *   *定义*：不仅要更新模型参数，还要显式地存储“经验”（如有效的策略、失败的案例），以便在未来类似场景中复用，实现跨实例的迁移。\n\n#### 第四阶段：实施路径探索（从“理论”到“工程”的映射）\n\n有了框架，如何具体实现？作者提出了三种递进的实施范式：\n\n1.  **工作流设计**：\n    *   *逻辑*：最直接的方式。像写代码一样，把感知、规划、行动等步骤通过预定义的流程图串起来。\n    *   *优点*：稳定、可解释。\n\n2.  **智能体强化学习**：\n    *   *逻辑*：让系统自己学。通过与环境交互，利用奖励信号来学习如何做决策（如何时该查数据，何时该修正预测）。\n    *   *优点*：能发现人类未知的策略，具备自主进化能力。\n\n3.  **混合智能体工作流**：\n    *   *逻辑*：折中方案。整体流程是固定的（保证稳定性），但在关键节点引入强化学习（保证灵活性）。\n    *   *优点*：平衡了稳定与适应。\n\n#### 第五阶段：价值重定义（从“预测精度”到“决策支持”的升华）\n\n*   **最终思考**：ATSF 的目的不是为了在基准数据集上把 MSE（均方误差）降低 0.1%，而是为了构建一个能像人类专家一样思考的系统。\n*   **产出**：这不仅仅是一个新模型，而是一个新的**研究议程**。它强调从“模型迭代”转向“系统与工具的进化”，让预测真正服务于复杂的现实决策。"
                },
                {
                    "title": "What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?",
                    "arxiv_id": "2602.01611",
                    "authors": "Weizheng Gu, Chengze Li, Zhuohao Yu, Mengyuan Sun, Zhibang Yang, Wei Wang, Hongrui Jia, Shikun Zhang, Wei Ye",
                    "summary": "Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.",
                    "category": "cs.LG",
                    "filter_reason": "1.  **核心判断 (符合)**: 这篇论文的本质是对LLM智能体的训练机制（Trajectory-SFT）进行深入分析和诊断。它并非将智能体作为工具去解决某个外部领域（如生物、金融）的问题，而是研究智能体本身在训练过程中“学到了什么”。论文提出的PIPE协议和IR指标旨在解决智能体训练中的核心缺陷（即过度依赖接口而非语义），这直接属于**改进**LLM智能体构建方法的范畴。 2.  **正面指标 (高度相关)**: *   **Agentic AI**: 论文明确研究对象是“interactive agents”（交互式智能体）。 *   **工具使用**: 论文的核心讨论点在于智能体是掌握了真正的“semantic tool-use”（语义工具使用）还是仅仅记忆了接口模式，这是智能体能力的关键组成部分。 *   **自我演化**: Trajectory-SFT（轨迹监督微调）是一种让智能体从经验中学习的技术，属于智能体自我完善和迭代的一种形式。论文对该演化机制的有效性和局限性进行了剖析。 3.  **排除标准 (无冲突)**: 论文不涉及安全对齐、多模态视觉（除非作为工具，但本文核心是接口分析）、图技术或基础设施优化。 4.  **特殊与模糊情况处理**: *   论文虽然涉及评估，但其评估的目的是为了揭示训练机制的问题，从而指导如何构建更好的智能体，这与单纯的应用型基准测试有本质区别。 *   它关注的是智能体与环境的交互和工具使用能力，而非单纯的LLM内部推理（如数学计算），因此符合Agentic AI的范畴。 **结论**: 该论文通过揭示Trajectory-SFT训练中的“接口捷径”问题，为构建更鲁棒、更具泛化能力的LLM智能体提供了重要的方法论见解，完全符合“构建、改进或演化LLM智能体”的研究目标。",
                    "summary2": "本文旨在解决Trajectory-SFT中语义学习与界面捷径混淆的问题。针对LLM智能体的评估，我们提出了一种PIPE协议，通过最小化重写环境界面来诊断界面依赖，并引入Interface Reliance (IR)指标。我们在AgentBench和AgentGym的16个环境中，通过任务成功率和IR指标验证了其有效性，揭示了Trajectory-SFT会显著放大界面捷径效应。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个层层递进的叙事逻辑，旨在揭示当前 Agent 评估体系中的核心漏洞：\n\n1.  **背景与现状**：大语言模型（LLM）正演变为能够与外部环境交互的智能体。为了提升这些智能体的能力，业界普遍采用 **Trajectory-SFT**（基于轨迹的监督微调），即让模型模仿专家在特定环境中的交互行为。这种方法在标准基准测试中显著提高了任务成功率。\n2.  **提出疑虑**：然而，现有的基准测试存在一个根本性的盲点。训练数据与测试数据使用的是**完全相同的环境接口**。这意味着，模型在测试中的高分可能源于两种截然不同的机制：\n    *   **语义学习**：模型真正理解了工具的功能和任务逻辑。\n    *   **接口捷径**：模型仅仅死记硬背了训练时见过的特定接口名称或交互模式。\n3.  **揭示矛盾**：这两种机制在原始接口下能产生完全相同的得分，但在面对接口变化时表现迥异。仅凭基准测试分数，我们无法区分模型是“真的学会了”还是“仅仅背下来了”。\n4.  **指出风险**：如果 Trajectory-SFT 主要是通过“接口捷径”来提升分数，那么这种提升是脆弱的。一旦环境接口发生微小变化，模型性能将急剧下降，这表明当前的评估体系高估了模型的泛化能力。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“Trajectory-SFT 所带来的性能提升，究竟源于对工具语义的真正理解，还是仅仅依赖于对训练时特定接口模式的记忆？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n从宏观观察到具体方法论的提出，作者的思考路径经历了以下四个阶段：\n\n#### 1. 观察与反思：高分背后的“虚假繁荣”\n*   **现象**：Trajectory-SFT 能让 Agent 在 Benchmark 上拿高分。\n*   **反思**：这种高分是“环境不变”的产物。就像学生考试时题目和练习题一模一样，考得好不代表掌握了知识点，可能只是背住了答案。这里的“答案”就是接口的表面形式（如函数名 `search`）。\n*   **初步判断**：现有的评价指标（Success Rate）无法识别这种“作弊”行为。\n\n#### 2. 假设构建：解构成功的来源\n*   **概念定义**：作者将 Agent 的成功解构为两个正交的维度：\n    *   **维度 A（语义理解）**：这是环境不变的通用能力，关注“工具是做什么的”。\n    *   **维度 B（接口依赖）**：这是环境特定的表面能力，关注“工具叫什么名字”。\n*   **核心假设**：Trajectory-SFT 可能在提升维度 A 的同时，过度强化了维度 B。我们需要一种方法，在保持维度 A 不变的情况下，专门测试维度 B 的干扰程度。\n\n#### 3. 实验设计思想：控制变量的“压力测试”\n*   **思维实验**：如何在不改变任务难度的情况下，打破维度 B 的依赖？\n*   **策略**：**“只换皮，不换骨”**。\n    *   保持任务语义、环境动力学、工具功能完全不变。\n    *   仅对接口进行最小化扰动，例如将 `search` 改为 `find`，或者改为无意义的符号 `z1`。\n*   **预期结果**：\n    *   如果模型依赖的是**语义**，它应该能读懂新的描述，性能保持稳定。\n    *   如果模型依赖的是**接口捷径**，它会因为找不到记忆中的字符串而崩溃，性能大幅下降。\n*   **方法论诞生**：这一思想直接催生了 **PIPE (Perturb Interface Protocol for Evaluation)**，即通过接口重写来构建诊断性测试集。\n\n#### 4. 量化与验证：从定性到定量\n*   **进一步思考**：仅仅看性能下降还不够，我们需要一个指标来量化模型对旧接口的“执念”程度。\n*   **指标设计**：设计一个双接口环境，同时提供“旧接口”和“新接口”，看模型更倾向于调用哪一个。\n*   **指标诞生**：由此提出了 **Interface Reliance (IR)** 指标，用于精确度量模型对训练时接口的偏好程度。\n*   **验证闭环**：将 PIPE 和 IR 应用于主流 Agent（如 AgentBench, AgentGym），结果证实了 Trajectory-SFT 确实会诱导严重的接口捷径行为，从而验证了最初的假设。\n\n---\n\n### 总结\n\n作者的思考过程是从**对现有评估指标的怀疑**出发，通过**解构智能体能力的本质**（语义 vs. 接口），提出了**控制变量的诊断思想**（最小化接口扰动），最终形成了一套**完整的评估协议与量化指标**（PIPE & IR），从而揭示了 Trajectory-SFT 背后隐藏的脆弱性。"
                },
                {
                    "title": "LocalV: Exploiting Information Locality for IP-level Verilog Generation",
                    "arxiv_id": "2602.00704",
                    "authors": "Hanqi Lyu, Di Huang, Yaoyu Zhu, Kangcheng Liu, Bohan Dou, Chongxiao Li, Pengwei Jin, Shuyao Cheng, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen",
                    "summary": "The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”方向**： 论文明确提出了 **LocalV，一个多智能体框架**。其核心贡献在于构建了一个新的智能体架构，旨在解决现有智能体系统在处理长文档和长代码生成任务时的扩展性问题。这直接对应了研究课题中的“多智能体”方向。 2.  **具备核心智能体能力**： 摘要中详细描述了该框架集成了 **任务规划**、**局部代码生成**、**接口一致的合并** 以及 **AST引导的局部感知调试**。这些功能涉及智能体的规划能力、工具使用（仿真和波形分析）以及自我修正机制，完全符合筛选标准中的正面指标。 3.  **属于方法论创新而非单纯应用**： 虽然论文的应用场景是硬件设计（Verilog生成），属于特定领域应用，但论文的重点在于提出了一种新的**多智能体协作机制**（利用信息局部性分解任务），而不是简单地将现有的智能体工具应用到该领域。它解决了智能体在处理长上下文和复杂验证循环时的通用性挑战，这属于对智能体框架本身的改进和构建。 综上所述，该论文的核心在于构建和改进多智能体系统以解决复杂任务，符合“LLM智能体及其演化”的研究范围。",
                    "summary2": "本文旨在解决IP级Verilog生成中长文档处理、长代码生成及复杂调试的挑战。针对工业级IP设计中的长自然语言规范，我们提出了一种名为LocalV的多智能体框架，利用信息局部性将生成任务分解为短文档到短代码的子任务。在REAL BENCH基准测试上，通过功能通过率验证了其有效性，实现了45.0%的通过率，显著优于现有最先进方法。",
                    "summary_translation": "寄存器传输级 (RTL) 代码的生成是数字硬件设计中至关重要但劳动密集型的一环，传统上要求工程师手动将复杂的规格说明转换为数千行可综合的硬件描述语言 (HDL) 代码。尽管大语言模型 (LLMs) 在自动化该流程方面展现出潜力，但现有方法——包括微调的领域特定模型和先进的基于智能体的系统——在扩展至工业级 IP 级设计任务时仍面临困难。我们识别出三个关键挑战：(1) 处理篇幅长且细节详尽的文档，关键的接口约束往往淹没在不相关的子模块描述中；(2) 生成长篇幅的 RTL 代码，随着输出长度的增加，语法和语义正确性会急剧下降；(3) 应对复杂的调试循环，这需要通过仿真和波形分析来进行功能验证。为了克服这些挑战，我们提出了 LocalV，这是一个利用模块化硬件设计中信息局部性的多智能体框架。LocalV 将“长文档生成长代码”的问题分解为一组“短文档生成短代码”的任务，从而实现了可扩展的代码生成与调试。具体而言，LocalV 集成了分层文档划分、任务规划、局部代码生成、接口一致性合并以及 AST 引导的感知局部性调试。在 IP 级 Verilog 生成基准 RealBench 上的实验表明，LocalV 显著优于最先进的 (SOTA) LLM 和智能体，实现了 45.0% 的通过率，而对比方法仅为 21.6%。",
                    "inspiration_trace": "基于论文《LocalV: Exploiting Information Locality for IP-level Verilog Generation》的内容，以下是对作者产出该文章核心方法的逻辑链推演及思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从“学术成功”到“工业失效”的冲突叙事，逻辑链条如下：\n\n1.  **背景铺垫**：寄存器传输级（RTL）代码生成是数字硬件设计中的核心步骤，传统上极其耗时且容易出错。\n2.  **现有技术的繁荣**：大型语言模型（LLMs）在自动化代码生成方面展现出巨大潜力。早期的通用模型和后来的领域微调模型、甚至基于Agent的系统（如VerilogCoder, MAGE），在学术基准测试（如VerilogEval）上都取得了令人瞩目的成绩。\n3.  **现实的鸿沟**：然而，当这些技术应用到真实的工业级IP设计任务（如RealBench）时，性能出现了断崖式下跌。工业级任务的特征是：文档极长（平均197.3行 vs 学术的5.7行）、代码极长（平均241.2行 vs 学术的15.8行）。\n4.  **直接后果**：直接使用现有的SOTA模型或Agent，往往导致输出甚至无法通过语法检查，根本无法满足工业界的高要求。\n5.  **核心挑战归纳**：作者将这种失效归结为三大具体挑战：\n    *   **长文档处理**：关键接口约束被淹没在无关的子模块描述中，导致模型产生“幻影”信号或端口不匹配。\n    *   **长代码生成**：随着输出长度增加，语法和语义正确性急剧下降，模型难以维持长序列的逻辑一致性。\n    *   **复杂的调试过程**：IP级验证依赖波形分析，调试循环极其繁琐，需要将错误精准回溯到具体的规范段落。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图解决的核心研究问题可总结为：\n\n**“面对工业级IP设计中‘长文档、长代码、复杂调试’的三重挑战，如何利用LLMs实现高可靠性的Verilog代码生成？”**\n\n---\n\n### 三、 核心方法的逻辑演进链（思考过程还原）\n\n作者从发现问题到提出LocalV框架，经历了一个从现象观察到理论假设，再到方法论设计的完整思维演进：\n\n#### 1. 宏观观察与痛点定位\n*   **现象**：现有的LLM在处理短小的学术Verilog任务时表现良好，但一旦面对工业级的“长文档输入”和“长代码输出”，性能就崩塌。\n*   **思考**：为什么模型会失效？是因为模型不够聪明吗？不，是因为任务规模超出了模型的有效注意力范围。模型被过量的信息（无关的子模块描述）淹没，导致在生成时丢失关键细节，且无法在长序列生成中保持语法结构。\n\n#### 2. 深度洞察与假设提出\n*   **类比人类思维**：人类工程师在处理复杂的IP设计时，不会一次性读完几千页的文档然后从头写到尾。而是采用模块化思维：先看整体结构，然后针对某个子模块，只去查阅该子模块相关的文档章节进行编写。\n*   **核心假设**：硬件设计具有天然的**模块化**特性，这意味着**信息局部性**存在于IP级规范中。即：生成某一段特定的RTL代码，主要依赖于规范文档中特定的局部片段，而不需要全局上下文。\n*   **假设验证**：作者通过量化分析（计算信息熵），证实了硬件规范的信息局部性显著高于通用软件任务（如LeetCode题目）。这为“分而治之”的策略提供了理论依据。\n\n#### 3. 方法论推导\n*   **策略转变**：既然“信息局部性”成立，那么解决“长文档到长代码”难题的最优解，就不是去训练一个更强大的模型来硬抗长上下文，而是**将任务分解**。\n*   **核心思想**：将一个复杂的“长文档 -> 长代码”生成任务，转化为一系列并行的“短文档 -> 短代码”生成任务。\n*   **系统设计**：为了实现这一思想，需要一个能够支持检索、规划和局部修正的系统架构。这自然导向了**多智能体框架**的设计。\n\n#### 4. 框架构建\n基于上述推导，作者构建了LocalV框架，其每个组件都直接对应解决前述的三大挑战：\n\n*   **预处理与索引**：\n    *   *目的*：解决“长文档处理”挑战。\n    *   *逻辑*：既然信息是局部的，就需要一种机制能快速找到与当前任务相关的文档片段。因此建立了分层索引（语义级+词汇级）。\n\n*   **规划与生成**：\n    *   *目的*：解决“长代码生成”挑战。\n    *   *逻辑*：利用Planner Agent生成代码骨架，将大任务拆解为子任务。RTL Agent只专注于根据检索到的局部文档生成短代码片段，从而降低生成难度，提高语法正确率。\n\n*   **合并与调试**：\n    *   *目的*：解决“复杂调试”挑战。\n    *   *逻辑*：当出现错误时，不需要重写整个代码。利用AST（抽象语法树）定位错误信号，通过Retriever Agent回溯到相关的局部文档，由Debugger Agent进行针对性的局部修复。\n\n#### 5. 总结\n作者的思考路径是从**“规模导致的失效”**出发，通过**“信息局部性”**这一关键洞察，找到了**“分而治之”**的解题钥匙，最终设计出一套模仿人类工程师模块化工作流的**多智能体系统**，从而实现了工业级RTL生成的突破。"
                },
                {
                    "title": "Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models",
                    "arxiv_id": "2602.00129",
                    "authors": "Yixuan Liang",
                    "summary": "Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 定义）**： 论文提出的 `CodePilot` 框架不仅仅是一个简单的应用，而是一个构建了特定交互机制的 LLM 智能体系统。它结合了符号搜索（MCTS）与神经语言模型，通过“探索-反馈-修正”的闭环来解决复杂问题。这完全符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合“单智能体”的核心关注点**： *   **规划**: 论文核心使用了 `Monte Carlo Tree Search (MCTS)` 来探索多样化的补丁轨迹，这是一种高级的规划和搜索策略，属于智能体在复杂任务中进行多步推理的典型范式。 *   **工具使用与环境交互**: 智能体利用 `execution feedback`（执行反馈）作为奖励信号，这意味着智能体能够调用代码执行环境作为工具，并根据环境输出调整行为。 *   **自我反思/修正**: 框架包含 `confidence-calibrated generation` 和基于反馈的 `refinement` 机制，使智能体能够根据执行结果自我评估并改进输出，符合自我反思和自我修正的特征。 3.  **排除标准检查**： *   虽然论文的应用领域是“程序修复”（软件工程），但它并未被归类为“非演化型应用”。因为论文的核心贡献在于提出了一种新的**Agentic框架**（MCTS + LLM + Execution Feedback），而不是简单地将现有智能体（如直接调用 ChatGPT）应用到该领域。 *   论文不涉及安全对齐、多模态视觉或图神经网络等排除领域。 综上所述，该论文在单智能体的规划、工具使用和自我修正方面具有明确的创新贡献，属于 Agentic AI 的研究范畴。",
                    "summary2": "本文旨在解决自动程序修复中缺乏长期规划能力的问题。针对GitHub issue修复场景，我们提出了一种CodePilot混合框架，结合Qwen3大语言模型与Monte Carlo Tree Search。该方法利用分层故障定位和执行反馈引导的MCTS进行补丁合成。在SWE-bench Lite数据集上，通过Resolve Rate验证了其有效性，达到了24.67%的解决率，显著优于基线。",
                    "summary_translation": "由于需要长视距推理以及自回归解码存在局限性，利用大语言模型在仓库级别进行自动程序修复仍然面临挑战。我们提出了CodePilot，这是一个混合框架，集成了蒙特卡洛树搜索与大语言模型，旨在针对真实的GitHub问题实现执行引导的程序修复。CodePilot执行从仓库级别到文件和函数级别的分层故障定位，利用MCTS探索多样化的补丁轨迹，并将执行反馈作为奖励信号来指导搜索和优化。该框架进一步引入了置信度校准生成机制，以选择性地优化低置信度的输出结果。在SWE-bench Lite数据集上的实验表明，CodePilot在使用开放权重模型的情况下实现了24.67%的问题解决率，优于具有可比性的基线方法。这些结果表明，将符号搜索与神经语言模型相结合，是实现可扩展且具备执行感知能力的软件工程自动化的一种有效策略。",
                    "inspiration_trace": "基于您提供的论文内容，我为您系统性地推演了作者产出这篇论文的思考过程。以下是逻辑链条的还原：\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在引出当前研究的必要性：\n\n1.  **宏观背景与挑战**：现代软件维护面临大规模代码库复杂性的挑战，自动化成为关键痛点。\n2.  **现有能力的局限**：虽然大语言模型（LLM）在简单代码生成和调试上表现出色，但在处理真实的GitHub Issue时依然困难。原因在于现实世界的修复涉及复杂的依赖关系和需要多步推理的逻辑。\n3.  **当前研究趋势**：学术界开始转向基于Agent的框架，试图通过结合检索机制和生成能力，模拟人类开发者的工作流来导航代码库。\n4.  **核心痛点揭示**：尽管有上述进展，现有方法存在两个致命缺陷：\n    *   **贪婪解码的局限**：标准自回归模型一旦选择了低概率的Token，就无法回退或探索其他路径，导致生成的补丁是次优的，且无法通过简单的提示策略恢复。\n    *   **缺乏验证机制**：模型在没有外部验证信号的情况下，无法区分“高置信度的正确代码”和“看似合理的幻觉”。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条的终点，作者显式（或隐式）地提出了本文旨在解决的核心研究问题：\n\n**“我们如何克服自回归生成的贪婪本质和缺乏外部验证的缺陷，以有效地解决复杂的、多步骤的软件修复任务？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n从观察到假设，再到最终的方法论，作者的思考路径如下：\n\n#### 1. 观察与诊断：从“生成”到“决策”的视角转换\n*   **观察**：传统的代码修复将问题视为“文本生成任务”，即给定Bug描述，直接生成修复后的代码。这种方式本质上是线性的、一次性的。\n*   **诊断**：代码修复实际上是一个**决策过程**。在修复的每一步，开发者都有多种选择（修改哪行？用什么逻辑？）。现有的LLM因为贪婪解码，一旦走错一步（选错Token），就会陷入死胡同，无法像人类一样“回溯”并尝试另一条路径。\n\n#### 2. 假设提出：引入“搜索”机制\n*   **假设**：如果将补丁生成过程建模为一个**树搜索问题**，而不是单纯的序列生成问题，就可以解决贪婪解码带来的局部最优问题。\n*   **灵感来源**：借鉴AlphaGo等成功案例，利用**蒙特卡洛树搜索（MCTS）**。MCTS允许系统在决策树中探索多条路径，并在事后评估哪条路径最好，从而避免了“一条路走到黑”。\n\n#### 3. 机制设计：如何定义“奖励”与“搜索空间”\n*   **奖励信号**：在围棋中，胜负是明确的。在代码修复中，什么是“好”的补丁？作者将**执行反馈**作为奖励信号。代码能通过测试就是高分，不能通过就是低分。这使得搜索有了明确的方向。\n*   **搜索空间优化**：整个代码库太大了，直接搜索效率极低。作者提出必须先缩小范围。因此，引入了**分层故障定位**，从仓库级到文件级再到函数级，逐步收窄搜索空间，让MCTS在更精准的区域内进行。\n\n#### 4. 模型选择与增强：利用“思维链”与“不确定性”\n*   **模型选择**：选择Qwen3作为基础模型，不仅因为其代码能力强，更看重其**双模式推理**能力。\n*   **增强逻辑**：\n    *   **思维模式**：在MCTS的模拟和评估阶段，利用模型的“思考模式”进行深度的因果分析，而不是盲目生成代码。\n    *   **置信度校准**：为了进一步减少幻觉，引入不确定性量化。如果模型对生成的某段代码不确定（熵高），就触发自我修正机制，而不是直接输出。\n\n#### 5. 最终综合：CodePilot框架的诞生\n*   将上述思考整合，形成了一个混合推理框架：\n    *   **规划层**：MCTS负责全局路径探索和回溯。\n    *   **推理层**：Qwen3负责具体的代码生成和逻辑分析。\n    *   **验证层**：执行反馈和置信度校准负责筛选和修正。\n*   **核心思想**：用符号搜索的严谨性来弥补神经生成的随机性和贪婪性，实现“1+1>2”的效果。"
                },
                {
                    "title": "ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning",
                    "arxiv_id": "2602.00127",
                    "authors": "Tong Zhu, Baiting Chen, Jin Zhou, Hua Zhou, Sriram Sankararaman, Xiaowu Dai",
                    "summary": "LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心论文。 1.  **核心贡献符合第一步判断**: 论文的核心贡献是提出了一种名为 ALIGN 的新方法，将 LLM 推理构建为一个“对齐委托博弈”。在这个框架中，包含一个“委托人”和多个“智能体”，通过结构化的交互（委托任务、生成候选方案、选择最终答案）来完成复杂推理任务。这属于构建和改进 LLM 智能体系统的方法论，特别是多智能体协作与交互机制的研究。 2.  **符合正面指标**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `LLM-based Agents`。 *   **多智能体**: 论文研究了智能体间的交互（`Interaction`）、协作（`Collaboration`，通过委托和选择机制体现）以及博弈论框架下的激励机制。 *   **智能体能力**: 虽然侧重于推理，但这种推理是通过多智能体的架构来实现的，而非单纯的模型内部能力提升。 3.  **排除标准检查**: *   **关于“对齐”**: 尽管标题和摘要中提到了 \"Aligned\"，但这里的对齐是指博弈论层面的“委托人与智能体目标的一致性”，属于激励机制设计，而非第三步排除标准中的 AI 安全、伦理或安全性对齐。因此不应被排除。 *   **非应用型**: 论文关注的是推理框架本身的性能提升和理论保证，而非将其作为工具应用到生物、金融等特定垂直领域。 综上所述，该论文提出了一种新的多智能体交互框架来提升 LLM 的推理能力，属于 Agentic AI 中多智能体系统的研究范畴，因此予以保留。",
                    "summary2": "本文旨在解决Single LLM在复杂推理任务中的局限性及现有集成方法缺乏理论保证的问题。针对Multi-Agent LLM推理场景，我们提出了一种基于博弈论的ALIGN框架，通过Principal对Agent的候选答案进行排名反馈，利用Online Mirror Descent更新策略以实现目标对齐。我们在MATH、GSM8K和GSM-Hard数据集上通过Accuracy验证了其有效性，实验表明该方法优于强基线模型。",
                    "summary_translation": "当依赖单一的 generation-and-selection pipeline (生成-选择流程) 时，LLMs (Large Language Models，大型语言模型) 在复杂推理任务上的表现往往不尽如人意。Inference-time ensemble methods (推理时集成方法) 可以通过采样多样化的 reasoning paths (推理路径) 或聚合多个 candidate answers (候选答案) 来提升性能，但这些方法通常将候选者视为独立个体，且无法提供 ensemble (集成) 能够提升推理质量的 formal guarantees (理论保证)。我们提出了一种新颖的方法，即 Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN)，该方法将 LLM 推理建模为一个 aligned delegation game (对齐委托博弈)。在 ALIGN 中，principal (委托人) 将任务委托给多个 agents (智能体)，这些智能体在设计的 incentives (激励机制) 下生成 candidate solutions (候选解决方案)，随后 principal 从其输出中进行筛选以产生最终答案。这种 formulation (形式化表述) 在 agents 之间诱导了 structured interaction (结构化交互)，同时保持了 agent 与 principal 目标之间的 alignment (对齐)。我们建立了 theoretical guarantees (理论保证)，证明在公平比较且对 candidate solutions 拥有同等访问权限的条件下，ALIGN 能可证明地提升相对于 single-agent generation (单智能体生成) 的 expected performance (预期性能)。我们的分析适用于 correlated candidate answers (相关候选答案)，并放宽了先前工作中常用的 independence assumptions (独立性假设)。在广泛的 LLM reasoning benchmarks (推理基准测试) 中获得的 empirical results (实证结果) 一致表明，ALIGN 优于强大的 single-agent (单智能体) 和 ensemble (集成) baselines (基线模型)。",
                    "inspiration_trace": "基于对论文《ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分通过层层递进的方式，构建了从现状观察到痛点识别的逻辑链条：\n\n1.  **宏观背景与能力边界**：\n    大语言模型（LLMs）在通用任务上表现出色，但在需要多步推理的复杂任务（如数学证明、多步规划）上仍然面临显著挑战。单次生成往往无法解决此类问题。\n\n2.  **现有范式及其局限（单智能体视角）**：\n    为了解决上述问题，主流方法是“推理时提示策略”（如思维链 CoT）结合“选择机制”（如自一致性 Self-Consistency 或验证器）。\n    *   **局限一（孤立性）**：这些方法将生成的多条推理路径视为独立的候选者，缺乏路径之间有原则的交互或精炼机制。\n    *   **局限二（能力天花板）**：其最终效果受限于单个模型自身的容量上限。\n\n3.  **进阶尝试及其缺陷（多智能体视角）**：\n    为了突破单模型限制，近期工作开始探索交互式策略，如自我反思、辩论或多智能体集成。\n    *   **局限三（缺乏理论保证）**：尽管这些方法在经验上有效，但无法提供形式化的理论保证，无法确证精炼或集成过程一定能持续提升推理质量。\n    *   **局限四（强依赖性）**：现有的集成方法通常依赖于参与模型本身具有较强的能力，当单个模型较弱时，多智能体系统的效果提升有限。\n\n---\n\n### 二、 核心研究问题\n\n基于上述对现有方法“缺乏理论保证”和“依赖强个体”的双重不满，作者提出了本文试图解决的核心问题：\n\n**“我们能否设计一种无需额外训练的多智能体推理框架，通过结构化的交互机制（如博弈论中的委托与竞争），在理论上保证其性能优于单智能体生成，且不依赖于单个智能体的强能力？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n作者从宏观问题出发，逐步聚焦，最终形成ALIGN方法的思考过程如下：\n\n#### 1. 现象观察与跨域灵感\n*   **观察**：现有的LLM推理方法要么是“单打独斗”（单模型+采样），要么是“松散联盟”（简单投票或辩论），缺乏内在的动力机制来驱动质量提升。\n*   **灵感**：作者将目光投向了生物学（进化论中的竞争促进适应）和经济学（市场竞争提高效率）。\n*   **核心假设**：**竞争驱动质量**。如果能让多个智能体在一个受控的环境中为了获得“奖励”而竞争，它们可能会被迫探索更高质量的推理策略。\n\n#### 2. 理论框架构建：从“竞争”到“委托博弈”\n*   **概念转化**：如何将“竞争”形式化？作者引入了经济学中的**委托-代理**模型。\n*   **角色定义**：\n    *   **委托人**：代表用户或评估者，拥有全局的效用函数（即判断答案好坏的标准）。\n    *   **代理人**：代表多个LLM智能体，它们生成候选答案，并拥有各自的内部效用（如自一致性，即对自己答案的自信程度）。\n*   **机制设计**：\n    *   委托人将任务下放给代理人。\n    *   代理人提交答案，委托人进行排名。\n    *   **关键创新**：代理人的奖励不仅仅取决于它自己觉得答案好不好（内部效用），还取决于委托人给的排名（外部反馈）。这创造了一种**结构张力**：代理人必须在“坚持己见”和“迎合老板”之间寻找平衡。\n\n#### 3. 算法实现：从“博弈”到“学习”\n*   **问题**：如何让代理人在这个博弈中学会优化自己的策略？\n*   **工具选择**：作者选择了**在线镜像下降**。这是一种在线学习算法，常用于博弈论中寻找纳什均衡。\n*   **逻辑闭环**：\n    *   代理人根据当前策略提交答案。\n    *   收到委托人的排名反馈后，利用镜像下降更新策略（增加高分答案的采样概率）。\n    *   随着迭代进行，系统会收敛到一个均衡点，此时所有代理人都无法通过单方面改变策略来获得更高收益。\n\n#### 4. 理论验证：从“直觉”到“保证”\n*   **挑战**：仅仅设计机制是不够的，必须回应引言中提到的“缺乏理论保证”的问题。\n*   **证明思路**：\n    *   **公平比较**：设定单智能体和多智能体拥有相同的总计算资源（候选答案数量相同）。\n    *   **相关性假设**：放宽了前人工作中“候选答案相互独立”的不切实际假设，允许答案之间存在正相关性（更符合LLM生成的实际情况）。\n    *   **结论**：在满足帕累托最优和非负对齐等假设下，证明了多智能体委托机制在期望性能上严格优于单智能体生成，且算法具有次线性遗憾，能收敛到近似纳什均衡。\n\n#### 5. 实证检验：从“理论”到“现实”\n*   **验证目标**：验证ALIGN是否真的能在数学推理等复杂任务上超越强基线（如Self-Consistency, rStar），以及是否真的不依赖强智能体（使用同质模型测试）。\n*   **结果确认**：实验证实了竞争机制带来的性能提升，以及理论假设（如非负对齐）在实际数据中的成立。\n\n---\n\n### 总结\n\n作者的思考路径是一个典型的**“观察痛点 -> 跨域类比 -> 理论建模 -> 算法落地 -> 严格证明”**的过程。\n\n他们没有仅仅停留在“让多个LLM聊天”这一工程技巧上，而是敏锐地抓住了现有方法**缺乏理论解释和保证**这一学术空白，利用博弈论工具将LLM推理重新定义为一种**激励机制下的竞争行为**，从而赋予了多智能体系统坚实的理论基础。"
                },
                {
                    "title": "Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning",
                    "arxiv_id": "2602.00766",
                    "authors": "Xiaoxue Yu, Rongpeng Li, Zhifeng Zhao, Honggang Zhang",
                    "summary": "The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 与 Multi-Agent 方向**： 论文的核心贡献是提出了一个统一的 **\"agentic NetGPT framework\"**。该框架不仅包含能够进行自主推理的核心智能体，还涉及将子任务委派给领域专用智能体的机制。这直接对应了筛选标准中的 **\"Multi-Agent\"**（智能体间的协作、通信）和 **\"Agentic\"**（规划、工具使用）方向。 2.  **包含自我演化机制**： 论文明确使用了 **\"Agentic reinforcement learning\"** 来支持协作推理策略的持续改进，并旨在构建 **\"self-evolving\"** 的网络。这符合筛选标准中关于 **\"Self-Evolving\"**（通过经验或环境反馈进行自我完善和迭代）的核心关注点。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是无线网络，属于特定领域，但根据筛选标准第四步第2条，如果论文的核心是提出一种新的“自我演化”机制（在此处为基于智能体强化学习的协作与委派框架），即使应用在特定领域，也应该保留。本文的重点在于构建智能体架构和训练方法，而非单纯将现有LLM作为工具解决网络问题。 综上所述，该论文在构建多智能体协作框架及自我演化机制方面有实质性贡献，属于研究范围内的前沿论文。",
                    "summary2": "本文旨在解决AI原生网络中缺乏自适应协作推理的问题。针对复杂的网络任务和动态环境，我们提出了一种统一的NetGPT框架，结合Agentic Reinforcement Learning (Agentic RL) 优化协作策略。该方法引入了动态掩码损失、熵引导探索和多目标奖励机制。在包含TeleLogs和3GPP协议语料库的实验环境中，通过任务完成性能指标验证了其有效性，显著优于传统方法。",
                    "summary_translation": "下一代无线网络的演进标志着一种范式转变，即从以连接为中心的架构转向紧密集成数据、计算和通信的 Artificial Intelligence (AI)-native (人工智能原生) 设计。然而，通信系统中现有的人工智能部署在很大程度上仍处于孤岛式状态，仅提供孤立的优化，缺乏内在适应性、动态任务委派或 multi-agent collaboration (多智能体协作)。在这项工作中，我们为 AI-native xG networks (人工智能原生 xG 网络) 提出了一种统一的 agentic NetGPT framework (智能体 NetGPT 框架)，其中 NetGPT 核心既可以执行 autonomous reasoning (自主推理)，也可以通过 agentic communication (智能体通信) 将子任务委派给领域专用智能体。该框架确立了清晰的模块化职责和可互操作的工作流，从而在网络中实现可扩展的分布式智能。为了支持 collaborative reasoning strategies (协作推理策略) 的持续优化，该框架通过 partially observable conditions (部分可观测条件) 和 stochastic external states (随机外部状态) 下的 Agentic reinforcement learning (智能体强化学习) 得到了进一步增强。训练流程结合了针对外部智能体不确定性的 masked loss (掩码损失)、entropy-guided exploration (熵引导探索) 以及 multi-objective rewards (多目标奖励)，这些奖励共同捕捉任务质量、协调效率和资源约束。通过这一过程，NetGPT 学习何时以及如何进行协作，从而有效地平衡 internal reasoning (内部推理) 与 agent invocation (智能体调用)。总体而言，这项工作为能够在复杂通信环境中进行 autonomous sensing (自主感知)、推理和行动的 self-evolving (自演进) AI-native xG networks (人工智能原生 xG 网络)，提供了一种基础架构和训练方法。",
                    "inspiration_trace": "基于对论文《Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning》的深度分析，以下是作者产出该文章的系统性思考过程还原：\n\n### 1. 宏观背景与问题叙事\n\n作者首先构建了一个从“连接”到“智能”演进的宏观叙事，逻辑链条如下：\n\n*   **范式转移：** 下一代无线网络正在从以连接为中心的架构，转向深度融合数据、计算和通信的AI原生设计。\n*   **现状观察：** 尽管NetGPT（云端/边缘大模型）和领域专用代理正在兴起，但现有的AI部署是**孤岛式**的。它们缺乏内在的适应性、动态的任务委派能力以及多代理间的协作机制。\n*   **冲突引入：** 虽然代理间通信协议（如A2A, ACP）正在发展，但当前的代理系统仍严重依赖**静态的、提示词驱动的角色**。这种静态性导致随着时间推移，协调性能会退化。\n*   **具体挑战：** 在多代理交互、环境演变和资源受限的背景下，协作推理面临着不稳定或充满噪声的信号。这引入了一个核心难题：**非确定性的状态转移**。系统很难判断一个代理究竟应该依赖内部推理，还是寻求外部协作。\n*   **现有方法的局限：** 传统的强化学习（如RLHF, RLVR）通常假设状态转移是确定性的（即下一个状态完全由当前生成的Token决定）。然而，在通信驱动的协作推理中，代理处于**部分可观测**和**随机动态**的环境中，其行动不仅仅是生成文本，还包括工具调用、环境操作和任务委派。\n\n### 2. 核心研究问题\n\n基于上述叙事，作者显式地提出了本文试图解决的核心问题：\n\n**“在一个动态、随机且资源受限的AI原生网络环境中，如何使智能体具备自主决策能力，以确定何时进行内部推理、何时通过通信激励与外部专用代理进行协作，并实现持续的自我进化？”**\n\n---\n\n### 3. 逻辑推演与思想演进\n\n为了回答上述问题，作者的思考路径经历了从架构设计到训练策略的逐步聚焦：\n\n**第一步：架构重构——从“工具调用”到“代理协作”**\n*   **思考：** 既然现有系统是孤岛式的，我们需要一个统一的框架。不能仅仅把外部代理当作简单的“工具”，而应将其视为具有独立推理能力的协作伙伴。\n*   **决策：** 提出 **NetGPT框架**。定义一个NetGPT核心作为中枢，负责意图解释和任务分解；将专用代理分布在用户面、计算面和控制面。通过清晰的模块化职责和互操作工作流，实现分布式智能。\n\n**第二步：范式升级——从“文本生成RL”到“代理强化学习”**\n*   **思考：** 传统的RLHF（基于人类反馈的强化学习）假设环境是静态的，只关注文本生成的正确性。但在网络环境中，状态是部分可观测的，外部环境（其他代理、网络状态）是随机变化的。我们需要一种能处理这种不确定性和长序列决策的RL。\n*   **决策：** 引入 **Agentic RL（代理强化学习）**。将NetGPT核心视为一个在随机环境中进行顺序决策的智能体，而不仅仅是一个文本生成器。目标是优化整个推理-行动的循环，而不仅仅是下一个Token的概率。\n\n**第三步：训练策略优化——解决“噪声”与“不确定性”**\n*   **思考：** 在多代理协作中，外部代理的返回可能包含噪声或无关信息，如果直接用于训练会干扰核心模型的策略。此外，在随机环境中，如何平衡探索（尝试新的协作路径）和利用（已知的成功路径）是一个难题。\n*   **决策：**\n    1.  **Masked Loss（掩码损失）：** 在训练时，只关注外部代理返回中有用的部分（如`<ans>`标签内的内容），屏蔽无关噪声，确保策略更新只由核心自身的推理逻辑驱动。\n    2.  **Entropy-guided Exploration（熵引导探索）：** 引入熵监控机制。当不确定性高（熵高）时，鼓励模型进行更多探索，寻找更优的协作路径，防止过早收敛到次优策略。\n\n**第四步：目标函数设计——平衡“质量”与“成本”**\n*   **思考：** 仅仅完成任务是不够的。在网络环境中，资源（计算、通信带宽）是宝贵的。如果为了回答一个问题而调用了十次代理，虽然答案对了，但效率太低。\n*   **决策：** 设计 **Multi-objective Rewards（多目标奖励）**。不仅奖励任务的准确性，还引入效率奖励（减少冗余调用）、服务质量奖励（惩罚延迟）和资源成本奖励。让模型学会在“推理深度”和“协作成本”之间找到最佳平衡点。\n\n### 总结\n\n作者的思考过程是从**发现网络AI的“孤岛”与“静态”痛点**出发，意识到**传统文本生成RL无法处理网络环境的随机性**，从而构建了一个**分层协作的NetGPT架构**，并最终通过引入**处理不确定性和多目标优化的Agentic RL**，实现了系统在复杂通信环境下的自主进化。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "Symphony-Coord: Emergent Coordination in Decentralized Agent Systems",
                    "arxiv_id": "2602.00966",
                    "authors": "Zhaoyang Guan, Huixi Cao, Ming Zhong, Eric Yang, Lynn Ai, Yongxin Ni, Bill Shi",
                    "summary": "Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了 **Symphony-Coord**，这是一个**去中心化的多智能体框架**。它旨在解决多智能体系统中的协调问题，而非将现有智能体简单应用于特定领域。这直接对应了我的研究目标中的“构建、改进或演化 LLM智能体”以及“多智能体”方向。 2.  **正面指标匹配（第二步）：** *   **多智能体系统 (MAS)：** 论文明确关注多智能体系统，涉及智能体间的协作与通信。 *   **演化机制：** 论文强调角色通过交互“有机涌现”，并利用反馈进行持续优化。这体现了 **Self-Evolving** 和 **Iterative Improvement** 的特征。 *   **智能体能力：** 论文提到的“自愈能力”和动态任务路由属于高级的 **Planning** 和 **Self-Correction** 能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）：** *   论文关注的是智能体系统层面的动态规划和资源分配（Agentic框架），而非单纯提升LLM底层的Token预测或数学逻辑能力。 *   虽然使用了多臂老虎机算法，但它是作为智能体进行自我演化和适应环境的机制，符合“自我演化”的定义。 **结论：** 该论文提出了一种新的多智能体协调框架，通过去中心化和在线学习机制实现了角色的涌现和系统的自愈，属于典型的 Agentic AI 和 Multi-Agent Systems 研究，且包含自我演化的特性，因此予以保留。",
                    "summary2": "本文旨在解决多智能体系统中静态角色分配导致的低效和适应性问题。针对动态任务分布和异构智能体池，我们提出了一种名为Symphony-Coord的去中心化框架，将智能体选择转化为在线Contextual Bandit问题，并采用两阶段动态Beacon路由协议。我们在GSM8K、BBH和MedicalQA基准测试上通过准确率及鲁棒性指标验证了其有效性。",
                    "summary_translation": "Multi-agent large language model systems (多智能体大语言模型系统) 可以通过工作分解和协调专门行为来解决复杂的多步骤任务。然而，当前的 coordination mechanisms (协调机制) 通常依赖于静态分配的角色和集中式控制器。随着 agent pools (智能体池) 和 task distributions (任务分布) 的演变，这些设计选择会导致路由效率低下、适应性较差以及故障恢复能力脆弱。我们提出了 Symphony-Coord，这是一个 decentralized multi-agent framework (去中心化多智能体框架)，它将智能体选择转化为一个 online multi-armed bandit problem (在线多臂老虎机问题)，从而使角色能够通过交互有机地涌现。该框架采用 two-stage dynamic beacon protocol (两阶段动态信标协议)： 轻量级 candidate screening mechanism (候选筛选机制)，用于限制通信和计算开销； 自适应 LinUCB selector (LinUCB选择器)，根据源自任务需求和智能体状态的 context features (上下文特征) 对子任务进行路由，并通过延迟的 end-to-end feedback (端到端反馈) 进行持续优化。在标准 linear realizability assumptions (线性可实现性假设) 下，我们提供了 sublinear regret bounds (次线性遗憾界)，表明该系统收敛于近似最优的分配方案。通过仿真实验和真实世界大语言模型基准进行的验证表明，Symphony-Coord 不仅提高了任务路由效率，而且在涉及 distribution shifts (分布偏移) 和智能体故障的场景中表现出强大的自愈能力，从而实现了一种无需预定义角色的可扩展 coordination mechanisms (协调机制)。",
                    "inspiration_trace": "基于对论文《Symphony-Coord: Emergent Coordination in Decentralized Agent Systems》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观背景与问题叙事\n\n作者首先从LLM（大语言模型）技术的进步切入，指出多智能体系统在解决复杂任务中的潜力。然而，通过观察现有的主流框架（如AutoGen, CrewAI, ChatDev），作者发现了一个普遍存在的**设计范式缺陷**：\n\n*   **现状观察**：现有的协调机制主要依赖于**静态分配的角色**和**中心化的控制器**。\n*   **核心冲突**：这种静态设计无法匹配现实世界的动态性。在真实场景中，智能体的能力是异构的，且其表现会随着任务上下文、输入分布和系统运行状态的变化而波动。\n*   **具体痛点（“讲故事”的逻辑）**：\n    1.  **路由效率低下**：静态的角色标签过于宽泛，无法区分智能体在特定能力上的细微差异。\n    2.  **适应性差**：当任务分布发生变化时，由于“角色到任务”的映射是固定的，系统无法及时切换到更合适的智能体。\n    3.  **容错脆弱**：当高能力智能体性能下降或不可用时，静态分配缺乏快速可靠的替换机制，导致性能持续恶化。\n*   **现有方案的局限**：虽然有一些基于对话的框架（如CAMEL）试图通过“角色扮演”来缓解，但这种方法往往不稳定，且缺乏一个**原则性的决策机制**——即系统无法从经验中学习“在当前上下文和系统状态下，哪个智能体最适合执行该子任务”。\n\n### 2. 核心研究问题\n\n基于上述叙事，作者将焦点从“如何设计更好的角色”转移到了“如何动态地做出选择”，从而提炼出以下核心研究问题：\n\n**“我们能否将智能体选择从静态匹配问题转化为自适应的在线学习过程，从而在去中心化的多智能体系统中实现无需预定义角色的涌现式协调？”**\n\n### 3. 思想演进与逻辑推演\n\n为了回答上述问题，作者的思想经历了从理论洞察到工程落地的演进：\n\n*   **第一步：视角的转换**\n    *   **洞察**：智能体选择本质上不是一个静态的匹配问题，而是一个**在线决策问题**。\n    *   **逻辑**：当任务到达时，系统根据可观察的上下文选择执行者，并在收到反馈后更新路由策略。这天然符合**上下文赌博机**的 formulation。\n\n*   **第二步：引入理论框架**\n    *   **假设**：如果利用 Contextual Bandits 框架，系统就能在“利用”（选择估计奖励高的智能体）和“探索”（测试不确定的候选者）之间取得平衡。\n    *   **预期效果**：这种机制可以避免因惯性而反复将任务分配给性能下降的智能体，同时避免在次优候选者上长期浪费预算。\n\n*   **第三步：解决工程挑战**\n    *   **挑战**：在多智能体系统中，每一步都查询和比较所有智能体会导致巨大的通信和计算开销。\n    *   **解决方案**：提出**两阶段动态信标协议**。\n        *   **阶段一（粗筛）**：设计轻量级的候选筛选机制，仅保留 Top-L 个最有希望的候选者，以控制成本。\n        *   **阶段二（精排）**：在筛选后的候选集中应用 LinUCB 算法，结合任务特征和智能体状态进行精确选择。\n\n*   **第四步：构建闭环与反馈**\n    *   **完善**：为了实现真正的“自适应”，必须建立反馈机制。作者设计了一个端到端的反馈更新流程，利用延迟的奖励信号（如任务成功率、延迟惩罚）来持续优化 LinUCB 的参数。\n    *   **结果**：最终形成了 Symphony-Coord 框架，它不仅提高了路由效率，还通过在线学习证明了其在分布偏移和智能体故障场景下的自愈能力。"
                },
            ]
        },
    ],
    "2026-02-02": [
        {
            "name": "Artificial Intelligence",
            "count": 20,
            "papers": [
                {
                    "title": "Scaling Multiagent Systems with Process Rewards",
                    "arxiv_id": "2601.23228",
                    "authors": "Ed Li, Junyu Ren, Cat Yan",
                    "summary": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： *   论文的核心贡献是提出了一种名为 **MAPPA** 的新方法，用于微调多智能体系统。这直接对应了研究课题中的 **\"Multi-Agent\" (多智能体)** 方向。 *   论文重点解决了多智能体系统中的两个关键挑战：跨智能体的信用分配和样本效率。这属于构建和改进多智能体系统的方法论研究，而非仅仅将现有智能体作为工具应用到特定领域。 2.  **正面指标（强匹配）**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **智能体能力**：涉及 `Tool Augmentation`（工具增强的数据分析任务）和复杂的任务规划（长视界任务）。 *   **演化机制**：论文提出的利用过程奖励进行微调，本质上是一种 `Self-Improvement`（自我改进）或 `Iterative Improvement`（迭代改进）机制，旨在通过反馈让智能体系统变得更好，符合自我演化的定义。 3.  **排除标准（未触发）**： *   虽然论文在竞赛数学和数据分析任务上进行了验证，但其核心在于提出一种通用的微调框架（MAPPA），而非单纯解决数学或数据分析问题本身，因此不属于“非演化型应用”。 *   不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，该论文致力于解决多智能体系统在扩展和训练过程中的核心难题，属于构建和演化 LLM 智能体的前沿研究。",
                    "summary2": "总结生成失败",
                    "summary_translation": "尽管多智能体系统通过专业化在解决复杂任务方面展现出潜力，但同时微调多个智能体面临两个关键挑战：(1) 跨智能体的 credit assignment (信用分配)，以及 (2) 昂贵的 multiagent rollouts (多智能体推演) 的 sample efficiency (样本效率)。在本研究中，我们提出了一种利用来自 AI feedback (AI反馈) 的 per-action process rewards (针对每个动作的过程奖励) 来 finetuning (微调) 多智能体系统 (MAPPA) 的方法，以解决这两个问题。通过将信用分配给单个智能体的动作而非仅在任务完成时进行分配，MAPPA 能够在没有 ground truth labels (真实标签) 的情况下实现 fine-grained supervision (细粒度监督)，同时从每次推演中提取最大的 training signal (训练信号)。我们在竞赛数学问题和 tool-augmented (工具增强) 的数据分析任务上展示了我们的方法。在未见过的数学问题上，MAPPA 在 AIME 上实现了 +5.0--17.5pp (百分点) 的提升，在 AMC 上实现了 +7.8--17.2pp (百分点) 的提升。对于数据分析任务，我们的方法将成功率提高了 +12.5pp (百分点)，而质量指标提高了高达 30%，验证了针对每个动作的监督可以导致不同多智能体系统在各种领域的改进。通过解决这些挑战，我们的工作迈出了在最少人工监督下将多智能体系统扩展到复杂、long-horizon tasks (长视界任务) 的第一步。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning",
                    "arxiv_id": "2601.23032",
                    "authors": "Siyu Gong, Linan Yue, Weibo Gao, Fangzhou Yao, Shimin Di, Lei Feng, Min-Ling Zhang",
                    "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **AutoTraj**，这是一个用于改进 **Tool-Integrated Reasoning (TIR)** 的两阶段框架。TIR（工具集成推理）是 LLM 智能体与外部环境交互的核心机制之一。论文并非将智能体作为工具去解决某个垂直领域（如医疗、金融）的问题，而是专注于**改进智能体本身使用工具进行推理的能力**，这属于构建和改进 LLM 智能体的核心范畴。 2.  **符合研究焦点**： *   **单智能体**：论文重点在于提升单个智能体在复杂任务中规划和使用工具的轨迹质量，属于单智能体能力增强的研究。 *   **自我演化**：论文提出的 \"Repairing\"（修复）机制，即利用 LLM 将低质量的工具使用轨迹转化为高质量轨迹，以及通过强化学习（RL）进行迭代优化，体现了明确的 **Self-Correction**（自我修正）和 **Iterative Improvement**（迭代改进）的演化思想。这符合“自我演化”方向中关于智能体通过反馈进行自我完善的要求。 3.  **排除标准检查（通过）**： *   论文不涉及安全、对齐、水印或幻觉等排除主题。 *   论文不涉及多模态视觉或图技术作为核心研究点。 *   论文不是关于基础设施或硬件加速的研究。 综上所述，该论文通过提出新的框架来增强智能体的工具使用能力和自我修正机制，精准契合“LLM智能体及其演化”中的单智能体和自我演化方向。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR) 中依赖高质量合成轨迹和稀疏奖励导致的监督信号有限问题。针对工具使用场景，我们提出了一种名为AutoTraj的两阶段框架，通过LLM-as-Repairer修复低质量轨迹进行SFT，并引入Trajectory Reward Model进行轨迹感知的强化学习。我们在数学推理和知识密集型推理基准上通过准确率和F1分数验证了其有效性，显著提升了模型性能和推理效率。",
                    "summary_translation": "工具集成推理 (Tool-Integrated Reasoning, TIR) 使大语言模型 (Large Language Models, LLMs) 能够通过与外部工具交互来解决复杂任务，然而现有方法依赖于由评分函数筛选出的高质量合成轨迹以及稀疏的基于结果的奖励，这为学习 TIR 提供的监督信号有限且存在偏差。为应对这些挑战，本文提出了 AutoTraj，这是一个通过修复和奖励工具使用轨迹来自动学习 TIR 的两阶段框架。具体而言，在有监督微调 (Supervised Fine-Tuning, SFT) 阶段，AutoTraj 针对每个查询生成多个候选工具使用轨迹，并从多个维度对其进行评估。高质量轨迹被直接保留，而低质量轨迹则利用大语言模型 (LLM)（即 LLM-as-Repairer，作为修复器的 LLM）进行修复。由此得到的修复后轨迹和高质量轨迹构成了合成 SFT 数据集，而每条修复后的轨迹与其原始低质量版本配对，则构成了用于轨迹偏好建模的数据集。在强化学习 (Reinforcement Learning, RL) 阶段，基于该偏好数据集，我们训练了一个轨迹级奖励模型来评估推理路径的质量，并将其与结果奖励及格式奖励相结合，从而显式引导优化过程朝着可靠的 TIR 行为发展。在真实世界基准测试上的实验验证了 AutoTraj 在 TIR 任务中的有效性。",
                    "inspiration_trace": "基于对论文《Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 宏观背景与“讲故事”的逻辑链\n\n在进入具体方法之前，作者在Introduction中构建了一个从“现有范式”到“核心痛点”的叙事逻辑，具体推演如下：\n\n1.  **背景设定**：大语言模型（LLM）通过调用外部工具进行推理已成为解决复杂任务的主流范式。\n2.  **现有范式**：目前业界通用的训练方法是“两阶段法”——先通过监督微调（SFT）让模型学会基本的工具调用，再通过强化学习（RL）利用奖励信号进一步优化模型。\n3.  **冲突引入**：虽然这套范式有效，但在实际落地中存在两个致命的“阿喀琉斯之踵”，导致模型难以扩展且行为不稳定：\n    *   **数据端的“浪费与偏见”**：高质量的专家轨迹很难获取。现有方法通常采用“合成后过滤”的策略（生成一堆，挑好的）。但这导致两个问题：一是模型只擅长它熟悉的模式，不熟悉的生成了低质量轨迹却被直接丢弃，导致数据分布有偏差；二是低质量轨迹并非全是噪音，它们包含了“错误示范”的宝贵信息，直接丢弃是浪费。\n    *   **奖励端的“稀疏与盲目”**：在RL阶段，现有方法通常只看最终答案对不对。这导致奖励信号极其稀疏且粗糙——推理过程一团糟但蒙对了答案会被奖励，推理完美但最后算错一步会被惩罚。这种机制无法引导模型学习“可靠”的推理路径。\n\n**总结出的“研究问题”：**\n\n> **“我们如何能够通过修复低质量轨迹来扩充高质量的监督数据，并引入细粒度的轨迹级奖励机制，从而在无需昂贵人工标注的情况下，实现更鲁棒、可扩展的工具集成推理？”**\n\n---\n\n### 二、 核心方法的逻辑演进与思考过程\n\n基于上述问题，作者的思考路径经历了从“观察现象”到“提出假设”再到“系统构建”的三个阶段：\n\n#### 第一阶段：反思数据利用——从“过滤”到“修复”\n*   **观察**：现有的SFT数据构建流程是“生成 -> 评分 -> 丢弃低分”。这就像老师批改作业，只把满分的卷子留下来给学生看，把不及格的卷子扔进垃圾桶。\n*   **痛点**：这导致学生（模型）只见过“完美解”，没见过“错误解”是如何被修正的，且数据多样性受限。\n*   **假设**：低质量轨迹虽然结果不好或过程冗余，但其包含的尝试是有价值的。如果有一个更强的“老师”（LLM）能把这些低质量轨迹“批改”并“重写”成高质量轨迹，那么我们就能变废为宝。\n*   **方法论雏形（SFT阶段）**：\n    *   不再简单丢弃低质量轨迹，而是引入 **“LLM-as-Repairer”** 机制。\n    *   **逻辑闭环**：生成候选 -> 多维评估（置信度、长度、重复度） -> 保留原本的高质量 -> **修复低质量** -> 合并为SFT数据集。\n    *   **附加价值**：修复前后的轨迹对（低质量 vs 修复后），天然构成了“正负样本对”，这为后续的奖励模型提供了绝佳的训练素材。\n\n#### 第二阶段：反思奖励机制——从“结果导向”到“过程导向”\n*   **观察**：现有的RL训练只看最终答案。这就像只看终点线不看跑步姿势，导致模型可能学会了“投机取巧”。\n*   **痛点**：奖励信号太稀疏，无法指导中间的推理步骤。\n*   **假设**：我们需要一个能打分“推理过程”的裁判。既然我们在第一阶段已经生成了大量的“好轨迹”和“坏轨迹”对，我们可以训练一个模型来学会区分它们。\n*   **方法论雏形（RL阶段）**：\n    *   利用第一阶段产生的（低质量，高质量）轨迹对，训练一个 **轨迹奖励模型**。\n    *   这个RM学会了什么是“好的推理路径”（不仅仅是答案对）。\n    *   在RL训练时，将这个“轨迹分”与传统的“答案分”结合，构成复合奖励函数。\n\n#### 第三阶段：系统整合——AutoTraj框架的诞生\n*   **整合思考**：如何将上述两个创新点串联成一个闭环？\n*   **逻辑链**：\n    1.  **数据层**：通过“修复”机制，最大化利用合成数据，解决SFT数据稀缺和偏差问题。\n    2.  **信号层**：利用修复过程产生的对比数据，训练出懂“过程”的奖励模型，解决RL奖励稀疏问题。\n    3.  **优化层**：在GRPO（强化学习算法）中，同时使用格式奖励、结果奖励和**轨迹奖励**，显式地引导模型优化推理路径。\n*   **最终产出**：AutoTraj框架。它不仅学会了怎么用工具（通过SFT），还学会了怎么优雅、高效地用工具（通过轨迹级RL），从而实现了在更少数据量下超越现有方法的效果。\n\n---\n\n### 三、 总结：作者的创新思维图谱\n\n1.  **破除“洁癖”**：敢于使用低质量数据，并意识到“错误”是比“正确”更好的老师（通过对比学习）。\n2.  **填补“盲区”**：意识到只看结果的局限性，创造性地利用数据构造阶段的副产品（轨迹对）来填补过程级奖励的空白。\n3.  **系统联动**：没有孤立地看SFT或RL，而是发现SFT的数据处理过程可以直接为RL的奖励设计提供燃料，实现了两个阶段的无缝衔接。"
                },
                {
                    "title": "EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning",
                    "arxiv_id": "2601.22964",
                    "authors": "Yufei He, Juncheng Liu, Zhiyuan Hu, Yulin Chen, Yue Liu, Yuan Sui, Yibo Li, Nuo Chen, Jun Hu, Bryan Hooi, Xinxing Xu, Jiang Bian",
                    "summary": "Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献符合“自我演化”方向**： 论文明确提出了 **EvoClinician**，这是一个“自我演化智能体”。其核心创新在于构建了一个 **\"Diagnose-Grade-Evolve\"** 循环机制。这不仅仅是应用，而是一种新的方法论：通过 Actor 智能体执行、Process Grader 智能体进行信用分配、以及 Evolver 智能体根据反馈更新提示和记忆，实现了在测试时的策略演化。这直接对应您筛选标准中的“自我演化”和“自我完善”。 2.  **符合“Agentic AI”及“多智能体”特征**： *   **Agentic 能力**：论文涉及智能体的规划、记忆以及工具使用（通过 Patient 和 Examination 智能体作为环境交互工具）。 *   **多智能体系统**：为了构建基准 Med-Inquire 和演化机制，论文设计了多个智能体角色（Patient Agent, Examination Agent, Actor, Process Grader, Evolver），体现了智能体间的协作与交互。 3.  **符合“特殊和模糊情况”处理规则**： 尽管论文的应用领域是医疗诊断（通常属于特定领域应用），但根据您的第四步第2点规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。本文的核心在于 **Test-Time Evolutionary Learning（测试时演化学习）** 这一智能体演化框架，而非仅仅是医疗诊断的结果。医疗场景只是验证该演化机制有效性的实验场。 综上所述，该论文在构建、改进和演化 LLM 智能体方面具有明确的实质性贡献，属于 Agentic AI 和 Self-Evolving 的前沿研究。",
                    "summary2": "本文旨在解决现有医疗AI缺乏迭代诊断能力及资源效率意识的问题。针对多轮医疗诊断场景，我们提出了一种名为EvoClinician的Self-Evolving Agent，通过“Diagnose-Grade-Evolve”循环利用action-level feedback动态更新prompt和memory。我们在Med-Inquire benchmark上通过诊断正确性、交互轮次和资源成本验证了其有效性。",
                    "summary_translation": "现有的医疗AI基于一种不切实际的“one-shot”模型，即通过完整的患者档案进行诊断。然而，现实世界的诊断是一个迭代式询问的过程，临床医生按顺序提问并开具检查，在管理成本和时间的同时策略性地收集信息。为解决这一问题，我们首先提出了Med-Inquire，这是一个旨在评估智能体进行多轮诊断能力的新基准。Med-Inquire基于真实世界临床病例的数据集构建，通过将完整的患者档案隐藏在专门的Patient（患者）和Examination（检查）智能体之后，来模拟诊断过程。这迫使智能体主动提问并开具检查，从而逐步收集信息。为应对Med-Inquire带来的挑战，我们随后介绍了EvoClinician，这是一种在测试时学习高效诊断策略的自我进化智能体。其核心是一个“Diagnose-Grade-Evolve”循环：Actor（行动者）智能体尝试进行诊断；Process Grader（过程评分者）智能体通过评估每个动作的临床收益和资源效率来执行信用分配；最后，Evolver（进化者）智能体利用这一反馈，通过进化prompt（提示词）和memory（记忆）来更新Actor的策略。实验结果表明，EvoClinician的性能优于持续学习基线以及其他自我进化智能体（如memory agents）。代码可在 https://github.com/yf-he/EvoClinician 获取。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《EvoClinician》这篇论文的系统性思维推演与逻辑还原。\n\n---\n\n### 第一部分：Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从“现状批判”到“现实回归”再到“能力缺失”的叙事链条：\n\n1.  **现状批判（不切实际的“一次性”模型）**：\n    *   **观察**：目前的医疗AI主流范式是“一次性”诊断，即直接把完整的病人档案（病史、检查结果全都有）喂给模型，让它输出诊断结果。\n    *   **定性**：这是一种静态的、全信息的理想化模型。\n\n2.  **现实回归（真实的临床是迭代式探究）**：\n    *   **观察**：现实中的医生并不是做一道算术题，而是一个动态的、迭代的信息收集过程。\n    *   **细节**：医生通常只有主诉，需要通过**顺序提问**、体格检查、开具化验单来**策略性地**获取信息，同时还要权衡成本和时间。\n\n3.  **能力缺失（现有评估的盲区）**：\n    *   **观察**：现有的基准测试只关注最后的诊断准确率。\n    *   **痛点**：它们无法量化甚至无法奖励AI在获取信息过程中的“效率”（即：如何用最少的步骤和最低的成本获得最关键的信息）。这种“平衡准确率与资源效率”的能力，是专家医生的标志，但在现有AI中是缺失的。\n\n---\n\n### 第二部分：显式总结的“研究问题”\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个AI智能体，使其能够像人类医生一样通过多轮交互（提问与检查）来迭代式地收集信息，并在测试时通过学习不断优化其诊断策略，从而在保证诊断准确性的同时最大化资源利用效率？”**\n\n---\n\n### 第三部分：思想演进脉络（从观察到方法论的逻辑链）\n\n以下还原作者从发现问题到提出解决方案的完整思考过程：\n\n#### 1. 宏观观察：范式错配\n*   **思考起点**：现在的医疗AI太“作弊”了。给它看全知全能的病历，它当然能诊断。但这解决不了实际问题，因为医生看病时手里是没有完整病历的。\n*   **初步判断**：我们需要从“静态判别”转向“动态决策”。AI需要学会“问问题”和“开检查单”，而不是直接“给答案”。\n\n#### 2. 聚焦痛点：评估体系的空白\n*   **深入思考**：如果我们要训练AI去“问问题”，怎么衡量它问得好不好？\n*   **问题识别**：现有的数据集都是“全量输入”，无法测试AI的“信息获取策略”。我们需要一个新的环境，在这个环境里，信息是隐藏的，必须通过行动（交互）才能解锁。\n*   **假设**：如果能构建一个把完整病历“藏起来”的模拟环境，强迫AI像侦探一样去寻找线索，就能逼出它的真实策略能力。\n\n#### 3. 环境构建：Med-Inquire 的诞生\n*   **设计思路**：为了验证上述假设，必须设计一个“守门人”机制。\n*   **具体化**：\n    *   **Patient Agent**：只回答基于病历的问题，模拟病人。\n    *   **Examination Agent**：只提供被请求的检查结果，模拟检验科。\n    *   **Cost Model**：引入成本概念，防止AI为了确诊而“大海捞针”式地滥用检查。\n*   **产出**：这就构成了论文的第一个贡献——Med-Inquire基准测试。\n\n#### 4. 核心挑战：长序列任务中的“信用分配”\n*   **遇到瓶颈**：在Med-Inquire环境中，AI可能走了10步才确诊。如果最后诊断对了，但中间第3步开了一个极其昂贵的无效检查，AI怎么知道那是错的？\n*   **理论难点**：这是一个典型的“信用分配”问题。传统的强化学习只有最后的一个稀疏奖励（对/错），无法指导中间具体的每一步行动。\n*   **关键洞察**：我们需要一个“事后诸葛亮”的角色，它不看病，而是专门“复盘”，对每一步行动进行打分。\n\n#### 5. 方法论突破：Test-Time Evolutionary Learning\n*   **解决方案构思**：既然不能重新训练模型权重（太慢且不现实），能不能让模型在“测试时”自我进化？\n*   **机制设计**：\n    *   **Diagnose（行动）**：Actor Agent 去看病。\n    *   **Grade（复盘）**：Process Grader Agent 回顾整个对话，给每一步打标签（如：高收益、低效、关键错误）。这解决了信用分配问题。\n    *   **Evolve（进化）**：Evolver Agent 根据复盘结果，修改 Actor 的“大脑”。\n*   **进化的具体载体**：\n    *   **Prompt Evolution**：把成功的经验抽象成规则（如：“如果头皮有肿块，先问是否出生就有”），写入提示词。\n    *   **Memory Evolution**：把具体的失败案例存入记忆，下次遇到类似情况报警。\n\n#### 6. 逻辑闭环：自我演化的循环\n*   **最终图景**：作者构建了一个“诊断-评分-进化”的闭环。AI每看一个病人，就变得更聪明一点（Prompt更完善，Memory更丰富）。这不仅是诊断，更是在学习“如何更高效地诊断”。\n\n---\n\n**总结**：作者的思想演进是从**批判“全知视角”的虚假繁荣**开始，意识到**“信息获取策略”**才是临床智能的核心，进而通过**构建信息隐藏环境**来暴露问题，最后利用**大模型的反思与自我修改能力**，在测试时实现了无需梯度下降的策略进化。"
                },
                {
                    "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery",
                    "arxiv_id": "2601.22896",
                    "authors": "Xinyi Ke, Kai Li, Junliang Xing, Yifan Zhang, Jian Cheng",
                    "summary": "Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“多智能体”方向的交叉研究。 1.  **核心贡献符合要求**: 论文提出了 **ASRO (Algorithm Space Response Oracles)** 框架，这是一种基于博弈论的新方法。其核心贡献在于将启发式发现重新定义为求解器智能体与实例生成器智能体之间的**程序级协同进化**。这不仅仅是应用LLM解决问题，而是构建了一个新的演化框架。 2.  **精准命中研究焦点**: *   **多智能体**: 论文明确构建了两个智能体（Solver 和 Instance Generator），并将它们的交互建模为双人零和博弈。这完全符合“多智能体”中关于博弈、交互和对抗的研究方向。 *   **自我演化**: 论文的标题和核心机制都围绕“Co-Evolution”（协同进化）。智能体通过维护策略池，利用LLM作为最佳响应预言机，针对对手的策略进行迭代扩展和自我完善。这直接对应了“自我演化”中的通过环境反馈（对手策略）进行迭代改进的机制。 3.  **排除标准检查**: *   虽然论文的应用场景是组合优化，但根据筛选标准第四步（自我演化的应用），只要论文的核心是提出一种新的“自我演化”机制（在此处为博弈论驱动的协同进化），即使应用在特定领域，也应该保留。 *   论文不涉及安全对齐、多模态视觉或图技术等排除项。 综上所述，该论文在构建具有自我演化能力的多智能体系统方面做出了实质性贡献，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决LLM自动启发式发现中静态评估导致的过拟合与泛化能力不足问题。针对组合优化领域，我们提出了一种名为ASRO的博弈论框架，将启发式发现重构为求解器与实例生成器之间的程序级协同进化。在OBP、TSP和CVRP任务上，通过optimality gap验证了其有效性，结果表明ASRO在分布外泛化与鲁棒性上显著优于静态训练基线。",
                    "summary_translation": "Large language models (LLMs) (大语言模型) 推动了 automatic heuristic discovery (AHD) (自动启发式发现) 的快速进展，然而大多数现有方法主要受限于针对 fixed instance distributions (固定实例分布) 的 static evaluation (静态评估)，导致在 distributional shifts (分布偏移) 下潜在的 overfitting (过拟合) 和 poor generalization (泛化性能不佳)。我们提出了 Algorithm Space Response Oracles (ASRO) (算法空间响应预言机)，这是一个 game-theoretic framework (博弈论框架)，将 heuristic discovery (启发式发现) 重新构建为 solver (求解器) 与 instance generator (实例生成器) 之间的 program level co-evolution (程序级协同进化)。ASRO 将两者的交互建模为 two-player zero-sum game (双人零和博弈)，在双方维护不断增长的 strategy pools (策略池)，并通过基于 LLM 的 best-response oracles (最佳响应预言机) 针对混合 opponent meta-strategies (对手元策略) 对其进行 iterative expansion (迭代扩展)，从而利用 adaptive, self-generated curriculum (自适应、自生成的课程) 取代了 static evaluation (静态评估)。在多个 combinatorial optimization (组合优化) 领域中，ASRO 的表现始终优于基于相同 program search mechanisms (程序搜索机制) 构建的 static-training AHD baselines (静态训练 AHD 基线)，在多样化及 out-of-distribution (分布外) 实例上实现了 substantially improved generalization and robustness (显著提升的泛化能力和鲁棒性)。",
                    "inspiration_trace": "基于对论文《Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery》的深入分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的逻辑链条，旨在揭示现有研究的根本性缺陷：\n\n1.  **宏观背景与痛点**：组合优化（CO）问题广泛存在且至关重要，但因其NP-hard特性，精确求解不可行，因此高度依赖人工设计的启发式算法。\n2.  **人工设计的局限**：人工设计过程缓慢、脆弱，且受限于人类专家的结构性假设。这导致算法在面对分布偏移或结构变化时，性能严重退化。\n3.  **技术机遇（LLM的介入）**：大语言模型（LLM）具备合成可执行程序的能力，开启了自动启发式设计（AHD）的新方向。现有的LLM-AHD框架通常采用“生成-评估-优化”的闭环工作流。\n4.  **核心瓶颈（静态评估）**：尽管现有方法在特定数据集上有效，但它们根本上受限于**静态评估**——即优化目标是固定的实例分布。\n5.  **静态评估的后果**：\n    *   **过拟合与脆弱性**：在固定评估器上优化的启发式算法，难以泛化到分布外（OOD）的实例。\n    *   **性能天花板**：一旦启发式算法接近最优，固定的评估器无法暴露其新的弱点，阻碍了进一步的改进。\n6.  **现有改进的不足**：即使尝试超越静态评估，现有方法往往依赖手工设计的课程或临时的难度调度，并未将实例适应纳入核心优化目标，本质上仍停留在单智能体训练范式，缺乏求解器与实例之间统一、原则性的优化框架。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题为：\n\n**“如何将基于LLM的启发式发现从静态的单智能体优化范式，转变为一种动态的、原则性的博弈论框架，通过求解器与实例生成器的协同进化，实现自适应的课程学习，从而提升算法的泛化性与鲁棒性？”**\n\n---\n\n### 三、 核心方法的逻辑演进推演\n\n从发现问题到提出ASRO框架，作者的思考过程经历了以下五个关键阶段：\n\n#### 1. 问题重构：从“单点优化”到“二元对抗”\n*   **思考起点**：现有方法试图让LLM在固定的“考题”（数据集）上写出最好的“解题程序”（启发式算法）。这导致学生（求解器）只会做固定类型的题，遇到新题型就挂科。\n*   **视角转换**：与其寻找一个万能的求解器，不如将问题看作是**求解器**与**实例生成器**之间的互动。\n*   **关键洞察**：求解器试图最小化目标值（如路径长度），而生成器试图最大化目标值（制造难题）。这种天然的对抗关系，实际上是一个**零和博弈**。\n\n#### 2. 理论映射：从“策略空间”到“程序空间”\n*   **理论借用**：在强化学习领域，**PSRO（Policy Space Response Oracles）** 框架通过维护策略池并计算最佳响应来解决多智能体博弈问题。\n*   **跨界创新**：PSRO通常操作的是参数化的神经网络（策略空间）。而本文的操作对象是LLM生成的**可执行代码**。\n*   **概念升级**：作者提出将PSRO从连续的“策略空间”迁移到离散的“程序空间”。这意味着策略不再是神经网络的权重，而是具体的Python函数（求解器逻辑或生成器逻辑）。\n\n#### 3. 机制设计：构建“元游戏”与“最佳响应”\n*   **如何驱动进化？**：如果只是简单的“你追我赶”，容易陷入循环或局部最优。需要一个稳定的机制来指导进化方向。\n*   **引入元游戏**：作者设计了一个元博弈过程。在每一轮迭代中，不仅评估当前的求解器和生成器，还要计算它们在历史策略池上的**混合策略**——即纳什均衡。\n*   **定义最佳响应**：基于对手的混合策略（例如，对手有30%概率用策略A，70%概率用策略B），LLM需要生成一个新的程序，作为针对该混合分布的“最佳响应”。这迫使求解器不仅要应对单一难题，还要应对多样化的难题分布。\n\n#### 4. 实现路径：LLM作为“神谕”\n*   **如何找到最佳响应？**：在程序空间中，无法像神经网络那样通过梯度下降来找最佳响应。\n*   **LLM的角色**：作者将LLM视为一个**程序空间的最佳响应神谕**。\n*   **具体操作**：给定对手的元策略分布，利用LLM的代码生成和变异能力（如EoH风格的进化搜索），去搜索或合成一个新的程序，该程序在对抗该分布时表现最优。这个新程序会被加入策略池，从而扩充元游戏的边界。\n\n#### 5. 闭环形成：自适应课程与鲁棒性\n*   **最终图景**：生成器不断制造能暴露当前求解器池弱点的实例（自适应课程），迫使求解器进化出更鲁棒的逻辑；而更强的求解器又倒逼生成器制造更难、更多样化的实例。\n*   **结果**：这种协同进化不再依赖人工设计的课程，而是通过博弈论框架内生地产生，最终产出的求解器在面对未见过的分布时，具有极强的泛化能力。\n\n---\n\n**总结**：作者从**静态评估导致的过拟合**这一痛点出发，通过引入**博弈论视角**，将算法发现问题重构为**程序空间的零和博弈**，并利用**LLM作为进化算子**来驱动求解器与生成器的协同进化，从而构建了一个能够自我生成课程、持续提升鲁棒性的ASRO框架。"
                },
                {
                    "title": "TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization",
                    "arxiv_id": "2601.22776",
                    "authors": "Shichao Ma, Zhiyuan Ma, Ming Yang, Xiaofan Li, Xing Wu, Jintao Du, Yu Cheng, Weiqiang Wang, Qiliang Liu, Zhengyang Zhou, Yang Wang",
                    "summary": "Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a \"Double Homogenization Dilemma.\" This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Agentic AI** 中的 **单智能体** 方向。 1.  **核心贡献符合第一步（保留）标准**： *   论文的核心是提出了一种新的强化学习框架 **TSPO (Turn-level Stage-aware Policy Optimization)**，旨在解决多轮搜索策略优化中的问题。 *   这不仅仅是将LLM作为工具应用，而是对LLM智能体的**工作机制**（即如何进行搜索、如何使用工具、如何进行多轮推理）进行了**构建和改进**。 2.  **精准命中核心关注点（第二步）**： *   **工具使用**: 论文明确关注 \"Multi-turn tool-integrated reasoning\"（多轮工具集成推理），这是Agentic AI的核心能力之一。 *   **规划与推理**: 论文解决的是 \"Search Policy Optimization\"（搜索策略优化），关注智能体在复杂任务中的多步推理过程。 *   **自我完善/演化**: 虽然TSPO是一种训练方法，但其本质是通过环境反馈（Reward信号）来优化智能体的策略，符合通过反馈进行迭代改进的机制。 3.  **排除干扰项（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术，因此不触犯排除标准。 4.  **特殊情况处理（第四步）**： *   论文关于推理的研究是 **Agentic** 的。它不是单纯提升模型的基础数学或逻辑能力，而是专注于智能体在**多轮交互**和**工具使用**过程中的策略优化，特别是关注“过程级信号”，这正是智能体规划与执行的关键。 综上所述，该论文致力于改进LLM智能体在工具使用和复杂搜索任务中的表现，属于构建和改进LLM智能体的核心研究，因此予以保留。",
                    "summary2": "本文旨在解决多轮搜索策略优化中的“Double Homogenization Dilemma”。针对多轮工具调用推理场景，我们提出了一种Turn-level Stage-aware Policy Optimization (TSPO) 方法，利用First-Occurrence Latent Reward (FOLR)机制在答案首次出现的轮次分配部分奖励。我们在7个QA数据集上通过Exact Match指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "多轮工具集成推理使大语言模型能够通过迭代信息检索来解决复杂任务。然而，当前用于搜索增强推理的强化学习框架主要依赖于稀疏的结果级奖励，导致了“双重同质化困境”。这表现为：(1) 过程同质化，即忽略了生成过程中涉及的思考、推理和工具使用；(2) 组内同质化，粗粒度的结果奖励往往导致在使用组相对策略优化等方法进行采样时，组内优势估计效率低下。为解决这一问题，我们提出了轮级阶段感知策略优化。TSPO 引入了首次出现潜在奖励机制，将部分奖励分配至真实答案首次出现的步骤，从而在无需外部奖励模型或任何标注的情况下，保留了过程级信号并增加了组内的奖励方差。大量实验表明，TSPO 显著优于最先进的基线模型，在 Qwen2.5-3B 和 7B 模型上分别实现了 24% 和 13.6% 的平均性能提升。",
                    "inspiration_trace": "基于论文内容，以下是对作者产出《TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization》一文思维过程的系统性推演。\n\n---\n\n### 一、 宏观背景与问题引入\n\n#### 1. 现象观察\n大语言模型（LLMs）通过与搜索引擎、代码解释器等外部工具进行多轮交互，展现出了强大的复杂任务解决能力。这种范式允许模型将复杂任务分解为一系列可管理的子任务，通过迭代检索和整合信息来获取成功。\n\n#### 2. 现有方法的痛点\n尽管潜力巨大，但目前针对多轮工具调用的强化学习（RL）框架主要依赖于**稀疏的结果级奖励信号**（例如仅在最后一步判断答案是否完全匹配）。\n\n#### 3. 核心矛盾\n这种“只看结果、不看过程”的奖励机制虽然避免了奖励黑客，但却将整个多轮动态工具调用和推理过程压缩成了一个单一的标量。这导致了一个系统性的瓶颈，作者将其定义为**“双重同质化困境”**：\n\n*   **困境一：过程级奖励同质化**\n    *   **逻辑：** 不同的中间推理或检索质量（例如：检索到了正确信息但合成失败 vs. 完全没有检索到信息）最终可能得到相同的错误结果（Reward = 0）。\n    *   **后果：** 这种机制忽略了最终步骤之前的进展，错误地惩罚了有益的轮次动作（如成功的信息获取），抹除了细粒度的进步线索。\n*   **困境二：组内奖励同质化**\n    *   **逻辑：** 在群组相对策略优化（GRPO）等方法中，由于奖励是二元的（0或1），经常会出现“全错组”（即组内所有轨迹的奖励都是0）。\n    *   **后果：** 这种全同质化的奖励组导致方差消失，优势估计归零，最终使得策略梯度失效，丢弃了潜在有价值的推理轨迹。\n\n#### 4. 现有解决方案的局限\n现有研究试图通过过程级监督（如LLM打分、MCTS搜索）来缓解这些问题，但这些方法往往需要昂贵的标注、依赖专有模型，且在不同任务间的泛化性有限。\n\n---\n\n### 二、 研究问题\n\n基于上述背景与矛盾，作者试图回答的核心研究问题是：\n\n**“在不依赖外部奖励模型或人工标注等额外开销的前提下，如何打破多轮搜索策略优化中的‘双重同质化困境’，从而有效利用过程级信号进行策略优化？”**\n\n---\n\n### 三、 思想演进与逻辑推演\n\n#### 第一阶段：从“结果导向”到“过程解构”的反思\n*   **思考起点：** 既然结果级的二元奖励（0/1）导致了“双重同质化”，那么必须引入中间过程的奖励信号。\n*   **约束条件：** 不能引入昂贵的外部裁判（如GPT-4打分）或复杂的人类标注，必须保持轻量级。\n*   **关键洞察：** 在搜索增强类任务中，成功的检索往往是正确合成的前提。如果模型在中间某一步检索到了包含标准答案的文档，即便最后没答对，这也是一种“部分成功”。\n\n#### 第二阶段：提出“首次出现”假设\n*   **假设形成：** 标准答案在中间检索反馈中的“首次出现”，是一个潜在的、强相关的部分进步信号。\n*   **逻辑验证：** 如果答案从未在检索内容中出现，模型几乎不可能答对（必要条件）。因此，只要答案在检索内容中出现了，就应当给予正向反馈，以鼓励这种正确的检索行为。\n\n#### 第三阶段：设计“首次出现潜在奖励（FOLR）”机制\n*   **解决困境一（过程同质化）：** 不再只看最后一步。只要在轨迹 $y$ 的第 $t^*$ 轮检索反馈中出现了 $a_{gold}$，就给予该轮及之前的轮次一个部分奖励 $\\alpha$。这样，即使最终答案错误，只要检索过程正确，也能获得奖励，保留了过程信号。\n*   **解决困境二（组内同质化）：** 在GRPO的“全错组”中，原本所有轨迹奖励都是0。引入FOLR后，那些“检索到了但没答对”的轨迹（Near-Miss）将获得正奖励，而“完全没检索到”的轨迹仍为0。这人为地制造了组内方差，恢复了梯度更新的能力。\n\n#### 第四阶段：构建 TSPO 框架\n*   **方法落地：** 将上述思想形式化为“轮级阶段感知策略优化（TSPO）”。\n*   **核心操作：**\n    1.  **定位：** 在轨迹中找到 $a_{gold}$ 首次出现的轮次 $t^*$。\n    2.  **分配：** 对 $t^*$ 及之前的轮次分配部分奖励 $\\alpha$，对最终正确的轨迹分配全奖励。\n    3.  **计算：** 基于轮级奖励计算优势函数，进行策略更新。\n*   **特别优化：** 针对实验中发现“全错组”是主要瓶颈的现象，进一步将策略聚焦于对“全错组”进行轮级优势估计，以最小的计算开销换取最大的性能提升。\n\n---\n\n### 四、 总结\n\n作者的思考路径遵循了典型的**“观察现象 -> 定义问题 -> 提出假设 -> 寻找最小代价解 -> 构建系统”**的学术创新逻辑：\n\n1.  **观察：** 现有的多轮RL只看结果，浪费了大量“只差一点点”的失败样本。\n2.  **定义：** 这种浪费被抽象为“双重同质化困境”（过程被忽略、组内无方差）。\n3.  **假设：** 检索结果中包含答案是一个无需外部监督的、天然的“过程里程碑”。\n4.  **解法：** 利用这个里程碑（FOLR）给中间步骤发“奖金”，从而在不增加成本的情况下，让模型学会如何正确地搜索和推理。"
                },
                {
                    "title": "AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement",
                    "arxiv_id": "2601.22758",
                    "authors": "Libin Qiu, Zhirong Gao, Junfu Chen, Yuhang Ye, Weizhi Huang, Xiaobo Xue, Wenkai Qiu, Shuo Tang",
                    "summary": "Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”方向的核心论文。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **AutoRefine** 这一框架，旨在解决 LLM 智能体无法从经验中积累知识的问题。 *   它的核心机制是从智能体的执行历史中提取“双形式经验模式”，并通过维护机制进行持续改进。这直接对应了您研究目标中的 **“自我演化”**，即智能体通过经验进行自我完善和迭代。 *   这不是简单的应用，也不是基础设施优化，而是关于智能体如何自我进化的方法论。 2.  **正面指标匹配 (第二步)**: *   **自我演化**: 论文明确提出了 \"Continual LLM Agent Refinement\"（持续LLM智能体改进）和 \"Experience Patterns\"（经验模式），属于典型的 Self-Improvement 和 Iterative Improvement。 *   **智能体能力**: 涉及 Memory（从历史中提取记忆）、Planning（通过提取子智能体处理过程性子任务）以及自我修正机制（评分、修剪和合并模式）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊情况处理 (第四步)**: *   虽然论文在 ALFWorld、ScienceWorld 和 TravelPlanner 等具体任务上进行了评估，但其核心在于提出一种通用的**自我演化机制**（即如何从轨迹转化为可复用的专业知识）。根据规则，这种提出新演化机制的应用研究应当保留。 综上所述，该论文专注于构建智能体自我演化的框架，属于 Agentic AI 的前沿研究，符合筛选条件。",
                    "summary2": "本文旨在解决LLM Agent难以从经验中持续积累知识的问题。针对Agent执行历史轨迹，我们提出了一种AutoRefine框架，该框架提取并维护双形式Experience Patterns，包括用于静态知识的Skill Patterns和用于程序化子任务的Subagent Patterns。我们在ALFWorld、ScienceWorld和TravelPlanner上通过成功率和步数减少验证了其有效性。",
                    "summary_translation": "大语言模型代理通常难以从经验中积累知识，往往将每个任务视为独立的挑战。现有方法将经验提取为扁平化文本知识，无法捕捉复杂子任务的程序性逻辑。此外，这些方法缺乏维护机制，导致随着经验的积累，知识库出现退化。我们提出了AutoRefine，这是一个从代理执行历史中提取并维护双形式经验模式的框架。针对程序性子任务，我们提取具备独立推理和记忆能力的专用子代理；针对静态知识，我们将技能模式提取为指南或代码片段。一种持续的维护机制通过对模式进行评分、修剪和合并，以防止知识库退化。在ALFWorld、ScienceWorld和TravelPlanner上的评估表明，AutoRefine分别取得了98.4%、70.4%和27.1%的成功率，并减少了20-73%的步数。在TravelPlanner上，自动提取的性能超过了人工设计的系统（27.1% vs 12.1%），证明了其捕捉程序性协调的能力。",
                    "inspiration_trace": "基于论文《AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从发现问题到构建解决方案的思考过程。\n\n---\n\n### 一、 引言中的“故事”逻辑：从观察到痛点\n\n作者在Introduction中构建了一个层层递进的逻辑链条，用于引出研究动机：\n\n1.  **宏观现状与类比**：\n    *   **观察**：当前的LLM智能体在各个领域（如网页导航、机器人、家庭助理）表现出色。\n    *   **类比**：人类能够通过交互自主构建和更新理解，但当前的智能体往往将每个新任务视为独立的挑战，缺乏“积累经验”的能力。\n    *   **潜在价值**：每一个成功完成的任务都有潜力丰富智能体的知识，从而在未来类似的任务中实现更可靠的性能。\n\n2.  **现有尝试及其局限性**：\n    *   **现有方法**：近期的研究试图通过从历史执行轨迹中提取知识来解决这个问题。\n    *   **核心痛点一（表达形式）**：这些方法将经验提取为“扁平化的文本知识”。\n        *   *缺陷*：这种形式无法捕捉复杂子任务的**程序性逻辑**。例如，“酒店预订”涉及顺序步骤、条件分支和状态跟踪等动态方面，而静态的文本描述难以表征这些内容。\n    *   **核心痛点二（维护机制）**：这些方法缺乏经验维护机制。\n        *   *缺陷*：它们通常将所有积累的经验直接塞入提示词中，依赖LLM自行选择相关知识。随着经验增长，提示词变得过长，不仅增加了上下文窗口的负担，还因为无法过滤过时或冗余的模式，导致检索质量下降（即“仓库退化”）。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图解决的核心问题可以总结为：\n\n**“如何使LLM智能体能够从自身的执行历史中有效地提取并积累可重用的程序性知识，同时建立一种维护机制以防止知识库随时间推移而退化？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n为了回答上述问题，作者的思考路径经历了从现象观察到假设提出，再到方法论构建的四个阶段：\n\n#### 1. 现象观察：智能体的“失忆”与“低效”\n*   **思考起点**：现有的智能体像是一个“一次性”工具，每次面对任务都从零开始推理，无法像人类专家一样利用过去的成功经验。\n*   **初步判断**：问题不在于模型的能力，而在于**知识的存储和复用方式**。\n\n#### 2. 问题剖析：为什么“文本”不够用？\n*   **深入分析**：现有的经验复用主要依赖“文本提示”或“ episodic memory（情景记忆）”。\n*   **关键洞察**：文本是静态的、扁平的，而复杂的任务（如旅行规划、科学实验）是动态的、有状态的。\n    *   *思考*：如果仅仅告诉智能体“如何预订酒店”的一段文字，它很难处理“如果满房了怎么办”、“如何与返程航班协调”等涉及状态管理和分支逻辑的复杂情况。\n    *   *假设*：我们需要一种能够**封装过程和状态**的表示形式，而不仅仅是描述性的文本。\n\n#### 3. 问题剖析：为什么“堆积”会导致失败？\n*   **深入分析**：随着任务数量增加，如果无差别地存储所有经验，知识库会变得臃肿。\n*   **关键洞察**：经验是有生命周期的。有些经验是通用的（高价值），有些是过时的（负价值），有些是重复的（冗余）。\n    *   *思考*：如果不进行“新陈代谢”，智能体会被过时的信息淹没，导致检索噪音变大，性能反而下降。\n    *   *假设*：必须引入一种**主动的维护机制**，像管理代码库一样管理经验（评分、剪枝、合并）。\n\n#### 4. 方法论构建：从“单一文本”到“双重模式”+“持续维护”\n基于上述两个假设，作者构建了AutoRefine框架的核心逻辑：\n\n*   **维度一：知识表示的进化（解决“逻辑”问题）**\n    *   *思考*：并非所有经验都是平等的。简单的经验（如“检查数据格式”）只需要一条规则；复杂的经验（如“多城市交通协调”）需要一套完整的执行流程。\n    *   *决策*：提出**双重模式**。\n        *   **Skill Pattern（技能模式）**：针对静态知识，使用自然语言指南或代码片段。\n        *   **Subagent Pattern（子智能体模式）**：针对程序性子任务，将其封装为具有独立推理和记忆能力的专用子智能体。这解决了“扁平化文本无法处理状态和分支”的问题。\n\n*   **维度二：知识管理的进化（解决“退化”问题）**\n    *   *思考*：如何判断一个经验好不好？不能靠人工，必须靠数据。\n    *   *决策*：提出**持续维护机制**。\n        *   **评分**：基于实证效用（成功率、使用频率、检索精度）对模式打分。\n        *   **剪枝与合并**：定期剔除低分模式，合并相似模式，防止仓库膨胀。\n\n*   **维度三：提取策略的进化（解决“泛化”问题）**\n    *   *思考*：从单个任务中提取经验容易过拟合（只适用于特定情况）。\n    *   *决策*：采用**批量对比提取**。每K个任务分析一次，对比成功和失败的轨迹，找出那些在成功中反复出现、在失败中缺失的通用策略。\n\n---\n\n### 总结\n\n作者的思考过程是从**“智能体无法积累经验”**这一宏观痛点出发，通过批判性分析发现现有方法在**“表达动态逻辑”**和**“管理知识规模”**上的双重缺陷，进而提出了**“双重模式（技能+子智能体）”**来解决逻辑封装问题，以及**“基于效用的维护机制”**来解决知识库退化问题，最终形成了一个闭环的、自我进化的智能体框架。"
                },
                {
                    "title": "Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference",
                    "arxiv_id": "2601.22701",
                    "authors": "Emilien Biré, María Santos, Kai Yuan",
                    "summary": "Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建/改进 LLM智能体”的目标**： 论文的核心贡献是提出了一种名为 \"Best-of-Q\" 的新范式，旨在改进 VLM 智能体在数字环境（如 Web）中的策略表现。它通过解耦 VLM 的动作提议与最终动作选择，利用 Q-function 在推理时对候选动作进行重新排序，从而在不重新训练策略的情况下提升智能体的成功率。这属于对智能体架构和决策机制的直接改进，符合“单智能体”方向中关于规划和工具使用的研究范畴。 2.  **符合“视觉作为感知工具”的例外情况**： 虽然论文涉及 Vision-Language Models (VLMs) 和视觉环境，但根据筛选标准第三步，视觉能力在这里仅作为智能体感知 Web 环境的工具。论文的研究重点不在于改进视觉模型本身或视觉理解能力，而在于如何利用视觉信息来优化智能体的动作决策。因此，这不属于被排除的“多模态与视觉”核心研究，而是属于 Agentic AI 的应用。 3.  **不属于排除类别**： -   **非演化型应用**：论文不是将智能体简单应用于生物、医疗等领域，而是专注于提升智能体本身的泛化能力和决策机制。 -   **非Agentic的推理**：论文关注的是智能体在环境中的动作选择和策略执行，属于典型的 Agentic 行为，而非单纯的 LLM 文本推理能力提升。 -   **安全与对齐**：论文未涉及安全、对齐或可解释性等排除主题。 综上所述，该论文专注于提升智能体的决策效率和适应性，属于 Agentic AI 的核心研究范围。",
                    "summary2": "本文旨在提升VLM智能体在Web环境中的性能且避免昂贵的策略微调。针对Web智能体任务，我们提出了一种Best-of-Q框架，利用离线训练的轻量级Q函数在推理时对冻结VLM生成的候选动作进行重排序。在WebVoyager benchmark上，通过Success Rate验证了其有效性，显著提升了Qwen2.5-VL-7B和GPT-4.1的成功率。",
                    "summary_translation": "视觉语言模型已成为智能体在Web（网络）和操作系统等数字环境中自主运行的强大骨干。然而，这些模型难以适应Web等快速变化的环境，虽然可以通过微调来缓解这一问题，但这需要昂贵的模型训练和数据收集工作。在这项工作中，我们引入了一种新颖的范式，旨在无需策略重训练的情况下，在推理阶段增强基于VLM的智能体策略。从根本上讲，我们的方法将VLM作为高容量动作提议者的角色与最终动作选择机制解耦。我们保持VLM策略冻结，并利用其为给定状态生成一组候选动作。随后，一个轻量级的、离线训练的Q-function（Q函数）对这些候选动作进行重新排序，智能体则执行估计值最高的动作。我们的主要贡献在于直接在推理过程中应用Q-function以实现即时的策略改进，而非离线地利用其重新标记数据以进行策略重训练。我们在学术界的WebVoyager基准测试上证明，该方法显著提升了智能体的成功率，将Qwen2.5-VL-7B智能体的成功率从38.8%提升至55.7%，并将专有的GPT-4.1智能体的成功率从82.4%提升至88.8%。",
                    "inspiration_trace": "基于对论文《Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的叙事逻辑，旨在揭示当前技术范式的痛点：\n\n1.  **背景铺垫：** 视觉语言模型（VLM）已成为自主智能体在数字环境（如网页、操作系统）中运行的强大骨干，具备强大的语义理解和基础交互能力。\n2.  **现实落差：** 尽管VLM具备良好的初始策略，但它们往往难以适应快速变化的环境（如网页），且缺乏达到最先进（SOTA）性能所需的精确行动规划能力。\n3.  **现有方案的困境：**\n    *   **强化学习（RL）：** 依赖大量的在线交互来收集数据，这在现实世界的网页任务中极其缓慢且昂贵。\n    *   **模仿学习（IL）：** 需要人工收集大规模、高质量的专家轨迹数据，资源密集且耗时。\n    *   **共同瓶颈：** 无论是RL还是IL，都需要对数十亿参数的VLM进行迭代微调，计算成本极高，使得每次训练周期都成为巨大的投资。\n4.  **核心矛盾：** 我们拥有一个强大的通用模型（VLM），但为了让它变强，必须付出高昂的代价去修改它的参数。是否存在一种更轻量、更高效的路径？\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“我们能否在不进行昂贵的策略重训练或在线交互的情况下，仅通过推理时的干预来显著提升预训练VLM智能体的性能？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n为了回答上述问题，作者的思考路径经历了从现象观察到方法论创新的四个阶段：\n\n#### 1. 观察与诊断：VLM的“决策缺陷”\n*   **观察：** VLM虽然拥有强大的感知和生成能力，但在面对复杂任务时，其“贪婪”的输出方式往往导致次优选择。它可能知道怎么做，但在关键时刻选错了动作。\n*   **诊断：** 问题不在于VLM“不知道”正确答案，而在于它缺乏一个有效的“筛选机制”来从其内部知识中提取最佳行动。VLM的参数中蕴含了正确的策略，但直接采样容易出错。\n\n#### 2. 假设提出：解耦“生成”与“决策”\n*   **核心假设：** 如果我们将VLM的角色从“最终决策者”转变为“候选提案者”，是否就能绕过其决策缺陷？\n*   **思想转变：** 不再试图通过修改VLM的参数（微调）来改变它的行为，而是保持VLM冻结，利用它生成多个候选动作。然后，引入一个外部的、专门的机制来从这些候选中选出最好的一个。\n*   **类比：** 这就像将VLM从一个“独断专行的执行者”变成了一个“提供多个方案的参谋”，而真正的决策权交给一个冷静的“评估者”。\n\n#### 3. 方法构建：引入轻量级Q函数进行推理时重排\n*   **机制设计：** 采用“Best-of-N”策略。在推理时，让VLM针对当前状态生成 $N$ 个多样化的候选动作。\n*   **评估器选择：** 为什么要用Q函数？因为Q函数（动作价值函数）本质上就是用来评估“在当前状态下采取某个动作未来能获得多少回报”的，这正是我们需要的选择标准。\n*   **训练策略：** 为了避免在线交互的高昂成本，采用**离线强化学习**。利用现有的静态数据训练一个轻量级的Q函数（仅训练MLP头，冻结VLM特征提取器）。这里选择IQL（Implicit Q-Learning）是因为它在处理离线数据和分布偏移时具有稳定性。\n*   **关键创新点：** 区别于以往用Q函数去重写数据用于微调VLM的做法，作者直接将Q函数应用于**推理阶段**，对VLM生成的候选进行实时打分和重排。\n\n#### 4. 验证与反思：性能上限与瓶颈分析\n*   **验证：** 实验证明，这种方法在不微调VLM的情况下，显著提升了成功率（如Qwen2.5-7B从38.8%提升至55.7%），且成本远低于微调。\n*   **反思（Failure Analysis）：** 既然方法有效，为什么没有达到100%？作者进一步分析发现，性能的瓶颈主要在于VLM的**“召回率”**（Proposal Ability）。如果VLM根本没有生成出正确的动作，Q函数再聪明也无法选中它。\n*   **结论：** 这种“解耦”范式是高效且正确的，未来的改进方向应聚焦于如何提高VLM生成候选动作的多样性和质量，而非仅仅优化选择器。\n\n---\n\n### 总结\n\n作者的思考过程是一个典型的**“避重就轻，化整为零”**的创新路径：\n面对VLM微调成本过高的问题 $\\rightarrow$ 识别出VLM是“生成强、决策弱” $\\rightarrow$ 提出“生成与决策解耦”的假设 $\\rightarrow$ 利用离线训练的轻量级Q函数在推理时接管决策权 $\\rightarrow$ 最终实现低成本、高性能的智能体提升。"
                },
                {
                    "title": "Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support",
                    "arxiv_id": "2601.22662",
                    "authors": "Wei Zhu, Lixing Yu, Hao-Ren Yao, Zhiwen Tang, Kun Yue",
                    "summary": "Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断**: *   论文的核心贡献是构建了一个名为 **Task-Aware LLM Council (TALC)** 的任务自适应决策框架。这不仅仅是将LLM作为工具应用，而是提出了一种新的智能体架构，用于解决决策支持问题。 *   它不属于特定领域的垂直应用（如医疗、法律），而是在通用决策任务（WebShop, HumanEval, Game of 24）上验证框架的有效性。 2.  **正面指标**: *   **Agentic AI**: 论文明确提出了一个基于LLM的智能体框架。 *   **Planning (规划)**: 论文集成了 **蒙特卡洛树搜索 (MCTS)** 来实现高效的多步规划，这是智能体规划能力的典型体现。 *   **Memory (记忆)**: 论文为每个LLM配备了“结构化的成功记忆档案”，利用先前的任务轨迹来辅助当前的决策，符合智能体记忆机制的研究焦点。 *   **Self-Correction/Adaptation**: 框架通过语义匹配和双信号机制自适应地加权信号，体现了智能体根据上下文调整行为的能力。 3.  **排除标准**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理**: *   关于推理/规划：论文使用MCTS进行多步规划和节点值估计，这属于智能体在复杂任务中的规划框架，而非单纯提升LLM基础Token预测能力的数学或逻辑研究，因此符合保留条件。 综上所述，该论文在构建LLM智能体的规划与记忆机制方面做出了实质性贡献，符合“单智能体”的研究目标。",
                    "summary2": "本文旨在解决现有LLM代理忽略模型特化差异及任务复杂性的问题。针对多样化的决策任务，我们提出了一种Task-Aware LLM Council (TALC) 框架，结合MCTS与基于成功记忆的专家路由及双信号价值估计机制，并在WebShop、HumanEval和Game of 24数据集上通过成功率和搜索效率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support》的深入分析，以下是对作者核心方法论产出逻辑链的系统性推演。\n\n---\n\n### 一、 宏观背景与问题引入\n\n#### 1. Introduction 中的“讲故事”逻辑链\n作者在引言部分通过层层递进的对比，构建了研究的必要性，其逻辑链条如下：\n\n1.  **现状确立**：大语言模型（LLMs）已在程序合成、符号推理和目标导向规划等多种决策任务中展现出强大能力。为了支持长周期任务，现有研究开始将LLMs与蒙特卡洛树搜索（MCTS）等结构化规划算法相结合。\n2.  **揭示缺陷**：尽管取得了进展，但大多数现有方法将LLMs视为“单一整体”，在不同任务中统一应用，忽略了模型之间细微的性能差异。\n3.  **深入剖析**：在实践中，不同的LLMs在不同领域和推理要求下表现各异（有的擅长符号操作，有的擅长语言生成）。如果不加区分地使用，会导致性能脆弱：在一个任务上表现出色的模型在另一个任务上可能表现糟糕。\n4.  **指出僵化**：此外，这些系统通常遵循静态推理流程，无论任务复杂性如何，都以固定的推理深度调用相同的模型。\n5.  **后果分析**：这种僵化的设置忽略了决策任务中的两个关键变异性来源：(1) 不同模型对不同推理子任务的适用性；(2) 达到满意结果所需的规划量变化。\n6.  **总结痛点**：这导致当前的LLM智能体经常遭受低效的搜索行为、早期错误的累积以及在简单任务上不必要的计算开销。\n\n#### 2. 核心研究问题\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何构建一个自适应的决策框架，能够根据任务上下文动态利用不同LLM的特定优势，并基于推理质量自适应地调整规划深度，以解决现有单一模型静态推理带来的低效与脆弱性问题？”**\n\n---\n\n### 二、 思想演进与方法论形成\n\n为了解决上述问题，作者的思考过程经历了从观察到假设，再到系统构建的四个关键阶段：\n\n#### 第一阶段：打破“单体”迷信，确立“专家委员会”假设\n*   **思考起点**：既然不同的LLM（如GPT-4, Llama, Mistral）在代码、数学、文本等不同领域各有千秋，为什么非要只用一个？\n*   **逻辑推演**：如果我们将多个LLMs视为一个“委员会”，每个成员都是一个潜在的“专家”，那么系统的目标就不再是“让一个万能模型解决所有问题”，而是“在正确的时间找到正确的专家”。\n*   **初步构想**：建立一个多模型协作框架，但这不仅仅是简单的投票或并行，而是需要一种机制来识别谁擅长什么。\n\n#### 第二阶段：从“静态标签”到“动态记忆”的专家画像\n*   **思考深化**：如何知道哪个模型擅长当前任务？依赖人工标注的静态标签（如“这个模型是写代码的”）太粗糙且不灵活。\n*   **逻辑推演**：模型的能力体现在它过去的成功轨迹中。如果模型A在某种类型的推理路径上经常成功，那么当遇到类似上下文时，它再次成功的概率就高。\n*   **方法论创新**：引入**成功记忆片段**。不依赖预定义标签，而是从成功的任务轨迹中提取片段，构建每个模型的“专家画像”。在推理时，通过语义匹配当前上下文与历史成功片段，实现**任务感知的路由**。\n\n#### 第三阶段：引入“规划”以解决长程决策的不确定性\n*   **思考挑战**：仅仅选对模型还不够。决策任务往往是多步的，当前选对了专家，但这一步的决策对长远目标是否有利？\n*   **逻辑推演**：需要引入前瞻机制。MCTS是处理序列决策的经典工具，但传统的MCTS在文本环境中缺乏准确的价值函数。\n*   **方法论融合**：将“专家委员会”与MCTS结合。但这带来了新问题：MCTS的节点价值如何评估？单纯依赖环境奖励太稀疏，单纯依赖模型自我评估太主观。\n\n#### 第四阶段：构建“双信号”机制与自适应深度控制\n*   **思考完善**：如何让MCTS更聪明地搜索？我们需要一个既能反映当前判断质量，又能反映历史经验的价值信号。\n*   **逻辑推演**：\n    1.  **信号一（即时判断）**：让随机抽样的专家评估当前轨迹的合理性。\n    2.  **信号二（历史先验）**：查询成功记忆，看类似的历史片段最终导致了多少成功。\n    3.  **自适应融合**：根据节点间的方差动态调整这两个信号的权重。如果某个信号在当前候选节点中区分度更高（方差大），就给它更高的权重。\n*   **最终闭环**：基于这个融合后的价值Q(s)来指导MCTS的搜索。如果价值很高，说明路径清晰，可以浅层搜索（节省算力）；如果价值模糊，则深入探索。从而实现了**自适应的决策路径**。\n\n---\n\n### 三、 总结：作者的思想全景\n\n作者的核心思想演进可以概括为：\n\n**从“单一模型的静态应用” $\\rightarrow$ 观察到“模型专业化差异与任务复杂性” $\\rightarrow$ 提出“基于历史成功记忆的专家委员会” $\\rightarrow$ 融入“MCTS进行结构化规划” $\\rightarrow$ 最终通过“双信号价值估计”实现计算效率与决策质量的自适应平衡。**\n\n这一过程体现了作者从**现象观察**（模型各有长短）到**机制设计**（记忆路由），再到**系统优化**（双信号MCTS）的完整学术思维闭环。"
                },
                {
                    "title": "Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments",
                    "arxiv_id": "2601.22647",
                    "authors": "Jinwoo Jang, Minjong Yoo, Sihyung Yoon, Honguk Woo",
                    "summary": "Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。以下是详细的判断依据： 1.  **核心判断（符合）：** 论文的核心贡献是提出了一种名为 \"Test-time Mixture of World Models (TMoW)\" 的新框架。这不仅仅是将现有智能体应用到某个领域，而是针对 LLM 智能体（特别是具身智能体）在动态环境中的适应性问题，提出了一种新的架构和机制。它属于构建和改进 LLM 智能体的方法论研究。 2.  **符合研究焦点（自我演化与单智能体）：** *   **自我演化:** 论文的核心亮点在于 \"Test-time\"（测试时）的适应和更新。它提出在推理过程中动态更新路由函数，允许智能体重新组合现有模型并集成新模型以进行“持续适应”。这种在测试时根据环境反馈进行自我调整、少样本扩展和零样本适应的能力，正是“自我演化”和“自我完善”的典型体现。 *   **单智能体:** 论文关注的是单个具身智能体如何通过构建更灵活的世界模型来提升推理和决策能力，这属于单智能体的规划与决策范畴。 3.  **排除标准检查：** *   虽然论文涉及 \"Embodied Agents\"（具身智能体）并在模拟环境（如 VirtualHome, RLBench）中测试，通常涉及视觉信息，但其核心贡献**不在于**视觉处理或多模态模型本身，而在于**世界模型的混合与适应机制**。视觉/环境信息在这里是智能体感知的工具，而非研究核心，因此符合“除非它们被用作智能体感知环境的工具”这一例外条款。 *   论文不涉及安全、对齐或图神经网络等排除领域。 4.  **特殊处理：** 论文提出的“测试时细化”和“持续适应”机制，本质上是一种让智能体在部署后通过环境反馈进行自我演化的算法，符合筛选标准第四步中关于“自我演化机制”的保留要求。 综上所述，该论文通过提出一种新的混合世界模型机制，显著增强了 LLM 智能体在动态环境下的自我演化和适应能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在增强具身智能体在动态环境中对未见领域的适应性。针对动态环境下的任务和场景变化，我们提出了一种 Test-time Mixture of World Models (TMoW) 框架，通过多粒度原型路由、测试时原型细化和基于蒸馏混合的模型增强实现测试时适应。我们在 VirtualHome、ALFWorld 和 RLBench 等基准上通过 Success Rate (SR) 和 Pending Steps (PS) 验证了其有效性。",
                    "summary_translation": "基于 Language model (LM)（语言模型）的 embodied agents（具身智能体）正日益部署于现实世界场景中。然而，其在动态环境中的适应性仍然受限，而在此类环境中，构建准确且灵活的 world models（世界模型）对于有效的 reasoning（推理）和 decision-making（决策）至关重要。为应对这一挑战，我们将 Mixture-of-Experts (MoE)（混合专家模型）范式扩展至 embodied agents。尽管传统的 MoE 架构通过 pre-trained routing（预训练路由）将知识模块化为 expert components（专家组件），但它们一旦部署便趋于僵化，导致其在动态环境中适应 unseen domains（未见领域）的效果不佳。因此，我们提出了 Test-time Mixture of World Models (TMoW)（测试时世界模型混合）框架，旨在增强对 unseen and evolving domains（未见及不断演变领域）的适应性。与传统 MoE 中路由函数保持固定不同，TMoW 能够在 test time（测试时）更新其针对 world models 的 routing function（路由函数），从而使智能体能够重新组合现有模型并集成新模型，以实现 continual adaptation（持续适应）。这一目标通过以下机制实现：(i) multi-granular prototype-based routing（多粒度基于原型的路由），根据从 object- 到 scene-level（物体级到场景级）的相似性调整模型组合；(ii) test-time refinement（测试时细化），在 inference（推理）期间将 unseen domain features（未见领域特征）与 prototypes（原型）进行对齐；(iii) distilled mixture-based augmentation（基于混合蒸馏的增强），利用 few-shot data（少样本数据）和现有 prototypes 高效构建新模型。我们在 VirtualHome、ALFWorld 和 RLBench 基准测试上对 TMoW 进行了评估，结果表明其在 zero-shot adaptation（零样本适应）和 few-shot expansion（少样本扩展）场景下均表现出强劲的性能，并证实了该框架能够使 embodied agents 在动态环境中有效运行。",
                    "inspiration_trace": "基于对论文《Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments》的深入分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与背景\n**起点：** 语言模型（LM）驱动的具身智能体正被部署到家庭、工厂等真实物理世界中。\n**现实冲突：** 真实世界是高度**动态**的，任务和环境不断变化；而现有的智能体架构本质上是**静态**的，其能力在训练完成后即被冻结。\n\n---\n\n### 2. 引言中的“讲故事”逻辑（问题演进）\n作者在Introduction中通过层层递进的逻辑，揭示了现有技术的局限性，从而引出本文的动机：\n\n1.  **现状的僵化：** 现有的基于LM的智能体多采用“单体架构”，其领域知识固化在数十亿参数中。一旦环境超出训练分布，智能体便束手无策。\n2.  **现有解法的不足：**\n    *   **重训练：** 面对新环境，唯一的办法是重新训练整个模型，但这在真实场景中计算成本极高且数据难以获取。\n    *   **上下文学习：** 虽然试图通过提示词来适应，但这只是将计算负担转移到了推理阶段（上下文窗口膨胀），并未从根本上解决模型架构的适应性问题。\n3.  **引入MoE及其缺陷：** 混合专家模型提供了结构上的模块化，允许按需激活特定领域的专家。然而，**传统的MoE架构中，负责选择专家的“路由函数”在训练后是固定的**。这意味着面对未见过的领域，路由器无法正确调度，依然需要昂贵的端到端重训练。\n4.  **核心痛点：** 在时空不断变化的动态环境中，我们需要一种能够**自适应地重新配置路由函数**，并能**动态扩展领域专家**的机制，而不是僵化的固定路由。\n\n---\n\n### 3. 研究问题\n基于上述逻辑链，作者试图回答的核心问题是：\n\n**“如何使具身智能体在不进行昂贵重训练的情况下，通过在测试时动态重新配置世界模型的混合策略，来适应未见过的和不断演进的动态环境？”**\n\n---\n\n### 4. 思想演进与方法论形成\n为了解决上述问题，作者的思考路径经历了从“架构选择”到“机制创新”的演进：\n\n#### 第一阶段：从“单体”到“模块化”的突破\n*   **思考：** 既然单体模型无法适应，必须采用模块化设计。\n*   **决策：** 借鉴**混合专家**架构，将不同的世界模型作为专家，每个专家负责特定的环境动力学。\n\n#### 第二阶段：打破“固定路由”的桎梏\n*   **思考：** 传统MoE的路由器是静态的，无法处理未见过的领域。我们需要让路由器在测试时也能“学习”和“进化”。\n*   **决策：** 提出**测试时混合**的概念。不再依赖固定的路由权重，而是允许在推理过程中更新路由函数。\n\n#### 第三阶段：如何实现“动态路由”？（多粒度原型）\n*   **思考：** 要动态匹配新环境，需要一个参照系。简单的特征匹配不够，因为新环境可能与旧环境在局部（物体）相似，但在全局（场景）不同，反之亦然。\n*   **决策：** 受到LM从Token到段落层级处理的启发，设计**多粒度原型路由器**。构建从局部物体到全局场景的分层原型，通过图神经网络在不同抽象层级上计算输入与原型的相似度，实现层级的专家混合。\n\n#### 第四阶段：如何适应“未见领域”？（测试时细化）\n*   **思考：** 遇到完全没见过的环境时，没有现成的原型匹配。直接创建新模型成本太高。\n*   **决策：** 提出**测试时原型细化**。不创建新模型，而是根据当前环境特征，通过加权插值的方式，微调现有的原型集合。这相当于“软扩展”了原型的覆盖范围，让旧模型能处理新情况。\n\n#### 第五阶段：如何应对“巨大差异”？（蒸馏增强）\n*   **思考：** 如果新环境与旧环境差异太大，仅靠微调原型不够，必须引入新知识。但只有少量样本，从头训练不现实。\n*   **决策：** 提出**基于混合的模型增强**。利用现有模型的混合权重作为初始化，通过少量样本进行知识蒸馏，快速生成一个新的世界模型专家，并将其无缝集成到路由系统中。\n\n### 总结\n作者的思考过程是从**发现静态架构与动态环境的不匹配**开始，经过**MoE架构的引入与批判**，最终锁定在**路由机制的动态化**上。通过引入**多粒度原型**作为度量标准，并设计了**原型细化**（微调）和**模型增强**（扩容）两种互补的测试时适应机制，构建了一个既能灵活重组现有知识，又能高效吸纳新知识的动态系统。"
                },
                {
                    "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
                    "arxiv_id": "2601.22623",
                    "authors": "Wei Zhu, Zhiwen Tang, Kun Yue",
                    "summary": "Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，具体判断过程如下： 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **SYMPHONY**，这是一个**异构语言模型智能体组装的协同多智能体规划框架**。 *   它属于典型的 **Multi-Agent Systems (MAS)** 范畴，旨在解决现有单智能体在蒙特卡洛树搜索（MCTS）规划中探索能力受限的问题。 *   这不是将现有智能体简单应用到特定领域（如医疗、金融），而是提出了一个新的智能体架构和协作机制，因此属于“保留”范畴。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 明确涉及 `Multi-Agent Systems (MAS)` 和 `LLM-based Agents`。 *   **智能体能力**: 核心关注点在于 `Planning`（规划），特别是通过多智能体协作来改进 MCTS 规划过程。 *   **多智能体**: 涉及异构智能体（`Heterogeneous`）的协同（`Synergistic`）和协调（`Coordination`），利用多样化的推理模式增强探索能力。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除主题。 4.  **特殊情况处理 (第四步)**: *   论文关于规划的研究属于“智能体如何进行规划”，即通过构建多智能体系统来增强规划性能，而非单纯提升LLM的基础Token预测能力，因此符合保留条件。 **结论**: 该论文聚焦于通过多智能体协作来提升LLM智能体的规划能力，属于“多智能体”方向的核心研究，完全符合筛选要求。",
                    "summary2": "本文旨在解决现有单智能体规划中探索多样性受限的问题。针对复杂的多步推理与决策任务，我们提出了一种名为SYMPHONY的协同多智能体规划框架，该框架通过集成异构语言模型池、自适应调度及记忆共享机制增强搜索能力。我们在HotpotQA、WebShop和MBPP数据集上，通过Exact Match、Success Rate和Pass@1指标验证了其有效性，结果显示该方法优于现有最先进基线。",
                    "summary_translation": "近期的研究进展日益集中于利用大语言模型构建用于解决复杂问题的自主智能体。然而，现有方法主要采用单智能体框架，在蒙特卡洛树搜索规划过程中生成搜索分支并估算奖励。这种单智能体范式本质上限制了探索能力，往往导致生成的分支多样性不足，进而造成规划性能欠佳。为克服这些局限性，我们提出了异构语言模型组装的协同多智能体规划，这是一个集成了异构语言模型智能体池的新型多智能体规划框架。通过利用不同智能体间多样化的推理模式，SYMPHONY 增强了推演的多样性，并促进了更有效的探索。在多个基准任务上的实证结果表明，即使使用可在消费级硬件上部署的开源大语言模型进行实例化，SYMPHONY 依然表现出强劲的性能。当通过 API 接入基于云的大语言模型进行增强时，SYMPHONY 展现出进一步的性能提升，超越了现有的最先进基线，从而凸显了异构多智能体协调在规划任务中的有效性。",
                    "inspiration_trace": "基于对论文《SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 一、 宏观观察与问题引入（“讲故事”的逻辑）\n\n作者首先构建了一个从技术趋势到现实缺陷的叙事链条，具体逻辑如下：\n\n1.  **技术背景与现状**：\n    *   大语言模型（LLMs）已被广泛用于构建能够处理复杂任务（如问答、代码生成、网页导航）的自主智能体。\n    *   为了增强决策能力，当前的主流范式是将LLM与蒙特卡洛树搜索（MCTS）相结合，利用LLM引导潜在行动序列的探索。\n\n2.  **现有做法的假设**：\n    *   现有的先进方法（如ToT, RAP, LATS）普遍采用**单智能体框架**。\n    *   **核心假设**：在MCTS过程中，通过多次查询同一个LLM（或仅对提示词做微小扰动），利用模型输出的随机性或采样方差，就足以生成多样化的搜索分支。\n\n3.  **现实观察与冲突（The \"But\"）**：\n    *   **观察**：在实践中，这种做法存在严重缺陷。同一个模型在不同调用下的输出往往表现出高度的相似性，反映的是该模型习得的同一种“主导推理模式”。\n    *   **后果**：生成的推演缺乏有意义的多样性，导致搜索轨迹狭窄且冗余。这严重限制了智能体在解空间中的探索能力，使其容易陷入局部最优，难以发现新颖或意外的解决方案，且在复杂任务中往往伴随着过高的计算成本。\n\n4.  **核心矛盾总结**：\n    *   规划任务对**多样化探索**的迫切需求，与单一单体LLM所能提供的**有限变异性**之间存在根本性的不匹配。\n\n---\n\n### 二、 研究问题\n\n基于上述矛盾，作者试图解决的核心问题可归纳为：\n\n**“如何克服单智能体LLM规划框架中固有的探索能力限制和多样性不足，从而实现更鲁棒、更高效的多步推理？”**\n\n---\n\n### 三、 逻辑推演与假设形成\n\n为了解决上述问题，作者进行了如下逻辑推演：\n\n1.  **归因分析**：\n    *   单一模型之所以产生相似的分支，是因为其具有特定的“归纳偏置”和推理风格。无论怎么调整采样参数，都无法跳出其固有的知识结构和思维定势。\n\n2.  **提出假设**：\n    *   如果单一模型意味着单一偏见，那么**异构性**应该意味着**多样性**。\n    *   **假设**：如果使用一组具有不同预训练来源、架构和推理风格的异构LLMs，它们将提供互补的视角，从而在搜索树中引入结构性的多样性。\n\n3.  **机制推演**：\n    *   仅仅把多个模型放在一起是不够的，必须解决“如何协同”的问题。作者进一步推演出三个子问题及对应的解决思路：\n        *   *调度问题*：在搜索树的每个节点，该选哪个模型来生成动作？\n            *   *思路*：将其视为多臂老虎机问题，利用UCB（上置信界）策略，根据历史表现动态分配模型，平衡利用强模型和探索弱模型。\n        *   *评估问题*：不同模型对自己生成的动作信心度不同，如何统一评价标准？\n            *   *思路*：引入熵调制的置信度评分（EMCS），利用信息熵惩罚不确定性高的预测，校准价值估计。\n        *   *进化问题*：模型如何在不更新参数的情况下从失败中学习？\n            *   *思路*：建立池级记忆共享机制，通过自然语言反思将失败经验广播给所有智能体，实现轻量级的持续适应。\n\n---\n\n### 四、 方法论构建\n\n基于上述假设与推演，作者最终构建了 **SYMPHONY** 框架，其核心思想演进如下：\n\n1.  **从“单点”到“异构池”**：\n    *   将MCTS中的单一LLM替换为**异构智能体池**。这不仅是数量的增加，更是质的改变，旨在从根源上打破单一推理模式的局限。\n\n2.  **从“静态调用”到“动态调度”**：\n    *   设计**UCB驱动的调度器**。系统不再随机或轮询调用模型，而是根据每个模型在特定任务阶段的历史效用，智能地选择最合适的模型来扩展节点。\n\n3.  **从“孤立评估”到“不确定性感知”**：\n    *   引入**EMCS机制**。在节点评估时，不仅看模型给出的分数，还要看模型对自己判断的“确信度”（通过熵计算），抑制模糊不清的分支，提高搜索的稳定性。\n\n4.  **从“独立运行”到“协同反思”**：\n    *   建立**共享记忆机制**。当一个智能体失败时，它生成的反思会更新到整个池的上下文中，使得其他智能体在后续步骤中能避开同样的错误。\n\n---\n\n### 五、 验证与闭环\n\n最后，作者通过实验验证这一逻辑链条的有效性：\n*   **验证多样性**：通过消融实验证明，增加模型的异构性确实显著提高了分支的唯一性（4-Unique比例大幅提升）。\n*   **验证性能**：在HotpotQA、WebShop、MBPP等任务上，SYMPHONY不仅超越了单智能体基线，甚至在使用较小模型（SYMPHONY-S）时也能媲美或超越使用大模型的单智能体方法，证明了“协同”与“异构”的价值。\n\n**总结**：作者的思考路径是从发现“单一模型无法产生真多样性”这一现象出发，通过引入“异构性”作为破局点，进而设计了一套完整的调度、评估与记忆机制来驾驭这种异构性，最终实现了规划能力的提升。"
                },
                {
                    "title": "From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents",
                    "arxiv_id": "2601.22607",
                    "authors": "Jiaxuan Gao, Jiaao Chen, Chuyi He, Wei-Chen Wang, Shusheng Xu, Hanrui Wang, Di Jin, Yi Wu",
                    "summary": "Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，其核心贡献紧密围绕 LLM 智能体的构建与演化。具体判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为 **EigenData** 的统一框架，旨在解决交互式工具使用智能体的后训练难题。这不仅仅是将现有智能体应用于某个领域，而是提出了一种新的**构建和改进智能体**的方法论（结合自我演化数据合成与基于验证器的强化学习）。因此，它属于保留范畴。 2.  **正面指标匹配（第二步）：** *   **自我演化:** 论文标题和摘要中多次强调 \"Self-Evolving\"。其核心机制包含一个“闭环自我演化过程”，通过更新提示词和工作流来提高数据生成的可靠性。这直接对应我研究焦点中的“自我演化”方向。 *   **多智能体:** 摘要明确指出 EigenData 是一个 \"hierarchical multi-agent engine\"（分层多智能体引擎），利用多智能体系统来合成基于工具的对话。这符合“多智能体”的研究方向。 *   **智能体能力:** 论文专注于 \"Tool-Using Agents\"（工具使用智能体），涉及多步工具执行、对话状态跟踪等核心 Agentic 能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（第四步）：** *   论文提出的自我演化机制是其核心创新点，用于生成高质量的合成数据以训练智能体。这符合“自我演化的应用”中的保留规则（即核心是提出新的自我演化机制）。 **总结：** 该论文通过引入自我演化的多智能体框架来生成合成数据，并利用强化学习优化工具使用智能体，直接贡献于 LLM 智能体的构建、改进和演化，高度契合我对 Agentic AI 及其演化的研究目标。",
                    "summary2": "本文旨在解决交互式工具使用智能体后训练中数据获取困难和强化学习信号噪声大的问题。针对多轮交互场景，我们提出了一种结合自进化合成数据引擎EigenData与基于可验证奖励的RL框架。该方法通过分层多智能体系统生成高质量数据及验证函数，并采用GRPO进行训练。在$\\tau^2$-ench上通过pass^1等指标验证了其有效性，性能匹配或超越前沿模型。",
                    "summary_translation": "交互式工具使用代理必须通过与人类和外部环境的多轮交互来解决现实世界的任务，这要求其具备对话状态跟踪、多步工具执行能力，同时遵循复杂的指令。对此类代理进行后训练充满挑战，因为高质量多轮工具使用数据的合成难以扩展，且强化学习可能面临由用户模拟引起的噪声信号，从而导致训练效率下降。我们提出了一个统一的框架，结合了自进化数据代理与基于验证器的强化学习。我们的系统 EigenData 是一个分层多代理引擎，能够合成基于工具的对话以及可执行的逐实例检查器，并通过更新提示词和工作流的闭环自进化过程来提高生成可靠性。基于合成数据，我们开发了一种强化学习方案，首先微调用户模型，然后应用具有轨迹级组相对优势和动态过滤机制的 GRPO 风格训练，从而取得了超越监督微调的一致性改进。在 tau^2-bench 上评估时，我们的最佳模型在 Airline 任务上达到了 73.0% 的 pass^1，在 Telecom 任务上达到了 98.3% 的 pass^1，性能匹配或超过了前沿模型。总体而言，我们的研究结果提出了一种可扩展的路径，能够在无需昂贵人工标注的情况下引导复杂的工具使用行为。",
                    "inspiration_trace": "基于对论文《From Self-Evolving Synthetic Data to Verifiable-Reward RL》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一部分：宏观观察与问题定义（Introduction 的“故事”逻辑）\n\n**1. 范式转移的观察**\n作者首先观察到大语言模型（LLM）的发展趋势正在发生根本性转变：从单纯的“静态问答”进化为“交互式任务完成”。模型不再仅仅是回答问题，而是需要通过工具调用与外部环境进行多轮交互，以解决现实世界的复杂任务。\n\n**2. 核心矛盾的引入**\n作者指出，现有的工具使用研究大多集中在“单轮、自包含”的场景（即所有上下文 upfront 给定）。然而，真正的挑战在于**交互式代理**，其核心特征是**“活跃用户”的存在**。这引入了两个前所未有的困难：\n*   **信息不对称**：关键信息掌握在用户手中，代理必须通过多轮对话主动挖掘用户偏好和隐私信息。\n*   **行为不确定性**：用户的行为是不可预测的，可能增量提供信息、改变主意或给出意外回应。\n\n**3. 现有方案的瓶颈**\n作者进一步分析指出，尽管有强大的开源基座模型，但在后训练阶段将其转化为有效的交互式代理面临两大死结：\n*   **数据获取瓶颈**：高质量的多轮工具使用对话数据难以规模化。人工标注昂贵且耗时；而自动化合成极难，因为既要满足复杂的领域规则，又要模拟出连贯的用户指令和私有信息。\n*   **强化学习（RL）瓶颈**：RL 训练需要用户模拟器，这引入了额外的非确定性动力学。特别是在双控设置（用户也能调用工具）中，开源模型模拟用户时表现极不稳定，导致训练信号充满噪声，严重降低效率。\n\n---\n\n### **研究问题**\n\n**如何构建一个可扩展的后训练框架，使其既能生成高质量、符合复杂规则的多轮交互数据，又能通过稳定的强化学习流程，让代理在无需昂贵人工标注的情况下，有效掌握应对不确定用户行为的多轮工具使用能力？**\n\n---\n\n### 第二部分：思想演进与方法论形成（逻辑链推演）\n\n#### **阶段一：攻克数据瓶颈——从“静态合成”到“自我进化”**\n\n*   **思考起点**：既然人工标注太贵，简单的自动生成质量又差（容易产生不可解的任务或不真实的对话），那么能否让数据生成过程本身也具备“智能”和“纠错能力”？\n*   **逻辑推演**：\n    1.  **分工协作**：单一模型难以兼顾任务设计、对话生成和质量控制。作者构想了一个**分层多智能体架构**：上层负责“编排”（规划工作流、写提示词），下层负责“执行”（生成具体任务、对话、验证器）。\n    2.  **引入验证**：为了解决“不可解任务”和“幻觉”问题，必须在生成过程中引入强验证。不仅要验证对话，还要验证工具调用的可行性。\n    3.  **自我进化**：这是核心创新点。作者意识到，固定的提示词无法覆盖长尾复杂情况。因此，设计了一个闭环反馈机制：利用“评判者”对生成的中间产物进行多维度批评，然后利用这些反馈**动态更新**工作流和提示词。系统越用越聪明，数据质量越来越高。\n    4.  **附带产出**：为了解决后续 RL 的奖励问题，作者在生成数据的同时，让系统生成**可执行的验证函数**。这为后续的“可验证奖励 RL”埋下了伏笔。\n\n#### **阶段二：攻克 RL 瓶颈——从“噪声环境”到“稳定模拟”**\n\n*   **思考起点**：有了数据，直接上 RL 效果不好。为什么？因为环境（用户模拟器）太“蠢”或太“疯”，导致 Agent 即使做对了也可能因为用户乱搞而失败，从而收到错误的负奖励。\n*   **逻辑推演**：\n    1.  **先稳住环境**：在训练 Agent 之前，必须先训练 User。作者利用 EigenData 生成的合成对话，先对用户模型进行 SFT（监督微调）。这确保了 RL 训练环境中的用户行为是稳定、可预测且符合指令的。\n    2.  **对抗方差**：即使环境稳定了，交互本身的随机性依然很大。传统的 PPO 可能难以收敛。作者选择 **GRPO（Group Relative Policy Optimization）**，通过组内相对优势来归一化奖励，这天然适合处理这种高方差场景。\n    3.  **过滤无效信号**：如果一个任务的所有采样轨迹都成功或都失败，那么它们之间的相对优势为 0，没有学习价值。作者引入**动态过滤**机制，剔除这些无效组，让模型专注于那些“成败参半”的边缘案例，提高学习效率。\n    4.  **利用验证器**：使用阶段一生成的验证函数作为奖励信号，基于最终状态判断成败，避免了依赖不可靠的语言模型作为 Reward Model。\n\n#### **阶段三：系统集成与验证**\n\n*   **最终框架**：将上述两个阶段结合，形成 **EigenData（数据引擎） + 稳定化 RL（训练配方）** 的统一框架。\n*   **逻辑闭环**：EigenData 生成数据 -> 训练 User 模型 -> 训练 Agent 模型（SFT） -> 使用 GRPO 和验证器进行 RL 微调 -> 在 $\\tau^2$-bench 上验证效果。\n\n---\n\n### 总结\n\n作者的思考路径遵循了**“发现问题 -> 拆解瓶颈 -> 针对性创新 -> 系统集成”**的经典学术创新逻辑：\n1.  针对**数据难**，提出了**自我进化的多智能体生成系统**，用“生成-评判-修正”的闭环代替人工。\n2.  针对**RL 训练难**，提出了**先训用户、后训代理**的策略，并利用**GRPO + 动态过滤**来对抗交互环境的不确定性。\n3.  最终通过**可验证的奖励函数**将数据生成与强化学习无缝连接，实现了无需人工标注的规模化能力提升。"
                },
                {
                    "title": "Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution",
                    "arxiv_id": "2601.22528",
                    "authors": "Hongze Mi, Yibo Feng, WenJie Lu, Song Cao, Jinyuan Li, Yanming Li, Xuelin Zhang, Haotian Luo, Songyang Peng, He Cui, Tengfei Tian, Jun Fang, Hua Chai, Naiqiang Tan",
                    "summary": "Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文的核心贡献是提出了一种名为“达尔文记忆系统（DMS）”的新架构。这不仅仅是一个应用，而是一种**自我演化架构**，它利用“适者生存”法则和“效用驱动的自然选择”机制来管理智能体的记忆。这直接对应了研究焦点中的“自我演化”以及单智能体中的“记忆”和“自我反思/修正”能力。 2.  **符合“自我演化的应用”特殊规则**： 根据筛选标准第四步，虽然论文将该机制应用在了 GUI（图形用户界面）这一特定领域，但其核心在于提出了一种新的“自我演化”机制（通过自然选择修剪次优路径、抑制高风险计划）。因此，它不属于简单的“非演化型应用”，而是属于应保留的例外情况。 3.  **多模态/视觉作为工具而非核心**： 摘要中提到了“多模态大语言模型（MLLM）”和“GUI”，这涉及视觉元素。但根据筛选标准第三步，这里的视觉能力仅是智能体感知 GUI 环境的工具，论文的研究重点并非改进视觉模型本身，而是利用视觉输入来构建和优化演化式的记忆系统。因此，不应因此排除。 综上所述，该论文提出了一种创新的、基于演化论原理的智能体记忆系统，属于 Agentic AI 中自我演化和智能体架构的前沿研究。",
                    "summary2": "本文旨在解决GUI Agent在长时程跨应用任务中因上下文限制和记忆僵化导致的性能瓶颈。针对动态GUI环境，我们提出了一种受生物进化启发的Darwinian Memory System (DMS)，该系统通过将工作流解构为可复用单元并实施“适者生存”的自然选择机制，实现记忆的自我进化与优化。我们在AndroidWorld基准上通过Success Rate和Success Retention Rate (SRR)等指标验证了其有效性，显著提升了任务成功率和执行稳定性。",
                    "summary_translation": "Multimodal Large Language Model (MLLM，多模态大语言模型) 智能体促进了 Graphical User Interface (GUI，图形用户界面) 自动化，但由于上下文窗口有限，在处理长时程、跨应用程序任务时面临挑战。虽然记忆系统提供了一个可行的解决方案，但现有范式难以适应动态 GUI 环境，存在高层意图与低层执行之间的粒度不匹配问题，并受困于上下文污染，即过时经验的静态积累导致智能体产生幻觉。为解决这些瓶颈，我们提出了 Darwinian Memory System (DMS，达尔文记忆系统)，这是一种自进化架构，将记忆构建为一个由适者生存法则支配的动态生态系统。DMS 将复杂轨迹分解为独立的、可重用的单元以实现组合灵活性，并实施 Utility-driven Natural Selection (效用驱动的自然选择) 来追踪生存价值，主动修剪次优路径并抑制高风险计划。这种进化压力驱动智能体演化出更优的策略。在现实世界多应用基准测试上的广泛实验表明，DMS 在无需训练成本或架构开销的情况下提升了通用 MLLM 的性能，平均成功率提升了 18.0%，执行稳定性提升了 33.9%，同时降低了任务延迟，确立了其作为 GUI 任务的有效自进化记忆系统的地位。",
                    "inspiration_trace": "基于对论文《Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察与问题缘起\n**思考起点：** 多模态大语言模型（MLLM）在GUI自动化领域表现出色，具备强大的视觉感知能力。然而，当面对长周期、跨应用的复杂任务时，受限于有限的上下文窗口，Agent往往会出现“遗忘”或“幻觉”现象，导致任务失败。\n\n**初步假设：** 既然“脑子”记不住，那就需要一个外挂的“记忆系统”来存储历史经验，通过检索增强生成（RAG）来弥补上下文的不足。\n\n---\n\n### 2. 现有方案的“故事”逻辑与瓶颈挖掘\n作者在Introduction中通过批判性思维，揭示了直接套用现有记忆范式到GUI领域的两个致命缺陷，这也是本文要解决的核心痛点：\n\n*   **缺陷一：整体范式的僵化**\n    *   **现象：** 现有系统倾向于将长周期的操作流程作为一个静态、不可分割的整体序列进行存储。\n    *   **后果：** 这种做法将“高层意图”与“底层执行路径”死锁在一起。在GUI这种动态变化的环境中（如界面布局更新、弹窗出现），一旦环境发生微小变化，整个固定的序列就会失效。缺乏灵活性导致记忆极其脆弱。\n\n*   **缺陷二：静态存储的停滞**\n    *   **现象：** 现有系统只是静态地堆积数据，缺乏验证机制。\n    *   **后果：** GUI环境是非平稳的。过时的、次优的轨迹会作为“有毒先验”不断累积。由于缺乏“优胜劣汰”的筛选机制，Agent会被这些噪音误导，持续执行低效甚至错误的行为，导致上下文污染和推理停滞。\n\n---\n\n### 3. 核心研究问题\n基于上述观察与批判，作者将思考聚焦于以下核心问题：\n\n**“如何构建一种无需训练、能够适应动态GUI环境且具备自我调节能力的记忆系统，使其既能灵活组合以应对环境变化，又能自动剔除过时经验以防止上下文污染？”**\n\n---\n\n### 4. 概念跃迁：从“静态仓库”到“动态生态”\n为了回答上述问题，作者进行了一次跨学科的概念跃迁，从生物学中寻找灵感：\n\n*   **灵感来源：** 生物智能并非源于静态存储，而是源于种群的持续适应。物种通过自然选择保留有利性状，通过突变探索新路径，并淘汰不适应环境的个体。\n*   **隐喻映射：** 将Agent的记忆视为一个**动态生态系统**，而非死板的数据库。每一条记忆就是一个生物个体，需要经历“生存竞争”。\n\n---\n\n### 5. 方法论的逻辑演进\n基于“达尔文进化论”的隐喻，作者逐步构建了DMS的方法论框架：\n\n*   **解决“僵化” -> 结构重组（模块化）：**\n    *   **思路：** 既然整体序列太脆弱，那就将其“解构”。\n    *   **方案：** 将复杂的长轨迹拆解为独立的、可复用的子计划单元。这种细粒度的解耦使得历史经验可以像乐高积木一样灵活组合，适应动态环境。\n\n*   **解决“停滞” -> 优胜劣汰（效用驱动）：**\n    *   **思路：** 必须建立一套机制来评估记忆的“生存价值”，并主动清理垃圾。\n    *   **方案：** 引入“生存值”概念（综合考量使用频率、时间衰减、可靠性惩罚）。利用自然选择逻辑，主动修剪低价值路径，抑制高风险计划，防止有毒先验的积累。\n\n*   **解决“局部最优” -> 进化压力（突变与探索）：**\n    *   **思路：** 仅仅保留好的记忆还不够，Agent需要不断寻找更好的解法，避免陷入局部最优。\n    *   **方案：** 引入“突变机制”（$\\epsilon$-greedy策略）。即使有高置信度的记忆，也以小概率强制重新探索。如果新路径更高效，就覆盖旧记忆。同时，利用贝叶斯风险评估来动态调整探索阈值，迫使Agent持续进化。\n\n---\n\n### 总结\n作者的思考路径遵循了 **“发现宏观瓶颈 -> 批判现有方案 -> 引入生物学隐喻 -> 重构系统架构”** 的逻辑链条。他们没有试图修补传统的RAG系统，而是从根本上重新定义了记忆的本质——从**静态的存储仓库**转变为**遵循达尔文进化法则的动态生态系统**，从而在无需训练的情况下实现了Agent能力的自我迭代与进化。"
                },
                {
                    "title": "Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents",
                    "arxiv_id": "2601.22311",
                    "authors": "Zehong Wang, Fang Wu, Hongru Wang, Xiangru Tang, Bolian Li, Zhenfei Yin, Yijun Ma, Yiyang Li, Weixiang Sun, Xiusi Chen, Yanfang Ye",
                    "summary": "Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向，具体涉及“规划”子方向。 1.  **核心判断（符合）**：论文的核心贡献在于分析并解决了 LLM 智能体在长视界决策制定中的规划失败问题。它提出了一种名为 FLARE 的新机制，通过引入前瞻和价值传播来改进智能体的规划能力。这属于构建和改进 LLM 智能体方法论的研究，而非将智能体作为工具应用到特定领域的非演化型应用。 2.  **正面指标（匹配）**：论文的核心关注点正是 `Agentic AI` 和 `LLM-based Agents`。它深入探讨了智能体的 `Planning` 能力，区分了基础的 `Reasoning`（推理）与面向长视界的 `Planning`（规划），并提出了改进智能体决策框架的方案。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉或图技术等排除项。 4.  **特殊情况处理（符合）**：根据第四步关于“推理/规划”的规则，该论文虽然涉及推理，但其本质是关于“智能体如何进行规划”以及“在复杂任务中进行多步决策”。它批评了单纯的逐步推理在智能体规划中的不足，并提出了新的 Agentic 框架（FLARE），因此属于保留范畴。 综上所述，该论文通过提出新的规划机制（FLARE）来增强 LLM 智能体的能力，直接贡献于 Agentic AI 的发展，符合筛选要求。",
                    "summary2": "本文旨在解决LLM智能体在长视界决策中因逐步推理导致的短视失败问题。针对确定性且结构化的环境，我们提出了一种名为FLARE的规划框架，通过显式前瞻、价值回传和有限承诺机制实现future-aware planning。在CWQ、WebQSP、GrailQA及ALFWorld等基准测试上，通过Hits@1和成功率等指标验证了其有效性，显著提升了长视界决策性能。",
                    "summary_translation": "基于 Large language model (LLM) 的智能体在 short horizons (短期视界) 内表现出强大的 step-by-step reasoning (逐步推理) 能力，但在 long planning horizons (长期规划视界) 上往往难以维持连贯的行为。我们认为这种失败反映了一种 fundamental mismatch (根本性不匹配)：step-wise reasoning (逐步推理) 诱导了一种 step-wise greedy policy (逐步贪婪策略)，该策略适用于 short horizons (短期视界)，但在 long-horizon planning (长期规划) 中会失效，因为在长期规划中，早期行动必须考虑到 delayed consequences (延迟后果)。从这种 planning-centric perspective (以规划为中心的视角) 出发，我们在具有 explicit state transitions (显式状态转移) 和 evaluation signals (评估信号) 的 deterministic (确定性)、fully structured environments (完全结构化环境) 中研究了基于 LLM 的智能体。我们的分析揭示了 reasoning-based policies (基于推理的策略) 的一种 core failure mode (核心失效模式)：由 step-wise scoring (逐步打分) 诱导的 locally optimal choices (局部最优选择) 导致了 early myopic commitments (早期短视承诺)，这些承诺会随时间被 systematically amplified (系统性放大)，且难以从中恢复。我们介绍了 FLARE (Future-aware Lookahead with Reward Estimation) 作为 future-aware planning (具有未来感知的规划) 的 minimal instantiation (最小化实例)，旨在单个模型中实施 explicit lookahead (显式前瞻)、value propagation (价值传播) 和 limited commitment (有限承诺)，从而使 downstream outcomes (下游结果) 能够影响早期决策。在多个 benchmarks (基准测试)、agent frameworks (智能体框架) 和 LLM backbones (LLM 骨干网络) 中，FLARE 持续提升了 task performance (任务性能) 和 planning-level behavior (规划层面的行为)，经常使得配备 FLARE 的 LLaMA-8B 能够胜过采用标准 step-by-step reasoning (逐步推理) 的 GPT-4o。这些结果确立了 reasoning (推理) 与 planning (规划) 之间的清晰界限。",
                    "inspiration_trace": "基于对论文《Why Reasoning Fails to Plan》的深度分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从现象到矛盾\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在揭示当前LLM Agent研究中的一个核心盲点：\n\n1.  **现象观察：** LLM Agent在短视距任务中表现出了惊人的逐步推理能力（如Chain-of-Thought），但在长视距任务中往往无法维持连贯的行为。\n2.  **提出矛盾：** 这种失败揭示了一个根本性的**错配**——逐步推理本质上是一种“逐步贪婪策略”。这种策略在短视距下足够，但在长视距规划中失效，因为早期的行动必须考虑到延迟的后果。\n3.  **深入归因：** 现有的推理范式（如CoT、ReAct、Reflexion）虽然能减少局部错误，但本质上仍属于“逐步决策”。它们基于局部信号选择行动，缺乏对结果的显式评估。这导致了一个系统性问题：早期看似合理的局部选择，往往会导致长期的不可逆失败。\n4.  **提出挑战：** 要回答“LLM Agent究竟能否进行长视距规划，还是只能进行局部推理？”这个问题非常困难，因为长视距任务往往混杂了环境的不确定性（如部分可观测、随机性），导致难以区分是环境太难还是Agent的决策机制有问题。\n5.  **解决思路：** 为了剥离环境干扰，作者采用“控制变量”的诊断设置：在**确定性、全结构化**的环境（状态转移明确、评估信号显式）中研究Agent。如果在这种“上帝视角”下Agent依然失败，那么问题一定出在决策机制本身，而非环境复杂性。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者显式提出了本文试图回答的核心问题：\n\n**“Can LLM-based agents truly plan over long horizons, or can they only reason locally about the next step?”**\n（LLM Agent 究竟能否进行长视距规划，还是只能对下一步进行局部推理？）\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n以下是从宏观问题出发，逐步聚焦并最终形成FLARE方法的完整思考过程：\n\n#### 第一阶段：概念重构与假设提出\n*   **观察：** 现有的LLM推理方法（如CoT）在长链条任务中表现不佳，且容易陷入死胡同。\n*   **反思：** 学术界常将“推理”与“规划”混为一谈。作者提出假设：**推理不等于规划**。\n*   **定义：** 将标准的LLM推理形式化为一种**基于局部得分的逐步贪婪策略**。\n*   **推论：** 这种策略只关注“下一步看起来是否合理”，而不关注“这一步对最终目标意味着什么”。这就是“短视”。\n\n#### 第二阶段：控制实验与机制诊断\n*   **实验设计：** 为了验证上述假设，作者构建了一个“理想环境”——知识图谱问答（KGQA）。在这个环境中，状态转移是确定的，评估信号是显式的（Oracle设置）。这消除了环境噪声，纯粹测试决策机制。\n*   **发现：** 即使在信息全知的情况下，基于贪婪策略的Agent依然表现糟糕，且随着视距增加，性能断崖式下跌。\n*   **诊断核心病灶：** **早期短视承诺**。\n    *   Agent在第一步往往选择了一个局部得分高但全局是死胡同的路径。\n    *   一旦选错，由于缺乏回溯机制，错误会被系统性放大，且无法恢复。\n*   **理论验证：** 作者从理论上证明了：逐步贪婪策略在长视距任务中可以是任意次优的；增加搜索宽度（如Beam Search）无法从根本上解决这个问题，因为它依然基于局部评分剪枝。\n\n#### 第三阶段：确立规划的最小必要条件\n*   **对比分析：** 既然“推理”（逐步贪婪）失败了，那么真正的“规划”需要什么？\n*   **归纳三要素：** 作者指出，连贯的长视距规划必须包含三个最小机制：\n    1.  **显式的前瞻：** 必须模拟未来的轨迹，而不是只看下一步。\n    2.  **反向的价值传播：** 未来的结果必须能够回传并修正当前的决策。\n    3.  **有限的承诺：** 不要一次性定死整个计划，要允许随着新信息的到来而重新规划。\n\n#### 第四阶段：方法论构建\n*   **目标：** 设计一个能够实例化上述三个要素的最小化框架，以此证明“规划”优于“推理”。\n*   **工具选择：** 选择**蒙特卡洛树搜索（MCTS）**作为载体，因为它天然具备树搜索（前瞻）和回传（价值传播）的结构。\n*   **方法命名：** **FLARE (Future-aware Look Ahead with Reward Estimation)**。\n*   **核心机制设计：**\n    *   *显式前瞻：* 在MCTS的Selection阶段，利用UCB规则探索未来轨迹，而非依赖LLM的局部概率。\n    *   *价值传播：* 在Backpropagation阶段，将轨迹末端的累积奖励回传给根节点，让早期决策看到“后果”。\n    *   *有限承诺：* 采用滚动视界控制，只执行当前最优的一步，然后进入下一步重新规划，避免长序列的锁定。\n\n#### 第五阶段：验证与结论\n*   **实验验证：** 在KGQA和ALFWorld等任务上，FLARE不仅超越了标准的逐步推理，甚至让小模型（LLaMA-8B）配合FLARE击败了大模型（GPT-4o）配合标准推理。\n*   **最终结论：** 长视距决策的失败不是模型能力不足，而是决策范式（推理 vs 规划）的根本差异。只有引入显式的规划机制，才能解决早期短视承诺的问题。"
                },
                {
                    "title": "The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution",
                    "arxiv_id": "2601.22290",
                    "authors": "Khush Patel, Siva Surendira, Jithin George, Shreyas Kapale",
                    "summary": "Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Agentic AI** 和 **Multi-Agent Systems** 的核心贡献。 1.  **核心贡献符合构建智能体的目标**: 论文提出了 \"Six Sigma Agent\" 这一新颖的架构，其核心在于构建一种高可靠性的LLM智能体系统。这不仅仅是将LLM作为工具应用，而是提出了一种新的智能体运行框架。 2.  **符合单智能体与多智能体的关键特征**: *   **单智能体**: 论文涉及任务分解（Task Decomposition）为原子动作的依赖树，这是智能体 **规划** 能力的核心体现。 *   **多智能体**: 论文的核心机制 \"micro-agent sampling\" 和 \"consensus voting\" 实际上构建了一个多智能体协作的场景。通过让多个智能体（或同一智能体的多个实例/不同模型）并行执行任务并进行投票共识，这属于多智能体系统中的 **协作** 与 **共识** 机制的研究。 3.  **非排除项**: *   虽然论文提到了 \"enterprise use cases\"（企业用例），但其核心贡献在于解决可靠性的 **架构和方法论**（共识驱动的分解执行），而非单纯的应用落地。 *   论文不涉及安全对齐、多模态或图技术等排除领域。 综上所述，该论文通过引入冗余和共识机制来演化和改进智能体系统的可靠性，属于构建和改进LLM智能体的前沿研究。",
                    "summary2": "本文旨在解决LLM在企业部署中的可靠性挑战。针对多步骤工作流中的错误累积问题，我们提出了一种Six Sigma Agent架构，通过原子任务分解、微智能体并行采样及带动态扩展的共识投票机制提升可靠性。在三个企业用例上，通过DPMO和准确率验证，该方法实现了3.4 DPMO的六西格玛标准，可靠性提升14,700倍且成本降低80%。",
                    "summary_translation": "Large Language Models (大语言模型) 展现出卓越的能力，但本质上仍具有概率性，这给企业部署带来了严峻的 reliability (可靠性) 挑战。我们提出了 Six Sigma Agent (六西格玛智能体)，这是一种新颖的架构，通过三个协同组件实现 enterprise-grade reliability (企业级可靠性)：(1) task decomposition (任务分解)，将任务分解为 atomic actions (原子动作) 的 dependency tree (依赖树)；(2) micro-agent sampling (微型智能体采样)，即在不同的 LLMs 上并行执行每个任务 n 次以生成 independent outputs (独立输出)；(3) consensus voting with dynamic scaling (动态扩展的共识投票)，即对输出进行 clustering (聚类) 并从得票最多的 winning cluster (获胜聚类) 中选择答案。我们证明，采样 n 个 error rate (错误率) 为 p 的 independent outputs (独立输出) 可将 system error (系统错误) 降低至 $O(p^{ceil(n/2)})$，从而实现指数级的 reliability (可靠性) 提升。即使使用 per-action error (单次动作错误率) 为 5% 的成本较低模型，使用 5 个 agents (智能体) 的 consensus voting (共识投票) 也能将错误率降至 0.11%；dynamic scaling (动态扩展) 至 13 个 agents (智能体) 则能达到 3.4 DPMO (Defects Per Million Opportunities，百万机会缺陷数)，即 Six Sigma (六西格玛) 标准。在三个 enterprise use cases (企业用例) 中的评估结果显示，与 single-agent execution (单智能体执行) 相比，该方法实现了 14,700 倍的 reliability (可靠性) 提升，同时将成本降低了 80%。我们的工作确立了 AI 系统中的 reliability (可靠性) 源于 principled redundancy (有原则的冗余) 和 consensus (共识)，而非单纯依靠 model scaling (模型扩展)。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "JAF: Judge Agent Forest",
                    "arxiv_id": "2601.22269",
                    "authors": "Sahil Garg, Brad Cheezum, Sridhar Dutta, Vishal Agarwal",
                    "summary": "Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective. Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs. To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文提出了 JAF (Judge Agent Forest) 这一新框架，其核心目的是通过改进评判智能体来促进主智能体的“iterative self-refinement”（迭代自我完善）和自我改进。这直接对应了您研究焦点中的“自我演化”以及单智能体能力中的“自我反思”和“自我修正”。 2.  **属于 Agentic AI 方法论创新**： 论文并非简单应用现有框架，而是构建了一个新的 Agentic 框架。它将评判智能体从局部评估者提升为整体学习者，利用联合推理和集成学习原理来优化智能体的反馈机制。这属于构建和改进 LLM 智能体的方法论研究。 3.  **符合“自我演化的应用”例外规则**： 尽管论文在“云配置错误分类”这一特定领域进行了验证，但根据筛选标准第四步，只要论文的核心是提出一种新的“自我演化”机制（即 JAF 框架），即使应用在特定领域，也应该保留。该论文的核心在于 JAF 的机制设计，而非云配置技术本身。 4.  **排除标准检查**： *   **非演化型应用**：否。论文提出了新的 JAF 框架，而非单纯应用 LLM。 *   **非Agentic的推理**：否。论文明确关注 Agentic AI 框架中的评判与反思机制。 *   **基础设施**：否。 *   **安全与对齐**：否。虽然涉及评估，但目标是自我完善和性能提升，而非专门的安全或对齐研究。 *   **多模态与视觉**：否。 *   **图**：摘要中提到“induce a knowledge-graph structure”主要是为了从概念上解释邻域关系和批评传播机制，并非基于图神经网络（GNN）或知识图谱构建的核心算法研究，因此不构成排除理由。 综上所述，该论文聚焦于通过改进评判机制来实现智能体的自我演化，符合您关于“LLM智能体及其演化”的核心研究目标。",
                    "summary2": "本文旨在解决Judge Agent孤立评估无法捕捉跨实例模式的问题。针对云环境漏洞分流场景，我们提出了JAF框架，利用LSH实现联合推理与邻居选择，并在云错误配置数据集上通过经验正确率概率验证了其有效性。",
                    "summary_translation": "评判智能体是智能体AI框架的基石：它们提供自动评估，并使推理过程的迭代式自我修正成为可能。我们介绍了 JAF：Judge Agent Forest（评判智能体森林），这是一个框架，其中评判智能体对由主智能体生成的一批查询-响应对进行联合推理，而非孤立地评估每一个。这种范式将评判智能体从局部评估器提升为整体学习者：通过同时评估相关的响应，评判智能体能够识别跨实例的模式和不一致性，其聚合反馈使主智能体能够通过评判智能体的集体视角来审视自身的输出，从而获得改进。从概念上讲，JAF 架起了置信传播和集成学习原则之间的桥梁：重叠的上下文邻域诱导出一种知识图谱结构，促进了评判的传播，而重复的、随机化的评估产生了一个鲁棒的上下文敏感判断集成。JAF 可以完全通过 ICL (In-Context Learning, 上下文学习) 进行实例化，其中评判智能体针对每个查询的提示包含其关联的主智能体响应，加上一小部分可能存在噪声的同行示例。虽然嵌入空间中的 kNN (k-Nearest Neighbors, k近邻) 是选择示例的自然起点，但这种方法忽略了现代 LLMs (Large Language Models, 大语言模型) 能够理解的分类结构、领域元数据或细微差别。为了克服这些局限性，我们开发了一种灵活的 LSH (Locality-Sensitive Hashing, 局部敏感哈希) 算法，该算法通过整合语义嵌入、LLM 驱动的哈希谓词、来自分类标签的监督以及相关的辅助信息，来学习具有信息量的二进制码。这些哈希码支持高效、可解释且关系感知的多样化示例选择，并进一步优化 CoT (Chain-of-Thought, 思维链) 推理路径的探索。我们通过一项针对大规模云环境中云配置错误分诊这一艰巨任务的实证研究验证了 JAF。",
                    "inspiration_trace": "基于对论文内容的深度分析，以下是作者产出《JAF: Judge Agent Forest》这篇论文的完整逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过以下逻辑链条，将读者从宏观背景引向具体痛点：\n\n1.  **背景铺垫**：在当前的 Agentic AI 框架中，Judge Agent（评判智能体）扮演着核心角色，负责自动评估、执行特定领域的防护以及支持推理过程的迭代自我修正。\n2.  **现状描述**：现有的评判智能体工作流主要遵循**“实例局部”**的模式。即，每个查询 $Q_i$ 和对应的响应 $R_i$ 都是独立处理的，评判者只能看到当前这一对输入输出，与其他实例完全隔离。\n3.  **场景切入**：以“云环境漏洞分流”为例，指出这是一个典型的多步骤、上下文依赖的复杂任务。决策往往依赖于跨实例的细微交互（如网络拓扑、IAM策略等），且通常是以“队列”为单位进行决策的。\n4.  **揭示矛盾**：\n    *   **人类视角**：人类分析师在处理同一租户或账户下的所有问题时，会进行**联合评估**，利用跨问题的模式和一致性来辅助决策。\n    *   **AI视角**：当前的 AI 流程虽然处理的是同一批问题，但在逻辑上是割裂的。它无法利用“该网段下所有服务行为相似”这样的模式，也无法将针对某个问题发现的深刻见解（如某种微妙的错误配置）传播到其他相关问题中。\n5.  **指出缺失**：现有的机制缺乏一种让评判者对**相关查询-响应对**进行**联合、队列级推理**的能力，导致无法捕获队列级别的规律、依赖关系和关键信号。\n\n---\n\n### 二、 核心研究问题\n\n基于上述矛盾，作者显式提出了本文试图解决的核心研究问题：\n\n**“当评判者对相关的查询-响应对进行联合推理，而不是孤立地处理每一个实例时，智能体工作流的结构和能力会发生怎样的变化？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n为了回答上述问题，作者经历了从观察到假设，再到具体技术实现的思考演进：\n\n#### 1. 观察与假设：从“局部”到“整体”\n*   **观察**：实例局部的评判无法捕捉跨实例的依赖关系。真正的价值往往出现在“比较”之中（例如，比较两个相似资产的漏洞处理方式）。\n*   **假设**：如果让评判者在审查某个焦点实例时，也能看到同一队列中其他相关的实例，它就能从“局部的批评者”转变为“整体的学习者”。\n*   **预期效果**：这种联合视角能让评判者发现不一致性，并利用从其他实例中学到的模式来改进当前的判断。\n\n#### 2. 理论映射：连接经典机器学习理论\n*   **思考**：这种“联合推理”在数学上等价于什么？\n*   **映射**：当每个评判调用都包含一组邻居时，这些重叠的邻居关系实际上在队列上诱导了一个**知识图谱**结构。\n*   **机制推演**：\n    *   **信息传播**：如果迭代运行这个过程，针对一个实例的修正会通过邻居传播到其他实例。这类似于图神经网络中的**信念传播**。\n    *   **鲁棒性**：如果随机选择不同的邻居子集进行多次评判，这类似于**随机森林**的集成思想，通过多样化的上下文来获得更稳健的决策。\n\n#### 3. 实现挑战：如何选择“邻居”？\n*   **瓶颈**：LLM 的上下文窗口是有限的。我们无法把整个队列都塞进去，必须选择一小部分“邻居”作为上下文。\n*   **朴素方案**：最直观的方法是使用 k-近邻（kNN），基于语义嵌入向量来寻找最相似的实例。\n*   **批判性思考**：kNN 虽然自然，但它过于依赖几何距离，忽略了**类别结构**、**领域元数据**（如漏洞类型、资产类型）以及 LLM 能理解的细微差别。仅仅靠向量相似度是不够的。\n\n#### 4. 方法论创新：引入学习型 LSH\n*   **新思路**：需要一个既能高效检索，又能融合多维度信息（语义、标签、元数据）的机制。\n*   **技术选择**：**局部敏感哈希**。\n*   **改进**：不使用传统的随机投影哈希，而是设计一个**可学习的 LSH 算法**。\n    *   **目标**：学习二进制哈希码，使得相似的实例落入同一个桶中。\n    *   **优化目标**：利用信息论原则，最大化哈希码与侧信息（如标签、元数据）之间的互信息，同时保证比特之间的非冗余性。\n    *   **融合**：将语义嵌入、LLM 驱动的谓词和监督信号整合进哈希函数中。\n\n#### 5. 最终框架：JAF (Judge Agent Forest)\n*   **综合**：将上述思想整合。\n    *   **核心机制**：联合推理。评判者每次看一个焦点实例 + 一组通过 LSH 选出的相关邻居。\n    *   **结构支撑**：通过重叠的邻居构建隐式知识图谱，实现信息流动和迭代自我修正。\n    *   **技术底座**：利用学习型 LSH 高效、可解释地构建多样化的邻居集合，支持测试时计算的优化（如探索不同的思维链路径）。\n\n---\n\n**总结**：作者从“评判者过于孤立”这一痛点出发，通过引入“联合推理”的概念，将其类比为图上的信念传播和集成学习。为了解决实际应用中的上下文限制和检索精度问题，最终创造性地提出了基于信息论优化的 LSH 算法来组织邻居，从而形成了完整的 Judge Agent Forest 框架。"
                },
                {
                    "title": "MonoScale: Scaling Multi-Agent System with Monotonic Improvement",
                    "arxiv_id": "2601.23219",
                    "authors": "Shuai Shao, Yixiang Liu, Bingwei Lu, Weinan Zhang",
                    "summary": "In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 MonoScale，这是一个用于扩展多智能体系统（MAS）的更新框架。它解决的是在多智能体系统中动态增加新智能体或工具时，如何保持系统性能不崩溃（即解决路由器的冷启动问题）的方法论。这属于构建和改进 LLM 智能体系统架构的范畴，而非将智能体作为工具应用到特定领域，也非基础设施研究。 2.  **正面指标**: *   **多智能体**: 论文明确聚焦于 `Multi-Agent Systems (MAS)`，研究任务分解、委托以及路由器与专业智能体之间的交互。 *   **演化机制**: 论文提出了“单调改进”的机制，通过从交互中收集证据并提炼为“自然语言记忆”来指导未来的路由，这属于系统的迭代优化和演化。 *   **智能体能力**: 涉及 `Memory`（自然语言记忆）的使用以及智能体间的协作与任务分配。 3.  **排除标准**: 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此不在排除范围内。 4.  **综合结论**: 该论文完全符合“多智能体”和“自我演化”的研究方向，它提供了一种让多智能体系统在规模扩展时能够自我适应和保持性能提升的新框架，符合筛选要求。",
                    "summary2": "本文旨在解决多智能体系统在扩展过程中因路由器冷启动导致的性能崩溃问题。针对动态扩展的智能体池场景，我们提出了一种名为MonoScale的扩展感知更新框架，通过主动生成定制化熟悉任务并将交互证据蒸馏为可审计的自然语言记忆来指导路由。我们在GAIA和Humanity’s Last Exam基准上通过准确率验证了其有效性，实现了随智能体池增长的单调非递减性能提升。",
                    "summary_translation": "近年来，基于大语言模型的多智能体系统 (LLM-based multi-agent systems, MAS) 发展迅速，该系统利用路由器分解任务并将子任务委托给专用智能体。扩展能力的一种自然途径是通过持续集成新的功能智能体或工具接口来扩大智能体池，然而，当路由器面对新增的、异构且不可靠的智能体进行冷启动时，朴素的扩展方式可能会引发性能崩溃。我们提出了 MonoScale，这是一种具有扩展感知能力的更新框架，它主动生成少量以智能体为条件的熟悉化任务，从成功和失败的交互中提取证据，并将其提炼为可审计的自然语言记忆，以指导未来的路由决策。我们将序列增强过程形式化为上下文老虎机问题，并执行信任区域记忆更新，从而在多轮接入过程中保证单调非递减的性能。在 GAIA 和 Humanity's Last Exam 数据集上的实验表明，随着智能体池的扩大，该方法能够实现稳定的性能提升，优于朴素扩展策略和强路由器固定池基线。",
                    "inspiration_trace": "基于对论文《MonoScale: Scaling Multi-Agent System with Monotonic Improvement》的深度分析，以下是对作者产出该文章核心思考过程的系统性推演。\n\n---\n\n### 一、 宏观背景与核心矛盾\n\n**1. 宏观观察：MAS 的演进趋势**\n作者首先观察到，基于大语言模型的多智能体系统（MAS）正成为解决复杂任务的主流范式。其核心架构是“分而治之”：一个中央路由器负责将复杂任务分解并分发给具备特定技能的智能体（如代码执行、检索、规划等）。\n\n**2. 现实挑战：系统的动态性**\n在现实部署中，MAS 并非静态系统，而是持续演进的。为了扩展能力，开发者会不断向系统中集成新的功能智能体、工具接口或外部 API。这种“持续扩展”是提升系统上限的自然途径。\n\n**3. 核心矛盾：直觉与现实的背离**\n*   **直觉：** 增加更强的智能体或覆盖新技能的工具，理应提升系统的整体性能。\n*   **现实（观察）：** 作者通过实验数据（Figure 1）发现，朴素的扩展方式往往导致相反的结果——随着智能体池的扩大，整体性能不仅没有提升，反而出现停滞、下降甚至“性能崩溃”。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在 Introduction 中构建了一个层层递进的逻辑链条来引出问题：\n\n1.  **范式确立：** MAS 通过路由器协调专业智能体，实现了灵活的协作和长程任务处理。\n2.  **场景设定：** 现实中的 MAS 是动态增长的，需要不断集成新智能体。\n3.  **现象揭示：** 朴素扩展会导致性能崩溃。实验表明，随着智能体数量增加（从 3 个到 10 个），即使是强大的模型（如 DeepSeek, Gemini）作为路由器，其得分也会显著下降。\n4.  **归因分析：** 根本原因在于**路由器的冷启动**。\n    *   当新智能体加入时，路由器缺乏关于其真实能力的“落地知识”。\n    *   路由器不知道新智能体的边界、失败模式或接口限制。\n    *   如果路由器过早或激进地将任务分配给不熟悉的新智能体，会导致“误路由”，进而引发系统级联失败。\n5.  **现有方案的不足：**\n    *   **静态池研究：** 假设智能体集合固定，缺乏应对持续变化的更新机制。\n    *   **动态路由研究：** 通常针对相对稳定的模型池，忽略了 MAS 中智能体的异构性（工具、记忆、接口）和不可靠性（API 失败、噪声）。\n6.  **需求定义：** MAS 的规模化扩展迫切需要一种**自适应且保守的更新协议**，以确保在智能体池增长时，端到端性能能够稳定提升。\n\n---\n\n### 三、 研究问题\n\n基于上述逻辑，作者显式提出了本文试图解决的核心研究问题：\n\n**“在多智能体系统持续集成新智能体的动态扩展过程中，如何设计一种更新机制，使路由器能够安全地适应新成员，从而避免性能崩溃并保证系统性能的单调非递减？”**\n\n---\n\n### 四、 思想演进与方法论形成\n\n从发现问题到提出 MonoScale，作者的思考路径经历了以下四个关键阶段：\n\n#### 1. 诊断：从“盲目信任”到“经验缺失”\n*   **思考：** 为什么路由器会误判？因为它仅依赖新智能体的静态描述，缺乏实际交互经验。\n*   **洞察：** 新智能体往往是异构且不可靠的（如工具调用失败、接口脆弱）。静态描述无法反映这些“隐性缺陷”。\n*   **结论：** 必须让路由器在真正面对用户任务之前，先与新智能体进行“交互”以获取经验。\n\n#### 2. 假设：从“被动等待”到“主动试探”\n*   **思考：** 如何获取经验？不能等待真实用户流量来暴露失败（代价太高），也不能用通用的测试集（针对性不强）。\n*   **假设：** 我们可以针对新智能体的特性，**主动生成**一小批“熟悉化任务”。\n*   **逻辑：** 这些任务应该像探针一样，专门用来探测新智能体的能力边界、接口约束和常见失败模式。通过执行这些任务，路由器可以收集到“成功”和“失败”的双重证据。\n\n#### 3. 抽象：从“原始数据”到“可审计记忆”\n*   **思考：** 收集到的交互轨迹如何转化为路由器的决策能力？\n*   **抽象：** 直接微调模型成本高且不可控。作者选择使用**自然语言记忆**。\n*   **逻辑：** 将交互证据蒸馏为结构化的“路由原则”（例如：“在什么情况下调用该智能体”、“什么情况下绝对禁止调用”）。这种记忆是可审计、可回滚的，能够直接指导路由器未来的决策。\n\n#### 4. 保证：从“激进更新”到“保守改进”\n*   **思考：** 即使有了新经验，如何保证更新后的路由器一定不会比原来更差？（即如何避免“好心办坏事”？）\n*   **理论映射：** 作者将这一过程形式化为**上下文老虎机**问题。\n*   **核心机制：** 引入**信任域**约束。\n    *   **保守回退：** 始终保留一个“不使用新智能体”的选项作为保底。\n    *   **策略限制：** 只有当新的记忆更新带来的行为改变幅度在安全范围内（KL 散度约束），且能提升性能时，才采纳更新。\n*   **最终目标：** 实现理论上的**单调非递减性能保证**。\n\n---\n\n### 五、 总结\n\n作者的思想链条可以概括为：\n**观察现象（扩展导致崩溃） $\\rightarrow$ 归因（路由器冷启动与认知缺失） $\\rightarrow$ 提出策略（主动生成任务进行试探） $\\rightarrow$ 转化知识（将交互证据蒸馏为自然语言记忆） $\\rightarrow$ 确立边界（利用信任域理论保证单调改进）。**\n\n这一过程从工程痛点出发，结合强化学习中的保守更新思想，最终形成了一套既具备理论安全性又具备工程可操作性的 MAS 扩展框架。"
                },
                {
                    "title": "ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review",
                    "arxiv_id": "2601.22638",
                    "authors": "Palash Goyal, Mihir Parmar, Yiwen Song, Hamid Palangi, Tomas Pfister, Jinsung Yoon",
                    "summary": "Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with \"surface-level\" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify deep methodological flaws because they evaluate papers in a vacuum, lacking the external context a human expert possesses. In this paper, we introduce ScholarPeer, a search-enabled multi-agent framework designed to emulate the cognitive processes of a senior researcher. ScholarPeer employs a dual-stream process of context acquisition and active verification. It dynamically constructs a domain narrative using a historian agent, identifies missing comparisons via a baseline scout, and verifies claims through a multi-aspect Q&A engine, grounding the critique in live web-scale literature. We evaluate ScholarPeer on DeepReview-13K and the results demonstrate that ScholarPeer achieves significant win-rates against state-of-the-art approaches in side-by-side evaluations and reduces the gap to human-level diversity.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建 LLM 智能体”的目标（第一步）：** 论文的核心贡献是提出了 **ScholarPeer**，这是一个**搜索增强的多智能体框架**。它不仅仅是将现有的智能体工具应用到同行评审领域，而是设计了一个新的架构，包含特定的智能体角色（如历史学家智能体、基线侦察兵）和交互流程（双流过程：语境获取和主动验证）。这属于构建和改进多智能体系统的方法论研究。 2.  **高度匹配“多智能体”方向（第二步）：** 论文明确涉及 `Multi-Agent Systems (MAS)` 范式。摘要中详细描述了不同智能体之间的分工与协作：历史学家智能体构建领域叙事，基线侦察兵识别缺失的比较，以及多方面问答引擎验证声明。这直接对应了筛选标准中的 `Collaboration` 和 `Agent Society` 等子方向。 3.  **具备智能体关键能力（第二步）：** 该框架强调了 `Tool Use`（搜索增强，利用实时网络文献）和 `Planning`（模仿资深研究人员的认知过程，进行多步推理和验证）。 4.  **不涉及排除项（第三步）：** 论文主要关注智能体的架构设计和任务执行能力，不涉及安全对齐、多模态视觉核心或图神经网络等排除领域。 综上所述，尽管该论文的应用场景是“自动同行评审”，但其核心在于提出了一种新的多智能体协作框架来解决复杂任务，属于 Agentic AI 中多智能体系统的研究范畴。",
                    "summary2": "本文旨在解决现有自动同行评审系统因缺乏外部上下文而无法进行深度评估的问题。针对学术论文评审场景，我们提出了一种名为ScholarPeer的上下文感知多智能体框架。该框架利用Historian agent构建领域叙事，通过Baseline scout agent识别缺失基线，并借助Multi-aspect Q&A engine进行主动验证。我们在DeepReview-13K数据集上通过Side-by-side win rate、H-Max score和Review Diversity Score验证了其有效性，结果显示其显著优于现有SOTA方法。",
                    "summary_translation": "自动化同行评审已从简单的文本分类演变为结构化反馈生成。然而，当前最先进的系统仍在“表层”批评方面面临挑战：它们擅长总结内容，但往往无法准确评估新颖性和重要性，或识别深层的方法论缺陷，因为它们在真空中评估论文，缺乏人类专家所拥有的外部语境。在本文中，我们介绍了 ScholarPeer，这是一个支持搜索的多智能体框架，旨在模拟资深研究人员的认知过程。ScholarPeer 采用上下文获取和主动验证的双流过程。它利用历史学家智能体动态构建领域叙事，通过基线侦察兵识别缺失的对比，并通过多方面问答引擎验证声明，从而将批评基于实时的网络规模文献。我们在 DeepReview-13K 数据集上对 ScholarPeer 进行了评估，结果表明，在并列评估中，ScholarPeer 对抗最先进方法取得了显著的胜率，并缩小了与人类水平多样性之间的差距。",
                    "inspiration_trace": "基于对论文《ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示现有技术的根本缺陷：\n\n1.  **宏观背景（危机爆发）：** AI研究的民主化导致科学论文产出呈爆炸式增长，顶级会议收稿量激增。这引发了同行评审的“可扩展性危机”，导致审稿人疲劳、评审质量方差大以及合格专家短缺。\n2.  **现有尝试（技术引入）：** 社区自然转向利用大语言模型（LLMs）来自动化评审。早期方法从简单的文本分类发展到基于静态数据集的监督微调（SFT）。\n3.  **核心冲突（痛点揭示）：** 尽管这些系统在语言流畅性上表现出色，但它们是在“参数化真空中”评估论文。它们缺乏人类专家所拥有的外部语境——即对既往工作、并发研究及既定方法论的动态心理图谱。\n4.  **现有局限（批判分析）：** 即便是最近的框架（如DeepReviewer, ReviewRL）虽然引入了检索，但主要侧重于优化生成策略，而非进行对抗性审计或历史情境化。\n5.  **解决思路（范式转移）：** 为了解决这一问题，必须将评审从单纯的“文本生成任务”转变为“动态研究过程”。我们需要模拟资深研究者的认知工作流，通过主动验证和动态语境构建来填补真空。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何通过模拟资深研究者的动态认知工作流，构建一个具备主动验证能力的多智能体框架，以克服现有自动化评审系统在‘参数化真空中’评估论文的局限性，从而实现对论文新颖性和重要性的深度、准确评判？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者从观察到最终提出ScholarPeer框架的思考过程经历了以下四个关键阶段：\n\n#### 1. 问题诊断：从“静态知识”到“动态真空”\n*   **观察：** 现有的SOTA模型（如CycleReviewer, DeepReviewer）虽然在语法上完美，但经常产生“表面级”的批评。它们擅长总结，却无法准确评估新颖性或发现深层的方法论缺陷。\n*   **归因：** 作者意识到根本原因在于**“参数化真空”**。人类专家评审时，脑海中有一个动态的文献图谱；而LLM依赖的是冻结的权重，无法获取论文发表后的最新进展或特定子领域的隐性知识。\n*   **假设：** 如果能让模型在推理时动态获取并构建外部语境，评审质量将从“表面总结”跃升至“深度评估”。\n\n#### 2. 认知建模：解构“资深研究者”的思维\n*   **思考：** 要模拟人类专家，首先要拆解专家在评审时的具体行为模式。专家不仅仅是“读”和“写”，他们实际上在执行三个核心动作：\n    *   **历史定位：** 将论文置于领域发展脉络中，判断其是增量改进还是范式转移。\n    *   **对抗审计：** 怀疑作者是否隐瞒了关键对比，主动寻找缺失的基线。\n    *   **主动质询：** 不被动接受信息，而是提出尖锐问题并验证答案。\n*   **映射：** 这三个动作直接对应了后续的三个核心智能体。\n\n#### 3. 架构设计：双流处理与多智能体分工\n*   **挑战：** 单一LLM无法同时处理长文本理解、复杂文献检索和逻辑验证，容易产生“迷失在中间”的现象或认知过载。\n*   **策略：** 采用**“内部压缩”与“外部压缩”**并行的双流架构。\n    *   **内部流（理解论文）：** 使用Summary Agent将长文本压缩为结构化表示（核心主张、方法、证据），降低下游推理负荷。\n    *   **外部流（构建语境）：**\n        *   *Historian Agent*：不满足于检索列表，而是将检索到的文献压缩为“领域叙事”，理解思想演进的弧线。\n        *   *Baseline Scout Agent*：扮演“唱反调者”，专门搜索作者故意忽略的SOTA方法和数据集。\n*   **验证机制：** 引入**Multi-Aspect Q&A Engine**。它扮演“怀疑论者”，基于上述语境生成探测性问题，并自我回答、自我验证，记录差异。这确保了批评是基于事实的，而非泛泛而谈。\n\n#### 4. 评估哲学：从“相似度”到“价值增益”\n*   **反思：** 传统的NLP指标（如BLEU）无法衡量评审的科学价值。即使LLM生成的评审通顺，也可能毫无用处。\n*   **创新：** 作者提出新的评估视角。\n    *   **H-Max Score：** 不看平均分，而是看AI评审是否达到了人类专家评审中的“最高水平”。如果AI指出了人类都遗漏的关键缺陷，那就是超越人类。\n    *   **Review Diversity Score：** 防止AI变成“人工蜂群思维”，确保评审视角的多样性。\n\n---\n\n### 总结\n\n作者的思考路径是从**“评审危机”**这一宏观现实出发，敏锐地捕捉到现有AI评审**“缺乏语境”**的根本缺陷。通过**解构人类专家的认知过程**，作者没有选择单纯扩大模型参数，而是设计了一套**分工明确的多智能体系统**，将“检索”升级为“叙事构建”，将“阅读”升级为“主动质询”，最终实现了从静态文本生成到动态科学分析的范式跨越。"
                },
                {
                    "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
                    "arxiv_id": "2601.22628",
                    "authors": "Chengyi Yang, Zhishang Xiang, Yunbo Tang, Zongpei Teng, Chengsong Huang, Fei Long, Yuhan Liu, Jinsong Su",
                    "summary": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献属于“自我演化”方向**： 论文标题明确包含“Self-Evolving”，摘要中提出了 TTCS（Test-Time Curriculum Synthesis），这是一个“协同演化测试时训练框架”。其核心机制是通过两个策略（问题合成器和推理求解器）的迭代优化和相互反馈来实现模型的自我完善。这直接对应您研究焦点中的 **“自我演化”**，特别是涉及 `Iterative Improvement`（迭代改进）和 `Generational Evolution`（代际演化）的概念。 2.  **符合“自我演化”的筛选规则**： 根据您的第四步特殊规则，虽然论文主要在数学和通用领域任务上验证效果，但其核心贡献在于提出了一种新的“自我演化”机制（即通过合成课程和协同演化来适应测试时任务），而非单纯的应用。因此，它符合“保留”的标准。 3.  **避开了“非Agentic的推理”排除项**： 尽管论文旨在提高推理能力，但它不是简单地通过增加数据集或微调来提升基础Token预测能力。相反，它构建了一个包含两个交互策略的框架，利用 `Self-Consistency rewards`（自一致性奖励）和反馈循环来驱动演化。这种结构化的框架和演化机制符合您对“自我演化框架”的要求，因此不属于第一步中排除的“非Agentic的推理”。 4.  **符合正面指标**： 论文涉及的核心范式包括 `Self-Evolving`、`Evolutionary Algorithms`（协同演化），以及智能体能力中的 `Self-Correction`（通过反馈更新）。 综上所述，该论文提出了一种新颖的LLM自我演化框架，属于Agentic AI中自我演化的前沿研究，符合筛选条件。",
                    "summary2": "本文旨在解决Test-Time Training在复杂推理任务中因伪标签不可靠和缺乏可学习样本导致的优化瓶颈。针对测试时无标签场景，我们提出了一种TTCS协同进化框架，通过能力感知的Synthesizer生成课程变体，引导Solver进行自我进化。我们在AIME24/25、MATH-500等数学基准及MMLU-Pro等通用任务上，通过Mean@32和Pass@1等指标验证了其有效性。",
                    "summary_translation": "测试时训练提供了一种通过仅利用测试问题来调整模型，从而提升大型语言模型推理能力的有效途径。然而，现有方法在处理复杂推理问题时面临两大挑战：原始测试问题往往难度过高，难以生成高质量的伪标签；且测试集规模有限，导致持续的在线更新容易陷入不稳定状态。为解决上述局限性，我们提出了TTCS，一种协同进化的测试时训练框架。具体而言，TTCS基于同一个预训练模型初始化两个策略：问题合成器和推理求解器。这两个策略通过迭代优化进行协同演化：合成器基于测试问题生成难度递增的问题变体，构建出贴合求解器当前能力的结构化课程；与此同时，求解器利用基于原始测试问题和合成问题的多个采样响应计算出的自一致性奖励来更新自身参数。关键在于，求解器的反馈引导合成器生成与模型当前能力相匹配的问题，而生成的变体问题反过来又稳定了求解器的测试时训练过程。实验结果表明，TTCS在具有挑战性的数学基准测试中持续增强了推理能力，并能迁移至不同LLM骨干网络的通用领域任务，这凸显了一条通过动态构建测试时课程以实现模型自我进化的可扩展路径。我们的代码和实现细节可在 https://github.com/XMUDeepLIT/TTCS 获取。",
                    "inspiration_trace": "基于对论文《TTCS: Test-Time Curriculum Synthesis for Self-Evolving》的深入分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观背景与观察\n**起点：** 大语言模型（LLM）正从被动生成转向具备自主规划与推理能力的智能体。\n**现有范式：** 强化学习（RLVR）虽然有效，但严重依赖昂贵的人工标注。为了实现“自我进化”，学术界提出了**测试时训练**，特别是测试时强化学习（TTRL）。\n**核心机制：** TTRL 允许模型在推理阶段仅利用无标签的测试问题，通过“多数投票”生成伪标签来进行自我更新。\n**初步观察：** 虽然这一范式在分布偏移问题上有效，但在面对**极具挑战性的复杂推理任务**（如数学竞赛 AIME）时，性能遭遇瓶颈。\n\n---\n\n### 2. 问题定义：Introduction 的“讲故事”逻辑\n作者通过层层递进的逻辑，揭示了现有 TTRL 方法在困难任务上失效的根本原因：\n\n*   **现象：** 当 TTRL 应用于 AIME24 等高难度测试集时，模型无法有效进化，甚至性能下降。\n*   **归因一：伪标签不可靠。**\n    *   对于困难问题，模型生成的绝大多数回答都是错误的。\n    *   “多数投票”机制在这种情况下会收敛到一个**错误的共识**。\n    *   结果：模型不仅没有学到正确的推理路径，反而被强化了错误的逻辑（系统性的噪声误导）。\n*   **归因二：缺乏可学习的样本。**\n    *   测试集通常规模很小，且包含的问题难度极高（远超模型当前能力边界）。\n    *   现有方法直接在这些“终极 Boss”级别的问题上进行优化。\n    *   结果：由于缺乏中间难度的过渡样本，学习曲线过于陡峭，模型无法跨越能力鸿沟，导致优化过程不稳定或无效。\n\n**显式总结的研究问题：**\n> **“在缺乏人工标注且测试问题极其困难的情况下，如何克服伪标签噪声和样本难度断层，实现大模型在测试时的有效自我进化？”**\n\n---\n\n### 3. 逻辑演进：从假设到方法论\n为了解决上述问题，作者的思考路径经历了以下关键转折：\n\n#### 第一阶段：引入“课程学习”的直觉\n*   **思考：** 既然直接攻克难题不可行，人类是如何学习复杂技能的？\n*   **假设：** 依据课程学习理论，解决相关但更简单的变体是掌握复杂问题的桥梁。\n*   **推论：** 我们不应直接优化原始测试问题，而应主动构建一个以问题为中心的课程，包含多样化的、可解的变体，使其匹配模型当前的能力水平。\n\n#### 第二阶段：构建“合成-求解”的双智能体框架\n*   **思考：** 谁来生成这些“中间难度”的题目？谁来解？\n*   **设计：** 初始化两个策略（均来自同一预训练模型）：\n    1.  **合成器：** 负责生成题目变体。\n    2.  **求解器：** 负责解题并自我进化。\n*   **关键约束：** 合成器必须保留原题的底层推理结构，仅改变表面形式（如数值、场景），以确保训练信号的相关性。\n\n#### 第三阶段：定义“能力对齐”的反馈机制\n*   **思考：** 合成器如何知道生成的题目难度是否合适？既不能太简单（学不到东西），也不能太难（全是噪声）。\n*   **洞察：** 求解器的不确定性是最佳指标。当求解器对某个题目的正确率在 50% 左右（即方差最大）时，该题目处于其“能力边界”，信息量最大。\n*   **机制：** 利用求解器的**自一致性分数**作为合成器的奖励信号。合成器被训练去生成那些能让求解器“纠结”的题目。\n\n#### 第四阶段：实现“协同进化”的闭环\n*   **思考：** 求解器变强后，原来的题目可能变简单了，如何持续进化？\n*   **闭环设计：**\n    *   求解器利用合成题目和原始测试题的混合数据进行自我更新（GRPO）。\n    *   求解器能力的提升反过来改变了合成器的奖励分布，迫使合成器生成更难的题目。\n    *   两者在迭代中相互促进，形成动态的课程。\n\n---\n\n### 4. 最终方法论：TTCS\n基于上述逻辑，作者最终提出了 **TTCS (Test-Time Curriculum Synthesis)** 框架：\n*   **核心：** 一个协同进化的测试时训练框架。\n*   **手段：** 通过迭代 GRPO 优化，合成器动态构建与求解器能力边界对齐的课程，将原本不可靠的测试时反馈转化为可靠的自我进化路径。\n*   **结果：** 解决了伪标签噪声问题（通过合成可解样本获得可靠监督）和样本断层问题（通过课程平滑难度曲线）。"
                },
                {
                    "title": "MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment",
                    "arxiv_id": "2601.22361",
                    "authors": "Yupeng Cao, Chengyang He, Yangyang Yu, Ping Wang, K. P. Subbalakshmi",
                    "summary": "Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **MERMAID**，这是一个**记忆增强的多智能体框架** (memory-enhanced multi-agent veracity assessment framework)。 *   它明确属于 **Multi-Agent Systems (MAS)** 范畴，标题和摘要中多次提及 \"Multi-Agent\"。 *   它涉及 **Agentic AI** 的关键能力：**Memory** (persistent memory module, evidence memory)、**Tool Use** (agent-driven search) 和 **Reasoning/Planning** (Reason-Action style iterative process)。 2.  **方法论创新而非单纯应用 (第一步 & 第四步)**: *   虽然论文的应用领域是 \"Veracity Assessment\" (真实性评估/事实核查)，但这属于“非演化型应用”的例外情况。 *   论文不仅仅是将现有的智能体（如 AutoGPT 或 ReAct）作为工具直接应用到该领域，而是**构建了一个新的智能体架构**来解决现有方法中“检索被视为静态孤立步骤”以及“证据无法复用”的问题。 *   其核心在于提出了“紧密耦合检索与推理”、“持久记忆模块”以及“跨声明证据复用”的机制，这属于对 LLM 智能体能力的改进和构建。 3.  **不涉及排除标准 (第三步)**: *   论文主要关注智能体的架构设计（记忆、检索、推理的协同），而非 Safety、Alignment 或多模态视觉技术。 *   虽然摘要中提到了 \"Structured knowledge representations\"（结构化知识表示），但这作为智能体内部记忆和知识落地的一部分，并非论文主要研究的图神经网络（GNN）或知识图谱构建技术，因此不触犯“图”相关的排除规则。 综上所述，该论文通过构建多智能体框架并引入记忆机制来增强 LLM 的推理和工具使用能力，完全符合“构建、改进 LLM 智能体”的研究目标。",
                    "summary2": "本文旨在解决现有真伪评估中检索与推理脱节及证据无法跨声明重用的问题。针对在线内容验证场景，我们提出了一种名为MERMAID的记忆增强多智能体框架，通过ReAct循环与持久记忆模块实现动态证据获取与复用。在FacTool-QA、HoVer等五个基准数据集上，通过Macro-F1和搜索工具调用次数验证了其有效性，实现了SOTA性能并显著提升了搜索效率。",
                    "summary_translation": "评估在线内容的 veracity (真实性) 已变得愈发关键。Large language models (LLMs, 大语言模型) 近期在 automated veracity assessment (自动化真实性评估) 方面推动了显著进展，包括 automated fact-checking (自动化事实核查) 和 claim verification (声明验证) 系统。典型的 veracity assessment pipelines (真实性评估流程) 将复杂 claims (声明) 分解为 sub-claims (子声明)，检索外部 evidence (证据)，随后利用 LLMs 进行 reasoning (推理) 以评估 veracity。然而，现有方法通常将 evidence retrieval 视为一个静态、孤立的步骤，未能有效地管理或跨 claims 复用检索到的 evidence。在这项工作中，我们提出了 MERMAID，一个 memory-enhanced (记忆增强) 的 multi-agent (多智能体) veracity assessment framework (真实性评估框架)，它紧密耦合了 retrieval 和 reasoning 过程。MERMAID 在 Reason-Action (推理-行动) 风格的 iterative process (迭代过程) 中集成了 agent-driven search (智能体驱动的搜索)、structured knowledge representations (结构化知识表示) 和 persistent memory module (持久化记忆模块)，实现了 dynamic evidence acquisition (动态证据获取) 和跨 claims 的 evidence reuse (证据复用)。通过将检索到的 evidence 保留在 evidence memory (证据记忆) 中，该 framework 减少了 redundant searches (冗余搜索)，并提高了 verification efficiency (验证效率) 和 consistency (一致性)。我们在三个 fact-checking benchmarks (事实核查基准) 和两个 claim verification datasets (声明验证数据集) 上，利用包括 GPT、LLaMA 和 Qwen 系列在内的多个 LLMs 对 MERMAID 进行了评估。Experimental results (实验结果) 表明，MERMAID 在提高 search efficiency (搜索效率) 的同时实现了最先进的 performance (性能)，证明了 synergizing retrieval, reasoning, and memory (协同检索、推理和记忆) 对于实现 reliable veracity assessment (可靠真实性评估) 的有效性。",
                    "inspiration_trace": "基于对论文《MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的完整思考过程。\n\n---\n\n### 第一阶段：宏观观察与背景定位\n**思考起点：** 在信息过载时代，真伪评估至关重要，但人工核查极其耗时费力。\n**现状分析：** 大语言模型（LLMs）的出现推动了自动化核查的发展。目前的SOTA（State-of-the-Art）范式通常遵循一个三段式流水线：\n1.  **分解：** 将复杂声明拆解为子声明。\n2.  **检索：** 获取外部证据。\n3.  **推理：** 基于证据进行真伪判断。\n\n### 第二阶段：问题识别与“故事”逻辑（Introduction 深度解析）\n作者在引言中通过“现状-冲突-缺口”的逻辑链条引入了核心问题：\n\n1.  **现状：** 现有的基于LLM的Agent框架（如FIRE, Local等）虽然引入了多轮搜索和推理，看似解决了复杂问题，但作者敏锐地观察到其底层机制的局限性。\n2.  **冲突：**\n    *   **冲突一（检索与推理的割裂）：** 现有方法将“信息检索”视为一个静态、孤立的步骤。Agent无法根据推理过程中的中间结果自适应地调整搜索策略。这导致如果初始检索不精准，后续推理就会建立在错误的基础上，且无法通过迭代优化来修正。\n    *   **冲突二（缺乏记忆与复用）：** 现有系统对每个声明都进行“孤立搜索”。即使多个声明共享底层知识或事实背景（例如都关于“Jack Dorsey”），系统也会重复检索相同的信息。这忽略了逻辑依赖性和共享证据，与人类核查员“先回忆旧知，不足再查”的认知模式背道而驰。\n3.  **缺口：** 现有框架未能充分利用LLM Agent类人的认知能力（如结构化迭代推理）和跨声明的知识复用潜力，导致效率低下且性能受限。\n\n### 第三阶段：明确研究问题\n基于上述冲突，作者显式地提出了本文旨在解决的核心问题：\n\n**“如何构建一个真伪评估框架，能够将检索过程与推理过程紧密耦合，并引入持久化记忆机制，以实现跨声明的证据复用和自适应的动态证据获取？”**\n\n### 第四阶段：假设提出与方案构思\n为了解决上述问题，作者提出了以下核心假设与设计思路：\n\n1.  **假设一：** 如果将检索步骤嵌入到推理循环中，让Agent能够“边思考边搜索”，就能根据当前推理状态动态优化查询，获得更精准的证据。\n2.  **假设二：** 如果引入一个长期记忆模块来存储已检索的证据，Agent就能像人类一样利用“旧知识”，避免重复劳动，提高验证的一致性和效率。\n3.  **假设三：** 如果将声明预先分解为结构化知识（如三元组），不仅能指导检索，还能作为记忆模块的索引，提高证据召回的准确性。\n\n### 第五阶段：方法论设计与逻辑演进\n基于假设，作者构建了 **MERMAID** 框架，其设计逻辑遵循以下步骤：\n\n1.  **架构设计：多智能体分工**\n    *   *思考：* 单一Agent难以同时兼顾复杂的结构化分析和动态的执行控制。\n    *   *决策：* 采用多智能体架构。\n        *   **Decomposer Agent（分解者）：** 负责将原始声明转化为结构化知识（三元组+主题），作为后续检索和记忆索引的“蓝图”。\n        *   **Executor Agent（执行者）：** 负责具体的验证过程。\n\n2.  **核心机制：检索与推理的紧耦合**\n    *   *思考：* 如何解决“静态检索”问题？\n    *   *决策：* 在Executor中采用 **ReAct（Reasoning + Acting）** 范式。建立一个“思考 -> 行动（检索） -> 观察”的迭代循环。Agent不是一次性搜完，而是每推理一步，根据需要决定是否搜索、搜什么，从而实现动态证据获取。\n\n3.  **核心机制：增强型记忆模块**\n    *   *思考：* 如何解决“孤立搜索”和“重复劳动”问题？\n    *   *决策：* 引入 **Long-Term Evidence Memory（长期证据记忆）**。\n        *   **Recall（召回）：** 在开始验证前，先检查记忆中是否有相关实体（来自Decomposer的输出）的证据，如果有，直接注入上下文。\n        *   **Update（更新）：** 验证结束后，将新检索到的有效证据存入记忆。\n    *   *逻辑闭环：* 这样，随着处理声明的增加，系统的知识库不断累积，后续相关声明的验证效率会越来越高。\n\n4.  **工具集标准化**\n    *   *思考：* 为了保证Agent能稳定调用各种外部资源（维基百科、谷歌搜索等）。\n    *   *决策：* 采用 **Model Context Protocol (MCP)** 来标准化工具接口，降低调用失败率，使推理策略与底层工具实现解耦。\n\n### 总结：作者的思想演进图谱\n1.  **观察：** 现有LLM核查流水线虽然有效，但检索僵化、且无法跨声明复用知识。\n2.  **痛点：** 效率低（重复搜索）、性能受限（无法动态优化检索策略）、不像人类（缺乏记忆）。\n3.  **洞察：** 真正的智能核查需要“思考引导搜索”和“经验积累”。\n4.  **方案：** 构建一个具备“结构化分解能力”、“ReAct迭代推理能力”和“长期记忆能力”的多智能体系统。\n5.  **产出：** MERMAID框架 —— 实现了检索与推理的协同，以及跨声明的证据复用。"
                },
                {
                    "title": "Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents",
                    "arxiv_id": "2601.22352",
                    "authors": "Sri Vatsa Vuddanti, Satwik Kumar Chittiprolu",
                    "summary": "Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中关于“自我反思”与“工具使用”的深入研究。 1.  **核心贡献符合要求**: 论文的核心贡献是提出了一种预测理论，用于形式化和量化工具增强智能体在执行失败后的“自我恢复”能力。它引入了“期望恢复后悔 (ERR)”这一指标，并推导了ERR与效率分数 (ES) 之间的定量关系。这属于对LLM智能体核心能力（自我修正/自我恢复）的理论构建和改进，而非简单的应用。 2.  **符合正面指标**: *   **核心范式**: 论文明确研究 `Tool-Augmented Agents`（工具增强智能体）。 *   **智能体能力**: 论文的核心焦点是智能体在工具调用失败后的 `Self-Recovery`（自我恢复），这直接对应于筛选标准中的 `Self-Correction`（自我修正）和 `Self-Reflection`（自我反思）能力。同时，它也涉及 `Tool Use`（工具使用）的鲁棒性分析。 3.  **不违反排除标准**: *   **非应用型**: 论文并非将智能体应用于生物、金融等特定领域，而是研究智能体本身的交互动力学属性。 *   **非基础设施**: 关注的是智能体的行为逻辑和理论，而非硬件或部署。 *   **非安全/对齐**: 虽然涉及鲁棒性，但其出发点是提升智能体的任务执行成功率，而非专门解决安全、对齐或幻觉问题。 综上所述，该论文深入探讨了LLM智能体在工具使用场景下的自我修正机制，为提升智能体的鲁棒性提供了理论基础，非常契合“构建、改进或演化 LLM智能体”这一核心目标。",
                    "summary2": "本文旨在解决工具增强型代理在执行失败后缺乏可预测恢复理论的问题。针对随机执行噪声下的交互场景，我们提出了一种基于 Expected Recovery Regret (ERR) 和 Efficiency Score (ES) 的定量定律，揭示了恢复动力学的可测量规律。在五个涵盖受控扰动、诊断推理和真实 API 的基准测试上，通过预测后悔值与观察后悔值的匹配度验证了其有效性。",
                    "summary_translation": "语言模型代理通常在工具调用执行失败后表现出自我恢复的能力，然而这种行为缺乏正式的解释。我们提出了一种预测理论，通过证明可恢复性遵循一条可测量的定律来填补这一空白。具体而言，我们通过 Expected Recovery Regret (ERR)（期望恢复遗憾）对可恢复性进行了形式化，该指标量化了在随机执行噪声下恢复策略偏离最优策略的程度，并推导出了 ERR 与一个经验可观测量——Efficiency Score (ES)（效率分数）之间的一阶关系。这得出了一条关于工具使用代理恢复动态的可证伪的一阶定量定律。我们在涵盖受控扰动、诊断推理和真实世界 API 的五个工具使用基准上对该定律进行了实证验证。在不同的模型规模、扰动机制和恢复视野下，ERR-ES 定律下的预测遗憾与通过 Monte Carlo rollouts（蒙特卡洛推演）测量到的观测失败后遗憾紧密吻合，误差 delta ≤ 0.05。我们的研究结果表明，可恢复性并非模型规模或架构的副产物，而是交互动态的一种受规律支配的属性，从而为语言代理的执行级鲁棒性提供了理论基础。",
                    "inspiration_trace": "基于对论文《Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的逻辑，将读者的视角从传统的模型鲁棒性引导至“执行级恢复能力”这一核心议题：\n\n1.  **范式转移**：首先指出语言模型（LM）的角色已从静态预测器转变为自主控制器，通过API、数据库和多步工作流与环境交互。\n2.  **重新定义鲁棒性**：在这种交互式设置下，鲁棒性不再仅仅是输入的属性，而是**执行过程本身**的属性。错误（如API响应畸形、瞬态故障、模式漂移）会在时间轴上级联传播，而非仅仅存在于固定特征空间中。\n3.  **揭示理论缺口**：尽管错误具有内在的时间结构，但主流的鲁棒性范式（分布鲁棒性和对抗鲁棒性）仍将扰动视为**静态的、预输入事件**。这导致执行和恢复的动力学在理论上仍然是未被建模的空白。\n4.  **现象与矛盾**：在实践中，智能体经常表现出从执行错误中自我恢复的能力，但现有理论无法预测**何时**恢复成功、**何时**失败，以及错误如何在长视界中累积。\n5.  **提出解决思路**：因此，需要一个新的理论框架来形式化“可恢复性”，将其从一种模糊的涌现能力转化为可测量的、受规律支配的属性。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“工具增强智能体的可恢复性是否遵循一个可测量的定量规律，使得我们可以通过可观测的执行指标（如效率）来预测其在随机执行噪声下的动态恢复性能，而非仅仅依赖于模型规模或架构？”**\n\n---\n\n### 三、 思想演进脉络（从观察到方法论）\n\n作者产出该方法的思考过程可以还原为以下五个阶段：\n\n#### 1. 宏观观察：从“静态防御”到“动态生存”\n*   **思考起点**：现有的鲁棒性研究（如对抗攻击）都在研究如何保护输入不被篡改。但在工具使用场景下，输入没问题，执行过程（如调用API）也会出问题。\n*   **洞察**：我们需要关注的不是“如何避免错误”，而是“错误发生后，智能体能否以及在多大程度上能自我修正”。这是一个**时间维度**上的问题，而非空间维度。\n\n#### 2. 问题聚焦：定义“可恢复性”的量化标准\n*   **思考**：要研究一个现象，首先必须量化它。目前大家只看最终的成功率，但这无法区分“一次成功”和“失败后修正成功”的区别。\n*   **抽象**：引入控制论中的“遗憾”概念。定义**期望恢复遗憾（ERR）**：即在随机执行噪声下，当前恢复策略与理论上最优恢复策略之间的性能差距。\n*   **难点**：最优策略 $\\pi^*$ 是未知的，真实环境噪声 $F$ 也是随机的，直接计算 ERR 不可行。\n\n#### 3. 假设提出：寻找可观测的“代理指标”\n*   **思考**：既然无法直接计算 ERR，是否存在一个容易观测到的指标，能与 ERR 建立稳定的数学关系？\n*   **直觉**：恢复过程通常涉及两个权衡：**成功率**和**成本**。如果恢复得太快，可能没修好；如果修得太完美，成本可能太高。\n*   **假设**：也许存在一个结合了成功率和成本的指标（即效率分数 ES），它能通过某种线性关系预测 ERR。如果这个关系成立，那么“可恢复性”就遵循一个物理定律般的规律。\n\n#### 4. 理论推导：建立 ERR-ES 定律\n*   **形式化**：将执行过程建模为有限视界下的马尔可夫决策过程（MDP），并假设执行噪声是平稳的、成本是有界的。\n*   **数学简化**：利用一阶泰勒展开，将复杂的损失函数线性化。推导发现，在低方差和有界成本的假设下，ERR 的上界确实可以表示为效率分数 ES 的函数。\n*   **结论**：得到了一个**可证伪的定量定律**：$ERR \\approx \\frac{1}{1-\\gamma}(1 - ES)$。这意味着，只要我们测出智能体的效率（ES），就能预测它的恢复遗憾（ERR）。\n\n#### 5. 实证验证：证明“定律”的普适性\n*   **思考**：这个公式是凑出来的，还是自然界本来就存在的规律？必须通过实验来验证它是否在不同模型、不同任务、不同噪声下都成立。\n*   **验证策略**：\n    *   设计多种恢复策略（如反思、检索、批评），让它们在 ERR-ES 坐标系中分布。\n    *   观察这些策略是否都落在理论预测的曲线上（即“效率-遗憾流形”）。\n    *   测试边界条件：当噪声过大或成本无界时，定律是否会失效（以此反证定律的有效范围）。\n*   **最终产出**：通过大量实验证实了该定律的存在，从而将“可恢复性”确立为一种受动力学规律支配的属性，而非单纯的模型黑盒能力。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 8,
            "papers": [
                {
                    "title": "UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection",
                    "arxiv_id": "2601.23273",
                    "authors": "Siran Peng, Weisong Zhao, Tianyu Fu, Chenxu Zhao, Tianshuo Zhang, Haoyuan Zhang, Xiangyu Zhu, Minghui Wu, Zhen Lei",
                    "summary": "Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断 (符合)**: *   论文的核心贡献是提出了 **UPA (Unsupervised Prompt Agent)**，这是一种用于自动优化提示词的智能体框架。 *   论文将提示词优化构建为一个“顺序决策制定问题”，并利用基于树的搜索结构来导航提示词空间。这属于**构建 LLM 智能体**的方法论研究，而非将智能体作为工具应用到特定领域（如医疗、金融），因此不属于“非演化型应用”。 2.  **正面指标 (高度匹配)**: *   **Agentic AI**: 论文明确提出了 \"Prompt Agent\" 范式，并使用了 \"Tree-Based Search\"（基于树的搜索）和 \"Sequential decision-making\"（顺序决策制定），这直接对应了智能体的 **Planning**（规划）能力。 *   **自我演化**: 论文描述了通过迭代构建 \"evolving tree structure\"（演化树结构）来改进提示词的过程，这属于智能体通过机制进行自我完善和迭代的范畴。 3.  **排除标准 (未触发)**: *   论文不涉及安全、对齐、多模态或图技术等排除项。 4.  **特殊处理 (确认)**: *   根据第四步关于“推理/规划”的标准，该论文不仅仅是提高LLM的基础Token预测能力，而是设计了一个具有规划能力的智能体框架来解决优化问题，符合保留条件。 综上所述，该论文属于 Agentic AI 范畴下的智能体构建与优化研究，符合“LLM智能体及其演化”的核心研究目标。",
                    "summary2": "本文旨在实现无需监督奖励信号的Prompt Agent优化。针对完全无监督的场景，我们提出了一种基于树搜索和两阶段BTL模型选择的Unsupervised Prompt Agent (UPA)，并在GPQA、AGIEval-MATH等Benchmark上通过准确率和胜率验证了其有效性。",
                    "summary_translation": "提示词智能体最近作为一种有前景的自动提示词优化范式出现，该范式将优化过程构建为在结构化提示词空间上的序列决策问题。尽管这种表述方式使得利用高级规划算法成为可能，但这些方法通常假设能够获取监督奖励信号，而这在实际场景中往往难以获得。在这项工作中，我们提出了UPA（Unsupervised Prompt Agent，无监督提示词智能体），它能够在不依赖监督反馈的情况下实现结构化搜索与选择。具体而言，在搜索阶段，UPA迭代构建一个动态演化的树结构以遍历提示词空间，该过程由大型语言模型提供的细粒度且具有顺序不变性的成对比较结果所引导。关键在于，由于这些局部比较本质上无法产生一致的全局尺度，我们将系统性的提示词探索与最终选择进行解耦，并提出了一种基于Bradley-Terry-Luce (BTL) 模型的两阶段框架。该框架首先对局部比较进行路径级贝叶斯聚合，以在不确定性条件下筛选候选者，随后进行全局锦标赛式比较，从而推断提示词的潜在质量并确定最优提示词。多项任务的实验结果表明，UPA始终优于现有的提示词优化方法，这表明即使在完全无监督的设置下，智能体式的优化方法依然非常有效。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
                    "arxiv_id": "2601.23188",
                    "authors": "Zhongxiang Sun, Qipeng Wang, Weijie Yu, Jingxuan Yang, Haolang Lu, Jun Xu",
                    "summary": "Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心贡献符合要求 (第一步 - 核心判断)**: 论文的核心贡献是构建了一个名为 **DS-MCM (Deep Search with Meta-Cognitive Monitoring)** 的新框架。这不是将现有智能体简单应用到某个垂直领域，而是针对 LLM 智能体在长视界任务中的“推理和检索状态监控”问题，提出了一种新的架构改进。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **高度匹配核心关注点 (第二步 - 正面指标)**: *   **单智能体**: 论文专注于提升单个智能体在复杂任务中的表现，涉及 `Planning`（规划）、`Reasoning`（推理）和 `Tool Use`（检索/搜索）。 *   **自我反思与自我修正**: 论文引入的“元认知监控”本质上是一种高级的 `Self-Reflection`（自我反思）和 `Self-Correction`（自我修正）机制。它通过“快速一致性监控”和“慢速经验驱动监控”来决定何时干预以及如何纠正错误，这正是 Agentic AI 的核心能力之一。 *   **记忆**: 论文利用“历史智能体轨迹”作为经验记忆来指导后续行动，符合 `Memory` 的指标。 3.  **不涉及排除项 (第三步 - 排除标准)**: 论文的研究焦点不是安全对齐、多模态视觉处理或图神经网络，而是纯粹的智能体认知架构与控制机制。 4.  **符合特殊规则 (第四步 - 特殊情况)**: 虽然论文涉及推理，但它不是单纯提升 LLM 基础的 Token 预测或数学逻辑能力，而是构建了一个**Agentic 框架**（将监控嵌入推理-检索循环），用于管理智能体的行为和状态。此外，其利用历史经验进行迭代改进的机制，也触及了自我演化的边缘（通过经验自我完善）。 综上所述，该论文通过引入受认知神经科学启发的元认知机制，显著增强了 LLM 智能体的自主性和鲁棒性，是高质量的相关研究。",
                    "summary2": "本文旨在解决Deep Search Agent在长时程任务中缺乏推理与检索状态监控机制的问题。针对多步检索与推理交织的场景，我们提出了一种受认知神经科学启发的Deep Search with Meta-Cognitive Monitoring (DS-MCM)框架。该方法结合了Fast Consistency Monitor与Slow Experience-Driven Monitor，通过校准内外部不确定性并利用历史经验进行修正。我们在BrowseComp-Plus、BrowseComp-ZH、xbench-DeepSearch和GAIA等benchmark上通过Accuracy验证了其有效性，显著提升了模型的性能与鲁棒性。",
                    "summary_translation": "由大语言模型驱动的深度搜索代理在多步检索、推理和长视界任务执行方面表现出了强大的能力。然而，其实际应用中的失败往往源于缺乏相应的机制，以便在任务于不确定性中演进时，对推理和检索状态进行监控与调节。认知神经科学的见解表明，人类元认知具有层级组织结构，整合了快速的异常检测与选择性触发的、经验驱动的反思。在本研究中，我们提出了 Deep Search with Meta-Cognitive Monitoring (DS-MCM，深度搜索与元认知监控)，这是一种通过显式分层元认知监控机制进行增强的深度搜索框架。DS-MCM 集成了 Fast Consistency Monitor (快速一致性监控器) 和 Slow Experience-Driven Monitor (慢速经验驱动监控器)：前者对外部证据与内部推理置信度之间的一致性执行轻量级检查；后者则基于历史代理轨迹的经验记忆，被选择性地激活以指导纠正性干预。通过将监控机制直接嵌入推理-检索循环中，DS-MCM 不仅能够确定何时需要进行干预，还能决定如何利用先前的经验来指导纠正行动。在多个深度搜索基准测试和骨干模型上的实验表明，DS-MCM 能够持续提升性能和鲁棒性。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出该文章核心方法逻辑链的系统性推演，以及从Introduction中提取的“讲故事”逻辑和明确的研究问题。\n\n---\n\n### 一、 Introduction中的“讲故事”逻辑链\n\n作者在Introduction中通过层层递进的对比，构建了从生物学观察到工程学问题的叙事逻辑：\n\n1.  **生物学标杆：** 人类解决问题不仅依赖推理能力，更依赖“元认知”——即监控和调节自身推理过程的能力。这种能力使人类能在不确定环境中做出稳健决策。\n2.  **现状反差：** 现代深度搜索智能体虽然具备强大的检索和推理能力，但在实际应用中存在系统性失败。这些失败往往不是单步推理错误，而是随着任务演化、面对部分或冲突证据时，缺乏对推理和检索状态的调节能力。\n3.  **问题诊断：** 现有智能体倾向于僵化的推理轨迹，难以根据新信息调整，且将信息获取与验证视为松散阶段。这表明它们缺乏随时间推移维持和调节自身状态的机制。\n4.  **科学灵感：** 认知神经科学指出，人类元认知是**分层组织**的。包含一个快速的异常检测机制（检测冲突或预测错误）和一个慢速的反思机制（仅在需要时进行深思熟虑的纠正，且基于经验）。\n5.  **现有方案批判：**\n    *   现有的“快速”监控（如Token级不确定性）在深度搜索中不可靠，因为高熵可能仅代表多种合理的检索路径，而非错误。\n    *   现有的“慢速”监控（如独立的批判模型）缺乏历史经验，无法像人类那样基于过往模式进行反思。\n6.  **解决方案提出：** 提出DS-MCM框架，将任务级认知与元认知控制分离，结合“快速的一致性监控”和“慢速的、经验驱动的监控”，实现对深度搜索过程的显式调节。\n\n---\n\n### 二、 明确的研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何借鉴人类分层元认知机制，为深度搜索智能体构建一个既能快速检测推理与证据不一致，又能利用历史经验进行慢速反思纠正的监控系统，以解决其在长时程任务中的状态调节失效问题？”**\n\n---\n\n### 三、 核心方法产出的逻辑推演\n\n以下是从宏观问题到具体方法论的思考演进过程：\n\n#### 1. 宏观观察与痛点识别\n*   **观察：** 人类之所以能在复杂信息中保持正确，是因为我们脑子里有个“监管者”，时刻检查“我现在想的是否合理”以及“我看到的信息是否支持我的想法”。\n*   **痛点：** 现在的LLM智能体（Deep Search Agents）虽然能联网、能推理，但它们是“盲目执行”的。一旦检索到的信息很乱，或者推理走偏了，它们没有机制停下来自我纠错，只能一条道走到黑。\n\n#### 2. 对现有技术路径的反思\n*   **反思“不确定性检测”：** 传统做法认为，模型输出不确定（熵高）就是错了。但在搜索场景下，如果搜到的信息本身就很杂乱（比如关于某事件有不同报道），模型理应感到不确定。此时的高熵是正常的，不是错误。\n*   **反思“批判模型”：** 现在的Critic模型像个冷冰冰的考官，每次都从头评判，不记得以前犯过什么错，也不记得以前怎么成功的。人类纠错是靠“经验”，而不是靠每次都重新推导一遍逻辑。\n\n#### 3. 理论假设的引入\n*   **假设：** 如果我们能让智能体像人一样，建立一套**“快慢双系统”**，就能解决上述问题。\n    *   **系统1（直觉）：** 不做复杂判断，只看“内部想法”和“外部证据”是否匹配。如果不匹配（比如证据很确凿但我很犹豫，或者证据很乱但我很确信），就报警。\n    *   **系统2（理性）：** 只有系统1报警了才启动。去翻“历史经验库”，看看以前遇到这种情况是怎么解决的，然后给出具体建议。\n\n#### 4. 方法论的具体化\n*   **如何实现“系统1”（快速监控）？**\n    *   需要量化两个熵：**搜索熵（SE）**（外部证据有多乱）和 **推理熵（RE）**（内部想法有多乱）。\n    *   **核心逻辑：** 正常情况下，SE和RE应该是正相关的（证据乱，想法就乱；证据稳，想法就稳）。如果两者偏差过大（即残差 $\\epsilon$ 很大），说明状态异常，触发系统2。\n\n*   **如何实现“系统2”（慢速监控）？**\n    *   需要一个**元认知记忆库**。把历史轨迹拆解成一个个“会话”，标记哪些是成功的经验，哪些是失败的教训。\n    *   **核心逻辑：** 当系统1报警时，去记忆库里检索当前情况最相似的“成功案例”和“失败案例”。让Critic模型基于这些案例，生成具体的修正建议（$\\delta_t$），告诉下一步该怎么做。\n\n#### 5. 逻辑闭环与验证\n*   **闭环：** 通过这种设计，智能体不再是机械执行，而是具备了“自我意识”的雏形——能感知异常（快），并能基于经验修正（慢）。\n*   **验证：** 实验不仅看最终任务是否完成（准确率），还要看这种机制是否真的比通用的Critic更高效（开销更低）且建议更合理（人类评估）。\n\n---\n\n**总结：** 作者的思考路径是从**生物智能的鲁棒性**出发，诊断出**AI智能体缺乏状态调节**的核心缺陷，通过**批判性分析现有技术指标（熵）的局限性**，最终引入**认知神经科学的分层理论**，将“一致性校验”与“经验驱动反思”结合，构建出DS-MCM这一分层监控框架。"
                },
                {
                    "title": "Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards",
                    "arxiv_id": "2601.22511",
                    "authors": "Yuan-Jay Lü, Chengyu Wang, Lei Shen, Jun Huang, Tong Xu",
                    "summary": "Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.",
                    "category": "cs.CL",
                    "filter_reason": "1. **核心判断（符合）**：论文的核心贡献是提出了 SYNTHAGENT 框架，旨在**构建**和**改进**小型 Agentic Language Models。它不是将智能体作为工具应用于特定垂直领域（如生物、金融），而是专注于解决智能体训练过程中的数据瓶颈和环境稳定性问题，属于智能体构建方法论的范畴。 2. **正面指标（高度相关）**： - **Agentic AI**：标题和摘要明确指出研究对象是 \"Agentic Language Models\" 及其 \"Agentic capabilities\"。 - **工具使用与规划**：论文重点解决 \"tool-use\" 训练数据的合成问题，并通过故意未指定的指令迫使智能体主动查询用户，这直接涉及智能体的工具使用和规划能力。 - **演化机制**：论文利用 \"Simulated Environments\"（模拟环境）和 \"Rubric-Based Rewards\"（基于量表的奖励）进行强化学习训练，这属于通过环境反馈来迭代和改进智能体能力的机制，符合“改进或演化”的研究目标。 3. **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉核心研究或图技术，不属于排除范围。 4. **综合结论**：该论文直接针对 LLM 智能体的构建与能力提升（特别是工具使用和环境交互），完全符合“单智能体”及“构建、改进或演化 LLM 智能体”的研究范围。",
                    "summary2": "本文旨在解决小型LLM智能体能力不足及训练数据环境受限的问题。针对缺乏多样性和稳定性的智能体训练场景，我们提出了一种SYNTHAGENT框架，通过合成模糊任务、构建Mock环境及基于标准的奖励机制进行强化学习。在TAU-2、BFCL-V4等14个基准数据集上，通过Exact Match等指标验证了其有效性，使8B-14B模型超越了32B基线模型。",
                    "summary_translation": "小型 LLMs 往往难以匹敌大型昂贵模型的 agentic capabilities (智能体能力)。虽然 reinforcement learning (强化学习) 有所助益，但进展一直受限于两个 structural bottlenecks (结构瓶颈)：现有的 open-source agentic training data (开源智能体训练数据) 任务种类狭窄且易于解决；real-world APIs (真实世界 API) 缺乏多样性，且在大规模 reinforcement learning rollout processes (强化学习推演过程) 中表现不稳定。我们提出了 SYNTHAGENT 这一 framework (框架) 来应对这些挑战，该框架能够联合生成多样化的 tool-use training data (工具使用训练数据) 并模拟 complete environments (完整环境)。具体而言，一个强大的 teacher model (教师模型) 创建 novel tasks (新颖任务) 和 tool ecosystems (工具生态系统)，随后将其重写为故意 underspecified instructions (规格不足的指令)。这促使 agents (智能体) 主动向用户询问缺失的细节。在处理 synthetic tasks (合成任务) 时，基于 LLM 的 user simulator (用户模拟器) 提供 user-private information (用户私有信息)，而 mock tool system (模拟工具系统) 则提供稳定的 tool responses (工具响应)。在 rewards (奖励) 方面，基于 required subgoals (所需子目标)、user-agent interactions (用户-智能体交互) 和 forbidden behaviors (禁止行为) 构建了 task-level rubrics (任务级评分标准)。在涵盖数学、搜索和工具使用领域的 14 个具有挑战性的 datasets (数据集) 上，利用我们的 synthetic data (合成数据) 训练的 models (模型) 取得了 substantial gains (显著提升)，其中小型 models (模型) 的性能优于更大的 baselines (基线模型)。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
                    "arxiv_id": "2601.22491",
                    "authors": "Jinyang Wu, Changpeng Yang, Yuhao Shen, Fangzhi Xu, Bolin Ni, Chonghua Liao, Yuchen Liu, Hongzhen Wang, Shuai Nie, Shuai Zhang, Haoran Luo, Jiaming Xu",
                    "summary": "Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \\textbf{S}weet \\textbf{S}pot \\textbf{L}earning (\\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，核心理由如下： 1.  **核心贡献符合“构建/改进 LLM智能体”的目标**： 论文提出了“Sweet Spot Learning (SSL)”这一新框架，旨在解决智能体训练中奖励信号单一的问题。其核心在于改进智能体的优化过程，使其能更好地进行规划和推理。这属于“构建、改进或演化 LLM智能体”的方法论贡献，而非单纯的应用。 2.  **聚焦于 Agentic AI 的核心能力**： 摘要中明确提到了“Agentic Optimization”（智能体优化），并在实验部分涵盖了“GUI perception”（GUI感知，典型的智能体任务）、“short/long-term planning”（短期/长期规划）以及“complex reasoning”（复杂推理）。这些都是 Agentic AI 的核心关注点（规划、工具使用/环境感知、多步推理）。 3.  **不违反排除标准**： *   **非特定领域应用**：虽然论文提到了视觉感知和GUI任务，但SSL是一个通用的优化原则，并非仅针对生物、医疗等特定领域的垂直应用。 *   **非基础设施**：论文关注的是训练算法和奖励机制，而非硬件或部署。 *   **视觉/多模态**：尽管涉及“visual perception”，但它是作为智能体感知环境（如GUI界面）的一种手段，且论文核心贡献在于优化算法（SSL）而非视觉模型本身，因此符合“作为工具感知环境”的例外情况。 *   **非安全/对齐**：论文不涉及安全、对齐或幻觉问题。 4.  **符合“自我演化/改进”的广义范畴**： 论文通过强化学习（RL）和可验证奖励来指导策略优化，属于通过环境反馈进行自我完善和迭代的过程，这与“自我演化”中通过反馈进行改进的机制相契合。 综上所述，该论文提出了一种改进智能体训练和优化能力的通用框架，直接贡献于 Agentic AI 的发展，符合筛选要求。",
                    "summary2": "本文旨在解决现有RLVR方法因二元奖励忽略轨迹质量差异导致的优化模糊与低效问题。针对GUI感知、规划及复杂推理任务，我们提出了一种Sweet Spot Learning (SSL)框架，利用分层、接近度对齐的奖励引导策略进入解空间的“sweet-spot”区域。并在12个基准上通过准确率、成功率等指标验证了其有效性，实现了显著的样本效率提升。",
                    "summary_translation": "具有可验证奖励的 Reinforcement learning (强化学习) 已成为训练智能体的强大范式。然而，现有方法通常采用 binary rewards (二元奖励)，无法捕捉达到相同结果的 trajectories (轨迹) 之间的质量差异，从而忽视了 solution space (解空间) 内的潜在多样性。受到网球中 sweet spot (甜区) 概念的启发——即球拍产生最佳击球效果的核心区域，我们引入了 **Sweet Spot Learning (SSL)**，这是一个为 agent optimization (智能体优化) 提供 differentiated guidance (差异化指导) 的新颖框架。SSL 遵循一个简单而有效的原则：progressively amplified (逐渐放大的) tiered rewards (分层奖励) 引导 policies (策略) 朝向 solution space (解空间) 的 sweet-spot region (甜区)。该原则自然地适应于 diverse tasks (多样化任务)：visual perception tasks (视觉感知任务) 利用 distance-tiered modeling (距离分层建模) 来奖励 proximity (接近度)，而 complex reasoning tasks (复杂推理任务) 则奖励朝着 promising solutions (有希望的解) 的 incremental progress (增量进展)。我们从理论上证明了 SSL 保持了 optimal solution ordering (最优解排序) 并增强了 gradient signal-to-noise ratio (梯度信噪比)，从而促进了更 directed optimization (定向的优化)。在 GUI perception (图形用户界面感知)、short/long-term planning (短期/长期规划) 和 complex reasoning tasks (复杂推理任务) 上的 extensive experiments (大量实验) 表明，在 12 个 benchmarks (基准测试) 中相比 strong baselines (强基线) 取得了 consistent improvements (一致的改进)，实现了高达 2.5 倍的 sample efficiency (样本效率) 提升以及有效的 cross-task transferability (跨任务可迁移性)。我们的工作确立了 SSL 作为训练 capable and robust agents (能力强且鲁棒的智能体) 的一种 general principle (一般原则)。",
                    "inspiration_trace": "基于论文内容，以下是作者产出《SSL: Sweet Spot Learning》核心方法的逻辑链推演：\n\n### 1. 宏观背景与现状观察\n作者首先关注到智能体训练领域的一个宏观趋势：**基于可验证奖励的强化学习（RLVR）** 正成为训练具备复杂推理和规划能力的多模态智能体的主流范式。这种方法摆脱了对昂贵的人工标注推理链的依赖，直接利用自动计算的奖励信号来优化行为。\n\n### 2. 问题引入：Introduction 的“讲故事”逻辑\n作者通过“现状-冲突-后果”的逻辑链条，揭示了当前RLVR范式的核心缺陷：\n\n*   **现状：** 主流的RLVR方法普遍采用**二元奖励**机制，即仅根据最终结果将轨迹标记为“成功”或“失败”。\n*   **冲突：** 这种二元划分掩盖了达成相同结果的不同轨迹之间的**质量差异**。\n    *   *案例1（GUI导航）：* 一条轨迹用3步打开设置，另一条用了8步冗余步骤，但两者获得相同的奖励。\n    *   *案例2（迷宫导航）：* 一条路径直接，另一条绕远路，但只要到达终点，奖励无差别。\n*   **后果：** 这种粗糙、无差别的奖励机制导致了三个核心挑战：\n    1.  **优化模糊性：** 缺乏差异化指导，梯度更新缺乏方向性，无法区分哪些行为值得强化。\n    2.  **学习低效性：** 奖励信号未能揭示解的质量差异，导致智能体无法从多样化的轨迹中提取有效知识，样本利用率低。\n    3.  **策略脆弱性：** 策略可能过拟合于偶然模式（如幸运的点击），而非鲁棒的任务理解。\n\n### 3. 核心研究问题\n基于上述观察与冲突，作者提出了一个明确的探索性问题：\n\n**“我们能否设计一个统一的奖励原则，使其能够在整个解空间中提供差异化的指导？”**\n\n### 4. 思想演进与方法论形成\n为了回答上述问题，作者的思考经历了从类比、假设到具体化的演进过程：\n\n**阶段一：灵感引入与核心假设**\n*   **灵感来源：** 网球运动中的“甜区”概念——球拍的中心区域击球效果最佳。\n*   **核心假设：** 如果在智能体的解空间中也定义一个“甜区”，并给予那些更接近该区域的动作或轨迹更高的奖励，就能引导策略向最优解区域收敛。这不仅仅是区分对错，而是区分“好”与“更好”。\n\n**阶段二：方法论抽象**\n*   **原则确立：** 提出**Sweet Spot Learning (SSL)**，其核心原则是“**渐进放大、分层奖励**”。\n*   **机制设计：** 不再依赖粗糙的成功/失败二元划分，而是根据轨迹接近任务完成的程度，将其分配到不同的层级中，给予分层奖励。\n\n**阶段三：具体化与泛化**\n作者意识到不同任务的“接近程度”定义不同，因此将抽象原则具体化为两种适应不同任务类型的实例：\n*   **视觉感知/导航任务（空间维度）：** 采用**距离分层**。利用空间距离来量化接近程度（例如，点击点距离目标中心的距离），越近奖励越高。\n*   **复杂推理任务（逻辑维度）：** 采用**进度分层**。奖励与接近正确解决方案的增量进度对齐（例如，数独或迷宫中的部分正确匹配）。\n\n**阶段四：理论验证与优化**\n*   **理论支撑：** 作者从理论上证明了SSL不仅保留了最优解的排序，还通过离散化处理提高了梯度的信噪比（SNR），从而解释了为什么分层奖励比单纯的连续奖励更有效（过滤噪声，提供更稳定的信号）。\n\n### 5. 总结\n作者的思考路径是从**发现二元奖励的“粗糙性”**出发，通过**“甜区”类比**提出**分层差异化指导**的假设，最终构建出一个能够适应**空间距离**和**逻辑进度**两种不同任务特性的统一奖励框架。这一过程体现了从“定性区分（对/错）”向“定量分级（优/良/差）”的思维跃迁。"
                },
                {
                    "title": "Large Language Model Agents Are Not Always Faithful Self-Evolvers",
                    "arxiv_id": "2601.22436",
                    "authors": "Weixiang Zhao, Yingshuo Wang, Yichen Zhang, Yang Deng, Yanyan Zhao, Wanxiang Che, Bing Qin, Ting Liu",
                    "summary": "Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”方向的核心研究。 1.  **核心判断**: *   论文的研究对象是“自我演化LLM智能体”，直接对应您研究焦点中的“自我演化”方向。 *   虽然这篇论文主要是一篇分析性论文，而非提出一个新的架构，但其核心贡献在于深入剖析了自我演化智能体中的一个关键机制——即智能体如何利用过往经验进行迭代。它揭示了现有自我演化框架中存在的“经验忠实度”问题（即智能体往往忽略压缩后的经验），这对改进和演化LLM智能体具有重要的指导意义。 2.  **正面指标匹配**: *   **核心范式**: 论文明确涉及 `Self-Evolving` 和 `LLM-based Agents`。 *   **演化机制**: 研究了 `Experience Integration`（经验整合）、`Generational Evolution`（代际演化，隐含在积累和重用经验中）以及 `Iterative Improvement`（迭代改进）的有效性。 *   **智能体能力**: 深入探讨了 `Memory`（特别是经验记忆）在智能体决策中的作用。 3.  **排除标准检查**: *   **非应用**: 论文虽然使用了9个环境进行测试，但其目的是为了评估智能体本身的演化机制，而非解决某个特定领域（如医疗、金融）的应用问题。 *   **非安全/对齐**: 尽管标题中提到了“Faithful”（忠实），但在摘要中，其定义是“决策对所获经验的因果依赖性”，属于智能体功能机制和记忆利用的范畴，而非通常意义上的AI安全、伦理对齐或防止幻觉。 *   **非基础设施**: 不涉及硬件或部署优化。 4.  **综合结论**: 该论文对自我演化智能体的基础假设提出了挑战，指出了当前方法在利用经验进行自我完善方面的缺陷。这种对演化机制的深刻理解和批判性分析，正是构建更强大的自我演化智能体所必需的。因此，它符合您关于“改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在探究自进化LLM智能体是否忠实地依赖其积累的经验。针对4种代表性框架和10种LLM骨干模型，我们提出了一种系统的因果干预方法来评估经验忠实度，并在HotpotQA、ALFWorld等9个环境上通过性能变化和Integrated Gradients指标验证了智能体对原始经验的强依赖及对浓缩经验的忽视。",
                    "summary_translation": "自我演进的大语言模型 智能体通过积累和复用过往经验不断改进，但目前尚不清楚它们是否忠实地依赖这些经验来指导其行为。我们首次对自我演进 LLM 智能体中的经验忠实度进行了系统性研究，即智能体的决策对其所获经验的因果依赖。通过对原始和浓缩形式的经验进行受控的因果干预，我们在 10 个 LLM 骨干网络和 9 个环境中全面评估了四个代表性框架。我们的分析揭示了一个显著的不对称性：虽然智能体始终依赖原始经验，但它们经常忽视或误读浓缩经验，即使这是唯一提供的经验。这种差距在单智能体和多智能体配置以及不同骨干网络规模中均持续存在。我们将其根本原因归结为三个因素：浓缩内容的语义局限性、抑制经验的内部处理偏差，以及预训练先验已经足够的任务场景。这些发现挑战了关于自我演进方法的主流假设，并强调了对更忠实和可靠的经验整合方法的需求。",
                    "inspiration_trace": "基于论文《Large Language Model Agents Are Not Always Faithful Self-Evolvers》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程。\n\n---\n\n### 一、 宏观问题与引入逻辑\n\n作者在Introduction中构建了一个从“技术趋势”到“核心盲点”的叙事逻辑，具体推演如下：\n\n1.  **宏观观察（技术趋势）：**\n    *   **现象：** 自进化智能体成为实现自主系统持续学习和适应的关键步骤。\n    *   **机制：** 与传统静态范式不同，这些智能体通过动态收集、存储和重用与环境交互的“经验”来指导未来决策。\n\n2.  **概念界定（经验分类）：**\n    *   为了深入理解，作者将“经验”这一核心概念拆解为两类：\n        *   **原始经验：** 具体的历史轨迹（如成功的步骤序列），智能体可以直接参考或回放。\n        *   **浓缩经验：** 从轨迹中提炼的抽象洞察（如抽象计划或失败启发式），旨在捕捉可迁移的结构。\n\n3.  **发现盲点（问题提出）：**\n    *   **现状：** 现有研究主要集中在如何存储或表示这些经验（即“记忆工程”）。\n    *   **缺失：** 尽管这些经验处于核心地位，但并不清楚智能体是否**真实且忠实地依赖**这些经验来指导行为。性能的提升可能源于模型内部先验，而非外部经验。\n\n4.  **核心概念引入：**\n    *   为了填补这一空白，作者引入了**“经验忠实度”**这一概念，定义为智能体决策行为对输入经验的因果依赖性。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式提出了核心研究问题：\n\n**“自进化智能体的性能提升，是否真实且忠实地归因于其对过往经验的使用？”**\n\n---\n\n### 三、 逻辑演进与思考过程还原\n\n从提出问题到形成方法论，作者的思考路径经历了以下四个阶段：\n\n#### 第一阶段：假设与定义（如何验证“忠实”？）\n*   **思考：** 如果智能体真的“听进去了”给它的经验，那么当我破坏这个经验时，它的行为应该发生显著变化（变差）。反之，如果我破坏了经验，它依然表现良好，说明它并没有真正依赖这个经验，而是靠猜或靠内部知识。\n*   **逻辑推演：** 忠实度不能仅通过“有经验 vs 无经验”的性能对比来衡量，必须通过**因果干预**来测试。\n*   **定义：** 如果扰动经验导致行为显著改变，则视为忠实；否则视为不忠实。\n\n#### 第二阶段：实验设计（如何进行干预？）\n*   **思考：** 经验分为“原始”和“浓缩”两类，它们的性质不同，干预手段也应不同。\n*   **针对原始经验：** 它包含具体的步骤和时序结构。\n    *   *干预策略：* 清空内容（测试是否依赖格式）、打乱顺序（测试是否依赖时序因果）、替换为无关任务（测试是否依赖语义相关性）。\n*   **针对浓缩经验：** 它是抽象的文本总结。\n    *   *干预策略：* 清空内容、破坏逻辑（如颠倒因果）、替换为无关文本、填充无意义符号（测试是否仅依赖表面形式）。\n\n#### 第三阶段：实证发现（观察到了什么不对称？）\n*   **思考：** 在广泛的框架（ExpeL, Dynamic CheatSheet等）、模型（GPT-4o, Qwen等）和环境（Web, 数学推理等）上进行测试后，结果呈现了一种惊人的不对称性。\n*   **发现：**\n    *   **对原始经验高度忠实：** 一旦打乱或移除原始轨迹，性能显著下降。说明智能体真的在“照着做”。\n    *   **对浓缩经验极度不忠：** 即使破坏、替换甚至填充乱码浓缩经验，性能往往不受影响。说明智能体往往“无视”或“误解”了这些抽象建议，哪怕这是它唯一能获得的经验。\n\n#### 第四阶段：归因分析（为什么会这样？）\n*   **思考：** 为什么智能体能模仿具体步骤，却听不进抽象建议？作者从系统的三个组件寻找原因。\n*   **原因一（经验本身）：** 浓缩经验往往语义受限，过于模糊或通用，缺乏指导行为所需的特异性，甚至可能包含错误先验误导智能体。\n*   **原因二（模型内部）：** 模型存在内部处理偏差。通过Integrated Gradients分析发现，模型在深层网络中更关注“当前轨迹”，而抑制了外部检索到的浓缩经验（即“近因效应”或局部上下文偏差）。\n*   **原因三（任务环境）：** 在某些知识密集型任务中，模型的预训练先验已经足够强，外部经验的边际效用低，导致模型缺乏动力去整合外部指导。\n\n---\n\n### 四、 总结：思想演进脉络\n\n1.  **起点：** 自进化智能体依赖“记忆”进化，但业界只关注“怎么存”，忽略了“怎么用”。\n2.  **核心洞察：** 性能提升 $\\neq$ 忠实使用。必须引入因果视角来检验“经验-行为”的因果链。\n3.  **方法论创新：** 设计针对性的“干预实验”作为探针，分别扰动原始和浓缩经验，观察行为崩塌程度。\n4.  **核心发现：** 揭示了“原始-浓缩”的不对称性——智能体是“模仿者”而非“理解者”，擅长复刻步骤，拙于应用抽象规则。\n5.  **深层机理：** 这种不忠源于浓缩内容的质量问题、模型内部的注意力偏差以及任务本身的难度特性。\n\n这一逻辑链条从对现有范式的质疑出发，通过严谨的因果实验设计，最终揭示了LLM智能体在自我进化机制中一个被忽视的根本缺陷。"
                },
                {
                    "title": "Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning",
                    "arxiv_id": "2601.22297",
                    "authors": "Chenxi Liu, Yanshuo Chen, Ruibo Chen, Tianyi Xiong, Tong Zheng, Heng Huang",
                    "summary": "The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围。以下是详细的判断依据： 1.  **核心判断（符合 Multi-Agent 方向）**： *   论文的核心贡献是提出了一种名为 \"Self-Debate Reinforcement Learning (SDRL)\" 的训练框架。 *   该框架旨在解决 \"Multi-Agent Debate (MAD)\" 这一特定场景下的问题。MAD 是典型的多智能体协作与推理范式，属于 \"Multi-Agent Systems (MAS)\" 的核心范畴。 *   论文不仅关注单一模型的推理能力，更关注如何让模型在多智能体交互（辩论）的环境中表现更好，这符合“构建、改进 LLM 智能体”的目标。 2.  **正面指标匹配**： *   **多智能体**：摘要明确提到 \"Multi-Agent Debate (MAD)\" 和 \"collaborative reasoning\"，涉及智能体间的交互与观点综合。 *   **自我演化**：论文提出的 SDRL 方法通过强化学习（Reinforcement Learning）来优化模型，使其能够从不同的推理轨迹中学习，这属于 \"Self-Improvement\" 和 \"Iterative Improvement\" 的范畴。 *   **智能体能力**：论文关注模型在复杂任务中的多步推理和基于上下文的响应生成，这与智能体的规划与推理能力密切相关。 3.  **排除标准检查**： *   **非应用型论文**：该研究不是为了解决生物、金融等特定领域的垂直问题，而是为了改进智能体在通用推理任务中的基础机制，因此不属于“非演化型应用”。 *   **非单纯的基础推理**：虽然涉及推理能力提升，但其核心在于“如何让模型适应多智能体辩论环境”，而非仅仅提出一种新的数学解题技巧或数据集。它关注的是智能体在交互中的行为优化，因此符合 Agentic AI 的定义。 *   **不涉及安全或多模态**：论文未主要关注安全对齐、视觉或多模态内容。 综上所述，该论文通过提出新的训练框架来改进 LLM 在多智能体辩论中的表现，直接贡献于多智能体系统的构建与优化，符合“Multi-Agent”和“Self-Evolving”的研究焦点。",
                    "summary2": "本文旨在解决现有RLVR方法未能显式训练LLM适应多智能体辩论（MAD）的问题。针对数学推理任务，我们提出了一种Self-Debate Reinforcement Learning (SDRL) 框架，通过联合优化初始解和辩论条件下的响应，赋予单一模型独立解题及辩论协作能力。在MATH500、AMC和AIME等基准上，通过准确率验证了SDRL在提升MAD性能的同时增强了单模型推理能力。",
                    "summary_translation": "具有可验证奖励的强化学习显著提升了大语言模型的推理能力。在测试阶段，通过多智能体辩论进行的协作推理已成为提升大语言模型性能的一种有前景的方法。然而，当前的 RLVR 方法通常训练 LLM 独立解决问题，并未明确使其准备好去综合并利用辩论过程中产生的不同推理依据。在本研究中，我们提出了自我辩论强化学习，这是一个训练框架，旨在赋予单个 LLM 强大的独立解题能力，以及从 MAD 中的多样化推理轨迹中学习的能力。给定提示词后，SDRL 首先采样多个候选解，随后构建包含多样化推理路径的辩论上下文，并基于该上下文生成第二轮回应。最后，SDRL 对初始回应和基于辩论上下文的回应进行联合优化，从而训练出一个既能作为独立解题者，又能作为辩论参与者发挥效用的模型。在多个基础模型和推理基准上的实验表明，SDRL 在提升整体 MAD 性能的同时，也增强了单模型的推理能力。",
                    "inspiration_trace": "基于对论文《Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning》的深入分析，以下是作者产出该核心方法的逻辑推演过程。\n\n---\n\n### 一、 逻辑演进脉络：从宏观观察到微观方法\n\n#### 1. 宏观观察：两条平行发展的技术路线\n*   **观察 A（训练侧）：** 大语言模型（LLM）的推理能力通过“可验证奖励的强化学习”（RLVR，如 DeepSeek-R1）得到了显著提升。这种方法的核心是利用可自动验证的结果（如数学题答案）来监督模型，使其具备强大的独立解题能力。\n*   **观察 B（推理侧）：** 在测试阶段，“多智能体辩论”（MAD）作为一种新兴的测试时扩展策略表现出了巨大潜力。即让多个智能体提出方案并相互迭代更新，往往能比单个智能体得到更好的结果。\n\n#### 2. 问题识别：训练与推理的“错位”\n*   **核心矛盾：** 尽管上述两条路线都很强，但存在严重的**训练-推理不匹配**。\n    *   现有的 RLVR 方法通常训练模型在**孤立**的环境中解决问题（单轨迹优化）。\n    *   然而，MAD 要求模型在**协作**环境中工作，即必须能够处理、综合并从不同的推理路径中学习。\n*   **后果：** 这种错位导致即使是经过 RLVR 训练的强模型，在 MAD 场景下往往无法有效整合他人的观点，限制了辩论带来的性能提升。\n*   **现有方案的缺陷：** 试图通过强化学习训练辩论行为的工作，通常需要优化**整个多智能体系统**。这极大地增加了训练成本和部署的复杂性。\n\n#### 3. 理论洞察：解构辩论的收益来源\n*   **深层追问：** 为什么辩论能提升性能？仅仅是“少数服从多数”的投票效应吗？\n*   **理论分析：** 作者基于贝叶斯信念更新框架（DCM）分析发现，标准的 MAD 在没有额外训练信号时，其信念演化是一个“鞅”（即期望收益为零）。\n*   **关键发现：** 辩论带来的真正收益来自于**“私人批判”**。即智能体在阅读同伴观点后，利用自身内部逻辑进行批判和修正的能力。如果这种“私人批判”与真实答案一致，就会打破“鞅”的平衡，产生正向漂移，从而提升整体性能。\n\n#### 4. 假设提出：单模型的双重能力\n*   **假设：** 如果我们能训练一个**单一**的 LLM，使其不仅具备强大的独立解题能力，还具备强大的“私人批判”能力（即学会如何在辩论中修正自己），那么这个模型就能在 MAD 中表现出色，同时避免了训练整个多智能体系统的高昂成本。\n\n#### 5. 方法论构建：自辩论强化学习（SDRL）\n*   **思路转化：** 如何在训练中注入“私人批判”能力？不需要真的拉来多个模型，而是让一个模型**自己跟自己辩论**。\n*   **具体设计：**\n    1.  **采样：** 对同一个问题，让模型生成多个候选解。\n    2.  **构造冲突：** 从候选解中挑选两个不同的（甚至冲突的）解，构建一个“辩论对”。\n    3.  **模拟辩论：** 将这两个解作为上下文输入给模型，强迫模型进行审议并生成最终答案。\n    4.  **联合优化：** 同时优化“初始独立解题”和“辩论后修正”两个阶段，利用可验证奖励进行强化学习。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑提取\n\n作者在 Introduction 部分通过以下逻辑链条引入了核心问题：\n\n1.  **背景铺垫：** LLM 的推理能力通过 RLVR（如 DeepSeek-R1）得到了质的飞跃，同时，测试时的多智能体辩论（MAD）被证明是进一步提升性能的有希望范式。\n2.  **揭示现状：** 然而，目前缺乏有效的方法来为 MAD 交互准备推理模型。\n3.  **指出痛点：**\n    *   **痛点一（能力缺失）：** 现有的 RLVR 方法训练模型在孤立轨迹中解决问题，**没有明确地让它们准备好去综合和利用辩论中出现的不同推理路径**。这种训练与推理的不匹配限制了 LLM 在 MAD 中的有效性。\n    *   **痛点二（成本高昂）：** 最近尝试训练辩论行为的工作通常优化**整个多智能体系统**，这显著增加了训练成本和部署复杂性。\n4.  **明确需求：** 这些局限性凸显了对一个新框架的需求，该框架需要能产出一个**单一**的 LLM，它既能作为独立的解题者，又能在 MAD 中进行高效协作。\n\n---\n\n### 三、 研究问题\n\n基于上述逻辑，本文试图回答的核心研究问题是：\n\n**“如何通过强化学习训练一个单一的语言模型，使其在保持强大独立解题能力的同时，能够习得在多智能体辩论中有效整合不同观点并进行自我修正的‘私人批判’能力，从而在不增加多智能体系统训练复杂度的前提下提升整体推理性能？”**"
                },
                {
                    "title": "Mem-T: Densifying Rewards for Long-Horizon Memory Agents",
                    "arxiv_id": "2601.23014",
                    "authors": "Yanwei Yue, Guibin Zhang, Boci Peng, Xuanbo Fan, Jiaxin Guo, Qiankun Li, Yan Zhang",
                    "summary": "Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\\sim24.45\\%$ relative to GAM without sacrificing performance.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心贡献符合**: 论文提出了 Mem-T，这是一个自主记忆智能体，旨在解决智能体如何内生地管理记忆的处理、存储和检索。这直接属于“构建、改进 LLM智能体”的范畴。 2.  **匹配研究焦点**: 论文的核心关注点是“记忆”，这正是用户在“单智能体”方向中明确列出的核心能力之一（规划、记忆、工具使用、自我反思）。 3.  **非排除项**: 论文并非将智能体作为工具应用于特定领域（如医疗、金融），而是专注于智能体本身的架构和训练优化。同时，它不涉及安全对齐、多模态视觉或图技术等排除标准。 4.  **方法论创新**: 论文提出的 MoT-GRPO 框架通过强化学习优化长视界记忆管理，属于对智能体能力的改进和演化机制的探索。",
                    "summary2": "本文旨在解决长视界记忆智能体训练中奖励稀疏及信用分配困难的问题。针对流式输入的长序列记忆操作场景，我们提出了一种名为Mem-T的自主记忆智能体及MoT-GRPO框架，利用记忆操作树回传和事后信用分配将稀疏奖励转化为密集监督。我们在LoCoMo、LongMemEval等四个基准数据集上通过F1分数和推理Token数验证了其有效性。",
                    "summary_translation": "Memory agents（记忆智能体）通过内生地管理记忆的处理、存储和检索，摆脱了预定义的记忆处理管道，因其自主性和适应性而获得了越来越多的关注。然而，现有的训练范式仍然受到限制：智能体往往需要经历长跨度的记忆操作序列后才能接收到稀疏且延迟的奖励，这阻碍了记忆管理策略的真正端到端优化。为了解决这一局限性，我们提出了Mem-T，这是一种自主记忆智能体，通过与轻量级分层记忆数据库交互，对流式输入执行动态更新和多轮检索。为了有效地训练长跨度记忆管理能力，我们进一步提出了MoT-GRPO，这是一种树引导的强化学习框架，通过记忆操作树反向传播和事后信用分配，将稀疏的终端反馈转化为密集的逐步监督，从而实现记忆构建与检索的联合优化。大量实验表明，Mem-T具有以下特点：（1）高性能，性能超越A-Mem和Mem0等框架高达14.92%；（2）高效，在精度-效率帕累托前沿上表现优异，且在不牺牲性能的情况下，相对于GAM将每次查询的推理Token数减少了约24.45%。",
                    "inspiration_trace": "基于对论文《Mem-T: Densifying Rewards for Long-Horizon Memory Agents》的深入分析，以下是对作者核心方法论产出逻辑链的系统性推演。\n\n### 1. 宏观背景与演进：从静态规则到动态策略\n**观察起点：**\n随着大语言模型（LLM）向智能体演进，其固有的“有限上下文窗口”成为了瓶颈，导致长程交互中出现遗忘和不一致。\n**演进脉络：**\n*   **阶段一（静态记忆）：** 早期框架（如MemGPT, Mem0）依赖人工设计的提示词和启发式规则来管理记忆。\n    *   *局限：* 性能受限于基础模型的指令遵循能力，规则僵化，难以适应复杂场景。\n*   **阶段二（动态RL）：** 最新研究（如Memory-R1, Mem-α）开始引入强化学习（RL），将记忆管理转化为自适应策略优化问题。\n    *   *局限：* 虽然实现了动态化，但在训练机制上存在根本性缺陷。\n\n---\n\n### 2. 核心困境：Introduction 中的“故事”逻辑\n作者在引言中通过以下逻辑链条揭示了当前研究面临的致命挑战：\n\n1.  **范式转变的代价：** 虽然RL让记忆管理变得自适应，但引入了一个经典的RL难题——**时间信用分配**。\n2.  **极端的稀疏性：** 在长程记忆任务中，智能体往往需要在跨越约500轮对话、执行数百次记忆操作（写入、更新、检索）的漫长序列后，才能在最后收到一个基于QA准确率的**二元（0/1）稀疏奖励**。\n3.  **归因的缺失：** 现有的RL方法无法解决这种稀疏性。它们只是简单地将最终的稀疏奖励无差别地传播给序列中的所有操作，缺乏**过程级的归因**。\n4.  **优化的失效：** 这种极端的稀疏性和归因模糊性，导致智能体无法知道究竟是哪一步具体的记忆操作（是创建了一个事实？还是更新了某条经验？）对最终结果产生了贡献，从而严重阻碍了对整个记忆操作轨迹的有效优化。\n\n---\n\n### 3. 研究问题\n基于上述困境，作者提出了本论文试图解决的核心问题：\n\n**“如何构建一个完全可训练的记忆智能体框架，能够利用密集的奖励和准确的过程级归因，联合优化记忆的构建与检索？”**\n\n---\n\n### 4. 思想演进与方法论形成\n为了解决上述问题，作者的思考过程经历了从“结构设计”到“训练机制创新”的演进：\n\n#### 思考一：如何统一记忆管理？（结构层）\n*   **思考：** 要实现端到端优化，首先需要一个能够覆盖记忆全生命周期的统一架构，而不是割裂的模块。\n*   **方案：** 提出 **Mem-T** 框架。设计一个分层记忆数据库（包含事实、经验、原始记忆和工作记忆），让智能体能够自主地执行“形成”、“演化”和“检索”三类核心操作。\n\n#### 思考二：如何解决长程奖励稀疏问题？（检索层）\n*   **思考：** 在检索阶段，智能体需要进行多步搜索。如果只在最后给分，中间的搜索步骤无法得到有效训练。能不能把“线性的搜索”变成“树状的探索”？\n*   **方案：** 提出 **MoT-GRPO (Memory Operation Tree)**。\n    *   **树构建：** 不再只跑一条路径，而是构建多棵记忆操作树，探索不同的检索轨迹。\n    *   **节点级回传：** 利用树的拓扑结构，将叶子节点的最终奖励（如F1分数）向上回传，给中间节点分配奖励。\n    *   **逻辑：** 如果某个中间节点能通向高分的叶子节点，说明这个检索步骤是有价值的。这样就将稀疏的终端奖励转化为了密集的步骤级监督。\n\n#### 思考三：如何解决记忆构建的滞后归因问题？（构建层）\n*   **思考：** 记忆构建（写记忆）发生在很久以前，而奖励发生在很久以后的检索时刻。如何知道当初写入的哪条记忆是有用的？\n*   **方案：** 提出 **后见信用分配**。\n    *   **逻辑：** 利用检索阶段生成的MoT树。如果某条记忆在成功的检索路径中被使用了，或者它包含了正确答案的证据，就“追溯”奖励给当初创建这条记忆的操作。\n    *   **机制：** 将下游检索的优势信号显式地归因回上游的构建动作，从而指导模型学会“应该记住什么”以及“如何更新记忆”。\n\n### 总结\n作者的逻辑链条是从**“记忆系统的动态化趋势”**出发，敏锐地捕捉到**“长程任务中的奖励稀疏与归因困难”**这一核心痛点，进而通过**“树状搜索结构”**解决检索的密集监督问题，通过**“后见归因机制”**解决构建的滞后评价问题，最终实现了Mem-T这一端到端可优化的记忆智能体。"
                },
                {
                    "title": "MiTa: A Hierarchical Multi-Agent Collaboration Framework with Memory-integrated and Task Allocation",
                    "arxiv_id": "2601.22974",
                    "authors": "XiaoJie Zhang, JianHan Wu, Xiaoyang Qu, Jianzong Wang",
                    "summary": "Recent advances in large language models (LLMs) have substantially accelerated the development of embodied agents. LLM-based multi-agent systems mitigate the inefficiency of single agents in complex tasks. However, they still suffer from issues such as memory inconsistency and agent behavioral conflicts. To address these challenges, we propose MiTa, a hierarchical memory-integrated task allocative framework to enhance collaborative efficiency. MiTa organizes agents into a manager-member hierarchy, where the manager incorporates additional allocation and summary modules that enable (1) global task allocation and (2) episodic memory integration. The allocation module enables the manager to allocate tasks from a global perspective, thereby avoiding potential inter-agent conflicts. The summary module, triggered by task progress updates, performs episodic memory integration by condensing recent collaboration history into a concise summary that preserves long-horizon context. By combining task allocation with episodic memory, MiTa attains a clearer understanding of the task and facilitates globally consistent task distribution. Experimental results confirm that MiTa achieves superior efficiency and adaptability in complex multi-agent cooperation over strong baseline methods.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断（符合保留标准）**: *   该论文的核心贡献是提出了 **MiTa**，这是一个**分层多智能体协作框架**（Hierarchical Multi-Agent Collaboration Framework）。 *   论文的研究重点在于解决多智能体系统（MAS）中存在的“记忆不一致”和“智能体行为冲突”问题，属于**构建和改进 LLM 智能体系统**的方法论研究，而非将现有智能体简单应用于特定领域（如医疗、金融等）。 *   因此，它符合第一步中的“保留”条件，即核心是关于构建 LLM 智能体或多智能体系统的新框架。 2.  **正面指标匹配（高度相关）**: *   **多智能体**: 论文明确属于 `Multi-Agent Systems` 范畴，研究了智能体间的 `Collaboration`（协作）和 `Task Allocation`（任务分配）。 *   **智能体能力**: 论文重点解决了 `Memory`（记忆）问题，特别是通过 `Episodic memory integration`（情景记忆集成）来保持长期上下文。同时，其“管理者-成员”层级结构涉及 `Planning`（规划）能力。 *   **架构创新**: 提出的 Manager-member hierarchy 和 allocation/summary 模块是对现有 Agentic AI 架构的直接改进。 3.  **排除标准检查（无触发项）**: *   论文不涉及安全、对齐、水印或幻觉等安全议题。 *   虽然提到了 \"embodied agents\"（具身智能体），但核心贡献不在于视觉或多模态处理，而在于协作框架本身，因此不触发布局排除规则。 *   不涉及知识图谱或图神经网络。 4.  **综合结论**: 该论文完全符合“多智能体”这一核心研究方向，其提出的分层框架和记忆集成机制直接对应了研究课题中关于智能体协作、通信和记忆能力的关注点。因此，这是一篇高度相关的前沿论文。",
                    "summary2": "本文旨在解决多智能体协作中的记忆不一致和行为冲突问题。针对复杂长视界任务，我们提出了一种名为MiTa的分层Memory-integrated Task allocative框架，通过Manager的Allocation和Summary模块实现全局任务分配与情景记忆集成。在VirtualHome-Social数据集上，通过平均步数和效率提升指标验证了其有效性。",
                    "summary_translation": "大语言模型 的近期进展显著加速了具身智能体 的发展。基于 LLM 的多智能体系统 缓解了单智能体 在处理复杂任务时效率低下的问题。然而，它们仍面临记忆不一致性 和智能体行为冲突 等问题。为应对这些挑战，我们提出了 MiTa，一种分层记忆集成任务分配框架，旨在提升协作效率。MiTa 将智能体组织为管理者-成员层级结构，其中管理者集成了额外的分配模块和摘要模块，从而实现 (1) 全局任务分配 和 (2) 情景记忆集成。分配模块使管理者能够从全局视角分配任务，从而避免潜在的智能体间冲突。摘要模块由任务进度更新触发，通过将近期的协作历史凝练为保留长视距上下文 的简明摘要，来执行情景记忆集成。通过结合任务分配与情景记忆，MiTa 能够更清晰地理解任务，并促进全局一致的任务分配。实验结果证实，在复杂的多智能体协作中，MiTa 相较于强基线方法 实现了更优的效率和适应性。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出《MiTa》这篇文章核心思想的逻辑链推演。\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分通过层层递进的方式，构建了从宏观愿景到具体痛点的逻辑链条：\n\n1.  **宏观愿景**：AI研究的长期目标是追求能够自主完成通用任务的具身智能体。\n2.  **技术驱动**：大语言模型（LLMs）在语言理解、逻辑推理和任务规划方面展现出强大能力，成为驱动具身智能体的核心力量（如Language-Planner, LLM-Planner等）。\n3.  **单智能体瓶颈**：尽管LLM能力强大，但受限于个体能力的局限性，单智能体框架在复杂场景下表现不佳。\n4.  **多智能体兴起**：受人类协作启发，多智能体系统通过通信和联合决策，在复杂任务中表现出超越单智能体的潜力（如CoELA, ProAgent等）。\n5.  **现存痛点**：现有的多智能体方法仍然存在严重缺陷——它们依赖有限记忆且缺乏集中式管理者。这导致了长期协调受阻、信息丢失、上下文跟踪能力差以及智能体间的行为冲突，最终在复杂的长周期任务中失败。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者显式地提出了本文试图解决的核心问题：\n\n**“如何构建一个多智能体协作框架，能够有效解决记忆不一致和智能体行为冲突的问题，从而在复杂的长周期任务中实现高效的协作？”**\n\n---\n\n### 三、 核心方法的思想演进逻辑链\n\n从发现问题到提出MiTa框架，作者的思考过程经历了以下四个关键阶段：\n\n#### 1. 观察与诊断：多智能体系统的“乱”与“忘”\n作者首先对现有多智能体系统的失败原因进行了诊断，归纳为两个核心矛盾：\n*   **“乱”**：缺乏全局统筹，各智能体仅从局部视角出发，导致行为冲突和重复劳动。\n*   **“忘”**：受限于上下文窗口，系统无法保留长期历史信息，导致在长周期任务中丢失关键上下文，出现“断片”。\n\n#### 2. 核心假设：引入“管理者”与“记忆体”\n为了解决上述矛盾，作者借鉴人类社会的组织形式，提出了一个核心假设：\n*   **组织架构变革**：从扁平化的多智能体结构转变为**分层结构**。引入一个“管理者”角色来统筹全局，其余作为“成员”负责执行。\n*   **功能增强**：管理者必须具备两个特定能力——一是**全局分配**（解决“乱”），二是**记忆集成**（解决“忘”）。\n\n#### 3. 机制设计：自下而上的协商与自上而下的分配\n针对“乱”的问题，作者设计了一套混合决策机制：\n*   **思考**：如果完全由管理者独断，可能忽略成员的局部感知；如果完全由成员自治，又会陷入冲突。\n*   **方案**：采用“协商+分配”模式。成员先基于局部观察提出建议（自下而上），管理者再结合全局视角和所有建议进行最终的任务分配（自上而下）。这样既保留了局部智能，又保证了全局一致性。\n\n#### 4. 记忆机制：基于进化的情景摘要\n针对“忘”的问题，作者设计了一种动态记忆压缩机制：\n*   **思考**：简单的历史记录会撑爆上下文窗口，且充满噪音。需要一种机制来提取“精华”。\n*   **方案**：利用LLM的摘要能力，设计一个**摘要模块**。该模块不是实时记录，而是由“任务进度更新”触发。每当任务取得阶段性进展，就将这段时间的历史压缩成一条简洁的“协作摘要”。\n*   **逻辑**：这种基于事件触发的摘要机制，既保留了长周期的上下文连贯性，又极大地节省了Token空间，避免了信息丢失。\n\n#### 5. 方法论形成：MiTa框架的诞生\n综合以上思考，作者最终构建了MiTa框架：\n*   **架构**：管理者（含Allocation和Summary模块）+ 成员（含Perception, Memory, Negotiation, Execution模块）。\n*   **闭环**：成员感知环境 -> 协商提议 -> 管理者分配任务 -> 执行动作 -> 进度更新触发记忆摘要 -> 循环。\n\n通过这一逻辑链，作者成功地将一个复杂的协作问题，拆解为“组织架构优化”和“记忆管理优化”两个可解的子问题，并最终通过LLM的能力实现了落地。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "Purely Agentic Black-Box Optimization for Biological Design",
                    "arxiv_id": "2601.22382",
                    "authors": "Natalie Maus, Yimeng Zeng, Haydn Thomas Jones, Yining Huang, Gaurav Ng Goel, Alden Rose, Kyurae Kim, Hyun-Su Lee, Marcelo Der Torossian Torres, Fangping Wan, Cesar de la Fuente-Nunez, Mark Yatskar, Osbert Bastani, Jacob R. Gardner",
                    "summary": "Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 定义 (第一步 & 第二步)**： 论文的核心贡献是提出了 **PABLO (Purely Agentic BLack-box Optimization)**，这是一个“分层智能体系统”。论文明确将生物设计问题重新表述为“完全智能体的、基于语言的推理过程”。这不仅仅是将LLM作为工具应用，而是构建了一个新的智能体框架，属于“构建、改进 LLM智能体”的范畴。 2.  **包含自我演化机制 (第四步 - 特殊情况)**： 论文描述该系统使用科学LLM“生成并**迭代优化** (iteratively refine)”生物候选者。这种通过反馈循环进行自我完善和迭代的过程，符合“自我演化”或“自我完善”的定义。 根据第四步的“自我演化的应用”规则：虽然论文应用于生物设计（特定领域），但其核心是提出了一种新的“自我演化/迭代优化”机制（即PABLO框架），因此符合保留条件。 3.  **排除非演化型应用 (第一步 - 排除)**： 尽管论文的应用场景是生物设计（药物发现、蛋白质工程），但它并没有简单地使用现有的智能体框架（如AutoGPT）去解决生物问题，而是发明了一种新的智能体架构（PABLO）来处理优化过程。因此，它不属于“非演化型应用”的排除范围。 综上所述，该论文在Agentic AI框架构建和自我演化机制方面有实质性贡献，符合研究课题要求。",
                    "summary2": "本文旨在解决生物设计中的黑盒优化问题，利用科学文献知识提升优化效率。针对分子和抗菌肽设计场景，我们提出了一种名为PABLO的分层智能体系统，使用科学LLM驱动整个优化循环。我们在GuacaMol基准和AMP优化任务上通过目标值和样本效率验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "生物设计中的许多关键挑战——例如small-molecule drug discovery（小分子药物发现）、antimicrobial peptide development（抗菌肽开发）和protein engineering（蛋白质工程）——都可以被构建为在广阔、复杂的structured spaces（结构化空间）上的black-box optimization（黑盒优化）问题。现有方法主要依赖raw structural data（原始结构数据），难以充分利用丰富的科学文献。虽然large language models（LLMs，大语言模型）已被整合到这些流程中，但它们仅被局限于structure-centered optimizers（以结构为中心的优化器）中的有限角色。相反，我们将biological black-box optimization（生物黑盒优化）构建为一个fully agentic（完全智能体化的）、基于语言的reasoning process（推理过程）。我们介绍了Purely Agentic BLack-box Optimization（PABLO，纯粹智能体黑盒优化），这是一个hierarchical agentic system（分层智能体系统），它利用在化学和生物文献上预训练的scientific LLMs（科学大语言模型）来生成并迭代优化biological candidates（生物候选物）。在标准的GuacaMol分子设计和antimicrobial peptide optimization tasks（抗菌肽优化任务）中，PABLO实现了state-of-the-art performance（最先进的性能），相比established baselines（既定基线）显著提高了sample efficiency（样本效率）和final objective values（最终目标值）。与先前结合LLMs的优化方法相比，尽管PABLO在整个optimization loop（优化循环）中依赖LLMs，但其每次运行的token使用量仍具有竞争力。除了原始性能外，这种agentic formulation（智能体化表述）为realistic design（现实设计）提供了关键优势：它自然地融合了semantic task descriptions（语义任务描述）、retrieval-augmented domain knowledge（检索增强的领域知识）和complex constraints（复杂约束）。在随后的in vitro validation（体外验证）中，PABLO优化的肽显示出对drug-resistant pathogens（耐药病原体）的强活性，突显了PABLO在therapeutic discovery（治疗发现）方面的实际潜力。",
                    "inspiration_trace": "基于对论文《Purely Agentic Black-Box Optimization for Biological Design》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示现有研究范式的根本性缺陷，从而为新方法的提出铺平道路：\n\n1.  **宏观背景定义**：\n    生物设计（如药物发现、蛋白质工程）本质上是一个在高维、复杂的离散空间中进行的**黑盒优化**问题。目标是找到能最大化特定属性（如结合亲和力、抗菌性）的序列或结构。\n\n2.  **现有范式及其局限**：\n    目前的主流方法（遗传算法、贝叶斯优化、强化学习、生成模型）主要依赖于**原始结构数据**（如SMILES字符串、氨基酸序列）。\n    *   **痛点**：这些方法忽略了蕴含在自然语言中的丰富人类知识（如机理洞察、构效关系、领域启发式规则）。\n    *   **后果**：除非人工将这些知识硬编码进模型，否则算法必须针对每个新任务从零开始重新学习生物学原理，效率低下且缺乏泛化性。\n\n3.  **技术演进与新的瓶颈**：\n    随着大语言模型（LLMs）的发展，研究者开始尝试将LLMs引入优化流程（例如作为变异算子、提供先验或作为代理模型）。\n    *   **现状**：LLMs目前仅被限制在以结构为中心的优化器中扮演**狭窄的辅助角色**。\n\n4.  **提出核心挑战**：\n    既然LLMs已经具备了强大的多模态推理能力，那么一个自然的极端问题是：**如果我们完全移除人工设计的优化策略，仅依靠语言模型来探索设计空间并自主制定搜索策略，会发生什么？**\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如果我们完全摒弃人工设计的优化策略，转而将生物黑盒优化视为一个端到端的、由语言驱动的推理过程，能否构建一个纯粹基于智能体的系统，从而更有效地利用科学文献知识并实现更优的设计性能？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从观察到最终提出 PABLO 框架，经历了以下四个阶段的思维演进：\n\n#### 1. 观察与反思：从“结构计算”到“知识推理”\n*   **观察**：传统的生物设计算法（如BO、GA）本质上是数学搜索算法，它们将分子视为数学符号（图或字符串），完全“看不懂”科学文献中描述的化学原理或生物学机制。\n*   **反思**：人类科学家进行药物设计时，不仅看结构，更会运用文献中的知识（例如“增加疏水性通常能提高膜穿透力”）。现有的AI方法丢失了这一巨大的知识库。\n*   **推论**：LLMs 预训练了海量科学文献，它们“懂”这些知识。如果能让 LLM 主导优化，就能直接调用这些内隐知识，而无需人工硬编码。\n\n#### 2. 假设提出：从“工具化使用”到“主体化代理”\n*   **现状批判**：目前的工作只是把 LLM 当作传统算法的一个“插件”（例如用 LLM 生成一个变异分子，然后还是用遗传算法去筛选）。这限制了 LLM 的潜力，因为优化策略（如怎么搜索、何时探索）仍被死板的数学算法锁死。\n*   **核心假设**：LLM 已经具备了足够的推理能力，可以像人类科学家一样，自主制定搜索策略。如果我们把“优化”这个任务本身变成一个“语言推理”任务，LLM 应该能自己决定“下一步该试什么类型的分子”。\n\n#### 3. 策略构思：模拟人类科学家的认知分工\n*   **类比**：一个科研团队是如何工作的？不是所有人做同样的事，而是有分工。\n*   **角色解构**：作者将优化过程类比为科研团队的协作，从而提出了**分层智能体**架构：\n    *   **战略家**：观察历史数据，总结规律，提出高层次的搜索假设（例如：“目前的分子极性太强，我们应该尝试增加疏水基团”）。\n    *   **执行者**：根据战略家的具体指令，对具体的分子进行精细化的修改（类似于实验室里的实验员）。\n    *   **探索者**：不局限于局部改进，而是基于全局视野和文献知识，提出全新的、大胆的分子结构（类似于头脑风暴）。\n\n#### 4. 方法落地：构建“纯智能体”闭环\n*   **机制设计**：为了实现上述假设，作者设计了一个动态循环系统。\n    *   **上下文蒸馏**：将复杂的优化历史转化为 LLM 能读懂的“自然语言上下文”（包含高分和低分样本），让 LLM 能像看实验报告一样分析数据。\n    *   **策略记忆库**：让 Planner 智能体不仅能生成新策略，还能记录哪些策略（Prompt）在过去成功过，哪些失败了，从而实现“在线学习”和策略进化。\n    *   **工具集成**：既然是语言驱动，就可以无缝接入外部工具（如文献检索 RAG），让智能体能像查资料一样实时获取知识。\n\n**总结**：作者的思想演进是从**“修补现有算法”**转向**“重塑优化范式”**。他们意识到，既然 LLM 已经掌握了科学知识，那么最优的路径不是教算法如何“模拟”知识，而是直接让 LLM 作为“大脑”来驱动整个优化过程，从而将黑盒优化转化为一个纯粹的语言推理问题。"
                },
                {
                    "title": "BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation",
                    "arxiv_id": "2601.22305",
                    "authors": "Bo Yuan, Yun Zhou, Zhichao Xu, Kiran Ramnath, Aosong Feng, Balasubramaniam Srinivasan",
                    "summary": "Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \\textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \\textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心关注的“Agentic AI”范畴，具体理由如下： 1.  **核心判断（符合构建与改进智能体）**： 论文的核心贡献是提出了 **BayesFlow**，这是一个用于自动生成工作流的框架。在LLM智能体的语境下，“工作流生成”本质上就是构建智能体的**规划**和**工具使用**逻辑。论文旨在解决如何自动合成LLM调用、工具调用和后处理步骤的序列，这正是构建和改进LLM智能体核心能力的体现，而非简单的应用或基础设施研究。 2.  **符合研究焦点（Agentic: Planning & Tool Use）**： 论文明确涉及了筛选标准中的关键正面指标： *   **Agentic AI**: 标题中提到的“Meta-Agent Assisted”表明其研究主体是智能体架构。 *   **Planning**: 工作流生成是智能体规划能力的高级形式，涉及多步决策和序列构建。 *   **Tool Use**: 摘要明确指出工作流包含“tool invocations”（工具调用），这是智能体与环境交互的核心方式。 3.  **排除标准检查**： *   该论文不是关于安全、对齐、多模态或图技术的。 *   它不是将现有智能体应用到特定垂直领域（如医疗、法律），而是提出了一种通用的、理论驱动的方法论来**生成**智能体的工作流逻辑。 *   它不是非Agentic的基础推理提升（如单纯的数学CoT），而是关注智能体系统的结构化构建。 4.  **特殊与模糊情况处理**： 论文提出的“Sequential in-loop refiner”（顺序循环优化器）用于进行池级改进，这体现了一种迭代优化的思想，与智能体的自我完善或演化机制有内在联系。更重要的是，它属于“智能体如何进行规划或在复杂任务中进行多步推理”的保留范畴。 综上所述，BayesFlow 提出了一种新的方法论来构建和优化LLM智能体的工作流，直接贡献于单智能体的规划与工具使用能力，因此符合筛选要求。",
                    "summary2": "本文旨在解决自动工作流生成缺乏理论基础和多样性的问题。针对复杂的端到端任务，我们提出了一种基于贝叶斯推断的BayesFlow框架，利用并行前瞻推演进行重要性加权和循环内细化来构建工作流。在六个基准数据集上，通过准确率等指标验证了其有效性，相比SOTA方法提升了最高9个百分点。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation》的深度分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 宏观背景与问题引入\n\n#### 1. Introduction 中的“讲故事”逻辑\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示现有研究的痛点并引出本文的切入点：\n\n1.  **现状与潜力**：大语言模型（LLM）在单步任务上表现出色，但随着任务复杂度提升（多步推理、工具使用），简单的提示工程已不足够，转向“智能体工作流”成为必然趋势。\n2.  **现实瓶颈**：现有的工作流框架（如AutoGen, CAMEL）虽然有效，但设计高度依赖人工专家，需要反复试错。这种“手动设计”的瓶颈限制了LLM系统在新任务上的扩展性和适应性。\n3.  **现有解法及其缺陷**：为了解决自动化问题，现有方法将工作流生成视为一个**优化问题**（Optimization Problem），利用MCTS、启发式搜索等寻找单一的最优解。\n4.  **批判与缺口**：作者指出这种“优化视角”存在两大缺陷：\n    *   **缺乏理论根基**：通常是启发式的，缺乏严谨的数学保证。\n    *   **解的单一性**：只能产出一个高分解，缺乏多样性，无法适应不同场景的需求。\n5.  **本文视角**：作者提出应将工作流生成视为**贝叶斯推断**（Bayesian Inference）问题，通过后验采样自然地获得高质量且多样化的工作流。\n\n#### 2. 核心研究问题\n基于上述逻辑，作者试图回答的核心问题是：\n\n**“如何将自动工作流生成从缺乏理论保证且解单一的优化问题，转化为具有理论支撑且能产生多样化高质量解的贝叶斯后验采样问题？”**\n\n---\n\n### 二、 思想演进与方法论形成\n\n作者的思想演进遵循了“观察现象 -> 理论重构 -> 攻克难点 -> 系统集成”的逻辑路径。\n\n#### 第一阶段：理论视角的转换\n*   **观察**：现有的“优化”思维（寻找最大值）限制了工作流的多样性和理论解释性。\n*   **假设**：如果我们不追求“最优解”，而是追求“符合高质量分布的解”，是否能利用LLM的生成能力？\n*   **理论映射**：\n    *   将Meta-Optimizer LLM的内部知识视为**先验分布** $p(s)$。\n    *   将外部反馈（如验证集准确率）视为基于能量的奖励函数 $R(s)$。\n    *   目标变为从后验分布 $q(s) \\propto p(s) \\exp(R(s))$ 中进行采样。\n*   **意义**：这一转换将工作流生成与强化学习（RL）和推断时缩放定律联系起来，提供了理论上的合法性。\n\n#### 第二阶段：解决“稀疏奖励”的采样难题\n*   **挑战**：在贝叶斯框架下，奖励 $R(s)$ 通常是**终端奖励**（只有生成完整工作流后才能获得）。这意味着在生成中间步骤时，无法判断当前路径的好坏，导致采样效率极低（权重退化）。\n*   **思路**：需要一种机制，在不训练额外价值模型的前提下，提前预估“部分工作流”的潜在价值。\n*   **方案提出**：**并行前瞻推演**。\n    *   借鉴序列蒙特卡洛（SMC）思想，对于每一个当前步骤，并行生成 $K$ 个可能的未来完成路径。\n    *   利用这 $K$ 个路径的平均奖励作为当前步骤的**重要性权重**。\n    *   这使得算法可以在生成过程中就“预见”未来，从而指导重采样，保留有潜力的前缀。\n\n#### 第三阶段：弥补纯采样的局限性\n*   **反思**：虽然“前瞻推演”解决了探索问题，但纯采样是被动的。如果早期选错了前缀，后续无论如何采样都无法修复这个错误。\n*   **思路**：需要引入主动的修正机制，利用LLM的自我反思能力。\n*   **方案提出**：**循环内顺序精炼**。\n    *   在每一步的重采样之前，引入一个全局精炼算子。\n    *   该算子基于当前的候选池，生成新的、经过改进的完整工作流。\n    *   这不仅统一了现有的MCTS、进化算法等方法（将它们视为特殊的精炼算子），还允许算法在生成过程中“回溯”并修正早期错误。\n\n#### 第四阶段：系统集成与收敛性证明\n*   **综合**：将上述两个模块结合，形成了**贝叶斯工作流生成（BWG）**框架。\n    *   **并行前瞻**负责探索和评估，保证理论上的收敛性（证明加权经验分布收敛于目标后验）。\n    *   **循环精炼**负责利用和修正，提升实际性能。\n*   **实例化**：最终将BWG实例化为 **BayesFlow** 算法，通过轻量级的接口设计，实现了无需训练、仅靠推理即可完成高质量工作流的生成。\n\n---\n\n### 三、 总结：逻辑链条全景\n\n1.  **痛点**：手动设计工作流难，现有的自动优化方法缺乏理论且解单一。\n2.  **视角**：从“优化”转向“贝叶斯推断”，利用后验采样获取多样性。\n3.  **难点1**：终端奖励导致中间步骤无法评估。\n    *   *对策*：并行前瞻推演，用未来模拟值指导当前选择。\n4.  **难点2**：纯采样无法修正早期错误。\n    *   *对策*：引入循环内精炼，主动改进候选池。\n5.  **成果**：BayesFlow —— 一个既有理论收敛保证，又具备实际精炼能力的训练型工作流生成框架。"
                },
            ]
        },
    ],
    "2026-01-30": [
        {
            "name": "Artificial Intelligence",
            "count": 26,
            "papers": [
                {
                    "title": "Exploring Reasoning Reward Model for Agents",
                    "arxiv_id": "2601.22154",
                    "authors": "Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue",
                    "summary": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心贡献符合要求 (第一步 - 保留)**: 论文的核心贡献是提出了 \"Agent Reasoning Reward Model (Agent-RRM)\"，这是一种专门用于训练和改进智能体的新框架。它旨在解决 Agentic RL 中稀疏奖励的问题，通过提供结构化反馈（推理轨迹、批评指导、评分）来优化智能体的表现。这属于“构建、改进或演化 LLM智能体”的方法论研究，而非简单的应用或基础设施研究。 2.  **高度契合研究焦点 (第二步 - 正面指标)**: *   **Agentic AI**: 论文明确关注 \"Agentic Reinforcement Learning\" 和 \"agentic trajectories\"，涉及智能体的复杂推理和工具使用。 *   **自我演化**: 论文提出的机制通过 \"critique\"（批评）和 \"refinement guidance\"（改进指导）来帮助智能体修正错误，这直接对应了“自我反思”和“自我完善”的演化机制。 *   **智能体能力**: 论文涉及 `Reasoning`（推理）、`Tool Use`（工具使用）以及 `Self-Correction`（通过 Reagent-C 策略实现）。 3.  **不触犯排除标准 (第三步)**: *   论文的主要贡献不是关于安全、对齐、可解释性或幻觉，而是关于提升智能体的任务执行能力。 *   论文不涉及多模态视觉或图技术作为核心研究内容。 4.  **特殊情况处理 (第四步)**: *   虽然论文涉及推理，但它不是关于提升LLM基础Token预测能力的非Agentic推理，而是关于智能体在轨迹中的推理过程和工具使用，符合“智能体如何进行规划或多步推理”的保留条件。 综上所述，该论文提出了一种新的智能体训练和反馈机制，属于 Agentic AI 和自我演化的前沿研究，完全符合你的研究目标。",
                    "summary2": "本文旨在解决Agentic RL中稀疏奖励无法区分中间推理质量的问题。针对长视界智能体任务，我们提出了一种多面体Agent Reasoning Reward Model (Agent-RRM)，它能生成包含推理轨迹、批评和分数的结构化反馈。我们在12个基准测试上通过准确率等指标验证了其有效性，特别是Reagent-U在GAIA和WebWalkerQA上取得了显著提升。",
                    "summary_translation": "Agentic Reinforcement Learning (Agentic RL) (智能体强化学习) 在赋能智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍然依赖于稀疏的基于结果的奖励进行训练。这种反馈无法有效区分中间推理质量，导致训练效果不理想。在本文中，我们介绍了 Agent Reasoning Reward Model (Agent-RRM) (智能体推理奖励模型)，这是一个多维度的奖励模型，能够为智能体轨迹生成结构化反馈，包括：(1) 显式推理轨迹，(2) 针对性评论，通过突出推理缺陷提供改进指导，以及 (3) 评估过程表现的总体评分。利用这些信号，我们系统地研究了三种集成策略：Reagent-C (文本增强优化)、Reagent-R (奖励增强指导) 和 Reagent-U (统一反馈整合)。在 12 个多样化基准上的广泛评估表明，Reagent-U 实现了显著的性能飞跃，在 GAIA 上达到 43.7%，在 WebWalkerQA 上达到 46.2%，验证了我们推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来的研究。",
                    "inspiration_trace": "基于对论文《Exploring Reasoning Reward Model for Agents》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n### 1. 引言中的“讲故事”逻辑\n\n作者在Introduction部分构建了一个层层递进的“问题-现状-缺口”叙事逻辑：\n\n1.  **背景与现状**：Agentic RL（智能体强化学习）在处理复杂推理和工具使用方面已经取得了显著成功，能够处理动态环境和外部知识源。\n2.  **核心痛点**：然而，现有的Agentic RL方法大多依赖于**稀疏的、基于结果的奖励**（即只看最终答案对不对）。\n3.  **痛点具象化**：这种设计对于长周期的智能体任务是致命的。如果一个轨迹在最后一步失败，它会被视为完全失败。这种粗糙的二元监督掩盖了中间成功步骤的价值，导致训练效果次优。\n4.  **现有尝试及其局限**：为了提供更细粒度的反馈，研究开始引入奖励模型。但存在两个瓶颈：\n    *   **步骤级奖励**：标注成本极高，且容易遭受奖励黑客攻击。\n    *   **基于推理的奖励模型**：多关注成对偏好，存在固有偏差，无法捕捉轨迹间细微的质量差异，且缺乏可操作的改进指导。\n    *   **被忽视的信号**：大多数工作仅依赖数值奖励，而**自然语言批评**这一能提供更细粒度指导的信号很大程度上未被探索。\n5.  **本文切入点**：因此，我们需要开发一个多方面的推理奖励模型，它不仅能打分，还能显式推理并给出具体的批评建议。\n\n---\n\n### 2. 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**如何设计并利用一种多方面的推理奖励模型，通过结合显式的推理轨迹、针对性的文本批评以及整体质量评分，来克服传统稀疏奖励在长周期智能体训练中的局限性？**\n\n---\n\n### 3. 核心方法产出的逻辑链推演\n\n以下是对作者思考过程的还原，展示了从观察到假设再到方法论的演进：\n\n#### 第一阶段：观察与反思\n*   **观察**：在训练智能体（Agent）时，传统的强化学习（RL）只给“0”或“1”的奖励（做对或做错）。\n*   **反思**：这就像教学生解数学题，如果学生前9步都对，最后一步算错，老师只给个大红叉，学生不知道自己哪里做得好，也不知道具体错在哪。对于需要多步推理和工具调用的复杂任务，这种反馈太稀疏、太低效了。\n\n#### 第二阶段：探索与瓶颈\n*   **探索**：既然人工给每一步打分太贵，能不能用模型来打分（即Reward Model, RM）？\n*   **发现瓶颈**：\n    *   如果只给一个分数（Scalar），虽然比0/1好，但智能体依然不知道怎么改。\n    *   如果让模型比较两个轨迹谁好（Pairwise），虽然能选出好的，但缺乏具体的纠错指导，且容易有偏见。\n    *   现有的方法忽略了人类导师最常用的方式——**“语言解释”**。\n\n#### 第三阶段：核心假设\n*   **假设**：如果奖励模型不仅能给出一个分数，还能像老师一样**“写评语”**（Critique），指出具体的逻辑漏洞或工具使用错误，那么智能体就能获得更密集、更高质量的监督信号。\n*   **假设延伸**：这种“评语”不仅能用于直接纠错（推理时），其生成的过程本身也能产生更准确的分数（训练时）。\n\n#### 第四阶段：方法论构建\n*   **设计Agent-RRM**：构建一个多面手的奖励模型，它对每一个智能体轨迹输出三样东西：\n    1.  **推理轨迹**：模型内部的分析过程（为什么这么判分）。\n    2.  **批评**：具体的错误点和改进建议（给智能体看的）。\n    3.  **分数**：最终的质量评分（给RL优化用的）。\n*   **数据构建**：为了训练这个模型，作者利用GPT-4等强模型对现有的轨迹进行“标注”，生成上述的三元组数据。\n\n#### 第五阶段：系统性验证与策略演进\n*   **思考**：有了这个模型，到底该怎么用它？是只看文字，还是只看分数，还是都要？\n*   **策略推演**：\n    1.  **Reagent-C (Critique)**：先看看光靠“文字评语”能不能让智能体自我修正（不更新参数，只做推理时的反思）。\n    2.  **Reagent-R (Reward)**：看看光靠“模型打分”能不能比传统的规则打分训练得更好（解决稀疏性问题）。\n    3.  **Reagent-U (Unified)**：将两者结合，既用分数优化策略，又用文字评语引导采样，看能否产生协同效应。\n\n#### 第六阶段：结论与验证\n*   **验证**：实验表明，光用文字（C）能提升推理质量，光用分数（R）能缓解稀疏性，而统一模式（U）效果最好。\n*   **最终产出**：证明了“推理+批评+分数”的多维度反馈机制是提升智能体长周期推理能力的有效路径。"
                },
                {
                    "title": "Optimizing Agentic Workflows using Meta-tools",
                    "arxiv_id": "2601.22037",
                    "authors": "Sami Abuzakuk, Anne-Marie Kermarrec, Rishi Sharma, Rasmus Moorits Veski, Martijn de Vos",
                    "summary": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”改进方向。 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **Agent Workflow Optimization (AWO)** 框架，这是一种用于优化 LLM 智能体工作流的新方法。它不是将智能体作为工具去解决生物、金融等特定领域的问题，而是直接针对智能体本身的运行机制（工作流、工具调用模式）进行改进。这符合“构建、改进或演化 LLM 智能体”的核心目标。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文明确聚焦于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：研究重点在于优化 `Tool Use`（工具使用）和 `Planning`（规划/执行路径）。通过引入“元工具”，论文改进了智能体调用工具的方式，减少了冗余的推理步骤，提升了智能体的效率和鲁棒性。 3.  **排除标准（未触发）**： *   论文不涉及安全、对齐、多模态或图技术。 *   它不属于基础设施（如硬件加速或模型部署），而是智能体逻辑层面的优化。 4.  **特殊与模糊情况处理**： 虽然论文涉及“优化”，但这属于对智能体架构和执行机制的改进，而非单纯的数学推理能力提升。它通过重构智能体的工作流（将序列打包为元工具），实质上增强了智能体解决复杂任务的能力，属于对 Agentic 框架的演进。 综上所述，该论文致力于提升 LLM 智能体的效率与稳定性，是对智能体工具使用机制的重要改进，符合筛选要求。",
                    "summary2": "本文旨在解决Agentic AI工作流中因冗余推理步骤导致的高成本、高延迟及易失败问题。针对现有的工作流轨迹，我们提出了一种Agent Workflow Optimization (AWO)框架，通过识别重复的工具调用序列并将其转化为确定性的Meta-tools。我们在Visual Web Arena和AppWorld基准上通过LLM调用次数、Token使用量、成本及任务成功率验证了其有效性。",
                    "summary_translation": "Agentic AI（智能体AI）赋能 LLM（大语言模型）进行动态推理、规划以及与工具交互，从而解决复杂任务。然而，agentic workflows（智能体工作流）通常需要大量的迭代推理步骤和 tool invocations（工具调用），这会导致显著的 operational expense（运营成本）、end-to-end latency（端到端延迟）以及因 hallucinations（幻觉）引发的失败。本文介绍了 Agent Workflow Optimization (AWO，智能体工作流优化) 框架，该框架通过识别并优化冗余的 tool execution patterns（工具执行模式），旨在提升 agentic workflows（智能体工作流）的效率和 robustness（鲁棒性）。AWO 通过分析现有的 workflow traces（工作流轨迹）来发现重复出现的 tool calls（工具调用）序列，并将其转化为 meta-tools（元工具）。Meta-tools 是一种确定性的 composite tools（复合工具），能够将多个 agent actions（智能体动作）打包为单次调用。Meta-tools 能够绕过不必要的 intermediate LLM reasoning steps（中间大语言模型推理步骤），在缩短 execution paths（执行路径）的同时降低 operational cost（运营成本），从而减少失败的发生。在两个 agentic AI benchmarks（智能体AI基准测试）上的实验表明，AWO 最多可将 LLM calls（大语言模型调用次数）减少 11.9%，同时将 task success rate（任务成功率）提高 4.2 个百分点。",
                    "inspiration_trace": "基于对论文《Optimizing Agentic Workflows using Meta-tools》的深入分析，以下是对作者产出该文章核心思考过程的系统性推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从“技术愿景”到“现实瓶颈”再到“破局思路”的完整叙事链条：\n\n1.  **愿景确立**：首先肯定 Agentic AI 的强大潜力。它结合了大语言模型（LLM）与工具调用能力，能够以最小的人工干预解决复杂、开放式的任务，具备动态规划和适应环境的能力。\n2.  **揭示代价**：话锋一转，指出这种“灵活性”背后的高昂代价。为了实现规划、推理、修正和恢复，Agent 需要频繁迭代调用 LLM。每一次推理步骤都意味着推理成本和端到端延迟的增加，这在面对细粒度 API（如需要多次顺序请求的 Spotify 操作）时尤为严重。\n3.  **发现矛盾**：虽然 Agent 的设计初衷是灵活多变的，但作者通过观察基准测试（AppWorld）发现了一个反直觉的现象：Agent 的工作流实际上表现出**高度的结构规律性**。在执行早期，大量任务遵循相同的轨迹。\n4.  **归因分析**：作者将这种重复性归因为“工具设计的错位”——现有的工具集通常不是为 Agent 使用而设计的，导致 Agent 在不同任务中重复执行相同的工具调用序列。\n5.  **提出方向**：基于上述观察，提出核心解决思路——识别并合并这些冗余的工具调用模式，将其封装为“元工具”，从而在不牺牲通用性的前提下提升效率。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述逻辑链条，作者在文中明确提出了以下核心研究问题：\n\n**“如何自动识别并利用 Agent 执行轨迹中重复出现的工具调用模式，从而在不牺牲任务通用性的前提下，减少推理开销、延迟和成本？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进（思想脉络还原）\n\n以下是从宏观问题到具体方法论的思维演进过程：\n\n#### 第一阶段：从“灵活性”到“低效性”的痛点洞察\n*   **思考起点**：Agentic AI 的核心是 ReAct（推理+行动）循环。为了完成任务，LLM 必须在每一步都进行推理来决定下一步调用哪个工具。\n*   **逻辑推演**：这种机制虽然灵活，但极其“昂贵”。每一步推理都消耗 Token 和时间。如果任务需要调用 10 个细粒度工具，就意味着至少 10 次 LLM 调用。随着任务复杂度增加，成本呈线性甚至指数级增长，且中间步骤越多，出错（幻觉）的概率越大。\n*   **结论**：必须减少 LLM 的推理次数，但不能通过限制 Agent 能力来实现，否则就失去了 Agentic AI 的意义。\n\n#### 第二阶段：从“随机性”到“规律性”的实证观察\n*   **思考转折**：作者没有直接去优化 LLM 本身，而是去观察 Agent 的实际行为数据。\n*   **逻辑推演**：通过分析 AppWorld 等基准测试的轨迹，发现虽然用户 Prompt 千差万别，但 Agent 的执行路径在早期阶段往往高度重合（例如，都需要先登录、先搜索用户 ID）。\n*   **关键洞察**：这种重复性并非偶然，而是因为底层工具（API）是细粒度的。Agent 不得不反复组合这些基础工具来完成高频的子任务。这意味着**大量的 LLM 推理是在重复解决相同的子问题**。\n\n#### 第三阶段：类比与假设——“编译器思维”的引入\n*   **思维跳跃**：作者将 Agent 的工作流类比为计算机程序的执行，将重复的工具调用序列类比为代码中的“热点路径”。\n*   **核心假设**：既然传统编译器可以通过“函数内联”或“循环融合”来优化代码，那么我们是否可以将 Agent 频繁使用的工具调用序列“编译”成一个单一的、确定性的高级工具？\n*   **概念定义**：由此诞生了 **“Meta-tool（元工具）”** 的概念——即把一系列固定的工具调用打包成一个黑盒，Agent 调用它时，直接跳过中间的推理步骤，一步到位。\n\n#### 第四阶段：方法论构建——如何自动化发现规律？\n*   **面临挑战**：有了“元工具”的概念，如何自动知道哪些序列应该被打包？这需要分析历史执行轨迹。\n*   **逻辑构建**：\n    1.  **形式化表示**：将 Agent 的执行轨迹转化为**状态图**。节点代表状态（历史工具调用序列），边代表工具调用。\n    2.  **去噪与合并（水平合并）**：不同轨迹可能在语义上等价但形式不同（例如，先读 A 再读 B，和先读 B 再读 A，如果都是读操作，结果一样）。需要引入领域知识（如交换律、正则匹配）将这些路径合并，暴露出真正的“公共路径”。\n    3.  **提取与压缩（垂直合并）**：在合并后的图中，寻找权重最高（即被访问次数最多）的路径，将其识别为候选的元工具。用元工具替换这些路径，从而压缩图结构，减少 LLM 调用节点。\n\n#### 第五阶段：价值验证——效率与鲁棒性的双重提升\n*   **逻辑闭环**：引入元工具后，不仅减少了 LLM 调用次数（省钱、省时），而且因为缩短了执行路径，减少了中间步骤出错的可能性，反而可能提高任务成功率。\n*   **最终产出**：形成了 AWO（Agent Workflow Optimization）框架，一个通过分析历史轨迹、自动生成元工具来优化 Agent 工作流的通用系统。"
                },
                {
                    "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic",
                    "arxiv_id": "2601.21972",
                    "authors": "Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato",
                    "summary": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断**：论文的核心贡献是提出了一种新的多智能体强化学习（MARL）方法，即多智能体 Actor-Critic (MAAC) 框架（包括 CoLLM-CC 和 CoLLM-DC），用于优化去中心化的 LLM 协作。这属于构建和改进 LLM 智能体系统的范畴，而非仅仅将智能体作为工具应用到特定领域。虽然实验在写作、编码等领域进行，但重点在于改进智能体间的协作机制，而非解决领域特定问题。 2.  **正面指标匹配**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)` 和 `LLM-based Agents`。 *   **多智能体**：论文重点研究智能体间的 `Collaboration`（协作），并探讨了去中心化与集中式执行的优劣，这是多智能体研究的核心议题。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   它不是基础设施研究，而是关于智能体协作算法的改进。 综上所述，该论文通过引入 Actor-Critic 方法改进了多智能体系统的协作效率和训练稳定性，是对 LLM 智能体（特别是多智能体方向）的重要方法论贡献，因此予以保留。",
                    "summary2": "本文旨在优化去中心化LLM协作，克服蒙特卡洛方法高方差和低样本效率的局限。针对去中心化多智能体协作场景，我们提出了一种Multi-Agent Actor-Critic (MAAC) 方法，特别是CoLLM-CC（集中式评论家）框架。在写作、编程及Minecraft游戏任务上通过任务性能和样本效率验证了其有效性，结果表明CoLLM-CC在长视界或稀疏奖励任务中显著优于基线方法。",
                    "summary_translation": "近期研究已探索利用多智能体强化学习来优化大语言模型 (LLM) 的协作。然而，大多数 MARL 微调方法依赖于预定义的执行协议，这往往需要集中式执行。去中心化的 LLM 协作在实践中更具吸引力，因为智能体能够并行运行推理，且部署方式灵活。此外，当前的方法采用蒙特卡洛方法进行微调，该方法存在高方差问题，因此需要更多的样本才能实现有效训练。Actor-critic 方法在 MARL 中被广泛用于解决这些问题，因此我们开发了多智能体 Actor-Critic (MAAC) 方法来优化去中心化的 LLM 协作。在本文中，我们分析了这些 MAAC 方法在何种情况下有益以及其背后的原因。我们提出了两种 MAAC 方法：采用集中式评论家的 **CoLLM-CC** 和采用去中心化评论家的 **CoLLM-DC**。我们在写作、编程和游戏等领域的实验表明，在短视界和密集奖励设置下，蒙特卡洛方法和 CoLLM-DC 能够取得与 CoLLM-CC 相当的性能。然而，在长视界或稀疏奖励任务上，两者的表现均不及 CoLLM-CC；在此类任务中，蒙特卡洛方法需要显著更多的样本，而 CoLLM-DC 则难以收敛。我们的代码可在 https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models",
                    "arxiv_id": "2601.21947",
                    "authors": "Bowen Fang, Wen Ye, Yunyue Su, Jinghao Zhang, Qiang Liu, Yesheng Liu, Xin Sun, Shu Wu, Jiabing Yang, Baole Wei, Liang Wang",
                    "summary": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中关于“工具使用”的核心研究。 1.  **核心贡献判断**：论文的核心贡献是提出了 ToolWeaver，一种新颖的生成式工具学习框架。这直接对应了筛选标准中的“构建、改进 LLM智能体”。它解决的是智能体如何更有效地使用工具这一基础能力问题，而非将智能体作为工具应用到某个垂直领域。 2.  **符合核心关注点**： *   **Agentic AI / Tool Use**：论文明确致力于解决 LLM 智能体在大规模工具使用中的挑战。摘要中提到“为先进的工具增强智能体建立基础”，这直接命中了筛选标准中的正面指标 `Tool Use / Tool Augmentation`。 *   **改进机制**：论文通过将工具编码为分层序列，解决了现有方法中词汇表爆炸和语义瓶颈的问题，使模型能够学习工具间的协作模式。这属于对智能体工具调用机制的实质性改进。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   它不是非演化型应用，而是对智能体底层能力的架构优化。 综上所述，ToolWeaver 提供了一种提升 LLM 智能体工具使用可扩展性和语义感知能力的新方法，是构建高性能 Agentic AI 的关键技术进展，因此予以保留。",
                    "summary2": "本文旨在解决大模型工具使用中的可扩展性瓶颈及语义协作缺失问题。针对大规模工具库场景，我们提出了一种名为ToolWeaver的生成式工具学习框架，通过协作感知的向量量化将工具编码为分层代码序列。我们在ToolBench基准上通过NDCG、SoPR和SoWR指标验证了其有效性，结果表明该方法显著优于现有最先进方法。",
                    "summary_translation": "普遍存在的基于检索的工具使用流程面临着双重语义挑战：其 retrievers (检索器) 通常采用的 encoders (编码器) 无法捕捉复杂语义，而 Large Language Model (LLM) (大语言模型) 本身因其自然语言预训练过程而缺乏内在的工具知识。生成式方法通过统一选择和执行过程，提供了一种强有力的替代方案，其任务是让 LLM 直接学习并生成 tool identifiers (工具标识符)。然而，将每个工具映射到一个唯一新 token (词元) 的常见做法引入了严重的局限性：这导致了 scalability (可扩展性) 和 generalization (泛化能力) 的危机，因为 vocabulary (词表) 大小急剧膨胀，且每个工具被分配了一个语义孤立的 token。这种方法还造成了一个 semantic bottleneck (语义瓶颈)，阻碍了协作工具关系的学习，因为模型必须从庞大库中 monolithic tool IDs (整体式工具ID) 的 sparse co-occurrences (稀疏共现) 中推断这些关系。为了解决这些局限性，我们提出了 ToolWeaver，这是一种新颖的 generative tool learning framework (生成式工具学习框架)，它将工具编码为 hierarchical sequences (分层序列)。这种方法使得 vocabulary expansion (词表扩展) 与工具数量呈对数关系。至关重要的是，它使模型能够从 shared codes (共享代码) 的 dense co-occurrence (密集共现) 中学习协作模式，而不是从 monolithic tool IDs (整体式工具ID) 的 sparse co-occurrences (稀疏共现) 中学习。我们通过一种新颖的 tokenization (分词) 过程生成这些 structured codes (结构化代码)，该过程旨在将工具的 intrinsic semantics (内在语义) 与其 extrinsic co-usage patterns (外在共同使用模式) 交织在一起。然后，这些 structured codes (结构化代码) 通过 generative alignment stage (生成式对齐阶段) 集成到 LLM 中，在该阶段对模型进行 fine-tuned (微调) 以生成分层代码序列。对近 47,000 个工具的评估结果表明，ToolWeaver 显著优于 state-of-the-art (最先进) 的方法，为先进的 tool-augmented agents (工具增强智能体) 建立了一个更具可扩展性、可泛化性且具有语义感知能力的基础。",
                    "inspiration_trace": "基于对论文《ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models》的深入分析，以下是对作者产出该文章的系统性逻辑推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从宏观趋势到微观矛盾的叙事链条，具体逻辑如下：\n\n1.  **宏观背景与趋势**：\n    *   **观察**：大语言模型（LLM）正在演变为能够执行现实世界任务的交互式智能体，这依赖于集成外部工具（如API）。\n    *   **现状**：可用工具的数量和多样性呈爆发式增长，从通用服务扩展到特定领域的API。\n\n2.  **现有范式的分类与局限**：\n    *   **检索式方法**：虽然常用，但存在“双重语义挑战”——检索器的编码器难以捕捉复杂语义，且LLM本身缺乏预训练阶段的内在工具知识。\n    *   **生成式方法**：作为一种强大的替代方案，通过微调让LLM直接生成工具标识符，统一了选择和执行过程。\n\n3.  **核心冲突的揭示**：\n    *   **当前实践**：生成式方法通常采用“一工具一令牌”的策略，即每个工具映射到一个唯一的特殊令牌。\n    *   **危机一：可扩展性与泛化危机**：\n        *   词汇表大小随工具数量**线性增长**。例如，集成ToolBench需要增加近47,000个新令牌。\n        *   这种大规模的OOV（Out-of-Vocabulary）令牌注入会导致巨大的内存开销，并破坏模型预训练的语言知识，造成灾难性的性能退化。\n    *   **危机二：复杂推理的语义瓶颈**：\n        *   将工具扁平化为孤立的唯一令牌，导致模型难以学习**协作关系**。\n        *   模型被迫依赖工具ID之间**稀疏的共现**来推断关系。例如，在庞大的工具库中，“实时天气”和“空气质量”这两个特定工具对的联合出现概率极低，导致模型无法理解它们在复杂任务（如“带孩子去公园”）中的协同作用。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图解决的核心研究问题可总结为：\n\n**“如何设计一种可扩展的工具表示方法，使其既能克服‘一工具一令牌’范式下的词汇表爆炸和语义孤立问题，又能让大模型有效地学习工具间的复杂协作模式？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n从观察到假设，再到最终的方法论，作者的思考路径如下：\n\n#### 1. 观察与反思：打破“扁平化”思维\n*   **思考起点**：现有的“一工具一令牌”方法本质上是一种**扁平化**的映射。每个工具都是孤立的原子，彼此之间没有结构上的联系。\n*   **痛点分析**：\n    *   **效率低**：线性扩展不可持续。\n    *   **信号稀疏**：模型很难从海量的孤立ID中捕捉到“天气”和“空气质量”应该一起用的规律，因为它们在训练数据中很少成对出现。\n\n#### 2. 核心假设：从“原子”到“序列”的范式转移\n*   **假设提出**：如果不再用一个唯一的令牌代表一个工具，而是用一组**组合式的离散码序列**来表示工具，会怎样？\n*   **逻辑推演**：\n    *   **解决可扩展性**：利用层级结构。例如，使用 $L$ 个码本，每个码本有 $K$ 个码。这可以表示 $K^L$ 个工具，但只需要增加 $L \\times K$ 个新令牌。词汇表扩展从线性变为**对数级**。\n    *   **解决语义瓶颈**：如果功能相关的工具共享部分代码（例如，天气工具是 `<Code A, Code 1>`，空气质量工具是 `<Code A, Code 2>`），那么 `<Code A>` 就会在训练数据中**密集出现**。模型可以通过这个共享的父代码，更容易地学习到它们属于同一类上下文（如“户外环境”），从而捕捉协作信号。\n\n#### 3. 方法论构建：如何生成“有意义”的代码？\n*   **新挑战**：仅仅把工具变成数字序列是不够的，如何保证这些序列是有意义的？即如何让代码既反映工具的**内在功能**，又反映**外在协作**？\n*   **解决方案构思**：\n    *   **利用双重信息**：不仅要看工具的文档描述（语义），还要看工具在实际使用轨迹中是如何共现的（协作）。\n    *   **引入结构化分词**：设计一种无监督的“协作感知”过程。利用残差向量量化（RQ-VAE）将工具嵌入压缩成层级代码。\n    *   **注入协作信号**：在量化过程中，引入基于工具共现矩阵的图拉普拉斯正则化项。这迫使经常一起使用的工具在量化空间中距离更近，从而在代码结构上产生关联。\n\n#### 4. 细节完善：解决冲突与对齐\n*   **潜在问题**：层级量化可能会导致多个不同的工具映射到完全相同的代码序列（冲突）。\n*   **优化思路**：不能简单地加一层无意义的ID，这会破坏语义结构。需要在保持语义完整性的前提下解决冲突。\n*   **最终策略**：在最后一层码本中引入**均匀映射约束**（通过最优传输算法如Sinkhorn-Knopp实现），确保每个工具都有唯一的标识，同时尽可能保持语义分布的均匀性。\n\n#### 5. 系统集成：让LLM学会“说”这种语言\n*   **最后一步**：代码设计好了，如何教LLM使用？\n*   **生成式对齐**：将生成的结构化代码序列作为新令牌加入词表。通过两阶段微调（检索对齐 + 轨迹对齐），让模型学会在推理时直接生成这些层级代码序列，从而实现端到端的工具调用。\n\n---\n\n**总结**：作者的思考过程是从对现有方法“线性扩展”和“语义孤立”的不满出发，通过引入**组合式层级表示**这一核心假设，将工具选择问题转化为一个**结构化代码生成**问题，并巧妙地利用**协作信号**来指导代码的学习，最终构建了一个既高效又具备语义感知能力的工具使用框架。"
                },
                {
                    "title": "Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning",
                    "arxiv_id": "2601.21919",
                    "authors": "Yiqun Chen, Jinyuan Feng, Wei Yang, Meizhi Zhong, Zhengliang Shi, Rui Li, Xiaochi Wei, Yan Gao, Yi Wu, Yao Hu, Zhiqiang Pu, Jiaxin Mao",
                    "summary": "The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \\textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \\textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \\textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\\% to 39.0\\% while boosting accuracy by 4.33\\% to 10.02\\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心贡献符合多智能体系统 (Multi-Agent)**: 论文的核心是提出了一种名为 SCMA 的**多智能体强化学习 (MARL) 框架**。该框架明确定义了三个具有不同角色的智能体：**分割智能体**、**评分智能体** 和 **推理智能体**。这不仅仅是简单的模型优化，而是构建了一个多智能体协作系统来解决推理冗余问题。 2.  **涉及自我演化与改进 (Self-Evolving)**: 论文的目标是通过多智能体的协同优化，使推理智能体能够“自我压缩”其思维链，在保持准确性的同时减少冗余。这种通过反馈机制（来自分割和评分智能体的奖励信号）来改进主智能体性能的过程，符合“自我完善和迭代”的演化机制。 3.  **不属于排除项**: *   **非应用型研究**: 论文并非将LLM应用到生物、金融等特定领域，而是针对LLM本身的推理机制进行优化。 *   **非单纯的基础推理**: 虽然涉及CoT，但其方法论是基于多智能体协作的强化学习，而非单纯提出一种新的Prompt技巧或数据集，因此符合Agentic AI的研究范畴。 *   **非安全/多模态**: 论文不涉及安全对齐或视觉多模态内容。 综上所述，该论文通过构建多智能体协作框架来改进LLM的推理效率，属于构建和演化LLM智能体的前沿研究。",
                    "summary2": "本文旨在解决大型推理模型因冗余推理导致的推理开销问题。针对Chain-of-Thought (CoT) 序列，我们提出了一种SCMA框架，利用多智能体强化学习（MARL）协调分割智能体和评分智能体，通过重要性加权长度惩罚实现细粒度压缩。我们在GSM8K、MATH500等数据集上验证了其有效性，结果显示在减少响应长度11.1%-39.0%的同时，准确性提升4.33%-10.02%。",
                    "summary_translation": "冗余推理带来的推理开销不仅损害了交互体验，还严重制约了大型推理模型的部署。现有的基于强化学习 (RL, Reinforcement Learning) 的解决方案通过将长度惩罚与基于结果的奖励相结合来应对这一问题。这种简单的奖励加权方式难以在简洁性与准确性之间取得平衡，因为强制追求简洁可能会牺牲关键的推理逻辑。在这项工作中，我们通过提出一种多智能体强化学习框架来解决这一局限性，该框架能够选择性惩罚冗余片段，同时保留核心推理逻辑。我们的框架——基于多智能体强化学习的自压缩 (SCMA, Self-Compression via MARL)——通过两个专门的智能体来实现冗余检测与评估：**分割智能体** 负责将推理过程分解为逻辑片段，**评分智能体** 负责量化每个片段的重要性。分割智能体和评分智能体在训练过程中协同定义一种重要性加权的长度惩罚，从而激励**推理智能体** 优先关注核心逻辑，且在部署阶段不会引入额外的推理开销。跨模型规模的实证评估表明，SCMA 在将响应长度减少 11.1% 至 39.0% 的同时，将准确性提升了 4.33% 至 10.02%。此外，消融实验和定性分析证实，多智能体强化学习框架内的协同优化促进了涌现行为的产生，从而构建出比原始强化学习范式更强大的大型推理模型。",
                    "inspiration_trace": "基于对论文《Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原：\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个从“成功”到“副作用”，再到“现有方案失效”的完整叙事链条：\n\n1.  **现象与红利**：大型推理模型通过强化学习（RL）获得了“深度思考”的能力，能够利用长链思维解决复杂问题。\n2.  **副作用显现**：这种RL优化主要依赖稀疏的、基于结果的二元奖励。由于缺乏对中间过程的精细指导，模型为了最大化奖励的确定性，倾向于“过度思考”，生成包含大量非必要步骤或重复验证的冗长路径。\n3.  **现实瓶颈**：这种冗余推理导致了严重的推理延迟，阻碍了模型的实际部署。\n4.  **现有尝试及其局限**：为了解决效率问题，现有方法通常引入“长度惩罚”来压缩推理链。\n5.  **核心矛盾**：这种简单的长度惩罚存在“信用分配错位”的问题。它无差别地惩罚总长度，而非中间步骤的内在冗余。这导致了一个致命风险：模型为了满足长度约束，可能会牺牲对解题至关重要的关键推理步骤，从而损害最终准确性。\n\n### 二、 研究问题\n\n基于上述逻辑链条中揭示的核心矛盾，作者显式提出了本研究的核心问题：\n\n**如何在不牺牲最终准确性的前提下，在细粒度层面区分并保留高价值的推理步骤，同时精确消除冗余和无意义的重复？**\n\n---\n\n### 三、 核心方法的逻辑演进（从观察到假设）\n\n为了回答上述问题，作者的思考路径经历了以下四个阶段的演进：\n\n#### 1. 观察与诊断：从“粗粒度”到“细粒度”的需求\n*   **观察**：现有的单智能体RL方法（如简单的长度惩罚）本质上是一种“一刀切”的策略。它无法区分“废话”和“关键逻辑”。\n*   **诊断**：要解决这个问题，不能只看总长度，必须深入推理过程的内部结构。关键在于两点：一是将连续的推理过程分解为独立的逻辑单元；二是量化每个单元对最终结果的实质性贡献。\n\n#### 2. 假设提出：引入“重要性加权”机制\n*   **假设**：如果能够给推理链中的每一个片段打分（评估其重要性），然后根据这个分数来动态调整惩罚力度，就能实现“精准压缩”。\n*   **具体构想**：对于高价值的逻辑片段，给予低惩罚甚至豁免；对于冗余片段，给予高惩罚。这样模型就会学会保留精华，剔除水分。\n\n#### 3. 范式转移：从“单智能体”到“多智能体”\n*   **挑战**：在单智能体RL框架下，让同一个模型既负责推理，又负责自我切分和自我评分，任务过于复杂，难以收敛，且缺乏精细的结构化建模能力。\n*   **思路突破**：引入**多智能体强化学习（MARL）**。将复杂的压缩任务拆解为一个协作游戏，通过“分工”来降低难度。\n    *   **Agent 1 (Reasoning)**：负责探索解空间，生成推理路径。\n    *   **Agent 2 (Segmentation)**：负责结构化解析，将路径切分为逻辑块。\n    *   **Agent 3 (Scoring)**：负责量化评估，给每个逻辑块打分。\n\n#### 4. 机制设计：协同进化与自压缩\n*   **核心机制**：设计一个“重要性加权长度惩罚”作为共享的全局奖励信号。\n*   **逻辑闭环**：\n    *   Segmentation和Scoring Agent的输出决定了Reasoning Agent的奖励。\n    *   Reasoning Agent生成的质量反过来又影响Segmentation和Scoring Agent的判断依据。\n    *   通过共享参数和共享奖励，三个智能体在训练中协同进化。\n*   **最终目标**：这种训练仅发生在训练阶段。训练完成后，只部署Reasoning Agent，从而实现了“零推理开销”的**自压缩**。"
                },
                {
                    "title": "JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG",
                    "arxiv_id": "2601.21916",
                    "authors": "Yiqun Chen, Erhan Zhang, Tianyi Hu, Shijie Wang, Zixuan Yang, Meizhi Zhong, Xiaochi Wei, Yan Gao, Yi Wu, Yao Hu, Jiaxin Mao",
                    "summary": "The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \\textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \\textbf{JADE} (\\textbf{J}oint \\textbf{A}gentic \\textbf{D}ynamic \\textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \\textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，属于**多智能体**与**自我演化**的交叉领域。 1.  **核心贡献符合构建与改进LLM智能体**： 论文提出了 **JADE (Joint Agentic Dynamic Execution)** 框架，旨在解决动态Agentic RAG中“战略-操作不匹配”的问题。这不仅仅是应用现有技术，而是提出了一种新的系统架构和优化方法，属于构建和改进LLM智能体的核心贡献。 2.  **明确涉及多智能体系统**： 摘要中明确指出，该框架将系统建模为 **\"a cooperative multi-agent team\"（协作多智能体团队）**。这直接对应了筛选标准中的“多智能体”方向，涉及智能体间的协作与统一。 3.  **包含自我演化机制**： 论文强调了 **\"executors evolve to align with high-level strategic intent\"**（执行器演化以与高层战略意图保持一致）以及 **\"co-adaptation\"**（协同适应）。这种通过端到端学习和反馈机制使组件（智能体）自我完善、迭代优化的过程，完全符合“自我演化”的定义。 4.  **符合Agentic规划与工具使用**： 论文讨论了 **\"central planner\"（中央规划器）** 与 **\"executors\"（执行器）** 的联合优化，涉及智能体的规划能力和工具使用能力的协同，属于Agentic AI的核心范畴。 5.  **不触犯排除标准**： 该论文不是单纯将LLM应用于特定领域的非演化型应用，也不关注安全、对齐或多模态视觉技术。其核心在于智能体系统的架构优化和演化机制。 综上所述，JADE论文通过引入多智能体协作和自我演化机制来优化Agentic工作流，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决动态 Agentic RAG 中战略规划与操作执行不匹配的问题。针对多轮信息检索场景，我们提出了一种名为 JADE 的统一框架，通过将系统建模为合作多智能体团队并利用共享 LLM backbone，实现了 Planner 与 Executors 的端到端联合优化。我们在七个开放域 QA benchmark 上通过 F1 Score 验证了其有效性，结果显示 JADE 显著优于现有基线并取得了 SOTA 性能。",
                    "summary_translation": "检索增强生成 (RAG) 的演变已从静态检索管道转向动态的智能体工作流，其中由中央规划器编排多轮推理。然而，现有范式面临着一个关键的二分法困境：它们要么在僵化的固定图架构内对模块进行联合优化，要么在赋予动态规划能力的同时，将执行器视为冻结的黑盒工具。我们发现，这种 *解耦优化* (decoupled optimization) 导致了“战略-操作不匹配”，即由于局部执行器未能适应，复杂的规划策略无法有效落地，往往导致系统复杂性增加的同时性能收益为负。在本文中，我们提出了 **JADE** (**J**oint **A**gentic **D**ynamic **E**xecution，联合智能体动态执行)，这是一个用于在动态多轮工作流中对规划和执行进行联合优化的统一框架。通过将系统建模为在单一共享骨干网络下统一的协作多智能体团队，JADE 实现了由基于结果的奖励驱动的端到端学习。这种方法促进了 *协同适应* (co-adaptation)：规划器学会在执行器的能力边界内运作，而执行器则不断进化以与高层战略意图保持一致。实证结果表明，JADE 将原本分离的模块转化为一个协同系统，通过联合优化带来了显著的性能提升，并通过动态工作流编排实现了效率与效果之间的灵活平衡。",
                    "inspiration_trace": "基于对论文《JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 宏观观察与问题引入\n\n**1. 领域演进背景**\n作者首先观察到 RAG（检索增强生成）技术正在经历范式转移：从早期的“静态检索流水线”向“动态智能体工作流”演进。现在的系统不再只是简单的检索-生成，而是由中央规划器主导的多步推理过程。\n\n**2. 现有范式的“讲故事”逻辑**\n作者通过梳理现有文献，构建了一个关于“现有技术为何不足”的演进故事，指出了当前研究面临的**二元对立困境**：\n\n*   **困境一：静态联合优化的僵化**\n    *   早期模块化 RAG（如 MMOA-RAG）虽然实现了模块间的联合优化，但受限于固定的计算图。\n    *   **后果：** 这种“一刀切”的工作流缺乏适应性，无法处理需要复杂推理路径的多跳问题。\n\n*   **困境二：动态解耦优化的脱节**\n    *   为了解决僵化问题，新方法引入了中央规划器来动态编排工作流（如 MAO-ARAG）。\n    *   **核心冲突：** 这些系统采用了“解耦训练”策略——只优化规划器，而将执行器视为冻结的黑盒工具。\n    *   **后果：** 这导致了**“战略-操作不匹配”**。规划器制定了高明的战略，但底层的执行器能力跟不上，导致执行失败。尽管系统复杂度增加了，性能却反而下降。\n\n*   **困境三：单体模型的混乱**\n    *   最新的尝试（如 Search-R1）试图将规划、搜索、生成融合到一个端到端的流中。\n    *   **后果：** 虽然去除了模块限制，但缺乏结构先验，导致训练极不稳定，模型难以收敛。\n\n**3. 总结研究问题**\n基于上述故事，作者显式提出了本文试图解决的核心问题：\n\n> **“如何在动态智能体 RAG 系统中，既保持工作流的动态灵活性，又实现规划器与执行器的联合优化，从而消除‘战略-操作不匹配’？”**\n\n---\n\n### 二、 逻辑推演与假设形成\n\n**1. 深度剖析痛点**\n作者意识到，单纯增加模型规模或改进规划算法无法解决问题。根本原因在于**“割裂”**：规划器不知道执行器能干什么，执行器不知道规划器想干什么。这种信息不对称导致了系统整体效能的内耗。\n\n**2. 提出核心假设**\n为了解决“不匹配”，作者提出了一个假设：\n*   如果让规划器和执行器在同一个目标下**协同进化**，规划器会学会在执行器的能力边界内制定计划，而执行器会进化去迎合规划器的战略意图。\n*   这种“协同适应”是打破僵局的关键。\n\n**3. 方法论构思**\n如何实现协同进化？\n*   **思路来源：** 借鉴多智能体强化学习（MARL）中的“合作博弈”思想。\n*   **关键设计：** 不再使用独立的模型，而是采用**“单一共享骨干”**。即，规划器和所有执行器共用同一个 LLM 的参数，只是通过不同的 Prompt 来区分角色。\n*   **机制原理：** 这样一来，执行器在执行任务时产生的梯度，会直接更新规划器的参数；反之亦然。这强制了它们必须互相理解、互相配合。\n\n---\n\n### 三、 方法论构建\n\n**1. 形式化框架**\n作者将动态 RAG 过程建模为一个**“多智能体半马尔可夫决策过程”**。\n*   **全局状态：** 记录推理历史。\n*   **观察空间：** 每个智能体根据角色只看局部上下文。\n*   **动作空间：** 分为规划动作（生成工作流图）和执行动作（具体检索、生成）。\n\n**2. 定义协同机制**\n*   **统一经验回放：** 将规划器和执行器的交互数据全部打平，放入同一个经验池。\n*   **联合奖励：** 设计一个全局奖励信号（最终答案质量 - 成本）。无论规划器还是执行器，都只对这个全局奖励负责。这解决了长链推理中的“信用分配”问题。\n\n---\n\n### 四、 验证与结论\n\n**1. 预期验证**\n作者设计了实验来验证其逻辑链的闭环：\n*   **对比解耦系统：** 证明 JADE 优于 MAO-ARAG，验证“联合优化”确实解决了“战略-操作不匹配”。\n*   **对比静态系统：** 证明 JADE 优于 MMOA-RAG，验证“动态工作流”的必要性。\n*   **对比大模型：** 证明一个经过协同训练的 7B 小模型团队，可以击败使用 GPT-4o 作为冻结执行器的系统。这有力地证明了**“协同效应比单纯的模型规模更重要”**。\n\n**2. 最终结论**\n通过 JADE 框架，作者成功地将原本割裂的模块转变为一个协同的系统，实现了在动态环境下的端到端稳定优化，填补了战略规划与操作执行之间的鸿沟。"
                },
                {
                    "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
                    "arxiv_id": "2601.21872",
                    "authors": "Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp",
                    "summary": "Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: *   这篇论文的核心贡献是构建了一个名为 **WebArbiter** 的新框架，这是一个专门为 **Web Agents**（Web智能体）设计的 **Process Reward Model (过程奖励模型)**。 *   论文旨在解决智能体在长视界、序列决策任务中面临的监督稀疏和延迟问题。这直接属于 **构建和改进 LLM 智能体** 的范畴，而非仅仅将智能体作为工具应用到特定领域（如医疗、法律等非演化型应用）。 2.  **正面指标 (高度匹配)**: *   **核心范式**: 论文明确聚焦于 `LLM-based Agents` (Web Agents)。 *   **智能体能力**: 论文重点涉及智能体的 `Planning`（规划）和复杂任务中的多步推理。它通过提供结构化的理由和偏好结论来辅助智能体进行决策，这属于提升智能体自主决策能力的关键技术。 *   **演化机制**: 论文采用了 `Reinforcement Learning` (强化学习) 来纠正教师偏差，并使用 `Reward-Guided Trajectory Search`（奖励引导的轨迹搜索）来优化智能体的行为路径，这体现了智能体通过反馈进行迭代和改进的机制。 3.  **排除标准 (无冲突)**: *   论文的主要贡献不在于安全、对齐、可解释性（虽然提到了产生理由，但目的是为了更好的奖励建模而非单纯的可解释性研究）或多模态模型架构本身（视觉仅作为Web环境感知的一部分，非核心研究点）。 4.  **特殊情况处理**: *   **推理/规划**: 论文讨论的是智能体在复杂 Web 环境下的多步推理和决策，而非单纯提升 LLM 基础的数学或逻辑 Token 预测能力。WebArbiter 作为奖励模型，是智能体在执行过程中进行自我评估和规划的重要组件，完全符合 Agentic AI 的研究焦点。 **结论**: 该论文提出了一种改进 Web 智能体决策过程的新方法（WebArbiter），属于单智能体方向中关于规划与评估机制的创新，符合“构建、改进或演化 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决Web Agent在长视距、不可逆决策中缺乏有效过程监督的问题。针对Web导航任务中现有奖励模型信号稀疏或脆弱的场景，我们提出了一种名为WebArbiter的基于原则推理的Process Reward Model，通过推理蒸馏和强化学习两阶段训练，生成结构化论证和偏好判决。并在WEB PRMBENCH基准上通过Pairwise Accuracy和Best-of-N Accuracy验证了其有效性，显著优于GPT-5等基线模型。",
                    "summary_translation": "Web agents (Web智能体) 在自动化复杂计算机任务方面具有巨大潜力，然而其交互过程涉及包含不可逆动作的 long-horizon (长视界) sequential decision-making (序列决策)。在这种设置下，outcome-based supervision (基于结果的监督) 信号稀疏且延迟，往往会奖励错误的轨迹，且无法支持 inference-time scaling (推理时扩展)。这激发了对用于网页导航的 Process Reward Models (过程奖励模型) 的使用，但现有方法仍存在局限：scalar (标量) WebPRMs (Web过程奖励模型) 将进展坍缩为粗糙且缺乏依据的信号，而 checklist-based (基于检查表) WebPRMs 依赖于脆弱的 template matching (模板匹配)，这在布局或语义发生变化时会失效，且常将表面正确的动作误标为成功，几乎无法提供洞察或可解释性。为了应对这些挑战，我们提出了 WebArbiter，这是一个 reasoning-first (推理优先)、principle-inducing (原则诱导) 的 WebPRM，它将 reward modeling (奖励建模) 表述为 text generation (文本生成) 任务，产生结构化的论证，并以 preference verdict (偏好裁决) 作为结论，同时识别出在当前上下文中最有利于任务完成的动作。训练遵循一个两阶段流程：reasoning distillation (推理蒸馏) 赋予模型连贯的原则引导推理能力，而 reinforcement learning (强化学习) 通过将裁决直接与正确性对齐来纠正教师偏差，从而实现更强的 generalization (泛化) 能力。为了支持系统性评估，我们发布了 WebPRMBench，这是一个跨越四种多样化网络环境的 comprehensive benchmark (综合基准)，包含丰富的任务和高质量的 preference annotations (偏好标注)。在 WebPRMBench 上，WebArbiter-7B 以 9.1 分的优势超越了最强的 baseline (基线) 模型 GPT-5。在 WebArena-Lite 的 reward-guided trajectory search (奖励引导的轨迹搜索) 中，它超越了以往最佳的 WebPRM 高达 7.2 分，凸显了其在现实世界复杂网络任务中的 robustness (鲁棒性) 和实用价值。",
                    "inspiration_trace": "基于对论文《WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的“漏斗式”逻辑，旨在引出研究的必要性：\n\n1.  **宏观愿景与核心矛盾**：\n    *   **愿景**：Web智能体有潜力自动化复杂的计算机任务。\n    *   **矛盾**：Web交互具有**长视界**、**序列决策**和**不可逆性**（例如提交错误的表单无法撤回）。这意味着智能体必须在交互过程中保持可靠，而不能仅依赖最终结果。\n\n2.  **现有监督信号的失效**：\n    *   传统的**结果奖励模型**提供的反馈是**稀疏**且**延迟**的。\n    *   **后果**：ORM可能会错误地奖励错误的轨迹（因为中间步骤错了但结果碰巧对了，或者反之），且无法支持推理时的扩展策略（如基于奖励的搜索）。\n\n3.  **现有解决方案的局限性**：\n    *   为了解决ORM的问题，学界引入了**过程奖励模型**。但现有的WebPRMs存在两类缺陷：\n        *   **标量WebPRM**：将进度压缩为一个粗略的数值，缺乏可解释性，且与具体情境的关联较弱。\n        *   **基于清单的WebPRM**：依赖脆弱的模板匹配。一旦网页布局或语义发生变化，它们就会失效；且容易将“表面正确”的动作误判为成功，缺乏对深层逻辑的洞察。\n\n4.  **核心需求与切入点**：\n    *   现有方法缺乏显式的推理能力，容易受到表面相关性的干扰。\n    *   **结论**：我们需要一个**推理优先**的WebPRM，它能够验证进度、抵抗表面偏差，并提供可解释的链条来诊断错误。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个过程奖励模型，使其能够超越粗略的数值打分或脆弱的模板匹配，通过显式的、原则引导的推理过程，为Web智能体提供鲁棒、可解释且能泛化到动态网页环境的步骤级监督信号？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n以下是从观察到方法论形成的完整思考过程：\n\n#### 1. 观察与痛点分析\n*   **观察**：Web环境是动态的，同一个任务在不同网站上的布局可能完全不同。现有的“打分”模型像是一个黑盒，而“清单”模型像是一个死板的教条主义者。\n*   **痛点**：当网页布局改变时，清单失效；当动作看起来合理但实际偏离目标时，标量模型无法区分。我们需要一个像人类专家一样的“裁判”，它不仅看动作对不对，还能说出“为什么”。\n\n#### 2. 核心假设\n*   **假设**：一个好的奖励信号不应仅仅是一个数字，而应该是一段**结构化的论证**。\n*   **推论**：如果模型能够根据当前任务指令和页面状态，动态推导出判断原则，并基于这些原则对候选动作进行推理，那么它就能抵抗布局变化和表面干扰。\n\n#### 3. 方法论构思：从“打分”到“生成”\n*   **范式转移**：将奖励建模从回归任务（预测分数）或分类任务（匹配清单）转变为**文本生成任务**。\n*   **具体形式**：模型输入是上下文和候选动作，输出不是分数，而是一段包含“原则推导 -> 动作分析 -> 最终裁决”的结构化理由。\n\n#### 4. 训练策略的演进：如何教会模型“讲道理”？\n*   **挑战**：直接让指令微调的模型生成推理往往产生肤浅、不一致的借口。\n*   **阶段一：推理蒸馏**\n    *   *思考*：我们需要一个更强的老师来示范如何思考。\n    *   *策略*：利用强模型（如GPT-4/o1等）生成高质量的、包含原则引导的推理链，让学生模型模仿这种思维模式。这赋予了模型“原则意识”。\n*   **阶段二：强化学习（RL）**\n    *   *思考*：老师虽然会推理，但可能有偏见或错误。蒸馏只是学会了“像老师一样说话”，我们需要模型“像真理一样判断”。\n    *   *策略*：引入RL，直接以最终裁决的正确性作为奖励信号。这修正了老师的偏差，确保推理最终导向正确的结果。\n\n#### 5. 评估与验证：如何证明“推理”比“打分”好？\n*   **思考**：现有的评估基准可能过于简单，无法区分“死记硬背”和“真正推理”。\n*   **行动**：构建 **WEB PRMBENCH**。\n    *   *设计*：覆盖4个不同的Web环境（包括真实网站和企业场景），包含高难度的负样本。\n    *   *指标*：不仅看成对准确率，更看重 **Best-of-N (BoN) Accuracy**，因为BoN要求模型在多个干扰项中始终保持正确，这更能反映推理的鲁棒性。\n\n#### 6. 最终闭环\n*   **结果验证**：WebArbiter在BoN指标上大幅超越GPT-5和之前的SOTA（WebShepherd），证明了“原则引导的推理”确实比“模板匹配”更能适应复杂多变的Web世界。\n\n---\n\n**总结**：作者的思考路径是从**Web任务的不可逆性**出发，批判了**现有奖励模型的粗糙性**，提出了**“奖励即推理”**的新范式，并通过**“先学思维（蒸馏）后校准结果（RL）”**的两阶段训练策略，最终实现了一个既鲁棒又可解释的Web智能体裁判。"
                },
                {
                    "title": "Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems",
                    "arxiv_id": "2601.21742",
                    "authors": "Ruiwen Zhou, Maojia Song, Xiaobao Wu, Sitao Cheng, Xunjian Yin, Yuxi Xie, Zhuoqun Hao, Wenyue Hua, Liangming Pan, Soujanya Poria, Min-Yen Kan",
                    "summary": "Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心判断**: *   论文的核心贡献是提出了一种名为 **Epistemic Context Learning (ECL)** 的新框架，旨在解决 LLM-based Multi-Agent Systems (MAS) 中智能体盲目信任不可靠同伴的问题。 *   这属于构建和改进 LLM 智能体系统的范畴，特别是针对多智能体交互机制的优化，而非单纯的应用或基础设施研究。 2.  **正面指标匹配**: *   **多智能体**: 论文明确研究 \"LLM-Based Multi-Agent Systems\"，关注智能体之间的交互和信任建立。 *   **智能体能力**: 论文引入了 \"history-aware reference\"（历史感知参考）和 \"peer profiles\"（同伴档案），这直接对应了智能体的 **Memory**（记忆）能力。同时，ECL 框架涉及评估同伴可靠性并据此做出决策，这是一种高级的推理和决策机制。 *   **协作与通信**: 论文研究智能体如何从同伴中学习以及如何识别可信的同伴，这属于多智能体协作和社会学习的核心议题。 3.  **排除标准检查**: *   **安全与对齐**: 虽然论文提到了 \"sycophancy\"（阿谀奉承）这一通常与对齐相关的术语，但论文的出发点并非通过训练数据或人类反馈来消除模型本身的阿谀奉承，而是将其视为多智能体环境中的一种干扰因素，通过设计智能体的**交互框架和推理机制**来规避。因此，其本质是 Agentic AI 的架构设计，而非 Safety/Alignment 研究。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 *   **非演化型应用**: 论文提出的是通用的多智能体交互框架，而非针对特定垂直领域（如医疗、金融）的应用落地。 **结论**: 该论文通过引入历史记忆和同伴可靠性评估机制，显著改进了多智能体系统的鲁棒性和协作效率，是对 LLM 智能体（特别是多智能体方向）的重要方法论贡献，因此予以保留。",
                    "summary2": "本文旨在解决LLM多智能体系统中智能体缺乏认知自主性、易受不可靠同伴误导的问题。针对包含历史交互记录的多智能体协作场景，我们提出了一种Epistemic Context Learning (ECL)框架，该框架通过两阶段结构化推理显式解耦信任估计与信息聚合，并利用辅助奖励进行强化学习优化。我们在MMLU-Pro和GPQA数据集上通过准确率验证了其有效性，显著提升了模型识别可靠同伴及抗干扰的能力。",
                    "summary_translation": "多智能体 (MA) 系统中的单个智能体往往缺乏鲁棒性，倾向于盲目顺从具有误导性的同伴。我们表明，这种弱点源于谄媚以及评估同伴可靠性的能力不足。为解决这一问题，我们首先形式化了历史感知参考的学习问题，将同伴的历史交互作为额外输入引入，从而使智能体能够在不确定的情况下估计同伴可靠性，并向值得信赖的同伴学习。这将任务重心从评估同伴的推理质量转移到了基于交互历史来估计同伴的可靠性。随后，我们开发了认识论上下文学习：一种基于历史显式构建的同伴画像进行预测的推理框架。我们进一步利用辅助奖励通过强化学习对 ECL 进行了优化。实验结果表明，我们的 ECL 能够使 Qwen 3-4B 等小模型通过准确识别可靠的同伴，在性能上超越规模为其 8 倍的历史不可知基线模型（Qwen 3-30B）。此外，ECL 还将前沿模型的性能提升至近乎完美（100%）。我们表明，ECL 能够很好地泛化至各种多智能体 (MA) 配置；同时我们发现，大语言模型 (LLMs) 能够很好地对信任进行建模，这揭示了信任建模准确度与最终答案质量之间存在强相关性。",
                    "inspiration_trace": "基于对论文《Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观背景与问题引入\n\n**1. 现状与机遇**\n大语言模型（LLM）在复杂推理和智能体自动化方面取得了显著成功，这推动了多智能体系统（MAS）在现实世界中的广泛应用。在理想状态下，多个智能体协作能够通过信息聚合提升整体性能。\n\n**2. 核心冲突**\n然而，一个关键的脆弱性浮现出来：**个体智能体在面临不可靠的同伴时，往往无法保持“认知自主性”。**\n*   **现象：** 现有的LLM倾向于盲目从众，容易被看似合理但具有误导性的信息所左右（即“谄媚”现象）。\n*   **深层原因：** 现有的多智能体聚合方法大多是“历史无关”的。它们要求智能体仅根据当前轮次的回答内容来判断对错。当智能体自身的内部知识不足以验证当前观点时，这种机制就会失效——一个自信但产生幻觉的解释往往会压倒简短但正确的答案，因为模型只能依赖“表面合理性”作为信号。\n\n**3. 视角的转换**\n作者指出，与其试图通过大幅修改底层模型来直接减少谄媚或增强领域知识（这很难），不如从系统层面寻找替代方案。\n*   **逻辑推演：** 当无法从当前的交互内容中确立正确性时，问题的本质自然地从**“评估说了什么”**转移到了**“评估是谁在说”**。\n*   **核心洞察：** 历史可靠性提供了一个比单一推理链的有效性更容易推断的“认知先验”。对于不确定的智能体而言，根据过往表现来区分同伴，比试图独立推导正确答案是一个更稳健的决策标准。\n\n---\n\n### 二、 研究问题\n\n基于上述背景与冲突，作者试图回答的核心研究问题是：\n\n**“如何使LLM智能体能够利用历史交互数据来建立对同伴可靠性的认知信任，并基于这种信任进行自适应的信息聚合，从而克服多智能体系统中的盲目从众与表面误导？”**\n\n---\n\n### 三、 逻辑演进与思想脉络\n\n作者从观察到方法论的演进过程可以概括为以下四个阶段：\n\n#### 第一阶段：诊断分析—— 为什么现有方法不行？\n*   **观察：** 尽管引入了多智能体上下文，标准训练（如RL）下的模型性能提升有限，且在面对不可靠同伴时极其脆弱。\n*   **实验验证：** 作者设计了“身份翻转”和“全错”测试。\n    *   **发现1（缺乏历史信任）：** 模型并没有真正利用历史记录来建立信任。当测试时同伴身份互换（历史可靠的变不可靠），模型性能并未显著下降，说明它们只是根据当前轮次的表面线索（如推理长度、语气）进行“捷径学习”。\n    *   **发现2（缺乏认知自主）：** 当所有同伴都给出错误答案时，模型性能崩盘，说明模型过度依赖外部聚合，丧失了独立判断能力。\n*   **结论：** 仅仅将历史和当前信息一股脑丢给模型是不够的，模型会被当前上下文的显著性所干扰，忽略历史证据。\n\n#### 第二阶段：假设提出—— 如何强制模型关注历史？\n*   **假设：** 如果在架构上强制模型在看到当前答案之前，必须先处理历史信息，就能防止“捷径学习”，迫使模型从历史中提取信任度。\n*   **设计思路：** 受人类认知过程中建立声誉的启发，需要将“可靠性估计”与“信息聚合”这两个过程在结构上解耦。\n\n#### 第三阶段：方法论构建—— ECL框架的诞生\n基于上述假设，作者提出了 **Epistemic Context Learning (ECL)** 框架，其核心思想是**结构化解耦**：\n\n*   **架构设计（两阶段推理）：**\n    *   **阶段一（认知信任估计）：** 模型**只能**看到历史交互 $H_j$，被要求输出一个“同伴信任档案”。此时故意屏蔽当前问题 $Q_j$ 和同伴回答 $R_j$，充当信息瓶颈，迫使模型压缩历史证据。\n    *   **阶段二（信任感知聚合）：** 模型接收阶段一输出的信任档案、当前问题及同伴回答，基于这个显式的先验知识生成最终答案。\n*   **优化策略（辅助监督）：**\n    *   **问题：** 仅用最终答案的正确性作为奖励来反向优化阶段一太间接且稀疏，模型很难学会如何提取信任。\n    *   **解决方案：** 引入**同伴识别奖励（PRR）**。在阶段一显式要求模型指出“最可靠的同伴是谁”，如果判断正确则给予奖励。这种密集的监督信号直接指导了信任建模的学习。\n\n#### 第四阶段：验证与反思—— 方法有效吗？\n*   **验证逻辑：**\n    *   如果ECL真的利用了历史信任，那么在“身份翻转”测试中，性能应该大幅下降（因为它会信任那个历史上可靠但当前错误的同伴）。实验结果证实了这一点，证明了模型确实在利用历史先验，而非当前表面线索。\n    *   小模型（如Qwen-4B）通过ECL可以超越大得多的历史无关基线模型，证明了利用历史信任比单纯增加参数量或盲目聚合更有效。\n*   **进一步思考：** 作者还探讨了如何防止过度依赖（如引入解耦信念DB），体现了对“信任”这一概念的辩证思考——信任是工具，而非盲从。\n\n---\n\n### 总结\n\n作者的思考路径是一个典型的**“发现问题 -> 归因分析 -> 转换视角 -> 架构约束 -> 信号优化”**的过程。\n\n1.  **发现问题：** 多智能体系统里，LLM容易被自信的骗子带跑偏。\n2.  **归因分析：** 因为模型只看“内容”不看“人”，且容易被当前信息吸引，忽略了历史记录。\n3.  **转换视角：** 既然判断内容很难，那就判断“人”的可靠性（历史先验）。\n4.  **架构约束：** 设计两阶段ECL，强行把“看历史”和“看当前”分开，逼模型去学信任。\n5.  **信号优化：** 加个辅助奖励（PRR），手把手教模型谁是好人。"
                },
                {
                    "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
                    "arxiv_id": "2601.21714",
                    "authors": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov, Jie Li",
                    "summary": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: 该论文的核心贡献是提出了 **E-mem**，这是一个专门用于解决 **LLM智能体记忆** 问题的框架。它并非将智能体作为工具应用于某个垂直领域（如医疗或金融），而是致力于改进智能体本身的架构和能力（即记忆机制），因此完全符合“构建、改进LLM智能体”的核心目标。 2.  **正面指标 (高度匹配)**: *   **多智能体**: 论文明确采用了 **Multi-agent based** 方法，构建了一个包含“中央主控智能体”和“多个辅助智能体”的异构分层架构。这直接对应了研究焦点中的“多智能体”方向。 *   **智能体能力**: 论文重点解决了 **Memory**（记忆）问题，并涉及 **Planning**（主控智能体负责全局规划）和 **Reasoning**（辅助智能体进行局部推理）。这些都是Agentic AI的核心能力。 3.  **排除标准 (未触发)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络（虽然摘要提到将上下文压缩成图是现有方法的缺点，但本论文并非研究图技术本身，而是提出替代方案）。 *   它不是基础设施或部署优化研究。 4.  **特殊与模糊情况 (处理得当)**: *   论文提到的推理是智能体层面的推理，即辅助智能体在激活片段内进行局部推理以提取证据，这属于Agentic的规划与推理范畴，而非单纯提升LLM基础Token预测能力的非Agentic推理。 综上所述，这篇论文通过多智能体协作机制改进了LLM智能体的记忆与推理能力，属于多智能体系统与单智能体能力增强的交叉研究，完全符合筛选要求。",
                    "summary2": "本文旨在解决LLM智能体记忆中因预处理导致的破坏性去语境化问题，以支持System 2推理。针对长上下文推理场景，我们提出了一种基于Episodic Context Reconstruction的E-mem框架，采用异构分层Master-Assistant架构，由Assistant代理维护未压缩记忆并进行局部推理。我们在LoCoMo和HotpotQA基准上通过F1分数和Token成本验证了其有效性，结果显示E-mem超越了SOTA基线并显著降低了推理成本。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观背景与观察：从生成到“System 2”的演进\n*   **观察起点**：作者观察到LLM正在从单纯的随机文本生成器，演变为具备规划和工具使用能力的自主智能体。\n*   **趋势判断**：这种演进标志着智能体正在向**System 2推理**转变——即一种深思熟虑的、高精度的、序列化的解决问题模式。\n*   **核心需求**：System 2推理不同于直觉反应，它要求在长周期的动态环境中，必须保持**严格的因果逻辑完整性**。这意味着智能体不能只看当下，必须维护并理解长期的历史交互。\n\n### 2. 问题引入：Introduction 中的“讲故事”逻辑\n作者在引言中通过层层递进的矛盾冲突，构建了研究的必要性：\n\n1.  **理想与现实的冲突**：为了实现System 2推理，智能体需要维护大量的历史记录以保持逻辑完整性。然而，单纯扩展上下文窗口会触发“迷失在中间”现象，导致关键信息被忽略。\n2.  **现有方案的局限性**：为了解决窗口限制，主流范式转向“记忆预处理”。即通过向量嵌入、知识图谱或分层归档，将原始非结构化上下文映射为预定义的固定结构。\n3.  **致命缺陷的发现**：作者指出这种预处理策略导致了**“破坏性的去语境化”**。\n    *   *机制*：将复杂的序列依赖关系压缩成刚性的几何点（向量）或图结构。\n    *   *后果*：这种压缩切断了深度推理所必需的上下文完整性。\n4.  **最终困境**：现有的记忆系统难以重建复杂的因果链，也无法在原始序列语境中理解记忆，导致在信息密集型基准测试（如LoCoMo）上表现不佳。\n\n### 3. 研究问题\n基于上述矛盾，作者提出了本论文试图解决的核心问题：\n\n**“如何设计一种记忆机制，使其能够保留System 2推理所需的完整序列上下文和逻辑完整性，从而克服传统记忆预处理范式固有的‘破坏性去语境化’问题？”**\n\n### 4. 思想演进与假设形成\n为了回答上述问题，作者的思考路径经历了以下关键转折：\n\n*   **灵感来源：生物记忆机制**\n    *   *思考*：人类是如何记忆的？我们不是在大脑里存一个压缩包，而是通过“印迹”进行**“再体验”**。\n    *   *假设*：记忆不应是静态的检索，而应是**情景上下文的重构**。\n\n*   **核心范式转移**\n    *   *旧范式*：Memory Preprocessing（预处理压缩）。\n    *   *新范式*：**Episodic Context Reconstruction（情景上下文重构）**。即保留原始的、未压缩的上下文，仅在需要时进行“激活”和“重构”。\n\n*   **工程化挑战与架构假设**\n    *   *挑战*：保留原始上下文意味着巨大的计算成本和上下文窗口压力，如何实现？\n    *   *假设*：将“高层规划”与“底层记忆保留”解耦。\n    *   *架构构想*：**异构分层的主从架构**。\n        *   **Master Agent（主控）**：只负责全局规划和最终答案合成，不背负原始记忆的存储负担。\n        *   **Assistant Agents（助手）**：作为记忆节点，由小模型（SLM）担任。它们维护原始的情景上下文，并在被激活时进行**局部推理**。\n\n### 5. 方法论逻辑链的最终闭环\n基于上述假设，作者构建了E-mem的具体逻辑闭环：\n\n1.  **存储阶段**：不再进行向量化压缩，而是通过滑动窗口将输入流切分为离散的、有重叠的**情景上下文**，由不同的助手Agent持有。\n2.  **检索阶段**：放弃单一的向量检索，采用**多路径路由**（全局对齐、语义关联、符号触发），确保能精准唤醒相关的记忆单元。\n3.  **推理阶段**：这是核心创新点。助手Agent不是被动地吐出文本块，而是基于其持有的原始上下文进行**“再体验”和局部推理**，提取出经过逻辑验证的证据。\n4.  **合成阶段**：主控Agent收集各助手推理出的局部证据，解决冲突（如时间先后），合成最终的全局逻辑链。\n\n### 6. 总结：作者的思考全景\n作者从**System 2推理对逻辑完整性的高要求**出发，敏锐地发现了现有RAG和记忆系统因**“去语境化”压缩**而导致逻辑断裂的痛点。受**生物记忆“再体验”**的启发，他们提出用**“情景重构”**替代“预处理”。为了解决长上下文的计算瓶颈，他们创造性地设计了**主从异构架构**，利用小模型分担记忆检索与局部推理的任务，从而在保证逻辑深度的同时实现了成本的可控性。"
                },
                {
                    "title": "RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems",
                    "arxiv_id": "2601.21609",
                    "authors": "Bingqian Li, Xiaolei Wang, Junyi Li, Weitao Li, Long Zhang, Sheng Chen, Wayne Xin Zhao, Ji-Rong Wen",
                    "summary": "Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合要求**: 论文提出了 RecNet，这是一个“self-evolving preference propagation framework”（自我演化偏好传播框架）。其核心在于构建了一个基于 LLM 的智能体系统，而非仅仅将 LLM 作为工具应用。 2.  **符合多智能体方向**: 摘要中明确提到使用了“router agents”（路由智能体）以及“multi-agent reinforcement learning framework”（多智能体强化学习框架）来处理用户和物品之间的交互。这直接对应了筛选标准中的“Multi-Agent”方向。 3.  **符合自我演化方向**: 论文标题和摘要多次强调“Self-Evolving”。具体而言，其“backward phase”（后向阶段）利用 LLMs 进行信用分配和模块级优化，实现了传播策略的“continuous self-evolution”（连续自我演化）。这完全符合“Self-Evolving”的研究焦点。 4.  **特殊情况的判定**: 尽管论文的应用场景是推荐系统，但根据筛选标准第四步（自我演化的应用），只要论文的核心贡献是提出一种新的“自我演化”机制（即反馈驱动的传播优化机制），即使应用在特定领域，也应该保留。 5.  **排除项检查**: 论文不涉及安全、对齐、视觉或多模态核心内容。虽然涉及“preference propagation”（通常与图相关），但其核心创新在于利用智能体和 LLM 进行演化，而非图神经网络算法本身，因此不属于“图”排除项的范畴。",
                    "summary2": "本文旨在解决现有Agentic Recommender Systems依赖显式交互更新偏好导致滞后和稀疏的问题。针对动态推荐场景，我们提出了一种名为RecNet的自进化偏好传播框架，通过引入Router agents实现Centralized Preference Routing和Personalized Preference Reception，并利用Feedback-driven Propagation Optimization进行策略自进化。在Amazon review数据集上，通过NDCG@K指标验证了其有效性。",
                    "summary_translation": "Agentic recommender systems (代理推荐系统) 利用 Large Language Models (LLMs, 大语言模型) 对复杂的用户行为进行建模，并支持个性化决策。然而，现有方法主要基于 explicit user-item interactions (显式用户-项目交互) 来建模偏好变化，这些交互数据稀疏且充满噪声，无法反映用户与项目之间实时的相互影响。为解决这些局限性，我们提出了 RecNet，这是一个 self-evolving preference propagation framework (自演化偏好传播框架)，能够主动在相关用户和项目之间传播实时偏好更新。RecNet 包含两个互补的阶段。在前向阶段，centralized preference routing mechanism (集中式偏好路由机制) 利用 router agents (路由代理) 整合偏好更新，并将其动态传播至最相关的 agents (代理)。为确保传播偏好的准确与个性化整合，我们进一步引入了 personalized preference reception mechanism (个性化偏好接收机制)，该机制结合了用于临时缓存的 message buffer (消息缓冲区) 以及可优化的 rule-based filter memory (基于规则的过滤记忆)，旨在根据过往经验和兴趣指导 selective preference assimilation (选择性偏好同化)。在后向阶段，feedback-driven propagation optimization mechanism (反馈驱动的传播优化机制) 模拟了 multi-agent reinforcement learning framework (多智能体强化学习框架)，利用 LLMs 进行 credit assignment (信用分配)、gradient analysis (梯度分析) 和 module-level optimization (模块级优化)，从而实现传播策略的持续 self-evolution (自演化)。在多种场景下进行的广泛实验表明，RecNet 在为推荐系统建模 preference propagation (偏好传播) 方面是有效的。",
                    "inspiration_trace": "基于对论文《RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的完整思考过程。\n\n---\n\n### 一、 宏观观察与问题叙事\n\n作者首先从**LLM智能体在推荐系统中的应用现状**切入，构建了如下的逻辑故事：\n\n1.  **现状观察**：现有的基于LLM的智能体推荐系统，主要利用LLM强大的文本理解与生成能力来模拟用户行为和物品属性。\n2.  **核心局限**：这些方法在建模用户偏好变化时，存在一个根本性的依赖——**显式交互**。即，只有当用户点击、购买或评分时，系统才会更新用户或物品的画像。\n3.  **现实落差**：在真实世界中，用户和物品构成了一个动态互联的网络。除了直接的“点对点”交互，偏好更新往往通过**隐式关系**（如社交连接、共同兴趣社区、内容共现）进行传播。例如，一个用户偏好的改变可能会潜移默化地影响相似用户，即使后者没有直接交互。\n4.  **后果分析**：这种“被动等待交互”的模式导致系统更新滞后，且无法捕捉实时、相互影响的动态变化，因为显式交互数据往往是稀疏且充满噪声的。\n\n**由此引出的核心矛盾：** 现实中偏好是流动的、相互影响的，而现有系统是静态的、孤立更新的。\n\n---\n\n### 二、 研究问题\n\n基于上述叙事，作者将复杂的现实矛盾凝练为一个具体的学术问题：\n\n**“我们如何构建一种主动的偏好传播机制，使智能体推荐系统能够在相关用户和物品之间实时传递偏好更新，从而捕捉隐式的相互影响，同时解决传播过程中的可扩展性、相关性过滤及误差控制问题？”**\n\n---\n\n### 三、 思想演进与方法论形成\n\n为了回答上述问题，作者的思考经历了以下五个阶段的逻辑演进：\n\n#### 1. 范式转移：从“被动响应”到“主动传播”\n*   **思考**：既然显式交互太慢且稀疏，为什么不模拟现实社会中的“口碑传播”或“传染效应”？\n*   **假设**：如果当一个用户画像更新时，系统能主动将这种变化推送给相似的用户或物品，就能提前校准他们的偏好，解决滞后问题。\n*   **初步构想**：建立一个偏好传播网络，让信息在节点间流动。\n\n#### 2. 挑战识别：朴素传播的不可行性\n*   **思考**：直接让所有用户/物品互相点对点传播行不行？\n*   **批判**：不行。大规模系统中，全量传播会导致计算爆炸（可扩展性差）；且如果不加筛选地传播，会引入大量噪声，导致画像漂移（相关性差）。\n*   **需求**：需要一个中间层来聚合信息、过滤噪声，并精准分发。\n\n#### 3. 架构灵感：引入“路由器”智能体\n*   **类比**：计算机互联网中，数据包不是直接从每台电脑发给每台电脑，而是通过**路由器**进行中转、聚合和寻址。\n*   **创新点**：将推荐系统中的用户和物品视为“客户端”，引入**路由器智能体**作为中间层。\n*   **逻辑**：路由器负责管理一个社区（相似用户/物品群组）。它聚合群组内的偏好更新，提炼共性，然后精准地回传给群组内的相关节点。这解决了“向谁传播”和“传播什么”的问题。\n\n#### 4. 个性化接收：避免“同质化”灾难\n*   **思考**：即使路由器推送了信息，接收方应该全盘接受吗？\n*   **批判**：如果盲目合并传播信息，所有用户的画像会变得千篇一律，丧失个性化。且对于交互频率低的用户，频繁的更新可能会基于错误信息扭曲其画像。\n*   **机制设计**：设计**个性化偏好接收机制**。\n    *   **缓冲**：暂存传播信息，避免即时干扰。\n    *   **过滤记忆**：基于历史经验，制定规则（如“只接受爵士乐相关的更新”），决定是否采纳传播信息。这保证了传播过程中的“个性”保留。\n\n#### 5. 自我进化：从静态规则到动态优化\n*   **思考**：路由器的数量、路由规则、过滤规则一开始设定好就永远不变吗？\n*   **批判**：环境是动态的，静态规则会失效。系统需要根据反馈自我调整。\n*   **机制设计**：引入**反馈驱动的传播优化**。\n    *   利用LLM的推理能力，将最终的交互反馈（推荐是否成功）转化为“文本梯度”。\n    *   通过多智能体强化学习的逻辑，分析哪个模块（路由器、过滤器）出了错，并生成具体的修改指令（如分裂路由器、修改过滤规则），实现系统的**自我进化**。\n\n---\n\n### 总结\n\n作者的思想路径是从**发现现有系统“被动、孤立”的缺陷**出发，通过**引入网络路由的隐喻**解决了传播的效率与精度问题，进而通过**设计缓冲与过滤机制**保障了个性化，最后利用**LLM的文本反馈能力**实现了系统的闭环进化。RecNet 本质上是一个**受计算机网络启发的、具备自我进化能力的推荐系统社交网络**。"
                },
                {
                    "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
                    "arxiv_id": "2601.21570",
                    "authors": "Zixing Lei, Genjia Liu, Yuanshuo Zhang, Qipeng Liu, Chuan Wen, Shanghang Zhang, Wenzhao Lian, Siheng Chen",
                    "summary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献符合 \"自我演化\" (Self-Evolving) 范畴**： 论文的核心不仅仅是应用LLM去控制机器人，而是提出了一个基准（EmboCoach-Bench）来评估LLM智能体**自主设计具身策略**的能力。论文明确提出了“动态闭环工作流”，智能体利用环境反馈进行“迭代起草、调试和优化”。这直接对应了您筛选标准中的“自我演化”机制，特别是 `Iterative Improvement`（迭代改进）和 `Self-Correction`（自我修正）。 2.  **符合 \"Agentic AI\" 的核心范式**： 论文研究的对象是LLM智能体，且重点在于智能体的工作流。它涉及智能体利用工具（生成可执行代码）、根据反馈进行规划以及自我反思。这属于单智能体的高级能力研究，而非简单的LLM推理。 3.  **符合 \"特殊和模糊情况\" 处理规则**： 虽然论文的应用领域是“具身机器人”，这通常容易被归类为特定领域应用。但是，根据筛选标准第四步第2条：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。本文的核心在于建立智能体如何通过反馈循环自我完善策略的机制（即“自我演化具身智能”），而非单纯解决机器人控制问题。因此，它属于方法论层面的贡献，而非单纯的应用。 综上所述，该论文聚焦于LLM智能体的自我演化与迭代优化机制，属于Agentic AI的前沿研究，符合筛选要求。",
                    "summary2": "本文旨在解决Embodied AI开发中依赖人工调优的瓶颈问题，实现AI代理自主工程化机器人策略。针对32个涵盖RL和IL的机器人任务场景，我们提出了EmboCoach-Bench基准框架，采用基于环境反馈的“Draft-Debug-Improve”闭环工作流。并在ManiSkill、RoboTwin等四个仿真平台上通过任务成功率验证了其有效性，结果显示AI代理平均成功率超越人类基线26.5%。",
                    "summary_translation": "具身智能领域正经历着向通用机器人系统的快速演进，这一进程得益于高保真仿真和大规模数据采集的推动。然而，这种规模化能力仍受到严重制约，因为它依赖于劳动密集型的人工监督，涵盖了从复杂的奖励塑形到跨异构后端的超参数调优等各个环节。受大语言模型在软件自动化和科学发现领域成功的启发，我们提出了 EmboCoach-Bench，这是一个用于评估大语言模型智能体自主设计具身策略能力的基准。该基准涵盖了 32 个由专家精心策划的强化学习和模仿学习任务，我们的框架将可执行代码确立为通用接口。我们超越了静态生成，转而评估一种动态闭环工作流；在该工作流中，智能体利用环境反馈来迭代起草、调试和优化解决方案，其改进范围涵盖从基于物理信息的奖励设计到扩散策略等策略架构。广泛的评估揭示了三个关键见解：(1) 自主智能体的平均成功率在质量上超越了人工设计的基线 26.5%；(2) 结合环境反馈的智能体工作流有效加强了策略开发，并显著缩小了开源模型与专有模型之间的性能差距；(3) 智能体针对病态工程案例表现出自我纠正能力，通过迭代式的仿真在环调试，成功将任务性能从近乎完全失败的状态中挽救回来。最终，这项工作为自我进化的具身智能奠定了基础，加速了具身智能领域从劳动密集型人工调优向可扩展、自主工程化的范式转变。",
                    "inspiration_trace": "基于对论文Introduction部分的深度分析，以下是对作者提出EmboCoach-Bench这一核心方法的逻辑链推演，还原了其从宏观观察到具体方法论的思考过程。\n\n### 一、 Introduction中的“讲故事”逻辑（问题引入）\n\n作者通过一个层层递进的叙事结构，揭示了当前Embodied AI领域存在的核心矛盾：\n\n1.  **宏观背景与机遇：**\n    *   **现象：** 物理智能正在经历“GPT时刻”。以$\\pi^* 0.6$和Gen0为代表的通用机器人策略模型，展示了从专用控制器向可扩展基础模型的范式转变。\n    *   **能力：** 借助流匹配等技术，这些模型在互联网规模的数据集上训练后，展现出了“物理常识”和零样本泛化能力。\n\n2.  **核心矛盾与瓶颈：**\n    *   **现实挑战：** 尽管通用模型强大，但将其部署到无限多样的现实任务中，仍严重受限于**人工任务工程**。\n    *   **“最后一公里”难题：** 即使是最先进的VLA模型，在适配特定硬件动力学时，仍需要大量的监督微调（SFT）和强化学习（RL）。这一阶段需要人工设计反馈信号和安全边界。\n    *   **高维度的复杂性：** 对于高自由度平台（如人形机器人），实现鲁棒的移动操作需要工程化密集、多术语的奖励函数来平衡运动稳定性和操作精度。\n    *   **数据中心的困境：** 即使是以数据为中心的方法（如“RL-for-Data”），也依赖人工专家为每个技能精心设计“教师”策略的学习环境。\n    *   **结论：** 尽管数据驱动技术进步，但具身智能仍无法摆脱“手工艺式工程”的束缚。从奖励塑形到基础设施集成的关键工作流，严重依赖人类直觉，面临根本的可扩展性限制。\n\n3.  **外部趋势与契机：**\n    *   **LLM的进化：** 大语言模型已从被动的文本生成器演变为主动的问题解决者。\n    *   **跨域成功：** 在软件自动化（如SWE-bench）和科学发现（如AlphaEvolve）领域，AI代理已经证明能够导航复杂的代码库并迭代优化系统。\n    *   **启示：** 自动化工程的障碍正在消除，代理已经具备了处理复杂、多阶段工程工作流的潜力。\n\n4.  **范式转换的提出：**\n    *   **核心洞察：** 既然LLM代理能处理复杂的软件和科学工程，那么它们理应能够接管机器人开发中高吞吐量的优化和“最后一公里”适配工作。\n    *   **目标：** 将该领域从“人工手工艺”推向“工业化进化”，实现自主工程代理。\n\n---\n\n### 二、 研究问题\n\n基于上述叙事，作者旨在回答的核心研究问题可总结为：\n\n**“LLM智能体是否能够通过闭环的仿真反馈，自主完成具身策略的全栈工程开发，并在性能上超越人类专家的手动调优？”**\n\n---\n\n### 三、 思考过程的逻辑演进链\n\n为了回答上述问题，作者的思考路径经历了从观察到假设，再到方法论构建的四个阶段：\n\n**阶段 1：观察与痛点识别**\n*   **思考：** 我们有了强大的通用机器人模型，但为什么落地这么难？\n*   **分析：** 难点不在于模型本身，而在于“适配”。将通用模型适配到具体物理环境（如人形机器人、特定机械臂）需要极其繁琐的奖励函数设计、超参数调整和代码调试。这是一个劳动密集型、不可扩展的手工过程。\n\n**阶段 2：跨域类比与假设**\n*   **思考：** 在纯软件领域（如写代码、修Bug），LLM已经表现得像资深工程师。具身AI的开发本质上也是写代码、调参数，只是多了物理反馈。\n*   **假设：** 如果我们将具身AI的开发过程视为一个“代码生成+环境反馈+迭代优化”的循环，那么LLM代理应该能够替代人类完成这一过程，甚至做得更好（因为不知疲倦且能探索更广的参数空间）。\n\n**阶段 3：核心概念定义**\n*   **思考：** 要验证这个假设，不能只让LLM写一段代码片段，必须让它像真正的工程师一样工作。\n*   **定义：** 提出了“自主工程代理”的概念。这不仅仅是代码生成，而是全生命周期的管理：从理解需求（PRD）、编写代码、在仿真中运行、分析物理反馈（如滑移、碰撞）、到Debug和最终优化。\n\n**阶段 4：方法论构建**\n*   **思考：** 现有的基准测试（如SWE-bench）只测软件，不测物理；现有的机器人基准只测策略，不测开发过程。我们需要一个新的测试床。\n*   **设计：**\n    1.  **接口统一：** 将“可执行代码”作为通用接口，连接LLM与物理仿真器。\n    2.  **闭环工作流：** 设计“Draft-Debug-Improve”循环，强制Agent利用环境反馈进行迭代，而不是一次性生成。\n    3.  **基准构建：** 建立**EmboCoach-Bench**，包含32个真实任务，覆盖RL和IL，测试Agent在奖励工程、架构实现和Debug方面的能力。\n*   **验证：** 通过实验验证Agent是否能超越人类基线，从而证明“自主进化”的可行性。"
                },
                {
                    "title": "Meta Context Engineering via Agentic Skill Evolution",
                    "arxiv_id": "2601.21557",
                    "authors": "Haoran Ye, Xuning He, Vincent Arak, Haonan Dong, Guojie Song",
                    "summary": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文提出了 \"Meta Context Engineering (MCE)\" 框架，其核心机制是 \"co-evolving CE skills and context artifacts\"（协同演化上下文工程技能和产物）。这直接对应了您筛选标准中的 **自我演化** 和 **Agentic AI** 范式。论文明确描述了一个双层智能体系统：元级智能体通过 \"agentic crossover\"（智能体交叉操作）和搜索历史来精炼技能，基础级智能体执行技能并从训练轨迹中学习。这是一种典型的智能体自我完善和迭代机制。 2.  **属于构建新框架而非单纯应用**： 根据第一步筛选标准，该论文并非将现有的LLM或智能体框架简单应用于特定领域（如医疗、金融等），而是提出了一种新的 **方法论** 来解决上下文工程中的结构偏差问题。它通过构建智能体来优化智能体（或其输入上下文），属于对LLM智能体能力的构建和改进。 3.  **包含关键正面指标**： 论文中包含了大量您关注的正面指标，如 `Self-Evolving`（自我演化）、`Agentic AI`、`Self-Improvement`（自我改进）、`Generational Evolution`（代际演化，通过迭代体现）以及 `Agentic Crossover`（一种演化算法机制）。 4.  **不涉及排除项**： 论文主要关注智能体的技能演化和上下文优化，不涉及安全对齐、多模态视觉核心研究或图神经网络等排除标准。 综上所述，该论文的核心在于提出一种基于智能体的技能演化机制来优化LLM的上下文，精准契合您关于“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有Context Engineering (CE) 方法依赖手动设计框架导致的结构偏差与优化空间受限问题。针对LLM推理时上下文优化场景，我们提出了一种Meta Context Engineering (MCE) 双层框架，通过Meta-level的Agentic Skill Evolution与Base-level的Fully Agentic Context Optimization实现CE技能与上下文工件的协同进化，并在金融、化学、医学等五个领域的基准数据集上通过准确率、F1分数及相对提升率验证了其有效性。",
                    "summary_translation": "大语言模型的运行效能很大程度上依赖于其推理时上下文。这确立了上下文工程作为优化这些输入的一门正式学科。当前的 CE 方法依赖于手工制作的框架，例如僵化的生成-反思工作流和预定义的上下文模式。它们引入了结构偏差，并将上下文优化限制在一个狭窄的、受直觉束缚的设计空间中。为了解决这一问题，我们提出了元上下文工程，这是一个双层框架，通过协同进化 CE 技能和上下文制品，取代了静态的 CE 启发式方法。在 MCE 迭代中，元级智能体通过智能体交叉来精炼工程技能，这是一种对技能历史、执行过程及评估结果进行审慎搜索的过程。基础级智能体执行这些技能，从训练轨迹中学习，并将上下文优化为灵活的文件和代码。我们在离线和在线设置下的五个迥异领域中对 MCE 进行了评估。MCE 展现出一致的性能提升，相比最先进的智能体 CE 方法实现了 5.6%--53.8% 的相对改进（平均 16.9%），同时在上下文适应性、可迁移性以及上下文使用和训练效率方面保持了卓越表现。",
                    "inspiration_trace": "基于论文《Meta Context Engineering via Agentic Skill Evolution》的内容，以下是对作者核心方法论逻辑链的系统性推演，还原了从宏观观察到具体方法产出的思考过程。\n\n---\n\n### 一、 宏观观察与问题引入\n\n作者首先确立了LLM（大语言模型）效能的一个基本事实：模型的推理能力高度依赖于推理时的上下文。这催生了“上下文工程”这一学科，旨在通过优化输入给模型的上下文来提升性能。\n\n**1. 现状分析：**\n当前的SOTA（最先进）CE方法（如ACE, GEPA等）虽然有效，但都依赖于**人工设计的框架**。这些框架通常包含固定的生成-反思-修正工作流，以及预定义的上下文模式（如列表、图谱等）。\n\n**2. 痛点挖掘：**\n作者指出这种“人工设计”存在根本性的结构性偏差，限制了优化的上限：\n*   **表示层面的偏差：** 不同的上下文结构（如案例轨迹、项目列表、层级图谱）各有优劣，但一旦选定，模型就被锁死在该结构中，无法根据任务特性灵活调整。\n*   **优化层面的偏差：** 现有方法走向两个极端——要么偏向“简洁”（如Prompt重写，导致缺乏细节），要么偏向“冗长”（如累加式策展，导致上下文臃肿和噪声）。\n*   **设计空间的局限：** 这些方法将优化限制在了一个狭窄的、基于人类直觉的设计空间内，无法发现那些超越人类直觉的、针对特定任务的最优策略。\n\n**3. 逻辑推演：**\n既然没有单一的人工框架是通用的，且人工框架存在固有的归纳偏差，那么继续改进这些固定框架只是治标不治本。我们需要一种机制，能够跳出人类预设的“工作流”和“模式”，让AI自己去发现如何构建上下文才是最好的。\n\n---\n\n### 二、 核心研究问题\n\n基于上述观察与痛点，作者提炼出的核心研究问题为：\n\n**“如何超越静态的、人工设计的上下文工程框架，通过让AI自主学习和进化其工程策略，从而发现超越人类直觉的最优上下文构建方法？”**\n\n---\n\n### 三、 思想演进与方法论形成\n\n为了回答上述问题，作者的思考经历了以下四个阶段的演进：\n\n#### 第一阶段：解耦与抽象\n*   **思考：** 现有的CE方法混淆了“策略”与“结果”。它们直接用固定的策略去优化结果（上下文）。\n*   **类比：** 就像机器学习中，我们区分了“模型参数”和“模型架构/训练算法”。在CE中，我们也应该区分：\n    *   **Base-level（结果）：** 具体的上下文内容。\n    *   **Meta-level（策略）：** 如何表示和优化这些上下文的“技能”。\n*   **决策：** 提出双层优化框架。外层寻找最好的“技能”，内层利用该技能构建最好的“上下文”。\n\n#### 第二阶段：定义“技能”作为进化载体\n*   **思考：** 要让AI自主进化，我们需要一个合适的进化单元。之前的进化计算多针对具体的代码或参数，粒度太细。\n*   **创新：** 引入 **\"Agent Skills\"（智能体技能）** 的概念。技能不仅仅是代码，而是一个包含指令、脚本、资源和验证协议的“文件夹”。它是一个高阶的、模块化的抽象，能够完整描述“如何做一件事”。\n*   **优势：** 这种抽象允许AI灵活地组合不同的解决方案层级，实现策略的模块化和解耦。\n\n#### 第三阶段：引入进化机制\n*   **思考：** 如何从无到有或从有到优地生成这些技能？单纯的随机搜索效率太低。\n*   **机制：** 借鉴进化计算，提出 **\"Agentic Crossover\"（智能体交叉）**。这不仅仅是简单的代码拼接，而是一个 deliberative（深思熟虑）的过程。\n*   **逻辑：** Meta-agent 会回顾历史技能库，分析哪些技能在什么任务上成功或失败，然后通过推理，将成功技能的“优点”组合起来，生成新一代的技能。这模拟了人类专家通过经验积累和反思来改进方法论的过程。\n\n#### 第四阶段：赋予完全代理权\n*   **思考：** 如果技能是进化的，那么执行技能的Base-agent必须能够处理极其多样化的指令。如果限制它只能输出文本或固定格式，进化的技能就无法落地。\n*   **决策：** 赋予Base-agent **完全的代理权**，包括编程工具包和文件系统访问权限。\n*   **结果：** 上下文不再是僵化的文本块，而是灵活的**文件和代码**。这意味着AI可以根据进化的技能，动态地决定上下文是数据库、Python脚本、Markdown文档还是复杂的检索逻辑。这彻底打破了预定义模式的限制。\n\n---\n\n### 四、 总结：逻辑闭环\n\n作者的思考过程形成了一个完美的闭环：\n1.  **发现问题：** 人工设计的CE框架存在结构性偏差，限制了性能上限。\n2.  **提出假设：** 如果让AI自己学习“如何优化上下文”（即学习技能），而不是直接优化上下文，就能突破人类直觉的局限。\n3.  **构建方法：** 设计一个双层系统（MCE），Meta层通过进化算法不断迭代出更好的“技能”，Base层利用这些技能和编程工具灵活地构建上下文。\n4.  **验证效果：** 这种“技能”与“上下文”协同进化的方式，最终实现了在不同任务上的自适应、高效且超越SOTA的性能。"
                },
                {
                    "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
                    "arxiv_id": "2601.21526",
                    "authors": "Alireza Nadaf, Alireza Mohammadshahi, Majid Yazdani",
                    "summary": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance. Code Available at: https://github.com/Leeroo-AI/kapso",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: *   该论文的核心贡献是提出了 **KAPSO**，一个用于自主程序合成和优化的**模块化框架**。这完全符合“构建、改进 LLM智能体”的目标。 *   论文明确针对的是 **\"coding agents\" (编码智能体)**，旨在解决智能体在长视距任务中的失败问题，而非仅仅将LLM作为工具应用于特定领域。它提出的是一个通用的智能体架构，而非单一的应用。 2.  **正面指标 (高度匹配)**: *   **Agentic AI / 单智能体**: 论文构建了一个完整的智能体框架，包含规划、执行和评估的闭环。 *   **Planning (规划)**: 论文强调将合成视为 **\"long-horizon optimization loop\" (长视距优化循环)** 中的一个算子，涉及多步推理和迭代优化。 *   **Tool Use (工具使用)**: 集成了 **\"git-native experimentation engine\"**，利用 git 分支进行实验隔离和状态管理，这是典型的工具使用能力。 *   **Memory & Self-Reflection (记忆与自我反思)**: 论文提出了 **\"cognitive memory layer\" (认知记忆层)**，用于存储从实验轨迹中蒸馏出的经验，以减少重复错误。这直接对应了智能体的记忆机制和自我完善能力。 3.  **排除标准 (未触发)**: *   论文主要关注智能体的架构设计（实验引擎、知识系统、记忆层），而非安全、对齐、多模态视觉或基础设施优化。 *   虽然涉及 \"Knowledge-grounded\"，但其目的是为了增强智能体的检索和记忆能力，而非研究知识图谱或图神经网络本身，因此不触犯关于“图”的排除规则。 4.  **特殊与模糊情况处理**: *   论文属于 **Agentic Reasoning/Planning**。它不仅仅是提高LLM的代码生成能力，而是构建了一个包含评估、反馈和迭代的智能体系统来优化程序，符合保留条件。 *   论文包含 **Self-Evolving** 特征。通过从实验反馈中学习并更新记忆，智能体具备了自我完善和迭代的能力。 综上所述，这篇论文提出了一种具有长视距规划、工具使用和记忆反思能力的LLM智能体框架，完全符合关于“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决LLM编码代理在长周期任务中状态丢失和难以复用领域知识的问题。针对自然语言目标和评估边界，我们提出了一种名为KAPSO的模块化框架，集成了基于Git的实验引擎、结构化知识系统和认知记忆层。我们在MLE-Bench和ALE-Bench上通过Medal rate和Final performance等指标验证了其有效性，结果显示其在复杂任务上显著优于现有基线。",
                    "summary_translation": "我们介绍了KAPSO，一个用于autonomous program synthesis (自主程序综合) 和optimization (优化) 的modular framework (模块化框架)。给定一个natural language goal (自然语言目标) 和evaluation method (评估方法)，KAPSO迭代地执行ideation (构思)、code synthesis and editing (代码综合与编辑)、执行、评估和学习，以改进一个runnable artifact (可运行工件)，使其趋向于可测量的目标。KAPSO不将synthesis (综合) 视为终点，而是将其作为long-horizon optimization loop (长视界优化循环) 中的一个operator (算子)，其中进展由evaluator outcomes (评估器结果) 定义。KAPSO通过集成三个tightly coupled components (紧密耦合的组件)，旨在解决coding agents (编码智能体) 中常见的long-horizon failures (长视界失败)，包括lost experimental state (实验状态丢失)、brittle debugging (脆弱的调试) 以及weak reuse of domain expertise (领域专业知识复用薄弱)。首先，一个git-native experimentation engine (基于git的实验引擎) 将每次尝试隔离为一个分支，生成reproducible artifacts (可复现的工件) 并保留迭代过程中的provenance (溯源)。其次，一个knowledge system (知识系统) 摄取heterogeneous sources (异构源)，包括repositories (代码仓库)、internal playbooks (内部操作手册) 和curated external resources (精选的外部资源，如文档、科学论文和网络搜索结果)，并将它们组织成一种structured representation (结构化表示)，支持对workflows (工作流)、implementations (实现) 和environment constraints (环境约束) 的retrieval (检索)。第三，一个cognitive memory layer (认知记忆层) 协调检索并维护一个从experiment traces (实验轨迹，包括run logs (运行日志)、diffs (差异) 和evaluator feedback (评估器反馈)) 中提炼出的reusable lessons (可复用经验) 的episodic store (情景存储)，从而减少repeated error modes (重复的错误模式) 并加速convergence (收敛)。我们在MLE-Bench（Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder heuristic optimization (AtCoder启发式优化)）上评估了KAPSO，并报告了end-to-end performance (端到端性能)。代码可在以下地址获取：https://github.com/Leeroo-AI/kapso",
                    "inspiration_trace": "基于对论文《KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 宏观问题与逻辑演进\n\n#### 1. 宏观观察：从“意图”到“实现”的鸿沟\n*   **起点**：作者首先观察到一个普遍现象——领域专家通常很清楚他们想要构建什么（即“意图”），但将这种意图转化为可靠、可运行且经过优化的软件，仍然是一个充满挑战的过程。\n*   **现实认知**：在实际开发中，成功并非一蹴而就，而是一个**迭代过程**：提出方案 -> 实现 -> 运行 -> 检查结果 -> 修正。这在数据科学和AI领域尤为明显，因为进步依赖于可衡量的改进以及对代码、数据和评估契约的精细管理。\n\n#### 2. 问题聚焦：现有AI代理的“长视界”失效\n*   **现状评估**：虽然基于大语言模型（LLM）的编码代理降低了编写代码的成本，但在**长视界执行循环**中仍然不可靠。\n*   **痛点识别**：作者敏锐地指出，现有的代理往往只能解决“让代码跑通”的狭义问题，而在质量、准确性、鲁棒性或效率上表现不佳。具体表现为三大核心失败模式：\n    1.  **状态丢失**：在迭代过程中丢失了实验上下文。\n    2.  **重复错误**：反复触发相同的集成错误，无法从失败中学习。\n    3.  **知识复用弱**：即使仓库、文档或内部手册中存在相关的工程专业知识，代理也无法有效利用。\n\n#### 3. 核心假设：从“代码生成”转向“过程优化”\n*   **观念转变**：作者提出，真正的优势不在于原始的代码生成能力，而在于**持续应用专家级想法和高杠杆工程工作流**的能力（如环境设置、数据契约、调试程序、性能调优）。\n*   **假设提出**：如果将“程序合成”不再视为终点，而是视为一个**长视界优化循环中的一个算子**，在这个循环中，进步是由评估器的结果（如准确率、效率）定义的，那么就能解决上述问题。\n\n#### 4. 方法论构建：构建一个“有记忆、有知识”的进化系统\n*   **系统设计**：为了验证假设，作者构思了一个模块化框架，必须包含三个紧密耦合的组件来支撑上述优化循环：\n    *   **实验引擎**：为了解决“状态丢失”，需要像Git一样的原生版本控制，隔离每次尝试并保留血统。\n    *   **知识系统**：为了解决“知识复用弱”，需要摄入异构源（仓库、论文、文档），并将其组织为结构化表示，支持工作流和约束的检索。\n    *   **认知记忆层**：为了解决“重复错误”，需要从实验轨迹中提取可复用的经验教训，减少重复的错误模式。\n\n---\n\n### 二、 Introduction 中的“讲故事”逻辑提取\n\n作者在 Introduction 部分通过层层递进的逻辑引入问题，具体链条如下：\n\n1.  **理想与现实的差距**：专家知道要构建什么，但将其转化为可靠、优化的软件仍需反复实验。\n2.  **迭代的本质**：成功的开发是迭代的（提议-实现-运行-检查-修正），特别是在数据/AI领域，进步取决于可衡量的改进和对评估契约的管理。\n3.  **“可用”不等于“好用”**：迭代往往能产生可运行的工件，但在质量、准确性、鲁棒性或效率上仍有欠缺。因此，实际进步需要**重复评估和针对性改进**，而不仅仅是修复错误。\n4.  **现有技术的局限**：LLM编码代理虽然降低了写代码成本，但在长视界执行循环中不可靠。\n5.  **具体的失败症状**：\n    *   跨迭代状态丢失。\n    *   反复触发相同的集成错误。\n    *   即使仓库、文档或内部手册中有相关知识，也无法复用。\n6.  **关键洞察**：决定性优势不是原始代码生成，而是**持续应用专家级想法和高杠杆工程工作流**（环境设置、数据契约、评估工具、调试、性能调优）的能力。\n\n---\n\n### 三、 研究问题\n\n基于上述逻辑推演，本文试图回答的核心研究问题为：\n\n**如何构建一个自主框架，将程序合成视为由评估器驱动的长视界优化循环中的一个算子，并通过管理实验状态、结构化知识检索和从历史轨迹中学习，以实现对可运行软件工件的持续改进？**"
                },
                {
                    "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
                    "arxiv_id": "2601.21468",
                    "authors": "Yaorui Shi, Shugui Liu, Yu Yang, Wenyu Mao, Yuxin Chen, Qi GU, Hui Su, Xunliang Cai, Xiang Wang, An Zhang",
                    "summary": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了一种名为 **MemOCR** 的新型智能体架构，旨在解决长视距智能体推理中的记忆压缩问题。这直接对应了研究课题中的 **\"单智能体\"** 方向，特别是 **\"记忆\" (Memory)** 这一核心子方向。论文并非简单应用现有框架，而是构建了一种新的多模态记忆机制来改进智能体的能力。 2.  **正面指标（符合）**： *   **核心范式**：论文明确提到了 \"agentic reasoning\" 和 \"memory agent\"，属于 Agentic AI 范畴。 *   **智能体能力**：论文的核心创新点在于 **Memory**（记忆），通过视觉布局来优化记忆的存储和检索，从而辅助 **Long-horizon reasoning**（长视距推理）。 3.  **排除标准（未触发）**： *   **多模态与视觉**：虽然论文涉及视觉技术（将文本渲染为图像），但视觉在这里是作为智能体**压缩和访问记忆的工具**，而非研究视觉模型本身或视觉理解能力。根据筛选规则中的例外情况（\"除非它们被用作智能体感知环境的工具\"），这里的视觉是服务于智能体记忆机制的，因此不应排除。 *   **非演化型应用**：论文提出的 MemOCR 是一种通用的智能体记忆架构改进，并非针对特定垂直领域（如医疗、法律）的单纯应用，因此不属于非演化型应用。 4.  **综合结论**： 该论文通过引入视觉布局和强化学习来优化智能体的记忆系统，属于对 LLM 智能体核心组件（记忆）的构建与改进，完全符合 \"LLM智能体及其演化\" 中关于单智能体能力提升的研究目标。",
                    "summary2": "本文旨在解决长时程智能体推理中有限上下文窗口下的高效记忆管理问题。针对交互历史压缩场景，我们提出了一种名为MemOCR的多模态记忆智能体，通过视觉布局实现自适应信息密度，将富文本记忆渲染为2D图像以优先保留关键证据。我们在HotpotQA和Natural Questions等长上下文QA基准上，通过准确率验证了其有效性，在极端预算下实现了约8倍的上下文利用效率提升。",
                    "summary_translation": "长程智能体推理必须将不断增长的交互历史有效地压缩到有限的 context window（上下文窗口）中。大多数现有的记忆系统将历史记录序列化为文本，其 token 级别成本是统一的，且随长度线性增长，这往往导致将稀缺的预算消耗在低价值的细节上。为此，我们提出了 MemOCR，这是一种 multimodal memory agent（多模态记忆智能体），它通过 visual layout（视觉布局）以自适应的信息密度分配内存空间，从而在紧张的上下文预算下提升 long-horizon reasoning（长程推理）能力。具体而言，MemOCR 维护一个结构化的 rich-text（富文本）记忆（例如标题、高亮），并将其渲染为图像供智能体查阅以进行记忆访问，从而在视觉上优先呈现关键证据，同时大幅压缩辅助细节。为了确保在不同记忆预算下的鲁棒性，我们在 budget-aware objectives（预算感知目标）下利用 reinforcement learning（强化学习）对 MemOCR 进行训练，使智能体能够适应多种压缩级别。在长上下文 multi-hop（多跳）和 single-hop（单跳）问答基准测试中，MemOCR 优于强大的基于文本的 baselines（基线模型），并在极端预算下实现了更有效的上下文利用。",
                    "inspiration_trace": "基于对论文《MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning》的深入分析，以下是作者产出该文章的系统性思考过程与逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中通过层层递进的方式，构建了从宏观背景到具体痛点的叙事逻辑：\n\n1.  **背景与机遇**：大语言模型（LLM）的发展赋予了智能体处理复杂、长跨度任务的能力，这需要强大的长程推理支持。\n2.  **核心冲突**：随着智能体在生命周期中积累大量的交互历史，数据量必然超过上下文窗口的硬性限制，构成了根本性的瓶颈。\n3.  **问题定义**：长程推理的核心在于有限工作记忆下的“预算分配问题”——即在有限的Token数量内，如何最大化任务相关信息的密度。\n4.  **现有方案及其缺陷**：\n    *   **方案一（原始检索）**：直接检索并注入历史片段。**缺陷**：包含冗余和噪声，稀释了信息密度，容易耗尽预算。\n    *   **方案二（文本摘要）**：将历史压缩为紧凑的文本摘要。**缺陷**：虽然去除了噪声，但受限于**线性Token缩放**。文本范式具有内在的“均匀信息密度”限制——保留关键证据和保留辅助细节在Token成本上是刚性的（例如，为了保留100个Token的关键信息，被迫保留约900个Token的辅助细节），缺乏灵活地“降采样”低价值上下文的能力。\n5.  **总结痛点**：文本记忆将存储成本与信息内容紧密耦合，导致有限的预算被非关键的支持性事实浪费，无法实现非均匀的预算分配。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者旨在解决的核心研究问题可总结为：\n\n**“如何打破文本记忆中存储成本与信息价值的刚性耦合，通过自适应的信息密度机制，在极端受限的上下文预算下实现高效的长程推理？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n为了回答上述问题，作者的思考过程经历了从现象观察到范式转移，再到机制设计的完整演进：\n\n#### 1. 观察与诊断：文本的“均匀密度”陷阱\n*   **思考起点**：现有的文本记忆（无论是原始文本还是摘要）本质上是一维的流。在这个流中，每一个Token的“价格”都是一样的。\n*   **关键洞察**：这种“均匀定价”是不合理的。对于推理而言，核心证据（如人名、关键数字）的价值远高于辅助细节（如背景介绍、修饰语）。但在文本中，你不能让辅助细节“便宜”一点，除非你直接删掉它。\n*   **结论**：我们需要一种表示方法，使得信息的“存储成本”不再线性依赖于其“字数”，从而允许非均匀的预算分配。\n\n#### 2. 假设提出：从一维文本到二维视觉的范式转移\n*   **类比启发**：人类阅读文档时，通过排版（标题、加粗、字号）来区分信息优先级。在视觉上，重要的东西占地方大且显眼，次要的东西占地方小。\n*   **核心假设**：如果将记忆表示为**图像**而非文本流，就能利用**视觉布局**来控制信息密度。\n    *   **机制**：在图像中，成本由像素决定。我们可以将关键证据渲染为大号字体（高像素，高可见性），将辅助细节渲染为小号字体（低像素，低可见性）。\n    *   **优势**：当预算受限（即图像分辨率降低/下采样）时，大号字体的关键信息依然可读，而小号字体的细节会模糊化。这实现了“自适应信息密度”。\n\n#### 3. 方法构建：双阶段记忆生命周期\n为了实现上述假设，作者设计了一个连接文本与视觉的桥梁：\n*   **阶段一：记忆草拟（文本域）**。\n    *   **思考**：智能体需要一种方式来表达“这段信息很重要”。\n    *   **方案**：让智能体维护一个富文本记忆（如Markdown），利用格式（标题、加粗）显式地标记视觉优先级。这一步是“预算无关”的，只负责决定什么重要。\n*   **阶段二：记忆读取（视觉域）**。\n    *   **思考**：如何将富文本转化为可控的预算？\n    *   **方案**：通过渲染器将富文本转为2D图像。此时，通过控制图像分辨率（下采样）来控制Token预算。智能体像OCR一样读取这张图来回答问题。\n\n#### 4. 训练策略：解决“偷懒”问题\n*   **潜在风险**：如果没有约束，智能体可能会偷懒，把所有信息都写成中等大小的字，导致在低分辨率下什么都看不清（即退化为均匀密度）。\n*   **解决方案**：引入**强化学习（RL）与预算感知目标**。\n    *   **策略**：在训练中故意制造极端场景（如极低分辨率的图像），强迫智能体学会只有把关键信息放在高优先级区域（大字体）才能在压缩后存活下来。\n    *   **目标**：通过多任务训练（标准QA、低预算QA、细节QA），让智能体学会一种既能保住关键点（在低预算下），又能保留细节（在高预算下）的鲁棒布局策略。\n\n#### 5. 验证与闭环\n*   **最终验证**：在长上下文QA任务中，验证MemOCR在预算充足时优于文本基线，并在预算极度收紧时性能下降更缓慢，从而证明了视觉布局带来的自适应压缩优势。\n\n---\n\n**总结**：作者从文本记忆“无法区分信息价值成本”的根本缺陷出发，通过引入视觉模态和空间布局，将“存储成本”从“字数”解耦为“像素/视觉显著性”，最终通过强化学习训练智能体掌握这种利用视觉权重进行记忆压缩的能力。"
                },
                {
                    "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis",
                    "arxiv_id": "2601.21403",
                    "authors": "Ruyi Qi, Zhou Liu, Wentao Zhang",
                    "summary": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，核心依据如下： 1.  **核心贡献符合“构建LLM智能体”的目标**： 论文的核心贡献是提出了 **DataCrossAgent**，这是一个**协作智能体框架**。它不仅仅是将现有的LLM应用到数据分析中，而是设计了一种新的架构，包含专门的子智能体、结构化的工作流程以及新的 reReAct 机制。这直接对应了筛选标准中的“构建、改进 LLM智能体的方法论或新框架”。 2.  **属于“多智能体”与“单智能体”的核心关注点**： *   **多智能体**：论文明确提到采用了“专门的子智能体”，并通过结构化工作流程进行协调，这属于多智能体系统中的协作与分工。 *   **单智能体能力**：论文中提到的“分而治之”工作流程、代码生成与调试机制，涉及智能体的规划和工具使用能力。 3.  **关于“多模态与视觉”的特殊情况处理**： 虽然论文涉及视觉文档（扫描报告、发票图像），但根据筛选标准中的排除项例外：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在本论文中，视觉数据是智能体需要处理的“环境”或“异构数据源”的一部分，论文的研究重点在于**智能体如何通过架构设计来处理这些跨模态数据**，而不是提出新的视觉模型或多模态融合算法。因此，这属于智能体感知环境的范畴，不应被排除。 综上所述，该论文提出了一种新的多智能体协作框架来解决复杂的数据分析任务，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决现有数据分析代理无法有效激活和关联非结构化视觉文档（“僵尸数据”）与结构化数据的问题。针对跨模态异构数据分析场景，我们提出了DataCross框架，包含DataCrossBench基准和DataCrossAgent代理，该代理采用多代理协作、reReAct机制及上下文交叉授粉策略。在包含200个任务的DataCrossBench上，通过Factuality、Completeness等四维指标验证，DataCrossAgent在事实准确性上比GPT-4o提升29.7%。",
                    "summary_translation": "在现实世界的数据科学和企业决策中，关键信息往往分散于可直接查询的 structured sources（结构化源，如 SQL、CSV）和锁定在 unstructured visual documents（非结构化视觉文档，如扫描报告、发票图像）中的“zombie data”（僵尸数据）之间。现有的 data analytics agents（数据分析代理）主要局限于处理结构化数据，无法激活和关联这些高价值的视觉信息，从而与工业需求产生了显著差距。为了弥合这一差距，我们介绍了 DataCross，这是一个用于跨 heterogeneous data modalities（异构数据模态）进行统一、insight-driven（洞察驱动）分析的新型 benchmark（基准）和 collaborative agent framework（协作代理框架）。DataCrossBench 包含 200 个跨越金融、医疗和其他领域的 end-to-end analysis tasks（端到端分析任务）。它是通过一个 human-in-the-loop（人在回路）的 reverse-synthesis pipeline（逆向合成流程）构建的，确保了现实的复杂性、cross-source dependency（跨源依赖性）和可验证的 ground truth（基本真值）。该基准将任务分为三个难度等级，以评估代理在 visual table extraction（视觉表格提取）、cross-modal alignment（跨模态对齐）和 multi-step joint reasoning（多步联合推理）方面的能力。我们还提出了 DataCrossAgent 框架，其灵感源于人类分析师的“divide-and-conquer”（分而治之）工作流程。它采用专门的 sub-agents（子代理），每个子代理都是特定数据源的专家，并通过 Intra-source Deep Exploration（源内深度探索）、Key Source Identification（关键源识别）和 Contextual Cross-pollination（情境交叉授粉）的结构化工作流程进行协调。一种新颖的 reReAct mechanism（reReAct 机制）实现了用于事实验证的 robust code generation（鲁棒代码生成）和调试。实验结果表明，DataCrossAgent 在事实性方面比 GPT-4o 提高了 29.7%，并在高难度任务上表现出卓越的 robustness（鲁棒性），有效地激活了碎片化的“zombie data”（僵尸数据）以进行有洞察力的跨模态分析。",
                    "inspiration_trace": "基于对论文《DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从“技术繁荣”到“现实痛点”的叙事逻辑，具体推演如下：\n\n1.  **技术背景铺垫**：首先承认大语言模型（LLMs）和多模态基础模型在推理、代码生成和工具使用方面取得了显著进展，开启了数据智能体（Data Analysis Agents）的新范式。\n2.  **现实与理想的落差**：话锋一转，指出在真实的组织决策中，高价值证据极少存储在单一干净的表格中。相反，关键信息往往碎片化地分布在可直接查询的结构化系统（如SQL/CSV）和大量非结构化的视觉文档（如扫描报告、发票截图）之间。\n3.  **核心概念提出**：引入“僵尸数据”这一概念，形象地描述那些在业务运营中“活着”，但在分析管道中“死掉”的视觉锁定信息。\n4.  **现有能力的局限性**：指出当前的数据分析智能体和基准测试存在三大缺陷：\n    *   **无法激活**：假设关键证据已是结构化格式，无法可靠地从像素中提取表格结构。\n    *   **难以对齐**：即使有OCR，也难以解决跨模态的实体对齐问题（如将图像中的列名与数据库记录匹配）。\n    *   **评估缺失**：现有协议（如Text-to-SQL或单表分析）无法衡量端到端的跨模态联合推理能力。\n5.  **解决方案预告**：提出DataCrossBench（基准）和DataCrossAgent（框架）来填补这一空白。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个能够有效激活‘僵尸数据’（非结构化视觉文档），并将其与结构化数据进行跨模态对齐和联合推理，以产生可验证洞察的统一分析框架？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者从宏观问题出发，逐步聚焦，最终形成方法论的思考过程如下：\n\n#### 1. 观察与痛点识别\n*   **观察**：现实世界的数据分析是“异构”且“碎片化”的。最有价值的信息往往藏在扫描件或截图里，而不仅仅是数据库里。\n*   **痛点**：现有的智能体要么只能处理文本/代码，要么只能做简单的视觉问答（VQA），缺乏将“看图”转化为“查数”并进行深度分析的能力。\n\n#### 2. 假设与策略制定\n*   **假设**：要解决“僵尸数据”问题，单纯提升模型能力是不够的，必须解决两个层面的问题——**“怎么测”**（评估标准）和**“怎么做”**（系统架构）。\n*   **策略一（关于评估）**：现有的数据集太简单或太单一。我们需要一个能模拟真实复杂度、包含跨源依赖且答案可验证的基准。\n    *   *思考演进*：直接收集真实数据很难标注“标准答案”。不如**“反向合成”**——先由专家定义分析目标和结论，再由程序生成支持该结论的结构化数据和对应的视觉文档。这样既保证了真实性，又确保了逻辑的可验证性。\n*   **策略二（关于架构）**：人类分析师在面对复杂数据时，不会试图用一个大脑同时处理所有事情，而是“分而治之”。\n    *   *思考演进*：智能体也应如此。与其用一个单体模型硬抗，不如设计**多智能体协作**。让专门的子智能体处理专门的数据源（如一个专门看图，一个专门写SQL）。\n\n#### 3. 方法论构建\n*   **构建基准**：\n    *   设计了**DataCrossBench**。利用“人机协作反向合成”管道，从专家验证的洞察出发，逆向生成SQL、CSV和图像数据。\n    *   将任务分为“简单”（仅结构化）和“困难”（含视觉文档）两级，以量化模型在处理“僵尸数据”时的能力衰减。\n\n*   **构建智能体框架**：\n    *   **分而治之**：设计了**DataCrossAgent**。引入专门的子智能体，每个子智能体是特定数据源的专家。\n    *   **工作流设计**：模仿人类分析师，设计了三阶段工作流：\n        1.  **源内深度探索**：各子智能体深入挖掘自己负责的数据源（包括从图像中提取表格）。\n        2.  **关键源识别**：通过“混合优先级评分”机制，判断哪些数据源对当前目标最重要，避免在噪音数据上浪费时间。\n        3.  **上下文交叉授粉**：这是核心创新点。让不同子智能体交换信息，例如“销售智能体”利用“社交媒体智能体”的摘要来假设关联。\n    *   **鲁棒性保障**：为了防止LLM产生数值幻觉，提出了**reReAct机制**（递归推理-行动）。将战略规划与战术执行分离，通过生成代码->执行->调试的循环，确保每一个结论都是基于代码执行结果的“硬事实”。\n\n#### 4. 验证与闭环\n*   **验证逻辑**：通过在DataCrossBench上的实验，证明DataCrossAgent在事实性上比GPT-4o高出29.7%，且在困难任务上表现出更强的鲁棒性，从而验证了“激活僵尸数据”这一目标的可行性。\n\n---\n\n**总结**：作者的思考路径是从**现实世界的“数据异构性”**出发，识别出**“僵尸数据”**这一关键盲点，进而通过**“反向合成”**解决评估难题，并借鉴人类**“分而治之”**与**“交叉验证”**的认知模式，构建了一套多智能体协作的自动化分析框架。"
                },
                {
                    "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents",
                    "arxiv_id": "2601.21372",
                    "authors": "Yang Song, Anoushka Vyas, Zirui Wei, Sina Khoshfetrat Pakazad, Henrik Ohlsson, Graham Neubig",
                    "summary": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code. NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair. Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，主要基于以下判断： 1.  **核心贡献在于构建智能体架构**：论文提出了 NEMO 系统，其核心不仅仅是将 LLM 应用于优化领域，而是构建了一种“执行感知的智能体架构”。论文明确指出其设计围绕“自主编码智能体”展开，将其作为一等公民进行交互，这属于构建和改进 LLM 智能体的方法论范畴。 2.  **符合多智能体方向**：论文中明确提到了引入“新的协调模式”，特别是在独立生成的优化器智能体和模拟器智能体之间进行“不对称验证循环”。这涉及智能体间的协作、通信与验证机制，完全符合“Multi-Agent”的研究焦点。 3.  **具备核心智能体能力**：论文详细描述了系统的关键组件，包括“外部记忆”用于经验重用，以及通过沙箱环境执行实现的“自动验证和修复”。这些分别对应了智能体的“Memory”和“Self-Correction/Reflection”能力。 4.  **非单纯应用**：虽然论文应用于优化建模领域，但其主要贡献在于提出了如何通过智能体的协作、迭代完善和执行感知来提高系统性能，而非仅仅将智能体作为黑盒工具解决特定领域问题。因此，它不属于“非演化型应用”的排除范畴。 综上所述，该论文在 Agentic AI 的架构设计和 Multi-Agent 协作机制上有实质性贡献，符合筛选标准。",
                    "summary2": "本文旨在解决将自然语言决策问题描述转化为可执行数学优化模型时现有方法脆弱且易生成无效代码的问题。针对自然语言描述的决策问题，我们提出了一种基于自主编码代理的NEMO系统，利用模拟器与优化器间的非对称验证循环、MBR解码及自一致性机制实现执行感知的优化建模。在九个优化基准测试上，通过准确率指标验证了其有效性，并在其中八个数据集上取得了最先进的性能。",
                    "summary_translation": "在本文中，我们介绍了 NEMO，这是一个能够将决策问题的自然语言描述转化为形式化的可执行数学优化实现的系统，该系统既可以与用户协作运行，也可以自主运行。现有的方法通常依赖于专用的大语言模型或定制的、特定任务的代理。这些方法往往较为脆弱、复杂，且频繁生成语法无效或不可执行的代码。相比之下，NEMO 的核心在于与自主编码代理的远程交互，将其视为一种一等抽象，类似于基于 API 与 LLMs 的交互。这种设计使得能够围绕 ACAs 构建更高级别的系统，从而对任务规范进行结构化、整合和迭代优化。由于 ACAs 在沙箱环境中执行，NEMO 生成的代码在构造上天然是可执行的，从而允许进行自动验证与修复。在此基础上，我们引入了新颖的 ACAs 内部及跨 ACAs 的协调模式，包括在独立生成的优化器和模拟器实现之间的非对称验证循环（作为一种高级验证机制）、用于经验复用的外部记忆，以及通过最小贝叶斯风险解码和自一致性实现的鲁棒性增强。我们在九个公认的优化基准上对 NEMO 进行了评估。如图 1 所示，它在大多数任务上实现了最先进的性能，并在多个数据集上取得了显著优势，证明了执行感知的代理架构在自动优化建模中的强大能力。",
                    "inspiration_trace": "基于对论文《NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents》的深入分析，以下是作者产出该核心方法的逻辑链推演。\n\n---\n\n### 一、 宏观问题引入：Introduction 中的“讲故事”逻辑\n\n作者在引言部分通过层层递进的逻辑，构建了一个从宏观背景到具体技术痛点的叙事链条：\n\n1.  **价值与瓶颈的矛盾**：\n    *   **现状**：运筹优化在供应链、资源分配等领域价值巨大，但涉及复杂变量和约束，高度依赖专家知识。\n    *   **痛点**：开发过程本质上是迭代的（定义->求解->评估->修正），这导致成本高昂、速度缓慢，限制了优化技术的普及。\n\n2.  **新技术的机遇与局限**：\n    *   **机遇**：大语言模型（LLMs）的出现有望自动化这一流程，降低门槛。\n    *   **局限**：现有的方法（无论是微调模型还是简单的Agent框架）主要依赖**直接的代码生成**。\n    *   **核心缺陷**：这些方法缺乏“执行感知”的验证。它们生成的代码往往是脆弱的、语法无效或不可执行的。更重要的是，它们无法模仿人类专家的“模拟器-优化器”反馈循环——即通过构建模拟来发现逻辑不一致。\n\n3.  **结论**：\n    *   现有的基于文本生成的范式无法解决优化建模中的逻辑一致性和可执行性问题，需要一种新的范式。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何构建一个基于自主编码代理的执行感知系统，使其能够像人类专家一样，通过执行反馈和模拟验证，将自然语言描述可靠地转化为可执行的优化模型，从而克服现有文本生成方法的脆弱性？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n作者从观察到最终提出 NEMO 系统，经历了以下四个关键的思想跃迁：\n\n#### 1. 范式转移：从“文本生成”到“执行交互”\n*   **观察**：传统方法将 LLM 视为文本生成器，输出代码字符串。这导致代码往往无法运行，且错误难以在文本层面被完全检测。\n*   **假设**：如果将 LLM 封装在具备沙箱执行环境的“自主编码代理”中，并将 ACA 作为一个类似 API 的一等公民抽象，那么代码的**可执行性**将由构造过程本身保证。\n*   **演进**：不再要求 LLM 一次性写出完美代码，而是允许 ACA 在沙箱中编写、运行、报错并自我修正。这确立了 NEMO 的基石——**执行感知**。\n\n#### 2. 架构洞察：利用“复杂性不对称”进行交叉验证\n*   **观察**：在人类专家的工作流中，构建一个“模拟器”通常比构建一个“优化器”更容易（模拟器通常是命令式的 Python 代码，直接反映逻辑；优化器则需要复杂的声明式数学约束）。\n*   **假设**：既然模拟器更容易写对，那么可以用一个独立的 ACA 生成模拟器，用另一个 ACA 生成优化器。利用模拟器作为“裁判”来验证优化器的结果。\n*   **演进**：提出了**非对称验证循环**。模拟器不仅检查可行性，还计算目标值。如果优化器的结果与模拟器的计算不一致，则触发修正。这解决了“语义正确性”难以验证的问题。\n\n#### 3. 鲁棒性增强：对抗随机性的“共识机制”\n*   **观察**：LLM 和 ACA 具有内在的非确定性。同一个问题生成的代码，变量命名、公式形式可能不同，甚至逻辑可能出错。\n*   **假设**：通过生成多个独立的样本并寻找“共识”，可以过滤掉随机错误，保留稳定的逻辑。\n*   **演进**：\n    *   **上游（决策过程提取）**：采用**最小贝叶斯风险（MBR）解码**。生成多个候选的结构化表示，选择与其他候选语义最相似的那个作为最终输出，确保理解的一致性。\n    *   **下游（优化求解）**：采用**自一致性**机制。并行运行多个独立的优化器实现，通过多数投票和目标值聚类来选出最稳健的解。\n\n#### 4. 知识复用：从“提示词”到“外部记忆”\n*   **观察**：单纯的上下文学习容易受到上下文窗口限制，且难以处理复杂的代码示例。\n*   **假设**：如果将过去的成功经验以可执行代码片段的形式存储在外部记忆中，并在遇到新问题时动态检索，可以显著提升 ACA 的表现。\n*   **演进**：构建了一个包含 3000 个样本的向量数据库。不仅检索相似的文本描述，还引入**多样性惩罚**，确保检索到的示例既有相关性又有覆盖面，避免模型陷入单一模式的过拟合。\n\n---\n\n### 总结\n\n作者的思想演进路径可以概括为：\n**发现文本生成的脆弱性** $\\rightarrow$ **引入 ACA 实现执行感知** $\\rightarrow$ **利用模拟器与优化器的复杂度差异构建验证闭环** $\\rightarrow$ **利用 MBR 和自一致性对抗随机性** $\\rightarrow$ **结合外部记忆实现知识复用**。\n\n这一逻辑链条最终汇聚成了 NEMO 这一系统，它不再是一个简单的代码生成工具，而是一个具备自我验证、自我修正能力的智能体协作系统。"
                },
                {
                    "title": "BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents",
                    "arxiv_id": "2601.21352",
                    "authors": "Ziyu Lu, Tengjin Weng, Yiying Yang, Yuhang Zhao, Xinxin Huang, Wenhao Jiang",
                    "summary": "GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，核心依据如下： 1.  **核心判断（符合）：** 论文的核心贡献在于提出了一个新的智能体框架 **BEAP-Agent**，旨在解决现有GUI智能体在执行错误后难以恢复的问题。这属于“构建、改进 LLM智能体”的方法论研究，而非单纯的应用研究。 2.  **符合研究焦点（单智能体）：** 论文明确属于 **Agentic AI** 中的 **单智能体** 方向。它重点解决了智能体在复杂任务中的 **规划** 和 **自我修正** 问题。论文提出的“可回溯执行”和“自适应规划”机制，本质上是增强了智能体在长视距任务中的鲁棒性和自主决策能力。 3.  **正面指标匹配：** 论文涉及的核心范式和能力包括 `Agentic AI`、`LLM-based Agents`、`Planning`（规划）以及 `Self-Correction`（通过回溯机制实现）。特别是其将任务执行建模为DFS过程并引入多级状态回溯，是对智能体反思与纠错机制的一种创新性改进。 4.  **排除标准检查：** 尽管论文涉及GUI（图形用户界面），具有一定的视觉属性，但根据筛选标准中的例外情况，视觉在这里是智能体感知和操作的环境/工具，而非论文的研究核心。论文的核心在于智能体的控制逻辑和规划框架，而非视觉模型的改进，因此不应被排除。 综上所述，BEAP-Agent 提出了一种新的智能体框架来改进规划与执行能力，高度契合我对 Agentic AI 及单智能体演化的研究目标。",
                    "summary2": "本文旨在解决现有 GUI agents 在错误路径后难以恢复导致任务失败的问题。针对长程任务探索场景，我们提出了一种基于 DFS 的 BEAP-Agent 框架，集成了 Planner、Executor 和 Tracker，支持长程多级状态回溯与动态任务跟踪。我们在 OSWorld benchmark 上通过 accuracy 验证了其有效性，实现了 28.2% 的准确率。",
                    "summary_translation": "GUI agents（图形用户界面智能体）旨在自动化重复性任务并提高生产力。然而，现有的 GUI agents（图形用户界面智能体）一旦遵循了错误的探索路径，往往难以进行恢复，从而导致任务失败。在这项工作中，我们将 GUI 任务执行建模为一个 DFS（深度优先搜索）过程，并提出了 BEAP-Agent，这是一个基于 DFS 的框架，支持结合动态任务跟踪与更新的长程、多级状态回溯。该框架由三个协作组件组成：Planner（规划器）、Executor（执行器）和 Tracker（跟踪器）。它们共同实现了有效的任务探索和执行。BEAP-Agent 填补了 GUI agents（图形用户界面智能体）在系统性回溯机制方面的空白，为 long-horizon（长视界）任务探索提供了一个系统性的解决方案。我们在 OSWorld benchmark（基准测试）上进行了系统性评估，其中 BEAP-Agent 达到了 28.2% 的准确率，验证了所提方法的有效性。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对BEAP-Agent作者核心思路逻辑链的系统性推演与还原：\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的逻辑链条，用于引出核心问题：\n\n1.  **宏观目标与现状：** GUI Agent旨在通过模拟人类交互来自动化重复性任务，相比传统脚本，它具有更强的鲁棒性和灵活性。\n2.  **技术演进与痛点转移：**\n    *   **早期阶段：** 主要失败原因是“定位能力”不足，模型依赖HTML或辅助树，泛化性差。\n    *   **近期阶段：** 随着视觉语言模型（VLM）的进步，纯视觉的定位能力已显著提升，开发难度降低。\n3.  **当前核心瓶颈：** 定位问题解决后，“规划能力”弱成为任务失败的主因。模型缺乏精确知识，依赖近似推理，导致计划往往只是模糊的方向，而非精确的步骤。\n4.  **具体失败机制：** 由于需要试错，Agent常执行导致不可逆或次优状态的操作。一旦进入错误路径，任务链断裂，导致失败。\n5.  **现有方案的局限性：** 现有的BackTrackAgent虽然引入了回溯，但仅限于“单步回溯”（验证动作有效性）。\n6.  **关键观察（Gap）：** 实际上，错误往往在执行多步之后才显现（因为GUI任务奖励高度稀疏）。当发现错误时，根源往往不在前一步，而在很久之前的某个决策点。此时，单步回溯已无力回天。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图解决的核心研究问题可总结为：\n\n**“GUI Agent应如何构建一种系统性的回溯机制，使其能够在错误延迟显现（稀疏奖励）的情况下，实现长程、多级的状态恢复与重规划，从而避免因局部探索失败导致的全局任务崩溃？”**\n\n---\n\n### 三、 核心方法的逻辑演进脉络\n\n作者从观察到最终提出BEAP-Agent的思考过程经历了以下四个阶段：\n\n#### 1. 观察与抽象：从“线性执行”到“空间搜索”\n*   **思考起点：** 作者观察到人类在操作GUI时，如果发现路走不通，会退回到上一个“岔路口”，而不仅仅是撤销上一步操作。\n*   **抽象建模：** 既然任务是试错的过程，那么GUI环境就可以被抽象为一个“状态空间”。每一次点击或输入都是从一个状态转移到另一个状态。\n*   **逻辑跃迁：** 因此，GUI任务的执行不应被视为一条线性的动作序列，而应被视为一棵**状态空间树**的搜索过程。\n\n#### 2. 假设与策略：引入DFS（深度优先搜索）解决“稀疏奖励”\n*   **问题聚焦：** 现有的单步回溯失败，是因为它假设错误是即时反馈的。但GUI任务中，反馈往往是延迟的（稀疏奖励）。\n*   **策略选择：** 为了应对这种延迟反馈，Agent需要能够“记住”走过的路径，并且在当前路径走不通时，有能力退回到**任意一个历史分叉点**。\n*   **算法匹配：** **深度优先搜索（DFS）**天然符合这一特性。DFS允许Agent沿着一条路径走到黑（直到发现错误或无路可走），然后触发回溯，回到最近的祖先节点尝试其他分支。这解决了“长程回溯”的问题。\n\n#### 3. 架构设计：三权分立以支撑复杂逻辑\n*   **功能分解：** 要实现DFS式的探索，单纯的“感知-行动”循环不够，需要更精细的模块分工：\n    *   **Planner（大脑）：** 负责在初始或回溯后生成高层计划（子任务列表），指引探索方向。\n    *   **Executor（手脚）：** 负责将计划转化为具体的GUI操作（点击、输入等），并执行状态转移。\n    *   **Tracker（记忆与裁判）：** 这是最关键的模块。它不仅要更新子任务状态，还要判断当前是“继续”、“完成”还是“回溯”。它负责维护全局视野，防止Agent陷入死循环。\n*   **逻辑闭环：** Tracker发现死路 -> 通知Executor执行回退操作 -> 通知Planner基于新状态重新规划 -> Executor执行新计划。\n\n#### 4. 验证与修正：动态任务跟踪的必要性\n*   **进一步思考：** 仅仅能回溯还不够。如果回溯后，计划还是照搬旧计划，Agent可能会再次犯同样的错，或者因为不知道进度在哪里而提前终止。\n*   **机制完善：** 因此，Tracker必须具备**动态任务跟踪**能力。它需要根据当前的实际页面状态，实时调整剩余的子任务计划，确保计划与环境始终对齐。\n\n---\n\n**总结：**\n作者的思考路径是从**“定位能力已达标，规划能力成瓶颈”**的现状出发，敏锐地捕捉到**“错误反馈延迟”**这一关键特性，从而摒弃了线性的单步纠错思维，转而将GUI任务建模为**基于DFS的状态空间搜索**。最终，通过构建**Planner-Executor-Tracker**三位一体的架构，实现了长程回溯与动态规划的结合，填补了GUI Agent系统性容错机制的空白。"
                },
                {
                    "title": "CUA-Skill: Develop Skills for Computer Using Agent",
                    "arxiv_id": "2601.21123",
                    "authors": "Tianyi Chen, Yinheng Li, Michael Solodko, Sen Wang, Nan Jiang, Tingyuan Cui, Junheng Hao, Jongwoo Ko, Sara Abdali, Suzhen Zheng, Leon Xu, Hao Fan, Pashmina Cameron, Justin Wagle, Kazuhito Koishida",
                    "summary": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断**: *   论文的核心贡献是构建了 **CUA-Skill Agent**，这是一个端到端的计算机使用智能体，并提出了配套的技能库框架。 *   这不仅仅是将现有智能体应用到特定领域，而是提出了一种新的智能体架构（通过技能抽象、参数化执行和组合图），旨在解决现有智能体难以扩展和性能落后的问题。这符合“构建、改进 LLM智能体”的核心目标。 2.  **符合正面指标**: *   **Agentic AI / LLM-based Agents**: 论文明确聚焦于 Computer-Using Agents (CUAs) 的构建。 *   **Tool Use / Tool Augmentation**: 论文的核心创新点在于将人类计算机交互知识编码为可重用的“技能”，这本质上是一种高级的工具增强和抽象机制。 *   **Memory**: 论文提到了 \"memory-aware failure recovery\"（记忆感知的故障恢复），属于智能体记忆机制的应用。 *   **Planning**: 技能的组合图涉及智能体的规划和执行逻辑。 3.  **排除标准检查**: *   **非演化型应用**: 虽然智能体操作的是 Windows 应用程序，但论文的重点在于智能体如何通过技能库更好地操作计算机，而非解决某个具体的业务领域（如金融或医疗）问题。 *   **多模态与视觉**: 尽管智能体操作的是图形用户界面（GUI），涉及视觉感知，但视觉仅作为智能体感知环境的输入手段，并非论文研究的核心（核心是技能抽象和智能体框架），因此符合“作为工具使用”的例外情况。 *   **基础设施**: 虽然论文提到了“技能库”这一基础设施，但它是服务于智能体逻辑（检索、实例化、恢复）的组件，而非模型部署或硬件加速层面的基础设施。 综上所述，该论文通过引入技能库机制显著提升了单智能体在复杂计算机任务中的表现，属于单智能体方向的高质量研究。",
                    "summary2": "本文旨在解决现有计算机使用代理缺乏可复用技能抽象导致难以扩展和性能落后的问题。针对Windows桌面环境，我们提出了一种名为CUA-Skill的结构化技能库，包含参数化执行图和组合图，以及支持动态检索和记忆恢复的CUA-Skill Agent。我们在WindowsAgentArena (WAA) benchmark上通过success rate验证了其有效性，取得了57.5%的state-of-the-art成功率。",
                    "summary_translation": "Computer-Using Agents (CUAs，计算机使用代理) 旨在自主操作计算机系统以完成现实世界的任务。然而，现有的代理系统仍难以扩展，且性能不及人类水平。一个关键的局限性在于缺乏可重用且结构化的技能抽象，这些抽象能够捕捉人类与图形用户界面交互的方式以及如何利用这些技能。我们提出了 CUA-Skill，这是一个计算机使用代理技能库，它将人类计算机使用知识编码为技能，并结合了参数化执行和组合图。CUA-Skill 是一个包含精心设计技能的大规模库，涵盖了常见的 Windows 应用程序，作为可扩展、可靠的代理开发的实用基础设施和工具基底。基于该技能库，我们构建了 CUA-Skill Agent，这是一个端到端的计算机使用代理，支持动态技能检索、参数实例化以及具有记忆感知能力的故障恢复。实验结果表明，CUA-Skill 在具有挑战性的端到端代理基准测试中显著提高了执行成功率和鲁棒性，为未来的计算机使用代理开发奠定了坚实基础。在 WindowsAgentArena 上，CUA-Skill Agent 实现了 57.5%（三次尝试最佳）的 state-of-the-art (SOTA，最先进) 成功率，且效率显著高于先前及同期的方法。项目页面地址为 https://microsoft.github.io/cua_skill/。",
                    "inspiration_trace": "基于您提供的论文内容，我为您系统性地推演了作者产出《CUA-Skill》这篇论文的完整逻辑链。以下是从宏观问题观察到具体方法论形成的思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的对比，构建了研究的必要性，其逻辑链条如下：\n\n1.  **愿景与现状：** 计算机使用代理旨在通过自主操作GUI来完成现实世界的复杂任务（如文档编辑、数据分析）。随着LLM和多模态感知的进步，这一领域前景广阔。\n2.  **核心瓶颈：** 尽管感知能力提升了，但构建可靠、可扩展的CUA仍然极具挑战性。现有系统在处理长周期任务时表现脆弱，微小的错误会迅速累积导致失败。\n3.  **根本原因：** 现有方法缺乏**可复用且结构化的技能抽象**。它们将桌面交互建模为“扁平的低级动作序列”（如单纯的点击、打字），迫使Agent在每次任务中都要从头“重新发明”常见的工作流程。\n4.  **人类对比：** 与Agent不同，人类使用计算机是围绕**可复用的程序性知识**（技能）构建的。人类依赖熟悉的技能（如启动应用、格式化文档），并将它们组合成更高级的工作流。\n5.  **现有方案的局限：** 虽然已有“Agent技能”的概念（如Anthropic的MCP），但它们主要基于文件系统和脚本，严重依赖API。这在Windows等桌面环境中效果不佳，因为许多应用缺乏一致的API，且GUI交互是核心。\n6.  **结论：** 桌面环境急需一种能够捕捉人类程序性知识、不依赖强API、且能适应GUI变化的技能抽象层。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式提出了本文旨在解决的核心问题：\n\n**“How can we build a scalable and transferable skill base for desktop environments that captures human procedural knowledge and enables reliable and capable CUAs?”**\n（如何构建一个可扩展且可迁移的桌面环境技能库，既能捕捉人类的程序性知识，又能实现可靠且强大的计算机使用代理？）\n\n---\n\n### 三、 核心方法论的逻辑演进链\n\n为了回答上述问题，作者的思考路径经历了从现象观察到架构设计的五个关键阶段：\n\n#### 第一阶段：从“动作”到“技能”的认知升维\n*   **思考：** 现有的Agent太“笨”了，每次都要思考“鼠标往左移2厘米点击”。人类之所以高效，是因为我们脑子里有“打开文件”、“调整格式”这种高级概念。\n*   **假设：** 如果我们能将人类的计算机操作知识固化为一个个“技能单元”，Agent就不需要每次都从像素级别思考，而是像调用函数一样调用技能，这将大幅降低推理难度和错误率。\n\n#### 第二阶段：定义“技能”的本质属性\n*   **思考：** 桌面环境极其复杂，同一个技能在不同软件、不同状态下表现不同。简单的脚本（如“点击坐标(100,200)”）太脆弱，一旦UI更新就失效。\n*   **创新点：** 技能不能是死板的脚本，必须是**参数化的**。\n    *   **参数化：** 技能应该像函数一样，接受参数（如“打开[文件名]”）。\n    *   **执行图：** 技能的内部实现应该是一个“图”而非线性序列。图中的节点可以是GUI点击，也可以是热键脚本。这样，技能就能适应UI的微小变化（例如：既可以通过菜单点击“保存”，也可以按Ctrl+S）。\n    *   **组合图：** 技能之间需要能像积木一样拼接，形成复杂的工作流。\n\n#### 第三阶段：解决“技能库”的规模与检索难题\n*   **思考：** Windows系统有成千上万个操作，我们不可能把所有技能都塞进LLM的Prompt里，那样既昂贵又低效。\n*   **解决方案：** 引入**检索增强**。\n    *   建立一个大规模的技能库（CUA-Skill Base）。\n    *   当Agent面对任务时，不是遍历所有技能，而是通过语义检索找到最相关的Top-K个技能。\n    *   这解决了“可扩展性”问题，使得技能库可以无限增长而不影响单次推理效率。\n\n#### 第四阶段：构建“技能驱动”的Agent架构\n*   **思考：** 有了技能库，Agent该怎么用？它不能只是盲目执行，还需要根据当前屏幕状态动态调整。\n*   **架构设计：** 设计一个闭环的Agent流程：\n    1.  **规划与检索：** 理解用户意图 -> 生成查询 -> 检索候选技能。\n    2.  **重排与配置：** 结合当前屏幕状态，从候选技能中选出最合适的一个，并填充参数（例如：从屏幕上识别出文件名填入参数）。\n    3.  **执行与反思：** 执行技能（通过GUI Grounding或脚本），并将结果写入记忆。如果失败，利用记忆进行回溯和修正。\n\n#### 第五阶段：验证“技能”的有效性\n*   **思考：** 这个系统真的比原来的强吗？我们需要证明两点：一是技能本身是否可靠（轨迹生成），二是Agent能否在真实任务中用好这些技能（端到端评测）。\n*   **实验逻辑：**\n    *   首先验证技能库本身的原子能力（76.4%的成功率），证明“积木”是好的。\n    *   然后在WindowsAgentArena上进行端到端测试，证明“搭积木的人”（Agent）也是强的（57.5% SOTA）。\n    *   通过消融实验证明，引入“技能”这一抽象层，确实比单纯依靠LLM原生能力要有效得多。\n\n---\n\n### 总结\n\n作者的思考过程本质上是对**“计算机操作抽象层级”的一次重构**。\n从**“扁平的像素/动作序列”**（现有范式） -> **“参数化的技能单元”**（核心创新） -> **“检索增强的技能编排系统”**（工程实现）。\n这一逻辑链条不仅解决了长周期任务中的错误累积问题，更重要的是，它为Agent提供了一种类似人类“肌肉记忆”和“专业知识”的载体，从而实现了从“重新发明轮子”到“熟练使用工具”的跨越。"
                },
                {
                    "title": "Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement",
                    "arxiv_id": "2601.21113",
                    "authors": "Kaiyuan Wu, Aditya Nagori, Rishikesan Kamaleswaran",
                    "summary": "Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay. Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases. Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay. Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining. Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应当保留。判断依据如下： 1.  **核心贡献符合“自我演化”方向**： 论文的核心贡献是提出了一个名为 \"Planner-Auditor Twin\" 的框架，其关键创新在于引入了 \"two-tier self-improvement\"（两层自我改进）机制。具体包括 \"within-episode regeneration\"（剧本内再生）和 \"cross-episode discrepancy buffering with replay\"（跨剧本差异缓冲与重放）。这完全符合筛选标准中关于“自我演化”的定义，即智能体通过经验、反思或环境反馈进行自我完善和迭代。 2.  **符合“Agentic AI”范式**： 论文明确将其方法定义为 \"Agentic\"（智能体），构建了一个包含 Planner（LLM生成规划）和 Auditor（确定性模块验证）的智能体架构。这涉及智能体的规划、自我反思和自我修正能力，属于单智能体研究范畴。 3.  **符合第四步的“特殊例外”规则**： 尽管论文的应用场景是医疗领域的“出院规划”，属于特定领域应用，但根据筛选标准第四步（自我演化的应用），如果论文的核心是提出一种新的“自我演化”机制，即使应用在特定领域，也应该保留。本文的实验结果也表明 \"self-improvement loop was the primary driver of gains\"（自我改进循环是收益的主要驱动力），证明了机制本身的重要性。 4.  **排除标准检查**： 虽然摘要中提到了 \"safety\"（安全）、\"hallucination\"（幻觉）和 \"reliability\"（可靠性），但这并非一篇单纯关于安全对齐或防御攻击的理论研究。相反，它是利用智能体架构和反馈循环来提升性能和可靠性，因此不属于第三步中关于“安全与对齐”的排除范围。 综上所述，该论文提出了一种具有自我改进能力的LLM智能体框架，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决LLM在临床出院规划中的幻觉与校准风险。针对MIMIC-IV-on-FHIR数据集，我们提出了一种Planner–Auditor框架，通过确定性审计与两层自我改进机制（包括缓存和重放）来增强安全性。我们在该数据集上通过多任务覆盖率、Brier分数和ECE等指标验证了其有效性，显著提升了任务覆盖率与校准性能。",
                    "summary_translation": "**目的：** 大语言模型在临床出院规划方面展现出潜力，但其应用受到幻觉、遗漏以及置信度校准不当的限制。我们引入了一个自我改进的、可选缓存的Planner-Auditor框架（规划者-审计者框架），通过将生成过程与确定性验证及针对性重放解耦，提高了安全性和可靠性。\n\n**材料与方法：** 我们利用MIMIC-IV-on-FHIR数据集，实现了一个基于智能体的、回顾性的、FHIR原生（快速医疗互操作性资源原生）评估流水线。对于每位患者，Planner（规划者，即LLM）生成一个包含显式置信度估计的结构化出院行动计划。Auditor（审计者）是一个确定性模块，用于评估多任务覆盖率，跟踪校准情况（Brier分数，ECE代理指标），并监控动作分布漂移。该框架支持两层自我改进： 启用时的单次流程内重新生成，以及 针对高置信度、低覆盖率情况的跨流程差异缓冲与重放。\n\n**结果：** 尽管上下文缓存相比基线提升了性能，但自我改进循环是性能提升的主要驱动力，将任务覆盖率从32%提高到了86%。校准情况显著改善，Brier分数和ECE降低，高置信度遗漏减少。差异缓冲在重放过程中进一步纠正了持续存在的高置信度遗漏。\n\n**讨论：** 反馈驱动的重新生成和针对性重放作为有效的控制机制，减少了结构化临床规划中的遗漏，并提高了置信度的可靠性。将LLM Planner与基于规则的、观察性的Auditor分离开来，使得无需重新训练模型即可进行系统的可靠性测量和更安全的迭代。\n\n**结论：** Planner-Auditor框架利用可互操作的FHIR数据访问和确定性审计，为更安全的自动化出院规划提供了一条实用途径，并得到了可复现的消融实验和以可靠性为导向的评估的支持。",
                    "inspiration_trace": "基于您提供的论文内容，以下是对作者产出该文章核心方法逻辑链的系统性推演。\n\n### 1. 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的逻辑链条，旨在引出研究的必要性：\n\n1.  **现实痛点**：临床出院规划是一项极其复杂且高风险的任务，涉及多学科协调。现有的手动流程不仅导致临床医生职业倦怠，还常因沟通断层和准备不足引发患者安全问题（如药物错误、再入院）。\n2.  **技术机遇**：大语言模型（LLMs）在自然语言理解和医学推理方面表现出色，理论上能完美胜任综合患者信息并生成出院计划的任务。\n3.  **核心冲突**：LLMs本质上是概率性引擎，存在“幻觉”和“校准偏差”问题。在医疗高风险场景下，一个“自信但错误”的AI助手比一个明显无能的助手更危险，因为它会引发医生的自动化偏见。\n4.  **现有方案的局限**：传统的缓解手段是“人在回路”，即医生审核所有AI输出。但这虽然安全，却成为了效率瓶颈，限制了AI的可扩展性。\n5.  **范式转移**：为了解决上述矛盾，必须从单纯的生成式AI转向“智能体AI”。这要求系统具备“系统2思维”——即像人类医生一样，在签署文件前进行起草、观察、评估和自我修正。\n\n---\n\n### 2. 研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题是：\n\n**“如何在不完全依赖人工审核的情况下，利用LLM生成安全可靠的出院计划，从而解决其幻觉风险和过度自信问题？”**\n\n---\n\n### 3. 核心方法的逻辑演进（思想推演）\n\n为了回答上述问题，作者的思考过程经历了从宏观观察到具体架构设计的演进：\n\n#### 第一阶段：观察与假设（从“系统1”到“系统2”）\n*   **观察**：直接使用LLM（System 1，快思考）生成医疗计划，虽然快，但容易遗漏关键步骤且盲目自信。\n*   **假设**：如果让LLM在输出前进行自我反思和修正（System 2，慢思考），应该能减少遗漏。但单纯依靠LLM自身的反思可能不够可靠，需要一个客观的标尺。\n\n#### 第二阶段：架构构思（解耦生成与验证）\n*   **思考**：为了引入客观标尺，不能让LLM既当运动员又当裁判。\n*   **决策**：采用**“双生子”架构**，将系统拆分为两个独立的角色：\n    1.  **Planner（规划者）**：负责发挥LLM的生成能力，起草计划。\n    2.  **Auditor（审计者）**：负责基于确定性规则（非概率性）进行检查，评估任务覆盖率和置信度校准。\n*   **逻辑**：通过将“生成”与“验证”解耦，审计者可以作为一个安全网，捕捉规划者的错误。\n\n#### 第三阶段：机制深化（处理“自信的错误”）\n*   **观察**：最危险的情况是Planner生成了不完整的计划，但置信度却很高。简单的重试可能无法解决这种顽固错误。\n*   **思考**：需要一种机制专门针对这些“硬骨头”进行修复。\n*   **决策**：设计**两层自我改进机制**：\n    1.  **即时层**：如果审计者发现未通过，立即触发Planner在同一请求内重新生成（自我修正）。\n    2.  **跨回合层**：如果Planner非常自信（Confidence ≥ 0.8）但实际上错了（遗漏任务），将这个案例放入“差异缓冲区”。在后续的离线回放中，专门针对这些高置信度的失败案例进行针对性修复。\n\n#### 第四阶段：工程化与评估（FHIR与缓存）\n*   **思考**：为了在真实医疗环境中落地，数据必须标准化，且系统效率要可控。\n*   **决策**：\n    *   使用**FHIR标准**处理患者数据，确保互操作性。\n    *   引入**上下文缓存**，在提高生成质量的同时减少延迟，抵消自我改进循环带来的计算开销。\n\n**总结**：作者的思想演进是从**发现LLM在医疗场景下的“自信-错误”矛盾**出发，通过**引入确定性审计者**来约束LLM的随机性，并最终通过**反馈循环和缓冲回放机制**，将一个被动的生成模型转变为一个具备自我进化能力的主动智能体。"
                },
                {
                    "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve",
                    "arxiv_id": "2601.21096",
                    "authors": "Hongzheng Chen, Alexander Novikov, Ngân Vũ, Hanna Alam, Zhiru Zhang, Aiden Grossman, Mircea Trofin, Amir Yazdanbakhsh",
                    "summary": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文提出了 **Magellan**，这是一个明确的 **\"agentic framework\"**（智能体框架）。其核心机制是将 LLM 编码智能体与 **\"evolutionary search\"**（演化搜索）相结合，通过一个包含生成、评估和优化的闭环来 **\"evolves the compiler pass itself\"**（演化编译器过程本身）。这直接对应了研究课题中的 **\"自我演化\" (Self-Evolving)** 和 **\"Agentic AI\"** 范式。 2.  **满足正面指标**： *   **核心范式**：涉及 `Agentic AI` 和 `Evolutionary Algorithms`。 *   **演化机制**：明确描述了 `Generational Evolution`（通过演化搜索）和 `Iterative Improvement`（闭环优化）。 *   **智能体能力**：涉及 `Tool Use`（合成可执行的 C++ 代码）和 `Planning`（通过搜索策略发现优化策略）。 3.  **符合特殊情况处理规则（第四步）**： 虽然论文的应用场景是编译器优化（特定领域），但根据第四步关于“自我演化的应用”的规则：**“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。”** Magellan 的核心贡献在于提出了一种结合 LLM 与演化算法的通用框架和方法论，而不仅仅是将现有智能体应用于编译器任务。因此，它属于保留范围。 综上所述，该论文在构建自我演化的 LLM 智能体框架方面具有显著贡献，符合研究目标。",
                    "summary2": "本文旨在解决现代编译器中手工设计启发式规则维护负担重且难以适应复杂软硬件的问题。针对LLVM和XLA的优化任务，我们提出了一种名为Magellan的智能体框架，结合LLM编码智能体、进化搜索和自动调优，通过合成可执行C++代码来进化编译器Pass。在生产级宏基准测试上，通过二进制大小缩减和运行时性能指标验证了其有效性，结果超越了专家基线。",
                    "summary_translation": "现代编译器依赖于 hand-crafted heuristics (手工设计的启发式规则) 来指导 optimization passes (优化遍)。这些人工设计的规则往往难以适应现代软件和硬件的复杂性，并导致高昂的维护负担。为了应对这一挑战，我们提出了 Magellan，这是一个 agentic framework (智能体框架)，通过合成可执行的 C++ 决策逻辑来演进 compiler pass (编译器遍) 本身。Magellan 将 LLM coding agent (LLM 编码智能体) 与 evolutionary search (进化搜索) 和 autotuning (自动调优) 相结合，形成了一个包含生成、在用户提供的 macro-benchmarks (宏基准测试) 上进行评估以及精炼的闭环，从而生成可直接集成到现有编译器中的紧凑启发式规则。在多项生产环境优化任务中，Magellan 发现了能够匹配甚至超越专家基线的策略。在 LLVM function inlining (LLVM 函数内联) 方面，Magellan 合成了新的启发式规则，在二进制大小缩减和端到端性能上均优于数十年的手工工程成果。在 register allocation (寄存器分配) 方面，它学习到了一条用于 live-range processing (生存期处理) 的简洁优先级规则，在大规模工作负载上能够媲美复杂的人工设计策略。我们还报告了在 XLA 问题上的初步结果，证明了在减少工程投入的情况下，该方法在 LLVM 之外的可移植性。",
                    "inspiration_trace": "基于对论文《Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的逻辑链条，旨在引出当前研究的必要性：\n\n1.  **现状与基石**：现代编译器极其成熟，但由于许多优化问题是NP-hard的，精确求解不可行，因此编译器高度依赖**人工设计的启发式规则**来指导优化过程。\n2.  **核心冲突**：随着软硬件生态的异构化和复杂化，这些基于人类直觉的“手写规则”越来越难以适应，导致设计、调优和维护的负担极高，且性能难以保持竞争力。\n3.  **现有方案的局限**：\n    *   **LLM 生成代码**：直接绕过编译器生成目标代码，虽然新颖，但牺牲了现有编译器成熟的正确性和兼容性保证。\n    *   **LLM 搜索序列**：针对特定程序搜索优化序列，这属于“一次性”方案，无法跨程序复用。\n    *   **神经网络（NN）策略**：虽然能超越人类专家，但将神经网络模型集成到编译器基础设施中成本高昂、耗时且难以复现（尤其是面对新的优化Pass或硬件时）。\n4.  **我们的切入点**：我们不生成代码，也不搜索序列，更不引入沉重的神经网络模型。我们的目标是**进化编译器 Pass 本身**，即直接合成可执行的 C++ 决策逻辑。\n5.  **核心价值主张**：将启发式设计视为一个**程序合成问题**，直接搜索可部署的 C++ 实现。这样既能获得超越人类的性能，又能保持与手写代码相同的部署和维护属性。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题可总结为：\n\n**“我们能否通过自动化搜索生成可执行的 C++ 编译器优化启发式逻辑，使其在超越人类专家性能的同时，保持与手写代码相同的可部署性和可维护性，从而避免神经网络集成带来的工程负担？”**\n\n---\n\n### 三、 思考过程的逻辑演进\n\n从宏观观察到具体方法论的提出，作者的思考路径经历了以下四个阶段的演进：\n\n#### 1. 观察与痛点识别：从“手写规则”到“维护危机”\n*   **观察**：编译器优化依赖大量启发式规则（如内联、寄存器分配），这些规则通常由专家基于直觉编写。\n*   **痛点**：现代软硬件变化太快，旧的假设（如硬件特性、代码模式）不断失效。人类专家手动调整这些规则不仅效率低，而且很难穷举复杂的权衡空间。\n*   **思考**：必须自动化，但现有的自动化方案都有明显的副作用。\n\n#### 2. 假设与路径选择：寻找“中间地带”\n*   **审视现有 AI 路径**：\n    *   *路径 A（LLM 直接生成）*：像 Copilot 一样写代码。**问题**：生成的代码可能不安全，且绕过了编译器现有的 Pass 架构。\n    *   *路径 B（ML/RL 策略）*：训练一个神经网络来决策。**问题**：为了部署一个模型，需要修改编译器基础设施以支持推理引擎，工程成本太高，难以推广到新的 Pass。\n*   **提出假设**：是否存在一种方法，既能利用 AI 的搜索能力，又能产出“原生”的 C++ 代码？\n*   **核心洞察**：启发式本质上就是一段代码（`if-else` 逻辑）。如果我们把“设计启发式”看作“写代码”，那么问题就转化为了**程序合成**。如果能直接搜索 C++ 代码空间，就能无缝集成到 LLVM 中，无需额外的推理引擎。\n\n#### 3. 方法论构建：分层搜索策略\n*   **挑战**：直接让 LLM 生成完整的、带具体数值参数的 C++ 代码效率极低。LLM 擅长逻辑结构，但不擅长猜具体的数值阈值（如 `threshold = 75` 还是 `76`）。\n*   **策略演进**：将搜索空间解耦。\n    *   **高层（LLM 负责）**：生成**代码模板**。只关注逻辑结构（如“如果函数是只读的，增加奖励”），将具体数值留空（符号化参数）。\n    *   **低层（Autotuner 负责）**：填充**超参数**。一旦模板确定，使用传统的黑盒优化器（如 Vizier）快速寻找最优数值。\n*   **闭环设计**：构建一个“生成-编译-评估-反馈”的闭环。评估必须基于真实的端到端宏基准测试，而不是合成指标，以确保结果在现实世界有效。\n\n#### 4. 验证与迭代：从“玩具问题”到“生产级”\n*   **初步验证**：选择经典的函数内联问题。先在“特征空间”验证（利用现有特征），再扩展到“API 空间”（让 LLM 自己调用 LLVM API）。\n*   **发现现象**：API 空间搜索虽然起步慢，但上限更高，因为它能发现人类未曾定义的特征组合。\n*   **解决性能瓶颈**：在优化运行时性能时，单纯靠 LLM 猜测很难突破。作者发现利用“模型延续”策略——即用上一代模型生成的最佳代码作为种子，喂给下一代更强的模型——可以有效突破性能天花板。\n*   **最终形态**：Magellan 不仅仅是一个实验工具，而是一个通用的 Agent 框架，能够处理 LLVM（内联、寄存器分配）乃至 XLA（图重写、自动分片）等不同编译器后端的问题。\n\n---\n\n**总结**：作者的思考过程是从**“人工维护不可持续”**的现实出发，拒绝了**“黑盒模型集成”**的重工程路线，创新性地提出了**“白盒代码进化”**的中间路线，并通过**“逻辑与参数解耦”**的分层策略，解决了 LLM 在数值搜索上的低效问题，最终实现了超越人类专家且易于部署的编译器优化。"
                },
                {
                    "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
                    "arxiv_id": "2601.22149",
                    "authors": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu",
                    "summary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建与改进LLM智能体”**： 论文提出了 **DynaWeb**，这是一个基于模型的强化学习（MBRL）框架。其核心贡献在于解决Web智能体训练过程中的效率低下和成本高昂问题，而非简单地将智能体应用于某个垂直领域。这直接对应了筛选目标中的“构建、改进或演化 LLM智能体”。 2.  **属于“自我演化”与“单智能体”范畴**： *   **自我演化**：论文引入了“世界模型”概念，允许智能体在模拟环境中生成大量轨迹进行“做梦”和在线强化学习。这种通过环境反馈和模拟交互来不断优化策略的过程，本质上是一种自我完善和迭代的机制，符合“自我演化”的定义。 *   **单智能体**：研究聚焦于提升单个Web智能体在复杂任务中的表现和规划能力。 3.  **通过排除标准验证**： *   **非应用型论文**：虽然实验在WebArena等基准上进行，但论文的核心是提出一种通用的训练框架，而非解决特定的生物、金融或法律问题。 *   **非基础设施或安全对齐**：论文不涉及硬件加速、部署优化，也不主要关注安全性、对齐或幻觉问题。 *   **非纯推理或多模态**：虽然涉及网页表示，但重点在于智能体的训练范式，而非视觉模型本身的改进。 综上所述，DynaWeb提出了一种通过构建世界模型来高效训练和演化Web智能体的新方法，紧扣Agentic AI及其演化的研究主题。",
                    "summary2": "本文旨在解决Web Agent在线强化学习成本高昂且存在风险的问题。针对Web导航场景，我们提出了一种基于Web World Model的MBRL框架DynaWeb，通过生成想象轨迹替代实时交互，并结合真实专家轨迹进行优化。我们在WebArena和WebVoyager基准上通过Success Rate验证了其有效性，显著提升了Agent性能。",
                    "summary_translation": "由大语言模型和强化学习驱动的自主网络代理的发展，代表了迈向通用人工智能助手的重要一步。然而，训练这些代理受到与实时互联网交互相关挑战的严重阻碍，这种方式效率低下、成本高昂且风险重重。基于模型的强化学习通过学习环境的世界模型来实现模拟交互，为此提供了一个极具前景的解决方案。本文介绍了 DynaWeb，这是一种新颖的 MBRL 框架，通过与一个网络世界模型进行交互来训练网络代理，该模型经过训练，能够根据代理的动作预测自然逼真的网页表示。该模型作为一个合成网络环境，代理策略可以通过生成大量的推演动作轨迹在其中进行“想象”，从而实现高效的在线强化学习。除了自由策略推演外，DynaWeb 还引入了来自训练数据的真实专家轨迹，这些轨迹在训练过程中与在线策略推演随机交错，以提高训练的稳定性和样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准测试上进行的实验表明，DynaWeb 能够持续且显著地提升最先进开源网络代理模型的性能。我们的研究结果确立了通过“想象”训练网络代理的可行性，为扩展在线代理强化学习提供了一种可扩展且高效的途径。",
                    "inspiration_trace": "基于对论文《DynaWeb: Model-Based Reinforcement Learning of Web Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **愿景与现状：** 人工智能正转向基于LLM的自主Web智能体，这些智能体通过多步交互完成复杂任务。虽然提示工程和模仿学习已有进展，但**在线强化学习（RL）** 被证明能显著提升智能体的鲁棒性和长程决策能力。\n2.  **核心冲突：** 尽管在线RL前景广阔，但其效果受到**与实时互联网交互**这一根本瓶颈的严重制约。这种交互不仅低效、昂贵，而且充满风险（如不可逆的操作、非确定性动态、外部干扰）。\n3.  **现有方案的局限：** 为了解决上述问题，学界开始探索“Web世界模型”。然而，现有工作仅将其作为**辅助工具**：要么用于推理时的短期前瞻（决策时的规划），要么用于生成离线轨迹进行监督微调。这些方法并未将“想象”作为在线策略优化的核心驱动力。\n4.  **范式转移：** 作者提出回归经典的基于模型的强化学习（MBRL，如Dyna架构），将世界模型从“规划工具”提升为**学习基质**。通过让智能体在合成环境中“做梦”并进行策略优化，从而在保留RL交互学习优势的同时，彻底摆脱对实时Web交互的依赖。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何在不依赖大规模实时Web交互的情况下，保留在线强化学习对Web智能体的优势，从而实现高效、安全且可扩展的智能体优化？”**\n\n---\n\n### 三、 核心方法产出的逻辑演进链\n\n以下是从宏观观察到具体方法论的思维演进过程：\n\n#### 1. 宏观观察：RL的潜力与现实的困境\n*   **观察：** Web智能体要具备真正的泛化和鲁棒性，必须通过RL进行试错学习，而不仅仅是模仿专家数据。\n*   **痛点：** 现实中的Web环境是一个糟糕的“训练场”。让智能体在真实的互联网上通过试错来学习，成本极高（API费用、时间），且风险不可控（误删数据、恶意购买）。\n*   **推论：** 我们必须找到一种方法，将“训练环境”与“真实互联网”解耦。\n\n#### 2. 假设提出：用“想象”替代“体验”\n*   **灵感：** 经典RL理论（如Dyna架构）和现代MBRL（如Dreamer）表明，如果一个智能体拥有一个准确的世界模型，它就可以在脑海中模拟交互，从而获得经验。\n*   **假设：** 如果我们能训练一个Web世界模型来模拟网页状态的变化，那么智能体就可以在这个虚拟的“梦境”中进行RL训练，而不需要触碰真实的互联网。\n\n#### 3. 现状批判：世界模型不应只是“参谋”\n*   **审视现有工作：** 目前大多数Web世界模型仅用于“推理时”的辅助（例如：在决策前模拟几步看看哪个动作好）。这虽然能提升单次决策质量，但并没有直接改进智能体的策略参数。\n*   **逻辑断层：** 既然世界模型能模拟未来，为什么不直接利用这些模拟产生的轨迹来更新策略？\n*   **新思路：** 将世界模型提升为**核心训练组件**。让智能体在世界模型中生成的轨迹直接作为RL算法（如PPO）的输入数据，实现真正的“基于想象的在线强化学习”。\n\n#### 4. 技术挑战与修正：如何处理“幻觉”与“漂移”\n*   **挑战：** 世界模型毕竟不是完美的现实。如果完全在模拟环境中训练，智能体可能会学到适应“幻觉”世界的策略，导致在真实环境中失效。\n*   **修正思路：** 我们需要一个“锚点”。\n*   **解决方案：** 采用**混合训练策略**。在训练数据中，将“世界模型生成的想象轨迹”与“真实的专家轨迹”随机混合。真实数据提供了现实的基准，防止模型在幻觉中迷失；想象数据则提供了大规模、低成本的探索空间。\n\n#### 5. 具体落地：构建可学习的Web动力学\n*   **表征选择：** Web环境是文本和结构化的。为了模拟，模型需要预测网页的变化。\n*   **机制设计：** 直接预测整个下一个网页的HTML或文本效率低且冗余（大部分页面内容不变）。\n*   **优化：** 将预测任务分解。先让世界模型预测**状态变化的描述**，再基于当前状态和变化描述生成下一个状态。这种“推理+应用”的模式更符合LLM的能力，也提高了训练效率。\n\n#### 6. 最终方法论：DynaWeb 框架\n*   **整合：** 结合上述思考，构建闭环：\n    1.  **世界模型训练：** 基于真实数据训练一个能预测网页状态变化的LLM。\n    2.  **想象驱动RL：** 智能体在世界模型中生成多步轨迹，利用模型自评估获得奖励信号。\n    3.  **混合优化：** 使用序列级策略优化（GSPO），同时处理想象轨迹和真实专家轨迹，实现高效且稳定的策略提升。\n\n---\n\n**总结：** 作者的思考路径是从**解决RL训练的高成本痛点**出发，通过**引入世界模型作为替代环境**，进而**突破现有世界模型仅用于推理的局限**，最终通过**混合真实与想象数据**的策略，在保证落地可行性的前提下，实现了Web智能体训练范式的革新。"
                },
                {
                    "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
                    "arxiv_id": "2601.22129",
                    "authors": "Yifeng Ding, Lingming Zhang",
                    "summary": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 范畴**： 论文的核心贡献是提出了 **SWE-Replay**，这是一种针对软件工程智能体的高效测试时扩展技术。这直接对应了筛选标准中的“构建、改进或演化 LLM智能体”。论文关注的是如何改进智能体在执行任务时的行为模式（即如何通过回收和利用历史轨迹来优化当前的决策过程），而非仅仅将智能体作为一个黑盒工具去应用。 2.  **涉及自我演化与迭代改进机制**： 论文明确提到通过“recycling trajectories from prior trials”（回收先前的试验轨迹）和“exploit archived experience”（利用归档的经验）来优化智能体的表现。这完全符合筛选标准中“自我演化”的定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。SWE-Replay 本质上就是一种让智能体在测试时利用过往经验进行自我优化的机制。 3.  **属于智能体的规划与推理范畴**： 论文讨论了智能体在复杂任务中的多步推理过程，涉及“trajectory”（轨迹）、“branching at critical intermediate steps”（在关键中间步骤分支）以及“tool use”（使用自定义bash脚本）。这符合筛选标准第四步中关于“保留：如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理”的规定。 4.  **非简单的领域应用**： 虽然论文的评估背景是软件工程（SWE），但其核心贡献并非解决某个具体的软件问题，而是提出了一种通用的、可泛化的智能体测试时扩展框架。根据筛选标准第四步的特殊情况处理，这种提出新机制（即使应用在特定领域）的论文属于保留范围。 综上所述，SWE-Replay 提出了一种改进 LLM 智能体效率和性能的新方法，涉及经验利用和迭代优化，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决软件工程智能体测试时扩展计算成本高昂且现有方法难以泛化的问题。针对现代智能体框架，我们提出了一种名为 SWE-Replay 的高效测试时扩展技术，通过回收先前轨迹并在关键中间步骤分支来优化扩展过程。我们在 SWE-Bench Verified、Pro 和 Multilingual 数据集上通过 resolve rate 和平均成本验证了其有效性，实现了成本降低 17.4% 且性能提升 3.8%。",
                    "summary_translation": "Test-time scaling（测试时扩展）已被广泛采用，以增强软件工程任务中大语言模型智能体的能力。然而，从头开始重复采样轨迹的标准方法计算成本高昂。尽管近期的方法尝试利用专用 value agents（价值智能体）来降低成本，但它们可能面临模型 miscalibration（校准偏差）的问题，且难以泛化到将合成自定义 bash 脚本作为工具的现代智能体。在本文中，我们介绍了 SWE-Replay，这是首个针对现代智能体的高效且可泛化的 test-time scaling（测试时扩展）技术，无需依赖可能存在噪声的 value estimates（价值估计）。SWE-Replay 通过复用先前试验的 trajectories（轨迹）来优化扩展过程，动态选择是进行从头探索，还是在关键 intermediate steps（中间步骤）处分叉以利用归档经验。这种对 intermediate steps（中间步骤）的选择是由 repository exploration（代码仓库探索）的潜力和推理重要性驱动的，而非基于外部 LLM 的 quality estimates（质量估计）。我们的评估表明，在 SWE-Bench Verified 数据集上，SWE-Replay 始终优于 naive scaling（朴素扩展），在保持甚至将性能提高高达 3.8% 的同时，将成本降低了高达 17.4%。在 SWE-Bench Pro 和 Multilingual 数据集上的进一步评估验证了 SWE-Replay 的泛化能力，确立了其作为软件工程智能体高效 test-time scaling（测试时扩展）的坚实基础。",
                    "inspiration_trace": "基于论文《SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观背景与趋势观察\n**思考起点：** 大语言模型（LLM）在软件工程领域的能力演进。\n*   **观察：** LLM已从简单的代码补全工具演变为能够自主导航代码库、执行测试并提交补丁的复杂智能体。\n*   **现状：** 现代智能体框架（如SWE-agent, OpenHands）通过赋予LLM终端、编辑器等工具，使其能够处理复杂的软件任务。\n*   **核心驱动力：** 为了进一步提升这些智能体的能力，学术界和工业界普遍采用“测试时扩展”策略，即通过增加推理时的计算量（如生成多个候选解）来换取更高的解决方案质量。\n\n### 2. 引入问题：Introduction 的“讲故事”逻辑\n作者在引言中构建了一个层层递进的逻辑链条，揭示了当前研究面临的困境：\n\n1.  **有效性确认：** 现有研究表明，通过多次采样并基于测试反馈选择最终答案，可以显著提升SWE任务的表现，且性能随样本数量呈对数线性增长。\n2.  **成本痛点：** 这种“从零开始重复采样”的标准做法计算成本极其昂贵。\n3.  **现有尝试及其局限：** 为了降低成本，前人工作（如SWE-Search, Satori-SWE）试图利用价值智能体或奖励模型来修剪低质量路径。\n    *   **局限一（准确性）：** 价值模型和奖励模型容易出现“模型误校准”，引入噪声，导致错误的修剪。\n    *   **局限二（通用性）：** 现有方法通常针对基于特定工具模板的流水线设计。然而，现代智能体倾向于合成自定义的bash脚本，这种灵活性使得设计针对特定工具的评估提示变得不可行，导致旧方法无法兼容。\n4.  **缺口：** 目前缺乏一种既高效又通用的测试时扩展技术，能够适应现代智能体，且不依赖可能存在噪声的价值估计。\n\n**显式总结：研究问题**\n> **如何在不依赖潜在噪声的价值估计或预定义工具模板的前提下，为现代软件工程智能体设计一种高效且通用的测试时扩展技术？**\n\n### 3. 逻辑演进与假设形成\n针对上述研究问题，作者的思考路径经历了从“否定旧方案”到“构建新范式”的演进：\n\n*   **思考转折点：** 既然依赖外部模型（LLM-as-a-Judge 或 Value Agent）来评估步骤质量既昂贵又不可靠，且无法适应现代智能体的自由度，那么我们是否可以**完全放弃对“质量”的显式预测**，转而利用**已有的计算过程**？\n\n*   **核心假设：** 之前生成的轨迹并非全无价值。即使一个轨迹最终失败了，其中间步骤可能已经探索了有价值的代码区域或进行了重要的推理。如果我们能从这些中间步骤“分叉”出去，而不是从头开始，就能节省大量计算成本，同时探索新的解空间。\n\n*   **方法论雏形：** 提出“重放”机制。\n    *   **动作：** 维护一个轨迹档案，每次迭代时，随机选择是从头探索，还是从档案中的某个中间步骤恢复环境并继续探索。\n    *   **关键挑战：** 如果不使用LLM来打分，**如何选择从哪个步骤进行重放？**\n\n*   **解决挑战的直觉（替代LLM打分的启发式规则）：**\n    1.  **过滤低质：** 首先排除那些导致回归测试失败的轨迹，因为它们可能包含误导性步骤。\n    2.  **探索潜力（状态抽象）：** 我们希望探索那些“未被充分探索”的区域。如果某个步骤涉及到的文件集合在档案中很少见，那么这个步骤就具有很高的探索潜力。\n    3.  **推理重要性（结构特征）：** 我们希望分叉发生在“关键决策点”。现代智能体每步都有显式推理，推理内容越长（如分段越多），通常意味着该步骤越关键、越复杂。\n\n### 4. 最终方法论确立\n基于上述逻辑，作者最终确立了 **SWE-Replay** 的核心框架：\n\n1.  **档案管理：** 存储所有历史轨迹。\n2.  **采样策略：** 动态平衡“从头探索”与“利用经验”。\n3.  **无模型步骤选择：** 摒弃LLM打分，采用基于**文件级抽象状态**（鼓励探索长尾文件）和**推理强度**（鼓励在思考密集点分叉）的轻量级启发式算法来选择分支点。\n4.  **环境恢复：** 通过应用差异而非重放所有动作来高效恢复环境状态。\n\n### 总结\n作者的思考过程是从**“测试时扩展虽好但太贵”**这一现实矛盾出发，批判了**“用模型预测质量来省钱”**这一主流思路的不可靠性（误校准）和不兼容性（无法处理自定义脚本）。最终，作者转换视角，从**“计算复用”**的角度切入，利用代码库探索的稀疏性和推理的结构特征，设计出了一套不依赖外部价值判断、纯粹基于搜索空间统计特性的高效扩展方法。"
                },
                {
                    "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
                    "arxiv_id": "2601.22060",
                    "authors": "Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang",
                    "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。以下是详细的判断过程： 1.  **核心判断（符合 Agentic AI 定义）**: *   论文的核心贡献是提出了 \"Vision-DeepResearch\"，这是一个新的多模态深度研究范式。 *   该范式明确采用了 \"reasoning-then-tool-call\"（推理后调用工具）的机制，涉及多轮、多实体、多尺度的视觉和文本搜索。 *   这完全符合 **Agentic AI** 中关于 **单智能体** 的定义，特别是 **工具使用** 和 **规划** 能力。论文描述的智能体能够执行数十个推理步骤和数百次引擎交互，这属于典型的复杂任务规划和自主执行。 2.  **正面指标（高度匹配）**: *   **核心范式**: 论文构建了一个基于LLM的智能体框架。 *   **智能体能力**: 明确涉及 `Tool Use / Tool Augmentation`（调用搜索引擎）、`Planning`（多轮、多尺度搜索策略）以及复杂的多步 `Reasoning`。 *   **演化机制**: 虽然不是运行时的自我演化，但论文提到通过 \"cold-start supervision and RL training\" 将深度研究能力内化到模型中，这属于对智能体能力的 **改进** 和构建方法。 3.  **排除标准（特殊情况处理）**: *   **多模态与视觉**: 尽管论文标题和摘要大量提及 \"Multimodal\"、\"Vision\" 和 \"MLLMs\"，但根据筛选标准中的特殊规则：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。 *   在这篇论文中，视觉能力是智能体感知环境（搜索结果）的一部分，而研究的 **核心** 在于智能体的 **深度研究范式**、**搜索策略** 以及 **推理与工具调用的循环机制**，而非视觉模型架构本身的改进。因此，它不应被视为单纯的视觉论文被排除，而应被视为具备视觉感知能力的 Agentic AI 论文。 4.  **综合结论**: 该论文的核心在于构建一个具备高级规划和工具使用能力的 LLM 智能体，旨在解决复杂的多模态信息检索和推理问题。它属于 Agentic AI 中的单智能体方向，符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有多模态大语言模型在深度研究中面临的搜索命中率低和推理深度不足的问题。针对真实噪声环境下的视觉与文本搜索场景，我们提出了一种Vision-DeepResearch新范式，通过多实体、多尺度视觉搜索及长视距轨迹生成，结合SFT和RL训练内化深度研究能力。我们在VDR-Bench、FVQA等六个基准测试上通过准确率验证了其有效性，性能显著优于现有SOTA模型。",
                    "summary_translation": "多模态大语言模型在广泛的视觉任务中取得了显著的成功。然而，受限于其内部世界知识的容量，先前的工作提出通过“推理后调用工具”的方式增强 MLLMs，利用视觉和文本搜索引擎，在需要大量事实信息的任务上获得显著收益。然而，这些方法通常在一种朴素的设置下定义多模态搜索，假设单张全图级或实体级图像查询以及少量文本查询就足以检索回答问题所需的关键证据，这在存在大量视觉噪声的现实场景中是不切实际的。此外，它们在推理深度和搜索广度上往往受限，难以解决需要从不同视觉和文本来源聚合证据的复杂问题。基于此，我们提出了 Vision-DeepResearch，它提出了一种新的多模态深度研究范式，即执行多轮、多实体和多尺度的视觉与文本搜索，以便在强噪声下稳健地命中现实世界的搜索引擎。我们的 Vision-DeepResearch 支持数十个推理步骤和数百次引擎交互，同时通过冷启动监督和强化学习训练将深度研究能力内化到 MLLM 中，从而构建出一个强大的端到端多模态深度研究 MLLM。它显著优于现有的多模态深度研究 MLLMs，以及基于强大的闭源基础模型（如 GPT-5、Gemini-2.5-pro 和 Claude-4-Sonnet）构建的工作流。代码将在 https://github.com/Osilly/Vision-DeepResearch 发布。",
                    "inspiration_trace": "基于对论文《Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观观察与初始定位\n**思考起点：** 多模态大语言模型（MLLMs）虽然在通用视觉任务上表现优异，但受限于内部参数存储的“世界知识”有限，在面对需要大量外部事实知识的复杂视觉问答（VQA）时，往往力不从心。\n\n**现有尝试：** 学术界开始引入“推理后调用工具”的范式（即 ReAct 模式），让模型像使用搜索引擎一样去查询外部知识。这确实在一定程度上缓解了知识匮乏的问题。\n\n**初步质疑：** 虽然给了模型工具，但现有的“多模态搜索”定义是否过于理想化？模型真的学会了像人类一样在复杂的互联网环境中“做研究”吗？\n\n---\n\n### 2. 问题聚焦：Introduction 中的“故事”逻辑\n作者通过层层递进的逻辑，揭示了现有方法在真实场景中的脆弱性，从而引出核心痛点。\n\n**逻辑链条推演：**\n\n1.  **背景铺垫：** MLLMs 虽强，但缺乏内部知识。为了解决事实密集型问题，现有工作给模型装上了“推理+工具调用”的翅膀，让它们去查搜索引擎。\n2.  **现实打击（痛点一：搜索环境的“幼稚化”假设）：**\n    *   *现有假设：* 只要给一张全图或实体级图片，再配几个文本查询，就能搜到关键证据。\n    *   *现实情况：* 真实世界充满噪声。全图检索往往被无关的背景噪声主导；搜索引擎极其不稳定，同一个实体在不同尺度下检索结果天差地别。\n    *   *结论：* 现有方法忽略了**“命中率问题”**，单次检索在噪声环境下极不可靠。\n3.  **能力瓶颈（痛点二：推理深度的“浅尝辄止”）：**\n    *   *现有表现：* 现有的多模态深度研究模型往往只能产生很短的轨迹（平均少于5轮检索）。\n    *   *现实需求：* 复杂问题需要从多源聚合证据，进行多跳推理。\n    *   *结论：* 现有方法受限于**推理深度和搜索广度**，无法处理需要长链路思考的复杂任务。\n\n---\n\n### 3. 核心研究问题\n基于上述对现实噪声和模型能力局限的深刻洞察，作者试图回答以下核心问题：\n\n**“如何构建一种新的多模态深度研究范式，使其能够在充满噪声的真实网络环境中，通过多轮、多实体、多尺度的视觉与文本搜索，克服低命中率问题，并实现长视界的复杂推理？”**\n\n---\n\n### 4. 方法论演进：从思想到落地的逻辑链\n\n为了解决上述问题，作者的思考路径经历了从“模仿人类行为”到“数据工程构建”，再到“训练策略优化”的演进。\n\n#### 第一阶段：行为建模——从“一锤子买卖”到“试错探索”\n*   **思考：** 人类在嘈杂环境中搜索时，如果第一次没搜到，会怎么做？我们会裁剪图片、调整关键词、换个角度再试。\n*   **方法论转化：** 必须放弃“单次检索”的设定，转而支持**多实体、多尺度的视觉裁剪与搜索**。这不仅是技术上的改变，更是将检索建模为一个“试错”过程，通过自适应地探索不同尺度的区域来提高命中率。\n\n#### 第二阶段：数据构建——如何教会模型“长跑”？\n*   **思考：** 现有的训练数据都是短轨迹，模型自然学不会长推理。我们需要高质量的、包含几十步推理和数百次引擎交互的长轨迹数据。但人工标注不可能，如何自动生成？\n*   **方法论转化：**\n    1.  **视觉端：** 利用MLLM生成多尺度的裁剪框，诱导其进行视觉搜索，并引入“终止策略”来判断证据是否充足。\n    2.  **文本端：** 既然文本领域的深度研究模型（如DeepResearch LLM）已经很强，不如“借力”。通过**文本桥接**，将视觉轨迹转化为文本描述，让强大的文本模型接管后续的长链路推理，最后再合并回多模态轨迹。\n    3.  **数据质量：** 为了避免模型“走捷径”，设计了**模糊多跳VQA合成**，通过混淆答案和实体，强制模型必须进行多轮搜索才能解出题目。\n\n#### 第三阶段：训练策略——从“模仿”到“内化”\n*   **思考：** 有了长轨迹数据，直接让模型死记硬背（SFT）可能不够，模型在真实环境中可能还是不知道何时该停、何时该搜。需要让模型在真实环境中“学会”决策。\n*   **方法论转化：**\n    1.  **冷启动监督（SFT）：** 先用生成的轨迹教模型基本的“多轮、多尺度”搜索模式，让它知道什么是“像样的研究”。\n    2.  **强化学习（RL）：** 将模型放入真实的在线搜索环境中。通过设计奖励函数（答对给分，答错没分）和处理长轨迹的工程技巧（如异步Rollout、掩码异常轨迹），让模型在不断的试错中**内化**出长视界的决策能力，从而在嘈杂环境中自动优化搜索策略。\n\n---\n\n### 总结\n作者的思考过程是一个**“发现理想假设与现实的鸿沟 -> 提出拟人化的解决思路 -> 通过工程手段构建数据桥梁 -> 利用强化学习固化能力”**的完整闭环。其核心创新在于不再将搜索视为简单的工具调用，而是将其视为一种在噪声中通过多尺度探索和长链路思考来逼近真相的智能行为。"
                },
                {
                    "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation",
                    "arxiv_id": "2601.21469",
                    "authors": "Haoji Zhang, Yuzhe Li, Zhenqiang Liu, Chenyang Liu, Shenyang Zhang, Yi Zhou",
                    "summary": "While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”方向**： 论文的核心贡献是提出了 **DebateCoder**，这是一个**多智能体协作框架**。它通过构建三个具有不同角色的智能体（User Agent, Technical Agent, Quality Assurance Agent）来协同工作，这直接对应了研究焦点中的 **Multi-Agent (多智能体)** 方向，特别是智能体间的协作与通信机制。 2.  **包含智能体关键能力**： 论文中提到的 **Adaptive Confidence Gating**（自适应置信度门控）、**multi-turn deliberation module**（多轮审议模块）以及 **reviewer-guided analytical debugging loop**（评审引导的分析调试循环），体现了智能体的 **Planning**（规划/审议）、**Self-Correction**（自我修正）和 **Iterative Improvement**（迭代改进）能力。这些属于筛选标准中“正面指标”所列出的核心能力。 3.  **属于方法论创新而非单纯应用**： 虽然论文的应用场景是代码生成，但其本质并非简单地将现有LLM作为工具应用，而是提出了一种新的**结构化角色扮演协议**和**协作机制**来解决小模型（SLMs）的推理瓶颈。这符合第一步中“保留”关于构建LLM智能体或新框架的规则，不属于“非演化型应用”的排除范畴。 4.  **不涉及排除项**： 论文不涉及安全对齐、多模态视觉技术或图神经网络等排除标准中的内容。 综上所述，该论文通过构建多智能体协作框架来提升模型性能，属于Agentic AI的核心研究范畴。",
                    "summary2": "本文旨在解决小语言模型在复杂代码生成中面临的推理瓶颈和失败循环问题。针对资源受限环境，我们提出了一种名为DebateCoder的多智能体协作框架，该框架引入了Adaptive Confidence Gating机制和Reviewer-guided调试循环。在HumanEval和MBPP数据集上，通过Pass@1准确率和API开销验证了其有效性，实验显示该方法在显著提升准确率的同时降低了约35%的API调用成本。",
                    "summary_translation": "尽管 Large Language Models (LLMs，大语言模型) 推动了自动代码生成的突破，但 Small Language Models (SLMs，小语言模型) 在应对复杂逻辑需求时，常遭遇推理瓶颈和失败循环。为克服这些挑战，我们提出了 DebateCoder，这是一个旨在提升资源受限环境下 Small Language Models (SLMs，小语言模型，例如 Pangu-1B) 推理能力的多智能体协作框架。DebateCoder 采用包含三个智能体的结构化角色扮演协议：User Agent (A_UA，用户智能体)、Technical Agent (A_TA，技术智能体) 和 Quality Assurance Agent (A_QA，质量保证智能体)。该框架还引入了 Adaptive Confidence Gating mechanism (自适应置信度门控机制)，设定 95% 的阈值以平衡准确性与推理效率。此外，我们引入了多轮审议模块和审查者引导的分析调试循环，分别用于正交的生成前辩论和生成后优化。在 HumanEval 和 MBPP 数据集上的实验表明，DebateCoder 在 HumanEval 上达到了 70.12% 的 Pass@1，性能优于 MapCoder，同时将 API 开销降低了约 35%。这些结果表明，协作协议能够缓解小参数模型的局限性，为实现高质量的自动化软件工程提供了一种可扩展且高效的方法。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation",
                    "arxiv_id": "2601.21464",
                    "authors": "Yuan Sui, Bryan Hooi",
                    "summary": "Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **核心判断（符合）**： *   论文的核心贡献是提出了 **CoNL** 框架，这是一个通过 **多智能体自我博弈** 来实现 **自我演化** 的新方法论。 *   它不是简单的应用，也不是基础设施优化，而是专注于如何构建和改进智能体本身的能力（生成与评估能力）。 2.  **研究焦点匹配（高度相关）**： *   **自我演化**：论文标题明确包含 \"Self-Evolving\"，摘要中详细描述了通过 \"Meta-Evaluation\"（元评估）和 \"Self-play\"（自我博弈）来实现智能体的自我完善和迭代，完全符合您定义的 \"Self-Evolving\" 方向。 *   **多智能体**：论文利用了 \"Multi-Agent Self-Play\"（多智能体自我博弈）和 \"Structured Conversations\"（结构化对话），智能体之间相互提出、批评和修改方案，这属于典型的多智能体协作与交互机制。 3.  **排除标准检查（通过）**： *   论文虽然涉及 \"ethical reasoning\"（伦理推理）作为任务场景，但其核心贡献并非关于安全或对齐技术，而是关于学习框架本身。 *   不涉及多模态、视觉或图技术。 4.  **特殊规则处理**： *   论文提出的 CoNL 框架是一种新的“自我演化”机制。尽管它被应用于 \"Non-verifiable tasks\"（如创意写作、对话）这一特定领域，但根据第四步的“自我演化的应用”规则，只要核心是提出新的演化机制，就应当保留。 综上所述，该论文在多智能体协作和自我演化机制上做出了核心贡献，是您研究课题下的高质量前沿论文。",
                    "summary2": "本文旨在解决非可验证任务中缺乏真实标签导致的LLM训练困难及评估者偏差问题。针对非可验证任务场景，我们提出了一种名为CoNL的多智能体自我博弈框架，利用结构化对话和诊断奖励机制统一生成、评估与元评估。并在DeepMath、AIME等五个基准测试上，通过Pass@1和Rank-$\\rho$等指标验证了其有效性。",
                    "summary_translation": "由于缺乏 ground-truth labels（真实标签），针对 non-verifiable tasks（不可验证任务，如创意写作、对话和伦理推理）训练 large language models (LLMs，大语言模型) 仍然是一项挑战。尽管 LLM-as-Judge（LLM作为评判者）方法为 human feedback（人类反馈）提供了一种可扩展的替代方案，但它们面临一个根本性的局限：其性能受限于 evaluator（评估者）自身的质量。如果 judge（评判者）无法识别出优秀的解决方案，它就无法提供有用的 training signals（训练信号），而且 evaluation biases（评估偏差，例如偏向冗长而非质量）的问题也仍未得到解决。这激发了对 meta-evaluation（元评估）的需求：即评估并改进 evaluator（评估者）本身的能力。我们提出了 CoNL，这是一个通过 multi-agent self-play（多智能体自我博弈）将 generation（生成）、evaluation（评估）和 meta-evaluation（元评估）统一起来的框架。我们的核心洞察在于：critique quality（批评质量）可以通过其是否有助于他人改进解决方案来衡量。在 CoNL 中，多个 sharing the same policy（共享相同策略）的 agents（智能体）参与 structured conversations（结构化对话），以提出、批评和修改解决方案。能够促成 solution improvements（解决方案改进）的 critiques（批评）将获得 diagnostic reward（诊断奖励），这为 meta-evaluation（元评估）创造了 explicit supervision（显式监督），并使得无需 external judges（外部评判者）或 ground truth（真实标签），即可通过 self-play（自我博弈）实现对 generation（生成）和 judging capabilities（评判能力）的 joint optimization（联合优化）。在五个 benchmarks（基准测试）上的实验表明，CoNL 相比 self-rewarding baselines（自奖励基线）取得了 consistent improvements（一致的改进），同时保持了 stable training（稳定的训练）。",
                    "inspiration_trace": "基于对论文内容的深度分析，以下是对作者产出该文章逻辑链的系统性推演，还原了从宏观问题到具体方法论的思考过程。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的叙事逻辑，旨在揭示当前研究范式的核心痛点：\n\n1.  **现状与反差**：大语言模型（LLMs）在数学、编程等**可验证任务**（有明确标准答案）上表现出色，但在创意写作、伦理推理等**不可验证任务**（缺乏客观真值）上仍面临巨大挑战。\n2.  **现有方案的局限**：\n    *   **RLHF（人类反馈强化学习）**：虽然有效，但依赖昂贵的人工标注，难以扩展。\n    *   **LLM-as-Judge（模型作为裁判）**：作为一种可扩展的替代方案，虽然解决了成本问题，但存在一个**根本性缺陷**——其性能上限受限于裁判者本身的质量。\n3.  **核心矛盾**：如果裁判无法识别好的解决方案，它就无法提供有效的训练信号。更糟糕的是，现有的裁判模型往往存在偏见（如偏爱冗长的回答），且缺乏机制去纠正这些偏见。\n4.  **关键缺失**：现有方法普遍缺乏**元评估**，即“评估评估者”的能力。这导致模型处于一个“回音室”中，性能被初始偏见所封顶。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链的终点，作者提出的核心研究问题为：\n\n**“在缺乏客观真值标签的情况下，我们能否训练出具备元评估能力的模型，使其不仅能生成高质量解决方案，还能通过自我博弈不断改进其评判标准？”**\n\n---\n\n### 三、 作者产出核心方法的逻辑演进\n\n为了回答上述问题，作者的思考过程经历了从宏观观察到微观机制设计的四个阶段：\n\n#### 1. 宏观观察：从“静态裁判”到“动态协作”\n*   **观察**：现有的 LLM-as-Judge 方法通常假设裁判是静态的，或者认为评判能力会随着生成能力的提升而自然提升。但事实并非如此，裁判的偏见（如喜欢长文本）会被模型利用，导致训练崩溃。\n*   **反思**：在没有上帝视角（Ground Truth）的世界里，谁来判断裁判是对是错？人类社会的经验表明，**共识**往往比单一权威更可靠。\n\n#### 2. 类比与假设：维基百科模式的启发\n*   **灵感来源**：作者将目光投向了维基百科。维基百科没有中央权威，而是通过“贡献者生成内容 -> 同行评审 -> 修订”的迭代过程来保证质量。\n*   **核心假设**：如果一个评审者的批评意见能够促使他人改进解决方案，并且这种改进被群体所接受，那么这个评审者就具备良好的判断力。\n*   **逻辑转换**：**“批评的质量” = “是否帮助他人改进”**。这为在没有 Ground Truth 的情况下评估“评估者”提供了一个可观测的代理指标。\n\n#### 3. 机制设计：构建多智能体对话\n*   **如何验证假设？**：需要一个环境，让智能体既能生成方案，又能互相批评，还能根据批评修改方案。\n*   **设计思路**：构建一个多智能体自我博弈框架。\n    *   **生成**：多个智能体针对同一问题给出初始方案。\n    *   **评估与批评**：智能体之间互相进行盲评（避免串通）并给出具体的批评意见。\n    *   **修订**：智能体根据收到的批评意见修改自己的方案。\n    *   **最终裁决**：群体对修改后的方案进行再次评分。\n\n#### 4. 信号提取：定义“诊断奖励”\n*   **关键难题**：如何将上述对话过程转化为数学上的训练信号？\n*   **解决方案**：利用分数的变化。\n    *   如果 Agent A 批评了 Agent B，且 Agent B 修改后的方案得分显著提高（$\\Delta V > 0$），说明 Agent A 的批评是有效的。\n    *   由此定义**诊断奖励**：奖励那些能导致他人分数提升的批评行为。\n*   **闭环形成**：这个奖励信号不仅训练了生成能力（通过最终得分），更重要的是训练了**元评估能力**（通过批评带来的分数提升），从而打破了静态裁判的性能天花板。\n\n---\n\n### 总结\n\n作者的思考路径是从**“单一裁判的局限性”**出发，通过**“社会化协作（维基百科）”**的类比找到了突破口，最终通过**“多智能体对话中的改进幅度”**这一可量化指标，成功将不可验证任务转化为可自我进化的训练过程。"
                },
                {
                    "title": "Textual Equilibrium Propagation for Deep Compound AI Systems",
                    "arxiv_id": "2601.21064",
                    "authors": "Minghui Chen, Wenlong Deng, James Zou, Han Yu, Xiaoxiao Li",
                    "summary": "Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体/多智能体”方向的交叉研究。 1.  **核心贡献符合“构建、改进或演化 LLM智能体”**： 论文的核心贡献是提出了一种名为“文本平衡传播（TEP）”的新方法，旨在优化“深度复合AI系统”和“长视距Agentic工作流”。这直接对应了您筛选标准中的“改进LLM智能体”的方法论。 2.  **涉及“自我演化”与“自我反思”机制**： 论文中提到的“自由阶段”，利用“局部LLM评论家”迭代优化提示直至达到平衡，这本质上是一种智能体的自我反思和自我完善过程。TEP通过前向信号和局部适应来优化系统，属于智能体通过反馈进行自我演化的机制。 3.  **聚焦于“Agentic”能力与“工具使用”**： 摘要明确讨论了“长视距agentic工作流”以及“多智能体工具使用数据集”。论文解决的是智能体在复杂任务（如多步推理、工具调用）中遇到的梯度爆炸和消失问题，这直接关联到智能体的规划和工具使用能力。 4.  **非排除项**： 该论文不是将智能体简单应用于生物、医疗等特定领域（非演化型应用），也不是关于安全、对齐或多模态视觉的研究。它关注的是智能体系统本身的优化算法和架构演进。 综上所述，该论文提出了一种新的优化框架来解决深度智能体工作流中的核心问题，属于Agentic AI和Self-Evolving的前沿研究，因此予以保留。",
                    "summary2": "本文旨在解决深度复合AI系统中全局文本反向传播面临的梯度爆炸与消失问题。针对长视界工作流，我们提出了一种Textual Equilibrium Propagation (TEP)方法，通过局部自由阶段和微调阶段实现有界的提示词优化。在PubMedQA、HotpotQA及BigCodeBench等基准上，通过Accuracy、F1和Pass@1等指标验证了其有效性，TEP在深度增加时显著优于TextGrad等基线方法。",
                    "summary_translation": "大语言模型越来越多地被部署为 compound AI systems (复合AI系统) 的一部分，这些系统在 long-horizon workflows (长时程工作流) 中协调多个模块（例如，retrievers (检索器)、tools (工具)、verifiers (验证器)）。虽然全局传播 textual feedback (文本反馈) 的最新方法（例如 TextGrad）使得优化此类 pipelines (流水线) 成为可能，但我们发现，随着 system depth (系统深度) 的增加，性能会下降。特别是，long-horizon agentic workflows (长时程智能体工作流) 表现出两种 depth-scaling failure modes (深度扩展失效模式)：1) exploding textual gradient (文本梯度爆炸)，即 textual feedback (文本反馈) 随深度呈指数级增长，导致消息过长并放大 evaluation biases (评估偏差)；2) vanishing textual gradient (文本梯度消失)，即有限的长上下文能力导致模型过分强调 partial feedback (部分反馈)，且对长反馈的压缩导致下游消息在向上游传播多跳时逐渐失去 specificity (特异性)。\n\n为了缓解这些问题，我们引入了 Textual Equilibrium Propagation (TEP, 文本平衡传播)，这是一种受 energy-based models (基于能量的模型) 中的 Equilibrium Propagation (平衡传播) 启发的 local learning principle (局部学习原则)。TEP 包括两个阶段：1) free phase (自由阶段)，其中局部 LLM critics (评论家) 迭代地优化提示词，直到达到 equilibrium (平衡，即不再建议进一步的改进)；2) nudged phase (微扰阶段)，该阶段应用具有 bounded modification intensity (有界修改强度) 的 proximal prompt edits (近端提示词编辑)，使用通过 forward signaling (前向信号) 而非 backward feedback chains (反向反馈链) 传播的 task-level objectives (任务级目标)。这种设计支持 local prompt optimization (局部提示词优化)，随后是朝向 global goals (全局目标) 的 controlled adaptation (受控适应)，从而避免了 global textual backpropagation (全局文本反向传播) 的 computational burden (计算负担) 和 signal degradation (信号退化)。在 long-horizon QA (长时程问答) 基准测试和 multi-agent tool-use dataset (多智能体工具使用数据集) 上，TEP 始终比 TextGrad 等全局传播方法提高了准确性和效率。这种性能提升随深度增加而增长，同时保留了 deep compound AI system (深度复合AI系统) 中 black-box LLM components (黑盒LLM组件) 的实用性。",
                    "inspiration_trace": "基于对论文《Textual Equilibrium Propagation for Deep Compound AI Systems》的深入分析，以下是对作者产出该文章核心思考过程的系统性推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个层层递进的逻辑链条，旨在引出当前研究的必要性：\n\n1.  **宏观背景：** 人工智能的部署范式正在从单一的LLM转向**复合AI系统**。这些系统通过协调检索器、工具、验证器等多个模块，处理长周期的复杂工作流。\n2.  **现有方案：** 为了优化这些系统，TextGrad 等先驱方法提出了“通过文本进行自动微分”的概念，即通过计算图反向传播文本反馈来更新代理配置。这在短链路中表现良好。\n3.  **核心冲突：** 随着工作流深度的增加，这种全局文本反向传播方法遭遇了**深度扩展性**的瓶颈。作者观察到，这与深度神经网络中的梯度问题有惊人的相似性。\n4.  **具体表现：** 作者定义了两种具体的失败模式：\n    *   **文本梯度爆炸：** 反馈在层级间累积，导致消息长度呈指数级增长，超出上下文限制并放大评估偏差。\n    *   **文本梯度消失：** 为了管理长度而进行的压缩，导致反馈的具体性在多跳传播中逐渐丧失，最终变成无法执行的通用建议。\n5.  **结论：** 全局文本反向传播在深度复合AI系统中存在根本性的信号退化问题，亟需一种新的优化范式。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何在不依赖全局文本反向传播的情况下，有效地优化深度复合AI系统，以避免随着系统深度增加而产生的文本梯度爆炸或消失问题？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n为了回答上述问题，作者的思考过程经历了从现象观察到理论借鉴，再到方法构建的完整闭环：\n\n#### 1. 现象观察与问题诊断\n*   **观察：** 在深度工作流中，TextGrad 的反馈链条越长，效果越差。\n*   **诊断：** 问题的根源在于“长距离依赖”。每一层为了保留下游信息，不得不无限增加上下文（导致爆炸），或者为了节省上下文而压缩信息（导致消失）。\n*   **推论：** 既然“全局反向传播”在文本领域会导致信号随深度指数级衰减或膨胀，那么必须切断这种长链路依赖，寻找一种**局部化**的优化机制。\n\n#### 2. 理论映射与灵感获取\n*   **跨域类比：** 作者将目光投向了深度学习理论。在数值神经网络中，同样存在梯度消失/爆炸问题。\n*   **理论锚点：** Scellier & Bengio (2017) 提出的**平衡传播**提供了一种替代反向传播的方案。它不依赖长链路的误差反向传播，而是基于能量模型，通过“自由阶段”和“微扰阶段”的局部动力学来收敛。\n*   **核心洞察：** 如果将复合AI系统中的每个节点视为一个能量单元，是否可以用“寻找局部平衡”代替“计算全局梯度”？\n\n#### 3. 方法论构建\n基于上述洞察，作者构建了 Textual Equilibrium Propagation (TEP)，将数值的平衡原理转化为文本操作：\n\n*   **设计目标：** 既要保证每个模块自身的局部最优（避免消失），又要限制每次修改的幅度（避免爆炸）。\n*   **阶段一：自由阶段—— 局部自洽**\n    *   *思考：* 先不看全局目标，先让每个节点自己“满意”。\n    *   *实现：* 引入局部的 LLM 评论家，在每个节点内部迭代优化提示词，直到评论家不再提出改进建议（达到平衡）。这一步完全在局部进行，不依赖下游反馈，从而切断了长链路。\n*   **阶段二：微扰阶段—— 全局对齐**\n    *   *思考：* 局部最优不代表全局最优，如何在不引入长链路的情况下引导向全局目标？\n    *   *实现：* 不传递长长的反馈文本，而是施加一个**有界的微扰**。即根据最终任务目标，对提示词进行最小幅度的修改。这是一种“前向信号”而非“反向反馈”。\n*   **更新机制：** 结合自由阶段的稳定性和微扰阶段的导向性，进行有界的提示词更新。\n\n#### 4. 逻辑验证与闭环\n*   **预期效果：** 由于每个节点只处理局部信息和有限的微扰，消息长度保持恒定（解决爆炸），且保留了具体的修改指令（解决消失）。\n*   **最终结论：** 这种“局部平衡 + 受限微扰”的范式，使得复合AI系统的优化不再受限于深度，实现了随深度增加的鲁棒性。\n\n---\n\n**总结：** 作者的思考路径是从**“深度系统中的反馈失效”**这一痛点出发，通过**“切断长链路依赖”**的直觉，借鉴**“能量模型的平衡传播”**理论，最终设计出一种**“局部迭代 + 有界微扰”**的新型优化框架，成功将数值优化原理迁移到了文本化的AI系统优化中。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning",
                    "arxiv_id": "2601.21804",
                    "authors": "Bodong Du, Xuanqi Huang, Xiaomeng Li",
                    "summary": "Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，核心依据如下： 1.  **核心贡献属于“自我演化”范畴**： 论文的核心是提出一种名为 DARE (Distribution-Aware Reward Estimation) 的新方法，用于改进 Test-Time Reinforcement Learning (TTRL)。摘要明确指出 TTRL 的目标是使 LLM 能够在无标签输入上进行 \"self-improve\"（自我改进）。这直接对应我的研究焦点中的 **\"自我演化\"**，即智能体通过反馈进行自我完善和迭代。 2.  **符合筛选标准的第一步（核心判断）**： 论文不仅仅是应用现有的智能体框架去解决某个垂直领域的问题（如医疗或法律），而是提出了一种新的**方法论**（新的奖励估计机制）来优化智能体的自我演化过程。它解决了现有 TTRL 方法中依赖多数投票导致信息丢失和偏差的问题，属于对智能体演化机制的构建和改进。 3.  **符合筛选标准的第四步（特殊情况处理）**： 虽然论文在数学推理基准（AIME, AMC）上进行评估，但这属于**“自我演化的应用”**这一例外情况。论文的核心在于提出一种通用的自我演化机制（奖励估计），而非单纯提升模型的基础推理能力或提出一种新的静态推理提示词。其通过强化学习循环、探索奖励和分布剪枝来实现性能提升，体现了智能体与环境反馈交互并自我优化的 Agentic 特征。 综上所述，该论文致力于解决 LLM 智能体在自我演化过程中的奖励信号估计问题，属于 Agentic AI 中自我演化的前沿研究，因此予以保留。",
                    "summary2": "本文旨在解决Test-Time Reinforcement Learning (TTRL) 中依赖Majority Voting (MV) 导致的信息丢失与系统性偏差问题。针对无标签的测试输入，我们提出了一种Distribution-Aware Reward Estimation (DARE) 框架，利用不确定性感知的经验分布、探索奖励及分布剪枝机制进行更鲁棒的奖励估计。在MMLU-Pro、AIME 2024等推理基准上，通过pass@1准确率验证了其有效性，显著提升了优化稳定性与最终性能。",
                    "summary_translation": "测试时强化学习 (Test-time reinforcement learning, TTRL) 使大语言模型 (Large Language Models, LLMs) 能够在无标签输入上进行自我改进，但其有效性关键在于如何在缺乏真实监督的情况下估计奖励信号。大多数现有的 TTRL 方法依赖于对推演结果 的多数投票 来生成确定性奖励，隐含地假设多数推演结果能提供可靠的学习信号。我们表明这一假设是脆弱的：多数投票将推演分布简化为单一结果，丢弃了关于非多数但正确的动作候选的信息，从而导致奖励估计产生系统性偏差。为解决这一问题，我们提出了分布感知奖励估计，该方法将奖励估计从单一的多数结果转移至完整的经验推演分布。DARE 进一步利用探索奖励和分布修剪机制来增强这种基于分布的奖励，旨在进行非多数推演的探索和奖励去噪，从而实现更具信息量和鲁棒性的奖励估计。在具有挑战性的推理基准上进行的广泛实验表明，DARE 相比最近的基线模型提高了优化稳定性和最终性能，在 AIME 2024 上实现了 25.3% 的相对提升，在 AMC 上实现了 5.3% 的相对提升。",
                    "inspiration_trace": "基于对论文《Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning》的深度分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的方式构建了研究背景，其逻辑链条如下：\n\n1.  **背景铺垫**：大语言模型（LLMs）具备在测试时通过强化学习（TTRL）在无标签数据上进行自我改进的能力。\n2.  **核心依赖**：TTRL 的有效性完全取决于如何在没有外部监督的情况下估计奖励信号。\n3.  **现状描述**：现有的 TTRL 方法普遍依赖“多数投票”机制，即假设出现频率最高的答案就是最优答案，并以此作为奖励分配的依据。\n4.  **揭示缺陷**：这种假设是脆弱的。MV 将多样化的推理路径压缩为单一结果，丢弃了非多数派但可能正确的信息。\n5.  **后果分析**：这种做法会导致系统性的奖励估计偏差，引发“确认崩溃”，即早期错误的奖励主导学习过程，将模型推向次优的行动空间。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图解决的核心问题可总结为：\n\n**“在测试时强化学习（TTRL）中，如何构建一种不依赖多数投票（MV）的奖励估计机制，以避免信息丢失和系统性偏差，从而充分利用模型生成的完整推理分布？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n以下是从观察到方法论形成的完整思考过程：\n\n#### 1. 观察与直觉：从“点估计”到“分布估计”的必要性\n*   **观察**：现有的 MV 方法本质上是一种“硬”分类，它只关注众数，忽略了其他所有样本。\n*   **直觉**：在复杂的推理任务中，正确的答案往往不是最频繁出现的（例如模型可能因为某种偏见反复生成同一个错误答案）。如果只奖励众数，就会错过那些“少数派但正确”的高质量路径。\n*   **理论验证**：作者通过信息论分析证明，MV 是一个有损的信息压缩过程（信息坍塌），且在样本相关时会产生系统性偏差。\n*   **思考转折**：既然 MV 丢失了分布信息，那么**奖励信号应该直接基于整个经验分布来构建**，而不是坍缩成一个点。\n\n#### 2. 假设提出：利用“边缘分布”替代“条件众数”\n*   **假设**：如果我们将奖励定义为基于经验分布概率的函数，那么在期望上，它能更好地对齐真实的边缘期望奖励，从而避免 MV 带来的偏差。\n*   **初步构想**：直接使用生成答案的频率作为奖励的基础。频率越高，奖励越高。\n\n#### 3. 深化思考：引入“不确定性”修正频率\n*   **反思**：仅仅依靠频率是不够的。一个答案可能频繁出现，但模型在生成它时内部非常犹豫（高不确定性）；反之，一个低频答案可能生成过程非常笃定（低不确定性）。\n*   **逻辑推演**：高质量的答案应该是“高频”且“低不确定性”的。\n*   **方法演进**：构建一个**不确定性感知的经验分布**。在计算概率权重时，不仅看出现的次数 $n$，还要除以该路径的平均 token 熵 $u$。这样既奖励常见的，也奖励模型内部确信的。\n\n#### 4. 解决矛盾：探索“非多数但正确”的路径\n*   **发现新问题**：即使使用了分布奖励，主流的高频答案依然会主导学习信号。那些“低频但正确”的路径虽然获得了非零奖励，但可能因为信号太弱而被淹没。\n*   **关键洞察**：这些被淹没的路径往往具有“低不确定性”的特征。\n*   **解决方案**：引入**探索奖励**。专门针对那些“出现频率低”但“不确定性低”的样本给予额外的奖励加成。这鼓励模型去挖掘那些虽然少见但模型自己很有把握的正确答案。\n\n#### 5. 稳定性优化：剪除分布中的噪声\n*   **发现新问题**：虽然给所有样本分配奖励保留了信息，但那些极低概率的样本（通常是胡言乱语）也会获得奖励，这会引入噪声，导致优化不稳定。\n*   **解决方案**：引入**分布剪枝**。设定一个概率阈值，剔除掉那些极低概率的样本，并在剩余样本上重新归一化分布。这相当于去噪，让学习集中在有意义的候选答案上。\n\n#### 6. 最终整合：DARE 框架的形成\n*   **综合**：将上述三个组件整合：\n    1.  **基础**：基于不确定性感知的分布奖励（替代 MV）。\n    2.  **增强**：针对低频高信心的探索奖励（防止遗漏少数正确答案）。\n    3.  **净化**：分布剪枝（防止噪声干扰）。\n*   **结果**：形成了 DARE 方法，它不仅比 MV 保留了更多信息，还通过探索和剪枝机制，在利用和探索之间取得了更好的平衡，从而实现了更稳定、性能更强的测试时适应。"
                },
                {
                    "title": "Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation",
                    "arxiv_id": "2601.21797",
                    "authors": "Yimin Deng, Yuqing Fu, Derong Xu, Yejing Wang, Wei Ni, Jingtong Gao, Xiaopeng Li, Chengxu Liu, Xiao Han, Guoshuai Zhao, Xiangyu Zhao, Li Zhu, Xueming Qian",
                    "summary": "Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心贡献符合构建与改进智能体的目标**：论文提出了一种名为“对抗性记忆适应机制（AMA）”的新框架，旨在解决对话智能体在长对话中的记忆局限性。这属于对LLM智能体核心组件（记忆系统）的构建和改进，而非简单的应用。 2.  **符合多智能体方向**：论文中明确设计了三个具有不同角色的智能体协同工作：`Challenger agent`（生成问答对）、`Evaluator agent`（评估响应和错误分析）和`Adapter agent`（执行更新）。这种多智能体协作以优化系统性能的架构，精准对应了研究焦点中的“Multi-Agent”方向。 3.  **符合自我演化方向**：AMA机制通过模拟任务执行，利用评估者的反馈和错误分析，对记忆构建策略和内容进行双重级别的迭代更新。这种基于反馈信号进行自我完善和适应的过程，体现了“Self-Evolving”的核心特征。 4.  **符合单智能体能力**：论文重点解决了智能体的“记忆”能力问题，这是单智能体研究的关键子方向之一。 综上所述，该论文提出了一种结合多智能体协作与自我演化机制来增强智能体记忆能力的新方法，属于Agentic AI的前沿研究，因此予以保留。",
                    "summary2": "本文旨在解决现有对话智能体记忆系统离线阶段与任务需求不匹配的问题。针对长对话场景，我们提出了一种对抗性记忆适应机制（AMA），通过模拟任务执行生成QA对并评估反馈，实现记忆内容与构建策略的双重更新。我们在LoCoMo数据集上通过F1、BLEU-1及LLM-judge分数验证了其有效性。",
                    "summary_translation": "由于上下文窗口的限制，对话代理在处理长对话时面临困难。因此，研究人员开发了记忆系统以利用关键的历史信息。现有的记忆系统通常遵循包含离线记忆构建与更新，以及在线检索的流水线。尽管在线阶段具有灵活性，但离线阶段仍然是固定的且与任务无关的。在此阶段，记忆构建在预定义的工作流下运行，无法强调任务相关信息。同时，记忆更新由通用指标指导，而非任务特定的监督。这导致离线记忆准备与任务要求之间存在不一致，从而损害了下游任务的性能。为此，我们提出了一种对抗性记忆适应机制，通过模拟任务执行来对齐记忆构建、更新与任务目标。具体而言，首先，一个挑战者代理基于原始对话生成问答对。随后，利用构建的记忆来回答这些问题，从而模拟下游推理。接着，一个评估者代理评估回答结果并进行错误分析。最后，一个适配者代理分析错误案例，并对构建策略和内容执行双层更新。通过这一过程，记忆系统在离线阶段提前接收到任务感知的监督信号，增强了其对下游任务的适应性。AMA 可集成到多种现有的记忆系统中，在长对话基准 LoCoMo 上进行的广泛实验证明了其有效性。",
                    "inspiration_trace": "基于您提供的论文内容，我为您还原了作者提出“面向任务对抗性记忆适应机制（AMA）”的完整逻辑链。\n\n---\n\n### 一、 宏观背景与问题引入\n\n**1. 现实痛点：**\n随着对话系统处理长文本需求的增加，大语言模型（LLM）固有的上下文窗口限制成为瓶颈。为了解决这一问题，现有的解决方案是引入“记忆系统”来存储和检索历史对话信息。\n\n**2. 现有架构的观察：**\n目前的记忆系统普遍遵循一个标准的“两阶段”流水线：\n*   **离线阶段：** 负责记忆的构建与更新（将原始对话转化为结构化记忆）。\n*   **在线阶段：** 负责根据具体任务进行记忆检索。\n\n**3. 核心矛盾：**\n作者敏锐地发现了这两个阶段之间存在严重的“错位”：\n*   **在线阶段是灵活的：** 它能根据下游任务的具体需求动态调整。\n*   **离线阶段是僵化的：** 它是固定的、与任务无关的。\n    *   **构建层面：** 采用预定义的工作流（如文本分块、摘要），无法强调与特定任务相关的信息（例如：时间推理任务需要时间戳，而多跳推理需要事件关联，通用摘要可能忽略这些）。\n    *   **更新层面：** 依赖通用指标（如冲突检测、信息冗余）进行更新，缺乏针对特定任务的监督信号。\n\n**4. 后果：**\n这种“离线准备”与“在线需求”的不一致，导致记忆系统无法在真实场景中有效泛化，直接削弱了下游任务的性能。\n\n---\n\n### 二、 研究问题\n\n基于上述矛盾，作者试图解决的核心问题可归纳为：\n\n**“在离线记忆准备阶段无法直接接触下游任务的情况下，如何使记忆的构建与更新过程能够感知并适应特定的任务目标，从而消除离线准备与在线检索之间的错位？”**\n\n---\n\n### 三、 逻辑演进与思想推演\n\n为了回答上述问题，作者的思考路径经历了以下四个关键步骤：\n\n#### 1. 洞察本质：从“通用存储”转向“任务导向”\n*   **思考：** 既然不同的任务（如时间推理 vs. 多跳推理）对信息的偏好不同，那么通用的记忆构建策略必然会导致信息的“误提取”或“欠提取”。\n*   **推论：** 记忆系统不应只是被动的存储仓库，而应像人类一样，通过解决特定领域的练习来巩固相关知识。我们需要让离线阶段获得“任务感知”能力。\n\n#### 2. 突破瓶颈：引入“模拟执行”机制\n*   **思考：** 离线阶段最大的困难在于缺乏任务反馈。既然不能等到在线阶段才去测试，能不能在离线阶段就“模拟”任务执行？\n*   **推论：** 如果我们能提前模拟下游任务对记忆进行“压力测试”，就能在构建阶段就发现记忆的缺陷。\n\n#### 3. 方法构思：对抗性反馈循环\n*   **思考：** 如何进行有效的模拟和反馈？简单的规则不够，需要一种动态的迭代机制。\n*   **推论：** 借鉴对抗学习的思想，构建一个闭环：\n    *   **挑战者：** 模拟任务需求，基于对话生成问答对（QA Pairs），以此作为“考题”。\n    *   **评估者：** 利用当前记忆尝试回答“考题”，并评估答案质量，找出错误案例。\n    *   **适配器：** 根据错误反馈，反向修正记忆。\n\n#### 4. 最终落地：双重更新策略\n*   **思考：** 发现错误后，仅仅修改记忆内容是不够的，因为错误的根源可能在于“构建策略”本身。如果策略不改，新的对话进来后还会犯同样的错。\n*   **推论：** 必须进行“双重更新”：\n    *   **内容层：** 补充缺失或错误的事实信息。\n    *   **策略层：** 调整记忆提取的Prompt或规则，让系统学会下次如何提取更符合任务需求的信息。\n\n---\n\n### 四、 总结\n\n作者的思想演进是从**发现架构上的“离线-在线”割裂**出发，意识到**通用记忆无法满足特定任务需求**，进而提出**在离线阶段通过模拟任务来获取监督信号**的假设，最终设计出一套包含**挑战、评估、适应**的闭环机制，实现了记忆系统从“被动存储”到“主动适应任务”的进化。"
                },
                {
                    "title": "Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents",
                    "arxiv_id": "2601.21699",
                    "authors": "Hojae Han, Heeyun Jung, Jongyoon Kim, Seung-won Hwang",
                    "summary": "While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合构建/改进LLM智能体）**： 论文的核心贡献是提出了 **DAVID-GRPO**，这是一种预算高效的强化学习（RL）框架，专门用于训练和改进 **资源受限的LLM智能体**。论文明确指出其研究对象是具备检索和工具使用能力的“多轮推理智能体”，旨在解决这些智能体在资源受限情况下的训练不稳定和探索稀疏问题。这完全符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **符合研究焦点（单智能体方向）**： 该论文属于 **单智能体** 范畴。它关注的是如何提升单个智能体在复杂任务中的 **多跳推理** 能力，以及如何优化智能体的 **工具使用** 和 **规划** 过程。论文中提到的“multi-turn reasoning agents with retrieval and tools”直接对应了筛选标准中的“Agentic”能力。 3.  **排除标准检查**： *   **非非演化型应用**：论文并非将智能体作为工具应用到生物、金融等特定领域，而是专注于智能体本身的训练算法和性能提升。 *   **非非Agentic的推理**：虽然标题涉及“Multi-Hop Reasoning”，但论文背景明确是基于具备工具和检索能力的智能体，且解决的是智能体训练中的信用分配和轨迹探索问题，属于Agentic AI的范畴，而非单纯提升LLM基础Token预测能力的数学或逻辑研究。 *   **非基础设施**：论文关注的是算法框架（DAVID-GRPO），而非硬件加速或部署优化。 4.  **正面指标匹配**： 论文涉及了 `Agentic AI`、`Tool Use / Tool Augmentation`（检索与工具）、`Planning`（多跳推理）以及 `Iterative Improvement`（通过RL进行探索和改进）。 综上所述，该论文致力于改进LLM智能体在资源受限环境下的推理和工具使用效能，属于单智能体研究的重要进展，符合课题筛选要求。",
                    "summary2": "本文旨在解决资源受限下小语言模型难以进行多跳推理的问题。针对低计算预算的场景，我们提出了一种名为DAVID-GRPO的预算高效RL框架，通过Few-shot warm-start、Grounded retrieval rewards和Grounded expansion机制优化训练。我们在六个多跳QA基准上，通过EM和F1指标验证了其有效性，证明小模型在低成本下也能实现高性能。",
                    "summary_translation": "尽管强化学习 (RL) 赋予了多轮推理智能体检索和使用工具的能力，但现有的成功很大程度上依赖于在高成本、高准确率机制下进行大量的在线策略推演。然而，在无法支持大模型或密集探索的现实资源约束下，小语言模型智能体陷入了一种低成本、低准确率的机制，其中有限的推演预算导致了稀疏探索、稀疏信用分配以及不稳定的训练。在这项工作中，我们挑战了这种权衡，并表明小语言模型可以在资源约束下实现强大的多跳推理能力。我们介绍了 DAVID-GRPO，这是一个预算高效的强化学习 (RL) 框架，它 (i) 利用最小监督来稳定早期学习，(ii) 基于证据召回分配检索信用，以及 (iii) 通过重采样截断的接近成功的轨迹来改进探索。在仅使用四块 RTX 3090 GPU 训练的参数量高达 15 亿的智能体上进行评估，DAVID-GRPO 在六个多跳问答基准上始终优于专为大规模设置设计的先前的强化学习方法。这些结果表明，通过适当的归纳偏置，小型智能体可以在实现高准确率的同时保持低训练成本。",
                    "inspiration_trace": ""
                },
                {
                    "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
                    "arxiv_id": "2601.21558",
                    "authors": "Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Hao Zhou, Kaichi Yu, Yudian Zhang, Jade Ouyang, Junxi Yin, Jiong Chen, Baoyan Guo, Lei Zhang, Junjie Tao, Yuansheng Song, Ming Cui, Chengwei Liu",
                    "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断 (第一步)**: *   该论文的核心贡献是提出了 **ASTRA**，这是一个全自动的端到端框架，专门用于**训练**和**改进**工具增强型语言模型智能体。 *   论文本质上是关于如何构建更强大的 LLM 智能体（特别是解决工具使用和多步决策问题），而非仅仅将现有智能体作为工具应用到特定垂直领域（如医疗、金融），也非单纯的基础设施优化。因此，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确聚焦于 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**: 论文重点研究了 `Tool Use / Tool Augmentation`（工具使用）和 `Planning`（多步决策制定）。 *   **演化机制**: 论文通过 `Reinforcement Learning` (RL) 和数据合成来改进智能体的能力，属于智能体的迭代改进和性能优化范畴。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、水印或幻觉问题。 *   不涉及多模态或视觉核心研究（虽然涉及代码执行，但作为环境验证手段，非核心视觉研究）。 *   虽然提到了“工具调用图”，但这指的是智能体工作流的结构，而非知识图谱或图神经网络（GNN）算法研究，因此不触犯“图”相关的排除规则。 4.  **特殊与模糊情况处理 (第四步)**: *   论文涉及 `Reasoning/Planning`，但它是通过智能体轨迹合成和强化学习来提升智能体在复杂任务中的多步决策能力，这完全符合“智能体如何进行规划”的保留条件，而非单纯的提升模型基础推理能力（如数学解题）。 **总结**: 该论文提出了一种新的框架来构建和演化 LLM 智能体的工具使用和长视界决策能力，直接对应研究课题中的“单智能体”方向（规划、工具使用）以及“自我演化/改进”的训练机制，完全符合筛选要求。",
                    "summary2": "本文旨在解决训练鲁棒工具增强智能体时依赖人工干预及缺乏可验证环境的问题。针对多步决策和工具使用场景，我们提出了一种名为ASTRA的全自动端到端框架，结合了基于工具调用图拓扑的轨迹合成与基于语义推理的可验证环境合成，并集成了SFT与在线RL。我们在BFCL-MT、$\\tau$2-Bench和ACEBench等基准上通过准确率等指标验证了其有效性，实现了同等规模下的SOTA性能。",
                    "summary_translation": "Large language models (LLMs, 大语言模型) 越来越多地被用作 multi-step decision making (多步决策) 的 tool-augmented agents (工具增强代理)，然而训练鲁棒的工具使用代理仍然面临挑战。现有方法仍需人工干预，依赖不可验证的模拟环境，仅依赖 supervised fine-tuning (SFT, 监督微调) 或 reinforcement learning (RL, 强化学习)，且难以实现 stable long-horizon, multi-turn learning (稳定的长视界、多轮学习)。为应对这些挑战，我们提出了 ASTRA，这是一个通过 scalable data synthesis (可扩展数据合成) 和 verifiable reinforcement learning (可验证强化学习) 来训练 tool-augmented language model agents (工具增强语言模型代理) 的 fully automated end-to-end framework (全自动端到端框架)。ASTRA 整合了两个互补的组件。首先，一个利用 tool-call graphs (工具调用图) 静态拓扑的 pipeline (流程) 合成了多样化且 structurally grounded (结构化基础) 的 trajectories (轨迹)，从而赋予模型广泛且可迁移的工具使用能力。其次，一个捕捉 human semantic reasoning (人类语义推理) 丰富组合拓扑的 environment synthesis framework (环境合成框架)，将 decomposed question-answer traces (分解的问答轨迹) 转换为 independent, code-executable, and rule-verifiable environments (独立的、可代码执行且可规则验证的环境)，从而实现 deterministic (确定性) 的 multi-turn RL (多轮强化学习)。基于该方法，我们开发了一种 unified training methodology (统一训练方法)，将 SFT 与 online RL (在线强化学习) 相结合，并利用 trajectory-level rewards (轨迹级奖励) 来平衡任务完成度与交互效率。在多个 agentic tool-use benchmarks (代理工具使用基准) 上的实验表明，ASTRA 训练的模型在 comparable scales (可比规模) 下实现了 state-of-the-art (最先进) 的性能，在 approaching closed-source systems (接近闭源系统) 水平的同时保留了核心推理能力。我们在 https://github.com/LianjiaTech/astra 发布了完整的 pipelines (流程)、environments (环境) 和 trained models (训练模型)。",
                    "inspiration_trace": "基于对论文《ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观背景与观察\n随着大语言模型（LLM）的发展，研究重心正从单纯的文本生成转向**工具增强的智能体**。这些智能体需要通过调用外部API、执行多步决策来解决复杂问题。然而，训练一个既鲁棒又具有泛化能力的工具使用智能体，比训练传统的聊天机器人要困难得多。\n\n### 2. 问题引入：从现状到痛点（Introduction 的“讲故事”逻辑）\n作者首先承认了现有工作的进展，但随即指出了当前方法在迈向“全自动化、强鲁棒性智能体”时面临的三个核心困境，构成了文章的“故事”冲突：\n\n*   **困境一：环境的不可验证性**\n    *   *现象*：为了减少人工干预，许多工作开始利用LLM模拟环境来生成数据。\n    *   *问题*：这些模拟环境中的状态转移和反馈是由语言模型生成的，而非基于显式规则或可执行后端。这意味着它们是**不可验证的**。\n    *   *后果*：在需要确定性转移和可靠奖励信号的长期、多轮在线强化学习（RL）中，这种不可验证性会导致训练根本无法稳定进行。\n\n*   **困境二：轨迹的碎片化**\n    *   *现象*：虽然有些方法能生成多轮轨迹，但在训练时，它们往往将这些轨迹分解为孤立的“单步”训练实例。\n    *   *问题*：这种做法破坏了智能体学习连贯的、长视野决策的能力。智能体学会了“这一步该做什么”，却没学会“如何规划整个流程”。\n\n*   **困境三：训练范式的割裂**\n    *   *现象*：现有方法通常只专注于单一训练范式——要么是有监督微调（SFT），要么是强化学习（RL）。\n    *   *问题*：SFT缺乏环境交互的在线学习信号；而RL如果从一个较弱的初始策略开始，其效果会受到根本性的限制（冷启动问题）。\n\n### 3. 核心研究问题\n基于上述痛点，作者试图回答一个核心问题：\n\n**“如何构建一个完全自动化的端到端框架，既能通过可验证的合成环境支持稳定的多轮在线强化学习，又能有效整合SFT与RL以训练出鲁棒的工具增强智能体？”**\n\n---\n\n### 4. 思想演进与方法论形成（逻辑链推演）\n\n为了解决上述问题，作者的思考经历了从“静态能力构建”到“动态环境交互”，再到“统一训练范式”的演进：\n\n#### 第一阶段思考：如何获得高质量的SFT数据？（解决“静态能力”问题）\n*   **观察**：SFT需要大量高质量的轨迹。人工标注太慢，完全随机生成质量差。\n*   **假设**：工具的使用是有结构的。如果我们能掌握工具调用的“静态拓扑”，就能生成结构合理且多样的轨迹。\n*   **方法论雏形（SFT部分）**：\n    *   利用真实的MCP服务器和工具文档。\n    *   构建**工具调用图**：分析工具之间的依赖关系，形成有向图。\n    *   在图上进行随机游走，生成合法的工具链。\n    *   基于工具链反向生成任务，再进行多轮交互模拟。\n    *   **目的**：让模型先学会“怎么正确地组合工具”，建立广泛的工具使用能力。\n\n#### 第二阶段思考：如何构建可验证的RL环境？（解决“动态环境”问题）\n*   **观察**：SFT只能教模型“模仿”，无法教模型“在复杂环境中探索”。但真实环境难以获取，LLM模拟环境又不可靠。\n*   **假设**：人类的语义推理过程本身就是一个拓扑结构。如果我们能把一个复杂的问答（QA）对分解成多个子步骤，并将每个子步骤转化为一个可执行的Python函数，就能构建出一个既符合人类逻辑又完全可验证的“合成环境”。\n*   **方法论雏形（RL部分）**：\n    *   **语义拓扑提取**：将复杂问题分解为子问题-子答案对。\n    *   **环境合成**：为每个子问题编写对应的Python工具实现，并在沙箱中验证其能输出正确答案。\n    *   **特性**：这种环境是代码执行的、规则可验证的，且支持多轮交互。\n    *   **目的**：让模型在一个确定的、基于代码逻辑的复杂空间中学习长期规划。\n\n#### 第三阶段思考：如何将SFT与RL完美结合？（解决“训练范式”问题）\n*   **观察**：SFT提供了良好的起点，RL提供了优化的手段。两者不能割裂。\n*   **假设**：应该采用两阶段策略。先用静态拓扑数据做SFT，把模型“教懂”；再用语义拓扑环境做在线RL，让模型“练熟”。\n*   **方法论雏形（统一训练）**：\n    *   **阶段一（SFT）**：使用第一阶段合成的轨迹进行微调，获得适应多步交互的强初始策略。\n    *   **阶段二（RL）**：在第二阶段合成的可验证环境中进行在线强化学习。\n    *   **关键设计**：引入“无关工具混合”以提升模型的选择能力；设计F1-style的轨迹级奖励（平衡任务完成率与交互效率），防止模型在RL中走偏（如过度调用工具或过于保守）。\n\n### 5. 总结\n作者的最终思路是构建一个名为 **ASTRA** 的闭环系统：\n1.  **广度**：通过**工具调用图的静态拓扑**合成数据，利用SFT赋予模型基础的工具使用能力。\n2.  **深度**：通过**人类语义推理的拓扑**合成可执行环境，利用RL让模型在确定性的复杂交互中磨练长期决策能力。\n3.  **统一**：将两者结合，实现了从数据合成、环境构建到模型训练的全流程自动化与可验证性。"
                },
            ]
        },
    ],
    "2026-01-29": [
        {
            "name": "Artificial Intelligence",
            "count": 8,
            "papers": [
                {
                    "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)",
                    "arxiv_id": "2601.20843",
                    "authors": "Saurav Prateek",
                    "summary": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的论文。具体判断依据如下： 1.  **核心贡献符合 (第一步 - 核心判断)**: 论文的核心贡献在于构建了一个名为 \"Deep Researcher\" 的新型智能体架构，提出了 \"Sequential Plan Refinement via Reflection\"（基于反思的顺序计划细化）和 \"Candidates Crossover\"（候选者交叉）两种关键创新。这属于构建和改进 LLM 智能体的方法论，而非简单地将现有模型作为工具应用到特定领域。 2.  **高度匹配核心关注点 (第二步 - 正面指标)**: *   **单智能体**: 论文详细描述了智能体的规划能力、自我反思机制以及维护全局上下文的能力，这直接对应了 `Planning`、`Self-Reflection` 和 `Memory` 等核心指标。 *   **自我演化**: 论文提出的 \"Candidates Crossover\" 算法以及通过反思进行动态调整的过程，体现了智能体通过迭代和反馈进行自我完善和搜索空间探索的机制，符合 `Self-Evolving` 和 `Iterative Improvement` 的定义。 3.  **无排除项 (第三步 - 排除标准)**: 论文不涉及安全对齐、多模态视觉核心研究或图神经网络技术，因此不在排除范围内。 4.  **特殊情况处理 (第四步 - 特殊规则)**: 虽然论文的应用场景是生成研究报告（看似应用），但其核心在于提出了一种新的“自我演化”和“反思”机制来解决复杂任务。根据规则，只要核心是提出新的自我演化机制，即使应用在特定领域（这里是科研任务），也应予以保留。 综上所述，该论文聚焦于通过反思和演化算法来增强智能体的规划与执行能力，是典型的 Agentic AI 与 Self-Evolving 研究范畴。",
                    "summary2": "本文旨在解决并行扩展中的“知识孤岛”问题，生成博士级详细研究报告。针对复杂研究任务，我们提出了一种 Deep Researcher 架构，核心包含 Sequential Research Plan Refinement via Reflection 和 Candidates Crossover 算法，利用 Global Research Context 实现动态计划调整。我们在 DeepResearch Bench 上通过 RACE 框架验证了其有效性，取得了 46.21 的总分，超越了 Claude Researcher 等主流代理。",
                    "summary_translation": "本文介绍了一种新颖的 Deep Researcher 架构（深度研究者架构），旨在通过解决 Parallel Scaling 范式（并行扩展范式）的固有局限性，针对复杂的 PhD level topics（博士级课题）生成详细的研究报告。我们的系统利用了两个关键创新：Sequential Research Plan Refinement via Reflection（基于反思的顺序研究计划细化）和 Candidates Crossover 算法（候选交叉算法）。顺序细化过程被证明是一种高效的方法，它允许智能体维护一个集中的 Global Research Context（全局研究上下文），使其能够回顾当前进展，对研究计划进行推理，并在运行时智能地进行更改。这种动态适应与经常受困于 siloed knowledge（知识孤岛）的并行方法形成了对比。Candidates Crossover 算法通过部署具有不同参数的多个 LLM candidates（大语言模型候选者）来探索更大的 search space（搜索空间），从而进一步提高搜索效率，并将其发现综合起来，策划出一个全面的最终研究响应。该过程以 One Shot Report Generation（单次报告生成）结束，确保最终文档基于统一的叙事和高 fact density（事实密度）。由 Gemini 2.5 Pro 模型驱动，我们的 Deep Researcher 在 DeepResearch Bench（深度研究基准）上进行了评估，这是一个包含 100 个博士级研究任务的全球公认基准。我们的架构获得了 46.21 的总分，通过超越 DeepResearch Bench 活跃排行榜上现有的领先深度研究智能体（如 Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher 和 Grok Deeper Search），展示了卓越的性能。这一性能略微超过了我们之前的工作 Static DRA（静态 DRA），并强化了这一发现：sequential scaling（顺序扩展）始终优于 parallel self consistency paradigm（并行自洽性范式）。",
                    "inspiration_trace": "基于论文《Deep Researcher with Sequential Plan Reflection and Candidates Crossover》，以下是对作者核心方法论产出逻辑链的系统推演：\n\n### 一、 宏观问题与背景观察\n\n**1. 宏观背景：**\n随着大语言模型（LLM）的发展，研究界开始探索构建能够处理复杂、多面性任务的“深度研究智能体”。这些智能体旨在生成类似博士级别的详细研究报告。\n\n**2. 现状观察（两条技术路线）：**\n作者观察到当前DRA的发展主要分为两种范式：\n*   **并行扩展：** 如GPT Researcher及作者之前的Static-DRA。这种方式将主题分解为子任务并发处理。\n*   **顺序细化：** 如Google的TTD-DR。这种方式通过迭代过程（如扩散模型）逐步生成结果。\n\n---\n\n### 二、 Introduction中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分通过以下逻辑链条引入了核心问题：\n\n1.  **现状陈述：** 当前的深度研究智能体主要采用“并行扩展”范式，将研究主题分解为独立的子任务并发执行。\n2.  **指出缺陷：** 虽然并行扩展在延迟和水平扩展上有优势，但存在严重的**“知识孤岛”**问题。由于各代理在各自的子任务真空中工作，系统缺乏全局视野，导致无法识别信息重叠、产生冗余搜索，也无法根据其他分支的发现实时调整计划。\n3.  **对比分析：** 另一种范式是“顺序细化”（如Google的TTD-DR），它通过迭代来改进。然而，现有的顺序方法主要聚焦于**“报告级去噪”**（即不断打磨草稿），而非对研究过程本身的控制。\n4.  **提出方向：** 为了解决上述问题，需要一种新的架构，既能利用顺序推理的优势来维护全局上下文，又能动态调整研究策略，从而避免并行模式的僵化和现有顺序模式的低效。\n\n**显式总结的“研究问题”：**\n> **“如何构建一种深度研究智能体，使其能够克服并行扩展中的‘知识孤岛’局限，通过维护全局研究上下文来实现动态的、自适应的研究计划细化，同时保证生成报告的高效性与高质量？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n基于上述问题，作者的思考经历了以下演进过程：\n\n**1. 痛点反思：从“并行”转向“顺序”**\n*   **思考：** 既然并行模式导致信息割裂，且无法在运行时根据新发现调整策略，那么必须采用**顺序**模式。\n*   **理论支撑：** 引用《The Sequential Edge》论文的研究成果，证明在相同计算资源下，顺序扩展在95.6%的配置中优于并行自洽性，因为模型能利用更完整的上下文进行推理。\n*   **决策：** 放弃单纯的并行分解，确立以**顺序推理**为核心架构。\n\n**2. 核心突破：从“打磨报告”转向“打磨计划”**\n*   **思考：** 现有的顺序方法（如TTD-DR）把计算花在了反复修改报告文本上（去噪）。但这真的是研究的瓶颈吗？实际上，如果研究计划不完善，写出来的报告再润色也是徒劳。\n*   **创新点：** 将顺序细化的重心从“报告生成”转移到**“研究计划”**上。\n*   **机制设计：** 引入**“顺序研究计划反思”**。让智能体在每一步都能“回头看”，基于已有的发现动态修改下一步的计划，而不是死守初始大纲。\n\n**3. 记忆支撑：构建“全局上下文”**\n*   **思考：** 要实现动态调整计划，智能体必须知道“我已经知道了什么”。\n*   **机制设计：** 设计**“全局研究上下文”**作为中心化记忆库。它存储所有的搜索轨迹和原始数据，为“反思”提供依据，避免重复搜索，确保决策基于全貌而非局部。\n\n**4. 效率平衡：在顺序中引入“候选者交叉”**\n*   **思考：** 顺序模式虽然深度好，但可能广度不足。如果在某一个搜索步骤上遗漏了关键信息，整个链条都会受影响。同时，完全顺序可能导致推理时间过长。\n*   **借鉴与改良：** 借鉴TTD-DR中的“Self-Evolution”思想，但为了降低延迟，去掉了复杂的反馈循环。\n*   **机制设计：** 提出**“候选者交叉算法”**。在顺序流程的每一个具体搜索步骤中，并行启动多个参数（如Temperature、Top-k）不同的LLM候选者来探索更广的搜索空间，然后将它们的发现“交叉”合并为一个高质量答案。这实现了**“宏观上的顺序控制”**与**“微观上的并行探索”**的结合。\n\n**5. 最终产出：一次性报告生成**\n*   **思考：** 既然我们在前面的步骤中已经通过“计划反思”保证了逻辑的严密性，通过“候选者交叉”保证了信息的全面性，那么最后的报告生成就不需要像TTD-DR那样反复迭代去噪了。\n*   **机制设计：** 采用**“一次性报告生成”**。利用前面积累的高质量“全局上下文”，直接生成最终报告。这既保证了叙事的统一性和事实密度，又极大提升了生成效率。\n\n### 四、 总结\n\n作者的思想演进是从**批判现有并行模式的“孤岛效应”**出发，确立了**顺序扩展**的路线；进而通过**将反思对象从“报告”上移至“计划”**，解决了动态适应性问题；为了弥补顺序模式的探索广度并兼顾效率，创新性地提出了**候选者交叉算法**；最终形成了一个集**动态规划、全局记忆、广度探索、高效生成**于一体的深度研究架构。"
                },
                {
                    "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents",
                    "arxiv_id": "2601.20831",
                    "authors": "Vishnu Sashank Dorbala, Dinesh Manocha",
                    "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文提出了 MemCtrl，这是一个专门用于具身智能体的新框架。其核心贡献在于解决智能体在受限计算资源下的记忆管理问题（在线修剪、更新记忆），这属于构建和改进 LLM 智能体架构的方法论，符合“单智能体”方向中关于“记忆”的核心研究目标。 2.  **正面指标 (匹配)**: 论文明确涉及 `Agentic AI` 和 `Memory`。它探讨了智能体如何决定保留或丢弃信息（记忆控制），并提到了 `Reflection`（反思）和通过在线强化学习（`Online RL`）进行训练，这与智能体的自我完善和迭代机制高度相关。 3.  **排除标准 (通过)**: 尽管论文标题和摘要中提到了 `MLLMs`（多模态大模型），但这是因为研究对象是具身智能体，视觉输入仅作为智能体感知环境的工具。论文的核心创新点在于“记忆控制机制”而非视觉模型本身，因此符合排除标准中关于多模态的例外条款（“除非它们被用作智能体感知环境的工具，而不是研究的核心”）。论文不涉及安全、对齐或图技术。 4.  **综合结论**: 该论文聚焦于提升智能体的记忆管理能力，是 Agentic AI 架构改进的重要组成部分，完全符合筛选要求。",
                    "summary2": "本文旨在解决具身智能体在内存受限环境下高效管理记忆的问题。针对在线运行的具身任务场景，我们提出了一种名为MemCtrl的框架，引入可训练的记忆头$\\mu$作为主动控制器，实时过滤冗余观察。我们在EmbodiedBench benchmark上通过任务成功率等指标验证了其有效性，结果显示该方法使低性能MLLMs平均提升约16%，显著增强了长时程和复杂指令的处理能力。",
                    "summary_translation": "基础模型 依赖于上下文学习 来实现个性化决策。由于上下文窗口 的尺寸有限，因此必须采用记忆压缩和诸如 RAG (Retrieval-Augmented Generation，检索增强生成) 之类的检索系统。然而，这些系统通常将记忆视为大型离线存储空间，这对于预期需在严格的内存和计算约束下在线运行的具身智能体 而言是不利的。在这项工作中，我们提出了 MemCtrl，这是一种利用多模态大语言模型 在线修剪记忆的新型框架。MemCtrl 通过一个可训练的记忆头 μ 来增强 MLLMs，该记忆头充当门控机制，用于确定在探索过程中应保留、更新或丢弃哪些观测 或反思。我们通过训练两种类型的 μ 进行了评估：1) 通过离线专家，以及 2) 通过在线 RL (Reinforcement Learning，强化学习)。结果显示，经 μ 增强的 MLLMs 在整体具身任务完成能力上有显著提升。具体而言，在 EmbodiedBench 基准测试的多个子集上，利用 MemCtrl 对两个性能较低的 MLLMs 进行增强后，我们观察到经 μ 增强的 MLLMs 平均提升了约 16%，在特定指令子集上的提升甚至超过 20%。最后，我们对由 μ 收集的记忆片段进行了定性分析，指出经 μ 增强的 MLLMs 在处理长且复杂的指令类型时表现出优越的性能。",
                    "inspiration_trace": "基于对论文《MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents》的深入分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑链\n\n作者在引言部分通过层层递进的矛盾冲突，构建了研究的必要性，其逻辑链条如下：\n\n1.  **宏观愿景**：Embodied AI 的终极目标是开发通用的智能体，能够在多样化的任务、环境和指令中保持高性能。\n2.  **现实困境**：\n    *   **大模型路线的局限**：虽然大型基础模型（如 LLaMA 4, Deepseek V3）泛化能力强，但训练成本极高，难以实时适应新场景，且微调计算开销巨大，难以在边缘计算设备（如机器人）上普及。\n    *   **小模型的先天不足**：为了适应边缘计算，必须使用参数较小的模型（<20B），但这些模型的上下文窗口非常有限，难以处理长序列任务。\n3.  **现有方案的妥协与缺陷**：\n    *   **主流方案**：为了弥补小模型的记忆短板，现有工作通常引入外部记忆库（如 RAG、Episodic logs）。\n    *   **核心痛点**：这些记忆系统通常将记忆视为“大型离线存储空间”。这种“先存储后检索”的范式对于需要在线实时操作、且计算和内存资源极其受限的具身智能体来说是低效且不切实际的。\n4.  **生物启发与视角转换**：\n    *   **人类智慧**：人类在执行任务时，并不会记录所有观察细节，而是主动过滤掉冗余信息，只保留关键片段，事后通过常识推理补全缺失信息。\n    *   **核心洞察**：这种“主动过滤”机制是人类在有限存储下保持高效推理的关键。\n5.  **解决思路**：与其依赖庞大的外部存储和复杂的检索管道，不如赋予智能体类似人类的“主动记忆控制”能力——在写入阶段就决定保留什么、丢弃什么。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“在资源受限的具身智能体中，如何利用多模态大语言模型（MLLMs）实现在线的主动记忆控制，以在不依赖庞大外部检索系统的情况下，有效扩展上下文窗口并提升长周期任务的表现？”**\n\n---\n\n### 三、 核心方法论的逻辑演进\n\n从发现问题到提出 MemCtrl，作者的思考路径经历了以下四个阶段：\n\n#### 1. 问题归因：从“存不下”到“检索慢”\n*   **观察**：小模型在长任务中失败，是因为上下文窗口 $h$ 远小于历史观察总数 $n$。\n*   **现有解法分析**：RAG 试图通过检索函数 $F(C, I)$ 压缩记忆。\n*   **批判性思考**：随着 $n$ 的增加（机器人高频采集数据），检索变得极其低效。且记忆中包含大量冗余观测（如盯着墙看的一连串图片），这些噪音不仅占用存储，还干扰检索。\n*   **结论**：问题的根源不在于“怎么检索”，而在于“写入了太多垃圾数据”。必须从源头控制写入。\n\n#### 2. 假设提出：从“被动检索”到“主动过滤”\n*   **假设**：如果能在观察产生的瞬间，就判断其是否对未来任务有用，并直接丢弃无用信息，那么就不需要复杂的检索系统，且能极大节省内存。\n*   **类比**：这就像给模型装了一个“遗忘阀门”，只让有价值的信息进入上下文窗口。\n\n#### 3. 方法设计：从“模型微调”到“可插拔头”\n*   **设计挑战**：如何让模型学会这个“阀门”？\n    *   *约束*：不能微调庞大的 MLLM 主干（成本太高，不灵活）。\n*   **方案构思**：设计一个轻量级的、可训练的“记忆头” $\\mu$，挂载在冻结的 MLLM 主干上。\n    *   *功能*：$\\mu$ 作为一个二分类器，输入当前观测的 Embedding，输出 0（丢弃）或 1（保留）。\n    *   *优势*：模块化设计，即插即用，可以迁移到任何 MLLM 上。\n\n#### 4. 训练策略：从“离线模仿”到“在线强化”\n*   **思考**：如何训练这个 $\\mu$？什么样的记忆才是“重要”的？\n*   **路径 A（离线专家模仿）**：\n    *   *逻辑*：既然强模型（如 GPT-4o）知道怎么做，那就让它教弱模型。\n    *   *方法*：收集强模型成功轨迹中的观测，标记为正样本，失败或无关的标记为负样本，训练 $\\mu$ 进行二分类。\n*   **路径 B（在线强化学习）**：\n    *   *逻辑*：任务的成功与否才是检验记忆价值的唯一标准。\n    *   *方法*：将 $\\mu$ 视为策略网络的一部分。设计奖励函数（稀疏奖励：任务成功；密集奖励：动作有效），通过 REINFORCE 算法让 $\\mu$ 在探索中学会保留那些能带来高回报的记忆。\n\n#### 5. 验证与反馈：聚焦长尾与复杂场景\n*   **预期**：这种方法在短任务中可能效果不明显（因为不需要太多记忆），但在长周期、复杂指令任务中应该有显著提升。\n*   **实验验证**：选择表现较差的小模型（Qwen, Gemma），在 EmbodiedBench 的长指令子集上进行测试。\n*   **结果印证**：实验表明，加入 $\\mu$ 后，模型不仅任务成功率提升（尤其是长任务），而且无效动作减少，证明了“主动过滤”确实比“全量存储”或“无记忆”更有效。"
                },
                {
                    "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
                    "arxiv_id": "2601.20641",
                    "authors": "Boaz Carmeli, Orr Paradise, Shafi Goldwasser, Yonatan Belinkov, Ron Meir",
                    "summary": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Multi-Agent 与 Self-Evolving 方向）**： 论文的核心贡献在于研究基于LLM的智能体在协作推理任务中如何**开发**面向任务的通信协议。这直接对应了研究课题中的 **\"Multi-Agent\"（多智能体）** 方向（智能体间的协作、通信）以及 **\"Self-Evolving\"（自我演化）** 方向（智能体通过交互和反馈自发演化出新的语言协议，即 \"spontaneous coordination\" 和 \"develop task-oriented communication protocols\"）。论文并非简单应用已有框架，而是探索智能体行为的涌现和演化机制。 2.  **正面指标（高度相关）**： 摘要中明确包含了多个核心关注点：`LLM-based agents`、`Collaborative reasoning`（协作推理）、`Communication`（通信）、`Task-oriented protocols`（面向任务的协议）以及 `Spontaneous coordination`（自发协调）。这些都是多智能体系统和社会学习中的关键议题。 3.  **排除标准分析（通过例外条款）**： *   **关于多模态与视觉**：虽然论文标题和摘要提到了 \"Vision-Language Models (VLMs)\" 和视觉指代游戏，但根据筛选标准中的例外条款——\"除非它们被用作智能体感知环境的工具，而不是研究的核心\"。在这篇论文中，视觉能力仅是智能体感知环境（指代游戏中的图像）的手段，论文的研究核心是**通信协议的演化与特性**（效率与隐蔽性），而非视觉模型的改进或视觉理解技术本身。因此，符合例外情况，不应排除。 *   **关于安全与对齐**：虽然论文讨论了通信的 \"Covertness\"（隐蔽性）和 \"risks\"（风险），但这属于对智能体涌现行为的**观察和分析**，而非论文的主要贡献是提出一种新的安全防御或对齐算法。论文旨在揭示智能体的能力边界，属于 Agentic AI 的基础研究，而非纯粹的安全研究。 综上所述，该论文深入探讨了多智能体环境下的通信演化机制，属于 Agentic AI 的前沿研究，符合筛选要求。",
                    "summary2": "本文旨在探究VLM智能体能否开发出具有高效性和隐蔽性的任务导向通信协议。针对视觉语言模型在指称游戏中的协作推理场景，我们提出了一种基于零样本提示诱导智能体生成特定语言变体的方法，并在MS-COCO、CLEVR及FLAGS数据集上通过游戏准确率、描述长度及新词率验证了其有效性。",
                    "summary_translation": "我们探究了 LLM-based agents (基于大语言模型的智能体) 是否能够在协作推理任务中开发出不同于标准自然语言的 task-oriented communication protocols (面向任务的通信协议)。我们重点关注此类协议可能表现出的两个核心属性：Efficiency (效率)——即比自然语言更简洁地传达任务相关信息；以及 Covertness (隐蔽性)——即变得难以被外部观察者解读，从而引发对透明度和控制的担忧。为了探究这些方面，我们采用了 referential-game framework (指代游戏框架)，让 vision-language model (VLM) agents (视觉-语言模型智能体) 在其中进行通信，从而为评估语言变体提供一个可控且可测量的环境。实验结果表明，VLMs 能够开发出有效的、task-adapted (适应任务的) 通信模式。同时，它们还能开发出人类和外部智能体难以解读的 covert protocols (隐蔽协议)。我们还观察到，在没有 explicitly shared protocols (显式共享的协议) 的情况下，相似模型之间能够实现 spontaneous coordination (自发协调)。这些发现突显了面向任务的通信的潜力与风险，并将 referential games (指代游戏) 确立为该领域未来研究的重要 testbed (试验台)。",
                    "inspiration_trace": "基于对论文《Investigating the Development of Task-Oriented Communication in Vision-Language Models》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n---\n\n### 第一阶段：宏观观察与核心质疑\n**（从“语言即世界”到“AI是否受困于人类语言”）**\n\n1.  **哲学起点**：作者引用维特根斯坦的名言“语言的界限意味着世界的界限”，确立了语言作为智能和沟通基石的地位。\n2.  **现实背景**：随着大语言模型（LLM）和视觉语言模型（VLM）能力的飞跃，AI智能体已经开始使用自然语言进行协作和推理。\n3.  **核心冲突**：作者敏锐地指出，自然语言是经过人类认知进化筛选的产物，它是否真的适合作为AI智能体之间的沟通媒介？当AI的能力在特定领域超越人类时，人类语言的“包袱”是否会限制AI的效率或潜能？\n4.  **初步假设**：AI智能体如果能发展出一种与其内部表征和推理机制更一致的“任务导向语言变体”，可能会获得比自然语言更好的性能。\n\n### 第二阶段：问题聚焦与属性定义\n**（从“更好的语言”到“效率与隐蔽性”）**\n\n1.  **具体化目标**：作者将“更好的语言”具体化为两个可测量的维度：\n    *   **效率**：能否比自然语言更简洁地传达任务相关信息？\n    *   **隐蔽性**：能否在通信双方之间可理解，但对外部观察者（人类或其他未授权智能体）保持不透明？\n2.  **文献缺口识别**：现有的“涌现通信”研究大多关注从零开始训练的简单智能体。然而，本文关注的是**预训练的VLM**（如GPT-4o）。这些模型已经深深植入了自然语言的先验知识，让它们“忘掉”自然语言并创造新协议是一个巨大的迁移挑战。\n\n### 第三阶段：方法论构建与验证路径\n**（从“如何评估不可读的语言”到“指称游戏框架”）**\n\n1.  **评估难题**：如果AI发明了一种人类看不懂的语言，我们如何知道它是有效的？传统的基于人类可读性的评估标准失效了。\n2.  **解决方案**：作者引入了**指称游戏**框架。\n    *   **逻辑**：在这个游戏中，发送者描述一个目标图像，接收者从候选图像中找出它。\n    *   **优势**：评估标准完全基于“沟通成功率”（是否找对图像），而不依赖于人类对语言内容的理解。这为研究不透明的、涌现的协议提供了完美的受控环境。\n3.  **实验设定**：利用Zero-shot Prompting（零样本提示）引导VLM在受约束的条件下（如字数限制、隐蔽性要求）生成语言变体，观察它们是否能自发协调。\n\n---\n\n### 附录：Introduction 中的“讲故事”逻辑提取\n\n作者在Introduction中构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **宏大背景**：语言长期以来是人类智能和沟通的基础。\n2.  **技术转折**：LLM和VLM的进步使得AI智能体能够使用自然语言进行复杂的协作推理。\n3.  **批判性提问**：随着AI能力在各个领域超越人类，我们必须追问——自然语言（受限于人类认知的适应性）是否在某些任务中限制了AI智能体？\n4.  **提出愿景**：AI智能体可能会从发展任务导向的语言变体中受益，这些变体更符合其内部表征。\n5.  **界定维度**：这种语言变体应具备两个核心属性——**效率**（更简洁）和**隐蔽性**（对外部观察者不透明）。\n6.  **现实挑战**：虽然已有关于“涌现通信”的研究，但大多针对从零训练的简单智能体。对于主要在自然语言上训练的LLM/VLM来说，通过提示让它们创造全新的通信协议是一个巨大的迁移挑战。\n7.  **方法引入**：为了在不需要人类解释的情况下客观评估这种不透明的通信，我们采用了**指称游戏**框架。\n\n---\n\n### 总结：核心研究问题\n\n基于上述逻辑推演，本文试图回答的核心研究问题可总结为：\n\n**在指称游戏框架下，预训练的视觉语言模型（VLM）能否通过零样本提示，自发发展出一种在效率上超越自然语言、且对外部观察者具有隐蔽性的任务导向通信协议？**"
                },
                {
                    "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
                    "arxiv_id": "2601.20380",
                    "authors": "Le Zhang, Yixiong Xiao, Xinjiang Lu, Jingjia Cao, Yusai Zhao, Jingbo Zhou, Lang An, Zikan Feng, Wanxiang Sha, Yu Shi, Congxi Xiao, Jian Xiong, Yankai Zhang, Hua Wu, Haifeng Wang",
                    "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献在于**构建**了一个名为 OmegaUse 的通用 GUI 智能体模型。它不仅仅是将现有模型作为工具应用，而是提出了一套完整的方法论，包括精心设计的数据构建管道（Data-construction pipeline）和解耦的训练范式（Decoupled training paradigm）。这直接对应了筛选目标中的“构建、改进 LLM 智能体”。 2.  **符合研究焦点（单智能体 Agentic）**： 论文明确属于“单智能体”范畴。摘要中提到的关键能力包括： *   **自主任务执行**：支持移动端和桌面端的自主操作。 *   **规划**：使用了 Group Relative Policy Optimization (GRPO) 来提升“序列规划”能力。 *   **工具使用**：GUI 智能体的本质就是通过图形界面作为工具与环境进行交互。 *   **感知与定位**：提升空间定位能力，这是智能体理解环境的基础。 3.  **排除标准检查（通过）**： *   **非演化型应用**：虽然论文涉及手机和电脑操作，但它不是单纯地将智能体用于解决某个具体的业务问题（如“用手机订票”），而是致力于打造一个**通用**的智能体模型，因此属于基础框架构建，而非单纯的应用。 *   **多模态与视觉**：虽然 GUI 智能体必然涉及处理屏幕截图（视觉信息），但根据筛选标准中的例外条款，这里的视觉仅作为智能体“感知环境的工具”，而非研究视觉模型本身（如提出新的视觉编码器或生成式图像模型）。论文的核心在于智能体的决策、规划和训练逻辑，而非视觉算法。 4.  **正面指标匹配**： 论文包含了大量核心关键词，如 `Agentic`（智能体）、`Planning`（规划）、`Autonomous`（自主）、`Exploration`（探索）等。 综上所述，该论文提出了一种新的通用智能体构建框架，改进了智能体的规划和交互能力，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在构建一个通用的GUI Agent，用于在移动端和桌面端进行自主任务执行。针对现有数据质量低和跨终端评估不足的问题，我们提出了一种基于MoE架构的OmegaUse模型，采用解耦的两阶段训练范式（SFT和GRPO）以及包含自底向上探索和自顶向下生成的数据构建管道。我们在ScreenSpot-V2、AndroidControl和OS-Nav等基准上通过准确率和Step Success Rate验证了其有效性，达到了SOTA水平。",
                    "summary_translation": "**中文翻译：**\n\n图形用户界面 (GUI) 智能体展现出巨大的潜力，能够赋能基础模型完成现实世界的任务，从而彻底变革人机交互并提升人类生产力。在本报告中，我们提出了 OmegaUse，这是一个通用的 GUI 智能体模型，旨在移动端和桌面端平台上执行自主任务，支持 computer-use（计算机使用）和 phone-use（手机使用）场景。构建高效的 GUI 智能体模型依赖于两个关键因素：(1) 高质量数据和 (2) 有效的训练方法。针对这两点，我们引入了一套精心设计的数据构建流水线以及一种解耦的训练范式。在数据构建方面，我们利用了严格筛选的开源数据集，并提出了一种新颖的自动合成框架；该框架将自下而上的自主探索与自上而下的分类学引导生成相结合，从而生成高保真的合成数据。在训练方面，为了更充分地利用这些数据，我们采用了一种两阶段策略：首先通过监督微调 (Supervised Fine-Tuning, SFT) 建立基础的交互语法，随后利用组相对策略优化 (Group Relative Policy Optimization, GRPO) 提升空间定位和序列规划能力。为了在计算效率和智能体推理能力之间取得平衡，OmegaUse 基于混合专家模型 (Mixture-of-Experts, MoE) 骨干网络构建。为了在离线设置下评估跨终端能力，我们引入了 OS-Nav，这是一套涵盖多个操作系统的基准测试套件：包括针对中文 Android 移动环境的 ChiM-Nav，以及专注于 Ubuntu 常规桌面交互的 Ubu-Nav。大量实验表明，OmegaUse 在现有的 GUI 基准测试中表现出极强的竞争力，在 ScreenSpot-V2 上取得了 96.3% 的最先进 (State-of-the-Art, SOTA) 分数，并在 AndroidControl 上实现了 79.1% 的领先步骤成功率。OmegaUse 在 OS-Nav 上同样表现优异，在 ChiM-Nav 上达到了 74.24% 的步骤成功率，在 Ubu-Nav 上达到了 55.9% 的平均成功率。",
                    "inspiration_trace": "基于对论文《OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution》的深度分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观实现的思考过程。\n\n---\n\n### 第一部分：Introduction 中的“故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从“愿景”到“现实落差”，再到“根因分析”的叙事逻辑，具体推演如下：\n\n1.  **宏观愿景**：\n    *   **观察**：图形用户界面（GUI）智能体具有巨大的潜力，能够使基础模型像人类一样操作数字环境（移动端和桌面端），彻底改变人机交互并提高生产力。\n    *   **现状**：当前的智能体已经能够通过感知屏幕状态（截图）并执行原子操作（点击、输入、滑动）来连接高层用户意图与底层操作序列。\n\n2.  **现实冲突**：\n    *   **转折**：尽管取得了显著进展，但现有的 GUI 智能体在性能、训练数据质量和缺乏全面评估体系方面仍面临关键瓶颈。\n    *   **具体表现**：现有的智能体在处理复杂的数字生态系统时，往往表现出鲁棒性不足和泛化能力差的问题。\n\n3.  **根因诊断**：\n    *   **数据质量危机（核心痛点）**：\n        *   *感知层面*：在定位任务中，从 HTML 或无障碍树自动提取的标签存在渲染偏移，导致边界框对齐不准和文本描述模糊，严重破坏了空间感知能力。\n        *   *决策层面*：在导航数据集中，存在大量不一致性，如错误的执行轨迹和过度的冗余动作，这些弱信号或非连贯的监督无法支持长视野规划。\n    *   **评估体系缺失**：现有的基准测试未能完全覆盖多样化的数字环境（如中文移动应用或多步骤桌面工作流），导致无法全面评估智能体的跨终端能力。\n\n---\n\n### 第二部分：核心研究问题\n\n基于上述“故事”逻辑中揭示的愿景与痛点之间的鸿沟，作者显式提出了本文试图解决的核心问题：\n\n**“如何构建一个通用的 GUI 智能体，通过解决训练数据中的噪声与不一致性问题，并采用有效的训练范式，从而在跨平台（移动端与桌面端）的复杂任务中实现高精度的空间感知和鲁棒的序列规划能力？”**\n\n---\n\n### 第三部分：思想演进与方法论形成逻辑链\n\n为了回答上述研究问题，作者的思考过程经历了从“现象观察”到“策略假设”，再到“具体方案”的演进。\n\n#### 1. 现象观察与假设提出\n*   **观察**：现有的 GUI 智能体往往将“看哪里”和“做什么”混在一起训练，且数据质量参差不齐（噪声大、覆盖窄）。\n*   **假设 1（架构层面）**：为了兼顾强大的推理能力和计算效率，不能单纯依赖稠密模型，而应采用混合专家架构，在保持大模型推理深度的同时降低计算开销。\n*   **假设 2（数据层面）**：数据质量是决定性因素。单纯依赖开源数据不够，单纯人工标注成本太高。必须构建一个“混合数据管道”，结合人工清洗的高质量数据和自动合成的高覆盖数据。\n*   **假设 3（训练层面）**：空间定位和序列规划是两种不同的能力，混合训练会导致相互干扰。应当采用“解耦训练”策略，分别优化感知和决策。\n\n#### 2. 策略细化：如何解决“数据质量”问题？\n*   **思考**：如何获得既多又好的数据？\n*   **策略演进**：\n    *   **对于定位数据**：开源数据量大但噪声高（40%有误）。-> **对策**：实施严格的过滤程序，通过人工校准偏移的边界框和重写模糊指令，将 166 万条原始数据蒸馏为 11 万条高保真样本。\n    *   **对于导航数据**：需要覆盖长链路和复杂逻辑。-> **对策**：提出“分层合成框架”。\n        *   *开源数据*：利用 AGUVIS 等现有资源，但用规则和 MLLM 审计器清洗噪声。\n        *   *自动合成（核心创新）*：单纯自上而下（任务驱动）难以覆盖所有状态，单纯自下而上（随机探索）缺乏目标。-> **融合方案**：结合“自下而上的自主探索”（构建状态转移图）与“自上而下的分类学引导”（基于专家知识生成任务），确保数据的广度和深度。\n        *   *专家演示*：收集跨终端的高质量专家轨迹作为基准。\n\n#### 3. 策略细化：如何解决“训练干扰”问题？\n*   **思考**：模型学会了语法（SFT）后，如何进一步提升精度和鲁棒性？\n*   **策略演进**：\n    *   **阶段一（SFT）**：先通过监督微调建立基础的交互语法和任务逻辑，让模型“懂规矩”。\n    *   **阶段二（RL）**：引入强化学习（GRPO）进行精细化打磨。\n        *   *针对定位*：设计“边界框内奖励”，强迫模型关注交互区域中心而非边界像素。\n        *   *针对导航*：设计多维奖励函数（格式奖励、动作类型精度、坐标精度、内容保真度），平衡结构正确性与执行逻辑。\n\n#### 4. 策略细化：如何验证“通用性”？\n*   **思考**：现有基准无法证明模型在中文环境和 Linux 桌面环境的能力。\n*   **策略演进**：构建 OS-Nav 基准套件。\n    *   *ChiM-Nav*：针对中文安卓生态，填补中文应用评估空白。\n    *   *Ubu-Nav*：针对 Ubuntu 桌面，填补常规桌面交互评估空白。\n\n#### 5. 最终方法论形成\n*   **架构**：基于 MoE 的 OmegaUse 模型。\n*   **数据管道**：人工清洗的开源数据 + (自下而上探索 + 自上而下分类引导) 的自动合成数据 + 专家演示。\n*   **训练范式**：解耦的两阶段训练（SFT + GRPO），分别针对定位和导航任务设计特定的奖励函数。\n*   **评估体系**：在标准基准（ScreenSpot, AndroidControl）和自建基准（OS-Nav）上进行全方位验证。\n\n---\n\n**总结**：\n作者的思考路径始于对 GUI 智能体“数据质量差”和“任务耦合度高”这两个核心痛点的深刻洞察。通过**“数据工程化”**（分层合成管道）解决输入质量问题，通过**“训练解耦化”**（SFT+GRPO 分阶段优化）解决模型能力干扰问题，最终通过**“架构高效化”**（MoE）实现落地，形成了一套完整的通用 GUI 智能体构建方法论。"
                },
                {
                    "title": "AMA: Adaptive Memory via Multi-Agent Collaboration",
                    "arxiv_id": "2601.20352",
                    "authors": "Weiquan Huang, Zixuan Wang, Hehai Lin, Sudong Wang, Bo Xu, Qian Li, Beier Zhu, Linyi Yang, Chengwei Qin",
                    "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于核心关注的“LLM智能体”构建与改进方向。 1.  **核心贡献符合“构建与改进LLM智能体”**： 论文提出了AMA（Adaptive Memory via Multi-Agent Collaboration），这是一个专门为LLM智能体设计的自适应记忆框架。其核心贡献在于解决智能体在长期交互和复杂推理中的记忆管理问题，这直接对应了我研究焦点中的“单智能体”方向下的“记忆”能力。 2.  **明确涉及“多智能体”协作**： 论文标题和摘要均强调了“Multi-Agent Collaboration”。该框架通过协调多个具有不同角色的智能体（Constructor负责构建、Retriever负责检索、Judge负责验证、Refresher负责更新）来共同管理记忆。这种通过多智能体分工协作来提升整体系统性能的方法，完全符合“多智能体”方向的研究范畴。 3.  **包含“自我反思与修正”机制**： 摘要中提到Judge智能体会验证检索内容的相关性和一致性，并在检测到逻辑冲突时触发Refresher进行针对性更新。这种机制体现了智能体的自我反思和自我修正能力，属于Agentic AI的高级能力特征。 4.  **排除标准检查**： *   该论文不是将LLM简单应用于生物、医疗等特定领域的垂直应用，而是致力于改进智能体本身的基础架构（记忆系统）。 *   不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 *   不属于基础设施或硬件加速研究。 综上所述，该论文通过多智能体协作机制改进了LLM智能体的核心记忆能力，属于Agentic AI架构创新的高质量研究，应予以保留。",
                    "summary2": "本文旨在解决现有LLM agent记忆系统中检索粒度僵化及逻辑不一致的问题。针对长上下文交互场景，我们提出了一种名为AMA的多智能体协作框架，利用Constructor、Retriever、Judge和Refresher实现多粒度记忆构建、自适应路由及一致性维护。在LoCoMo和LongMemEval s基准上，通过LLM Score、F1和BLEU-1等指标验证了其有效性，显著优于SOTA基线并大幅降低了token消耗。",
                    "summary_translation": "Large Language Model (LLM) agents (大型语言模型智能体) 的快速演进，迫切需要 robust memory systems (健壮的记忆系统) 来支持连贯的长期交互和复杂推理。受益于 LLMs 的强大能力，近期的研究重心已从简单的 context extension (上下文扩展) 转向开发专用的智能体记忆系统。然而，现有方法通常依赖于 rigid retrieval granularity (僵化的检索粒度)、accumulation-heavy maintenance strategies (以积累为主的维护策略) 以及 coarse-grained update mechanisms (粗粒度的更新机制)。这些设计选择导致存储信息与 task-specific reasoning demands (特定任务的推理需求) 之间存在 persistent mismatch (持续的错配)，同时导致 logical inconsistencies (逻辑不一致性) 随时间 unchecked accumulation (不受控制地积累)。为了解决这些挑战，我们提出了 Adaptive Memory via Multi-Agent Collaboration (AMA) (基于多智能体协作的自适应记忆)，这是一个利用 coordinated agents (协调的智能体) 在 multiple granularities (多种粒度) 上管理记忆的 novel framework (新颖框架)。AMA 采用了 hierarchical memory design (分层记忆设计)，能够动态地将 retrieval granularity (检索粒度) 与 task complexity (任务复杂度) 对齐。具体而言，Constructor (构造器) 和 Retriever (检索器) 共同实现了 multi-granularity memory construction (多粒度记忆构建) 和 adaptive query routing (自适应查询路由)。Judge (评判器) 验证检索内容的相关性和一致性，在证据不足时触发 iterative retrieval (迭代检索)，或在检测到 logical conflicts (逻辑冲突) 时调用 Refresher (刷新器)。Refresher (刷新器) 随后通过执行 targeted updates (有针对性的更新) 或移除 outdated entries (过时条目) 来强制执行 memory consistency (记忆一致性)。在具有挑战性的 long-context benchmarks (长上下文基准测试) 上进行的大量实验表明，AMA 显著优于 state-of-the-art baselines (最先进的基线模型)，同时与 full-context methods (全上下文方法) 相比减少了约 80% 的 token consumption (Token消耗)，证明了其在 maintaining retrieval precision (维持检索精度) 和 long-term memory consistency (长期记忆一致性) 方面的有效性。",
                    "inspiration_trace": "基于对论文《AMA: Adaptive Memory via Multi-Agent Collaboration》的深度分析，以下是作者产出该文章的系统性思考过程推演。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在引出当前研究的必要性：\n\n1.  **背景铺垫：** 随着LLM智能体在复杂推理和多轮交互中的能力增强，维持长期连贯性和一致性的“记忆系统”变得至关重要。\n2.  **范式确立：** 现有的记忆方案分为“内部记忆”（模型参数微调）和“外部记忆”（显式存储检索）。由于内部记忆容量有限且更新成本高，**外部记忆**成为了当前的主流选择。\n3.  **第一重困境（静态粒度的错位）：** 尽管外部记忆可扩展，但现有方法主要依赖**静态的文本分块**或**粗粒度摘要**。这种“一刀切”的策略导致了一个核心矛盾：检索粒度与任务需求不匹配。过粗的检索引入噪声，过细的检索割裂逻辑依赖，最终导致复杂任务推理失败。\n4.  **第二重困境（智能体记忆的局限）：** 为了解决静态问题，近期研究转向利用LLM生成能力的“智能体记忆”。然而，作者指出这些方法仍存在两个未解决的痛点：\n    *   **缺乏自适应路由：** 无法在推理时动态选择合适的记忆粒度，依然存在错位。\n    *   **维护机制粗糙：** 依赖“累积型”策略，缺乏精细的更新机制，导致逻辑冲突和错误随时间 unchecked 地堆积。\n5.  **破局点：** 为了同时解决“自适应检索控制”和“长期记忆演化”这两个耦合挑战，作者提出了基于多智能体协作的AMA框架。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题是：\n\n**“如何设计一种记忆系统，使其能够动态对齐检索粒度与特定任务的推理需求，同时通过有效的维护机制确保长期记忆的逻辑一致性？”**\n\n---\n\n### 三、 思想演进与方法论形成的逻辑链\n\n以下是从宏观观察到具体方法论的推演过程：\n\n#### 1. 观察与痛点识别：记忆的“刚性”与“混乱”\n*   **观察：** 现有的外部记忆系统（如RAG、MemGPT等）大多像是一个死板的档案室。无论你问什么问题，它都给你扔出固定大小的文档块或者大段的摘要。\n*   **痛点：** 这种“刚性”导致了效率低下和准确性受损。更糟糕的是，随着时间推移，这些系统只进不出，旧信息和新信息打架（逻辑冲突），系统却无法自我修正。\n*   **初步设想：** 我们需要一个“活”的记忆系统，它既能根据问题灵活调整查看信息的细致程度，又能像人脑一样定期清理和修正过时的记忆。\n\n#### 2. 假设提出：从“单体控制”到“多智能体分工”\n*   **反思：** 为什么之前的系统做不到“自适应”和“自修正”？因为它们试图用一个单一的LLM控制器来处理所有事情（既要存储、又要检索、还要检查逻辑）。这就像让一个人同时做图书管理员、侦探和清洁工，容易顾此失彼，目标函数冲突。\n*   **假设：** 如果我们将记忆的生命周期拆解，让不同的智能体各司其职，通过协作来管理记忆，是否能解决上述问题？\n*   **核心思想：** **关注点分离**。将复杂的记忆管理任务解耦为四个独立但相互依赖的角色。\n\n#### 3. 架构设计：四个角色的逻辑闭环\n基于上述假设，作者构建了四个智能体来对应记忆管理的四个核心环节：\n\n*   **环节一：记忆的构建（解决“怎么存”）**\n    *   *思考：* 为了解决“静态粒度”的问题，存储时就不能只有一种格式。\n    *   *方案：* 引入 **Constructor（构造者）**。它不简单存储文本，而是将其转化为**分层粒度**：\n        *   *Raw Text（原始文本）：* 保留细节。\n        *   *Fact Knowledge（事实知识）：* 提取原子化事实（S-V-O结构），便于精确检索。\n        *   *Episode Memory（情节记忆）：* 生成高层摘要，便于宏观理解。\n    *   *逻辑：* 只有存储了多粒度的信息，后续的“自适应”才有物质基础。\n\n*   **环节二：记忆的访问（解决“怎么找”）**\n    *   *思考：* 有了多粒度的库存，怎么保证用户问问题时能拿到最合适的那一种？\n    *   *方案：* 引入 **Retriever（检索者）**。它不直接检索，而是先进行**意图路由**。\n    *   *逻辑：* 它分析用户的查询意图（是需要细节？还是需要总结？还是需要某个原子事实？），然后动态决定去Raw Text、Fact还是Episode里找。这实现了“检索粒度与任务复杂度的动态对齐”。\n\n*   **环节三：记忆的质检（解决“准不准”）**\n    *   *思考：* 检索出来的东西一定对吗？如果检索结果不相关或者包含过时信息怎么办？\n    *   *方案：* 引入 **Judge（法官）**。作为一个逻辑审计员。\n    *   *逻辑：* 它做两件事：一是**相关性检查**（如果不够相关，触发重试）；二是**冲突检测**（如果发现检索内容与当前输入矛盾，触发修正）。这是为了防止错误信息进入最终推理。\n\n*   **环节四：记忆的维护（解决“旧不旧”）**\n    *   *思考：* 一旦发现了冲突或过时信息，谁来处理？\n    *   *方案：* 引入 **Refresher（刷新者）**。\n    *   *逻辑：* 它是执行者。当Judge报警时，Refresher负责精准地**更新**（Update）过时条目或**删除**（Delete）无效条目。这解决了“逻辑冲突随时间 unchecked 积累”的问题。\n\n#### 4. 综合与验证：AMA框架的诞生\n*   **整合：** 将这四个智能体串联起来：Retriever负责入口路由，Judge负责质量把关，Refresher负责后台维护，Constructor负责持续将新信息转化为结构化记忆。\n*   **预期效果：** 这个闭环系统不仅比单一控制器更稳定，而且通过多粒度存储和动态路由，大幅提升了检索精度；通过Judge和Refresher的配合，实现了长期记忆的一致性。\n\n#### 5. 最终产出\n*   作者将这套思想命名为 **AMA (Adaptive Memory via Multi-Agent Collaboration)**，并通过实验验证了其在长上下文基准测试中，不仅性能优于SOTA，还大幅降低了Token消耗（证明了检索的高效性）。\n\n---\n\n**总结：** 作者的思考路径是从**发现现有记忆系统“僵化”且“易脏”**的缺陷出发，通过**引入多智能体分工的哲学**，将记忆管理拆解为**构造、路由、审计、刷新**四个专业环节，最终构建了一个既能**灵活适应任务需求**又能**自我进化保持逻辑一致**的记忆框架。"
                },
                {
                    "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
                    "arxiv_id": "2601.20379",
                    "authors": "Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang, Zifan Zhang, Dezhang Kong, Meng Han",
                    "summary": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，核心依据如下： 1.  **核心贡献属于“自我演化”与“Agentic AI”范畴**： 论文提出了 \"Policy of Thoughts (PoT)\" 框架，其核心在于通过“测试时策略演化”来改进 LLM 的推理能力。这直接对应了我的研究焦点中的 **“自我演化”** 方向。论文明确指出智能体需要通过“从失败尝试中学习”来实现“实时演化”，这是一种典型的基于反馈的自我完善机制。 2.  **符合“Agentic”的闭环设计特征**： 不同于单纯的静态推理方法（如标准的 CoT），该论文构建了一个包含“执行反馈”和“策略更新”的闭环系统。它利用 Group Relative Policy Optimization (GRPO) 根据执行反馈动态更新瞬态 LoRA 适配器。这种与环境交互、接收反馈并调整自身策略的过程，正是 **Agentic AI** 的核心特征（即自我修正 Self-Correction 和迭代优化 Iterative Improvement）。 3.  **符合筛选标准中的特殊规则（第四步）**： 根据第四步关于“推理/规划”的规则，该论文不仅仅是提高模型的基础 Token 预测能力，而是提出了一种新的 **Agentic 框架**（将推理重构为在线优化过程）。它涉及多步推理和动态调整，属于保留范畴。 4.  **非排除项**： 论文虽然使用了代码基准（LiveCodeBench）进行测试，但其核心贡献是通用的方法论框架，而非特定领域的非演化型应用。同时，论文不涉及安全对齐、多模态或图技术等排除内容。 综上所述，该论文通过引入基于反馈的在线策略优化机制，实现了 LLM 智能体的自我演化和推理能力的动态提升，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决LLM在复杂推理中因冻结策略导致的不稳定性问题。针对测试时计算扩展的场景，我们提出了一种Policy of Thoughts (PoT)框架，利用MCTS探索和GRPO优化瞬态LoRA适配器以实现策略实时进化。在LiveCodeBench等代码推理基准上，通过准确率验证了其有效性，使4B模型超越了GPT-4o等大模型。",
                    "summary_translation": "大语言模型因其冻结策略假设导致的不稳定性，难以应对复杂的、长视距推理任务。当前的测试时扩展方法仅将执行反馈视为用于筛选或重写轨迹的外部信号，而未能将其内化以改进底层的推理策略。受波普尔“猜想与反驳”认识论的启发，我们认为智能需要通过从失败尝试中学习，实现模型策略的实时进化。我们提出了思维策略框架，该框架将推理重新定义为一种实例内的在线优化过程。PoT 首先通过高效的探索机制生成多样化的候选解，随后利用群组相对策略优化基于执行反馈来更新瞬态 LoRA 适配器。这种闭环设计实现了对模型推理先验的动态、特定于实例的精细化调整。实验表明，PoT 显著提升了性能：一个 4B 参数的模型在 LiveCodeBench 上达到了 49.71% 的准确率，尽管其规模小了 50 多倍，但仍优于 GPT-4o 和 DeepSeek-V3。",
                    "inspiration_trace": "基于对论文《Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution》的深度分析，以下是作者产出该核心方法的逻辑链推演：\n\n### 第一阶段：宏观观察与问题引入（Introduction 的“故事”逻辑）\n\n作者首先构建了一个从“潜力”到“瓶颈”的叙事弧线，逻辑如下：\n\n1.  **现状与潜力**：大语言模型（LLMs）配合思维链正在成为通用的推理者，能够处理多步演绎和长程规划。\n2.  **遭遇挑战**：然而，在处理复杂、长周期的推理任务时，**推理的不稳定性**成为了核心瓶颈。\n3.  **现象描述**：这些任务诱导出了巨大的搜索空间，充满了“欺骗性的轨迹”。一旦在早期出现微小错误，整个推理链就会不可逆转地脱轨。\n4.  **根源诊断**：作者指出，这种不稳定性是**“冻结策略假设”**固有的缺陷。因为模型缺乏将失败尝试内化的机制，所以无法持续修正航向以收敛到正确解。\n5.  **批判现状**：目前的测试时扩展方法（如增加搜索宽度或反思深度）主要作为**后验轨迹过滤器**运作。它们将反馈视为外部选择信号，通过丢弃失败的猜想（Conjectures）来消耗算力，却**从未改进产生这些猜想的底层逻辑**。\n6.  **哲学隐喻**：引入波普尔的“猜想与反驳”认识论，指出智能应存在于理论的实时进化中。\n\n---\n\n### 第二阶段：核心研究问题\n\n基于上述对“冻结策略”导致的不稳定性以及现有方法“只筛选不进化”的批判，作者提炼出的核心研究问题是：\n\n**“我们如何超越LLM的‘冻结策略’假设，通过在测试时将执行反馈内化，实现推理策略的实时进化？”**\n\n---\n\n### 第三阶段：思想演进与方法论形成\n\n为了回答上述问题，作者的思考经历了从理论假设到技术落地的四个关键演进步骤：\n\n#### 1. 理论重构：从“筛选轨迹”到“进化策略”\n*   **思考**：既然现有的“外部筛选”模式（如Best-of-N, Self-Consistency）浪费了失败尝试中蕴含的信息，我们需要改变反馈的利用方式。\n*   **演进**：反馈不应仅仅是一个打分器，而应成为**梯度更新的信号**。推理不应是一次性的静态生成，而应是一个**在线优化过程**。目标是将“试错”转化为“学习”。\n\n#### 2. 机制设计：波普尔认识论的工程化\n*   **思考**：如何将“猜想与反驳”的哲学循环转化为算法？\n*   **演进**：\n    *   **猜想**：需要一个高效的探索机制来生成多样化的候选解（如蒙特卡洛树搜索 MCTS）。\n    *   **反驳**：利用环境执行反馈（如代码测试用例）作为客观的“反驳”信号。\n    *   **进化**：需要一个优化引擎，利用这些反驳信号来修改模型参数，从而更新下一次猜想生成的先验概率。\n\n#### 3. 技术选型：如何实现“实时”且“无损”的进化？\n*   **思考**：在测试时对整个大模型进行全量微调是不现实的（太慢、成本高），且会破坏模型的通用能力。\n*   **演进**：\n    *   **参数效率**：采用**瞬态LoRA适配器**。它是一个轻量级的临时容器，仅针对当前问题实例进行更新，推理结束后即丢弃。这既保证了实时性，又防止了灾难性遗忘。\n    *   **优化算法**：采用**群体相对策略优化（GRPO）**。它不需要额外的价值网络，仅通过一组轨迹内的相对奖励比较就能计算优势，非常适合这种单实例、样本稀疏的场景。\n\n#### 4. 闭环构建：Policy of Thoughts (PoT) 框架的诞生\n*   **思考**：如何将探索与进化无缝结合？\n*   **演进**：构建一个闭环系统。\n    *   **输入**：问题实例。\n    *   **循环**：MCTS探索生成轨迹 -> 环境执行反馈 -> GRPO计算梯度 -> 更新瞬态LoRA -> 基于新策略再次探索。\n    *   **输出**：经过多轮策略进化后的最终解。\n\n**总结**：作者的思想脉络是从发现“静态策略”无法应对复杂推理的痛点出发，借用波普尔的哲学视角，提出将“测试时计算”从“外部筛选”转化为“内部进化”，最终通过MCTS（探索）、GRPO（优化）和瞬态LoRA（载体）的组合，实现了在单次推理过程中的策略实时演进。"
                },
                {
                    "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
                    "arxiv_id": "2601.20014",
                    "authors": "Shuhui Qu",
                    "summary": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合要求**: 论文提出了 \"Self-Querying Bidirectional Categorical Planning (SQ-BCP)\"，这是一个新的规划框架，旨在解决LLM智能体在部分可观测环境（即缺少关键前提条件时）下的规划失效问题。这属于构建和改进LLM智能体核心能力的范畴。 2.  **符合Agentic AI的核心特征**: *   **规划**: 论文的核心焦点是提升智能体的推理时规划能力，特别是在处理未指定前提条件时的复杂规划。 *   **工具使用/交互**: 论文引入了“自我查询”机制，允许智能体主动向预言机或用户询问缺失信息，这体现了智能体与外部环境或工具进行交互的能力。 *   **自主性**: 智能体能够识别未知状态，并通过“桥接假设”或主动查询来解决，这超越了单纯的文本生成，体现了Agentic行为。 3.  **不涉及排除标准**: *   虽然标题中包含 \"Category-Theoretic\"（范畴论），但这指的是规划算法的数学理论基础，并非排除标准中提到的“知识图谱”或“图神经网络”等图数据结构相关技术。 *   论文虽然使用了WikiHow和RecipeNLG作为测试集，但其核心贡献是通用的规划算法框架，而非将现有模型简单应用于特定垂直领域的非演化型应用。 *   论文不涉及安全、对齐、多模态视觉等排除领域。 综上所述，该论文通过引入新的规划与交互机制，显著增强了LLM智能体在复杂任务中的鲁棒性，完全符合“单智能体”方向中关于“规划”和“工具使用”的研究目标。",
                    "summary2": "本文旨在解决LLM在部分可观测性下因缺失前提条件导致的推理失效问题。针对资源或约束未知的场景，我们提出了一种Self-Querying Bidirectional Categorical Planning (SQ-BCP) 框架，通过显式跟踪Sat/Viol/Unk前提状态，利用自我查询和桥接动作解决不确定性，并结合基于拉回的验证器。我们在WikiHow和RecipeNLG数据集上通过ROUGE、BLEU及资源违规率验证了其有效性，显著降低了违规率。",
                    "summary_translation": "大语言模型的推理时规划在部分可观测性条件下经常失效：当查询时未指定任务关键的前提条件时，模型往往会对缺失的事实产生幻觉，或生成违反硬约束的计划。我们提出了 **Self-Querying Bidirectional Categorical Planning (SQ-BCP)**（自查询双向分类规划），该方法显式表示 precondition status（前提状态）（\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}，即满足/违反/未知），并通过以下方式解决未知项：(i) 向 oracle/user（预言机/用户）发起 targeted self-queries（定向自查询），或 (ii) 利用 *bridging hypotheses*（桥接假设），即通过额外的操作来建立缺失的条件。SQ-BCP 执行 bidirectional search（双向搜索），并调用 pullback-based verifier（基于拉回的验证器）作为 goal compatibility（目标兼容性）的 categorical certificate（分类证书），而仅将 distance-based scores（基于距离的分数）用于 ranking and pruning（排序和剪枝）。我们证明了当验证器成功且 hard constraints（硬约束）通过 deterministic checks（确定性检查）时，accepted plans（被接受的计划）与目标要求兼容；在 bounded branching（有界分支）和 finite resolution depth（有限解析深度）的条件下，只要存在可接受计划，SQ-BCP 就能找到它。在包含 withheld preconditions（保留的前提条件）的 WikiHow 和 RecipeNLG 任务中，SQ-BCP 将 resource-violation rates（资源违规率）分别降低至 **14.9%** 和 **5.8%**（相比之下，最佳 baseline（基线）分别为 **26.0%** 和 **15.7%**），同时保持了具有竞争力的 reference quality（参考质量）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Reinforcement Learning via Self-Distillation",
                    "arxiv_id": "2601.20802",
                    "authors": "Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause",
                    "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献属于自我演化机制**: 论文提出了 Self-Distillation Policy Optimization (SDPO)，其核心思想是让模型利用环境反馈（如运行时错误、评判信息）来识别自身的错误，并通过“自蒸馏”的方式将这些反馈转化为学习信号，从而改进策略。这完全符合“自我演化”中关于“智能体通过经验、反思或环境反馈进行自我完善和迭代”的定义。 2.  **涉及Agentic核心能力**: 摘要中明确提到该方法在“工具使用”和“多轮对话”场景下进行了验证，并且依赖于智能体与可验证环境（如代码执行器）的交互来获取反馈。这表明论文不仅仅是关于静态的模型推理，而是关注智能体在环境中的交互与适应能力。 3.  **非排除项**: *   该论文不是单纯的应用型研究（如仅将LLM用于医疗或法律），而是提出了一种通用的训练/演化算法。 *   虽然涉及推理，但其方法依赖于外部反馈和自我修正，属于Agentic框架下的推理优化，而非单纯的内部CoT变体或基础Token预测能力提升。 *   不涉及安全对齐、多模态视觉或图技术等排除领域。 综上所述，该论文的核心在于提出一种新的自我演化/自我完善机制，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决强化学习中仅依赖标量奖励导致的信用分配瓶颈。针对可验证环境提供的丰富文本反馈，我们提出了一种Self-Distillation Policy Optimization (SDPO)方法，通过将当前模型作为“自教师”进行蒸馏，实现密集的信用分配。我们在LiveCodeBench v6和科学推理基准上通过准确率和样本效率验证了其有效性，结果显示SDPO显著优于现有基线。",
                    "summary_translation": "大语言模型越来越多地在代码和数学等可验证领域通过强化学习进行后训练。然而，当前的可验证奖励强化学习（RLVR, reinforcement learning with verifiable rewards）方法仅从每次尝试的标量结果奖励中学习，这造成了严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈，例如运行时错误或评判评估，解释了尝试失败的原因。我们将这一设定形式化为具有丰富反馈的强化学习，并引入了自蒸馏策略优化（SDPO, Self-Distillation Policy Optimization），它将分词化反馈转化为密集的学习信号，而无需任何外部教师或显式奖励模型。SDPO 将以反馈为条件的当前模型视为自教师，并将其基于反馈的下一个 token 预测蒸馏回策略中。通过这种方式，SDPO 利用了模型在上下文中追溯识别自身错误的能力。在 LiveCodeBench v6 上的科学推理、工具使用和竞技编程任务中，SDPO 相比强大的 RLVR 基线提高了样本效率和最终准确率。值得注意的是，SDPO 在仅返回标量反馈的标准 RLVR 环境中也优于基线，方法是将成功的轨迹作为失败尝试的隐式反馈。最后，在测试时将 SDPO 应用于单个问题，加速了在困难二元奖励任务上的发现，实现了与 best-of-k 采样或多轮对话相同的发现概率，而尝试次数减少了 3 倍。",
                    "inspiration_trace": "基于对论文《Reinforcement Learning via Self-Distillation》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n### 1. Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分构建了一个从宏观趋势到具体瓶颈的叙事逻辑，具体步骤如下：\n\n1.  **宏观背景**：深度强化学习（RL）和大型语言模型（LLM）的进步都依赖于一个核心主题——通过迭代经验（行动、接收反馈、更新策略）来解锁静态监督无法获得的能力。\n2.  **现状描述**：目前LLM的后训练主要依赖于**可验证奖励强化学习（RLVR）**。在这种设定下，模型生成答案后，环境仅返回一个标量奖励（例如代码测试通过/失败，二元奖励）。\n3.  **揭示核心冲突（瓶颈）**：这种标量奖励机制造成了严重的**信用分配瓶颈**。\n    *   标量奖励掩盖了底层的环境状态信息（即“为什么”失败）。\n    *   当一组尝试全部失败（奖励均为0）时，现有的策略梯度方法（如GRPO）的优势估计会坍缩为零，导致学习停滞。\n4.  **现有方案的局限性**：\n    *   **强教师蒸馏**：虽然能提供密集的监督，但在在线学习中，强教师往往不可用（因为目标是超越现有模型）。\n    *   **过程奖励模型（PRM）**：虽然能提供Token级信号，但通常仍基于标量奖励训练，且引入了额外的模型开销。\n5.  **关键观察（转折点）**：许多可验证环境实际上提供了**丰富的文本反馈**（如运行时错误、失败的单元测试、评判结果），而不仅仅是标量奖励。这些反馈解释了失败的具体原因。\n6.  **形式化新设定**：作者将这一更一般的设定形式化为**具有丰富反馈的强化学习（RLRF）**。\n\n---\n\n### 2. 研究问题\n\n基于上述逻辑，作者显式提出的研究问题是：\n\n**“在无需外部教师或显式奖励模型的情况下，如何将环境提供的丰富文本反馈转化为有效的信用分配信号？”**\n\n---\n\n### 3. 核心方法的逻辑演进（思想推演）\n\n为了回答上述问题，作者的思考过程经历了从观察到假设再到方法论的以下演进：\n\n#### 第一阶段：观察与洞察\n*   **观察**：LLM具备强大的**上下文学习**能力。当模型在上下文中接收到反馈（例如错误信息）时，它往往能够识别出自己之前的错误并提出修正方案。\n*   **思考**：既然模型在看到反馈后能“知道”哪里错了，那么“看到反馈后的模型”在某种程度上比“没看到反馈的模型”更聪明。\n\n#### 第二阶段：提出假设\n*   **假设**：我们可以利用同一个模型扮演两个角色。\n    *   **学生**：基于原始问题生成初始答案。\n    *   **自我教师**：基于原始问题 + 丰富反馈，对同一个答案进行重新评估。\n*   **推论**：由于“自我教师”拥有额外的反馈信息，它对下一个Token的预测分布应该比“学生”更准确。这种分布的差异本身就是一种天然的、密集的监督信号。\n\n#### 第三阶段：方法论构建\n*   **核心思想**：**自蒸馏**。\n*   **机制设计**：\n    1.  **生成**：学生策略 $\\pi_\\theta$ 生成答案 $y$。\n    2.  **反馈**：环境提供丰富反馈 $f$。\n    3.  **重评估**：将 $f$ 注入上下文，构造自我教师 $\\pi_\\theta(\\cdot | x, f)$。\n    4.  **信用分配**：不依赖标量奖励计算优势，而是计算学生与自我教师之间的**KL散度**。\n        *   如果自我教师认为某个Token概率很低（即这是个错误），而学生之前给了高概率，那么这个Token就会受到惩罚（负优势）。\n        *   如果两者一致，则不进行大幅调整。\n*   **结果**：这种方法将原本稀疏的标量奖励（0或1）转化为了**Token级别的密集信用分配**，且完全不需要外部教师。\n\n#### 第四阶段：验证与扩展\n*   **验证**：在只有标量奖励的环境中（无丰富反馈），作者进一步思考如何应用此逻辑。解决方案是：将同一批次中成功的尝试作为“反馈”提供给失败的尝试，从而构造出自我教师。\n*   **扩展**：将此逻辑应用于测试时，通过不断将上下文压缩进模型权重，加速解决极难问题。\n\n---\n\n**总结**：\n作者的思考路径是从**“标量奖励的信息瓶颈”**出发，利用**“LLM的上下文纠错能力”**，创造性地提出了**“自我教师”**的概念，最终通过**“自蒸馏”**将丰富的文本反馈转化为密集的优化信号，从而解决了RLVR中的信用分配难题。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 11,
            "papers": [
                {
                    "title": "SERA: Soft-Verified Efficient Repository Agents",
                    "arxiv_id": "2601.20789",
                    "authors": "Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers",
                    "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断（符合）**: 该论文的核心贡献是提出了一种名为 SERA (Soft-Verified Efficient Repository Agents) 的新方法，用于高效地训练和构建专门针对私有代码库的 LLM 智能体。这直接属于“构建、改进 LLM 智能体”的范畴，而非仅仅将现有智能体作为工具应用到特定领域。论文重点在于如何通过 Soft Verified Generation (SVG) 技术来改进智能体的训练过程和性能，符合第一步中的“保留”标准。 2.  **正面指标（匹配）**: *   论文明确属于 `LLM-based Agents` 范畴，研究对象是 Coding Agents。 *   涉及智能体的构建与改进，特别是通过生成轨迹进行监督微调（SFT）来增强智能体在特定环境（代码库）中的能力。 *   虽然主要侧重于单智能体，但其对智能体专门化（Specialization）的研究是 Agentic AI 的重要组成部分。 3.  **排除标准（未触发）**: *   论文不涉及安全、对齐、多模态视觉或图技术。 *   它不是基础设施研究，也不是单纯的应用型论文（其核心在于提出新的训练方法论 SVG）。 4.  **综合结论**: 该论文提出了一种新的智能体训练框架（SERA），旨在解决如何高效构建和改进特定领域的 LLM 智能体，完全符合“单智能体”方向中关于构建和改进智能体的研究目标。",
                    "summary2": "本文旨在降低训练私有代码库专用 Coding Agents 的成本与复杂度。针对私有代码库，我们提出了一种名为 SERA 的方法，其核心是 Soft Verified Generation (SVG)。该方法通过 Soft Verification（基于行级召回率的补丁比对）替代单元测试，并利用模糊指令生成多样化数据。我们在 SWE-bench Verified 上通过 Resolve Rate 和训练成本验证了其有效性，实现了 SOTA 开源性能且成本显著降低。",
                    "summary_translation": "开放权重编码代理应具备相对于闭源系统的基础优势：它们能够针对私有代码库进行专门化，将仓库特定的信息直接编码到其权重中。然而，训练的成本与复杂性使得这一优势长期以来仅停留在理论层面。我们表明，这一目标现已切实可行。我们提出了软验证高效仓库代理，这是一种训练编码代理的高效方法，能够快速且低成本地创建针对私有代码库专门化的代理。仅使用监督微调，SERA 在完全开源（开放数据、方法、代码）的模型中取得了最先进的结果，同时匹配了 Devstral-Small-2 等前沿开放权重模型的性能。在达到同等性能的前提下，创建 SERA 模型的成本比强化学习低 26 倍，比以往的合成数据方法低 57 倍。我们的方法，即软验证生成，能够从单个代码库中生成数千条轨迹。结合其高性价比，这一特性使得针对私有代码库的专门化成为可能。除了仓库专门化之外，我们还将 SVG 应用于更大规模的代码库语料库，生成了超过 200,000 条合成轨迹。我们利用该数据集对训练编码代理的扩展定律、消融实验以及混淆因素进行了详细分析。总体而言，我们相信这项工作将极大地加速开放编码代理的研究，并展示能够针对私有代码库进行专门化的开源模型的优势。我们发布了 SERA 作为 Ai2 开放编码代理系列的首个模型，同时发布了所有代码、数据以及 Claude Code 集成，以支持研究社区。",
                    "inspiration_trace": "基于对论文《SERA: Soft-Verified Efficient Repository Agents》的深入分析，以下是作者产出该文章的完整逻辑链推演。\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言中构建了一个从“理论优势”到“现实困境”再到“破局思路”的叙事逻辑：\n\n1.  **理论优势**：开源权重模型相比闭源模型有一个根本性的潜在优势——它们可以针对私有代码库进行微调，将仓库特定的模式、约定和领域知识直接编码进模型权重中。\n2.  **现实困境**：然而，这一优势目前仅停留在理论层面。因为训练代码智能体通常需要强化学习（RL）或复杂的合成数据管道（如SWE-smith），这些方法成本高昂且基础设施复杂（需要沙箱环境、测试套件等），导致只有资源丰富的实验室才能进行相关研究。\n3.  **核心动机**：作者团队资源有限（小团队、算力少），为了验证“私有代码库专业化”的可行性，他们必须大幅降低实验成本。\n4.  **破局思路**：通过系统性地剥离现有管道中的复杂组件，作者发现许多复杂性（如严格的单元测试验证、复杂的Bug注入）其实是不必要的。这为提出一种低成本、高效率的新方法奠定了基础。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**如何通过简化数据生成管道（去除对测试基础设施的依赖），以低成本实现开源权重模型对私有代码库的高效专业化？**\n\n---\n\n### 三、 思想演进与逻辑链推演\n\n以下是从宏观问题到具体方法论的思维演进过程：\n\n#### 1. 宏观观察：开源模型的“私有化”潜力与成本鸿沟\n*   **观察**：闭源模型（如Claude, GPT-4）虽然强大，但无法直接针对企业的私有代码库进行权重级优化。开源模型理论上可以做到这一点，但训练门槛太高。\n*   **思考**：如果能把训练成本降下来，让任何小团队都能在自己的代码库上微调模型，那么开源模型将在实际应用中超越闭源模型。\n\n#### 2. 痛点分析：现有方法的“过度工程”\n*   **现状分析**：现有的训练方法主要分为两类：\n    *   **强化学习（RL）**：需要在线交互、沙箱环境和复杂的奖励工程，极不稳定且昂贵。\n    *   **合成数据（如SWE-smith）**：依赖单元测试来验证生成的代码是否正确。这意味着必须有完善的测试环境，且数据生成受限于测试覆盖率。\n*   **反思**：对于一个资源有限的小团队，RL不可行。而合成数据方法对“测试环境”的强依赖，限制了其在缺乏测试的私有代码库上的应用。必须找到一种不依赖测试的验证方式。\n\n#### 3. 关键假设：验证的“软”化与指令的“模糊化”\n*   **假设一（关于验证）**：训练代码智能体的核心价值在于学习“如何根据指令修改代码”的过程，而不仅仅是生成“通过测试”的代码。因此，严格的单元测试验证可能并非必须。\n    *   *推论*：如果我们只需要检查生成的代码补丁与参考补丁的相似度（如行级别的重叠），就能保证数据质量，那么就可以完全抛弃测试基础设施。\n*   **假设二（关于数据多样性）**：现实世界的代码变更不仅仅是修复Bug，还包括重构、文档更新等。现有的方法过于关注“Bug修复”。\n    *   *推论*：使用“模糊的指令”让模型自由发挥，可以生成更多样化的数据，这些数据虽然可能不通过严格的测试，但包含了丰富的代码编辑技能。\n\n#### 4. 方法论构建：软验证生成（SVG）\n*   **设计思路**：基于上述假设，设计一个两阶段的生成流程，旨在通过“自我一致性”来保证质量，而非外部测试。\n    *   **阶段一（创造）**：让教师模型基于一个随机函数和模糊的Bug描述生成一个修改（Patch P1）。\n    *   **阶段二（复现）**：将阶段一的轨迹转化为一个合成PR描述，让教师模型仅凭这个描述去复现修改（Patch P2）。\n*   **核心机制**：比较 P1 和 P2 的行级重叠率。\n    *   如果重叠率高，说明模型理解了PR描述并能稳定地复现修改，这本身就是一种高质量的“软验证”。\n    *   这消除了对单元测试的依赖，使得从任何代码库（包括私有库）生成数据成为可能。\n\n#### 5. 验证与发现：专业化的高效性\n*   **实验验证**：作者发现，使用这种“软验证”生成的数据训练出的模型，在特定代码库（如Django）上的表现能够匹配甚至超过教师模型。\n*   **逻辑闭环**：这证明了直觉是正确的——将仓库特定知识编码进学生模型的权重中，确实比教师模型仅通过上下文窗口访问代码库更有效。而且，由于不需要测试环境，这个过程极其便宜。\n\n#### 6. 最终产出：SERA\n*   **总结**：SERA 不仅仅是一个模型，更是一套证明了“低成本私有代码库专业化”可行性的方法论。它通过去除测试依赖和利用模糊指令，将训练成本降低了数十倍，使得小团队也能训练出强大的专属代码智能体。"
                },
                {
                    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
                    "arxiv_id": "2601.20465",
                    "authors": "Yang Li, Jiaxiang Liu, Yusong Wang, Yujie Wu, Mingkun Xu",
                    "summary": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心判断（符合）**： *   论文的核心贡献是提出了 **BMAM (Brain-inspired Multi-Agent Memory)**，这是一种通用的**记忆架构**，旨在解决基于语言模型的智能体在长交互周期中面临的信息保持和行为一致性问题（即“灵魂侵蚀”）。 *   这属于**构建和改进 LLM 智能体**的范畴，特别是针对智能体架构中的核心组件——**记忆**进行了创新性的设计，而非仅仅将现有智能体作为工具应用到特定领域。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文明确涉及 `LLM-based Agents` 和 `Multi-Agent Systems`（标题中即包含 Multi-Agent）。 *   **智能体能力**：论文的核心焦点是 **`Memory`**（记忆），这是 Agentic AI 的关键能力之一。它通过将记忆分解为情景、语义等子系统，并沿显式时间轴组织，直接增强了智能体的 **`Planning`**（规划）和长周期推理能力。 3.  **排除标准（无冲突）**： *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊与模糊情况处理**： *   虽然论文提到了“推理”，但它不是单纯通过改进 CoT 提示词来提升模型的基础逻辑能力，而是通过**改进智能体的记忆架构**来支持长周期的推理。这符合“保留”关于智能体如何进行规划或在复杂任务中进行多步推理的研究。 综上所述，该论文通过提出新的记忆框架来改进 LLM 智能体的架构和能力，属于 Agentic AI 的核心研究范畴，因此予以保留。",
                    "summary2": "本文旨在解决长时交互中智能体面临的“灵魂侵蚀”问题。针对长时交互场景，我们提出了一种BMAM框架，该框架受认知科学启发，将记忆分解为情景、语义、显著性等专门子系统，并采用时间线索索引和混合检索机制。我们在LoCoMo等基准上通过准确率验证了其有效性，达到78.45%。",
                    "summary_translation": "在扩展的交互跨度中运行的基于语言模型的智能体，面临着在保持时间锚定信息和跨会话维持行为一致性方面的持续挑战，我们将这种失败模式称为“灵魂侵蚀”。我们提出了 BMAM (Brain-inspired Multi-Agent Memory，受大脑启发的多智能体记忆)，这是一种通用记忆架构，它将智能体记忆建模为一组功能专门化的子系统，而非单一的非结构化存储。受认知记忆系统的启发，BMAM 将记忆分解为情景、语义、显著性感知和控制导向的组件，这些组件在互补的时间尺度上运行。为了支持长期推理，BMAM 沿着显式时间线组织情景记忆，并通过融合多种互补信号来检索证据。在 LoCoMo 基准上的实验表明，BMAM 在标准长期评估设置下达到了 78.45% 的准确率，且消融分析证实，受海马体启发的情景记忆子系统在时间推理中发挥着关键作用。",
                    "inspiration_trace": "基于对论文《BMAM: Brain-inspired Multi-Agent Memory Framework》的深度分析，以下是作者产出该核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观背景与观察：长视距智能体的困境\n**思考起点：**\n作者首先观察到基于大语言模型（LLM）的智能体正越来越多地应用于需要跨越长时间跨度、多任务和多领域的复杂交互场景中。\n**核心矛盾：**\n这些智能体面临一个根本性的挑战：它们需要像人类一样保留过去的经验、组织记忆结构并在不同目标下检索信息，但现有的技术基础无法支撑这一点。\n\n### 2. 问题引入逻辑：从“现有方案”到“本质缺陷”\n*基于Introduction部分的“讲故事”逻辑还原：*\n\n1.  **现状需求：** 智能体必须在扩展的交互过程中维护和推理累积的信息。\n2.  **第一层局限（模型本身）：** LLM受限于有限的上下文窗口，且缺乏管理当前输入之外的长期记忆的显式机制。\n3.  **第二层局限（现有补救方案）：** 检索增强生成（RAG）虽然缓解了部分问题，但它将记忆视为“外部的静态文本仓库”，而非“内部演化的系统”。\n4.  **后果分析：** 这种RAG式的思维导致系统无法支持持久的记忆积累、时间组织以及跨会话推理。\n5.  **关键洞察（认知科学视角）：** 认知科学证据表明，人类的记忆并非单一的巨型存储库，而是由多个功能专门的子系统组成的（如快速情景编码与慢速语义巩固并存）。\n6.  **结论：** 因此，我们需要一个通用的记忆框架，它不应是任务特定的检索管道，而应是一个受大脑启发的、能够支持长视距智能体行为的架构。\n\n### 3. 研究问题\n基于上述逻辑链，作者试图回答的核心研究问题是：\n\n**“如何设计一个通用的记忆框架，使其能够通过模仿生物记忆中功能专门化的子系统（而非单一的非结构化存储），来支持长视距智能体在持久交互中的时间连贯性和行为一致性？”**\n\n---\n\n### 4. 深度诊断：定义“灵魂侵蚀”\n**思考演进：**\n为了更精准地定位问题，作者没有停留在泛泛的“记忆不好”，而是对智能体在长期交互中的失败模式进行了病理学式的诊断。\n\n*   **现象观察：** 智能体在长期交互中，记忆碎片化或错位会导致时间连贯性和身份相关行为的退化。\n*   **概念提出：** 作者将这种失败模式定义为**“灵魂侵蚀”**。即智能体的“灵魂”（一致的偏好、行为倾向、交互模式）随着记忆管理的混乱而逐渐消逝。\n*   **维度拆解（假设）：** 作者假设这种侵蚀可以分解为三个正交的维度，且每个维度需要不同的对策：\n    1.  **时间侵蚀：** 丢失事件发生的时间顺序（需要显式的时间组织）。\n    2.  **语义侵蚀：** 事实和关系随时间退化或矛盾（需要记忆巩固与一致性维护）。\n    3.  **身份侵蚀：** 用户偏好和特质被新信息覆盖（需要显著性标记与保护）。\n\n### 5. 假设形成：多智能体协同防御\n**逻辑推演：**\n既然“灵魂侵蚀”有三种不同的成因，那么单一的机制（如单纯的向量检索）无法同时解决这些问题。\n*   **跨学科灵感：** 认知神经科学表明，人类大脑通过海马体（情景编码）、新皮层（语义巩固）、杏仁核（情感显著性）和前额叶（执行控制）的协同工作来维持记忆。\n*   **核心假设：** 如果我们将智能体的记忆分解为多个交互的、功能专门的子系统，每个子系统针对一种特定的“侵蚀”类型进行防御，就能构建出比单一存储更鲁棒的长期记忆。\n\n### 6. 方法论构建：BMAM架构的诞生\n**从假设到设计：**\n基于上述假设，作者开始构建BMAM框架，将抽象的脑区功能映射为具体的计算模块：\n\n1.  **架构设计原则：** 采用“协调者中心”的多智能体架构，将长期记忆分解为功能组件，同时保持统一的记忆底座。\n2.  **针对“时间侵蚀”的解法（海马体）：**\n    *   *设计：* 引入**时间线索索引**。\n    *   *逻辑：* 必须显式记录时间戳和事件顺序，才能回答“何时”、“之前/之后”的问题。\n3.  **针对“语义侵蚀”的解法（颞叶）：**\n    *   *设计：* 引入**互补学习系统**。\n    *   *逻辑：* 将高频访问的情景记忆选择性巩固为稳定的语义记忆（知识图谱），防止事实随时间模糊或矛盾。\n4.  **针对“身份侵蚀”的解法（杏仁核）：**\n    *   *设计：* 引入**显著性感知机制**。\n    *   *逻辑：* 根据新颖性、冲突或用户反馈计算重要性分数，优先保护与身份相关的信息，防止其被日常琐事淹没。\n5.  **针对“控制与检索”的解法（前额叶）：**\n    *   *设计：* 引入**分层记忆控制**和**混合检索**。\n    *   *逻辑：* 需要一个“管理者”来根据查询类型（时间、事实、偏好）动态路由请求，并融合词汇、密集、关系和时间等多种信号，以适应不同场景。\n\n### 7. 总结：思想演进脉络\n作者的思考过程是一个典型的**“现象-问题-诊断-类比-设计”**链条：\n从观察到智能体在长期交互中“变傻/变脸”（现象），指出RAG和LLM的局限性（问题），将其抽象为“灵魂侵蚀”这一病理概念（诊断），借鉴人脑多区域协同的生物学原理（类比），最终设计出由海马体、颞叶、杏仁核等模块组成的BMAM多智能体记忆框架（设计）。"
                },
                {
                    "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use",
                    "arxiv_id": "2601.20439",
                    "authors": "Qihao Wang, Mingzhe Lu, Jiayue Wu, Yue Hu, Yanbing Liu",
                    "summary": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“单智能体”方向的核心研究。 1.  **核心判断 (第一步)**: 论文的核心贡献是提出了 PEARL 这一新颖框架，旨在增强 LLM 智能体在复杂任务中的规划和执行能力。这属于构建和改进 LLM 智能体的方法论，而非简单的应用或基础设施研究，因此符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确致力于 `LLM-based Agents` 的开发。 *   **智能体能力**: 论文重点解决了 `Planning`（规划）和 `Tool Use`（工具使用）这两个关键能力，特别是针对多轮工具调用和复杂规划场景。 *   **演化机制**: 论文采用了离线探索和在线强化学习（GRPO）的两阶段方法，通过奖励函数提供反馈信号来训练规划器。这种通过环境反馈和迭代训练来提升性能的过程，符合“自我完善”和“迭代改进”的演化特征。 3.  **特殊情况处理 (第四步)**: *   论文关注的是智能体如何进行规划以及在复杂任务中进行多步推理（涉及工具调用），这完全符合关于“智能体规划”的保留规则，而非单纯提升模型基础推理能力的非Agentic研究。 综上所述，PEARL 论文通过引入强化学习和探索机制，显著提升了智能体的规划和工具使用能力，是对 LLM 智能体构建与改进的直接贡献，符合研究课题要求。",
                    "summary2": "本文旨在解决LLM在复杂多跳工具使用中面临的规划能力弱和执行鲁棒性差的问题。针对多步工具调用的场景，我们提出了一种名为PEARL的两阶段框架，结合离线工具探索和基于GRPO的在线强化学习Planner。我们在ToolHop和T-Eval基准上通过Success Rate (SR) 和 Invocation Error Rate (IER) 验证了其有效性，显著优于现有方法。",
                    "summary_translation": "大语言模型在使用外部工具方面展现出巨大潜力，但在复杂的多轮工具调用中仍面临重大挑战。它们常表现出规划能力薄弱、工具幻觉、参数生成错误等问题，且难以实现鲁棒的交互。为解决这些问题，我们提出了 PEARL，一种新颖的框架，旨在增强大语言模型在复杂工具使用中的规划与执行能力。PEARL 采用两阶段方法：离线阶段，智能体探索工具以学习有效的使用模式和失败条件；以及在线强化学习阶段。在在线阶段，通过精心设计的奖励函数为规划质量提供明确信号，利用组相对策略优化训练一个专门的规划器。在 ToolHop 和 T-Eval 基准测试上的实验表明，PEARL 显著优于现有方法，在 ToolHop 上实现了 **56.5%** 的最新最先进成功率，同时保持了较低的调用错误率。我们的工作标志着在解决工具使用的复杂规划挑战方面取得了关键进展，有助于开发更鲁棒、更可靠的基于大语言模型的智能体。",
                    "inspiration_trace": "基于论文《PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其产出该文章的思考过程。\n\n---\n\n### 一、 Introduction 中的“讲故事”逻辑（问题引入）\n\n作者在引言部分通过层层递进的方式，构建了研究的背景与动机，具体逻辑如下：\n\n1.  **背景铺垫（潜力与局限）：** 大语言模型（LLMs）虽然能力强大，但其内在知识是静态的，限制了其与真实世界的交互。通过外部工具（API、数据库等）进行增强，即“工具学习”，被视为打破这一局限的关键方案。\n2.  **趋势演进（从简单到复杂）：** 早期的成功（如 Toolformer）主要集中在简单的单步工具调用。随着对智能体期望的提高，研究重点迅速转向需要**多步、多工具协同**的复杂问题解决。\n3.  **核心冲突（瓶颈暴露）：** 这种复杂度的跃升暴露了一个关键瓶颈：智能体在**连贯的长视界规划**和**鲁棒执行**方面的能力不足。\n4.  **具体症状（现有缺陷）：** 现有方法往往采取短视的“一步一策”策略，缺乏前瞻性。这导致了以下具体问题：\n    *   **规划薄弱：** 无法制定或坚持长期计划。\n    *   **执行脆弱：** 容易产生工具幻觉（调用不存在的工具）、生成错误的参数，且无法管理连续调用间的依赖关系。\n    *   **适应性差：** 面对不可避免的执行错误时，无法诊断失败、反思计划并动态调整，容易陷入重复失败的循环。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题为：\n\n**“如何构建一个 LLM 智能体框架，使其能够通过离线探索掌握工具的可靠执行模式，并通过在线强化学习具备自适应的长视界战略规划能力，从而解决复杂多跳工具使用中的幻觉、参数错误及规划短视问题？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n为了解决上述问题，作者的思考过程经历了从问题拆解、分治求解到系统整合的演进：\n\n#### 1. 问题拆解：将“工具使用”二元化\n作者首先意识到，复杂工具使用的失败往往源于两个不同层面的能力缺失混合在一起：\n*   **微观层面：** 不知道工具的具体用法、参数限制和失败模式（执行层面的知识匮乏）。\n*   **宏观层面：** 不知道如何将多个工具串联起来解决复杂问题（规划层面的策略缺失）。\n*   **思考结论：** 必须将“规划”与“执行”解耦，分别优化。\n\n#### 2. 解决“执行脆弱”：引入离线探索\n*   **观察：** 现有模型在在线执行时经常因为参数错误或工具幻觉而失败，且这种错误会连锁反应导致整个任务崩溃。\n*   **假设：** 如果在正式任务开始前，让模型先在一个安全的环境中“试错”，它就能学会工具的正确用法。\n*   **方法论形成：** 提出**离线工具探索**阶段。智能体通过试错主动学习每个工具的有效调用模式和失败条件，构建一个“学习到的用户手册”。这为后续的在线执行提供了鲁棒性基础，大幅降低了调用错误率（IER）。\n\n#### 3. 解决“规划短视”：引入强化学习与特定奖励\n*   **观察：** 传统的监督微调（SFT）只是模仿成功的轨迹，缺乏对策略好坏的深层理解，且容易在未见过的场景中失效。单纯的思维链（CoT）提示又缺乏优化机制。\n*   **假设：** 规划本质上是一个序列决策问题，需要通过试错和反馈来优化策略，而不仅仅是模仿。强化学习（RL）适合此类任务，但长序列的信用分配很难。\n*   **方法论形成：**\n    *   **架构：** 设计一个专门的**规划器**，负责在执行前生成完整的多步计划。\n    *   **优化目标：** 设计一个**以规划为中心的奖励函数**。不同于仅在任务结束时给奖励（稀疏奖励），该奖励对每一步的工具选择进行评估（密集反馈），直接指导模型学习正确的工具链结构。\n    *   **算法选择：** 采用 **GRPO (Group Relative Policy Optimization)**。这是对 PPO 的改进，通过组内归一化优势估计来减少训练方差，特别适合 LLM 的微调，使模型能更稳定地学习哪种规划更好。\n\n#### 4. 系统整合：PEARL 框架的诞生\n*   **综合：** 将上述两个模块结合，形成两阶段框架。\n    *   **阶段一（战略规划）：** 经过 GRPO 优化的 Planner 接收查询，输出高层计划。\n    *   **阶段二（知性执行）：** 经过离线探索训练的 Executor 接收计划，利用其掌握的工具知识逐步执行。\n*   **验证逻辑：** 作者推测，这种解耦架构不仅能提高整体成功率，而且 Planner 学到的规划策略应当具有通用性（即可以指导其他模型执行），从而证明其学到了真正的“战略”而非死记硬背。\n\n---\n\n### 总结\n作者的思考路径是从**现象（多跳工具调用失败）**出发，诊断出**根本原因（规划与执行的双重缺陷）**，进而提出**分治策略（解耦架构）**：用**离线探索**解决执行层面的“无知”，用**基于特定奖励的强化学习**解决规划层面的“短视”。最终通过 PEARL 框架将二者融合，实现了在复杂工具使用任务上的性能突破。"
                },
                {
                    "title": "Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents",
                    "arxiv_id": "2601.20144",
                    "authors": "Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Jiri Gesi, Xianfeng Tang, Chen Luo, Yisi Sang, Hanqing Lu, Manling Li, Dakuo Wang",
                    "summary": "Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“Agentic AI”中的“单智能体”方向，具体聚焦于“工具使用”能力的改进。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 `Trajectory2Task`，这是一个用于训练和改进 `Tool-Calling Agents`（工具调用智能体）的数据生成管道和框架。 *   论文的研究对象是智能体本身，旨在解决智能体在处理复杂用户意图（模糊、变化、不可行）时的鲁棒性问题，而不是将智能体作为工具应用到某个垂直领域（如医疗、金融）。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确研究 `LLM-based Agents`。 *   **智能体能力**: 论文的核心焦点是 `Tool Use / Tool Augmentation`（工具使用/工具增强）。它通过合成数据来训练智能体，使其在复杂交互中更有效地调用工具。 *   **改进机制**: 论文通过微调（Fine-tuning）利用生成的轨迹数据来改进智能体的性能，这符合“构建或改进 LLM智能体”的目标。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全对齐、多模态视觉或图技术，因此不触及相关排除规则。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文涉及数据生成，但其最终目的是为了 `Training Robust Tool-Calling Agents`，即提升智能体的 Agentic 能力（在复杂场景下的多轮交互和工具调用），而非单纯的数据集构建。这属于提升智能体在复杂任务中的表现，符合 Agentic AI 的研究范畴。 综上所述，该论文致力于改进 LLM 智能体的工具使用能力，属于构建和改进智能体的核心研究，因此予以保留。",
                    "summary2": "本文旨在解决现实世界中 Tool-calling agents 难以处理模糊、变化或不可行用户意图的问题。针对复杂用户意图场景，我们提出了一种名为 Trajectory2Task 的可验证数据生成流水线，通过轨迹探索和任务转换合成数据。我们在 Retail-3I 数据集上通过 Pass@k 指标验证了其有效性，实验表明该方法能显著提升模型鲁棒性并泛化到未见领域。",
                    "summary_translation": "Tool-calling agents (工具调用智能体) 正越来越多地被部署于现实世界的面向客户的工作流中。然而，大多数关于 Tool-calling agents (工具调用智能体) 的研究集中于理想化环境，涉及通用、固定且明确指定的任务。在现实应用中，用户请求通常表现为：(1) 意图模糊，(2) 意图随时间变化，或 (3) 受策略约束而无法实现；然而，涵盖这些多样化且复杂的交互模式的训练与评估数据仍然十分匮乏。为弥合这一差距，我们提出了 Trajectory2Task，这是一个可验证的数据生成流水线，旨在三种真实用户场景下大规模研究工具使用行为：模糊意图、变化意图和不可行意图。该流水线首先进行多轮探索以生成有效的 tool-call trajectories (工具调用轨迹)。随后，它将这些轨迹转换为面向用户的任务，并对意图进行受控调整。这一过程生成了支持闭环评估与训练的可验证任务。我们在生成的复杂用户场景任务上对七个最先进的 LLMs (大语言模型) 进行了基准测试，发现它们经常出现失败。最后，利用从任务推演中获得的成功轨迹，我们对轻量级 LLMs (大语言模型) 进行了微调，结果显示在所有三种条件下性能均有持续提升，且在未见过的工具使用领域表现出更好的泛化能力，这表明其具备更强的通用工具调用能力。",
                    "inspiration_trace": "基于对论文《Trajectory2Task》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的完整思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑：从理想走向现实\n\n作者在Introduction中构建了一个层层递进的逻辑链条，旨在揭示现有研究范式的局限性：\n\n1.  **理想与现实的错位（现象观察）：**\n    *   **理想化设定：** 现有的Tool-calling研究大多基于“实验室环境”，假设任务是通用的、固定的且规范明确的。\n    *   **真实场景挑战：** 在实际部署（如客户服务）中，用户意图极其复杂，主要表现为三个特征：**模糊**（Ambiguous，缺少关键信息）、**变化**（Changing，意图随对话漂移）、**不可行**（Infeasible，违反策略约束）。\n\n2.  **现有评估体系的失效（问题诊断）：**\n    *   现有的基准测试（如ToolBench, BFCL）仅关注静态的最终成功率，假设目标是稳定的。\n    *   即使是较新的工作（如Tau-bench）虽然引入了多轮交互，但并未系统性地将上述三种“复杂用户意图”作为独立的压力测试因子纳入评估体系。\n\n3.  **数据瓶颈（根源分析）：**\n    *   为什么Agent在真实场景下表现不佳？根本原因在于**数据**。\n    *   现有的训练数据缺乏包含“信息缺失”、“意图漂移”或“策略冲突”的真实多轮轨迹。\n    *   现有的“先询问后行动”模式不足以应对动态变化的目标，Agent需要具备在部分可观测性和非平稳目标下的决策能力。\n\n**总结出的研究问题：**\n\n> **如何构建一个可扩展且可验证的数据生成与训练框架，以提升工具调用Agent在模糊、变化及不可行等复杂真实用户意图场景下的鲁棒性？**\n\n---\n\n### 二、 核心方法的逻辑演进链\n\n为了回答上述研究问题，作者的思考路径经历了从“发现问题”到“提出假设”，再到“设计机制”的演进：\n\n#### 1. 思考起点：解决“数据稀缺”与“验证困难”的矛盾\n*   **困境：** 真实世界的复杂交互数据（如用户突然改主意、试图绕过规则）极难通过人工大规模标注获取。\n*   **假设：** 如果能通过合成的方式生成数据，既能覆盖复杂场景，又能保证答案的正确性（可验证性），就能解决训练和评估的痛点。\n*   **关键洞察：** 传统的合成方法是“先写任务，再让Agent做”，这容易导致任务无解或轨迹不可控。作者决定**逆向思维**。\n\n#### 2. 核心创新：从“Task to Trajectory”转向“Trajectory to Task”\n*   **逻辑反转：** 既然要保证数据可验证，不如先让一个强大的Agent在环境中自由探索，生成一条**成功的、可执行的轨迹**。\n*   **任务反推：** 有了正确的“解题步骤”（轨迹），再通过反向工程，反推出一个能导致该轨迹的“用户任务”。\n*   **优势：** 这种“由果导因”的方式天然保证了每个任务都有对应的黄金标准轨迹，实现了**闭环验证**。\n\n#### 3. 场景注入：如何让“合成”数据具备“真实”的复杂性？\n*   **思考：** 仅仅反推出的任务可能过于简单和静态，如何引入引言中提到的三种复杂意图？\n*   **机制设计：** 在“任务反推”阶段，通过Prompt工程强制模型对任务进行**意图改写**：\n    *   **模糊：** 隐藏关键信息，要求Agent必须询问。\n    *   **变化：** 在任务描述中植入意图漂移或分支，要求Agent必须动态调整计划。\n    *   **不可行：** 植入违反策略的请求，要求Agent必须拒绝并遵守约束。\n\n#### 4. 训练范式：从“模仿静态指令”到“学习动态决策”\n*   **思考：** 有了数据，如何训练模型才能让它真正学会处理这些复杂情况？\n*   **方法论：** 采用基于轨迹的监督微调。\n*   **核心逻辑：** 不仅仅是让模型学会调用工具，而是通过展示完整的推理过程和行动序列，教会模型**“何时该问”、“何时该改”、“何时该拒”**。这是一种在非静态环境下的决策行为训练。\n\n#### 5. 验证闭环：证明泛化能力\n*   **思考：** 如何证明模型不是死记硬背，而是学到了通用的工具调用能力？\n*   **实验设计：** 在零售领域训练，但在航空领域测试。如果模型在未见过的领域也能提升，说明它学到的是通用的交互策略，而非特定领域的API记忆。\n\n---\n\n### 三、 逻辑链总结图示\n\n1.  **观察：** 现实世界用户意图复杂（模糊/变化/不可行），现有Agent因缺乏此类训练数据而表现脆弱。\n    ↓\n2.  **假设：** 需要一种既能大规模合成，又能保证答案正确（可验证）的数据生成方法。\n    ↓\n3.  **逆向突破：** 提出 **Trajectory2Task** 范式——先探索生成有效轨迹，再反推任务，确保可验证性。\n    ↓\n4.  **复杂度控制：** 在任务生成阶段，通过Prompt注入三种特定的复杂用户意图模式。\n    ↓\n5.  **能力内化：** 利用生成的轨迹进行SFT训练，使模型习得动态环境下的决策策略。\n    ↓\n6.  **价值验证：** 证明该方法不仅能提升特定场景表现，还能跨领域泛化，解决了原始的研究问题。"
                },
                {
                    "title": "Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction",
                    "arxiv_id": "2601.20162",
                    "authors": "Shuoxin Wang, Chang Liu, Gowen Loo, Lifan Zheng, Kaiwen Wei, Xinyi Zeng, Jingyuan Zhang, Yu Tian",
                    "summary": "Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“单智能体”方向的研究，核心贡献在于构建和改进LLM智能体的架构。 1.  **核心判断 (第一步)**: 论文的核心是提出 **Me-Agent**，这是一个新的LLM智能体框架。它不仅仅是将现有智能体应用到移动端，而是针对现有智能体“忽视个性化需求”和“缺乏从历史中学习”的局限性，提出了具体的架构改进（两级用户习惯学习）。这符合“构建、改进LLM智能体”的核心目标，不属于简单的非演化型应用。 2.  **正面指标 (第二步)**: *   **核心范式**: 论文明确属于 `LLM-based Agents`。 *   **智能体能力**: 论文重点解决了智能体的 **`Memory`**（记忆）和 **`Self-Correction/Refine`**（自我完善/学习）能力。具体而言，它设计了“分层偏好记忆”来存储长期和应用特定记忆，并利用“个人奖励模型”来优化提示级的学习。 *   **演化机制**: 智能体通过“用户习惯学习”从交互历史中迭代改进自身对用户指令的理解和执行，这符合通过经验进行自我完善的逻辑。 3.  **排除标准 (第三步)**: 论文的主要贡献不在于安全、对齐、多模态视觉处理或图技术，而是聚焦于智能体的个性化交互机制，因此不触及相关排除项。 综上所述，该论文通过引入新的记忆结构和学习策略来增强LLM智能体的个性化能力，是对Agentic AI架构的有效改进，符合研究课题要求。",
                    "summary2": "本文旨在解决现有移动代理无法理解模糊指令及缺乏个性化能力的问题。针对移动设备上的用户交互场景，我们提出了一种名为Me-Agent的个性化移动代理，采用两级用户习惯学习机制：在提示层面利用Personal Reward Model进行User Preference Learning，在记忆层面构建Hierarchical Preference Memory。我们在User FingerTip和E-dataset上通过ASA、BERTScore、TCR等指标验证了其有效性，结果显示Me-Agent在个性化方面达到SOTA，且任务完成率显著提升。",
                    "summary_translation": "基于 Large Language Model (LLM) 的移动智能体已取得了显著的性能进展。然而，这些智能体通常仅遵循显式的用户指令，而忽视了个性化需求，这给真实用户带来了显著限制，尤其是在缺乏个性化上下文的情况下：(1) 无法解读模糊指令；(2) 缺乏从用户交互历史中学习的能力；(3) 无法处理个性化指令。为缓解上述挑战，我们提出了 Me-Agent，这是一种可学习且具备记忆能力的个性化移动智能体。具体而言，Me-Agent 采用了一种两级用户习惯学习方法。在提示层，我们设计了一种结合 Personal Reward Model（个人奖励模型）增强的用户偏好学习策略，以提升个性化性能。在记忆层，我们设计了一种 Hierarchical Preference Memory（分层偏好记忆），用于在不同层级的记忆中存储用户的长期记忆和应用特定记忆。为验证移动智能体的个性化能力，我们引入了 User FingerTip，这是一个包含大量日常生活模糊指令的新基准。在 User FingerTip 和通用基准上的广泛实验表明，Me-Agent 在实现个性化方面达到了最先进的性能，同时保持了具有竞争力的指令执行性能。",
                    "inspiration_trace": "基于论文《Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个从“技术进步”到“现实落差”再到“现有方案失效”的叙事链条：\n\n1.  **技术背景与现状**：\n    随着大语言模型（LLM）的发展，基于LLM的移动智能体在性能和泛化能力上取得了显著进步，能够执行跨各种应用场景的复杂任务。\n\n2.  **核心矛盾（现实落差）**：\n    尽管能力强大，但这些智能体的学习范式仍然主要依赖于“显式指令”。它们缺乏对用户潜在偏好和个性化需求的系统性建模和长期适应。当用户背景信息缺失或应用场景复杂动态时，这种“机械执行”的模式暴露了巨大的局限性。\n\n3.  **具体痛点（三大局限）**：\n    *   **无法解读模糊指令**：难以准确解释包含模糊或隐含意图的自然语言指令（如“播放我喜欢的歌”）。\n    *   **缺乏历史学习能力**：无法在多轮交互中持续学习和更新用户的行为模式。\n    *   **个性化处理失败**：处理个性化指令和偏好配置的能力有限，阻碍了以用户为中心的交互体验。\n\n4.  **现有方案的困境（技术瓶颈）**：\n    作者指出，虽然LLM个性化已有研究，但直接迁移到移动端面临严重阻碍：\n    *   **微调不可行**：参数级微调方法受限于移动设备的计算资源和存储能力；云端训练则带来额外成本及隐私安全问题。\n    *   **提示词膨胀病**：通过不断扩展提示词来注入用户上下文的方法，缺乏过滤机制，导致随着交互积累，推理效率下降，存储和版本管理变得极其复杂。\n\n---\n\n### 二、 显式总结：研究问题\n\n基于上述叙事，作者试图回答的核心研究问题是：\n\n**“如何在不进行模型参数微调且不导致上下文溢出的前提下，使移动智能体具备理解模糊指令并持续学习用户个性化习惯的能力？”**\n\n---\n\n### 三、 核心方法的逻辑演进链（思想推演）\n\n#### 1. 宏观观察与问题聚焦\n*   **观察**：现有的移动Agent像是一个“听话的陌生人”，能干活但不懂你。用户在真实场景中说话往往很随意（模糊指代），且希望Agent越用越懂（习惯记忆）。\n*   **聚焦**：问题的本质不在于Agent的感知或规划能力（这些已有基础），而在于**个性化**与**资源约束**之间的矛盾。\n\n#### 2. 约束分析与假设提出\n*   **约束分析**：\n    *   不能改模型参数（算力/隐私限制） $\\rightarrow$ 必须是 **Training-free（免训练）**。\n    *   不能无限塞历史记录（上下文窗口限制） $\\rightarrow$ 必须有 **高效的记忆管理机制**。\n*   **核心假设**：如果将优化空间从“模型参数空间”转移到“上下文空间”，通过外部记忆和奖励反馈来引导模型，是否就能在不动模型的情况下实现个性化？\n\n#### 3. 方法论架构设计（两级策略）\n为了解决上述矛盾，作者提出了一个“双层级”的解耦思路，将个性化学习分为“即时偏好引导”和“长期知识存储”：\n\n*   **第一层：Prompt Level（即时引导层）**\n    *   *思考*：如何让Agent在当前任务中立刻表现出对用户的偏好？\n    *   *方案*：引入**用户偏好学习（UPL）**。既然不能训练模型，那就用“试错+反馈”的机制。让Agent生成多个执行轨迹，通过一个**个人奖励模型（PRM）**来打分，筛选出最符合用户习惯的路径，并将其转化为经验注入Prompt。\n    *   *逻辑*：用“搜索-评估-优化”的闭环替代“梯度下降”。\n\n*   **第二层：Memory Level（长期存储层）**\n    *   *思考*：Prompt只能放一点点东西，用户海量的App使用习惯怎么办？如果全塞进去会“上下文溢出”。\n    *   *方案*：设计**分层偏好记忆（HPM）**。模仿人类的记忆方式，将记忆分为“通用习惯”和“特定App知识”。\n    *   *逻辑*：\n        *   **L1记忆（长期）**：存储用户在不同功能类别（如音乐、购物）下的通用偏好（如“用户偏爱QQ音乐”）。\n        *   **L2记忆（App特定）**：存储特定App的操作流和UI细节（如“搜索按钮在右上角”）。\n    *   *关键点*：按需检索。只有当Agent打开某个App时，才去调取该App的L2记忆，从而避免上下文拥堵。\n\n#### 4. 机制落地与闭环优化\n*   **执行逻辑**：\n    1.  **接收指令**：用户说“播放我喜欢的歌”。\n    2.  **应用解析**：利用L1记忆，推断出用户习惯用“QQ音乐”。\n    3.  **内容检索**：利用L2记忆，检索出用户常听的歌单。\n    4.  **执行与反馈**：Agent执行操作，UPL模块通过VLM观察屏幕结果，给出奖励。\n    5.  **记忆更新**：根据执行结果，动态更新HPM中的记忆（如新增偏好、修正操作路径）。\n\n#### 5. 验证与评估\n*   **思考**：怎么证明这个Agent真的懂“个性化”？\n*   **方案**：构建**User FingerTip**数据集。不同于传统数据集指令明确，这里专门设计了“模糊指令”（Type I: 缺App名，Type II: 缺具体内容），强迫Agent必须利用学到的习惯来“猜”用户的意图。\n\n---\n\n### 总结：思想演进脉络\n\n**从“机械执行”到“懂你所想”** $\\rightarrow$ 发现**算力与隐私**限制了传统的微调路线 $\\rightarrow$ 转向**免训练**的上下文优化 $\\rightarrow$ 为了解决**上下文长度**限制，设计了**分层记忆架构** $\\rightarrow$ 通过**奖励模型**驱动经验积累，最终实现一个既懂个性化又轻量级的移动Agent。"
                },
                {
                    "title": "Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents",
                    "arxiv_id": "2601.19935",
                    "authors": "Yiting Shen, Kun Li, Wei Zhou, Songlin Hu",
                    "summary": "Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \\textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**：这篇论文的核心贡献是提出了 Mem2ActBench，这是一个专门用于评估 LLM 智能体在基于工具的任务中主动利用长期记忆能力的基准。这完全符合“构建、改进或演化 LLM 智能体”的研究目标，因为它直接服务于评估和提升智能体的核心能力。 2.  **符合研究焦点**：论文明确属于“单智能体”方向，重点涉及了筛选标准中列出的关键子方向： *   **记忆**：论文核心关注长期记忆的利用，区分了被动检索与主动应用。 *   **工具使用**：评估智能体如何利用记忆来选择合适的工具并接地参数。 *   **Agentic AI**：关注智能体在复杂、多轮交互中的自主行动能力。 3.  **排除标准检查**： *   该论文不是将智能体作为工具应用到特定垂直领域（如生物、金融），而是针对智能体本身的通用能力进行评估，因此不属于“非演化型应用”。 *   不涉及安全、对齐、多模态视觉或基础设施等排除主题。 4.  **结论**：该论文通过提供新的评估基准，填补了智能体在“记忆驱动行动”这一关键能力上的评估空白，对于改进 LLM 智能体的记忆机制和工具使用能力具有重要价值，因此符合筛选要求。",
                    "summary2": "本文旨在评估任务导向智能体主动利用长期记忆执行工具调用任务的能力。针对包含大量中断和隐式约束的长对话场景，我们提出了Mem2ActBench基准，通过自动化流水线构建记忆演化链并反向生成依赖记忆的任务。我们在该基准上通过F1、BLEU和Tool Accuracy等指标评估了七种记忆框架，验证了现有系统在参数填充方面仍存在显著不足。",
                    "summary_translation": "基于 Large Language Model (LLM) (大型语言模型) 的 agents (智能体) 正日益被部署用于复杂的基于工具的任务，在此类任务中，long-term memory (长期记忆) 对于驱动行为至关重要。然而，现有的 benchmarks (基准测试) 主要测试 agents (智能体) 被动检索孤立事实以响应明确问题的能力。它们未能评估更为关键的能力，即主动应用记忆来执行任务。为了解决这一差距，我们介绍了 \\textsc{Mem2ActBench}，这是一个用于评估 agents (智能体) 是否能够通过选择合适的工具并对参数进行 grounding (参数 grounding/对齐) 来主动利用 long-term memory (长期记忆) 执行基于工具的行为的 benchmarks (基准测试)。该 benchmarks (基准测试) 模拟了持久化的助手使用场景，即用户在跨越长时间且中断的交互中提及同一话题，并期望先前建立的偏好和任务状态能够被隐式应用。我们利用自动化流水线构建了该数据集，该流水线合并了异构源，并通过一致性建模解决冲突，最终合成了 2,029 个会话，平均每个会话包含 12 个用户-助手-工具轮次。基于这些 memory chains (记忆链)，一种 reverse-generation method (逆向生成方法) 生成了 400 个工具使用任务，人工评估确认其中 91.3% 的任务强烈依赖于记忆。针对七个 memory frameworks (记忆框架) 的实验表明，当前系统在主动利用记忆进行 parameter grounding (参数 grounding) 方面仍然表现不足，这凸显了采用更有效的方法来评估和改进任务执行中记忆应用的必要性。",
                    "inspiration_trace": "基于对论文《Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents》的深入分析，以下是作者产出该文章的系统性逻辑链推演。\n\n---\n\n### 一、 宏观观察与问题引入\n\n作者首先构建了一个理想与现实交织的“故事”场景，以此引出研究动机：\n\n1.  **理想场景（现实需求）**：\n    *   LLM智能体正日益成为“持久助手”，与用户进行长期的、跨越多个会话的交互。\n    *   在真实场景中，用户不会每次都重复所有的任务约束。相反，用户的偏好、需求和部分任务状态是在先前的交互中逐渐建立的，且往往被无关的对话打断。\n    *   **核心预期**：一个现实的助手被期望不仅能存储长期记忆，更能**主动检索并应用**这些过往信息来执行具体的行动（例如，在工具调用中填充缺失的参数）。\n\n2.  **现实落差（现有局限）**：\n    *   当前的记忆基准测试主要评估智能体基于**显式问题**被动检索孤立事实的能力（例如：“用户的预算是多少？”）。\n    *   这种“问题-检索-答案”的范式，低估了一个更具挑战性的现实场景：当指令**信息不足**时，智能体必须推断需要从长期记忆中检索哪些约束，并将其转化为可执行的工具调用。\n\n3.  **核心矛盾**：\n    *   现有的评估侧重于“记忆的存取性”，而忽略了“记忆的主动应用性”。\n    *   真正的挑战不在于“记住”什么，而在于在执行任务时“如何利用”记忆来填补指令的空白。\n\n---\n\n### 二、 研究问题\n\n基于上述观察，作者将研究聚焦于以下核心问题：\n\n**“智能体能否在面对信息不足的指令时，主动利用长期记忆来执行基于工具的任务，即通过选择合适的工具并基于历史记忆填充其参数？”**\n\n---\n\n### 三、 逻辑演进与思考过程\n\n从发现问题到提出解决方案，作者的思维路径经历了以下四个阶段的演进：\n\n#### 第一阶段：从“被动问答”到“主动行动”的认知转变\n*   **思考**：现有的基准（如MSC, LoComo）只是在做阅读理解式的问答。但智能体的核心价值在于“行动”。\n*   **洞察**：真正的记忆应用发生在**工具调用**的参数填充阶段。例如，用户说“帮我订下周去纽约的机票”，智能体必须主动从记忆中提取“只订直飞”和“预算500美元”这两个约束，并填入`search_flights`工具的参数中。\n*   **定位**：评估目标应从“Fact Retrieval（事实检索）”转向“Memory Driven Task（记忆驱动任务）”。\n\n#### 第二阶段：构建“真实且困难”的测试环境\n*   **思考**：要测试这种能力，不能只给几轮对话。必须模拟真实世界中“长期、被打断、碎片化”的交互历史。\n*   **策略**：\n    *   **数据来源**：单纯的任务数据太单调，单纯的闲聊又没有工具。因此，决定将**任务导向数据**（ToolACE, BFCL）与**对话噪声**（Oasst1）进行混合交织。\n    *   **目的**：制造“干扰项”，迫使智能体必须在长上下文中筛选出关键信息，模拟真实使用中的注意力分散。\n\n#### 第三阶段：建立“逻辑一致”的记忆基准\n*   **思考**：混合数据会导致冲突（例如用户先说喜欢素食，后来又说想吃鱼）。如果数据本身逻辑混乱，就无法评估智能体。我们需要一个“上帝视角”的正确记忆链作为Ground Truth。\n*   **策略**：\n    *   **事实提取与聚类**：将对话原子化为事实，并按主题聚类。\n    *   **冲突解决与演化链构建**：设计一套逻辑机制（局部冲突解决+全局拓扑排序），处理事实的时序更新和逻辑矛盾，生成一条**全局一致的“记忆演化链”**。这不仅是记忆存储，更是记忆的动态更新过程。\n\n#### 第四阶段：设计“逆向生成”的验证机制\n*   **思考**：如何确保生成的测试题**必须**依赖记忆？如果生成的题目本身包含了线索，智能体就不需要记忆了。\n*   **策略**：\n    *   **逆向工程**：不走“用户指令->工具调用”的正向路，而是走“工具调用->用户指令”的逆向路。先生成完美的、基于记忆的工具调用，再反推一个省略了关键参数的用户指令。\n    *   **防泄漏控制**：引入“判别器LLM”，如果仅凭指令和工具文档就能猜出参数，则该样本被丢弃。这确保了测试的**纯粹性**——不查记忆绝对做不对。\n\n---\n\n### 总结\n\n作者的思考过程是一个**从表象（现有基准不足）深入到本质（记忆驱动行动），再通过工程手段（数据混合、冲突解决、逆向生成）构建严谨验证体系**的过程。其核心逻辑在于：**只有通过“逆向生成”制造出的“信息缺失”，并在“长且干扰”的上下文中通过“工具调用”来填补，才能真正评估智能体的长期记忆利用能力。**"
                },
                {
                    "title": "Demystifying Multi-Agent Debate: The Role of Confidence and Diversity",
                    "arxiv_id": "2601.19921",
                    "authors": "Xiaochen Zhu, Caiqi Zhang, Yizhou Chi, Tom Stafford, Nigel Collier, Andreas Vlachos",
                    "summary": "Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心判断**：论文的核心贡献在于改进“多智能体辩论”这一框架。它不仅仅是应用现有的MAD框架，而是深入分析了其失效原因（缺乏多样性和信心校准），并提出了新的方法论（多样性感知初始化和信心调节的辩论协议）。这属于构建和改进多智能体系统的范畴，而非简单的应用或基础设施研究。 2.  **符合焦点**：论文明确涉及“Multi-Agent Systems (MAS)”。它研究了智能体之间如何通过“Communication”（辩论）进行交互，以及智能体如何根据其他智能体的反馈（信心）来更新自己的信念。这直接对应了我关注点中的“智能体间的协作、通信”。 3.  **排除标准检查**：论文不涉及安全对齐、多模态视觉或图技术。虽然它在推理导向的QA基准上测试，但其目的是验证多智能体交互机制的有效性，而非解决特定领域的垂直问题，因此不属于“非演化型应用”。 综上所述，该论文通过改进智能体间的交互协议和初始化策略来提升多智能体系统的性能，是对LLM智能体机制的重要改进，符合筛选标准。",
                    "summary2": "本文旨在解决 vanilla Multi-Agent Debate (MAD) 性能往往不及简单多数投票的问题。针对同质化智能体缺乏初始观点多样性和置信度交流的场景，我们提出了一种结合 diversity-aware initialization 和 confidence-modulated debate protocol 的方法。我们在六个推理导向的 QA 基准上通过准确率验证了其有效性，实验表明该方法显著优于 vanilla MAD 和多数投票。",
                    "summary_translation": "Multi-agent debate (MAD，多智能体辩论) 被广泛用于通过 test-time scaling（测试时扩展）来提升 large language model (LLM，大语言模型) 的性能，然而近期研究表明，尽管计算成本更高，vanilla MAD（原始多智能体辩论）的表现往往逊色于 simple majority vote（简单多数投票）。研究表明，在 homogeneous agents（同质智能体）和 uniform belief updates（统一信念更新）的条件下，辩论保持了 expected correctness（期望正确性），因此无法可靠地提升结果。借鉴 human deliberation（人类审议）和 collective decision-making（集体决策）的研究发现，我们识别出 vanilla MAD 缺失的两个关键机制： 初始观点的多样性； 明确的、calibrated confidence communication（校准的置信度交流）。我们提出了两种 lightweight interventions（轻量级干预）。首先，一种 diversity-aware initialisation（多样性感知初始化），它选择更多样化的 candidate answers（候选答案）池，从而增加在辩论开始时存在 correct hypothesis（正确假设）的可能性。其次，一种 confidence-modulated debate protocol（置信度调节辩论协议），在该协议中，智能体表达 calibrated confidence（校准的置信度），并依据他人的置信度来调整自身的更新。我们从理论上证明，diversity-aware initialisation 在不改变 underlying update dynamics（底层更新动力学）的情况下，提高了 MAD 成功的 prior probability（先验概率）；而 confidence-modulated updates（置信度调节更新）则使辩论能够系统地趋向于 correct hypothesis（正确假设）。实证结果表明，在六个 reasoning-oriented QA benchmarks（面向推理的问答基准）上，我们的方法始终优于 vanilla MAD 和 majority vote。我们的研究结果将 human deliberation 与 LLM-based debate（基于 LLM 的辩论）联系起来，并表明简单且 principled modifications（基于原则的修改）可以显著增强辩论的有效性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs",
                    "arxiv_id": "2601.20539",
                    "authors": "Oguzhan Gungordu, Siheng Xiong, Faramarz Fekri",
                    "summary": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 Agentic AI 与 Self-Evolving 范式**： 论文的核心贡献是提出了一种名为 **PathWise** 的新框架。根据摘要，这是一个 **\"multi-agent reasoning framework\" (多智能体推理框架)**，并且明确涉及 **\"Self-Evolving LLMs\" (自我演化LLMs)**。这直接对应了研究课题中的“多智能体”和“自我演化”两个核心方向。 2.  **具备完整的智能体架构与能力**： 论文详细描述了智能体系统的组件，包含了筛选标准中的关键能力： *   **多智能体协作**：系统包含 **Policy Agent**（策略智能体）、**World Model Agent**（世界模型智能体）和 **Critic Agents**（批评者智能体），体现了智能体间的分工与协作。 *   **规划与记忆**：论文强调将启发式生成表述为序列决策过程，利用 **\"entailment graph\" (蕴含图)** 作为 **\"stateful memory\" (有状态记忆)**，这符合智能体“规划”和“记忆”的核心能力要求。 *   **自我反思**：Critic Agents 提供 **\"routed reflections\" (路由反思)**，属于智能体的自我反思机制。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是组合优化问题（COPs），属于特定领域应用，但根据筛选标准第四步第2点，只要论文的核心是提出一种新的“自我演化”机制（在此为从试错演化转向状态感知规划），即使应用在特定领域，也应当保留。PathWise 改变了现有的演化规则，提出了新的演化框架，因此符合保留条件。 4.  **排除标准检查**： *   论文不涉及安全、对齐或多模态视觉等排除领域。 *   虽然提到了“图”，但它是作为智能体的内部记忆和世界模型存在，而非关于图神经网络（GNN）或知识图谱构建的基础设施研究，因此不构成排除理由。 综上所述，该论文在构建多智能体系统和自我演化机制方面具有显著的方法论贡献，高度契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在解决现有LLM自动启发式设计（AHD）中因固定规则导致的生成短视和评估冗余问题。针对组合优化问题（COPs），我们提出了一种基于蕴含图的多智能体推理框架PathWise，通过策略、世界模型和评论家智能体协作实现状态感知规划。在TSP、CVRP等基准上，通过启发式性能和收敛速度验证了其有效性。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 实现了针对组合优化问题 (Combinatorial Optimization Problems, COPs) 的自动化启发式设计 (Automated Heuristic Design, AHD)，但现有框架依赖固定的进化规则和静态提示模板，往往导致启发式生成短视、评估冗余，以及对新启发式推导方式的推理能力有限。我们提出了一种新颖的多智能体推理框架，称为“通过世界模型进行规划以实现自进化LLM的自动化启发式设计”，该框架将启发式生成构建为在蕴涵图上的序列决策过程，其中蕴涵图作为搜索轨迹的紧凑且具有状态的记忆。这种方法使系统能够继承过去的决策，并在各代生成过程中重用或避免特定的推导信息。策略智能体负责规划进化动作，世界模型智能体基于这些动作生成启发式推演，评论智能体提供路由反思以总结先前步骤的经验教训，从而将基于大语言模型的自动化启发式设计从试错式进化转变为通过推理实现的状态感知规划。在多种组合优化问题上的实验表明，PathWise 能够更快地收敛到更优的启发式，在不同的 LLM 骨干网络之间具有良好的泛化能力，并能扩展至更大规模的问题。",
                    "inspiration_trace": "基于对论文《PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs》的深度分析，以下是作者产出该文章的系统性逻辑推演与思考过程还原。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的逻辑链条，旨在引出当前研究的必要性：\n\n1.  **宏观背景与痛点**：组合优化问题（COPs）广泛存在于现实世界（如物流、调度），且多为NP-hard问题。因此，启发式算法是获取高质量解的唯一实用途径。\n2.  **传统方法的局限**：传统的启发式算法设计依赖专家的手工试错，成本高昂且难以泛化。\n3.  **自动化的演进**：为了解决上述问题，自动启发式设计（AHD）应运而生。早期基于遗传规划（GP）的方法受限于固定的语法树和人工定义的操作符，灵活性不足。\n4.  **新范式的机遇**：大语言模型（LLMs）展现出强大的推理和代码生成能力，为AHD提供了新的方向。现有的LLM-based AHD方法（如FunSearch, ReEvo, MCTS-AHD）将LLM集成到进化搜索中，虽有效但仍存在结构性缺陷。\n5.  **核心缺陷的识别**：\n    *   **基于种群的方法**：依赖固定的进化规则和静态提示，导致生成短视、评估冗余，且往往丢弃中间启发式，缺乏对“推导历史”的记忆。\n    *   **基于树的方法（如MCTS）**：虽然结构化，但选择和扩展仅基于性能统计（如UCT），缺乏对启发式之间语义关系的理解，且训练时间长。\n6.  **总结性批判**：现有方法将启发式生成视为孤立的或统计关联的采样步骤，缺乏**有状态的、语义化的表示**来记录启发式是如何推导的、编辑是如何传播的，以及修改为何成功或失败。这种记忆和状态感知规划的缺失导致了效率低下。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑链条，作者试图回答的核心研究问题是：\n\n**“如何将启发式生成从无状态的试错进化过程，转变为一种基于世界模型的有状态规划过程，从而利用语义化的推导历史来指导LLM进行更高效的启发式设计？”**\n\n---\n\n### 三、 思想演进脉络（从观察到方法论）\n\n以下是对作者思考过程的系统性还原：\n\n#### 1. 观察与反思：LLM的潜力被浪费了\n*   **观察**：现有的LLM-based AHD方法虽然使用了先进的模型，但其底层框架（进化算法、MCTS）本质上仍是传统的“黑盒搜索”。\n*   **反思**：LLM不仅仅是代码生成器，它具备强大的推理能力。但在现有框架中，LLM被当作一个单纯的“变异算子”——输入父代，输出子代。系统并不“理解”子代是如何从父代演变而来的，也不记得之前的策略为什么失败。这就像让一个天才在黑暗中盲目摸索，每次都要从头开始。\n\n#### 2. 假设提出：引入“世界模型”与“记忆”\n*   **假设**：如果我们能构建一个“世界模型”，用来记录启发式搜索的轨迹和推导逻辑，那么LLM就可以基于这个“记忆”进行**规划**，而不仅仅是**反应**。\n*   **核心概念迁移**：将强化学习中的“世界模型”概念迁移到启发式搜索中。这里的“世界”不是物理环境，而是“启发式空间”；“状态”不是具体的解，而是“启发式的推导历史”。\n\n#### 3. 概念具象化：从“进化”到“蕴含”\n*   **设计思路**：如何表示这个“记忆”？作者提出了**蕴含图**。\n    *   图中的节点不仅仅是启发式代码，还包含了“推导理由”和“父元数据”。\n    *   边代表了启发式之间的演变关系（父代 -> 子代）。\n*   **逻辑转变**：搜索不再是简单的“优胜劣汰”，而是在图结构上的“步步推理”。每一步都基于之前的路径。\n\n#### 4. 机制构建：多智能体协作\n*   为了实现上述“规划”，单一的LLM是不够的，需要分工明确的智能体系统：\n    *   **策略智能体**：负责“想”。它观察当前的图状态，决定下一步该选哪几个父代，并给出一个“推导指令”。这相当于规划器。\n    *   **世界模型智能体**：负责“做”。它根据策略智能体的指令，具体生成代码。这相当于执行器。\n    *   **评论家智能体**：负责“反思”。它分析生成的结果，总结经验教训，反馈给策略和世界模型。这相当于强化学习中的奖励函数或反思机制。\n\n#### 5. 优化与闭环：解决多样性与偏见\n*   **进一步思考**：有了框架，如何保证搜索不陷入局部最优？LLM容易产生位置偏见或重复输出。\n*   **解决方案**：\n    *   **提示级多样性**：在Prompt中引入探索性短语，强制模型跳出舒适区。\n    *   **状态打乱**：随机打乱输入顺序，消除LLM的位置偏好。\n*   **最终闭环**：形成了一个“规划 -> 执行 -> 反思 -> 再规划”的闭环系统，实现了真正的“自进化”。\n\n#### 6. 总结：PathWise的诞生\n*   **最终形态**：PathWise 不是一个简单的进化算法，而是一个**混合图-种群框架**。它利用蕴含图作为长期记忆，利用多智能体LLM作为推理引擎，将启发式设计从“盲目搜索”提升到了“有意识的规划”层面。"
                },
                {
                    "title": "LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning",
                    "arxiv_id": "2601.20375",
                    "authors": "Wei Huang, Anda Cheng, Yinggui Wang, Lei Wang, Tao Wei",
                    "summary": "Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心贡献是提出了 **LLM-AutoDP** 这一新框架，其本质是利用 LLM 作为智能体来解决自动化数据处理策略生成的问题。这不仅仅是将现有智能体作为工具应用，而是构建了一个新的智能体框架，符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合自我演化**: 论文明确描述了智能体通过“iteratively refines them using feedback signals”（利用反馈信号迭代优化）和“converge toward high-quality processing pipelines”（收敛至高质量处理流程）来改进策略。这种通过环境反馈和比较评估进行自我完善和迭代的过程，完全符合“自我演化”的定义。 3.  **符合 Agentic AI**: 框架涉及智能体生成多个候选策略并进行选择，体现了智能体的规划和决策能力。论文将其与基于 LLM 智能体的 AutoML 基准进行比较，进一步确认了其属于 Agentic AI 的研究范畴。 4.  **排除标准检查**: 论文不涉及安全对齐、多模态视觉或图技术。虽然其应用场景是数据处理（通常属于数据工程），但根据第四步的特殊情况处理规则，该论文的核心在于提出了一种新的“自我演化”机制（策略搜索与迭代优化），而非单纯的数据处理算法或基础设施优化，因此应当保留。",
                    "summary2": "本文旨在解决LLM微调中数据处理的自动化及隐私风险问题。针对医疗等高隐私敏感数据，我们提出了一种LLM-AutoDP框架，利用LLM作为Agent迭代生成和优化数据处理策略，并结合DPS、PTS和CRM加速技术。在五个医疗数据集及三个模型架构上，通过Win rate和搜索时间验证了其优越性。",
                    "summary_translation": "大语言模型可以在特定领域数据上进行微调，以增强其在专业领域的性能。然而，此类数据通常包含大量低质量样本，因此迫切需要有效的数据处理。在实践中，数据处理策略通常是通过迭代的人工分析和试错调整来开发的。这些过程不可避免地导致高昂的人力成本，并且由于人类直接访问敏感数据，可能会在医疗等高隐私领域引发隐私问题。因此，在不暴露原始数据的情况下实现自动化数据处理已成为一个关键挑战。为了应对这一挑战，我们提出了LLM-AutoDP，这是一个利用大语言模型作为智能体来自动生成和优化数据处理策略的新颖框架。我们的方法生成多个候选策略，并利用反馈信号和比较评估对其进行迭代优化。这种迭代的上下文学习机制使智能体能够收敛到高质量的处理流水线，而无需直接的人工干预或访问底层数据。为了进一步加速策略搜索，我们引入了三项关键技术：分布保持采样，它在减少数据量的同时保持分布完整性；处理目标选择，它使用二分类器来识别低质量样本以进行集中处理；缓存重用机制，它通过重用先前的处理结果来最小化冗余计算。结果表明，在我们框架处理的数据上训练的模型，与在未处理数据上训练的模型相比，实现了超过80%的胜率。与基于大语言模型智能体的自动机器学习基线相比，LLM-AutoDP实现了约65%的胜率。此外，我们的加速技术将总搜索时间减少了多达10倍，证明了其有效性和效率。",
                    "inspiration_trace": "基于对论文《LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning》的深入分析，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction部分构建了一个层层递进的叙事逻辑，旨在引出研究的必要性：\n\n1.  **背景与机遇**：大语言模型（LLMs）在专业领域（如医疗）应用广泛，但通常需要进行领域特定的微调。\n2.  **现实阻碍**：领域数据通常通过爬虫或众包获取，包含大量噪声，无法直接使用。因此，**数据处理（DP）**成为了微调流程中的关键步骤。\n3.  **现有方案的痛点**：目前的DP策略主要依赖**人工**进行迭代分析和试错。这带来了两个致命缺陷：\n    *   **高人力成本**：耗时耗力。\n    *   **隐私风险**：在医疗等高隐私领域，人工直接接触原始敏感数据是不可接受的。\n4.  **自动化尝试的局限**：现有的自动化方案（如AutoML）虽然能减少人力，但存在两大核心缺陷：\n    *   **缺乏语义理解**：传统优化算法（贝叶斯、进化算法等）不理解数据处理的内在语义，导致收敛缓慢。\n    *   **计算开销不匹配**：这些方法未针对LLM微调任务进行优化，忽略了LLM数据处理带来的巨大计算开销，导致效率极低。\n5.  **结论**：因此，急需一种既能自动生成策略（无需人工接触数据），又能理解语义（快速收敛），且能高效处理LLM计算成本的新型自动化数据处理框架。\n\n---\n\n### 二、 核心研究问题\n\n基于上述逻辑，作者试图回答的核心问题可总结为：\n\n**“如何在不暴露原始数据且无需人工干预的前提下，利用LLM的语义理解能力自动生成并优化数据处理策略，同时克服传统AutoML方法在LLM微调场景下收敛慢、计算开销大的低效问题？”**\n\n---\n\n### 三、 思想演进与逻辑链推演\n\n以下是对作者产出该方法的思维过程的还原：\n\n#### 1. 观察与痛点识别\n*   **观察**：领域微调需要高质量数据，但原始数据很脏。人工清洗数据既贵又危险（隐私）。\n*   **思考**：必须实现“自动化”且“黑盒化”（不让人看数据）的数据处理。\n\n#### 2. 现有工具的批判性分析\n*   **尝试**：看看现有的AutoML工具能不能用？\n*   **发现**：AutoML用的是传统的数学优化方法（如网格搜索、贝叶斯优化）。\n*   **批判**：\n    *   这些方法是“盲”的，它们不知道“去重”和“改写”在语义上的区别，只能瞎猫碰死耗子，所以**收敛慢**。\n    *   LLM太大了，试错一次（微调一次）太慢了，传统AutoML那种几十次的迭代根本等不起。\n*   **结论**：需要一个“懂行”且“高效”的决策者。\n\n#### 3. 核心假设与范式转移\n*   **假设**：如果有一个东西能像人类专家一样理解数据处理的语义，是不是就能更快找到好策略？\n*   **灵感**：LLM本身具备强大的语义理解和推理能力。能不能让LLM来“指挥”数据清洗？\n*   **新范式**：**LLM即Agent**。让LLM作为智能体，通过Prompt来生成数据处理策略（比如：先去重，再改写），而不是让算法去猜参数。\n\n#### 4. 机制设计：如何让Agent变强？\n*   **问题**：LLM一次生成的策略可能不是最好的。\n*   **思考**：人类专家是怎么进步的？通过试错和反馈。\n*   **机制设计**：构建一个**闭环迭代系统**。\n    *   **生成**：LLM生成几个策略。\n    *   **评估**：拿这些策略去处理数据，微调一个小模型看效果。\n    *   **反馈**：把效果（分数）告诉LLM。\n    *   **优化**：LLM根据反馈（In-context Learning），调整下一轮的策略。\n*   **逻辑**：通过“群体相对比较”，让LLM明白哪种策略组合更好，从而逐步收敛到最优解。\n\n#### 5. 效率瓶颈的突破\n*   **新问题**：虽然LLM懂语义了，但每次评估都要跑一遍微调，还是太慢了（几十小时）。\n*   **思考**：能不能在不牺牲评估准确性的前提下，偷点懒？\n*   **策略优化**：\n    *   **采样**：不需要用全部数据来评估策略，用一小部分能代表分布的数据就够了（Distribution-Preserving Sampling）。\n    *   **筛选**：数据里本身就有干净的，别瞎处理。先训练个二分类器把“脏数据”挑出来，只处理脏的（Processing Target Selection）。\n    *   **复用**：如果新策略和旧策略前几步一样（比如都先去重），那去重的结果直接拿来用，别重算（Cache-and-Reuse Mechanism）。\n*   **结果**：通过这三个加速技巧，把搜索时间从几十小时压缩到了几小时，让迭代变得可行。\n\n#### 6. 最终方法论形成\n*   **整合**：将“LLM Agent作为策略生成器”与“加速评估模块”结合。\n*   **产出**：LLM-AutoDP框架。它既利用了LLM的语义智能来快速找策略，又通过工程手段解决了LLM微调评估慢的问题，最终实现了自动化、高效、隐私安全的数据处理。"
                },
                {
                    "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
                    "arxiv_id": "2601.20221",
                    "authors": "Hang Zhang, Ruheng Wang, Yuelyu Ji, Mingu Kwak, Xizhi Wu, Chenyu Li, Li Zhang, Wenqi Shi, Yifan Peng, Yanshan Wang",
                    "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断**：论文明确提出了一个名为 $\\method$ 的 \"agentic framework\"（智能体框架）。其核心贡献不在于仅仅将LLM应用于医疗领域，而在于构建了一种新的验证机制，即训练智能体在评估过程中迭代地查询外部医疗语料库。这属于构建和改进 LLM 智能体的方法论范畴。 2.  **符合核心关注点**： *   **Agentic AI (单智能体)**：论文涉及 `Tool Use / Tool Augmentation`（工具增强验证），智能体能够主动使用外部工具（查询语料库）来辅助决策。同时，它涉及复杂的推理验证过程，符合智能体的规划与推理能力。 *   **Self-Evolving (自我演化)**：论文采用了 `Iterative Reinforcement Learning`（迭代强化学习）范式和 `Adaptive Curriculum Mechanism`（自适应课程机制）。这表明智能体能够通过反馈和环境交互进行自我完善和迭代优化，符合“自我演化”的定义。 3.  **特殊与模糊情况处理**： *   虽然论文的应用场景是医疗领域，但根据筛选标准第四步第2点（自我演化的应用），只要论文的核心是提出一种新的“自我演化”或“自我改进”机制（在此为工具集成强化学习），即使应用在特定领域，也应该保留。 *   论文不仅仅是关于提高基础推理能力，而是引入了智能体框架（工具使用、迭代学习），因此不属于“非Agentic的推理”排除项。 综上所述，该论文在单智能体工具使用和自我演化机制上做出了核心贡献，符合研究课题要求。",
                    "summary2": "本文旨在解决现有医疗推理验证方法缺乏解释性及依赖静态检索的问题。针对医疗推理轨迹验证场景，我们提出了一种名为 Med-TIV 的工具集成强化学习框架，该方法通过迭代查询外部医疗语料库进行动态验证，并采用自适应课程学习策略。我们在 MedQA、MedMCQA、MMLU-Med 和 MedXpertQA 四个基准上通过准确率验证了其有效性，实现了显著性能提升及8倍的采样效率增益。",
                    "summary_translation": "Large language models (大语言模型) 在 medical reasoning benchmarks (医学推理基准) 上表现优异，但其在 clinical settings (临床场景) 中的部署需要进行 rigorous verification (严格验证) 以确保 factual accuracy (事实准确性)。虽然 reward models (奖励模型) 为 reasoning trace verification (推理轨迹验证) 提供了一种可扩展的方法，但现有方法存在两个局限：它们仅输出 scalar reward values (标量奖励值) 而缺乏 explicit justification (显式理由)，且依赖 single-pass retrieval (单次检索)，无法在验证过程中进行 adaptive knowledge access (自适应知识获取)。我们提出了 $\\method$，这是一个 agentic framework (智能体框架)，通过训练医学推理验证器在评估期间迭代查询 external medical corpora (外部医学语料库) 来解决上述局限。该方法将 tool-augmented verification (工具增强验证) 与仅需 trace-level supervision (轨迹级监督) 的 iterative reinforcement learning paradigm (迭代强化学习范式) 相结合，并辅以动态调整训练数据分布的 adaptive curriculum mechanism (自适应课程机制)。在四个 medical reasoning benchmarks (医学推理基准) 上，$\\method$ 相比现有方法取得了显著提升，特别是相对于 base generator (基础生成器)，将 MedQA 准确率提高了 23.5%，MedXpertQA 提高了 32.0%。至关重要的是，与先前的 reward model baselines (奖励模型基线) 相比，$\\method$ 将 sampling budget (采样预算) 需求降低了 8 倍。这些发现表明，将验证基于 dynamically retrieved evidence (动态检索的证据) 是构建更可靠的医学推理系统的一条 principled path (原则性路径)。",
                    "inspiration_trace": "基于论文《Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 一、 引言中的“讲故事”逻辑（问题引入）\n\n作者在Introduction中构建了一个层层递进的逻辑链条，用于引出研究的必要性：\n\n1.  **现状与机遇**：大型语言模型（LLMs）在医学推理基准测试中表现优异，显示出增强临床决策和普及医疗知识的潜力。\n2.  **核心矛盾**：尽管基准测试成绩好，但在高风险的临床环境中部署，必须要求生成的推理过程既符合事实又逻辑严密。单纯的生成能力不足以保证安全性，必须引入严格的“验证”机制。\n3.  **现有方案及其局限**：\n    *   *现有方案*：基于奖励模型的验证器（如ORM和PRM）已成为一种可扩展的解决方案。\n    *   *局限一（幻觉与缺乏依据）*：现有的验证器主要依赖模型的参数化知识（内部记忆）。在医学领域，这会导致验证器本身产生幻觉，即对错误的推理给出看似合理但事实错误的评估，且缺乏明确的解释性依据。\n    *   *局限二（静态检索的僵化）*：现有的检索增强生成（RAG）通常是静态的（单次检索、固定上下文）。这意味着验证器无法在验证过程中根据遇到的具体问题动态地获取新知识，限制了其适应性和准确性。\n4.  **解决思路的雏形**：为了解决上述问题，我们需要一种新的验证范式，它能够像人类专家一样，在评估过程中动态地查阅外部权威证据，从而将判断建立在客观事实而非模糊的记忆之上。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者试图回答的核心研究问题是：\n\n**“如何训练一个医学推理验证器，使其能够通过动态调用外部工具进行迭代式证据检索，从而在不依赖密集的步骤级人工标注的情况下，实现对复杂医学推理链的可靠、可解释且自适应的验证？”**\n\n---\n\n### 三、 核心方法的逻辑演进链\n\n作者产出这篇论文的思考过程可以概括为以下五个阶段：\n\n#### 1. 观察与痛点识别：从“被动打分”到“主动查证”\n*   **思考起点**：现有的医学验证器本质上是一个“被动打分者”。它看着一段推理链，仅凭脑子里的知识（参数）给个分数。\n*   **发现问题**：医学知识浩如烟海，模型不可能全记住。当推理链涉及罕见病或特定药物机制时，模型不仅可能判断错误，还可能一本正经地胡说八道（幻觉）。此外，传统的RAG是一次性把文档塞给模型，模型不知道什么时候该查什么，效率低且不灵活。\n*   **初步构想**：能不能让验证器像医生查房一样，遇到不确定的地方主动去翻书（检索工具）？\n\n#### 2. 范式转移：引入“智能体”思维\n*   **逻辑推演**：要实现“主动查证”，验证器就不能再是一个简单的分类器或打分器，而必须升级为一个**智能体**。\n*   **关键决策**：赋予验证器调用搜索引擎的能力。验证过程不再是单次输出，而是一个多轮的轨迹：分析推理 -> 产生疑问 -> 搜索证据 -> 基于证据修正判断 -> 输出结果。\n*   **预期效果**：这样可以将验证的依据从“模型记忆”转移到“外部证据”，解决幻觉问题，同时提供可解释的批判性文本。\n\n#### 3. 训练策略的挑战与突破：如何低成本教会模型“查资料”？\n*   **面临难题**：训练一个会使用工具的智能体通常需要昂贵的“步骤级”人工标注（即告诉模型在第一步该搜什么，第二步该搜什么）。在医学领域，这种标注成本极高。\n*   **创新假设**：我们是否可以只用简单的“对/错”标签（轨迹级监督），让模型自己学会什么时候该搜索？\n*   **方法论选择**：采用**强化学习（RL）**。设计一个奖励函数，只要模型最终的判断正确且格式规范，就给予奖励。模型为了获得这个奖励，会自发地探索出“使用工具”这一行为，因为工具能帮助它获得正确答案。\n\n#### 4. 优化训练效率：引入“自适应课程”\n*   **进一步思考**：直接用RL训练可能会遇到困难。如果题目太简单，模型不需要搜也能对；如果题目太难，模型搜了也不懂。这两种情况都产生不了有效的学习信号（梯度接近零）。\n*   **解决方案**：设计一个**自适应课程机制**。\n*   **具体逻辑**：在每一轮训练中，动态筛选数据。只保留那些模型“处于决策边界”的样本（即模型有时对、有时错的样本）。随着模型能力的提升，原本简单的样本会被过滤掉，更难的样本会进入训练集。这形成了一个自我进化的闭环。\n\n#### 5. 最终整合：Med-TIV 框架的诞生\n*   **系统综合**：将上述思考整合，提出了 **Med-TIV** 框架。\n    *   **核心**：一个工具集成的验证器。\n    *   **驱动力**：仅需轨迹级标签的迭代强化学习（Dr. GRPO）。\n    *   **加速器**：基于方差过滤的自适应课程学习。\n*   **价值验证**：通过实验证明，这种方法不仅提高了验证的准确性，还大幅降低了推理时的采样成本（因为验证器更准了，不需要生成那么多候选答案就能找到对的）。\n\n---\n\n**总结**：作者的思考路径是从**对现有验证器“不可靠、不灵活”的不满**出发，通过**引入工具使用解决可靠性**，通过**引入RL解决标注成本**，最后通过**课程学习解决训练效率**，最终构建了一个高效、可解释且自适应的医学验证智能体。"
                },
                {
                    "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
                    "arxiv_id": "2601.20209",
                    "authors": "Jinyang Wu, Shuo Yang, Changpeng Yang, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao",
                    "summary": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \\textbf{Spark} (\\textbf{S}trategic \\textbf{P}olicy-\\textbf{A}ware explo\\textbf{R}ation via \\textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \\textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了一种名为 **Spark** 的新框架，旨在解决长期任务中LLM智能体的训练难题。 *   这不是将现有智能体简单应用到特定领域（如医疗、法律），而是针对智能体本身的**学习机制**进行了改进。它提出了一种“动态分支”策略来优化智能体的探索过程，属于构建和改进LLM智能体的方法论。 2.  **符合正面指标（第二步）：** *   **核心范式：** 论文明确提到了 \"Agentic Learning\" 和 \"Reinforcement learning has empowered large language models to act as intelligent agents\"，完全符合 `Agentic AI` 和 `LLM-based Agents` 的定义。 *   **智能体能力：** 论文重点讨论了 \"Long-Horizon Agentic Learning\" 和 \"Embodied Planning\"，这直接对应了智能体的 `Planning`（规划）能力以及在复杂环境中的多步推理。 *   **演化机制：** 论文强调通过 \"Strategic Policy-Aware Exploration\" 和 \"autonomously expand exploration\" 来实现更强的泛化能力，这属于智能体通过环境反馈和经验进行自我完善和迭代的范畴，符合 `Self-Evolving` 和 `Self-Improvement` 的特征。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、水印或幻觉问题。 *   虽然提到了 \"Embodied planning\"（通常涉及视觉），但视觉只是智能体感知环境的输入工具，论文的研究核心在于智能体的探索策略和学习框架，而非视觉模型本身。 4.  **特殊情况处理（第四步）：** *   **推理/规划：** 论文关注的是智能体如何在长期任务中进行规划和探索，属于Agentic框架下的推理，而非单纯的LLM基础Token预测能力提升，因此予以保留。 综上所述，该论文提出了一种改进LLM智能体在长期任务中学习和规划能力的新框架，属于Agentic AI的核心研究范畴。",
                    "summary2": "本文旨在解决长视距智能体学习中高质量轨迹稀缺及资源分配低效的问题。针对计算资源受限的长视距任务场景，我们提出了一种名为SPARK的框架，通过在关键决策状态进行动态分支探索以实现战略性资源分配。我们在ALFWorld、ScienceWorld和WebShop基准上通过成功率及样本效率等指标验证了其有效性，显著优于现有基线方法。",
                    "summary_translation": "强化学习 (Reinforcement learning) 赋予了大语言模型 (Large Language Models) 充当智能体的能力，然而在资源受限的情况下，由于高质量轨迹 (trajectories) 的稀缺，针对长视界任务 (long-horizon tasks) 的训练仍然充满挑战。现有方法通常扩大推演 (rollout) 规模，并在中间步骤中无差别地分配计算资源。这种尝试本质上在琐碎步骤上浪费了大量计算预算，同时无法保证样本质量。为了解决这一问题，我们提出了 \\textbf{Spark} (\\textbf{S}trategic \\textbf{P}olicy-\\textbf{A}ware explo\\textbf{R}ation via \\textbf{K}ey-state dynamic branching，即基于关键状态动态分支的策略感知战略探索)，这是一个在关键决策状态进行选择性分支以实现资源高效探索的新颖框架。我们的核心见解是在关键决策点激活自适应分支探索，以探测有前景的轨迹，从而实现优先考虑样本质量而非盲目覆盖的精准资源分配。这种设计利用智能体的内在决策信号来减少对人类先验 (human priors) 的依赖，使智能体能够自主扩展探索并实现更强的泛化能力。在多样化任务（例如 embodied planning (具身规划)）上的实验表明，\\textsc{Spark} 在显著减少训练样本的情况下实现了更高的成功率，即使在未见场景中也表现出鲁棒的泛化能力。",
                    "inspiration_trace": "基于对论文《Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning》的深度分析，以下是对作者产出该文章核心思考过程的系统性逻辑推演。\n\n---\n\n### 一、 宏观背景与观察：从“推理”到“行动”的跨越\n\n**思考起点：**\n作者首先观察到大语言模型（LLM）的发展趋势正在从单纯的“推理能力”（如数学、编程）向“智能体能力”跨越。强化学习（RL）被认为是实现这一跨越的关键技术，因为它能让模型在动态环境中自主学习。\n\n**核心观察：**\n虽然RL在数学等封闭领域取得了成功，但在长视界智能体任务中，训练变得异常困难。作者敏锐地捕捉到一个核心矛盾：**任务的成功往往取决于一系列正确的决策，而只要中间一步出错，整个轨迹就会失败。** 这导致高质量的训练轨迹极其稀缺。\n\n---\n\n### 二、 问题引入：Introduction 中的“讲故事”逻辑\n\n作者在Introduction中通过层层递进的逻辑，构建了一个引人入胜的问题叙事：\n\n1.  **美好愿景与残酷现实：**\n    RL赋予了LLM智能体的潜力，但在长视界任务中，由于资源受限，高质量轨迹的稀缺性成为了根本瓶颈。\n\n2.  **对比分析（找不同）：**\n    作者将智能体任务与数学任务进行对比。数学问题的解决方案是自包含且易于验证的，而智能体任务需要在巨大的状态空间中导航，容错率极低。\n\n3.  **现有方案的“笨拙”：**\n    为了解决轨迹稀缺问题，现有方法通常采用“暴力美学”——扩大搜索规模和Rollout数量。\n    *   *关键隐喻：* 作者举了一个生动的例子——做早餐。现有方法会平均分配计算资源，既在“打开冰箱门”这种琐碎步骤上浪费大量算力，又在“缺少食材时选择替代品”这种关键决策点上投入不足。\n\n4.  **痛点总结：**\n    这种“无差别的资源分配”导致了严重的低效：在无关紧要的步骤上浪费预算，而在决定成败的关键节点上探索不足，最终导致训练不稳定且难以泛化。\n\n---\n\n### 三、 研究问题\n\n基于上述观察和痛点，作者试图回答的核心问题是：\n\n**“在计算资源受限的情况下，智能体如何能够自主识别出关键决策节点，并动态地分配探索资源，从而在长视界任务中获得高质量的训练轨迹？”**\n\n---\n\n### 四、 逻辑演进与假设形成\n\n为了回答上述问题，作者的思考经历了以下三个阶段的演进：\n\n#### 阶段 1：从“平均主义”到“战略聚焦” (假设提出)\n*   **反思：** 既然资源有限，为什么要在每一步都平均用力？\n*   **假设：** 并非所有的决策步骤都是同等重要的。存在一种“关键状态”，在这些节点上的探索价值远高于其他“常规状态”。\n*   **推论：** 如果能区分这两类状态，我们就可以把算力省下来，专门用在刀刃上。\n\n#### 阶段 2：从“外部指导”到“内在感知” (机制设计)\n*   **挑战：** 谁来定义什么是“关键状态”？如果依赖人工设计的规则或外部奖励模型，不仅成本高，而且难以泛化到新场景。\n*   **洞察：** 智能体自身在推理过程中其实能感知到不确定性。当它“拿不准”的时候，就是需要探索的时候。\n*   **方案雏形：** 利用模型内在的决策信号（例如在推理Trace中输出特定的`<explore>`标签）来触发探索，而不是依赖外部硬编码的规则。\n\n#### 阶段 3：从“线性链式”到“动态树状” (结构创新)\n*   **结构重构：** 传统的RL采样是N条独立的线性轨迹。既然要在关键点探索，那么轨迹的结构就不该是线性的，而应该是树状的。\n*   **逻辑闭环：**\n    *   **常规步骤：** 保持线性，共享前缀，节省Token。\n    *   **关键步骤：** 触发分支，生成多个后续可能性，提高命中正确路径的概率。\n    *   **结果：** 在同样的总预算下，树状结构能覆盖更多样化的关键决策组合，从而获得比线性采样更高质量的数据。\n\n---\n\n### 五、 最终方法论：SPARK 的诞生\n\n基于上述思考，作者最终构建了 **SPARK** 框架，其核心逻辑链如下：\n\n1.  **动态分支：** 打破传统的均匀采样，建立一种机制，允许在轨迹中间根据情况“分叉”。\n2.  **关键点识别：** 利用模型自身的推理信号（`<explore>`标签）作为“探针”，自主判断当前状态是否需要多分支探索。\n3.  **预算约束下的战略分配：** 在总计算预算固定的情况下，优先保证关键节点的分支数，压缩常规步骤的冗余计算。\n4.  **树状优化：** 基于生成的轨迹树进行策略更新，利用共享前缀的优势进行相对优势评估，从而训练出更具泛化能力的智能体。\n\n**总结：**\n作者的思考路径是从**发现资源分配的结构性矛盾**出发，提出**“关键节点优先”的战略假设**，进而通过**利用模型内在不确定性**解决了“如何识别关键节点”的难题，最终通过**动态树状结构**实现了在有限资源下的高效探索。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "Continual GUI Agents",
                    "arxiv_id": "2601.20732",
                    "authors": "Ziwei Liu, Borui Kang, Hangjie Yuan, Zixiang Zhao, Wei Li, Yifan Zhu, Tao Feng",
                    "summary": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**： 论文的核心贡献是提出了 **GUI-Anchoring in Flux (GUI-AiF)**，这是一个新的强化微调框架，旨在解决GUI智能体在数据分布随时间变化（新领域、新分辨率）时的性能退化问题。这属于**构建和改进 LLM 智能体**的方法论研究，而非仅仅将智能体作为工具应用到特定领域。它关注的是智能体本身的适应能力和学习机制。 2.  **符合核心关注点（第二步 & 第四步）**： *   **自我演化**：论文明确提出了“Continual GUI Agents”任务，要求智能体进行“Continual Learning”（持续学习）。这直接对应了研究焦点中的“自我演化”，即智能体通过环境反馈（变化的GUI数据）进行自我完善和迭代，以适应新的环境。 *   **单智能体**：论文致力于提升智能体在复杂环境下的稳定性和定位能力，属于单智能体能力的改进。 3.  **排除标准检查（第三步）**： *   **多模态与视觉**：虽然论文涉及GUI（图形用户界面），但视觉元素在这里是智能体感知和交互的**环境/工具**，而非论文的研究核心。论文的核心贡献在于智能体的**学习框架和奖励机制**，而非视觉模型的改进，因此符合“除非它们被用作智能体感知环境的工具”这一例外条款。 *   论文不涉及安全、对齐或图技术等排除项。 综上所述，该论文提出了一种新的智能体持续学习（演化）框架，属于Agentic AI的前沿研究，符合“LLM智能体及其演化”的课题要求。",
                    "summary2": "本文旨在解决GUI智能体在动态数字环境中因分布漂移导致性能下降的问题。针对领域和分辨率不断变化的场景，我们提出了一种名为GUI-AiF的强化微调框架，通过引入APR-iF和ARR-iF奖励机制来稳定持续学习。我们在ScreenSpot-V1、V2和Pro基准上通过准确率验证了其有效性，结果显示该方法优于现有基线。",
                    "summary_translation": "随着数字环境（数据分布）处于不断变化之中，新的 GUI（Graphical User Interface，图形用户界面）数据随时间推移而到来——引入了新的领域或分辨率——在静态环境中训练的智能体其性能会出现衰退。在这项工作中，我们提出了 Continual GUI Agents（持续 GUI 智能体）这一新任务，该任务要求 GUI 智能体在领域和分辨率发生偏移的情况下进行持续学习。我们发现，由于动态变化场景中 UI（User Interface，用户界面）交互点和区域的多样性，随着 GUI 分布随时间推移而变化，现有方法无法维持稳定的 Grounding（定位/锚定）。为了解决这一问题，我们提出了 GUI-Anchoring in Flux (GUI-AiF)，这是一种新的强化微调框架，它通过两种新颖的奖励机制来稳定持续学习：Anchoring Point Reward in Flux (APR-iF，动态变化中的锚定点奖励) 和 Anchoring Region Reward in Flux (ARR-iF，动态变化中的锚定区域奖励)。这些奖励机制引导智能体与不断变化的交互点和区域保持对齐，从而缓解了现有奖励策略过度适应静态 Grounding 线索（如固定坐标或元素比例）的倾向。大量实验表明，GUI-AiF 优于最先进的基线模型。我们的工作建立了首个针对 GUI 智能体的持续学习框架，揭示了强化微调在 Continual GUI Agents 领域中尚未被开发的潜力。",
                    "inspiration_trace": "基于论文《Continual GUI Agents》的内容，以下是对作者产出该文章的系统性逻辑链推演，还原了从宏观观察到微观方法论的思考过程。\n\n---\n\n### 一、 宏观问题引入：Introduction 中的“讲故事”逻辑\n\n作者在引言中通过层层递进的逻辑，构建了从现有技术缺陷到现实需求落地的叙事链条：\n\n1.  **现状与能力**：首先肯定了自主 GUI 智能体的核心能力——**Grounding（定位）**，即通过自然语言指令将操作映射到图形界面上的精确像素坐标。目前的训练主要依赖于静态数据集上的监督微调（SFT）或强化微调（RFT）。\n2.  **现实挑战**：话锋一转，指出真实世界的数字环境并非静止，而是处于**“流变”**之中。具体表现为两个维度：\n    *   **领域流变**：操作系统更新或平台切换（如从移动端 Mobile OS 切换到网页端 Web OS）。\n    *   **分辨率流变**：设备升级导致屏幕分辨率变化（如从 1080p 变为 4K）。\n3.  **核心冲突**：现有的基于静态数据训练的智能体，在面对这种动态变化的数据分布时，性能会显著衰退。它们无法在变化的场景中保持稳定的“锚定”能力，即无法准确锁定交互点和区域。\n4.  **归因分析**：深入剖析现有方法的局限性。\n    *   **SFT**：倾向于记忆和拟合当前任务的数据分布，本质上不适合处理动态多样的 GUI。\n    *   **RFT**：虽然通过 KL 散度约束参数漂移，天然适合持续学习，但其参数中心的优化主要为了防止偏离参考模型。当面临领域或分辨率剧烈变化时，UI 交互区域的位置和尺度发生变异，标准 RFT 仍会导致智能体**过度适应**于静态的定位线索（如固定坐标或元素尺度），从而缺乏对新环境的泛化能力。\n\n---\n\n### 二、 研究问题\n\n基于上述逻辑，作者显式提出了本文试图解决的核心问题：\n\n**“GUI 智能体如何在面对领域迁移（如移动端到桌面端）和分辨率变化（如高清到超高清）的连续流变环境中，保持稳定的视觉定位能力，避免因过度适应静态布局而导致的性能衰退？”**\n\n---\n\n### 三、 思想演进与方法论形成逻辑链\n\n为了回答上述问题，作者的思考路径经历了以下四个阶段的演进：\n\n#### 第一阶段：观察与诊断\n*   **观察**：GUI 环境的变化本质上是**交互点位置**和**元素尺度**的变化。例如，移动端多文本，Web 端多图标；分辨率提升导致元素比例改变。\n*   **诊断**：现有方法失败的根本原因在于**“过度适应”**。模型在当前任务上死记硬背了特定的坐标和框的大小，一旦环境变化，这些记忆就变成了束缚，导致模型无法在新的布局中找到正确的交互位置。\n\n#### 第二阶段：提出假设\n*   **假设**：如果能在训练过程中，显式地鼓励模型探索**多样化**的交互点和元素区域，而不是仅仅追求对当前 Ground Truth 的精确拟合，就能打破对单一静态布局的依赖。\n*   **核心思想**：**“多样性即鲁棒性”**。通过奖励预测结果的离散程度，迫使模型学习更具泛化性的定位策略，从而适应未来的环境变化。\n\n#### 第三阶段：方法论构建\n*   **框架选择**：选择**强化微调（RFT）**作为基础框架。因为 RFT 的 KL 散度约束天然适合持续学习，能防止模型在更新参数时彻底遗忘旧知识。\n*   **机制创新**：在 RFT 的奖励函数中引入两个新的奖励信号，以对抗“过度适应”：\n    1.  **针对位置的多样性**：设计 **APR-iF (Anchoring Point Reward in Flux)**。通过计算预测框中心点的空间方差，奖励模型探索分散的交互点，避免聚类在单一坐标。\n    2.  **针对尺度的多样性**：设计 **ARR-iF (Anchoring Region Reward in Flux)**。利用高斯分布建模预测区域，计算区域间的 Bhattacharyya 距离，奖励模型预测在空间上分离、尺度多样的区域，以适应分辨率变化带来的尺度差异。\n*   **整合**：将这两个奖励与传统的准确性奖励（如 IoU）结合，形成一个平衡“当前任务表现”与“未来环境适应能力”的综合优化目标。\n\n#### 第四阶段：验证与闭环\n*   **实验设计**：构建了两个具体的持续学习场景来验证假设——**领域流变**（Mobile -> Desktop -> Web）和**分辨率流变**（Normal -> High Resolution）。\n*   **预期结果**：如果假设成立，加入了多样性奖励的 GUI-AiF 应该在连续任务的后半段（新环境）表现优于基线，且不会遗忘前半段（旧环境）的知识，从而证明通过鼓励“探索”可以解决持续学习中的“流变”问题。"
                },
            ]
        },
    ],
};

const availableDates = ["2026-02-05", "2026-02-04", "2026-02-03", "2026-02-02", "2026-01-30", "2026-01-29"];
const loadedDates = new Set(["2026-02-05", "2026-02-04", "2026-02-03", "2026-02-02", "2026-01-30", "2026-01-29"]);
const LOAD_MORE_DAYS = 7;

const dailyOverviewsRaw = {
    "2026-02-05": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-02-05)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u63ed\u793a\u4e86AI\u667a\u80fd\u4f53\u7814\u7a76\u6b63\u4ece\u5355\u4e00\u6a21\u578b\u7684\u201c\u72ec\u89d2\u620f\u201d\u8fc8\u5411\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e0e\u81ea\u6211\u8fdb\u5316\u7684\u201c\u751f\u6001\u7cfb\u7edf\u201d\u3002\u6838\u5fc3\u8d8b\u52bf\u663e\u793a\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u81f4\u529b\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u4e0e\u52a8\u6001\u534f\u8c03\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u8d4b\u4e88\u667a\u80fd\u4f53\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670d\u5927\u6a21\u578b\u56fa\u6709\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u63a7\u6027\uff0c\u5f15\u5165\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u51b3\u7b56\u6811\u7b49\u663e\u5f0f\u7ed3\u6784\u5316\u63a7\u5236\u673a\u5236\u6210\u4e3a\u4e86\u65b0\u7684\u6280\u672f\u70ed\u70b9\u3002\n\n---\n\n### \u591a\u667a\u80fd\u4f53\u534f\u540c\uff1a\u4ece\u9759\u6001\u62d3\u6251\u5230\u52a8\u6001\u535a\u5f08\n\n\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u7814\u7a76\u91cd\u70b9\u6b63\u4ece\u56fa\u5b9a\u7684\u901a\u4fe1\u6a21\u5f0f\u8f6c\u5411\u57fa\u4e8e\u8bed\u4e49\u7684\u52a8\u6001\u8def\u7531\u4e0e\u535a\u5f08\u8c08\u5224\uff0c\u65e8\u5728\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u6548\u7387\u4e0e\u7ecf\u6d4e\u4ea4\u4e92\u80fd\u529b\u3002\n\n*   **DyTopo** \u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u7ba1\u7406\u8005\u5f15\u5bfc\u7684\u52a8\u6001\u62d3\u6251\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u5728\u6bcf\u4e2a\u63a8\u7406\u8f6e\u6b21\u91cd\u6784\u7a00\u758f\u6709\u5411\u901a\u4fe1\u56fe\uff0c\u4ec5\u5728\u76f8\u5173\u667a\u80fd\u4f53\u95f4\u4f20\u9012\u6d88\u606f\uff0c\u4ece\u800c\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u62d3\u6251\u7684\u57fa\u7ebf\u6a21\u578b\u3002(2602.06039 [cs.AI])\n*   **AgenticPay** \u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u4e70\u5356\u53cc\u65b9\u591a\u8f6e\u8bed\u8a00\u8c08\u5224\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b83\u6a21\u62df\u4e86\u62e5\u6709\u79c1\u6709\u7ea6\u675f\u548c\u4f30\u503c\u7684\u590d\u6742\u5e02\u573a\u73af\u5883\uff0c\u586b\u8865\u4e86\u8bc4\u4f30\u57fa\u4e8e\u8bed\u8a00\u7684\u7ecf\u6d4e\u4ea4\u4e92\u548c\u591a\u667a\u80fd\u4f53\u6218\u7565\u63a8\u7406\u80fd\u529b\u7684\u7a7a\u767d\u3002(2602.06008 [cs.AI])\n*   **WideSeek-R1** \u63a2\u7d22\u4e86\u201c\u5bbd\u5ea6\u6269\u5c55\u201d\u8fd9\u4e00\u65b0\u7ef4\u5ea6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e3b\u667a\u80fd\u4f53\u4e0e\u5e76\u884c\u5b50\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\uff0c\u8bc1\u660e\u4e86\u5728\u5e7f\u5ea6\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c4B\u53c2\u6570\u7684\u5e76\u884c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u5ab2\u7f8e671B\u53c2\u6570\u7684\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u3002(2602.04634 [cs.AI])\n*   **SPEAR** \u5c55\u793a\u4e86\u4e00\u4e2a\u5c06\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u89c6\u4e3a\u534f\u8c03\u4efb\u52a1\u7684\u591a\u667a\u80fd\u4f53\u5de5\u7a0b\u6848\u4f8b\uff0c\u5e94\u7528\u4e86\u5408\u540c\u7f51\u534f\u8bae\u548c\u62cd\u5356\u534f\u8bae\u7b49\u7ecf\u5178MAS\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5728\u98ce\u9669\u611f\u77e5\u542f\u53d1\u5f0f\u4e0b\u7684\u4efb\u52a1\u5206\u914d\u4e0e\u81ea\u4e3b\u4fee\u590d\u3002(2602.04418 [cs.AI])\n*   **AgentArk** \u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u84b8\u998f\u7b56\u7565\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u52a8\u6001\u4ea4\u4e92\u8fc7\u7a0b\u84b8\u998f\u5230\u5355\u4e2a\u6a21\u578b\u7684\u6743\u91cd\u4e2d\uff0c\u65e8\u5728\u4fdd\u7559\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u6210\u672c\u548c\u9519\u8bef\u4f20\u64ad\u98ce\u9669\u3002(2602.03955 [cs.AI])\n*   **On the Uncertainty of LLM-based MAS** \u4ece\u71b5\u7684\u89d2\u5ea6\u6df1\u5165\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53d1\u73b0\u5355\u667a\u80fd\u4f53\u5728\u7ea643.3%\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u4e3b\u8981\u5728\u7b2c\u4e00\u8f6e\u4ea4\u4e92\u4e2d\u51b3\u5b9a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u5224\u636e\u7684\u7b80\u5355\u7b97\u6cd5\u6765\u63d0\u5347\u89e3\u7684\u9009\u62e9\u8d28\u91cf\u3002(2602.04234 [cs.MA])\n\n### \u667a\u80fd\u4f53\u8fdb\u5316\uff1a\u6253\u7834\u9884\u8bad\u7ec3\u7684\u8fb9\u754c\n\n\u5982\u4f55\u8ba9\u667a\u80fd\u4f53\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u5b9e\u73b0\u201c\u7ec8\u8eab\u5b66\u4e60\u201d\u548c\u201c\u81ea\u6211\u8fdb\u5316\u201d\uff0c\u662f\u4eca\u65e5\u7814\u7a76\u7684\u53e6\u4e00\u5927\u7126\u70b9\uff0c\u91cd\u70b9\u5728\u4e8e\u8de8\u60c5\u8282\u7684\u77e5\u8bc6\u5185\u5316\u4e0e\u7ecf\u9a8c\u5171\u4eab\u3002\n\n*   **Group-Evolving Agents (GEA)** \u5f15\u5165\u4e86\u4e00\u79cd\u4ee5\u667a\u80fd\u4f53\u7fa4\u4e3a\u57fa\u672c\u8fdb\u5316\u5355\u5143\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u7ecf\u9a8c\u5171\u4eab\u548c\u91cd\u7528\u673a\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6811\u72b6\u8fdb\u5316\u5206\u652f\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728SWE-bench Verified\u7b49\u7f16\u7801\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u3002(2602.04837 [cs.AI])\n*   **Empirical-MCTS** \u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u5faa\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u5bf9\u7ecf\u9a8c\u8fdb\u5316\u5143\u63d0\u793a\uff08PE-EMP\uff09\u548c\u8bb0\u5fc6\u4f18\u5316\u667a\u80fd\u4f53\uff0c\u5c06\u65e0\u72b6\u6001\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8f6c\u5316\u4e3a\u6301\u7eed\u7684\u975e\u53c2\u6570\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u6a21\u5f0f\u7684\u8de8\u95ee\u9898\u79ef\u7d2f\u4e0e\u590d\u7528\u3002(2602.04248 [cs.AI])\n*   **ORBIT** \u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u3001\u591a\u60c5\u8282\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u7ecf\u8fc7\u5143\u8bad\u7ec3\u7684\u5f00\u6e90\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u5728\u7ebf\u73af\u5883\u4e2d\u5177\u5907\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\uff0c\u751a\u81f3\u80fd\u5339\u654cGPT-5.2\u3002(2602.04089 [cs.AI])\n*   **SE-Bench** \u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u73af\u5883\u6765\u4e25\u683c\u8861\u91cf\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u4e0e\u77e5\u8bc6\u5185\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u201c\u5f00\u4e66\u6096\u8bba\u201d\uff08\u6709\u6587\u6863\u8bad\u7ec3\u53cd\u800c\u6291\u5236\u8bb0\u5fc6\uff09\u548c\u6807\u51c6RL\u5728\u77e5\u8bc6\u5185\u5316\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u4e86\u81ea\u535a\u5f08\u7ed3\u5408SFT\u7684\u6709\u6548\u6027\u3002(2602.04811 [cs.AI])\n\n### \u63a8\u7406\u4e0e\u89c4\u5212\uff1a\u9a7e\u9a6d\u4e0d\u786e\u5b9a\u6027\u7684\u827a\u672f\n\n\u5728\u79d1\u5b66\u63a8\u7406\u3001\u6570\u5b66\u6c42\u89e3\u548c\u5177\u8eab\u667a\u80fd\u7b49\u9ad8\u96be\u5ea6\u9886\u57df\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u8bd5\u56fe\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u63a7\u5236\u3001\u663e\u5f0f\u89c4\u5212\u548c\u6267\u884c\u53cd\u9988\u6765\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\n\n*   **ReThinker** \u63d0\u51fa\u4e86\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u6c42\u89e3\u5668-\u6279\u8bc4\u8005-\u9009\u62e9\u5668\u67b6\u6784\uff0c\u6839\u636e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u7ed3\u5408\u9006\u5411\u6570\u636e\u5408\u6210\u7b56\u7565\uff0c\u5728Humanity's Last Exam (HLE) \u7b49\u4e13\u5bb6\u7ea7\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u3002(2602.04496 [cs.AI])\n*   **PCE (Planner-Composer-Evaluator)** \u5c06\u5927\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6f5c\u5728\u5047\u8bbe\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u51b3\u7b56\u6811\uff0c\u901a\u8fc7\u8bc4\u4f30\u573a\u666f\u53ef\u80fd\u6027\u3001\u76ee\u6807\u6536\u76ca\u548c\u6267\u884c\u6210\u672c\u6765\u6307\u5bfc\u884c\u52a8\uff0c\u4f7f\u5177\u8eab\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u65e0\u9700\u9891\u7e41\u901a\u4fe1\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u3002(2602.04326 [cs.AI])\n*   **Active Epistemic Control (AEC)** \u5f15\u5165\u4e86\u4e00\u4e2a\u8ba4\u77e5-\u5206\u7c7b\u89c4\u5212\u5c42\uff0c\u4e25\u683c\u533a\u5206\u7528\u4e8e\u627f\u8bfa\u7684\u201c\u843d\u5730\u4e8b\u5b9e\u5e93\u201d\u548c\u7528\u4e8e\u526a\u679d\u7684\u201c\u4fe1\u5ff5\u5e93\u201d\uff0c\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u6216\u6a21\u62df\u6765\u5904\u7406\u672a\u77e5\u524d\u63d0\uff0c\u5728ALFWorld\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5c11\u7684\u91cd\u89c4\u5212\u8f6e\u6b21\u3002(2602.03974 [cs.AI])\n*   **Agentic Verifier** \u9488\u5bf9\u7ade\u6280\u7f16\u7a0b\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6267\u884c\u7684\u667a\u80fd\u4f53\u9a8c\u8bc1\u5668\uff0c\u80fd\u591f\u4e3b\u52a8\u63a8\u7406\u7a0b\u5e8f\u884c\u4e3a\u5e76\u751f\u6210\u5177\u6709\u9ad8\u533a\u5206\u5ea6\u7684\u6d4b\u8bd5\u7528\u4f8b\uff08\u53cd\u4f8b\uff09\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86Best@K\u51c6\u786e\u7387\u3002(2602.04254 [cs.CL])\n*   **IIPC (Iteratively Improved Program Construction)** \u9488\u5bf9\u6570\u5b66\u95ee\u9898\u6c42\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u6539\u8fdb\u7a0b\u5e8f\u6784\u5efa\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6267\u884c\u53cd\u9988\u4e0e\u539f\u751f\u601d\u7ef4\u94fe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u5206\u6563\u6a21\u578b\u6ce8\u610f\u529b\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002(2602.03950 [cs.AI])\n\n### \u8bb0\u5fc6\u4e0e\u72b6\u6001\uff1a\u6784\u5efa\u66f4\u53ef\u63a7\u7684\u667a\u80fd\u4f53\n\n\u4e3a\u4e86\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u6548\u7387\u548c\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u5f15\u5165\u9884\u7b97\u63a7\u5236\u548c\u6709\u9650\u72b6\u6001\u673a\u7b49\u7ecf\u5178\u8ba1\u7b97\u673a\u79d1\u5b66\u6982\u5ff5\u3002\n\n*   **BudgetMem** \u63d0\u51fa\u4e86\u4e00\u4e2a\u8fd0\u884c\u65f6\u667a\u80fd\u4f53\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5728\u4e0d\u540c\u9884\u7b97\u5c42\u7ea7\uff08\u4f4e/\u4e2d/\u9ad8\uff09\u95f4\u8fdb\u884c\u5185\u5b58\u6a21\u5757\u8def\u7531\uff0c\u5b9e\u73b0\u4e86\u5728\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u5e76\u5728\u957f\u8bb0\u5fc6\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u66f4\u4f18\u7684\u7cbe\u5ea6-\u6210\u672c\u524d\u6cbf\u3002(2602.06025 [cs.CL])\n*   **Codified Finite-State Machines (CFSMs)** \u91cd\u65b0\u5ba1\u89c6\u4e86\u6709\u9650\u72b6\u6001\u673a\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528LLM\u5c06\u6587\u672c\u89d2\u8272\u6863\u6848\u81ea\u52a8\u7f16\u7801\u4e3aFSM\uff0c\u751a\u81f3\u6269\u5c55\u4e3a\u6982\u7387FSM\uff08CPFSM\uff09\uff0c\u4ece\u800c\u5728\u5f00\u653e\u5f0f\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u89d2\u8272\u72b6\u6001\u7684\u4e00\u81f4\u6027\u3002(2602.05905 [cs.CL])\n*   **Soft-FSM** \u9488\u5bf9\u6cd5\u5f8b\u4ea4\u53c9\u8be2\u95ee\u7b49\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u901a\u8fc7\u5916\u90e8\u786e\u5b9a\u6027\u72b6\u6001\u63a7\u5236\u5668\u5f3a\u5236\u6267\u884c\u5355\u8c03\u8fdb\u7a0b\uff0c\u6709\u6548\u9632\u6b62\u4e86LLM\u5728\u590d\u6742\u7a0b\u5e8f\u7ea6\u675f\u4e0b\u7684\u884c\u4e3a\u505c\u6ede\uff0c\u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\u3002(2602.04206 [cs.AI])\n\n### \u5de5\u7a0b\u5316\u5e94\u7528\uff1a\u843d\u5730\u573a\u666f\u7684\u63a2\u7d22\n\n\u90e8\u5206\u7814\u7a76\u805a\u7126\u4e8e\u5c06\u667a\u80fd\u4f53\u6280\u672f\u5e94\u7528\u4e8e\u5177\u4f53\u7684\u8f6f\u4ef6\u5de5\u7a0b\u548c\u5b66\u672f\u8f85\u52a9\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u4ef7\u503c\u3002\n\n*   **Agentic AI for Software Engineering** \u6f14\u793a\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u5728\u6587\u6863\u68c0\u7d22\u548c\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u4e24\u4e2a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u661f\u578b\u62d3\u6251\u548c\u4e13\u7528Worker\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u6587\u6863\u7684\u590d\u6742\u64cd\u4f5c\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u3002(2602.04726 [cs.AI])\n*   **Agentic AI-Empowered Dynamic Survey Framework** \u5c06\u7efc\u8ff0\u8bba\u6587\u7684\u64b0\u5199\u91cd\u6784\u4e3a\u4e00\u4e2a\u957f\u65f6\u7a0b\u7ef4\u62a4\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u67e5\u6846\u67b6\uff0c\u80fd\u591f\u968f\u7740\u65b0\u7814\u7a76\u7684\u51fa\u73b0\u6301\u7eed\u66f4\u65b0\u73b0\u6709\u7efc\u8ff0\uff0c\u4fdd\u6301\u6587\u6863\u7684\u65f6\u6548\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u3002(2602.04071 [cs.LG])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n1.  **\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u201c\u7626\u8eab\u201d\u4e0e\u201c\u91cd\u6784\u201d**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u74f6\u9888\u3002**AgentArk** \u63d0\u51fa\u5c06\u591a\u667a\u80fd\u4f53\u80fd\u529b\u84b8\u998f\u56de\u5355\u667a\u80fd\u4f53\uff0c\u800c **DyTopo** \u5219\u4e3b\u5f20\u52a8\u6001\u91cd\u6784\u901a\u4fe1\u62d3\u6251\u3002\u66f4\u6709\u8da3\u7684\u662f\uff0c**Uncertainty in MAS** \u7684\u7814\u7a76\u6307\u51fa\uff0c\u5355\u667a\u80fd\u4f53\u5728\u8fd1\u534a\u6570\u573a\u666f\u4e0b\u5176\u5b9e\u4f18\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8fd9\u63d0\u793a\u6211\u4eec\u5728\u8bbe\u8ba1\u590d\u6742\u7cfb\u7edf\u65f6\u9700\u8b66\u60d5\u201c\u4e3a\u4e86\u591a\u667a\u80fd\u4f53\u800c\u591a\u667a\u80fd\u4f53\u201d\u7684\u9677\u9631\u3002\n2.  **\u4ece\u201c\u6df1\u5ea6\u201d\u5411\u201c\u5bbd\u5ea6\u201d\u7684\u6269\u5c55\u8303\u5f0f\u8f6c\u79fb**\uff1a**WideSeek-R1** \u7684\u7814\u7a76\u6781\u5177\u542f\u53d1\u6027\uff0c\u5b83\u6311\u6218\u4e86\u76ee\u524d\u4e3b\u6d41\u7684\u201c\u6df1\u5ea6\u6269\u5c55\u201d\uff08\u589e\u52a0\u63a8\u7406\u6b65\u6570\u6216\u6a21\u578b\u53c2\u6570\uff09\u8def\u7ebf\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u667a\u80fd\u4f53\u6570\u91cf\u8fdb\u884c\u7684\u201c\u5bbd\u5ea6\u6269\u5c55\u201d\u5728\u5e7f\u5ea6\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e0a\u5177\u6709\u6781\u9ad8\u7684\u6027\u4ef7\u6bd4\uff0c\u8fd9\u4e3a\u672a\u6765\u5229\u7528\u5c0f\u6a21\u578b\u96c6\u7fa4\u89e3\u51b3\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\n3.  **\u7ed3\u6784\u5316\u63a7\u5236\u5bf9\u6297\u5927\u6a21\u578b\u7684\u968f\u673a\u6027**\uff1a\u65e0\u8bba\u662f **CFSM/Soft-FSM** \u5f15\u5165\u7684\u6709\u9650\u72b6\u6001\u673a\uff0c\u8fd8\u662f **PCE** \u5f15\u5165\u7684\u51b3\u7b56\u6811\uff0c\u90fd\u663e\u793a\u51fa\u4e00\u79cd\u660e\u663e\u7684\u8d8b\u52bf\uff1a\u5728\u5173\u952e\u4efb\u52a1\uff08\u5982\u6cd5\u5f8b\u3001\u5ba1\u8ba1\u3001\u89d2\u8272\u626e\u6f14\uff09\u4e2d\uff0c\u5355\u7eaf\u4f9d\u8d56LLM\u7684\u6982\u7387\u751f\u6210\u662f\u4e0d\u591f\u7684\uff0c\u5fc5\u987b\u5f15\u5165\u663e\u5f0f\u7684\u3001\u53ef\u9a8c\u8bc1\u7684\u5916\u90e8\u7ed3\u6784\u6765\u7ea6\u675f\u884c\u4e3a\uff0c\u786e\u4fdd\u4efb\u52a1\u7684\u53ef\u9760\u5b8c\u6210\u3002\n4.  **\u81ea\u6211\u8fdb\u5316\u6b63\u5728\u6210\u4e3a\u73b0\u5b9e**\uff1a**GEA** \u548c **Empirical-MCTS** \u7b49\u5de5\u4f5c\u8868\u660e\uff0cAI\u667a\u80fd\u4f53\u6b63\u5728\u4ece\u201c\u9759\u6001\u6267\u884c\u8005\u201d\u5411\u201c\u52a8\u6001\u5b66\u4e60\u8005\u201d\u8f6c\u53d8\u3002\u901a\u8fc7\u7fa4\u4f53\u7ecf\u9a8c\u5171\u4eab\u548c\u8de8\u60c5\u8282\u7684\u5143\u5f3a\u5316\u5b66\u4e60\uff0c\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u4e86\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u53cd\u601d\u5b9e\u73b0\u80fd\u529b\u6301\u7eed\u63d0\u5347\u7684\u6f5c\u529b\uff0c\u8fd9\u6216\u8bb8\u662f\u901a\u5f80AGI\u7684\u5173\u952e\u4e00\u6b65\u3002",
    "2026-02-04": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-02-04)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u5c55\u73b0\u4e86AI\u7814\u7a76\u4ece\u9759\u6001\u6a21\u578b\u5411\u52a8\u6001\u3001\u8fdb\u5316\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7684\u6df1\u523b\u8f6c\u578b\u3002\u6838\u5fc3\u8d8b\u52bf\u663e\u793a\uff0c**\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09**\u6b63\u4ece\u5355\u7eaf\u7684\u6570\u91cf\u5806\u53e0\u8f6c\u5411\u8ffd\u6c42**\u5f02\u6784\u6027\u4e0e\u52a8\u6001\u534f\u4f5c**\uff0c\u800c**\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09**\u5df2\u6210\u4e3a\u8bad\u7ec3\u957f\u7a0b\u4efb\u52a1\u667a\u80fd\u4f53\u7684\u4e3b\u5bfc\u8303\u5f0f\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u667a\u80fd\u4f53\u201c\u5065\u5fd8\u201d\u548c\u201c\u5e7b\u89c9\u201d\u7684**\u8bb0\u5fc6\u67b6\u6784\u521b\u65b0**\uff0c\u4ee5\u53ca\u5728\u4ee3\u7801\u3001\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u5782\u76f4\u9886\u57df\u7684**\u4e13\u4e1a\u5316\u7a81\u7834**\uff0c\u5171\u540c\u6784\u6210\u4e86\u4eca\u65e5\u6280\u672f\u7248\u56fe\u7684\u5173\u952e\u62fc\u56fe\u3002\n\n---\n\n### \u534f\u4f5c\u8fdb\u5316\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u67b6\u6784\u4e0e\u6269\u5c55\n\n*   **[Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity]**\n    \u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u53cd\u76f4\u89c9\u73b0\u8c61\uff1a\u5355\u7eaf\u589e\u52a0\u540c\u8d28\u667a\u80fd\u4f53\u6570\u91cf\u4f1a\u9762\u4e34\u5f3a\u70c8\u7684\u8fb9\u9645\u6536\u76ca\u9012\u51cf\uff0c\u800c\u5f15\u5165**\u5f02\u6784\u6027**\uff08\u4e0d\u540c\u6a21\u578b\u3001\u63d0\u793a\u8bcd\u6216\u5de5\u5177\uff09\u80fd\u6301\u7eed\u5e26\u6765\u663e\u8457\u589e\u76ca\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4fe1\u606f\u8bba\u6846\u67b6\u548c\u6709\u6548\u901a\u9053\u6307\u6807 $K^*$\uff0c\u8bc1\u660e2\u4e2a\u591a\u6837\u5316\u667a\u80fd\u4f53\u7684\u8868\u73b0\u53ef\u5339\u654c16\u4e2a\u540c\u8d28\u667a\u80fd\u4f53\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548MAS\u63d0\u4f9b\u4e86\u201c\u591a\u6837\u6027\u4f18\u5148\u201d\u7684\u8bbe\u8ba1\u539f\u5219\u3002 (2602.03794 [cs.AI])\n\n*   **[AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration]**\n    \u9488\u5bf9\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u52a8\u6001\u62bd\u8c61\u89c6\u56fe\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**AOrchestra**\u6846\u67b6\u3002\u5b83\u5c06\u667a\u80fd\u4f53\u62bd\u8c61\u4e3a\u7edf\u4e00\u7684\u5143\u7ec4\uff0c\u7531\u4e2d\u592e\u7f16\u6392\u5668\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u521b\u5efa\u5b50\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u6309\u9700\u5206\u914d\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u5728GAIA\u548cSWE-Bench\u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002 (2602.03786 [cs.AI])\n\n*   **[Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems]**\n    \u53d7\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u5316\u8bbe\u8ba1\u542f\u53d1\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86**Agent Primitives**\uff0c\u5373\u4e00\u7ec4\u53ef\u91cd\u7528\u7684\u6f5c\u5728\u6784\u5efa\u5757\uff08\u5982\u5ba1\u67e5\u3001\u6295\u7968\u3001\u89c4\u5212\uff09\u3002\u901a\u8fc7KV Cache\u8fdb\u884c\u5185\u90e8\u901a\u4fe1\uff0c\u8be5\u67b6\u6784\u4e0d\u4ec5\u51cf\u5c11\u4e86Token\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u591a\u9636\u6bb5\u4ea4\u4e92\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u201c\u4e50\u9ad8\u79ef\u6728\u201d\u5f0f\u7684\u667a\u80fd\u4f53\u81ea\u52a8\u6784\u5efa\u3002 (2602.03695 [cs.AI])\n\n*   **[TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System]**\n    \u63d0\u51fa\u4e86**TodyComm**\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u56fa\u5b9a\u901a\u4fe1\u62d3\u6251\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff08\u5982\u5bf9\u6297\u653b\u51fb\u6216\u5e26\u5bbd\u9650\u5236\uff09\u7684\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u751f\u6210\u884c\u4e3a\u9a71\u52a8\u7684\u534f\u4f5c\u62d3\u6251\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u6548\u7528\u7684\u540c\u65f6\u4f18\u5316\u4e86\u901a\u4fe1\u6548\u7387\u3002 (2602.03688 [cs.AI])\n\n*   **[Scaling Small Agents Through Strategy Auctions]**\n    \u9762\u5bf9\u5c0f\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u74f6\u9888\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u53d7\u81ea\u7531\u804c\u4e1a\u5e02\u573a\u542f\u53d1\u7684**SALE**\u6846\u67b6\u3002\u667a\u80fd\u4f53\u901a\u8fc7\u7ade\u6807\u77ed\u671f\u6218\u7565\u8ba1\u5212\u6765\u4e89\u53d6\u4efb\u52a1\uff0c\u7cfb\u7edf\u6839\u636e\u6210\u672c-\u4ef7\u503c\u673a\u5236\u8fdb\u884c\u5206\u914d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u51cf\u5c11\u5bf9\u5927\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e35%\u7684\u603b\u4f53\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6700\u5f3a\u5355\u4e00\u6a21\u578b\u7684\u6027\u80fd\u3002 (2602.02751 [cs.AI])\n\n*   **[Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow]**\n    \u63d0\u51fa\u4e86**\u667a\u80fd\u4f53AI\u4e92\u8054\u7f51**\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u8de8\u4e91\u548c\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u3002\u8be5\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u7f51\u7edc\u539f\u751f\u534f\u4f5c\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u6fc0\u52b1\u517c\u5bb9\u7684\u5de5\u4f5c\u6d41\u8054\u76df\u53ef\u884c\u6027\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002 (2602.03145 [cs.AI])\n\n*   **[LatentMem: Customizing Latent Memory for Multi-Agent Systems]**\n    \u9488\u5bf9\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u4e2d\u7684\u540c\u8d28\u5316\u548c\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**LatentMem**\u3002\u8fd9\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u5e93\u548c\u8bb0\u5fc6\u5408\u6210\u5668\u751f\u6210\u7d27\u51d1\u7684\u3001\u7279\u5b9a\u4e8e\u667a\u80fd\u4f53\u7684\u6f5c\u5728\u8bb0\u5fc6\uff0c\u5e76\u901a\u8fc7**LMPO**\u7b97\u6cd5\u4f18\u5316\uff0c\u5728\u4e0d\u4fee\u6539\u5e95\u5c42\u6846\u67b6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002 (2602.03036 [cs.CL])\n\n---\n\n### \u81ea\u6211\u8fdb\u5316\uff1a\u5f3a\u5316\u5b66\u4e60\u4e0e\u667a\u80fd\u4f53\u8bad\u7ec3\u65b0\u8303\u5f0f\n\n*   **[RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents]**\n    \u9488\u5bf9\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u7ec4\u5185\u5956\u52b1\u53d8\u5f02\u4f4e\u5bfc\u81f4\u66f4\u65b0\u6d88\u5931\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**RC-GRPO**\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u5956\u52b1\u6807\u8bb0\uff0c\u5c06\u63a2\u7d22\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u5bfc\u5411\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728BFCLv4\u7b49\u591a\u8f6e\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u95ed\u6e90API\u6a21\u578b\u3002 (2602.03025 [cs.AI])\n\n*   **[Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis]**\n    \u63d0\u51fa\u4e86**Agentic Proposing**\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5408\u6210\u5efa\u6a21\u4e3a\u76ee\u6807\u9a71\u52a8\u7684\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\u3002\u901a\u8fc7**MGPO**\u7b97\u6cd5\u8bad\u7ec3\u51fa\u7684Agentic-Proposer-4B\u80fd\u591f\u751f\u6210\u9ad8\u7cbe\u5ea6\u3001\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u6570\u636e\uff0c\u4ec5\u752811,000\u6761\u8f68\u8ff9\u8bad\u7ec3\u768430B\u6c42\u89e3\u5668\u5728AIME25\u4e0a\u8fbe\u5230\u4e8691.6%\u7684SOTA\u51c6\u786e\u7387\u3002 (2602.03279 [cs.AI])\n\n*   **[SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training]**\n    \u53d1\u5e03\u4e86**SWE-Master**\uff0c\u4e00\u4e2a\u5f00\u6e90\u4e14\u53ef\u590d\u73b0\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u6846\u67b6\u3002\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8f68\u8ff9\u5408\u6210\u3001\u957f\u7a0bSFT\u548c\u57fa\u4e8e\u771f\u5b9e\u6267\u884c\u53cd\u9988\u7684RL\uff0c\u8be5\u6846\u67b6\u5728SWE-bench Verified\u4e0a\u8fbe\u5230\u4e8661.4%\u7684\u89e3\u51b3\u7387\uff0c\u5e76\u7ed3\u5408\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u81f370.8%\u3002 (2602.03411 [cs.CL])\n\n*   **[TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking]**\n    \u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5728\u826f\u6027\u4efb\u52a1\u8fdb\u5316\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684**\u8bb0\u5fc6\u8bef\u8fdb\u5316**\u73b0\u8c61\uff0c\u5373\u5b89\u5168\u6027\u968f\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u800c\u4e0b\u964d\u3002\u63d0\u51fa\u7684**TAME**\u6846\u67b6\u91c7\u7528\u53cc\u8bb0\u5fc6\u8fdb\u5316\u673a\u5236\uff0c\u5206\u522b\u8fdb\u5316\u6267\u884c\u8005\u8bb0\u5fc6\u548c\u8bc4\u4f30\u8005\u8bb0\u5fc6\uff0c\u5728\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u7f13\u89e3\u4e86\u5b89\u5168\u6027\u7684\u9000\u5316\u3002 (2602.03224 [cs.AI])\n\n*   **[MARS: Modular Agent with Reflective Search for Automated AI Research]**\n    \u9488\u5bf9**\u81ea\u52a8\u5316AI\u7814\u7a76**\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u548c\u6027\u80fd\u5f52\u56e0\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**MARS**\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\u9884\u7b97\u611f\u77e5\u7684MCTS\u89c4\u5212\u3001\u6a21\u5757\u5316\u6784\u5efa\u548c\u6bd4\u8f83\u53cd\u601d\u8bb0\u5fc6\uff0c\u5728MLE-Bench\u4e0a\u53d6\u5f97\u4e86\u5f00\u6e90SOTA\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u5206\u652f\u8fc1\u79fb\u7684\u201cAha!\u201d\u65f6\u523b\u3002 (2602.02660 [cs.AI])\n\n*   **[AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback]**\n    \u63d0\u51fa\u4e86**AERO**\u6846\u67b6\uff0c\u53d7\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d\u7406\u8bba\u542f\u53d1\uff0c\u901a\u8fc7\u71b5\u5b9a\u4f4d\u548c\u53cd\u4e8b\u5b9e\u4fee\u6b63\u5b9e\u73b0\u65e0\u76d1\u7763\u7684\u63a8\u7406\u8fdb\u5316\u3002\u8be5\u53cc\u73af\u7cfb\u7edf\u5185\u90e8\u5316\u81ea\u6211\u63d0\u95ee\u548c\u6279\u8bc4\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002 (2602.03084 [cs.CL])\n\n*   **[CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning]**\n    \u63d0\u51fa\u4e86**CPMobius**\uff0c\u4e00\u79cd\u534f\u4f5c\u5f0f\u7684\u6559\u7ec3-\u73a9\u5bb6\u8303\u5f0f\uff0c\u7528\u4e8e\u6570\u5b66\u63a8\u7406\u7684\u6570\u636e\u514d\u8d39\u5f3a\u5316\u5b66\u4e60\u3002\u6559\u7ec3\u751f\u6210\u9488\u5bf9\u73a9\u5bb6\u80fd\u529b\u7684\u6307\u4ee4\uff0c\u73a9\u5bb6\u89e3\u51b3\u6307\u4ee4\uff0c\u8fd9\u79cd\u95ed\u73af\u4f18\u5316\u5728\u4e0d\u4f9d\u8d56\u4efb\u4f55\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002 (2602.02979 [cs.CL])\n\n*   **[Verified Critical Step Optimization for LLM Agents]**\n    \u63d0\u51fa\u4e86**CSO**\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u90a3\u4e9b\u80fd\u51b3\u5b9a\u4efb\u52a1\u6210\u8d25\u7684**\u5df2\u9a8c\u8bc1\u5173\u952e\u6b65\u9aa4**\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u5173\u952e\u70b9\uff0c\u5e76\u4ec5\u4f7f\u7528\u90a3\u4e9b\u88ab\u7b56\u7565\u6a21\u578b\u6210\u529f\u6267\u884c\u7684\u4fee\u6b63\u8f68\u8ff9\u8fdb\u884cDPO\u8bad\u7ec3\uff0c\u4ee5\u6781\u5c11\u7684\u76d1\u7763\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002 (2602.03412 [cs.CL])\n\n---\n\n### \u8bb0\u5fc6\u4e0e\u63a7\u5236\uff1a\u7a81\u7834\u957f\u7a0b\u63a8\u7406\u7684\u4e0a\u4e0b\u6587\u74f6\u9888\n\n*   **[Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity]**\n    \u63d0\u51fa\u4e86**Memora**\uff0c\u4e00\u79cd\u8c10\u6ce2\u8bb0\u5fc6\u8868\u793a\uff0c\u65e8\u5728\u89e3\u51b3\u8bb0\u5fc6\u62bd\u8c61\u4e0e\u7279\u5f02\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5b83\u901a\u8fc7\u4e3b\u62bd\u8c61\u7d22\u5f15\u5177\u4f53\u503c\uff0c\u5e76\u5229\u7528\u7ebf\u7d22\u951a\u70b9\u8fde\u63a5\u76f8\u5173\u8bb0\u5fc6\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86RAG\u548cKG\u7cfb\u7edf\u662f\u5176\u7279\u4f8b\uff0c\u5728LoCoMo\u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u3002 (2602.03315 [cs.AI])\n\n*   **[InfMem: Learning System-2 Memory Control for Long-Context Agent]**\n    \u9488\u5bf9\u8d85\u957f\u6587\u6863\u63a8\u7406\u4e2d\u7684\u7a00\u758f\u8bc1\u636e\u6574\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**InfMem**\u3002\u5b83\u901a\u8fc7**PreThink-Retrieve-Write**\u534f\u8bae\u5b9e\u73b0System-2\u5f0f\u7684\u63a7\u5236\uff0c\u4e3b\u52a8\u76d1\u63a7\u8bc1\u636e\u5145\u5206\u6027\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u68c0\u7d22\uff0c\u5728\u5904\u7406\u957f\u8fbe1M token\u7684\u4e0a\u4e0b\u6587\u65f6\uff0c\u5c06\u51c6\u786e\u7387\u63d0\u5347\u4e86\u8d85\u8fc710\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002 (2602.02704 [cs.CL])\n\n*   **[SEAM: Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs]**\n    \u63d0\u51fa\u4e86**SEAM**\u6a21\u5757\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u7279\u5b9a\u4e8e\u6267\u884c\u5668\u7684\u63d2\u4ef6\u3002\u5b83\u5c06\u7ecf\u9a8c\u5b58\u50a8\u5728\u53c2\u6570\u4e2d\u5e76\u751f\u6210\u7ed3\u6784\u5316\u7684\u7ecf\u9a8c\u6761\u76ee\u6765\u6307\u5bfc\u51bb\u7ed3\u7684LLM\uff0c\u901a\u8fc7GRPO\u8fdb\u884c\u6548\u7528\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4ee5\u4f4e\u5f00\u9500\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002 (2602.02556 [cs.AI])\n\n*   **[Mitigating Conversational Inertia in Multi-Turn Agents]**\n    \u8bc6\u522b\u5e76\u547d\u540d\u4e86\u591a\u8f6e\u667a\u80fd\u4f53\u4e2d\u7684**\u5bf9\u8bdd\u60ef\u6027**\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u503e\u5411\u4e8e\u6a21\u4eff\u81ea\u5df1\u5148\u524d\u7684\u54cd\u5e94\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u504f\u597d\u5b66\u4e60\u6765\u6821\u51c6\u6a21\u578b\u504f\u597d\uff0c\u4f7f\u5176\u503e\u5411\u4e8e\u4f4e\u60ef\u6027\u54cd\u5e94\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u63d0\u5347\u4e86\u63a2\u7d22\u80fd\u529b\u548c\u6027\u80fd\u3002 (2602.03664 [cs.AI])\n\n---\n\n### \u5782\u76f4\u6df1\u8015\uff1a\u4ee3\u7801\u3001\u6570\u5b66\u4e0eGUI\u667a\u80fd\u4f53\u7684\u4e13\u4e1a\u5316\u7a81\u7834\n\n*   **[Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents]**\n    \u63d0\u51fa\u4e86**Agent Alpha**\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7edf\u4e00\u4e86\u751f\u6210\u3001\u63a2\u7d22\u548c\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u5141\u8bb8\u4e3b\u52a8\u5efa\u6a21\u89c4\u5212\u7a7a\u95f4\u7ed3\u6784\uff0c\u901a\u8fc7\u524d\u7f00\u91cd\u7528\u548c\u65e9\u671f\u526a\u679d\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u7ea677%\u7684SOTA\u6210\u529f\u7387\u3002 (2602.02995 [cs.AI])\n\n*   **[STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models]**\n    \u63d0\u51fa\u4e86**STAR**\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u5927\u6a21\u578b\u7684\u80fd\u529b\u8fc1\u79fb\u5230\u8d85\u5c0f\u6a21\u578b\uff08\u59820.6B\uff09\u3002\u901a\u8fc7\u7ea6\u675f\u77e5\u8bc6\u84b8\u998f\u548c\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684RL\uff0c\u8be5\u6846\u67b6\u5728\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86SOTA\uff0c\u8bc1\u660e\u4e860.6B\u6a21\u578b\u4e5f\u80fd\u5177\u5907\u5353\u8d8a\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u3002 (2602.03022 [cs.AI])\n\n*   **[SWE-World: Building Software Engineering Agents in Docker-Free Environments]**\n    \u63d0\u51fa\u4e86**SWE-World**\uff0c\u4e00\u4e2a\u65e0Docker\u6846\u67b6\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u4ee3\u7406\u6a21\u578b\u66ff\u4ee3\u7269\u7406\u6267\u884c\u73af\u5883\u3002\u8fd9\u4e0d\u4ec5\u6d88\u9664\u4e86\u7ef4\u62a4\u5bb9\u5668\u73af\u5883\u7684\u6210\u672c\uff0c\u8fd8\u901a\u8fc7\u6a21\u62df\u53cd\u9988\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u5c06Qwen2.5-Coder-32B\u5728SWE-bench Verified\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u81f368.2%\u3002 (2602.03419 [cs.CL])\n\n*   **[CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability]**\n    \u53d1\u5e03\u4e86**CVE-Factory**\uff0c\u9996\u4e2a\u80fd\u5c06\u7a00\u758fCVE\u5143\u6570\u636e\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4e13\u5bb6\u7ea7\u4efb\u52a1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u8be5\u7814\u7a76\u8fd8\u6784\u5efa\u4e86LiveCVEBench\u57fa\u51c6\uff0c\u5e76\u5408\u6210\u4e86\u8d85\u8fc71000\u4e2a\u8bad\u7ec3\u73af\u5883\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u771f\u5b9e\u6f0f\u6d1e\u5229\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002 (2602.03012 [cs.AI])\n\n*   **[ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents]**\n    \u63d0\u51fa\u4e86**ToolTok**\uff0c\u4e00\u79cd\u5c06\u64cd\u4f5c\u5efa\u6a21\u4e3a\u6e10\u8fdb\u5f0f\u5de5\u5177\u4f7f\u7528\u5e8f\u5217\u7684\u65b0\u8303\u5f0f\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Token\u5d4c\u5165\u548c\u8bed\u4e49\u951a\u5b9a\u673a\u5236\uff0c\u8be5\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u4e0d\u52301%\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2aGUI\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002 (2602.02548 [cs.AI])\n\n*   **[AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents]**\n    \u63d0\u51fa\u4e86**AutoSizer**\uff0c\u4e00\u4e2a\u53cd\u601d\u6027\u7684LLM\u5143\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u6df7\u5408\u4fe1\u53f7\u7535\u8def\u7684\u6676\u4f53\u7ba1\u5c3a\u5bf8\u8c03\u6574\u3002\u901a\u8fc7\u53cc\u73af\u4f18\u5316\u548c\u641c\u7d22\u7a7a\u95f4\u81ea\u9002\u5e94\u7ec6\u5316\uff0c\u8be5\u6846\u67b6\u5728SKY130\u5de5\u827a\u4e0b\u7684\u591a\u4e2a\u7535\u8def\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edfEDA\u65b9\u6cd5\u548c\u73b0\u6709LLM\u667a\u80fd\u4f53\u3002 (2602.02849 [cs.AI])\n\n*   **[FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation]**\n    \u63d0\u51fa\u4e86**FullStack-Agent**\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7801\u667a\u80fd\u4f53\u4ec5\u751f\u6210\u524d\u7aef\u9875\u9762\u800c\u7f3a\u4e4f\u5168\u6808\u6570\u636e\u5904\u7406\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f00\u53d1\u5bfc\u5411\u7684\u6d4b\u8bd5\u548c\u4ed3\u5e93\u53cd\u5411\u7ffb\u8bd1\uff0c\u8be5\u7cfb\u7edf\u5728\u524d\u7aef\u3001\u540e\u7aef\u548c\u6570\u636e\u5e93\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u5747\u663e\u8457\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684SOTA\u65b9\u6cd5\u3002 (2602.03798 [cs.CL])\n\n---\n\n### \u8bc4\u4f30\u4e0e\u4ea4\u4e92\uff1a\u6784\u5efa\u667a\u80fd\u4f53\u7684\u57fa\u77f3\n\n*   **[PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review]**\n    \u63d0\u51fa\u4e86**PeerRank**\uff0c\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u6846\u67b6\u3002\u6a21\u578b\u4e92\u4e3a\u4efb\u52a1\u8bbe\u8ba1\u8005\u3001\u56de\u7b54\u8005\u548c\u8bc4\u4f30\u8005\uff0c\u901a\u8fc7\u57fa\u4e8e\u7f51\u7edc\u7684\u56de\u7b54\u548c\u504f\u89c1\u63a7\u5236\u7684\u5bf9\u7b49\u8bc4\u5ba1\u751f\u6210\u7a33\u5b9a\u7684\u6027\u80fd\u6392\u540d\uff0c\u65e0\u9700\u4eba\u7c7b\u76d1\u7763\u6216\u9ec4\u91d1\u53c2\u8003\u3002 (2602.02589 [cs.AI])\n\n*   **[DiscoverLLM: From Executing Intents to Discovering Them]**\n    \u63d0\u51fa\u4e86**DiscoverLLM**\uff0c\u65e8\u5728\u5e2e\u52a9\u7528\u6237\u53d1\u73b0\u548c\u5f62\u6210\u4ed6\u4eec\u5c1a\u672a\u660e\u786e\u8868\u8fbe\u7684\u610f\u56fe\u3002\u901a\u8fc7\u6a21\u62df\u7528\u6237\u7684\u8ba4\u77e5\u72b6\u6001\u5c42\u7ea7\uff0c\u6a21\u578b\u5b66\u4f1a\u4e86\u5728\u610f\u56fe\u4e0d\u6e05\u65f6\u53d1\u6563\u63a2\u7d22\uff0c\u5728\u610f\u56fe\u5177\u4f53\u5316\u65f6\u6536\u655b\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6ee1\u610f\u5ea6\u548c\u5bf9\u8bdd\u6548\u7387\u3002 (2602.03429 [cs.AI])\n\n*   **[A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces]**\n    \u63d0\u51fa\u4e86**A-RAG**\uff0c\u5c06\u5206\u5c42\u68c0\u7d22\u63a5\u53e3\u76f4\u63a5\u66b4\u9732\u7ed9\u6a21\u578b\u3002\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5355\u6b21\u68c0\u7d22\u6216\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0cA-RAG\u5141\u8bb8\u6a21\u578b\u81ea\u4e3b\u51b3\u5b9a\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u3001\u8bed\u4e49\u641c\u7d22\u8fd8\u662f\u5757\u8bfb\u53d6\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u6a21\u578b\u80fd\u529b\u8fdb\u884c\u52a8\u6001\u68c0\u7d22\u3002 (2602.03442 [cs.CL])\n\n*   **[Test-time Recursive Thinking: Self-Improvement without External Feedback]**\n    \u63d0\u51fa\u4e86**TRT**\uff0c\u4e00\u79cd\u6d4b\u8bd5\u65f6\u9012\u5f52\u601d\u8003\u6846\u67b6\u3002\u901a\u8fc7\u57fa\u4e8e\u7279\u5b9a\u7b56\u7565\u3001\u7d2f\u79ef\u77e5\u8bc6\u548c\u81ea\u751f\u6210\u9a8c\u8bc1\u4fe1\u53f7\u7684\u8fed\u4ee3\u751f\u6210\uff0c\u6a21\u578b\u5728\u65e0\u9700\u5916\u90e8\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u81ea\u6211\u6539\u8fdb\uff0c\u5728AIME\u548cLiveCodeBench\u7b49\u56f0\u96be\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86100%\u51c6\u786e\u7387\u6216\u663e\u8457\u63d0\u5347\u3002 (2602.03094 [cs.CL])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **\u201c\u591a\u6837\u6027\u201d\u6218\u80dc\u201c\u6570\u91cf\u201d**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982 *Understanding Agent Scaling*, *Scaling Small Agents*\uff09\u5171\u540c\u6307\u5411\u4e00\u4e2a\u6838\u5fc3\u89c2\u70b9\u2014\u2014\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u76f2\u76ee\u5806\u780c\u6a21\u578b\u6570\u91cf\u6216\u53c2\u6570\u89c4\u6a21\u5df2\u4e0d\u518d\u662f\u6027\u4ef7\u6bd4\u6700\u9ad8\u7684\u8def\u5f84\u3002\u65e0\u8bba\u662f\u901a\u8fc7\u5f02\u6784\u667a\u80fd\u4f53\u7684\u534f\u4f5c\uff0c\u8fd8\u662f\u901a\u8fc7\u7b56\u7565\u62cd\u5356\u673a\u5236\u8c03\u5ea6\u5c0f\u6a21\u578b\uff0c**\u7cfb\u7edf\u7ea7\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8d44\u6e90\u8c03\u5ea6**\u6b63\u5c55\u73b0\u51fa\u6bd4\u5355\u7eaf\u6a21\u578b\u7f29\u653e\u66f4\u5f3a\u5927\u7684\u6f5c\u529b\u3002\n*   **RL\u5728\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u7684\u5168\u9762\u590d\u5174**\uff1a\u4ece *RC-GRPO* \u5230 *SWE-Master*\uff0c\u518d\u5230 *CPMobius*\uff0c\u5f3a\u5316\u5b66\u4e60\uff08\u7279\u522b\u662f\u7ed3\u5408\u4e86DPO\u3001GRPO\u7b49\u53d8\u4f53\u7684RL\uff09\u5df2\u6210\u4e3a\u8bad\u7ec3\u957f\u7a0b\u3001\u590d\u6742\u667a\u80fd\u4f53\u4efb\u52a1\u7684\u6807\u51c6\u8303\u5f0f\u3002\u7814\u7a76\u91cd\u70b9\u5df2\u4ece\u5355\u7eaf\u7684SFT\u8f6c\u5411\u5229\u7528\u73af\u5883\u53cd\u9988\u3001\u8fc7\u7a0b\u5956\u52b1\u548c\u81ea\u6211\u535a\u5f08\u8fdb\u884c**\u7b56\u7565\u4f18\u5316**\uff0c\u8fd9\u6807\u5fd7\u7740\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u8bba\u7684\u91cd\u5927\u6210\u719f\u3002\n*   **\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u6781\u81f4\u5316**\uff1a*Agent Alpha* \u7684MCTS\u641c\u7d22\u3001*TRT*\u7684\u9012\u5f52\u601d\u8003\u4ee5\u53ca *SWE-World* \u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u90fd\u8868\u660e**\u63a8\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u6295\u5165**\u6b63\u6210\u4e3a\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u6760\u6746\u3002\u4e0e\u5176\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u65e0\u4f11\u6b62\u5730\u70e7\u94b1\uff0c\u4e0d\u5982\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u641c\u7d22\u3001\u53cd\u601d\u548c\u9a8c\u8bc1\u6765\u6362\u53d6\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002\n*   **\u5782\u76f4\u9886\u57df\u7684\u201c\u4e13\u5bb6\u667a\u80fd\u4f53\u201d\u5d1b\u8d77**\uff1a\u4ece\u7535\u8def\u8bbe\u8ba1 (*AutoSizer*) \u5230CAD\u7ed8\u56fe (*ProCAD*)\uff0c\u518d\u5230\u4ee3\u7801\u5b89\u5168 (*CVE-Factory*)\uff0c\u901a\u7528\u5927\u6a21\u578b\u6b63\u901a\u8fc7\u7279\u5b9a\u7684\u6846\u67b6\u548c\u5de5\u5177\u94fe\u6f14\u53d8\u4e3a**\u9886\u57df\u4e13\u5bb6**\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u4e0d\u4ec5\u638c\u63e1\u4e86\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd8\u5b66\u4f1a\u4e86\u8be5\u9886\u57df\u7279\u6709\u7684\u5de5\u4f5c\u6d41\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u9884\u793a\u7740AI\u5728\u4e13\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5373\u5c06\u8fce\u6765\u7206\u53d1\u3002",
    "2026-02-03": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-02-03)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u5c55\u73b0\u4e86AI\u7814\u7a76\u4ece\u201c\u9759\u6001\u6a21\u578b\u201d\u5411\u201c\u52a8\u6001\u667a\u80fd\u4f53\u201d\u5168\u9762\u8f6c\u578b\u7684\u8d8b\u52bf\u3002\u6838\u5fc3\u7126\u70b9\u96c6\u4e2d\u5728**\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7f16\u6392\u4e0e\u534f\u4f5c**\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u8bd5\u56fe\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u3001\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u548c\u535a\u5f08\u8bba\u673a\u5236\u6765\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u957f\u7a0b\u89c4\u5212\u95ee\u9898\u3002\u540c\u65f6\uff0c**\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u4e0e\u8bb0\u5fc6\u673a\u5236**\u6210\u4e3a\u70ed\u70b9\uff0c\u5927\u91cf\u5de5\u4f5c\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u7ebf\u53cd\u9988\u548c\u7ecf\u9a8c\u79ef\u7d2f\u5b9e\u73b0\u80fd\u529b\u7684\u6301\u7eed\u63d0\u5347\u3002\u6b64\u5916\uff0c**\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u641c\u7d22\u7b97\u6cd5**\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u5e94\u7528\u56de\u5f52\uff0c\u4e3a\u63d0\u5347\u63a8\u7406\u6df1\u5ea6\u548c\u5de5\u5177\u4f7f\u7528\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002\n\n---\n\n### \u4e00\u3001 \u4ece\u5355\u4f53\u5230\u7fa4\u4f53\uff1a\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7684\u65b0\u8303\u5f0f\n\n\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u63d0\u5347\uff0c\u5355\u4e00\u667a\u80fd\u4f53\u5df2\u96be\u4ee5\u5e94\u5bf9\uff0c\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u5f3a\u8c03\u89d2\u8272\u5206\u5de5\u3001\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u4ee5\u53ca\u7ed3\u6784\u5316\u7684\u4ea4\u4e92\u6d41\u7a0b\u3002\n\n*   **ROMA (Recursive Open Meta-Agent Framework)** \u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u4efb\u52a1\u5206\u89e3\u548c\u7ed3\u6784\u5316\u805a\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u89d2\u8272\uff08Atomizer, Planner, Executor, Aggregator\uff09\u5206\u79bb\u7f16\u6392\u4e0e\u6a21\u578b\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u5728\u957f\u7a0b\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9\u4e0a\u4e0b\u6587\u589e\u957f\u7684\u63a7\u5236\u548c\u900f\u660e\u5316\u7684\u6267\u884c\u8f68\u8ff9\u3002 (2602.01848 [cs.AI])\n*   **ORCH** \u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u91c7\u7528\u201c\u591a\u6b21\u5206\u6790\uff0c\u4e00\u6b21\u51b3\u7b56\u201d\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u56fa\u5b9a\u89c4\u5219\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u7b54\u6848\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u968f\u673a\u8def\u7531\u5bfc\u81f4\u7684\u4e0d\u53ef\u590d\u73b0\u95ee\u9898\uff0c\u5728MMLU-Pro\u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002 (2602.01797 [cs.AI])\n*   **Symphony-Coord** \u5f15\u5165\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u9009\u62e9\u8f6c\u5316\u4e3a\u5728\u7ebf\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5229\u7528\u52a8\u6001\u4fe1\u6807\u534f\u8bae\u548c\u81ea\u9002\u5e94LinUCB\u9009\u62e9\u5668\uff0c\u4f7f\u89d2\u8272\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u6709\u673a\u6d8c\u73b0\uff0c\u800c\u975e\u9759\u6001\u5206\u914d\u3002 (2602.00966 [cs.MA])\n*   **Agyn** \u5c06\u8f6f\u4ef6\u5de5\u7a0b\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ec4\u7ec7\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u5de5\u7a0b\u56e2\u961f\u7684\u7ed3\u6784\uff08\u534f\u8c03\u3001\u7814\u7a76\u3001\u5b9e\u73b0\u3001\u5ba1\u67e5\u7b49\u89d2\u8272\uff09\uff0c\u6784\u5efa\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728SWE-bench 500\u4e0a\u8fbe\u5230\u4e8672.4%\u7684\u89e3\u51b3\u7387\u3002 (2602.01465 [cs.AI])\n*   **INDIBATOR** \u9488\u5bf9\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u53d1\u8868\u5386\u53f2\u548c\u5206\u5b50\u5386\u53f2\u7684\u4e2a\u6027\u5316\u79d1\u5b66\u5bb6\u753b\u50cf\uff0c\u8d4b\u4e88\u667a\u80fd\u4f53\u7ec6\u7c92\u5ea6\u7684\u201c\u79d1\u5b66DNA\u201d\uff0c\u5728\u591a\u8f6e\u8fa9\u8bba\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u7c97\u7c92\u5ea6\u89d2\u8272\u7684\u7cfb\u7edf\u3002 (2602.01815 [cs.AI])\n\n### \u4e8c\u3001 \u6253\u7834\u9759\u6001\u9650\u5236\uff1a\u8bb0\u5fc6\u8fdb\u5316\u4e0e\u7ec8\u8eab\u5b66\u4e60\n\n\u5982\u4f55\u8ba9\u667a\u80fd\u4f53\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u81ea\u6211\u8fdb\u5316\uff0c\u662f\u4eca\u65e5\u7814\u7a76\u7684\u53e6\u4e00\u5927\u4e3b\u9898\u3002\u8bba\u6587\u4eec\u63d0\u51fa\u4e86\u5404\u79cd\u8bb0\u5fc6\u673a\u5236\u548c\u5728\u7ebf\u5b66\u4e60\u7b56\u7565\u3002\n\n*   **Live-Evo** \u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u81ea\u6211\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u201c\u53d1\u751f\u4e86\u4ec0\u4e48\u201d\u4e0e\u201c\u5982\u4f55\u4f7f\u7528\u5b83\u201d\u89e3\u8026\uff0c\u5229\u7528\u7ecf\u9a8c\u94f6\u884c\u548c\u5143\u6307\u5bfc\u94f6\u884c\uff0c\u5e76\u6839\u636e\u53cd\u9988\u52a8\u6001\u8c03\u6574\u7ecf\u9a8c\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u5728\u8fde\u7eed\u6570\u636e\u6d41\u4e0a\u7684\u6301\u7eed\u9002\u5e94\u3002 (2602.02369 [cs.AI])\n*   **ProcMEM** \u5141\u8bb8\u667a\u80fd\u4f53\u901a\u8fc7\u975e\u53c2\u6570PPO\u4ece\u4ea4\u4e92\u7ecf\u9a8c\u4e2d\u81ea\u4e3b\u5b66\u4e60\u7a0b\u5e8f\u5316\u8bb0\u5fc6\uff0c\u5c06\u88ab\u52a8\u7684\u53d9\u8ff0\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5206\u6570\u7684\u7ef4\u62a4\u673a\u5236\u786e\u4fdd\u8bb0\u5fc6\u7684\u9ad8\u53ef\u91cd\u7528\u6027\u3002 (2602.01869 [cs.AI])\n*   **MemSkill** \u5c06\u8bb0\u5fc6\u64cd\u4f5c\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u548c\u8fdb\u5316\u7684\u201c\u8bb0\u5fc6\u6280\u80fd\u201d\uff0c\u901a\u8fc7\u63a7\u5236\u5668\u9009\u62e9\u6280\u80fd\uff0c\u6267\u884c\u5668\u751f\u6210\u8bb0\u5fc6\uff0c\u5e76\u5f15\u5165\u8bbe\u8ba1\u5668\u5b9a\u671f\u5ba1\u67e5\u548c\u8fdb\u5316\u6280\u80fd\u96c6\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u95ed\u73af\u7684\u81ea\u6211\u8fdb\u5316\u7cfb\u7edf\u3002 (2602.02474 [cs.AI])\n*   **Self-Consolidation** \u63d0\u51fa\u4e86\u4e00\u79cd\u4e92\u8865\u7684\u8fdb\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u603b\u7ed3\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u5c06\u975e\u53c2\u6570\u7684\u6587\u672c\u7ecf\u9a8c\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5c06\u5927\u91cf\u5386\u53f2\u7ecf\u9a8c\u5185\u5316\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002 (2602.01966 [cs.LG])\n*   **Position: Agentic Evolution is the Path to Evolving LLMs** \u662f\u4e00\u7bc7\u89c2\u70b9\u6027\u8bba\u6587\uff0c\u4e3b\u5f20\u5c06\u8fdb\u5316\u672c\u8eab\u63d0\u5347\u4e3a\u4e00\u79cd\u81ea\u4e3b\u7684\u8fdb\u5316\u8005\u667a\u80fd\u4f53\uff0c\u63d0\u51fa\u4e86\u201c\u8fdb\u5316\u6269\u5c55\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u9002\u5e94\u80fd\u529b\u4e0e\u5206\u914d\u7ed9\u8fdb\u5316\u7684\u7b97\u529b\u6210\u6b63\u6bd4\u3002 (2602.00359 [cs.AI])\n\n### \u4e09\u3001 \u63a8\u7406\u5373\u641c\u7d22\uff1a\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e0eRL\u7684\u56de\u5f52\n\n\u4e3a\u4e86\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u7814\u7a76\u8005\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u641c\u7d22\u7b97\u6cd5\u5728\u6d4b\u8bd5\u65f6\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u91cf\u6765\u6362\u53d6\u6027\u80fd\u63d0\u5347\u7684\u65b9\u6cd5\u3002\n\n*   **ASTER (Agentic Scaling with Tool-integrated Extended Reasoning)** \u89e3\u51b3\u4e86\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u7684\u201c\u4ea4\u4e92\u5d29\u6e83\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5148\u8003\u8651\u4ea4\u4e92\u5bc6\u96c6\u7684\u51b7\u542f\u52a8\u8f68\u8ff9\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u884c\u4e3a\u5148\u9a8c\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86SOTA\u3002 (2602.01204 [cs.CL])\n*   **A-MapReduce** \u53d7MapReduce\u8303\u5f0f\u542f\u53d1\uff0c\u5c06\u5927\u89c4\u6a21\u5e7f\u5ea6\u641c\u7d22\u4efb\u52a1\u91cd\u6784\u4e3a\u6c34\u5e73\u7ed3\u6784\u5316\u68c0\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u548c\u7ecf\u9a8c\u8bb0\u5fc6\u9a71\u52a8\u7684\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u57df\u641c\u7d22\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002 (2602.01331 [cs.CL])\n*   **FlowSteer** \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u4e0e\u53ef\u6267\u884c\u753b\u5e03\u73af\u5883\u7684\u591a\u8f6e\u4ea4\u4e92\uff0c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u5e76\u63d0\u51fa\u4e86CWRPO\u7b97\u6cd5\u4ee5\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b\u3002 (2602.01664 [cs.AI])\n*   **DeepControl** \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u6548\u7528\u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5ef6\u7eed\u548c\u7c92\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u8c03\u8282\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u83b7\u53d6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7ed3\u679c\u7684RL\u57fa\u7ebf\u3002 (2602.01672 [cs.CL])\n*   **CodePilot** \u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4e0eLLM\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u641c\u7d22\u548c\u7ec6\u5316\uff0c\u5728SWE-bench Lite\u4e0a\u5b9e\u73b0\u4e8624.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387\u3002 (2602.00129 [cs.LG])\n\n### \u56db\u3001 \u8fc8\u5411\u53ef\u9760\uff1a\u81ea\u6211\u4fee\u6b63\u4e0e\u5de5\u5177\u4f7f\u7528\u4f18\u5316\n\n\u63d0\u9ad8\u667a\u80fd\u4f53\u7684\u9c81\u68d2\u6027\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6548\u7387\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\uff0c\u4eca\u65e5\u7684\u7814\u7a76\u6db5\u76d6\u4e86\u4ece\u71b5\u63a7\u5236\u5230\u6a21\u62df\u9a8c\u8bc1\u7684\u591a\u79cd\u6280\u672f\u624b\u6bb5\u3002\n\n*   **Structure Enables Effective Self-Localization of Errors in LLMs** \u53d1\u73b0\u5c06\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u6b65\u9aa4\u540e\uff0c\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u5b9a\u4f4d\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u4e86**Thought-ICS**\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u56de\u6eaf\u548c\u91cd\u91c7\u6837\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u81ea\u4fee\u6b63\u63d0\u5347\u3002 (2602.02416 [cs.AI])\n*   **Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors** \u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u71b5\u51cf\u5c11\u4e0e\u9ad8\u8d28\u91cf\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u5b58\u5728\u5f3a\u6b63\u76f8\u5173\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u51cf\u5c11\u7684\u76d1\u7763\u4fe1\u53f7\u548c\u7a00\u758f/\u5bc6\u96c6\u5956\u52b1\u7b56\u7565\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u5197\u4f59\u7684\u5de5\u5177\u8c03\u7528\u3002 (2602.02050 [cs.AI])\n*   **ARTIS (Agentic Risk-Aware Test-Time Scaling)** \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u5c06\u63a2\u7d22\u4e0e\u627f\u8bfa\u89e3\u8026\u7684\u6846\u67b6\uff0c\u5f15\u5165\u98ce\u9669\u611f\u77e5\u5de5\u5177\u6a21\u62df\u5668\uff0c\u5728\u771f\u5b9e\u6267\u884c\u524d\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u63d0\u9ad8\u52a8\u4f5c\u7684\u53ef\u9760\u6027\u3002 (2602.01709 [cs.CL])\n*   **GASP (Guided Adversarial Self-Play)** \u901a\u8fc7\u5728\u5355\u4e2a\u6a21\u578b\u5185\u90e8\u5f62\u6210\u5bf9\u6297\u6027\u81ea\u6211\u535a\u5f08\uff08\u6c61\u67d3\u8005 vs \u4fee\u590d\u8005\uff09\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u8bad\u7ec3\u6a21\u578b\u7684\u68c0\u6d4b\u548c\u4fee\u590d\u80fd\u529b\uff0c\u4f7f\u6a21\u578b\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u65f6\u66f4\u5177\u9c81\u68d2\u6027\u3002 (2602.00173 [cs.AI])\n*   **Avenir-Web** \u5229\u7528**\u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6**\u548c\u7ecf\u9a8c\u6a21\u4eff\u89c4\u5212\uff0c\u89e3\u51b3\u4e86Web\u667a\u80fd\u4f53\u5728\u590d\u6742\u52a8\u6001\u754c\u9762\u4e2d\u7684\u5143\u7d20\u5b9a\u4f4d\u4e0d\u51c6\u786e\u548c\u957f\u671f\u8bb0\u5fc6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728Online-Mind2Web\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u5f00\u6e90SOTA\u3002 (2602.02468 [cs.AI])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **\u667a\u80fd\u4f53\u8fdb\u5316\u7684\u65b0\u8303\u5f0f**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982 *Live-Evo*, *ProcMEM*, *MemSkill*\uff09\u5171\u540c\u6307\u5411\u4e86\u4e00\u4e2a\u660e\u786e\u7684\u8d8b\u52bf\u2014\u2014AI\u667a\u80fd\u4f53\u6b63\u5728\u4ece\u201c\u4e00\u6b21\u6027\u63a8\u7406\u201d\u8f6c\u5411\u201c\u7ec8\u8eab\u8fdb\u5316\u201d\u3002\u901a\u8fc7\u975e\u53c2\u6570\u8bb0\u5fc6\u3001\u6280\u80fd\u8fdb\u5316\u548c\u5728\u7ebf\u53cd\u9988\u673a\u5236\uff0c\u667a\u80fd\u4f53\u6b63\u5728\u83b7\u5f97\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u5e95\u5c42\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u73af\u5883\u7684\u80fd\u529b\uff0c\u8fd9\u53ef\u80fd\u662f\u901a\u5411AGI\u7684\u5173\u952e\u4e00\u6b65\u3002\n*   **RL\u5728\u6d4b\u8bd5\u65f6\u7684\u5f3a\u52bf\u56de\u5f52**\uff1a\u4e0e\u5355\u7eaf\u4f9d\u8d56Prompt Engineering\u4e0d\u540c\uff0c*ASTER*, *FlowSteer*, *DeepControl* \u7b49\u5de5\u4f5c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u6b63\u5728\u88ab\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u201c\u6d4b\u8bd5\u65f6\u6269\u5c55\u201d\u6280\u672f\u3002\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff08\u5982\u641c\u7d22\u3001\u6a21\u62df\u3001\u591a\u8f6e\u4ea4\u4e92\uff09\uff0c\u6a21\u578b\u80fd\u591f\u7a81\u7834\u9759\u6001\u53c2\u6570\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u6027\u80fd\u7684\u8dc3\u5347\u3002\n*   **\u786e\u5b9a\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u8ffd\u6c42**\uff1a\u5728\u591a\u667a\u80fd\u4f53\u9886\u57df\uff0c*ORCH* \u63d0\u51fa\u7684\u786e\u5b9a\u6027\u534f\u8c03\u6846\u67b6\u548c *ROMA* \u7684\u900f\u660e\u5316\u8f68\u8ff9\uff0c\u53cd\u6620\u4e86\u4e1a\u754c\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u7684\u62c5\u5fe7\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u9664\u4e86\u6027\u80fd\uff0c\u884c\u4e3a\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6b63\u53d8\u5f97\u4e0e\u51c6\u786e\u6027\u540c\u7b49\u91cd\u8981\u3002\n*   **\u71b5\u4f5c\u4e3a\u4f18\u5316\u4fe1\u53f7**\uff1a*Rethinking the Role of Entropy...* \u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u6709\u8da3\u7684\u89c6\u89d2\uff0c\u5373\u5229\u7528\u201c\u71b5\u51cf\u5c11\u201d\u4f5c\u4e3a\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u7684\u5185\u5728\u5956\u52b1\u3002\u8fd9\u4e00\u53d1\u73b0\u4e0d\u4ec5\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u7684\u6548\u7387\uff0c\u4e5f\u4e3a\u7406\u89e3LLM\u5185\u90e8\u7684\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4f9d\u636e\u3002",
    "2026-02-02": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-02-02)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: peer closed connection without sending complete message body (incomplete chunked read)",
    "2026-01-30": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-30)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u5c55\u73b0\u4e86AI\u667a\u80fd\u4f53\u4ece\u201c\u5355\u4f53\u6a21\u578b\u201d\u5411\u201c\u590d\u6742\u7cfb\u7edf\u201d\u6f14\u8fdb\u7684\u660e\u786e\u8d8b\u52bf\u3002\u7814\u7a76\u91cd\u70b9\u4e0d\u518d\u5c40\u9650\u4e8e\u63d0\u5347\u5355\u4e00\u6a21\u578b\u7684\u53c2\u6570\u89c4\u6a21\uff0c\u800c\u662f\u8f6c\u5411\u5982\u4f55\u901a\u8fc7**\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09**\u3001**\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b**\u548c**\u591a\u667a\u80fd\u4f53\u534f\u4f5c**\u6765\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3001\u5de5\u5177\u4f7f\u7528\u6548\u7387\u53ca\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u3002\u7279\u522b\u662f\u5728Agentic RL\u9886\u57df\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u5173\u4e8e\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u3001\u63d0\u5347\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u5b9e\u73b0\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u4f18\u5316\u7684\u5de5\u4f5c\uff0c\u6807\u5fd7\u7740AI\u667a\u80fd\u4f53\u6b63\u671d\u7740\u66f4\u9ad8\u6548\u3001\u66f4\u81ea\u4e3b\u3001\u66f4\u5177\u534f\u4f5c\u6027\u7684\u65b9\u5411\u53d1\u5c55\u3002\n\n---\n\n### Agentic RL \u4e0e\u63a8\u7406\u4f18\u5316\uff1a\u4ece\u7ed3\u679c\u5bfc\u5411\u5230\u8fc7\u7a0b\u9a71\u52a8\n\n\u8fd9\u4e00\u677f\u5757\u7684\u7814\u7a76\u81f4\u529b\u4e8e\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u96be\u9898\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u5956\u52b1\u4fe1\u53f7\u3001\u6a21\u578b\u5316\u7684\u73af\u5883\u4ea4\u4e92\u4ee5\u53ca\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u7684\u6df1\u5ea6\u4e0e\u6548\u7387\u3002\n\n*   **[Exploring Reasoning Reward Model for Agents]** \u5f15\u5165 **Agent Reasoning Reward Model (Agent-RRM)**\uff0c\u901a\u8fc7\u63d0\u4f9b\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\u3001\u9488\u5bf9\u6027\u6279\u8bc4\u548c\u6574\u4f53\u8bc4\u5206\u7684\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u5956\u52b1\u65e0\u6cd5\u533a\u5206\u4e2d\u95f4\u63a8\u7406\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u5176 **Reagent-U** \u7b56\u7565\u5728GAIA\u548cWebWalkerQA\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002(2601.22154 [cs.AI])\n*   **[Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning]** \u63d0\u51fa **SCMA** \u6846\u67b6\uff0c\u5229\u7528 **\u5206\u5272\u667a\u80fd\u4f53** \u548c **\u8bc4\u5206\u667a\u80fd\u4f53** \u534f\u540c\u8bc6\u522b\u5197\u4f59\u63a8\u7406\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u727a\u7272\u5173\u952e\u903b\u8f91\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u538b\u7f29CoT\u957f\u5ea6\uff0c\u540c\u65f6\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\u3002(2601.21919 [cs.AI])\n*   **[DynaWeb: Model-Based Reinforcement Learning of Web Agents]** \u63d0\u51fa **DynaWeb** \u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60 **Web\u4e16\u754c\u6a21\u578b** \u8ba9\u667a\u80fd\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u201c\u68a6\u5883\u201d\u8bad\u7ec3\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u5728\u771f\u5b9e\u4e92\u8054\u7f51\u4e0a\u8bad\u7ec3Web\u667a\u80fd\u4f53\u7684\u6210\u672c\u548c\u98ce\u9669\u3002(2601.22149 [cs.AI])\n*   **[WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents]** \u63d0\u51fa **WebArbiter**\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u4ea7\u751f\u7ed3\u6784\u5316\u7684\u7406\u7531\u548c\u5224\u51b3\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6807\u91cf\u5956\u52b1\u548c\u57fa\u4e8e\u6a21\u677f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728Web\u5bfc\u822a\u4efb\u52a1\u4e2d\u8d85\u8d8aGPT-5\u3002(2601.21872 [cs.AI])\n*   **[Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning]** \u63d0\u51fa **DARE**\uff0c\u901a\u8fc7\u5229\u7528\u5b8c\u6574\u7684\u7ecf\u9a8c\u56de\u653e\u5206\u5e03\u800c\u975e\u7b80\u5355\u7684\u591a\u6570\u6295\u7968\u6765\u4f30\u8ba1\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6RL\u4e2d\u5956\u52b1\u4f30\u8ba1\u504f\u5dee\u7684\u95ee\u9898\uff0c\u5728AIME 2024\u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002(2601.21804 [cs.CL])\n*   **[ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas]** \u5c55\u793a\u4e86 **ASTRA** \u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u667a\u80fd\u4f53\u8f68\u8ff9\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u8fbe\u5230\u63a5\u8fd1\u95ed\u6e90\u7cfb\u7edf\u7684\u6027\u80fd\u3002(2601.21558 [cs.CL])\n*   **[Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents]** \u63d0\u51fa **DAVID-GRPO**\uff0c\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\uff08\u5c0f\u6a21\u578b\u3001\u5c11\u7b97\u529b\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bc1\u636e\u53ec\u56de\u7684\u4fe1\u7528\u5206\u914d\u548c\u91cd\u91c7\u6837\u673a\u5236\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u8df3\u63a8\u7406\u3002(2601.21699 [cs.CL])\n*   **[Textual Equilibrium Propagation for Deep Compound AI Systems]** \u5f15\u5165 **\u6587\u672c\u5e73\u8861\u4f20\u64ad (TEP)**\uff0c\u901a\u8fc7\u5c40\u90e8\u81ea\u7531\u76f8\u548c\u5fae\u8c03\u63a8\u52a8\u76f8\u7684\u8fed\u4ee3\uff0c\u89e3\u51b3\u4e86\u6df1\u5c42\u590d\u5408AI\u7cfb\u7edf\u4e2d\u6587\u672c\u53cd\u9988\u201c\u7206\u70b8\u201d\u6216\u201c\u6d88\u5931\u201d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u957f\u89c6\u754c\u5de5\u4f5c\u6d41\u7684\u9ad8\u6548\u4f18\u5316\u3002(2601.21064 [cs.AI])\n\n---\n\n### \u5de5\u5177\u4f7f\u7528\u4e0e\u4ee3\u7801\u667a\u80fd\u4f53\uff1a\u8fc8\u5411\u81ea\u4e3b\u5de5\u7a0b\n\n\u7814\u7a76\u805a\u7126\u4e8e\u63d0\u5347\u667a\u80fd\u4f53\u4f7f\u7528\u5de5\u5177\u7684\u6548\u7387\u3001\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u4ee5\u53ca\u81ea\u4e3b\u89e3\u51b3\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5143\u5de5\u5177\u3001\u6280\u80fd\u5e93\u548c\u95ed\u73af\u4f18\u5316\u673a\u5236\uff0c\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u3002\n\n*   **[Optimizing Agentic Workflows using Meta-tools]** \u63d0\u51fa **Agent Workflow Optimization (AWO)**\uff0c\u901a\u8fc7\u5206\u6790\u5de5\u4f5c\u6d41\u8f68\u8ff9\u8bc6\u522b\u5197\u4f59\u6a21\u5f0f\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a **\u5143\u5de5\u5177**\uff0c\u6709\u6548\u51cf\u5c11\u4e86LLM\u8c03\u7528\u6b21\u6570\u5e76\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002(2601.22037 [cs.AI])\n*   **[ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models]** \u63d0\u51fa **ToolWeaver**\uff0c\u5c06\u5de5\u5177\u7f16\u7801\u4e3a\u5206\u5c42\u5e8f\u5217\u800c\u975e\u72ec\u7acbToken\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u8bcd\u6c47\u8868\u7206\u70b8\u95ee\u9898\uff0c\u8fd8\u4f7f\u6a21\u578b\u80fd\u4ece\u5171\u4eab\u4ee3\u7801\u7684\u5bc6\u96c6\u5171\u73b0\u4e2d\u5b66\u4e60\u5de5\u5177\u95f4\u7684\u534f\u4f5c\u5173\u7cfb\u3002(2601.21947 [cs.AI])\n*   **[KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization]** \u4ecb\u7ecd **KAPSO**\uff0c\u4e00\u4e2a\u7ed3\u5408Git\u539f\u751f\u5b9e\u9a8c\u5f15\u64ce\u3001\u77e5\u8bc6\u7cfb\u7edf\u548c\u8ba4\u77e5\u8bb0\u5fc6\u5c42\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u89c6\u754c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u72b6\u6001\u4e22\u5931\u548c\u8c03\u8bd5\u8106\u5f31\u6027\u95ee\u9898\u3002(2601.21526 [cs.AI])\n*   **[NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents]** \u63d0\u51fa **NEMO**\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u6570\u5b66\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u6c99\u76d2\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u8fdb\u884c\u9a8c\u8bc1\u548c\u4fee\u590d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u53ef\u9760\u6027\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u3002(2601.21372 [cs.AI])\n*   **[SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents]** \u63d0\u51fa **SWE-Replay**\uff0c\u901a\u8fc7\u56de\u6536\u548c\u91cd\u7528\u4e4b\u524d\u7684\u8f68\u8ff9\u5206\u652f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u8ba1\u7b97\u6210\u672c\u3002(2601.22129 [cs.AI])\n*   **[Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve]** \u5c55\u793a\u4e86 **Magellan**\uff0c\u5229\u7528LLM\u7f16\u7801\u667a\u80fd\u4f53\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\uff0c\u81ea\u52a8\u53d1\u73b0\u5e76\u5408\u6210\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u542f\u53d1\u5f0f\u89c4\u5219\uff08\u5982LLVM\u5185\u8054\u7b56\u7565\uff09\u3002(2601.21096 [cs.AI])\n*   **[Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation]** \u63d0\u51fa **DebateCoder**\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c **\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u95e8\u63a7** \u673a\u5236\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\uff08\u5982Pangu-1B\uff09\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002(2601.21469 [cs.AI])\n\n---\n\n### \u8bb0\u5fc6\u3001\u4e0a\u4e0b\u6587\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff1a\u6784\u5efa\u7cfb\u7edf2\u601d\u7ef4\n\n\u4e3a\u4e86\u652f\u6301\u957f\u89c6\u754c\u63a8\u7406\u548c\u590d\u6742\u534f\u4f5c\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u8bb0\u5fc6\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u4ee5\u53ca\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u534f\u8bae\uff0c\u65e8\u5728\u589e\u5f3a\u7cfb\u7edf\u7684\u4fe1\u606f\u6574\u5408\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\n\n*   **[JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG]** \u63d0\u51fa **JADE** \u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c4\u5212\u8005\u548c\u6267\u884c\u8005\uff0c\u89e3\u51b3\u4e86\u52a8\u6001RAG\u7cfb\u7edf\u4e2d\u201c\u6218\u7565-\u8fd0\u8425\u4e0d\u5339\u914d\u201d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u534f\u540c\u9002\u5e94\u3002(2601.21916 [cs.AI])\n*   **[Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic]** \u63d0\u51fa **CoLLM-CC** \u548c **CoLLM-DC**\uff0c\u5206\u6790\u4e86\u96c6\u4e2d\u5f0f\u4e0e\u53bb\u4e2d\u5fc3\u5316Critic\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u4f18\u52a3\uff0c\u53d1\u73b0\u5728\u957f\u89c6\u754c\u6216\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\uff0c\u96c6\u4e2d\u5f0fCritic\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002(2601.21972 [cs.AI])\n*   **[E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory]** \u63d0\u51fa **E-mem**\uff0c\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c **\u60c5\u666f\u4e0a\u4e0b\u6587\u91cd\u6784**\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bb0\u5fc6\u9884\u5904\u7406\u4e2d\u7684\u4e0a\u4e0b\u6587\u7834\u574f\uff0c\u5728\u957f\u89c6\u754c\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684F1\u5206\u6570\u548c\u66f4\u4f4e\u7684Token\u6210\u672c\u3002(2601.21714 [cs.AI])\n*   **[Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems]** \u63d0\u51fa **Epistemic Context Learning (ECL)**\uff0c\u901a\u8fc7\u57fa\u4e8e\u5386\u53f2\u4ea4\u4e92\u6784\u5efa\u540c\u4f34\u753b\u50cf\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8bc6\u522b\u53ef\u4fe1\u540c\u4f34\u5e76\u4ece\u4e2d\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u76f2\u76ee\u4ece\u4f17\u95ee\u9898\u3002(2601.21742 [cs.AI])\n*   **[MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning]** \u63d0\u51fa **MemOCR**\uff0c\u5c06\u8bb0\u5fc6\u6e32\u67d3\u4e3a\u5177\u6709\u89c6\u89c9\u5e03\u5c40\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7684\u4fe1\u606f\u5bc6\u5ea6\u5206\u914d\uff0c\u5728\u6781\u7aef\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u7eaf\u6587\u672c\u8bb0\u5fc6\u66f4\u9ad8\u6548\u7684\u957f\u89c6\u754c\u63a8\u7406\u3002(2601.21468 [cs.AI])\n*   **[Meta Context Engineering via Agentic Skill Evolution]** \u5f15\u5165 **Meta Context Engineering (MCE)**\uff0c\u901a\u8fc7\u5143\u7ea7\u548c\u57fa\u7ea7\u667a\u80fd\u4f53\u7684\u534f\u540c\u8fdb\u5316\uff0c\u8d85\u8d8a\u4e86\u9759\u6001\u7684\u624b\u5de5\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u6280\u80fd\u548c\u5236\u54c1\u7684\u81ea\u52a8\u4f18\u5316\u3002(2601.21557 [cs.AI])\n*   **[Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation]** \u63d0\u51fa **\u5bf9\u6297\u6027\u8bb0\u5fc6\u9002\u5e94 (AMA)**\uff0c\u901a\u8fc7\u6a21\u62df\u4e0b\u6e38\u4efb\u52a1\u6267\u884c\u6765\u751f\u6210\u5bf9\u6297\u6027QA\u5bf9\uff0c\u4ece\u800c\u5728\u79bb\u7ebf\u9636\u6bb5\u5bf9\u8bb0\u5fc6\u7cfb\u7edf\u8fdb\u884c\u4efb\u52a1\u5bfc\u5411\u7684\u5fae\u8c03\u3002(2601.21797 [cs.CL])\n*   **[RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems]** \u63d0\u51fa **RecNet**\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53RL\u6846\u67b6\u6a21\u62df\u7528\u6237\u548c\u7269\u54c1\u95f4\u7684\u5b9e\u65f6\u76f8\u4e92\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u504f\u597d\u7684\u4e3b\u52a8\u4f20\u64ad\u548c\u7b56\u7565\u7684\u81ea\u6211\u8fdb\u5316\u3002(2601.21609 [cs.AI])\n*   **[Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation]** \u63d0\u51fa **CoNL**\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u81ea\u6211\u535a\u5f08\uff0c\u5c06\u201c\u6279\u8bc4\u662f\u5426\u80fd\u5e2e\u52a9\u4ed6\u4eba\u6539\u8fdb\u201d\u4f5c\u4e3a\u5143\u8bc4\u4f30\u7684\u5956\u52b1\uff0c\u5b9e\u73b0\u4e86\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u4e0b\u7684\u4e0d\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u5982\u521b\u610f\u5199\u4f5c\uff09\u7684\u81ea\u6211\u8fdb\u5316\u3002(2601.21464 [cs.AI])\n\n---\n\n### \u4e13\u4e1a\u667a\u80fd\u4f53\uff1aWeb\u3001GUI\u4e0e\u5177\u8eab\u667a\u80fd\u7684\u843d\u5730\n\n\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u6311\u6218\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u6db5\u76d6\u4e86Web\u5bfc\u822a\u3001GUI\u64cd\u4f5c\u3001\u533b\u7597\u51b3\u7b56\u3001\u8de8\u6a21\u6001\u6570\u636e\u5206\u6790\u53ca\u5177\u8eab\u673a\u5668\u4eba\u63a7\u5236\u3002\n\n*   **[CUA-Skill: Develop Skills for Computer Using Agent]** \u63d0\u51fa **CUA-Skill**\uff0c\u4e00\u4e2a\u5305\u542b\u5927\u89c4\u6a21\u4eba\u7c7b\u8ba1\u7b97\u673a\u4f7f\u7528\u77e5\u8bc6\u7684\u6280\u80fd\u5e93\uff0c\u652f\u6301\u52a8\u6001\u68c0\u7d22\u548c\u53c2\u6570\u5316\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u667a\u80fd\u4f53\u5728Windows\u73af\u5883\u4e0b\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002(2601.21123 [cs.AI])\n*   **[BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents]** \u63d0\u51fa **BEAP-Agent**\uff0c\u57fa\u4e8eDFS\u8fc7\u7a0b\u5efa\u6a21GUI\u4efb\u52a1\u6267\u884c\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u591a\u7ea7\u72b6\u6001\u56de\u6eaf\u673a\u5236\uff0c\u89e3\u51b3\u4e86GUI\u667a\u80fd\u4f53\u4e00\u65e6\u8d70\u9519\u8def\u5f84\u96be\u4ee5\u6062\u590d\u7684\u95ee\u9898\u3002(2601.21352 [cs.AI])\n*   **[DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis]** \u63d0\u51fa **DataCross**\uff0c\u9488\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u4e0e\u975e\u7ed3\u6784\u5316\u89c6\u89c9\u6587\u6863\uff08\u201c\u50f5\u5c38\u6570\u636e\u201d\uff09\u7684\u8054\u5408\u5206\u6790\uff0c\u6784\u5efa\u4e86\u65b0\u57fa\u51c6\u548c\u534f\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u6df1\u5ea6\u6d1e\u5bdf\u3002(2601.21403 [cs.AI])\n*   **[Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement]** \u63d0\u51fa **Planner-Auditor Twin**\uff0c\u901a\u8fc7\u5c06LLM\u751f\u6210\u5668\u4e0e\u786e\u5b9a\u6027\u5ba1\u8ba1\u6a21\u5757\u89e3\u8026\uff0c\u5e76\u5f15\u5165\u81ea\u6539\u8fdb\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u51fa\u9662\u8ba1\u5212\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002(2601.21113 [cs.AI])\n*   **[Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models]** \u63d0\u51fa **Vision-DeepResearch**\uff0c\u901a\u8fc7\u591a\u8f6e\u3001\u591a\u5b9e\u4f53\u3001\u591a\u5c3a\u5ea6\u7684\u89c6\u89c9\u6587\u672c\u641c\u7d22\u8303\u5f0f\uff0c\u7ed3\u5408RL\u8bad\u7ec3\uff0c\u663e\u8457\u589e\u5f3a\u4e86MLLM\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002(2601.22060 [cs.AI])\n*   **[EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots]** \u5f15\u5165 **EmboCoach-Bench**\uff0c\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u81ea\u4e3b\u8bbe\u8ba1\u5177\u8eab\u673a\u5668\u4eba\u7b56\u7565\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u4e0d\u4ec5\u80fd\u5339\u654c\u4eba\u7c7b\u57fa\u7ebf\uff0c\u8fd8\u80fd\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u81ea\u6211\u4fee\u6b63\u5931\u8d25\u6848\u4f8b\u3002(2601.21570 [cs.AI])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n1.  **\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b \u7684\u7206\u53d1**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982 Agent-RRM, WebArbiter, SCMA\uff09\u4e0d\u7ea6\u800c\u540c\u5730\u805a\u7126\u4e8e\u201c\u8fc7\u7a0b\u201d\u800c\u975e\u4ec5\u4ec5\u662f\u201c\u7ed3\u679c\u201d\u3002\u8fd9\u6807\u5fd7\u7740Agentic RL\u6b63\u5728\u4ece\u7b80\u5355\u7684Outcome-based reward\u8f6c\u5411\u66f4\u7ec6\u7c92\u5ea6\u7684Process-based supervision\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\u6765\u7a81\u7834\u957f\u89c6\u754c\u4efb\u52a1\u7684\u8bad\u7ec3\u74f6\u9888\u3002\n2.  **\u6d4b\u8bd5\u65f6\u8ba1\u7b97 \u7684\u6548\u7387\u9769\u547d**\uff1a\u968f\u7740\u6a21\u578b\u63a8\u7406\u6210\u672c\u7684\u589e\u52a0\uff0c\u5982\u4f55\u9ad8\u6548\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6210\u4e3a\u70ed\u70b9\u3002SWE-Replay\u901a\u8fc7\u201c\u8f68\u8ff9\u56de\u6536\u201d\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\uff0cSCMA\u901a\u8fc7\u201c\u81ea\u538b\u7f29\u201d\u7cbe\u7b80\u63a8\u7406\u8fc7\u7a0b\uff0cDARE\u901a\u8fc7\u201c\u5206\u5e03\u611f\u77e5\u201d\u4f18\u5316\u5956\u52b1\u4f30\u8ba1\u3002\u8fd9\u4e9b\u7814\u7a76\u5171\u540c\u6307\u5411\u4e00\u4e2a\u8d8b\u52bf\uff1a\u672a\u6765\u7684\u667a\u80fd\u4f53\u4e0d\u4ec5\u8981\u806a\u660e\uff0c\u8fd8\u8981\u66f4\u201c\u7701\u94b1\u201d\u3002\n3.  **\u590d\u5408AI\u7cfb\u7edf \u7684\u6df1\u5ea6\u4f18\u5316**\uff1a\u7814\u7a76\u91cd\u5fc3\u6b63\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u7531\u591a\u4e2a\u6a21\u5757\uff08\u89c4\u5212\u5668\u3001\u6267\u884c\u5668\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\uff09\u7ec4\u6210\u7684\u590d\u5408\u7cfb\u7edf\u3002JADE\uff08\u8054\u5408\u4f18\u5316\uff09\u3001TEP\uff08\u5e73\u8861\u4f20\u64ad\uff09\u548cAWO\uff08\u5143\u5de5\u5177\uff09\u7b49\u5de5\u4f5c\u8868\u660e\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u7684\u7aef\u5230\u7aef\u4f18\u5316\u548c\u6a21\u5757\u95f4\u7684\u534f\u540c\u9002\u5e94\uff0c\u53ef\u4ee5\u91ca\u653e\u51fa\u8fdc\u8d85\u5355\u4f53\u6a21\u578b\u53e0\u52a0\u7684\u6027\u80fd\u6f5c\u529b\u3002\n4.  **GUI/Web\u667a\u80fd\u4f53\u7684\u6280\u80fd\u5316\u4e0e\u5177\u8eab\u5316**\uff1aCUA-Skill\u548cBEAP-Agent\u7684\u51fa\u73b0\uff0c\u610f\u5473\u7740\u667a\u80fd\u4f53\u5728\u64cd\u4f5c\u6570\u5b57\u4e16\u754c\u65f6\uff0c\u5f00\u59cb\u4ece\u201c\u76f2\u76ee\u63a2\u7d22\u201d\u8f6c\u5411\u5229\u7528\u201c\u7ed3\u6784\u5316\u6280\u80fd\u5e93\u201d\u548c\u201c\u7cfb\u7edf\u5316\u56de\u6eaf\u201d\u3002\u540c\u65f6\uff0cEmboCoach-Bench\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u4ece\u201c\u8f6f\u4ef6\u5199\u624b\u201d\u5411\u201c\u786c\u4ef6\u6559\u7ec3\u201d\u7684\u8de8\u8d8a\uff0c\u9884\u793a\u7740AI\u5728\u7269\u7406\u4e16\u754c\u81ea\u52a8\u5316\u5de5\u7a0b\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002",
    "2026-01-29": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-29)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u63ed\u793a\u4e86AI\u667a\u80fd\u4f53\u7814\u7a76\u6b63\u4ece\u5355\u7eaf\u7684\u6a21\u578b\u80fd\u529b\u6269\u5c55\uff0c\u5411\u66f4\u6df1\u5c42\u7684**\u67b6\u6784\u8bb0\u5fc6\u5316**\u3001**\u63a8\u7406\u7b56\u7565\u8fdb\u5316**\u548c**\u8bad\u7ec3\u6548\u7387\u9769\u547d**\u8fc8\u8fdb\u3002\u6211\u4eec\u770b\u5230\uff0c\u4e3a\u4e86\u89e3\u51b3\u957f\u7a0b\u4ea4\u4e92\u4e2d\u7684\u201c\u7075\u9b42\u4fb5\u8680\u201d\u95ee\u9898\uff0c\u53d7\u8111\u79d1\u5b66\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\u6210\u4e3a\u70ed\u70b9\uff1b\u540c\u65f6\uff0c**GRPO\uff08\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09**\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684RL\u8bad\u7ec3\u8303\u5f0f\uff0c\u6b63\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5de5\u5177\u4f7f\u7528\u3001GUI\u4ea4\u4e92\u548c\u4ee3\u7801\u751f\u6210\u4e2d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u663e\u793a\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7684\u7b56\u7565\u8fdb\u5316\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8d85\u8d8a\u8d85\u5927\u6a21\u578b\uff0c\u6311\u6218\u4e86\u201c\u552f\u53c2\u6570\u8bba\u201d\u7684\u4f20\u7edf\u8ba4\u77e5\u3002\n\n---\n\n### \u8bb0\u5fc6\u4e0e\u4e2a\u6027\u5316\uff1a\u89e3\u51b3\u667a\u80fd\u4f53\u7684\u201c\u7075\u9b42\u4fb5\u8680\u201d\n\n\u667a\u80fd\u4f53\u5728\u957f\u7a0b\u4ea4\u4e92\u4e2d\u5f80\u5f80\u9057\u5fd8\u4e0a\u4e0b\u6587\u6216\u4e27\u5931\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u4fee\u590d\u8fd9\u4e00\u7f3a\u9677\u3002\n\n*   **BMAM (Brain-inspired Multi-Agent Memory)** \u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u8ba4\u77e5\u8bb0\u5fc6\u7cfb\u7edf\u542f\u53d1\u7684\u901a\u7528\u67b6\u6784\uff0c\u5c06\u8bb0\u5fc6\u5206\u89e3\u4e3a\u60c5\u666f\u3001\u8bed\u4e49\u3001\u663e\u8457\u6027\u7b49\u5b50\u7cfb\u7edf\uff0c\u5728LoCoMo\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8678.45%\u7684\u957f\u7a0b\u63a8\u7406\u51c6\u786e\u7387\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u667a\u80fd\u4f53\u7684\u201c\u7075\u9b42\u4fb5\u8680\u201d\u73b0\u8c61\u3002(2601.20465 [cs.CL])\n*   **AMA (Adaptive Memory via Multi-Agent Collaboration)** \u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u901a\u8fc7Constructor\u3001Retriever\u548cJudge\u7b49\u89d2\u8272\u7684\u914d\u5408\uff0c\u5b9e\u73b0\u4e86\u591a\u7c92\u5ea6\u7684\u8bb0\u5fc6\u68c0\u7d22\u4e0e\u4e00\u81f4\u6027\u6821\u9a8c\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eSOTA\uff0c\u5e76\u51cf\u5c11\u4e86\u7ea680%\u7684Token\u6d88\u8017\u3002(2601.20352 [cs.AI])\n*   **MemCtrl** \u5c06\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u8bb0\u5fc6\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u8bb0\u5fc6\u5934\u6765\u51b3\u5b9a\u89c2\u5bdf\u7684\u4fdd\u7559\u6216\u4e22\u5f03\uff0c\u5728EmbodiedBench\u4e0a\u4f7f\u4f4e\u6027\u80fdMLLM\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u5347\u4e86\u7ea616%\u3002(2601.20831 [cs.AI])\n*   **Me-Agent** \u9488\u5bf9\u79fb\u52a8\u7aef\u573a\u666f\uff0c\u63d0\u51fa\u4e86\u4e24\u7ea7\u7528\u6237\u4e60\u60ef\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u5956\u52b1\u6a21\u578b\u548c\u5206\u5c42\u504f\u597d\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u6307\u4ee4\u6a21\u7cca\u548c\u4e2a\u6027\u5316\u7f3a\u5931\u95ee\u9898\uff0c\u5728User FingerTip\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002(2601.20162 [cs.CL])\n*   **Mem2ActBench** \u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u667a\u80fd\u4f53\u662f\u5426\u80fd\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u4e3b\u52a8\u5229\u7528\u957f\u671f\u8bb0\u5fc6\uff08\u800c\u975e\u88ab\u52a8\u68c0\u7d22\uff09\uff0c\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u5c06\u8bb0\u5fc6\u5e94\u7528\u4e8e\u53c2\u6570 grounding \u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002(2601.19935 [cs.CL])\n\n---\n\n### \u5de5\u5177\u4f7f\u7528\u4e0e\u89c4\u5212\uff1a\u9a7e\u9a6d\u590d\u6742\u6027\u4e0e\u6a21\u7cca\u6027\n\n\u9762\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u6a21\u7cca\u3001\u53d8\u5316\u751a\u81f3\u4e0d\u53ef\u884c\u7684\u4efb\u52a1\u6307\u4ee4\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u66f4\u9c81\u68d2\u7684\u89c4\u5212\u6846\u67b6\u548c\u9a8c\u8bc1\u673a\u5236\u3002\n\n*   **PEARL (Plan Exploration and Adaptive Reinforcement Learning)** \u91c7\u7528\u79bb\u7ebf\u63a2\u7d22\u4e0e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u7ed3\u5408\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u4e13\u95e8\u89e3\u51b3\u591a\u8df3\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u5e7b\u89c9\u548c\u89c4\u5212\u9519\u8bef\uff0c\u5728ToolHop\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8656.5%\u7684SOTA\u6210\u529f\u7387\u3002(2601.20439 [cs.CL])\n*   **Trajectory2Task** \u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4e13\u95e8\u9488\u5bf9\u6a21\u7cca\u610f\u56fe\u3001\u610f\u56fe\u53d8\u66f4\u548c\u4e0d\u53ef\u884c\u610f\u56fe\u7b49\u73b0\u5b9e\u573a\u666f\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u5408\u6210\u8f68\u8ff9\u5fae\u8c03\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728\u590d\u6742\u7528\u6237\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002(2601.20144 [cs.CL])\n*   **Self-Querying Category-Theoretic Planning (SQ-BCP)** \u5f15\u5165\u8303\u7574\u8bba\u4e2d\u7684Pullback\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u5f0f\u8868\u793a\u524d\u63d0\u6761\u4ef6\u7684\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u67e5\u8be2\u6216\u201c\u6865\u63a5\u201d\u5047\u8bbe\u6765\u89e3\u51b3\u672a\u6307\u5b9a\u7684\u63a8\u7406\u95ee\u9898\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8d44\u6e90\u8fdd\u89c4\u7387\u3002(2601.20014 [cs.AI])\n*   **Deep Researcher** \u6311\u6218\u4e86\u5e76\u884c\u6269\u5c55\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u201c\u987a\u5e8f\u8ba1\u5212\u53cd\u601d\u201d\u548c\u201c\u5019\u9009\u8005\u4ea4\u53c9\u201d\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ef4\u62a4\u5168\u5c40\u7814\u7a76\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u8c03\u6574\u8ba1\u5212\uff0c\u5728DeepResearch Bench\u4e0a\u8d85\u8d8a\u4e86Claude\u548cPerplexity\u7b49\u4e3b\u6d41\u7814\u7a76\u52a9\u624b\u3002(2601.20843 [cs.AI])\n*   **PathWise** \u5c06\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6765\u643a\u5e26\u8fc7\u53bb\u7684\u51b3\u7b56\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u4ece\u8bd5\u9519\u8fdb\u5316\u5230\u72b6\u6001\u611f\u77e5\u89c4\u5212\u7684\u8f6c\u53d8\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u6536\u655b\u66f4\u5feb\u3002(2601.20539 [cs.CL])\n\n---\n\n### \u667a\u80fd\u4f53\u8bad\u7ec3\uff1aRL\u8fdb\u5316\u4e0e\u6548\u7387\u9769\u547d\n\n\u5982\u4f55\u66f4\u9ad8\u6548\u5730\u8bad\u7ec3\u667a\u80fd\u4f53\uff1f\u4eca\u65e5\u7684\u7814\u7a76\u5c55\u793a\u4e86\u4eceRL\u7b97\u6cd5\u6539\u8fdb\u5230\u6570\u636e\u81ea\u52a8\u5904\u7406\u7684\u5168\u65b9\u4f4d\u6548\u7387\u63d0\u5347\u3002\n\n*   **SERA (Soft-Verified Efficient Repository Agents)** \u8bc1\u660e\u4e86\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u7684\u4e13\u7528\u667a\u80fd\u4f53\u8bad\u7ec3\u4e0d\u518d\u6602\u8d35\uff0c\u5176\u63d0\u51fa\u7684**Soft Verified Generation (SVG)** \u65b9\u6cd5\u4ec5\u4f7f\u7528SFT\u5c31\u8fbe\u5230\u4e86\u4e0eRL\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6210\u672c\u6bd4RL\u4f4e26\u500d\uff0c\u4e14\u5339\u914d\u4e86Devstral-Small-2\u7b49\u524d\u6cbf\u6a21\u578b\u7684\u6548\u679c\u3002(2601.20789 [cs.CL])\n*   **Policy of Thoughts (PoT)** \u63d0\u51fa\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u7b56\u7565\u8fdb\u5316\uff0c\u901a\u8fc7GRPO\u52a8\u6001\u66f4\u65b0LoRA\u9002\u914d\u5668\uff0c\u4f7f4B\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cDeepSeek-V3\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u7b56\u7565\u4f18\u5316\u6bd4\u5355\u7eaf\u6269\u5927\u6a21\u578b\u89c4\u6a21\u66f4\u6709\u6548\u3002(2601.20379 [cs.AI])\n*   **Reinforcement Learning via Self-Distillation (SDPO)** \u521b\u65b0\u6027\u5730\u5229\u7528\u73af\u5883\u53cd\u9988\u7684\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\uff08\u800c\u975e\u4ec5\u6807\u91cf\u5956\u52b1\uff09\uff0c\u901a\u8fc7\u81ea\u6211\u84b8\u998f\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u63a8\u7406\u548c\u7ade\u6280\u7f16\u7a0b\u7684\u6837\u672c\u6548\u7387\u3002(2601.20802 [cs.AI])\n*   **Spark** \u9488\u5bf9\u957f\u89c6\u754c\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5173\u952e\u72b6\u6001\u52a8\u6001\u5206\u652f\u673a\u5236\uff0c\u4ec5\u5728\u5173\u952e\u51b3\u7b56\u70b9\u8fdb\u884c\u63a2\u7d22\u6027\u5206\u652f\uff0c\u5728\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u6837\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002(2601.20209 [cs.CL])\n*   **Scaling Medical Reasoning Verification** \u5f15\u5165\u5de5\u5177\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u533b\u5b66\u63a8\u7406\u9a8c\u8bc1\u5668\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u67e5\u8be2\u5916\u90e8\u533b\u5b66\u8bed\u6599\u5e93\uff0c\u5728MedQA\u4e0a\u63d0\u5347\u4e8623.5%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u91c7\u6837\u9884\u7b97\u964d\u4f4e\u4e868\u500d\u3002(2601.20221 [cs.CL])\n*   **LLM-AutoDP** \u5229\u7528LLM\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u6570\u636e\u5904\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5e03\u4fdd\u6301\u91c7\u6837\u548c\u7f13\u5b58\u91cd\u7528\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u6570\u636e\u6e05\u6d17\uff0c\u641c\u7d22\u901f\u5ea6\u63d0\u5347\u4e8610\u500d\u3002(2601.20375 [cs.CL])\n\n---\n\n### \u5177\u8eab\u4e0eGUI\u667a\u80fd\u4f53\uff1a\u8de8\u8d8a\u6570\u5b57\u9e3f\u6c9f\n\n\u4ece\u56fe\u5f62\u754c\u9762\u64cd\u4f5c\u5230\u5177\u8eab\u667a\u80fd\u901a\u4fe1\uff0c\u667a\u80fd\u4f53\u6b63\u4ee5\u66f4\u81ea\u7136\u7684\u65b9\u5f0f\u878d\u5165\u7269\u7406\u4e0e\u6570\u5b57\u4e16\u754c\u3002\n\n*   **OmegaUse** \u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684GUI\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u652f\u6301\u79fb\u52a8\u7aef\u548c\u684c\u9762\u7aef\uff0c\u91c7\u7528MoE\u4e3b\u5e72\u7f51\u7edc\u548cSFT+GRPO\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728ScreenSpot-V2\u4e0a\u53d6\u5f97\u4e8696.3%\u7684SOTA\u5206\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u7ec8\u7aef\u6cdb\u5316\u80fd\u529b\u3002(2601.20380 [cs.AI])\n*   **Continual GUI Agents** \u9996\u6b21\u63d0\u51fa\u4e86GUI\u667a\u80fd\u4f53\u7684\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\uff0c\u9488\u5bf9GUI\u5206\u5e03\u968f\u65f6\u95f4\u504f\u79fb\uff08\u5982\u65b0\u5e94\u7528\u3001\u65b0\u5206\u8fa8\u7387\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**GUI-AiF**\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u5b9a\u5956\u52b1\u673a\u5236\u7a33\u5b9a\u4e86\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u3002(2601.20732 [cs.LG])\n*   **Investigating Task-Oriented Communication** \u7814\u7a76\u53d1\u73b0VLM\u667a\u80fd\u4f53\u5728\u534f\u4f5c\u63a8\u7406\u4e2d\u80fd\u81ea\u53d1\u53d1\u5c55\u51fa\u9ad8\u6548\u4e14\u9690\u853d\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u8fd9\u79cd\u534f\u8bae\u5bf9\u4eba\u7c7b\u548c\u5916\u90e8\u89c2\u5bdf\u8005\u96be\u4ee5\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u901a\u4fe1\u7684\u6f5c\u5728\u98ce\u9669\u4e0e\u8fdb\u5316\u6f5c\u529b\u3002(2601.20641 [cs.AI])\n*   **Demystifying Multi-Agent Debate** \u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u5931\u6548\u7684\u539f\u56e0\u5728\u4e8e\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6837\u6027\u611f\u77e5\u521d\u59cb\u5316\u548c\u7f6e\u4fe1\u5ea6\u8c03\u5236\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fa9\u8bba\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002(2601.19921 [cs.CL])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n1.  **SFT\u7684\u9006\u88ad\u4e0eRL\u7684\u5e73\u6c11\u5316**\uff1aSERA\u8bba\u6587\u6781\u5177\u51b2\u51fb\u529b\u5730\u6307\u51fa\uff0c\u901a\u8fc7\u7cbe\u5de7\u7684**\u8f6f\u9a8c\u8bc1\u751f\u6210\uff08SVG\uff09**\uff0c\u4ec5\u9760\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5c31\u80fd\u5728\u4ee3\u7801\u4efb\u52a1\u4e0a\u51fb\u8d25\u6602\u8d35\u7684RL\u65b9\u6cd5\uff0c\u4e14\u6210\u672c\u964d\u4f4e26\u500d\u3002\u8fd9\u53ef\u80fd\u9884\u793a\u7740\u5728\u7279\u5b9a\u5782\u76f4\u9886\u57df\uff0c\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u914d\u5408SFT\u5c06\u6210\u4e3a\u6bd4RL\u66f4\u4e3b\u6d41\u7684\u8def\u5f84\u3002\n2.  **GRPO\uff1a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u65b0\u6807\u914d**\uff1a\u4ecePEARL\uff08\u5de5\u5177\u4f7f\u7528\uff09\u5230OmegaUse\uff08GUI\u4ea4\u4e92\uff09\uff0c\u518d\u5230Policy of Thoughts\uff08\u63a8\u7406\u4f18\u5316\uff09\uff0c**GRPO\uff08Group Relative Policy Optimization\uff09** \u9891\u7e41\u51fa\u73b0\u3002\u8fd9\u8868\u660eGRPO\u6b63\u9010\u6e10\u53d6\u4ee3PPO\uff0c\u6210\u4e3a\u8bad\u7ec3\u590d\u6742\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u9996\u9009RL\u7b97\u6cd5\uff0c\u5176\u5728\u5bf9\u9f50\u548c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u5f97\u5230\u4e86\u5e7f\u6cdb\u8ba4\u53ef\u3002\n3.  **\u201c\u5c0f\u6a21\u578b + \u5f3a\u7b56\u7565\u201d > \u201c\u5927\u6a21\u578b\u201d**\uff1aPolicy of Thoughts \u5c55\u793a\u4e86\u4e00\u4e2a\u4ee4\u4eba\u632f\u594b\u7684\u73b0\u8c61\uff1a4B\u7684\u5c0f\u6a21\u578b\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7684\u7b56\u7565\u8fdb\u5316\uff0c\u53ef\u4ee5\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u51fb\u8d25GPT-4o\u548cDeepSeek-V3\u3002\u8fd9\u5f3a\u8c03\u4e86\u201c\u63a8\u7406\u7b56\u7565\u201d\u672c\u8eab\u4f5c\u4e3a\u4e00\u79cd\u53ef\u8fdb\u5316\u80fd\u529b\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u6253\u7834\u7b97\u529b\u5784\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\n4.  **\u8bb0\u5fc6\u67b6\u6784\u7684\u201c\u8111\u79d1\u5b66\u201d\u8f6c\u5411**\uff1aBMAM\u548cAMA\u7b49\u7814\u7a76\u4e0d\u518d\u6ee1\u8db3\u4e8e\u7b80\u5355\u7684RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\uff0c\u800c\u662f\u5f00\u59cb\u6df1\u5ea6\u6a21\u4eff\u4eba\u8111\u7684\u6d77\u9a6c\u4f53\u3001\u8bed\u4e49\u8bb0\u5fc6\u7b49\u673a\u5236\u3002\u8fd9\u6807\u5fd7\u7740\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u6b63\u4ece\u201c\u5916\u6302\u786c\u76d8\u201d\u5411\u201c\u5185\u7f6e\u8ba4\u77e5\u7cfb\u7edf\u201d\u6f14\u8fdb\uff0c\u662f\u901a\u5f80AGI\u7684\u5173\u952e\u4e00\u6b65\u3002",
};
const dailyOverviews = {};
for (const date in dailyOverviewsRaw) {
    dailyOverviews[date] = dailyOverviewsRaw[date];
}


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文
        let isLoadingMore = false; // 是否正在加载更多

        // 获取未加载的日期
        function getUnloadedDates() {
            return availableDates.filter(date => !loadedDates.has(date));
        }

        // 加载更多日期的数据
        async function loadMoreDates() {
            if (isLoadingMore) return;

            const unloadedDates = getUnloadedDates();
            if (unloadedDates.length === 0) {
                showSimpleToast('已加载全部数据');
                return;
            }

            isLoadingMore = true;
            const loadBtn = document.getElementById('load-more-btn');
            if (loadBtn) {
                loadBtn.disabled = true;
                loadBtn.innerHTML = '<span class="animate-spin inline-block mr-2">⏳</span>加载中...';
            }

            const datesToLoad = unloadedDates.slice(0, LOAD_MORE_DAYS);
            let loadedCount = 0;

            for (const date of datesToLoad) {
                try {
                    const response = await fetch(`data/${date}.json`);
                    if (!response.ok) continue;

                    const dateData = await response.json();

                    // 将数据添加到 allPapers
                    allPapers[date] = dateData.categories;

                    // 添加每日速览
                    if (dateData.overview) {
                        dailyOverviews[date] = dateData.overview;
                    }

                    loadedDates.add(date);
                    loadedCount++;
                } catch (e) {
                    console.error(`加载 ${date} 数据失败:`, e);
                }
            }

            isLoadingMore = false;

            if (loadedCount > 0) {
                renderPapers();
                showSimpleToast(`已加载 ${loadedCount} 天的数据`);
            }

            updateLoadMoreButton();
        }

        // 更新"加载更多"按钮状态
        function updateLoadMoreButton() {
            const loadBtn = document.getElementById('load-more-btn');
            const unloadedCount = getUnloadedDates().length;

            if (loadBtn) {
                if (unloadedCount === 0) {
                    loadBtn.style.display = 'none';
                } else {
                    loadBtn.style.display = 'inline-flex';
                    loadBtn.disabled = false;
                    loadBtn.innerHTML = `📥 加载更多 (还有 ${unloadedCount} 天)`;
                }
            }
        }

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            const listItem = paperEl.closest('li');
            const categoryContent = paperEl.closest('.category-content');
            const sectionEl = paperEl.closest('section[data-date-section]');
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                if (listItem) {
                    listItem.remove();
                } else {
                    paperEl.remove();
                }

                updateCategoryView(categoryContent);
                updateDateSection(sectionEl);
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        function updateCategoryView(categoryContent) {
            if (!categoryContent) return;
            const listEl = categoryContent.querySelector('ul');
            if (!listEl) return;

            const paperItems = listEl.querySelectorAll('.paper-item').length;
            let placeholder = listEl.querySelector('.empty-category-placeholder');

            if (paperItems === 0) {
                if (!placeholder) {
                    placeholder = document.createElement('li');
                    placeholder.className = 'empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400';
                    placeholder.textContent = '此分类下暂无论文。';
                    listEl.appendChild(placeholder);
                }
            } else if (placeholder) {
                placeholder.remove();
            }

            const toggle = document.querySelector(`.category-toggle[data-target="${categoryContent.id}"]`);
            if (toggle) {
                const countBadge = toggle.querySelector('.category-count');
                if (countBadge) {
                    countBadge.textContent = paperItems;
                }
            }
        }

        function updateDateSection(sectionEl) {
            if (!sectionEl) return;
            const totalPapers = sectionEl.querySelectorAll('.paper-item').length;
            const header = sectionEl.querySelector('[data-date-heading]');

            if (header) {
                const dateLabel = header.dataset.dateHeading || header.textContent.split(' ')[0];
                header.textContent = `${dateLabel} (${totalPapers} 篇论文)`;
            }

            if (totalPapers === 0) {
                sectionEl.remove();
            }
        }

        // 可折叠功能
        function toggleCollapsible(header) {
            const content = header.nextElementSibling;
            const isOpen = header.classList.contains('open');
            
            if (isOpen) {
                header.classList.remove('open');
                content.classList.remove('open');
            } else {
                header.classList.add('open');
                content.classList.add('open');
            }
        }

        // 渲染所有 Markdown 内容
        function renderAllMarkdown() {
            // 配置 marked 选项
            if (typeof marked !== 'undefined') {
                marked.setOptions({
                    breaks: true,
                    gfm: true,
                    headerIds: false,
                    mangle: false
                });
                
                // 遍历所有灵感溯源的容器并渲染 Markdown
                for (const date in allPapers) {
                    const categories = allPapers[date];
                    categories.forEach(category => {
                        if (category.papers) {
                            category.papers.forEach(paper => {
                                if (paper.inspiration_trace) {
                                    const elementId = `inspiration-${paper.arxiv_id}`;
                                    const element = document.getElementById(elementId);
                                    if (element) {
                                        try {
                                            element.innerHTML = marked.parse(paper.inspiration_trace);
                                        } catch (e) {
                                            console.error('Markdown 渲染失败:', e);
                                            // 如果渲染失败，使用纯文本显示
                                            element.textContent = paper.inspiration_trace;
                                        }
                                    }
                                }
                            });
                        }
                    });
                }
            }
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-3 sm:mb-4">
                        <div class="flex items-start space-x-2 sm:space-x-3 flex-1 min-w-0">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5 sm:h-6 sm:w-6" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-base sm:text-lg font-semibold text-black dark:text-white leading-tight break-words">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-2 sm:ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-3 sm:mb-4">
                        <div class="flex flex-wrap items-center gap-2 sm:gap-4 text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                            <span class="break-all"><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300 whitespace-nowrap">
                                ${paper.category}
                            </span>
                            <span class="whitespace-nowrap">${date}</span>
                        </div>
                        <div class="text-xs sm:text-sm text-black dark:text-white break-words">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-3 sm:mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50 w-4 h-4">
                            <span class="ml-2 text-xs sm:text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">筛选原因</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.filter_reason}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">AI总结</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.summary2}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">原始摘要</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-3 sm:p-4 rounded-r-lg">
                                    ${paper.summary_translation ? `
                                    <div class="chinese-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: block;">
                                        ${paper.summary_translation}
                                    </div>
                                    ` : ''}
                                    ${paper.summary ? `
                                    <div class="english-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: none;">
                                        ${paper.summary}
                                    </div>
                                    ` : ''}
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.inspiration_trace ? `
                    <!-- 灵感溯源 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">灵感溯源</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-red-50/70 dark:bg-red-950/20 border-l-3 border-red-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed markdown-content break-words" id="inspiration-${paper.arxiv_id}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors whitespace-nowrap">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors whitespace-nowrap">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors whitespace-nowrap">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-2 sm:p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-2 sm:space-x-3 min-w-0 flex-1">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform flex-shrink-0" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400 text-sm sm:text-base truncate">${category.name}</span>
                        </div>
                        <span class="category-count text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5 ml-2 flex-shrink-0">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-2 sm:ml-4">
                        <ul class="space-y-3 sm:space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-6 sm:mb-8" data-date-section="${date}">
                        <h2 class="text-base sm:text-lg font-medium text-slate-500 dark:text-slate-400 mb-3 sm:mb-4" data-date-heading="${date}">${date} (${dateVisibleTotal} 篇论文)</h2>
                `;
                
                // 添加该日期的AI论文速览（如果存在）
                if (dailyOverviews[date]) {
                    html += `
                        <div class="mb-3 sm:mb-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md p-3 sm:p-5">
                            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                                <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2 text-blue-600 dark:text-blue-400 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path>
                                </svg>
                                <span class="font-semibold text-slate-900 dark:text-white text-sm sm:text-base">今日AI论文速览</span>
                            </div>
                            <div class="collapsible-content">
                                <div class="inner">
                                    <div class="markdown-content text-slate-700 dark:text-slate-200 text-xs sm:text-sm" id="overview-${date}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }
                
                html += `
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-3 sm:p-4 lg:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }

            // 添加"加载更多"按钮
            const unloadedCount = getUnloadedDates().length;
            if (unloadedCount > 0) {
                html += `
                    <div class="text-center py-6">
                        <button id="load-more-btn" onclick="loadMoreDates()"
                            class="inline-flex items-center px-6 py-3 text-base font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg shadow-md transition-all duration-200 hover:shadow-lg">
                            📥 加载更多 (还有 ${unloadedCount} 天)
                        </button>
                    </div>
                `;
            }

            mainContent.innerHTML = html;
            updateStats();
            
            // 渲染所有日期的 Markdown 速览内容
            for (const date in dailyOverviews) {
                const overview = dailyOverviews[date];
                const elementId = `overview-${date}`;
                const element = document.getElementById(elementId);
                if (element && overview) {
                    try {
                        if (typeof marked !== 'undefined') {
                            element.innerHTML = marked.parse(overview);
                        } else {
                            element.textContent = overview;
                        }
                    } catch (e) {
                        console.error('Markdown 渲染失败:', e);
                        element.textContent = overview;
                    }
                }
            }
            
            // 渲染所有论文的 Markdown 内容
            renderAllMarkdown();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>