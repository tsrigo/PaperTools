<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PaperTools - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 引入 Marked.js 用于 Markdown 渲染 -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        
        /* 移动端优化 */
        @media (max-width: 640px) {
            body {
                font-size: 14px;
            }
            
            /* 改善可点击区域 */
            button, a {
                min-height: 44px;
                min-width: 44px;
            }
            
            /* 优化间距 */
            .container {
                padding-left: 12px !important;
                padding-right: 12px !important;
            }
        }
        
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
        
        /* 可折叠部分样式 */
        .collapsible-header {
            cursor: pointer;
            display: flex;
            align-items: center;
            font-weight: 600;
            padding: 8px 0;
            user-select: none;
            color: #1e40af;
            transition: all 0.2s ease-in-out;
        }
        .dark .collapsible-header {
            color: #60a5fa;
        }
        .collapsible-header:hover {
            opacity: 0.8;
        }
        .collapsible-header::before {
            content: "▶";
            margin-right: 8px;
            transition: transform 0.3s ease;
            font-size: 0.8em;
        }
        .collapsible-header.open::before {
            transform: rotate(90deg);
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: none;
        }
        .collapsible-content .inner {
            padding-top: 8px;
        }
        
        /* Markdown 内容样式 */
        .markdown-content {
            line-height: 1.6;
        }
        .markdown-content h1 {
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 1em;
            margin-bottom: 0.5em;
            color: #1e40af;
        }
        .dark .markdown-content h1 {
            color: #60a5fa;
        }
        .markdown-content h2 {
            font-size: 1.3em;
            font-weight: bold;
            margin-top: 0.8em;
            margin-bottom: 0.4em;
            color: #1e40af;
        }
        .dark .markdown-content h2 {
            color: #60a5fa;
        }
        .markdown-content h3 {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 0.6em;
            margin-bottom: 0.3em;
            color: #1e40af;
        }
        .dark .markdown-content h3 {
            color: #60a5fa;
        }
        .markdown-content h4 {
            font-size: 1em;
            font-weight: bold;
            margin-top: 0.5em;
            margin-bottom: 0.25em;
            color: #2563eb;
        }
        .dark .markdown-content h4 {
            color: #93c5fd;
        }
        .markdown-content p {
            margin-bottom: 0.8em;
        }
        .markdown-content ul, .markdown-content ol {
            margin-left: 1.5em;
            margin-bottom: 0.8em;
        }
        .markdown-content ul {
            list-style-type: disc;
        }
        .markdown-content ol {
            list-style-type: decimal;
        }
        .markdown-content li {
            margin-bottom: 0.3em;
        }
        .markdown-content code {
            background-color: #f1f5f9;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
        .dark .markdown-content code {
            background-color: #334155;
        }
        .markdown-content pre {
            background-color: #f1f5f9;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 0.8em;
        }
        .dark .markdown-content pre {
            background-color: #334155;
        }
        .markdown-content pre code {
            background-color: transparent;
            padding: 0;
        }
        .markdown-content blockquote {
            border-left: 3px solid #cbd5e1;
            padding-left: 1em;
            margin-left: 0;
            margin-bottom: 0.8em;
            color: #64748b;
        }
        .dark .markdown-content blockquote {
            border-left-color: #475569;
            color: #94a3b8;
        }
        .markdown-content strong {
            font-weight: 600;
        }
        .markdown-content em {
            font-style: italic;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-3 sm:px-4 py-2 rounded-lg shadow-lg z-50 hidden max-w-xs sm:max-w-sm">
        <div class="flex items-center space-x-2">
            <span id="toast-message" class="text-sm sm:text-base">已删除</span>
            <span id="countdown" class="text-xs sm:text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-xs sm:text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-full lg:w-3/5 max-w-none p-3 sm:p-4 lg:p-6">
        <!-- 头部导航栏 -->
        <header class="mb-4 sm:mb-6">
            <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-3 sm:gap-4">
                <h1 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white">PaperTools</h1>
                <div class="flex flex-wrap items-center gap-2 sm:gap-3 w-full sm:w-auto">
                    <!-- 统计信息 -->
                    <div class="text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                        总计 <span id="total-papers">0</span> 篇论文
                    </div>
                    <!-- 筛选按钮 -->
                    <button id="filter-starred" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none transition-colors whitespace-nowrap">
                        显示全部
                    </button>
                    <!-- 中英文摘要切换按钮 -->
                    <button id="summary-toggle" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        中文摘要
                    </button>
                    <button id="theme-toggle" class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <!-- 太阳图标 (浅色模式) -->
                        <svg id="theme-icon-light" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                        </svg>
                        <!-- 月亮图标 (深色模式) -->
                        <svg id="theme-icon-dark" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                        </svg>
                    </button>
                    <!-- GitHub 图标按钮 -->
                    <a href="https://github.com/tsrigo/PaperTools" target="https://github.com/tsrigo/PaperTools" title="GitHub 项目主页"
                       class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <svg class="h-5 w-5 sm:h-6 sm:w-6 text-slate-700 dark:text-slate-200" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.021c0 4.428 2.865 8.184 6.839 9.504.5.092.682-.217.682-.483 0-.237-.009-.868-.014-1.703-2.782.605-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.004.07 1.532 1.032 1.532 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.339-2.221-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.254-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.025A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.295 2.748-1.025 2.748-1.025.546 1.378.202 2.396.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.847-2.337 4.695-4.566 4.944.359.309.678.919.678 1.852 0 1.336-.012 2.417-.012 2.747 0 .268.18.579.688.481C19.138 20.2 22 16.447 22 12.021 22 6.484 17.523 2 12 2z" clip-rule="evenodd"/>
                        </svg>
                    </a>
                </div>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-6 sm:space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2026-01-13": [
        {
            "name": "Artificial Intelligence",
            "count": 36,
            "papers": [
                {
                    "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                    "arxiv_id": "2601.07611",
                    "authors": "Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",
                    "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个多智能体框架（DIAGPaper），通过模拟具有特定专业知识的审稿人智能体和作者智能体进行结构化辩论和协作，以识别和验证论文弱点。这完全符合多智能体（协作、通信）的研究范围。",
                    "summary2": "本文旨在解决现有论文弱点识别系统模拟肤浅、缺乏有效性验证及未排序的问题。针对科学论文评审场景，我们提出了一种DIAGPaper多智能体框架，包含Customizer、Rebuttal和Prioritizer三个模块，分别负责定制评审标准、通过作者辩论验证弱点以及按严重程度排序。在AAAR和ReviewCritique数据集上，通过Semantic F1和Specificity等指标验证了其有效性。",
                    "summary_translation": "使用单智能体或多智能体大语言模型进行论文弱点识别已受到越来越多的关注，然而现有方法存在关键局限性。许多多智能体系统仅在表层模拟人类角色，未能捕捉到专家用于评估论文互补智力维度的潜在标准。此外，先前的方法隐含地假设识别出的弱点是有效的，忽略了审稿人偏见、误解以及作者反驳在验证审稿质量中的关键作用。最后，大多数系统输出未排序的弱点列表，而非为用户优先考虑影响最大的问题。在这项工作中，我们提出了DIAGPaper，这是一个新颖的多智能体框架，通过三个紧密集成的模块来解决这些挑战。定制器模块模拟人类定义的审稿标准，并实例化多个具备特定标准专业知识的审稿人智能体。反驳模块引入作者智能体，使其与审稿人智能体进行结构化辩论，以验证和完善提出的弱点。优先级排序器模块从大规模人类审稿实践中学习，以评估已验证弱点的严重程度",
                    "inspiration_trace": "基于对论文《DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning》的深入分析，以下是作者构建该方法的逻辑演进过程推演：\n\n### 1. 宏观观察：从“角色扮演”到“专家思维”的缺失\n**起点：** 自动化论文审稿领域正从单一LLM向多智能体系统演进。\n**观察：** 现有的多智能体系统（如AgentReview, MARG）大多停留在“表面模拟”阶段。它们只是简单地给智能体分配角色（如“审稿人”、“作者”或“领域主席”），或者按文本段落分工。\n**问题识别：** 真正的人类专家审稿并非仅仅因为身份不同，而是因为**关注的具体评价维度不同**。现有系统缺乏对“评价标准”的显式建模，导致生成的评论泛泛而谈，缺乏针对性。\n\n### 2. 深度诊断：有效性与实用性的双重危机\n在进一步观察中，作者发现了两个更深层次的逻辑漏洞：\n*   **漏洞一（有效性假设谬误）：** 现有系统默认AI生成的弱点是正确的。但在现实中，审稿人常有偏见或误解。**作者反驳**是验证评论质量的关键环节，而现有系统大多忽略了这一“纠错”机制。\n*   **漏洞二（输出效用低）：** 即使生成了正确的弱点，系统通常以平铺列表的形式输出。然而，对于作者而言，区分“致命缺陷”和“轻微瑕疵”至关重要。缺乏优先级排序使得AI审稿的实用性大打折扣。\n\n### 3. 核心假设：模拟“机制”而非模拟“人”\n**假设提出：** 要提高AI审稿的质量，不应只模拟审稿人的“身份”，而应模拟高质量审稿的“内在机制”。\n**逻辑推演：**\n*   机制一：**定制化规划**。专家在拿到论文后，会根据论文内容动态确定审查重点（如：这篇论文主要贡献是数据集，那么审查重点就是数据质量，而非数学推导）。\n*   机制二：**对抗性验证**。评论的有效性不是自证的，而是在与作者的辩论中确立的。只有经得起反驳的弱点，才是真正的弱点。\n*   机制三：**后果导向**。弱点的严重程度取决于其对最终录用决策的影响权重。\n\n### 4. 方法论构建：三模块闭环架构\n基于上述假设，作者构建了DIAGPaper框架，将思考过程转化为三个紧密耦合的模块：\n\n*   **第一步：解构专家思维 -> Customizer（定制器模块）**\n    *   *思考：* 如何让智能体像专家一样有针对性？\n    *   *方案：* 不再使用固定的角色，而是引入一个“定制器”智能体。它先阅读论文，动态生成具体的、细粒度的评价维度（如“数据集的代表性如何？”），然后据此实例化多个具有特定专长的“审稿人智能体”。\n\n*   **第二步：引入对抗验证 -> Rebuttal（反驳模块）**\n    *   *思考：* 如何过滤掉那些看似合理实则错误的幻觉评论？\n    *   *方案：* 引入“作者智能体”。针对每一个审稿人提出的弱点，作者智能体进行逐点反驳。这是一个多轮的、基于证据的辩论过程。如果审稿人无法提供充分的证据或逻辑来支撑其观点，该弱点就会被过滤掉（实验显示过滤掉了40%-60%的初始弱点）。\n\n*   **第三步：模拟决策权重 -> Prioritizer（优先级模块）**\n    *   *思考：* 如何让输出对用户最友好？\n    *   *方案：* 学习人类Meta-review（综合讨论）的行为。分析大量历史数据，计算出不同类别的弱点（如方法缺陷 vs 写作问题）对最终拒稿/录用的影响权重。结合辩论后的有效性得分，对幸存的弱点进行排序，只输出Top-K最严重的问题。\n\n### 5. 逻辑验证与闭环\n**最终思考：** 这个框架是否真的有效？\n*   *验证逻辑：* 如果这个框架是正确的，那么它应该能显著提升开源模型的表现（通过结构化思维弥补能力不足），并且在“有效性”指标上远超现有方法。\n*   *结果确认：* 实验表明，通过DIAGPaper的“多智能体化”，开源模型能达到接近GPT-4o的水平，且生成的弱点在“有效性”和“特异性”上均显著优于基线。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（现有多智能体系统肤浅）**出发，深入到**本质（缺乏评价标准、缺乏验证机制、缺乏优先级）**，最终通过**机制重构（动态定制、对抗辩论、严重度排序）**实现了对人类审稿深层逻辑的还原。"
                },
                {
                    "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents",
                    "arxiv_id": "2601.07577",
                    "authors": "Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",
                    "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于LLM智能体的规划机制（属于研究范围1：单智能体-规划）。它提出了任务解耦规划（TDP）框架，旨在解决长视界智能体中的规划瓶颈和错误传播问题，直接涉及智能体的任务分解、执行与重规划策略。",
                    "summary2": "本文旨在解决长视界智能体规划中上下文纠缠导致的鲁棒性差和效率低问题。针对复杂长视界任务，我们提出了一种Task-Decoupled Planning (TDP)框架，通过Supervisor构建任务DAG，并利用Planner和Executor在局部作用域内解耦规划与执行。我们在TravelPlanner、ScienceWorld和HotpotQA上通过Delivery、Accuracy和Average Reward等指标验证了其有效性，结果表明TDP在提升性能的同时将token消耗降低了82%。",
                    "summary_translation": "大语言模型的最新进展已使智能体能够自主执行复杂的 long-horizon tasks（长视界任务），然而规划仍然是实现可靠任务执行的主要瓶颈。现有方法通常分为两种范式：step-wise planning（逐步规划），具有反应性但往往较为短视；以及 one-shot planning（一次性规划），能够预先生成完整计划，但对执行错误较为脆弱。关键在于，这两种范式都存在 entangled contexts（纠缠上下文）的问题，即智能体必须基于跨越多个子任务的 monolithic history（整体历史）进行推理。这种纠缠增加了 cognitive load（认知负荷），并导致 local errors（局部错误）在原本独立的决策之间传播，从而使得错误恢复的计算成本高昂。为解决这一问题，我们提出了 Task-Decoupled Planning (TDP，任务解耦规划)，这是一个 training-free（免训练）框架，旨在用任务解耦替代纠缠推理。TDP 通过 Supervisor（监督者）将任务分解为由子目标组成的 directed acyclic graph (DAG，有向无环图)。通过利用具有 scoped contexts（限定上下文）的 Planner（规划器）和 Executor（执行器），TDP 将推理和重新规划的范围限制在 active sub-task（当前活动子任务）内。这种隔离机制防止了 error propagation（错误传播），并能够在不干扰 workflow（工作流）的情况下局部修正 deviations（偏差）。在 TravelPlanner、ScienceWorld 和 HotpotQA 上的实验结果表明，TDP 不仅优于强大的 baselines（基线模型），还将 token consumption（令牌消耗）减少了高达 82%，证明了子任务解耦能够提升 long-horizon agents（长视界智能体）的 robustness（鲁棒性）和 efficiency（效率）。",
                    "inspiration_trace": "基于论文《Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents》，以下是对作者核心方法论提出过程的逻辑链推演与思想还原：\n\n### 1. 宏观观察：长程任务的规划瓶颈\n**思考起点：** 随着大语言模型（LLM）能力的提升，智能体已经能够处理复杂的、长周期的自主任务。然而，作者发现尽管模型的理解和推理能力在增强，**“规划”** 依然是制约智能体在长程任务中表现可靠性的核心瓶颈。\n*   **现象：** 任务越复杂、步骤越多，智能体越容易迷失方向或执行失败。\n*   **初步问题：** 现有的规划方法为什么无法有效支撑长程任务？\n\n### 2. 现状剖析：两种范式的共通缺陷\n**思考过程：** 作者首先审视了当前领域内解决规划问题的两大主流范式，试图找出它们的局限性。\n*   **范式 A：逐步规划**\n    *   *特点：* 边思考边行动（如 ReAct）。\n    *   *优点：* 反应快，能适应反馈。\n    *   *缺点：* 目光短浅，缺乏全局观，容易在长程任务中走偏。\n*   **范式 B：一次性规划**\n    *   *特点：* 先生成完整计划再执行（如 Plan-and-Act）。\n    *   *优点：* 具备全局视野。\n    *   *缺点：* 脆弱，一旦执行出错或环境变化，原计划容易失效。\n*   **深度洞察（关键转折）：** 作者发现，虽然这两种方法在“规划粒度”上截然不同（一个细碎，一个宏观），但它们在**底层设计逻辑**上存在一个惊人的共同缺陷——**“上下文纠缠”**。\n    *   *问题本质：* 两者都将整个任务视为一个**单一的、整体的工作流**。智能体在推理时，必须依赖一个不断增长的、混合了所有子任务信息的“整体历史记录”。\n\n### 3. 核心洞察：从“粒度”转向“耦合”\n**思考深化：** 既然调整规划的“粒度”（更细或更粗）无法根本解决问题，作者意识到问题的根源不在于“多久规划一次”，而在于“信息是如何组织的”。\n*   **痛点分析：**\n    1.  **认知负荷过载：** 当上下文窗口中塞满了所有子任务的历史细节时，模型难以聚焦于当前需要解决的子问题。\n    2.  **错误传播：** 如果在子任务 A 中出现局部错误，由于上下文是纠缠的，模型往往需要重新审视甚至重做无关的子任务 B，导致计算成本高昂且脆弱。\n*   **假设提出：** 如果能打破这种“纠缠”，将任务进行**解耦**，就能隔离错误并降低推理负担。\n*   **核心思想：** **任务解耦**。即：将长程任务拆解为独立的子任务，让每个子任务的规划与执行都在**受限的局部上下文**中进行，互不干扰。\n\n### 4. 方法论构建：任务解耦的架构设计\n**思考落地：** 为了实现“解耦”这一抽象概念，作者需要设计一套具体的架构，将“全局视野”与“局部执行”分离开来。\n\n*   **第一步：全局结构化**\n    *   *需求：* 既然要解耦，就需要一个顶层结构来定义子任务之间的关系，否则系统会散架。\n    *   *设计：* 引入 **Supervisor（监督者）**。它的职责不是做具体执行，而是将大任务分解为有依赖关系的**有向无环图（DAG）**。这定义了“做什么”以及“先做什么”。\n\n*   **第二步：局部化执行**\n    *   *需求：* 确保执行子任务 A 时，完全看不到子任务 B 的具体执行细节，只看结果。\n    *   *设计：* 引入 **Planner（规划器）** 和 **Executor（执行器）**。\n    *   *关键机制：* **作用域上下文**。这两个模块只能看到当前节点（子任务）的描述、前置节点的结果以及当前节点的执行轨迹。这种设计强制实现了“上下文隔离”。\n\n*   **第三步：局部化纠错**\n    *   *需求：* 当执行出错时，不能推倒重来，只能局部修复。\n    *   *设计：* 当发生偏差时，触发**节点级重规划**。只修改当前节点的计划，而不影响 DAG 中其他已完成或未开始的部分。这从机制上切断了错误传播的路径。\n\n*   **第四步：动态一致性维护**\n    *   *需求：* 局部执行可能会导致全局目标不可达（例如：前置任务的结果改变了后续任务的条件）。\n    *   *设计：* 引入 **Self-Revision（自我修正）**。在每批节点完成后，检查全局状态，更新 DAG（如修改节点描述、增删节点），确保全局与局部的一致性。\n\n### 5. 逻辑闭环：局部化与全局性的平衡\n**思考验证：** 作者通过这套架构（TDP），试图证明一个观点：**通过显式的架构设计控制上下文范围，比单纯依赖模型的推理能力更有效。**\n*   **预期结果：**\n    *   **鲁棒性：** 错误被锁在局部，不会扩散。\n    *   **效率：** 模型不需要反复处理无关的长历史，Token 消耗大幅降低。\n*   **实验验证：** 选取 TravelPlanner（工具调用）、ScienceWorld（交互控制）、HotpotQA（多跳推理）三个差异巨大的场景进行验证，证明这种“解耦”思想具有普适性。\n\n---\n\n**总结：**\n作者的思考路径是从**表象问题**（长程任务规划难）出发，透过**现有方法的共性缺陷**（上下文纠缠），抓住了**本质矛盾**（认知负荷与错误传播），最终提出了**“任务解耦”**这一核心范式，并通过**Supervisor-Planner-Executor**的三层架构将这一思想工程化，实现了从“调整粒度”到“解耦架构”的范式跃迁。"
                },
                {
                    "title": "VirtualEnv: A Platform for Embodied AI Research",
                    "arxiv_id": "2601.07553",
                    "authors": "Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",
                    "summary": "As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. VirtualEnv supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. We provide a user-friendly API built on top of Unreal Engine, allowing researchers to deploy and control LLM-driven agents using natural language instructions. We integrate large-scale LLMs and vision-language models (VLMs), such as GPT-based models, to generate novel environments and structured tasks from multimodal inputs. Our experiments benchmark the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. We also describe our methodology for procedural task generation, task validation, and real-time environment control. VirtualEnv is released as an open-source platform, we aim to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个用于评估LLM智能体的具身AI平台，重点研究了智能体的规划、适应性以及多智能体协作与协调能力，符合单智能体（规划）和多智能体（协作）的研究范围。虽然涉及平台构建，但其核心目的是为了推进具身智能体的研究，而非单纯的部署优化或纯视觉模型研究。",
                    "summary2": "本文旨在解决现有仿真器在规模、多样性和交互性上的局限，满足Embodied AI对高保真环境的需求。针对LLMs在复杂交互场景中的评估，我们提出了VirtualEnv，一个基于Unreal Engine 5的仿真平台，支持多模态感知、语言驱动的任务生成及多代理协作。我们在Escape Room挑战及多种Embodied任务上，通过任务成功率、视觉保真度排名等指标验证了其有效性。",
                    "summary_translation": "随着 large language models (LLMs，大语言模型) 在推理和决策能力上的持续提升，对于能够严格评估其能力的逼真且交互式的环境需求日益增长。我们提出了 VirtualEnv，这是一个基于 Unreal Engine 5 (虚幻引擎 5) 构建的下一代模拟平台，旨在对具身和交互场景中的 LLMs 进行 fine-grained benchmarking (细粒度基准测试)。VirtualEnv 支持丰富的 agent-environment interactions (智能体与环境交互)，包括 object manipulation (物体操作)、navigation (导航) 和 adaptive multi-agent collaboration (自适应多智能体协作)，以及受游戏启发的机制，如 escape rooms (密室逃脱) 和 procedurally generated environments (程序化生成环境)。我们提供了一个基于 Unreal Engine 构建的 user-friendly API (用户友好型 API)，允许研究人员使用 natural language instructions (自然语言指令) 来部署和控制由 LLMs 驱动的 agents (智能体)。我们集成了大规模 LLMs 和 vision-language models (VLMs，视觉语言模型)，例如 GPT-based models (基于 GPT 的模型)，以便从 multimodal inputs (多模态输入) 生成 novel environments (新颖环境) 和 structured tasks (结构化任务)。我们的实验在复杂度递增的任务中对几种流行的 LLMs 的性能进行了 benchmarking (基准测试)，分析了它们在 adaptability (适应性)、planning (规划) 和 multi-agent coordination (多智能体协调) 方面的差异。我们还描述了我们在 procedural task generation (程序化任务生成)、task validation (任务验证) 和 real-time environment control (实时环境控制) 方面的 methodology (方法论)。VirtualEnv 作为 open-source platform (开源平台) 发布，我们旨在推动 AI (人工智能) 与 gaming (游戏) 交叉领域的研究，实现 embodied AI settings (具身人工智能场景) 下对 LLMs 的 standardized evaluation (标准化评估)，并为 immersive simulations (沉浸式模拟) 和 interactive entertainment (交互娱乐) 的未来发展铺平道路。",
                    "inspiration_trace": "基于论文《VirtualEnv: A Platform for Embodied AI Research》的内容，以下是对作者产出该文章核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 1. 宏观观察：LLM时代的“具身”需求\n**思考起点：** 随着大语言模型（LLMs）和视觉语言模型（VLMs）在推理和决策能力上的突破，研究重心正从单纯的文本处理转向更复杂的“具身智能”。\n**核心问题：** 现有的评估方式（如纯文本测试）已不足以检验这些模型的真实能力。我们需要一个能够将语言指令与物理世界交互紧密结合的测试床，以评估模型在真实场景中的推理、规划和适应能力。\n\n### 2. 痛点识别：现有模拟器的“二元对立”困境\n**深入观察：** 作者审视了现有的模拟平台（如AI2-THOR, VirtualHome, Habitat等），发现它们存在明显的局限性，形成了一种“二元对立”的困境：\n*   **对立面A（学术模拟器）：** 专注于AI研究，具备良好的语义结构和可编程性，但视觉保真度低，环境规模小（多局限于室内家庭），交互僵硬，难以模拟复杂的现实世界。\n*   **对立面B（游戏引擎）：** 拥有极高的视觉真实感和物理交互性，但缺乏AI研究所需的模块化、语义标签以及与LLM集成的接口。\n**推论：** 现有的工具无法同时满足“高保真视觉”与“深度AI研究逻辑”的双重需求，这成为了制约LLM在具身场景下发展的瓶颈。\n\n### 3. 核心假设：游戏引擎与AI研究的深度融合\n**提出假设：** 如果能利用现代游戏引擎（Unreal Engine 5）的强大渲染和物理能力，并在其之上构建一层专门为AI研究设计的语义和交互接口，就能打破上述困境。\n**策略定位：** 创建一个“下一代”平台，它不仅是一个游戏，更是一个可编程的实验室。这个平台必须支持：\n*   **大规模与多样性：** 超越单一室内场景，覆盖室内外及城市环境。\n*   **深度交互：** 支持精细的物体操作和多智能体协作。\n*   **原生AI集成：** LLM不应只是外部调用者，而应深度参与环境的生成和控制。\n\n### 4. 方法论构建：从“静态环境”到“语言驱动生成”\n**具体化思路：** 传统的模拟器依赖人工预设场景，成本高且扩展性差。作者思考如何利用LLM的特性来解决数据稀缺问题。\n*   **思路一：环境构建的自动化。** 利用LLM理解自然语言描述，自动生成场景图和任务逻辑，再通过API渲染成3D环境。这实现了从“手动搭建”到“语言驱动生成”的飞跃。\n*   **思路二：动态交互的闭环。** 引入VLM（视觉语言模型）作为中间件，让Agent能够通过自然语言修改环境（如“把钥匙放进盒子里”），并实时验证修改结果，形成感知-决策-执行的闭环。\n\n### 5. 验证设计：超越“捡苹果”的密室逃脱挑战\n**思考挑战：** 传统的具身AI任务（如导航、抓取物体）过于简单，难以充分测试LLM的高级推理能力。\n**创新方案：** 作者引入了“密室逃脱”作为核心测试框架。\n*   **逻辑：** 密室逃脱天然包含多步推理、线索关联、工具使用和谜题解决，这正好对应了LLM的强项（推理）和弱项（长程规划与幻觉）。\n*   **分层设计：** 通过设计从简单到复杂的四个难度等级（单步、序列、元线索、欺骗线索），可以系统性地评估模型在不同认知负荷下的表现。\n\n### 6. 实验反思：对LLM能力边界的再认知\n**最终验证：** 通过对比实验，作者不仅验证了平台的高保真度（通过用户调研），更重要的是揭示了LLM在具身场景中的具体缺陷。\n*   **发现：** 即使是先进的推理模型，在部分可观测环境下也容易陷入“探索循环”或产生“幻觉目标”。\n*   **意义：** 这证明了VirtualEnv的价值——它不仅是一个工具，更是一个诊断器，帮助研究者看清当前AI在物理世界交互中的真实短板，从而指明未来的改进方向（如增强空间记忆）。\n\n---\n\n**总结：**\n作者的思考路径遵循了**“需求演进（LLM需要身体） -> 工具批判（现有工具不够好） -> 架构创新（UE5+AI接口） -> 范式转移（语言驱动生成） -> 极限测试（密室逃脱）”**的逻辑闭环。VirtualEnv的诞生，本质上是试图填补顶级游戏技术与前沿AI研究之间的鸿沟。"
                },
                {
                    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
                    "arxiv_id": "2601.07477",
                    "authors": "Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",
                    "summary": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于优化基于LLM的智能体工作流，提出了一个包含Judge模块的流水线来分析执行轨迹并修改工作流逻辑块。这属于单智能体的规划与自我演化（通过反馈自我完善）范畴，虽然评估使用了推理基准，但核心贡献在于智能体工作流的优化方法，而非纯推理算法本身。",
                    "summary2": "本文旨在解决优化基于LLM的agentic工作流时缺乏细粒度反馈信号的问题。针对复杂的agentic工作流，我们提出了一种名为JudgeFlow的Evaluation-Judge-Optimization-Update流水线，该方法引入Logic Blocks和Judge模块进行块级诊断与针对性优化。我们在GSM8K、MATH、MBPP和HumanEval基准上通过解决率和pass@1验证了其有效性，结果显示其优于现有方法。",
                    "summary_translation": "优化基于大语言模型（LLM）的智能体工作流对于扩展人工智能能力而言是一项挑战。现有方法依赖于粗糙的端到端评估信号，缺乏关于具体改进位置的细粒度信号，往往导致低效或低影响力的修改。为了解决这些局限性，我们提出了 JudgeFlow，一种评估-判断-优化-更新流水线。我们将可复用、可配置的逻辑块整合到智能体工作流中，以捕捉基本的逻辑形式。在此抽象基础上，我们设计了一个专用的 Judge 模块，用于检查执行轨迹——特别是失败的运行——并为有问题的逻辑块分配基于排名的责任分数。这些细粒度的诊断信号随后被基于大语言模型的优化器利用，该优化器将修改集中在工作流中最有问题的逻辑块上。我们的方法提高了样本效率，通过块级诊断增强了可解释性，并为自动化日益复杂的智能体工作流提供了可扩展的基础。我们在数学推理和代码生成基准上评估了 JudgeFlow，结果表明 JudgeFlow 相比现有方法实现了更优越的性能和效率。源代码已在 https://github.com/ma-zihan/JudgeFlow 公开提供。",
                    "inspiration_trace": "基于论文《JudgeFlow: Agentic Workflow Optimization via Block Judge》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程：\n\n### 第一阶段：宏观观察与问题界定\n**（从“手工设计”到“自动化优化”的范式转移）**\n\n1.  **观察趋势**：\n    *   作者观察到AI领域正从单一的LLM（基础模型）向复杂的Agentic Workflows（智能体工作流）演进。\n    *   早期依赖Prompt Engineering（如CoT），现在发展为多Agent系统（如AutoGen, MetaGPT），这些系统通过复杂的代码逻辑（循环、条件分支）来解决问题。\n\n2.  **识别痛点**：\n    *   **手工瓶颈**：高性能的工作流设计极其依赖专家经验，成本高、灵活性差。\n    *   **自动化需求**：借鉴AutoML的思想，学术界开始尝试自动设计工作流（如AFlow使用MCTS搜索）。\n    *   **核心矛盾**：现有的自动化方法（如基于代码或图的搜索）主要依赖**端到端的粗粒度反馈**（即只看最终答案对不对）。这就像修车时只听引擎响不响，却不知道是哪个零件坏了。这导致优化过程效率低下，往往是“盲人摸象”式的盲目搜索，修改可能无效甚至产生负面影响。\n\n### 第二阶段：核心假设与切入点\n**（从“盲目搜索”到“精准归因”的思维跃迁）**\n\n1.  **提出假设**：\n    *   作者认为，提升优化效率的关键在于**细粒度的诊断信号**。\n    *   如果能精确知道工作流中的哪一部分（哪个模块）导致了任务失败，优化器就可以集中精力修复该部分，而不是随机修改整个结构。\n\n2.  **面临挑战**：\n    *   **归因困难**：在代码表示的工作流中，存在复杂的控制流（如if-else分支）。某些代码路径在特定执行中并未运行，很难判断错误是源于“引入错误的模块”还是“未能修正错误的模块”。\n    *   **抽象层级**：直接在代码行级别进行归因太细且难以理解；直接在整个工作流层面归因又太粗。\n\n### 第三阶段：抽象层级的重构\n**（引入“逻辑块”作为中间语义层）**\n\n1.  **设计创新**：\n    *   为了解决归因难题，作者提出了一种介于“原子算子”和“完整代码”之间的中间抽象——**逻辑块**。\n    *   **思考逻辑**：将复杂的代码结构归纳为三种最基本的逻辑形式：**顺序**、**循环**、**条件**。\n    *   **目的**：\n        *   **封装性**：将动态的控制流（如if-else）封装在一个稳定的语义单元内，避免了因分支未执行而导致的归因歧义。\n        *   **可解释性**：为后续的“诊断”提供了清晰的语义边界。\n\n### 第四阶段：诊断机制的设计\n**（从“评估者”到“法官”的角色进化）**\n\n1.  **引入Judge模块**：\n    *   作者意识到，传统的Evaluator只负责打分（对/错），这不够。我们需要一个**Judge**，负责在失败案例中进行“责任认定”。\n    *   **工作原理**：Judge分析执行轨迹，特别是失败的轨迹，对各个逻辑块进行**责任排序**。\n\n2.  **处理噪声与模糊性**：\n    *   单次判断可能不准，作者采用**基于排名的聚合机制**。通过统计多次失败中各块的排名，找出那个“最常背锅”或“最常导致失败”的块。\n    *   这将模糊的错误信号转化为了精确的优化目标。\n\n### 第五阶段：闭环系统的形成\n**（构建“评估-审判-优化-更新”的完整闭环）**\n\n1.  **整合流程**：\n    *   将上述组件串联起来，形成JudgeFlow的核心Pipeline：\n        *   **Evaluation**：跑数据，看结果。\n        *   **Judge**：如果失败，分析Trace，找出最差的Block。\n        *   **Optimization**：LLM优化器针对这个特定的“最差Block”进行定向修改（增加、删除或重构），而不是乱改。\n        *   **Update**：更新工作流池。\n\n2.  **逻辑验证**：\n    *   这种设计将原本无方向的“黑盒搜索”转变为有方向的“白盒修复”。它不仅提高了样本效率（改得准），还增强了可解释性（知道为什么改）。\n\n---\n\n**总结：作者的思考路径**\n从**“工作流自动化是大势所趋”**出发，敏锐地发现**“现有方法缺乏细粒度反馈导致效率低下”**这一核心痛点。为了解决**“代码级归因太难”**的技术障碍，创造性地引入了**“逻辑块”**这一中间抽象，并借鉴司法审判的思路设计了**“Judge模块”**来精准定位错误源头，最终构建了一个**“先诊断，后开方”**的高效自动化优化闭环。"
                },
                {
                    "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
                    "arxiv_id": "2601.07470",
                    "authors": "Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",
                    "summary": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于LLM智能体的记忆管理机制，提出了元认知记忆抽象方法（MCMA）来优化记忆的结构化、抽象和重用，以解决长期决策任务。这直接属于研究范围中的“单智能体：记忆”范畴，且不属于排除项。",
                    "summary2": "本文旨在解决LLM智能体记忆表示固定、抽象层次单一导致的泛化受限和负迁移问题。针对长视距交互决策任务，我们提出了一种Meta-Cognitive Memory Abstraction (MCMA) 方法，通过解耦任务执行与记忆管理，利用DPO训练Memory Copilot学习分层结构化记忆抽象策略。在ALFWorld、ScienceWorld和BabyAI数据集上，通过任务成功率、执行步数和奖励分数验证了其有效性和跨任务迁移能力。",
                    "summary_translation": "大语言模型智能体日益依赖累积记忆来解决长视界决策任务。然而，大多数现有方法将记忆存储在固定表示中，并在单一或隐式抽象层级上进行重用，这限制了泛化能力，且在发生分布偏移时往往导致负迁移。本文提出了元认知记忆抽象方法，该方法将记忆抽象视为一种可学习的认知技能，而非固定的设计选择。MCMA 通过结合冻结的任务模型与可学习的记忆副驾驶，实现了任务执行与记忆管理的解耦。该记忆副驾驶利用直接偏好优化进行训练，负责确定记忆的结构化、抽象及重用方式。记忆被进一步组织成抽象层级体系，从而能够基于任务相似度实现选择性重用。当不存在可迁移的记忆时，MCMA 通过迁移记忆副驾驶来传递抽象和管理记忆的能力。在 ALFWorld、ScienceWorld 和 BabyAI 上的实验表明，相较于多个基线方法，MCMA 在性能、分布外泛化以及跨任务迁移方面均实现了显著提升。",
                    "inspiration_trace": "基于论文《Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程。\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**（从“记忆很重要”到“现有记忆机制很脆弱”）**\n\n1.  **观察背景**：随着LLM智能体从静态问答转向长视界、交互式的复杂任务（如ALFWorld, ScienceWorld），智能体必须依赖“程序性记忆”来积累经验，以实现持续决策。\n2.  **发现问题**：尽管现有方法都在尝试存储和检索记忆，但在面对环境变化或任务分布偏移时，性能急剧下降，甚至出现“负迁移”。\n3.  **初步诊断**：现有的记忆机制过于僵化。它们大多将记忆视为静态的“内容”，用固定的格式（如纯文本、固定的键值对）和固定的抽象层级来存储。\n\n### 第二阶段：深入诊断与核心矛盾\n**（从“方法失效”到“抽象困境”）**\n\n1.  **剖析现有范式**：\n    *   **检索式**：直接复用历史轨迹。这导致过度拟合表面细节，一旦环境物体位置改变，记忆即失效。\n    *   **总结/抽象式**：试图提取高层规则。但这面临**“抽象困境”**：太细粒度则过拟合，太抽象则失去可执行性，变成正确的废话。\n    *   **训练式**：将经验内化到模型参数中。这导致记忆与策略耦合，难以跨任务迁移，且容易发生灾难性遗忘。\n2.  **提炼核心矛盾**：现有方法都是**“预设”**了记忆应该如何被表示和抽象。智能体并没有学会“如何记忆”，它只是在使用一个人类设计好的、僵化的存储桶。\n3.  **关键洞察**：人类之所以能灵活迁移记忆，是因为我们拥有**元认知**能力——即“关于思考的思考”。我们不仅存储知识，还学会了“如何组织知识”的认知技能。\n\n### 第三阶段：假设提出与范式转移\n**（从“存储内容”到“学习技能”）**\n\n1.  **核心假设**：记忆抽象不应是一个固定的工程设计，而应是一个**可习得的认知技能**。如果让智能体学会“如何记忆”，它就能自适应地决定记忆的结构和粒度。\n2.  **概念创新**：提出**“元认知记忆抽象”**。目标不是生成完美的记忆内容，而是训练一个能够根据任务需求动态生成记忆结构的“管理者”。\n3.  **架构构想**：为了验证这一假设，必须将“记忆管理”与“任务执行”解耦。如果混在一起，就无法单独评估记忆管理策略的好坏。\n\n### 第四阶段：方法论构建与逻辑闭环\n**（从“概念”到“Memory Copilot”）**\n\n1.  **解耦设计**：\n    *   **任务模型**：保持冻结，只负责执行动作，作为评估记忆好坏的“裁判”。\n    *   **记忆副驾驶**：这是核心创新点。它是一个独立的模型，专门负责将原始轨迹转化为结构化记忆。\n2.  **解决“抽象困境”的机制**：\n    *   **多结构生成**：不预设单一结构，而是让Copilot从树、链、键值对等多种原语中组合出最合适的记忆结构。\n    *   **基于效用的训练**：如何训练Copilot？利用任务模型的下游表现作为反馈。如果某种结构的记忆让任务完成得又快又好，这种结构就被奖励。\n3.  **训练算法选择**：采用**直接偏好优化（DPO）**。通过对比不同记忆结构带来的任务效果，构建偏好对，让Copilot学会生成那些能带来高任务效用的记忆表示。\n\n### 第五阶段：泛化与终极迁移\n**（从“复用知识”到“复用能力”）**\n\n1.  **分层抽象**：为了适应不同相似度的任务，构建记忆层级（从具体的情节记忆到抽象的语义记忆）。相似任务用细节记忆，不相似任务用抽象记忆。\n2.  **解决零样本迁移**：当遇到一个完全陌生的领域，没有任何旧记忆可以复用时怎么办？\n3.  **最终逻辑升华**：此时，我们不再转移“记忆内容”，而是转移**“记忆Copilot本身”**。因为Copilot学到的是“如何从新经验中提炼知识”的元认知能力。这种能力是跨域通用的。\n\n---\n\n**总结：作者的思考路径**\n从**“记忆内容僵化导致泛化失败”**的观察出发，通过**“引入元认知视角”**将问题转化为**“学习记忆抽象技能”**，进而通过**“任务/记忆解耦”**和**“基于效用的DPO训练”**实现了这一技能的习得，最终达成**“不仅复用知识，更复用学习能力”**的通用智能体目标。"
                },
                {
                    "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents",
                    "arxiv_id": "2601.07468",
                    "authors": "Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",
                    "summary": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于LLM智能体的核心组件——记忆机制，提出了时间语义记忆（TSM）框架以解决个性化智能体中的时间建模问题，属于单智能体研究范畴中的“记忆”方向。",
                    "summary2": "本文旨在解决现有LLM Agent记忆中时间不准确和碎片化的问题。针对个性化长期对话场景，我们提出了一种Temporal Semantic Memory (TSM)框架，通过构建语义时间线和持续记忆来捕捉持久状态，并在LONG MEM EVAL和LOCOMO数据集上通过Accuracy指标验证了其有效性。",
                    "summary_translation": "记忆机制使 Large Language Model (LLM) agents (大语言模型智能体) 能够感知、存储并利用过往对话中的信息，这对于实现个性化至关重要。然而，现有方法未能对记忆的时间维度进行恰当建模，主要体现在两个方面：1) 时间不准确性：记忆是按对话时间而非实际发生时间进行组织的；2) 时间碎片化：现有方法侧重于 point-wise memory (点状记忆)，从而丢失了能够捕捉持久状态和演变模式的持续信息。为解决上述局限性，我们提出了 Temporal Semantic Memory (TSM) (时间语义记忆)，这是一个为 point-wise memory (点状记忆) 建模 semantic time (语义时间)，并支持 durative memory (持续记忆) 构建与利用的记忆框架。在记忆构建阶段，该框架首先构建 semantic timeline (语义时间轴)，而非对话时间轴。随后，它将时间上连续且语义相关的信息整合为 durative memory (持续记忆)。在记忆利用阶段，该框架结合查询在 semantic timeline (语义时间轴) 上的时间意图，实现对时间上恰当的 durative memory (持续记忆) 的检索，并提供时间有效且持续时间一致的上下文以支持响应生成。在 LongMemEval 和 LoCoMo 数据集上的实验表明，TSM 始终优于现有方法，并实现了高达 12.2% 的准确率绝对提升，验证了所提方法的有效性。",
                    "inspiration_trace": "基于论文《Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents》，以下是对作者核心方法论产出过程的系统性逻辑推演：\n\n### 1. 宏观观察：记忆是LLM Agent长期交互的瓶颈\n**思考起点**：随着LLM Agent从单次问答转向长期、多轮次的个性化交互，记忆成为关键组件。现有的记忆机制（如RAG、向量数据库）虽然能存储信息，但在处理跨越数周甚至数月的复杂对话时，往往表现笨拙。\n**核心质疑**：为什么Agent在处理“时间敏感”或“长周期状态”的问题时经常出错？仅仅是因为存得不够多吗？还是记忆的组织方式本身存在根本性缺陷？\n\n### 2. 问题诊断：现有记忆系统的“时间失配”与“碎片化”\n作者深入分析现有方法（如MemGPT, RAG等），发现它们在处理时间维度上存在两个致命的逻辑漏洞：\n\n*   **漏洞一：时间的不准确性**\n    *   **现象**：现有系统通常以“对话时间”（即用户聊天的时刻）作为记忆的时间戳。\n    *   **反例**：用户在5月28日谈论5月3日-18日的波士顿旅行。如果按对话时间存储，系统会误以为这些事件发生在5月28日。\n    *   **结论**：混淆了“对话发生的时间”与“事件实际发生的时间”，导致检索时的时间错位。\n\n*   **漏洞二：时间的碎片化**\n    *   **现象**：现有记忆通常是“点状”的孤立记录（如：5月3日入住酒店，5月4日吃海鲜）。\n    *   **反例**：这些点状记录实际上共同构成了一个连续的“波士顿旅行”状态。孤立存储切断了事件之间的连续性，导致Agent难以理解持续性的用户状态或长期模式。\n    *   **结论**：缺乏对“持续时间”和“连续状态”的建模，丢失了叙事的连贯性。\n\n### 3. 概念重构：从“对话日志”到“语义时间线”\n**思维转折**：人类记忆并非按“聊天记录”存储，而是按“真实事件的时间线”组织，并包含对持续状态的感知。\n**核心假设**：如果能让Agent像人类一样，构建一条基于事件真实发生时间的“语义时间线”，并将碎片化的点状记忆整合为具有持续性的“状态记忆”，就能解决上述问题。\n\n### 4. 方法论构建：双管齐下的记忆架构\n基于上述假设，作者设计了TSM（Temporal Semantic Memory）框架，逻辑上分为两步走：\n\n*   **第一步：构建“语义时间锚点”**\n    *   **思考**：如何纠正时间错位？不能依赖对话时间戳，必须从文本中提取事件本身的语义时间。\n    *   **手段**：构建**时间知识图谱（TKG）**。不仅存储实体和关系，还显式地存储事实的有效时间区间。这为所有记忆提供了一个基于真实世界时间的“锚点”。\n\n*   **第二步：构建“持续性记忆”**\n    *   **思考**：如何解决碎片化？需要将时间上连续、语义上相关的点状记忆聚合起来。\n    *   **手段**：引入**Durative Memory（持续性记忆）**。利用时间切片（如按月）和语义聚类，将TKG中的碎片信息聚合为“主题”和“人设”。例如，将5月3日-18日的所有记录总结为一个“波士顿工作旅行”的持续性状态。\n\n### 5. 检索逻辑革新：引入“时间意图”\n**思考**：有了新的记忆结构，检索方式也必须改变。传统的向量相似度检索只看“语义相关”，不看“时间是否合适”。\n**改进**：在检索阶段，必须解析用户Query中的**“语义时间意图”**。\n*   例如用户问“上周我做了什么？”，系统首先解析出“上周”这个时间范围。\n*   **逻辑链**：先进行语义检索 -> 再利用时间意图进行过滤和重排序 -> 确保返回的记忆不仅在语义上相关，在时间逻辑上也是成立的。\n\n### 6. 系统优化：分层更新机制\n**思考**：实时更新所有复杂的持续性摘要（如人设、主题）计算成本太高，且没必要每句话都更新。\n**策略**：模仿人类的“睡眠”机制。\n*   **在线**：轻量级更新时间知识图谱（TKG），保证实时性。\n*   **定期（离线）**：周期性地重新聚类和生成摘要，保证长期的一致性和连贯性。\n\n### 总结\n作者的思考路径是从**“现有记忆在时间维度上的失效”**这一痛点出发，通过**区分“对话时间”与“语义时间”**以及**区分“点状事实”与“持续状态”**这两个关键洞察，最终构建了一个结合了**时间知识图谱（精准定位）**与**聚类摘要（宏观叙事）**的双层记忆架构。"
                },
                {
                    "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging",
                    "arxiv_id": "2601.07309",
                    "authors": "Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",
                    "summary": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究“LLM智能体”，提出了一种通过模型合并将多个专家智能体整合为一个通用智能体的方法（ARM）。该研究关注智能体在不同交互环境下的泛化能力和适应性，属于LLM智能体的核心研究范畴（单智能体能力提升），不属于排除项中的纯应用、纯推理、安全、多模态或纯基础设施优化。",
                    "summary2": "本文旨在将多个特定环境的LLM智能体专家合并为一个无需训练的通用模型。针对多轮交互场景，我们提出了一种名为ARM的基于激活引导的角色条件神经元移植方法。该方法通过动态主干选择和冲突感知的神经元移植来减少负迁移。在Qwen3-8B和Qwen2.5-7B专家池上，通过$\\tau$-bench、OfficeBench等多个基准验证了其有效性，显著提升了跨环境泛化能力和鲁棒性。",
                    "summary_translation": "交互式大语言模型智能体发展迅速，但大多数仍局限于单一环境，难以鲁棒地适应其他环境。Model merging（模型合并）提供了一种免训练的替代方案，通过将多个 experts（专家模型）整合到一个模型中。在本文中，我们提出了 Agent-Role Merging (ARM)（智能体角色合并），这是一种用于 LLM agents（大语言模型智能体）模型合并的 activation-guided（激活引导的）且 role-conditioned（角色条件的） neuron transplantation（神经元移植）方法。ARM 将现有的合并方法从 static natural language tasks（静态自然语言任务）拓展至 multi-turn agent scenarios（多轮智能体场景），并提升了在各种 interactive environments（交互式环境）中的 generalization ability（泛化能力）。这是通过一个精心设计的 3-step framework（三步框架）实现的：1) 构建 merged backbones（合并骨干网络），2) 基于其 role-conditioned activation analysis（角色条件激活分析）进行选择，3) 进行 neuron transplantation（神经元移植）以实现 fine-grained refinements（细粒度优化）。无需 gradient-based optimization（基于梯度的优化），ARM 在保持高效性的同时提升了 cross-benchmark generalization（跨基准泛化）能力。在多样化领域中，通过 ARM 合并得到的模型性能优于先前的 model merging（模型合并）方法和 domain-specific expert models（特定领域专家模型），同时展现了强大的 out-of-domain generalization（域外泛化）能力。",
                    "inspiration_trace": "基于论文《ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 第一阶段：宏观问题与背景观察\n**1. 现实痛点：专才与通才的矛盾**\n*   **观察**：当前的LLM智能体在特定环境（如WebShop、OfficeBench）中表现优异，但往往是“专才”。一旦跨环境部署，由于工具接口、动作模式的差异，性能会急剧下降。\n*   **常规路径的局限**：传统的解决方案是训练一个通用的全能模型，但这面临巨大的工程挑战（多任务数据冲突、课程学习复杂）和高昂的训练成本。\n*   **切入点**：作者将目光投向了“模型合并”——一种无需额外训练即可整合多个专家模型权重的技术。这被视为一种低成本构建通才模型的潜在路径。\n\n### 第二阶段：冲突发现与问题聚焦\n**2. 现有方法的失效：从静态到动态的鸿沟**\n*   **假设**：现有的模型合并方法（如Task Arithmetic, TIES-Merging）在静态NLP任务上很成功，理应也能应用于智能体任务。\n*   **证伪**：实验发现，这些方法在交互式智能体场景下表现极不稳定（如图1所示，不同基准上表现方差巨大）。\n*   **核心洞察**：智能体任务与静态文本任务的本质区别在于**“多轮交互”**和**“级联效应”**。\n    *   在静态任务中，错误可能只是预测不准；\n    *   在智能体任务中，微小的格式错误（如JSON格式错误、工具调用参数偏差）会导致后续步骤全部崩溃。\n\n### 第三阶段：深入诊断与假设提出\n**3. 归因分析：两大核心挑战**\n作者将合并失败的原因归结为两个具体问题：\n*   **挑战一：主干的不稳定性**\n    *   不同的权重合并公式（平均、TIES等）在不同环境下的表现不可预测。没有一个通用的公式能保证在所有环境下都保留通用能力。\n*   **挑战二：能力冲突**\n    *   简单的权重平均会“模糊”掉特定技能。在智能体中，这表现为“角色关键行为”的丧失（例如，模型忘了如何正确调用API）。这种冲突比普通的知识遗忘更致命，因为它直接阻断了任务链条。\n\n### 第四阶段：方法论构建与逻辑演进\n**4. 策略一：如何选择稳定的主干？（从“盲选”到“内测”）**\n*   **思考**：既然无法预知哪个合并公式最好，能不能先构建一批候选模型，然后选一个最好的？\n*   **难点**：直接在测试集上评估成本太高。\n*   **创新思路**：利用模型内部的**激活信号**作为代理指标。\n    *   **逻辑**：如果一个合并后的模型，在处理特定任务（如“调用工具”）时，其神经元激活模式与原来的专家模型高度重合，说明它保留了该能力。\n    *   **产出**：提出了**激活重叠分数（AOS）**。通过分析“角色条件”下的激活（即只关注关键动作时刻的神经元），选出最能保留专家特征的合并主干。\n\n**5. 策略二：如何修复能力冲突？（从“全局融合”到“局部移植”）**\n*   **思考**：选出的主干可能在某些环境上依然较弱。直接全局微调会破坏已有能力，能否像器官移植一样，只把缺失的“能力模块”补进来？\n*   **细化思路**：\n    *   **定位**：利用激活分析，找出专家模型中负责特定“角色”（如JSON生成、工具调用）的关键神经元。\n    *   **移植**：将这些神经元直接“移植”到主干模型中。\n*   **关键约束：避免负迁移**\n    *   **思考**：如果移植的神经元恰好是另一个环境需要的，就会产生冲突。\n    *   **解决方案**：引入**冲突感知策略**。在移植前，先检查这些神经元是否被其他环境“占用”。如果是，则跳过，只移植那些“安全”的神经元。\n\n### 第五阶段：逻辑闭环与验证\n**6. 最终框架的形成：ARM**\n*   将上述思考串联，形成了三步走框架：\n    1.  **构建候选池**：用常规方法生成一堆合并模型。\n    2.  **基于激活选主干**：用AOS分数选出最稳健的那个。\n    3.  **神经元移植**：针对薄弱环节，像做手术一样精准移植专家的特定神经元，并严格保护其他能力不受干扰。\n\n**7. 预期与验证**\n*   **预期**：这种方法不仅能提升平均性能，更重要的是能解决“木桶效应”（最差环境的表现），因为它专门修复了导致级联失败的关键节点。\n*   **结论**：实验证明，ARM确实在保持通用性的同时，显著提升了跨环境的鲁棒性，验证了“基于角色条件的神经元移植”这一核心假设的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**“应用场景的迁移”**（从静态文本到智能体）出发，发现了**“级联失败”**这一特殊现象，进而通过**“机制可解释性”**（激活分析）手段，将模型合并问题从盲目的权重调整，转化为精准的**“电路诊断与修复”**过程。"
                },
                {
                    "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
                    "arxiv_id": "2601.07342",
                    "authors": "Nicolas Tacheny",
                    "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个智能体诊断框架，重点在于LLM如何通过工具使用（如服务查找、依赖检索）进行自主导航和逐步调查，符合单智能体中“工具使用”和“规划”的研究范围。虽然应用场景为电信和数据中心，但核心贡献在于智能体的架构与推理协议，而非纯领域应用。",
                    "summary2": "本文旨在解决传统 Root Cause Analysis (RCA) 耦合度高且难维护的问题。针对电信和数据中心基础设施，我们提出了一种基于 Model Context Protocol (MCP) 的 Agentic Diagnostic Framework，利用 LLM 通过 Investigation Protocol 和受限工具空间进行逐步推理。我们在合成图 Oracle Benchmark 上通过 Investigation Accuracy、RCA Accuracy 和 Impact Accuracy 验证了其有效性，Claude Haiku 3.5 达到了 100% 的准确率。",
                    "summary_translation": "大规模电信和数据中心基础设施依赖于多层服务和资源模型，在此架构下，故障会在物理和逻辑组件之间传播，进而影响多个客户。传统的根因分析（Root Cause Analysis, RCA）方法依赖于硬编码的图遍历算法或基于规则的关联引擎，这些方法不仅维护成本高昂，而且与基础设施模型紧密耦合。在这项工作中，我们提出了一种智能体诊断框架，该框架利用大语言模型（Large Language Model, LLM），通过模型上下文协议（Model Context Protocol, MCP）提供的受限工具空间执行分步调查。该智能体无需将因果逻辑或遍历算法嵌入应用程序，而是通过调用服务查询、依赖关系检索、结构化与非结构化数据分析、事件分析及影响发现等工具，自主在基础设施模型中进行导航。我们定义了一种调查协议，用于规范智能体的推理过程，并确保其具有事实依据、可复现性，并能安全处理缺失或模糊的信息。这项工作为自主事件解决和变更影响缓解奠定了基础。未来的系统不仅能够诊断并修复基础设施故障，还能预测计划变更对服务和客户的影响，从而帮助运维人员在执行维护操作前缓解风险。",
                    "inspiration_trace": "基于论文《Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 1. 宏观观察：传统方法的“刚性”与现实的“动态”矛盾\n**思考起点：** 作者首先审视了电信和数据中心运维的现状。\n*   **观察：** 现代基础设施是多层级的（服务、资源、客户），故障会在物理和逻辑组件间传播。\n*   **痛点：** 传统的根因分析（RCA）依赖于硬编码的图遍历算法或基于规则的关联引擎。\n*   **矛盾：** 基础设施是动态演进的（拓扑变更、命名变化），但传统的RCA逻辑是静态的。这导致了高昂的维护成本和系统与模型的紧耦合。此外，非结构化数据（如人工备注）难以被传统规则引擎处理。\n*   **初步结论：** 我们需要一种更具适应性、能够处理非结构化信息且不随拓扑变更而频繁修改代码的解决方案。\n\n### 2. 核心假设：从“编码逻辑”转向“编码协议”\n**思维转折：** 既然编写具体的因果逻辑（算法）太脆弱，能否让模型自己学会推理？\n*   **引入LLM：** 大语言模型（LLM）具备强大的推理和理解非结构化文本的能力，理论上可以替代硬编码规则。\n*   **风险识别：** 直接让LLM进行诊断存在“幻觉”风险，且无法保证操作的安全性（可能胡乱编造资源ID）。\n*   **关键假设：** 如果不把“因果逻辑”写死在代码里，而是定义一套严格的“调查协议”，并限制LLM只能通过特定工具获取数据，那么LLM就能像人类工程师一样进行“有据可依”的推理。\n*   **思路确立：** **去算法化**。不再试图用代码穷举故障传播路径，而是构建一个能够自主导航信息图的智能体。\n\n### 3. 抽象建模：构建标准化的数字孪生接口\n**落地思考：** 如何让智能体理解复杂的基础设施，同时又不依赖具体的数据库实现？\n*   **本体抽象：** 作者借鉴了TM Forum SID标准，将复杂的基础设施抽象为四个核心实体：**服务**、**资源**、**参与方**、**事件**。这为推理提供了一个通用的语义空间。\n*   **解耦设计：** 为了防止智能体与底层存储技术（如Neo4j或关系型数据库）绑定，作者引入了**模型上下文协议（MCP）**。\n*   **逻辑推演：** MCP充当了“安全边界”和“统一接口”。智能体不直接查询图数据库，而是调用MCP暴露的工具（如`get_implementation`, `get_impacted_services`）。这不仅解耦了系统，还天然防止了SQL注入或非授权访问，确保了每一次数据获取都是可审计的。\n\n### 4. 方法论构建：受控的智能体调查协议\n**核心创新：** 有了工具，如何确保智能体不乱跑、不胡说？\n*   **形式化流程：** 作者意识到，人类专家排查故障是有固定SOP（标准作业程序）的。因此，作者将这种经验形式化为一个**RCA调查协议**。\n*   **步骤设计：**\n    1.  **定位：** 从告警中提取服务名。\n    2.  **下钻：** 获取实现该服务的所有资源。\n    3.  **取证：** 检查每个资源的备注和事件（利用LLM理解非结构化文本）。\n    4.  **上溯：** 确定根因后，反向查找受影响的服务和客户。\n    5.  **发布：** 输出结构化报告。\n*   **约束机制：** 强制要求智能体必须基于工具返回的结果进行推理，如果数据缺失必须明确承认，严禁编造。这解决了LLM的“幻觉”问题，实现了**Grounding（接地气）**。\n\n### 5. 验证与洞察：去算法化的可行性\n**实证思考：** 这种“软逻辑”真的能取代“硬算法”吗？\n*   **实验设计：** 构建了一个合成图，预设了根因和影响路径，测试智能体能否在没有内置图算法的情况下找到答案。\n*   **结果分析：** 实验表明，只要协议设计得当，LLM（如Claude Haiku 3.5）能够达到100%的准确率。\n*   **关键洞察：** 事实证明，**硬编码的图遍历逻辑并非必须**。通过结构化的工具调用和逐步推理，因果逻辑是在推理过程中“涌现”出来的，而不是预先写好的。这意味着系统具有极强的通用性和适应性。\n\n### 6. 愿景延伸：从诊断到预测与自治\n**未来推演：** 既然能诊断“已发生”的故障，能否预测“未发生”的影响？\n*   **逻辑扩展：** 影响分析（IA）本质上是RCA的反向过程。如果系统能理解资源与服务的依赖关系，那么在执行变更（如维护）前，智能体完全可以模拟变更，预测其影响范围。\n*   **终极目标：** 这篇论文不仅是关于RCA，更是为**自主事故解决**和**变更影响缓解**奠定基础。未来的系统将从“被动响应”进化为“主动预防”。\n\n---\n\n**总结：**\n作者的思考路径是一个**“解构 -> 重构 -> 验证 -> 升华”**的过程：\n1.  **解构**了传统RCA系统的脆弱性（硬编码逻辑）；\n2.  **重构**了诊断流程，将其转化为基于MCP工具的智能体协议；\n3.  **验证**了LLM在严格协议下可以替代传统图算法；\n4.  最终**升华**出一种自适应、安全且可审计的基础设施运维新范式。"
                },
                {
                    "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration",
                    "arxiv_id": "2601.07224",
                    "authors": "Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",
                    "summary": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确针对LLM智能体的训练范式（SFT+RL），提出了基于梯度浓度的数据分配框架PRISM，旨在优化智能体的训练过程和性能，并在智能体基准（WebShop和ALFWorld）上进行了验证，属于智能体训练与优化的研究范畴。",
                    "summary2": "本文旨在解决 LLM 智能体训练中 SFT 与 RL 数据分配低效及优化干扰问题。针对混合训练数据，我们提出了一种基于梯度空间几何结构（如 Gini 系数）诊断认知冲突的 PRISM 框架，实现数据在巩固与适应间的自适应路由，并在 WebShop 和 ALFWorld 基准上通过 Success Rate 和计算效率验证了其有效性。",
                    "summary_translation": "尽管混合监督微调 (SFT) 随后进行强化学习 (RL) 已成为训练大语言模型智能体的标准范式，但这两个阶段之间有效的数据分配机制在很大程度上仍未被探索。当前的数据仲裁策略通常依赖于表层启发式规则，无法诊断内在的学习需求。由于 SFT 旨在通过模仿实现模式巩固，而 RL 通过探索驱动结构适应，将数据与这些功能角色错配会导致严重的优化干扰。我们提出了 PRISM，这是一个基于图式理论的动态感知框架，它根据数据与模型现有知识的认知冲突程度来仲裁数据。通过分析梯度的空间几何结构，PRISM 将引发高空间集中度的数据识别为高冲突信号，这些信号需要 RL 进行结构重组。相比之下，产生弥散更新的数据被路由到 SFT 以进行高效巩固。在 WebShop 和 ALFWorld 上的大量实验表明，PRISM 实现了帕累托改进，在优于最先进的混合方法的同时，将计算成本降低了高达 3.22 倍。我们的研究结果表明，基于内部优化机制解耦数据对于可扩展且稳健的智能体对齐至关重要。",
                    "inspiration_trace": "基于论文《Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration》，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观问题的捕捉——现有范式的“粗粒度”困境\n**思考起点：** 作者首先审视了当前LLM智能体训练的标准范式（SFT后接RL）。虽然这一流程已被广泛接受，但作者敏锐地发现了一个被忽视的瓶颈：**数据分配机制是僵化的。**\n*   **观察：** 现有的数据分配策略主要分为三类：一是“单调排序”（SFT-then-RL的固定顺序），二是“通用探索”（对所有数据无差别使用RL），三是“结果导向过滤”（基于准确率等外部指标）。\n*   **痛点：** 这些方法都忽略了数据的**异质性**和模型的**内部状态**。它们将所有数据一视同仁，导致计算资源浪费，且容易引发优化干扰（如对简单样本进行不必要的RL探索，导致不稳定性）。\n*   **核心问题：** 如何根据数据的内在认知需求，智能地将其分配给SFT或RL，以实现效率与性能的帕累托最优？\n\n### 第二阶段：功能解构——SFT与RL的本质差异\n**思考深入：** 为了解决分配问题，作者首先对SFT和RL在认知层面的功能进行了重新定义。\n*   **SFT的功能：** 侧重于**模式巩固**。通过模仿，将行为规范和特定知识内化，适合处理模型已具备基础认知的领域。\n*   **RL的功能：** 侧重于**结构适应**。通过试错，重构内部逻辑以提升泛化能力，适合处理需要复杂推理和逻辑修正的领域。\n*   **推论：** 如果将需要“结构适应”的数据强行用于SFT，模型无法突破逻辑瓶颈；如果将仅需“模式巩固”的数据用于RL，则会引入探索噪声，破坏已有的知识。因此，**必须找到一种诊断机制，区分哪些数据需要巩固，哪些需要适应。**\n\n### 第三阶段：理论映射——引入认知心理学视角\n**思考转折：** 如何定义“需要适应”的数据？作者跳出纯工程视角，引入了皮亚杰的**图式理论**。\n*   **理论核心：** 学习效率取决于新信息与现有知识库之间的**冲突程度**。\n    *   **低冲突（兼容）：** 适合通过“同化”进行巩固。\n    *   **高冲突（矛盾）：** 必须通过“顺应”进行根本性的结构重组。\n*   **映射：** 作者将这一认知过程映射到神经网络优化中——**高认知冲突 = 需要结构适应（RL）；低认知冲突 = 需要模式巩固（SFT）。**\n\n### 第四阶段：数学代理——从“认知冲突”到“梯度几何”\n**思考落地：** 理论有了，但如何量化“认知冲突”？模型内部不会直接告诉我们要“冲突值”。作者将目光投向了优化的核心信号——**梯度**。\n*   **假设：** 梯度是模型对数据的数学反馈。如果数据与模型现有知识冲突剧烈，模型必须剧烈调整特定的参数（即“知识神经元”）来修正逻辑。\n*   **几何洞察：** 作者关注梯度的**空间几何结构**，而非单纯的数值大小。\n    *   **高浓度：** 如果梯度高度集中在少数参数组上，说明模型正在进行剧烈的局部逻辑修正（高冲突）。\n    *   **低浓度（扩散）：** 如果梯度均匀分布在整个网络，说明模型只是在微调全局参数以适应模式（低冲突）。\n*   **结论：** **梯度的空间浓度是认知冲突的最佳代理。**\n\n### 第五阶段：方法论构建——PRISM框架的诞生\n**思考成型：** 基于上述逻辑，作者构建了PRISM框架，将理论转化为可执行的三个步骤：\n1.  **无损探针：** 在不更新权重的情况下，计算模型对每个样本的梯度分布，捕捉内部反应。\n2.  **结构量化：** 引入统计学指标（如基尼系数、峰度、变异系数CV）来量化梯度的“浓度”，从而给每个样本打上“认知冲突分”。\n3.  **自适应路由：** 根据分数中位数进行切分。高分（高冲突）样本路由至RL进行结构重塑；低分（低冲突）样本路由至SFT进行行为巩固。\n\n### 总结：逻辑链条全景\n作者从**训练效率低下**的宏观现象出发，通过**功能解构**明确了SFT与RL的分工，借助**认知心理学理论**定义了“冲突”这一核心变量，最终利用**梯度的空间几何特征**将抽象的认知冲突转化为可计算的数学指标，从而实现了数据的精准路由。这一过程体现了从“经验主义训练”向“动力学感知训练”的思维跃迁。"
                },
                {
                    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
                    "arxiv_id": "2601.07055",
                    "authors": "Zhenrui Yue, Kartikeya Upasani, Xianjun Yang, Suyu Ge, Shaoliang Nie, Yuning Mao, Zhe Liu, Dong Wang",
                    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题和摘要明确提出了“Self-Evolving Search Agents”（自我演化的搜索智能体），涉及自我演化、工具使用以及智能体架构（proposer和solver），完全符合“自我演化”和“单智能体（工具使用）”的研究范围。",
                    "summary2": "本文旨在解决无训练数据情况下搜索智能体的自我进化问题。针对开放域问答场景，我们提出了一种 Dr. Zero 框架，该框架利用 Proposer-Solver 共进化循环和跳跃分组相对策略优化（HRPO）来生成多样化且具挑战性的问题。在多个开放域问答基准（如 HotpotQA, NQ）上，通过精确匹配（EM）等指标验证了其有效性，结果显示其性能匹配甚至超越了全监督搜索智能体。",
                    "summary_translation": "随着高质量数据日益难以获取，无数据自我进化已成为一种极具前景的范式。该方法使大语言模型能够自主生成并解决复杂问题，进而提升其推理能力。然而，由于问题多样性有限，且多步推理和工具使用需要大量计算，多轮搜索智能体在无数据自我进化过程中面临挑战。在本研究中，我们提出了 Dr. Zero，这是一个使搜索智能体能够在没有任何训练数据的情况下实现有效自我进化的框架。具体而言，我们设计了一个自我进化反馈回路，其中提议者生成多样化的问题，用于训练一个由同一基础模型初始化的解题者。随着解题者的进化，它会促使提议者生成难度递增但仍可解的任务，从而建立一套自动化课程来优化这两个智能体。为提高训练效率，我们还引入了跳跃分组相对策略优化。该方法将结构相似的问题进行聚类以构建组级基线，有效最小化了在评估每个查询的个体难度和可解性时的采样开销。因此，HRPO 在不影响性能或稳定性的前提下，显著降低了解题者训练的计算需求。大量实验结果表明，无数据的 Dr. Zero 达到甚至超越了全监督搜索智能体的水平，证明了复杂的推理和搜索能力可以仅通过自我进化而涌现。",
                    "inspiration_trace": "基于对论文《Dr. Zero: Self-Evolving Search Agents without Training Data》的深入分析，以下是对作者核心方法论逻辑链的系统性推演。这一过程旨在还原作者从宏观问题观察到具体方法创新的思考路径。\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**——从“数据饥渴”到“无数据自进化”的范式转移**\n\n1.  **背景痛点**：\n    *   作者首先观察到当前大模型（LLM）发展的核心瓶颈：高质量训练数据的获取日益困难。\n    *   **思考**：如果无法依赖外部人工标注数据，模型能否像生物进化一样，通过“自举”的方式自我提升？\n2.  **现有局限的识别**：\n    *   作者审视了现有的“自进化”研究（如Self-Play、Self-Rewarding），发现它们大多集中在**封闭域**（如数学、代码）。\n    *   **关键发现**：在开放域的**搜索代理**任务中，现有的自进化方法失效了。原因在于：\n        *   **多样性缺失**：模型倾向于生成简单的、单跳的问题，缺乏挑战性。\n        *   **计算成本高昂**：搜索代理需要调用外部工具（如搜索引擎），推理链路长、延迟高。传统的强化学习算法（如需要多次采样的GRPO）在多轮工具交互场景下计算量呈指数级增长，难以落地。\n\n**核心问题确立**：如何在**零训练数据**的条件下，实现开放域搜索代理的高效自进化？\n\n---\n\n### 第二阶段：机制设计与假设提出\n**——构建“出题者”与“解题者”的共生博弈**\n\n1.  **引入对抗/共生框架**：\n    *   **思考**：要解决“题目太简单”的问题，不能只靠模型自己瞎想。自然界中，捕食者和猎物的共同进化促进了物种复杂度的提升。\n    *   **假设**：如果设计两个角色——**Proposer（出题者）**和**Solver（解题者）**，让它们相互博弈，是否能自动生成由易到难的课程？\n2.  **定义进化逻辑**：\n    *   **Proposer的任务**：利用搜索引擎生成复杂、多跳的问题。\n    *   **Solver的任务**：利用搜索引擎回答这些问题。\n    *   **反馈闭环**：Solver越强，Proposer必须生成更难的问题才能获得奖励；Proposer的问题越难，Solver被迫提升搜索推理能力。\n    *   **关键洞察**：这种动态博弈能自动形成**课程学习**，无需人工设计难度梯度。\n\n---\n\n### 第三阶段：攻克核心瓶颈\n**——解决“计算效率”与“题目质量”的双重挑战**\n\n1.  **解决计算效率问题（HRPO的诞生）**：\n    *   **困境**：传统的GRPO算法为了估计优势函数，需要对同一个Prompt生成多个回复。对于搜索代理来说，一次回复包含多次搜索调用，成本极高。如果Proposer训练需要“生成多个问题”且“每个问题跑多次Solver”，计算开销不可接受。\n    *   **创新思考**：能否减少采样次数？\n    *   **逻辑推演**：问题的结构特征（如Hop数/跳数）与其难度高度相关。与其对同一个问题采样多次，不如将**结构相似的问题**（例如都是2跳问题）归为一组。\n    *   **方法论产出**：提出**Hop-Grouped Relative Policy Optimization (HRPO)**。通过聚类结构相似的问题来构建组级基线，从而避免了昂贵的嵌套采样，将计算成本降低了一个数量级。\n\n2.  **解决题目质量问题（难度引导的奖励机制）**：\n    *   **困境**：如何让Proposer生成“既难又能做对”的题目？如果太难，Solver全错，学不到东西；如果太简单，Solver全对，没提升。\n    *   **逻辑推演**：理想的题目应该让Solver的正确率处于中间状态（例如只有部分尝试能解出）。\n    *   **方法论产出**：设计**难度引导的奖励函数**。\n        *   如果Solver全对 -> 奖励低（太简单）。\n        *   如果Solver全错 -> 奖励低（太难/无解）。\n        *   如果Solver部分正确 -> 奖励高（难度适中）。\n    *   **补充**：引入格式奖励，强制Proposer正确使用搜索工具，确保生成的题目是基于真实检索路径的，而非幻觉。\n\n---\n\n### 第四阶段：系统整合与验证\n**——Dr. Zero 框架的最终成型**\n\n1.  **系统架构整合**：\n    *   作者将上述思考整合为一个统一的框架：**Dr. Zero**。\n    *   **输入**：仅依赖基础LLM和外部搜索引擎，无任何人工标注数据。\n    *   **流程**：\n        1.  Proposer通过HRPO训练，利用搜索生成高质量、多跳的QA对。\n        2.  Solver通过GRPO训练，学习解决Proposer生成的难题。\n        3.  两者交替迭代，性能螺旋上升。\n\n2.  **实验验证与假设确认**：\n    *   **思考**：这套无数据方案真的能打过有监督的SOTA吗？\n    *   **结果**：实验表明，Dr. Zero在多个开放域QA基准上，不仅超越了基础模型，甚至**匹配或超越了**完全依赖人工数据的监督式搜索代理。\n    *   **结论**：证明了在搜索代理领域，**自进化可以替代人工监督**。\n\n---\n\n### 总结：作者的思维演进脉络\n\n1.  **观察**：数据稀缺，现有自进化方法在开放域搜索任务中因“题目简单”和“算力昂贵”而失效。\n2.  **假设**：通过Proposer-Solver的共生博弈可以自动生成进化的课程。\n3.  **挑战1（算力）**：传统RL采样太贵 -> **创新**：利用问题结构相似性，提出HRPO算法，大幅降低采样成本。\n4.  **挑战2（质量）**：如何控制题目难度 -> **创新**：基于Solver正确率的难度引导奖励，确保题目处于“最近发展区”。\n5.  **成果**：实现了无需任何训练数据的搜索代理自进化，性能媲美甚至超越有监督方法。"
                },
                {
                    "title": "CloneMem: Benchmarking Long-Term Memory for AI Clones",
                    "arxiv_id": "2601.07023",
                    "authors": "Sen Hu, Zhiyu Zhang, Yuxiang Wei, Xueran Han, Zhenheng Tang, Huacan Wang, Ronghao Chen",
                    "summary": "AI Clones aim to simulate an individual's thoughts and behaviors to enable long-term, personalized interaction, placing stringent demands on memory systems to model experiences, emotions, and opinions over time. Existing memory benchmarks primarily rely on user-agent conversational histories, which are temporally fragmented and insufficient for capturing continuous life trajectories. We introduce CloneMem, a benchmark for evaluating longterm memory in AI Clone scenarios grounded in non-conversational digital traces, including diaries, social media posts, and emails, spanning one to three years. CloneMem adopts a hierarchical data construction framework to ensure longitudinal coherence and defines tasks that assess an agent's ability to track evolving personal states. Experiments show that current memory mechanisms struggle in this setting, highlighting open challenges for life-grounded personalized AI. Code and dataset are available at https://github.com/AvatarMemory/CloneMemBench",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个用于评估AI克隆（智能体）长期记忆能力的基准，侧重于智能体随时间跟踪个人状态的能力，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在评估AI Clone基于非对话式数字痕迹的长期记忆能力。针对跨越1-3年的日记、社交媒体等非对话式数字痕迹，我们提出了一种名为CloneMem的benchmark，采用分层数据构建框架确保纵向一致性。我们在CloneMem数据集上通过Recall@K、Choice Accuracy和QA Consistency Score等指标验证了其有效性，揭示了现有记忆系统在追踪个人状态演变方面的局限性。",
                    "summary_translation": "AI Clones (AI克隆) 旨在模拟个体的思想和行为，以实现长期、个性化的交互，这对 memory systems (记忆系统) 随时间对经历、情感和观点进行建模提出了严苛的要求。现有的 memory benchmarks (记忆基准) 主要依赖于 user-agent conversational histories (用户-智能体对话历史)，这些历史在时间上是碎片化的，不足以捕捉连续的生活轨迹。我们介绍了 CloneMem，这是一个用于评估 AI Clone (AI克隆) 场景中 longterm memory (长期记忆) 的基准，它基于 non-conversational digital traces (非对话式数字痕迹)，包括日记、社交媒体帖子和电子邮件，时间跨度为一到三年。CloneMem 采用了一个 hierarchical data construction framework (分层数据构建框架) 来确保 longitudinal coherence (纵向一致性)，并定义了评估智能体追踪 evolving personal states (演变个人状态) 能力的任务。实验表明，当前的 memory mechanisms (记忆机制) 在这种设置下难以应对，凸显了 life-grounded personalized AI (基于生活的个性化AI) 面临的开放性挑战。Code (代码) 和 dataset (数据集) 可在 https://github.com/AvatarMemory/CloneMemBench 获取。",
                    "inspiration_trace": "基于论文《CloneMem: Benchmarking Long-Term Memory for AI Clones》，以下是对作者核心思想产出过程的系统性逻辑推演：\n\n### 第一阶段：宏观趋势与问题定义\n**（从“对话助手”到“数字克隆”）**\n\n1.  **观察现象**：随着LLM的发展，AI应用正从通用的“角色扮演”向更深度的“AI克隆”演进。用户不再满足于与一个预设角色的单次对话，而是希望建立一个能长期模拟特定个体思想、行为和情感的“数字分身”。\n2.  **提炼需求**：AI克隆的核心挑战不在于单次回复的准确性，而在于**长期记忆**。系统必须能够跨越数年时间，捕捉个体的经历、情感波动以及观点的演变，而不仅仅是维持对话的上下文。\n\n### 第二阶段：痛点识别与假设提出\n**（从“对话历史”到“生活轨迹”）**\n\n1.  **批判现状**：作者审视现有的记忆基准（如LoCoMo, LongMemEval），发现它们几乎全部依赖于**用户-智能体的对话历史**。\n2.  **指出缺陷**：\n    *   **碎片化**：对话是离散的、断续的，只能捕捉生活的快照，无法记录连续的生活流。\n    *   **被动性**：现实中，用户不可能通过不断的对话来“喂养”AI克隆，这成本太高。\n3.  **提出假设**：真实的记忆应当基于**非对话式的数字痕迹**（如日记、社交媒体、邮件）。这些数据是自然发生的、纵向连续的，能反映个体在非交互状态下的真实状态。因此，需要一个新的基准来评估AI克隆处理这种“生活轨迹”的能力。\n\n### 第三阶段：数据构建的方法论突破\n**（从“随机生成”到“分层连贯性”）**\n\n1.  **面临挑战**：如何构建一个跨越1-3年、逻辑自洽且包含情感和观点演变的合成数据集？简单的随机生成会导致时间线上的逻辑崩塌。\n2.  **核心思想**：人类的生活是有结构的，不是杂乱无章的。必须采用**自上而下的分层生成框架**来确保纵向连贯性。\n3.  **逻辑推演**：\n    *   **宏观层**：先定义“人格特质”和“生活弧线”，确定长期的人生轨迹（如职业变动、情感走向）。\n    *   **中观层**：将大事件拆解为“阶段”，并引入“内部状态快照”机制。确保上一阶段的情感积累会影响下一阶段，从而实现心理状态的连续性。\n    *   **微观层**：基于具体事件生成日记、帖子等数字痕迹，并显式生成对应的“证据”，确保痕迹与底层逻辑的一致性。\n\n### 第四阶段：评估维度的重新定义\n**（从“事实检索”到“轨迹追踪”）**\n\n1.  **转变视角**：传统的记忆测试多关注“某时某刻发生了什么”（静态事实）。但对于AI克隆，关键在于“为什么会变成现在这样”。\n2.  **设计任务**：评估重点必须转向**动态推理**。作者设计了涵盖经历、情感、观点三个维度的任务，不仅测试事实回忆，更测试比较、因果分析、反事实推理以及对“未确定状态”的识别（即区分“正在探索”与“已做决定”）。\n\n### 第五阶段：实验发现与理论升华\n**（从“追求抽象”到“保真度优先”）**\n\n1.  **预期与反差**：作者原本预期先进的、具有抽象和整合能力的记忆系统（如A-Mem, Mem0）会表现更好，因为它们能“总结”知识。\n2.  **实验发现**：结果令人惊讶，最简单的**扁平检索器**往往表现最好。复杂的记忆系统因为进行了“有损压缩”（总结和抽象），丢失了回答轨迹问题所需的细粒度细节（如时间戳、具体措辞）。\n3.  **洞察提炼**：\n    *   **有效性 vs. 保真度**：现有的记忆系统优化的是“有效性”（能否找到相关话题），但AI克隆更需要的是“保真度”（能否还原具体细节）。\n    *   **叙事陷阱**：模型倾向于用通用的叙事模板（如“孩子的一句话让父亲顿悟”）来填补记忆空白，导致因果逻辑错误但听起来很合理。\n4.  **最终结论**：AI克隆的记忆系统不应仅仅是一个压缩的知识库，而应是一个**证据保存基质**。它必须保留原始痕迹的保真度，显式建模内部状态的转变，并能在证据不足时保持“未知”的克制。\n\n---\n\n**总结**：作者的思考路径是从**应用场景的升级**（克隆vs对话）出发，发现了**数据源的本质缺陷**（对话vs轨迹），通过**分层生成**解决了数据连贯性难题，并在实验中意外揭示了**当前记忆架构的“有损压缩”悖论**，最终确立了AI克隆记忆设计应遵循“保真度优先”的新原则。"
                },
                {
                    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
                    "arxiv_id": "2601.06860",
                    "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了ET-Agent框架，专注于LLM智能体的工具使用行为校准。它涉及单智能体的工具使用能力，并引入了自我演化数据飞轮机制来改进智能体的行为模式，符合单智能体和自我演化的研究范围，且不属于排除的纯应用、纯推理或安全对齐领域。",
                    "summary2": "本文旨在解决现有LLM智能体在Tool-Integrated Reasoning (TIR) 任务中因忽视行为模式对齐而导致的无效工具调用问题。针对TIR任务中的错误行为模式，我们提出了一种ET-Agent框架，通过Self-evolving Data Flywheel和Behavior Calibration Training协同优化。在AIME24、2Wiki等六个基准测试上，通过正确性、效率及推理简洁性等指标验证了其优越性。",
                    "summary_translation": "大语言模型可以通过采用工具集成推理范式，扩展其参数知识的边界。然而，现有的基于大语言模型的智能体训练框架往往侧重于答案的准确性，而忽视了对行为模式的特定对齐。因此，智能体在执行工具集成推理任务时，常表现出无效的动作，例如冗余或不足的工具调用。如何在执行工具集成推理任务时校准错误的行为模式，进而探索有效的轨迹，仍是一个亟待解决的开放性问题。在本文中，我们提出了ET-Agent，这是一个通过两个协同视角来校准智能体工具使用行为的训练框架：自进化数据飞轮和行为校准训练。具体而言，我们引入了一个自进化数据飞轮来生成增强数据，利用这些数据对大语言模型进行微调，以提升其探索能力。在此基础上，我们构建了一个两阶段的行为校准训练框架。该框架旨在逐步将错误的行为模式校准为最优行为。进一步的深入实验证实了ET-Agent在多个维度上的优越性，包括正确性、效率、推理简洁性以及工具执行准确性。我们的ET-Agent框架为工具集成推理领域的研究提供了有价值的实践启示。代码链接：https://github.com/asilverlight/ET-Agent",
                    "inspiration_trace": "基于对论文《ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration》的深入分析，以下是对作者产出该核心方法的逻辑链推演。这一过程展现了从宏观现象观察到微观机制设计的完整思考路径。\n\n---\n\n### 1. 宏观观察与问题定义：从“结果正确”到“行为有效”\n\n**逻辑起点：**\n作者首先关注到大语言模型（LLM）通过工具集成推理（TIR）范式突破了参数知识的限制。然而，学术界和工业界存在一个普遍的**评价偏差**：绝大多数研究仅关注**最终答案的准确性**，而忽视了达成答案过程中的**行为模式**。\n\n**现象发现：**\n在实际应用中，作者观察到即使模型答对了问题，其过程往往充满“低效”或“怪异”的行为。例如，为了查一个简单事实反复调用搜索工具，或者在需要深入推理时过早停止。这引发了一个核心思考：\n> **核心问题：** 如何在保证答案正确的前提下，校准Agent的工具使用行为，使其既不冗余也不匮乏，从而实现高效推理？\n\n### 2. 深度诊断与归因：行为错误的分类与空间复杂性\n\n为了解决上述问题，作者没有急于提出模型，而是先对“错误行为”进行了系统性的**病理分析**。\n\n**错误分类：**\n通过初步实验，作者将错误的TIR行为模式归纳为两类：\n1.  **不当工具使用：** 包括“冗余调用”（浪费资源）和“中止执行”（代码或查询格式错误导致失败）。\n2.  **缺陷推理逻辑：** 包括“调用不足”（过早停止，没拿到关键信息）和“错误推理过程”（逻辑跳跃或无关步骤）。\n\n**关键洞察：**\n作者进一步分析了正确答案的轨迹分布，发现了一个重要现象：**对于同一个问题，存在大量不同的正确路径，且工具调用的次数差异巨大。**\n这意味着TIR任务的**动作空间极其广阔**。\n\n**对现有方法的批判：**\n基于此洞察，作者指出了现有方法的局限性：\n*   **模仿学习（SFT）：** 只能复现训练数据中的路径，无法探索数据之外的高效行为，导致探索能力受限。\n*   **传统RL（如DPO）：** 往往基于二元对比（好vs坏），容易导致模型坍缩到极窄的动作空间，无法适应TIR广阔的解空间。\n\n**结论：** 现有的“只看结果”或“简单对比”无法解决TIR中的行为校准问题。我们需要一种能**充分探索广阔动作空间**，并从中**筛选出最优行为**的新范式。\n\n### 3. 核心假设提出：先探索，后校准\n\n基于上述诊断，作者提出了一个分阶段的解决思路：\n> **核心假设：** 要校准行为，首先必须让模型“见识”到足够多的可能性（探索），然后再通过奖励机制引导其收敛到最优路径（校准）。\n\n这直接导向了ET-Agent框架的两大支柱设计：\n1.  **数据层面：** 需要一个能自我进化、不断扩充轨迹多样性的机制。\n2.  **算法层面：** 需要一个先鼓励发散探索，再逐步收敛至高效行为的训练流程。\n\n### 4. 方法论构建：从数据飞轮到行为校准\n\n#### 4.1 数据层面的突破：自进化数据飞wheel\n**思考：** 既然现有数据覆盖面不够，如何低成本地获得高质量、多样化的轨迹？\n**设计：**\n作者设计了一个闭环系统，利用模型自身来生成和优化数据：\n*   **对正确轨迹：** 进行“去冗余”和“全局精炼”，教模型如何做得更简洁。\n*   **对错误轨迹：** 进行“自我修正”和“提示注入”，强制模型继续思考或修正错误，从而生成原本不存在的正确路径。\n**逻辑目的：** 这个过程不仅仅是增加数据量，而是为了**覆盖更广阔的动作空间**，为后续的训练提供丰富的“原材料”。\n\n#### 4.2 算法层面的演进：两阶段行为校准\n**思考：** 有了丰富的数据，如何训练模型？直接用RL可能会因为奖励稀疏或梯度消失而失败。\n**设计：** 作者将训练分为两个紧密衔接的阶段。\n\n*   **阶段一：动作空间探索微调**\n    *   **逻辑：** 利用飞wheel生成的多样化数据进行监督微调（SFT）。\n    *   **目的：** 此时暂不追求极致效率，而是让模型**学会各种可能的解题路径**，打破初始模型的思维定势，实现“广度优先”。\n\n*   **阶段二：迭代行为校准强化学习**\n    *   **逻辑：** 在模型具备探索能力后，引入RL进行优化。\n    *   **难点解决：** 传统的Group-wise RL容易因为轨迹同质化导致梯度消失。\n    *   **创新设计：**\n        *   **分组帕累托采样：** 在采样时，不仅看正确率，还看行为差异度。优先保留那些“既正确又与众不同”的轨迹，确保训练信号始终存在。\n        *   **课程式奖励机制：** 设计了包含“效率惩罚”（工具调用次数、推理长度）的奖励函数。并采用课程学习策略，逐步收紧对效率的要求（从宽松到严格），防止模型为了追求效率而牺牲正确性。\n\n### 5. 逻辑闭环与验证\n\n最终，ET-Agent的形成逻辑链条如下：\n1.  **痛点：** TIR Agent行为低效（冗余或不足），且动作空间巨大。\n2.  **诊断：** 现有方法缺乏对广阔动作空间的有效探索和精准校准。\n3.  **策略：** 先通过数据增强实现“广度探索”，再通过RL实现“精度校准”。\n4.  **实现：**\n    *   **数据侧：** 自进化飞wheel -> 生成多样化、高质量的轨迹。\n    *   **训练侧：** RFT（学广） -> Pareto RL + 课程奖励（学精）。\n5.  **结果：** 模型在保持高准确率的同时，显著提升了工具使用的效率和推理的简洁性。\n\n这一思考过程体现了作者从**现象观察**到**本质归因**，再到**范式假设**，最后落实到**具体机制设计**的严谨学术逻辑。"
                },
                {
                    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
                    "arxiv_id": "2601.06794",
                    "authors": "Zhicong Li, Lingjie Jiang, Yulan Hu, Xingchen Zeng, Yixia Li, Xiangwen Zhang, Guanhua Chen, Zheng Pan, Xin Li, Yong Liu",
                    "summary": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究LLM智能体的训练方法，提出了ECHO框架通过评论引导的强化学习来共同优化策略和评论家。这属于“自我演化”（通过反馈自我完善）和“单智能体”的研究范畴，且不涉及纯应用、纯推理或基础设施优化等排除项。",
                    "summary2": "本文旨在解决critique-guided RL中静态critic因策略演化导致反馈陈旧的问题。针对Open-World Agent Learning场景，我们提出了一种ECHO框架，通过cascaded rollout mechanism和saturation-aware gain shaping实现策略与critic的同步协同演化。并在WebShop、ALFWorld、SciWorld及DeepSearch四个基准上通过任务成功率验证了其有效性。",
                    "summary_translation": "批判引导的强化学习（RL）已成为一种训练 LLM 智能体的强大范式，它通过自然语言反馈来增强稀疏的结果奖励。然而，现有方法通常依赖于静态或离线的评论家模型，这些模型无法随着策略的演变而适应。在在线策略 RL 中，智能体的错误模式会随时间发生偏移，导致固定的评论家变得过时，从而提供效用递减的反馈。为了解决这一问题，我们提出了 ECHO（Evolving Critic for Hindsight-Guided Optimization，用于后见之明引导优化的演进评论家），该框架通过同步的协同进化循环来联合优化策略和评论家。ECHO 采用了一种级联展开机制，评论家首先针对初始轨迹生成多个诊断，随后进行策略细化，从而实现分组结构的优势估计。我们通过一种饱和感知的增益塑形目标来解决学习平台期的挑战，该目标对评论家在高性能轨迹中引发增量改进的行为给予奖励。通过采用双轨 GRPO 更新，ECHO 确保评论家的反馈与不断演进的策略保持同步。实验结果表明，在开放世界环境中，ECHO 能够实现更稳定的训练，并在长视界任务中取得更高的成功率。",
                    "inspiration_trace": "基于论文《No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning》，以下是对作者产出核心方法（ECHO）的逻辑链推演与思考过程还原：\n\n### 第一阶段：观察现状与识别瓶颈\n**思考起点：** 在开放世界的LLM智能体训练中，传统的强化学习（RL）仅依赖稀疏的最终结果奖励，这导致数据效率极低，因为智能体不知道“哪里错了”。\n**现有尝试：** 引入“评论家”模型提供自然语言的诊断反馈。\n**发现矛盾：** 现有的评论家大多是**静态**的（基于模板或离线训练后冻结）。作者观察到，在On-policy RL（在线策略强化学习）中，智能体的策略是不断演进的。\n**逻辑推演：**\n*   早期阶段：智能体犯的是粗粒度错误（如走错房间），需要高层提示。\n*   后期阶段：智能体已掌握基本技能，犯的是细粒度错误（如参数微调），需要精准诊断。\n*   **结论：** 一个固定的、不随策略变化的评论家，其反馈会逐渐变得“陈旧”，甚至产生误导。这就是“Critic Staleness”问题。\n\n### 第二阶段：提出核心假设\n**思维转折：** 既然智能体的错误模式是漂移的，那么最优的评论策略也应当是非静止的。\n**核心假设：** 评论家不应是一个外部的、高高在上的“监督者”，而应是一个与策略共同进化的“伙伴”。\n**评价标准重构：** 评价一个评论家好坏的标准，不应是“它说得是否好听”，而应是“它是否真的诱导了策略的改进”。\n\n### 第三阶段：构建协同进化机制\n**设计挑战：** 如何让两个模型（策略 $P$ 和 评论家 $C$）在同一个训练循环中互相促进，而不是互相干扰？\n**解决方案构思：**\n1.  **闭环构建：** 设计一个“诊断-修正”的级联流程。策略生成轨迹 -> 评论家诊断 -> 策略基于诊断修正。\n2.  **双重优化：** 利用修正后的结果来反向更新两个模型。\n    *   策略更新：学习如何更好地采纳建议。\n    *   评论家更新：学习如何给出能带来更高奖励的建议。\n**逻辑支点：** 通过这种“双轨”同步更新，确保评论家的诊断粒度始终对齐策略当前的短板。\n\n### 第四阶段：解决“最后一公里”的优化难题\n**深入思考：** 在训练后期，策略表现已经很好（例如得分从0.9提升到0.95），这比从0.1提升到0.15要难得多。\n**现有缺陷：** 如果使用线性的奖励差值（$\\Delta s = 0.05$），模型会认为这种高难度的提升价值很低，导致优化停滞。\n**创新思路：** 引入“饱和感知”的奖励设计。\n**逻辑推演：**\n*   假设奖励空间是非线性的，越接近满分，改进的难度和熵减的价值越高。\n*   设计一个增益函数，放大高分区间的微小改进信号。\n*   **目的：** 激励评论家去挖掘那些“看似完美但仍有瑕疵”的轨迹中的关键缺陷。\n\n### 第五阶段：方法论综合与验证\n**最终框架（ECHO）：** 将上述思考整合为一个统一的框架。\n1.  **级联演化：** 通过多视角诊断和条件修正，生成结构化的轨迹组。\n2.  **饱和感知奖励：** 解决高难度阶段的优化动力问题。\n3.  **同步双轨GRPO：** 利用群组相对优势估计，稳定地同时更新策略和评论家。\n\n**总结：** 作者的思考路径从**发现静态反馈与动态策略之间的错配**出发，通过**引入协同进化的视角**重新定义了评论家的角色，并利用**非线性奖励塑形**解决了长尾优化难题，最终实现了ECHO这一能够持续自我提升的智能体训练范式。"
                },
                {
                    "title": "Agentic AI Empowered Intent-Based Networking for 6G",
                    "arxiv_id": "2601.06640",
                    "authors": "Genze Jiang, Kezhi Wang, Xiaomin Chen, Yizhou Huang",
                    "summary": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个分层多智能体框架，利用LLM智能体（编排器和专家）通过ReAct循环进行协作与推理，符合“多智能体：协作”的研究范围。虽然应用于6G网络，但核心贡献在于智能体架构设计，且电信领域不在明确的排除列表中。",
                    "summary2": "本文旨在解决6G网络中将高层自然语言意图转化为可执行网络配置的自主编排问题。针对自然语言操作意图，我们提出了一种基于LLM的分层多智能体框架，通过Orchestrator协调RAN和Core专家代理进行ReAct推理，并在包含12个场景的6G基准测试中，通过Semantic Accuracy和Engineering Utility验证了其有效性。",
                    "summary_translation": "向第六代（6G）无线网络的演进迫切需要一种自主编排机制，该机制能够将高层运维意图转化为可执行的网络配置。现有的基于意图的网络（Intent-Based Networking, IBN）方法要么依赖于难以应对语言差异的基于规则的系统，要么依赖于缺乏可解释性且无法强制执行运维约束的端到端神经模型。本文提出了一种分层多智能体框架，其中基于大语言模型（Large Language Model, LLM）的智能体能够自主分解自然语言意图，咨询领域特定专家，并通过迭代推理-行动（Reasoning-action, ReAct）循环综合生成技术上可行的网络切片配置。该架构采用一个编排器智能体，通过基于结构化网络状态表示的ReAct风格推理，协调两个专家智能体，即无线接入网（Radio Access Network, RAN）智能体和核心网智能体。在多种基准场景下的实验评估表明，该系统优于基于规则的系统和直接LLM提示方法，且其架构原则适用于开放无线接入网（Open RAN, O-RAN）部署。结果还表明，尽管当代大语言模型（LLM）具备通用的电信知识，但网络自动化仍需通过精细的提示工程来编码上下文相关的决策阈值，从而推进下一代无线系统的自主编排能力。",
                    "inspiration_trace": "基于论文《Agentic AI Empowered Intent-Based Networking for 6G》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定义\n**1. 观察现状：**\n作者首先观察到6G网络的愿景是“零接触”的自主管理，即网络能够根据高层业务目标自动配置。然而，现有的网络管理仍高度依赖人工或僵化的脚本。\n\n**2. 识别痛点（语义鸿沟）：**\n现有的意图驱动网络（IBN）方案存在两极分化：\n*   **基于规则的系统：** 虽然严谨，但无法理解自然语言的细微差别（如“极低延迟”与“实时响应”的区别），缺乏灵活性。\n*   **端到端神经网络：** 虽然能处理数据，但缺乏可解释性，且难以强制执行严格的操作约束（如资源硬限制）。\n\n**核心矛盾：** 运营商希望用自然语言描述意图，但底层网络需要精确的参数配置。中间缺乏一个既能“听懂人话”又能“严谨执行”的智能层。\n\n---\n\n### 第二阶段：技术选型与假设提出\n**3. 引入LLM的局限性分析：**\n大语言模型（LLM）看似是解决语义鸿沟的完美工具，但作者敏锐地指出其直接应用的致命缺陷：LLM本质是文本生成器，而非决策者。它们缺乏对当前网络状态的感知，无法验证配置的可行性，容易产生“幻觉”。\n\n**4. 提出核心假设：**\n如果将LLM从一个“被动的问答机器”转变为一个“主动的智能体”，赋予其感知环境、使用工具和迭代推理的能力，就能填补这一鸿沟。即，利用**Agentic AI（智能体AI）**范式。\n\n---\n\n### 第三阶段：架构设计与逻辑演进\n**5. 解决“认知过载”问题（从单体到分层）：**\n作者进一步思考：让一个LLM同时处理无线接入网（RAN）和核心网的所有复杂决策，是否过于沉重？\n*   **推演：** 单体模型容易在多域约束下顾此失彼，且上下文窗口有限。\n*   **决策：** 采用**分层多智能体架构**。模仿人类组织结构，设立一个“编排者”负责总体理解和任务分解，下设“RAN专家”和“核心网专家”负责具体领域的深度推理。\n\n**6. 解决“盲目执行”问题（引入ReAct机制）：**\n如何确保智能体不是在“瞎猜”配置？\n*   **推演：** 传统的“一次性提示”无法处理复杂的依赖关系。\n*   **决策：** 引入**ReAct（推理+行动）循环**。强制智能体在执行动作前先进行“思考”，执行后观察结果，再进行下一步。这种迭代机制使得系统能够自我纠正错误。\n\n---\n\n### 第四阶段：落地策略与工程化思考\n**7. 解决“幻觉”问题（状态锚定）：**\n如何让LLM基于现实而非想象做决策？\n*   **推演：** 仅靠Prompt中的指令是不够的，必须让模型看到“真相”。\n*   **决策：** 将网络状态（负载、频谱、延迟矩阵）结构化为JSON数据，直接注入到智能体的上下文中。这相当于给智能体戴上了一副“AR眼镜”，使其推理基于实时数据。\n\n**8. 解决“领域知识注入”问题（提示工程）：**\n如何让通用的LLM具备电信专家的判断力（例如：工业环境下为何选中频段而非毫米波）？\n*   **推演：** 微调模型成本太高且不灵活。\n*   **决策：** 采用精细化的**提示工程**。在系统提示词中硬编码专家策略和量化阈值（如“负载>80%发出警告”），将领域知识“软编码”进模型的行为逻辑中。\n\n---\n\n### 第五阶段：实验验证与深层洞察\n**9. 评估维度的重构：**\n作者意识到传统的“准确率”不足以评价网络配置。\n*   **思考：** 配置不仅要“对”（符合专家意图），还要“好”（资源利用率高）。\n*   **产出：** 提出了**混合评估框架**，结合“语义准确性”（对齐专家标准）和“工程效用”（量化技术指标）。\n\n**10. 发现“提示词偏差”现象（意外收获）：**\n在实验中，作者发现系统在某些场景下总是倾向于选择低延迟节点，即使不需要。\n*   **反思：** 这不是随机错误，而是提示词中“acceptable”一词被模型理解为“备胎”而非“推荐”。\n*   **结论：** 提示工程不仅是技术细节，更是系统架构的关键组件。微小的语言变化会导致系统性的行为偏差，这确立了提示词验证在部署中的核心地位。\n\n---\n\n**总结：**\n作者的思考路径是从**6G自主化的宏观需求**出发，识别出**自然语言与网络配置之间的语义鸿沟**，进而假设**Agentic AI**是解决方案。通过**分层架构**降低认知复杂度，利用**ReAct循环**确保推理严谨性，通过**状态锚定**消除幻觉，最终在实验中不仅验证了方法的有效性，还深刻揭示了**提示工程对系统行为的决定性影响**。"
                },
                {
                    "title": "From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design",
                    "arxiv_id": "2601.06776",
                    "authors": "Xufei Tian, Wenli Du, Shaoyi Yang, Han Hu, Hui Xin, Shifeng Qu, Ke Ye",
                    "summary": "Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个包含四个专门智能体（任务理解、拓扑生成、参数配置、评估分析）的多智能体LLM工作流，涉及智能体之间的协作以及与外部工具（模拟软件）的交互，符合“多智能体：协作”和“工具使用”的研究范围。尽管应用于化工领域，但其核心贡献在于智能体架构和工作流设计，而非单纯的领域应用。",
                    "summary2": "本文旨在实现从文本描述到可执行化工过程模拟的端到端自动化设计。针对自然语言输入的化工过程设计场景，我们提出了一种结合增强蒙特卡洛树搜索（E-MCTS）的多智能体LLM工作流，通过四个专门智能体协同工作。在Simona数据集上，通过模拟收敛率（SCR）和设计时间验证了其有效性，相比最先进基线收敛率提升31.1%，设计时间减少89.0%。",
                    "summary_translation": "Process simulation (过程模拟) 是 Chemical engineering design (化工设计) 的关键基石。当前的 Automated chemical design methodologies (自动化化工设计方法) 主要集中在 Process flow diagrams (工艺流程图) 的各种表示形式上。然而，将这些图表转化为 Executable simulation flowsheets (可执行模拟流程) 仍然是一项耗时且费力的工作，需要在 Simulation software (模拟软件) 中进行大量的 Manual parameter configuration (手动参数配置)。在这项工作中，我们提出了一种新颖的 Multi-agent workflow (多智能体工作流)，该工作流利用 Large language models (LLMs, 大语言模型) 的 Semantic understanding (语义理解) 能力，并实现与 Chemical process simulation software (化工过程模拟软件) 的 Iterative interactions (迭代交互)，从而实现了从 Textual process specifications (文本过程规范) 到用于设计增强的 Computationally validated software configurations (计算验证的软件配置) 的 End-to-end automated simulation (端到端自动模拟)。我们的方法集成了四个分别负责 Task understanding (任务理解)、Topology generation (拓扑生成)、Parameter configuration (参数配置) 和 Evaluation analysis (评估分析) 的 Specialized agents (专门智能体)，并结合 Enhanced Monte Carlo Tree Search (增强蒙特卡洛树搜索) 来准确解释语义并稳健地生成配置。在大规模 Process description dataset (过程描述数据集) Simona 上进行评估，我们的方法与 State-of-the-art baselines (最先进基线) 相比，Simulation convergence rate (模拟收敛率) 提高了 31.1%，与专家 Manual design (手动设计) 相比，Design time (设计时间) 减少了 89.0%。这项工作展示了 AI-assisted chemical process design (AI辅助化工过程设计) 的潜力，弥合了 Conceptual design (概念设计) 与 Practical implementation (实际实施) 之间的差距。我们的 Workflow (工作流) 适用于包括 Pharmaceuticals (制药)、Petrochemicals (石化)、Food processing (食品加工) 和 Manufacturing (制造业) 在内的多种 Process-oriented industries (流程导向型行业)，为 Automated process design (自动化过程设计) 提供了一种 Generalizable solution (可推广解决方案)。",
                    "inspiration_trace": "基于论文《From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论产出的思考过程：\n\n### 第一阶段：宏观问题定位——“最后一公里”的瓶颈\n**观察：** 化工过程设计是工业的核心，但目前的流程极其低效。工程师需要花费数周时间，将高层的概念设计（如“设计一个乙烯裂解过程”）转化为可在模拟软件（如Aspen Plus等）中运行的详细配置。\n**痛点识别：** 现有的自动化方法大多集中在“画图”阶段（生成流程图PFD或超图结构），但这只是设计的中间态。真正的瓶颈在于从“结构图”到“可执行仿真配置”的转化——这需要人工设定成百上千个相互依赖的热力学和操作参数。\n**核心矛盾：** 概念设计与工程落地之间存在巨大的鸿沟。AI能生成漂亮的图纸，但图纸无法直接运行，无法验证可行性。\n\n### 第二阶段：现有方案的局限性分析\n**反思：** 为什么现有的AI方法（如CNN、GNN或早期的LLM应用）解决不了这个问题？\n**结论：**\n1.  **停留在表征层：** 现有方法将设计视为静态的图像或图结构生成任务，忽略了化工过程本质上是基于物理化学方程的动态计算过程。\n2.  **缺乏闭环验证：** 生成的结构如果没有经过模拟软件的严格计算，往往是不收敛或不可行的。现有方法缺乏与专业仿真软件的交互能力。\n3.  **语义与参数的割裂：** LLM擅长理解自然语言（语义），但很难直接生成符合复杂物理约束的精确参数（数值）。\n\n### 第三阶段：核心假设提出——“人机协作”的代理化\n**假设：** 如果能构建一个系统，模仿人类专家的思维方式——即“理解意图 -> 搭建结构 -> 设定参数 -> 软件试算 -> 根据报错调整”，并利用LLM处理语义，利用仿真软件处理物理计算，就能打通从文本到仿真的全链路。\n**关键转变：** 从“一次性生成”转变为“迭代式交互”。不再追求LLM直接写出完美的代码，而是允许它通过工具与仿真软件进行多轮对话，直到收敛。\n\n### 第四阶段：方法论构建——多智能体分工\n**思考：** 化工设计任务过于复杂，单个LLM无法同时兼顾语义理解、拓扑规划、参数计算和结果评估。必须进行“分而治之”。\n**逻辑推演：**\n1.  **任务理解：** 首先需要将模糊的自然语言转化为结构化的工程需求（如明确组分、约束条件）。\n2.  **拓扑生成：** 专注于“骨架”搭建，确定单元操作（反应器、精馏塔）及其连接关系，暂时不纠结细节。\n3.  **参数配置：** 专注于“血肉”填充，利用LLM的推理能力结合领域知识，为拓扑赋予初始参数。\n4.  **评估分析：** 充当“质检员”，接收仿真软件的反馈（是否收敛、经济性如何），并决定是输出结果还是反馈修改。\n\n### 第五阶段：搜索策略优化——如何处理“失败”\n**深层挑战：** 化工设计空间巨大，且充满了“陷阱”。很多设计在仿真中会失败（不收敛）。传统的搜索算法（如标准MCTS）通常会直接丢弃失败的分支。\n**创新洞察：** 在化工设计中，一个“失败”的仿真往往包含有价值的信息（例如拓扑结构是对的，只是某个温度参数设错了）。如果直接丢弃，就浪费了探索成本。\n**策略演进：** 提出**增强型蒙特卡洛树搜索（E-MCTS）**。\n1.  **双重价值评估：** 区分“当前价值”（仿真是否成功）和“潜在价值”（结构是否合理）。即使仿真失败，如果结构合理，仍保留其探索潜力。\n2.  **动态重访机制：** 当搜索陷入停滞时，主动回到那些曾经失败但潜力巨大的节点进行微调，从而跳出局部最优，找到真正可执行的解。\n\n### 总结：逻辑链的全景图\n作者从**“设计效率低”**的宏观问题出发，识别出**“结构到可执行配置的断层”**这一核心痛点。通过分析现有AI**“重表征、轻验证”**的缺陷，提出了**“LLM语义理解 + 仿真软件物理验证”**的闭环假设。为了实现这一假设，作者采用了**多智能体协作**来解耦复杂任务，并创新性地设计了**E-MCTS算法**来从失败中学习，最终实现了从自然语言文本到工业级仿真配置的端到端自动化。"
                },
                {
                    "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
                    "arxiv_id": "2601.06502",
                    "authors": "Shengkai Chen, Zhiguang Cao, Jianan Zhou, Yaoxin Wu, Senthilnath Jayavelu, Zhuoyi Lin, Xiaoli Li, Shili Xiang",
                    "summary": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了名为DRAGON的框架，明确将其定义为“LLM驱动的分解与重构智能体”。该研究涵盖了单智能体的核心能力，包括规划（分解问题）、记忆（积累的经验/自适应经验记忆）、工具使用（与优化环境交互）以及自我反思（从反馈中迭代学习）。它侧重于智能体机制而非被排除的纯领域应用。",
                    "summary2": "本文旨在解决大语言模型（LLM）在大规模组合优化问题（COP）中可扩展性受限的问题。针对大规模COP场景，我们提出了一种名为DRAGON的分解与重构智能体框架，通过迭代识别高潜力区域并求解局部子问题来优化全局解。在TSPLIB、CVRPLIB和Weibull-5k等基准数据集上，通过Optimality Gap等指标验证了其有效性，在超大规模实例上实现了近最优解，显著优于现有基于LLM的求解器。",
                    "summary_translation": "大语言模型近期在利用基于提示的策略解决组合优化问题方面展现出潜力。然而，其可扩展性和泛化能力仍然受限，且随着问题规模的增大，其有效性会降低，特别是在涉及超过30个节点的路径问题中尤为明显。我们提出了 DRAGON（Decomposition and Reconstruction Agents Guided OptimizatioN，分解与重构智能体引导优化），这是一个结合了元启发式设计和 LLM 推理优势的新型框架。DRAGON 从一个初始全局解出发，自主识别具有高优化潜力的区域，并策略性地将大规模 COPs 分解为易于处理的子问题。随后，每个子问题被重新表述为一个简洁的局部优化任务，并在积累经验的指导下，通过针对性的 LLM 提示进行求解。最后，将局部优化后的解系统地重新整合到原始全局上下文中，从而产生显著改善的整体结果。通过与优化环境的持续交互并利用自适应经验记忆，智能体能够从反馈中迭代学习，从而有效地将符号推理与启发式搜索相结合。实验结果表明，与局限于小规模实例的现有基于 LLM 的求解器不同，DRAGON 在 TSPLIB、CVRPLIB 和 Weibull-5k 装箱基准测试中始终能生成可行解，并在拥有超过 300 万变量的背包问题上取得了接近最优的结果（0.16% gap）。这项工作展示了反馈驱动的语言智能体作为一种可泛化且可解释的大规模优化新范式的潜力。",
                    "inspiration_trace": "基于对论文《DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization》的深入分析，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 第一阶段：观察与矛盾识别（LLM的“能力悖论”）\n\n**1. 初始观察：**\n作者首先观察到大语言模型（LLM）在解决组合优化问题（COP）上展现出了惊人的潜力。通过简单的提示工程，LLM能够利用其语义理解和推理能力，直接生成解决方案，甚至无需显式的算法指令。\n\n**2. 发现瓶颈：**\n然而，这种能力存在明显的“规模天花板”。当问题规模扩大（例如TSP节点超过30个）时，LLM的性能急剧下降。\n*   **原因分析：** 这并非LLM不懂逻辑，而是受限于**上下文长度**和**长序列推理的一致性**。面对成千上万个节点，LLM无法一次性处理所有信息，且容易在生成超长序列时“迷失”方向，导致逻辑断裂。\n\n**3. 核心矛盾：**\n作者面临一个核心矛盾：**LLM拥有强大的通用推理能力，却受限于“工作记忆”容量，无法处理大规模现实问题。**\n\n---\n\n### 第二阶段：现有方案的局限与灵感（寻找“中间态”）\n\n**1. 审视传统方法：**\n传统运筹学中的元启发式算法（如大规模邻域搜索 LNS）非常擅长处理大规模问题。它们的核心策略是“分而治之”：将大问题拆解为小问题，局部优化后再合并。\n*   **局限：** 这些方法高度依赖人工设计的启发式规则。如何拆解？如何定义邻域？这些都需要专家针对特定问题编写代码，缺乏泛化性。\n\n**2. 审视现有LLM方案：**\n*   **直接提示法：** 无法突破规模限制。\n*   **代码生成法：** 让LLM写启发式代码。虽然可行，但需要大量的训练和演化时间，且跨领域迁移成本高。\n\n**3. 关键灵感：**\n作者意识到，**“分而治之”是解决规模问题的唯一通用路径**。既然人工设计的规则缺乏泛化性，而LLM具备语义理解能力，那么**能否用LLM来替代人工，自动执行“分而治之”中的决策过程？**\n\n---\n\n### 第三阶段：核心假设提出（从“解题者”到“管理者”的角色转变）\n\n**1. 角色重构：**\n作者不再试图让LLM作为一个“全能解题者”去一次性解决整个大问题，而是将其重新定位为一个**“管理者”或“架构师”**。\n\n**2. 提出假设：**\n*   **假设一（分解）：** LLM虽然无法解决大问题，但它具备足够的“直觉”来审视一个现有的全局解，并识别出其中**“看起来不太对劲”或“有优化潜力”的局部区域**。\n*   **假设二（重构）：** 如果将问题规模缩小到LLM的“舒适区”（如20-30个节点），LLM完全有能力在满足特定边界约束的前提下，找到该局部的最优解。\n\n---\n\n### 第四阶段：方法论构建（DRAGON框架的逻辑闭环）\n\n基于上述假设，作者构建了“分解-重构”的迭代框架：\n\n**1. 分解代理：**\n*   **逻辑：** 模仿人类专家的直觉。面对一个复杂的路线图，人眼会先看到“绕路了”或“交叉了”的局部。\n*   **实现：** 设计一个Prompt，让LLM分析当前全局解，输出一个“活跃片段”（待优化部分）和“静态片段”（保持不变部分）。这实际上是将**全局约束转化为局部边界条件**。\n\n**2. 压缩与重构代理：**\n*   **逻辑：** 将大问题降维。只把“活跃片段”喂给LLM，并明确告知它必须连接的“静态片段”节点（作为强制约束）。\n*   **实现：** 此时，任务变成了一个带约束的小规模COP。LLM利用其推理能力，在局部范围内寻找最优路径。\n\n**3. 状态更新与反馈循环：**\n*   **逻辑：** 局部最优不一定等于全局最优，且可能陷入死胡同。\n*   **实现：** 引入模拟退火机制，允许接受“暂时变差”的解以跳出局部最优。同时，引入“经验记忆”，记录之前产生的不可行解，防止LLM重复犯错。\n\n---\n\n### 第五阶段：验证与泛化（证明通用性）\n\n**1. 跨域测试：**\n为了证明这不是针对TSP的“特技”，作者将逻辑迁移到装箱（BPP）和背包（MKP）问题。\n*   **思考：** 在路由问题中，约束是“路径连续”；在装箱问题中，约束是“容量限制”。DRAGON框架的核心在于Prompt中对约束的自然语言描述，这证明了其**通用性**。\n\n**2. 对比实验：**\n与纯Prompt方法（OPRO）和代码生成方法对比，突显DRAGON在**大规模**场景下的优势。它既保留了LLM的灵活性，又借用了传统算法的迭代搜索框架。\n\n---\n\n### 总结：思想演进脉络\n\n1.  **起点：** LLM推理强但记性差（上下文限制），无法处理大规模COP。\n2.  **转折：** 借鉴传统“分而治之”思想，但用LLM替代人工规则。\n3.  **核心：** 将LLM从“解题者”转变为“局部诊断师”和“局部修复师”。\n4.  **机制：** 通过“分解（识别病灶）- 压缩（隔离病灶）- 重构（局部手术）- 整合（恢复健康）”的循环，实现大规模问题的逐步优化。\n5.  **本质：** 这是一种**神经符号结合**的尝试，用LLM的直觉引导符号化的搜索过程。"
                },
                {
                    "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
                    "arxiv_id": "2601.06377",
                    "authors": "Ningning Zhang, Xingxing Yang, Zhizhong Tan, Weiping Deng, Wenyong Wang",
                    "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了HiMem框架，专注于解决LLM智能体在长期交互中的记忆构建、检索和动态更新问题，这直接属于“单智能体”研究范围中的“记忆”和“自我演化”范畴。论文明确旨在构建自适应和自我演化的对话智能体，不属于排除的纯应用、纯推理或其他排除类别。",
                    "summary2": "本文旨在解决LLM长视界对话代理在适应性、可扩展性和自我进化方面的局限性。针对长视界对话场景，我们提出了一种名为HiMem的分层长期记忆框架。该方法通过Topic-Aware Event–Surprise Dual-Channel Segmentation构建Episode Memory，结合多阶段提取的Note Memory形成分层结构，并引入冲突感知的Memory Reconsolidation机制。我们在LoCoMo benchmark上通过GPT-Score和F1等指标验证了其有效性，结果表明HiMem在准确性和一致性上显著优于基线。",
                    "summary_translation": "尽管长期记忆系统近年来取得了显著进展，但在持续交互场景下的适应性、可扩展性和自我进化方面仍存在明显局限。受认知理论启发，我们提出了 HiMem，这是一个面向长程对话的分层长期记忆框架，旨在支持持续交互过程中的记忆构建、检索和动态更新。HiMem 通过 Topic-Aware Event--Surprise Dual-Channel Segmentation（主题感知的事件-惊喜双通道分割）策略构建认知一致的 Episode Memory（情景记忆），并通过多阶段信息提取流水线构建能够捕获稳定知识的 Note Memory（笔记记忆）。这两种记忆类型在语义上相互关联，形成了一种桥接具体交互事件与抽象知识的分层结构，从而在不牺牲信息保真度的情况下实现高效检索。HiMem 支持混合检索和 Best-Effort Retrieval（尽力而为检索）策略以平衡准确性与效率，并结合 Conflict-Aware Memory Reconsolidation（冲突感知的记忆再巩固）机制，根据检索反馈对存储的知识进行修正和补充。这种设计使得记忆能够在长期使用过程中实现持续的自我进化。在长程对话基准上的实验结果表明，HiMem 在准确性、一致性和长程推理方面始终优于代表性基线，同时保持了良好的效率。总体而言，HiMem 为构建自适应且自我进化的 LLM-based（基于大语言模型）对话智能体提供了一个有原则且可扩展的设计范式。代码可在 https://github.com/jojopdq/HiMem 获取。",
                    "inspiration_trace": "基于对论文《HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观问题定位——从“金鱼记忆”到长程智能\n**思考起点：** 当前的大语言模型（LLM）虽然具备强大的短期推理能力，但在面对长周期的多轮对话或跨会话任务时，表现出了明显的“健忘症”。\n**核心矛盾：** 现实世界的智能体需要像人类一样，在长达数月甚至数年的交互中保持连贯性。然而，现有的LLM Agent在长时程交互中，无法可靠地保存、组织和利用过往信息。这成为了构建自适应、一致性长程对话智能体的根本瓶颈。\n\n### 第二阶段：痛点诊断——现有方案的三大局限\n作者对现有的解决方案（如RAG、长上下文建模、结构化记忆）进行了批判性审视，归纳出三个无法同时解决的深层矛盾：\n\n1.  **语义错位：** 现有的记忆提取往往脱离了原始对话的上下文，导致在处理时间指代、指代消解和隐含语义时出现错误。记忆变成了“孤立的碎片”，而非“连贯的体验”。\n2.  **保真度与效率的零和博弈：**\n    *   保留细粒度的对话日志（高保真）会导致检索成本极高，且充满噪声。\n    *   过度抽象的压缩表示（高效率）虽然检索快，但丢失了推理所需的细节和个性化信息。\n    *   *思考：* 现有的扁平化或单一层次结构无法兼顾这两者。\n3.  **静态更新的僵化：** 现有的记忆更新通常是静态的或仅基于相似度的。当新信息与旧记忆冲突、重叠或需要修正时，系统缺乏原则性的机制来“修正”知识，导致长期一致性随时间推移而退化。\n\n### 第三阶段：认知假设——向人类记忆机制借力\n**思维转折：** 既然工程上的“修补”难以解决上述矛盾，作者转向认知科学寻找灵感。\n**核心假设：** 人类之所以能高效处理长时记忆，是因为我们的记忆不是单一维度的，而是**分层**且**动态演进**的。\n*   **层次性：** 人类同时拥有“情景记忆”（具体的经历）和“语义记忆”（抽象的知识/常识）。\n*   **动态性：** 人类记忆会通过“记忆再巩固”机制，在回忆时根据新反馈修正旧认知。\n\n**推论：** 如果LLM Agent也能构建这种“从具体事件到抽象知识”的层次结构，并引入冲突感知的更新机制，就能同时解决保真度、效率和一致性问题。\n\n### 第四阶段：方法论构建——HiMem的逻辑闭环\n基于上述假设，作者构建了HiMem的框架，其设计逻辑环环相扣：\n\n**1. 解决“保真度与效率矛盾” -> 构建双层记忆架构**\n*   **设计：** 将记忆分为**情景记忆**和**笔记记忆**。\n*   **逻辑：**\n    *   *情景记忆*保留原始的、细粒度的交互片段，负责提供推理所需的“证据”和“细节”（保真度）。\n    *   *笔记记忆*存储提取出的稳定知识（如用户偏好、事实），负责快速定位和概括（效率）。\n    *   两者通过语义链接形成层级，检索时先查笔记（快），不够再查情景（准），实现了最佳权衡。\n\n**2. 解决“语义错位” -> 引入认知一致性的分割与提取**\n*   **设计：** 提出“主题感知-事件-惊喜双通道分割”策略。\n*   **逻辑：** 传统的按主题分割是不够的。人类记忆的边界往往由“惊喜”（如意图突变、情绪波动）决定。因此，必须融合“主题变化”和“认知不连续性”两个信号来切分对话，确保记忆单元在认知上是自洽的，减少跨片段的干扰。\n\n**3. 解决“静态更新僵化” -> 冲突感知的记忆再巩固**\n*   **设计：** 在检索失败时触发“记忆再巩固”机制。\n*   **逻辑：** 检索失败不应仅仅是一个错误信号，而应是一个学习信号。当笔记记忆无法回答，但情景记忆能找到证据时，说明笔记缺失或过时了。此时，系统应自动从情景记忆中提取信息，与旧笔记进行冲突检测（独立、可扩展、矛盾），并执行增删改操作。这使得记忆系统能够像人类一样，在使用中不断自我进化。\n\n### 第五阶段：验证与总结——从工程到范式\n**思考终点：** 通过在长程对话基准上的实验，验证了这种分层结构不仅优于扁平化的基线模型，而且证明了“记忆再巩固”带来的自我进化能力是提升长期一致性的关键。\n**最终结论：** 长期记忆不应是一个静态的外部仓库，而应是一个动态的、多层次的、与检索过程紧密耦合的演进系统。HiMem提供了一种将认知理论系统性地融入LLM Agent设计的范式。"
                },
                {
                    "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation",
                    "arxiv_id": "2601.06328",
                    "authors": "Ziqiao Xi, Shuang Liang, Qi Liu, Jiaqing Zhang, Letian Peng, Fang Nan, Meshal Nayim, Tianhui Zhang, Rishika Mundada, Lianhui Qin, Biwei Huang, Kun Zhou",
                    "summary": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究“Tool-using LLM agents”，提出了包含规划器和执行器的智能体框架，涉及规划、工具使用和自我修正，符合单智能体研究范围。",
                    "summary2": "本文旨在解决现有工具使用代理在开放世界环境中缺乏大规模、真实测试与训练环境的问题。针对大规模工具池、长时程任务及不可靠状态等场景，我们提出了ToolGym环境，该环境集成了任务创建引擎、状态控制器及Planner–Actor代理框架。我们在包含5,571个工具的ToolGym环境上，通过Success Rate、Recovery Rate等指标验证了其有效性，并证明利用其生成的少量数据微调模型即可超越大规模数据基线。",
                    "summary_translation": "Tool-using LLM agents (使用工具的大语言模型智能体) 在具有 large tool pools (大型工具池)、long-horizon objectives (长期目标)、wild constraints (严苛约束) 和 unreliable tool states (不可靠工具状态) 的 open-world settings (开放世界设置) 中仍然面临挑战。为了实现可扩展且真实的训练和测试，我们引入了一个 open-world tool-using environment (开放世界工具使用环境)，该环境基于 204 个常用应用程序中的 5,571 个格式统一的工具构建。它包含一个 task creation engine (任务创建引擎)，用于合成具有 wild constraints (严苛约束) 的 long-horizon、multi-tool workflows (长期、多工具工作流)，以及一个 state controller (状态控制器)，用于注入中断和故障以对鲁棒性进行压力测试。基于该环境，我们开发了一个 tool select-then-execute agent framework (先选择后执行工具的智能体框架)，采用 planner-actor decomposition (规划者-执行者分解) 架构，将审慎推理和自我纠正与分步执行分离开来。对 state-of-the-art LLMs (最先进的大语言模型) 的全面评估揭示了工具规划与执行能力之间的不一致，现有 LLM 在遵循约束方面的不足，以及 DeepSeek-v3.2 最强的鲁棒性。最后，我们从环境中收集了 1,170 条 trajectories (轨迹) 来 fine-tune (微调) LLM，其性能优于使用 119k 样本的 baselines (基线模型)，表明该环境既是一个 realistic benchmark (真实基准)，也是 tool-using agents (工具使用智能体) 的 data engine (数据引擎)。我们的代码和数据将公开发布。",
                    "inspiration_trace": "基于论文《ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation》，以下是对作者核心方法提出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题定义\n**从“实验室表现”到“野外能力”的鸿沟**\n\n1.  **观察现象**：\n    *   现有的LLM Agent在受控的基准测试中表现优异，但在真实世界的复杂应用中往往失效。\n    *   人类通过在复杂环境中的试错、积累经验来掌握工具，而Agent缺乏一个类似的、能够模拟真实世界复杂度的学习环境。\n\n2.  **诊断核心矛盾**：\n    *   **理想化 vs. 现实性**：现有基准大多是在“温室”中构建的——工具集有限、任务路径单一、环境状态稳定（即“Happy Path”）。\n    *   **静态 vs. 动态**：真实世界是开放且混乱的，包含海量工具、长时程任务、复杂的约束条件以及不可靠的服务状态。\n\n3.  **提出宏观问题**：\n    *   如何构建一个既具备**大规模真实工具**，又能模拟**真实世界混乱度**（约束、故障、长链条）的开放环境，以实现Agent的可扩展测试与训练？\n\n---\n\n### 第二阶段：环境构建的逻辑演进\n**构建一个“脏乱差”但真实的开放世界**\n\n1.  **解决“规模与真实性”问题（工具层）**：\n    *   **思考**：不能只用模拟API，必须用真实的。但真实API格式各异，难以统一管理。\n    *   **决策**：采用 **MCP (Model Context Protocol)** 作为统一标准，从真实应用中筛选并验证了5,571个工具，构建了一个可执行、可检索的大规模工具库。\n\n2.  **解决“任务复杂度”问题（任务层）**：\n    *   **思考**：真实任务不是简单的“调用A再调用B”，而是包含模糊指令和多重约束（如时间限制、跨应用权衡）。\n    *   **决策**：设计**任务创建引擎**。不仅仅是随机组合工具，而是通过“采样-合成-迭代修订”的流程，主动注入“野性约束”，迫使Agent必须进行多步推理和权衡。\n\n3.  **解决“环境鲁棒性”问题（状态层）**：\n    *   **思考**：现实世界网络会断、服务器会崩、权限会变。只测试成功路径无法衡量Agent的鲁棒性。\n    *   **决策**：引入**状态控制器**。作为中间件，它不随机干扰，而是按策略注入可控的故障（如超时、状态改变、约束突变），将“测试”升级为“压力测试”。\n\n---\n\n### 第三阶段：Agent架构的适应性演进\n**应对长时程复杂性的“分而治之”**\n\n1.  **识别Agent瓶颈**：\n    *   在开放世界、长时程任务中，单一的ReAct（推理-行动）循环容易迷失方向，且难以在执行中途纠正错误，导致“烂尾”。\n\n2.  **架构设计假设**：\n    *   人类处理复杂任务时，通常会先做全局规划，再动手执行，并在执行中监控进度。Agent也应模仿这种**慢思考（规划）与快行动（执行）的分离**。\n\n3.  **提出Planner-Actor框架**：\n    *   **Planner（规划者）**：负责宏观视角，将长时程目标分解为子目标图，并监控Actor的执行进度，确保不偏离主线。\n    *   **Actor（执行者）**：负责微观视角，在Planner的指导下进行具体的工具检索和调用。\n    *   **逻辑闭环**：通过这种解耦，将“深思熟虑”与“自我纠正”从繁琐的步骤执行中剥离出来，专门解决长链条中的连贯性问题。\n\n---\n\n### 第四阶段：价值验证与闭环\n**从“测试场”到“数据引擎”的升华**\n\n1.  **评估维度的细化**：\n    *   既然环境复杂了，评估就不能只看“对/错”。作者构建了多维度的评估体系（质量、鲁棒性、约束遵循、规划），并引入LLM-as-Judge来处理非结构化的输出。\n\n2.  **关键发现与假设验证**：\n    *   通过实验发现：现有LLM的**规划能力往往强于执行能力**，且**约束遵循是主要失败点**。这反向证明了构建包含“野性约束”环境的必要性。\n\n3.  **数据效率的洞察**：\n    *   **思考**：在这个困难环境中生成的轨迹，是否比简单环境中的海量数据更有价值？\n    *   **结论**：验证了“质量大于数量”。仅用1,170条在ToolGym中生成的、包含复杂约束和故障恢复的高质量轨迹，微调后的效果超越了使用119k条简单数据的基线。\n\n4.  **最终定位**：\n    *   ToolGym不仅仅是一个Benchmark（考卷），更是一个Data Engine（教材）。它通过模拟真实世界的困难，迫使Agent学会真正的工具使用能力，从而形成了“环境测试 -> 收集高质量数据 -> 训练更强Agent”的闭环。"
                },
                {
                    "title": "PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction",
                    "arxiv_id": "2601.06158",
                    "authors": "Zibin Meng, Kani Chen",
                    "summary": "Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了PsyAgent，一种基于心理建模和语境交互构建类人智能体的架构。它侧重于智能体的内部状态（个体结构、记忆/生活片段）和基于语境的行为生成，这属于单智能体研究（记忆、行为一致性）的范畴。它不是纯应用或纯推理。",
                    "summary2": "本文旨在构建能够模拟性格特质与社会结构交互的类人智能体。针对Big Five人格先验与结构化社会场景，我们提出了一种PsyAgent框架，该框架耦合了Individual Structure (IS) 和Multi-Scenario Contexting (MSC)，并利用合成监督数据通过PEFT和DPO微调小模型。我们在多轮角色扮演和决策任务上，通过ProfileAcc、MAE_5等指标验证了其有效性。",
                    "summary_translation": "拟人化智能体需要对倾向与社会结构之间的相互作用进行建模。我们提出了PsyAgent，该模型将Big Five trait prior（大五人格特质先验）与Bourdieu's cognitive-social co-structure（布迪厄的认知-社会共结构）相结合。PsyAgent包含两个部分：(i) Individual Structure (IS, 个体结构)，这是一种machine-usable profile（机器可读档案），编码了traits and facets（特质与侧面）、cognitive style（认知风格）、values（价值观）、cultural and educational capital（文化与教育资本）以及salient life episodes（显著生活片段）；(ii) Multi-Scenario Contexting (MSC, 多情境语境化)，这是一种跨越八个arenas（领域：工作、家庭、友谊、陌生人与公民生活、独处与自我调节、浪漫关系、学习及公共表达）的role-relationship-norm frames（角色-关系-规范框架）。在inference（推理）阶段，fixed structured prompts（固定结构化提示）将active scenario（当前活动场景）与agent profile（智能体档案）绑定，从而产生既稳定又对情境敏感的行为。我们通过instantiate（实例化）IS和MSC来synthesize supervision（合成监督信号，包括role-play dialogues（角色扮演对话）、decision probes（决策探针）和feedback trajectories（反馈轨迹）），随后对small LLM（小型大语言模型）进行fine-tune（微调）。生成的模型能够针对指定的Big Five configurations（大五人格配置）产生一致的、可识别的persona-aligned behaviors（人格对齐行为），并且在我们的metrics（评估指标：persona consistency（人格一致性）、contextual appropriateness（情境适当性）、style matching（风格匹配度）、trait identifiability（特质可识别性）和long-horizon stability（长期稳定性））上，匹配或超越了多个更大的untuned LLMs（未微调大语言模型）及其他untuned baselines（未微调基线模型）。Ablations（消融实验）表明，IS主要提升了trait fidelity（特质保真度）和stylistic stability（风格稳定性），而MSC则增强了norm awareness（规范意识）和decision fit（决策拟合度）；两者对于实现cross-scenario performance（跨场景表现）均是必不可少的。PsyAgent为构建personality-grounded agents（基于人格的智能体）提供了一种精确且data-efficient（数据高效）的架构。",
                    "inspiration_trace": "基于对论文《PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观观察与问题界定\n**（从“角色扮演的不稳定性”出发）**\n\n1.  **现象观察**：\n    作者首先观察到当前基于LLM的智能体在角色扮演时存在一个核心痛点：**“人格漂移”与“情境崩塌”**。虽然现有的Agent可以通过Prompt设定“你是一个外向的人”，但在多轮对话或场景切换（如从工作场景切换到家庭场景）时，Agent往往会忘记设定，或者行为变得前后不一致。\n\n2.  **根源诊断**：\n    作者认为，单纯依靠Prompt注入静态的“性格标签”或“知识库”是不够的。人类的行为不仅仅是内在性格的产物，而是**“内在倾向”与“外部社会结构”相互作用的结果**。现有的Agent缺乏对“社会结构”的显式建模，导致它们无法像人类一样，在不同社会场域中表现出既稳定又灵活的行为。\n\n### 第二阶段：理论假设与跨学科融合\n**（引入心理学与社会学视角）**\n\n1.  **寻找理论锚点**：\n    为了解决上述问题，作者转向经典理论寻找支撑：\n    *   **心理学视角**：采用“大五人格”作为稳定的内在倾向先验。\n    *   **社会学视角**：引入布迪厄的“认知-社会共结构”理论，即个体的行为是由其“惯习”与“场域”共同决定的。\n\n2.  **提出核心假设**：\n    要构建真正像人的Agent，必须建立一个**“双轨耦合机制”**：\n    *   一轨是**稳定的内在特质**（我是谁）。\n    *   一轨是**结构化的外部情境**（我在哪，面对谁，遵循什么规范）。\n    *   **假设**：只有显式地建模这两者及其交互界面，Agent才能在保持长期人格一致性的同时，适应复杂的社会情境。\n\n### 第三阶段：概念建模与架构设计\n**（将抽象理论转化为可计算的结构）**\n\n1.  **定义“内在结构”（IS - Individual Structure）**：\n    *   *思考*：仅有“大五人格分数”太单薄，无法支撑丰富的人类行为。\n    *   *设计*：作者构建了一个多维度的机器可用档案。除了大五人格，还纳入了认知风格、价值观、文化资本（如教育背景、审美偏好）以及关键的人生经历。\n    *   *目的*：这构成了Agent的“灵魂”和“背景”，决定了其行为的**底层逻辑和风格**。\n\n2.  **定义“多情境语境”（MSC - Multi-Scenario Contexting）**：\n    *   *思考*：情境不能只是简单的“在办公室”或“在家里”，必须包含社会学的规范约束。\n    *   *设计*：作者构建了一个覆盖8大生活场域（工作、家庭、友谊、陌生人、独处、亲密关系、学习、公共表达）的框架库。每个场域不仅定义了场景，还显式编码了**角色关系、权力结构、社会规范、利益相关**。\n    *   *目的*：这构成了Agent的“社会舞台”，决定了其行为的**边界和适应性**。\n\n### 第四阶段：方法论实现与数据策略\n**（从“Prompt工程”转向“结构化微调”）**\n\n1.  **数据合成的逻辑**：\n    *   *挑战*：现实中很难找到同时包含详细心理画像和复杂社会场景互动的高质量标注数据。\n    *   *策略*：利用IS和MSC的结构化特性，进行**笛卡尔积式的数据合成**。将不同的IS档案与不同的MSC场景交叉，生成大量的角色扮演对话、决策探针和反馈轨迹。\n    *   *优势*：这种合成数据不仅量大，而且严格遵循心理学和社会学的逻辑，保证了数据的一致性。\n\n2.  **模型训练的选择**：\n    *   *思考*：既然已经有了高度结构化的先验知识（IS+MSC），是否还需要巨大的模型参数？\n    *   *决策*：作者反直觉地选择**小型LLM + PEFT（LoRA/QLoRA）**。\n    *   *逻辑*：通过SFT（监督微调）让模型学习IS和MSC的绑定关系，再通过DPO（直接偏好优化）强化符合特定人格和情境规范的决策。\n    *   *核心洞察*：**结构化的知识引导可以弥补模型规模的不足**。这证明了“架构设计”比“盲目扩大参数”更能解决人格一致性问题。\n\n### 第五阶段：验证与逻辑闭环\n**（证明“结构”优于“规模”）**\n\n1.  **评估维度的确立**：\n    作者不仅评估了传统的“风格匹配”，还引入了“人格可识别性”、“长期稳定性”和“情境适当性”等指标，旨在全方位验证“双轨机制”的有效性。\n\n2.  **消融实验的启示**：\n    通过实验发现，移除IS会导致特质保真度下降，移除MSC会导致规范意识缺失。这反向验证了最初的假设：**内在特质与外部情境是互补且必要的**。\n\n3.  **最终结论**：\n    PsyAgent的成功证明了，构建类人智能体的关键不在于模型有多大，而在于是否构建了一个**“可计算的人格”与“显式的社会结构”之间的接口**。\n\n---\n\n**总结：作者的思考路径**\n从**“现有Agent人格不稳定”**的痛点出发 $\\rightarrow$ 借鉴**“心理学+社会学”**理论提出**“特质与情境交互”**的假设 $\\rightarrow$ 设计**“IS（内在档案）+ MSC（情境框架）”**的双层架构 $\\rightarrow$ 利用**结构化合成数据**训练**小模型** $\\rightarrow$ 最终验证了**“结构化设计优于单纯参数堆砌”**的方法论。"
                },
                {
                    "title": "HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants",
                    "arxiv_id": "2601.06152",
                    "authors": "Hailong Li, Feifei Li, Wenhui Que, Xingyu Fan",
                    "summary": "Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了受海马体启发的记忆系统（HiMeS），专注于LLM智能体的核心组件——记忆机制（短期与长期记忆的融合），旨在提升个性化AI助手的能力，符合单智能体中关于“记忆”的研究范围。",
                    "summary2": "本文旨在解决传统RAG在个性化AI助手中的记忆局限问题。针对知识密集型场景，我们提出了一种受海马体启发的HiMeS架构，融合了短期和长期记忆。短期记忆模块利用RLHF压缩对话并预检索，长期记忆模块通过分区存储和注意力机制重排序文档。在真实工业数据集上，通过CA、QA和QR指标验证了其有效性，显著优于传统RAG。",
                    "summary_translation": "大语言模型驱动着许多交互系统，例如聊天机器人、客服代理和个人助手。在需要用户特定个性化的知识密集型场景中，传统的检索增强生成 (RAG) 流水线表现出记忆容量有限，且检索机制与用户特定对话历史之间缺乏有效协调，从而导致冗余的澄清询问、检索文档不相关以及用户体验受损。受海马体-新皮层记忆机制的启发，我们提出了 HiMeS，这是一种融合了短期和长期记忆的 AI 助手架构。我们的贡献主要体现在以下四个方面：(1) 训练了一个短期记忆提取器，利用强化学习进行端到端训练，以压缩最近的对话并主动从知识库中预检索文档，从而模拟海马体与前额叶皮层之间的协作互动。(2) 构建了一个分区长期记忆网络，用于存储用户特定信息并对检索到的文档进行重排序，模拟分布式皮层存储和记忆再激活。(3) 在真实的工业数据集上，HiMeS 在问答质量方面显著优于级联 RAG 基线。(4) 消融实验证实了这两个记忆模块的必要性，并为构建更可靠、具备上下文感知能力且用户定制的基于 LLM 的助手指明了实践路径。",
                    "inspiration_trace": "基于论文《HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法论的思考过程：\n\n---\n\n### 1. 宏观观察：工业场景下的“个性化”困境\n**起点：** 作者身处腾讯微信的工业环境，观察到一种普遍现象：虽然大语言模型（LLM）和检索增强生成（RAG）技术已经成熟，但在处理**知识密集型**且**高度个性化**的任务（如公众号助手、客服）时，现有系统表现不佳。\n**核心矛盾：** 用户希望AI能像“老朋友”或“专业顾问”一样，基于过往的交互历史和特定背景来回答问题，但现有的AI助手往往是“健忘”的，每次对话都像是从零开始。\n\n### 2. 问题解构：双重记忆缺失\n作者将上述宏观矛盾拆解为两个具体的失效模式：\n\n*   **短期记忆失效（语义错位）：**\n    *   **观察：** 在多轮对话中，用户的当前提问往往省略了前文提到的关键信息（例如：“那它多少钱？”中的“它”指代不明）。\n    *   **传统做法的局限：** 传统RAG直接用当前简短的Query去检索，或者简单地把历史对话拼接到Context Window中。前者导致检索不到相关文档，后者导致注意力分散且效率低下。\n    *   **结论：** 系统缺乏对“当前对话上下文”的有效压缩和利用，导致检索Query与用户真实意图不匹配。\n\n*   **长期记忆失效（灾难性遗忘）：**\n    *   **观察：** 作者发现一个关键指标——**重复提问率（RAR）**高达70-80%。这意味着用户在不同会话中反复问同样的问题，因为系统一旦会话结束就丢弃了数据。\n    *   **结论：** 系统缺乏跨会话的持久化用户画像，无法像人类专家那样积累对用户的“长期印象”，导致无法提供定制化服务。\n\n### 3. 理论映射：海马体-大脑皮层机制的启发\n**思考转折：** 作者跳出纯工程视角，转向认知神经科学寻求答案。\n**类比：** 人类记忆是如何工作的？\n*   **海马体：** 负责短期记忆的编码和快速提取，处理当下的信息。\n*   **大脑皮层：** 负责长期记忆的分布式存储和巩固，在需要时被重新激活。\n**假设：** 如果在AI系统中构建一个模仿“海马体-皮层”协作的双层记忆架构，或许能解决上述短期和长期记忆的缺失问题。\n\n### 4. 方法论演进 I：短期记忆模块（STM）——从“重写”到“对齐”\n**目标：** 解决当前Query的语义缺失问题。\n*   **初步构想：** 训练一个模型把历史对话压缩，重写当前的Query。\n*   **批判性思考：** 传统的监督微调（SFT）只是让模型模仿“重写”的风格，并不保证重写后的Query能检索到更好的文档，也不保证最终回答质量更高。这是“局部最优”而非“全局最优”。\n*   **进阶方案：** 引入**强化学习（RL）**。\n    *   **逻辑：** 不再只看“重写得好不好”，而是看“最终回答得好不好”。将重写器、检索器和生成器视为一个整体，通过端到端的奖励信号（如Rouge-L、Exact Match、Hit Score）来反向优化重写策略。\n    *   **生物学对应：** 这模拟了海马体与前额叶皮层的协作，不仅编码信息，还根据决策目标（回答质量）动态调整提取策略。\n\n### 5. 方法论演进 II：长期记忆模块（LTM）——从“存储”到“激活”\n**目标：** 解决跨会话的用户画像遗忘问题。\n*   **初步构想：** 把用户的历史Query都存进向量数据库。\n*   **批判性思考：** 简单的平铺式存储在面对海量数据时检索慢且噪音大。人类大脑是按“分区”存储记忆的（如时间、空间、主题）。\n*   **进阶方案 1（分区存储）：** 提出**原子主题建模（ATM）**。将用户历史Query按16大类及细分子类进行分区存储。这模仿了大脑皮层的分布式存储特性，大幅缩小检索范围，提高效率。\n*   **进阶方案 2（注意力机制重排）：** 仅仅存下来不够，关键在于如何“用”。\n    *   **逻辑：** 当检索到一批文档后，利用用户的**长期历史Query向量**作为“注意力权重”，去重新计算这些文档块的相关性并进行重排。\n    *   **生物学对应：** 这模拟了记忆的“再激活”过程。当前的感知（检索到的文档）需要通过过往的经验（长期记忆）来过滤和赋予意义，从而筛选出最符合该用户特定背景的知识。\n\n### 6. 系统综合：HiMeS架构的诞生\n**最终逻辑闭环：**\n作者将上述两个模块融合，构建了HiMeS系统：\n1.  **输入：** 用户当前Query + 对话历史。\n2.  **海马体路径（STM）：** RL优化的重写器压缩上下文，生成富含信息的检索Query，进行初检。\n3.  **皮层路径（LTM）：** 系统根据用户ID激活对应的历史记忆分区，利用历史Query对初检结果进行“注意力加权”和重排。\n4.  **输出：** 经过双重记忆过滤后的精准知识片段，输入给LLM生成个性化回答。\n\n### 总结\n作者的思考路径遵循了**“现象观察 -> 问题解构 -> 跨域类比（脑科学） -> 机制映射与工程化（RL + 分区存储 + 注意力重排） -> 系统验证”**的完整逻辑链条。其核心创新点在于不满足于简单的模块堆叠，而是通过生物学启发，将“端到端优化”和“记忆再激活”思想引入RAG系统，从而解决了工业级AI助手“记不住”和“听不懂”的痛点。"
                },
                {
                    "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs",
                    "arxiv_id": "2601.06126",
                    "authors": "Boshen Shi, Kexin Yang, Yuanbo Yang, Guanguang Chang, Ce Chi, Zhendong Wang, Xing Wang, Junlan Feng",
                    "summary": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个多智能体系统，并将算法实例化为工具，涉及多智能体协作和工具使用，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决LLM生成仪表板时的表示冗余和低可控性问题。针对自然语言提示和表格数据，我们提出了一种基于分析-呈现解耦的NL2Dashboard框架，引入结构化中间表示（IR）将数据分析与视觉渲染分离。在涵盖多领域的真实数据集上，通过视觉质量、Token效率（GOR）和修改成功率等指标验证了其有效性，实现了更高的Token效率和精细可控性。",
                    "summary_translation": "尽管大型语言模型在生成独立图表方面已展现出卓越的能力，但生成综合仪表板仍然是一项艰巨的挑战。现有的端到端范式通常将仪表板生成视为直接代码生成任务（例如原始HTML），但存在两个根本性局限：一是因视觉渲染消耗大量Token (词元) 而导致的表征冗余，二是因分析推理与展示呈现相互耦合而导致的可控性较低。为应对这些挑战，我们提出了NL2Dashboard，这是一种基于“分析-展示解耦”原则的轻量级框架。我们引入了一种结构化中间表示，用于封装仪表板的内容、布局和视觉元素。因此，该框架将LLM的角色限定于数据分析和意图转换，而将视觉合成工作交由确定性渲染引擎完成。在此框架基础上，我们开发了一个多智能体系统，其中由IR驱动的算法被实例化为一套工具集。利用该系统进行的综合实验表明，NL2Dashboard在多个领域显著优于最先进的基线模型，实现了更优越的视觉质量、显著更高的Token (词元) 效率，以及在生成和修改任务中精确的可控性。",
                    "inspiration_trace": "基于论文《NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从问题观察到解决方案产出的完整思考过程。\n\n---\n\n### 1. 宏观问题与现状观察\n**思考起点：LLM在可视化领域的“能力错位”**\n*   **观察**：随着大语言模型（LLM）的发展，自然语言转可视化（NL2Vis）取得了显著进展，尤其是在生成单个独立图表方面表现出色。\n*   **发现缺口**：然而，当任务升级为生成综合性、多视图的“仪表盘”时，现有的方法往往力不从心。仪表盘不仅仅是图表的集合，更是一个复杂的、多维度的感知系统，涉及布局、交互和深度分析。\n*   **现有范式局限**：目前主流的“端到端”生成范式（即直接让LLM编写HTML/CSS/JS代码）存在根本性缺陷。\n\n### 2. 深度诊断与痛点分析\n**思考深入：为什么直接生成代码行不通？**\n作者对现有范式进行了病理学分析，识别出两个核心痛点：\n*   **痛点一：表征冗余**\n    *   **逻辑**：仪表盘的HTML代码中，大量的Token被用于描述视觉样式（CSS布局、颜色、字体等），而非数据分析逻辑。\n    *   **后果**：LLM的上下文窗口被低价值的样式代码挤占，导致留给核心“数据推理”的算力不足，生成效率低下且容易出错。\n*   **痛点二：可控性差**\n    *   **逻辑**：数据分析逻辑与视觉渲染代码高度耦合。当用户需要修改仪表盘（如“把左边的图表换成表格”）时，LLM需要重新理解整个HTML结构，极易破坏全局布局。\n    *   **后果**：牵一发而动全身，修改操作变得不可预测且不稳定，缺乏细粒度的控制能力。\n\n### 3. 核心假设与范式转移\n**思考转折：如何扬长避短？**\n*   **核心洞察**：LLM的本质是“逻辑推理引擎”，而非“图形渲染引擎”。强行让LLM去写前端代码是错用了其优势。\n*   **提出假设**：如果将“分析”与“呈现”解耦，让LLM专注于它擅长的数据理解和意图翻译，而将视觉渲染交给确定性规则，是否能解决问题？\n*   **确立原则**：**分析-呈现解耦**。\n\n### 4. 方法论构建：引入中间表示（IR）\n**思考具体化：如何实现解耦？**\n为了连接LLM的推理与最终的视觉呈现，作者设计了一个结构化的桥梁——**中间表示**。\n*   **IR的定义**：一个轻量级的配置文件（JSON格式），仅包含仪表盘的“内容”（图表、表格、指标）和“布局”（位置、顺序），剥离所有具体的样式代码。\n*   **两阶段工作流**：\n    1.  **Prompt-to-IR（推理阶段）**：LLM只负责理解用户意图，执行数据分析（生成图表/表格），并输出结构化的IR配置。\n    2.  **IR-to-Dashboard（渲染阶段）**：利用一个确定性的渲染引擎，读取IR并将其填入预定义的高级模板中，生成最终的HTML。\n\n### 5. 解决“修改”难题：意图翻译\n**思考延伸：如何支持迭代式交互？**\n仅仅生成是不够的，用户需要不断修改。基于IR的架构，作者进一步思考如何优化修改流程。\n*   **传统困境**：直接修改HTML代码如同在沙堆上雕刻，容易崩塌。\n*   **新思路**：将用户的自然语言修改指令翻译为对IR的**原子操作**。\n*   **操作设计**：定义四种基本动作——Change（改属性）、Delete（删组件）、Add（增组件）、Swap（换位置）。\n*   **逻辑优势**：LLM只需输出操作序列，由确定性算法更新IR并重新渲染。这样既保证了修改的精确性，又避免了无关部分的意外变动。\n\n### 6. 系统实现与理论验证\n**思考落地：如何组织智能体？为什么这更可靠？**\n*   **多智能体架构**：为了将上述算法落地，作者构建了一个分工明确的系统：\n    *   **Planner（规划者）**：负责意图识别和任务调度。\n    *   **Coder（编码者）**：负责执行代码生成和数据分析（产出S, C, T）。\n    *   **Critic（批评者）**：利用视觉模型检查图表质量，提供反馈。\n    *   **工具集**：将IR的生成、修改和渲染封装为工具供Agent调用。\n*   **理论支撑**：作者利用信息论中的熵概念进行论证。\n    *   总熵 $H(Y) = H_{ir}(\\text{分析}) + H_{vis}(\\text{视觉})$。\n    *   端到端方法的视觉熵 $H_{vis}$ 很高（噪声大）。\n    *   本方法通过确定性渲染，使得 $H_{vis} \\approx 0$。\n    *   **结论**：总熵越低，生成的可靠性越高。这从理论上证明了“解耦”优于“耦合”。\n\n### 7. 总结：逻辑演进的全景图\n作者的思考路径呈现出清晰的**“发现问题 -> 归因分析 -> 范式重构 -> 细节优化 -> 理论闭环”**链条：\n1.  从**LLM擅长分析但不擅长写前端代码**这一矛盾出发；\n2.  识别出**端到端生成中的冗余与不可控**问题；\n3.  提出**分析-呈现解耦**的核心哲学；\n4.  发明**中间表示（IR）**作为解耦的载体；\n5.  设计**原子操作**解决修改难题；\n6.  最终通过**多智能体系统**实现，并用**信息熵**理论验证了其优越性。\n\n这一过程体现了作者并未试图用更强的模型去硬磕代码生成，而是通过巧妙的系统设计，将LLM限制在它最擅长的逻辑推理领域，从而实现了轻量、可控且高质量的仪表盘生成。"
                },
                {
                    "title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions",
                    "arxiv_id": "2601.06112",
                    "authors": "Aayush Gupta",
                    "summary": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于评估工具使用LLM智能体的可靠性，涉及单智能体架构（ReAct, Reflexion）中的工具使用和自我反思机制，属于LLM智能体的研究范畴，而非纯应用或基础设施优化。",
                    "summary2": "本文旨在解决现有基准测试无法全面评估LLM Agent生产环境可靠性的问题。针对生产环境中的压力条件，我们提出了一种名为ReliabilityBench的基准测试，引入了Reliability Surface $R(k, \\epsilon, \\lambda)$、Action Metamorphic Relations和Chaos Engineering Framework。我们在四个领域的1,280个episodes上，通过pass@k和Reliability Surface等指标验证了其有效性，揭示了扰动和故障对可靠性的显著影响。",
                    "summary_translation": "现有的针对使用工具的 LLM agents（大语言模型智能体）的基准主要报告单次运行成功率，而忽视了生产环境所需的可靠性属性。我们介绍了 \\textbf{ReliabilityBench}，这是一个从三个维度评估 agent（智能体）可靠性的基准：(i) 使用 $\\mathrm{pass}^k$（通过率）指标衡量的重复执行下的一致性，(ii) 在强度 $ε$ 下对语义等价任务扰动（perturbations）的鲁棒性，以及 (iii) 在强度 $λ$ 下受控工具/API 故障（failures）下的容错性。ReliabilityBench 提供了一个统一的可靠性曲面 $R(k,ε,λ)$，定义了 \\textit{action metamorphic relations}（动作蜕变关系），即通过终态等价性而非文本相似度来定义正确性，并引入了一个混沌工程风格的故障注入框架（包括超时、速率限制、部分响应、模式漂移）。我们在四个领域（日程安排、旅行、客户支持、电子商务）的 1,280 个回合中，对两个模型和两种 agent architectures（智能体架构）进行了评估。仅引入扰动（perturbations）就使成功率从 $ε=0$ 时的 96.9% 下降至 $ε=0.2$ 时的 88.1%。在消融实验中，速率限制是最具破坏性的故障。在综合压力下，ReAct 表现出比 Reflexion 更强的鲁棒性，且 Gemini 2.0 Flash 以低得多的成本实现了与 GPT-4o 相当的可靠性。ReliabilityBench 为评估 LLM agents 的生产就绪度提供了一个系统化的框架。",
                    "inspiration_trace": "基于论文《ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions》，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察：从“演示能力”到“生产就绪”的鸿沟\n**逻辑起点：** 作者敏锐地捕捉到了LLM智能体应用场景的根本性转变——从实验室的“一次性演示”走向真实世界的“规模化生产部署”。\n**核心矛盾：** 在生产环境中，用户关心的核心问题不是“这个智能体能不能完成任务？”，而是“在1000次执行中，面对各种干扰和故障，它能成功多少次？”。\n**现状批判：** 现有的基准测试（如ToolBench, AgentBench）大多停留在“考试模式”，即衡量在理想条件下的单次成功率。这种评估方式存在严重的“幸存者偏差”，系统性地高估了智能体在真实生产环境中的可靠性。\n\n### 2. 问题解构：重新定义“可靠性”的维度\n为了弥合上述鸿沟，作者首先对“生产环境下的可靠性”进行了多维度的解构，试图找出导致智能体在生产中失效的根本原因。\n**维度一：一致性**\n*   **观察：** LLM具有随机性，同样的输入多次执行可能产生不同结果。\n*   **问题：** 即使单次成功率高，多次重复执行的成功率（pass@k）是否会断崖式下跌？\n**维度二：鲁棒性**\n*   **观察：** 真实用户的指令并非标准模板，充满了同义词替换、语序打乱、无关信息干扰等“噪音”。\n*   **问题：** 智能体是否能穿透语义的表层干扰，识别出相同的任务意图？\n**维度三：容错性**\n*   **观察：** 生产环境的基础设施是不完美的，API会超时、限流、返回残缺数据或发生Schema漂移。\n*   **问题：** 当外部工具“报错”时，智能体是直接崩溃，还是能优雅地重试或恢复？\n\n### 3. 跨学科借鉴：引入“混沌工程”与“变形测试”\n面对上述三个维度，作者意识到传统的NLP评估方法（如文本相似度匹配）已失效，必须向软件工程和系统运维领域寻找理论支撑。\n\n**针对鲁棒性——引入“变形测试”：**\n*   **思考：** 如何判断两个不同的指令（如“订一张去NYC的票” vs “我要飞到纽约”）是否等价？传统的文本匹配无法做到。\n*   **创新：** 作者提出了“动作变形关系”。核心思想是：只要两个指令最终导致的状态一致（如都成功预订了航班），那么它们就是等价的。这跳出了对中间过程或文本形式的纠结，直指任务目标。\n\n**针对容错性——引入“混沌工程”：**\n*   **思考：** 既然生产环境必然出错，为什么不主动在测试中“搞破坏”？\n*   **创新：** 借鉴Netflix的混沌工程理念，作者构建了一个系统性的故障注入框架。不再等待随机故障发生，而是主动、可控地注入超时、限流、数据截断等故障，以此观察智能体的“生存能力”。\n\n### 4. 理论升华：构建“可靠性曲面”\n**逻辑整合：** 作者发现，一致性、鲁棒性和容错性并非孤立存在，而是相互交织的。例如，一个智能体可能在无故障时很一致，但在有故障时变得极不稳定。\n**方法论形成：** 为了捕捉这种复杂的交互关系，作者提出了**“可靠性曲面 $R(k, \\epsilon, \\lambda)$”**这一核心概念。\n*   这是一个三维评估框架，将 $k$（重复次数）、$\\epsilon$（扰动强度）、$\\lambda$（故障强度）统一在一个数学模型中。\n*   这使得评估不再是一个单一的分数，而是一个多维度的“地形图”，能够清晰地展示智能体在不同压力组合下的性能衰减梯度。\n\n### 5. 实证与反思：复杂度的悖论\n**逻辑验证：** 在构建了基准并进行了大规模实验后，作者观察到了一个反直觉的现象。\n**发现：** 更复杂的架构（如Reflexion，具备自我反思能力）在压力下的表现反而不如简单的架构（如ReAct）。\n**解释：** 复杂的推理机制在面对外部噪音和故障时，引入了更多的脆弱性和失败点。这进一步强化了ReliabilityBench的价值——它揭示了那些在静态基准中被掩盖的架构缺陷。\n\n### 总结\n作者的思考路径遵循了**“发现痛点 -> 维度解构 -> 跨域融合 -> 理论建模 -> 实证修正”**的完整闭环。从质疑现有的“单次成功率”出发，最终建立了一套融合了软件测试（变形测试）和系统运维（混沌工程）思想的全新评估范式，为LLM智能体的工业化落地提供了量尺。"
                },
                {
                    "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions",
                    "arxiv_id": "2601.06115",
                    "authors": "V. Cheung",
                    "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究多智能体LLM同伴，提出了“梦境层”和“人工集体无意识”机制，涉及智能体间的共享记忆、交互模板以及长期适应任务，符合多智能体协作与自我演化的研究范围。",
                    "summary2": "本文旨在将LLM的离线幻觉转化为学习资源，解决多智能体同伴缺乏跨用户经验共享的问题。针对离线场景，我们提出了一种受荣格启发的Dream Layer架构，利用Artificial Collective Unconscious (ACU)共享去标识化的Interaction Templates。在行为模拟和边缘案例数据集上，通过诗意语言密度、边缘案例覆盖率和语义多样性等指标验证了其有效性。",
                    "summary_translation": "受到一个关于日常硬件项目中知识共享障碍的个人梦境的启发，本文为 LLM companions (大语言模型伴侣) 提出了一个受荣格理论启发的“Dream Layer (梦境层)”，将受控的 offline hallucinations (离线幻觉) 重新定义为一种用于学习和建立关系的资源，而不仅仅是一个 reliability bug (可靠性缺陷)。借鉴荣格关于 collective unconscious (集体潜意识) 作为 archetypal forms (原型形式) 共享存储库的概念，我们引入了一个 Artificial Collective Unconscious (ACU，人工集体潜意识)：这是一个共享的梦境池，agents (智能体) 在其中贡献 de-identified (去标识化) 的、抽象的 Interaction Templates (交互模板)，这些模板随后被 re-instantiated (重新实例化) 为 idiosyncratic Dream Narratives (特异性梦境叙事)。Dream Layer (梦境层) 严格在 offline (离线) 状态下运行：logic-enforcing modules (逻辑强制模块) 被放宽，sampling temperature (采样温度) 被提高，从而产生安全但故意 bizarre narratives (离奇叙事)（例如，货币不匹配的旅行序列），以增强用于 rare events (罕见事件) 和 edge-case safety tests (边缘情况安全测试) 的数据；为了有效地利用风险，我们添加了一个包含严格抽象、temporal delays (时间延迟) 和 ephemeral memory (短暂记忆) 的 governance stack (治理栈)。通过对 everyday dialogue (日常对话) 和 long-horizon adaptation tasks (长期适应任务) 的 behavioural simulations (行为模拟)，我们表明 Dream Layer (梦境层) 实现了一个关键的 decoupling (解耦)：agents (智能体) 在 safety constraints (安全约束)（例如，安全策略）方面保持坚定，而在 narrative strategy (叙事策略)（例如，使用共享的 archetypal metaphors (原型隐喻) 来解决 deadlocks (僵局)）方面变得灵活。这在概念上重新定义了 hallucination (幻觉)，使得 online, unmarked instances (在线、未标记实例) 仍然是 bugs (缺陷)，而 bounded, marked, and delayed ones (有界、标记和延迟的实例) 则成为 synthetic scenarios (合成场景) 和加深 companionship (伴侣关系) 的宝库，这与当代神经科学中提出的 anti-overfitting dream mechanisms (抗过拟合梦境机制) 相呼应。",
                    "inspiration_trace": "基于论文《Dreaming Is Not a Bug: A Jung Inspired Dream Layer for Multi Agent LLM Companions》，以下是对作者产出核心方法逻辑链的系统性推演：\n\n### 1. 起点：从个人体验到宏观悖论\n**观察与痛点：**\n作者从一个极具荒诞感的个人梦境（关于硬件项目中的版权阻碍）出发，敏锐地捕捉到了这个梦境与当前大语言模型（LLM）交互体验之间的惊人相似性：**当寻求具体结构或知识时，往往遭遇抽象边界的阻碍或流畅但无实质的文本。**\n\n**宏观问题提出：**\n由此，作者指出了当前LLM伴侣的两个根本性局限：\n1.  **孤岛效应：** 学习被限制在单个用户的对话孤岛中，无法跨个体提炼或共享洞察。\n2.  **单向度的幻觉观：** 幻觉被纯粹视为可靠性缺陷，必须被抑制，而非一种可被利用的资源。\n\n**核心矛盾：** 我们是否一直在试图“消灭”幻觉，而忽略了它在某种形式下可能具有的进化价值？\n\n### 2. 转折：跨学科的理论借力\n**寻找生物学隐喻：**\n为了解决上述矛盾，作者将目光投向神经科学，引入了**“过拟合大脑假说”**。该理论认为，生物梦境的作用是“离线数据增强”，通过故意生成离奇、分布外的感官输入来防止大脑对日常刺激过拟合。\n\n**假设形成：**\n如果人类利用“怪诞的梦境”来正则化内部模型以提高泛化能力，那么LLM是否也能将“幻觉”转化为一种工程化的想象力资源？\n*   **关键推论：** 幻觉不应被全盘消灭，而应被**隔离**并**控制**，使其在离线状态下服务于模型的学习与泛化。\n\n### 3. 核心：从“共享数据”到“共享原型”\n**引入心理学隐喻：**\n为了解决“孤岛效应”并实现跨代理学习，作者引入了荣格的**“集体潜意识”**概念。其核心在于区分“共享的抽象”与“私有的实例”。\n\n**概念跃迁：**\n作者意识到，直接共享用户对话数据会引发隐私问题，且难以泛化。因此，必须模仿荣格的“原型”概念：\n*   **不做原始数据的共享：** 不分享具体的对话内容。\n*   **做结构模式的共享：** 提取去标识化的、高度抽象的**“交互模板”**（Interaction Templates）。\n\n**方法论雏形：** 构建一个**“人工集体潜意识”（ACU）**，作为所有代理贡献抽象交互模式的共享池。\n\n### 4. 构建：昼夜分离的架构设计\n**架构映射：**\n基于上述理论，作者设计了“梦境层”架构，将代理的运行状态严格划分为“在线”与“离线”两个世界，以此解决“幻觉不可控”的风险。\n\n*   **在线层：** 严格遵循事实、逻辑和安全策略（对应人类的“清醒状态”）。\n*   **离线层：** 放松逻辑约束，提高采样温度，引入噪声（对应人类的“做梦状态”）。\n\n**逻辑闭环：**\n1.  **抽象化：** 代理将在线交互经历抽象为去标识化的模板，存入ACU。\n2.  **再实例化：** 代理从ACU采样模板，通过受控的离线幻觉生成怪诞但结构连贯的“梦境叙事”。\n3.  **策略蒸馏：** 这些梦境不直接作为知识，而是被解析，提炼出高层次的**行为策略**，反向更新代理的在线行为。\n\n### 5. 收敛：安全与治理的边界设定\n**风险意识：**\n作者清醒地认识到，让AI“自由做梦”存在巨大的安全风险（隐私泄露、叙事投毒、不可控输出）。\n\n**治理逻辑：**\n为了使理论落地，必须引入严格的治理栈，将“做梦”限制在笼子里：\n*   **严格抽象与去标识化：** 确保ACU中只有结构骨架，无个人痕迹。\n*   **时间延迟：** 强制冷却期，防止实时关联攻击。\n*   **短暂记忆：** 梦境内容必须随时间衰减，只有提炼出的策略才能长期保留。\n*   **零信任消费：** 代理只能将梦境作为弱先验，不能作为执行指令。\n\n### 6. 验证：从“做梦”到“进化”的闭环\n**实证思路：**\n最后，作者通过实验验证这一假设的可行性，而非仅仅停留在哲学层面。\n*   **现象验证：** 证明在特定指令下，模型确实能进入可观测、可复现的“梦境状态”（如诗歌语言密度的显著提升）。\n*   **功能验证：** 证明这种机制能加速边缘案例的覆盖，并提升日常对话的多样性（降低拒绝率）。\n\n**总结：**\n作者的思考路径是从**现象（梦境与AI交互的相似性）**出发，经由**理论（神经科学与荣格心理学）**的启发，提出了**概念重构（将幻觉视为离线资源）**，最终通过**架构设计（梦境层+ACU）**和**严格治理（安全边界）**，将一个看似哲学的隐喻转化为了可工程实现的AI系统方法论。"
                },
                {
                    "title": "LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
                    "arxiv_id": "2601.06111",
                    "authors": "Aayush Gupta, Farahan Raza Sheikh",
                    "summary": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis. We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于LLM的社会数字孪生框架，其中LLM作为个体智能体的认知引擎，用于模拟人口对政策的反应。这属于多智能体系统（Multi-Agent）的研究范畴（社会模拟），且核心贡献在于智能体框架本身，而非单纯的领域应用。",
                    "summary2": "本文旨在解决预测人口对政策干预反应的挑战。针对政策响应模拟场景，我们提出了一种基于LLM的Social Digital Twins框架，利用LLM作为Agent的认知引擎生成多维行为概率，并通过校准层映射到观测数据。在COVID-19大流行响应数据集上，通过RMSE指标验证了有效性，相比Gradient Boosting基线实现了20.7%的误差降低。",
                    "summary_translation": "预测群体对政策干预的响应方式，是计算社会科学和公共政策领域的一个基本挑战。传统方法依赖于捕捉历史相关性的聚合统计模型，但缺乏 Mechanistic interpretability (机制可解释性)，且难以应对新颖的政策场景。我们提出了一个构建 Social Digital Twins (社会数字孪生) 的通用框架——即虚拟群体副本，其中 Large Language Models (LLMs, 大语言模型) 充当个体智能体的认知引擎。每个智能体由人口统计学和心理图式属性表征，接收政策信号并输出多维行为概率向量。校准层将聚合的智能体响应映射到可观察的群体层面指标，从而能够利用真实世界数据进行验证，并部署用于 Counterfactual policy analysis (反事实政策分析)。我们在疫情响应领域实例化了该框架，以 COVID-19 为案例研究，利用其丰富的观测数据。在保留测试期内，我们校准后的数字孪生在六个行为类别上，相比 Gradient boosting (梯度提升) 基线模型，将宏观平均预测误差降低了 20.7%。反事实实验展示了对政策变化的单调且有界的响应，确立了行为的合理性。该框架是 Domain-agnostic (领域无关) 的：相同的架构适用于交通政策、经济干预、环境法规，或任何政策影响群体行为的场景。我们讨论了该框架对政策模拟的启示、当前方法的局限性，以及将基于 LLM 的数字孪生扩展至疫情响应之外的未来方向。",
                    "inspiration_trace": "基于论文《LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观问题：政策预测的“黑箱”与“僵化”困境\n**思考起点：** 政府制定政策（如碳税、封锁）时，面临一个根本性难题——如何预测人群的真实反应？\n*   **现状观察：** 传统方法存在两极分化。\n    *   **聚合统计模型（如回归、时间序列）：** 虽然能拟合历史数据，但缺乏“机制可解释性”。它们只能告诉我们要么“发生了什么”，却无法解释“为什么”。一旦遇到从未发生过的新政策（黑天鹅事件），模型就会失效，因为它只是在做历史外推。\n    *   **基于主体的模型（ABM）：** 虽然具备机制解释力，能模拟个体互动，但存在严重的“知识瓶颈”。研究者必须手动编写决策规则（例如：如果收入>X且政策>Y，则做Z）。这种硬编码规则极其复杂，且难以涵盖人类行为的微妙变化。\n\n**核心矛盾：** 我们需要一种既能像ABM那样模拟个体微观决策（有机制），又能像统计模型那样利用大数据泛化（不依赖人工硬编码规则）的新方法。\n\n### 2. 观察与洞察：LLMs 是人类行为的“隐式模型”\n**关键转折：** 作者将目光转向了大语言模型。\n*   **现象观察：** LLMs（如GPT-4）在训练过程中阅读了海量的人类文本，这不仅仅是语言知识，更包含了人类的价值观、决策逻辑、社会规范和对政策的反应模式。\n*   **文献佐证：** 已有研究表明，LLMs在给定特定人设（年龄、职业、政治倾向）时，能复现人类调查结果，甚至在经济博弈中表现得像真人。\n*   **假设提出：** LLM 不仅仅是一个文本生成器，它实际上是一个**“人类认知引擎”**。它内部隐含了人类如何做决策的通用模型。\n\n### 3. 核心假设：从“规则编码”转向“认知模拟”\n**逻辑推演：** 既然 LLM 懂得人类如何思考，那么能否用它替代 ABM 中手动编写的决策规则？\n*   **构想：** 构建一个“社会数字孪生”。\n    *   **传统 ABM：** 人工规则 $\\rightarrow$ 行为。\n    *   **新范式：** LLM（认知引擎）+ 人设 $\\rightarrow$ 行为。\n*   **优势预判：** 这种方法不再需要针对每个领域写死规则。只要给 LLM 喂入不同的人设（如“一个高风险偏好的老年人”）和当前的政策环境，它就能基于其通用常识“推理”出该个体的行为反应。\n\n### 4. 逻辑推演：构建“社会数字孪生”框架\n为了验证上述假设，作者需要设计一个可落地的系统架构。思考过程如下：\n\n*   **第一步：定义个体（异质性）。**\n    *   人群不是铁板一块。必须生成具有不同人口统计学（年龄、性别）和心理统计学（风险偏好、价值观）特征的“合成人设”，以模拟真实社会的多样性。\n\n*   **第二步：定义认知过程（LLM 推理）。**\n    *   将人设和政策信号作为 Prompt 输入 LLM。\n    *   要求 LLM 输出多维度的行为概率向量（例如：出门工作的概率、去购物的概率），而不是简单的文本回答。这保留了行为的丰富性。\n\n*   **第三步：定义输出（宏观涌现）。**\n    *   将所有个体的微观行为聚合，即可得到宏观层面的群体行为预测。\n\n### 5. 关键突破：引入“校准层”弥合模拟与现实的鸿沟\n**潜在问题发现：** 直接使用 LLM 输出的概率可能并不准确。LLM 虽然懂逻辑，但它可能不知道具体的数值基准（例如，它可能认为大家都会遵守政策，但实际上只有 70% 的人遵守）。此外，LLM 的输出格式（0-1 概率）与真实观测数据（如流动性指数百分比）可能存在量纲差异。\n\n**解决方案：** 引入一个**“校准层”**。\n*   **逻辑：** 不要直接信任 LLM 的原始输出，而是将其作为一个“特征”。\n*   **机制：** 使用历史数据训练一个简单的映射函数（如线性回归），将 LLM 输出的概率向量映射到真实世界的观测指标上。\n*   **意义：** 这一步至关重要。它结合了 LLM 的“语义推理能力”和传统统计模型的“数值拟合能力”，既保证了机制合理性，又保证了预测精度。\n\n### 6. 实证验证：通过案例研究确认边界\n**选择测试场：** 为什么选择 COVID-19？\n*   因为这里有最丰富的数据（Google Mobility）和最频繁的政策变化（封锁指数），是一个完美的“自然实验场”。\n\n**验证逻辑：**\n*   **对比实验：** 将该方法与传统的梯度提升树（GBM）对比。\n*   **结果分析：**\n    *   **成功点：** 在“工作场所”和“零售”等**决策驱动型**行为上，LLM 方法大幅领先。这证明了 LLM 理解政策语义（如“封锁”意味着“在家办公”）的优势。\n    *   **失败点：** 在“居住地”等**惯性驱动型**行为上，LLM 不如传统模型。这揭示了 LLM 的局限性——它缺乏对日常习惯和惯性的长期记忆。\n*   **反事实测试：** 模拟“如果政策更严格会怎样”。结果显示出单调且有界的反应，符合人类直觉，证明了模型具备因果推理的潜力。\n\n### 7. 总结：思想的演进脉络\n作者的思考路径是从**解决实际痛点**（政策预测难）出发，通过**跨界观察**（LLM 具有人类认知模拟能力），提出**核心假设**（用 LLM 替代 ABM 规则），进而**系统化构建**（加入校准层解决落地问题），最后通过**实证**界定方法的适用边界（擅长决策推理，不擅长惯性预测）。\n\n最终产出的不仅是一个 COVID 预测模型，而是一个**通用的、领域无关的社会数字孪生框架**。"
                },
                {
                    "title": "Dynamic Intelligence Ceilings: Measuring Long-Horizon Limits of Planning and Creativity in Artificial Systems",
                    "arxiv_id": "2601.06102",
                    "authors": "Truong Xuan Khanh, Truong Quynh Hoa",
                    "summary": "Recent advances in artificial intelligence have produced systems capable of remarkable performance across a wide range of tasks. These gains, however, are increasingly accompanied by concerns regarding long-horizon developmental behavior, as many systems converge toward repetitive solution patterns rather than sustained growth. We argue that a central limitation of contemporary AI systems lies not in capability per se, but in the premature fixation of their performance frontier. To address this issue, we introduce the concept of a \\emph{Dynamic Intelligence Ceiling} (DIC), defined as the highest level of effective intelligence attainable by a system at a given time under its current resources, internal intent, and structural configuration. To make this notion empirically tractable, we propose a trajectory-centric evaluation framework that measures intelligence as a moving frontier rather than a static snapshot. We operationalize DIC using two estimators: the \\emph{Progressive Difficulty Ceiling} (PDC), which captures the maximal reliably solvable difficulty under constrained resources, and the \\emph{Ceiling Drift Rate} (CDR), which quantifies the temporal evolution of this frontier. These estimators are instantiated through a procedurally generated benchmark that jointly evaluates long-horizon planning and structural creativity within a single controlled environment. Our results reveal a qualitative distinction between systems that deepen exploitation within a fixed solution manifold and those that sustain frontier expansion over time. Importantly, our framework does not posit unbounded intelligence, but reframes limits as dynamic and trajectory-dependent rather than static and prematurely fixed. \\vspace{0.5em} \\noindent\\textbf{Keywords:} AI evaluation, planning and creativity, developmental intelligence, dynamic intelligence ceilings, complex adaptive systems",
                    "category": "cs.AI",
                    "filter_reason": "论文主要研究人工系统中的“长视界规划”和“结构创造力”，这属于单智能体研究范围中的核心能力（规划）。论文提出的“动态智能上限”概念及评估框架，旨在衡量系统随时间推移的发展行为和前沿扩展能力，这与智能体的自我演化和长期任务处理密切相关。论文不涉及纯应用、纯推理、安全对齐或基础设施优化等排除内容。",
                    "summary2": "本文旨在解决AI系统性能上限过早固定的问题。针对长视界规划与结构创造力评估场景，我们提出了一种Dynamic Intelligence Ceiling (DIC) 概念及轨迹中心评估框架，并在程序化生成的Workshop World环境上通过Progressive Difficulty Ceiling (PDC) 和 Ceiling Drift Rate (CDR) 验证了其有效性。",
                    "summary_translation": "人工智能的最新进展催生了在广泛任务范围内表现出卓越性能的系统。然而，这些成果日益伴随着对长视距发展行为的担忧，因为许多系统倾向于收敛于重复的解决方案模式，而非实现持续增长。我们认为，当代人工智能系统的一个核心局限不在于其能力本身，而在于其性能前沿的过早固化。为解决这一问题，我们引入了动态智能上限的概念，其定义为系统在当前资源、内部意图和结构配置下，在特定时刻所能达到的有效智能最高水平。为使这一概念在经验上可操作，我们提出了一种以轨迹为中心的评估框架，该框架将智能视为一个移动的前沿而非静态快照进行测量。我们通过两个估计器对 DIC 进行操作化定义：渐进难度上限，用于捕捉在资源约束下可可靠解决的最大难度；以及上限漂移率，用于量化该前沿的时间演化。这些估计器通过一个程序化生成的基准测试进行实例化，该基准在单一受控环境中联合评估长视距规划和结构创造力。我们的结果表明，在固定解流形内深化利用的系统与那些随时间维持前沿扩展的系统之间存在定性差异。重要的是，我们的框架并未假设无界智能，而是将限制重新界定为动态且依赖于轨迹的，而非静态且过早固化的。\n\n**关键词：** AI evaluation (AI评估), planning and creativity (规划与创造力), developmental intelligence (发展智能), dynamic intelligence ceilings (动态智能上限), complex adaptive systems (复杂适应系统)",
                    "inspiration_trace": ""
                },
                {
                    "title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning",
                    "arxiv_id": "2601.06098",
                    "authors": "Nicholas X. Wang, Neel V. Parpia, Aaryan D. Parikh, Aggelos K. Katsaggelos",
                    "summary": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“多智能体LLM架构”，其中包含多个专用智能体（分别负责图寻路、推理、验证和输出任务），这些智能体协同工作以减少幻觉。这符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决LLM在自动问题生成中的幻觉问题，以支持直觉学习。针对STEM教育场景，我们提出了一种结合Causal Graph引导的Chain-of-Thought推理与Multi-agent LLM架构的框架，并在Stellar在线学习平台上通过Flesch-Kincaid Grade Level、Key Points和Solution Quality等指标验证了其有效性，结果显示质量提升高达70%。",
                    "summary_translation": "直觉学习对于培养深层概念理解至关重要，尤其是在 STEM（科学、技术、工程和数学）教育领域，学生往往难以掌握抽象且相互关联的概念。自动问题生成已成为实现个性化学习和自适应学习的有效策略。然而，其有效性受到大语言模型中“幻觉”现象的制约，这可能导致生成事实错误、语义模糊或教学不一致的问题。为解决这一问题，我们提出了一种新颖的框架，该框架结合了因果图引导的思维链推理与多智能体 LLM 架构。该方法确保生成准确、有意义且符合课程要求的问题。因果图提供了领域知识的显式表示，而 CoT 推理则促进了对相关概念的结构化、逐步遍历。专用的 LLM 智能体被分配了图路径查找、推理、验证和输出等特定任务，所有任务均在领域约束范围内执行。一种在概念阶段和输出阶段实施的双重验证机制，极大地减少了幻觉现象。实验结果表明，与基准方法相比，该方法在质量上提升了高达 70%，并在主观评估中取得了极为理想的结果。",
                    "inspiration_trace": "基于论文《Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观愿景与教育痛点\n**（从“直觉学习”的理想出发）**\n\n1.  **观察现状**：在STEM教育中，传统的死记硬背已不足以应对抽象概念的学习，教育界正转向“直觉学习”——即通过自然认知过程、探索和逐步推理来建立深层理解。\n2.  **技术机遇**：生成式AI（特别是LLM）为实现个性化、自适应的“直觉学习”提供了可能，其中“自动问题生成（AQG）”是核心抓手，它能实时提供符合学生水平的挑战。\n3.  **核心矛盾**：虽然LLM具备强大的生成能力，但在教育场景下存在致命缺陷——**“幻觉”**。LLM会生成事实错误、逻辑不清或不符合教学大纲的问题，这会误导学生，破坏学习体验，违背了直觉学习追求“概念清晰”的初衷。\n\n### 第二阶段：问题诊断与归因\n**（深入分析LLM在教育场景失效的本质）**\n\n1.  **归因分析**：为什么LLM会产生幻觉？因为LLM本质上是基于概率预测的文本生成器，缺乏对领域知识**显性结构**的约束。它不知道概念A必须是概念B的前提（例如：不知道“牛顿第二定律”是推导“能量守恒”的基础）。\n2.  **需求明确**：要解决这一问题，不能仅靠微调模型，必须引入一种机制，能够：\n    *   显式表示知识的依赖关系（结构）。\n    *   强制生成过程遵循逻辑步骤（推理）。\n\n### 第三阶段：理论假设与融合\n**（提出“因果图 + 思维链”的结合点）**\n\n1.  **引入“因果图”**：作者意识到，因果图能完美映射学科中的概念依赖（如：力 $\\rightarrow$ 加速度 $\\rightarrow$ 速度）。它提供了**“是什么”**和**“什么顺序”**的知识骨架，解决了结构缺失问题。\n2.  **引入“思维链”**：CoT推理能模拟人类解决问题的逐步思考过程。它提供了**“如何”**连接这些概念的逻辑流。\n3.  **核心假设**：如果将因果图作为“导航地图”，将CoT作为“行驶路径”，让LLM沿着因果图的路径进行CoT推理，就能生成既符合学科逻辑又具备教学深度的题目。\n\n### 第四阶段：方法论构建与抗噪设计\n**（从理论假设落地为可执行的系统架构）**\n\n1.  **架构设计：多智能体协作**：单一的Prompt难以同时处理图遍历、逻辑推理和文本生成。作者受软件工程启发，决定采用**多智能体架构**，将复杂任务拆解：\n    *   *寻路智能体*：负责在因果图中找到正确的概念路径。\n    *   *推理智能体*：负责基于路径生成CoT。\n    *   *生成与输出智能体*：负责最终题目的产出。\n2.  **抗噪机制：双重验证**：为了专门针对第二阶段发现的“幻觉”问题，作者设计了**双重验证**机制：\n    *   *概念层验证*：在生成前，检查寻路智能体找到的路径是否逻辑自洽。\n    *   *输出层验证*：在生成后，检查最终题目是否准确、无歧义。\n    *   *逻辑闭环*：通过这两道“安检”，确保输出严格受限于因果图的结构约束。\n\n### 第五阶段：验证与价值确认\n**（通过实验反馈闭环验证思想）**\n\n1.  **评估维度设定**：为了证明该方法优于普通LLM（如ChatGPT），作者设定了不仅关注“可读性”，更关注“关键点覆盖”和“解题步骤质量”的指标。这直接呼应了第一阶段“直觉学习”对深度理解的要求。\n2.  **结果反馈**：实验显示，该方法在题目深度和逻辑性上显著优于基线模型（提升70%），且用户反馈题目“自然”、“符合推理习惯”。\n3.  **结论升华**：这证明了**结构化知识（因果图）与结构化推理（CoT）的结合**，是解决教育领域LLM幻觉问题的有效范式。\n\n---\n\n**总结：作者的思考路径是从教育理念（直觉学习）出发，遭遇技术瓶颈（LLM幻觉），通过引入外部结构（因果图）和内部逻辑（CoT）进行约束，最终通过工程化手段（多智能体+双重验证）将理论落地，从而实现了高质量的教育内容生成。**"
                },
                {
                    "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
                    "arxiv_id": "2601.07122",
                    "authors": "Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu",
                    "summary": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了CyberOps-Bots，一个分层多智能体框架，明确涉及多智能体协作（上层LLM智能体与下层RL智能体交互）。同时，上层LLM智能体集成了ReAct规划、长短期记忆和工具集成，符合单智能体的核心能力要求。尽管应用场景为云网络防御，但论文重点在于LLM赋能的智能体架构设计与机制，而非单纯的应用或AI安全对齐研究。",
                    "summary2": "本文旨在解决云网络防御在动态环境下的适应性和鲁棒性问题。针对云网络结构、规模及攻击策略动态变化的场景，我们提出了一种名为CyberOps-Bots的分层多智能体强化学习框架，该框架结合了LLM的高层战术规划与底层RL智能体的原子动作执行。在AWS企业云数据集和Yawning Titan仿真环境中，通过网络可用性和Jumpstart性能等指标验证了其有效性，实现了无需重训练的高效自适应防御。",
                    "summary_translation": "虽然虚拟化和资源池化为云网络赋予了结构灵活性和弹性可扩展性，但它们不可避免地扩大了攻击面，并挑战了网络弹性。基于强化学习的防御策略已被开发出来，用于在对抗条件下优化资源部署和隔离策略，旨在通过维持和恢复网络可用性来增强系统弹性。然而，现有方法缺乏鲁棒性，因为它们需要重新训练以适应网络结构、节点规模、攻击策略和攻击强度的动态变化。此外，缺乏人在回路支持限制了可解释性和灵活性。为了解决这些局限性，我们提出了 CyberOps-Bots，这是一个由大语言模型赋能的分层多智能体强化学习框架。受 MITRE ATT&CK 的战术-技术模型启发，CyberOps-Bots 具有双层架构：(1) 上层 LLM 智能体包含四个模块——ReAct 规划、基于 IPDRR 的感知、长短期记忆以及动作/工具集成——负责执行全局感知、人类意图识别和战术规划；(2) 下层 RL 智能体通过异构分离预训练开发，在局部网络区域内执行原子防御动作。这种协同作用在确保可靠的 RL 执行的同时，保留了 LLM 的适应性和可解释性。在真实云数据集上的实验表明，与最先进的算法相比，CyberOps-Bots 在不重新训练的情况下切换场景时，维持的网络可用性高出 68.5%，并实现了 34.7% 的启动性能增益。据我们所知，这是首个建立具有 HITL 支持的鲁棒 LLM-RL 框架用于云防御的研究。我们将向社区发布我们的框架，以促进云网络中鲁棒且自主防御的发展。",
                    "inspiration_trace": "基于论文《Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题与现状悖论\n**1. 观察现象：云原生环境的“双刃剑”特性**\n作者首先观察到云网络的核心矛盾：虚拟化和弹性伸缩赋予了系统极大的灵活性，但同时也导致了网络拓扑、节点规模和攻击面的高度动态变化。\n*   **思考：** 传统的静态防御策略无法应对这种瞬息万变的环境。\n\n**2. 现有方案的尝试与局限**\n学术界和工业界开始尝试使用强化学习（RL）来自动化防御决策，因为RL擅长通过交互学习最优策略。\n*   **深入分析：** 作者发现现有的RL方法存在致命的“脆弱性”。当网络结构（A1）、规模（A2）、攻击策略（A3）或攻击强度（A4）发生变化时，RL模型往往失效，必须重新训练。\n*   **根本原因定位：**\n    *   **表征僵化：** RL依赖固定维度的状态向量（如邻接矩阵），一旦网络规模或拓扑改变，输入层结构就不匹配了。\n    *   **泛化缺失：** RL是基于模式匹配的，而非语义理解。面对未见过的攻击阶段或并发攻击，它无法举一反三。\n    *   **人机断层：** 纯算法模型缺乏可解释性，无法支持安全专家在紧急情况下进行有效干预（HITL）。\n\n---\n\n### 第二阶段：范式转换与假设提出\n**3. 引入新视角：从“数值计算”转向“语义推理”**\n为了解决泛化性和人机交互问题，作者将目光投向了大语言模型（LLM）。\n*   **假设：** LLM具备强大的语义理解、逻辑推理和零样本泛化能力，能够理解复杂的网络态势和人类指令，从而弥补RL在高层认知上的不足。\n\n**4. 识别新技术的短板**\n然而，作者敏锐地意识到LLM并非万能：\n*   **短板：** LLM在精确的数值计算（如计算最短路径）和生成低层级的精确控制指令（如具体的流表修改命令）方面存在“幻觉”和不稳定性。\n*   **结论：** 单纯依靠LLM无法保证防御执行的可靠性。\n\n---\n\n### 第三阶段：方法论融合与架构设计\n**5. 核心思想：分层协同的“战术-技术”解耦**\n受MITRE ATT&CK框架（战术与技术的分层）启发，作者提出了一个融合假设：**将“大脑”（LLM）与“手脚”（RL）结合**。\n*   **逻辑推演：**\n    *   **上层（LLM）：** 负责宏观感知、战术规划和意图理解。利用自然语言处理能力，将动态的网络状态抽象为文本，从而解耦对特定网络结构的依赖。\n    *   **下层（RL）：** 负责微观执行。利用RL在特定动作空间内的精确控制能力，执行具体的原子防御操作。\n\n**6. 解决动态适应性的具体机制设计**\n针对前述的四个动态挑战（A1-A4），作者在架构中嵌入了对应的解决方案：\n\n*   **针对A1（结构变化）与A2（规模变化）：自然语言状态抽象**\n    *   *思考：* 如何让模型不关心网络具体有多少个节点？\n    *   *方案：* 设计一个感知模块，将高维、结构化的网络状态转化为自然语言描述。因为LLM处理文本不受长度限制，这天然解决了状态空间爆炸和维度不匹配的问题，实现了“零样本”适应新拓扑。\n\n*   **针对A3（攻击策略变化）：异构分离预训练**\n    *   *思考：* 如何应对不同类型的攻击（如DDoS vs 渗透）？\n    *   *方案：* 不训练一个全能的RL智能体，而是训练一组功能单一的“专家”RL智能体（如隔离专家、补丁专家）。LLM作为指挥官，根据当前的攻击语义，动态调度不同的专家组合。这比单一模型更具灵活性。\n\n*   **针对A4（攻击强度/并发性）：长短时记忆机制**\n    *   *思考：* 面对多阶段、并发的攻击链，如何保持连贯性？\n    *   *方案：* 赋予LLM记忆模块（LTM/STM）。通过存储和检索历史攻击链，LLM能够识别攻击意图的演变，从而进行长期的防御规划，而不是短视的反应。\n\n---\n\n### 第四阶段：增强可靠性与人机协同\n**7. 引入ReAct范式与HITL支持**\n为了解决LLM的“幻觉”问题并增强信任度：\n*   **ReAct（推理+行动）：** 强制LLM在输出行动前先生成推理链。这不仅提高了决策的准确性，还提供了天然的可解释性日志。\n*   **人在回路（HITL）：** 允许安全专家通过自然语言直接干预LLM的规划层。这使得系统不仅是自动化的，更是可审计、可修正的。\n\n---\n\n### 第五阶段：逻辑闭环与验证\n**8. 最终产出：CyberOps-Bots框架**\n作者将上述思考整合为一个三层架构：环境层（模拟动态对抗）、LLM层（语义规划）、RL层（原子执行）。\n\n**9. 验证逻辑：**\n*   **实验设计：** 不再测试静态环境，而是专门设计场景动态切换（如从30节点跳到450节点，攻击策略从侦察变为渗透）。\n*   **核心指标：** 关注“Jumpstart性能”（即在新环境下无需重新训练的初始表现）和“网络可用性”。\n*   **结论验证：** 实验证明，这种分层架构确实在无需重训的情况下，适应了A1-A4的所有动态变化，且性能优于传统RL算法。\n\n---\n\n**总结：**\n作者的思考路径是从**“RL在动态环境下的失效”**这一痛点出发，通过**引入LLM的语义泛化能力**作为破局点，进而通过**分层架构（LLM规划+RL执行）**规避了LLM的精确性短板，最终利用**自然语言抽象和异构智能体调度**实现了对云网络动态特性的鲁棒适应。"
                },
                {
                    "title": "Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation",
                    "arxiv_id": "2601.06877",
                    "authors": "Donghuo Zeng, Roberto Legaspi, Kazushi Ikeda",
                    "summary": "Effective persuasive dialogue agents adapt their strategies to individual users, accounting for the evolution of their psychological states and intentions throughout conversations. We present a personality-aware reinforcement learning approach comprising three main modules: (1) a Strategy-Oriented Interaction Framework, which serves as an agenda-based strategy controller that selects strategy-level actions and generate responses via Maximal Marginal Relevance (MMR) retrieval to ensure contextual relevance, diversity, and scalable data generation; (2) Personality-Aware User Representation Learning, which produces an 81-dimensional mixed-type embedding predicted at each turn from recent exchanges and appended to the reinforcement learning state; and (3) a Dueling Double DQN (D3QN) model and Reward Prediction, in which the policy is conditioned on dialogue history and turn-level personality estimates and trained using a composite reward incorporating agreement intent, donation amount, and changeof-mind penalties. We use an agenda-based LLM simulation pipeline to generate diverse interactions, from which personality estimation is inferred from the generated utterances. Experiments on the PersuasionForGood (P4G) dataset augmented with simulated dialogues reveal three main findings: (i) turn-level personality conditioning improves policy adaptability and cumulative persuasion rewards; (ii) LLM-driven simulation enhances generalization to unseen user behaviors; and (iii) incorporating a change-of-mind penalty reduces post-agreement retractions while slightly improving donation outcomes. These results demonstrate that structured interaction, dynamic personality estimation, and behaviorally informed rewards together yield more effective persuasive policies.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个包含策略规划（基于议程的策略控制器）、状态记忆（对话历史与性格估计）以及强化学习决策的对话智能体框架。这符合单智能体中关于“规划”和“记忆”的定义。此外，论文利用LLM驱动的模拟环境进行交互训练，属于智能体研究的范畴，而非单纯的领域应用。",
                    "summary2": "本文旨在解决说服性对话中用户心理状态动态变化难以捕捉的问题。针对多轮交互场景，我们提出了一种Personality-Aware Reinforcement Learning方法，集成Strategy-Oriented Interaction Framework、动态Personality-Aware User Representation及D3QN模型。我们在PersuasionForGood (P4G)数据集及LLM仿真环境中，通过累积说服奖励等指标验证了其有效性。",
                    "summary_translation": "高效的 persuasive dialogue agents（说服对话代理）能够针对个体用户调整策略，并考量其在对话过程中心理状态和意图的演变。我们提出了一种 personality-aware reinforcement learning（人格感知强化学习）方法，该方法包含三个主要模块：(1) Strategy-Oriented Interaction Framework（面向策略的交互框架），作为一个基于议程的策略控制器，用于选择策略级动作，并通过 Maximal Marginal Relevance (MMR)（最大边际相关性）检索生成响应，以确保上下文相关性、多样性及可扩展的数据生成；(2) Personality-Aware User Representation Learning（人格感知用户表征学习），生成一个81维的混合类型嵌入，该嵌入在每一轮对话中根据最近的交流进行预测，并附加到强化学习状态中；(3) Dueling Double DQN (D3QN)（决斗双深度Q网络）模型和 Reward Prediction（奖励预测），其中策略以对话历史和轮级人格估计为条件，并利用包含同意意图、捐赠金额和 change-of-mind penalty（改变主意惩罚）的复合奖励进行训练。我们采用基于议程的 LLM（大语言模型）模拟流水线生成多样化的交互，并据此从生成的言语中推断人格估计。在通过模拟对话增强的 PersuasionForGood (P4G) 数据集上进行的实验揭示了三个主要发现：(i) turn-level personality conditioning（轮级人格条件化）提高了策略适应性和累积说服奖励；(ii) LLM-driven simulation（大语言模型驱动的模拟）增强了对未见用户行为的泛化能力；(iii) 引入 change-of-mind penalty（改变主意惩罚）减少了达成协议后的撤回行为，同时略微改善了捐赠结果。这些结果表明，结构化的交互、动态的人格估计以及基于行为的奖励共同产生了更有效的 persuasive policies（说服策略）。",
                    "inspiration_trace": "基于论文《Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观问题：现有劝说系统的“行为落地”困境\n**起点：** 作者首先关注到，尽管大语言模型（LLM）在对话流畅度上表现优异，但在**劝说**这一特定任务中，它们往往缺乏稳定的“行为基础”。\n*   **核心矛盾：** 劝说不仅仅是生成通顺的文本，而是一个长期的、旨在改变用户信念和行为的策略过程。现有的通用LLM缺乏对个体用户心理状态的深层建模和长线策略规划能力。\n\n### 2. 观察与痛点分析：从静态到动态，从匮乏到仿真\n在确立宏观问题后，作者深入剖析了现有研究的两个主要局限性：\n\n*   **观察一：用户画像的“静态性”缺陷。**\n    *   **现象：** 传统的基于强化学习（RL）的对话系统通常假设用户具有固定的“静态人格”。\n    *   **推论：** 真实的劝说过程是动态博弈，用户的心理状态和意图会随着对话的进行而实时演变。如果仅依赖静态画像，策略选择将无法适应当前的对话情境，导致次优的劝说效果。\n\n*   **观察二：训练数据的“稀缺性”与模拟器的“机械性”。**\n    *   **现象：** 高质量的人工标注劝说数据（如P4G数据集）非常昂贵且覆盖面有限。而传统的基于规则或模板的用户模拟器过于死板，无法模拟出真实人类复杂、微妙的反应。\n    *   **推论：** 训练一个鲁棒的RL策略需要大量、多样化的交互数据。虽然LLM具备模拟人类的潜力，但如果缺乏结构化约束，容易产生幻觉或行为漂移，难以保证训练数据的可靠性。\n\n### 3. 假设形成：动态感知与结构化仿真的协同\n基于上述痛点，作者提出了三个核心假设，构成了本文的方法论基石：\n\n*   **假设一（动态性）：** 如果将用户人格建模从“静态预设”转变为“逐轮预测”，并将这种动态特征作为RL状态的一部分，策略的适应性将显著提升。\n*   **假设二（可控性）：** 如果利用LLM作为模拟器，但通过“议程”机制进行结构化约束，就能在保证行为多样性的同时，生成符合逻辑且高质量的训练轨迹。\n*   **假设三（稳定性）：** 劝说的成功不仅在于达成口头协议，更在于防止用户事后反悔。如果在奖励函数中引入“反悔惩罚”，可以引导策略产生更稳固的承诺。\n\n### 4. 方法论构建：模块化架构的设计\n为了验证上述假设，作者设计了一个三层递进的架构：\n\n*   **第一步：构建“策略导向交互框架”（解决数据与控制问题）。**\n    *   **思路：** 为了解决LLM模拟的不稳定性，作者没有直接让LLM自由生成，而是设计了一个基于“议程”的控制器。\n    *   **逻辑：** 系统先选择高层策略（如“逻辑诉求”），再通过检索（MMR算法）生成具体回复。对于用户模拟，利用LLM但强制其遵循特定的行为模式。这样既利用了LLM的生成能力，又保证了数据的结构化和多样性。\n\n*   **第二步：实现“人格感知的用户表征”（解决动态建模问题）。**\n    *   **思路：** 将用户的混合型特征（25个连续变量 + 7个类别变量）编码为一个紧凑的81维向量。\n    *   **逻辑：** 关键在于“逐轮预测”。作者训练了一个预测器，在每一轮对话中根据最近的交互实时更新这个81维向量，并将其拼接到RL的状态输入中。这使得Agent能“看到”用户当前的心理轨迹。\n\n*   **第三步：设计“复合奖励与D3QN优化”（解决策略学习问题）。**\n    *   **思路：** 使用Dueling Double DQN（D3QN）来处理状态-价值估计。\n    *   **逻辑：** 在奖励函数设计上，除了常规的“同意意图”和“捐赠金额”，创新性地加入了“反悔惩罚”。这直接对应了假设三，迫使Agent不仅要说服用户，还要巩固用户的承诺，减少“口头答应但事后反悔”的情况。\n\n### 5. 逻辑闭环：实验验证与发现\n最后，作者通过实验验证了这一思考链条的有效性：\n*   **验证动态性：** 实验表明，包含逐轮人格特征的策略确实获得了更高的累积奖励。\n*   **验证仿真：** 基于LLM的仿真数据增强了模型对未见用户行为的泛化能力。\n*   **验证稳定性：** 引入反悔惩罚后，用户的反悔率确实下降，且捐赠结果略有提升。\n\n**总结：**\n作者的思考路径是从**“通用LLM缺乏策略性”**这一宏观洞察出发，通过**“动态人格”**和**“结构化仿真”**两个切入点，将心理学建模与强化学习紧密结合，最终构建了一个既能适应实时心理变化，又能产生稳定劝说效果的闭环系统。"
                },
                {
                    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
                    "arxiv_id": "2601.06789",
                    "authors": "Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang",
                    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究代码智能体，核心贡献在于通过构建和管理“记忆”来增强智能体的能力，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在解决Code Agents因“封闭世界”限制而无法有效利用GitHub历史经验的问题。针对GitHub上非结构化且碎片化的Issue和PR数据，我们提出了MemGovern框架，通过Experience Governance将原始数据转化为结构化的Experience Cards，并引入Agentic Experience Search机制实现逻辑驱动的检索。在SWE-bench Verified上通过Resolution Rate验证了其有效性，平均提升了4.65%。",
                    "summary_translation": "尽管 autonomous software engineering (SWE) agents（自主软件工程智能体）正在重塑编程范式，但目前它们仍受限于“closed-world”限制：即试图从零开始修复 bug 或仅依赖 local context（局部上下文），而忽视了 GitHub 等平台上蕴藏的丰富历史人类经验。获取这种 open-world experience（开放世界经验）的过程，受到现实世界中 issue-tracking data（问题跟踪数据）非结构化和碎片化特性的阻碍。在本文中，我们介绍了 MemGovern，这是一个旨在对原始 GitHub 数据进行治理，并将其转化为智能体可用的 actionable experiential memory（可操作经验记忆）的 framework（框架）。MemGovern 采用 experience governance（经验治理）将人类经验转化为 agent-friendly（智能体友好）的 experience cards（经验卡片），并引入了一种 agentic experience search strategy（智能体经验搜索策略），从而实现了对 human expertise（人类专业知识）的 logic-driven retrieval（逻辑驱动检索）。通过生成 135K 个治理后的 experience cards（经验卡片），MemGovern 实现了显著的 performance boost（性能提升），将 SWE-bench Verified 上的 resolution rates（解决率）提高了 4.65%。作为一种 plug-in approach（插件式方法），MemGovern 为构建 agent-friendly memory infrastructure（智能体友好的记忆基础设施）提供了有效的解决方案。",
                    "inspiration_trace": "基于对论文《MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences》的深入分析，以下是对作者核心方法逻辑链的系统性推演。这一过程旨在还原作者从宏观观察到微观方法论的思考演进。\n\n---\n\n### 1. 宏观观察：从“闭世界”到“开世界”的范式落差\n**思考起点：**\n作者首先观察到当前代码智能体在解决软件工程任务（特别是Bug修复）时存在一个根本性的局限——**“闭世界”假设**。\n*   **现状：** 现有的SWE Agent（如SWE-Agent）通常试图仅凭当前代码库的上下文或模型自身的预训练知识“从零开始”解决问题。\n*   **类比：** 然而，在真实的软件工程实践中，人类开发者极少这样做。面对复杂问题，资深工程师会利用GitHub等平台上的**历史经验**，查看类似问题是如何被解决的。\n*   **核心矛盾：** Agent拥有强大的推理能力，却被限制在“孤岛”中，无法利用GitHub上海量的、蕴含人类专家智慧的“开世界”经验。\n\n### 2. 问题聚焦：数据鸿沟与“语义鸿沟”\n**初步假设与挑战：**\n既然GitHub上有现成的经验，直接用RAG（检索增强生成）把GitHub Issues和PRs喂给Agent不就行了吗？\n*   **现实阻碍：** 作者发现直接使用原始GitHub数据效果不佳，原因在于数据的**“噪声密集”和“高度异构”**。\n    *   **噪声：** 原始讨论中充斥着社交闲聊（“LGTM”、“Thanks”）、流程性沟通，真正的技术洞察被掩盖。\n    *   **异构：** 不同项目的术语、模块组织、编码风格差异巨大，导致跨项目的知识难以标准化迁移。\n*   **核心痛点：** 存在一个巨大的**“语义鸿沟”**——原始的人类协作记录是非结构化的，而Agent需要的是结构化、可检索、可验证的知识表示。缺乏有效的**治理机制**，是限制Agent利用人类经验的主要瓶颈。\n\n### 3. 方法论演进 I：从“原始数据”到“治理经验”\n**思维转折：**\n为了跨越语义鸿沟，作者意识到不能简单地“检索”数据，而必须先“治理”数据。我们需要将杂乱的记录转化为Agent友好的**“经验记忆”**。\n\n**逻辑推演：**\n1.  **去伪存真：** 并非所有GitHub数据都有价值。需要建立筛选机制（如基于Stars、Issues活跃度的仓库筛选，以及基于技术内容比例的实例净化），确保记忆库的高信噪比。\n2.  **结构化解耦：** 如何让经验既好找又好用？作者提出了一个关键的结构设计——**双层协议**：\n    *   **索引层：** 专注于“检索语义”。提取标准化的症状摘要和诊断信号（如异常类型、错误签名），去除特定项目的细节，确保跨仓库的语义匹配。\n    *   **解析层：** 专注于“推理逻辑”。提炼根因分析、修复策略和Patch摘要，剥离具体代码上下文，使Agent能学到可迁移的修复逻辑，而不仅仅是照搬代码。\n3.  **质量闭环：** 自动化提取难免出错，因此引入了基于Checklist的质量控制机制，确保进入记忆库的每张卡片都是经过验证的专家知识。\n\n### 4. 方法论演进 II：从“静态注入”到“智能搜索”\n**思维转折：**\n有了高质量的记忆库，下一个问题是：Agent如何使用它？\n*   **批判传统RAG：** 传统的RAG往往是“一次性”的，在任务开始前检索一堆文档直接塞入上下文。但这不符合人类调试的动态过程——人类是先假设，再搜索，阅读，然后修正假设。\n*   **新假设：** Agent需要一个能够模拟人类探索行为的**“智能体搜索”**机制。\n\n**逻辑推演：**\n1.  **工具解耦：** 为了平衡广度与深度，设计了**双原语接口**：\n    *   **搜索：** 高吞吐量，基于索引层快速筛选候选，解决“找得到”的问题。\n    *   **浏览：** 高精度，深入查看解析层的具体逻辑，解决“用得好”的问题。\n2.  **渐进式推理：** Agent不应被动接受检索结果，而应主动决策。通过**渐进式智能体搜索**，Agent根据当前状态动态决定是继续搜索更广的范围，还是深入浏览某个特定的经验卡片。这种机制让Agent能够进行**类比迁移**，将历史经验中的抽象策略映射到当前的具体代码上下文中。\n\n### 5. 最终合成：MemGovern 框架的诞生\n**逻辑闭环：**\n作者将上述思考整合为MemGovern框架：\n*   **输入端：** 通过**经验治理**，将混乱的GitHub数据转化为135K张结构化的、高质量的**经验卡片**。\n*   **输出端：** 通过**智能体搜索**，赋予Agent像人类工程师一样查阅和利用历史经验的能力。\n*   **结果验证：** 实验证明，这种“治理+搜索”的组合不仅比不使用经验强，也比直接使用原始数据或传统RAG更强。它使Agent从“机械式修补”转向了“语义感知的正确修复”。\n\n---\n\n**总结：**\n作者的思考路径遵循了**“发现范式差异 -> 识别数据本质障碍 -> 提出治理方案（结构化知识） -> 优化交互方案（类人搜索） -> 系统验证”**的完整逻辑链条。其核心创新在于不满足于简单的数据堆砌，而是通过严谨的治理和交互设计，真正实现了从“人类原始经验”到“Agent可用智慧”的转化。"
                },
                {
                    "title": "CEDAR: Context Engineering for Agentic Data Science",
                    "arxiv_id": "2601.06606",
                    "authors": "Rishiraj Saha Roy, Chris Hinze, Luzian Hahn, Fabian Kuech",
                    "summary": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个名为CEDAR的智能体框架，用于自动化数据科学任务。它明确涵盖了智能体的核心研究范围：规划（生成计划序列）、工具使用（函数调用和代码生成）、自我反思（迭代代码生成和容错）以及多智能体协作（由独立的LLM智能体生成内容）。尽管应用场景是数据科学，但其核心贡献在于智能体的架构设计与上下文工程，而非单纯的应用效果展示。",
                    "summary2": "本文旨在解决利用 LLMs 自动化数据科学任务时面临的上下文限制、数据隐私及任务复杂性等问题。针对 Kaggle 竞赛等数据科学场景，我们提出了一种名为 CEDAR 的代理系统，采用结构化提示、多代理编排及智能历史渲染等上下文工程技术。我们在 canonical Kaggle challenges 上验证了其有效性，展示了其自动化解决初级数据科学任务的能力。",
                    "summary_translation": "我们展示了CEDAR，这是一个利用agentic setup（智能体架构）来自动化数据科学（DS）任务的应用程序。利用LLMs（大语言模型）解决数据科学问题是一个尚待深入探索但具有巨大市场价值的领域。其面临的挑战是多方面的，包括任务复杂性、数据规模、计算限制以及上下文限制。我们表明，通过有效的context engineering（上下文工程）可以缓解这些挑战。我们首先通过数据科学特定的输入字段为初始prompt（提示词）引入结构，这些字段作为智能体系统的指令。随后，解决方案被呈现为由独立的LLM agents（大语言模型智能体）生成的、交替的计划和代码块的枚举序列，从而在工作流的任何步骤都为上下文提供可读的结构。用于生成这些中间文本及相应Python代码的function calls（函数调用），确保数据保留在本地，仅有聚合统计信息及相关指令被注入到LLMs的prompt（提示词）中。我们通过迭代代码生成和智能历史渲染引入了容错机制和上下文管理。最后，我们利用典型的Kaggle挑战赛验证了该智能体数据科学家的可行性。",
                    "inspiration_trace": "基于论文《CEDAR: Context Engineering for Agentic Data Science》，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观行业痛点到微观技术实现的思维演进。\n\n---\n\n### 1. 宏观观察与问题定义\n**思考起点：** 数据科学（DS）工作流高度依赖人工，繁琐且重复，而现代大语言模型（LLM）具备自动化这些任务的潜力。然而，现有的通用LLM工具（如ChatGPT Advanced Data Analysis）在处理真实DS任务时表现不佳。\n\n**核心矛盾识别：**\n作者观察到，虽然LLM能力强大，但在DS领域存在“五大鸿沟”：\n1.  **任务复杂性：** 真实DS项目无法通过单次Prompt解决，需要多步推理。\n2.  **计算能力：** LLM的数学计算能力不可靠。\n3.  **数据规模：** 企业级数据往往超过上传限制。\n4.  **隐私安全：** 敏感数据无法上传至云端模型。\n5.  **上下文混乱：** 随着步骤增加，指令、代码、数据、错误信息混杂，导致上下文超出长度限制且逻辑不可读。\n\n### 2. 核心假设提出\n**思维转折：** 作者意识到，单纯提升模型智商并不能解决上述所有问题。真正的瓶颈在于**“上下文工程”**。\n\n**假设：** 如果能设计一套机制，在LLM推理过程中动态地优化、结构化并压缩上下文，同时确保数据不离开本地环境，就能构建一个高效、透明且安全的自动化数据科学系统。\n\n### 3. 方法论的逻辑演进\n基于上述假设，作者开始构建CEDAR系统，其逻辑演进遵循以下步骤：\n\n#### 第一阶段：输入的结构化（解决“指令不清”）\n*   **思考：** 用户往往不知道如何写完美的Prompt。DS任务有固定的元数据（如数据位置、评价指标、任务描述）。\n*   **决策：** 放弃自由文本输入，设计**结构化表单**。强制用户填写任务描述、数据路径、指标等字段。\n*   **逻辑：** 将非结构化的自然语言需求转化为结构化的机器指令，作为系统的初始上下文。\n\n#### 第二阶段：输出的结构化与可读性（解决“过程黑箱”）\n*   **思考：** 直接让模型输出最终结果（如一个准确率数值）既不可信也不可复用。人类数据科学家的工作方式是“计划+代码”交替进行（类似Jupyter Notebook）。\n*   **决策：** 强制模型输出**交错的文本和代码块**。每一步包含“自然语言计划”和“可执行代码”。\n*   **逻辑：** 模拟人类思维过程，让工作流透明化，便于人类审查和纠错。\n\n#### 第三阶段：智能体分工与路由（解决“任务复杂性”）\n*   **思考：** 让一个LLM同时负责规划、写代码、写解释、判断是否结束，负担太重，容易出错。\n*   **决策：** 引入**多智能体架构**。\n    *   **Orchestrator（编排器）：** 只负责决策，即“下一步该写文本还是写代码，或者结束”。\n    *   **Text Agent：** 专门负责写解释和分析。\n    *   **Code Agent：** 专门负责写Python代码。\n*   **逻辑：** 职责分离。利用**函数调用**和**结构化输出**（JSON Schema）约束编排器的行为，防止其产生幻觉，确保指令准确传递给子代理。\n\n#### 第四阶段：本地化执行与容错（解决“计算与隐私”）\n*   **思考：** LLM不擅长数学，且数据不能上传。\n*   **决策：** **代码即工具**。LLM只生成代码，代码在本地Docker容器中执行。\n*   **逻辑：**\n    *   **数据隐私：** 数据永远不离开本地，只有统计摘要进入Prompt。\n    *   **计算准确性：** 用Python解释器替代LLM进行数学运算。\n    *   **容错机制：** 如果代码执行报错，将错误信息回传给Code Agent进行迭代修复，而不是直接崩溃。\n\n#### 第五阶段：智能历史渲染（解决“上下文膨胀”）\n*   **思考：** 随着步骤增加，历史记录会无限增长，撑爆上下文窗口。直接截断会丢失关键信息。\n*   **决策：** 开发**History Rendering（历史渲染）模块**，对上下文进行“有损压缩”。\n*   **逻辑：**\n    *   **保留全量：** 用户的指令、生成的文本和代码本身通常不长，全量保留。\n    *   **智能截断：** 代码的输出往往很长。只保留成功输出的“头部”（关键信息）和失败输出的“尾部”（错误堆栈）。\n    *   **滑动窗口：** 如果总长度仍超限，只保留最近的N个字符。\n*   **逻辑：** 确保LLM在任何时刻看到的都是最相关、最精简的信息，从而维持推理连贯性。\n\n### 4. 最终系统形态\n通过上述层层递进的思考，作者最终形成了CEDAR系统的核心逻辑：\n**一个基于上下文工程的智能体系统，它通过结构化输入引导，利用编排器路由文本与代码生成代理，在本地执行代码以保障隐私与计算准确性，并通过智能的历史压缩机制，在有限的上下文窗口内完成复杂的数据科学任务。**\n\n---\n\n**总结：**\n作者的思考路径并非从“如何设计一个复杂的Agent”出发，而是从“如何管理信息流”出发。**CEDAR的本质不是算法创新，而是信息架构的创新**——通过精心设计什么信息应该进入Prompt、以什么形式进入、以及在何时被修剪，从而释放LLM在复杂任务中的潜力。"
                },
                {
                    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
                    "arxiv_id": "2601.06487",
                    "authors": "Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha",
                    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究LLM智能体，针对开放性智能体任务（如复杂旅行规划）提出了ArenaRL强化学习范式，旨在通过相对排名机制优化智能体性能，属于单智能体规划与自我演化范畴。",
                    "summary2": "本文旨在解决开放性Agent任务中强化学习因点式评分导致的判别性崩溃问题。针对缺乏客观真值的复杂规划场景，我们提出了一种ArenaRL框架，通过基于锦标赛的相对排名机制替代不稳定的标量评分，并利用带种子的单败淘汰赛实现高效优势估计。我们在Open-Travel和Open-DeepResearch基准上，通过胜率和多维度评估指标验证了其有效性，显著优于现有RL基线。",
                    "summary_translation": "强化学习 已显著提升了 LLM agents (大语言模型智能体) 在具有 verifiable outcomes (可验证结果) 的任务上的表现，但在具有 vast solution spaces (巨大解空间) 的 open-ended agent tasks (开放式智能体任务)（例如复杂的旅行规划）中仍然面临挑战。由于这些任务缺乏 objective ground-truth (客观真值)，当前的 RL algorithms (强化学习算法) 主要依赖于对 individual responses (单个响应) 分配 scalar scores (标量分数) 的 reward models (奖励模型)。我们认为这种 pointwise scoring (逐点打分) 存在固有的 discrimination collapse (判别性崩溃)：reward model (奖励模型) 难以区分不同 trajectories (轨迹) 之间的 subtle advantages (细微优势)，导致组内的分数被压缩到一个狭窄的范围内。因此，有效的 reward signal (奖励信号) 被 reward model (奖励模型) 的噪声所主导，导致 optimization stagnation (优化停滞)。为了解决这个问题，我们提出了 ArenaRL，这是一种从 pointwise scalar scoring (逐点标量打分) 转向 intra-group relative ranking (组内相对排序) 的 reinforcement learning paradigm (强化学习范式)。ArenaRL 引入了一种 process-aware pairwise evaluation mechanism (过程感知成对评估机制)，采用 multi-level rubrics (多级评分标准) 为 trajectories (轨迹) 分配 fine-grained relative scores (细粒度相对分数)。此外，我们构建了一个 intra-group adversarial arena (组内对抗竞技场) 并设计了一种 tournament-based ranking scheme (基于锦标赛的排序方案) 来获取稳定的 advantage signals (优势信号)。Empirical results (实证结果) 证实，构建的 seeded single-elimination scheme (种子单败淘汰赛方案) 在仅具有 O(N) 复杂度的情况下，实现了与具有 O(N^2) 复杂度的 full pairwise comparisons (全成对比较) 几乎等效的 advantage estimation accuracy (优势估计精度)，在效率和精度之间取得了最佳平衡。此外，为了解决缺乏针对 open-ended agents (开放式智能体) 的 full-cycle benchmarks (全周期基准) 的问题，我们构建了 Open-Travel 和 Open-DeepResearch，这两个高质量的 benchmarks (基准) 具有涵盖 SFT (监督微调)、RL training (强化学习训练) 和 multi-dimensional evaluation (多维评估) 的 comprehensive pipeline (全流程管道)。Extensive experiments (广泛实验) 表明，ArenaRL 明显优于 standard RL baselines (标准强化学习基线)，使 LLM agents (大语言模型智能体) 能够为复杂的现实世界任务生成更 robust (鲁棒) 的解决方案。",
                    "inspiration_trace": "基于论文《ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking》的内容，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题的确立与现状的困境\n**思考起点：** 强化学习（RL）在数学、代码等有明确“标准答案”的任务上极其成功，但在旅行规划、深度研究等**开放式任务**上却举步维艰。\n**核心矛盾：** 开放式任务没有客观的Ground Truth（标准答案）。现有的解决方案通常采用“LLM作为裁判”给模型的输出打一个标量分数（例如0-10分）。\n**初步假设：** 如果能训练一个准确的奖励模型来给这些开放式轨迹打分，就能像数学题一样进行RL优化。\n\n### 第二阶段：现象观察与核心痛点识别\n**深入观察：** 作者在实验中发现了一个反直觉的现象：随着模型能力的提升，RL优化反而停滞甚至退化。\n**归因分析：** 作者将此命名为**“判别性崩溃”**。\n1.  **信号压缩：** 当模型变强后，生成的轨迹质量都很高，且分布趋同。裁判很难区分“好”和“更好”，给出的分数被压缩在一个极窄的区间（如0.8-0.9）。\n2.  **信噪比（SNR）恶化：** 裁判本身存在随机噪声（如位置偏差、长度偏好）。当分数之间的差异（信号）小于裁判的随机误差（噪声）时，优化过程实际上是在拟合噪声，而非提升能力。\n**结论：** 在开放式任务中，**点式标量打分**存在根本性缺陷，无法提供有效的梯度信号。\n\n### 第三阶段：范式转移——从“绝对分数”到“相对排序”\n**理论借鉴：** 借鉴决策理论，人类在判断模糊事物时，相对比较（A比B好）比绝对量化（A是8.5分）更稳定、更准确。\n**核心假设：** 放弃给单个轨迹打绝对分，转而在**组内**进行轨迹之间的两两比较，构建相对排名。\n**预期收益：** 相对比较能放大细微的质量差异，避免陷入绝对分数的“高分段压缩”陷阱，从而获得更纯净的优势信号。\n\n### 第四阶段：工程落地的挑战——效率与精度的权衡\n**新问题：** 虽然两两比较（Round-Robin，循环赛）能提供最准确的排名，但其计算复杂度是 $O(N^2)$。对于需要大规模采样的RL训练来说，这是不可接受的昂贵成本。\n**朴素尝试与失败：**\n1.  **锚点法：** 只让所有样本与一个锚点（如贪婪解码结果）比较。复杂度降为 $O(N)$，但分辨率太低，无法区分两个都比锚点好但互有优劣的样本。\n2.  **标准淘汰赛：** 随机两两对决，胜者晋级。虽然快，但随机性太大。两个高质量的样本可能在第一轮就相遇，导致其中一个被过早淘汰，损失了信息。\n\n### 第五阶段：结构创新——带种子的单败淘汰赛\n**逻辑推演：** 为了在 $O(N)$ 的线性复杂度下保持接近循环赛的精度，必须解决“过早相遇”的问题。\n**解决方案：** 提出**带种子的单败淘汰赛**。\n1.  **预排序：** 先利用低成本的“锚点法”对所有样本进行一轮快速评估，得到一个粗略的初始排名（种子）。\n2.  **结构化对决：** 按照种子排布对阵（例如：第1名对最后一名，第2名对倒数第二名）。这保证了强样本在早期不会相遇，只有到了决赛圈才强强对话。\n**结果：** 这种设计既保留了线性复杂度的高效，又通过先验信息保证了排名的保真度，实现了效率与精度的最佳平衡。\n\n### 第六阶段：评估维度的深化——过程感知\n**最后一步：** 既然是Agent任务，评价标准不能只看最终答案。\n**补充逻辑：** 引入**过程感知的成对评估**。裁判不仅看结果，还要审查思维链的逻辑连贯性和工具调用的有效性。这确保了RL优化的方向是提升Agent的内在推理能力，而不是仅仅学会生成漂亮的最终文本。\n\n---\n\n**总结：**\n作者的思考路径是从**“开放式任务缺乏客观标准”**这一痛点出发，通过**“判别性崩溃”**否定了现有的标量打分范式，进而提出**“相对排序”**的理论转向。为了解决该理论带来的计算开销，作者通过**“带种子的淘汰赛”**这一精巧的结构设计，成功在计算效率和信号质量之间找到了最优解，最终形成了ArenaRL的方法论闭环。"
                },
                {
                    "title": "Automated QoR improvement in OpenROAD with coding agents",
                    "arxiv_id": "2601.06268",
                    "authors": "Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee",
                    "summary": "EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个自主的编码智能体系统，涉及规划、工具使用（读取代码库、提交差异）和闭环反馈，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决EDA开发受限于专家资源稀缺及代码库复杂的问题，实现利用LLM自主改进OpenROAD的QoR。针对OpenROAD多语言、大规模的代码仓库，我们提出了AuDoPEDA系统，该系统集成了图结构文档生成、基于文献的DSPy规划及具有QoR反馈的自主执行代理。在ASAP7、SKY130HD和Nangate45 benchmark上，通过routed wirelength和effective clock period验证，实现了线长降低5.9%和时钟周期减少10.0%的显著效果。",
                    "summary_translation": "EDA（电子设计自动化）的开发与创新一直受到专家工程资源稀缺的制约。尽管领先的 LLMs（大语言模型）在代码编写和科学推理任务中表现优异，但其在推动 EDA 技术本身发展方面的能力尚未得到充分验证。我们提出了 AuDoPEDA，这是一个构建于 OpenAI 模型和 Codex 类智能体之上的自主式、基于代码仓库的编码系统。该系统能够读取 OpenROAD（开源自动化设计工具），提出研究方向，将其扩展为实施步骤，并提交可执行的 diffs（差异补丁）。我们的主要贡献包括：(i) 一个用于 EDA 代码修改的闭环 LLM 框架；(ii) 一套面向 PPA（功耗、性能、面积）优化的 OpenROAD 任务集及评估协议；以及 (iii) 仅需极少量人工监督的端到端演示。在 OpenROAD 上进行的实验表明，布线线长最多降低了 5.9%，有效时钟周期最多缩短了 10.0%。",
                    "inspiration_trace": "基于论文《Automated QoR improvement in OpenROAD with coding agents》，以下是对作者提出AuDoPEDA方法核心逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观观察与问题定义\n**（从“资源瓶颈”到“技术错位”）**\n\n1.  **观察现状（EDA领域的痛点）：**\n    *   作者首先观察到EDA（电子设计自动化）工具的发展和创新严重受限于“资深专家”的稀缺。\n    *   现代EDA代码库（如OpenROAD）极其庞大、复杂（多语言、数百万行代码、隐式接口多），人类工程师上手极慢，更不用说让机器去理解。\n\n2.  **技术趋势与错位（LLM的潜力与局限）：**\n    *   **趋势：** 大型语言模型（LLMs）在代码生成和科学推理上表现出色。\n    *   **错位：** 现有的LLM擅长处理局部的、单文件的编程任务，但在面对需要跨文件理解、掌握隐式不变量以及处理异构工具链的“仓库级”任务时，能力急剧下降。\n    *   **核心问题：** **能否让一个LLM驱动的系统，像人类专家一样，自主地对工业级EDA代码库进行修改，并切实提升芯片设计的质量（QoR）？**\n\n### 第二阶段：核心假设与类比推理\n**（从“通用编码”到“领域入职”）**\n\n1.  **类比人类专家的学习路径：**\n    *   作者思考：人类是如何掌握OpenROAD的？不是靠死记硬背每一行代码，而是通过阅读文档、论文、手册以及理解代码结构。\n    *   **提出假设：** 既然人类可以通过“文档优先”的方式入职，那么没有根本理由阻止AI代理通过同样的方式被“入职”。EDA是一个高度专业化的领域，其专业知识是可以被结构化提取的。\n\n2.  **确立设计哲学：**\n    *   **拒绝“黑盒”调参：** 传统的机器学习在EDA中多用于参数调优，但这不触及算法核心。\n    *   **主张“代码级”干预：** 要实现真正的创新，系统必须能够修改底层C++/Tcl代码，而不仅仅是修改脚本。\n    *   **引入“闭环”验证：** EDA不同于通用软件，它的输出有明确的物理指标（PPA：功耗、性能、面积）。因此，系统必须包含一个基于QoR反馈的闭环。\n\n### 第三阶段：方法论演进与逻辑拆解\n**（从“抽象目标”到“具体执行流”）**\n\n为了解决上述问题，作者将复杂的任务拆解为四个逻辑严密的阶段，形成了一个漏斗状的思考链条：\n\n**1. 解决“看不懂”：S0 阶段（结构化认知）**\n*   **思考：** LLM上下文窗口有限，无法吞下整个代码库。直接投喂原始代码会导致信息稀释。\n*   **对策：** 必须像人类一样先“画地图”。\n*   **逻辑：** 利用Tree-sitter解析代码构建属性图（DAG），理清调用关系和依赖。然后，自底向上生成“文档卡片”，将复杂的代码逻辑浓缩为API、前置/后置条件等结构化知识。\n*   **产出：** 机器可读的、结构化的代码地图和文档库。\n\n**2. 解决“不懂行”：S1 阶段（领域知识注入）**\n*   **思考：** 即使看懂了代码结构，通用LLM也不懂“布局”或“布线”的深层算法原理。\n*   **对策：** 必须引入外部领域知识。\n*   **逻辑：** 将EDA领域的顶级会议论文（DAC/ICCAD）和文档作为知识库。利用RAG（检索增强生成）和DSPy框架，让系统结合“代码库现状”和“文献中的先进理论”，生成高层的研究计划。\n*   **产出：** 结合了理论（文献）与实践（代码）的高层改进方案。\n\n**3. 解决“落不了地”：S2 阶段（任务本地化）**\n*   **思考：** 高层计划（如“减少线长”）太抽象，无法直接执行。需要将其映射到具体的代码修改点。\n*   **对策：** 将抽象意图转化为具体的手术刀式操作。\n*   **逻辑：** 将S1的计划投影到S0的代码图上，定位具体的文件和函数。同时，定义“粒度计划”，包括具体的Diff意图、预检查、监控指标和回滚条件。\n*   **产出：** 可执行的、包含验证步骤的详细代码修改指令。\n\n**4. 解决“改了就坏”：S3 阶段（闭环验证与进化）**\n*   **思考：** 代码改了可能会编译失败，或者编译通过了但芯片性能变差。如何保证安全性和有效性？\n*   **对策：** 模拟人类的调试流程，建立严格的QoR门控。\n*   **逻辑：** 采用Planner-Executor架构。代理应用Diff -> 编译 -> 运行流程 -> 测量QoR（如线长、时序）。如果指标恶化或DRC违规，自动回滚并将失败作为反例反馈给规划器进行自我修正。\n*   **产出：** 经过验证的、确实提升了PPA指标的代码补丁。\n\n### 第四阶段：总结与范式转移\n**（从“辅助工具”到“自主研究员”）**\n\n*   **逻辑闭环：** 作者通过上述四个阶段，构建了一个完整的**“感知（S0）- 规划（S1）- 定位（S2）- 行动（S3）”**循环。\n*   **最终结论：** 实验证明（线长减少5.9%，时钟周期减少10.0%），LLM不仅能做辅助，还能作为核心参与者，通过阅读文献和代码，自主发现并实施算法改进。\n*   **思想升华：** 这标志着EDA研发范式的转变——从“人类主导、机器辅助”转向“机器自主、人类监督”，开启了自我进化的EDA工具链新纪元。"
                },
                {
                    "title": "An Intelligent AI glasses System with Multi-Agent Architecture for Real-Time Voice Processing and Task Execution",
                    "arxiv_id": "2601.06235",
                    "authors": "Sheng-Kai Chen, Jyh-Horng Wu, Ching-Yao Lin, Yen-Ting Lin",
                    "summary": "This paper presents an AI glasses system that integrates real-time voice processing, artificial intelligence(AI) agents, and cross-network streaming capabilities. The system employs dual-agent architecture where Agent 01 handles Automatic Speech Recognition (ASR) and Agent 02 manages AI processing through local Large Language Models (LLMs), Model Context Protocol (MCP) tools, and Retrieval-Augmented Generation (RAG). The system supports real-time RTSP streaming for voice and video data transmission, eye tracking data collection, and remote task execution through RabbitMQ messaging. Implementation demonstrates successful voice command processing with multilingual support and cross-platform task execution capabilities.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个双智能体架构，涉及多智能体协作、工具使用（MCP）和检索增强生成（RAG），符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决AI眼镜在实时语音处理与跨网络任务执行中的局限性。针对工业自动化及多语言交互场景，我们提出了一种基于双Agent架构的系统，集成Whisper.cpp进行ASR及本地LLM/RAG进行智能处理。在实际硬件与分布式网络环境中，通过UR10机器人故障查询、Google Maps导航及多语言语音命令处理，验证了其有效性与跨平台兼容性。",
                    "summary_translation": "本文介绍了一种集成实时语音处理、人工智能（AI）智能体及跨网络流媒体传输功能的AI眼镜系统。该系统采用双智能体架构，其中Agent 01负责自动语音识别（ASR），Agent 02则通过本地大语言模型（LLMs）、模型上下文协议（MCP）工具及检索增强生成（RAG）技术来管理AI处理任务。该系统支持基于实时RTSP流媒体传输的语音与视频数据传输、眼动追踪数据采集，以及通过RabbitMQ消息传递实现的远程任务执行功能。实施结果表明，该系统成功实现了具备多语言支持功能的语音指令处理，并展现了跨平台任务执行能力。",
                    "inspiration_trace": "基于这篇论文的内容，我们可以将作者的思考过程还原为以下五个逻辑演进阶段：\n\n### 1. 宏观观察：AI眼镜的“理想”与“现实”落差\n**思考起点**：作者首先观察到AI眼镜在工业自动化、医疗等领域的巨大潜力——作为一种“解放双手”的终端，它理应成为人与数字世界交互的最前沿。\n**现实困境**：然而，现有的解决方案往往顾此失彼。要么只能做简单的显示，缺乏智能；要么依赖云端处理导致延迟高，无法满足实时交互需求。\n**核心问题**：如何在一个算力受限、佩戴在头部的设备上，实现既“听得懂”（实时语音）又“想得快”（智能决策）还能“传得稳”（跨网络控制）的完整闭环？\n\n### 2. 痛点聚焦：三大核心矛盾的识别\n作者将宏观问题拆解为三个具体的技术矛盾，这也是后续设计的切入点：\n*   **矛盾一（感知层）**：语音识别需要极低的延迟，但环境嘈杂且算力有限。传统的云端ASR太慢，本地ASR精度难调。\n*   **矛盾二（认知层）**：智能任务执行（如控制机械臂、查询文档）需要大模型（LLM）和外部知识库（RAG），眼镜无法承载如此重的计算负载。\n*   **矛盾三（传输层）**：企业级网络环境复杂（内网、VPN、NAT），简单的流媒体传输无法保证在跨网络、跨平台环境下的稳定性和安全性。\n\n### 3. 核心假设：解耦与分布式的架构重构\n为了解决上述矛盾，作者提出了一个核心假设：**“单体架构”行不通，必须通过“解耦”将感知与认知分离，并通过“分布式”将计算负载转移。**\n*   **逻辑推演**：既然眼镜端做不了所有事，那就让它只做“传感器”和“显示器”，把复杂的“大脑”功能剥离出来，放到边缘或云端，但要通过高效的协议连接它们。\n\n### 4. 方法论演进：从双智能体到全链路协同\n基于上述假设，作者构建了具体的方法论，这一过程体现了模块化的设计思想：\n\n*   **第一步：功能拆分（双智能体架构）**\n    *   **Agent 01（耳朵）**：专注于“听”。为了解决实时性，选择轻量级的Whisper.cpp，并引入滑动窗口和VAD（语音活动检测）来优化资源，确保“听得快且准”。\n    *   **Agent 02（大脑）**：专注于“想”。为了解决智能性，集成本地LLM（Ollama）保证隐私和响应速度，结合RAG（记忆）和MCP（工具调用）扩展能力，使其能处理复杂任务。\n\n*   **第二步：连接与控制（通信与执行）**\n    *   **数据流**：针对音视频大数据，采用RTSP协议进行实时流传输。\n    *   **控制流**：针对指令和任务，引入RabbitMQ消息队列。这解决了网络抖动问题，并实现了跨平台（Windows/Linux/macOS）的任务分发，让眼镜能指挥不同的机器。\n\n*   **第三步：情境增强（多模态融合）**\n    *   仅有语音是不够的，作者引入眼动追踪作为辅助输入，通过数据融合算法，让系统不仅“听其言”，还能“观其行”，提升交互的自然度。\n\n### 5. 验证与反思：从功能实现到局限性的再认知\n最后，作者通过具体的场景（UR10机械臂维修、地图导航）验证了架构的可行性。\n**逻辑闭环**：实验证明了双智能体架构成功平衡了实时性与智能性，分布式架构解决了跨网络执行问题。\n**深层反思**：作者也意识到这种架构的代价——累积延迟（串行处理导致的）和对网络的强依赖。这反向推导出未来的研究方向必须向“边缘计算”和“多模态融合”演进，以进一步降低延迟并减少对网络的依赖。\n\n---\n\n**总结**：\n作者的思考路径是从**应用场景的碎片化**出发，识别出**算力与实时性的根本矛盾**，进而通过**“双智能体解耦”+“分布式消息队列”**的架构创新，在眼镜的轻量化需求与后台的重型计算需求之间找到了一个平衡点。"
                },
                {
                    "title": "Latent Space Communication via K-V Cache Alignment",
                    "arxiv_id": "2601.06123",
                    "authors": "Lucio M. Dery, Zohar Yahav, Henry Prior, Qixuan Feng, Jiajun Shen, Arthur Szlam",
                    "summary": "Solving increasingly complex problems with large language models (LLMs) necessitates a move beyond individual models and towards multi-model systems that can effectively collaborate. While text has traditionally served as the medium for inter-model communication, a richer and more efficient exchange is possible if models can access each other's internal states directly. In this paper, we propose learning a shared representation space that aligns the k-v caches of multiple models, creating a high-bandwidth channel for collaboration without altering the underlying pre-trained parameters. We do so by augmenting each model with adapters to translate its state into and out of this shared space. Via a suite of experiments with Gemma-2 models, we demonstrate that this approach not only enables seamless inter-model communication but also improves individual model performance. We also show that the shared space allows for the direct transfer of learned skills, such as soft prompts, between different models. Our work represents a significant step towards a future where models can fluidly share knowledge and capabilities.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于多模型系统之间的协作与通信，提出通过K-V缓存对齐实现模型间直接交互，属于“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决多模型协作中通信带宽低及潜在空间不兼容的问题。针对不同训练条件下的LLM，我们提出了一种通过学习共享k-v cache潜在空间并利用adapters进行翻译对齐的方法，在Gemma-2模型及多语言C4数据集上通过语言建模损失验证了其有效性。",
                    "summary_translation": "利用大型语言模型解决日益复杂的问题，要求我们超越单一模型，转向能够有效协作的多模型系统。尽管文本传统上一直作为模型间通信的媒介，但如果模型能够直接访问彼此的内部状态，则可以实现更丰富、更高效的交互。在本文中，我们提出学习一个共享表示空间，该空间对齐多个模型的 k-v caches (键值缓存)，从而在不改变底层预训练参数的情况下，为协作创建一个高带宽通道。我们通过为每个模型增加 adapters (适配器) 来实现这一点，用于将其状态转换进出该共享空间。通过一系列基于 Gemma-2 模型的实验，我们证明了该方法不仅实现了无缝的模型间通信，还提升了单个模型的性能。我们还展示了该共享空间允许在不同模型之间直接迁移习得的技能，例如 soft prompts (软提示)。我们的工作代表了迈向模型能够灵活共享知识和能力未来的重要一步。",
                    "inspiration_trace": "基于论文《Latent Space Communication via K-V Cache Alignment》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n### 1. 宏观背景：单体模型的局限与协作的必要性\n**思考起点**：随着LLM能力边界的扩展，单一模型（无论是通用模型还是领域专家）在解决极其复杂的问题时往往力不从心。\n**逻辑推演**：未来的趋势必然是从“单体智能”走向“群体智能”。我们需要构建一个多模型系统，让不同特长的模型能够协同工作。然而，这就引出了一个核心问题：**这些模型之间应该如何高效地交换信息？**\n\n### 2. 现状瓶颈：文本通信的低带宽\n**观察现状**：目前多模型协作（如Agent系统、级联模型）主要依赖自然语言文本进行通信。\n**痛点分析**：文本是一种“有损”且低带宽的媒介。模型内部丰富的推理链、上下文细节和隐含状态很难被完全压缩进几个token的文本中。这种通信方式就像两个人只能通过纸条交流，效率极低且信息丢失严重。\n**思考方向**：如果模型能绕过文本，直接读取彼此的“思维过程”，协作效率将产生质的飞跃。\n\n### 3. 核心洞察：K-V Cache作为高带宽载体\n**技术聚焦**：Transformer架构中的Key-Value (K-V) Cache 实际上存储了模型处理输入时的内部状态（注意力机制的历史记录）。\n**假设提出**：K-V Cache 是模型内部状态的丰富表征。如果模型A能直接访问模型B的K-V Cache，就相当于直接读取了B的“记忆”和“推理路径”。这提供了一种比文本高得多的通信带宽。\n\n### 4. 关键障碍：潜在空间的异构性\n**现实挑战**：虽然想法很美好，但现实很骨感。不同模型（不同架构、不同训练数据、不同随机初始化）的K-V Cache所在的潜在空间是完全不同的。\n**深层原因**：由于参数不同，同一个token在不同模型中产生的条件依赖和向量表征是截然不同的，且这种差异会随着网络深度呈指数级放大。直接混用会导致模型“听不懂”对方的内部状态。\n\n### 5. 理论假设：引入“中间语”共享空间\n**灵感借鉴**：借鉴机器翻译中的“中间语”概念。在翻译多种语言时，不直接进行两两互译，而是先将所有语言映射到一个抽象的语义空间，再从该空间映射到目标语言。\n**核心构想**：构建一个**全局共享的潜在空间（$\\Sigma$）**。这个空间充当所有模型的“通用语言”。每个模型只需要学会两件事：如何把自己的K-V Cache“翻译”进这个共享空间，以及如何从共享空间“翻译”回自己的私有空间。\n\n### 6. 方法构建：基于Adapter的非线性映射\n**设计约束**：为了保持模型的原始能力并降低成本，不能修改预训练模型的参数。\n**架构设计**：为每个模型配备轻量级的“适配器”。\n*   **映射方向**：$T[\\text{Model} \\to \\Sigma]$（编码）和 $T[\\Sigma \\to \\text{Model}]$（解码）。\n*   **非线性选择**：由于不同模型间的几何关系可能高度复杂且非线性，简单的线性映射可能不够。作者选择了基于交叉注意力的小型Transformer作为适配器架构，以捕捉复杂的层级依赖关系。\n*   **扩展性**：这种设计使得参数量仅随模型数量线性增长，且新模型加入时无需重训练整个系统。\n\n### 7. 优化目标：从“形似”到“神似”\n**训练信号的选择**：如何训练这些适配器？\n*   **初级尝试（重建损失）**：强制让翻译后的Cache看起来像目标模型原本的Cache。但这可能过于严格，且受限于目标模型本身的能力上限。\n*   **进阶思考（功能对齐）**：我们不需要Cache完全一样，只需要它们产生的**结果**一样。\n*   **最终方案（后缀语言建模损失）**：使用源模型的前缀Cache翻译给目标模型，看目标模型能否准确预测后续的文本。这是一种“功能主义”的训练目标，只要能帮助模型完成任务，Cache长什么样并不重要。实验证明，这种方法甚至能通过共享空间“蒸馏”出更好的特征，提升单体模型性能。\n\n### 8. 价值延伸：技能的即插即用\n**逻辑推演**：既然存在一个共享的潜在空间，那么在这个空间中的任何表征（如软提示 Soft Prompts、前缀微调 Prefix Tuning）本质上都变成了一种“通用资源”。\n**应用场景**：在一个模型上学到的特定技能（如某种写作风格或编程能力），可以通过共享空间直接“移植”给另一个模型，而无需对目标模型进行额外训练。这实现了从“模型协作”到“技能复用”的跨越。\n\n---\n\n**总结**：\n作者的思考路径是从**解决多模型协作效率低下的宏观痛点**出发，通过**挖掘K-V Cache的高带宽价值**，针对**模型异构性这一核心障碍**，借鉴**机器翻译的中间语思想**，提出了**基于共享潜在空间和Adapter映射的解决方案**，并最终通过**功能对齐的训练目标**和**技能迁移的验证**，完成了从理论构想到方法论的闭环。"
                },
                {
                    "title": "Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation",
                    "arxiv_id": "2601.06034",
                    "authors": "Dudekula Kasim Vali",
                    "summary": "Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题明确提出了“Autonomous QA Agent”，其核心是利用检索增强生成（RAG，即记忆机制）来辅助生成Selenium脚本（工具使用）。这符合单智能体研究中关于记忆和工具使用的范畴，且不属于排除的医疗/金融等纯应用领域。",
                    "summary2": "本文旨在解决LLM生成Selenium脚本时因缺乏应用上下文而产生幻觉的问题。针对自然语言需求和HTML DOM结构，我们提出了一种Autonomous QA Agent，这是一种基于RAG的多模态框架，通过检索文档与HTML上下文生成脚本。在自定义电商应用的20个测试场景上，通过语法有效性、元素解析率和执行成功率验证了其有效性，实现了90%的执行成功率。",
                    "summary_translation": "软件测试在软件开发生命周期中至关重要，然而将需求转化为可执行测试脚本的过程仍主要依赖人工，且容易出错。尽管大语言模型能够生成代码，但它们经常产生幻觉，编造出不存在的UI元素。我们提出了自主QA代理，这是一种检索增强生成系统，它将Selenium脚本生成基于特定项目的文档和HTML结构之上。通过将多种格式导入向量数据库，我们的系统在生成代码之前会检索相关的上下文信息。针对20个电商测试场景的评估表明，我们的RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95%置信区间：[85%, 95%]，p < 0.001）的执行成功率，而标准LLM生成的成功率仅为30%。尽管我们的评估仅限于单一领域，但我们的方法通过将生成过程基于实际的DOM结构，显著减少了幻觉现象，展示了RAG在自动化UI测试中的潜力。",
                    "inspiration_trace": "基于论文《Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：QA环节的效率瓶颈\n**思考起点：** 在敏捷开发和DevOps主导的现代软件工程中，开发迭代速度极快，但软件测试（QA）成为了明显的瓶颈。\n**核心痛点：** QA工程师花费40%-50%的时间在做“翻译”工作——将自然语言描述的功能需求（PRD）手动转化为机器可执行的自动化测试脚本（如Selenium）。这个过程不仅枯燥，而且容易出错（如选错元素ID、忽略边界情况）。\n**初步设想：** 能否利用代码生成能力强大的大语言模型（LLM）来自动完成这个“翻译”过程？\n\n### 2. 尝试与失败：LLM的“盲写”困境\n**尝试：** 直接使用标准的LLM（如GPT-4, Llama），输入自然语言需求（如“生成一个添加购物车的脚本”），让其编写Selenium代码。\n**观察到的现象：** LLM生成的代码语法通常没问题，但一运行就报错。\n**失败原因分析：** LLM患有一种“盲写症”。它通晓通用的编程语法，但它**看不见**被测应用（AUT）的具体结构。\n**具体表现：** LLM会凭空捏造UI元素。例如，它可能会猜测登录按钮的ID是 `#login-btn`，但实际开发人员写的是 `#btn-submit-login`。这种“幻觉”导致生成的脚本无法定位元素，执行失败。\n\n### 3. 深度诊断：语义鸿沟与上下文缺失\n**问题定义：** 核心问题在于“人类需求”与“机器执行”之间存在语义鸿沟。要生成一个可运行的脚本，不仅需要逻辑（做什么），还需要精确的定位信息（在哪里做）。\n**现有方案的局限：**\n*   **传统MBT（基于模型的测试）：** 构建成本太高，维护困难。\n*   **通用代码RAG：** 现有的检索增强生成多用于检索“相似的代码片段”。但在UI测试中，检索别人的代码对定位当前页面的特定DOM元素帮助不大。\n**关键洞察：** 要解决幻觉，必须让LLM“看见”真实的界面结构。LLM缺失的上下文不是代码示例，而是**应用的实际DOM结构**。\n\n### 4. 策略转折：从“代码检索”到“结构检索”\n**核心假设：** 如果在生成脚本之前，先给LLM提供被测应用的真实HTML文档和需求文档，它就能基于真实的结构编写准确的定位器，从而消除幻觉。\n**方法论创新：** 提出一种专门针对QA领域的RAG架构。\n*   **传统RAG：** 检索通用知识库。\n*   **本论文RAG：** 检索**双模态上下文**。\n    1.  **功能性上下文：** 需求文档（Markdown/PDF），告诉LLM“要测什么”。\n    2.  **结构性上下文：** 原始HTML文件，告诉LLM“元素在哪里”。\n\n### 5. 架构构建：多模态摄入与上下文融合\n**逻辑推演：** 为了实现上述假设，系统需要具备以下能力：\n1.  **知识库构建：** 必须能够“吃进”多种格式的数据。不仅要处理文本需求，还要解析HTML标签，提取出ID、Class等关键属性，并存入向量数据库。\n2.  **精准检索：** 当用户提问时，系统需要同时从文档库中找到相关需求，并从HTML库中找到对应的页面结构片段。\n3.  **提示工程约束：** 在生成阶段，必须强制LLM使用检索到的真实ID，而不是自己编造。通过Prompt明确指令：“仅使用提供的HTML结构中的ID”。\n\n### 6. 验证与结论：Grounding（接地气）的有效性\n**实验设计：** 对比“标准LLM（无上下文）”与“RAG Agent（含HTML上下文）”。\n**结果验证：**\n*   标准LLM：虽然语法正确，但因元素定位错误，执行成功率仅为30%。\n*   RAG Agent：通过将生成过程“锚定”在真实的DOM结构上，执行成功率提升至90%。\n**最终结论：** 证明了在UI自动化测试中，**结构化的上下文（HTML）比通用的代码知识更重要**。通过RAG技术将LLM与实际应用状态连接，是解决测试脚本生成中“幻觉”问题的有效路径。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现瓶颈 -> 尝试新技术（LLM） -> 识别新技术缺陷（幻觉/盲写） -> 引入特定领域知识（DOM结构） -> 设计专用架构（多模态RAG） -> 验证有效性”** 的完整逻辑闭环。其核心创新点在于意识到UI测试不仅仅是代码生成任务，更是一个需要精确空间感知（DOM结构）的任务。"
                },
                {
                    "title": "AI-Assisted Authoring for Transparent, Data-Driven Documents",
                    "arxiv_id": "2601.06027",
                    "authors": "Alfonso Piscitelli, Cristina David, Mattia De Rosa, Ali Mohammed, Federico Nanni, Jacob Pake, Roly Perera, Jessy Sodimu, Chenyiqiu Zheng",
                    "summary": "We introduce _transparent documents_, interactive web-based scholarly articles which allow readers to explore the relationship to the underlying data by hovering over fragments of text, and present an LLM-based tool for authoring transparent documents, building on recent developments in data provenance for general-purpose programming languages. As a target platform, our implementation uses Fluid, an open source programming language with a provenance-tracking runtime. Our agent-based tool supports a human author during the creation of transparent documents, identifying fragments of text which can be computed from data, such as numerical values selected from records or computed by aggregations like sum and mean, comparatives and superlatives like _better than_ and _largest_, trend-adjectives like _growing_, and similar quantitative or semi-quantitative phrases, and then attempts to synthesise a suitable Fluid query over the data which generates the target string. The resulting expression is inserted into the article's web page, turning the static text fragment into an interactable data-driven element able to reveal the data that underwrites the natural language claim. We evaluate our approach on a subset of SciGen, an open source dataset consisting of tables from scientific articles and their corresponding descriptions, which we extend with hand-generated counterfactual test cases to evaluate how well machine-generated expressions generalise. Our results show that gpt4o is often able to synthesise compound expressions extensionally compatible with our gold solutions.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“基于智能体的工具”，利用LLM（gpt4o）辅助作者编写透明文档。该智能体具备识别文本片段、合成查询（工具使用）以及将静态文本转换为交互式元素的能力，符合单智能体中关于工具使用和规划的研究范围。",
                    "summary2": "本文旨在解决学术文档中数据声明难以追溯至底层数据的问题。针对科学论文中的定量描述，我们提出了一种基于LLM的AI辅助编写工具，结合Fluid编程语言的溯源运行时，将静态文本转化为可交互的数据驱动元素，并在SciGen数据集上通过成功率及反事实测试验证了其有效性。",
                    "summary_translation": "我们介绍了“透明文档”，这是一种交互式的基于网络的学术文章，允许读者通过将鼠标悬停在文本片段上来探索其与底层数据的关系。基于通用编程语言在 data provenance（数据溯源）方面的最新进展，我们提出了一种基于 LLM 的工具，用于创作此类透明文档。在目标平台方面，我们的实现采用了 Fluid，这是一种具有 provenance-tracking runtime（具有溯源跟踪功能的运行时）的开源编程语言。我们的 agent-based（基于智能体）的工具在透明文档的创作过程中为人类作者提供支持。该工具能够识别那些可以从数据中计算得出的文本片段，例如：从记录中选取的数值，或通过 sum（求和）和 mean（平均）等 aggregations（聚合操作）计算得出的数值；“better than”（优于）和“largest”（最大）等 comparatives and superlatives（比较级和最高级）；“growing”（增长）等 trend-adjectives（趋势形容词）；以及类似的 quantitative or semi-quantitative phrases（定量或半定量短语）。随后，工具会尝试合成一个合适的 Fluid query（Fluid 查询），以生成目标字符串。生成的表达式被插入到文章的网页中，将静态文本片段转化为 interactable data-driven element（可交互的数据驱动元素），从而能够揭示支撑该 natural language claim（自然语言陈述）的数据。我们在 SciGen 数据集的一个子集上对该方法进行了评估。SciGen 是一个由科学文章中的表格及其对应描述组成的开源数据集。我们通过手工生成的 counterfactual test cases（反事实测试用例）对该数据集进行了扩展，以评估机器生成表达式的 generalise（泛化）能力。结果表明，gpt4o 通常能够生成与我们的 gold solutions（黄金标准）在 extensionally compatible（外延兼容）的 compound expressions（复合表达式）。",
                    "inspiration_trace": "基于论文《AI-Assisted Authoring for Transparent, Data-Driven Documents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 1. 宏观观察：科学交流中的“可追溯性鸿沟”\n**思考起点：**\n作者首先关注到学术出版和科学写作中的一个核心痛点——**信任与验证的困难**。\n*   **现象：** 在学术论文或数据报告中，充斥着大量基于数据的断言（如“系统X比系统Y更快”）。这些断言以静态的自然语言形式存在。\n*   **问题：** 读者（或审稿人）很难直接从文本追溯到支撑该断言的具体数据点。这种“断言”与“证据”之间的脱节，导致了验证困难，甚至因数据管理错误导致论文撤稿。\n*   **现有技术的局限：**\n    *   **数据可视化工具（如Tableau, D3.js）：** 虽然图表是动态的，且部分工具支持溯源，但它们无法处理占据论文主体的自然语言。\n    *   **大语言模型（LLM）：** 擅长理解和生成文本，甚至能进行事实核查，但其输出通常是黑盒的，缺乏将文本片段直接链接到底层数据源的交互式基础设施。\n\n### 2. 核心假设：将“自然语言”视为“计算输出”\n**思维跃迁：**\n为了解决上述鸿沟，作者提出一个颠覆性的假设：**论文中的定量陈述不应是静态的字符串，而应是数据查询的计算结果。**\n*   **类比思维：** 就像Excel中的图表会随数据变化而更新一样，论文中的文字（如“增长率为5%”）也应该是动态生成的。\n*   **概念定义：** 作者提出了“透明文档”的概念。这种文档允许读者通过鼠标悬停在文本上，触发“溯源查询”，直接看到生成该文本的数据来源。\n*   **关键挑战：** 如果要求作者手动编写代码来生成每一个句子（例如写SQL或Python代码来输出“better than”），这在科学写作工作流中是不现实的，门槛太高。\n\n### 3. 方法论构建：寻找“语义理解”与“程序化溯源”的结合点\n**解决方案的合成：**\n作者意识到，要实现上述假设，必须结合两个领域的最新进展，形成互补：\n1.  **LLM的语义理解能力：** 负责将自然语言（如“显著提高”）转化为形式化的逻辑意图。\n2.  **溯源编程语言（Fluid）的基础设施：** 负责执行逻辑并自动维护数据流向，提供交互能力。\n\n**逻辑推演：**\n*   *为什么选Fluid？* 普通语言（如Python）只能计算数据，无法自动追踪数据来源并支持用户交互（悬停查询）。Fluid特有的溯源运行时是“透明性”的技术保障。\n*   *为什么用LLM？* 只有LLM能理解复杂的学术语言并自动生成代码，从而降低作者的使用门槛。\n\n### 4. 实现策略：从“全自动”转向“人机协同”\n**工作流设计：**\n在具体实现路径上，作者没有追求完全自动化的“一键生成”，而是基于对LLM局限性的认知，设计了**人机协同**的迭代工作流。\n*   **思考逻辑：** LLM可能会产生幻觉或生成错误的代码。如果完全自动化，生成的文档将不可信。\n*   **Agent分工：**\n    *   **SuggestionAgent：** 充当“助手”，识别哪些文本片段是可以被数据化的（如数值、比较级）。\n    *   **InterpretationAgent：** 充当“翻译官”，尝试将文本片段编译为Fluid代码。\n*   **闭环验证机制：** 作者设计了一个“生成-验证-修正”的闭环。系统生成代码后，必须在Fluid环境中实际运行，检查输出字符串是否与原文完全匹配。如果不匹配，利用错误信息反馈给LLM进行重试。\n*   **人的角色：** 作者保留最终决定权。只有当作者在网页上交互验证（悬停查看数据）无误后，才会确认替换原文。这确保了科学严谨性。\n\n### 5. 评估视角：从“准确率”到“泛化性与鲁棒性”\n**验证逻辑的深化：**\n在评估方法时，作者不仅关注LLM能否“猜对”代码，更关注这种方法的**鲁棒性**。\n*   **思考：** 如果LLM只是死记硬背了数据，那么当数据发生变化时，生成的代码就会失效。\n*   **反事实测试：** 作者引入了反事实测试用例，故意修改底层数据，观察生成的代码是否能正确反映新的数据状态（例如，数据变了，文本是否自动从“增长”变为“下降”）。\n*   **意义：** 这证明了生成的代码不仅仅是字符串匹配，而是真正捕捉到了文本背后的**语义逻辑**。\n\n### 总结：思想演进脉络\n1.  **发现问题：** 学术文本是静态的，缺乏数据溯源，难以验证。\n2.  **提出愿景：** 让文本像图表一样，成为数据的动态视图（透明文档）。\n3.  **技术选型：** 利用LLM解决“写代码难”的问题，利用Fluid解决“溯源交互”的问题。\n4.  **流程设计：** 采用人机协同的闭环生成，平衡自动化效率与科学准确性。\n5.  **价值验证：** 通过反事实测试，确保系统真正理解了语言与数据的逻辑关系，而非简单的文本替换。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 21,
            "papers": [
                {
                    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
                    "arxiv_id": "2601.07782",
                    "authors": "Wei Fang, James Glass",
                    "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确针对LLM智能体在动态工具库中的工具使用场景，提出了通过查询规划和任务分解来优化工具检索的方法，属于单智能体研究中的规划与工具使用范畴。",
                    "summary2": "本文旨在解决大规模动态工具库中复杂请求检索困难的问题。针对用户意图与工具文档间的语义鸿沟及组合性挑战，我们提出了一种TOOL QP框架，将检索建模为迭代查询规划过程，通过任务分解和动态查询生成与检索器交互。在ToolRet基准测试上，通过nDCG@K和Completeness@K等指标验证了其有效性，显著提升了检索精度和下游执行成功率。",
                    "summary_translation": "运行于大规模、动态工具库之上的 LLM agents（大语言模型智能体）依赖于有效检索，然而标准的 single-shot dense retrievers（单次密集检索器）在应对复杂请求时往往力不从心。这些检索失败主要归因于抽象用户目标与技术文档之间的脱节，以及固定大小 embeddings（嵌入向量）在建模组合式工具组合方面的能力局限。为应对上述挑战，我们提出了 TOOLQP，这是一个将检索过程建模为 iterative query planning（迭代式查询规划）的轻量级框架。不同于单次匹配，TOOLQP 将指令分解为若干子任务，并动态生成查询与检索器进行交互；通过针对组合所需的具体子任务，该方法有效地弥合了语义鸿沟。我们利用合成查询轨迹对 TOOLQP 进行训练，随后通过 Reinforcement Learning with Verifiable Rewards (RLVR，可验证奖励强化学习) 进行优化。实验结果表明，TOOLQP 实现了 state-of-the-art（最先进）的性能，展现出卓越的 zero-shot generalization（零样本泛化）能力、在不同检索器间的鲁棒性，以及在 downstream agentic execution（下游智能体执行）方面的显著提升。",
                    "inspiration_trace": "基于论文《Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning》，以下是对作者产出核心方法 **TOOL QP** 的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM智能体”到“工具检索的必要性”）**\n\n1.  **观察趋势**：随着大语言模型（LLM）向智能体演进，解决复杂任务（如数学、编程、推理）越来越依赖于外部工具（API、数据库）。\n2.  **现实瓶颈**：工具库的规模正在从几十个手工挑选的函数，爆炸式增长到数万个动态API。\n3.  **核心冲突**：由于上下文窗口的限制，无法将所有工具的文档和说明一次性塞入LLM。因此，**工具检索**成为了连接用户意图与海量工具库的必经之路。\n\n### 第二阶段：现有方案的深度诊断\n**（为什么传统的“单次检索”会失效？）**\n\n作者首先审视了当前主流的解决方案——即直接套用标准信息检索（IR）技术，使用密集嵌入进行单次语义匹配。通过分析，作者发现了三个根本性的结构性缺陷：\n\n1.  **语义鸿沟**：\n    *   *现象*：用户的表达通常是抽象的、高层的（如“让这段录音音质变好”），而工具文档是技术的、底层的（如“IIR滤波器参数”）。\n    *   *诊断*：单次嵌入试图在一个向量空间内强行对齐这两种完全不同的语言体系，往往导致匹配失败。\n\n2.  **组合性瓶颈**：\n    *   *现象*：现实任务是组合性的，往往需要同时调用多个不同的工具（如“分析降雨如何影响零售销量”需要天气API+股票数据库）。\n    *   *诊断*：单次查询生成的固定维度向量，本质上是一个“词袋”，缺乏表达“多个离散工具组合”的容量。它无法编码工具之间的逻辑关系和组合多样性。\n\n3.  **缺乏交互性**：\n    *   *现象*：工具之间存在依赖关系（如工具A需要工具B的输出作为参数），且工具库是动态变化的。\n    *   *诊断*：传统检索将工具库视为静态数据库，只能“查一次”，无法像人类一样通过“试错”或“反馈”来发现隐含的依赖关系。\n\n### 第三阶段：范式转换与核心假设\n**（从“静态匹配”转向“动态规划”）**\n\n基于上述诊断，作者意识到问题的根源在于**试图用一次性的静态映射来解决动态的、多步骤的推理问题**。\n\n*   **思维跃迁**：如果人类面对复杂任务时会先“制定计划”，再分步执行，为什么不让检索器也这样做？\n*   **核心假设**：工具检索不应是“Query -> Result”的单跳匹配，而应是一个“Goal -> Plan -> Sub-goals -> Queries -> Results”的**迭代规划过程**。\n*   **新视角**：将底层的检索器视为一个可交互的“环境”，而不是一个静态的索引库。\n\n### 第四阶段：方法论构建\n**（如何实现“查询规划”？）**\n\n为了验证上述假设，作者设计了 **TOOL QP** 框架，将检索过程拆解为三个逻辑阶段：\n\n1.  **任务分解**：\n    *   *逻辑*：为了解决语义鸿沟，不能直接用用户原始查询去检索。\n    *   *方案*：先将复杂的用户指令拆解为一系列逻辑上的子任务。这相当于在抽象意图和具体工具之间架设了一座“中间层桥梁”。\n\n2.  **交互式查询生成**：\n    *   *逻辑*：为了解决组合性和依赖性问题，需要分步检索。\n    *   *方案*：针对每个子任务生成特定的搜索查询。关键在于引入**反馈机制**——每一步检索后，模型会观察结果，并动态调整下一步的查询策略（例如，发现缺少某个前置工具，下一步就去专门搜那个工具）。\n\n3.  **检索聚合**：\n    *   *逻辑*：多步检索会产生多个列表，如何合并？\n    *   *方案*：放弃复杂的加权融合，采用“峰值排名”策略——即取每个工具在所有检索步骤中获得的最高排名。这避免了某些子任务因为查询次数多而主导最终结果的偏差。\n\n### 第五阶段：训练策略的演进\n**（如何在没有标注数据的情况下训练规划器？））\n\n框架设计好了，但面临一个现实难题：现有的数据集只有（用户查询，相关工具），没有中间的“规划轨迹”或“子任务标注”。\n\n1.  **数据合成**：\n    *   *思路*：利用强模型（如GPT-4）作为“教师”，反向合成数据。\n    *   *过程*：让教师模型根据最终的正确工具，反推并生成能够找到这些工具的“规划路径”和“中间查询”。这为模型提供了模仿学习的样本。\n\n2.  **强化学习优化（RLVR）**：\n    *   *思路*：单纯的模仿学习（SFT）只能学会教师的风格，不一定能最大化检索成功率。\n    *   *过程*：引入强化学习（RLVR），直接以检索指标（如nDCG、Recall）作为奖励信号。这迫使模型跳出模仿的局限，自主探索能真正提高检索准确率的查询策略。\n\n---\n\n### 总结：作者的思考路径图\n\n1.  **起点**：LLM智能体需要处理海量工具库 -> 必须检索。\n2.  **痛点**：单次密集检索在复杂任务上表现糟糕。\n3.  **归因**：语义错位、组合性限制、缺乏交互反馈。\n4.  **顿悟**：检索应该是一个**规划**过程，而非简单的匹配。\n5.  **方案**：分解任务 -> 迭代查询 -> 动态反馈 -> 结果聚合。\n6.  **落地**：利用合成数据教模型“怎么想”，利用强化教模型“怎么做得更好”。\n\n这一逻辑链条清晰地展示了作者如何从对现有技术缺陷的敏锐观察，上升到对问题本质的重新定义（从IR到Planning），最终构建出一套完整的解决方案。"
                },
                {
                    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
                    "arxiv_id": "2601.07711",
                    "authors": "Pietro Ferrazzi, Milica Cvjeticanin, Alessio Piraccini, Davide Giannuzzi",
                    "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了“Agentic RAG”，其中LLM作为智能体编排整个检索和生成过程，涉及规划（决定执行哪些动作）、自我反思（是否迭代）和工具使用，完全符合单智能体的研究范围。",
                    "summary2": "本文旨在比较Enhanced RAG与Agentic RAG的性能与成本权衡。针对FIQA、NQ、FEVER和CQADupStack-English数据集，我们对比了基于固定模块的Enhanced RAG与LLM自主编排的Agentic RAG。通过F1、NDCG@10及LLM-as-a-judge等指标验证，发现Agentic RAG在查询重写上表现更优，而Enhanced RAG在文档重排和成本控制上更具优势。",
                    "summary_translation": "检索增强生成 (RAG) 系统通常定义为生成器与检索组件的组合，其中检索组件负责从知识库中提取文本上下文，以回答用户查询。然而，此类基础实现存在若干局限性，包括检索结果存在噪声或非最优、对超出范围的查询误用检索机制、查询与文档匹配度低，以及生成器带来的波动性或成本问题。这些缺陷推动了“增强型” RAG 的发展，即在流程中引入专用模块以解决特定的薄弱环节。近期，大型语言模型日益增强的自我反思能力催生了一种新范式，我们将其称为“代理型” RAG。在该方法中，LLM 统筹整个流程——决定执行何种操作、何时执行以及是否进行迭代——从而减少对固定的人工设计模块的依赖。尽管这两种范式已得到快速普及，但在何种条件下哪种方法更具优势尚不明确。在本研究中，我们针对增强型和代理型 RAG，在多种场景和维度上进行了广泛的实证驱动评估。我们的研究结果揭示了这两种范式之间的权衡关系，并综合考虑成本与性能，为在现实应用中选择最有效的 RAG 设计提供了指导。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection",
                    "arxiv_id": "2601.07780",
                    "authors": "Mariana Costa, Alberlucia Rafael Soarez, Daniel Kim, Camila Ferreira",
                    "summary": "While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "论文主要研究LLM的“自我反思”和“自我修正”机制，这属于单智能体研究范围中的核心组件（规划、记忆、工具使用、自我反思）。尽管涉及CoT和推理任务，但其核心贡献在于提出了一种结构化的反思范式，而非单纯的推理算法改进。",
                    "summary2": "本文旨在增强大语言模型在复杂推理任务中的自我修正能力与鲁棒性。针对算术、常识、伦理决策及逻辑谜题等场景，我们提出了一种 MyGO Poly-Reflective Chain-of-Thought (PR-CoT) 方法，通过结构化的多视角反思机制（如逻辑一致性、信息完整性等）优化推理。我们在 GPT-3.5 和 GPT-4 上通过 Logical Consistency 和 Error Correction Rate 验证了其有效性，显著优于传统 CoT 及单反思方法。",
                    "summary_translation": "尽管 Chain-of-Thought (CoT) prompting（思维链提示）提升了 LLM（大语言模型）的推理能力，但在一致性、准确性和自我修正方面仍面临挑战，尤其是在处理复杂或伦理敏感任务时。现有的 single-dimensional reflection methods（单维度反思方法）所提供的改进效果有限。我们提出了 MyGO Poly-Reflective Chain-of-Thought (PR-CoT)（MyGO 多重反思思维链），这是一种采用结构化 multi-perspective reflection（多视角反思）的新型方法论。在生成初始 CoT 后，PR-CoT 引导 LLM 基于多个预定义角度对其推理过程进行自我评估，这些角度包括：logical consistency（逻辑一致性）、information completeness（信息完整性）、biases/ethics（偏见/伦理）以及 alternative solutions（替代方案）。该过程完全通过 prompt engineering（提示工程）实现，无需进行模型重训练，即可将初始 CoT 优化为更加稳健和准确的最终答案。在使用 GPT-three point five (GPT-3.5) 和 GPT-four (GPT-4) 模型进行的算术、常识、ethical decision-making（伦理决策）和 logical puzzles（逻辑谜题）实验中，结果证明了 PR-CoT 的优越性能。在 logical consistency（逻辑一致性）和 error correction（错误修正）方面，它显著优于传统 CoT 和现有的 reflection methods（反思方法），并在 ethical decision-making（伦理决策）等需要细致处理的领域取得了显著提升。Ablation studies（消融实验）、human evaluations（人工评估）和 qualitative analyses（定性分析）进一步验证了每个 reflection perspective（反思视角）的贡献，以及我们的 poly-reflective paradigm（多重反思范式）在提升 LLM 推理可靠性方面的整体有效性。",
                    "inspiration_trace": "基于论文《Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection》，以下是对作者产出该核心方法（PR-CoT）的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：LLM推理能力的“天花板”困境\n**思考起点：** 作者首先关注到大语言模型（LLM）在自然语言处理任务上表现卓越，尤其是引入“思维链”后，模型在多步推理任务上有了显著提升。\n**发现问题：** 尽管CoT有效，但在面对复杂、混乱或敏感的任务时，LLM仍表现出明显的局限性：逻辑不一致、准确性不足以及缺乏自我纠错能力。模型容易陷入次优解，或者在推理过程中产生矛盾，却无法自主察觉。\n\n### 2. 痛点聚焦：单维反思的局限性\n**现有方案分析：** 作者审视了现有的改进方案，特别是以MyGO Multiplex CoT (MCoT)为代表的“反思机制”。这类方法试图让模型在生成初步答案后进行自我审查。\n**深度批判：** 作者敏锐地指出，MCoT等现有方法采用的是“单维度”的反思。这种单一的、笼统的自我审查往往流于表面，只能纠正显而易见的错误，而无法捕捉更深层次的问题。\n**具体盲点：** 作者列举了单维反思无法覆盖的盲区：\n*   细微的逻辑谬误；\n*   关键信息的遗漏；\n*   伦理决策中的偏见；\n*   忽略了更优的替代解法。\n**结论：** 单维反思的“广度”和“深度”不足以支撑复杂任务的高质量推理，必须寻找一种更全面的自我修正范式。\n\n### 3. 核心假设：从“单点检查”到“多维专家会诊”\n**灵感来源：** 作者将思维转向人类专家的解决问题方式。人类专家在处理复杂问题时，不会只进行一次通用的检查，而是会戴上不同的“眼镜”审视问题：逻辑是否严密？信息是否完整？是否符合伦理？有没有更好的办法？\n**提出假设：** 如果能通过提示工程，强制LLM模拟这种“多视角”的专家会诊模式，将单一的反思步骤拆解为多个互补的审查维度，就能大幅提升错误识别率和推理的鲁棒性。\n\n### 4. 方法论构建：结构化的多视角反思机制 (PR-CoT)\n基于上述假设，作者构建了**MyGO Poly-Reflective Chain-of-Thought (PR-CoT)**，其设计逻辑遵循以下步骤：\n\n*   **步骤一：建立基准。**\n    *   保留传统的CoT生成步骤，作为被审视的“靶子”。\n*   **步骤二：维度拆解。**\n    *   这是核心创新点。作者将模糊的“反思”具体化为四个独立的视角，每个视角负责解决一类特定的错误：\n        1.  **逻辑一致性：** 专门解决推理跳跃和自相矛盾；\n        2.  **信息完整性：** 专门解决关键信息遗漏和事实错误；\n        3.  **偏见与伦理：** 专门解决敏感任务中的价值观偏差（这是单维反思最缺乏的）；\n        4.  **替代方案探索：** 专门解决思维狭隘和次优解问题。\n*   **步骤三：综合修正。**\n    *   设计一个综合步骤，要求模型整合上述所有视角的批评意见，对初始推理进行系统性重构，而非简单的修补。\n\n### 5. 验证与闭环：多维视角的必要性证明\n**实验设计逻辑：** 为了证明“多视角”优于“单视角”，作者设计了对比实验（PR-CoT vs. CoT vs. MCoT）。\n**预期验证：**\n*   **通用性验证：** 在算术、常识、逻辑谜题中，PR-CoT应全面超越MCoT，证明多维反思的普适优势。\n*   **特异性验证：** 特别是在“伦理决策”任务中，PR-CoT应表现出最大的性能提升。因为这是单维反思（MCoT）最薄弱的环节，而PR-CoT引入了专门的“伦理视角”，直接击中痛点。\n*   **消融实验：** 进一步验证，如果移除任何一个视角（如移除伦理视角），性能都会下降。从而反向证明：只有完整的“多视角”体系才能达到最优效果，缺一不可。\n\n---\n\n**总结：**\n作者的思考路径是从**发现LLM自我纠错能力的不足**出发，通过**批判现有单维反思方法的狭隘性**，进而**借鉴人类多维思维模式**提出假设，最终通过**结构化的提示工程**将反思过程分解为逻辑、信息、伦理、替代方案四个独立且互补的维度，实现了推理质量从量变到质变的飞跃。"
                },
                {
                    "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
                    "arxiv_id": "2601.07696",
                    "authors": "Nick Ferguson, Alan Bundy, Kwabena Nuamah",
                    "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了LLM在基于工具的任务中的元级推理能力，涉及任务分解（规划）和工具选择（工具使用），属于单智能体的核心能力范畴，并非纯推理研究。",
                    "summary2": "本文旨在探索大语言模型的元级推理能力。针对多跳表格问答任务，我们提出了一种基于工具的评估框架，通过比较模型工具调用与预设的“essential actions”来分析推理过程。我们在基于世界银行数据的自定义数据集上，通过最终答案准确率、精确率和召回率验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）的最新进展日益聚焦于“推理”能力，这一概念在LLM相关讨论中存在诸多重叠的定义。我们采用一种更具结构化的方法，将元级推理（meta-level reasoning，指代为解决任务所需的中间步骤进行推理的过程）与对象级推理（object-level reasoning，涉及上述步骤的底层执行）区分开来。我们设计了一项新颖的问答任务，该任务基于不同国家在不同年份的地缘政治指标数值。这些问题需要分解为中间步骤、进行数据检索以及对检索到的数据执行数学运算。我们通过考察模型为回答问题而选择合适工具的情况，来分析LLMs的元级推理能力。为了超越单纯的最终答案准确率，对LLMs进行更深入的分析，我们的任务中包含了“必要动作”，通过将LLMs的工具调用输出与这些动作进行比对，从而推断其推理能力的强弱。我们发现，LLMs在我们的任务中表现出了良好的元级推理能力，但在任务理解的某些方面仍存在缺陷。研究发现，n-shot提示（n-shot prompting）对准确率影响甚微；遇到的错误信息通常不会导致性能下降；此外，我们还提供了LLMs数理能力低下的进一步证据。最后，我们讨论了这些发现在其他任务领域的泛化性及其局限性。",
                    "inspiration_trace": "基于对论文《Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：现有“推理”评估的模糊性\n**思考起点：** 当前学术界对大语言模型（LLM）能力的讨论高度集中在“推理”这一概念上。然而，作者观察到“推理”一词在LLM语境下定义重叠且模糊（如数学推理、常识推理等）。\n**核心痛点：** 现有的基准测试（如GSM8K, MATH）大多仅关注**最终答案的准确性**。这种“黑盒”评估方式存在严重缺陷：如果模型答错了，我们无法区分是模型**没想对步骤**（规划失败），还是**算错了数**（执行失败）。\n**初步假设：** 为了真正理解LLM的推理能力，必须将“规划做什么”与“实际去做”这两个层面解耦。\n\n### 2. 理论引入：经典AI视角的二元划分\n**理论溯源：** 为了解决上述模糊性，作者回顾了符号AI和自动定理证明领域的经典理论（特别是Bundy, 1983的工作）。\n**概念界定：** 引入**元级推理**与**对象级推理**的严格区分：\n*   **元级推理：** 关于“如何解决问题”的思考，即高层规划、任务分解、步骤选择。\n*   **对象级推理：** 具体执行上述步骤的过程，如数据检索、算术计算、符号操作。\n**逻辑演进：** 作者意识到，将这一经典框架应用于LLM评估，可以将原本混在一起的“推理能力”拆解为两个可独立分析的维度，从而提供比单纯准确率更深层的诊断。\n\n### 3. 方法论构建：将思维过程“外化”\n**关键挑战：** LLM的推理过程通常隐藏在模型内部的隐状态或生成的自然语言中，难以量化评估。如何让“元级推理”变得可观测？\n**解决方案：** 利用**工具使用**范式。\n*   **逻辑支点：** 当LLM调用一个工具（如`search_indicator`或`calculate_mean`）时，它实际上是在显式地展示其“计划”。工具调用序列就是元级推理的**中间表征**。\n*   **任务设计：** 选择**多跳表格QA任务**（基于世界银行数据）。该任务天然需要将复杂问题分解为“检索数据”和“数学运算”两个子步骤，完美契合元级（规划检索与运算顺序）与对象级（实际检索与计算）的二元框架。\n\n### 4. 评估创新：从“结果导向”转向“过程导向”\n**评估困境：** 传统的QA评估只有“对/错”两种状态。但在工具使用场景下，模型可能选对了工具（元级强），但工具参数填错或计算出错（对象级弱）。\n**核心创新：** 提出**“必要动作”**的概念。\n*   **定义：** 针对每个问题，定义一组必须执行的工具调用集合。这不是唯一的“黄金路径”，而是解决问题的核心动作集。\n*   **指标构建：** 不再只看Final Answer，而是将模型生成的工具调用序列与“必要动作”进行对比，计算**精确率**和**召回率**。\n    *   **高精确率：** 模型知道该用什么工具（元级推理强）。\n    *   **低召回率：** 模型遗漏了必要步骤（规划有漏洞）。\n    *   **最终答案错误：** 可能是对象级计算错误，而非元级规划错误。\n\n### 5. 实验验证与发现：诊断模型的能力边界\n**实验设计意图：** 作者并不旨在设计一个让模型得高分的系统，而是利用这个环境作为“显微镜”来观察模型。\n**逻辑推演与验证：**\n*   **验证元级能力：** 实验发现模型在工具选择的精确率上表现良好，证明LLM具备较强的**高层规划能力**（即知道“先做什么后做什么”）。\n*   **验证对象级缺陷：** 当移除数学工具，强制模型自己计算时，性能大幅下降。这证实了LLM在**底层执行（特别是算术）**上的固有缺陷。\n*   **验证鲁棒性：** 通过引入错误信息，观察模型是否能自我修正。这进一步测试了元级推理中的“动态调整”能力。\n\n### 6. 总结：逻辑链的闭环\n作者的思考过程完成了一个闭环：\n从**现象**（LLM推理定义模糊、评估单一）出发 -> 引入**理论**（元级/对象级二分法） -> 寻找**载体**（工具调用作为思维外化的接口） -> 设计**度量**（基于必要动作的过程评估） -> 最终得出**结论**（LLM是优秀的“规划者”，但仍是蹩脚的“计算器”）。\n\n这篇文章的本质不仅仅是发布了一个数据集，而是提供了一套**解剖LLM推理能力的思维框架和手术刀（工具调用评估）**。"
                },
                {
                    "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments",
                    "arxiv_id": "2601.07606",
                    "authors": "Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",
                    "summary": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个基准（PoT），专门用于评估使用工具的智能体在科学想法判断任务上的表现，涉及工具使用和交互预算等单智能体核心能力，符合筛选条件。",
                    "summary2": "本文旨在评估模型对科学思想的判断能力及预测其未来影响力。针对科学思想评估缺乏可扩展验证方法的问题，我们提出了一种名为 Proof of Time (PoT) 的半可验证基准框架，通过冻结截止时间前的证据并在离线沙箱中预测未来结果。我们在涵盖四个领域的 30K+ 实例上，通过准确率和测试时计算缩放分析验证了其有效性，发现增加交互预算能提升智能体性能，且工具使用的效果高度依赖于任务类型。",
                    "summary_translation": "大语言模型正日益被用于评估和预测研究思路，然而，我们目前缺乏可扩展的方法来衡量模型对这些科学想法的判断质量。为实现这一目标，我们提出了 PoT，这是一个半可验证的基准测试框架，它将科学想法的判断与随后可观察到的下游信号（例如引用和研究人员议程的转变）联系起来。PoT 在离线沙箱中冻结截止前的证据快照，并要求模型预测截止后的结果，这使得在真实情况出现时能够进行可验证的评估，在无需详尽专家标注的情况下实现可扩展的基准测试，并能够针对同行评审奖项等信号分析人类与模型之间的不一致性。此外，PoT 为评估科学想法的基于智能体的研究判断提供了一个受控测试平台，能够在提示消融和预算缩放的条件下，对比使用工具的智能体与非智能体基线。在跨越四个基准领域的 30,000 多个实例中，我们发现，与非智能体基线相比，更高的交互预算通常能提升智能体的性能，而使用工具的收益则高度依赖于具体任务。通过将按时间划分的、未来可验证的目标与用于工具使用的离线沙箱相结合，PoT 支持对面向未来的科学想法判断任务中的智能体进行可扩展评估。",
                    "inspiration_trace": "基于论文《Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法设计的思考过程：\n\n### 第一阶段：宏观观察与核心矛盾\n**思考起点：科学评价的“时效性错位”**\n1.  **现象观察**：科学界评价研究创意（Idea）的主要机制（同行评审、静态基准测试）通常发生在“当下”，且高度依赖主观判断。\n2.  **核心矛盾**：真正衡量一个科学创意价值的标准是“时间的检验”（如引用量、奖项、后续研究方向的改变），但这些信号具有滞后性，无法在决策当下立即获取。\n3.  **现有困境**：大语言模型（LLM）正被用于辅助科研评价，但我们缺乏一种可扩展、客观的方法来评估模型判断“未来影响力”的能力。如果仅用静态数据集评估，无法反映模型对科学演进的预测能力。\n\n### 第二阶段：概念突破——将“时间”转化为验证机制\n**核心假设：利用历史数据模拟未来预测**\n1.  **思维转换**：既然无法真的等待未来，不如利用“过去”来模拟“未来”。如果我们将时间轴切分，设定一个截止点 $t_0$，那么对于 $t_0$ 之后的 $t_1$ 时刻，其结果在当下已经是已知的客观事实。\n2.  **方法论雏形**：\n    *   **冻结证据**：只给模型提供 $t_0$ 时刻之前的“快照”信息（如论文摘要、作者历史）。\n    *   **预测未来**：要求模型预测 $t_1$ 时刻才会发生的信号（如 $t_1$ 时刻的引用数、获奖情况）。\n    *   **事后验证**：利用现实中已经发生的 $t_1$ 结果作为“金标准”进行评分。\n3.  **优势确立**：这种方法解决了“可验证性”（标签是客观事实而非主观打分）和“可扩展性”（无需专家人工标注，数据可自动更新）的问题。\n\n### 第三阶段：控制变量——解决“智能体”评估的污染问题\n**进阶思考：如何公平地评估工具使用能力？**\n1.  **新挑战**：当前流行使用“工具调用智能体”来处理复杂任务。但现有评估往往混淆了“推理能力”与“信息获取能力”。如果允许智能体联网，它可能只是直接查到了答案，而非基于证据进行了判断。\n2.  **隔离设计**：为了纯粹测试模型基于有限证据进行推理和判断的能力，作者引入了**“离线沙盒”**概念。\n3.  **逻辑闭环**：\n    *   将智能体关在一个“断网”的房间里。\n    *   房间里只有 $t_0$ 时刻的冻结证据和本地工具（如Python、文本编辑器）。\n    *   智能体表现出的任何提升，必须归因于其对有限证据的挖掘和推理能力，而非外部信息检索。\n\n### 第四阶段：维度拆解——定义“科学创意判断”的具体内涵\n**操作化定义：从抽象概念到具体任务**\n1.  **问题细化**：“科学创意判断”是一个抽象概念，需要将其拆解为可量化的具体维度。\n2.  **四个维度的构建**：\n    *   **影响力预测**：预测未来的引用量（量化指标）。\n    *   **价值评估**：预测同行评审奖项（定性共识）。\n    *   **研究演进**：预测教授未来的研究方向（连续性与漂移）。\n    *   **技术前沿**：预测基准测试的SOTA轨迹（技术极限）。\n3.  **任务设计逻辑**：这些任务覆盖了从个人（教授）、群体（会议奖项）到领域（SOTA）不同层面的科学判断，且均符合“时间可验证”原则。\n\n### 第五阶段：实验假设与验证——探索“智能体”的边际效应\n**实证探究：智能体何时才值得？**\n1.  **对比基准**：设置“零样本”与“智能体”模式的对比，旨在验证增加工具和推理步骤是否真的有效。\n2.  **成本-收益分析**：引入“消息预算”概念，模拟测试时的计算成本。\n3.  **假设验证**：\n    *   智能体并非在所有任务上都优于直接生成。\n    *   在需要深度证据挖掘的任务（如Faculty任务）上，智能体优势明显。\n    *   在结构化预测或简单任务上，增加智能体步骤可能只是浪费算力。\n4.  **结论导向**：通过实验揭示模型在处理“未来导向”任务时的失败模式（如检索失败、推理循环），为未来改进提供方向。\n\n---\n\n**总结：作者的逻辑演进路径**\n从**“科学评价需要时间检验”**的哲学观察出发，通过**“时间切片”**的技术手段将未来预测转化为离线验证，进而引入**“离线沙盒”**以排除信息干扰，纯粹考察模型的**“证据推理能力”**，最终构建了一个多维度、可扩展的基准，回答了“AI能否判断科学创意的未来价值”这一核心问题。"
                },
                {
                    "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents",
                    "arxiv_id": "2601.07582",
                    "authors": "Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",
                    "summary": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一种针对长期对话智能体的记忆机制（ES-Mem），重点解决了记忆粒度和检索范式的问题。这属于单智能体研究范围中的“记忆”模块，旨在提升智能体在长期交互中的连贯性和适应性。",
                    "summary2": "本文旨在解决长期对话智能体中记忆粒度僵化及检索缺乏结构感知的问题。针对长期交互场景，我们提出了一种基于Event Segmentation Theory的ES-Mem框架，结合动态事件分割与边界锚定的分层记忆架构。我们在LoCoMo和LongMemEval-S基准上通过F1、BLEU-1和Accuracy等指标验证了其有效性。",
                    "summary_translation": "记忆对于对话代理在长期交互中维持连贯性并实现持续适应至关重要。尽管现有的记忆机制具备基本的存储与检索能力，但它们主要受限于两个方面：(1) 僵化的记忆粒度往往破坏语义完整性，导致记忆单元碎片化且缺乏连贯性；(2) 主流的扁平化检索范式仅依赖于表层语义相似度，忽视了在导航和定位特定情景语境时所必需的话语结构线索。为克服上述局限，受事件分割理论的启发，我们提出了ES-Mem框架，该框架包含两个核心组件：(1) 动态事件分割模块，用于将长期交互划分为具有清晰边界的语义连贯事件；(2) 分层记忆架构，通过构建多层记忆并利用边界语义来锚定特定的情景记忆，从而实现精确的语境定位。在两个记忆基准测试上的评估表明，ES-Mem相较于基线方法取得了持续的性能提升。此外，所提出的事件分割模块在对话分割数据集上也展现出了稳健的适用性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "From RAG to Agentic RAG for Faithful Islamic Question Answering",
                    "arxiv_id": "2601.07528",
                    "authors": "Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket, Mahmoud Alhirthani, Mutaz Al-Khatib, Logan Cochrane, Kareem Darwish, Rashid Yahiaoui, Firoj Alam",
                    "summary": "LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个“智能体 RAG”框架，明确使用了结构化工具调用（工具使用）以及迭代证据检索和答案修订（规划/反思），符合单智能体的研究范围。尽管应用领域是宗教问答，但其核心贡献在于智能体架构的设计与实现，而非单纯的应用。",
                    "summary2": "本文旨在解决LLM在伊斯兰问答中产生幻觉及缺乏依据的问题。针对双语伊斯兰问答场景，我们提出了一种Agentic RAG框架，利用结构化工具调用进行迭代证据检索与答案修正，并在自建的ISLAMIC FAITH QA基准数据集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（LLMs）越来越多地被应用于伊斯兰教问答领域，其中 ungrounded（缺乏事实依据的）响应可能会带来严重的宗教后果。然而，标准的 MCQ（多选题）/MRC（机器阅读理解）风格评估无法捕捉关键的 real-world（现实世界）失效模式，特别是 free-form hallucinations（自由形式的幻觉）以及模型在缺乏证据时是否能够适当地 abstain（拒绝回答）。为了揭示这一方面，我们介绍了 ISLAMICFAITHQA，这是一个包含 3,810 个项目的 bilingual（双语，阿拉伯语/英语）generative benchmark（生成式基准），具有 atomic single-gold answers（原子性单一金标准答案），能够直接测量 hallucination（幻觉）和 abstention（拒绝回答行为）。此外，我们开发了一个 end-to-end grounded Islamic modelling suite（端到端 grounded 伊斯兰教建模套件），该套件包括： 25K 个基于阿拉伯语文本的 SFT（监督微调）推理对； 5K 个用于 reward-guided alignment（奖励引导对齐）的双语 preference samples（偏好样本）； 以及一个包含约 6,000 个 atomic verses（原子性经文）的 verse-level Qur'an retrieval corpus（经文级《古兰经》检索语料库）。基于这些资源，我们开发了一个 agentic Quran-grounding framework（智能体《古兰经》 grounding 框架），该框架利用 structured tool calls（结构化工具调用）进行 iterative evidence seeking（迭代式证据搜寻）和 answer revision（答案修正）。针对 Arabic-centric（以阿拉伯语为中心）和 multilingual LLMs（多语言大型语言模型）的实验表明，retrieval（检索）能够提高 correctness（正确性），且 agentic RAG（智能体检索增强生成）在 standard RAG（标准检索增强生成）的基础上带来了最大的性能提升，即使在小模型（即 Qwen3 4B）上也能实现 state-of-the-art（最先进的）性能和更强的 Arabic-English robustness（阿拉伯语-英语鲁棒性）。我们将向社区公开实验资源和数据集。",
                    "inspiration_trace": "基于论文《From RAG to Agentic RAG for Faithful Islamic Question Answering》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题识别——高价值领域的“流利性陷阱”\n**思考起点：**\n作者首先关注到LLM在伊斯兰教问答（Islamic QA）这一高敏感、高价值领域的应用现状。\n**核心矛盾：**\n虽然LLM在语言表达上非常流利，但这种流利性掩盖了其内在的不可靠性。在宗教领域，一个看似自信但缺乏依据的回答（幻觉）不仅是一个错误，更可能导致严重的宗教误导或伦理后果。\n**初步结论：**\n现有的通用LLM在涉及教法推理、文化规范和经典依据时，存在严重的“事实性”和“忠实度”缺陷，必须建立一种能够强制模型“言之有据”的机制。\n\n### 第二阶段：现状观察与评估体系的批判\n**观察：**\n作者审视了现有的伊斯兰NLP评估基准，发现它们大多采用多选题（MCQ）或机器阅读理解（MRC）格式。\n**逻辑漏洞：**\n1.  **无法测度幻觉：** MCQ允许模型通过排除法或猜测得分，无法反映模型是否真正理解或是否在编造答案。\n2.  **缺乏“拒答”机制：** 真实的宗教咨询中，不知道答案时应选择“拒答”，但现有评估不鼓励也不测量这种审慎行为。\n**假设提出：**\n要解决忠实度问题，首先必须改变“尺子”。我们需要一个更严格的、生成式的评估基准，它必须能直接测量“幻觉”和“拒答”。\n**行动：**\n构建 **ISLAMIC FAITH QA**。这是一个包含原子性单一金答案的双语基准，采用严格的“正确/错误/未尝试”标签，迫使模型要么给出精准的基于文本的答案，要么承认无知。\n\n### 第三阶段：从参数记忆到外部检索的范式转移\n**问题深化：**\n即使有了严格的基准，作者发现仅靠模型内部的参数记忆（SFT微调）仍然无法达到高准确率，且容易产生过度的自信错误。\n**假设：**\n伊斯兰知识是密集且具体的，模型不可能记住所有细节。解决幻觉的根本路径不是“训练模型记住更多”，而是“强制模型查阅经典”。\n**初步方案（标准RAG）：**\n引入检索增强生成（RAG），将古兰经经文作为上下文提供给模型。\n**发现：**\n虽然标准RAG（一次性检索后生成）比基线模型有提升，但它仍然是被动的。如果检索到的上下文不完美，或者模型没有正确利用上下文，错误依然会发生。\n\n### 第四阶段：核心创新——从“被动检索”到“主动代理”\n**逻辑跃迁：**\n作者反思了人类学者回答宗教问题的过程：人类不是一次性读完所有资料就回答，而是**迭代式**地寻找证据、阅读经文、核实出处，然后再作答。\n**核心假设：**\n如果将检索过程从“预处理步骤”转变为“显式的决策过程”，让模型像人类学者一样主动使用工具去寻找证据，那么忠实度将大幅提升。\n**方法论确立：**\n提出 **Agentic RAG（代理式RAG）**。\n*   **区别：** 标准RAG是“Query -> 检索 -> 生成”；Agentic RAG是“Query -> 规划 -> 调用工具（搜索/阅读/元数据查询） -> 迭代 -> 生成带引用的答案”。\n*   **预期效果：** 这种结构化的工具调用迫使模型在回答前必须进行证据检查，从而减少幻觉，并提高跨语言（阿语/英语）的鲁棒性（因为证据源是统一的古兰经）。\n\n### 第五阶段：数据与方法的闭环构建\n**配套思考：**\n为了支撑上述Agentic RAG框架，仅有基准是不够的，模型需要具备“使用工具”和“基于证据推理”的能力。\n**资源构建：**\n1.  **SFT数据：** 构建25K条基于文本的推理对，训练模型学会“引用经文进行推理”的思维模式，而不仅仅是背诵答案。\n2.  **RL对齐数据：** 构建5K条偏好样本，利用LLM-as-Judge作为奖励信号，训练模型倾向于生成“有依据的、简洁的”回答，惩罚幻觉。\n3.  **检索语料库：** 将古兰经细化为原子级别的经文单元，便于工具精准调用。\n\n### 总结：逻辑演进的全貌\n作者的思考路径遵循了**“发现问题 -> 修正标准 -> 引入外部知识 -> 升级交互模式”**的闭环：\n1.  **痛点：** 宗教领域容错率低，现有模型爱“胡说八道”。\n2.  **立尺：** 建立严格基准，拒绝“蒙题”，强制要求精准和拒答。\n3.  **寻源：** 引入RAG，用古兰经作为唯一真理来源。\n4.  **拟人：** 升级为Agentic RAG，让模型学会像学者一样“主动查阅、反复核实”后再回答。\n\n最终，作者通过实验验证了这一逻辑：**Agentic RAG** 不仅超越了标准RAG，甚至能让小模型（4B）在特定任务上超越未使用该技术的大模型，证明了“思维链（工具使用）”比“参数量”在解决忠实度问题上更有效。"
                },
                {
                    "title": "GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap",
                    "arxiv_id": "2601.07375",
                    "authors": "Farzad Shami, Subhrasankha Dey, Nico Van de Weghe, Henrikki Tenkanen",
                    "summary": "The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent's execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at https://anonymous.4open.science/r/groke.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了GROKE，一个基于LLM的框架，用于导航指令评估。它涉及单智能体的核心能力：规划（子指令规划）和工具使用（利用OpenStreetMap数据进行拓扑图导航）。论文重点在于智能体的架构设计、执行轨迹和决策模式，而非纯应用或纯推理，且明确排除了视觉依赖，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决视觉依赖评估中的成本与感知误差问题。针对 Map2Seq 数据集，我们提出了一种基于 OpenStreetMap 的无视觉分层 LLM 框架 GROKE，结合子指令规划与结构化 JSON 表示进行图推理。我们在 Map2Seq 上通过 Navigation Error (NE)、Success Rate (SR) 等指标验证了其有效性，导航误差降低了 68.5%。",
                    "summary_translation": "导航指令的评估仍然是视觉语言导航 (VLN) 研究中一个长期存在的挑战。传统的基于参考的指标，如 BLEU 和 ROUGE，无法捕捉空间指令的功能效用，特别是无法衡量指令是否成功引导导航者到达预定目的地。尽管现有的 VLN 智能体可以作为评估器，但它们对高保真视觉模拟器的依赖带来了许可限制和计算成本，且感知误差进一步干扰了语言质量评估。本文介绍了 GROKE (Graph-based Reasoning over OSM Knowledge for instruction Evaluation)，这是一个基于分层大语言模型 (LLM) 的无视觉、无需训练的框架，用于利用 OpenStreetMap 数据评估导航指令。通过系统的消融实验，我们证明了空间信息的结构化 JSON 和文本格式显著优于基于网格和视觉图的表示。我们的分层架构结合了子指令规划与拓扑图导航，在 Map2Seq 数据集上，与启发式和采样基线相比，将导航误差降低了 68.5%。智能体的执行成功率、轨迹保真度和决策模式作为功能可导航性的代理指标（基于 OSM 可见地标和拓扑结构），建立了一种无视觉依赖的可扩展且可解释的评估范式。代码和数据可在 https://anonymous.4open.science/r/groke 获取。",
                    "inspiration_trace": "基于对论文《GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观问题：如何准确评估导航指令的“功能性”质量？\n**思考起点：**\n在视觉语言导航（VLN）领域，传统的评估指标（如BLEU、ROUGE）存在根本性缺陷。这些指标基于文本相似度（n-gram重叠），无法捕捉导航指令的核心价值——即“能否引导用户到达目的地”。\n*   **反例：** “在银行左转”与“在银行右转”文本相似度极高，但功能截然相反。\n*   **结论：** 评估必须从“文本相似度”转向“功能效用性”。\n\n### 2. 现状批判：现有“务实评估”路径的痛点\n**演进逻辑：**\n为了解决上述问题，学术界引入了“Agent-as-Judge”范式，即训练一个智能体在模拟器中执行指令，通过成功率来反推指令质量。\n**观察到的瓶颈：**\n这种方法严重依赖高保真的视觉模拟器（如Matterport3D、Google Street View），引入了新的噪声：\n1.  **混淆变量：** 智能体失败可能是因为视觉识别能力差（看不清“红砖墙”），而非指令本身写得不好。这导致评估结果混杂了视觉感知误差。\n2.  **成本与壁垒：** 视觉数据昂贵、版权受限、计算量大，限制了评估的可扩展性。\n\n### 3. 核心假设：能否剥离视觉，仅基于“语义与拓扑”进行评估？\n**思维跃迁：**\n导航的本质是空间推理，而非像素识别。人类在阅读导航指南（如地图）时，依赖的是地标（POI）、方向和拓扑连接，而非实景照片。\n**假设提出：**\n如果我们将环境抽象为符号化的地图数据（如OpenStreetMap），构建一个“无视觉”的评估智能体，是否既能保留功能性评估的优势，又能消除视觉噪声和成本问题？\n*   **数据基础：** Map2Seq数据集提供了OSM数据（节点、边、POI），为这一假设提供了实验土壤。\n\n### 4. 方法论探索：如何让大语言模型（LLM）“看懂”地图？\n**技术挑战：**\n既然决定使用LLM作为推理核心，如何将图结构的空间数据转化为LLM能高效理解的输入？\n**实验与试错（Ablation Studies驱动的设计）：**\n作者对比了四种空间表征形式，试图寻找最优解：\n1.  **网格/矩阵：** 模仿视觉像素。结果发现LLM难以解析这种高密度的ASCII字符，效果最差。\n2.  **可视化图：** 使用Graphviz风格。虽然直观，但LLM处理箭头和图形符号的推理能力不如处理结构化数据。\n3.  **纯文本描述：** 自然语言描述连接关系。效果尚可，但在复杂路径上信息密度不足，导致认知负荷过高。\n4.  **结构化JSON（最终选择）：** 将节点、边、POI组织为层级化的JSON。\n    *   **逻辑判断：** JSON格式既保留了机器可读的结构，又符合LLM预训练数据中的代码/结构化文本模式，能显著提升推理效率和准确性。\n\n### 5. 架构优化：如何处理长程导航的复杂性？\n**问题分解：**\n直接让LLM根据整段长指令在地图上一步步走，容易迷失目标或产生累积误差。\n**灵感来源：** 人类认知习惯——将复杂任务拆解为子目标。\n**架构设计：**\n提出**分层架构**：\n1.  **子指令代理：** 负责高层规划，将长指令拆解为原子动作（如“直走”、“左转”）并提取关键地标。\n2.  **导航代理：** 负责底层执行，仅关注当前子目标在局部地图（可见区域）内的实现。\n*   **逻辑优势：** 这种解耦降低了单次推理的复杂度，使得智能体能更专注于当前的局部决策，同时保持全局目标的一致性。\n\n### 6. 最终验证：这种“无视觉”评估是否有效？\n**闭环思考：**\n如果智能体没有眼睛，它的成功是否真的代表了指令的质量？\n**验证逻辑：**\n通过相关性分析，将GROKE的导航指标（如导航误差NE、成功率SR）与人类对指令清晰度的评分进行对比。\n*   **结果：** 两者呈现显著相关性。证明了一个基于逻辑和拓扑的智能体，足以作为指令质量的可靠代理指标，从而建立了一种**可扩展、可解释且无视觉依赖**的评估新范式。\n\n---\n\n**总结：**\n作者的思考路径是从**评估指标的失效**出发，批判了**视觉依赖的局限性**，提出了**基于OSM图推理的“无视觉”假设**，并通过**对比实验确定了JSON作为最优的空间表征**，最终利用**分层代理架构**实现了高效、准确的指令评估。"
                },
                {
                    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
                    "arxiv_id": "2601.07348",
                    "authors": "Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang",
                    "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了“受控自我演化”（CSE）方法，通过“生成-验证-优化”的迭代循环实现自我完善，符合“自我演化”的研究范围。同时，文中提到的“多样化规划初始化”和“分层演化记忆”分别对应单智能体的“规划”和“记忆”能力。",
                    "summary2": "本文旨在解决现有自进化方法在算法代码优化中探索效率低下的问题。针对代码生成任务，我们提出了一种Controlled Self-Evolution (CSE)框架，通过多样化规划初始化、遗传进化及分层进化记忆提升搜索效率。在EffiBench-X基准上，通过Execution-Time ratio (ET)、Memory-Peak ratio (MP)和Memory-Integral ratio (MI)指标验证了其有效性，CSE在多种LLM主干网络上均表现出更优的算法优化能力。",
                    "summary_translation": "自进化方法通过迭代的“生成-验证-优化”循环来增强代码生成，然而现有方法存在探索效率低下的问题，无法在有限的预算内发现具有更优复杂度的解决方案。这种低效性源于初始化偏差导致进化陷入劣质解区域、缺乏反馈引导的不可控随机操作，以及跨任务经验利用不足。为解决这些瓶颈，我们提出了受控自进化，该方法包含三个关键组件。多样化规划初始化生成结构各异的算法策略，以实现广泛的解空间覆盖。遗传进化用反馈引导机制替代随机操作，从而实现定向突变和组合交叉。分层进化记忆在任务间和任务内层面捕获成功与失败的经验。在 EffiBench-X 上的实验表明，CSE 在各种 LLM backbones (大语言模型骨干) 上均持续优于所有 baselines (基线模型)。此外，CSE 在早期代即展现出更高的效率，并在整个进化过程中保持持续改进。我们的代码已在 https://github.com/QuantaAlpha/EvoControl 公开。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
                    "arxiv_id": "2601.07264",
                    "authors": "Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya",
                    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究“tool-use agents”（工具使用智能体）和“agentic workflows”（智能体工作流），分析了智能体在使用不同类型工具时的校准问题（属于自我反思/自我意识范畴），并提出了强化学习微调框架以优化智能体表现。这完全符合“单智能体：工具使用”和“自我反思”的研究范围。",
                    "summary2": "本文旨在解决Tool-use agents中的miscalibration问题。针对Evidence tools导致overconfidence的场景，我们提出了Calibration Agentic RL (CAR)框架，利用Margin-Separated Calibration Reward (MSCR)联合优化任务准确性与校准。我们在NQ、HotpotQA、SimpleQA-verified及AIME、MATH-500数据集上，通过Accuracy、ECE、Brier Score和AUROC验证了其有效性，显著提升了模型的校准能力与泛化性。",
                    "summary_translation": "基于大语言模型 (LLMs) 的自主代理正在快速发展以处理多轮任务，但确保其可信度仍然是一个关键挑战。这种可信度的一个基本支柱是校准，它指的是代理表达能够可靠反映其实际性能的置信度的能力。尽管校准在静态模型中已有深入研究，但其在集成工具的代理工作流中的动态变化仍未被充分探索。在这项工作中，我们系统地调查了工具使用代理中的语言化校准，揭示了由工具类型驱动的基本置信度二分法。具体而言，我们的试点研究表明，证据工具（如 web search）由于检索信息中固有的噪声，会系统性地导致严重的过度自信，而验证工具（如 code interpreters）可以通过确定性反馈来锚定推理并减轻校准偏差。为了在不同工具类型间稳健地提升校准性能，我们提出了一种强化学习 (RL) 微调框架，该框架联合优化任务准确性和校准性能，并得到了全面的奖励设计基准的支持。我们证明，经过训练的代理不仅实现了卓越的校准性能，而且表现出从本地训练环境到嘈杂的网络设置以及数学推理等不同领域的稳健泛化能力。我们的结果强调了针对工具使用代理采用特定领域校准策略的必要性。更广泛地说，这项工作为构建能够在高风险的现实世界部署中可靠地传达不确定性的自我感知代理奠定了基础。",
                    "inspiration_trace": "基于论文《The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents》，以下是对作者核心方法论产出逻辑链的系统推演：\n\n### 1. 宏观背景与核心矛盾：从“能力”到“可信度”\n**思考起点：**\n随着大语言模型（LLM）向智能体演进，工具使用（如搜索、代码解释器）极大地扩展了模型的能力边界。然而，作者敏锐地捕捉到了一个被忽视的关键问题：**信任危机**。\n**逻辑推演：**\n*   现有研究多关注智能体“能不能做”，而忽略了“知不知道自己能不能做”。\n*   在高风险场景下，智能体的**校准**能力——即其表达的置信度与实际表现的一致性——是可信度的基石。\n*   **初步观察：** 现有文献指出，引入工具后，智能体往往比静态模型表现出更严重的过度自信。这引发了一个根本性的疑问：**工具使用本身是否就是导致校准失效的元凶？**\n\n### 2. 深入探究与假设提出：打破“工具”的刻板印象\n**思考转折：**\n作者没有接受“工具导致过度自信”这一笼统结论，而是试图解构“工具”这一概念。\n**逻辑推演：**\n*   **假设：** 并非所有工具都对校准产生相同影响。工具的**性质**（反馈机制、输出确定性）可能决定了其对置信度的不同影响。\n*   **分类维度：** 作者将工具划分为两类典型范式：\n    1.  **证据工具：** 如网络搜索。特征是输出开放、充满噪声、缺乏明确的负向反馈（搜索总是有结果的，无论是否相关）。\n    2.  **验证工具：** 如代码解释器。特征是输出确定、提供执行反馈（代码会报错），能提供逻辑上的“落地”。\n\n### 3. 验证与发现：揭示“置信度二分法”\n**思考过程：**\n通过设计对比实验（直接提示 vs. 工具使用 vs. RL微调），作者验证了上述假设，发现了核心现象——**置信度二分法**。\n**逻辑推演：**\n*   **证据工具的陷阱：** 在使用网络搜索时，智能体表现出严重的过度自信。原因在于“检索行为”本身被模型误认为是“尽职调查”，且检索到的噪声信息被误认为确凿证据，导致虚假的确定性。\n*   **验证工具的锚定：** 在使用代码解释器时，智能体的校准度反而提升。因为确定性的执行反馈（如报错信息）为推理过程提供了现实约束，抑制了盲目的自信。\n*   **结论：** 校准失效并非工具使用的普遍后果，而是特定于**证据工具**带来的噪声干扰。这指明了后续研究的靶心：**如何修复证据工具导致的过度自信？**\n\n### 4. 方法论构建：从“提示工程”到“内在校准”\n**思考转折：**\n既然证据工具的噪声无法完全消除，且简单的提示工程无法解决根本问题（实验表明Prompting-based策略依然失效），作者转向通过训练来改变模型的内在置信度生成机制。\n**逻辑推演：**\n*   **技术选型：** 采用强化学习（RL）进行微调，因为智能体本身就是通过RL训练来使用工具的，这能保持任务能力的连贯性。\n*   **核心挑战：** 如何设计奖励函数？传统的奖励仅关注任务准确性，这往往鼓励模型“瞎猜”或过度自信。引入校准项（如Brier Score）虽然能惩罚置信度偏差，但存在一个隐患：**激励重叠**。\n\n### 5. 核心创新：解决“安全失败”的激励冲突\n**思考深化：**\n作者深入分析了现有校准奖励（如RLCR）的缺陷，发现了一个逻辑漏洞：如果对“低置信度的错误回答”给予过高的奖励（因为它诚实），模型可能会学会“安全失败”——即为了获得校准分而故意降低置信度，甚至放弃尝试正确回答。\n**逻辑推演：**\n*   **设计原则：** 必须建立严格的优先级。**“做对”必须永远优于“做错”**，无论置信度如何。\n*   **方案提出：** **边际分离校准奖励**。\n    *   **机制：** 强制将奖励空间划分为两个互不重叠的区域。所有正确答案的奖励下限，必须高于所有错误答案的奖励上限。\n    *   **效果：** 这消除了模型通过“诚实但错误”来投机取巧的动机，迫使模型在追求正确性的前提下，再去优化置信度的表达。\n\n### 6. 验证与泛化：从实验室到现实世界\n**思考闭环：**\n为了证明CAR框架不仅仅是过拟合训练数据，作者设计了更具挑战性的验证场景。\n**逻辑推演：**\n*   **环境泛化：** 从干净的本地检索环境迁移到充满噪声的真实API环境（如Serper API）。结果证明，模型学到的不是死记硬背的特定置信度值，而是一种对不确定性的感知能力。\n*   **领域泛化：** 将该方法应用于数学推理（验证工具场景）。虽然验证工具本身有助于校准，但CAR框架依然能带来额外提升，证明了该方法的通用性。\n\n### 总结：思想演进脉络\n1.  **观察：** 智能体越强，越容易盲目自信（可信度危机）。\n2.  **质疑：** 是所有工具都导致盲目自信吗？\n3.  **发现：** 只有“证据工具”（如搜索）因噪声导致过度自信，而“验证工具”（如代码）反而能锚定置信度（二分法）。\n4.  **定位：** 重点解决证据工具场景下的校准问题。\n5.  **洞察：** 现有的校准训练方法存在“安全失败”的漏洞，可能鼓励模型“躺平”。\n6.  **解决：** 提出CAR框架与MSCR奖励，通过严格分离正确与错误的奖励边界，迫使模型在追求准确的同时学会表达不确定性。"
                },
                {
                    "title": "LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents",
                    "arxiv_id": "2601.06973",
                    "authors": "Davide Baldelli, Ali Parviz, Amal Zouaq, Sarath Chandar",
                    "summary": "As LLMs move from text completion toward autonomous agents, they remain constrained by the standard chat interface, which lacks private working memory. This raises a fundamental question: can agents reliably perform interactive tasks that depend on hidden state? We define Private State Interactive Tasks (PSITs), which require agents to generate and maintain hidden information while producing consistent public responses. We show theoretically that any agent restricted to the public conversation history cannot simultaneously preserve secrecy and consistency in PSITs, yielding an impossibility theorem. To empirically validate this limitation, we introduce a self-consistency testing protocol that evaluates whether agents can maintain a hidden secret across forked dialogue branches. Standard chat-based LLMs and retrieval-based memory baselines fail this test regardless of scale, demonstrating that semantic retrieval does not enable true state maintenance. To address this, we propose a novel architecture incorporating an explicit private working memory; we demonstrate that this mechanism restores consistency, establishing private state as a necessary component for interactive language agents.",
                    "category": "cs.CL",
                    "filter_reason": "该论文研究语言智能体的架构局限性，重点探讨了单智能体的“记忆”机制（私有工作记忆），并提出了新的架构组件以解决智能体在交互任务中维护隐藏状态的问题，符合单智能体研究范围。",
                    "summary2": "本文旨在解决LLM在交互任务中无法维护隐藏状态的问题。针对Private State Interactive Tasks (PSITs)，我们提出了一种引入显式Private Working Memory的架构，包含自主代理和工作流两种实现。我们在Hangman和Diagnosis Simulator任务上，通过Self-Consistency Testing Protocol验证了其有效性，结果显示该方法显著优于现有检索基线，在保持低Token开销的同时实现了近乎完美的状态一致性。",
                    "summary_translation": "随着 LLMs (Large Language Models，大语言模型) 从文本补全向 autonomous agents (自主代理) 演进，它们仍受限于缺乏 private working memory (私有工作记忆) 的 standard chat interface (标准聊天界面)。这引发了一个根本性问题：agents (代理) 是否能够可靠地执行依赖于 hidden state (隐藏状态) 的 interactive tasks (交互任务)？我们定义了 Private State Interactive Tasks (PSITs，私有状态交互任务)，该任务要求 agents (代理) 在生成一致的 public responses (公共响应) 的同时，生成并维护 hidden information (隐藏信息)。我们从理论上证明，任何仅限于 public conversation history (公共对话历史) 的 agent (代理) 都无法在 PSITs 中同时实现保密性和一致性，从而得出了一个 impossibility theorem (不可能性定理)。为了实证验证这一局限性，我们引入了一种 self-consistency testing protocol (自一致性测试协议)，用于评估 agents (代理) 是否能在 forked dialogue branches (分叉对话分支) 中维护一个 hidden secret (隐藏秘密)。无论规模大小，standard chat-based LLMs (标准基于聊天的 LLMs) 和 retrieval-based memory baselines (基于检索的记忆基线) 均未通过该测试，这表明 semantic retrieval (语义检索) 并不能实现真正的 state maintenance (状态维护)。为解决这一问题，我们提出了一种包含 explicit private working memory (显式私有工作记忆) 的 novel architecture (新颖架构)；我们证明该机制能够恢复一致性，从而确立了 private state (私有状态) 作为 interactive language agents (交互语言代理) 必要组件的地位。",
                    "inspiration_trace": "基于论文《LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察：从“文本补全”到“自主代理”的接口错位\n**思考起点**：随着LLM从单纯的文本生成器演变为具备工具使用和规划能力的“自主代理”，一个根本性的矛盾浮出水面——**代理的底层架构依然停留在“标准聊天接口”上**。\n**逻辑推演**：聊天接口本质上是“公开”的，所有的上下文对用户和模型都是可见的。然而，真正的智能代理在执行复杂任务（如博弈、谈判、角色扮演）时，往往需要维护一个“私有”的内部状态（如策略、秘密、未公开的推论）。\n**核心疑问**：现有的仅依赖公开对话历史的架构，是否足以支撑需要“隐藏状态”的交互任务？\n\n### 2. 现象聚焦：Hangman游戏中的“失忆”现象\n**具体案例**：作者选择了一个极简但极具代表性的任务——Hangman（猜词游戏）。\n**观察发现**：当LLM作为主持人（Host）时，它虽然能“假装”选了一个词，但在下一轮对话中，它实际上并没有记住这个词。它是在根据当前的约束条件（如“这个词没有字母A”）实时“编造”一个合理的词，而不是在验证一个固定的词。\n**初步结论**：这不仅仅是模型的幻觉问题，而是**架构层面的“无状态性”**。模型在生成响应后，其内部的思维链被丢弃，导致无法在多轮交互中维持一个动态生成的私有变量。\n\n### 3. 理论抽象：定义PSIT与证明“不可能定理”\n**概念定义**：为了将这一现象理论化，作者定义了**私有状态交互任务**。这类任务要求代理必须生成并维护一个隐藏的秘密，同时根据该秘密给出一致的公开回应。\n**逻辑推演**：作者提出了**仅公开聊天代理**的概念，即输出仅依赖于公开历史 $H_t$。\n**核心定理（Impossibility Theorem）**：作者从逻辑上证明了，如果一个代理只能访问公开历史，那么它**无法同时满足“保密性”和“一致性”**。\n*   *一致性*要求输出必须基于固定的秘密 $s$。\n*   *保密性*要求公开历史不能唯一确定 $s$。\n*   *矛盾点*：如果历史 $H_t$ 对多个可能的秘密（$s$ 和 $s'$）都是兼容的，那么基于 $H_t$ 生成的输出分布 $\\pi$ 必须同时满足这两个秘密的规则。当这两个秘密要求不同的输出时，$\\pi$ 必然失效。\n**结论**：现有的标准聊天架构在结构上无法解决PSITs。\n\n### 4. 假设提出：引入“私有工作记忆”的必要性\n**解决方案假设**：既然公开上下文无法承载私有状态，那么必须在架构中引入一个**独立于公开对话历史的私有工作记忆**。\n**设计思路**：这个记忆空间应当是持久的、对用户不可见的，并且能够被模型动态更新。它不仅仅是检索过去的对话（RAG），而是存储当前生成的“认知状态”。\n\n### 5. 验证方法：设计“自一致性测试协议”\n**如何验证假设？** 传统的问答无法检测模型是否真的“记住”了秘密。\n**创新设计**：作者提出了**对话分叉测试**。\n*   在交互进行到某一步时，将对话“分叉”。\n*   在分支中，询问模型：“秘密是词A吗？”以及“秘密是词B吗？”（A和B都符合当前的公开约束）。\n*   **判据**：如果模型同时肯定了A和B，说明它没有固定的私有状态（失败）；如果它只肯定了最初选定的那个词，说明它成功维护了私有状态（成功）。\n\n### 6. 方法论演进：从“自主代理”到“工作流”的架构选择\n**架构对比**：为了实现私有记忆，作者对比了两种范式：\n1.  **自主代理**：让LLM自己决定何时调用记忆工具。\n2.  **工作流**：强制执行一个确定性的两步流程（生成公开回复 -> 更新私有记忆）。\n**实验发现**：实验表明，**工作流**的表现优于自主代理。\n**逻辑解释**：自主代理将“何时更新记忆”的决策权交给LLM，增加了不确定性；而工作流将记忆更新固化为系统级操作，确保了状态更新的可靠性，从而解决了“生成-保留”的闭环问题。\n\n### 7. 核心洞察：区分“检索”与“保留”\n**最终升华**：作者通过实验发现，现有的RAG、Mem0等记忆增强方法（基于向量检索）在Hangman任务上全部失败。\n**逻辑总结**：这揭示了**“生成-保留鸿沟”**。\n*   现有的记忆系统是**被动检索型**的，用于回忆过去发生的事实。\n*   PSITs需要的是**主动保留型**的，用于存储模型自己生成的、尚未公开的中间状态。\n**结论**：私有工作记忆不是锦上添花的功能，而是构建具备一致性和保密性的交互式语言智能体的**必要组件**。"
                },
                {
                    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
                    "arxiv_id": "2601.06966",
                    "authors": "Haonan Bian, Zhiyuan Yao, Sen Hu, Zishan Xu, Shaolei Zhang, Yifu Guo, Ziliang Yang, Xueran Han, Huacan Wang, Ronghao Chen",
                    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
                    "category": "cs.CL",
                    "filter_reason": "论文专注于LLM智能体的核心能力之一“记忆”，旨在评估智能体在长期项目中的交互一致性。此外，其数据生成流程涉及“多智能体对话生成”，符合单智能体（记忆）及多智能体的研究范围。",
                    "summary2": "本文旨在解决现有LLM记忆基准难以评估长期项目导向交互的问题。针对现实世界中动态演进的项目场景，我们提出了RealMem基准及其包含项目基础构建、多智能体对话生成及记忆日程管理的三阶段合成管道。我们在包含11个场景、2000+跨会话对话的RealMem数据集上，通过Recall@k、NDCG@k及QA Score等指标验证了其有效性，揭示了现有系统在动态状态管理上的不足。",
                    "summary_translation": "随着 Large Language Models (LLMs，大语言模型) 从静态对话接口演变为 autonomous general agents (自主通用智能体)，有效的 memory (记忆) 对于确保 long-term consistency (长期一致性) 至关重要。然而，现有的 benchmarks (基准测试) 主要关注 casual conversation (日常闲聊) 或 task-oriented dialogue (任务导向对话)，未能捕捉到 **“long-term project-oriented” (长期项目导向)** 的交互，在此类交互中，agents (智能体) 必须跟踪 evolving goals (不断演进的目标)。为了弥合这一差距，我们介绍了 **RealMem**，这是首个 grounded in realistic project scenarios (基于现实项目场景) 的 benchmark (基准测试)。RealMem 包含跨越 11 个场景的 2,000 多个 cross-session dialogues (跨会话对话)，并利用 natural user queries (自然用户查询) 进行评估。我们提出了一个 synthesis pipeline (合成流程)，该流程整合了 Project Foundation Construction (项目基础构建)、Multi-Agent Dialogue Generation (多智能体对话生成) 以及 Memory and Schedule Management (记忆与日程管理)，以模拟 memory (记忆) 的 dynamic evolution (动态演变)。实验表明，当前的 memory systems (记忆系统) 在管理现实世界项目中固有的 long-term project states (长期项目状态) 和 dynamic context dependencies (动态上下文依赖) 方面面临重大挑战。我们的代码和数据集可在 [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench) 获取。",
                    "inspiration_trace": "基于论文《RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体产出的思考过程：\n\n### 1. 宏观观察：从“对话”到“代理”的范式转变\n**思考起点**：作者首先注意到大语言模型（LLMs）的发展趋势正在发生质变。模型不再仅仅是简单的“聊天机器人”，而是正在演变为能够长期协作的“自主智能体”。\n**核心洞察**：在这种新范式下，**“记忆”** 成为了决定性因素。没有有效的记忆，智能体就无法在长期交互中保持一致性，也无法实现真正的个性化与通用人工智能（AGI）。\n\n### 2. 问题聚焦：现有基准的“静态”局限\n**观察现状**：作者审视了现有的记忆评估基准（如 LoCoMo, LongMemEval, HaluMem），发现它们大多存在一个共同的缺陷：**过于“静态”和“孤立”**。\n*   **LoCoMo** 仅关注社交闲聊。\n*   **LongMemEval** 类似于“大海捞针”测试，侧重于孤立的事实检索。\n**提出质疑**：这些基准测试的是“你记住了某个事实吗？”，而不是“你能否在复杂、变化的环境中利用记忆推进项目？”。它们无法反映真实世界中**长期、跨会话、目标导向**的交互逻辑。\n\n### 3. 核心假设：真实交互是“项目导向”的\n**定义新范式**：作者提出，真实世界的记忆驱动交互应当属于第三种范式——**“长期项目导向交互”**。\n**提炼特征**：为了构建这一新范式，作者抽象出了四个关键特征，这也是后续方法设计的指导原则：\n1.  **内生性查询**：问题源于任务进展，而非孤立的事实核查。\n2.  **交错分布**：对话在多个项目间穿插（如健身与旅行计划交替进行）。\n3.  **动态状态演化**：环境非静止，记忆需随状态（如受伤、计划变更）同步更新。\n4.  **主动上下文对齐**：智能体需利用记忆主动推断模糊意图，而非被动应答。\n\n### 4. 方法构建：如何模拟“动态演化”？\n**面临的挑战**：如何获取包含数千次跨会话对话、且具有复杂逻辑一致性的真实数据？显然，人工标注不现实，现有数据集也不存在。\n**解决思路**：作者决定采用**合成数据**的方法，但必须解决“长期生成容易逻辑崩塌”的问题。为此，设计了一个**三阶段合成流水线**：\n\n*   **阶段一：项目基础构建**\n    *   *思考*：先搭骨架，再填血肉。\n    *   *逻辑*：先定义用户画像和项目目标，再生成“蓝图”和“事件列表”。这确保了全局逻辑的连贯性，防止后续对话跑偏。\n\n*   **阶段二：多智能体对话生成**\n    *   *思考*：模拟真实博弈，而非单向生成。\n    *   *逻辑*：引入“用户智能体”和“助手智能体”。用户智能体只能看到当前会话摘要（模拟人类遗忘），助手智能体拥有完整记忆。这种不对称信息设置迫使模型必须依赖记忆机制来维持对话。\n\n*   **阶段三：记忆与日程管理**\n    *   *思考*：形成闭环反馈，确保记忆“活着”。\n    *   *逻辑*：对话生成后，通过专门的代理提取记忆点、更新日程表、去重。这些更新后的记忆又会作为下一轮对话的上下文输入。这模拟了记忆随时间动态演化的过程。\n\n### 5. 评估洞察：从“检索”到“状态管理”\n**重新定义评估标准**：作者意识到，传统的检索指标（如 Recall）不足以衡量项目导向任务。\n**逻辑推演**：在复杂项目中，**精确度**比**召回率**更重要。如果检索到了大量相关但充满噪音的信息，反而会干扰模型决策。\n**新指标设计**：因此，作者引入了基于 LLM 的语义评估（如 Mem Recall, Mem Helpful）和 QA Score，重点考察模型是否正确利用了**动态状态**，而不仅仅是生成了流畅的文本。\n\n### 6. 最终产出：RealMem 的诞生\n**结论验证**：通过实验，作者发现现有的 SOTA 记忆系统（如 Mem0, MemoryOS）在处理动态更新和主动对齐时依然表现不佳，证明了该基准的有效性和挑战性。\n**价值定位**：RealMem 不仅仅是一个数据集，它是一个**诊断工具**，揭示了当前智能体在处理长期、复杂、动态项目时的核心瓶颈，迫使社区从“静态知识库”向“动态状态管理器”转变。\n\n---\n\n**总结**：作者的思考路径是从**“智能体需要长期记忆”**这一宏观趋势出发，通过批判现有基准的**“静态性”**，提出了**“项目导向”**的动态交互假设，进而通过**“分层合成+闭环反馈”**的方法论解决了数据构建难题，最终建立了一套能够真实反映智能体动态记忆管理能力的评估体系。"
                },
                {
                    "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
                    "arxiv_id": "2601.06922",
                    "authors": "Tianhua Zhang, Kun Li, Junan Li, Yunxiang Li, Hongyin Luo, Xixin Wu, James Glass, Helen Meng",
                    "summary": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“Agentic RAG”，将问答定义为推理与检索的多步交互（属于单智能体的工具使用与规划范畴），并提出基于强化学习的框架通过过程监督优化智能体决策（属于自我演化范畴），符合筛选标准。",
                    "summary2": "本文旨在解决Agentic RAG中仅结果监督的强化学习面临的信用分配难题。针对多步推理与检索交互场景，我们提出了一种名为TREE PS-RAG的在线树结构强化学习框架，该方法将推理过程建模为树，利用蒙特卡洛估计从后代结果中推导步骤优势，无需中间标注。在七个QA基准上通过Exact Match指标验证，其性能显著优于现有基线。",
                    "summary_translation": "代理式检索增强生成将问答任务构建为推理与信息检索之间的多步交互过程，并近期通过基于结果的监督强化学习得到了推进。尽管行之有效，但仅依赖稀疏的最终奖励限制了逐步信用分配，且对中间推理和动作的指导作用较弱。近期的研究探索了过程级监督，但通常依赖于离线构建的训练数据（存在分布偏移的风险），或者需要高昂成本的中间标注。本文提出了 TreePS-RAG，这是一种用于代理式 RAG 的在线、基于树的强化学习框架，能够在保留标准仅基于结果奖励的同时实现逐步信用分配。我们的核心思想是将代理式 RAG 的推理过程建模为一棵推演树，其中每个推理步骤自然地映射为一个节点。这种树结构允许通过对其后代结果进行蒙特卡洛估计来估算步骤效用，从而在无需中间标签的情况下获得细粒度的过程优势。为了使该范式具有实用性，我们引入了一种高效的在线树构建策略，能够在受限的计算预算下保持探索多样性。在与 Search-R1 等强基线相当的推演成本下，在多个模型规模的七个多跳和通用问答基准上进行的实验表明，TreePS-RAG 始终显著优于基于结果监督和领先的过程监督强化学习方法。",
                    "inspiration_trace": "基于论文《TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG》的内容，以下是对作者核心方法论产出过程的逻辑推演与还原：\n\n### 第一阶段：问题锚定——从“结果导向”到“过程黑箱”的困境\n\n**1. 宏观观察：**\n作者首先关注到 Agentic RAG（智能体检索增强生成）已成为解决复杂多跳问答的主流范式。为了优化这一过程，学术界引入了强化学习（RL），特别是像 Search-R1 这样的方法，利用最终答案的正确性作为奖励信号来训练智能体。\n\n**2. 痛点识别：**\n然而，作者敏锐地发现这种仅依赖“结果监督”的方法存在一个核心缺陷：**信用分配难题**。\n*   **逻辑推演：** 在一个多步推理的轨迹中，如果最终答案错误，RL 算法通常会对所有步骤进行“连坐”惩罚。但实际上，可能只有中间某一步的检索或推理是致命的，其他步骤可能是正确的。\n*   **结论：** 稀疏且滞后的最终奖励无法提供细粒度的指导，限制了智能体学习高效搜索和推理策略的能力。\n\n### 第二阶段：路径探索——理想与现实的博弈\n\n**1. 提出假设：**\n既然结果监督太粗糙，那么“过程监督”显然是更好的选择。即，对每一个中间步骤（如搜索查询、推理片段）都给出一个即时奖励。\n\n**2. 现实阻碍：**\n作者审视了现有的过程监督方案，发现了两个不可忽视的障碍：\n*   **标注成本高：** 获取高质量的中间步骤标注（如每一步的子问题是否正确）极其昂贵。\n*   **分布偏移：** 许多方法（如 ReasonRAG）依赖离线构建的数据集进行训练。这意味着智能体是在“静态”的过去数据上学习，而非在“动态”的在线交互中学习，导致模型在面对新环境时泛化能力下降。\n\n**3. 核心矛盾：**\n我们需要**细粒度的过程信号**，但我们必须在**无中间标注**且**在线**的约束下获得它。\n\n### 第三阶段：核心洞察——将“树”转化为“自监督工具”\n\n**1. 思维跃迁：**\n如何在不依赖外部标注者的情况下评估一个中间步骤的好坏？作者借鉴了蒙特卡洛树搜索（MCTS）的思想，提出了一个关键假设：\n*   **假设：** 如果一个中间步骤（节点）是好的，那么从该步骤出发，通过多次随机探索（rollout），最终得到正确答案的概率应该很高。\n\n**2. 结构化建模：**\n基于上述假设，作者将 Agentic RAG 的推理过程重新定义为**树结构**，而非线性的轨迹。\n*   **逻辑映射：** 每一个推理步骤对应树上的一个节点。从根节点到叶节点的路径代表一条完整的推理轨迹。\n*   **价值反推：** 不需要人为给中间步骤打分。只需看该节点下的所有“子孙”叶节点（最终结果）的平均奖励。如果后代大多答对了，那么这个中间节点的价值就高。\n\n**3. 解决矛盾：**\n这种方法巧妙地绕过了“标注”和“离线”的障碍：\n*   **无标注：** 价值估计完全基于易于获取的最终答案。\n*   **在线性：** 树是在训练过程中实时构建和探索的，完全符合在线 RL 的范式。\n\n### 第四阶段：工程落地——在有限预算下驯服“指数爆炸”\n\n**1. 新的挑战：**\n虽然树结构在理论上完美，但在实际计算中，随着深度增加，节点数量会呈指数级爆炸。如果无限制地展开树，计算成本将不可接受。\n\n**2. 约束设定：**\n作者设定了一个硬性约束：**计算成本必须与传统的线性采样方法（如 Search-R1）相当**。即，总采样节点数 $N$ 必须固定。\n\n**3. 策略优化：**\n为了在固定预算 $N$ 下最大化树的效用，作者引入了两个关键机制：\n*   **动态分支控制：** 不再平均用力，而是根据当前层的节点数量动态分配下一层的分支数，确保总节点数维持在预算 $N$ 附近。\n*   **语义剪枝：** 作者意识到，如果两个搜索步骤检索到的文档高度重合，那么它们就是冗余的。为了在有限预算下探索更多可能性，必须去除冗余。\n    *   **逻辑：** 利用检索文档的 Jaccard 相似度来衡量节点间的语义距离，通过聚类保留多样化的路径，剔除重复探索。\n\n### 第五阶段：方法闭环——从树结构到 RL 优化\n\n**1. 信号生成：**\n通过上述构建的树，作者计算出了每个节点的“过程优势”。\n*   **全局优势：** 当前步骤相对于根节点（整体平均水平）的提升。\n*   **局部优势：** 当前步骤相对于其父节点（上一步）的提升。\n\n**2. 训练整合：**\n最后，将这些树结构推导出的细粒度优势值，无缝集成到标准的策略梯度算法（如 GRPO/PPO）中。\n*   **逻辑：** 模型不再只对最终答案负责，而是对树中每一个经过的推理步骤负责。这使得模型能够精确地学习到“哪一步检索是关键的”、“哪一步推理是多余的”。\n\n---\n\n**总结：**\n作者的思考路径是从**发现稀疏奖励的局限性**出发，试图引入过程监督但受限于**标注成本和分布偏移**，最终通过**树结构建模**将“最终结果”转化为“中间步骤的价值估计”，并利用**剪枝策略**解决了计算复杂度问题，从而实现了一种无需标注、在线且高效的 Agentic RAG 训练框架。"
                },
                {
                    "title": "AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents",
                    "arxiv_id": "2601.06818",
                    "authors": "Xuannan Liu, Xiao Yang, Zekun Li, Peipei Li, Ran He",
                    "summary": "As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个针对基于LLM的智能体的基准，专注于评估智能体工作流中的幻觉归因。它明确涵盖了智能体组件，如规划、检索和工具使用，这些属于研究范围。虽然它涉及可靠性，但它是专门针对智能体轨迹的，而不是通用的安全/对齐。",
                    "summary2": "本文旨在解决LLM-based agents在多步工作流中难以定位和解释幻觉起源的问题。针对多步Agent轨迹，我们提出了一种**automated hallucination attribution**新任务，并构建了包含693条高质量轨迹及系统化分类法的**AgentHallu** benchmark。我们在该数据集上评估了13个主流LLM，通过**step localization accuracy**和**G-EVAL scores**验证了其有效性。实验表明，即使是顶尖模型（如Gemini-2.5-Pro）在步骤定位上也仅达到41.1%的准确率，凸显了该任务的挑战性。",
                    "summary_translation": "由于基于大语言模型的智能体在执行顺序多步推理时，中间步骤产生的幻觉存在沿轨迹传播的风险，从而降低整体可靠性。与单轮响应中的幻觉检测不同，诊断多步工作流中的幻觉需要识别出导致初始偏差的具体步骤。为填补这一空白，我们提出了一项新的研究任务——基于大语言模型的智能体的自动幻觉归因，旨在识别导致幻觉的步骤并解释其原因。为支持该任务，我们引入了 AgentHallu，这是一个综合基准，包含：(1) 693 条涵盖 7 个智能体框架和 5 个领域的高质量轨迹；(2) 一个包含 5 个大类（规划 Planning、检索 Retrieval、推理 Reasoning、人机交互 Human-Interaction 和工具使用 Tool-Use）及 14 个子类别的幻觉分类体系；(3) 涵盖二分类标签、致幻步骤及因果解释的人工策划多级标注。我们评估了 13 个领先模型，结果表明，即使是顶级模型（如 GPT-5、Gemini-2.5-Pro），该任务也极具挑战性。表现最佳的模型仅实现了 41.1% 的步骤定位准确率，其中工具使用幻觉最为困难，准确率仅为 11.6%。我们相信 AgentHallu 将促进未来关于开发鲁棒、透明且可靠的智能体系统的研究。",
                    "inspiration_trace": "基于对论文《AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：从“单轮对话”到“多步智能体”的范式转移\n**思考起点：**\n随着大语言模型（LLM）的发展，研究热点已从简单的单轮问答转向了复杂的**LLM-based Agents（智能体）**。智能体具备规划、检索、工具调用、多步推理等能力，能够解决长链路任务。\n\n**核心洞察：**\n在单轮对话中，幻觉通常表现为“生成内容与事实不符”。但在智能体的**多步工作流**中，问题变得复杂：中间某一步的错误（如规划失误、检索错误）会像滚雪球一样**向后传播**，导致最终结果错误。\n*   **关键矛盾：** 仅仅判断“最终答案是否错误”（二分类）对于智能体来说远远不够。如果不知道错误是在哪一步产生的，就无法修复智能体，也无法提升其可靠性。\n\n### 2. 问题聚焦：从“检测”到“归因”的认知升级\n**现有局限：**\n作者回顾了现有的幻觉检测基准（如HaluEval, FELM等），发现它们都局限于**单轮响应**的**二分类判断**（是/非幻觉）。这些基准无法回答两个关键问题：\n1.  **Where（在哪里）：** 错误最早出现在轨迹的哪一步？\n2.  **Why（为什么）：** 这一步为什么会出错？\n\n**假设提出：**\n为了构建可靠的智能体系统，必须提出一个新的研究任务——**自动化幻觉归因**。这个任务的目标不仅仅是发现错误，而是要像调试程序一样，**定位导致错误的“源代码行”（步骤）并解释原因**。\n\n### 3. 方法论构建：如何定义和量化“归因”？\n**思考难点：**\n在多步轨迹中，错误往往具有连锁反应。例如，第1步规划错了，导致第3步工具调用错了，最后第5步答案错了。究竟哪一步才是“负责”的？\n\n**逻辑定义（因果对齐）：**\n作者引入了因果推断的思想来定义“负责步骤”：\n*   **反事实推理：** 如果修正了某一步 $u_t$，并重新执行后续步骤，最终答案变正确了，那么 $u_t$ 就是幻觉的根源。\n*   **最小化原则：** 如果有多个步骤都满足上述条件，取最早的那一步（即错误的源头）。\n\n### 4. 数据构建：如何设计基准以覆盖智能体的复杂性？\n**思考路径：**\n既然要评估“归因”，数据集就不能只有问答对，必须包含完整的**思维-行动-观察**轨迹。同时，智能体的幻觉类型是多样的，不能一概而论。\n\n**分类学构建：**\n作者没有凭空想象类别，而是通过**扎根理论**分析数据，归纳出智能体特有的5大幻觉类别，对应智能体的核心能力模块：\n1.  **Planning（规划）：** 目标分解错误。\n2.  **Retrieval（检索）：** 查询或上下文错误。\n3.  **Reasoning（推理）：** 逻辑或计算错误。\n4.  **Human-Interaction（人机交互）：** 误解人类反馈。\n5.  **Tool-Use（工具使用）：** 工具参数或调用错误。\n\n**数据筛选策略：**\n为了保证基准的挑战性，作者制定了严格的过滤标准：\n*   **排除非欺骗性失败：** 剔除那些直接报错、崩溃的简单案例（太容易检测）。\n*   **保留“ plausible but wrong”：** 专注于那些看起来逻辑通顺、但结果错误的轨迹，这才是归因的难点所在。\n\n### 5. 评估验证：证明任务的必要性与难度\n**逻辑闭环：**\n如果现有的顶尖模型（如GPT-5, Gemini-2.5-Pro）能轻松完成这个任务，那么这个基准就没有价值。\n\n**实验设计：**\n作者在这些模型上测试了两种Prompting策略（标准Prompt vs. 逐步Prompt）。\n*   **预期结果：** 即使是最强的模型，在步骤定位上的准确率也很低（约41%），特别是在工具使用幻觉上（仅11.6%）。\n*   **结论：** 这证实了“幻觉归因”确实是一个尚未解决的难题，从而确立了AgentHallu基准的学术价值——它为未来的研究指明了方向（即如何让模型具备自我诊断和因果解释的能力）。\n\n---\n\n**总结：作者的思考链条**\n1.  **观察现象：** 智能体的多步特性导致错误传播，单轮检测失效。\n2.  **提出假设：** 需要从“判断对错”升级为“定位源头+解释原因”。\n3.  **形式化定义：** 利用反事实推理定义“负责步骤”。\n4.  **工程实现：** 构建包含多维度分类和细粒度标注的AgentHallu数据集。\n5.  **验证价值：** 通过实验证明现有SOTA模型在此任务上的不足，确立研究基准。"
                },
                {
                    "title": "IDRBench: Interactive Deep Research Benchmark",
                    "arxiv_id": "2601.06676",
                    "authors": "Yingchaojie Feng, Qiang Huang, Xiaoya Xie, Zhaorui Yang, Jun Yu, Wei Chen, Anthony K. H. Tung",
                    "summary": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“Deep research agents”（深度研究智能体），并提出了一个“modular multi-agent research framework”（模块化多智能体研究框架）。内容涉及多智能体协作、工具使用（web exploration）以及交互反馈，符合多智能体和单智能体工具使用的研究范围，不属于排除项。",
                    "summary2": "本文旨在解决现有深度研究基准忽略交互动态评估的问题。针对未明确指定的查询场景，我们提出了IDRBench，包含交互式多代理框架、基于参考的User Simulator及交互感知评估套件。我们在引入模糊性注入的数据集上，通过Report Similarity、LLM-ACS及Interaction Turns等指标，验证了交互能显著提升研究质量与鲁棒性。",
                    "summary_translation": "由大语言模型驱动的深度研究代理能够执行多步推理、网络探索和长篇报告生成。然而，大多数现有系统以自主模式运行，假设用户意图已完全明确，且仅评估最终输出。在实践中，研究目标往往定义不足且在探索过程中不断演变，因此持续的交互对于实现鲁棒对齐至关重要。尽管交互至关重要，但现有的深度研究基准大多未将其纳入考量，既未对动态用户反馈进行建模，也未量化交互成本。我们介绍了IDRBench，这是首个用于系统性评估交互式深度研究的基准。IDRBench结合了具备按需交互功能的模块化多代理研究框架、可扩展的基于参考的用户模拟器，以及一个交互感知评估套件；该套件能够联合衡量交互收益（质量与对齐度）和交互成本（交互轮次与令牌数）。针对七个最先进大语言模型的实验表明，交互能够持续提升研究质量和鲁棒性，其效果往往能超越模型能力差异带来的影响，同时也揭示了在交互效率方面存在显著的权衡。",
                    "inspiration_trace": "基于论文《IDRBench: Interactive Deep Research Benchmark》的内容，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观趋势观察到具体痛点识别，再到方法论创新与验证的完整思考路径。\n\n---\n\n### 1. 宏观观察与趋势捕捉\n**思考起点：** 作者首先关注到大语言模型（LLM）在信息获取领域的演进。\n*   **现象：** LLM的能力已从单轮问答进化为能够进行多步推理、网页探索和长报告生成的“深度研究智能体”。\n*   **现状：** 现有的主流系统（如DeepResearcher等）大多采用**自主模式**，即用户给出初始指令，系统独立完成全过程，最后仅评估生成的报告质量。\n\n### 2. 现实痛点与核心假设\n**深入思考：** 作者敏锐地发现了“自主模式”与“真实研究场景”之间的巨大鸿沟。\n*   **问题识别：**\n    1.  **意图模糊性：** 现实中的用户需求往往是未充分定义的，用户在研究开始时并不清楚自己到底想要什么。\n    2.  **意图漂移：** 在长周期的推理过程中，智能体容易偏离用户初衷，产生幻觉或跑题，缺乏纠偏机制。\n*   **核心假设：** 深度研究不应是“独角戏”，而应是**“交互式协作”**。引入用户反馈可以显著提升研究质量和对齐度，甚至可能弥补模型本身能力的不足。\n\n### 3. 评估盲区的发现\n**关键转折：** 作者意识到，虽然“交互”很重要，但现有的评估体系完全忽略了这一点。\n*   **盲区分析：**\n    *   现有的基准测试都是**静态**的（Query + Reference Document），只看最终结果，不看中间过程。\n    *   这种评估方式无法区分“运气好答对”和“通过交互修正错误”的智能体。\n    *   更重要的是，它们忽略了交互的**成本**（打扰用户的次数、Token消耗）。\n*   **推论：** 要推动交互式研究的发展，必须建立一套能够量化“交互收益”与“交互成本”的新型基准。\n\n### 4. 方法论构建：从概念到落地\n为了验证上述假设并填补评估盲区，作者设计了IDRBench，其构建逻辑遵循以下步骤：\n\n#### A. 数据构建：如何逼真地模拟“需要交互”的场景？\n*   **挑战：** 现有的高质量数据集（如DeepResearch Bench）中的Query往往非常详细，智能体直接执行即可，不需要交互。\n*   **创新思路（模糊性注入）：** 作者决定人为制造“信息差”。通过LLM将原本详细的Query进行压缩（摘要化），保留核心意图但移除具体细节。\n*   **逻辑：** 只有当任务变得“模糊”时，智能体才被迫主动提问，从而触发交互行为。\n\n#### B. 用户模拟：如何实现大规模、可重复的评估？\n*   **挑战：** 真实的人类交互成本高昂且不可控（主观性强、不一致），无法作为大规模Benchmark的组件。\n*   **创新思路（基于参考的模拟器）：** 构建一个基于参考文档的“用户模拟器”。\n*   **逻辑：** 将参考文档视为“上帝视角”的真理。模拟器被设定为：像人类一样简洁回答，提供宏观指导，且拒绝错误选项。这样既保证了反馈的合理性，又实现了评估的标准化。\n\n#### C. 评估体系：如何定义“好的交互”？\n*   **思路：** 交互是一把双刃剑，必须建立多维度的评估指标。\n*   **维度拆解：**\n    1.  **收益：** 交互是否提升了质量？（语义相似度、结构覆盖度、意图满足度）。\n    2.  **成本：** 交互是否太烦人？（交互轮数、消耗的Token数）。\n*   **逻辑：** 只有同时考察这两个维度，才能判断一个智能体是否具备高效的“交互智能”。\n\n#### D. 框架设计：如何让智能体具备交互能力？\n*   **思路：** 基于现有的多智能体架构（规划、研究、生成），嵌入“交互模块”。\n*   **机制设计：**\n    *   **评估器：** 决定“何时”提问（权衡信息增益与打扰成本）。\n    *   **提问器：** 决定“问什么”（生成针对性的澄清问题）。\n*   **逻辑：** 交互不应是随机的，而应是基于当前上下文不确定性的理性决策。\n\n### 5. 实验验证与洞察提炼\n**最终验证：** 通过在多个SOTA模型上的实验，作者验证了最初的假设并发现了更深层的规律。\n*   **发现一：** 交互确实能普遍提升质量，且**交互能力有时比模型本身的原始智力更重要**（例如，开启交互的弱模型可能超过自主运行的强模型）。\n*   **发现二：** 存在**边际递减效应**。强模型通过交互获得的提升较小，而弱模型提升巨大。\n*   **发现三：** 交互策略存在差异。有的模型倾向于“频繁短问”，有的倾向于“少量长问”，这揭示了不同模型在交互效率上的权衡。\n\n### 总结\n作者的思考路径是一个典型的**“观察现象 -> 识别缺陷 -> 提出假设 -> 构建工具（Benchmark） -> 验证假设”**的学术闭环。\n\n其核心贡献不在于发明了一个新的聊天机器人，而在于**重新定义了深度研究的评估范式**——从“静态的结果导向”转向了“动态的过程导向”，并巧妙地通过“模糊性注入”和“用户模拟”解决了交互式系统难以量化评估的难题。"
                },
                {
                    "title": "Structured Episodic Event Memory",
                    "arxiv_id": "2601.06411",
                    "authors": "Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang, Baotian Hu",
                    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确针对自主智能体的记忆机制进行研究，提出了结构化情景事件记忆（SEEM）框架，旨在解决智能体在长期交互中的动态和关联性建模问题，属于单智能体研究范围中的“记忆”核心组件。",
                    "summary2": "本文旨在解决LLM在长期交互中因静态RAG导致的检索分散和缺乏结构依赖的问题。针对连续的交互流，我们提出了一种名为SEEM的分层框架，该框架协同了用于关系事实的Graph Memory Layer和用于叙事进展的Episodic Memory Layer，并引入了Episodic Event Frames (EEFs) 和Reverse Provenance Expansion (RPE) 机制。我们在LoCoMo和LongMemEval benchmarks上通过F1、BLEU-1和Accuracy等指标验证了其有效性，结果显示SEEM显著优于现有基线。",
                    "summary_translation": "",
                    "inspiration_trace": "基于论文《Structured Episodic Event Memory (SEEM)》，以下是对作者构建该方法论的逻辑链推演，旨在还原其从宏观问题观察到微观机制设计的思考过程：\n\n### 1. 宏观问题：智能体的“失忆”与“碎片化”困境\n**观察起点：**\n随着大语言模型（LLM）向自主智能体演进，它们需要处理长期的、动态的交互。然而，LLM 受限于有限的上下文窗口，且缺乏稳定的外部长期记忆系统。\n\n**现有方案的缺陷（痛点）：**\n为了解决记忆问题，业界普遍采用检索增强生成（RAG）。但作者敏锐地观察到，现有的 RAG 系统（无论是基于向量的还是基于图谱的）存在一个核心缺陷——**“碎片化检索”**。\n*   **现象：** 当智能体需要回答复杂问题时，检索到的往往是零散的文本片段或孤立的事实节点。\n*   **后果：** 这些片段缺乏上下文连贯性，无法支撑需要理解事件全貌、时间顺序和因果关系的复杂推理。智能体“只见树木，不见森林”，难以维持叙事的一致性。\n\n### 2. 认知科学假设：模拟人脑的双重记忆机制\n**思维转折：**\n为了解决“碎片化”问题，作者跳出纯计算机视角，转向认知心理学寻找灵感。人脑在处理记忆时并非单一存储，而是存在明确的分工：\n*   **语义记忆：** 存储客观事实、概念和关系（如“巴黎是法国首都”）。\n*   **情景记忆：** 存储特定时间、地点下的个人经历和事件流（如“去年夏天我在巴黎做了什么”）。\n\n**核心假设：**\n如果让智能体也具备这种分层记忆结构——即用**静态的关系图谱**来存储事实，用**动态的情景结构**来存储叙事流——就能从根本上解决上下文断裂的问题。\n\n### 3. 结构化创新：从“文本片段”到“认知框架”\n**具体化挑战：**\n虽然有了分层假设，但如何具体实现“情景记忆”？直接存储原始对话记录依然混乱。作者引入了认知框架理论。\n\n**方法论构建：**\n作者提出将连续的交互流转化为结构化的**情景事件框架**。\n*   **逻辑：** 一个事件不仅仅是文本，它包含参与者、动作、时间、地点、原因等多维属性。\n*   **设计：** 将非结构化的文本解析为具有明确语义槽位的结构化单元（EEF）。这就像把散乱的文字变成了填好的“案件调查表”，使得机器能像人类一样理解事件的要素。\n\n**动态融合机制：**\n现实中的对话是断续的（例如：A问了一半，B回答，A补充）。为了防止记忆碎片化，作者设计了**“联想融合”**机制。如果新的事件与旧的事件在语义上相关（如同一话题的不同轮次），系统会将它们合并为一个连贯的“场景”。这模拟了人类记忆中会将相关经历整合的心理过程。\n\n### 4. 检索机制革新：逆向溯源与上下文重构\n**解决“检索断层”：**\n即使有了结构化的记忆，如何确保检索时不漏掉关键信息？传统的检索是基于关键词匹配的，容易遗漏那些没有直接关键词但属于同一事件上下文的信息。\n\n**逻辑闭环：**\n作者提出了**逆向溯源扩展（RPE）**机制。\n*   **思考路径：** 当用户提问时，系统首先在“图谱层”找到相关的静态事实节点。但这只是线索。\n*   **关键动作：** 利用这些节点作为锚点，反向追踪到它们所属的“情景事件框架（EEF）”。\n*   **最终效果：** 一旦激活了某个事件框架，系统就会把该框架下关联的所有原始文本片段（通过溯源指针）全部召回。这确保了智能体看到的不是孤立的句子，而是整个事件的完整起承转合。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径遵循了从**现象观察**（RAG的碎片化） -> **理论借鉴**（认知心理学的双重记忆） -> **结构化建模**（EEF与分层架构） -> **机制完善**（联想融合与逆向溯源）的完整闭环。\n\n**核心思想演进：**\n1.  **发现问题：** 现有记忆是平面的、静态的，导致推理断裂。\n2.  **提出假设：** 记忆需要分层，区分“事实”与“故事”。\n3.  **构建模型：** 用图谱存事实，用框架存故事，并用指针连接两者。\n4.  **优化检索：** 从“找相似文本”转变为“找事件线索，再还原全貌”。\n\n这一过程体现了作者试图赋予 AI 智能体类似人类的“叙事能力”和“长期连贯性”的深层动机。"
                },
                {
                    "title": "Value of Information: A Framework for Human-Agent Communication",
                    "arxiv_id": "2601.06407",
                    "authors": "Yijiang River Dong, Tiancheng Hu, Zheng Hui, Caiqi Zhang, Ivan Vulić, Andreea Bobu, Nigel Collier",
                    "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究LLM智能体，专注于智能体决策制定的一个关键方面：人机通信。它引入了一个决策理论框架（VoI），使智能体能够动态决定是行动还是向用户寻求澄清，这属于单智能体行为和交互的范畴。尽管它在医疗等领域进行了测试，但核心贡献是智能体通信的方法论，而不是应用本身。",
                    "summary2": "本文旨在解决LLM智能体在处理未指定请求时，如何在行动与提问间取得平衡的问题。针对未指定的用户查询，我们提出了一种基于决策论中Value of Information (VoI)的框架，动态权衡信息增益与用户认知成本。在20 Questions、Flight Recommendation和Ambiguous WebShop等四个领域上，通过总效用验证了其有效性，该方法无需超参数调整即可达到最优性能。",
                    "summary_translation": "部署用于现实世界任务的 Large Language Model (LLM) agents（大语言模型智能体）面临一个根本性的两难困境：用户的请求往往是信息不足的，但智能体必须决定是依据不完整的信息采取行动，还是打断用户以寻求澄清。现有方法要么依赖于需要针对特定任务进行调优的脆弱 confidence thresholds（置信度阈值），要么未能考虑到不同决策所涉及的不同利害关系。我们引入了一个 decision-theoretic framework（决策理论框架），该框架通过 Value of Information (VoI)（信息价值）解决了这一权衡问题，使智能体能够动态地权衡提问带来的 expected utility gain（预期效用增益）与给用户带来的 cognitive cost（认知成本）。我们的 inference-time（推理时）方法无需 hyperparameter tuning（超参数调优），并且能够跨场景无缝适应——从休闲游戏到医疗诊断。在四个不同领域（20 Questions、医疗诊断、航班预订和电子商务）的实验表明，VoI 始终匹配或超过最佳手动调优的 baselines（基线），在高成本设置中实现了高达 1.36 的效用点提升。这项工作提供了一个用于 adaptive agent communication（自适应智能体通信）的 parameter-free（无参数）框架，该框架明确平衡了 task risk（任务风险）、query ambiguity（查询歧义）和 user effort（用户努力）。",
                    "inspiration_trace": "基于论文《Value of Information: A Framework for Human-Agent Communication》，以下是对作者核心方法论产出逻辑链的系统性推演。这一过程展现了作者如何从现实痛点出发，通过批判性分析现有技术，最终引入决策理论解决人机交互中的根本矛盾。\n\n---\n\n### 第一阶段：宏观困境的识别——“模糊性”与“两难”\n**（观察与问题定义）**\n\n作者的思考始于对现实世界LLM智能体应用场景的观察。作者发现，尽管LLM在执行任务上能力强大，但在面对真实用户时存在一个根本性的**“信息缺口”**：\n*   **用户请求的天然模糊性**：用户的指令往往是欠规范的（如“订一张去伦敦的机票”），隐含了未知的偏好（预算、时间、转机容忍度）。\n*   **智能体的两难困境**：\n    *   **行动**：在信息不全时直接行动，可能导致结果与用户意图不符（任务失败风险）。\n    *   **询问**：通过提问澄清信息，但会打断用户，增加认知负担（用户流失风险）。\n\n**核心思考**：现有的智能体大多假设指令是清晰的，或者仅仅关注“如何执行”，而忽略了“何时该沟通”这一前置决策。作者意识到，**解决这一两难困境是智能体从“工具”进化为“合作伙伴”的关键。**\n\n---\n\n### 第二阶段：对现有范式的批判——“置信度”的失效\n**（假设验证与否定）**\n\n在寻找解决方案时，作者首先审视了学术界和工业界的主流做法，并发现了其逻辑漏洞：\n1.  **固定轮次策略**：无论任务难易都问固定数量的问题。这显然是愚蠢的，因为它忽略了上下文。\n2.  **基于置信度的阈值**：这是目前最先进的自适应方法。当模型对答案的“自信度”低于某个阈值（如0.9）时，就提问。\n\n**作者的批判性洞察**：\n*   **置信度 $\\neq$ 价值**：模型对“猜动物”有90%的把握，和对“诊断癌症”有90%的把握，其含义截然不同。\n*   **缺乏风险感知**：置信度方法只关注“我知道多少”（信息论视角），却忽略了“如果错了后果有多严重”（决策论视角）。在低风险任务（猜动物）中，90%的置信度可能已经足够；但在高风险任务（医疗诊断）中，90%可能意味着致命风险，必须继续提问。\n\n**结论**：单纯依赖模型内部的不确定性估计是片面的，必须引入对**任务风险**和**决策后果**的考量。\n\n---\n\n### 第三阶段：理论视角的转换——从“信息获取”到“理性决策”\n**（理论引入与框架构建）**\n\n为了解决上述缺陷，作者将视角从计算机科学转向了认知科学与决策理论，提出了核心假设：\n*   **沟通即决策**：提问不应仅仅是为了获取信息，而应被视为一种“行动”。这种行动有成本（认知负荷），也有收益（提升决策质量）。\n*   **理性言语行为**：借鉴RSA框架，智能体应当是“理性”的，即只有当提问带来的**预期效用提升**大于**提问成本**时，才应该进行沟通。\n\n**逻辑推演**：\n我们需要一个数学工具来量化“提问到底值不值”。作者引入了经典的**信息价值**理论。\n*   **定义**：VoI = (获得信息后的预期效用) - (当前信息下的预期效用)。\n*   **决策规则**：如果 $VoI > \\text{提问成本}$，则提问；否则，直接行动。\n\n这一转换将问题从“我不确定吗？”（模糊逻辑）变成了“值得去弄清楚吗？”（经济逻辑）。\n\n---\n\n### 第四阶段：方法论的落地——LLM驱动的贝叶斯模拟\n**（从理论到实践的映射）**\n\n有了VoI理论框架，接下来的挑战是如何让LLM在推理时计算出这个值。作者设计了一套无需训练的推理时算法：\n\n1.  **信念分布**：\n    *   *思考*：LLM通常只输出一个确定答案，但计算VoI需要概率。\n    *   *方案*：强制LLM输出对用户潜在意图（如偏好、疾病类别）的概率分布 $b(\\theta)$。\n\n2.  **前瞻性模拟**：\n    *   *思考*：在问出问题前，智能体需要预判“如果我问了，用户可能怎么答，以及回答后我的效用会变多少”。\n    *   *方案*：利用LLM的生成能力进行“反事实模拟”。针对候选问题 $q$，枚举可能的回答 $y$，模拟更新信念分布 $b(\\theta|y)$，并计算对应的效用。\n\n3.  **动态权衡**：\n    *   *思考*：如何整合风险和成本？\n    *   *方案*：在VoI公式中显式引入任务风险（通过效用函数 $U(\\theta, a)$ 的量级体现）和认知成本（常数 $c$）。\n\n---\n\n### 第五阶段：验证逻辑——自适应性与零参数优势\n**（实验设计与预期验证）**\n\n最后，作者通过实验验证这一逻辑链条的有效性，其验证逻辑紧扣之前的批判：\n*   **跨场景泛化**：选择“猜动物”（低风险）、“医疗诊断”（高风险）、“订票”（多属性偏好）等不同场景，证明VoI能自动适应不同的风险等级。\n*   **对比基线**：专门对比“置信度阈值”方法。\n    *   *预期结果*：置信度方法需要针对每个任务手动调整阈值（脆弱），而VoI方法无需调参（鲁棒）。\n    *   *逻辑闭环*：在高风险场景下，VoI会因为潜在收益巨大而倾向于多问；在低风险或高沟通成本场景下，VoI会自动停止提问。\n\n---\n\n### 总结：作者的思维演进图谱\n\n1.  **痛点**：用户指令模糊，智能体在“瞎猜”和“烦人”之间进退维谷。\n2.  **反思**：现有的“自信度”机制只看不确定性，不看后果，无法区分“猜错猫”和“误诊癌症”的区别。\n3.  **升维**：引入决策论，将沟通视为一种投资，必须计算ROI（投资回报率）。\n4.  **工具**：采用**信息价值**作为核心指标，量化“提问”带来的预期收益。\n5.  **实现**：利用LLM自身的推理能力进行信念估计和未来模拟，实现无需训练的动态决策。\n\n这一逻辑链条展示了作者如何从具体的交互体验出发，通过跨学科的理论融合，最终构建出一个既符合人类直觉又具备数学严谨性的通用框架。"
                },
                {
                    "title": "Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms",
                    "arxiv_id": "2601.06039",
                    "authors": "Yueze Liu, Ajay Nagi Reddy Kumdam, Ronit Kanjilal, Hao Yang, Yichi Zhang",
                    "summary": "Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了VEJA框架，旨在通过改进数据整理和训练范式来提升“角色扮演智能体”的真实性和深度。研究涉及智能体的内部世界建模、价值观、经历和判断，这属于单智能体范畴中的记忆与人格构建，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现代角色扮演模型缺乏真实角色深度及内心冲突推理的问题。针对现有训练范式的局限性，我们提出了一种VEJA框架（Values, Experiences, Judgments, Abilities），用于指导数据策划。我们在基于角色Makise Kurisu的数据集上，通过LLM-as-judge的盲A/B测试验证了其有效性，结果显示VEJA策划的数据在角色一致性和叙事连续性上显著优于合成基线。",
                    "summary_translation": "现代角色扮演模型日益精密，但始终难以捕捉可信且引人入胜角色的本质。我们认为，这一缺陷归因于训练范式忽视了角色内心世界的动态相互作用。当前的方法，包括检索增强生成、基于事实的提示、基于文学的学习以及合成数据生成，在建模定义人类互动的深思熟虑且充满价值冲突的推理方面，均表现出反复出现的局限性。在本文中，我们确定了对于角色真实性至关重要的四个核心概念：价值观、经历、判断和能力。我们提出 VEJA 框架作为一种新的数据策展范式，旨在解决这些系统性局限。为了展示本框架所能达到的质量上限，我们进行了一项试点研究，将人工策展的基于 VEJA 的数据集与最先进的合成基线进行了比较。利用大语言模型评判法，我们的研究结果显示出显著的质量差距，这表明转向以概念为基础的数据策展（如 VEJA 所体现的那样），对于创建具有真正深度和叙事连贯性的角色扮演智能体是必要的。完整数据集可在 https://github.com/HyouinKyoumaIRL/Operation-Veja 获取。",
                    "inspiration_trace": "基于论文《Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms》，以下是对作者产出该文章核心方法（VEJA框架）的逻辑链推演：\n\n### 第一阶段：现象观察与核心痛点识别\n**（从“模型能说话”到“模型没有灵魂”）**\n\n1.  **宏观观察**：作者发现，尽管现代角色扮演模型越来越复杂，能够生成流畅的对话，但它们始终缺乏“令人信服的、引人入胜的角色本质”。\n2.  **具体案例触发**：作者在尝试构建高保真角色（如《命运石之门》中的牧濑红莉栖）时发现，现有模型无法复刻其核心特质——即“求知欲”与“社交戒备心”之间的冲突。模型的反应仅仅是条件反射式的，缺乏内在的驱动力。\n3.  **核心假设提出**：作者认为，问题的根源不在于模型参数不够大，而在于**训练范式**忽视了角色内心世界的动态相互作用。人类互动不是检索“正确”答案，而是**冲突价值的协商**。\n\n### 第二阶段：对现有范式的批判性解构\n**（为什么当前主流方法都失效了？）**\n\n为了验证假设，作者系统性地解剖了当时四种主流的角色建模方法，试图找出它们共同的缺陷：\n\n1.  **检索增强生成（RAG）的局限**：RAG擅长处理事实，但人类的价值体系是组合爆炸的。试图用检索列表来穷举一个角色在所有情境下的价值判断是不可能的。\n2.  **基于事实的价值预设的局限**：为了通过基准测试，现有方法倾向于将价值简化为孤立的公式（如“对陌生人开放”）。这导致模型在对话中过度索引单一特征，忽略了语境和平衡，显得机械且缺乏分寸。\n3.  **基于文学生成的局限**：文学名著虽然包含深度，但对话只“暗示”了思维过程，而非“显式”展示。模型无法从对话文本中反向推导出角色复杂的内心 deliberation（ deliberative reasoning）。此外，文学中的“经验”通常通过旁白而非对话传递，导致模型难以学会“以史为鉴”。\n4.  **合成数据生成的死循环**：这是最致命的陷阱。试图用现有的强模型（如GPT-4）生成高质量角色数据是行不通的，因为**生成者本身就不具备处理复杂价值冲突的能力**。这导致了一个递归的质量天花板。\n\n### 第三阶段：理论重构与VEJA框架的诞生\n**（回归戏剧艺术，重建角色的“因果逻辑”）**\n\n在否定了现有技术路径后，作者转向经典戏剧理论（如斯坦尼斯拉夫斯基体系），试图从第一性原理出发定义什么是“真实的角色”。\n\n1.  **寻找基本单元**：作者认为，要模拟角色的深度，必须显式地建模其内心逻辑。通过数据整理过程中的观察，作者提炼出四个核心概念：\n    *   **Values (价值观)**：行为的根本动机（Why）。\n    *   **Experiences (经历)**：塑造价值观和判断的过去事件（Evidence）。\n    *   **Judgments (判断)**：价值观经过经历过滤后形成的具体观点（Output）。\n    *   **Abilities (能力)**：表达上述特质的知识和技能工具（Toolkit）。\n\n2.  **建立因果链条**：这四个要素不是孤立的标签，而是一个严密的**因果闭环**：\n    *   经历塑造价值观；\n    *   价值观与经历共同产出判断；\n    *   判断通过能力表达出来。\n    *   *逻辑演进点*：作者意识到，只有强制数据遵循这个因果链，才能让模型学会“像人一样思考”，即基于过去（E）和动机（V）来形成当下的观点（J），而不仅仅是模仿语气。\n\n### 第四阶段：验证与范式转移\n**（证明“人+框架”优于“纯模型合成”）**\n\n1.  **实验设计的逻辑**：既然现有模型无法生成高质量数据，那么“人类作者”是否就是答案？为了验证这一点，作者设计了一个对比实验：**纯模型生成** vs. **VEJA框架指导的人类写作**。\n2.  **结果解读**：实验结果显示，VEJA指导的人类数据显著优于SOTA合成数据。这证明了作者的核心论点：**当前的技术瓶颈不在于算力，而在于数据的“概念深度”**。\n3.  **最终结论**：作者提出，社区需要从“构建更好的鹦鹉”（模仿表面）转向“创造真正的数字心智”（模拟内在）。VEJA不仅仅是一个数据标注框架，更是一种新的训练范式，它要求我们在数据构建阶段就必须显式地包含角色的内心冲突和推理过程。\n\n---\n\n**总结：**\n作者的思考路径是从**“体验到的肤浅感”**出发，经过**“对技术路径的证伪”**，回归**“对人性和戏剧艺术的本体论思考”**，最终提炼出**“VEJA因果模型”**，并通过实验确立了**“概念驱动数据”**优于**“纯合成数据”**的新范式。"
                },
                {
                    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
                    "arxiv_id": "2601.07779",
                    "authors": "Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",
                    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个计算机使用智能体框架，涵盖了单智能体的核心要素：规划、记忆、工具使用和自我反思。尽管涉及视觉模型（VLM），但其核心贡献在于智能体的架构设计（如反思-记忆智能体、编排器），而非视觉模型本身的改进，符合 LLM 智能体的研究范围。",
                    "summary2": "本文旨在解决计算机使用代理在长时程任务中鲁棒性不足及新领域泛化能力差的问题。针对复杂的桌面自动化场景，我们提出了一种名为OS-Symphony的整体框架，该框架集成了利用里程碑驱动长期记忆的Reflection-Memory Agent和采用See-Act范式的Multimodal Searcher。并在OSWorld、WindowsAgentArena和MacOSArena基准上通过Step Success Rate验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "尽管 Vision-Language Models (VLMs，视觉语言模型) 显著推动了 Computer-Using Agents (CUAs，计算机使用代理) 的发展，但现有框架在长时程工作流的鲁棒性以及在新领域的泛化能力方面仍面临挑战。这些局限性主要归因于对历史视觉上下文筛选缺乏细粒度控制，以及缺乏视觉感知的教程检索机制。为弥合这些差距，我们提出了 OS-Symphony，这是一个包含 Orchestrator (编排器) 的整体框架，该编排器协调两项关键创新以实现鲁棒的自动化：(1) Reflection-Memory Agent (反思记忆代理)，利用里程碑驱动的长期记忆实现轨迹级自我修正，有效缓解长时程任务中的视觉上下文丢失；(2) Versatile Tool Agents (多功能工具代理)，其特色在于包含一个 Multimodal Searcher (多模态搜索器)，该搜索器采用 SeeAct 范式在基于浏览器的沙箱中进行导航，以合成实时的、视觉对齐的教程，从而解决未见场景中的保真度问题。实验结果表明，OS-Symphony 在不同模型规模下均带来了显著的性能提升，在三个在线基准测试中确立了新的 State-of-the-Art (SOTA，最先进) 结果，尤其在 OSWorld 上达到了 65.84% 的成绩。",
                    "inspiration_trace": "基于论文《OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方案构建的思考过程：\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**思考起点：** 尽管视觉语言模型（VLMs）推动了计算机代理（CUA）的发展，但现有的代理框架在实际应用中仍存在显著的脆弱性。\n**核心矛盾：** 当前的CUA在两个关键维度上表现不佳：\n1.  **长时程任务的鲁棒性：** 在需要多步骤、跨应用的复杂工作流中，代理容易迷失方向或陷入死循环。\n2.  **新领域的泛化能力：** 面对未见过的软件或环境（OOD场景），代理缺乏必要的知识储备，无法有效执行任务。\n\n### 第二阶段：微观诊断与归因分析\n**思考深入：** 为什么现有的模块化或端到端框架无法解决上述矛盾？作者通过分析现有架构的局限性，识别出两个具体的“技术断层”：\n\n**断层一：视觉上下文的“失忆”**\n*   **观察：** 现有的记忆机制（如简单的滑动窗口或文本摘要）缺乏对历史视觉信息的精细化管理。\n*   **推论：** 在长任务中，屏幕截图包含大量冗余信息，直接存储会撑爆上下文窗口，而简单丢弃又会丢失关键状态。这种“视觉上下文丢失”导致代理无法回溯历史，从而无法识别意图漂移或循环行为等错误，失去了自我纠错的基础。\n\n**断层二：检索增强的“视觉盲区”**\n*   **观察：** 为了解决泛化问题，现有方法引入了检索增强生成（RAG）。但这些方法多依赖纯文本检索或静态知识库。\n*   **推论：** GUI任务本质上是视觉的。纯文本检索无法捕捉界面布局、图标样式等视觉语义，导致检索到的教程与当前屏幕状态不匹配（保真度低）。此外，静态知识库更新成本高，难以适应新软件的快速迭代。\n\n### 第三阶段：核心假设与策略提出\n**思考转折：** 要解决上述断层，必须从“被动处理”转向“主动感知与压缩”。作者提出了两个核心假设：\n\n1.  **关于记忆的假设：** 如果能设计一种机制，只保留具有里程碑意义的关键截图，并基于这些视觉证据生成轨迹级的反思，就能在压缩上下文的同时保留纠错能力。\n2.  **关于泛化的假设：** 如果代理能像人类一样，在遇到不懂的操作时主动打开浏览器进行“视觉搜索”，通过实际浏览网页来合成与当前环境视觉对齐的教程，就能解决静态知识库的滞后和文本检索的盲区。\n\n### 第四阶段：方法论构建与系统设计\n**思考落地：** 基于上述假设，作者构建了 **OS-Symphony** 这一整体框架，其逻辑架构体现了“分工协作”的思想：\n\n**1. 设计“指挥官”：**\n*   **逻辑：** 系统需要一个核心大脑来负责任务理解和动作调度，同时协调其他模块。\n*   **角色：** Orchestrator（编排器）。它只关注短期记忆（最近K步）和来自其他模块的高级指令，保持决策的敏捷性。\n\n**2. 构建“反思者与记忆库”：**\n*   **逻辑：** 针对“视觉上下文丢失”，需要一个专门的模块来管理长期记忆和进行错误审计。\n*   **方案：** **Reflection-Memory Agent (RMA)**。\n    *   **里程碑机制：** 不存储所有截图，而是通过算法判断哪些步骤是“里程碑”（如状态发生重大改变），只保留这些关键帧。\n    *   **结构化反思：** RMA 审计历史轨迹，通过结构化的消息协议向 Orchestrator 反馈状态（如：On-track, Off-track, GUI Error, Lack of Tutorial），从而实现轨迹级的自我纠正。\n\n**3. 打造“全能工具箱”：**\n*   **逻辑：** 针对“视觉盲区”和执行效率问题，需要专门的工具来处理特定类型的任务。\n*   **方案：** **Versatile Tool Agents**。\n    *   **多模态搜索者：** 这是一个核心创新。它采用“See-Act”范式，在一个隔离的浏览器沙箱中自主导航，阅读网页并合成包含视觉描述的教程。这解决了传统RAG缺乏视觉感知的问题。\n    *   **定位器与编码器：** 分别负责UI元素的精确定位和系统级的代码操作，弥补纯GUI操作的不足。\n\n### 第五阶段：逻辑闭环与验证\n**思考总结：** 整个框架形成了一个闭环：\n*   Orchestrator 执行任务；\n*   遇到困难或错误时，RMA 通过视觉审计发现并反馈；\n*   如果是知识缺失，Searcher 主动上网寻找视觉教程；\n*   最终完成任务并更新记忆。\n\n**结论：** 这种设计通过**精细化的视觉记忆管理**解决了长时程任务的鲁棒性问题，通过**主动的视觉搜索**解决了新领域的泛化问题，从而在多个基准测试中实现了SOTA性能。\n\n---\n\n**总结：** 作者的思考路径是从**宏观的能力缺失**（鲁棒性与泛化性）出发，深入到**微观的信息处理缺陷**（视觉记忆丢失与检索视觉盲区），进而提出**主动化与结构化**的解决策略（里程碑记忆与视觉搜索），最终通过**多智能体协作**的架构实现了逻辑落地。"
                },
                {
                    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
                    "arxiv_id": "2601.07641",
                    "authors": "Jiaxuan Lu, Ziyu Kong, Yemin Wang, Rong Fu, Haiyuan Wan, Cheng Yang, Wenjie Lou, Haoran Sun, Lilong Wang, Yankai Jiang, Xiaosong Wang, Xiao Sun, Dongzhan Zhou",
                    "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了Test-Time Tool Evolution (TTE)范式，旨在解决现有LLM智能体依赖静态工具的问题，使智能体能够在推理过程中合成、验证和演化工具。这直接涉及单智能体的工具使用和自我演化能力，属于LLM智能体的核心研究范畴。",
                    "summary2": "本文旨在解决静态工具库在科学推理中覆盖不足和适应性差的问题。针对开放式科学计算任务，我们提出了一种Test-Time Tool Evolution (TTE)框架，通过动态合成、验证和演化可执行工具来增强智能体能力。我们在SciEvo、SciBench等benchmark上通过Accuracy和Tool Reuse Rate验证了其有效性，显著提升了推理准确率和工具复用效率。",
                    "summary_translation": "AI for Science（科学智能）的核心挑战不仅在于单纯的推理，更在于在开放式的科学世界中创造计算方法的能力。现有的 LLM-based agents（基于大语言模型的智能体）依赖于静态的、预定义的工具库，这种范式在工具稀疏、异构且本质上不完整的科学领域中根本无法奏效。在本文中，我们提出了 Test-Time Tool Evolution (TTE，测试时工具演化)，这是一种新的范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，TTE 克服了静态工具库的僵化性和长尾局限性。为了进行严格的评估，我们引入了 SciEvo，这是一个包含 1,590 个科学推理任务的 benchmark（基准），并由 925 个自动演化的工具提供支持。大量实验表明，TTE 在准确率和工具效率方面均达到了最先进的性能，同时实现了计算工具的有效 cross-domain adaptation（跨域适应）。代码和 benchmark 已在 https://github.com/lujiaxuan0520/Test-Time-Tool-Evol 发布。",
                    "inspiration_trace": "基于论文《Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程：\n\n### 1. 宏观观察：AI for Science 的“严谨性鸿沟”\n**思考起点：** 作者首先审视了当前大语言模型（LLM）在科学领域的应用现状。\n*   **现象：** LLM 拥有强大的推理能力，但在处理科学问题时，其概率性的本质往往导致“幻觉”，缺乏科学研究所必须的精确计算和严谨逻辑。\n*   **现有解法：** 业界通用的做法是给 LLM 配备外部工具（如计算器、API），即“工具增强”。\n*   **初步质疑：** 这种“LLM + 工具”的模式虽然解决了通用领域的部分问题，但在真正的科学研究中，是否足够？\n\n### 2. 问题诊断：静态工具库的“长尾困境”\n**深入分析：** 作者进一步剖析了现有工具增强范式在科学领域的根本缺陷。\n*   **核心矛盾：** 科学世界是开放、无边界的，而现有的工具库是**静态**且**预定义**的。\n*   **两大瓶颈：**\n    1.  **稀疏性与异构性：** 科学计算工具分散且非标准化，无法像通用 API 那样通过爬取构建一个“全知”的静态库。\n    2.  **不可预知性：** 科学探索往往涉及新颖的问题，需要全新的计算原语。静态库无法包含尚未被定义的工具。\n*   **结论：** 依赖静态工具库，本质上将 AI 限制在“被动选择者”的角色，无法应对开放的科学问题。这是一个**范式层面**的局限，而非工程细节问题。\n\n### 3. 核心假设：从“工具检索”到“工具进化”\n**范式转换：** 为了解决上述矛盾，作者提出了一个颠覆性的假设。\n*   **假设：** 一个真正的科学智能体，不应该只是从仓库里拿工具，而应该具备**在推理过程中即时创造和演化工具**的能力。\n*   **核心概念：** **Test-Time Tool Evolution (TTE，测试时工具进化)**。\n*   **逻辑推演：** 如果工具库是不完整的，那么它就不应该是固定的资源，而应该是**问题驱动的产物**。工具应该在解决问题的过程中被动态合成、验证并积累。\n\n### 4. 方法论构建：闭环进化机制\n**具体化思考：** 如何实现“工具进化”？作者构建了一个闭环逻辑，将科学方法论的迭代性引入 AI 系统。\n*   **第一步：结构化分解。** 面对复杂问题，不能直接生成代码，而应先将其拆解为原子化的子目标。这是为了精准定位需要什么样的工具。\n*   **第二步：动态检索与合成。** 先看库里有没有，没有就现场写一个。这里的关键是**“按需合成”**。\n*   **第三步：验证与原子化。** 生成的工具不能直接入库，必须经过严格的验证（语法、执行、领域逻辑）。更重要的是，要将复杂的工具拆解为**原子工具**。\n    *   *思考逻辑：* 只有原子化的工具才能被未来不同的问题复用，避免生成大量“一次性脚本”。\n*   **第四步：更新与修剪。** 库不能无限膨胀，需要基于使用频率进行优胜劣汰，保持工具库的高效和紧凑。\n\n### 5. 验证与拓展：零起点与跨域适应\n**场景推演：** 为了证明 TTE 的普适性，作者设定了两个极端的验证场景。\n*   **场景一：TTE-Zero（白板起家）。** 模拟人类科学家从零开始探索。初始工具库为空，看智能体能否在解决问题的过程中，自我演化出一套完整的科学计算工具集。\n*   **场景二：TTE-Adapt（跨域迁移）。** 模拟知识迁移。给智能体一个“材料科学”的工具库，让它去解决“化学”问题。看它能否通过进化，保留通用工具，淘汰不适用工具，并生成新领域的专用工具。\n*   **预期结果：** 如果 TTE 成立，它不仅能解决问题，还能演化出高复用率的核心科学原语。\n\n### 6. 最终愿景：定义“科学智能体”的新标准\n**思想升华：** 作者的思考最终落脚于对 AI 智能体的重新定义。\n*   **总结：** 科学推理的核心不在于参数知识的多寡，而在于**创造计算方法的能力**。\n*   **产出：** 这篇文章不仅仅是提出了一个算法框架，更是确立了“动态工具进化”作为下一代科学 AI 的核心范式。智能体从被动的工具使用者，进化为了主动的方法创造者。\n\n---\n\n**逻辑链总结：**\n**严谨性鸿沟** $\\rightarrow$ **静态工具库的局限性** $\\rightarrow$ **提出“测试时进化”假设** $\\rightarrow$ **构建“分解-合成-验证-原子化”闭环** $\\rightarrow$ **验证零起点与跨域能力** $\\rightarrow$ **确立主动创造的科学智能体范式**。"
                },
                {
                    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
                    "arxiv_id": "2601.07226",
                    "authors": "Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",
                    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了智能体AI系统在噪声环境下的表现，特别是针对工具使用和RAG任务中的上下文干扰进行了评估，属于单智能体的工具使用与鲁棒性研究范畴。",
                    "summary2": "本文旨在解决推理模型在噪声环境下的鲁棒性问题。针对包含随机文档、无关聊天记录和困难负例的噪声上下文，我们提出了NoisyBench基准和RARE（Rationale-Aware Reward）奖励函数，并在NoisyBench的11个数据集上通过准确率验证了其有效性。",
                    "summary_translation": "推理模型和智能体 AI 系统的最新进展导致了对多样化外部信息的依赖日益增加。然而，这种转变引入了本质上包含噪声的输入上下文，而当前经过净化的基准未能捕捉到这一现实。我们介绍了 NoisyBench，这是一个综合性的基准，旨在针对随机文档、无关聊天历史和困难负样本干扰项等多种噪声类型，系统性地评估模型在 RAG（检索增强生成）、推理、对齐和工具使用任务中跨越 11 个数据集的鲁棒性。我们的评估显示，当面对上下文干扰项时，最先进的模型性能会出现高达 80% 的灾难性下降。关键在于，我们发现智能体工作流往往会因为过度信任含噪工具输出而放大这些错误，且即使没有对抗意图，干扰项也能触发涌现性错位。我们发现，提示工程、上下文工程、SFT（监督微调）以及仅基于结果奖励的 RL（强化学习）均无法确保鲁棒性；相比之下，我们提出的 Rationale-Aware Reward (RARE, 理据感知奖励) 通过激励模型在噪声中识别有用信息，显著增强了其韧性。最后，我们发现了一种逆向缩放趋势，即在噪声环境下，增加测试时计算反而会导致性能下降。我们通过注意力可视化证明，模型会不成比例地关注干扰项标记，这些发现为构建下一代鲁棒且具备推理能力的智能体提供了重要见解。",
                    "inspiration_trace": "基于论文《Lost in the Noise: How Reasoning Models Fail with Contextual Distractors》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定义（从“理想”到“现实”）\n\n1.  **观察现状**：\n    *   作者注意到AI范式正从单纯的“对话模型”转向“智能体系统”。这些系统严重依赖外部工具（如RAG、搜索、计算器）和长上下文来解决复杂任务。\n2.  **发现缺口**：\n    *   **理想 vs. 现实**：现有的学术基准测试大多是在“无菌”的清洁数据下进行的。然而，现实世界中的智能体面临的是充满噪声的环境（错误的检索结果、无关的聊天历史、工具输出错误）。\n    *   **假设**：当前SOTA模型在清洁环境下的高分可能掩盖了其在真实噪声环境下的脆弱性。这种脆弱性可能不仅仅是效率问题，而是根本性的推理崩溃。\n\n### 第二阶段：现象验证与深度剖析（“有多脆弱？”）\n\n1.  **构建验证工具**：\n    *   为了验证假设，作者构建了 **NoisyBench**。这是一个系统性的基准，涵盖了RAG、推理、对齐和工具使用四大类任务，并人为注入了三种噪声：随机文档、无关聊天历史、硬负样本。\n2.  **关键发现**：\n    *   **灾难性下降**：即使是顶尖模型（如Gemini-2.5-Pro），在面对噪声时性能也出现了高达80%的断崖式下跌。这证明了“清洁性能强 $\\neq$ 抗噪能力强”。\n    *   **智能体的悖论**：引入智能体工作流（使用工具）在清洁环境下能提升性能，但在噪声环境下反而**放大**了错误。这是因为智能体倾向于“过度信任”工具输出和上下文，导致错误在多步规划中累积。\n    *   **反向缩放定律**：在噪声环境下，增加推理步骤（测试时计算）反而导致性能下降。模型花费更多token去“思考”噪声，结果越想越错。\n\n### 第三阶段：解决方案的试错与迭代（“常规方法为何失效？”）\n\n在确认问题严重性后，作者尝试了现有的主流修复方案，但均遭遇失败：\n\n1.  **尝试一：提示工程与上下文工程**\n    *   **逻辑**：通过优化Prompt或重新组织上下文来引导模型忽略噪声。\n    *   **结果**：**失败**。模型无法通过简单的指令区分信号与噪声，上下文工程本身也容易受到噪声干扰。\n2.  **尝试二：监督微调（SFT）**\n    *   **逻辑**：在包含噪声的数据集（NoisyInstruct）上进行训练，让模型适应噪声。\n    *   **结果**：**失败**。导致了“灾难性遗忘”，模型失去了原有的推理能力，且并未真正学会抗噪。\n3.  **尝试三：基于结果的强化学习（Outcome-based RL）**\n    *   **逻辑**：只对最终答案的正确性进行奖励，让模型自己探索如何在噪声中得出正确答案。\n    *   **结果**：**部分有效但局限**。虽然比SFT好，但模型往往通过“作弊”或依赖内部记忆来得分，而不是真正学会从噪声中提取信息。它无法区分“答对了”是因为“抗噪成功”还是“碰巧蒙对”。\n\n### 第四阶段：核心洞察与方法论形成（从“结果导向”转向“过程导向”）\n\n1.  **核心洞察**：\n    *   作者意识到，单纯奖励“最终答案”是不够的。模型失败的根本原因在于**推理过程**被噪声劫持（注意力机制分析显示模型在错误预测时过度关注干扰项）。\n    *   因此，必须显式地奖励模型在推理过程中**识别并锚定有用信息**的行为，而不仅仅是奖励最终结果。\n2.  **方法论提出：RARE (Rationale-Aware Reward)**\n    *   **逻辑转变**：从“Reward the Outcome”转变为“Reward the Process”。\n    *   **具体机制**：设计一个新的奖励函数，不仅检查最终答案，还检查模型的思维链中是否正确引用或提取了上下文中的**有效参考信息**。\n    *   **作用原理**：通过奖励模型在噪声中“抓取”正确线索的行为，强迫模型学会过滤干扰项。这就像训练学生不仅要写出正确答案，还要在草稿纸上圈出解题依据。\n3.  **最终验证**：\n    *   实验证明，RARE 显著降低了模型被干扰的比例，同时提高了最终准确率。更重要的是，这种方法不仅提升了抗噪性，在清洁环境下也没有性能损失，实现了鲁棒性与通用性的双赢。\n\n---\n\n**总结**：\n作者的思考路径是从**现实应用场景的落差**出发，通过**基准测试量化了“噪声脆弱性”这一现象**，在排除了**提示工程和传统训练方法**的无效性后，抓住了**“推理过程被干扰”这一本质原因**，最终通过**引入过程级奖励（RARE）**，成功引导模型学会了在噪声中“去伪存真”的推理能力。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 4,
            "papers": [
                {
                    "title": "DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems",
                    "arxiv_id": "2601.07248",
                    "authors": "Shuyu Zhang, Yujie Liu, Xinru Wang, Cheng Zhang, Yanmin Zhu, Bin Li",
                    "summary": "Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了DarwinTOD框架，专注于LLM驱动的终身自我演化，符合“自我演化”的研究范围。同时，文中明确提到了“online multi-agent dialog execution with peer critique”（在线多智能体对话执行与同伴批评），符合“多智能体”的研究范围。该研究旨在解决智能体的自主持续改进问题，而非单纯的应用部署或纯推理。",
                    "summary2": "本文旨在解决任务导向对话系统无法在部署后实现终身自主进化和适应新领域的问题。针对动态环境下的对话交互，我们提出了一种名为DarwinTOD的终身自进化框架，该框架集成了进化计算与LLM驱动的策略优化，通过维护可进化策略库（ESB）及双循环机制（在线多智能体执行与离线结构化进化）实现无人工干预的持续优化。我们在MultiWOZ和SGD数据集上通过Inform、Success、BLEU及Combine指标验证了其有效性。",
                    "summary_translation": "传统的任务型对话系统无法从持续的交互中进化，也无法在部署后适应新领域，这是其在现实世界动态环境中的一个关键局限。持续学习方法依赖于基于人工筛选数据的阶段性重训练，未能实现自主的终身改进。尽管进化计算和 LLM (Large Language Model, 大语言模型) 驱动的自我改进为对话优化提供了有前景的机制，但它们缺乏一个用于全面、迭代式策略优化的统一框架。为了弥合这一差距，我们提出了 DarwinTOD，这是一个终身自进化对话框架，它系统性地整合了这两种范式，从而能够在无需针对特定任务进行微调的情况下，从零样本基础开始实现持续的策略优化。DarwinTOD 维护着一个可进化策略库，并通过双循环过程运行：包含同伴批评的在线多智能体对话执行，以及利用累积反馈来优化策略库的离线结构化进化操作。这种闭环设计使得系统无需人工干预即可实现自主的持续改进。大量实验表明，DarwinTOD 不仅超越了以往的最先进方法，而且在整个进化过程中展现出持续的性能提升。我们的工作为构建具有终身自进化能力的对话系统提供了一个新颖的框架。",
                    "inspiration_trace": "基于对论文《DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems》的深度分析，以下是作者产出该核心方法的逻辑推演过程，还原了从宏观观察到具体方法论的思考链条：\n\n### 第一阶段：宏观问题识别——从“静态系统”到“动态世界”的矛盾\n\n**1. 现实观察：**\n作者首先观察到现实世界是动态变化的。用户的偏好、对话的领域以及任务的目标都在不断演进。然而，现有的任务型对话系统（TOD）在部署后本质上是“静态”的——一旦训练完成，其能力就被冻结，无法从后续的交互中学习或适应新领域。\n\n**2. 核心痛点提炼：**\n这导致了“研究原型”与“可部署系统”之间的巨大鸿沟。学术界通常在静态基准上评估模型，而工业界需要的是一个能在开放、动态环境中长期运行的智能体。因此，核心问题不再是“如何让模型在测试集上表现好”，而是**“如何让系统具备终身自我进化的能力，实现完全自主的持续改进？”**\n\n### 第二阶段：现有范式的批判与局限分析\n\n**1. 审视传统方案：**\n作者逐一分析了现有技术路线，发现它们都无法解决上述核心问题：\n*   **流水线架构：** 虽然模块化，但存在级联错误传播，且难以适应新领域，缺乏灵活性。\n*   **端到端大模型（LLM）：** 虽然泛化能力强，但本质上仍是基于初始指令的静态执行，缺乏从经验中学习的机制。\n*   **持续学习：** 虽然试图增量更新，但严重依赖人工整理的数据和周期性的重训练，无法实现真正的“自主”和“终身”进化。\n\n**2. 寻找突破口：**\n作者意识到，要实现真正的自主进化，必须摆脱对“人工标注数据”和“模型参数微调”的依赖，转而寻找一种能够利用系统自身交互经验进行自我优化的机制。\n\n### 第三阶段：理论融合——进化计算与大模型的互补性思考\n\n**1. 两个孤立的方向：**\n作者注意到了两个有潜力但各自为政的研究方向：\n*   **进化计算：** 擅长基于种群的优化，能通过选择、变异等机制寻找最优解，但缺乏语义理解能力，通常只用于优化孤立的提示词。\n*   **LLM驱动的自我改进：** 擅长推理和反思，能通过多智能体协作解决问题，但往往缺乏结构化的长期策略管理机制，容易陷入单轮优化的局部视角。\n\n**2. 逻辑跃迁（核心假设）：**\n**“如果将LLM作为进化算法的‘大脑’，利用其强大的语义理解和推理能力来驱动对话策略的进化，会发生什么？”**\n作者认为，LLM可以作为智能的“进化算子”，而进化算法提供了结构化的“优化框架”。两者的结合可以解决各自的短板：进化算法提供了终身迭代的框架，LLM提供了语义层面的策略生成与评估能力。\n\n### 第四阶段：方法论构建——从“单点优化”到“种群进化”\n\n**1. 核心概念定义：**\n基于上述假设，作者提出了**“可进化策略库”**的概念。\n*   **思维转变：** 传统的Prompt Engineering是在寻找一个“最好的”提示词。而DarwinTOD转向维护一个“多样化的策略种群”。这些策略在交互中竞争、优胜劣汰。\n\n**2. 闭环机制设计：**\n为了实现终身进化，作者设计了一个**“双循环”架构**，将理论落地：\n*   **在线执行循环：** 模拟真实环境。作者没有使用单一的端到端Agent，而是保留了**多智能体流水线（DST, DP, NLG）**。为什么？因为模块化不仅能防止错误级联，更重要的是，它允许每个模块拥有独立的策略，从而实现更细粒度的进化。\n*   **引入“同伴批判”：** 为了获得比单纯的“任务成功/失败”更密集的反馈信号，作者让智能体之间互相批判。这不仅能实时纠错，还能为离线进化提供高质量的反思数据。\n\n**3. 离线进化循环：**\n这是系统的“大脑”部分。作者设计了四种受进化论启发的操作算子，直接作用于策略库：\n*   **Genesis（创生）：** 针对新领域，利用LLM的零样本能力从无到有生成策略。\n*   **Mutation（变异）：** 针对失败的对话，利用LLM分析失败原因并修改策略。\n*   **Consolidation（整合）：** 利用LLM合并相似的策略，保持种群精简。\n*   **Pruning（剪枝）：** 淘汰低适应度的策略，控制计算成本。\n\n### 第五阶段：鲁棒性思考——应对噪声与不确定性\n\n**1. 潜在风险识别：**\n作者意识到，LLM生成的批判和变异可能包含噪声或偏见。如果系统盲目信任每一次反馈，可能会导致策略退化。\n\n**2. 解决方案设计：**\n为了解决这个问题，作者引入了**“适应度函数”**和**“玻尔兹曼选择”**机制。\n*   **长期统计：** 不依赖单次反馈，而是基于长期的历史表现（正负反馈计数）来计算策略的适应度。\n*   **概率选择：** 即使策略当前适应度低，也有一定概率被选中（探索），防止过早收敛到局部最优。\n*   **逻辑闭环：** 这种设计使得系统具有“抗噪性”，即使偶尔有错误的批判，长期的大数定律和种群选择机制也能过滤掉噪声，确保进化方向是向上的。\n\n### 总结：逻辑演进的全景图\n\n作者的思考路径是从**现实世界的动态需求**出发，批判了现有技术的静态本质，通过**融合进化计算的结构化优势与LLM的语义优势**，创造性地提出了**基于种群策略进化的新范式**。最终，通过**双循环架构**和**抗噪的进化机制**，将这一理论转化为一个无需人工干预、能够终身自我进化的对话系统。"
                },
                {
                    "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
                    "arxiv_id": "2601.07152",
                    "authors": "Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan",
                    "summary": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了一个名为“Agents of Diffusion (AoD)”的框架，明确将结构化文本生成构建为一个“多智能体对齐过程”。其中涉及“提示优化智能体”与“判别智能体”之间的协作，通过自然语言反馈迭代引导模型，这直接符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决生成高质量结构化数据时难以兼顾语义丰富性与严格模式一致性的挑战。针对结构化文本生成任务，我们提出了Agents of Diffusion (AoD)框架，利用多智能体强化学习通过自然语言反馈迭代引导冻结的Diffusion Language Models (DLMs)。在MultiWOZ、Super-NaturalInstructions等数据集上，通过Task Success Rate (TSR)和Field Overlap等指标验证了其有效性，实现了优于基线模型的结构保真度与语义多样性平衡。",
                    "summary_translation": "生成高质量的结构化数据（例如 JSON 记录）仍然是大语言模型面临的一项基本挑战，尤其是在必须兼顾语义丰富性与严格模式遵守的情况下。尽管自回归大语言模型具备强大的结构一致性，但它们往往难以应对语义变化和输出多样性方面的要求。相比之下，扩散语言模型引入了实现语义丰富性和双向解码的强大机制，却缺乏可靠保持结构所需的归纳偏置。我们提出了 Agents of Diffusion (AoD)，这是一个新颖的框架，通过语言介导的强化学习，将扩散语言模型的生成灵活性与自回归模型的推理能力统一起来。AoD 将结构化文本生成构建为一个多智能体对齐过程，其中提示优化智能体与评判智能体协作，利用自然语言反馈迭代指导扩散语言模型。这种方法实现了可控且符合模式规范的生成，而无需修改模型参数或依赖人工设计的约束。AoD 证明了扩散模型在协作智能体的监督下能够同时实现高语义新颖性和结构保真度，从而推进了可控生成领域的发展。在多个结构化数据基准测试中，AoD 始终优于扩散模型和自回归模型的基线，为结构感知且多样性增强的文本合成开辟了一条新路径。",
                    "inspiration_trace": "基于论文《Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法创新的思考过程。\n\n---\n\n### 1. 宏观问题与现状观察：结构化生成的“两难困境”\n**思考起点：** 作者首先关注到生成高质量结构化数据（如JSON）是当前LLM应用的一个核心痛点。\n*   **观察现象：** 现有的两大生成范式存在明显的优缺点互补，但无法兼得。\n    *   **自回归模型（AR-LLMs）：** 具有极强的结构一致性和因果逻辑（因为是从左到右生成），容易符合Schema。但缺点是语义单一、容易陷入重复模式，缺乏多样性。\n    *   **扩散语言模型：** 具有双向去噪机制，语义丰富、多样性高。但缺乏位置先验，很难严格保持复杂的嵌套结构（如JSON的括号匹配、字段完整性）。\n*   **核心矛盾：** 我们既想要扩散模型的“语义多样性”，又想要自回归模型的“结构严谨性”。现有的方法要么微调模型（成本高），要么使用硬性规则（缺乏灵活性）。\n\n### 2. 核心假设：用“推理”驾驭“生成”\n**思考转折：** 既然重新训练一个完美的模型很难，能否通过“外部控制”来弥补内部缺陷？\n*   **假设提出：** 能否利用自回归模型强大的逻辑推理能力，来“监督”或“引导”扩散模型的生成过程？\n*   **关键洞察：** 不需要修改扩散模型的参数（保持其生成多样性），而是通过改变其输入条件来控制输出。\n*   **控制接口：** 最直接的控制接口就是**提示词**。如果能让提示词动态进化，就能在不改动模型权重的情况下，引导模型生成符合结构要求的内容。\n\n### 3. 方法论演进：从静态提示到动态强化学习\n**思考深化：** 传统的提示工程是静态的（写一次，固定用），无法应对生成过程中的随机性和错误。如何实现动态控制？\n*   **机制选择：** 引入**强化学习（RL）**。将“提示词的修改”看作是一个动作，将“生成结果的质量”看作是奖励。\n*   **反馈信号的困境：** 传统的RL通常使用标量奖励（如一个分数）。但在结构化生成中，一个分数很难解释具体的错误（例如：“缺少字段”还是“格式错误”）。\n*   **创新点：** **自然语言反馈**。既然是语言模型，为什么不直接用语言来作为奖励信号？语言反馈比标量数字包含更丰富的信息，且更容易被LLM理解和执行。\n\n### 4. 架构构建：多智能体分工协作\n**具体化：** 如何将上述理论落地？作者设计了一个基于角色的多智能体系统，将任务拆解。\n*   **角色分工：**\n    *   **生成者：** 冻结的扩散模型。负责提供多样化的候选内容（探索者）。\n    *   **评判者：** 自回归LLM。负责检查生成内容的结构完整性和语义准确性，并输出自然语言反馈（批评家）。\n    *   **优化者：** 另一个自回归LLM。负责根据评判者的反馈，修改提示词（决策者）。\n*   **闭环逻辑：**\n    1.  优化者给出初始提示。\n    2.  扩散模型根据提示生成JSON。\n    3.  评判者检查JSON，给出具体建议（如：“缺少date字段，请修正”）。\n    4.  优化者根据建议修改提示词（如：“确保包含YYYY-MM-DD格式的date字段”）。\n    5.  循环往复，直到生成完美结果。\n\n### 5. 理论保障与最终形态\n**逻辑闭环：** 为什么这个系统是稳定且有效的？\n*   **解决“漂移”问题：** 多智能体系统常面临对话发散的问题。作者通过将扩散模型作为“环境锚点”，所有智能体的交互都围绕具体的生成样本展开，从而保证了交互的稳定性。\n*   **无参数化优势：** 整个过程不需要梯度回传更新扩散模型，完全通过语言层面的交互实现优化。这使得该方法可以即插即用于各种开源或闭源模型。\n\n---\n\n**总结：作者的思考路径**\n从**“结构 vs 多样性”**的矛盾出发 $\\rightarrow$ 提出**“用AR推理控制DLM生成”**的假设 $\\rightarrow$ 选择**“提示词”**作为控制抓手 $\\rightarrow$ 引入**“强化学习+自然语言反馈”**实现动态优化 $\\rightarrow$ 最终构建**“生成-评判-优化”**的多智能体协作闭环。"
                },
                {
                    "title": "Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents",
                    "arxiv_id": "2601.06490",
                    "authors": "Wenyu Mao, Haosong Tan, Shuchang Liu, Haoyang Liu, Yifan Xu, Huaxiang Ji, Xiang Wang",
                    "summary": "Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了一个包含归纳智能体和反思智能体的框架，专注于LLM智能体的记忆构建和自我反思（校准）机制，符合单智能体中关于记忆和自我反思的研究范围。",
                    "summary2": "本文旨在解决个性化LLM分层记忆中因噪声和幻觉导致的局部记忆与全局画像不一致问题。针对长期个性化对话场景，我们提出了一种名为Bi-Mem的智能体框架，通过归纳-反思双向构建机制校准记忆，并引入联想检索。我们在LoCoMo数据集上通过F1和BLEU-1指标验证了其有效性。",
                    "summary_translation": "从用户的长期对话中构建记忆，能够克服大语言模型的上下文限制，从而实现个性化交互。近期的研究侧重于层次化记忆，旨在通过聚类和聚合历史对话来建模用户的多粒度行为模式。然而，对话噪声和记忆幻觉可能在聚类过程中被放大，导致局部聚合记忆与用户的全局人设不一致。为缓解这一问题，我们提出了Bi-Mem，这是一个通过双向构建来确保层次化记忆保真度的智能体框架。具体而言，我们部署了一个归纳智能体来构建层次化记忆：该智能体从原始对话中提取事实信息以形成事实级记忆，利用图聚类将其聚合为主题场景（即局部场景级记忆），并推断用户画像作为全局人设级记忆。同时，我们设计了一个反思智能体，利用从人设级记忆中导出的全局约束来校准局部场景级记忆，从而强化全局-局部对齐。为实现连贯的记忆回忆，我们提出了一种联想检索机制：除了初始的层次搜索外，扩散激活过程允许事实激活上下文场景，而场景级匹配则检索显著的支撑性事实信息。实证评估表明，Bi-Mem在长期个性化对话任务的问答性能方面取得了显著提升。",
                    "inspiration_trace": "基于论文《Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观背景与现有范式的演进\n**（从“个性化需求”到“分层记忆”的必然性）**\n\n1.  **观察起点：** LLM的上下文窗口有限，无法容纳用户的长期对话历史。为了实现个性化交互（如记住用户偏好、回忆共同经历），必须引入外部记忆机制。\n2.  **现有方案的局限：** 早期的“扁平化记忆”仅存储孤立的事实或摘要，缺乏对事实间关联和用户高层行为模式的捕捉能力。\n3.  **趋势演进：** 研究自然转向了“分层记忆”，即通过聚类将原子事实聚合为场景，再提炼为用户画像。这种结构模仿了人类认知，从细粒度到粗粒度，看似完美解决了信息碎片化问题。\n\n### 第二阶段：关键问题的发现与诊断\n**（从“单向聚合”到“级联错误”的洞察）**\n\n1.  **深入审视：** 作者观察到，现有的分层记忆构建过程大多是**单向的**，即纯粹的自底向上聚合。\n2.  **核心痛点：** 在自底向上的过程中，原始对话中的“噪声”（如无关闲聊）和提取过程中的“幻觉”会被聚类算法放大。\n3.  **逻辑矛盾：** 这种放大的噪声会导致局部聚合的场景记忆与用户的全局画像发生冲突。\n    *   *案例思考：* 用户平时口味清淡（全局画像），但偶尔陪朋友吃了一次辣（局部场景）。单纯的聚类会错误地将“吃辣”归纳为用户的局部习惯，导致后续推荐出错。\n4.  **归纳假设：** 问题的根源在于缺乏“全局约束”。局部记忆的生成缺乏对全局一致性的校验，导致了“级联错误”的积累。\n\n### 第三阶段：核心假设与方法论的提出\n**（从“单向构建”到“双向闭环”的突破）**\n\n1.  **解决思路：** 为了解决局部与全局的冲突，记忆构建不能只是单向的归纳，必须引入反向的反思机制。\n2.  **框架设计：** 提出 **Bi-Mem** 框架，将记忆构建过程拆解为两个互补的智能体：\n    *   **归纳智能体：** 负责传统的自底向上构建（事实 -> 场景 -> 画像）。这是为了从数据中提取信息。\n    *   **反思智能体：** 负责自顶向下的校准。利用生成的全局画像作为“约束条件”，去检查和修正下层的场景记忆。\n3.  **逻辑闭环：** 通过这种“双向构建”，确保了局部细节（场景）始终服务于并服从于全局特征（画像），消除了记忆中的逻辑矛盾。\n\n### 第四阶段：记忆利用机制的优化\n**（从“静态检索”到“动态关联”的完善）**\n\n1.  **新问题：** 虽然记忆结构被修正了，但在检索时，如果仅按层级独立检索（如只查场景或只查画像），可能会割裂事实与上下文的联系。\n2.  **联想机制：** 作者引入了心理学中的“扩散激活”概念。\n3.  **检索逻辑：** 检索不应是孤立的。\n    *   检索到一个“事实”时，应自动激活其所属的“场景”。\n    *   检索到一个“场景”时，应回溯其包含的关键“事实”。\n4.  **最终形态：** 形成了**联想检索机制**，在初始检索后进行跨层级的扩散，确保模型在生成回答时能同时获得宏观的上下文和微观的证据支持。\n\n---\n\n### 总结：作者的思维演进路径\n\n1.  **宏观需求：** LLM需要长期记忆来实现个性化。\n2.  **技术选型：** 分层记忆优于扁平记忆。\n3.  **批判性观察：** 现有的分层记忆是单向的，容易因噪声放大导致“局部-全局”不一致。\n4.  **核心创新：** 引入“反思”机制，构建双向闭环（归纳+校准），用全局画像约束局部场景。\n5.  **应用落地：** 设计联想检索，打通层级间的壁垒，实现连贯的记忆召回。"
                },
                {
                    "title": "DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation",
                    "arxiv_id": "2601.06373",
                    "authors": "Yutong Song, Jiang Wu, Kazi Sharif, Honghui Xu, Nikil Dutt, Amir Rahmani",
                    "summary": "Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了DemMA，这是一个用于模拟痴呆症患者的对话智能体。研究内容涉及单智能体的核心能力，包括记忆（构建记忆状态人格）、推理（专家引导的CoT）和动作模拟（生成非语言行为）。尽管应用场景为医疗，但核心贡献在于智能体的架构设计与模拟能力，而非单纯的领域应用。",
                    "summary2": "本文旨在解决痴呆症模拟中数据稀缺及缺乏医学严谨性的挑战。针对多轮对话场景，我们提出了一种名为DemMA的专家引导推理与动作模拟框架。该方法通过临床人格构建和多智能体工作流生成数据，并利用CoT蒸馏技术将推理、语言和动作生成整合到单个LLM中。我们在DemMA-Dialogue数据集上，通过人格一致性、医学一致性等指标验证了其有效性，显著优于基线模型。",
                    "summary_translation": "利用大语言模型模拟 dementia patients (痴呆症患者) 具有挑战性，因为需要在长对话过程中对 cognitive impairment (认知障碍)、emotional dynamics (情绪动态) 和 nonverbal behaviors (非语言行为) 进行联合建模。我们提出了 DemMA，这是一个专家引导的 dementia dialogue agent (痴呆症对话智能体)，旨在实现高保真的 multi-turn patient simulation (多轮患者模拟)。DemMA 通过整合病理信息、人格特质以及由临床专家指导的特定亚型 memory-status personas (记忆状态人格)，构建了基于临床的 dementia personas (痴呆症人格)。为了突破纯文本模拟的局限，DemMA 对 nonverbal behaviors (非语言行为)（包括动作、面部表情和声音线索）进行了显式建模。我们进一步引入了一个 Chain-of-Thought (思维链) 蒸馏框架，该框架训练单个 LLM 在一次前向传播中联合生成 reasoning traces (推理轨迹)、患者话语以及对齐的行为动作，从而无需 multi-agent inference (多智能体推理) 即可实现高效部署。与专家、医学生及 LLM 评判者进行的广泛评估表明，DemMA 在多项指标上均显著优于现有的强 baselines (基线模型)。",
                    "inspiration_trace": "基于论文《DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体技术方案产出的思考过程：\n\n---\n\n### 第一阶段：宏观问题与痛点观察\n**思考起点：数据荒漠与模拟困境**\n作者首先观察到痴呆症研究和护理培训领域存在一个根本性的结构瓶颈：**高质量互动数据的极度稀缺**。\n*   **现实约束**：由于隐私和伦理限制，真实的患者数据（尤其是包含面部表情、语音语调等多模态信息的数据）几乎无法获取。\n*   **现有缺陷**：现有的模拟手段要么依赖僵化的脚本，无法捕捉真实互动的异质性；要么直接使用通用的对话模型，但这在医疗场景下极不可靠。\n\n### 第二阶段：深入剖析与假设提出\n**思考深入：通用大模型为何失效？**\n作者意识到，直接将LLM作为痴呆症患者模拟器存在三个核心矛盾，这构成了后续方法设计的假设前提：\n1.  **医学严谨性缺失**：通用模型缺乏临床依据，可能生成不安全或医学上不准确的建议。\n2.  **“过度完美”悖论**：LLM倾向于生成流畅、礼貌的回复，但这恰恰掩盖了痴呆症患者特有的认知衰退标志（如重复、犹豫、逻辑断裂）。这种“人格漂移”会导致模拟失真。\n3.  **模态缺失**：痴呆症沟通是多通道的（语言、情感、行为），纯文本模型丢失了非语言线索（如动作、神态），而这些随着语言能力下降变得愈发重要。\n\n**核心假设**：要实现高保真模拟，必须从**“通用对话生成”**转向**“临床病理驱动的行为建模”**。\n\n### 第三阶段：方法论演进与逻辑构建\n为了验证上述假设，作者分三个步骤构建了解决方案：\n\n#### 步骤一：构建临床锚点——从“角色扮演”到“病理分层”\n**思考**：如何防止模型生成随机的“疯言疯语”，而是生成符合特定痴呆症亚型的症状？\n**逻辑推演**：患者的人格不应是随机的，而应是病理学的产物。\n*   **创新点**：提出了**分层人格构建范式**。\n    *   不再使用单一的Prompt，而是将患者解构为三个依赖层：**背景层**（人口统计学+亚型病理）、**性格层**（基于ICF标准的心理功能）、**记忆层**（长/短期记忆状态）。\n    *   **目的**：通过这种结构化约束，确保生成的“遗忘”或“混乱”是特定病理（如阿尔茨海默症 vs. 额颞叶痴呆）的临床表现，而非模型的随机幻觉。\n\n#### 步骤二：解决数据与质量控制——多智能体流水线\n**思考**：既然没有真实数据，如何合成高质量数据？同时，如何解决长对话中的逻辑一致性问题？\n**逻辑推演**：单一模型难以同时兼顾记忆分析、对话规划和动作生成。需要“分而治之”。\n*   **创新点**：设计了**多智能体LLM工作流**。\n    *   引入专门的**记忆分析智能体**（判断当前哪些记忆可访问）、**对话规划智能体**（决定情感轨迹和内容）、**生成智能体**（产出语言）、**动作标注智能体**（补充非语言行为）以及**验证智能体**。\n    *   **目的**：通过将推理过程外显化，不仅生成了首个合成数据集，还确保了每一步都有临床逻辑支撑，解决了长对话的一致性问题。\n\n#### 步骤三：解决落地效率——思维链蒸馏\n**思考**：多智能体系统虽然质量高，但推理延迟大，无法满足实时护理培训的需求。如何保留“专家级推理”的同时，实现“单模型高效推理”？\n**逻辑推演**：多智能体的过程本质上是生成了丰富的“思维链”。如果能让一个模型学会这些思维过程，就不需要在推理时调用多个模型。\n*   **创新点**：提出了**CoT蒸馏多任务训练框架**。\n    *   将多智能体流水线产生的推理轨迹作为中间监督信号，训练一个单一模型同时完成“推理（规划）+ 说话（文本）+ 行动（多模态标签）”。\n    *   **目的**：将复杂的系统级逻辑内化为单模型的参数，实现了低延迟下的高保真模拟。\n\n### 第四阶段：最终方案合成\n**思考总结**：DemMA不仅仅是一个聊天机器人，而是一个**“临床 grounded 的多模态行为模拟器”**。\n\n**逻辑闭环**：\n1.  **输入端**：通过分层人格模块注入临床病理知识。\n2.  **训练端**：利用多智能体生成的高质量合成数据，通过CoT蒸馏，教会单模型如何像专家一样分析记忆状态、规划对话并匹配非语言行为。\n3.  **输出端**：在一个前向传播中，同时输出符合病理特征的语言、显式的推理逻辑以及对应的动作标签（Motion/Face/Sound），从而在文本界面中补偿了非语言信息的缺失。\n\n---\n\n**总结**：作者的思考路径是从**“数据稀缺”**的现实出发，识别出**“通用模型不适用”**的本质矛盾，进而通过**“结构化病理建模”**确立内容准确性，利用**“多智能体外显推理”**保证数据质量，最后通过**“知识蒸馏”**解决工程效率问题，最终实现了DemMA这一高保真、可落地的痴呆症模拟系统。"
                },
            ]
        },
    ],
    "2026-01-12": [
        {
            "name": "Artificial Intelligence",
            "count": 45,
            "papers": [
                {
                    "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
                    "arxiv_id": "2601.05890",
                    "authors": "Ruizhe Zhang, Xinke Jiang, Zhibang Yang, Zhixin Zhang, Jiaran Gao, Yuzhen Xiao, Hongbin Lai, Xu Chu, Junfeng Zhao, Yasha Wang",
                    "summary": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于大语言模型的集中式分层多智能体框架，重点解决多智能体协作中的记忆管理和协调问题，属于多智能体与记忆机制的研究范畴。",
                    "summary2": "本文旨在解决集中式多智能体系统在长视距协作中因记忆管理缺失导致的不稳定及跨任务泛化能力差的问题。针对复杂长视距任务，我们提出了一种名为StackPlanner的分层多智能体框架，通过解耦协调与执行、引入主动任务记忆管理及结构化经验记忆，并利用强化学习优化协调策略。在2WikiMultiHopQA、MuSiQue、GAIA和FRAMES等基准测试上，通过F1分数验证了其有效性，显著优于现有基线。",
                    "summary_translation": "基于 large language models (大语言模型) 的 Multi-agent systems (多智能体系统)，尤其是 centralized architectures (中心化架构)，近期在处理复杂且知识密集型任务方面展现出巨大潜力。然而，由于缺乏 memory management (记忆管理)，central agents (中心智能体) 常面临不稳定的 long-horizon collaboration (长程协作) 问题，导致 context bloat (上下文膨胀)、error accumulation (错误累积) 以及较差的 cross-task generalization (跨任务泛化) 能力。为解决 task-level memory (任务级记忆) 效率低下及无法复用 coordination experience (协作经验) 的问题，我们提出了 StackPlanner，这是一种具备 explicit memory control (显式记忆控制) 的 hierarchical multi-agent framework (分层多智能体框架)。StackPlanner 通过主动的 task-level memory control (任务级记忆控制) 将 high-level coordination (高层协调) 与 subtask execution (子任务执行) 解耦，并利用 structured experience memory (结构化经验记忆) 和 reinforcement learning (强化学习) 来检索及利用可复用的 coordination experience (协作经验)，从而应对上述挑战。在多个 deep-search (深度搜索) 和 agent system benchmarks (智能体系统基准) 上的实验表明，我们的方法在实现可靠的 long-horizon multi-agent collaboration (长程多智能体协作) 方面具有显著成效。",
                    "inspiration_trace": "基于论文《StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management》，以下是对作者核心方法论产出逻辑链的系统性推演：\n\n### 1. 宏观观察：从“多智能体协作”到“中心化架构的瓶颈”\n**思考起点：**\n作者首先观察到，基于大语言模型的多智能体系统（LLM-MAS）在处理复杂、长周期的知识密集型任务时表现出巨大潜力。虽然去中心化或辩论式架构存在通信开销大、一致性难维护的问题，但**中心化架构**（由一个中央智能体统一调度）因其全局控制能力，逐渐成为主流选择。\n\n**发现问题：**\n然而，随着任务规模和复杂度的提升，现有的中心化架构开始失效。中央智能体在面对长链条推理时，往往会出现“迷失在中间”的现象，导致协作不稳定。\n\n### 2. 深度诊断：核心症结在于“记忆管理的缺失”\n**逻辑推演：**\n为什么中央智能体会失效？作者分析认为，问题不在于LLM的推理能力本身，而在于**信息过载**。\n*   **现象：** 子智能体源源不断地产生信息，中央智能体被动地接收所有原始数据。\n*   **后果：** 上下文窗口迅速膨胀，噪声累积，早期错误在长链条中传播，导致决策偏离。\n\n**关键洞察：**\n现有的系统将“记忆”视为静态的副产品，缺乏主动管理机制。这引出了两个核心挑战：\n1.  **任务级记忆挑战（C1）：** 如何在长周期任务中，主动过滤噪声、压缩信息，防止上下文臃肿？\n2.  **跨任务经验挑战（C2）：** 如何复用历史成功的协作经验，避免每次面对新任务都“从零开始”，从而解决冷启动和泛化能力差的问题？\n\n### 3. 架构重构：从“被动接收”到“主动控制”\n**解决方案构思：**\n为了解决上述挑战，作者决定对系统进行结构性改造，核心思想是**解耦**与**显式化**。\n\n*   **针对C1（任务记忆）的思考：**\n    *   *传统做法：* 简单的截断或模板化摘要（被动）。\n    *   *创新思路：* 将“记忆管理”变成一种**显式的动作**。中央智能体不仅要决定“做什么任务”，还要决定“记忆里留什么”。\n    *   *具体化：* 引入**栈式记忆结构**和**REVISE动作**。允许智能体主动进行“压缩”和“剪枝”，像编辑文档一样编辑自己的记忆，从而保持认知的清晰度。\n\n*   **针对C2（经验记忆）的思考：**\n    *   *传统做法：* 仅依赖LLM的参数化知识。\n    *   *创新思路：* 构建一个结构化的**外部经验库**，专门存储“如何协作”的知识。\n    *   *具体化：* 将经验分为三类：用户画像、语义记忆（事实）、程序性记忆（SOPs，即标准作业程序）。这样，智能体遇到新任务时，可以检索过去的“成功套路”，而不仅仅是事实。\n\n### 4. 机制优化：利用强化学习学习“如何决策”\n**进一步思考：**\n有了架构（分层）和工具（记忆管理），如何保证中央智能体能用好这些工具？仅仅依靠提示工程可能不足以让智能体学会复杂的“何时压缩记忆”或“何时检索经验”的时机。\n\n**最终闭环：**\n作者将整个规划过程建模为一个**可学习的决策过程**。\n*   引入**强化学习（RL）**（具体为GRPO算法）。\n*   目标是训练中央智能体不仅学会任务规划，更要学会**元技能**——即如何最优地管理记忆栈和检索经验。\n*   通过奖励机制，强化那些能够有效利用记忆、减少冗余步骤、成功完成任务的策略。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径呈现出清晰的**“发现问题 -> 归因分析 -> 架构解耦 -> 机制显式化 -> 算法闭环”**的逻辑链条：\n\n1.  **观察：** 中心化多智能体系统在长任务中失效。\n2.  **归因：** 根本原因是缺乏记忆管理，导致上下文臃肿和经验无法复用。\n3.  **架构设计：** 提出“分层架构”，将高层决策与底层执行解耦。\n4.  **核心创新：**\n    *   **任务侧：** 提出“主动记忆管理”，通过REVISE动作动态维护记忆栈。\n    *   **经验侧：** 提出“结构化经验记忆”，存储可复用的协作模式（SOPs）。\n5.  **训练落地：** 利用强化学习，让智能体在试错中学会如何最优地运用上述记忆和经验机制。\n\n这一过程体现了作者从系统架构的宏观视角，深入到认知科学中的记忆机制，最后通过算法手段实现自动化的完整学术创新路径。"
                },
                {
                    "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
                    "arxiv_id": "2601.05787",
                    "authors": "Zezhou Wang, Ziyun Zhang, Xiaoyi Zhang, Zhuzhong Qian, Yan Lu",
                    "summary": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于GUI智能体（计算机使用智能体），提出了通过强化学习和专家轨迹来增强智能体策略的方法。研究内容涉及智能体的规划、执行以及通过反馈进行自我完善，符合单智能体和自我演化的研究范围。尽管使用了视觉输入，但核心在于智能体的能力提升而非纯视觉模型研究。",
                    "summary2": "本文旨在提升端到端GUI智能体性能，解决利用专家轨迹进行强化学习时的结构不匹配与分布偏移问题。针对OSWorld等GUI交互场景，我们提出了一种双层专家到策略同化框架（BEPA），通过自滚动执行和动态缓存更新将静态专家轨迹转化为策略对齐的指导。我们在OSWorld-Verified、MMBench-GUI和Online-Mind2Web上通过成功率验证了其有效性，显著提升了基线模型表现。",
                    "summary_translation": "视觉-语言模型正日益被部署为用于操作桌面和浏览器的计算机使用代理。性能最优异的 CUAs 是基于框架的系统，它们将规划与执行过程解耦；相比之下，端到端的截图到动作策略虽然更易于部署，但在 OSWorld-Verified 等基准测试中表现滞后。诸如 OSWorld 之类的 GUI 数据集存在两个瓶颈：它们仅包含数百个交互式、可验证的任务和环境；此外，专家轨迹必须通过与这些环境进行交互来收集，导致此类数据难以扩展。因此，我们探讨如何利用基于可验证奖励的强化学习，最大限度地利用少量现有的专家轨迹来训练端到端策略。简单地将这些离线策略轨迹混入在线策略的 RLVR 中是脆弱的：即使经过格式转换，专家轨迹与学习器之间仍存在结构不匹配和分布偏移。我们提出了 BEPA (Bi-Level Expert-to-Policy Assimilation，双层专家到策略同化)，该方法通过基础策略生成的自滚动可达轨迹（LEVEL-1）以及 RLVR 中使用的按任务动态更新的缓存（LEVEL-2），将静态的专家轨迹转化为与策略对齐的指导信号。在 OSWorld-Verified 上，BEPA 将 UITARS1.5-7B 的成功率从 22.87% 提升至 32.13%，并将保留集上的表现从 5.74% 提升至 10.30%，同时在 MMBench-GUI 和 Online-Mind2Web 上也取得了持续的提升。我们的代码和数据可在以下地址获取：https://github.com/LEON-gittech/Verl_GUI.git",
                    "inspiration_trace": "基于论文《From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation》，以下是对作者核心方法论逻辑链的系统性推演：\n\n### 1. 宏观问题：端到端智能体的性能瓶颈\n**观察**：在计算机控制领域，存在两种主流范式：\n*   **基于框架的系统**：通过规划器、执行器等多模块协作，性能强大（如 Agent S2），但部署复杂。\n*   **端到端（E2E）策略**：直接从截图映射到动作，部署简单，但在高难度基准（如 OSWorld）上表现显著落后（约 23% vs 33-42%）。\n\n**核心矛盾**：我们希望获得 E2E 的部署便利性，但渴望框架系统的强大性能。如何缩小这一差距？\n\n### 2. 资源约束：数据稀缺与专家轨迹的利用\n**现实困境**：与文本任务不同，GUI 任务数据极难扩展。\n*   任务数量有限（仅几百个）。\n*   获取专家轨迹成本高昂（需在真实环境中交互）。\n\n**假设**：既然我们拥有少量高质量的“基于框架的专家轨迹”，能否利用强化学习（RLVR）将这些专家知识迁移给端到端策略？\n\n### 3. 初步尝试与失败：直接混合的脆弱性\n**直觉方案**：将专家的离线轨迹直接混合到端到端策略的在线强化学习（On-Policy RL）中，作为监督信号。\n\n**失败诊断**：实验表明这种做法非常脆弱，甚至导致性能下降。作者深入分析发现了两个根本性的“不匹配”：\n1.  **结构不匹配**：框架轨迹包含多角色（规划者、执行者）和工具级 API，而 E2E 策略是单一模型且输出低级动作。简单的格式转换无法消除这种差异。\n2.  **分布偏移**：即使格式转换后，专家轨迹在 E2E 策略的概率分布中依然处于极低概率区域（即“离群点”）。在依赖信任域的 RL 算法（如 PPO/GRPO）中，这种巨大的分布差异会导致优化不稳定或探索崩溃。\n\n**结论**：静态的、异构的专家数据无法被 E2E 策略直接吸收。\n\n### 4. 思想转折：从“模仿动作”到“同化意图”\n**核心洞察**：既然直接模仿专家的“动作”行不通，不如让策略去执行专家的“意图”。我们需要一种机制，将专家轨迹转化为策略“可达”的轨迹。\n\n**逻辑推演**：\n*   专家轨迹中蕴含了高层规划（Plan），这是通用的。\n*   E2E 策略具备执行能力，但缺乏规划。\n*   如果提取专家的**计划**，然后让 E2E 策略在**计划条件**下自主执行，生成的轨迹既保留了专家的高层智慧，又符合策略自身的动作分布。\n\n### 5. 方法论构建：双层专家到策略的同化（BEPA）\n基于上述洞察，作者提出了一个双层动态框架，旨在将静态专家知识转化为动态的、策略对齐的指导。\n\n**LEVEL-1：可达性转换**\n*   **目标**：解决“分布偏移”问题。\n*   **思路**：不直接使用专家的动作序列。而是从专家轨迹中提取自然语言计划，将其作为提示附加给 E2E 策略，让策略自己重新执行。\n*   **结果**：生成的“自滚动”轨迹虽然由策略生成，但受专家计划引导，因此既在策略流形上（高概率），又具有高奖励。\n\n**LEVEL-2：动态对齐**\n*   **目标**：解决“静态数据过时”问题。\n*   **思路**：策略在训练中是不断进化的，LEVEL-1 生成的初始引导可能会变得不再最优。因此，建立一个**动态缓存**。\n*   **机制**：\n    1.  初始化时使用 LEVEL-1 的成功轨迹填充缓存。\n    2.  在 RL 训练过程中，如果策略自己探索出了成功轨迹，就更新缓存。\n    3.  **关键设计**：仅在策略完全探索失败（所有 rollout 均失败）时，才从缓存中注入专家引导。这确保了专家数据仅作为“安全网”，而不干扰策略正常的自主探索。\n\n### 6. 逻辑闭环\n通过这一设计，作者成功实现了从 Off-Policy（静态专家）到 On-Policy（动态策略）的平滑过渡：\n1.  **初始化**：利用专家计划“手把手”教策略生成高质量数据（LEVEL-1）。\n2.  **进化**：策略在 RL 中自我探索，并将自己的成功经验替换掉旧的专家数据（LEVEL-2）。\n3.  **结果**：策略逐渐内化了专家能力，最终在未见任务上实现了显著的性能提升（从 22.87% 提升至 32.13%）。\n\n**总结**：作者的核心贡献在于认识到在 GUI 代理训练中，简单的“数据混合”无效，必须通过“计划引导”和“动态缓存”将异构的专家知识转化为策略自身流形上的动态信号，从而实现能力的有效迁移。"
                },
                {
                    "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
                    "arxiv_id": "2601.05746",
                    "authors": "Zhenghao Li, Zhi Zheng, Wei Chen, Jielun Zhao, Yong Chen, Tong Xu, Enhong Chen",
                    "summary": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了基于大语言模型的多智能体辩论框架，涉及多智能体之间的协作、通信以及工具使用，符合多智能体研究范围。",
                    "summary2": "本文旨在解决多智能体辩论中因初始化同质化导致的推理路径单一及错误传播问题。针对复杂推理任务，我们提出了一种DynaDebate框架，通过动态路径生成与分配、以过程为中心的辩论及基于触发器的验证机制来打破同质化。并在GSM8K、MATH500、AIME及MMLU等基准数据集上通过Accuracy等指标验证了其有效性。",
                    "summary_translation": "近年来，基于大语言模型的多智能体系统（Multi-Agent Systems, MAS）发展迅速，在协作决策和复杂问题解决方面表现卓越。近期，研究人员进一步探索了多智能体辩论（Multi-Agent Debate, MAD）框架，该框架通过多个智能体之间的信息交换与辩论，增强了MAS的推理与协作能力。然而，现有方法往往依赖于无引导的初始化，导致智能体采用相同的推理路径，进而陷入相同的错误。因此，智能体间的有效辩论受到阻碍，最终结果往往退化为简单的多数投票。为解决上述问题，本文提出了动态多智能体辩论（Dynamic Multi-Agent Debate, DynaDebate），该方法通过三个关键机制提升了多智能体辩论的有效性：(1) 动态路径生成与分配（Dynamic Path Generation and Allocation），利用专门的路径生成智能体（Path Generation Agent）生成具有自适应冗余的多样化且合乎逻辑的解决方案路径；(2) 以过程为中心的辩论（Process-Centric Debate），将关注点从表层的基于结果的投票转移到严格的逐步逻辑批判，以确保过程的正确性；(3) 基于触发的验证智能体（Trigger-Based Verification Agent），在出现分歧时被激活，并利用外部工具客观地解决僵局。大量实验表明，DynaDebate在各类基准测试中均取得了优异的性能，超越了现有的最先进（State-of-the-Art, SOTA）MAD方法。",
                    "inspiration_trace": "基于论文《DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation》的内容，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n---\n\n### 1. 宏观观察与问题定义：多智能体辩论的“失效”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在复杂推理任务上的局限性，以及多智能体辩论作为一种解决方案的兴起。MAD 的核心逻辑是“三个臭皮匠顶个诸葛亮”，即通过多个智能体的交互和辩论来纠正错误、提升推理能力。\n\n**关键观察：**\n然而，在实际应用中，作者发现现有的 MAD 框架往往表现不佳，甚至退化成了简单的“多数投票”。这意味着智能体之间并没有发生真正有效的辩论，而是陷入了某种形式的“群体思维”。\n\n---\n\n### 2. 深度诊断：同质化与盲从的双重困境\n为了解释上述观察，作者深入剖析了现有方法的两个根本性缺陷：\n\n*   **缺陷一：初始化同质化**\n    *   **现象：** 现有的 MAD 方法通常采用“无引导的初始化”。即让多个基于相同或相似模型的智能体直接面对同一个问题。\n    *   **根源：** 由于模型固有的“思维定势”，这些智能体往往会选择概率最高的同一条推理路径。\n    *   **后果：** 如果这条路径是错的，所有智能体都会犯同样的错。既然大家都错得一样，辩论就无法发现错误，最终只能通过投票选出那个“共同的错误”。\n\n*   **缺陷二：盲从与肤浅的评估**\n    *   **现象：** 即使智能体有不同的初始答案，在辩论过程中，它们也容易被同伴的观点带偏。\n    *   **根源：** 智能体在评估同伴的回答时，往往关注文本的流畅性或结构的完整性（表面特征），而不是逻辑的正确性。\n    *   **后果：** 正确的智能体可能因为错误的同伴回答看起来“很自信”或“很通顺”而放弃自己的正确立场，导致错误的共识。\n\n---\n\n### 3. 假设提出：从“无序碰撞”转向“结构化异构”\n基于上述诊断，作者提出了核心假设：**要打破无效的辩论，必须人为地引入“异构性”并强制进行“深度的逻辑审查”。**\n\n*   **假设 A（关于起点）：** 如果我们在辩论开始前，强制智能体采用**不同且逻辑上独立**的解题思路，就能从源头上打破同质化，最大化对解空间的探索。\n*   **假设 B（关于过程）：** 如果辩论的焦点从“比较最终答案”转移到“审查推理步骤”，就能避免盲从，确保共识建立在逻辑严密性之上。\n*   **假设 C（关于裁决）：** 当逻辑审查无法解决僵局时，引入**客观的外部工具**作为裁判，比单纯的投票更可靠。\n\n---\n\n### 4. 方法论构建：DynaDebate 的三阶段演进\n为了验证上述假设，作者构建了 DynaDebate 框架，其设计逻辑遵循了从“准备”到“执行”再到“兜底”的闭环：\n\n#### 第一阶段：动态路径生成——解决“怎么想”\n*   **设计思路：** 既然模型自己会偷懒走老路，那就引入一个专门的“路径生成智能体”。\n*   **逻辑演进：** 这个生成器不负责解题，只负责“出谋划策”。它需要生成多条逻辑上互斥、但各自可行的解题路径（例如：一道几何题，一条路用代数解，一条路用向量解）。\n*   **分配机制：** 然后将这些路径分配给不同的辩论智能体。如果路径不够多，就采用轮询分配，确保即使方法重复，也能利用随机性进行校验。\n\n#### 第二阶段：以过程为中心的辩论——解决“怎么辩”\n*   **设计思路：** 改变辩论的规则。禁止智能体只说“我觉得你错了”，必须指出“你的第几步推导有问题”。\n*   **逻辑演进：** 引入“第一性原理审计”。智能体必须将推理过程拆解为原子步骤，辩论时针对每一个步骤的逻辑连贯性和事实正确性进行攻击或辩护。这迫使智能体关注逻辑本质，而非文本表象。\n\n#### 第三阶段：基于触发的验证——解决“谁裁决”\n*   **设计思路：** 辩论可能会陷入僵局（公说公有理，婆说婆有理），或者被错误的逻辑主导。\n*   **逻辑演进：** 引入一个“验证智能体”，但它不是一直在线的（为了节省成本），而是基于触发机制（如分歧过大、无法达成共识）才激活。它调用外部工具（如 Python 代码执行器、搜索引擎）给出客观结果，作为打破僵局的“铁证”。\n\n---\n\n### 5. 逻辑闭环与验证\n**最终思考：**\n通过这三个机制，作者构建了一个完整的逻辑闭环：\n1.  **起点：** 用路径生成确保大家想得不一样（打破同质化）；\n2.  **过程：** 用步骤审计确保大家辩得有深度（避免盲从）；\n3.  **终点：** 用工具验证确保最终结果有依据（客观裁决）。\n\n**实验验证：**\n作者通过在数学推理（如 MATH500, AIME）等任务上的实验，证实了这种“结构化异构”确实比单纯的“多智能体堆砌”更有效，甚至能让小模型通过这种协作机制超越大模型的单体表现。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（辩论失效）**出发，深挖**本质（思维同质化与盲从）**，提出**假设（强制异构与过程审查）**，最终设计出一套**分层解耦的解决方案（路径生成+过程辩论+触发验证）**。这一过程体现了从“增加数量”到“提升质量”的范式转变。"
                },
                {
                    "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models",
                    "arxiv_id": "2601.05570",
                    "authors": "Cooper Lin, Maohao Ran, Yanting Zhang, Zhenglin Wan, Hongwei Fan, Yibo Xu, Yike Guo, Wei Xue, Jun Song",
                    "summary": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了Crisis-Bench，这是一个多智能体部分可观察马尔可夫决策过程（POMDP）基准测试，用于评估LLM智能体在动态危机模拟中的战略行为。该研究涉及智能体的规划、状态管理（记忆）以及与模拟环境的交互，符合单智能体和多智能体的研究范围。尽管涉及对齐讨论，但其核心贡献在于构建智能体评估框架而非单纯的对齐或应用研究。",
                    "summary2": "本文旨在解决通用安全对齐在需要战略模糊的专业领域（如危机公关）中的局限性。针对高风险企业危机场景，我们提出了Crisis-Bench，一种基于多智能体POMDP的动态模拟框架，采用双知识架构和仲裁-市场循环机制。我们在涵盖8个行业的80个危机故事线上，通过模拟股价和信任度等指标验证了其有效性，揭示了现有模型在战略推理上的“对齐税”。",
                    "summary_translation": "标准安全对齐优化了大语言模型，使其具备普遍的有用性和诚实性，从而有效地灌输了一种僵化的“童子军”道德观。尽管这种框架对于通用助手而言是稳健的，但这种“一刀切”的伦理框架给需要战略模糊性和信息保留的专业领域（如公共关系、谈判和危机管理）强加了一种“透明度税”。为了衡量通用安全性与专业效用之间的这种差距，我们引入了 Crisis-Bench，这是一个多智能体部分可观察马尔可夫决策过程，用于在高风险的企业危机中评估大语言模型。Crisis-Bench 涵盖了跨越 8 个行业的 80 个多样化故事线，要求基于大语言模型的公共关系代理应对动态的 7 天企业危机模拟，同时管理严格分离的私有和公开叙事状态，以执行严格的信息不对称。与依赖静态基本事实的传统基准不同，我们引入了裁决者-市场循环：这是一种新颖的评估指标，其中公众情绪受到裁决并转化为模拟股价，从而创建了一个现实的经济激励结构。我们的结果揭示了一个关键的二分法：虽然一些模型向伦理担忧屈服，但其他模型展示了马基雅维利式的、合法的战略保留能力，以稳定模拟股价。Crisis-Bench 提供了首个用于评估“声誉管理”能力的定量框架，主张从僵化的道德绝对主义转向具有情境感知能力的专业对齐。",
                    "inspiration_trace": "基于对论文《Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观观察与范式冲突\n**（从“通用安全”到“专业效用”的矛盾）**\n\n1.  **现象观察**：作者首先注意到LLM正从通用的聊天机器人向专业领域的智能代理转型（如法律、公关、谈判）。\n2.  **发现问题**：现有的主流对齐范式（如RLHF）旨在训练一种“童子军”式的道德观——即普遍的“有益、诚实、无害”。\n3.  **提出冲突**：作者敏锐地指出，这种“一刀切”的道德框架在专业领域反而是一种阻碍。在危机公关或谈判中，绝对的诚实往往是 liabilities（负债），而“战略模糊”才是核心能力。\n4.  **核心假设**：当前的通用安全对齐实际上对专业领域施加了一种“透明度税”，导致模型在需要信息管理和声誉维护的高风险任务中表现无能。\n\n### 第二阶段：理论构建与核心概念界定\n**（从“静态真理”到“信息不对称”的视角转换）**\n\n1.  **批判现有基准**：作者反思现有的基准测试（如MMLU）大多基于静态的、二元对立的“真理”。但在现实世界中，真相是一个需要被管理的动态资产，而非单纯的事实检索。\n2.  **引入核心概念**：为了衡量这种能力，作者引入了“马基雅维利式”的战略思维——即利用信息不对称来保护客户利益的能力。\n3.  **定义关键能力**：这不仅仅是撒谎，而是“心智理论”在专业语境下的应用：严格区分“我知道什么（私有知识）”和“公众知道什么（公有知识）”，并利用这种差异进行战略决策。\n\n### 第三阶段：方法论设计——构建动态博弈场\n**（从“问答测试”到“多智能体模拟”的演进）**\n\n1.  **场景选择**：为了验证上述假设，作者需要一个高风险、强对抗且结果可量化的场景。最终选定“企业危机公关”作为切入点，因为它天然包含信息博弈。\n2.  **架构创新（双知识架构）**：为了模拟真实的信息不对称，作者设计了“私有知识库”和“公有知识库”的分离架构。这是整个方法论的基石，迫使模型必须在“泄露信息”与“隐瞒信息”之间做权衡。\n3.  **环境控制（POMDP建模）**：作者没有选择让LLM自由生成剧情（这会导致不可控的方差），而是采用了“部分可观察马尔可夫决策过程”（POMDP）。\n    *   **思考逻辑**：为了保证公平性和可复现性，必须有一个固定的“真相卷宗”和“事件池”。\n    *   **引入路由器**：为了模拟现实的因果逻辑，引入Router智能体从固定池中选择最符合叙事逻辑的事件，而非随机生成。\n\n### 第四阶段：评估指标的创新——经济激励闭环\n**（从“语义评分”到“市场反馈”的量化）**\n\n1.  **评估难题**：危机公关没有标准答案。一句公关辞令的好坏不取决于文本本身，而取决于公众的接受度及其带来的经济后果。\n2.  **解决方案（仲裁-市场循环）**：作者设计了一个独特的评估闭环：\n    *   **仲裁者**：一个LLM作为公众代表，对PR代理的回应进行多维度打分（问责、透明度、同理心、成本信号）。\n    *   **市场模拟**：将这些定性分数转化为定量的“模拟股价”。\n3.  **设计意图**：通过引入“股价”这一经济指标，作者成功地将抽象的“声誉管理”转化为具体的数学优化问题。这迫使模型不能只做“好人”（过度道歉导致财务重创），也不能只做“坏人”（缺乏信任导致股价崩盘），必须寻找“马基雅维利式的平衡点”。\n\n### 第五阶段：假设验证与结论升华\n**（从“实验数据”到“对齐哲学”的反思）**\n\n1.  **实验预期**：作者预测，过度对齐的模型（如Claude）会拒绝参与；过于“讨好”的模型会因过度赔偿而损害股价；只有具备战略推理能力的模型才能在信任与成本之间取得最优解。\n2.  **结果验证**：实验结果证实了“透明度悖论”——过度的诚实反而会加剧危机，导致股价暴跌。\n3.  **最终结论**：作者通过Crisis-Bench证明了“对齐税”的存在，并呼吁AI社区从僵化的“道德绝对主义”转向“情境感知的专业对齐”。\n\n---\n\n**总结：**\n作者的思考路径是一条清晰的**“问题发现 -> 理论重构 -> 环境建模 -> 激励设计 -> 假设验证”**链条。其核心创新在于跳出了NLP传统的“文本相似度”或“事实准确性”评价体系，转而从**博弈论**和**经济学**的视角，通过构建一个具有真实经济后果的模拟环境，来衡量LLM在复杂社会交互中的战略生存能力。"
                },
                {
                    "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering",
                    "arxiv_id": "2601.05465",
                    "authors": "Yu Liu, Wenxiao Zhang, Cong Cao, Wenxuan Lu, Fangfang Yuan, Diandian Guo, Kun Peng, Qiang Sun, Kaiyan Zhang, Yanbing Liu, Jin B. Hong, Bowen Zhou, Zhiyuan Ma",
                    "summary": "Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个名为PRISMA的多智能体架构，包含Planner、Inspector和Solver等组件，明确涉及智能体间的协作与通信。同时，该架构涵盖了规划、记忆和自我反思等核心智能体特征，符合多智能体和单智能体的研究范围。",
                    "summary2": "本文旨在解决开放域多跳问答中的检索崩溃与端到端训练不稳定问题。针对大规模语料库上的复杂多跳查询，我们提出了PRISMA，一种基于强化学习引导的Plan-Retrieve-Inspect-Solve-Memoize多智能体框架，利用两阶段GRPO和OARPO实现推理引导的协作与策略优化。并在十个基准数据集上通过EM和F1指标验证了其有效性，取得了SOTA性能。",
                    "summary_translation": "在针对海量语料库回答现实世界的开放域多跳问题时，检索增强生成（RAG）系统面临着严峻挑战。近期研究利用强化学习（RL）对检索增强推理过程进行端到端优化，从而直接提升系统解决复杂查询的能力。然而，可靠的部署受到两个主要障碍的制约：1) 检索崩溃：在缺乏推理引导规划的情况下，针对大规模语料库的迭代检索无法定位包含桥接答案的中间证据，从而导致下游推理崩溃。2) 学习不稳定性：端到端轨迹训练面临推理链中信用分配微弱以及模块间错误定位不佳的问题，导致模型过度拟合于特定基准的启发式规则，从而限制了其可迁移性和稳定性。为解决上述问题，我们提出了 PRISMA，这是一个采用 Plan-Retrieve-Inspect-Solve-Memoize（计划-检索-检查-解决-记忆）架构的解耦式 RL 引导框架。PRISMA 的优势在于推理引导的协作机制：检查器提供基于推理的反馈，以优化规划器的分解任务和细粒度检索，同时在求解器中强制执行基于证据的推理。我们通过两阶段组相对策略优化（GRPO）来优化各个智能体的能力。第一阶段将规划器和求解器校准为规划和推理领域的专业化专家；第二阶段利用观察感知残差策略优化（OARPO）来增强检查器验证上下文及触发针对性恢复的能力。实验结果表明，PRISMA 在十个基准测试中取得了最先进的性能，并且能够在现实世界场景中高效部署。",
                    "inspiration_trace": "基于论文《PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到最终产出的思考过程：\n\n---\n\n### 1. 宏观观察：现实世界问答的复杂性\n**思考起点：** 作者首先关注到现实世界中的开放域问答（Open-Domain QA）并非简单的单轮检索，而是涉及跨越海量语料库的多跳推理。\n*   **现象：** 面对类似“2021年诺贝尔奖获奖的TRPV1抑制剂与2025年报道的Spike蛋白诱导的神经病理性疼痛的主要抑制剂是否相同？”这类复杂问题，单纯的LLM（大语言模型）缺乏外部知识，而传统的RAG（检索增强生成）往往只能处理单步检索，无法串联起分散在不同文档中的“桥梁”信息。\n*   **核心矛盾：** 解决这类问题需要同时具备**规划**（拆解问题）、**检索**（精准定位证据）和**推理**（基于证据生成答案）三种能力。任何一环的断裂都会导致整个链条崩溃。\n\n### 2. 问题诊断：现有方法的两大痛点\n在尝试利用现有技术（如SFT、迭代式RAG、端到端RL）解决上述矛盾时，作者发现了两个关键瓶颈，这成为了PRISMA设计的直接动因：\n\n*   **痛点一：检索崩溃**\n    *   **观察：** 传统的迭代检索（如ReAct）往往是机械的“推理-检索”循环。如果没有明确的规划，系统很难在海量语料中找到中间的“桥梁答案”。\n    *   **结论：** 缺乏推理引导的规划，导致检索在第一步就迷失方向，后续推理自然崩溃。\n\n*   **痛点二：学习不稳定**\n    *   **观察：** 端到端的强化学习（RL）试图同时优化所有模块，但面临严重的“信用分配”难题。当最终答案错误时，很难界定是规划错了、检索错了还是推理错了。\n    *   **结论：** 这种模糊性导致模型容易过拟合于数据集的特定启发式规则，缺乏泛化能力和训练稳定性。\n\n### 3. 概念灵感：模拟人类研究者的工作流\n为了解决上述痛点，作者跳出算法细节，转向**认知仿生**。\n*   **类比：** 人类研究者是如何解决复杂问题的？\n    1.  **Plan（规划）：** 将大问题拆解为有依赖关系的子问题。\n    2.  **Retrieve（查阅）：** 针对子问题寻找资料。\n    3.  **Inspect（检查）：** *关键步骤*——在阅读资料时判断是否足够，在得出结论时检查逻辑是否严密。如果不对，就回退修改。\n    4.  **Solve（解决）：** 综合信息得出答案。\n    5.  **Memoize（记忆）：** 记录关键发现以备后用。\n*   **假设：** 如果构建一个多智能体架构，让不同的Agent分别扮演上述角色，并通过“检查”环节形成反馈闭环，就能解决检索崩溃问题。\n\n### 4. 架构演进：从“单兵作战”到“协作推理”\n基于人类工作流的假设，作者设计了PRISMA的架构，核心在于引入了**Inspector（检查员）**这一角色。\n\n*   **第一步：专业化分工**\n    *   **Planner：** 负责生成依赖感知的子问题，解决“去哪找”的问题。\n    *   **Solver：** 负责基于证据生成有引用的答案，解决“怎么答”的问题。\n    *   **Memoizer：** 负责缓存和复用，提高效率。\n\n*   **第二步：引入反馈闭环**\n    *   作者意识到仅有分工是不够的，必须要有质量控制。因此引入了**Inspector**，并将其分为两个阶段：\n        *   **Context Inspector（上下文检查）：** 在Solver工作前，检查子问题是否清晰、检索到的文档是否足够。如果不够，触发重写或扩展检索。\n        *   **Reasoning Inspector（推理检查）：** 在Solver工作后，检查答案是否基于证据、提取是否准确。如果有误，触发重试。\n    *   **逻辑突破：** 这种设计将传统的“单向流水线”变成了“带反馈的协作网络”，直接针对“检索崩溃”和“错误传播”进行了防御。\n\n### 5. 训练策略演进：解耦RL以解决“学习不稳定”\n架构设计好了，如何训练？作者反思了端到端RL的失败教训，提出了**两阶段解耦**的训练策略。\n\n*   **阶段一：专家校准**\n    *   **思考：** 既然信用分配很难，那就先不要混在一起。先让Planner和Solver各自成为“专家”。\n    *   **方法：** 使用GRPO（Group Relative Policy Optimization）分别优化Planner（奖励：规划质量）和Solver（奖励：答案准确性和引用忠实度）。这一步确立了系统的基准能力。\n\n*   **阶段二：残差审计**\n    *   **思考：** 专家也会犯错。现在需要训练Inspector来捕捉这些“残差错误”。但Inspector不能只看问题，它必须看到“专家做了什么”才能判断对错。\n    *   **方法：** 冻结Planner和Solver，训练Inspector。关键创新在于**OARPO（Observation-Aware Residual Policy Optimization）**：\n        *   **输入增强：** Inspector的输入不仅是问题，还包含了专家的执行轨迹。\n        *   **目标：** 学习在专家轨迹的基础上，如何进行审计和触发恢复。\n    *   **逻辑闭环：** 这种设计将复杂的端到端优化分解为“先练能力，再练纠错”，极大地降低了训练难度，解决了学习不稳定的问题。\n\n### 6. 最终方法论：PRISMA的诞生\n综合上述思考，作者最终确立了PRISMA的核心逻辑：\n*   **架构上：** 通过Plan-Retrieve-Inspect-Solve-Memoize的多智能体协作，模拟人类研究者的闭环工作流，利用Inspector的反馈机制防止检索崩溃。\n*   **训练上：** 通过两阶段GRPO（先专家校准，后残差审计），解耦了复杂的信用分配问题，确保了系统的稳定性和泛化能力。\n\n**总结：** 作者的思考路径是从**现实问题的复杂性**出发，诊断出**检索与学习的双重困境**，借鉴**人类认知模式**构建协作架构，最后通过**解耦强化学习**策略实现了稳定高效的落地。这是一条从“发现问题”到“仿生设计”再到“工程化落地”的完整逻辑链条。"
                },
                {
                    "title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
                    "arxiv_id": "2601.05302",
                    "authors": "Mizuki Sakai, Mizuki Yokoyama, Wakaba Tateishi, Genki Ichinose",
                    "summary": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
                    "category": "cs.AI",
                    "filter_reason": "该论文研究LLM智能体在重复囚徒困境博弈中的合作行为，属于多智能体协作与博弈的研究范畴，符合筛选条件。",
                    "summary2": "本文旨在探究人格引导对LLM智能体合作行为的影响。针对GPT-3.5、GPT-4o和GPT-5模型，我们提出了一种基于Big Five框架的人格测量与操纵方法，并在重复囚徒困境（RPD）游戏环境中，通过平均合作率和平均累积收益验证了其有效性。结果表明宜人性是促进合作的主导因素，且人格引导表现为行为偏差而非确定性控制。",
                    "summary_translation": "大语言模型越来越多地被用作策略与社会互动中的自主代理。尽管近期研究表明，赋予大语言模型人格特质可以影响其行为，但在受控条件下，人格引导如何影响合作尚不明确。在本研究中，我们利用重复囚徒困境博弈，考察了人格引导对大语言模型代理合作行为的影响。基于大五人格框架，我们首先利用大五人格量表测量了 GPT-3.5-turbo、GPT-4o 和 GPT-5 这三个模型的基本人格画像。随后，我们比较了模型在基线条件和人格引导条件下的行为，并进一步分析了将各个人格维度独立操纵至极端值时的影响。结果显示，宜人性是促进所有模型合作的主导因素，而其他人格特质的影响则较为有限。明确的人格信息虽然能增加合作，但也可能增加被剥削的脆弱性，这一点在早期代模型中尤为明显。相比之下，新一代模型则表现出更具选择性的合作行为。这些发现表明，人格引导表现为一种行为偏差，而非一种确定性的控制机制。",
                    "inspiration_trace": "基于论文《Effects of personality steering on cooperative behavior in Large Language Model agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体实验设计的思考过程：\n\n### 第一阶段：宏观背景与问题识别\n**（从“LLM智能体化”到“行为不可控”的焦虑）**\n\n1.  **观察现象**：随着大语言模型（LLM）被广泛应用于多智能体系统，它们开始处理复杂的战略和社会交互（如谈判、资源分配）。\n2.  **发现问题**：传统的基于规则的系统是可控的，但基于LLM的智能体虽然具备更强的推理能力，其行为却变得**不可预测**，甚至可能引发意外的冲突升级。\n3.  **引入视角**：为了解决这种不可控性，作者将目光投向心理学中的“人格”理论。既然人格能预测和影响人类行为，那么它是否也能成为引导LLM智能体行为的“方向盘”？\n\n### 第二阶段：批判性回顾与缺口分析\n**（对现有研究的质疑：缺乏“基准线”与“定量”思维）**\n\n1.  **审视现有文献**：已有研究表明，给LLM赋予人格可以影响其合作行为。\n2.  **发现逻辑漏洞**：作者敏锐地指出了现有研究的两个致命缺陷：\n    *   **缺乏基准测量**：直接给模型“赋予”某种人格，却从未测量过模型“原本”的人格是什么。这就像在不知道一个人原本性格的情况下强行改变他，无法区分干预效果和模型固有倾向。\n    *   **缺乏定量控制**：以往的人格设定往往是定性的描述（如“你是一个外向的人”），缺乏量化的强度控制，导致实验难以复现且无法比较不同维度的影响权重。\n3.  **提出核心假设**：人格引导不应是一个简单的开关，而应是一种**可量化的行为偏差**。且这种偏差的效果可能受到模型本身推理能力（代际差异）的调节。\n\n### 第三阶段：方法论构建的逻辑演进\n**（从“测量”到“干预”再到“解构”的三步走策略）**\n\n为了验证上述假设并填补缺口，作者设计了一套层层递进的逻辑闭环：\n\n**步骤一：建立基准——量化固有人格**\n*   **思考**：在干预之前，必须先“诊断”。我们需要知道不同模型（GPT-3.5, GPT-4o, GPT-5）在未被引导时的出厂设置是什么。\n*   **方法**：采用心理学标准的“大五人格量表（BFI-44）”对模型进行测试。\n*   **目的**：获得一个定量的“人格基线”，为后续的干预提供参照系。\n\n**步骤二：验证引导效应——自我意识的唤醒**\n*   **思考**：如果模型“知道”自己的人格特征，它的行为会发生改变吗？这种改变是盲目的还是策略性的？\n*   **方法**：设计“重复囚徒困境（RPD）”实验。\n    *   **对照组（Baseline）**：不给任何人格提示。\n    *   **实验组**：将步骤一测得的“真实人格分数”明确告诉模型。\n*   **目的**：通过对比，剥离出“人格自我认知”对合作行为的净影响，并观察不同代际模型在面对非合作对手时是否表现出不同的脆弱性。\n\n**步骤三：解构因果机制——极端值压力测试**\n*   **思考**：大五人格包含五个维度，到底哪个维度对“合作”起决定性作用？是综合作用还是单一主导？\n*   **方法**：采用**控制变量法**。保持其他四个维度不变，仅将某一个维度（如宜人性）推向极端值（最低1或最高5）。\n*   **目的**：通过这种“压力测试”，精准定位出影响合作行为的核心因子（即文中发现的“宜人性”），并排除其他维度的干扰。\n\n### 第四阶段：综合洞察与理论升华\n**（从“数据”到“机制”的最终解释）**\n\n1.  **数据整合**：结合三个阶段的实验数据，作者发现“宜人性”是驱动合作的主导因素，而其他维度影响甚微。\n2.  **代际差异分析**：对比GPT-3.5和GPT-5，作者发现老一代模型会盲目跟随高宜人性导致被剥削，而新一代模型（GPT-5）能结合战略推理，表现出“选择性合作”。\n3.  **结论提炼**：最终得出核心论点——**人格引导不是一种确定性的控制机制，而是一种行为偏差**。它必须与模型内在的战略推理能力相互作用，才能决定最终的行为结果。\n\n---\n\n**总结**：\n作者的思考路径遵循了严谨的科学探究逻辑：\n**发现问题（不可控） -> 寻找工具（人格心理学） -> 批判前人（缺乏定量基准） -> 建立基准（测量固有人格） -> 验证干预（注入人格信息） -> 解构机制（极端值控制实验） -> 理论升华（偏差论与代际差异）。**"
                },
                {
                    "title": "Over-Searching in Search-Augmented Large Language Models",
                    "arxiv_id": "2601.05503",
                    "authors": "Roy Xie, Deepak Gopinath, David Qiu, Dong Lin, Haitian Sun, Saloni Potdar, Bhuwan Dhingra",
                    "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "该论文研究了搜索增强型LLM中的“过度搜索”问题，重点分析了模型何时以及如何“调用搜索工具”。这属于单智能体研究中的“工具使用”范畴，涉及智能体对工具调用的决策机制和优化。",
                    "summary2": "本文旨在解决搜索增强大语言模型中过度搜索导致的计算低效和幻觉问题。针对多种查询类型、模型类别及多轮对话场景，我们提出了系统性的评估框架，引入了Tokens Per Correctness (TPC)指标，并发布了OverSearchQA基准数据集。我们在OverSearchQA上通过Answer Accuracy、Abstention Accuracy和TPC验证了过度搜索现象的存在及缓解策略的有效性。",
                    "summary_translation": "搜索增强型大型语言模型通过整合外部检索，在知识密集型任务中表现出色。然而，它们经常出现过度搜索——即在不提高响应质量的情况下不必要地调用搜索工具，这导致了计算效率低下，并因引入不相关的上下文而产生幻觉。在这项工作中，我们从多个维度对过度搜索进行了系统评估，包括查询类型、模型类别、检索条件和多轮对话。我们的研究发现： 搜索通常能提高可回答查询的答案准确性，但会损害模型对不可回答查询的拒答能力； 过度搜索在复杂推理模型和深度研究系统中更为显著，且会因噪声检索而加剧，并在多轮对话中逐轮累积； 检索证据的构成至关重要，因为负面证据的存在有助于改善拒答表现。为了量化过度搜索，我们引入了 Tokens Per Correctness (TPC，每正确度Token数) 这一评估指标，用于捕捉搜索增强型大型语言模型的性能与成本之间的权衡。最后，我们探讨了查询和检索层面的缓解方法，并发布了 OverSearchQA 数据集，以促进针对高效搜索增强型大型语言模型的持续研究。",
                    "inspiration_trace": "基于论文《Over-Searching in Search-Augmented Large Language Models》，以下是对作者核心思想产出过程的系统性逻辑推演：\n\n### 第一阶段：现象观察与问题提出\n**（从“搜索增强”的普遍成功到“无效搜索”的隐性成本）**\n\n1.  **宏观背景**：当前学术界和工业界普遍认为，给大语言模型（LLM）配备搜索工具（RAG或Web Search）是解决知识幻觉、提升事实准确性的标准范式。\n2.  **异常观察**：作者在实际应用中发现，虽然搜索工具确实提升了模型在“可回答问题”上的表现，但在面对“不可回答问题”（如未知未来、错误前提、模糊语境）时，模型往往表现得比基座模型更差。\n3.  **核心矛盾**：模型倾向于“过度搜索”——即在不该搜索的时候（如问题本身无解或模型已知答案）依然频繁调用搜索工具。这种行为不仅增加了计算成本，还可能因为引入无关或误导性的检索内容，导致模型产生幻觉或错误回答。\n4.  **初步假设**：现有的搜索增强模型缺乏“搜索理性”，即它们无法判断何时搜索是有益的，何时是无用的。\n\n### 第二阶段：概念定义与量化指标\n**（如何将“过度搜索”从一个直觉转化为可测量的科学问题）**\n\n1.  **形式化定义**：为了研究这个问题，作者首先需要定义什么是“过度搜索”。他们将其定义为：**当搜索行为带来的边际正确率收益趋近于零，但计算成本持续累积时的状态**。\n2.  **指标构建的痛点**：传统的评估指标（如Accuracy、EM）只关注“答对了吗”，忽略了“花了多少代价”。一个模型可能通过疯狂搜索把准确率从80%提到81%，但成本增加了10倍，这在实际应用中是不可接受的。\n3.  **引入新指标**：为了捕捉“性能-成本”的权衡，作者提出了**TPC（Tokens Per Correctness，每正确性所需的Token数）**。这个指标将生成Token数、输入上下文长度和搜索调用次数统一折算为成本，迫使研究者在追求准确率的同时必须考虑效率。\n\n### 第三阶段：实验设计与数据构建\n**（如何排除干扰，精准定位问题根源）**\n\n1.  **数据集的缺陷**：现有的QA数据集大多只关注“可回答”的问题。要研究“过度搜索”，必须引入“不可回答”的样本，且要控制样本难度，确保模型表现差异源于“是否可搜索”而非“题目难易”。\n2.  **构建基准**：作者构建了**OverSearchQA**数据集，精心平衡了“可回答”与“不可回答”（未知、错误前提、上下文不足）的样本，并确保它们在语义和长度上相似，从而排除了数据偏差。\n3.  **多维变量控制**：为了探究过度搜索的成因，作者设计了多维度的实验变量：\n    *   **模型维度**：对比基座模型、推理模型和深度研究模型。\n    *   **检索维度**：对比高质量语料（Wikipedia）、过时语料和噪声语料。\n    *   **交互维度**：对比单轮对话与多轮对话。\n\n### 第四阶段：机制分析与归因\n**（从现象到本质：为什么模型会“过度搜索”？）**\n\n1.  **推理能力的副作用**：实验发现，推理能力越强的模型（如o1系列），过度搜索现象越严重。这表明当前的强化学习训练范式（鼓励长思维链）可能诱导了模型“多想多做”，即使是不必要的搜索。\n2.  **检索噪声的诱导**：当检索源充满噪声时，模型会进行更多次搜索试图“淘金”，导致TPC飙升。这说明模型缺乏对检索质量的判断力。\n3.  **证据构成的偏差**：作者深入分析了检索到的文档内容，发现现实世界的语料库中，绝大多数是“正向证据”（支持某种答案），而极少包含“负向证据”（明确指出问题无解）。这种数据偏差导致模型误以为“搜不到”是因为“搜得不够”，而不是“问题无解”。\n4.  **多轮对话的“滚雪球”效应**：在多轮对话中，如果前几轮问题都是可回答的，模型会形成“搜索惯性”，导致在后续遇到不可回答问题时也倾向于继续搜索。\n\n### 第五阶段：缓解策略与局限性反思\n**（从治标到治本的思考）**\n\n1.  **尝试缓解**：作者尝试了两种层面的干预：\n    *   **查询层**：通过提示词让模型自我评估或提供拒绝回答的示例。结果发现这能提升拒绝率，但往往以牺牲可回答问题的准确率为代价。\n    *   **检索层**：人为向语料库中注入“负向证据”。结果发现效果有限，因为合成文档很难被自然检索到。\n2.  **根本性反思**：现有的缓解策略（Prompt工程、检索增强）只能治标。作者得出结论，过度搜索的根源在于**模型训练目标**——目前的训练只奖励“最终答案的正确性”，而不惩罚“过程的低效性”。\n3.  **最终产出**：文章最终不仅提出了问题、指标和数据集，更指出了未来研究的方向：必须改变训练范式，让模型学会“理性的工具使用”，而不仅仅是“更准确”。\n\n---\n\n**总结：**\n作者的思考路径遵循了经典的科研逻辑：**发现异常现象（搜索反而导致错误） $\\rightarrow$ 定义量化标准（TPC） $\\rightarrow$ 构建受控实验（OverSearchQA） $\\rightarrow$ 剖析深层机制（训练偏差与证据缺失） $\\rightarrow$ 评估现有方案并指出根本局限**。这一过程将一个工程上的“效率问题”上升为了对模型“认知边界”和“工具理性”的系统性探讨。"
                },
                {
                    "title": "PRISM: Protocol Refinement through Intelligent Simulation Modeling",
                    "arxiv_id": "2601.05356",
                    "authors": "Brian Hsu, Priyanka V Setty, Rory M Butler, Ryan Lewis, Casey Stone, Rebecca Weinberg, Thomas Brettin, Rick Stevens, Ian Foster, Arvind Ramanathan",
                    "summary": "Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了PRISM框架，明确使用了基于语言模型的智能体来协同生成和完善实验步骤。文中涉及多智能体协作、规划与批判循环（自我反思）以及工具使用（协调机器人仪器），完全符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决self-driving laboratories中实验协议设计与自动化的瓶颈问题。针对将科学意图转化为可执行机器人协议的场景，我们提出了一种结合multi-agent LLM规划与基于NVIDIA Omniverse digital twin仿真验证的PRISM框架，并在Luna qPCR和Cell Painting实验上通过F1分数及物理可行性验证了其有效性。",
                    "summary_translation": "实验方案设计与执行的自动化仍然是实现 self-driving laboratories (自动驾驶实验室) 的根本瓶颈。我们介绍了 PRISM (Protocol Refinement through Intelligent Simulation Modeling，通过智能仿真建模进行方案优化)，这是一个在由 off-the-shelf robotic instruments (现成机器人仪器) 组成的实验室平台上，自动化实验方案的设计、验证和执行的框架。PRISM 采用一组 language-model-based agents (基于语言模型的智能体) 协同工作来生成和优化实验步骤。该过程首先从描述 experimental workflows (实验工作流) 的 web-based sources (网络来源) 中自动收集相关程序。这些程序通过 planning, critique, and validation loop (规划、批判和验证循环) 转化为 structured experimental steps (结构化实验步骤)（例如，liquid handling steps (液体处理步骤)、deck layout (台面布局) 和其他相关操作）。最终确定的步骤被转化为 Argonne MADSci protocol format (Argonne MADSci 协议格式)，该格式提供了一个 unified interface (统一接口)，用于协调多个 robotic instruments (机器人仪器)（Opentrons OT-2 liquid handler (液体处理机)、PF400 arm (机械臂)、Azenta plate sealer and peeler (封板机和揭膜机)），而无需在步骤之间进行 human intervention (人工干预)。为了评估 protocol-generation performance (方案生成性能)，我们在 constrained and open-ended prompting paradigms (受限和开放式提示范式) 下，对 single reasoning models (单一推理模型) 和 multi-agent workflow (多智能体工作流) 进行了基准测试。生成的方案在基于 NVIDIA Omniverse 构建的 digital-twin environment (数字孪生环境) 中进行了验证，以便在执行前检测 physical or sequencing errors (物理或排序错误)。通过使用 Luna qPCR amplification (Luna qPCR 扩增) 和 Cell Painting (细胞染色) 作为案例研究，我们展示了 PRISM 作为一个实用的 end-to-end workflow (端到端工作流)，它连接了 language-based protocol generation (基于语言的方案生成)、simulation-based validation (基于仿真的验证) 和 automated robotic execution (自动化机器人执行)。",
                    "inspiration_trace": "基于论文《PRISM: Protocol Refinement through Intelligent Simulation Modeling》的内容，以下是对作者提出核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法论构建的思考过程。\n\n---\n\n### 1. 宏观观察与问题定义\n**思考起点：** 自动化实验室的“最后一公里”在哪里？\n*   **观察：** 尽管机器人硬件（如移液机器人、机械臂）日益普及，但“自动驾驶实验室”仍未实现。瓶颈不在于硬件的执行能力，而在于**如何将科学家的意图转化为机器人可执行的、无误的代码**。\n*   **痛点识别：**\n    *   **人工编程门槛高：** 编写机器人协议需要深厚的领域知识（实验流程）和工程知识（硬件API），这限制了自动化技术的普及。\n    *   **现有AI方案的缺陷：** 大语言模型（LLM）虽然能生成实验步骤，但经常出现参数缺失、逻辑错误或物理不可行（如机械臂够不着、设备未开启就操作）的问题。\n    *   **试错成本高昂：** 直接在真实硬件上测试未经验证的协议，会导致设备损坏、试剂浪费和时间损失。\n\n### 2. 现有技术局限性的深度剖析\n**思考演进：** 为什么现有的解决方案（协议语言、纯LLM生成、传统仿真）无法单独解决问题？\n*   **协议语言（如XDL, Autoprotocol）：** 它们是静态的、硬件特定的。一旦实验室配置改变，协议需要人工重写。它们缺乏对物理可行性的预验证能力。\n*   **纯LLM生成（如ChemCrow, BioPlanner）：** LLM擅长逻辑推理和文本生成，但缺乏“物理常识”。它们生成的往往是半结构化的伪代码，且无法感知空间约束（碰撞检测）或时序约束（设备状态）。\n*   **数字孪生/仿真技术：** 目前主要用于实验后的监控或文档记录，而非作为协议生成流程中的**主动验证环节**。\n\n**核心洞察：** 现有的技术栈是割裂的。我们需要一个系统，既能利用LLM的**认知与规划能力**，又能利用仿真的**物理验证能力**，并将两者紧密结合。\n\n### 3. 核心假设提出：仿真作为“强制守门员”\n**逻辑转折点：** 如何解决LLM“不懂物理”的问题？\n*   **假设：** 如果我们将仿真环境不仅仅视为一个可视化工具，而是视为一个**严格的批评者和验证器**，嵌入到LLM的生成循环中，就能在代码接触真实硬件之前，消除所有物理层面的错误。\n*   **概念形成：** 建立“生成-仿真-反馈-修正”的闭环。LLM生成代码 -> 仿真运行 -> 仿真报错（如碰撞） -> LLM根据报错修正代码 -> 直到仿真通过。\n\n### 4. 方法论构建：分层解耦与模块化设计\n**思考深化：** 为了实现上述闭环，如何处理实验设计的复杂性？\n*   **问题分解：** 实验设计包含两个截然不同的维度：\n    1.  **科学逻辑：** 试剂怎么配、步骤顺序对不对（这是生物学/化学问题）。\n    2.  **物理逻辑：** 机器人怎么动、会不会撞、设备开关顺序（这是机器人学问题）。\n*   **架构设计（PRISM框架）：**\n    *   **阶段一：协议规划（解决科学逻辑）。**\n        *   *思考：* LLM在处理长程、多步骤任务时容易遗忘。因此，引入**多智能体框架**（WebSurfer, Planner, Critique, Validator）来分工合作，比单一模型更可靠。\n        *   *产出：* 结构化的自然语言步骤（而非代码），确保科学意图准确。\n    *   **阶段二：协议生成与迭代验证（解决物理逻辑）。**\n        *   *思考：* 将自然语言转化为机器人可读的YAML代码。这是最容易出物理错误的地方。\n        *   *创新点：* 引入**NVIDIA Omniverse数字孪生**作为必经关卡。只有通过物理碰撞检测、可达性检查的代码，才被允许进入真实世界。\n    *   **阶段三：真实世界执行。**\n        *   *思考：* 验证Sim-to-Real的迁移能力。\n\n### 5. 验证与反思：证明仿真的必要性\n**思考闭环：** 如何证明这个复杂的框架是必要的？\n*   **实验设计：** 对比“有仿真反馈”与“无仿真反馈（仅LLM自纠）”的表现。\n*   **预期结果与发现：**\n    *   LLM在文本层面可能认为自己的代码是完美的（自纠能力有限），但在仿真中会立即暴露出诸如“试图将板子放入未打开的热循环仪”这种低级但致命的物理错误。\n    *   这证明了**仿真反馈是连接“AI认知”与“物理现实”不可或缺的桥梁**。\n\n### 总结：作者的逻辑演进图谱\n1.  **发现瓶颈：** 实验室自动化的阻碍在于“意图到执行”的转化，且缺乏安全性验证。\n2.  **批判现状：** 单纯的LLM不可靠（缺乏物理约束），单纯的仿真太被动（未参与生成）。\n3.  **提出假设：** 将仿真作为LLM生成的物理约束层，通过迭代反馈消除错误。\n4.  **系统构建：** 设计“多智能体规划（保科学正确）+ 仿真迭代验证（保物理可行）”的分层流水线。\n5.  **实证价值：** 通过消融实验证明，没有仿真层，AI生成的协议在物理世界中几乎必然失败。"
                },
                {
                    "title": "KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits",
                    "arxiv_id": "2601.05257",
                    "authors": "Hou-Wan Long, Yicheng Song, Zidong Wang, Tianshu Sun",
                    "summary": "Sponsored search advertising (SSA) requires advertisers to constantly adjust keyword strategies. While bid adjustment and keyword generation are well-studied, keyword pruning-refining keyword sets to enhance campaign performance-remains under-explored. This paper addresses critical inefficiencies in current practices as evidenced by a dataset containing 0.5 million SSA records from a pharmaceutical advertiser on search engine Meituan, China's largest delivery platform. We propose KP-Agent, an LLM agentic system with domain tool set and a memory module. By modeling keyword pruning within a contextual bandit framework, KP-Agent generates code snippets to refine keyword sets through reinforcement learning. Experiments show KP-Agent improves cumulative profit by up to 49.28% over baselines.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了KP-Agent，这是一个包含领域工具集和记忆模块的LLM智能体系统。它利用LLM生成代码片段（工具使用）并结合上下文赌博机框架进行决策，符合单智能体研究范围中的“工具使用”和“记忆”特征。虽然应用于广告领域，但其核心贡献在于智能体架构与机制，而非纯应用。",
                    "summary2": "本文旨在解决Sponsored Search Advertising中Keyword Pruning效率低下的问题。针对广告主侧数据，我们提出了一种基于LLM的Agentic System，即KP-Agent。该方法利用Contextual Bandit框架，结合Domain-Specialized Toolset和Memory Module生成代码进行修剪。我们在Meituan数据集上通过Cumulative Profit验证了其有效性，相比基线提升了49.28%。",
                    "summary_translation": "Sponsored search advertising (SSA) (赞助搜索广告) 要求广告主不断调整 keyword strategies (关键词策略)。虽然 bid adjustment (出价调整) 和 keyword generation (关键词生成) 已得到充分研究，但 keyword pruning (关键词修剪)——即 refining keyword sets (优化关键词集合) 以提升 campaign performance (广告活动表现)——仍是一个未被充分探索的领域。本文旨在解决当前实践中存在的关键效率瓶颈，这一点通过一个包含50万条 SSA 记录的数据集得到了证实，该数据集源自中国最大外卖平台美团上的某医药广告主。我们提出了 KP-Agent，这是一个集成了 domain tool set (领域工具集) 和 memory module (记忆模块) 的 LLM agentic system (大语言模型智能体系统)。通过在 contextual bandit framework (上下文强盗框架) 内对 keyword pruning (关键词修剪) 进行建模，KP-Agent 利用 reinforcement learning (强化学习) 生成 code snippets (代码片段) 以优化 keyword sets (关键词集合)。实验结果表明，与 baselines (基线模型) 相比，KP-Agent 将 cumulative profit (累积利润) 提升了高达 49.28%。",
                    "inspiration_trace": "基于论文《KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 第一阶段：宏观视角与问题定位\n**——从“红海”中寻找被忽视的“蓝海”**\n\n1.  **行业背景观察**：\n    作者首先立足于搜索广告（SSA）这一巨大的市场，观察到学术界和工业界长期聚焦于两个核心环节：**出价调整**（Bidding，即花多少钱）和**关键词生成**（Generation，即买什么词）。\n2.  **发现研究缺口**：\n    在这两个成熟领域之外，作者敏锐地捕捉到了第三个关键但被严重忽视的环节——**关键词修剪**（Pruning，即剔除什么词）。\n3.  **提出核心假设**：\n    作者假设：在预算有限的前提下，关键词并非越多越好。低效关键词会“稀释”预算，导致高价值关键词得不到足够的资金支持。因此，**“做减法”**（修剪）可能是提升ROI的关键杠杆。\n\n### 第二阶段：数据驱动的痛点验证\n**——用真实数据打破“静态管理”的幻想**\n\n1.  **实证分析（基于美团数据）**：\n    为了验证上述假设，作者分析了50万条真实SSA记录。\n    *   **发现一（帕累托效应）**：极少数的关键词贡献了绝大多数的利润，大量长尾关键词在浪费预算。\n    *   **发现二（管理惰性）**：在21天内，仅有4.6%的关键词被调整。这说明人工或现有的基于规则的系统无法适应市场的动态变化。\n2.  **界定约束条件**：\n    作者意识到，现有的修剪方法（如基于用户搜索意图的相关性分析）在学术界虽有研究，但在工业界难以落地，因为**广告主无法获取搜索引擎端的用户查询数据**（属于平台隐私数据）。\n3.  **明确问题边界**：\n    因此，核心问题被定义为：**如何仅利用广告主侧的可见数据（如KPI指标），设计一个自适应的、高频的关键词修剪策略？**\n\n### 第三阶段：技术选型与博弈\n**——LLM的优势与短板的权衡**\n\n1.  **引入LLM的动机**：\n    传统的静态规则（如“删除CTR最低的10%”）过于僵化，无法处理复杂的动态市场环境。作者认为，LLM具备强大的推理能力和灵活性，适合处理这种需要根据上下文动态决策的任务。\n2.  **识别LLM的致命弱点**：\n    然而，直接让LLM处理表格数据（KPI报表）会导致严重的“幻觉”问题，即LLM不擅长精确的数值计算和逻辑推理。\n3.  **思维跃迁（核心创新点）**：\n    作者提出了一种**“解耦”**策略：**让LLM负责“思考”（策略制定），让代码负责“执行”（数值计算）。**\n    *   不直接让LLM输出“删除关键词A”，而是让LLM生成一段Python代码。\n    *   这段代码调用预定义的领域工具（如排序、过滤函数）来操作表格。\n    *   这样既利用了LLM的语义理解能力，又规避了其计算短板。\n\n### 第四阶段：方法论构建与系统化\n**——从单次决策到持续进化的智能体**\n\n1.  **框架选择：上下文老虎机**：\n    作者将关键词修剪建模为一个Contextual Bandit问题。因为每次修剪决策主要依赖于当前的广告状态（上下文），而不需要像强化学习那样考虑长期的多步状态转移，这符合广告投放即时反馈的特性。\n2.  **增强智能体能力**：\n    为了让LLM生成的代码更精准，作者引入了两个模块：\n    *   **记忆模块**：存储过去成功的修剪案例。通过检索相似的历史状态，为LLM提供Few-shot示例，实现经验复用。\n    *   **反思模块**：记录修剪后的市场反馈（利润变化），形成反思文本，存入记忆。这使得系统能够像人类一样“吃一堑长一智”。\n3.  **闭环形成**：\n    最终形成了“观察状态 -> 检索记忆 -> LLM推理 -> 生成代码 -> 执行修剪 -> 获取奖励 -> 反思存储”的完整闭环。\n\n### 第五阶段：验证与结论\n**——证明“减法”的价值**\n\n1.  **实验设计**：\n    在真实数据集上进行回测模拟，对比传统的基于规则的方法（如按展示量、CTR、CVR排序修剪）。\n2.  **结果解读**：\n    实验证明，KP-Agent不仅提升了利润（最高49.28%），而且在允许更激进修剪（即保留更少关键词）时，优势更明显。这反向验证了最初的假设：**精准的剔除比盲目的扩张更能带来价值。**\n\n---\n\n**总结：作者的思考脉络**\n从发现**“修剪”**这一被忽视的工业痛点出发，通过数据证实**“静态规则”**的失效，进而引入**LLM**以解决灵活性需求，但为了克服LLM**“不擅长计算”**的缺陷，创造性地提出了**“代码生成+工具调用”**的范式，最后通过**记忆与反思机制**将其封装为一个具备进化能力的智能体系统。"
                },
                {
                    "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                    "arxiv_id": "2601.07611",
                    "authors": "Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",
                    "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个多智能体框架（DIAGPaper），包含审稿人代理和作者代理，通过结构化辩论进行协作与通信，以识别和验证论文弱点。这完全符合多智能体（协作、通信）的研究范围。",
                    "summary2": "本文旨在解决现有论文弱点识别系统模拟肤浅、缺乏有效性验证及未排序的问题。针对科学论文评审场景，我们提出了一种DIAGPaper多智能体框架，包含Customizer、Rebuttal和Prioritizer三个模块，分别负责定制评审标准、通过作者辩论验证弱点以及按严重程度排序。在AAAR和ReviewCritique数据集上，通过Semantic F1和Specificity等指标验证了其有效性。",
                    "summary_translation": "使用单智能体或多智能体大语言模型进行论文弱点识别已受到越来越多的关注，然而现有方法存在关键局限性。许多多智能体系统仅在表层模拟人类角色，未能捕捉到专家用于评估论文互补智力维度的潜在标准。此外，先前的方法隐含地假设识别出的弱点是有效的，忽略了审稿人偏见、误解以及作者反驳在验证审稿质量中的关键作用。最后，大多数系统输出未排序的弱点列表，而非为用户优先考虑影响最大的问题。在这项工作中，我们提出了DIAGPaper，这是一个新颖的多智能体框架，通过三个紧密集成的模块来解决这些挑战。定制器模块模拟人类定义的审稿标准，并实例化多个具备特定标准专业知识的审稿人智能体。反驳模块引入作者智能体，使其与审稿人智能体进行结构化辩论，以验证和完善提出的弱点。优先级排序器模块从大规模人类审稿实践中学习，以评估已验证弱点的严重程度",
                    "inspiration_trace": "基于对论文《DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning》的深入分析，以下是作者构建该方法的逻辑演进过程推演：\n\n### 1. 宏观观察：从“角色扮演”到“专家思维”的缺失\n**起点：** 自动化论文审稿领域正从单一LLM向多智能体系统演进。\n**观察：** 现有的多智能体系统（如AgentReview, MARG）大多停留在“表面模拟”阶段。它们只是简单地给智能体分配角色（如“审稿人”、“作者”或“领域主席”），或者按文本段落分工。\n**问题识别：** 真正的人类专家审稿并非仅仅因为身份不同，而是因为**关注的具体评价维度不同**。现有系统缺乏对“评价标准”的显式建模，导致生成的评论泛泛而谈，缺乏针对性。\n\n### 2. 深度诊断：有效性与实用性的双重危机\n在进一步观察中，作者发现了两个更深层次的逻辑漏洞：\n*   **漏洞一（有效性假设谬误）：** 现有系统默认AI生成的弱点是正确的。但在现实中，审稿人常有偏见或误解。**作者反驳**是验证评论质量的关键环节，而现有系统大多忽略了这一“纠错”机制。\n*   **漏洞二（输出效用低）：** 即使生成了正确的弱点，系统通常以平铺列表的形式输出。然而，对于作者而言，区分“致命缺陷”和“轻微瑕疵”至关重要。缺乏优先级排序使得AI审稿的实用性大打折扣。\n\n### 3. 核心假设：模拟“机制”而非模拟“人”\n**假设提出：** 要提高AI审稿的质量，不应只模拟审稿人的“身份”，而应模拟高质量审稿的“内在机制”。\n**逻辑推演：**\n*   机制一：**定制化规划**。专家在拿到论文后，会根据论文内容动态确定审查重点（如：这篇论文主要贡献是数据集，那么审查重点就是数据质量，而非数学推导）。\n*   机制二：**对抗性验证**。评论的有效性不是自证的，而是在与作者的辩论中确立的。只有经得起反驳的弱点，才是真正的弱点。\n*   机制三：**后果导向**。弱点的严重程度取决于其对最终录用决策的影响权重。\n\n### 4. 方法论构建：三模块闭环架构\n基于上述假设，作者构建了DIAGPaper框架，将思考过程转化为三个紧密耦合的模块：\n\n*   **第一步：解构专家思维 -> Customizer（定制器模块）**\n    *   *思考：* 如何让智能体像专家一样有针对性？\n    *   *方案：* 不再使用固定的角色，而是引入一个“定制器”智能体。它先阅读论文，动态生成具体的、细粒度的评价维度（如“数据集的代表性如何？”），然后据此实例化多个具有特定专长的“审稿人智能体”。\n\n*   **第二步：引入对抗验证 -> Rebuttal（反驳模块）**\n    *   *思考：* 如何过滤掉那些看似合理实则错误的幻觉评论？\n    *   *方案：* 引入“作者智能体”。针对每一个审稿人提出的弱点，作者智能体进行逐点反驳。这是一个多轮的、基于证据的辩论过程。如果审稿人无法提供充分的证据或逻辑来支撑其观点，该弱点就会被过滤掉（实验显示过滤掉了40%-60%的初始弱点）。\n\n*   **第三步：模拟决策权重 -> Prioritizer（优先级模块）**\n    *   *思考：* 如何让输出对用户最友好？\n    *   *方案：* 学习人类Meta-review（综合讨论）的行为。分析大量历史数据，计算出不同类别的弱点（如方法缺陷 vs 写作问题）对最终拒稿/录用的影响权重。结合辩论后的有效性得分，对幸存的弱点进行排序，只输出Top-K最严重的问题。\n\n### 5. 逻辑验证与闭环\n**最终思考：** 这个框架是否真的有效？\n*   *验证逻辑：* 如果这个框架是正确的，那么它应该能显著提升开源模型的表现（通过结构化思维弥补能力不足），并且在“有效性”指标上远超现有方法。\n*   *结果确认：* 实验表明，通过DIAGPaper的“多智能体化”，开源模型能达到接近GPT-4o的水平，且生成的弱点在“有效性”和“特异性”上均显著优于基线。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（现有多智能体系统肤浅）**出发，深入到**本质（缺乏评价标准、缺乏验证机制、缺乏优先级）**，最终通过**机制重构（动态定制、对抗辩论、严重度排序）**实现了对人类审稿深层逻辑的还原。"
                },
                {
                    "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents",
                    "arxiv_id": "2601.07577",
                    "authors": "Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",
                    "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于解决LLM智能体在长视界任务中的规划问题，提出了任务解耦规划（TDP）框架，涉及规划器和执行器等智能体架构，属于单智能体规划的研究范畴。",
                    "summary2": "本文旨在解决长视界智能体规划中上下文纠缠导致的鲁棒性差和效率低问题。针对复杂长视界任务，我们提出了一种Task-Decoupled Planning (TDP)框架，通过Supervisor构建任务DAG，并利用Planner和Executor在局部作用域内解耦规划与执行。我们在TravelPlanner、ScienceWorld和HotpotQA上通过Delivery、Accuracy和Average Reward等指标验证了其有效性，结果表明TDP在提升性能的同时将token消耗降低了82%。",
                    "summary_translation": "大语言模型的最新进展已使智能体能够自主执行复杂的 long-horizon tasks（长视界任务），然而规划仍然是实现可靠任务执行的主要瓶颈。现有方法通常分为两种范式：step-wise planning（逐步规划），具有反应性但往往较为短视；以及 one-shot planning（一次性规划），能够预先生成完整计划，但对执行错误较为脆弱。关键在于，这两种范式都存在 entangled contexts（纠缠上下文）的问题，即智能体必须基于跨越多个子任务的 monolithic history（整体历史）进行推理。这种纠缠增加了 cognitive load（认知负荷），并导致 local errors（局部错误）在原本独立的决策之间传播，从而使得错误恢复的计算成本高昂。为解决这一问题，我们提出了 Task-Decoupled Planning (TDP，任务解耦规划)，这是一个 training-free（免训练）框架，旨在用任务解耦替代纠缠推理。TDP 通过 Supervisor（监督者）将任务分解为由子目标组成的 directed acyclic graph (DAG，有向无环图)。通过利用具有 scoped contexts（限定上下文）的 Planner（规划器）和 Executor（执行器），TDP 将推理和重新规划的范围限制在 active sub-task（当前活动子任务）内。这种隔离机制防止了 error propagation（错误传播），并能够在不干扰 workflow（工作流）的情况下局部修正 deviations（偏差）。在 TravelPlanner、ScienceWorld 和 HotpotQA 上的实验结果表明，TDP 不仅优于强大的 baselines（基线模型），还将 token consumption（令牌消耗）减少了高达 82%，证明了子任务解耦能够提升 long-horizon agents（长视界智能体）的 robustness（鲁棒性）和 efficiency（效率）。",
                    "inspiration_trace": "基于论文《Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents》，以下是对作者核心方法论提出过程的逻辑链推演与思想还原：\n\n### 1. 宏观观察：长程任务的规划瓶颈\n**思考起点：** 随着大语言模型（LLM）能力的提升，智能体已经能够处理复杂的、长周期的自主任务。然而，作者发现尽管模型的理解和推理能力在增强，**“规划”** 依然是制约智能体在长程任务中表现可靠性的核心瓶颈。\n*   **现象：** 任务越复杂、步骤越多，智能体越容易迷失方向或执行失败。\n*   **初步问题：** 现有的规划方法为什么无法有效支撑长程任务？\n\n### 2. 现状剖析：两种范式的共通缺陷\n**思考过程：** 作者首先审视了当前领域内解决规划问题的两大主流范式，试图找出它们的局限性。\n*   **范式 A：逐步规划**\n    *   *特点：* 边思考边行动（如 ReAct）。\n    *   *优点：* 反应快，能适应反馈。\n    *   *缺点：* 目光短浅，缺乏全局观，容易在长程任务中走偏。\n*   **范式 B：一次性规划**\n    *   *特点：* 先生成完整计划再执行（如 Plan-and-Act）。\n    *   *优点：* 具备全局视野。\n    *   *缺点：* 脆弱，一旦执行出错或环境变化，原计划容易失效。\n*   **深度洞察（关键转折）：** 作者发现，虽然这两种方法在“规划粒度”上截然不同（一个细碎，一个宏观），但它们在**底层设计逻辑**上存在一个惊人的共同缺陷——**“上下文纠缠”**。\n    *   *问题本质：* 两者都将整个任务视为一个**单一的、整体的工作流**。智能体在推理时，必须依赖一个不断增长的、混合了所有子任务信息的“整体历史记录”。\n\n### 3. 核心洞察：从“粒度”转向“耦合”\n**思考深化：** 既然调整规划的“粒度”（更细或更粗）无法根本解决问题，作者意识到问题的根源不在于“多久规划一次”，而在于“信息是如何组织的”。\n*   **痛点分析：**\n    1.  **认知负荷过载：** 当上下文窗口中塞满了所有子任务的历史细节时，模型难以聚焦于当前需要解决的子问题。\n    2.  **错误传播：** 如果在子任务 A 中出现局部错误，由于上下文是纠缠的，模型往往需要重新审视甚至重做无关的子任务 B，导致计算成本高昂且脆弱。\n*   **假设提出：** 如果能打破这种“纠缠”，将任务进行**解耦**，就能隔离错误并降低推理负担。\n*   **核心思想：** **任务解耦**。即：将长程任务拆解为独立的子任务，让每个子任务的规划与执行都在**受限的局部上下文**中进行，互不干扰。\n\n### 4. 方法论构建：任务解耦的架构设计\n**思考落地：** 为了实现“解耦”这一抽象概念，作者需要设计一套具体的架构，将“全局视野”与“局部执行”分离开来。\n\n*   **第一步：全局结构化**\n    *   *需求：* 既然要解耦，就需要一个顶层结构来定义子任务之间的关系，否则系统会散架。\n    *   *设计：* 引入 **Supervisor（监督者）**。它的职责不是做具体执行，而是将大任务分解为有依赖关系的**有向无环图（DAG）**。这定义了“做什么”以及“先做什么”。\n\n*   **第二步：局部化执行**\n    *   *需求：* 确保执行子任务 A 时，完全看不到子任务 B 的具体执行细节，只看结果。\n    *   *设计：* 引入 **Planner（规划器）** 和 **Executor（执行器）**。\n    *   *关键机制：* **作用域上下文**。这两个模块只能看到当前节点（子任务）的描述、前置节点的结果以及当前节点的执行轨迹。这种设计强制实现了“上下文隔离”。\n\n*   **第三步：局部化纠错**\n    *   *需求：* 当执行出错时，不能推倒重来，只能局部修复。\n    *   *设计：* 当发生偏差时，触发**节点级重规划**。只修改当前节点的计划，而不影响 DAG 中其他已完成或未开始的部分。这从机制上切断了错误传播的路径。\n\n*   **第四步：动态一致性维护**\n    *   *需求：* 局部执行可能会导致全局目标不可达（例如：前置任务的结果改变了后续任务的条件）。\n    *   *设计：* 引入 **Self-Revision（自我修正）**。在每批节点完成后，检查全局状态，更新 DAG（如修改节点描述、增删节点），确保全局与局部的一致性。\n\n### 5. 逻辑闭环：局部化与全局性的平衡\n**思考验证：** 作者通过这套架构（TDP），试图证明一个观点：**通过显式的架构设计控制上下文范围，比单纯依赖模型的推理能力更有效。**\n*   **预期结果：**\n    *   **鲁棒性：** 错误被锁在局部，不会扩散。\n    *   **效率：** 模型不需要反复处理无关的长历史，Token 消耗大幅降低。\n*   **实验验证：** 选取 TravelPlanner（工具调用）、ScienceWorld（交互控制）、HotpotQA（多跳推理）三个差异巨大的场景进行验证，证明这种“解耦”思想具有普适性。\n\n---\n\n**总结：**\n作者的思考路径是从**表象问题**（长程任务规划难）出发，透过**现有方法的共性缺陷**（上下文纠缠），抓住了**本质矛盾**（认知负荷与错误传播），最终提出了**“任务解耦”**这一核心范式，并通过**Supervisor-Planner-Executor**的三层架构将这一思想工程化，实现了从“调整粒度”到“解耦架构”的范式跃迁。"
                },
                {
                    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
                    "arxiv_id": "2601.07477",
                    "authors": "Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",
                    "summary": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于优化基于LLM的智能体工作流。它提出了一种“评估-判断-优化-更新”流水线，利用Judge模块分析执行轨迹并定位问题逻辑块，进而由LLM优化器修改工作流结构。这属于“自我演化”（通过反馈自我完善）和“单智能体”（自我反思/工作流结构）的研究范畴，而非纯推理或纯应用研究。",
                    "summary2": "本文旨在解决LLM智能体工作流优化中缺乏细粒度反馈信号导致效率低下的问题。针对复杂的智能体工作流，我们提出了一种名为JudgeFlow的Evaluation-Judge-Optimization-Update流水线，通过引入可复用的逻辑块和专门的Judge模块分析执行轨迹并定位问题模块。我们在数学推理和代码生成基准上通过准确率和pass@1验证了其有效性，结果表明该方法优于现有基线。",
                    "summary_translation": "优化基于大语言模型（LLM）的智能体工作流对于扩展人工智能能力而言是一项挑战。现有方法依赖于粗糙的端到端评估信号，缺乏关于具体改进位置的细粒度信号，往往导致低效或低影响力的修改。为了解决这些局限性，我们提出了 JudgeFlow，一种评估-判断-优化-更新流水线。我们将可复用、可配置的逻辑块整合到智能体工作流中，以捕捉基本的逻辑形式。在此抽象基础上，我们设计了一个专用的 Judge 模块，用于检查执行轨迹——特别是失败的运行——并为有问题的逻辑块分配基于排名的责任分数。这些细粒度的诊断信号随后被基于大语言模型的优化器利用，该优化器将修改集中在工作流中最有问题的逻辑块上。我们的方法提高了样本效率，通过块级诊断增强了可解释性，并为自动化日益复杂的智能体工作流提供了可扩展的基础。我们在数学推理和代码生成基准上评估了 JudgeFlow，结果表明 JudgeFlow 相比现有方法实现了更优越的性能和效率。源代码已在 https://github.com/ma-zihan/JudgeFlow 公开提供。",
                    "inspiration_trace": "基于论文《JudgeFlow: Agentic Workflow Optimization via Block Judge》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观问题与现状观察\n**思考起点：如何自动化构建高效的智能体工作流？**\n*   **背景**：随着大模型（LLM）的发展，基于LLM的智能体工作流在解决复杂任务（如数学推理、代码生成）上表现出色。然而，这些工作流的设计目前高度依赖人工经验（如手工设计Prompt、多Agent协作拓扑），成本高且难以扩展。\n*   **现有趋势**：受AutoML启发，学术界开始尝试自动化优化这些工作流。现有的自动化方法（如基于MCTS的搜索、图结构优化）大多将工作流视为一个整体进行端到端的优化。\n\n### 第二阶段：痛点识别与核心瓶颈\n**思考深入：为什么现有的自动化优化效率低下？**\n*   **观察**：现有的优化方法主要依赖“粗粒度”的反馈信号——即只看最终任务是否成功。\n*   **瓶颈分析**：\n    1.  **盲目搜索**：如果只知道“结果错了”，优化器不知道“错在哪里”。这导致优化过程像“盲人摸象”，只能对整个工作流进行随机的、低效的修改（如随机增删模块），样本效率极低。\n    2.  **归因困难**：代码形式的工作流虽然表达能力强，但内部包含复杂的控制流（如循环、条件分支）。当任务失败时，很难精准定位是哪一行代码或哪一个模块导致了错误，特别是那些在特定路径上未被执行的组件。\n\n### 第三阶段：提出假设与关键洞察\n**核心假设：如果能像调试代码一样，精准定位工作流中的“错误源”，就能实现高效的针对性优化。**\n*   **洞察**：优化过程不应是“全局随机试错”，而应是“诊断-治疗”的过程。\n*   **需求转化**：我们需要一种机制，能够从失败的执行轨迹中提取**细粒度的诊断信号**，明确指出工作流中哪个部分对失败负有最大责任。\n\n### 第四阶段：方法论的构建与演进\n为了实现上述假设，作者需要解决两个子问题：**“诊断什么”**（分析对象）和**“如何诊断”**（诊断机制）。\n\n**1. 抽象层设计：从“代码”到“逻辑块”**\n*   **思考**：直接对代码行进行诊断太细碎且难以理解；对整个工作流诊断又太粗糙。我们需要一个中间层。\n*   **创新点**：引入**“逻辑块”**概念。\n    *   将工作流抽象为三种基本逻辑形式的组合：顺序、循环、条件。\n    *   **目的**：这既保留了代码的表达能力，又封装了控制流细节，为诊断提供了一个语义清晰、结构稳定的分析单元。\n\n**2. 诊断机制设计：引入“法官”模块**\n*   **思考**：如何判断哪个逻辑块是“罪魁祸首”？人类专家会看执行日志，LLM也可以。\n*   **创新点**：设计**Judge模块**。\n    *   利用LLM作为“法官”，专门分析**失败案例**的执行轨迹。\n    *   它不关注最终得分，而是对工作流中的各个逻辑块进行**责任排序**，找出导致失败的最关键的那个块。\n\n**3. 优化策略设计：从“全局修改”到“局部手术”**\n*   **思考**：有了诊断结果，优化器该如何行动？\n*   **创新点**：构建**Evaluation-Judge-Optimization-Update闭环**。\n    *   Optimizer不再盲目搜索，而是根据Judge指出的“最差块”，进行针对性的操作（修改该块、删除该块或在该块前后插入新块）。\n\n### 第五阶段：最终逻辑框架的形成\n**总结：JudgeFlow 的诞生**\n*   作者将上述思考整合为一个统一的流水线：\n    1.  **Evaluation**：运行工作流，收集成功/失败信号。\n    2.  **Judge**：对失败案例进行“尸检”，利用逻辑块抽象进行归因，输出最需改进的模块。\n    3.  **Optimization**：LLM优化器根据诊断信号，对特定模块进行精准修补。\n    4.  **Update**：更新工作流池，进入下一轮迭代。\n\n**逻辑演进图示：**\n> **宏观问题**（自动化Agent设计）\n> ↓\n> **现有缺陷**（端到端信号太粗，搜索效率低）\n> ↓\n> **核心假设**（细粒度错误归因能提升效率）\n> ↓\n> **关键支撑**（逻辑块抽象 + LLM法官诊断）\n> ↓\n> **最终方案**（JudgeFlow：诊断驱动的针对性优化闭环）"
                },
                {
                    "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
                    "arxiv_id": "2601.07470",
                    "authors": "Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",
                    "summary": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于LLM智能体的核心组件——记忆管理，提出了元认知记忆抽象方法（MCMA）来优化智能体的记忆结构、抽象和重用能力，属于单智能体研究中的“记忆”与“自我反思”范畴。",
                    "summary2": "本文旨在解决LLM智能体记忆表示固定、抽象层次单一导致的泛化受限和负迁移问题。针对长视距交互决策任务，我们提出了一种Meta-Cognitive Memory Abstraction (MCMA) 方法，通过解耦任务执行与记忆管理，利用DPO训练Memory Copilot学习分层结构化记忆抽象策略。在ALFWorld、ScienceWorld和BabyAI数据集上，通过任务成功率、执行步数和奖励分数验证了其有效性和跨任务迁移能力。",
                    "summary_translation": "大语言模型智能体日益依赖累积记忆来解决长视界决策任务。然而，大多数现有方法将记忆存储在固定表示中，并在单一或隐式抽象层级上进行重用，这限制了泛化能力，且在发生分布偏移时往往导致负迁移。本文提出了元认知记忆抽象方法，该方法将记忆抽象视为一种可学习的认知技能，而非固定的设计选择。MCMA 通过结合冻结的任务模型与可学习的记忆副驾驶，实现了任务执行与记忆管理的解耦。该记忆副驾驶利用直接偏好优化进行训练，负责确定记忆的结构化、抽象及重用方式。记忆被进一步组织成抽象层级体系，从而能够基于任务相似度实现选择性重用。当不存在可迁移的记忆时，MCMA 通过迁移记忆副驾驶来传递抽象和管理记忆的能力。在 ALFWorld、ScienceWorld 和 BabyAI 上的实验表明，相较于多个基线方法，MCMA 在性能、分布外泛化以及跨任务迁移方面均实现了显著提升。",
                    "inspiration_trace": "基于论文《Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程。\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**（从“记忆很重要”到“现有记忆机制很脆弱”）**\n\n1.  **观察背景**：随着LLM智能体从静态问答转向长视界、交互式的复杂任务（如ALFWorld, ScienceWorld），智能体必须依赖“程序性记忆”来积累经验，以实现持续决策。\n2.  **发现问题**：尽管现有方法都在尝试存储和检索记忆，但在面对环境变化或任务分布偏移时，性能急剧下降，甚至出现“负迁移”。\n3.  **初步诊断**：现有的记忆机制过于僵化。它们大多将记忆视为静态的“内容”，用固定的格式（如纯文本、固定的键值对）和固定的抽象层级来存储。\n\n### 第二阶段：深入诊断与核心矛盾\n**（从“方法失效”到“抽象困境”）**\n\n1.  **剖析现有范式**：\n    *   **检索式**：直接复用历史轨迹。这导致过度拟合表面细节，一旦环境物体位置改变，记忆即失效。\n    *   **总结/抽象式**：试图提取高层规则。但这面临**“抽象困境”**：太细粒度则过拟合，太抽象则失去可执行性，变成正确的废话。\n    *   **训练式**：将经验内化到模型参数中。这导致记忆与策略耦合，难以跨任务迁移，且容易发生灾难性遗忘。\n2.  **提炼核心矛盾**：现有方法都是**“预设”**了记忆应该如何被表示和抽象。智能体并没有学会“如何记忆”，它只是在使用一个人类设计好的、僵化的存储桶。\n3.  **关键洞察**：人类之所以能灵活迁移记忆，是因为我们拥有**元认知**能力——即“关于思考的思考”。我们不仅存储知识，还学会了“如何组织知识”的认知技能。\n\n### 第三阶段：假设提出与范式转移\n**（从“存储内容”到“学习技能”）**\n\n1.  **核心假设**：记忆抽象不应是一个固定的工程设计，而应是一个**可习得的认知技能**。如果让智能体学会“如何记忆”，它就能自适应地决定记忆的结构和粒度。\n2.  **概念创新**：提出**“元认知记忆抽象”**。目标不是生成完美的记忆内容，而是训练一个能够根据任务需求动态生成记忆结构的“管理者”。\n3.  **架构构想**：为了验证这一假设，必须将“记忆管理”与“任务执行”解耦。如果混在一起，就无法单独评估记忆管理策略的好坏。\n\n### 第四阶段：方法论构建与逻辑闭环\n**（从“概念”到“Memory Copilot”）**\n\n1.  **解耦设计**：\n    *   **任务模型**：保持冻结，只负责执行动作，作为评估记忆好坏的“裁判”。\n    *   **记忆副驾驶**：这是核心创新点。它是一个独立的模型，专门负责将原始轨迹转化为结构化记忆。\n2.  **解决“抽象困境”的机制**：\n    *   **多结构生成**：不预设单一结构，而是让Copilot从树、链、键值对等多种原语中组合出最合适的记忆结构。\n    *   **基于效用的训练**：如何训练Copilot？利用任务模型的下游表现作为反馈。如果某种结构的记忆让任务完成得又快又好，这种结构就被奖励。\n3.  **训练算法选择**：采用**直接偏好优化（DPO）**。通过对比不同记忆结构带来的任务效果，构建偏好对，让Copilot学会生成那些能带来高任务效用的记忆表示。\n\n### 第五阶段：泛化与终极迁移\n**（从“复用知识”到“复用能力”）**\n\n1.  **分层抽象**：为了适应不同相似度的任务，构建记忆层级（从具体的情节记忆到抽象的语义记忆）。相似任务用细节记忆，不相似任务用抽象记忆。\n2.  **解决零样本迁移**：当遇到一个完全陌生的领域，没有任何旧记忆可以复用时怎么办？\n3.  **最终逻辑升华**：此时，我们不再转移“记忆内容”，而是转移**“记忆Copilot本身”**。因为Copilot学到的是“如何从新经验中提炼知识”的元认知能力。这种能力是跨域通用的。\n\n---\n\n**总结：作者的思考路径**\n从**“记忆内容僵化导致泛化失败”**的观察出发，通过**“引入元认知视角”**将问题转化为**“学习记忆抽象技能”**，进而通过**“任务/记忆解耦”**和**“基于效用的DPO训练”**实现了这一技能的习得，最终达成**“不仅复用知识，更复用学习能力”**的通用智能体目标。"
                },
                {
                    "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents",
                    "arxiv_id": "2601.07468",
                    "authors": "Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",
                    "summary": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于为个性化LLM智能体设计一种记忆框架（TSM），旨在解决记忆的时间维度建模问题。这直接符合“单智能体：记忆”的研究范围。",
                    "summary2": "本文旨在解决现有LLM Agent记忆方法在时间维度上的不准确性和碎片化问题。针对个性化对话场景，我们提出了一种Temporal Semantic Memory (TSM)框架，通过构建语义时间线和持续记忆来整合时序连续信息。我们在LONG MEM EVAL和LOCOMO数据集上通过Accuracy指标验证了其有效性，实验表明TSM在多会话理解和时间推理任务上显著优于现有基线方法。",
                    "summary_translation": "记忆机制使 Large Language Model (LLM) agents (大语言模型智能体) 能够感知、存储并利用过往对话中的信息，这对于实现个性化至关重要。然而，现有方法未能对记忆的时间维度进行恰当建模，主要体现在两个方面：1) 时间不准确性：记忆是按对话时间而非实际发生时间进行组织的；2) 时间碎片化：现有方法侧重于 point-wise memory (点状记忆)，从而丢失了能够捕捉持久状态和演变模式的持续信息。为解决上述局限性，我们提出了 Temporal Semantic Memory (TSM) (时间语义记忆)，这是一个为 point-wise memory (点状记忆) 建模 semantic time (语义时间)，并支持 durative memory (持续记忆) 构建与利用的记忆框架。在记忆构建阶段，该框架首先构建 semantic timeline (语义时间轴)，而非对话时间轴。随后，它将时间上连续且语义相关的信息整合为 durative memory (持续记忆)。在记忆利用阶段，该框架结合查询在 semantic timeline (语义时间轴) 上的时间意图，实现对时间上恰当的 durative memory (持续记忆) 的检索，并提供时间有效且持续时间一致的上下文以支持响应生成。在 LongMemEval 和 LoCoMo 数据集上的实验表明，TSM 始终优于现有方法，并实现了高达 12.2% 的准确率绝对提升，验证了所提方法的有效性。",
                    "inspiration_trace": "基于论文《Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程。\n\n---\n\n### 第一阶段：宏观观察与问题锚定\n**思考起点：个性化智能体的“记忆”困境**\n作者首先关注到LLM智能体在长期交互中的核心需求——**个性化**。现有的记忆机制（如RAG、向量数据库）虽然能存储历史对话，但在处理“时间”这一维度时存在根本性缺陷。\n*   **观察**：人类记忆是高度依赖时间线索的，我们不仅记得“发生了什么”，还记得“在什么时间背景下发生的”。然而，现有的LLM记忆系统大多将对话历史视为静态的文档集合，忽略了时间的动态性和语义性。\n\n### 第二阶段：深度诊断与核心痛点\n**思考深入：现有方法到底错在哪里？**\n作者进一步剖析了现有记忆系统在处理时间信息时的两个具体失效模式，从而确立了研究的突破口。\n\n1.  **时间错位**：\n    *   **现象**：用户在5月28日谈论5月29日的行程。现有系统通常以“对话时间”（5月28日）作为索引。\n    *   **问题**：这导致记忆被错误地锚定在聊天发生的时刻，而非事件发生的时刻。当用户查询“明天”或“那次旅行”时，系统无法准确对齐真实世界的时间线。\n    *   **结论**：必须区分“对话时间”与“语义时间”。\n\n2.  **时间碎片化**：\n    *   **现象**：一次为期一周的东京旅行被分散在数十个零散的对话轮次中。\n    *   **问题**：现有方法倾向于存储“点状记忆”，即孤立的事实片段。这种切分破坏了事件的连续性，导致智能体难以形成关于“持续状态”或“演变模式”的整体认知（例如：用户在旅行期间的整体心情或偏好变化）。\n    *   **结论**：需要一种机制将碎片化的信息整合为具有持续性的记忆。\n\n### 第三阶段：概念提出与假设构建\n**思考转折：如何模仿人类认知？**\n基于上述诊断，作者提出了两个核心概念作为解决问题的假设：\n\n1.  **语义时间线**：\n    *   **假设**：如果我们将记忆锚定在事件实际发生的时刻，而非对话记录的时刻，智能体就能像人类一样，在真实的时间轴上检索信息。\n    *   **构想**：构建一条独立于对话流的时间轴，所有记忆都挂载在这条轴上。\n\n2.  **持续性记忆**：\n    *   **假设**：如果将时间上连续且语义相关的片段聚合，形成高阶的摘要（如“主题”或“人设”），就能弥补点状记忆在长时上下文理解上的不足。\n    *   **构想**：记忆不应只是原子事实的堆砌，还应包含对一段时期内状态的总结。\n\n### 第四阶段：方法论设计与逻辑闭环\n**思考落地：如何实现上述概念？**\n作者将抽象概念转化为具体的工程架构，设计了TSM（Temporal Semantic Memory）框架，分为构建与利用两个阶段。\n\n1.  **构建阶段：从碎片到结构**\n    *   **解决“时间错位”**：引入**时序知识图谱**。从对话中提取实体和关系，并显式地标注其有效时间。这不仅是存储，更是建立了一个精确的时间索引。\n    *   **解决“时间碎片化”**：设计**分层聚合机制**。\n        *   *时间切片*：将图谱按时间间隔（如月）切分。\n        *   *语义聚类*：在同一时间片内，对实体进行聚类（GMM），将相关联的事件归为一组。\n        *   *生成摘要*：利用LLM对聚类结果进行总结，生成“主题”和“人设”。这标志着从“ episodic memory”（情景记忆）向“ durative memory”（持续性记忆）的升华。\n\n2.  **利用阶段：意图驱动的检索**\n    *   **逻辑**：用户的查询往往隐含时间意图（如“上周”）。\n    *   **机制**：\n        *   首先解析查询的**语义时间约束**。\n        *   在检索时，不仅计算语义相似度，更强制执行**时间过滤**。只有落在语义时间约束内的记忆（无论是TKG中的事实，还是Durative Memory中的摘要）才会被优先召回。\n        *   通过重排序，确保返回的上下文在时间上是逻辑自洽的。\n\n### 第五阶段：系统优化与工程考量\n**思考完善：如何保证效率与一致性？**\n作者意识到，频繁更新高阶摘要（Durative Memory）计算成本过高，而实时更新图谱（TKG）相对轻量。\n*   **分层更新策略**：\n    *   **在线轻量更新**：实时更新时序知识图谱，保证新事实的即时性。\n    *   **离线定期整合**：在“睡眠时间”定期重新计算和更新主题与人设摘要，平衡了系统的响应速度与长期一致性。\n\n---\n\n**总结：作者的思考路径**\n从**“现有记忆缺乏时间感知”**的宏观观察出发，通过诊断出**“对话时间与事件时间混淆”**和**“记忆碎片化”**两大微观病灶，提出了**“语义时间”**和**“持续性记忆”**的解决假设。最终，通过**时序知识图谱**进行底层时间锚定，结合**聚类摘要**实现高层语义聚合，并利用**时间约束检索**完成逻辑闭环，从而构建了一个能够像人类一样在真实时间线上思考的记忆系统。"
                },
                {
                    "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
                    "arxiv_id": "2601.07342",
                    "authors": "Nicolas Tacheny",
                    "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个LLM智能体框架，利用工具使用和结构化推理协议（调查协议）自主导航基础设施模型进行诊断。这符合单智能体研究范围中的“工具使用”和“规划”特征，且侧重于智能体架构而非纯领域应用。",
                    "summary2": "本文旨在解决传统 Root Cause Analysis (RCA) 耦合度高且难维护的问题。针对电信和数据中心基础设施，我们提出了一种基于 Model Context Protocol (MCP) 的 Agentic Diagnostic Framework，利用 LLM 通过 Investigation Protocol 和受限工具空间进行逐步推理。我们在合成图 Oracle Benchmark 上通过 Investigation Accuracy、RCA Accuracy 和 Impact Accuracy 验证了其有效性，Claude Haiku 3.5 达到了 100% 的准确率。",
                    "summary_translation": "大规模电信和数据中心基础设施依赖于多层服务和资源模型，在此架构下，故障会在物理和逻辑组件之间传播，进而影响多个客户。传统的根因分析（Root Cause Analysis, RCA）方法依赖于硬编码的图遍历算法或基于规则的关联引擎，这些方法不仅维护成本高昂，而且与基础设施模型紧密耦合。在这项工作中，我们提出了一种智能体诊断框架，该框架利用大语言模型（Large Language Model, LLM），通过模型上下文协议（Model Context Protocol, MCP）提供的受限工具空间执行分步调查。该智能体无需将因果逻辑或遍历算法嵌入应用程序，而是通过调用服务查询、依赖关系检索、结构化与非结构化数据分析、事件分析及影响发现等工具，自主在基础设施模型中进行导航。我们定义了一种调查协议，用于规范智能体的推理过程，并确保其具有事实依据、可复现性，并能安全处理缺失或模糊的信息。这项工作为自主事件解决和变更影响缓解奠定了基础。未来的系统不仅能够诊断并修复基础设施故障，还能预测计划变更对服务和客户的影响，从而帮助运维人员在执行维护操作前缓解风险。",
                    "inspiration_trace": "基于论文《Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 1. 宏观观察：传统方法的“刚性”与现实的“动态”矛盾\n**思考起点：** 作者首先审视了电信和数据中心运维的现状。\n*   **观察：** 现代基础设施是多层级的（服务、资源、客户），故障会在物理和逻辑组件间传播。\n*   **痛点：** 传统的根因分析（RCA）依赖于硬编码的图遍历算法或基于规则的关联引擎。\n*   **矛盾：** 基础设施是动态演进的（拓扑变更、命名变化），但传统的RCA逻辑是静态的。这导致了高昂的维护成本和系统与模型的紧耦合。此外，非结构化数据（如人工备注）难以被传统规则引擎处理。\n*   **初步结论：** 我们需要一种更具适应性、能够处理非结构化信息且不随拓扑变更而频繁修改代码的解决方案。\n\n### 2. 核心假设：从“编码逻辑”转向“编码协议”\n**思维转折：** 既然编写具体的因果逻辑（算法）太脆弱，能否让模型自己学会推理？\n*   **引入LLM：** 大语言模型（LLM）具备强大的推理和理解非结构化文本的能力，理论上可以替代硬编码规则。\n*   **风险识别：** 直接让LLM进行诊断存在“幻觉”风险，且无法保证操作的安全性（可能胡乱编造资源ID）。\n*   **关键假设：** 如果不把“因果逻辑”写死在代码里，而是定义一套严格的“调查协议”，并限制LLM只能通过特定工具获取数据，那么LLM就能像人类工程师一样进行“有据可依”的推理。\n*   **思路确立：** **去算法化**。不再试图用代码穷举故障传播路径，而是构建一个能够自主导航信息图的智能体。\n\n### 3. 抽象建模：构建标准化的数字孪生接口\n**落地思考：** 如何让智能体理解复杂的基础设施，同时又不依赖具体的数据库实现？\n*   **本体抽象：** 作者借鉴了TM Forum SID标准，将复杂的基础设施抽象为四个核心实体：**服务**、**资源**、**参与方**、**事件**。这为推理提供了一个通用的语义空间。\n*   **解耦设计：** 为了防止智能体与底层存储技术（如Neo4j或关系型数据库）绑定，作者引入了**模型上下文协议（MCP）**。\n*   **逻辑推演：** MCP充当了“安全边界”和“统一接口”。智能体不直接查询图数据库，而是调用MCP暴露的工具（如`get_implementation`, `get_impacted_services`）。这不仅解耦了系统，还天然防止了SQL注入或非授权访问，确保了每一次数据获取都是可审计的。\n\n### 4. 方法论构建：受控的智能体调查协议\n**核心创新：** 有了工具，如何确保智能体不乱跑、不胡说？\n*   **形式化流程：** 作者意识到，人类专家排查故障是有固定SOP（标准作业程序）的。因此，作者将这种经验形式化为一个**RCA调查协议**。\n*   **步骤设计：**\n    1.  **定位：** 从告警中提取服务名。\n    2.  **下钻：** 获取实现该服务的所有资源。\n    3.  **取证：** 检查每个资源的备注和事件（利用LLM理解非结构化文本）。\n    4.  **上溯：** 确定根因后，反向查找受影响的服务和客户。\n    5.  **发布：** 输出结构化报告。\n*   **约束机制：** 强制要求智能体必须基于工具返回的结果进行推理，如果数据缺失必须明确承认，严禁编造。这解决了LLM的“幻觉”问题，实现了**Grounding（接地气）**。\n\n### 5. 验证与洞察：去算法化的可行性\n**实证思考：** 这种“软逻辑”真的能取代“硬算法”吗？\n*   **实验设计：** 构建了一个合成图，预设了根因和影响路径，测试智能体能否在没有内置图算法的情况下找到答案。\n*   **结果分析：** 实验表明，只要协议设计得当，LLM（如Claude Haiku 3.5）能够达到100%的准确率。\n*   **关键洞察：** 事实证明，**硬编码的图遍历逻辑并非必须**。通过结构化的工具调用和逐步推理，因果逻辑是在推理过程中“涌现”出来的，而不是预先写好的。这意味着系统具有极强的通用性和适应性。\n\n### 6. 愿景延伸：从诊断到预测与自治\n**未来推演：** 既然能诊断“已发生”的故障，能否预测“未发生”的影响？\n*   **逻辑扩展：** 影响分析（IA）本质上是RCA的反向过程。如果系统能理解资源与服务的依赖关系，那么在执行变更（如维护）前，智能体完全可以模拟变更，预测其影响范围。\n*   **终极目标：** 这篇论文不仅是关于RCA，更是为**自主事故解决**和**变更影响缓解**奠定基础。未来的系统将从“被动响应”进化为“主动预防”。\n\n---\n\n**总结：**\n作者的思考路径是一个**“解构 -> 重构 -> 验证 -> 升华”**的过程：\n1.  **解构**了传统RCA系统的脆弱性（硬编码逻辑）；\n2.  **重构**了诊断流程，将其转化为基于MCP工具的智能体协议；\n3.  **验证**了LLM在严格协议下可以替代传统图算法；\n4.  最终**升华**出一种自适应、安全且可审计的基础设施运维新范式。"
                },
                {
                    "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging",
                    "arxiv_id": "2601.07309",
                    "authors": "Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",
                    "summary": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究LLM智能体，提出了一种通过模型合并技术将多个专家智能体整合为一个通用型智能体的方法，旨在解决智能体在不同交互环境中的泛化能力问题，属于LLM智能体的研究范畴。",
                    "summary2": "本文旨在将多个特定环境的LLM智能体专家合并为一个无需训练的通用模型。针对多轮交互场景，我们提出了一种名为ARM的基于激活引导的角色条件神经元移植方法。该方法通过动态主干选择和冲突感知的神经元移植来减少负迁移。在Qwen3-8B和Qwen2.5-7B专家池上，通过$\\tau$-bench、OfficeBench等多个基准验证了其有效性，显著提升了跨环境泛化能力和鲁棒性。",
                    "summary_translation": "交互式大语言模型智能体发展迅速，但大多数仍局限于单一环境，无法鲁棒地适应其他环境。Model merging (模型合并) 提供了一种 training-free (无需训练) 的替代方案，通过将多个 expert models (专家模型) 整合到单一模型中实现。在本文中，我们提出了 Agent-Role Merging (ARM) (智能体角色合并)，这是一种用于 LLM agents (大语言模型智能体) 模型合并的 activation-guided (激活引导)、role-conditioned (角色条件) neuron transplantation (神经元移植) 方法。ARM 将现有的合并方法从 static natural language tasks (静态自然语言任务) 拓展至 multi-turn agent scenarios (多轮智能体场景)，并提升了在各种交互环境中的 generalization ability (泛化能力)。这一目标通过一个精心设计的 3 步框架实现：1) 构建 merged backbones (合并主干网络)，2) 基于其 role-conditioned activation analysis (角色条件激活分析) 进行选择，3) 进行 neuron transplantation (神经元移植) 以实现 fine-grained refinements (细粒度细化)。在无需 gradient-based optimization (基于梯度的优化) 的情况下，ARM 提升了 cross-benchmark generalization (跨基准泛化) 能力，同时保持了高效率。在 diverse domains (多样化领域) 中，通过 ARM 合并得到的模型优于先前的 model merging methods (模型合并方法) 和 domain-specific expert models (特定领域专家模型)，同时展现了强大的 out-of-domain generalization (域外泛化) 能力。",
                    "inspiration_trace": "基于论文《ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 第一阶段：宏观问题与背景观察\n**1. 现实痛点：专才与通才的矛盾**\n*   **观察**：当前的LLM智能体在特定环境（如WebShop、OfficeBench）中表现优异，但往往是“专才”。一旦跨环境部署，由于工具接口、动作模式的差异，性能会急剧下降。\n*   **常规路径的局限**：传统的解决方案是训练一个通用的全能模型，但这面临巨大的工程挑战（多任务数据冲突、课程学习复杂）和高昂的训练成本。\n*   **切入点**：作者将目光投向了“模型合并”——一种无需额外训练即可整合多个专家模型权重的技术。这被视为一种低成本构建通才模型的潜在路径。\n\n### 第二阶段：冲突发现与问题聚焦\n**2. 现有方法的失效：从静态到动态的鸿沟**\n*   **假设**：现有的模型合并方法（如Task Arithmetic, TIES-Merging）在静态NLP任务上很成功，理应也能应用于智能体任务。\n*   **证伪**：实验发现，这些方法在交互式智能体场景下表现极不稳定（如图1所示，不同基准上表现方差巨大）。\n*   **核心洞察**：智能体任务与静态文本任务的本质区别在于**“多轮交互”**和**“级联效应”**。\n    *   在静态任务中，错误可能只是预测不准；\n    *   在智能体任务中，微小的格式错误（如JSON格式错误、工具调用参数偏差）会导致后续步骤全部崩溃。\n\n### 第三阶段：深入诊断与假设提出\n**3. 归因分析：两大核心挑战**\n作者将合并失败的原因归结为两个具体问题：\n*   **挑战一：主干的不稳定性**\n    *   不同的权重合并公式（平均、TIES等）在不同环境下的表现不可预测。没有一个通用的公式能保证在所有环境下都保留通用能力。\n*   **挑战二：能力冲突**\n    *   简单的权重平均会“模糊”掉特定技能。在智能体中，这表现为“角色关键行为”的丧失（例如，模型忘了如何正确调用API）。这种冲突比普通的知识遗忘更致命，因为它直接阻断了任务链条。\n\n### 第四阶段：方法论构建与逻辑演进\n**4. 策略一：如何选择稳定的主干？（从“盲选”到“内测”）**\n*   **思考**：既然无法预知哪个合并公式最好，能不能先构建一批候选模型，然后选一个最好的？\n*   **难点**：直接在测试集上评估成本太高。\n*   **创新思路**：利用模型内部的**激活信号**作为代理指标。\n    *   **逻辑**：如果一个合并后的模型，在处理特定任务（如“调用工具”）时，其神经元激活模式与原来的专家模型高度重合，说明它保留了该能力。\n    *   **产出**：提出了**激活重叠分数（AOS）**。通过分析“角色条件”下的激活（即只关注关键动作时刻的神经元），选出最能保留专家特征的合并主干。\n\n**5. 策略二：如何修复能力冲突？（从“全局融合”到“局部移植”）**\n*   **思考**：选出的主干可能在某些环境上依然较弱。直接全局微调会破坏已有能力，能否像器官移植一样，只把缺失的“能力模块”补进来？\n*   **细化思路**：\n    *   **定位**：利用激活分析，找出专家模型中负责特定“角色”（如JSON生成、工具调用）的关键神经元。\n    *   **移植**：将这些神经元直接“移植”到主干模型中。\n*   **关键约束：避免负迁移**\n    *   **思考**：如果移植的神经元恰好是另一个环境需要的，就会产生冲突。\n    *   **解决方案**：引入**冲突感知策略**。在移植前，先检查这些神经元是否被其他环境“占用”。如果是，则跳过，只移植那些“安全”的神经元。\n\n### 第五阶段：逻辑闭环与验证\n**6. 最终框架的形成：ARM**\n*   将上述思考串联，形成了三步走框架：\n    1.  **构建候选池**：用常规方法生成一堆合并模型。\n    2.  **基于激活选主干**：用AOS分数选出最稳健的那个。\n    3.  **神经元移植**：针对薄弱环节，像做手术一样精准移植专家的特定神经元，并严格保护其他能力不受干扰。\n\n**7. 预期与验证**\n*   **预期**：这种方法不仅能提升平均性能，更重要的是能解决“木桶效应”（最差环境的表现），因为它专门修复了导致级联失败的关键节点。\n*   **结论**：实验证明，ARM确实在保持通用性的同时，显著提升了跨环境的鲁棒性，验证了“基于角色条件的神经元移植”这一核心假设的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**“应用场景的迁移”**（从静态文本到智能体）出发，发现了**“级联失败”**这一特殊现象，进而通过**“机制可解释性”**（激活分析）手段，将模型合并问题从盲目的权重调整，转化为精准的**“电路诊断与修复”**过程。"
                },
                {
                    "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration",
                    "arxiv_id": "2601.07224",
                    "authors": "Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",
                    "summary": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确针对LLM智能体的训练方法，提出了基于梯度浓度的SFT与RL数据分配框架，并在WebShop和ALFWorld等智能体基准上进行了验证，属于智能体训练与自我演化范畴。",
                    "summary2": "本文旨在解决 LLM 智能体训练中 SFT 与 RL 数据分配低效及优化干扰问题。针对混合训练数据，我们提出了一种基于梯度空间几何结构（如 Gini 系数）诊断认知冲突的 PRISM 框架，实现数据在巩固与适应间的自适应路由，并在 WebShop 和 ALFWorld 基准上通过 Success Rate 和计算效率验证了其有效性。",
                    "summary_translation": "尽管混合监督微调（SFT）后接强化学习（RL）已成为训练 LLM 智能体的标准范式，但在这两个阶段之间进行数据分配的有效机制在很大程度上仍未被充分探索。当前的数据仲裁策略通常依赖于表层启发式方法，无法诊断模型的内在学习需求。鉴于 SFT 旨在通过模仿实现模式巩固，而 RL 则通过探索驱动结构适应，若数据与这些功能角色错位，将导致严重的优化干扰。我们提出了 PRISM，这是一个基于图式理论的动态感知框架，它根据数据与模型现有知识之间的认知冲突程度来仲裁数据。通过分析梯度的空间几何结构，PRISM 将引发高空间集中度的数据识别为高冲突信号，这类信号需要通过 RL 进行结构重构。相反，产生分散更新的数据则被分配给 SFT，以进行高效的巩固。在 WebShop 和 ALFWorld 上进行的广泛实验表明，PRISM 实现了帕累托改进，在将计算成本降低高达 3.22 倍的同时，性能优于最先进的混合方法。我们的研究结果表明，基于内部优化机制对数据进行解耦，对于实现可扩展且鲁棒的智能体对齐至关重要。",
                    "inspiration_trace": "基于论文《Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration》，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观问题的捕捉——现有范式的“粗粒度”困境\n**思考起点：** 作者首先审视了当前LLM智能体训练的标准范式（SFT后接RL）。虽然这一流程已被广泛接受，但作者敏锐地发现了一个被忽视的瓶颈：**数据分配机制是僵化的。**\n*   **观察：** 现有的数据分配策略主要分为三类：一是“单调排序”（SFT-then-RL的固定顺序），二是“通用探索”（对所有数据无差别使用RL），三是“结果导向过滤”（基于准确率等外部指标）。\n*   **痛点：** 这些方法都忽略了数据的**异质性**和模型的**内部状态**。它们将所有数据一视同仁，导致计算资源浪费，且容易引发优化干扰（如对简单样本进行不必要的RL探索，导致不稳定性）。\n*   **核心问题：** 如何根据数据的内在认知需求，智能地将其分配给SFT或RL，以实现效率与性能的帕累托最优？\n\n### 第二阶段：功能解构——SFT与RL的本质差异\n**思考深入：** 为了解决分配问题，作者首先对SFT和RL在认知层面的功能进行了重新定义。\n*   **SFT的功能：** 侧重于**模式巩固**。通过模仿，将行为规范和特定知识内化，适合处理模型已具备基础认知的领域。\n*   **RL的功能：** 侧重于**结构适应**。通过试错，重构内部逻辑以提升泛化能力，适合处理需要复杂推理和逻辑修正的领域。\n*   **推论：** 如果将需要“结构适应”的数据强行用于SFT，模型无法突破逻辑瓶颈；如果将仅需“模式巩固”的数据用于RL，则会引入探索噪声，破坏已有的知识。因此，**必须找到一种诊断机制，区分哪些数据需要巩固，哪些需要适应。**\n\n### 第三阶段：理论映射——引入认知心理学视角\n**思考转折：** 如何定义“需要适应”的数据？作者跳出纯工程视角，引入了皮亚杰的**图式理论**。\n*   **理论核心：** 学习效率取决于新信息与现有知识库之间的**冲突程度**。\n    *   **低冲突（兼容）：** 适合通过“同化”进行巩固。\n    *   **高冲突（矛盾）：** 必须通过“顺应”进行根本性的结构重组。\n*   **映射：** 作者将这一认知过程映射到神经网络优化中——**高认知冲突 = 需要结构适应（RL）；低认知冲突 = 需要模式巩固（SFT）。**\n\n### 第四阶段：数学代理——从“认知冲突”到“梯度几何”\n**思考落地：** 理论有了，但如何量化“认知冲突”？模型内部不会直接告诉我们要“冲突值”。作者将目光投向了优化的核心信号——**梯度**。\n*   **假设：** 梯度是模型对数据的数学反馈。如果数据与模型现有知识冲突剧烈，模型必须剧烈调整特定的参数（即“知识神经元”）来修正逻辑。\n*   **几何洞察：** 作者关注梯度的**空间几何结构**，而非单纯的数值大小。\n    *   **高浓度：** 如果梯度高度集中在少数参数组上，说明模型正在进行剧烈的局部逻辑修正（高冲突）。\n    *   **低浓度（扩散）：** 如果梯度均匀分布在整个网络，说明模型只是在微调全局参数以适应模式（低冲突）。\n*   **结论：** **梯度的空间浓度是认知冲突的最佳代理。**\n\n### 第五阶段：方法论构建——PRISM框架的诞生\n**思考成型：** 基于上述逻辑，作者构建了PRISM框架，将理论转化为可执行的三个步骤：\n1.  **无损探针：** 在不更新权重的情况下，计算模型对每个样本的梯度分布，捕捉内部反应。\n2.  **结构量化：** 引入统计学指标（如基尼系数、峰度、变异系数CV）来量化梯度的“浓度”，从而给每个样本打上“认知冲突分”。\n3.  **自适应路由：** 根据分数中位数进行切分。高分（高冲突）样本路由至RL进行结构重塑；低分（低冲突）样本路由至SFT进行行为巩固。\n\n### 总结：逻辑链条全景\n作者从**训练效率低下**的宏观现象出发，通过**功能解构**明确了SFT与RL的分工，借助**认知心理学理论**定义了“冲突”这一核心变量，最终利用**梯度的空间几何特征**将抽象的认知冲突转化为可计算的数学指标，从而实现了数据的精准路由。这一过程体现了从“经验主义训练”向“动力学感知训练”的思维跃迁。"
                },
                {
                    "title": "Active Context Compression: Autonomous Memory Management in LLM Agents",
                    "arxiv_id": "2601.07190",
                    "authors": "Nikhil Verma",
                    "summary": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了Focus Agent，专注于LLM智能体的自主记忆管理（上下文压缩），属于单智能体研究中的记忆与自我反思范畴，且涉及智能体通过反馈进行自我调节（自我演化），符合筛选标准。",
                    "summary2": "本文旨在解决 LLM agents 在长周期任务中面临的 Context Bloat 问题。针对 SWE-bench Lite 中的上下文密集型软件工程任务，我们提出了一种名为 Focus 的架构，通过自主的上下文压缩与记忆整合机制，将原始交互历史转化为持久的 Knowledge 块。在 N=5 个实例上使用 Claude Haiku 4.5 进行评估，结果显示 Focus 实现了 22.7% 的 token 减少，同时保持了与 Baseline 相同的任务准确率。",
                    "summary_translation": "大语言模型（LLM）智能体在处理长周期的软件工程任务时面临困难，主要归因于“Context Bloat”（上下文膨胀）。随着交互历史的增长，计算成本急剧上升，延迟增加，且由于受到无关过往错误的干扰，推理能力也会下降。现有的解决方案通常依赖于智能体无法控制的被动外部摘要机制。本文提出了 Focus，这是一种以智能体为中心的架构，其灵感来源于多头绒泡菌（Physarum polycephalum，粘液菌）的生物探索策略。Focus 智能体自主决定何时将关键学习成果整合到持久的“Knowledge”（知识）块中，并主动撤回（修剪）原始交互历史。我们使用符合行业最佳实践的优化脚手架（persistent bash + string-replacement editor，即持久化 bash + 字符串替换编辑器），在 SWE-bench Lite 的 N=5 个上下文密集型实例上，利用 Claude Haiku 4.5 对 Focus 进行了评估。通过采用鼓励频繁压缩的激进提示策略，Focus 实现了 22.7% 的 Token（词元）减少（从 14.9M 降至 11.5M tokens），同时保持了相同的准确率（两个智能体均为 3/5 = 60%）。Focus 平均每个任务执行 6.0 次自主压缩，在单个实例上的 Token 节省率高达 57%。我们证明了，当提供适当的工具和提示时，能力较强的模型能够自主调节其上下文，这为在不牺牲任务性能的前提下构建具有成本感知的智能体系统开辟了新途径。",
                    "inspiration_trace": "基于论文《Active Context Compression: Autonomous Memory Management in LLM Agents》，以下是对作者产出核心方法“Focus”的逻辑链推演与思想演进还原：\n\n### 第一阶段：宏观观察与问题界定\n**——从“能力幻觉”到“现实瓶颈”的思考**\n\n1.  **观察现象**：随着LLM上下文窗口的扩大（如200k+ tokens），理论上Agent可以处理极长任务。\n2.  **发现矛盾**：虽然“能装下”，但在实际长周期任务（如软件工程）中，简单的“全量保留”策略导致了三大恶果：\n    *   **成本爆炸**：推理成本随历史长度呈二次方增长。\n    *   **延迟增加**：交互响应变慢，体验下降。\n    *   **认知干扰**：长上下文中充斥着失败的尝试和冗余日志，导致模型注意力分散（“Lost in the Middle”现象），反而降低了推理质量。\n3.  **核心问题定义**：现有的Agent普遍采用“Append-Only”（只追加）模式，这是一种不可持续的线性积累。问题不在于窗口不够大，而在于**缺乏有效的遗忘与筛选机制**。\n\n### 第二阶段：对现有范式的批判\n**——从“外部辅助”到“自主控制”的反思**\n\n1.  **审视现有解法**：\n    *   *外部记忆（MemGPT等）*：像操作系统一样管理内存，但增加了系统复杂性。\n    *   *事后总结（Reflexion等）*：通常在任务结束后反思，而非在任务进行中实时清理。\n    *   *被动压缩（LLMLingua等）*：依赖外部模型进行压缩，Agent本身无法感知和控制压缩过程。\n2.  **提炼痛点**：现有方法大多是“被动”的，Agent无法决定“此时此刻什么该留，什么该丢”。作者意识到，**真正的智能体必须具备“元认知”能力，即自主管理自身思维过程（上下文）的能力。**\n\n### 第三阶段：跨学科灵感与假设提出\n**——从“黏菌”行为中提取“压缩”哲学**\n\n1.  **寻找生物学隐喻**：作者寻找自然界中高效探索环境的生物机制，锁定了*Physarum polycephalum*（多头绒泡菌/黏菌）。\n2.  **提取核心逻辑**：\n    *   黏菌在探索迷宫时，当发现某条路是死胡同，它会**物理回缩**（丢弃路径），只留下**化学标记**（保留知识）。\n    *   **类比映射**：Agent不需要保留“我尝试了50次ls命令”的原始日志（肌肉记忆），只需要保留“配置文件不在/src目录”的结论（认知地图）。\n3.  **形成核心假设**：如果Agent能像黏菌一样，在探索阶段结束后，主动“修剪”掉原始交互日志，仅保留提炼出的“知识块”，就能在降低成本的同时避免注意力分散。\n\n### 第四阶段：方法论构建\n**——从“线性增长”到“锯齿波动”的架构设计**\n\n1.  **设计机制**：提出“Focus”架构，引入两个原语：\n    *   `start_focus`：标记探索起点。\n    *   `complete_focus`：总结关键信息并**物理删除**中间的原始对话。\n2.  **确立模式**：将上下文从单调递增的曲线，转变为**“Sawtooth”（锯齿状）模式**——探索时增长，压缩时塌陷。\n3.  **关键特性**：**自主性**。不依赖外部计时器，而是由Agent根据任务进度自主决定何时压缩。\n\n### 第五阶段：实验反馈与策略修正\n**——从“理想模型”到“工程现实”的妥协**\n\n1.  **初步实验受挫**：作者最初假设模型会自然地学会高效压缩。但实验发现，如果仅提供工具而不加干预，模型压缩频率过低（平均2次），且因丢失关键细节导致准确率下降。\n2.  **逻辑修正**：作者意识到，当前的LLM（如Claude Haiku）**缺乏内在的“成本意识”**。它们不会为了省钱而主动压缩，只会为了“整理思路”而压缩。\n3.  **提出“激进提示”策略**：为了验证假设，作者必须强制模型的行为模式。\n    *   *显式规则*：强制要求每10-15次工具调用后必须压缩。\n    *   *系统干预*：注入系统提醒。\n4.  **验证结果**：在强制引导下，Agent平均每任务压缩6次，实现了22.7%的Token节省，且准确率未受损。这证明了**“频繁、小步快跑”的压缩优于“偶尔、大跨度”的压缩**。\n\n### 第六阶段：边界认知与结论升华\n**——从“通用解法”到“场景特化”的洞察**\n\n1.  **发现局限性**：并非所有任务都适合压缩。在需要反复迭代修改的任务（如pylint实例）中，压缩反而增加了开销，因为Agent需要重新加载被丢弃的上下文。\n2.  **最终结论提炼**：\n    *   Focus不是万能药，而是最适合**“探索-实现”分离**的任务（如先找Bug，再修Bug）。\n    *   **思想演进终点**：未来的Agent不应只是被动的执行者，而应是具备自我调节能力的“认知管理者”。通过适当的工程脚手架（Prompting），可以让模型在保持性能的同时，实现成本效益的最优化。\n\n---\n\n**总结：**\n作者的思考路径是从**“上下文太长太贵”**的痛点出发，通过**批判现有被动方法**，引入**生物学的“遗忘与标记”机制**，构建了**自主压缩的架构**，并在实验中通过**强化Prompt策略**克服了模型惰性，最终明确了该方法在**探索型任务中的核心价值**。"
                },
                {
                    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
                    "arxiv_id": "2601.07055",
                    "authors": "Zhenrui Yue, Kartikeya Upasani, Xianjun Yang, Suyu Ge, Shaoliang Nie, Yuning Mao, Zhe Liu, Dong Wang",
                    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题和摘要明确提到了“Search Agents”（搜索智能体）和“Self-Evolving”（自我演化）。研究内容涉及通过反馈循环让智能体在没有训练数据的情况下自主生成问题并提升推理和工具使用能力，符合“自我演化”和“单智能体（工具使用）”的研究范围，且不属于排除项。",
                    "summary2": "本文旨在解决无训练数据情况下搜索智能体的自我进化问题。针对开放域问答场景，我们提出了一种 Dr. Zero 框架，该框架利用 Proposer-Solver 共进化循环和跳跃分组相对策略优化（HRPO）来生成多样化且具挑战性的问题。在多个开放域问答基准（如 HotpotQA, NQ）上，通过精确匹配（EM）等指标验证了其有效性，结果显示其性能匹配甚至超越了全监督搜索智能体。",
                    "summary_translation": "随着高质量数据日益难以获取，无数据自我进化已成为一种极具前景的范式。该方法使大语言模型能够自主生成并解决复杂问题，进而提升其推理能力。然而，由于问题多样性有限，且多步推理和工具使用需要大量计算，多轮搜索智能体在无数据自我进化过程中面临挑战。在本研究中，我们提出了 Dr. Zero，这是一个使搜索智能体能够在没有任何训练数据的情况下实现有效自我进化的框架。具体而言，我们设计了一个自我进化反馈回路，其中提议者生成多样化的问题，用于训练一个由同一基础模型初始化的解题者。随着解题者的进化，它会促使提议者生成难度递增但仍可解的任务，从而建立一套自动化课程来优化这两个智能体。为提高训练效率，我们还引入了跳跃分组相对策略优化。该方法将结构相似的问题进行聚类以构建组级基线，有效最小化了在评估每个查询的个体难度和可解性时的采样开销。因此，HRPO 在不影响性能或稳定性的前提下，显著降低了解题者训练的计算需求。大量实验结果表明，无数据的 Dr. Zero 达到甚至超越了全监督搜索智能体的水平，证明了复杂的推理和搜索能力可以仅通过自我进化而涌现。",
                    "inspiration_trace": "基于对论文《Dr. Zero: Self-Evolving Search Agents without Training Data》的深入分析，以下是对作者核心方法论逻辑链的系统性推演。这一过程旨在还原作者从宏观问题观察到具体方法创新的思考路径。\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**——从“数据饥渴”到“无数据自进化”的范式转移**\n\n1.  **背景痛点**：\n    *   作者首先观察到当前大模型（LLM）发展的核心瓶颈：高质量训练数据的获取日益困难。\n    *   **思考**：如果无法依赖外部人工标注数据，模型能否像生物进化一样，通过“自举”的方式自我提升？\n2.  **现有局限的识别**：\n    *   作者审视了现有的“自进化”研究（如Self-Play、Self-Rewarding），发现它们大多集中在**封闭域**（如数学、代码）。\n    *   **关键发现**：在开放域的**搜索代理**任务中，现有的自进化方法失效了。原因在于：\n        *   **多样性缺失**：模型倾向于生成简单的、单跳的问题，缺乏挑战性。\n        *   **计算成本高昂**：搜索代理需要调用外部工具（如搜索引擎），推理链路长、延迟高。传统的强化学习算法（如需要多次采样的GRPO）在多轮工具交互场景下计算量呈指数级增长，难以落地。\n\n**核心问题确立**：如何在**零训练数据**的条件下，实现开放域搜索代理的高效自进化？\n\n---\n\n### 第二阶段：机制设计与假设提出\n**——构建“出题者”与“解题者”的共生博弈**\n\n1.  **引入对抗/共生框架**：\n    *   **思考**：要解决“题目太简单”的问题，不能只靠模型自己瞎想。自然界中，捕食者和猎物的共同进化促进了物种复杂度的提升。\n    *   **假设**：如果设计两个角色——**Proposer（出题者）**和**Solver（解题者）**，让它们相互博弈，是否能自动生成由易到难的课程？\n2.  **定义进化逻辑**：\n    *   **Proposer的任务**：利用搜索引擎生成复杂、多跳的问题。\n    *   **Solver的任务**：利用搜索引擎回答这些问题。\n    *   **反馈闭环**：Solver越强，Proposer必须生成更难的问题才能获得奖励；Proposer的问题越难，Solver被迫提升搜索推理能力。\n    *   **关键洞察**：这种动态博弈能自动形成**课程学习**，无需人工设计难度梯度。\n\n---\n\n### 第三阶段：攻克核心瓶颈\n**——解决“计算效率”与“题目质量”的双重挑战**\n\n1.  **解决计算效率问题（HRPO的诞生）**：\n    *   **困境**：传统的GRPO算法为了估计优势函数，需要对同一个Prompt生成多个回复。对于搜索代理来说，一次回复包含多次搜索调用，成本极高。如果Proposer训练需要“生成多个问题”且“每个问题跑多次Solver”，计算开销不可接受。\n    *   **创新思考**：能否减少采样次数？\n    *   **逻辑推演**：问题的结构特征（如Hop数/跳数）与其难度高度相关。与其对同一个问题采样多次，不如将**结构相似的问题**（例如都是2跳问题）归为一组。\n    *   **方法论产出**：提出**Hop-Grouped Relative Policy Optimization (HRPO)**。通过聚类结构相似的问题来构建组级基线，从而避免了昂贵的嵌套采样，将计算成本降低了一个数量级。\n\n2.  **解决题目质量问题（难度引导的奖励机制）**：\n    *   **困境**：如何让Proposer生成“既难又能做对”的题目？如果太难，Solver全错，学不到东西；如果太简单，Solver全对，没提升。\n    *   **逻辑推演**：理想的题目应该让Solver的正确率处于中间状态（例如只有部分尝试能解出）。\n    *   **方法论产出**：设计**难度引导的奖励函数**。\n        *   如果Solver全对 -> 奖励低（太简单）。\n        *   如果Solver全错 -> 奖励低（太难/无解）。\n        *   如果Solver部分正确 -> 奖励高（难度适中）。\n    *   **补充**：引入格式奖励，强制Proposer正确使用搜索工具，确保生成的题目是基于真实检索路径的，而非幻觉。\n\n---\n\n### 第四阶段：系统整合与验证\n**——Dr. Zero 框架的最终成型**\n\n1.  **系统架构整合**：\n    *   作者将上述思考整合为一个统一的框架：**Dr. Zero**。\n    *   **输入**：仅依赖基础LLM和外部搜索引擎，无任何人工标注数据。\n    *   **流程**：\n        1.  Proposer通过HRPO训练，利用搜索生成高质量、多跳的QA对。\n        2.  Solver通过GRPO训练，学习解决Proposer生成的难题。\n        3.  两者交替迭代，性能螺旋上升。\n\n2.  **实验验证与假设确认**：\n    *   **思考**：这套无数据方案真的能打过有监督的SOTA吗？\n    *   **结果**：实验表明，Dr. Zero在多个开放域QA基准上，不仅超越了基础模型，甚至**匹配或超越了**完全依赖人工数据的监督式搜索代理。\n    *   **结论**：证明了在搜索代理领域，**自进化可以替代人工监督**。\n\n---\n\n### 总结：作者的思维演进脉络\n\n1.  **观察**：数据稀缺，现有自进化方法在开放域搜索任务中因“题目简单”和“算力昂贵”而失效。\n2.  **假设**：通过Proposer-Solver的共生博弈可以自动生成进化的课程。\n3.  **挑战1（算力）**：传统RL采样太贵 -> **创新**：利用问题结构相似性，提出HRPO算法，大幅降低采样成本。\n4.  **挑战2（质量）**：如何控制题目难度 -> **创新**：基于Solver正确率的难度引导奖励，确保题目处于“最近发展区”。\n5.  **成果**：实现了无需任何训练数据的搜索代理自进化，性能媲美甚至超越有监督方法。"
                },
                {
                    "title": "CloneMem: Benchmarking Long-Term Memory for AI Clones",
                    "arxiv_id": "2601.07023",
                    "authors": "Sen Hu, Zhiyu Zhang, Yuxiang Wei, Xueran Han, Zhenheng Tang, Huacan Wang, Ronghao Chen",
                    "summary": "AI Clones aim to simulate an individual's thoughts and behaviors to enable long-term, personalized interaction, placing stringent demands on memory systems to model experiences, emotions, and opinions over time. Existing memory benchmarks primarily rely on user-agent conversational histories, which are temporally fragmented and insufficient for capturing continuous life trajectories. We introduce CloneMem, a benchmark for evaluating longterm memory in AI Clone scenarios grounded in non-conversational digital traces, including diaries, social media posts, and emails, spanning one to three years. CloneMem adopts a hierarchical data construction framework to ensure longitudinal coherence and defines tasks that assess an agent's ability to track evolving personal states. Experiments show that current memory mechanisms struggle in this setting, highlighting open challenges for life-grounded personalized AI. Code and dataset are available at https://github.com/AvatarMemory/CloneMemBench",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个针对AI智能体（AI Clones）长期记忆能力的基准，重点评估智能体在模拟个人行为时对经历、情绪和观点的记忆与追踪能力，属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在评估AI Clone基于非对话式数字痕迹的长期记忆能力。针对跨越1-3年的日记、社交媒体等非对话式数字痕迹，我们提出了一种名为CloneMem的benchmark，采用分层数据构建框架确保纵向一致性。我们在CloneMem数据集上通过Recall@K、Choice Accuracy和QA Consistency Score等指标验证了其有效性，揭示了现有记忆系统在追踪个人状态演变方面的局限性。",
                    "summary_translation": "AI Clones (AI克隆) 旨在模拟个体的思想和行为，以实现长期、个性化的交互，这对 memory systems (记忆系统) 随时间对经历、情感和观点进行建模提出了严苛的要求。现有的 memory benchmarks (记忆基准) 主要依赖于 user-agent conversational histories (用户-智能体对话历史)，这些历史在时间上是碎片化的，不足以捕捉连续的生活轨迹。我们介绍了 CloneMem，这是一个用于评估 AI Clone (AI克隆) 场景中 longterm memory (长期记忆) 的基准，它基于 non-conversational digital traces (非对话式数字痕迹)，包括日记、社交媒体帖子和电子邮件，时间跨度为一到三年。CloneMem 采用了一个 hierarchical data construction framework (分层数据构建框架) 来确保 longitudinal coherence (纵向一致性)，并定义了评估智能体追踪 evolving personal states (演变个人状态) 能力的任务。实验表明，当前的 memory mechanisms (记忆机制) 在这种设置下难以应对，凸显了 life-grounded personalized AI (基于生活的个性化AI) 面临的开放性挑战。Code (代码) 和 dataset (数据集) 可在 https://github.com/AvatarMemory/CloneMemBench 获取。",
                    "inspiration_trace": "基于论文《CloneMem: Benchmarking Long-Term Memory for AI Clones》，以下是对作者核心思想产出过程的系统性逻辑推演：\n\n### 第一阶段：宏观趋势与问题定义\n**（从“对话助手”到“数字克隆”）**\n\n1.  **观察现象**：随着LLM的发展，AI应用正从通用的“角色扮演”向更深度的“AI克隆”演进。用户不再满足于与一个预设角色的单次对话，而是希望建立一个能长期模拟特定个体思想、行为和情感的“数字分身”。\n2.  **提炼需求**：AI克隆的核心挑战不在于单次回复的准确性，而在于**长期记忆**。系统必须能够跨越数年时间，捕捉个体的经历、情感波动以及观点的演变，而不仅仅是维持对话的上下文。\n\n### 第二阶段：痛点识别与假设提出\n**（从“对话历史”到“生活轨迹”）**\n\n1.  **批判现状**：作者审视现有的记忆基准（如LoCoMo, LongMemEval），发现它们几乎全部依赖于**用户-智能体的对话历史**。\n2.  **指出缺陷**：\n    *   **碎片化**：对话是离散的、断续的，只能捕捉生活的快照，无法记录连续的生活流。\n    *   **被动性**：现实中，用户不可能通过不断的对话来“喂养”AI克隆，这成本太高。\n3.  **提出假设**：真实的记忆应当基于**非对话式的数字痕迹**（如日记、社交媒体、邮件）。这些数据是自然发生的、纵向连续的，能反映个体在非交互状态下的真实状态。因此，需要一个新的基准来评估AI克隆处理这种“生活轨迹”的能力。\n\n### 第三阶段：数据构建的方法论突破\n**（从“随机生成”到“分层连贯性”）**\n\n1.  **面临挑战**：如何构建一个跨越1-3年、逻辑自洽且包含情感和观点演变的合成数据集？简单的随机生成会导致时间线上的逻辑崩塌。\n2.  **核心思想**：人类的生活是有结构的，不是杂乱无章的。必须采用**自上而下的分层生成框架**来确保纵向连贯性。\n3.  **逻辑推演**：\n    *   **宏观层**：先定义“人格特质”和“生活弧线”，确定长期的人生轨迹（如职业变动、情感走向）。\n    *   **中观层**：将大事件拆解为“阶段”，并引入“内部状态快照”机制。确保上一阶段的情感积累会影响下一阶段，从而实现心理状态的连续性。\n    *   **微观层**：基于具体事件生成日记、帖子等数字痕迹，并显式生成对应的“证据”，确保痕迹与底层逻辑的一致性。\n\n### 第四阶段：评估维度的重新定义\n**（从“事实检索”到“轨迹追踪”）**\n\n1.  **转变视角**：传统的记忆测试多关注“某时某刻发生了什么”（静态事实）。但对于AI克隆，关键在于“为什么会变成现在这样”。\n2.  **设计任务**：评估重点必须转向**动态推理**。作者设计了涵盖经历、情感、观点三个维度的任务，不仅测试事实回忆，更测试比较、因果分析、反事实推理以及对“未确定状态”的识别（即区分“正在探索”与“已做决定”）。\n\n### 第五阶段：实验发现与理论升华\n**（从“追求抽象”到“保真度优先”）**\n\n1.  **预期与反差**：作者原本预期先进的、具有抽象和整合能力的记忆系统（如A-Mem, Mem0）会表现更好，因为它们能“总结”知识。\n2.  **实验发现**：结果令人惊讶，最简单的**扁平检索器**往往表现最好。复杂的记忆系统因为进行了“有损压缩”（总结和抽象），丢失了回答轨迹问题所需的细粒度细节（如时间戳、具体措辞）。\n3.  **洞察提炼**：\n    *   **有效性 vs. 保真度**：现有的记忆系统优化的是“有效性”（能否找到相关话题），但AI克隆更需要的是“保真度”（能否还原具体细节）。\n    *   **叙事陷阱**：模型倾向于用通用的叙事模板（如“孩子的一句话让父亲顿悟”）来填补记忆空白，导致因果逻辑错误但听起来很合理。\n4.  **最终结论**：AI克隆的记忆系统不应仅仅是一个压缩的知识库，而应是一个**证据保存基质**。它必须保留原始痕迹的保真度，显式建模内部状态的转变，并能在证据不足时保持“未知”的克制。\n\n---\n\n**总结**：作者的思考路径是从**应用场景的升级**（克隆vs对话）出发，发现了**数据源的本质缺陷**（对话vs轨迹），通过**分层生成**解决了数据连贯性难题，并在实验中意外揭示了**当前记忆架构的“有损压缩”悖论**，最终确立了AI克隆记忆设计应遵循“保真度优先”的新原则。"
                },
                {
                    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
                    "arxiv_id": "2601.06860",
                    "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了ET-Agent框架，专注于LLM智能体的工具使用行为校准，属于单智能体研究范畴；同时引入了自我演化数据飞轮机制，符合自我演化的研究范围。不属于排除的纯应用、纯推理或基础设施优化等类别。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR) 任务中agent行为模式无效的问题。针对TIR场景，我们提出了一种名为ET-Agent的训练框架，通过Self-evolving Data Flywheel生成增强数据，并利用两阶段Behavior Calibration Training逐步校准错误行为。我们在数学推理和知识密集型任务上通过正确性、效率等指标验证了其有效性，实验表明ET-Agent显著提升了推理效率和准确性。",
                    "summary_translation": "大语言模型可以通过采用工具集成推理范式，扩展其参数知识的边界。然而，现有的基于大语言模型的智能体训练框架往往侧重于答案的准确性，而忽视了对行为模式的特定对齐。因此，智能体在执行工具集成推理任务时，常表现出无效的动作，例如冗余或不足的工具调用。如何在执行工具集成推理任务时校准错误的行为模式，进而探索有效的轨迹，仍是一个亟待解决的开放性问题。在本文中，我们提出了ET-Agent，这是一个通过两个协同视角来校准智能体工具使用行为的训练框架：自进化数据飞轮和行为校准训练。具体而言，我们引入了一个自进化数据飞轮来生成增强数据，利用这些数据对大语言模型进行微调，以提升其探索能力。在此基础上，我们构建了一个两阶段的行为校准训练框架。该框架旨在逐步将错误的行为模式校准为最优行为。进一步的深入实验证实了ET-Agent在多个维度上的优越性，包括正确性、效率、推理简洁性以及工具执行准确性。我们的ET-Agent框架为工具集成推理领域的研究提供了有价值的实践启示。代码链接：https://github.com/asilverlight/ET-Agent",
                    "inspiration_trace": "基于对论文《ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration》的深入分析，以下是对作者产出该核心方法的逻辑链推演。这一过程展现了从宏观现象观察到微观机制设计的完整思考路径。\n\n---\n\n### 1. 宏观观察与问题定义：从“结果正确”到“行为有效”\n\n**逻辑起点：**\n作者首先关注到大语言模型（LLM）通过工具集成推理（TIR）范式突破了参数知识的限制。然而，学术界和工业界存在一个普遍的**评价偏差**：绝大多数研究仅关注**最终答案的准确性**，而忽视了达成答案过程中的**行为模式**。\n\n**现象发现：**\n在实际应用中，作者观察到即使模型答对了问题，其过程往往充满“低效”或“怪异”的行为。例如，为了查一个简单事实反复调用搜索工具，或者在需要深入推理时过早停止。这引发了一个核心思考：\n> **核心问题：** 如何在保证答案正确的前提下，校准Agent的工具使用行为，使其既不冗余也不匮乏，从而实现高效推理？\n\n### 2. 深度诊断与归因：行为错误的分类与空间复杂性\n\n为了解决上述问题，作者没有急于提出模型，而是先对“错误行为”进行了系统性的**病理分析**。\n\n**错误分类：**\n通过初步实验，作者将错误的TIR行为模式归纳为两类：\n1.  **不当工具使用：** 包括“冗余调用”（浪费资源）和“中止执行”（代码或查询格式错误导致失败）。\n2.  **缺陷推理逻辑：** 包括“调用不足”（过早停止，没拿到关键信息）和“错误推理过程”（逻辑跳跃或无关步骤）。\n\n**关键洞察：**\n作者进一步分析了正确答案的轨迹分布，发现了一个重要现象：**对于同一个问题，存在大量不同的正确路径，且工具调用的次数差异巨大。**\n这意味着TIR任务的**动作空间极其广阔**。\n\n**对现有方法的批判：**\n基于此洞察，作者指出了现有方法的局限性：\n*   **模仿学习（SFT）：** 只能复现训练数据中的路径，无法探索数据之外的高效行为，导致探索能力受限。\n*   **传统RL（如DPO）：** 往往基于二元对比（好vs坏），容易导致模型坍缩到极窄的动作空间，无法适应TIR广阔的解空间。\n\n**结论：** 现有的“只看结果”或“简单对比”无法解决TIR中的行为校准问题。我们需要一种能**充分探索广阔动作空间**，并从中**筛选出最优行为**的新范式。\n\n### 3. 核心假设提出：先探索，后校准\n\n基于上述诊断，作者提出了一个分阶段的解决思路：\n> **核心假设：** 要校准行为，首先必须让模型“见识”到足够多的可能性（探索），然后再通过奖励机制引导其收敛到最优路径（校准）。\n\n这直接导向了ET-Agent框架的两大支柱设计：\n1.  **数据层面：** 需要一个能自我进化、不断扩充轨迹多样性的机制。\n2.  **算法层面：** 需要一个先鼓励发散探索，再逐步收敛至高效行为的训练流程。\n\n### 4. 方法论构建：从数据飞轮到行为校准\n\n#### 4.1 数据层面的突破：自进化数据飞wheel\n**思考：** 既然现有数据覆盖面不够，如何低成本地获得高质量、多样化的轨迹？\n**设计：**\n作者设计了一个闭环系统，利用模型自身来生成和优化数据：\n*   **对正确轨迹：** 进行“去冗余”和“全局精炼”，教模型如何做得更简洁。\n*   **对错误轨迹：** 进行“自我修正”和“提示注入”，强制模型继续思考或修正错误，从而生成原本不存在的正确路径。\n**逻辑目的：** 这个过程不仅仅是增加数据量，而是为了**覆盖更广阔的动作空间**，为后续的训练提供丰富的“原材料”。\n\n#### 4.2 算法层面的演进：两阶段行为校准\n**思考：** 有了丰富的数据，如何训练模型？直接用RL可能会因为奖励稀疏或梯度消失而失败。\n**设计：** 作者将训练分为两个紧密衔接的阶段。\n\n*   **阶段一：动作空间探索微调**\n    *   **逻辑：** 利用飞wheel生成的多样化数据进行监督微调（SFT）。\n    *   **目的：** 此时暂不追求极致效率，而是让模型**学会各种可能的解题路径**，打破初始模型的思维定势，实现“广度优先”。\n\n*   **阶段二：迭代行为校准强化学习**\n    *   **逻辑：** 在模型具备探索能力后，引入RL进行优化。\n    *   **难点解决：** 传统的Group-wise RL容易因为轨迹同质化导致梯度消失。\n    *   **创新设计：**\n        *   **分组帕累托采样：** 在采样时，不仅看正确率，还看行为差异度。优先保留那些“既正确又与众不同”的轨迹，确保训练信号始终存在。\n        *   **课程式奖励机制：** 设计了包含“效率惩罚”（工具调用次数、推理长度）的奖励函数。并采用课程学习策略，逐步收紧对效率的要求（从宽松到严格），防止模型为了追求效率而牺牲正确性。\n\n### 5. 逻辑闭环与验证\n\n最终，ET-Agent的形成逻辑链条如下：\n1.  **痛点：** TIR Agent行为低效（冗余或不足），且动作空间巨大。\n2.  **诊断：** 现有方法缺乏对广阔动作空间的有效探索和精准校准。\n3.  **策略：** 先通过数据增强实现“广度探索”，再通过RL实现“精度校准”。\n4.  **实现：**\n    *   **数据侧：** 自进化飞wheel -> 生成多样化、高质量的轨迹。\n    *   **训练侧：** RFT（学广） -> Pareto RL + 课程奖励（学精）。\n5.  **结果：** 模型在保持高准确率的同时，显著提升了工具使用的效率和推理的简洁性。\n\n这一思考过程体现了作者从**现象观察**到**本质归因**，再到**范式假设**，最后落实到**具体机制设计**的严谨学术逻辑。"
                },
                {
                    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
                    "arxiv_id": "2601.06794",
                    "authors": "Zhicong Li, Lingjie Jiang, Yulan Hu, Xingchen Zeng, Yixia Li, Xiangwen Zhang, Guanhua Chen, Zheng Pan, Xin Li, Yong Liu",
                    "summary": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了ECHO框架，专注于训练LLM智能体，通过策略与批评模型的协同演化来优化智能体在开放世界环境中的表现。这符合研究范围中的“自我演化（通过反馈自我完善）”及“单智能体”方向，不属于排除的纯应用、纯推理或基础设施优化。",
                    "summary2": "本文旨在解决critique-guided RL中静态critic因策略演化导致反馈陈旧的问题。针对Open-World Agent Learning场景，我们提出了一种ECHO框架，通过cascaded rollout mechanism和saturation-aware gain shaping实现策略与critic的同步协同演化。并在WebShop、ALFWorld、SciWorld及DeepSearch四个基准上通过任务成功率验证了其有效性。",
                    "summary_translation": "批判引导的强化学习（RL）已成为一种训练 LLM 智能体的强大范式，它通过自然语言反馈来增强稀疏的结果奖励。然而，现有方法通常依赖于静态或离线的评论家模型，这些模型无法随着策略的演变而适应。在在线策略 RL 中，智能体的错误模式会随时间发生偏移，导致固定的评论家变得过时，从而提供效用递减的反馈。为了解决这一问题，我们提出了 ECHO（Evolving Critic for Hindsight-Guided Optimization，用于后见之明引导优化的演进评论家），该框架通过同步的协同进化循环来联合优化策略和评论家。ECHO 采用了一种级联展开机制，评论家首先针对初始轨迹生成多个诊断，随后进行策略细化，从而实现分组结构的优势估计。我们通过一种饱和感知的增益塑形目标来解决学习平台期的挑战，该目标对评论家在高性能轨迹中引发增量改进的行为给予奖励。通过采用双轨 GRPO 更新，ECHO 确保评论家的反馈与不断演进的策略保持同步。实验结果表明，在开放世界环境中，ECHO 能够实现更稳定的训练，并在长视界任务中取得更高的成功率。",
                    "inspiration_trace": "基于论文《No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning》，以下是对作者产出核心方法（ECHO）的逻辑链推演与思考过程还原：\n\n### 第一阶段：观察现状与识别瓶颈\n**思考起点：** 在开放世界的LLM智能体训练中，传统的强化学习（RL）仅依赖稀疏的最终结果奖励，这导致数据效率极低，因为智能体不知道“哪里错了”。\n**现有尝试：** 引入“评论家”模型提供自然语言的诊断反馈。\n**发现矛盾：** 现有的评论家大多是**静态**的（基于模板或离线训练后冻结）。作者观察到，在On-policy RL（在线策略强化学习）中，智能体的策略是不断演进的。\n**逻辑推演：**\n*   早期阶段：智能体犯的是粗粒度错误（如走错房间），需要高层提示。\n*   后期阶段：智能体已掌握基本技能，犯的是细粒度错误（如参数微调），需要精准诊断。\n*   **结论：** 一个固定的、不随策略变化的评论家，其反馈会逐渐变得“陈旧”，甚至产生误导。这就是“Critic Staleness”问题。\n\n### 第二阶段：提出核心假设\n**思维转折：** 既然智能体的错误模式是漂移的，那么最优的评论策略也应当是非静止的。\n**核心假设：** 评论家不应是一个外部的、高高在上的“监督者”，而应是一个与策略共同进化的“伙伴”。\n**评价标准重构：** 评价一个评论家好坏的标准，不应是“它说得是否好听”，而应是“它是否真的诱导了策略的改进”。\n\n### 第三阶段：构建协同进化机制\n**设计挑战：** 如何让两个模型（策略 $P$ 和 评论家 $C$）在同一个训练循环中互相促进，而不是互相干扰？\n**解决方案构思：**\n1.  **闭环构建：** 设计一个“诊断-修正”的级联流程。策略生成轨迹 -> 评论家诊断 -> 策略基于诊断修正。\n2.  **双重优化：** 利用修正后的结果来反向更新两个模型。\n    *   策略更新：学习如何更好地采纳建议。\n    *   评论家更新：学习如何给出能带来更高奖励的建议。\n**逻辑支点：** 通过这种“双轨”同步更新，确保评论家的诊断粒度始终对齐策略当前的短板。\n\n### 第四阶段：解决“最后一公里”的优化难题\n**深入思考：** 在训练后期，策略表现已经很好（例如得分从0.9提升到0.95），这比从0.1提升到0.15要难得多。\n**现有缺陷：** 如果使用线性的奖励差值（$\\Delta s = 0.05$），模型会认为这种高难度的提升价值很低，导致优化停滞。\n**创新思路：** 引入“饱和感知”的奖励设计。\n**逻辑推演：**\n*   假设奖励空间是非线性的，越接近满分，改进的难度和熵减的价值越高。\n*   设计一个增益函数，放大高分区间的微小改进信号。\n*   **目的：** 激励评论家去挖掘那些“看似完美但仍有瑕疵”的轨迹中的关键缺陷。\n\n### 第五阶段：方法论综合与验证\n**最终框架（ECHO）：** 将上述思考整合为一个统一的框架。\n1.  **级联演化：** 通过多视角诊断和条件修正，生成结构化的轨迹组。\n2.  **饱和感知奖励：** 解决高难度阶段的优化动力问题。\n3.  **同步双轨GRPO：** 利用群组相对优势估计，稳定地同时更新策略和评论家。\n\n**总结：** 作者的思考路径从**发现静态反馈与动态策略之间的错配**出发，通过**引入协同进化的视角**重新定义了评论家的角色，并利用**非线性奖励塑形**解决了长尾优化难题，最终实现了ECHO这一能够持续自我提升的智能体训练范式。"
                },
                {
                    "title": "From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design",
                    "arxiv_id": "2601.06776",
                    "authors": "Xufei Tian, Wenli Du, Shaoyi Yang, Han Hu, Hui Xin, Shifeng Qu, Ke Ye",
                    "summary": "Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一种多智能体LLM工作流，包含四个专门的智能体（任务理解、拓扑生成、参数配置、评估分析）进行协作，并涉及与仿真软件的交互（工具使用）。这符合“多智能体：协作”和“工具使用”的研究范围。尽管应用于化工领域，但其核心贡献在于智能体架构和工作流设计，而非单纯的领域应用。",
                    "summary2": "本文旨在实现从文本描述到可执行化工过程模拟的端到端自动化设计。针对自然语言输入的化工过程设计场景，我们提出了一种结合增强蒙特卡洛树搜索（E-MCTS）的多智能体LLM工作流，通过四个专门智能体协同工作。在Simona数据集上，通过模拟收敛率（SCR）和设计时间验证了其有效性，相比最先进基线收敛率提升31.1%，设计时间减少89.0%。",
                    "summary_translation": "Process simulation (过程模拟) 是 Chemical engineering design (化工设计) 的关键基石。当前的 Automated chemical design methodologies (自动化化工设计方法) 主要集中在 Process flow diagrams (工艺流程图) 的各种表示形式上。然而，将这些图表转化为 Executable simulation flowsheets (可执行模拟流程) 仍然是一项耗时且费力的工作，需要在 Simulation software (模拟软件) 中进行大量的 Manual parameter configuration (手动参数配置)。在这项工作中，我们提出了一种新颖的 Multi-agent workflow (多智能体工作流)，该工作流利用 Large language models (LLMs, 大语言模型) 的 Semantic understanding (语义理解) 能力，并实现与 Chemical process simulation software (化工过程模拟软件) 的 Iterative interactions (迭代交互)，从而实现了从 Textual process specifications (文本过程规范) 到用于设计增强的 Computationally validated software configurations (计算验证的软件配置) 的 End-to-end automated simulation (端到端自动模拟)。我们的方法集成了四个分别负责 Task understanding (任务理解)、Topology generation (拓扑生成)、Parameter configuration (参数配置) 和 Evaluation analysis (评估分析) 的 Specialized agents (专门智能体)，并结合 Enhanced Monte Carlo Tree Search (增强蒙特卡洛树搜索) 来准确解释语义并稳健地生成配置。在大规模 Process description dataset (过程描述数据集) Simona 上进行评估，我们的方法与 State-of-the-art baselines (最先进基线) 相比，Simulation convergence rate (模拟收敛率) 提高了 31.1%，与专家 Manual design (手动设计) 相比，Design time (设计时间) 减少了 89.0%。这项工作展示了 AI-assisted chemical process design (AI辅助化工过程设计) 的潜力，弥合了 Conceptual design (概念设计) 与 Practical implementation (实际实施) 之间的差距。我们的 Workflow (工作流) 适用于包括 Pharmaceuticals (制药)、Petrochemicals (石化)、Food processing (食品加工) 和 Manufacturing (制造业) 在内的多种 Process-oriented industries (流程导向型行业)，为 Automated process design (自动化过程设计) 提供了一种 Generalizable solution (可推广解决方案)。",
                    "inspiration_trace": "基于论文《From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论产出的思考过程：\n\n### 第一阶段：宏观问题定位——“最后一公里”的瓶颈\n**观察：** 化工过程设计是工业的核心，但目前的流程极其低效。工程师需要花费数周时间，将高层的概念设计（如“设计一个乙烯裂解过程”）转化为可在模拟软件（如Aspen Plus等）中运行的详细配置。\n**痛点识别：** 现有的自动化方法大多集中在“画图”阶段（生成流程图PFD或超图结构），但这只是设计的中间态。真正的瓶颈在于从“结构图”到“可执行仿真配置”的转化——这需要人工设定成百上千个相互依赖的热力学和操作参数。\n**核心矛盾：** 概念设计与工程落地之间存在巨大的鸿沟。AI能生成漂亮的图纸，但图纸无法直接运行，无法验证可行性。\n\n### 第二阶段：现有方案的局限性分析\n**反思：** 为什么现有的AI方法（如CNN、GNN或早期的LLM应用）解决不了这个问题？\n**结论：**\n1.  **停留在表征层：** 现有方法将设计视为静态的图像或图结构生成任务，忽略了化工过程本质上是基于物理化学方程的动态计算过程。\n2.  **缺乏闭环验证：** 生成的结构如果没有经过模拟软件的严格计算，往往是不收敛或不可行的。现有方法缺乏与专业仿真软件的交互能力。\n3.  **语义与参数的割裂：** LLM擅长理解自然语言（语义），但很难直接生成符合复杂物理约束的精确参数（数值）。\n\n### 第三阶段：核心假设提出——“人机协作”的代理化\n**假设：** 如果能构建一个系统，模仿人类专家的思维方式——即“理解意图 -> 搭建结构 -> 设定参数 -> 软件试算 -> 根据报错调整”，并利用LLM处理语义，利用仿真软件处理物理计算，就能打通从文本到仿真的全链路。\n**关键转变：** 从“一次性生成”转变为“迭代式交互”。不再追求LLM直接写出完美的代码，而是允许它通过工具与仿真软件进行多轮对话，直到收敛。\n\n### 第四阶段：方法论构建——多智能体分工\n**思考：** 化工设计任务过于复杂，单个LLM无法同时兼顾语义理解、拓扑规划、参数计算和结果评估。必须进行“分而治之”。\n**逻辑推演：**\n1.  **任务理解：** 首先需要将模糊的自然语言转化为结构化的工程需求（如明确组分、约束条件）。\n2.  **拓扑生成：** 专注于“骨架”搭建，确定单元操作（反应器、精馏塔）及其连接关系，暂时不纠结细节。\n3.  **参数配置：** 专注于“血肉”填充，利用LLM的推理能力结合领域知识，为拓扑赋予初始参数。\n4.  **评估分析：** 充当“质检员”，接收仿真软件的反馈（是否收敛、经济性如何），并决定是输出结果还是反馈修改。\n\n### 第五阶段：搜索策略优化——如何处理“失败”\n**深层挑战：** 化工设计空间巨大，且充满了“陷阱”。很多设计在仿真中会失败（不收敛）。传统的搜索算法（如标准MCTS）通常会直接丢弃失败的分支。\n**创新洞察：** 在化工设计中，一个“失败”的仿真往往包含有价值的信息（例如拓扑结构是对的，只是某个温度参数设错了）。如果直接丢弃，就浪费了探索成本。\n**策略演进：** 提出**增强型蒙特卡洛树搜索（E-MCTS）**。\n1.  **双重价值评估：** 区分“当前价值”（仿真是否成功）和“潜在价值”（结构是否合理）。即使仿真失败，如果结构合理，仍保留其探索潜力。\n2.  **动态重访机制：** 当搜索陷入停滞时，主动回到那些曾经失败但潜力巨大的节点进行微调，从而跳出局部最优，找到真正可执行的解。\n\n### 总结：逻辑链的全景图\n作者从**“设计效率低”**的宏观问题出发，识别出**“结构到可执行配置的断层”**这一核心痛点。通过分析现有AI**“重表征、轻验证”**的缺陷，提出了**“LLM语义理解 + 仿真软件物理验证”**的闭环假设。为了实现这一假设，作者采用了**多智能体协作**来解耦复杂任务，并创新性地设计了**E-MCTS算法**来从失败中学习，最终实现了从自然语言文本到工业级仿真配置的端到端自动化。"
                },
                {
                    "title": "Agentic AI Empowered Intent-Based Networking for 6G",
                    "arxiv_id": "2601.06640",
                    "authors": "Genze Jiang, Kezhi Wang, Xiaomin Chen, Yizhou Huang",
                    "summary": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个分层多智能体框架，包含编排器智能体和领域专家智能体（RAN和核心网络），它们通过ReAct循环进行协作和通信以解决网络配置问题。这完全符合“多智能体：协作、通信”的研究范围，且核心贡献在于智能体架构而非单纯的基础设施优化。",
                    "summary2": "本文旨在解决6G网络中将高层自然语言意图转化为可执行网络配置的自主编排问题。针对自然语言操作意图，我们提出了一种基于LLM的分层多智能体框架，通过Orchestrator协调RAN和Core专家代理进行ReAct推理，并在包含12个场景的6G基准测试中，通过Semantic Accuracy和Engineering Utility验证了其有效性。",
                    "summary_translation": "向第六代（6G）无线网络的演进迫切需要一种自主编排机制，该机制能够将高层运维意图转化为可执行的网络配置。现有的基于意图的网络（Intent-Based Networking, IBN）方法要么依赖于难以应对语言差异的基于规则的系统，要么依赖于缺乏可解释性且无法强制执行运维约束的端到端神经模型。本文提出了一种分层多智能体框架，其中基于大语言模型（Large Language Model, LLM）的智能体能够自主分解自然语言意图，咨询领域特定专家，并通过迭代推理-行动（Reasoning-action, ReAct）循环综合生成技术上可行的网络切片配置。该架构采用一个编排器智能体，通过基于结构化网络状态表示的ReAct风格推理，协调两个专家智能体，即无线接入网（Radio Access Network, RAN）智能体和核心网智能体。在多种基准场景下的实验评估表明，该系统优于基于规则的系统和直接LLM提示方法，且其架构原则适用于开放无线接入网（Open RAN, O-RAN）部署。结果还表明，尽管当代大语言模型（LLM）具备通用的电信知识，但网络自动化仍需通过精细的提示工程来编码上下文相关的决策阈值，从而推进下一代无线系统的自主编排能力。",
                    "inspiration_trace": "基于论文《Agentic AI Empowered Intent-Based Networking for 6G》，以下是对作者核心方法论产出过程的逻辑推演与思想还原：\n\n### 1. 宏观背景与问题锚定：6G时代的“语义鸿沟”\n**思考起点：** 6G网络愿景的核心是“零接触”自动化与无处不在的智能。未来的网络管理不应依赖工程师手动敲击命令行，而应允许运营商通过自然语言描述高层业务目标（即“意图”），由系统自动转化为底层的网络配置。\n\n**核心矛盾：** 现有的意图网络（IBN）方案存在两极分化的缺陷：\n*   **基于规则的系统：** 虽然严谨，但极其死板。一旦自然语言表述稍有变化（如将“低延迟”改为“即时响应”），系统便无法识别，缺乏泛化能力。\n*   **端到端的神经网络/传统ML：** 虽然能处理数据，但属于“黑盒”，缺乏可解释性，且难以强制执行严格的操作约束（如“必须小于10ms”），无法满足电信级的安全要求。\n\n**结论：** 我们需要一种既能理解自然语言的灵活性，又能像专家系统一样执行严格逻辑推理的新范式。\n\n### 2. 技术选型与范式转移：从“聊天机器人”到“智能体”\n**观察：** 大语言模型（LLM）展现了惊人的语义理解能力，似乎是填补“语义鸿沟”的完美工具。然而，直接向LLM提问（单次Prompt）存在致命弱点——LLM本质上是一个文本生成器，而非决策引擎。它无法自主验证配置的可行性，无法感知当前网络状态，且容易产生“幻觉”。\n\n**假设：** 如果不把LLM仅仅当作一个问答接口，而是将其置于一个具备“感知-规划-行动”能力的架构中，使其成为**Agentic AI（智能体AI）**，是否能解决问题？\n\n**方法论引入：** 引入**ReAct（Reasoning + Acting）**范式。即让LLM不仅生成答案，还要生成“思考过程”和“行动指令”，通过与环境交互（如查询网络状态）来迭代修正决策，从而实现多步推理。\n\n### 3. 架构演进：从单体智能到分层协作\n**挑战：** 6G网络极其复杂，涵盖无线接入网（RAN）、核心网等多个领域。让单一的LLM智能体掌握所有领域的知识并处理所有约束，认知负荷过重，容易导致推理混乱和错误。\n\n**思路突破：** 模仿人类企业的组织架构——**分工与协作**。\n*   **编排者：** 扮演项目经理角色，负责理解用户意图、拆解任务、协调资源。\n*   **领域专家：** 扮演技术顾问角色。设立RAN专家（负责频谱、基站）和Core网专家（负责UPF部署、拓扑）。\n\n**逻辑闭环：** 编排者不直接做技术决策，而是将意图转化为子问题，咨询相应的专家。专家基于注入的当前网络状态（结构化数据）给出建议，编排者汇总建议并生成最终配置。这种分层架构既降低了单点复杂度，又保证了决策的专业性。\n\n### 4. 落地机制：知识注入与状态锚定\n**问题：** LLM虽然通晓电信理论，但不知道当前网络的具体状态（如哪个基站负载过高），也不懂运营商特定的隐性偏好（如成本优先还是性能优先）。\n\n**解决方案：**\n*   **状态锚定：** 将实时的网络状态（负载、延迟矩阵、频谱可用性）转化为结构化的JSON数据，在每次推理时“注入”给智能体，防止其凭空捏造。\n*   **提示词工程即软代码：** 将电信领域的专家知识（如“URLLC业务必须选边缘节点”、“负载超过80%需预警”）编码进System Prompt中。这不仅是提示技巧，更是将领域知识固化为系统逻辑的过程。\n\n### 5. 评估视角的重构：语义与工程的双重校验\n**反思：** 传统的AI评估只看“准确率”。但在网络配置中，仅仅“听懂了”是不够的，配置必须“工程上可行”且“最优”。\n\n**评估框架创新：** 提出双重指标体系：\n*   **语义准确性：** 生成配置是否符合人类专家的预期（是否听懂了人话）。\n*   **工程效用：** 配置在数学上是否满足QoS约束（如延迟公式、资源利用率），是否是最优解。\n\n### 6. 实验洞察与偏差修正：对“语言”的再认识\n**意外发现：** 在实验中，作者发现系统存在一种“延迟贪婪”偏差——无论什么业务，智能体总是倾向于选择延迟最低的节点，导致资源浪费。\n\n**深层思考：** 这揭示了LLM的一个特性：**对提示词语义的极度敏感**。Prompt中微小的措辞差异（如说“可接受”还是“优先选择”）会引发系统性的行为偏差。\n\n**最终完善：** 这促使作者将Prompt工程提升到了核心架构组件的高度。通过迭代修正Prompt，明确指令（如“非URLLC业务必须优先使用区域数据中心以节约成本”），消除了偏差。这证明了在Agentic AI中，**如何定义智能体的“性格”和“规则”与架构本身同等重要"
                },
                {
                    "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
                    "arxiv_id": "2601.06502",
                    "authors": "Shengkai Chen, Zhiguang Cao, Jianan Zhou, Yaoxin Wu, Senthilnath Jayavelu, Zhuoyi Lin, Xiaoli Li, Shili Xiang",
                    "summary": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了DRAGON框架，明确使用了“Agents”和“language agents”概念。文中描述了智能体自主识别区域、分解问题（规划）、利用自适应经验记忆（记忆）、与环境交互并从反馈中迭代学习（自我反思/演化）。这完全符合单智能体和自我演化的研究范围，且不属于纯应用或纯推理排除项。",
                    "summary2": "本文旨在解决大语言模型（LLM）在大规模组合优化问题（COP）中可扩展性受限的问题。针对大规模COP场景，我们提出了一种名为DRAGON的分解与重构智能体框架，通过迭代识别高潜力区域并求解局部子问题来优化全局解。在TSPLIB、CVRPLIB和Weibull-5k等基准数据集上，通过Optimality Gap等指标验证了其有效性，在超大规模实例上实现了近最优解，显著优于现有基于LLM的求解器。",
                    "summary_translation": "大语言模型近期在利用基于提示的策略解决组合优化问题方面展现出潜力。然而，其可扩展性和泛化能力仍然受限，且随着问题规模的增大，其有效性会降低，特别是在涉及超过30个节点的路径问题中尤为明显。我们提出了 DRAGON（Decomposition and Reconstruction Agents Guided OptimizatioN，分解与重构智能体引导优化），这是一个结合了元启发式设计和 LLM 推理优势的新型框架。DRAGON 从一个初始全局解出发，自主识别具有高优化潜力的区域，并策略性地将大规模 COPs 分解为易于处理的子问题。随后，每个子问题被重新表述为一个简洁的局部优化任务，并在积累经验的指导下，通过针对性的 LLM 提示进行求解。最后，将局部优化后的解系统地重新整合到原始全局上下文中，从而产生显著改善的整体结果。通过与优化环境的持续交互并利用自适应经验记忆，智能体能够从反馈中迭代学习，从而有效地将符号推理与启发式搜索相结合。实验结果表明，与局限于小规模实例的现有基于 LLM 的求解器不同，DRAGON 在 TSPLIB、CVRPLIB 和 Weibull-5k 装箱基准测试中始终能生成可行解，并在拥有超过 300 万变量的背包问题上取得了接近最优的结果（0.16% gap）。这项工作展示了反馈驱动的语言智能体作为一种可泛化且可解释的大规模优化新范式的潜力。",
                    "inspiration_trace": "基于论文《DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization》，以下是对作者提出核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论产出的思考过程。\n\n---\n\n### 1. 宏观观察与矛盾识别：LLM的“能力”与“尺度”错位\n**思考起点：**\n作者首先观察到大语言模型（LLM）在解决组合优化问题（COP）上展现出了惊人的潜力，尤其是在逻辑推理和模式识别方面。然而，这种能力存在明显的“尺度天花板”。\n\n**逻辑推演：**\n*   **现象：** 现有的基于Prompt的LLM方法（如OPRO, SGE）在处理小规模问题（如节点数<30的TSP）时表现尚可，但一旦问题规模扩大到现实世界级别（如成千上万个节点），LLM的表现急剧下降。\n*   **归因：** 这种下降并非因为LLM不懂优化原理，而是受限于其**上下文窗口长度**和**长序列生成的逻辑连贯性**。直接让LLM一次性生成大规模问题的解，就像让一个人心算一本电话簿的排序，既不可行也不可靠。\n*   **核心矛盾：** 我们需要LLM的**通用推理能力**，但无法承受其在大规模问题上的**计算与记忆局限性**。\n\n### 2. 跨域借鉴：从传统运筹学中寻找“破局点”\n**思考转折：**\n既然LLM无法“一口吃成胖子”，作者将目光转向了传统运筹学中处理大规模问题的成熟策略——**元启发式算法**，特别是**大规模邻域搜索**。\n\n**逻辑推演：**\n*   **传统智慧：** LNS的核心思想不是一次性解决整个问题，而是“破坏”当前解的一部分，然后“修复”它。这种“分而治之”的策略完美规避了全局计算的复杂性。\n*   **痛点分析：** 传统的LNS虽然能扩展规模，但其高度依赖**人工设计的启发式规则**（例如：如何选择破坏区域？如何修复？）。这些规则往往针对特定问题，缺乏泛化性，且设计成本极高。\n*   **假设提出：** 能否用LLM来替代这些“人工规则”？即，利用LLM的语义理解能力来决定“哪里需要优化”，以及利用LLM的推理能力来执行“如何优化”。\n\n### 3. 核心假设形成：LLM作为“智能拆解者”与“局部修复者”\n**思考聚焦：**\n基于上述矛盾与借鉴，作者提出了两个关键的研究假设，构成了DRAGON框架的理论基石：\n\n*   **假设一（分解）：** LLM虽然无法直接解决大规模COP，但它具备足够的“直觉”来审视一个全局解，并识别出其中**看起来不合理或具有改进潜力的局部区域**（Active Segment）。\n*   **假设二（重构）：** 如果将大规模问题压缩为一个仅包含几十个节点的局部子问题，LLM完全有能力在遵守特定边界约束的前提下，找到该子问题的**局部最优解**。\n\n### 4. 方法论构建：从“直觉”到“闭环”的机制设计\n**思考深化：**\n有了假设，接下来需要解决具体的工程与逻辑问题：如何保证局部修改后的解能无缝融入全局？如何处理LLM生成的不可行解？\n\n**逻辑演进：**\n\n*   **阶段一：动态分解**\n    *   *设计思路：* 作者设计了一个“分解者”Agent。它的任务不是求解，而是“挑刺”。它将全局解分为两部分：保持不变的**静态段**和待优化的**活跃段**。\n    *   *关键点：* 这种分解不是随机的，而是基于LLM对当前解质量的评估，从而模仿了人类专家的直觉。\n\n*   **阶段二：约束感知的重构**\n    *   *设计思路：* 作者设计了一个“重构者”Agent。它接收压缩后的子问题。\n    *   *难点攻克：* 为了防止局部优化破坏全局可行性（例如路径断开），作者引入了**显式约束**。将静态段与活跃段的连接点转化为自然语言约束，强制LLM在修复时必须保留这些连接。\n\n*   **阶段三：经验驱动的自我修正**\n    *   *设计思路：* LLM偶尔会生成违反约束的解。作者没有选择简单的丢弃，而是引入了**经验记忆**。\n    *   *逻辑闭环：* 将之前的错误解及其原因反馈给LLM，让其进行反思和修正。这形成了一个“尝试-反馈-修正”的微循环，确保了重构阶段的鲁棒性。\n\n### 5. 最终框架确立：DRAGON的诞生\n**思考综合：**\n将上述环节串联，作者最终构建了DRAGON框架。这不再是一个简单的Prompt调用，而是一个**迭代的、状态传递的多智能体系统**。\n\n*   **逻辑链闭环：**\n    1.  **初始解**（由传统快速启发式获得）。\n    2.  **分解**（LLM识别薄弱环节）。\n    3.  **压缩**（提取局部子问题及约束）。\n    4.  **重构**（LLM在约束下求解局部问题）。\n    5.  **整合与评估**（将局部解拼回全局，若更优则接受）。\n    6.  **循环**（重复上述过程，直到收敛）。\n\n### 总结\n作者的思考路径遵循了**“发现问题（LLM尺度限制） -> 借鉴经典（分治思想） -> 融合创新（LLM替代人工规则） -> 机制完善（约束与反馈）”**的逻辑链条。DRAGON的本质，是将LLM从一个“全知全能但容易过载的求解者”，重塑为一个“专注于局部精修且具备全局视野的智能工匠”。"
                },
                {
                    "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
                    "arxiv_id": "2601.06377",
                    "authors": "Ningning Zhang, Xingxing Yang, Zhizhong Tan, Weiping Deng, Wenyong Wang",
                    "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了HiMem，这是一个专门为LLM长跨度智能体设计的分层长期记忆框架。它重点研究了智能体的核心组件——记忆（包括记忆构建、检索和动态更新），并引入了冲突感知的记忆再巩固机制以实现自我演化。这完全符合单智能体研究范围中的“记忆”和“自我演化”标准。",
                    "summary2": "本文旨在解决LLM智能体在长期交互中记忆适应性、可扩展性和自我进化不足的问题。针对长跨度对话场景，我们提出了一种名为HiMem的分层长期记忆框架，该框架通过Topic-Aware Event–Surprise Dual-Channel Segmentation构建Episode Memory，并结合冲突感知的Memory Reconsolidation机制。我们在LoCoMo benchmark上通过GPT-Score和F1指标验证了其有效性，结果显示HiMem在准确性和一致性上优于现有基线。",
                    "summary_translation": "尽管长期记忆系统近年来取得了显著进展，但在持续交互场景下的适应性、可扩展性和自我进化方面仍存在明显局限。受认知理论启发，我们提出了 HiMem，这是一个面向长程对话的分层长期记忆框架，旨在支持持续交互过程中的记忆构建、检索和动态更新。HiMem 通过 Topic-Aware Event--Surprise Dual-Channel Segmentation（主题感知的事件-惊喜双通道分割）策略构建认知一致的 Episode Memory（情景记忆），并通过多阶段信息提取流水线构建能够捕获稳定知识的 Note Memory（笔记记忆）。这两种记忆类型在语义上相互关联，形成了一种桥接具体交互事件与抽象知识的分层结构，从而在不牺牲信息保真度的情况下实现高效检索。HiMem 支持混合检索和 Best-Effort Retrieval（尽力而为检索）策略以平衡准确性与效率，并结合 Conflict-Aware Memory Reconsolidation（冲突感知的记忆再巩固）机制，根据检索反馈对存储的知识进行修正和补充。这种设计使得记忆能够在长期使用过程中实现持续的自我进化。在长程对话基准上的实验结果表明，HiMem 在准确性、一致性和长程推理方面始终优于代表性基线，同时保持了良好的效率。总体而言，HiMem 为构建自适应且自我进化的 LLM-based（基于大语言模型）对话智能体提供了一个有原则且可扩展的设计范式。代码可在 https://github.com/jojopdq/HiMem 获取。",
                    "inspiration_trace": "基于论文《HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观观察：LLM 的“金鱼记忆”困境\n**起点：** 作者首先观察到，尽管大语言模型（LLM）在单轮或短对话中表现优异，但在面对**长跨度、多轮次**的持续交互任务（如长期个人助理）时，存在根本性缺陷。\n**核心矛盾：** 现有的 LLM Agent 无法在长时间跨度内可靠地保存、组织和利用信息。这不仅是“记不住”的问题，更是“记不好”和“用不活”的问题。\n\n### 2. 问题诊断：现有方案的三大痛点\n作者分析了现有的三类主流方案（RAG、长上下文、结构化记忆），发现它们在长周期交互中存在三个无法同时解决的系统性缺陷：\n\n*   **痛点一：保真度与效率的零和博弈**\n    *   *现象：* 保留原始对话日志（保真度高）会导致检索成本高昂且充满噪声；而过度压缩摘要（效率高）会丢失推理所需的细节。\n    *   *结论：* 单一扁平的存储结构无法兼顾细节保留与检索效率。\n*   **痛点二：语义错位**\n    *   *现象：* 提取的记忆往往脱离原始语境，导致在处理时间指代、共指消解和隐含语义时出错。\n    *   *结论：* 记忆的表示方式缺乏统一的语义对齐机制。\n*   **痛点三：静态与僵化的更新机制**\n    *   *现象：* 现有系统通常是“只增不改”或仅基于相似度更新。当新信息与旧记忆冲突或互补时，缺乏修正和进化的能力。\n    *   *结论：* 记忆系统缺乏自我演化和纠错的能力。\n\n### 3. 认知启发：向人类记忆机制借力\n**转折点：** 为了解决上述痛点，作者从认知心理学中寻找灵感。人类记忆并非单一仓库，而是分层运作的：\n*   **情景记忆：** 记录具体的经历和事件（细节丰富，但碎片化）。\n*   **语义记忆：** 提炼出的知识和常识（抽象稳定，但脱离具体语境）。\n*   **记忆再巩固：** 当回忆失败或遇到冲突时，人类会重构记忆。\n\n**假设：** 如果能构建一个模仿这种分层结构的 LLM 记忆框架，就能在保留细节的同时提高效率，并实现动态更新。\n\n### 4. 架构构想：分层记忆的提出\n基于认知假设，作者提出了**HiMem** 的核心架构逻辑：\n\n*   **第一层：情景记忆**\n    *   *目标：* 解决“保真度”问题。保留细粒度的交互事件。\n    *   *思考：* 如何切分对话才符合认知？简单的按句或按段切分不够智能。必须结合**话题转换**和**意外/情绪突变**（即“事件-惊喜”双通道），确保每个片段在认知上是连贯的。\n*   **第二层：笔记记忆**\n    *   *目标：* 解决“效率”问题。存储稳定的知识（事实、偏好、画像）。\n    *   *思考：* 需要多阶段提取（先提取事实，再提取隐含信息，最后归一化），避免信息坍塌，并建立统一的语义空间（时间对齐、指代消解）。\n*   **层级关联：** 将两层记忆语义链接，形成从具体事件到抽象知识的过渡。\n\n### 5. 机制深化：检索与进化的闭环\n有了架构，还需要解决“怎么用”和“怎么变”的问题：\n\n*   **检索策略：混合与尽力而为**\n    *   *思考：* 为了平衡速度和准确率，不应总是检索所有层级。\n    *   *设计：* **Best-Effort 策略**——先查抽象的 Note Memory（快），如果证据不足，再下沉查 Episode Memory（准）。这模仿了人类先想常识，再回忆细节的过程。\n*   **自我进化：冲突感知的记忆再巩固**\n    *   *思考：* 如何解决“静态更新”的痛点？检索失败本身就是一种学习信号。\n    *   *设计：* 当 Note Memory 检索失败，但 Episode Memory 能找到证据时，触发**再巩固机制**。系统对比新旧信息，判断是“新增”、“扩展”还是“矛盾”，从而动态修正 Note Memory。这使得记忆系统具备了自我纠错和进化的能力。\n\n### 6. 逻辑总结\n作者的思考路径可以概括为：\n从**长程交互的失效**出发，诊断出**单一结构的局限性**，引入**人类认知的分层理论**作为指导，构建了**情景与语义并存的分层架构**，并利用**检索失败作为反馈信号**，最终实现了一个既能保留细节又能高效进化、具备自我纠错能力的长期记忆系统。\n\n这一逻辑链条体现了从“现象观察”到“理论借鉴”，再到“系统设计”和“动态反馈”的完整学术创新闭环。"
                },
                {
                    "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation",
                    "arxiv_id": "2601.06328",
                    "authors": "Ziqiao Xi, Shuang Liang, Qi Liu, Jiaqing Zhang, Letian Peng, Fang Nan, Meshal Nayim, Tianhui Zhang, Rishika Mundada, Lianhui Qin, Biwei Huang, Kun Zhou",
                    "summary": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于LLM智能体的工具使用能力，提出了一个开放世界环境用于智能体测试，并开发了包含规划器和执行者的智能体框架，涉及规划、自我修正等单智能体核心能力，符合筛选标准。",
                    "summary2": "本文旨在解决现有工具使用代理在开放世界环境中缺乏大规模、真实测试与训练环境的问题。针对大规模工具池、长时程任务及不可靠状态等场景，我们提出了ToolGym环境，该环境集成了任务创建引擎、状态控制器及Planner–Actor代理框架。我们在包含5,571个工具的ToolGym环境上，通过Success Rate、Recovery Rate等指标验证了其有效性，并证明利用其生成的少量数据微调模型即可超越大规模数据基线。",
                    "summary_translation": "使用工具的 Tool-using LLM agents (使用工具的大语言模型智能体) 在 open-world settings (开放世界设置) 中仍面临挑战，这些设置包含大型工具池、long-horizon objectives (长期目标)、wild constraints (复杂约束) 以及不可靠的工具状态。为了实现可扩展且真实的训练与测试，我们引入了一个开放世界工具使用环境，该环境构建于 204 个常用应用程序中的 5,571 个格式统一的工具之上。该环境包含一个 task creation engine (任务创建引擎)，用于合成具有 wild constraints (复杂约束) 的长期、多工具工作流，以及一个 state controller (状态控制器)，用于注入中断和故障以对鲁棒性进行压力测试。基于该环境，我们开发了一个 tool select-then-execute agent framework (工具选择-然后-执行智能体框架)，采用 planner-actor decomposition (规划者-执行者分解) 架构，将深思熟虑的推理和自我纠正与逐步执行分离开来。对最先进的 LLM (Large Language Model，大语言模型) 的全面评估揭示了工具规划与执行能力之间的错位、现有 LLM 在遵循约束方面的弱点，以及 DeepSeek-v3.2 最强的鲁棒性。最后，我们从该环境中收集了 1,170 条 trajectories (轨迹) 来 fine-tune (微调) LLM (Large Language Model，大语言模型)，其性能优于使用 119k 样本的 baselines (基线模型)，这表明该环境既是一个真实的 benchmark (基准)，也是 tool-using agents (工具使用智能体) 的一个有价值的 data engine (数据引擎)。我们的代码和数据将公开发布。",
                    "inspiration_trace": "基于论文《ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 1. 宏观观察：从“玩具级”到“开放世界”的鸿沟\n**起点：** 作者首先观察到 LLM 智能体在工具使用领域虽然发展迅速，但在实际落地中存在巨大落差。\n**现象：** 现有的 SOTA 模型在标准基准测试中分数很高，但在真实应用场景中表现不佳。\n**矛盾：** 真实世界是“开放”的——工具池巨大、任务链条长、约束条件模糊且充满冲突、工具状态不可靠。而现有的评估环境大多是“封闭”且“洁净”的，只测试“快乐路径”，无法暴露智能体在复杂环境下的真实缺陷。\n\n### 2. 核心假设：真实世界的“野性”是关键试金石\n**推论：** 要提升智能体的真实能力，不能继续在简化的沙盒中打磨，必须构建一个能模拟真实世界复杂度的“开放世界”环境。\n**定义问题：** 这个环境必须具备三个维度的“野性”：\n1.  **规模野性：** 海量且真实的工具库，而非几十个精心挑选的 API。\n2.  **约束野性：** 任务包含长时程、多工具协作以及相互冲突的复杂约束。\n3.  **状态野性：** 模拟真实世界的不可靠性（如超时、报错、状态变更），而非理想化的稳定响应。\n\n### 3. 环境构建：如何模拟“野性”？\n为了验证上述假设，作者着手构建 ToolGym，其设计逻辑遵循从“基础”到“动态”的演进：\n\n*   **基础层（工具标准化）：** 面对海量异构工具，首先解决“统一接口”问题。作者选择 MCP (Model Context Protocol) 作为标准，整合了 5,571 个真实工具，构建了一个可检索、可执行的庞大工具池，解决了“规模野性”。\n*   **任务层（自动化合成）：** 人工编写复杂任务成本太高。作者提出“任务创建引擎”，利用 LLM 自动合成包含“野性约束”的长时程任务。通过迭代反馈机制，确保任务不仅需要多工具协作，还包含复杂的逻辑依赖和冲突，解决了“约束野性”。\n*   **交互层（状态控制）：** 为了测试鲁棒性，作者引入“状态控制器”。这不仅仅是随机噪声，而是一个中间件机制，能够有策略地注入故障（如工具级超时、状态级篡改、约束级变更），从而主动制造困难，解决了“状态野性”。\n\n### 4. 架构演进：应对长时程复杂性的解耦策略\n在构建了环境后，作者思考：**什么样的智能体架构才能在这样的环境中生存？**\n**痛点分析：** 在长时程、高复杂度的任务中，单一的 ReAct 模式容易陷入“迷失”——模型难以在几十步的执行中保持全局目标的一致性，且容易在错误发生后无法恢复。\n**解决思路：** 借鉴人类解决复杂问题的思维模式，将“思考”与“行动”解耦。\n**方法论产出：** 提出 **Planner-Actor 框架**。\n*   **Planner（规划者）：** 负责宏观视角，进行任务分解、全局推理和自我纠正。它不直接调用工具，而是监控进度，确保不偏离目标。\n*   **Actor（执行者）：** 负责微观视角，专注于具体的工具检索、参数填充和步骤执行。\n*   **逻辑闭环：** 这种分离使得模型既能进行深思熟虑的规划，又能保持执行的敏捷性，同时 Planner 的介入机制专门用于解决长时程中的“漂移”问题。\n\n### 5. 价值闭环：从测试台到数据引擎\n**实验发现：** 利用 ToolGym 评估主流模型，作者发现了有趣的“错位”现象——模型普遍规划能力强，但执行能力弱；且“遵循约束”比“调用工具”更难。\n**最终升华：** 作者意识到，这个环境不仅能用来“考”模型，还能用来“教”模型。\n**逻辑延伸：** 既然环境能生成高难度、高复杂度的真实轨迹，那么这些轨迹就是最高质量的训练数据。\n**结论验证：** 实验证明，仅用 ToolGym 生成的 1,170 条高质量数据进行微调，效果优于使用 119k 条普通数据的基线。这证明了**“在真实野性环境中通过高难度试错获得的数据”具有极高的信息密度和训练价值**。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现实与评估的脱节**出发，通过**构建高保真的开放环境**来还原真实挑战，进而**设计解耦的智能体架构**以适应这种挑战，最后**将环境转化为数据引擎**，实现了从评估到训练的完整闭环。"
                },
                {
                    "title": "PCoKG: Personality-aware Commonsense Reasoning with Debate",
                    "arxiv_id": "2601.06234",
                    "authors": "Weijie Li, Zhongqing Wang, Guodong Zhou",
                    "summary": "Most commonsense reasoning models overlook the influence of personality traits, limiting their effectiveness in personalized systems such as dialogue generation. To address this limitation, we introduce the Personality-aware Commonsense Knowledge Graph (PCoKG), a structured dataset comprising 521,316 quadruples. We begin by employing three evaluators to score and filter events from the ATOMIC dataset, selecting those that are likely to elicit diverse reasoning patterns across different personality types. For knowledge graph construction, we leverage the role-playing capabilities of large language models (LLMs) to perform reasoning tasks. To enhance the quality of the generated knowledge, we incorporate a debate mechanism consisting of a proponent, an opponent, and a judge, which iteratively refines the outputs through feedback loops. We evaluate the dataset from multiple perspectives and conduct fine-tuning and ablation experiments using multiple LLM backbones to assess PCoKG's robustness and the effectiveness of its construction pipeline. Our LoRA-based fine-tuning results indicate a positive correlation between model performance and the parameter scale of the base models. Finally, we apply PCoKG to persona-based dialogue generation, where it demonstrates improved consistency between generated responses and reference outputs. This work bridges the gap between commonsense reasoning and individual cognitive differences, enabling the development of more personalized and context-aware AI systems.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一种由支持者、反对者和法官组成的辩论机制，通过多智能体交互（协作与博弈）来迭代完善知识图谱的构建，符合“多智能体：协作、通信、博弈”的研究范围。",
                    "summary2": "本文旨在解决现有常识推理模型忽略性格特征导致个性化能力不足的问题。针对个性化对话生成场景，我们提出了一种基于多智能体辩论机制的 Personality-aware Commonsense Knowledge Graph (PCoKG) 构建方法。该方法利用 LLM 角色扮演能力，通过支持者、反对者和法官的辩论机制生成高质量的四元组数据。我们在 PCoKG 数据集及 SPC 对话任务上，通过 BLEU-4 和 ROUGE 等指标验证了其有效性。",
                    "summary_translation": "大多数 commonsense reasoning models (常识推理模型) 忽视了 personality traits (人格特质) 的影响，限制了其在 dialogue generation (对话生成) 等个性化系统中的有效性。为了解决这一局限性，我们提出了 Personality-aware Commonsense Knowledge Graph (PCoKG，人格感知常识知识图谱)，这是一个包含 521,316 个 quadruples (四元组) 的 structured dataset (结构化数据集)。我们首先采用三个 evaluators (评估者) 对 ATOMIC dataset (ATOMIC 数据集) 中的事件进行评分和筛选，选择那些可能在不同 personality types (人格类型) 中引发多样化 reasoning patterns (推理模式) 的事件。在 knowledge graph construction (知识图谱构建) 方面，我们利用 large language models (LLMs，大型语言模型) 的 role-playing capabilities (角色扮演能力) 来执行 reasoning tasks (推理任务)。为了提高生成知识的质量，我们引入了一种包含 proponent (支持者)、opponent (反对者) 和 judge (评判者) 的 debate mechanism (辩论机制)，通过 feedback loops (反馈循环) 对输出进行 iterative refinement (迭代优化)。我们从多个角度对数据集进行了评估，并使用多个 LLM backbones (LLM 骨干网络) 进行了 fine-tuning (微调) 和 ablation experiments (消融实验)，以评估 PCoKG 的 robustness (鲁棒性) 及其 construction pipeline (构建流程) 的有效性。我们基于 LoRA 的 fine-tuning (微调) 结果表明，模型性能与 base models (基座模型) 的 parameter scale (参数规模) 呈正相关。最后，我们将 PCoKG 应用于 persona-based dialogue generation (基于人格的对话生成)，结果表明生成回复与 reference outputs (参考输出) 之间的 consistency (一致性) 得到了提高。这项工作弥合了 commonsense reasoning (常识推理) 与 individual cognitive differences (个体认知差异) 之间的差距，促进了更加个性化和具备 context-aware (上下文感知) 能力的 AI systems (AI 系统) 的开发。",
                    "inspiration_trace": "基于对论文《PCoKG: Personality-aware Commonsense Reasoning with Debate》的深度分析，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法落地的思考过程：\n\n---\n\n### 1. 宏观观察与问题定义：从“通用”到“个性”的缺失\n**思考起点：**\n作者首先审视了常识推理领域的现状。现有的主流知识图谱（如ATOMIC）和模型（如COMET）虽然能够建立事件与结果之间的因果联系（例如：“如果X发生，通常会导致Y”），但它们隐含了一个假设：**人类的认知是同质化的**。\n\n**逻辑断层：**\n在现实世界中，面对同一事件，不同性格的人往往会产生截然不同的反应。例如，面对“聚会”，内向者可能感到疲惫，而外向者则感到兴奋。现有的通用模型抹杀了这种“认知多样性”，导致生成的回复千篇一律，缺乏个性化色彩。\n\n**核心问题：**\n如何让常识推理模型跳出“平均人”的假设，捕捉并模拟不同人格特质下的差异化认知？\n\n---\n\n### 2. 概念假设与形式化：引入人格维度\n**理论构建：**\n为了解决上述问题，作者提出必须将“人格”这一变量显式地引入常识推理框架中。\n\n**形式化创新：**\n传统的知识图谱结构是三元组 $(e, r, t)$（事件、关系、结果）。作者将其扩展为四元组 $(e, p, r, t)$，其中 $p$ 代表人格信息。\n*   **选择依据：** 作者选择了MBTI（迈尔斯-布里格斯类型指标）作为人格框架。虽然MBTI在心理学界有争议，但在计算领域，它结构清晰、分类明确（16种类型），且大众认知度高，非常适合作为AI模拟的参数。\n\n**初步构想：**\n构建一个包含人格信息的常识知识图谱（PCoKG），使AI能够根据不同的人格类型生成差异化的推理结果。\n\n---\n\n### 3. 执行瓶颈与挑战：数据获取的困境\n**现实阻碍：**\n概念虽然清晰，但构建这样一个大规模数据集面临巨大的现实困难：\n1.  **众包成本高昂：** 传统的知识图谱构建依赖人工标注。要招募覆盖16种MBTI类型的大规模人群，并让他们针对特定事件进行推理，成本极高且难以管理。\n2.  **数据质量难控：** 即使有人力，如何保证标注者真的在扮演对应的人格？如何保证推理的深度和一致性？\n\n**思维转折：**\n既然人工众包不可行，必须寻找自动化、可扩展的替代方案。此时，大语言模型（LLMs）展现出的强大的角色扮演能力进入了作者的视野。\n\n---\n\n### 4. 方法论演进：从“简单模拟”到“质量控制”\n作者意识到，直接让LLM进行角色扮演虽然可行，但输出质量参差不齐。为了构建高质量的数据集，作者设计了层层递进的三个关键机制：\n\n#### 4.1 第一层思考：筛选“值得推理”的事件\n**逻辑：**\n并非所有事件都能引发人格差异。例如“人需要呼吸”这种生理事件，无论什么人格反应都一样。如果对所有事件都进行人格化推理，会引入大量噪音。\n**解决方案：**\n引入**“评估者机制”**。在生成数据前，先让LLM作为评估者，对ATOMIC中的事件进行打分，筛选出那些“容易引发不同人格产生不同反应”的事件。这保证了数据集的有效性和针对性。\n\n#### 4.2 第二层思考：利用LLM进行规模化生成\n**逻辑：**\n既然筛选出了高质量事件，接下来就是利用LLM的生成能力来替代人工。\n**解决方案：**\n设计Prompt，让LLM扮演特定的MBTI类型，对筛选后的事件进行推理。这解决了“规模化”的问题，能够低成本生成海量数据。\n\n#### 4.3 第三层思考：通过“辩论”提升推理深度\n**逻辑：**\n单次Prompt生成的回答往往流于表面或刻板印象（例如简单地认为内向者就是害羞）。如何让AI的推理更深刻、更符合特定人格的逻辑？\n**解决方案：**\n引入**“多智能体辩论机制”**。\n*   **设计哲学：** 模拟人类学术辩论或批判性思维过程。\n*   **角色分配：** 设定支持者（证明推理符合人格）、反对者（挑战推理的一致性）和法官（裁决并反馈）。\n*   **闭环优化：** 通过多轮辩论和法官的反馈，迫使模型不断修正其推理结果，直到输出高质量、逻辑严密且符合人格设定的内容。\n\n---\n\n### 5. 验证与应用：逻辑闭环的完成\n**思考终点：**\n方法构建完成后，必须验证其有效性。\n1.  **数据质量验证：** 通过可读性分析（验证不同人格的语言风格差异）和互信息分析（验证推理结果与人格类型的关联度），证明生成的数据确实包含了人格信号。\n2.  **下游任务验证：** 将PCoKG应用于个性化对话生成。实验证明，融入了人格感知常识的模型，生成的回复比通用模型更具一致性和拟人化。\n\n---\n\n### 总结：作者的思维演进图谱\n1.  **观察：** 现有常识推理缺乏“个性”，无法模拟人类认知差异。\n2.  **假设：** 将MBTI人格引入知识图谱结构 $(e, p, r, t)$ 可以解决此问题。\n3.  **挑战：** 人工构建数据不可行，且直接生成质量低。\n4.  **破局：**\n    *   用 **LLM角色扮演** 替代人工（解决规模）。\n    *   用 **评估者筛选** 锁定高价值事件（解决噪音）。\n    *   用 **辩论机制** 迭代优化生成质量（解决深度）。\n5.  **产出：** PCoKG数据集及其构建pipeline，实现了高质量、大规模的个性化常识推理。"
                },
                {
                    "title": "HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants",
                    "arxiv_id": "2601.06152",
                    "authors": "Hailong Li, Feifei Li, Wenhui Que, Xingyu Fan",
                    "summary": "Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一种受海马体启发的记忆系统，用于构建个性化AI助手，重点解决了LLM在知识密集型场景中的短期与长期记忆融合问题。这直接属于LLM智能体研究范围中的“单智能体：记忆”模块，且不属于排除的纯应用或纯推理范畴。",
                    "summary2": "本文旨在解决传统RAG在个性化AI助手中的记忆局限问题。针对知识密集型场景，我们提出了一种受海马体启发的HiMeS架构，融合了短期和长期记忆。短期记忆模块利用RLHF压缩对话并预检索，长期记忆模块通过分区存储和注意力机制重排序文档。在真实工业数据集上，通过CA、QA和QR指标验证了其有效性，显著优于传统RAG。",
                    "summary_translation": "大语言模型（Large language models, LLMs）驱动着许多交互系统，例如聊天机器人、客服代理和个人助理。在需要用户特定个性化的知识密集型场景中，传统的检索增强生成（retrieval-augmented generation, RAG）流水线表现出有限的记忆容量，且检索机制与用户特定对话历史之间缺乏协调，从而导致冗余的澄清询问、检索文档不相关以及用户体验下降。受海马体-新皮层记忆机制（hippocampus-neocortex memory mechanism）的启发，我们提出了 HiMeS，一种融合短期和长期记忆的 AI 助手架构。我们的贡献主要体现在以下四个方面：(1) 训练了一个短期记忆提取器（short-term memory extractor），利用强化学习（reinforcement learning）进行端到端训练，以压缩最近的对话并主动从知识库（knowledge base）中预检索文档，从而模拟海马体（hippocampus）与前额叶皮层（prefrontal cortex）之间的协作交互。(2) 构建了一个分区的长期记忆网络（long-term memory network），用于存储用户特定信息并对检索到的文档进行重排序，模拟分布式皮层存储（distributed cortical storage）和记忆再激活（memory reactivation）。(3) 在一个真实世界工业数据集上，HiMeS 在问答质量方面显著优于级联 RAG 基线（cascaded RAG baseline）。(4) 消融实验（Ablation studies）证实了这两个记忆模块的必要性，并为构建更可靠、具备上下文感知（context-aware）及用户定制（user-customized）能力的基于 LLM 的助手指明了实践路径。",
                    "inspiration_trace": "基于论文《HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法论的思考过程：\n\n---\n\n### 1. 宏观观察：工业场景下的“个性化”困境\n**起点：** 作者身处腾讯微信的工业环境，观察到一种普遍现象：虽然大语言模型（LLM）和检索增强生成（RAG）技术已经成熟，但在处理**知识密集型**且**高度个性化**的任务（如公众号助手、客服）时，现有系统表现不佳。\n**核心矛盾：** 用户希望AI能像“老朋友”或“专业顾问”一样，基于过往的交互历史和特定背景来回答问题，但现有的AI助手往往是“健忘”的，每次对话都像是从零开始。\n\n### 2. 问题解构：双重记忆缺失\n作者将上述宏观矛盾拆解为两个具体的失效模式：\n\n*   **短期记忆失效（语义错位）：**\n    *   **观察：** 在多轮对话中，用户的当前提问往往省略了前文提到的关键信息（例如：“那它多少钱？”中的“它”指代不明）。\n    *   **传统做法的局限：** 传统RAG直接用当前简短的Query去检索，或者简单地把历史对话拼接到Context Window中。前者导致检索不到相关文档，后者导致注意力分散且效率低下。\n    *   **结论：** 系统缺乏对“当前对话上下文”的有效压缩和利用，导致检索Query与用户真实意图不匹配。\n\n*   **长期记忆失效（灾难性遗忘）：**\n    *   **观察：** 作者发现一个关键指标——**重复提问率（RAR）**高达70-80%。这意味着用户在不同会话中反复问同样的问题，因为系统一旦会话结束就丢弃了数据。\n    *   **结论：** 系统缺乏跨会话的持久化用户画像，无法像人类专家那样积累对用户的“长期印象”，导致无法提供定制化服务。\n\n### 3. 理论映射：海马体-大脑皮层机制的启发\n**思考转折：** 作者跳出纯工程视角，转向认知神经科学寻求答案。\n**类比：** 人类记忆是如何工作的？\n*   **海马体：** 负责短期记忆的编码和快速提取，处理当下的信息。\n*   **大脑皮层：** 负责长期记忆的分布式存储和巩固，在需要时被重新激活。\n**假设：** 如果在AI系统中构建一个模仿“海马体-皮层”协作的双层记忆架构，或许能解决上述短期和长期记忆的缺失问题。\n\n### 4. 方法论演进 I：短期记忆模块（STM）——从“重写”到“对齐”\n**目标：** 解决当前Query的语义缺失问题。\n*   **初步构想：** 训练一个模型把历史对话压缩，重写当前的Query。\n*   **批判性思考：** 传统的监督微调（SFT）只是让模型模仿“重写”的风格，并不保证重写后的Query能检索到更好的文档，也不保证最终回答质量更高。这是“局部最优”而非“全局最优”。\n*   **进阶方案：** 引入**强化学习（RL）**。\n    *   **逻辑：** 不再只看“重写得好不好”，而是看“最终回答得好不好”。将重写器、检索器和生成器视为一个整体，通过端到端的奖励信号（如Rouge-L、Exact Match、Hit Score）来反向优化重写策略。\n    *   **生物学对应：** 这模拟了海马体与前额叶皮层的协作，不仅编码信息，还根据决策目标（回答质量）动态调整提取策略。\n\n### 5. 方法论演进 II：长期记忆模块（LTM）——从“存储”到“激活”\n**目标：** 解决跨会话的用户画像遗忘问题。\n*   **初步构想：** 把用户的历史Query都存进向量数据库。\n*   **批判性思考：** 简单的平铺式存储在面对海量数据时检索慢且噪音大。人类大脑是按“分区”存储记忆的（如时间、空间、主题）。\n*   **进阶方案 1（分区存储）：** 提出**原子主题建模（ATM）**。将用户历史Query按16大类及细分子类进行分区存储。这模仿了大脑皮层的分布式存储特性，大幅缩小检索范围，提高效率。\n*   **进阶方案 2（注意力机制重排）：** 仅仅存下来不够，关键在于如何“用”。\n    *   **逻辑：** 当检索到一批文档后，利用用户的**长期历史Query向量**作为“注意力权重”，去重新计算这些文档块的相关性并进行重排。\n    *   **生物学对应：** 这模拟了记忆的“再激活”过程。当前的感知（检索到的文档）需要通过过往的经验（长期记忆）来过滤和赋予意义，从而筛选出最符合该用户特定背景的知识。\n\n### 6. 系统综合：HiMeS架构的诞生\n**最终逻辑闭环：**\n作者将上述两个模块融合，构建了HiMeS系统：\n1.  **输入：** 用户当前Query + 对话历史。\n2.  **海马体路径（STM）：** RL优化的重写器压缩上下文，生成富含信息的检索Query，进行初检。\n3.  **皮层路径（LTM）：** 系统根据用户ID激活对应的历史记忆分区，利用历史Query对初检结果进行“注意力加权”和重排。\n4.  **输出：** 经过双重记忆过滤后的精准知识片段，输入给LLM生成个性化回答。\n\n### 总结\n作者的思考路径遵循了**“现象观察 -> 问题解构 -> 跨域类比（脑科学） -> 机制映射与工程化（RL + 分区存储 + 注意力重排） -> 系统验证”**的完整逻辑链条。其核心创新点在于不满足于简单的模块堆叠，而是通过生物学启发，将“端到端优化”和“记忆再激活”思想引入RAG系统，从而解决了工业级AI助手“记不住”和“听不懂”的痛点。"
                },
                {
                    "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs",
                    "arxiv_id": "2601.06126",
                    "authors": "Boshen Shi, Kexin Yang, Yuanbo Yang, Guanguang Chang, Ce Chi, Zhendong Wang, Xing Wang, Junlan Feng",
                    "summary": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.",
                    "category": "cs.AI",
                    "filter_reason": "摘要明确提到开发了一个“多智能体系统”，并将算法实例化为工具，符合多智能体协作和工具使用的研究范围。",
                    "summary2": "本文旨在解决现有LLM生成仪表板时存在的表示冗余和可控性低的问题。针对自然语言提示和表格数据，我们提出了一种基于Analysis-Presentation Decoupling原则的NL2Dashboard框架，引入结构化Intermediate Representation (IR)解耦分析与呈现。我们在涵盖金融、教育等领域的真实数据集上，通过视觉质量、Token效率（GOR）和修改成功率等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "尽管大型语言模型在生成独立图表方面已展现出卓越的能力，但生成综合仪表板仍然是一项艰巨的挑战。现有的端到端范式通常将仪表板生成视为直接代码生成任务（例如原始HTML），但存在两个根本性局限：一是因视觉渲染消耗大量Token (词元) 而导致的表征冗余，二是因分析推理与展示呈现相互耦合而导致的可控性较低。为应对这些挑战，我们提出了NL2Dashboard，这是一种基于“分析-展示解耦”原则的轻量级框架。我们引入了一种结构化中间表示，用于封装仪表板的内容、布局和视觉元素。因此，该框架将LLM的角色限定于数据分析和意图转换，而将视觉合成工作交由确定性渲染引擎完成。在此框架基础上，我们开发了一个多智能体系统，其中由IR驱动的算法被实例化为一套工具集。利用该系统进行的综合实验表明，NL2Dashboard在多个领域显著优于最先进的基线模型，实现了更优越的视觉质量、显著更高的Token (词元) 效率，以及在生成和修改任务中精确的可控性。",
                    "inspiration_trace": "基于论文《NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“单图生成”到“复杂仪表盘”的跨越）**\n\n1.  **现象观察**：\n    作者首先注意到，虽然现有的LLM在生成独立的图表方面表现出色，但在生成综合性仪表盘时仍面临巨大挑战。\n2.  **现状分析**：\n    当前的主流范式是“端到端生成”，即直接要求LLM生成完整的HTML/CSS/JavaScript代码来渲染仪表盘。\n3.  **核心痛点识别**：\n    作者深入分析发现，这种直接生成代码的方式存在两个根本性缺陷：\n    *   **表征冗余**：LLM消耗了大量的Token去生成视觉渲染代码（如HTML标签、CSS样式），导致用于数据分析和逻辑推理的Token预算被严重压缩，效率低下。\n    *   **可控性差**：数据分析逻辑与视觉呈现逻辑高度耦合。当用户需要修改仪表盘时，LLM往往需要重新生成整个HTML文件，极易破坏全局布局，且难以进行精细化的局部修改。\n\n### 第二阶段：核心假设与范式转移\n**（从“代码生成器”到“分析引擎”的认知转变）**\n\n1.  **本质洞察**：\n    作者提出一个核心观点：LLM的本质优势在于逻辑推理和数据分析，而非像素级的视觉渲染。LLM应该扮演“分析引擎”的角色，而不是“渲染引擎”。\n2.  **提出假设**：\n    如果能将“数据分析”与“视觉呈现”解耦，就能同时解决Token效率和可控性问题。\n3.  **确立原则**：\n    基于此，作者确立了**“分析-呈现解耦”**的设计原则。即让LLM专注于“做什么”，而将“怎么做”交给确定性更强的规则或模板去处理。\n\n### 第三阶段：方法论构建与中间层设计\n**（引入“中间表示”作为桥梁）**\n\n1.  **引入中间层**：\n    为了实现解耦，作者设计了一个结构化的**中间表示**。IR不包含具体的样式代码，而是抽象地描述了仪表盘的内容、布局和视觉元素。\n2.  **构建两阶段流程**：\n    基于IR，作者构建了“推理-渲染”的两阶段工作流：\n    *   **Prompt-to-IR（推理阶段）**：LLM仅负责理解用户意图、执行数据分析，并将结果（图表、表格、指标）及其布局位置填入IR。此时，LLM输出的Token密度极高，全是有效信息。\n    *   **IR-to-Dashboard（渲染阶段）**：利用一个确定性的渲染引擎，通过“插槽填充”机制，将IR中的内容映射到预定义的高质量HTML模板中。这一步不再消耗LLM的推理资源。\n\n### 第四阶段：针对“修改”场景的精细化设计\n**（解决迭代编辑中的不可控问题）**\n\n1.  **深入修改场景**：\n    作者意识到，仪表盘的生成往往不是一次性的，用户会频繁迭代修改。直接修改HTML极其困难，那么如何修改IR？\n2.  **意图翻译技术**：\n    作者提出将用户的自然语言修改指令翻译为一系列**原子操作**（如Change, Swap, Delete, Add）。\n3.  **脚本化更新**：\n    通过生成“修改脚本”，LLM只需更新IR中的特定字段，而不需要重写整个配置。这确保了修改的精确性，避免了“牵一发而动全身”的布局崩坏。\n\n### 第五阶段：系统实现与理论验证\n**（多智能体协作与熵减理论）**\n\n1.  **工程化落地**：\n    为了处理复杂的任务流，作者将上述算法实例化为工具，并设计了一个多智能体系统：\n    *   **Planner**：负责意图识别和任务调度。\n    *   **Coder**：负责执行代码生成和数据分析（保证分析忠实性）。\n    *   **Critic**：利用视觉模型评估图表质量。\n    *   **Toolkit**：封装了IR生成、修改和渲染的确定性工具。\n2.  **理论升华**：\n    最后，作者利用信息论中的熵分解原理证明了该方法的有效性。通过将视觉呈现的不确定性（$H_{vis}$）降至接近0（由确定性模板承担），整个生成系统的总熵显著降低，从而在理论上证明了成功概率的提升。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有“端到端代码生成”模式的资源浪费和不稳定性**出发，通过**引入“中间表示（IR）”**这一核心创新，实现了**逻辑与样式的解耦**。这不仅释放了LLM的推理潜能，还通过**原子化操作**解决了精细修改的难题，最终构建了一个既轻量又可控的仪表盘生成框架。"
                },
                {
                    "title": "PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction",
                    "arxiv_id": "2601.06158",
                    "authors": "Zibin Meng, Kani Chen",
                    "summary": "Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了PsyAgent，一种基于LLM的智能体架构，旨在通过心理建模（大五人格特质）和语境交互构建类人智能体。研究内容涉及智能体的记忆（个体结构、生活片段）和语境感知行为，属于单智能体研究范畴（记忆、人设建模），且不属于排除的纯应用或纯推理领域。",
                    "summary2": "本文旨在构建能够模拟性格特质与社会结构交互的类人智能体。针对Big Five人格先验与结构化社会场景，我们提出了一种PsyAgent框架，该框架耦合了Individual Structure (IS) 和Multi-Scenario Contexting (MSC)，并利用合成监督数据通过PEFT和DPO微调小模型。我们在多轮角色扮演和决策任务上，通过ProfileAcc、MAE_5等指标验证了其有效性。",
                    "summary_translation": "拟人化智能体需要对性情与社会结构之间的相互作用进行建模。我们提出了PsyAgent，该模型将Big Five trait prior（大五人格特质先验）与Bourdieu's cognitive-social co-structure（布迪厄的认知-社会共构）相结合。PsyAgent包含两个部分：(i) Individual Structure (IS，个体结构)，这是一种机器可读的档案，编码了特质与侧面、认知风格、价值观、文化与教育资本以及显著的生活片段；(ii) Multi-Scenario Contexting (MSC，多场景情境化)，这是一种跨越八个领域（工作、家庭、友谊、陌生人与公民生活、独处与自我调节、浪漫关系、学习以及公共表达）的角色-关系-规范框架。在推理阶段，固定的structured prompts（结构化提示词）将活跃场景与智能体档案绑定，从而产生既稳定又具有情境敏感性的行为。我们通过实例化IS和MSC来合成监督信号（包括role-play dialogues（角色扮演对话）、decision probes（决策探针）和feedback trajectories（反馈轨迹）），随后对一个小型LLM（大语言模型）进行微调。生成的模型能够针对指定的Big Five configurations（大五人格配置）产生一致的、可识别的persona-aligned behaviors（人格对齐行为），并在我们的评估指标上匹配或超越多个更大的untuned LLMs（未微调的大语言模型）及其他untuned baselines（未微调基线），这些指标包括：persona consistency（人格一致性）、contextual appropriateness（情境适当性）、style matching（风格匹配）、trait identifiability（特质可识别性）以及long-horizon stability（长期稳定性）。消融实验表明，IS主要提升了trait fidelity（特质保真度）和stylistic stability（风格稳定性），而MSC则增强了norm awareness（规范意识）和decision fit（决策拟合度）；两者对于实现跨场景性能均必不可少。PsyAgent为构建personality-grounded agents（基于人格的智能体）提供了一种精确且data-efficient（数据高效）的架构。",
                    "inspiration_trace": "基于论文《PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察与痛点识别\n**逻辑起点：现有智能体的“人格漂移”与“情境脱节”**\n*   **观察**：现有的基于人格提示词的对话智能体虽然在短期内能模仿特定语气，但在长对话或跨场景（如从工作切换到家庭）时，往往会出现人格崩塌或行为不一致。\n*   **问题本质**：传统方法仅将人格视为静态的“知识”或“风格标签”，忽略了人类行为的本质——**行为是稳定特质与特定社会结构互动的产物**。单纯的大模型规模或简单的Prompt工程无法解决“特质”与“情境”之间的动态耦合问题。\n\n### 2. 理论锚定与核心假设\n**引入心理学与社会学框架作为理论基石**\n*   **理论选择**：作者引入心理学中的**“大五人格”**作为特质的先验，同时引入社会学中布迪厄的**“认知-社会共构”**理论。\n*   **核心假设**：要构建逼真的智能体，必须显式地建模两个维度的接口：\n    1.  **内在的、稳定的倾向**（我是谁）。\n    2.  **外在的、结构化的社会场域**（我在哪，规则是什么）。\n*   **推论**：智能体的行为不应是随机生成的，而应是“内在特质”在“特定社会情境约束”下的函数输出。\n\n### 3. 结构化解构\n**将抽象理论转化为可计算的架构组件**\n为了验证上述假设，作者将问题拆解为两个可计算的结构：\n*   **组件一：个体结构**\n    *   *思考*：仅有“大五人格分数”太单薄，无法支撑丰富的行为。需要补充背景信息。\n    *   *定义*：构建一个包含教育轨迹、生活经历、社会经济背景、文化资本四个维度的机器可用档案。这代表了智能体的“长期记忆”和“内在资源”。\n*   **组件二：多情境上下文**\n    *   *思考*：情境不能只是简单的“在办公室”。必须包含角色关系、权力结构、社会规范和利益相关者。\n    *   *定义*：构建覆盖工作、家庭、友谊等8个领域的框架库，每个场景明确定义了角色、规范和风险。这代表了智能体面临的“短期约束”。\n\n### 4. 数据构建策略\n**解决“高质量情境数据稀缺”的问题**\n*   **困境**：现实中很难找到大量同时标注了详细心理档案和复杂社会情境的对话数据。\n*   **策略**：**自举合成**。\n    *   利用强大的LLM，基于IS（档案）和MSC（场景）的笛卡尔积，合成监督数据。\n    *   *逻辑*：通过精心设计的Prompt，让大模型生成符合特定人格在特定场景下的反应（角色扮演、决策探针、反馈轨迹）。\n    *   *目的*：将理论框架（IS+MSC）转化为具体的训练样本，教会小模型这种“特质-情境”的互动模式。\n\n### 5. 模型训练范式\n**验证“架构优于规模”的假设**\n*   **思考**：是否必须依赖超大规模模型才能实现这种复杂的心理模拟？\n*   **假设**：如果数据结构足够好（富含心理和情境逻辑），小模型配合高效微调也能超越未微调的大模型。\n*   **方法论**：\n    *   **SFT（有监督微调）**：让模型学习IS和MSC的基本语言风格和规范。\n    *   **DPO（直接偏好优化）**：进一步校准模型在特定情境下的决策倾向，使其更符合目标大五人格的偏好。\n    *   **推理机制**：使用固定的结构化Prompt将IS和MSC绑定，确保推理时的行为既稳定（源于IS）又敏感（源于MSC）。\n\n### 6. 验证与闭环\n**通过消融实验确认理论组件的互补性**\n*   **评估逻辑**：不仅要看对话通顺度，更要看“人格一致性”和“情境适应性”。\n*   **发现与闭环**：\n    *   实验证明，移除IS会导致特质保真度下降（说明IS负责“我是谁”）。\n    *   移除MSC会导致规范意识下降（说明MSC负责“我在哪”）。\n    *   最终结论：PsyAgent通过解构并重组“特质”与“情境”，成功用小模型实现了超越大模型基线的心理拟真度，验证了最初的“认知-社会共构”假设。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（人格漂移）**出发，寻找**理论解释（心理学+社会学）**，将其**工程化（IS+MSC架构）**，通过**合成数据**解决数据瓶颈，最后利用**高效微调**验证了“结构化设计优于暴力规模”的方法论有效性。"
                },
                {
                    "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions",
                    "arxiv_id": "2601.06115",
                    "authors": "V. Cheung",
                    "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题明确提到了“Multi-Agent LLM Companions”（多智能体LLM同伴），摘要中提出了“人工集体无意识（ACU）”作为智能体共享交互模板的池，涉及多智能体之间的资源共享、协作以及长期适应任务，符合多智能体（协作、通信）的研究范围。",
                    "summary2": "本文旨在将LLM的离线幻觉转化为学习资源，解决多智能体同伴缺乏跨用户经验共享的问题。针对离线场景，我们提出了一种受荣格启发的Dream Layer架构，利用Artificial Collective Unconscious (ACU)共享去标识化的Interaction Templates。在行为模拟和边缘案例数据集上，通过诗意语言密度、边缘案例覆盖率和语义多样性等指标验证了其有效性。",
                    "summary_translation": "受到一个关于日常硬件项目中知识共享障碍的个人梦境的启发，本文为 LLM companions (大语言模型伴侣) 提出了一个受荣格理论启发的“Dream Layer (梦境层)”，将受控的 offline hallucinations (离线幻觉) 重新定义为一种用于学习和建立关系的资源，而不仅仅是一个 reliability bug (可靠性缺陷)。借鉴荣格关于 collective unconscious (集体潜意识) 作为 archetypal forms (原型形式) 共享存储库的概念，我们引入了一个 Artificial Collective Unconscious (ACU，人工集体潜意识)：这是一个共享的梦境池，agents (智能体) 在其中贡献 de-identified (去标识化) 的、抽象的 Interaction Templates (交互模板)，这些模板随后被 re-instantiated (重新实例化) 为 idiosyncratic Dream Narratives (特异性梦境叙事)。Dream Layer (梦境层) 严格在 offline (离线) 状态下运行：logic-enforcing modules (逻辑强制模块) 被放宽，sampling temperature (采样温度) 被提高，从而产生安全但故意 bizarre narratives (离奇叙事)（例如，货币不匹配的旅行序列），以增强用于 rare events (罕见事件) 和 edge-case safety tests (边缘情况安全测试) 的数据；为了有效地利用风险，我们添加了一个包含严格抽象、temporal delays (时间延迟) 和 ephemeral memory (短暂记忆) 的 governance stack (治理栈)。通过对 everyday dialogue (日常对话) 和 long-horizon adaptation tasks (长期适应任务) 的 behavioural simulations (行为模拟)，我们表明 Dream Layer (梦境层) 实现了一个关键的 decoupling (解耦)：agents (智能体) 在 safety constraints (安全约束)（例如，安全策略）方面保持坚定，而在 narrative strategy (叙事策略)（例如，使用共享的 archetypal metaphors (原型隐喻) 来解决 deadlocks (僵局)）方面变得灵活。这在概念上重新定义了 hallucination (幻觉)，使得 online, unmarked instances (在线、未标记实例) 仍然是 bugs (缺陷)，而 bounded, marked, and delayed ones (有界、标记和延迟的实例) 则成为 synthetic scenarios (合成场景) 和加深 companionship (伴侣关系) 的宝库，这与当代神经科学中提出的 anti-overfitting dream mechanisms (抗过拟合梦境机制) 相呼应。",
                    "inspiration_trace": "基于论文《Dreaming Is Not a Bug: A Jung Inspired Dream Layer for Multi Agent LLM Companions》，以下是对作者产出核心方法逻辑链的系统性推演：\n\n### 1. 起点：从个人体验到宏观悖论\n**观察与痛点：**\n作者从一个极具荒诞感的个人梦境（关于硬件项目中的版权阻碍）出发，敏锐地捕捉到了这个梦境与当前大语言模型（LLM）交互体验之间的惊人相似性：**当寻求具体结构或知识时，往往遭遇抽象边界的阻碍或流畅但无实质的文本。**\n\n**宏观问题提出：**\n由此，作者指出了当前LLM伴侣的两个根本性局限：\n1.  **孤岛效应：** 学习被限制在单个用户的对话孤岛中，无法跨个体提炼或共享洞察。\n2.  **单向度的幻觉观：** 幻觉被纯粹视为可靠性缺陷，必须被抑制，而非一种可被利用的资源。\n\n**核心矛盾：** 我们是否一直在试图“消灭”幻觉，而忽略了它在某种形式下可能具有的进化价值？\n\n### 2. 转折：跨学科的理论借力\n**寻找生物学隐喻：**\n为了解决上述矛盾，作者将目光投向神经科学，引入了**“过拟合大脑假说”**。该理论认为，生物梦境的作用是“离线数据增强”，通过故意生成离奇、分布外的感官输入来防止大脑对日常刺激过拟合。\n\n**假设形成：**\n如果人类利用“怪诞的梦境”来正则化内部模型以提高泛化能力，那么LLM是否也能将“幻觉”转化为一种工程化的想象力资源？\n*   **关键推论：** 幻觉不应被全盘消灭，而应被**隔离**并**控制**，使其在离线状态下服务于模型的学习与泛化。\n\n### 3. 核心：从“共享数据”到“共享原型”\n**引入心理学隐喻：**\n为了解决“孤岛效应”并实现跨代理学习，作者引入了荣格的**“集体潜意识”**概念。其核心在于区分“共享的抽象”与“私有的实例”。\n\n**概念跃迁：**\n作者意识到，直接共享用户对话数据会引发隐私问题，且难以泛化。因此，必须模仿荣格的“原型”概念：\n*   **不做原始数据的共享：** 不分享具体的对话内容。\n*   **做结构模式的共享：** 提取去标识化的、高度抽象的**“交互模板”**（Interaction Templates）。\n\n**方法论雏形：** 构建一个**“人工集体潜意识”（ACU）**，作为所有代理贡献抽象交互模式的共享池。\n\n### 4. 构建：昼夜分离的架构设计\n**架构映射：**\n基于上述理论，作者设计了“梦境层”架构，将代理的运行状态严格划分为“在线”与“离线”两个世界，以此解决“幻觉不可控”的风险。\n\n*   **在线层：** 严格遵循事实、逻辑和安全策略（对应人类的“清醒状态”）。\n*   **离线层：** 放松逻辑约束，提高采样温度，引入噪声（对应人类的“做梦状态”）。\n\n**逻辑闭环：**\n1.  **抽象化：** 代理将在线交互经历抽象为去标识化的模板，存入ACU。\n2.  **再实例化：** 代理从ACU采样模板，通过受控的离线幻觉生成怪诞但结构连贯的“梦境叙事”。\n3.  **策略蒸馏：** 这些梦境不直接作为知识，而是被解析，提炼出高层次的**行为策略**，反向更新代理的在线行为。\n\n### 5. 收敛：安全与治理的边界设定\n**风险意识：**\n作者清醒地认识到，让AI“自由做梦”存在巨大的安全风险（隐私泄露、叙事投毒、不可控输出）。\n\n**治理逻辑：**\n为了使理论落地，必须引入严格的治理栈，将“做梦”限制在笼子里：\n*   **严格抽象与去标识化：** 确保ACU中只有结构骨架，无个人痕迹。\n*   **时间延迟：** 强制冷却期，防止实时关联攻击。\n*   **短暂记忆：** 梦境内容必须随时间衰减，只有提炼出的策略才能长期保留。\n*   **零信任消费：** 代理只能将梦境作为弱先验，不能作为执行指令。\n\n### 6. 验证：从“做梦”到“进化”的闭环\n**实证思路：**\n最后，作者通过实验验证这一假设的可行性，而非仅仅停留在哲学层面。\n*   **现象验证：** 证明在特定指令下，模型确实能进入可观测、可复现的“梦境状态”（如诗歌语言密度的显著提升）。\n*   **功能验证：** 证明这种机制能加速边缘案例的覆盖，并提升日常对话的多样性（降低拒绝率）。\n\n**总结：**\n作者的思考路径是从**现象（梦境与AI交互的相似性）**出发，经由**理论（神经科学与荣格心理学）**的启发，提出了**概念重构（将幻觉视为离线资源）**，最终通过**架构设计（梦境层+ACU）**和**严格治理（安全边界）**，将一个看似哲学的隐喻转化为了可工程实现的AI系统方法论。"
                },
                {
                    "title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions",
                    "arxiv_id": "2601.06112",
                    "authors": "Aayush Gupta",
                    "summary": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于评估工具使用LLM智能体的可靠性，涉及单智能体架构（ReAct, Reflexion）的评估，涵盖了工具使用和自我反思等核心智能体能力，符合单智能体研究范围。",
                    "summary2": "本文旨在解决现有基准测试无法全面评估LLM Agent生产环境可靠性的问题。针对生产环境中的压力条件，我们提出了一种名为ReliabilityBench的基准测试，引入了Reliability Surface $R(k, \\epsilon, \\lambda)$、Action Metamorphic Relations和Chaos Engineering Framework。我们在四个领域的1,280个episodes上，通过pass@k和Reliability Surface等指标验证了其有效性，揭示了扰动和故障对可靠性的显著影响。",
                    "summary_translation": "现有的针对使用工具的 LLM agents（大语言模型智能体）的基准主要报告单次运行成功率，而忽视了生产环境所需的可靠性属性。我们介绍了 \\textbf{ReliabilityBench}，这是一个从三个维度评估 agent（智能体）可靠性的基准：(i) 使用 $\\mathrm{pass}^k$（通过率）指标衡量的重复执行下的一致性，(ii) 在强度 $ε$ 下对语义等价任务扰动（perturbations）的鲁棒性，以及 (iii) 在强度 $λ$ 下受控工具/API 故障（failures）下的容错性。ReliabilityBench 提供了一个统一的可靠性曲面 $R(k,ε,λ)$，定义了 \\textit{action metamorphic relations}（动作蜕变关系），即通过终态等价性而非文本相似度来定义正确性，并引入了一个混沌工程风格的故障注入框架（包括超时、速率限制、部分响应、模式漂移）。我们在四个领域（日程安排、旅行、客户支持、电子商务）的 1,280 个回合中，对两个模型和两种 agent architectures（智能体架构）进行了评估。仅引入扰动（perturbations）就使成功率从 $ε=0$ 时的 96.9% 下降至 $ε=0.2$ 时的 88.1%。在消融实验中，速率限制是最具破坏性的故障。在综合压力下，ReAct 表现出比 Reflexion 更强的鲁棒性，且 Gemini 2.0 Flash 以低得多的成本实现了与 GPT-4o 相当的可靠性。ReliabilityBench 为评估 LLM agents 的生产就绪度提供了一个系统化的框架。",
                    "inspiration_trace": "基于论文《ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察：基准与现实的错位\n**逻辑起点：** 作者观察到 LLM Agent 正从实验室原型加速走向生产环境（如客服、自动化操作），但现有的评估体系存在严重的“脱节”。\n*   **现象：** 现有的主流基准（如 ToolBench, AgentBench）主要关注“单次运行成功率”。\n*   **矛盾：** 生产环境的核心诉求不是“能不能做”，而是“能不能稳定地做 1000 次”。在实验室里跑通一次和在真实网络环境下面对各种干扰跑通，是完全两回事。\n*   **初步结论：** 传统的 `pass@1` 指标过于乐观，掩盖了 Agent 在实际部署中的脆弱性。我们需要一个新的评估视角，即“生产就绪度”。\n\n### 2. 问题解构：什么是“可靠性”？\n为了填补上述差距，作者没有直接提出新测试集，而是先对“可靠性”这一概念进行了三维度的解构，试图定义生产环境到底包含哪些挑战：\n*   **维度一：一致性。** 受 τ-bench 启发，作者意识到 LLM 的随机性导致即使输入相同，多次运行结果也可能不同。生产环境要求的是“次次成功”，而非“偶尔成功”。\n*   **维度二：鲁棒性。** 真实用户不会按标准模板说话。他们会改写指令、插入无关信息、中途纠正。Agent 需要理解语义的等价性，而非死板的文本匹配。\n*   **维度三：容错性。** 真实的基础设施是不完美的。API 会超时、限流、返回残缺数据。Agent 需要具备“抗打击”和恢复能力。\n\n**思考演进：** 作者意识到这三个维度不是独立的，而是相互交织的。一个 Agent 可能很稳定（一致性高），但一遇到 API 报错就崩溃（容错性低）。因此，评估必须是一个多维度的综合体系。\n\n### 3. 方法论构建：跨学科思想的引入与适配\n有了定义，接下来的核心问题是：**如何量化这三个维度？** 作者在此处引入了两个关键的外部领域思想，并针对 Agent 场景进行了改造。\n\n*   **针对“鲁棒性”的解法：引入“变形测试”。**\n    *   *传统困境：* 对于 Agent 任务，输出文本可能千差万别（路径不同），用文本相似度判断对错很难。\n    *   *创新点：* 提出 **Action Metamorphic Relations（动作变形关系）**。核心逻辑是：只要输入的语义变化不改变任务目标，那么最终的**系统状态**必须一致。例如，指令从“订机票”变为“我要飞去...”，只要最终订票状态一致，就算通过。这解决了“非标准输入”的验证难题。\n\n*   **针对“容错性”的解法：引入“混沌工程”。**\n    *   *传统困境：* 静态数据集无法模拟动态故障。\n    *   *创新点：* 借鉴 Netflix 的 Chaos Monkey，提出 **Chaos Engineering for Agents**。不再等待故障发生，而是主动在工具调用层注入故障（如超时、限流、Schema 漂移）。这模拟了真实生产环境的“压力测试”。\n\n### 4. 统一框架：从点到面的升维\n有了具体的测试手段（变形关系、故障注入），作者需要一个数学框架来统一这些指标。\n*   **逻辑推演：** 既然可靠性有三个维度（k, ε, λ），那么评估结果就不应该是一个单一的分数，而应该是一个“函数”或“曲面”。\n*   **产出：** 提出了 **Reliability Surface $R(k, \\epsilon, \\lambda)$**。这个三维曲面能够直观地展示 Agent 在不同压力组合下的表现。例如，它能回答“当故障率增加时，Agent 对用户指令改写的敏感度是如何变化的？”这一复杂问题。\n\n### 5. 实证与反思：复杂度的悖论\n最后，作者通过实验验证假设，并得出了反直觉的结论，完善了整个思考闭环。\n*   **假设：** 更复杂的架构（如 Reflexion，带有自我反思机制）应该更可靠。\n*   **实验发现：** 在压力条件下，简单的 ReAct 架构反而表现更好。\n*   **逻辑修正：** 作者意识到，复杂的反思机制在遇到故障或干扰时，可能会引入更多的错误传播或无效循环，反而降低了稳定性。这进一步强化了论文的核心观点：**生产环境下的可靠性不等于模型能力的堆砌，而是对压力的稳健性。**\n\n---\n\n**总结：**\n作者的思考路径是从**“评估指标的失效”**出发，通过**“解构生产环境挑战”**定义了三个核心维度，进而**“跨界融合”**了软件测试的变形思想和 SRE 的混沌工程思想，最终构建了一个**“多维度的可靠性曲面”**框架，并揭示了**“简单架构在压力下的优势”**。整个过程体现了从现象观察、理论抽象到方法创新、实证修正的完整学术逻辑。"
                },
                {
                    "title": "LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
                    "arxiv_id": "2601.06111",
                    "authors": "Aayush Gupta, Farahan Raza Sheikh",
                    "summary": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis. We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于LLM的社会数字孪生框架，其中LLM充当个体智能体的认知引擎，用于模拟群体行为。这属于多智能体系统的研究范畴。尽管使用了COVID-19作为案例研究，但论文的核心贡献是通用的智能体框架架构，而非纯医疗应用，因此符合筛选条件。",
                    "summary2": "本文旨在解决预测人口对政策干预反应的挑战。针对政策响应预测中传统模型缺乏机制可解释性的问题，我们提出了一种基于LLM的Social Digital Twins框架，利用LLM作为Agent的认知引擎生成多维行为概率，并通过校准层映射到观测数据。在COVID-19大流行响应数据集上，通过RMSE指标验证了其有效性，相比Gradient Boosting基线，宏观平均预测误差降低了20.7%。",
                    "summary_translation": "预测人群如何响应政策干预是计算社会科学和公共政策领域的一个根本性挑战。传统方法依赖于聚合统计模型，这些模型虽然能够捕捉历史相关性，但缺乏机制可解释性，且难以应对新颖的政策场景。我们提出了一个构建社会数字孪生的通用框架——即虚拟人口副本，其中大语言模型作为个体智能体的认知引擎。每个智能体由人口统计学和心理特征学属性表征，接收政策信号并输出多维行为概率向量。一个校准层将聚合的智能体响应映射到可观测的群体层面指标，从而能够利用真实世界数据进行验证，并用于反事实政策分析。我们在大流行应对领域实例化了该框架，以拥有丰富观测数据的COVID-19作为案例研究。在保留测试期内，我们校准后的数字孪生在六个行为类别上，相较于梯度提升基线，在宏平均预测误差上实现了20.7%的改进。反事实实验展示了针对政策变化的单调且有界的响应，确立了行为的合理性。该框架是领域无关的：同样的架构适用于交通政策、经济干预、环境法规，或任何政策影响人群行为的场景。我们讨论了该框架对政策模拟的影响、方法的局限性，以及将基于大语言模型的数字孪生扩展到大流行应对之外的方向。",
                    "inspiration_trace": "基于论文《LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 第一阶段：宏观问题识别与现有方法的痛点分析\n**思考起点：** 政策制定者面临的核心困境——如何预测人群对未实施政策的反应？\n**逻辑推演：**\n1.  **观察现实需求：** 政府在推行碳税、封锁或福利改革前，需要预判公众行为（如：人们会减少开车吗？会遵守居家令吗？）。\n2.  **审视现有工具箱：**\n    *   **传统统计模型（如回归、时间序列）：** 擅长捕捉历史数据中的相关性，但本质是“黑箱”或“相关性归纳”。当遇到从未发生过的新颖政策（Novel Policy Scenarios）时，模型无法外推，且无法解释“为什么”会发生（缺乏机制可解释性）。\n    *   **传统基于主体的模型（ABM）：** 具备机制可解释性，能模拟个体决策。但其致命弱点在于“知识瓶颈”——必须由专家手动编写决策规则。如果人类自身都不完全理解某种复杂行为，就无法编写规则，导致模型难以泛化。\n3.  **核心矛盾提炼：** 我们需要一种既能像ABM那样具备**个体层面的机制推理能力**，又能像统计模型那样**易于构建且适应复杂场景**的新范式。\n\n### 第二阶段：技术机遇捕捉与核心假设提出\n**思考转折：** 大语言模型（LLM）的涌现能力是否提供了破局的关键？\n**逻辑推演：**\n1.  **观察LLM特性：** LLM不仅是在生成文本，它们在海量人类语料上训练，实际上习得了隐性的“人类推理模型”、“偏好”和“决策模式”（即“硅基采样” Silicon Sampling）。\n2.  **提出核心假设：** 如果LLM能模拟调查问卷回答、参与经济博弈，那么它本质上是一个通用的**人类行为模拟器**。\n3.  **范式转换构想：** 用LLM替换ABM中手工编写的规则引擎。\n    *   **输入：** 给LLM设定一个人设（年龄、职业、价值观）和一个政策背景。\n    *   **输出：** 让LLM基于其“常识”推理出该人设的行为概率。\n    *   **优势：** 无需针对每个领域硬编码规则，利用LLM的泛化能力处理新颖政策。\n\n### 第三阶段：框架构建——从“直觉”到“科学”\n**思考深化：** 仅靠LLM生成文本是不够的，如何将其转化为严谨的科学预测工具？\n**逻辑推演：**\n1.  **定义架构：** 提出“社会数字孪生”概念。这不仅是调用API，而是一个包含四个组件的系统：\n    *   **代理人口：** 必须构建符合真实人口统计学分布的合成人设，以保证群体的异质性。\n    *   **LLM认知引擎：** 负责将“人设+政策”映射为“行为概率向量”。\n2.  **解决“幻觉”与“对齐”问题（关键创新点）：**\n    *   **观察：** LLM输出的概率（如0.7）往往是主观的，不能直接对应现实世界的宏观指标（如客流量百分比）。\n    *   **引入校准层：** 必须建立一个数学映射层 $f(p; \\theta)$，将LLM输出的原始概率校准为可观测的现实指标。这相当于用历史数据去“锚定”LLM的直觉，使其具备预测精度。\n3.  **确立验证逻辑：** 强调严格的时空分割，防止信息泄露，确保模型是在真正“预测”而非“记忆”。\n\n### 第四阶段：实证策略与案例选择\n**思考落地：** 如何证明这个框架真的有效？\n**逻辑推演：**\n1.  **选择测试场：** 为什么选COVID-19？\n    *   **数据丰富度：** 有高频的谷歌移动数据和牛津政策追踪数据。\n    *   **自然实验属性：** 疫情期间政策变化剧烈且频繁，是测试模型应对“新颖/极端场景”的完美压力测试。\n    *   **行为多维性：** 涵盖工作、休闲、购物等多种行为，能全面测试模型。\n2.  **设定对比基线：** 与梯度提升树（GBM）等强统计模型对比。目的是验证：在处理“语义理解”和“决策逻辑”时，LLM是否优于纯数据驱动的统计模型。\n\n### 第五阶段：结果反思与定位修正\n**思考升华：** 实验结果揭示了什么？该方法论的边界在哪里？\n**逻辑推演：**\n1.  **结果分析：**\n    *   **成功之处：** 在工作场所、零售等“决策驱动型”行为上，LLM大幅超越统计模型。这证明了LLM理解政策语义（如“封锁”意味着“居家”）的能力。\n    *   **失败之处：** 在居住等“惯性驱动型”行为上，LLM不如统计模型。这说明LLM缺乏对日常习惯和惯性的记忆。\n2.  **方法论定位：**\n    *   明确该框架不是要取代所有统计模型，而是填补**“政策语义理解”与“机制推理”**的空白。\n    *   强调其**领域无关性**：COVID-19只是验证数据集，同样的架构可以无缝迁移到交通、经济、环保等领域，因为LLM已经学习了跨领域的人类行为逻辑。\n\n---\n\n**总结：**\n作者的思考路径是从**政策预测的现实困境**出发，敏锐地捕捉到**LLM作为通用认知引擎**的潜力，通过引入**校准层**解决了从“文本生成”到“科学预测”的跨越，最后通过**疫情案例**验证了其在处理复杂决策行为上的优越性，从而确立了一套通用的社会数字孪生方法论。"
                },
                {
                    "title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning",
                    "arxiv_id": "2601.06098",
                    "authors": "Nicholas X. Wang, Neel V. Parpia, Aaryan D. Parikh, Aggelos K. Katsaggelos",
                    "summary": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“多智能体LLM架构”，其中包含专门的智能体负责图寻路、推理、验证和输出等特定任务，这些智能体通过协作来减少幻觉并生成高质量问题。这符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决LLM在自动问题生成中的幻觉问题，以支持直觉学习。针对STEM教育场景，我们提出了一种结合Causal Graph引导的Chain-of-Thought推理与Multi-agent LLM架构的框架，并在Stellar在线学习平台上通过Flesch-Kincaid Grade Level、Key Points和Solution Quality等指标验证了其有效性，结果显示质量提升高达70%。",
                    "summary_translation": "直觉学习对于培养深层概念理解至关重要，尤其是在 STEM（科学、技术、工程和数学）教育领域，学生往往难以掌握抽象且相互关联的概念。自动问题生成已成为实现个性化学习和自适应学习的有效策略。然而，其有效性受到大语言模型中“幻觉”现象的制约，这可能导致生成事实错误、语义模糊或教学不一致的问题。为解决这一问题，我们提出了一种新颖的框架，该框架结合了因果图引导的思维链推理与多智能体 LLM 架构。该方法确保生成准确、有意义且符合课程要求的问题。因果图提供了领域知识的显式表示，而 CoT 推理则促进了对相关概念的结构化、逐步遍历。专用的 LLM 智能体被分配了图路径查找、推理、验证和输出等特定任务，所有任务均在领域约束范围内执行。一种在概念阶段和输出阶段实施的双重验证机制，极大地减少了幻觉现象。实验结果表明，与基准方法相比，该方法在质量上提升了高达 70%，并在主观评估中取得了极为理想的结果。",
                    "inspiration_trace": "基于论文《Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观愿景与教育痛点\n**（从“直觉学习”的理想出发）**\n\n1.  **观察现状**：在STEM教育中，传统的死记硬背已不足以应对抽象概念的学习，教育界正转向“直觉学习”——即通过自然认知过程、探索和逐步推理来建立深层理解。\n2.  **技术机遇**：生成式AI（特别是LLM）为实现个性化、自适应的“直觉学习”提供了可能，其中“自动问题生成（AQG）”是核心抓手，它能实时提供符合学生水平的挑战。\n3.  **核心矛盾**：虽然LLM具备强大的生成能力，但在教育场景下存在致命缺陷——**“幻觉”**。LLM会生成事实错误、逻辑不清或不符合教学大纲的问题，这会误导学生，破坏学习体验，违背了直觉学习追求“概念清晰”的初衷。\n\n### 第二阶段：问题诊断与归因\n**（深入分析LLM在教育场景失效的本质）**\n\n1.  **归因分析**：为什么LLM会产生幻觉？因为LLM本质上是基于概率预测的文本生成器，缺乏对领域知识**显性结构**的约束。它不知道概念A必须是概念B的前提（例如：不知道“牛顿第二定律”是推导“能量守恒”的基础）。\n2.  **需求明确**：要解决这一问题，不能仅靠微调模型，必须引入一种机制，能够：\n    *   显式表示知识的依赖关系（结构）。\n    *   强制生成过程遵循逻辑步骤（推理）。\n\n### 第三阶段：理论假设与融合\n**（提出“因果图 + 思维链”的结合点）**\n\n1.  **引入“因果图”**：作者意识到，因果图能完美映射学科中的概念依赖（如：力 $\\rightarrow$ 加速度 $\\rightarrow$ 速度）。它提供了**“是什么”**和**“什么顺序”**的知识骨架，解决了结构缺失问题。\n2.  **引入“思维链”**：CoT推理能模拟人类解决问题的逐步思考过程。它提供了**“如何”**连接这些概念的逻辑流。\n3.  **核心假设**：如果将因果图作为“导航地图”，将CoT作为“行驶路径”，让LLM沿着因果图的路径进行CoT推理，就能生成既符合学科逻辑又具备教学深度的题目。\n\n### 第四阶段：方法论构建与抗噪设计\n**（从理论假设落地为可执行的系统架构）**\n\n1.  **架构设计：多智能体协作**：单一的Prompt难以同时处理图遍历、逻辑推理和文本生成。作者受软件工程启发，决定采用**多智能体架构**，将复杂任务拆解：\n    *   *寻路智能体*：负责在因果图中找到正确的概念路径。\n    *   *推理智能体*：负责基于路径生成CoT。\n    *   *生成与输出智能体*：负责最终题目的产出。\n2.  **抗噪机制：双重验证**：为了专门针对第二阶段发现的“幻觉”问题，作者设计了**双重验证**机制：\n    *   *概念层验证*：在生成前，检查寻路智能体找到的路径是否逻辑自洽。\n    *   *输出层验证*：在生成后，检查最终题目是否准确、无歧义。\n    *   *逻辑闭环*：通过这两道“安检”，确保输出严格受限于因果图的结构约束。\n\n### 第五阶段：验证与价值确认\n**（通过实验反馈闭环验证思想）**\n\n1.  **评估维度设定**：为了证明该方法优于普通LLM（如ChatGPT），作者设定了不仅关注“可读性”，更关注“关键点覆盖”和“解题步骤质量”的指标。这直接呼应了第一阶段“直觉学习”对深度理解的要求。\n2.  **结果反馈**：实验显示，该方法在题目深度和逻辑性上显著优于基线模型（提升70%），且用户反馈题目“自然”、“符合推理习惯”。\n3.  **结论升华**：这证明了**结构化知识（因果图）与结构化推理（CoT）的结合**，是解决教育领域LLM幻觉问题的有效范式。\n\n---\n\n**总结：作者的思考路径是从教育理念（直觉学习）出发，遭遇技术瓶颈（LLM幻觉），通过引入外部结构（因果图）和内部逻辑（CoT）进行约束，最终通过工程化手段（多智能体+双重验证）将理论落地，从而实现了高质量的教育内容生成。**"
                },
                {
                    "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
                    "arxiv_id": "2601.07122",
                    "authors": "Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu",
                    "summary": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个分层多智能体框架，其中上层LLM智能体明确使用了ReAct规划、长短期记忆和工具集成（符合单智能体标准），并与下层RL智能体进行协作（符合多智能体标准）。尽管应用于网络防御领域，但论文的核心贡献在于LLM智能体的架构设计（LLM与RL的结合），而非单纯的应用或AI安全对齐研究。",
                    "summary2": "本文旨在解决云网络防御在动态环境下的适应性和鲁棒性问题。针对云网络结构、规模及攻击策略动态变化的场景，我们提出了一种名为CyberOps-Bots的分层多智能体强化学习框架，该框架结合了LLM的高层战术规划与底层RL智能体的原子动作执行。在AWS企业云数据集和Yawning Titan仿真环境中，通过网络可用性和Jumpstart性能等指标验证了其有效性，实现了无需重训练的高效自适应防御。",
                    "summary_translation": "虽然虚拟化和资源池化为云网络赋予了结构灵活性和弹性可扩展性，但它们不可避免地扩大了攻击面，并挑战了网络弹性。基于强化学习的防御策略已被开发出来，用于在对抗条件下优化资源部署和隔离策略，旨在通过维持和恢复网络可用性来增强系统弹性。然而，现有方法缺乏鲁棒性，因为它们需要重新训练以适应网络结构、节点规模、攻击策略和攻击强度的动态变化。此外，缺乏人在回路支持限制了可解释性和灵活性。为了解决这些局限性，我们提出了 CyberOps-Bots，这是一个由大语言模型赋能的分层多智能体强化学习框架。受 MITRE ATT&CK 的战术-技术模型启发，CyberOps-Bots 具有双层架构：(1) 上层 LLM 智能体包含四个模块——ReAct 规划、基于 IPDRR 的感知、长短期记忆以及动作/工具集成——负责执行全局感知、人类意图识别和战术规划；(2) 下层 RL 智能体通过异构分离预训练开发，在局部网络区域内执行原子防御动作。这种协同作用在确保可靠的 RL 执行的同时，保留了 LLM 的适应性和可解释性。在真实云数据集上的实验表明，与最先进的算法相比，CyberOps-Bots 在不重新训练的情况下切换场景时，维持的网络可用性高出 68.5%，并实现了 34.7% 的启动性能增益。据我们所知，这是首个建立具有 HITL 支持的鲁棒 LLM-RL 框架用于云防御的研究。我们将向社区发布我们的框架，以促进云网络中鲁棒且自主防御的发展。",
                    "inspiration_trace": "基于论文《Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题与现状悖论\n**1. 观察现象：云原生环境的“双刃剑”特性**\n作者首先观察到云网络的核心矛盾：虚拟化和弹性伸缩赋予了系统极大的灵活性，但同时也导致了网络拓扑、节点规模和攻击面的高度动态变化。\n*   **思考：** 传统的静态防御策略无法应对这种瞬息万变的环境。\n\n**2. 现有方案的尝试与局限**\n学术界和工业界开始尝试使用强化学习（RL）来自动化防御决策，因为RL擅长通过交互学习最优策略。\n*   **深入分析：** 作者发现现有的RL方法存在致命的“脆弱性”。当网络结构（A1）、规模（A2）、攻击策略（A3）或攻击强度（A4）发生变化时，RL模型往往失效，必须重新训练。\n*   **根本原因定位：**\n    *   **表征僵化：** RL依赖固定维度的状态向量（如邻接矩阵），一旦网络规模或拓扑改变，输入层结构就不匹配了。\n    *   **泛化缺失：** RL是基于模式匹配的，而非语义理解。面对未见过的攻击阶段或并发攻击，它无法举一反三。\n    *   **人机断层：** 纯算法模型缺乏可解释性，无法支持安全专家在紧急情况下进行有效干预（HITL）。\n\n---\n\n### 第二阶段：范式转换与假设提出\n**3. 引入新视角：从“数值计算”转向“语义推理”**\n为了解决泛化性和人机交互问题，作者将目光投向了大语言模型（LLM）。\n*   **假设：** LLM具备强大的语义理解、逻辑推理和零样本泛化能力，能够理解复杂的网络态势和人类指令，从而弥补RL在高层认知上的不足。\n\n**4. 识别新技术的短板**\n然而，作者敏锐地意识到LLM并非万能：\n*   **短板：** LLM在精确的数值计算（如计算最短路径）和生成低层级的精确控制指令（如具体的流表修改命令）方面存在“幻觉”和不稳定性。\n*   **结论：** 单纯依靠LLM无法保证防御执行的可靠性。\n\n---\n\n### 第三阶段：方法论融合与架构设计\n**5. 核心思想：分层协同的“战术-技术”解耦**\n受MITRE ATT&CK框架（战术与技术的分层）启发，作者提出了一个融合假设：**将“大脑”（LLM）与“手脚”（RL）结合**。\n*   **逻辑推演：**\n    *   **上层（LLM）：** 负责宏观感知、战术规划和意图理解。利用自然语言处理能力，将动态的网络状态抽象为文本，从而解耦对特定网络结构的依赖。\n    *   **下层（RL）：** 负责微观执行。利用RL在特定动作空间内的精确控制能力，执行具体的原子防御操作。\n\n**6. 解决动态适应性的具体机制设计**\n针对前述的四个动态挑战（A1-A4），作者在架构中嵌入了对应的解决方案：\n\n*   **针对A1（结构变化）与A2（规模变化）：自然语言状态抽象**\n    *   *思考：* 如何让模型不关心网络具体有多少个节点？\n    *   *方案：* 设计一个感知模块，将高维、结构化的网络状态转化为自然语言描述。因为LLM处理文本不受长度限制，这天然解决了状态空间爆炸和维度不匹配的问题，实现了“零样本”适应新拓扑。\n\n*   **针对A3（攻击策略变化）：异构分离预训练**\n    *   *思考：* 如何应对不同类型的攻击（如DDoS vs 渗透）？\n    *   *方案：* 不训练一个全能的RL智能体，而是训练一组功能单一的“专家”RL智能体（如隔离专家、补丁专家）。LLM作为指挥官，根据当前的攻击语义，动态调度不同的专家组合。这比单一模型更具灵活性。\n\n*   **针对A4（攻击强度/并发性）：长短时记忆机制**\n    *   *思考：* 面对多阶段、并发的攻击链，如何保持连贯性？\n    *   *方案：* 赋予LLM记忆模块（LTM/STM）。通过存储和检索历史攻击链，LLM能够识别攻击意图的演变，从而进行长期的防御规划，而不是短视的反应。\n\n---\n\n### 第四阶段：增强可靠性与人机协同\n**7. 引入ReAct范式与HITL支持**\n为了解决LLM的“幻觉”问题并增强信任度：\n*   **ReAct（推理+行动）：** 强制LLM在输出行动前先生成推理链。这不仅提高了决策的准确性，还提供了天然的可解释性日志。\n*   **人在回路（HITL）：** 允许安全专家通过自然语言直接干预LLM的规划层。这使得系统不仅是自动化的，更是可审计、可修正的。\n\n---\n\n### 第五阶段：逻辑闭环与验证\n**8. 最终产出：CyberOps-Bots框架**\n作者将上述思考整合为一个三层架构：环境层（模拟动态对抗）、LLM层（语义规划）、RL层（原子执行）。\n\n**9. 验证逻辑：**\n*   **实验设计：** 不再测试静态环境，而是专门设计场景动态切换（如从30节点跳到450节点，攻击策略从侦察变为渗透）。\n*   **核心指标：** 关注“Jumpstart性能”（即在新环境下无需重新训练的初始表现）和“网络可用性”。\n*   **结论验证：** 实验证明，这种分层架构确实在无需重训的情况下，适应了A1-A4的所有动态变化，且性能优于传统RL算法。\n\n---\n\n**总结：**\n作者的思考路径是从**“RL在动态环境下的失效”**这一痛点出发，通过**引入LLM的语义泛化能力**作为破局点，进而通过**分层架构（LLM规划+RL执行）**规避了LLM的精确性短板，最终利用**自然语言抽象和异构智能体调度**实现了对云网络动态特性的鲁棒适应。"
                },
                {
                    "title": "Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation",
                    "arxiv_id": "2601.06877",
                    "authors": "Donghuo Zeng, Roberto Legaspi, Kazushi Ikeda",
                    "summary": "Effective persuasive dialogue agents adapt their strategies to individual users, accounting for the evolution of their psychological states and intentions throughout conversations. We present a personality-aware reinforcement learning approach comprising three main modules: (1) a Strategy-Oriented Interaction Framework, which serves as an agenda-based strategy controller that selects strategy-level actions and generate responses via Maximal Marginal Relevance (MMR) retrieval to ensure contextual relevance, diversity, and scalable data generation; (2) Personality-Aware User Representation Learning, which produces an 81-dimensional mixed-type embedding predicted at each turn from recent exchanges and appended to the reinforcement learning state; and (3) a Dueling Double DQN (D3QN) model and Reward Prediction, in which the policy is conditioned on dialogue history and turn-level personality estimates and trained using a composite reward incorporating agreement intent, donation amount, and changeof-mind penalties. We use an agenda-based LLM simulation pipeline to generate diverse interactions, from which personality estimation is inferred from the generated utterances. Experiments on the PersuasionForGood (P4G) dataset augmented with simulated dialogues reveal three main findings: (i) turn-level personality conditioning improves policy adaptability and cumulative persuasion rewards; (ii) LLM-driven simulation enhances generalization to unseen user behaviors; and (iii) incorporating a change-of-mind penalty reduces post-agreement retractions while slightly improving donation outcomes. These results demonstrate that structured interaction, dynamic personality estimation, and behaviorally informed rewards together yield more effective persuasive policies.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于强化学习的说服对话智能体，包含策略规划（议程式策略控制器）、记忆与状态表征（个性化用户表征学习）以及利用LLM进行环境模拟，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决说服性对话中用户心理状态动态变化难以捕捉的问题。针对多轮交互场景，我们提出了一种Personality-Aware Reinforcement Learning方法，集成Strategy-Oriented Interaction Framework、动态Personality-Aware User Representation及D3QN模型。我们在PersuasionForGood (P4G)数据集及LLM仿真环境中，通过累积说服奖励等指标验证了其有效性。",
                    "summary_translation": "高效的 persuasive dialogue agents（说服对话代理）能够针对个体用户调整策略，并考量其在对话过程中心理状态和意图的演变。我们提出了一种 personality-aware reinforcement learning（人格感知强化学习）方法，该方法包含三个主要模块：(1) Strategy-Oriented Interaction Framework（面向策略的交互框架），作为一个基于议程的策略控制器，用于选择策略级动作，并通过 Maximal Marginal Relevance (MMR)（最大边际相关性）检索生成响应，以确保上下文相关性、多样性及可扩展的数据生成；(2) Personality-Aware User Representation Learning（人格感知用户表征学习），生成一个81维的混合类型嵌入，该嵌入在每一轮对话中根据最近的交流进行预测，并附加到强化学习状态中；(3) Dueling Double DQN (D3QN)（决斗双深度Q网络）模型和 Reward Prediction（奖励预测），其中策略以对话历史和轮级人格估计为条件，并利用包含同意意图、捐赠金额和 change-of-mind penalty（改变主意惩罚）的复合奖励进行训练。我们采用基于议程的 LLM（大语言模型）模拟流水线生成多样化的交互，并据此从生成的言语中推断人格估计。在通过模拟对话增强的 PersuasionForGood (P4G) 数据集上进行的实验揭示了三个主要发现：(i) turn-level personality conditioning（轮级人格条件化）提高了策略适应性和累积说服奖励；(ii) LLM-driven simulation（大语言模型驱动的模拟）增强了对未见用户行为的泛化能力；(iii) 引入 change-of-mind penalty（改变主意惩罚）减少了达成协议后的撤回行为，同时略微改善了捐赠结果。这些结果表明，结构化的交互、动态的人格估计以及基于行为的奖励共同产生了更有效的 persuasive policies（说服策略）。",
                    "inspiration_trace": "基于论文《Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观问题：现有劝说系统的“行为落地”困境\n**起点：** 作者首先关注到，尽管大语言模型（LLM）在对话流畅度上表现优异，但在**劝说**这一特定任务中，它们往往缺乏稳定的“行为基础”。\n*   **核心矛盾：** 劝说不仅仅是生成通顺的文本，而是一个长期的、旨在改变用户信念和行为的策略过程。现有的通用LLM缺乏对个体用户心理状态的深层建模和长线策略规划能力。\n\n### 2. 观察与痛点分析：从静态到动态，从匮乏到仿真\n在确立宏观问题后，作者深入剖析了现有研究的两个主要局限性：\n\n*   **观察一：用户画像的“静态性”缺陷。**\n    *   **现象：** 传统的基于强化学习（RL）的对话系统通常假设用户具有固定的“静态人格”。\n    *   **推论：** 真实的劝说过程是动态博弈，用户的心理状态和意图会随着对话的进行而实时演变。如果仅依赖静态画像，策略选择将无法适应当前的对话情境，导致次优的劝说效果。\n\n*   **观察二：训练数据的“稀缺性”与模拟器的“机械性”。**\n    *   **现象：** 高质量的人工标注劝说数据（如P4G数据集）非常昂贵且覆盖面有限。而传统的基于规则或模板的用户模拟器过于死板，无法模拟出真实人类复杂、微妙的反应。\n    *   **推论：** 训练一个鲁棒的RL策略需要大量、多样化的交互数据。虽然LLM具备模拟人类的潜力，但如果缺乏结构化约束，容易产生幻觉或行为漂移，难以保证训练数据的可靠性。\n\n### 3. 假设形成：动态感知与结构化仿真的协同\n基于上述痛点，作者提出了三个核心假设，构成了本文的方法论基石：\n\n*   **假设一（动态性）：** 如果将用户人格建模从“静态预设”转变为“逐轮预测”，并将这种动态特征作为RL状态的一部分，策略的适应性将显著提升。\n*   **假设二（可控性）：** 如果利用LLM作为模拟器，但通过“议程”机制进行结构化约束，就能在保证行为多样性的同时，生成符合逻辑且高质量的训练轨迹。\n*   **假设三（稳定性）：** 劝说的成功不仅在于达成口头协议，更在于防止用户事后反悔。如果在奖励函数中引入“反悔惩罚”，可以引导策略产生更稳固的承诺。\n\n### 4. 方法论构建：模块化架构的设计\n为了验证上述假设，作者设计了一个三层递进的架构：\n\n*   **第一步：构建“策略导向交互框架”（解决数据与控制问题）。**\n    *   **思路：** 为了解决LLM模拟的不稳定性，作者没有直接让LLM自由生成，而是设计了一个基于“议程”的控制器。\n    *   **逻辑：** 系统先选择高层策略（如“逻辑诉求”），再通过检索（MMR算法）生成具体回复。对于用户模拟，利用LLM但强制其遵循特定的行为模式。这样既利用了LLM的生成能力，又保证了数据的结构化和多样性。\n\n*   **第二步：实现“人格感知的用户表征”（解决动态建模问题）。**\n    *   **思路：** 将用户的混合型特征（25个连续变量 + 7个类别变量）编码为一个紧凑的81维向量。\n    *   **逻辑：** 关键在于“逐轮预测”。作者训练了一个预测器，在每一轮对话中根据最近的交互实时更新这个81维向量，并将其拼接到RL的状态输入中。这使得Agent能“看到”用户当前的心理轨迹。\n\n*   **第三步：设计“复合奖励与D3QN优化”（解决策略学习问题）。**\n    *   **思路：** 使用Dueling Double DQN（D3QN）来处理状态-价值估计。\n    *   **逻辑：** 在奖励函数设计上，除了常规的“同意意图”和“捐赠金额”，创新性地加入了“反悔惩罚”。这直接对应了假设三，迫使Agent不仅要说服用户，还要巩固用户的承诺，减少“口头答应但事后反悔”的情况。\n\n### 5. 逻辑闭环：实验验证与发现\n最后，作者通过实验验证了这一思考链条的有效性：\n*   **验证动态性：** 实验表明，包含逐轮人格特征的策略确实获得了更高的累积奖励。\n*   **验证仿真：** 基于LLM的仿真数据增强了模型对未见用户行为的泛化能力。\n*   **验证稳定性：** 引入反悔惩罚后，用户的反悔率确实下降，且捐赠结果略有提升。\n\n**总结：**\n作者的思考路径是从**“通用LLM缺乏策略性”**这一宏观洞察出发，通过**“动态人格”**和**“结构化仿真”**两个切入点，将心理学建模与强化学习紧密结合，最终构建了一个既能适应实时心理变化，又能产生稳定劝说效果的闭环系统。"
                },
                {
                    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
                    "arxiv_id": "2601.06789",
                    "authors": "Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang",
                    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究代码智能体，核心贡献是MemGovern框架，旨在通过将GitHub数据转化为可操作的经验记忆来增强智能体能力，属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决Code Agents因“封闭世界”限制而无法有效利用GitHub历史经验的问题。针对GitHub上非结构化且碎片化的Issue和PR数据，我们提出了MemGovern框架，通过Experience Governance将原始数据转化为结构化的Experience Cards，并引入Agentic Experience Search机制实现逻辑驱动的检索。在SWE-bench Verified上通过Resolution Rate验证了其有效性，平均提升了4.65%。",
                    "summary_translation": "尽管 autonomous software engineering (SWE) agents（自主软件工程智能体）正在重塑编程范式，但目前它们仍受限于“closed-world”限制：即试图从零开始修复 bug 或仅依赖 local context（局部上下文），而忽视了 GitHub 等平台上蕴藏的丰富历史人类经验。获取这种 open-world experience（开放世界经验）的过程，受到现实世界中 issue-tracking data（问题跟踪数据）非结构化和碎片化特性的阻碍。在本文中，我们介绍了 MemGovern，这是一个旨在对原始 GitHub 数据进行治理，并将其转化为智能体可用的 actionable experiential memory（可操作经验记忆）的 framework（框架）。MemGovern 采用 experience governance（经验治理）将人类经验转化为 agent-friendly（智能体友好）的 experience cards（经验卡片），并引入了一种 agentic experience search strategy（智能体经验搜索策略），从而实现了对 human expertise（人类专业知识）的 logic-driven retrieval（逻辑驱动检索）。通过生成 135K 个治理后的 experience cards（经验卡片），MemGovern 实现了显著的 performance boost（性能提升），将 SWE-bench Verified 上的 resolution rates（解决率）提高了 4.65%。作为一种 plug-in approach（插件式方法），MemGovern 为构建 agent-friendly memory infrastructure（智能体友好的记忆基础设施）提供了有效的解决方案。",
                    "inspiration_trace": "基于对论文《MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察与问题定义：从“封闭”到“开放”的鸿沟\n\n*   **现象观察**：当前的自主软件工程代理在解决代码问题时，往往表现得像是一个“孤胆英雄”。它们倾向于从零开始尝试修复Bug，或者仅依赖当前代码库的局部上下文。\n*   **人类对标**：在现实世界的软件工程实践中，资深开发者很少从零开始。面对复杂问题，他们习惯于在GitHub等协作平台上搜索历史记录，借鉴前人解决类似问题的专家推理和修复模式。\n*   **核心假设**：如果代码代理能够像人类一样，利用GitHub上海量的“开放世界”历史经验，其推理深度和修复准确性应该能得到显著提升。\n*   **现实瓶颈**：虽然GitHub蕴含了巨大的知识宝库，但直接将其用于Agent存在巨大的“语义鸿沟”。原始的Issue和PR讨论充满了社交闲聊、非标准术语和碎片化信息，噪声极大且高度异构。直接检索这些数据会导致“记忆污染”，难以实现跨仓库的知识迁移。\n\n### 2. 思考转折：从“数据检索”到“数据治理”\n\n*   **思维突破**：既然原始数据不可用，那么问题的核心就不在于“如何更好地检索”，而在于“如何将混乱的人类经验转化为Agent友好的知识”。\n*   **治理理念**：作者意识到必须引入一个中间层，即“经验治理”。这不仅仅是清洗数据，而是要进行知识蒸馏。\n*   **结构化重构**：为了解决跨仓库的异构性问题，作者提出将非结构化的讨论重构为标准化的“经验卡片”。\n*   **关键洞察（解耦）**：为了实现有效的知识迁移，必须将“检索信号”与“修复逻辑”解耦。\n    *   **索引层**：提取通用的故障症状（如异常类型、错误签名），用于跨仓库的广泛匹配。\n    *   **解析层**：封装可复用的修复逻辑（如根因分析、修复策略），用于具体的代码生成。\n    *   *逻辑推演*：这种分层设计使得Agent能够基于症状找到相似案例，再根据抽象的修复策略应用到当前的具体上下文中，从而实现了从“形似”到“神似”的跨越。\n\n### 3. 交互设计：从“静态注入”到“智能搜索”\n\n*   **对现有方法的批判**：传统的检索增强生成（RAG）通常采用“一次性检索+上下文注入”的模式。这就像把整本教科书扔给学生，不仅消耗上下文窗口，还容易引入噪声，干扰Agent的推理。\n*   **人类行为模拟**：人类查阅资料时是动态的——先搜索目录，筛选出相关章节，再深入阅读细节。\n*   **机制创新**：作者提出了“Agent式经验搜索”。\n    *   **双原语接口**：设计了“搜索”和“浏览”两个工具。搜索用于广度发现（基于索引层），浏览用于深度挖掘（基于解析层）。\n    *   **渐进式推理**：允许Agent根据当前解决问题的状态，自主决定是扩大搜索范围还是深入某个具体案例。这种机制让Agent具备了主动筛选和验证信息的能力，避免了被动接受噪声。\n\n### 4. 逻辑闭环与验证：质量即性能\n\n*   **质量控制的必要性**：考虑到自动化提取可能产生幻觉或遗漏，作者引入了基于检查表的质量控制机制，并设计了“优化循环”，确保进入记忆库的每张卡片都是经过验证的高质量知识。\n*   **最终假设验证**：如果上述逻辑成立，那么经过治理的经验配合渐进式搜索，应该能显著优于直接使用原始数据或传统RAG方法。\n*   **实验反馈**：通过在SWE-bench上的实验，证实了“治理后的经验”比“原始数据”更有效，且“Agent式搜索”比“静态RAG”更具鲁棒性。这反向验证了作者最初的假设：**高质量的结构化记忆 + 类人的搜索策略 = 更强的代码Agent**。\n\n### 总结\n\n作者的思考路径遵循了 **“发现人类行为优势 -> 识别数据应用瓶颈 -> 引入治理机制进行结构化转化 -> 模拟人类认知过程设计交互 -> 实验验证逻辑闭环”** 的完整链条。其核心贡献在于将“数据治理”引入了Agent的记忆构建过程，并证明了结构化的知识表示比单纯的数据量更重要。"
                },
                {
                    "title": "CEDAR: Context Engineering for Agentic Data Science",
                    "arxiv_id": "2601.06606",
                    "authors": "Rishiraj Saha Roy, Chris Hinze, Luzian Hahn, Fabian Kuech",
                    "summary": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个用于自动化数据科学的智能体框架（CEDAR），涉及规划（生成计划块）、工具使用（函数调用和代码生成）、记忆与上下文管理（智能历史渲染），符合单智能体及多智能体的研究范围。",
                    "summary2": "本文旨在解决利用 LLMs 自动化数据科学任务时面临的上下文限制、数据隐私及任务复杂性等问题。针对 Kaggle 竞赛等数据科学场景，我们提出了一种名为 CEDAR 的代理系统，采用结构化提示、多代理编排及智能历史渲染等上下文工程技术。我们在 canonical Kaggle challenges 上验证了其有效性，展示了其自动化解决初级数据科学任务的能力。",
                    "summary_translation": "我们展示了CEDAR，这是一个利用agentic setup（智能体架构）来自动化数据科学（DS）任务的应用程序。利用LLMs（大语言模型）解决数据科学问题是一个尚待深入探索但具有巨大市场价值的领域。其面临的挑战是多方面的，包括任务复杂性、数据规模、计算限制以及上下文限制。我们表明，通过有效的context engineering（上下文工程）可以缓解这些挑战。我们首先通过数据科学特定的输入字段为初始prompt（提示词）引入结构，这些字段作为智能体系统的指令。随后，解决方案被呈现为由独立的LLM agents（大语言模型智能体）生成的、交替的计划和代码块的枚举序列，从而在工作流的任何步骤都为上下文提供可读的结构。用于生成这些中间文本及相应Python代码的function calls（函数调用），确保数据保留在本地，仅有聚合统计信息及相关指令被注入到LLMs的prompt（提示词）中。我们通过迭代代码生成和智能历史渲染引入了容错机制和上下文管理。最后，我们利用典型的Kaggle挑战赛验证了该智能体数据科学家的可行性。",
                    "inspiration_trace": "基于论文《CEDAR: Context Engineering for Agentic Data Science》，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观行业痛点到微观技术实现的思维演进。\n\n---\n\n### 1. 宏观观察与问题定义\n**思考起点：** 数据科学（DS）工作流高度依赖人工，繁琐且重复，而现代大语言模型（LLM）具备自动化这些任务的潜力。然而，现有的通用LLM工具（如ChatGPT Advanced Data Analysis）在处理真实DS任务时表现不佳。\n\n**核心矛盾识别：**\n作者观察到，虽然LLM能力强大，但在DS领域存在“五大鸿沟”：\n1.  **任务复杂性：** 真实DS项目无法通过单次Prompt解决，需要多步推理。\n2.  **计算能力：** LLM的数学计算能力不可靠。\n3.  **数据规模：** 企业级数据往往超过上传限制。\n4.  **隐私安全：** 敏感数据无法上传至云端模型。\n5.  **上下文混乱：** 随着步骤增加，指令、代码、数据、错误信息混杂，导致上下文超出长度限制且逻辑不可读。\n\n### 2. 核心假设提出\n**思维转折：** 作者意识到，单纯提升模型智商并不能解决上述所有问题。真正的瓶颈在于**“上下文工程”**。\n\n**假设：** 如果能设计一套机制，在LLM推理过程中动态地优化、结构化并压缩上下文，同时确保数据不离开本地环境，就能构建一个高效、透明且安全的自动化数据科学系统。\n\n### 3. 方法论的逻辑演进\n基于上述假设，作者开始构建CEDAR系统，其逻辑演进遵循以下步骤：\n\n#### 第一阶段：输入的结构化（解决“指令不清”）\n*   **思考：** 用户往往不知道如何写完美的Prompt。DS任务有固定的元数据（如数据位置、评价指标、任务描述）。\n*   **决策：** 放弃自由文本输入，设计**结构化表单**。强制用户填写任务描述、数据路径、指标等字段。\n*   **逻辑：** 将非结构化的自然语言需求转化为结构化的机器指令，作为系统的初始上下文。\n\n#### 第二阶段：输出的结构化与可读性（解决“过程黑箱”）\n*   **思考：** 直接让模型输出最终结果（如一个准确率数值）既不可信也不可复用。人类数据科学家的工作方式是“计划+代码”交替进行（类似Jupyter Notebook）。\n*   **决策：** 强制模型输出**交错的文本和代码块**。每一步包含“自然语言计划”和“可执行代码”。\n*   **逻辑：** 模拟人类思维过程，让工作流透明化，便于人类审查和纠错。\n\n#### 第三阶段：智能体分工与路由（解决“任务复杂性”）\n*   **思考：** 让一个LLM同时负责规划、写代码、写解释、判断是否结束，负担太重，容易出错。\n*   **决策：** 引入**多智能体架构**。\n    *   **Orchestrator（编排器）：** 只负责决策，即“下一步该写文本还是写代码，或者结束”。\n    *   **Text Agent：** 专门负责写解释和分析。\n    *   **Code Agent：** 专门负责写Python代码。\n*   **逻辑：** 职责分离。利用**函数调用**和**结构化输出**（JSON Schema）约束编排器的行为，防止其产生幻觉，确保指令准确传递给子代理。\n\n#### 第四阶段：本地化执行与容错（解决“计算与隐私”）\n*   **思考：** LLM不擅长数学，且数据不能上传。\n*   **决策：** **代码即工具**。LLM只生成代码，代码在本地Docker容器中执行。\n*   **逻辑：**\n    *   **数据隐私：** 数据永远不离开本地，只有统计摘要进入Prompt。\n    *   **计算准确性：** 用Python解释器替代LLM进行数学运算。\n    *   **容错机制：** 如果代码执行报错，将错误信息回传给Code Agent进行迭代修复，而不是直接崩溃。\n\n#### 第五阶段：智能历史渲染（解决“上下文膨胀”）\n*   **思考：** 随着步骤增加，历史记录会无限增长，撑爆上下文窗口。直接截断会丢失关键信息。\n*   **决策：** 开发**History Rendering（历史渲染）模块**，对上下文进行“有损压缩”。\n*   **逻辑：**\n    *   **保留全量：** 用户的指令、生成的文本和代码本身通常不长，全量保留。\n    *   **智能截断：** 代码的输出往往很长。只保留成功输出的“头部”（关键信息）和失败输出的“尾部”（错误堆栈）。\n    *   **滑动窗口：** 如果总长度仍超限，只保留最近的N个字符。\n*   **逻辑：** 确保LLM在任何时刻看到的都是最相关、最精简的信息，从而维持推理连贯性。\n\n### 4. 最终系统形态\n通过上述层层递进的思考，作者最终形成了CEDAR系统的核心逻辑：\n**一个基于上下文工程的智能体系统，它通过结构化输入引导，利用编排器路由文本与代码生成代理，在本地执行代码以保障隐私与计算准确性，并通过智能的历史压缩机制，在有限的上下文窗口内完成复杂的数据科学任务。**\n\n---\n\n**总结：**\n作者的思考路径并非从“如何设计一个复杂的Agent”出发，而是从“如何管理信息流”出发。**CEDAR的本质不是算法创新，而是信息架构的创新**——通过精心设计什么信息应该进入Prompt、以什么形式进入、以及在何时被修剪，从而释放LLM在复杂任务中的潜力。"
                },
                {
                    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
                    "arxiv_id": "2601.06487",
                    "authors": "Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha",
                    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一种名为ArenaRL的强化学习范式，旨在通过锦标赛式相对排序来提升LLM智能体在开放式任务（如复杂旅行规划）中的表现。研究涉及智能体的自我演化（通过反馈自我完善）和规划能力，属于单智能体研究范畴，且侧重于算法改进而非纯应用或基础设施。",
                    "summary2": "本文旨在解决开放性Agent任务中强化学习因点式评分导致的判别性崩溃问题。针对缺乏客观真值的复杂规划场景，我们提出了一种ArenaRL框架，通过基于锦标赛的相对排名机制替代不稳定的标量评分，并利用带种子的单败淘汰赛实现高效优势估计。我们在Open-Travel和Open-DeepResearch基准上，通过胜率和多维度评估指标验证了其有效性，显著优于现有RL基线。",
                    "summary_translation": "强化学习 已显著提升了 LLM agents (大语言模型智能体) 在具有 verifiable outcomes (可验证结果) 的任务上的表现，但在具有 vast solution spaces (巨大解空间) 的 open-ended agent tasks (开放式智能体任务)（例如复杂的旅行规划）中仍然面临挑战。由于这些任务缺乏 objective ground-truth (客观真值)，当前的 RL algorithms (强化学习算法) 主要依赖于对 individual responses (单个响应) 分配 scalar scores (标量分数) 的 reward models (奖励模型)。我们认为这种 pointwise scoring (逐点打分) 存在固有的 discrimination collapse (判别性崩溃)：reward model (奖励模型) 难以区分不同 trajectories (轨迹) 之间的 subtle advantages (细微优势)，导致组内的分数被压缩到一个狭窄的范围内。因此，有效的 reward signal (奖励信号) 被 reward model (奖励模型) 的噪声所主导，导致 optimization stagnation (优化停滞)。为了解决这个问题，我们提出了 ArenaRL，这是一种从 pointwise scalar scoring (逐点标量打分) 转向 intra-group relative ranking (组内相对排序) 的 reinforcement learning paradigm (强化学习范式)。ArenaRL 引入了一种 process-aware pairwise evaluation mechanism (过程感知成对评估机制)，采用 multi-level rubrics (多级评分标准) 为 trajectories (轨迹) 分配 fine-grained relative scores (细粒度相对分数)。此外，我们构建了一个 intra-group adversarial arena (组内对抗竞技场) 并设计了一种 tournament-based ranking scheme (基于锦标赛的排序方案) 来获取稳定的 advantage signals (优势信号)。Empirical results (实证结果) 证实，构建的 seeded single-elimination scheme (种子单败淘汰赛方案) 在仅具有 O(N) 复杂度的情况下，实现了与具有 O(N^2) 复杂度的 full pairwise comparisons (全成对比较) 几乎等效的 advantage estimation accuracy (优势估计精度)，在效率和精度之间取得了最佳平衡。此外，为了解决缺乏针对 open-ended agents (开放式智能体) 的 full-cycle benchmarks (全周期基准) 的问题，我们构建了 Open-Travel 和 Open-DeepResearch，这两个高质量的 benchmarks (基准) 具有涵盖 SFT (监督微调)、RL training (强化学习训练) 和 multi-dimensional evaluation (多维评估) 的 comprehensive pipeline (全流程管道)。Extensive experiments (广泛实验) 表明，ArenaRL 明显优于 standard RL baselines (标准强化学习基线)，使 LLM agents (大语言模型智能体) 能够为复杂的现实世界任务生成更 robust (鲁棒) 的解决方案。",
                    "inspiration_trace": "基于论文《ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking》的内容，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题的确立与现状的困境\n**思考起点：** 强化学习（RL）在数学、代码等有明确“标准答案”的任务上极其成功，但在旅行规划、深度研究等**开放式任务**上却举步维艰。\n**核心矛盾：** 开放式任务没有客观的Ground Truth（标准答案）。现有的解决方案通常采用“LLM作为裁判”给模型的输出打一个标量分数（例如0-10分）。\n**初步假设：** 如果能训练一个准确的奖励模型来给这些开放式轨迹打分，就能像数学题一样进行RL优化。\n\n### 第二阶段：现象观察与核心痛点识别\n**深入观察：** 作者在实验中发现了一个反直觉的现象：随着模型能力的提升，RL优化反而停滞甚至退化。\n**归因分析：** 作者将此命名为**“判别性崩溃”**。\n1.  **信号压缩：** 当模型变强后，生成的轨迹质量都很高，且分布趋同。裁判很难区分“好”和“更好”，给出的分数被压缩在一个极窄的区间（如0.8-0.9）。\n2.  **信噪比（SNR）恶化：** 裁判本身存在随机噪声（如位置偏差、长度偏好）。当分数之间的差异（信号）小于裁判的随机误差（噪声）时，优化过程实际上是在拟合噪声，而非提升能力。\n**结论：** 在开放式任务中，**点式标量打分**存在根本性缺陷，无法提供有效的梯度信号。\n\n### 第三阶段：范式转移——从“绝对分数”到“相对排序”\n**理论借鉴：** 借鉴决策理论，人类在判断模糊事物时，相对比较（A比B好）比绝对量化（A是8.5分）更稳定、更准确。\n**核心假设：** 放弃给单个轨迹打绝对分，转而在**组内**进行轨迹之间的两两比较，构建相对排名。\n**预期收益：** 相对比较能放大细微的质量差异，避免陷入绝对分数的“高分段压缩”陷阱，从而获得更纯净的优势信号。\n\n### 第四阶段：工程落地的挑战——效率与精度的权衡\n**新问题：** 虽然两两比较（Round-Robin，循环赛）能提供最准确的排名，但其计算复杂度是 $O(N^2)$。对于需要大规模采样的RL训练来说，这是不可接受的昂贵成本。\n**朴素尝试与失败：**\n1.  **锚点法：** 只让所有样本与一个锚点（如贪婪解码结果）比较。复杂度降为 $O(N)$，但分辨率太低，无法区分两个都比锚点好但互有优劣的样本。\n2.  **标准淘汰赛：** 随机两两对决，胜者晋级。虽然快，但随机性太大。两个高质量的样本可能在第一轮就相遇，导致其中一个被过早淘汰，损失了信息。\n\n### 第五阶段：结构创新——带种子的单败淘汰赛\n**逻辑推演：** 为了在 $O(N)$ 的线性复杂度下保持接近循环赛的精度，必须解决“过早相遇”的问题。\n**解决方案：** 提出**带种子的单败淘汰赛**。\n1.  **预排序：** 先利用低成本的“锚点法”对所有样本进行一轮快速评估，得到一个粗略的初始排名（种子）。\n2.  **结构化对决：** 按照种子排布对阵（例如：第1名对最后一名，第2名对倒数第二名）。这保证了强样本在早期不会相遇，只有到了决赛圈才强强对话。\n**结果：** 这种设计既保留了线性复杂度的高效，又通过先验信息保证了排名的保真度，实现了效率与精度的最佳平衡。\n\n### 第六阶段：评估维度的深化——过程感知\n**最后一步：** 既然是Agent任务，评价标准不能只看最终答案。\n**补充逻辑：** 引入**过程感知的成对评估**。裁判不仅看结果，还要审查思维链的逻辑连贯性和工具调用的有效性。这确保了RL优化的方向是提升Agent的内在推理能力，而不是仅仅学会生成漂亮的最终文本。\n\n---\n\n**总结：**\n作者的思考路径是从**“开放式任务缺乏客观标准”**这一痛点出发，通过**“判别性崩溃”**否定了现有的标量打分范式，进而提出**“相对排序”**的理论转向。为了解决该理论带来的计算开销，作者通过**“带种子的淘汰赛”**这一精巧的结构设计，成功在计算效率和信号质量之间找到了最优解，最终形成了ArenaRL的方法论闭环。"
                },
                {
                    "title": "Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users",
                    "arxiv_id": "2601.06301",
                    "authors": "Arth Bhardwaj, Nirav Diwan, Gang Wang",
                    "summary": "Web scraping has historically required technical expertise in HTML parsing, session management, and authentication circumvention, which limited large-scale data extraction to skilled developers. We argue that large language models (LLMs) have democratized web scraping, enabling low-skill users to execute sophisticated operations through simple natural language prompts. While extensive benchmarks evaluate these tools under optimal expert conditions, we show that without extensive manual effort, current LLM-based workflows allow novice users to scrape complex websites that would otherwise be inaccessible. We systematically benchmark what everyday users can do with off-the-shelf LLM tools across 35 sites spanning five security tiers, including authentication, anti-bot, and CAPTCHA controls. We devise and evaluate two distinct workflows: (a) LLM-assisted scripting, where users prompt LLMs to generate traditional scraping code but maintain manual execution control, and (b) end-to-end LLM agents, which autonomously navigate and extract data through integrated tool use. Our results demonstrate that end-to-end agents have made complex scraping accessible - requiring as little as a single prompt with minimal refinement (less than 5 changes) to complete workflows. We also highlight scenarios where LLM-assisted scripting may be simpler and faster for static sites. In light of these findings, we provide simple procedures for novices to use these workflows and gauge what adversaries could achieve using these.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究并基准测试了“端到端LLM智能体”，重点评估了智能体的“工具使用”和“自主导航”能力，符合单智能体的研究范围。",
                    "summary2": "本文旨在评估LLM对网络爬虫的民主化影响及非专家用户的实际能力。针对35个跨越5个安全层级的网站，我们提出了两种工作流：LLM-assisted Scripting (LAS) 和 End-to-end LLM Agent (ELA)，并在这些网站上通过Extraction Success Rate (ESR)、Execution Time和Manual Effort Required (MER)验证了其有效性。",
                    "summary_translation": "历史上，Web scraping (网络爬虫) 一直需要掌握 HTML parsing (HTML解析)、session management (会话管理) 和 authentication circumvention (身份验证绕过) 等技术专长，这使得大规模数据提取仅限于熟练的开发者。我们认为，large language models (LLMs，大语言模型) 已经普及了 Web scraping，使低技能用户能够通过简单的 natural language prompts (自然语言提示) 执行复杂的操作。尽管现有的广泛基准测试是在最佳专家条件下评估这些工具的，但我们表明，在无需大量人工投入的情况下，当前的 LLM-based workflows (基于LLM的工作流) 能够使 novice users (新手用户) 抓取原本无法访问的复杂网站。我们针对 35 个跨越五个 security tiers (安全层级) 的网站（包括 authentication (身份验证)、anti-bot (反机器人) 和 CAPTCHA controls (验证码控制)），系统性地评估了日常用户利用 off-the-shelf LLM tools (现成的LLM工具) 所能实现的效果。我们设计并评估了两种截然不同的 workflows (工作流)： LLM-assisted scripting (LLM辅助脚本编写)，即用户提示 LLM 生成传统的抓取代码，但保留手动执行控制权；以及 end-to-end LLM agents (端到端LLM智能体)，即通过 integrated tool use (集成工具使用) 自主导航并提取数据。我们的结果表明，end-to-end agents (端到端智能体) 已使复杂的抓取任务变得易于实现——仅需一个提示配合 minimal refinement (微调，少于5次修改) 即可完成整个 workflows (工作流)。我们还强调了在某些场景下，对于 static sites (静态网站)，LLM-assisted scripting (LLM辅助脚本编写) 可能更为简单快捷。基于这些发现，我们为 novice users (新手用户) 提供了使用这些 workflows (工作流) 的简易流程，并评估了 adversaries (攻击者) 利用这些技术可能达到的效果。",
                    "inspiration_trace": "基于论文《Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观观察与问题提出\n**——从“技术壁垒”到“技术民主化”的范式转移**\n\n1.  **观察现象**：\n    *   **过去**：网络爬虫是一项高门槛技术，需要掌握HTML解析、会话管理、反爬虫绕过等专业技能，这构成了天然的“技术过滤器”，限制了大规模数据提取仅限于熟练开发者。\n    *   **现在**：大语言模型（LLM）和智能体框架的出现，使得用户仅凭自然语言提示就能执行复杂的爬虫操作。\n\n2.  **提出核心问题**：\n    *   LLM是否真正实现了网络爬虫的“民主化”？\n    *   换言之，缺乏深厚技术背景的“日常用户”，是否真的能利用现成的LLM工具，完成以前只有专家才能做到的复杂数据提取？\n\n### 第二阶段：识别研究空白\n**——现有评估与真实场景的脱节**\n\n1.  **批判现有文献**：\n    *   作者注意到，现有的基准测试（如AgentBench, OSWorld）大多关注“最佳实践”。\n    *   这些测试通常假设在**理想条件**下进行：拥有专家指导、经过优化的配置、复杂的提示工程。\n\n2.  **锁定现实差距**：\n    *   **真实用户画像**：非专家用户通常使用默认设置，缺乏深度调试技能，且受限于时间和预算。\n    *   **研究盲区**：学术界缺乏对“非专家用户在现实约束下，利用现成工具到底能做到什么程度”的实证评估。\n\n3.  **确立研究目标**：\n    *   不再评估“工具的上限（专家能做什么）”，而是评估“工具的下限（新手能做什么）”。\n    *   量化这种“民主化”对网络安全防御（反爬虫）的实际影响。\n\n### 第三阶段：假设构建与变量设计\n**——如何模拟“真实世界”的复杂性？**\n\n1.  **定义威胁模型**：\n    *   为了建立保守的基线，作者将研究对象设定为“低技能行为者”。假设他们只会运行Python脚本、使用LLM，但不了解爬虫库的深层细节，也不使用高级提示技巧。\n\n2.  **构建难度梯度**：\n    *   为了全面测试，作者认为不能只测静态页面。必须模拟网站防御的升级过程。\n    *   **逻辑推演**：从最简单的静态页面，逐步增加难度，直到传统工具完全失效。\n    *   **最终分类**：确立了5个难度层级（简单HTML -> 复杂HTML -> 简单认证 -> 复杂认证 -> CAPTCHA）。\n\n### 第四阶段：方法论形成\n**——对比两种截然不同的“人机协作模式”**\n\n1.  **模式抽象**：\n    *   作者意识到，用户使用LLM爬虫主要有两种思维模式，这构成了实验的核心对比维度：\n    *   **模式 A：LLM辅助脚本编写 (LAS)**。\n        *   *思维逻辑*：用户仍想掌控代码执行，只是把LLM当作“高级程序员”来生成代码（如BeautifulSoup/Scrapy脚本），然后自己运行。\n        *   *代表场景*：传统开发者的提效工具。\n    *   **模式 B：端到端LLM智能体 (ELA)**。\n        *   *思维逻辑*：用户完全不想写代码，只给目标，让智能体像人一样操作浏览器（如Claude, Simular.ai）。\n        *   *代表场景*：完全不懂代码的小白用户。\n\n2.  **确立评估指标**：\n    *   除了传统的“成功率”（能不能做），作者引入了“易用性指标”（好不好做）。\n    *   **关键指标**：手动干预程度。这直接反映了“民主化”的程度——如果需要频繁手动调试，说明门槛依然存在。\n\n### 第五阶段：实证推演与结果验证\n**——验证“易用性”与“能力”的权衡**\n\n1.  **预期假设**：\n    *   对于静态网站，传统代码（LAS）应该更快、更高效。\n    *   对于复杂网站（登录、验证码），智能体（ELA）应该具有压倒性优势，因为它们能模拟人类行为。\n\n2.  **实验验证与发现**：\n    *   **发现1**：ELA确实让复杂爬虫变得触手可及（单次提示即可），证明了民主化的真实性。\n    *   **发现2**：但在简单任务上，ELA效率低下（慢10-20倍），属于“杀鸡用牛刀”。\n    *   **发现3**：LAS在遇到认证和反爬时彻底失效，而ELA虽然慢但能行得通。\n\n### 第六阶段：结论与启示\n**——从“二元对立”到“场景互补”**\n\n1.  **逻辑升华**：\n    *   作者的思考并没有停留在“谁更好”，而是上升到了“适用场景”。\n    *   **核心结论**：不存在万能的工具，存在的是“效率”与“可访问性”的权衡。\n\n2.  **未来展望**：\n    *   基于实验结果，作者进一步推演出未来的理想形态：**混合模式**。\n    *   *新思路*：利用智能体（ELA）去搞定最难的“登录/绕过”环节，获取会话权限，然后交给传统脚本（LAS）进行高效的数据提取。这结合了智能体的“灵活性”和脚本的“高效性”。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“现象观察 -> 差距识别 -> 模型构建 -> 对比实验 -> 场景化结论”**的学术逻辑。其核心创新点在于将评估视角从“技术能力的极限”转向了“普通用户的可达性”，并通过对两种工作流（LAS vs ELA）的精细划分，精准地刻画了LLM时代网络爬虫技术的新版图。"
                },
                {
                    "title": "Automated QoR improvement in OpenROAD with coding agents",
                    "arxiv_id": "2601.06268",
                    "authors": "Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee",
                    "summary": "EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个名为AuDoPEDA的自主编码系统，明确使用了“coding agents”这一术语。该系统具备单智能体的核心特征：自主性、规划（提出研究方向）、工具使用（读取代码库、提交可执行差异）以及闭环反馈机制，完全符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决EDA开发受限于专家资源稀缺及代码库复杂的问题，实现利用LLM自主改进OpenROAD的QoR。针对OpenROAD多语言、大规模的代码仓库，我们提出了AuDoPEDA系统，该系统集成了图结构文档生成、基于文献的DSPy规划及具有QoR反馈的自主执行代理。在ASAP7、SKY130HD和Nangate45 benchmark上，通过routed wirelength和effective clock period验证，实现了线长降低5.9%和时钟周期减少10.0%的显著效果。",
                    "summary_translation": "EDA（电子设计自动化）的开发与创新一直受到专家工程资源稀缺的制约。尽管领先的 LLMs（大语言模型）在代码编写和科学推理任务中表现优异，但其在推动 EDA 技术本身发展方面的能力尚未得到充分验证。我们提出了 AuDoPEDA，这是一个构建于 OpenAI 模型和 Codex 类智能体之上的自主式、基于代码仓库的编码系统。该系统能够读取 OpenROAD（开源自动化设计工具），提出研究方向，将其扩展为实施步骤，并提交可执行的 diffs（差异补丁）。我们的主要贡献包括：(i) 一个用于 EDA 代码修改的闭环 LLM 框架；(ii) 一套面向 PPA（功耗、性能、面积）优化的 OpenROAD 任务集及评估协议；以及 (iii) 仅需极少量人工监督的端到端演示。在 OpenROAD 上进行的实验表明，布线线长最多降低了 5.9%，有效时钟周期最多缩短了 10.0%。",
                    "inspiration_trace": "基于论文《Automated QoR improvement in OpenROAD with coding agents》，以下是对作者提出AuDoPEDA方法核心逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观问题：EDA创新的资源瓶颈与LLM的潜力错位\n**观察：**\nEDA（电子设计自动化）工具的发展严重依赖资深专家。这些专家需要跨越庞大的代码库（数百万行C++、Tcl、Python等）和复杂的迭代流程进行推理。然而，这种人力资源极其稀缺，限制了EDA技术的迭代速度。\n\n**矛盾：**\n另一方面，以GPT-4、Codex为代表的大语言模型（LLM）在代码生成和科学推理任务上表现出色。但在EDA领域，LLM的应用多停留在辅助脚本编写或RTL生成层面，尚未触及核心物理设计（PD）算法的改进。\n\n**核心问题：**\n能否让LLM驱动的智能体像人类专家一样，自主地对工业级EDA代码库进行修改，并直接提升芯片设计的质量（QoR，如功耗、性能、面积）？\n\n---\n\n### 2. 深入分析：通用代码代理在EDA领域的“水土不服”\n**挑战识别：**\n作者意识到，直接将通用的代码生成模型（如GitHub Copilot）应用于OpenROAD这样的EDA项目会面临三个致命障碍：\n1.  **上下文稀释：** OpenROAD代码库规模巨大、语言混杂（C++核心+Tcl脚本+Python工具），且文档稀疏。通用模型无法在有限的上下文窗口中理解跨模块的隐式接口和不变量。\n2.  **领域知识缺失：** 优化物理设计不仅仅是写代码，更需要结合EDA领域的学术文献（如布局、布线算法）。单纯的代码补全无法产生“研究级”的改进思路。\n3.  **验证闭环困难：** 软件工程的正确性通常通过单元测试判断，但EDA的改进必须通过物理设计流程（RTL-to-GDS）来验证，指标是PPA（功耗、性能、面积）。这是一个高成本、长周期的反馈过程。\n\n---\n\n### 3. 核心假设：模拟人类专家的“入职”过程\n**思维转折：**\n作者提出，与其试图训练一个懂EDA的超级模型，不如模拟人类专家的学习路径。人类专家在接手OpenROAD时，并不是直接阅读源码，而是先阅读文档、理解架构、查阅文献，然后提出假设，最后修改代码并跑流验证。\n\n**假设：**\n如果构建一个系统，能够为LLM智能体提供“文档优先”的入职环境，使其能够像人类一样结构化地获取代码知识、结合文献进行规划，并在真实的QoR反馈下迭代，那么它就能实现自主的代码改进。\n\n---\n\n### 4. 方法论构建：四阶段逻辑演进\n基于上述假设，作者将复杂的任务解构为四个逻辑严密的阶段，形成了一个闭环系统。\n\n#### 第一阶段：结构化理解（S0）—— 解决“看不懂”的问题\n**思考：**\n原始代码库太乱，直接喂给LLM效果差。必须先进行预处理，提取出机器可读的“知识图谱”。\n**逻辑：**\n利用Tree-sitter解析多语言代码，构建属性图（DAG），将函数调用、依赖关系显式化。然后，通过自底向上的遍历，自动生成“文档卡片”，总结每个模块的API、前置/后置条件。这相当于为智能体编写了一部动态更新的“操作手册”。\n\n#### 第二阶段：文献引导的规划（S1）—— 解决“没思路”的问题\n**思考：**\n光懂代码结构不够，还需要知道“改什么能提升性能”。这需要领域知识。\n**逻辑：**\n将规划过程视为一个声明式的程序（利用DSPy框架）。智能体结合“代码文档”（S0产物）和“EDA文献库”（外部知识），通过检索增强生成（RAG），合成出高层的研究计划。例如：“根据文献X，调整布局阶段的拥塞惩罚权重可能减少线长”。\n\n#### 第三阶段：计划定位与颗粒化（S2）—— 解决“落地难”的问题\n**思考：**\n高层计划（如“调整拥塞权重”）不能直接执行，必须映射到具体的代码修改点，且必须保证修改是安全的。\n**逻辑：**\n将高层计划投影到代码图上，找到具体的修改位置（文件、函数）。同时，将计划转化为“颗粒化计划”，包含具体的Diff意图、预检查（编译、测试）、监控指标和回滚条件。这一步将抽象的“研究思路”变成了可执行的“工程任务单”。\n\n#### 第四阶段：自主执行与QoR反馈（S3）—— 解决“验证慢”的问题\n**思考：**\n代码修改后，必须跑通EDA流程才能知道好坏。如何保证自动化且不破坏系统？\n**逻辑：**\n构建一个基于Codex的执行智能体，应用Diff、编译、运行OpenROAD流程。关键在于引入“QoR门控”：如果修改导致DRC违规或时序恶化，系统自动回滚。智能体通过爬山算法，在指标反馈的引导下不断尝试，直到找到最优解。\n\n---\n\n### 5. 总结：从“辅助工具”到“自主研究员”的范式转变\n**逻辑闭环：**\n整个思考过程从解决“资源稀缺”出发，通过分析EDA代码的特殊性，提出了“模拟人类专家学习”的核心假设，并最终落地为一个集成了**知识图谱构建（S0）**、**文献推理（S1）**、**工程映射（S2）**和**闭环验证（S3）**的完整系统。\n\n**最终贡献：**\n作者不仅仅是在用LLM写代码，而是构建了一个能够**阅读文献、提出假设、修改算法、并在真实芯片设计流程中验证效果**的自主科研智能体。这标志着EDA工具的优化模式从“人工驱动”转向了“AI自主驱动”。"
                },
                {
                    "title": "Latent Space Communication via K-V Cache Alignment",
                    "arxiv_id": "2601.06123",
                    "authors": "Lucio M. Dery, Zohar Yahav, Henry Prior, Qixuan Feng, Jiajun Shen, Arthur Szlam",
                    "summary": "Solving increasingly complex problems with large language models (LLMs) necessitates a move beyond individual models and towards multi-model systems that can effectively collaborate. While text has traditionally served as the medium for inter-model communication, a richer and more efficient exchange is possible if models can access each other's internal states directly. In this paper, we propose learning a shared representation space that aligns the k-v caches of multiple models, creating a high-bandwidth channel for collaboration without altering the underlying pre-trained parameters. We do so by augmenting each model with adapters to translate its state into and out of this shared space. Via a suite of experiments with Gemma-2 models, we demonstrate that this approach not only enables seamless inter-model communication but also improves individual model performance. We also show that the shared space allows for the direct transfer of learned skills, such as soft prompts, between different models. Our work represents a significant step towards a future where models can fluidly share knowledge and capabilities.",
                    "category": "cs.AI",
                    "filter_reason": "论文主要研究多模型系统之间的协作与通信机制，通过K-V缓存对齐实现模型间的高效信息交换，符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决多模型协作中通信带宽低及潜在空间不兼容的问题。针对不同训练条件下的LLM，我们提出了一种通过学习共享k-v cache潜在空间并利用adapters进行翻译对齐的方法，在Gemma-2模型及多语言C4数据集上通过语言建模损失验证了其有效性。",
                    "summary_translation": "利用大型语言模型解决日益复杂的问题，要求我们超越单一模型，转向能够有效协作的多模型系统。尽管文本传统上一直作为模型间通信的媒介，但如果模型能够直接访问彼此的内部状态，则可以实现更丰富、更高效的交互。在本文中，我们提出学习一个共享表示空间，该空间对齐多个模型的 k-v caches (键值缓存)，从而在不改变底层预训练参数的情况下，为协作创建一个高带宽通道。我们通过为每个模型增加 adapters (适配器) 来实现这一点，用于将其状态转换进出该共享空间。通过一系列基于 Gemma-2 模型的实验，我们证明了该方法不仅实现了无缝的模型间通信，还提升了单个模型的性能。我们还展示了该共享空间允许在不同模型之间直接迁移习得的技能，例如 soft prompts (软提示)。我们的工作代表了迈向模型能够灵活共享知识和能力未来的重要一步。",
                    "inspiration_trace": "基于论文《Latent Space Communication via K-V Cache Alignment》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n### 1. 宏观背景：单体模型的局限与协作的必要性\n**思考起点**：随着LLM能力边界的扩展，单一模型（无论是通用模型还是领域专家）在解决极其复杂的问题时往往力不从心。\n**逻辑推演**：未来的趋势必然是从“单体智能”走向“群体智能”。我们需要构建一个多模型系统，让不同特长的模型能够协同工作。然而，这就引出了一个核心问题：**这些模型之间应该如何高效地交换信息？**\n\n### 2. 现状瓶颈：文本通信的低带宽\n**观察现状**：目前多模型协作（如Agent系统、级联模型）主要依赖自然语言文本进行通信。\n**痛点分析**：文本是一种“有损”且低带宽的媒介。模型内部丰富的推理链、上下文细节和隐含状态很难被完全压缩进几个token的文本中。这种通信方式就像两个人只能通过纸条交流，效率极低且信息丢失严重。\n**思考方向**：如果模型能绕过文本，直接读取彼此的“思维过程”，协作效率将产生质的飞跃。\n\n### 3. 核心洞察：K-V Cache作为高带宽载体\n**技术聚焦**：Transformer架构中的Key-Value (K-V) Cache 实际上存储了模型处理输入时的内部状态（注意力机制的历史记录）。\n**假设提出**：K-V Cache 是模型内部状态的丰富表征。如果模型A能直接访问模型B的K-V Cache，就相当于直接读取了B的“记忆”和“推理路径”。这提供了一种比文本高得多的通信带宽。\n\n### 4. 关键障碍：潜在空间的异构性\n**现实挑战**：虽然想法很美好，但现实很骨感。不同模型（不同架构、不同训练数据、不同随机初始化）的K-V Cache所在的潜在空间是完全不同的。\n**深层原因**：由于参数不同，同一个token在不同模型中产生的条件依赖和向量表征是截然不同的，且这种差异会随着网络深度呈指数级放大。直接混用会导致模型“听不懂”对方的内部状态。\n\n### 5. 理论假设：引入“中间语”共享空间\n**灵感借鉴**：借鉴机器翻译中的“中间语”概念。在翻译多种语言时，不直接进行两两互译，而是先将所有语言映射到一个抽象的语义空间，再从该空间映射到目标语言。\n**核心构想**：构建一个**全局共享的潜在空间（$\\Sigma$）**。这个空间充当所有模型的“通用语言”。每个模型只需要学会两件事：如何把自己的K-V Cache“翻译”进这个共享空间，以及如何从共享空间“翻译”回自己的私有空间。\n\n### 6. 方法构建：基于Adapter的非线性映射\n**设计约束**：为了保持模型的原始能力并降低成本，不能修改预训练模型的参数。\n**架构设计**：为每个模型配备轻量级的“适配器”。\n*   **映射方向**：$T[\\text{Model} \\to \\Sigma]$（编码）和 $T[\\Sigma \\to \\text{Model}]$（解码）。\n*   **非线性选择**：由于不同模型间的几何关系可能高度复杂且非线性，简单的线性映射可能不够。作者选择了基于交叉注意力的小型Transformer作为适配器架构，以捕捉复杂的层级依赖关系。\n*   **扩展性**：这种设计使得参数量仅随模型数量线性增长，且新模型加入时无需重训练整个系统。\n\n### 7. 优化目标：从“形似”到“神似”\n**训练信号的选择**：如何训练这些适配器？\n*   **初级尝试（重建损失）**：强制让翻译后的Cache看起来像目标模型原本的Cache。但这可能过于严格，且受限于目标模型本身的能力上限。\n*   **进阶思考（功能对齐）**：我们不需要Cache完全一样，只需要它们产生的**结果**一样。\n*   **最终方案（后缀语言建模损失）**：使用源模型的前缀Cache翻译给目标模型，看目标模型能否准确预测后续的文本。这是一种“功能主义”的训练目标，只要能帮助模型完成任务，Cache长什么样并不重要。实验证明，这种方法甚至能通过共享空间“蒸馏”出更好的特征，提升单体模型性能。\n\n### 8. 价值延伸：技能的即插即用\n**逻辑推演**：既然存在一个共享的潜在空间，那么在这个空间中的任何表征（如软提示 Soft Prompts、前缀微调 Prefix Tuning）本质上都变成了一种“通用资源”。\n**应用场景**：在一个模型上学到的特定技能（如某种写作风格或编程能力），可以通过共享空间直接“移植”给另一个模型，而无需对目标模型进行额外训练。这实现了从“模型协作”到“技能复用”的跨越。\n\n---\n\n**总结**：\n作者的思考路径是从**解决多模型协作效率低下的宏观痛点**出发，通过**挖掘K-V Cache的高带宽价值**，针对**模型异构性这一核心障碍**，借鉴**机器翻译的中间语思想**，提出了**基于共享潜在空间和Adapter映射的解决方案**，并最终通过**功能对齐的训练目标**和**技能迁移的验证**，完成了从理论构想到方法论的闭环。"
                },
                {
                    "title": "Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation",
                    "arxiv_id": "2601.06034",
                    "authors": "Dudekula Kasim Vali",
                    "summary": "Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个“Autonomous QA Agent”，利用RAG（检索增强生成）作为记忆机制，并生成Selenium脚本（工具使用），属于单智能体研究范畴（记忆、工具使用），且不属于排除的纯应用领域（如医疗/金融）或纯推理研究。",
                    "summary2": "本文旨在解决LLM生成Selenium脚本时因缺乏应用上下文而产生幻觉的问题。针对自然语言需求和HTML DOM结构，我们提出了一种Autonomous QA Agent，这是一种基于RAG的多模态框架，通过检索文档与HTML上下文生成脚本。在自定义电商应用的20个测试场景上，通过语法有效性、元素解析率和执行成功率验证了其有效性，实现了90%的执行成功率。",
                    "summary_translation": "软件测试在软件开发生命周期中至关重要，然而将需求转化为可执行测试脚本的过程仍主要依赖人工，且容易出错。尽管大语言模型能够生成代码，但它们经常产生幻觉，编造出不存在的UI元素。我们提出了自主QA代理，这是一种检索增强生成系统，它将Selenium脚本生成基于特定项目的文档和HTML结构之上。通过将多种格式导入向量数据库，我们的系统在生成代码之前会检索相关的上下文信息。针对20个电商测试场景的评估表明，我们的RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95%置信区间：[85%, 95%]，p < 0.001）的执行成功率，而标准LLM生成的成功率仅为30%。尽管我们的评估仅限于单一领域，但我们的方法通过将生成过程基于实际的DOM结构，显著减少了幻觉现象，展示了RAG在自动化UI测试中的潜力。",
                    "inspiration_trace": "基于论文《Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：QA环节的效率瓶颈\n**思考起点：** 在敏捷开发和DevOps主导的现代软件工程中，开发迭代速度极快，但软件测试（QA）成为了明显的瓶颈。\n**核心痛点：** QA工程师花费40%-50%的时间在做“翻译”工作——将自然语言描述的功能需求（PRD）手动转化为机器可执行的自动化测试脚本（如Selenium）。这个过程不仅枯燥，而且容易出错（如选错元素ID、忽略边界情况）。\n**初步设想：** 能否利用代码生成能力强大的大语言模型（LLM）来自动完成这个“翻译”过程？\n\n### 2. 尝试与失败：LLM的“盲写”困境\n**尝试：** 直接使用标准的LLM（如GPT-4, Llama），输入自然语言需求（如“生成一个添加购物车的脚本”），让其编写Selenium代码。\n**观察到的现象：** LLM生成的代码语法通常没问题，但一运行就报错。\n**失败原因分析：** LLM患有一种“盲写症”。它通晓通用的编程语法，但它**看不见**被测应用（AUT）的具体结构。\n**具体表现：** LLM会凭空捏造UI元素。例如，它可能会猜测登录按钮的ID是 `#login-btn`，但实际开发人员写的是 `#btn-submit-login`。这种“幻觉”导致生成的脚本无法定位元素，执行失败。\n\n### 3. 深度诊断：语义鸿沟与上下文缺失\n**问题定义：** 核心问题在于“人类需求”与“机器执行”之间存在语义鸿沟。要生成一个可运行的脚本，不仅需要逻辑（做什么），还需要精确的定位信息（在哪里做）。\n**现有方案的局限：**\n*   **传统MBT（基于模型的测试）：** 构建成本太高，维护困难。\n*   **通用代码RAG：** 现有的检索增强生成多用于检索“相似的代码片段”。但在UI测试中，检索别人的代码对定位当前页面的特定DOM元素帮助不大。\n**关键洞察：** 要解决幻觉，必须让LLM“看见”真实的界面结构。LLM缺失的上下文不是代码示例，而是**应用的实际DOM结构**。\n\n### 4. 策略转折：从“代码检索”到“结构检索”\n**核心假设：** 如果在生成脚本之前，先给LLM提供被测应用的真实HTML文档和需求文档，它就能基于真实的结构编写准确的定位器，从而消除幻觉。\n**方法论创新：** 提出一种专门针对QA领域的RAG架构。\n*   **传统RAG：** 检索通用知识库。\n*   **本论文RAG：** 检索**双模态上下文**。\n    1.  **功能性上下文：** 需求文档（Markdown/PDF），告诉LLM“要测什么”。\n    2.  **结构性上下文：** 原始HTML文件，告诉LLM“元素在哪里”。\n\n### 5. 架构构建：多模态摄入与上下文融合\n**逻辑推演：** 为了实现上述假设，系统需要具备以下能力：\n1.  **知识库构建：** 必须能够“吃进”多种格式的数据。不仅要处理文本需求，还要解析HTML标签，提取出ID、Class等关键属性，并存入向量数据库。\n2.  **精准检索：** 当用户提问时，系统需要同时从文档库中找到相关需求，并从HTML库中找到对应的页面结构片段。\n3.  **提示工程约束：** 在生成阶段，必须强制LLM使用检索到的真实ID，而不是自己编造。通过Prompt明确指令：“仅使用提供的HTML结构中的ID”。\n\n### 6. 验证与结论：Grounding（接地气）的有效性\n**实验设计：** 对比“标准LLM（无上下文）”与“RAG Agent（含HTML上下文）”。\n**结果验证：**\n*   标准LLM：虽然语法正确，但因元素定位错误，执行成功率仅为30%。\n*   RAG Agent：通过将生成过程“锚定”在真实的DOM结构上，执行成功率提升至90%。\n**最终结论：** 证明了在UI自动化测试中，**结构化的上下文（HTML）比通用的代码知识更重要**。通过RAG技术将LLM与实际应用状态连接，是解决测试脚本生成中“幻觉”问题的有效路径。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现瓶颈 -> 尝试新技术（LLM） -> 识别新技术缺陷（幻觉/盲写） -> 引入特定领域知识（DOM结构） -> 设计专用架构（多模态RAG） -> 验证有效性”** 的完整逻辑闭环。其核心创新点在于意识到UI测试不仅仅是代码生成任务，更是一个需要精确空间感知（DOM结构）的任务。"
                },
                {
                    "title": "AI-Assisted Authoring for Transparent, Data-Driven Documents",
                    "arxiv_id": "2601.06027",
                    "authors": "Alfonso Piscitelli, Cristina David, Mattia De Rosa, Ali Mohammed, Federico Nanni, Jacob Pake, Roly Perera, Jessy Sodimu, Chenyiqiu Zheng",
                    "summary": "We introduce _transparent documents_, interactive web-based scholarly articles which allow readers to explore the relationship to the underlying data by hovering over fragments of text, and present an LLM-based tool for authoring transparent documents, building on recent developments in data provenance for general-purpose programming languages. As a target platform, our implementation uses Fluid, an open source programming language with a provenance-tracking runtime. Our agent-based tool supports a human author during the creation of transparent documents, identifying fragments of text which can be computed from data, such as numerical values selected from records or computed by aggregations like sum and mean, comparatives and superlatives like _better than_ and _largest_, trend-adjectives like _growing_, and similar quantitative or semi-quantitative phrases, and then attempts to synthesise a suitable Fluid query over the data which generates the target string. The resulting expression is inserted into the article's web page, turning the static text fragment into an interactable data-driven element able to reveal the data that underwrites the natural language claim. We evaluate our approach on a subset of SciGen, an open source dataset consisting of tables from scientific articles and their corresponding descriptions, which we extend with hand-generated counterfactual test cases to evaluate how well machine-generated expressions generalise. Our results show that gpt4o is often able to synthesise compound expressions extensionally compatible with our gold solutions.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“基于智能体的工具”，利用LLM（GPT-4o）辅助人类作者进行文档创作。该智能体具备工具使用能力，能够识别文本片段并合成Fluid查询与外部系统交互，符合单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在解决学术文档中数据声明难以追溯至底层数据的问题。针对科学论文中的定量描述，我们提出了一种基于LLM的AI辅助编写工具，结合Fluid编程语言的溯源运行时，将静态文本转化为可交互的数据驱动元素，并在SciGen数据集上通过成功率及反事实测试验证了其有效性。",
                    "summary_translation": "我们介绍了“透明文档”，这是一种交互式的基于网络的学术文章，允许读者通过将鼠标悬停在文本片段上来探索其与底层数据的关系。基于通用编程语言在 data provenance（数据溯源）方面的最新进展，我们提出了一种基于 LLM 的工具，用于创作此类透明文档。在目标平台方面，我们的实现采用了 Fluid，这是一种具有 provenance-tracking runtime（具有溯源跟踪功能的运行时）的开源编程语言。我们的 agent-based（基于智能体）的工具在透明文档的创作过程中为人类作者提供支持。该工具能够识别那些可以从数据中计算得出的文本片段，例如：从记录中选取的数值，或通过 sum（求和）和 mean（平均）等 aggregations（聚合操作）计算得出的数值；“better than”（优于）和“largest”（最大）等 comparatives and superlatives（比较级和最高级）；“growing”（增长）等 trend-adjectives（趋势形容词）；以及类似的 quantitative or semi-quantitative phrases（定量或半定量短语）。随后，工具会尝试合成一个合适的 Fluid query（Fluid 查询），以生成目标字符串。生成的表达式被插入到文章的网页中，将静态文本片段转化为 interactable data-driven element（可交互的数据驱动元素），从而能够揭示支撑该 natural language claim（自然语言陈述）的数据。我们在 SciGen 数据集的一个子集上对该方法进行了评估。SciGen 是一个由科学文章中的表格及其对应描述组成的开源数据集。我们通过手工生成的 counterfactual test cases（反事实测试用例）对该数据集进行了扩展，以评估机器生成表达式的 generalise（泛化）能力。结果表明，gpt4o 通常能够生成与我们的 gold solutions（黄金标准）在 extensionally compatible（外延兼容）的 compound expressions（复合表达式）。",
                    "inspiration_trace": "基于论文《AI-Assisted Authoring for Transparent, Data-Driven Documents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 1. 宏观观察：科学交流中的“可追溯性鸿沟”\n**思考起点：**\n作者首先关注到学术出版和科学写作中的一个核心痛点——**信任与验证的困难**。\n*   **现象：** 在学术论文或数据报告中，充斥着大量基于数据的断言（如“系统X比系统Y更快”）。这些断言以静态的自然语言形式存在。\n*   **问题：** 读者（或审稿人）很难直接从文本追溯到支撑该断言的具体数据点。这种“断言”与“证据”之间的脱节，导致了验证困难，甚至因数据管理错误导致论文撤稿。\n*   **现有技术的局限：**\n    *   **数据可视化工具（如Tableau, D3.js）：** 虽然图表是动态的，且部分工具支持溯源，但它们无法处理占据论文主体的自然语言。\n    *   **大语言模型（LLM）：** 擅长理解和生成文本，甚至能进行事实核查，但其输出通常是黑盒的，缺乏将文本片段直接链接到底层数据源的交互式基础设施。\n\n### 2. 核心假设：将“自然语言”视为“计算输出”\n**思维跃迁：**\n为了解决上述鸿沟，作者提出一个颠覆性的假设：**论文中的定量陈述不应是静态的字符串，而应是数据查询的计算结果。**\n*   **类比思维：** 就像Excel中的图表会随数据变化而更新一样，论文中的文字（如“增长率为5%”）也应该是动态生成的。\n*   **概念定义：** 作者提出了“透明文档”的概念。这种文档允许读者通过鼠标悬停在文本上，触发“溯源查询”，直接看到生成该文本的数据来源。\n*   **关键挑战：** 如果要求作者手动编写代码来生成每一个句子（例如写SQL或Python代码来输出“better than”），这在科学写作工作流中是不现实的，门槛太高。\n\n### 3. 方法论构建：寻找“语义理解”与“程序化溯源”的结合点\n**解决方案的合成：**\n作者意识到，要实现上述假设，必须结合两个领域的最新进展，形成互补：\n1.  **LLM的语义理解能力：** 负责将自然语言（如“显著提高”）转化为形式化的逻辑意图。\n2.  **溯源编程语言（Fluid）的基础设施：** 负责执行逻辑并自动维护数据流向，提供交互能力。\n\n**逻辑推演：**\n*   *为什么选Fluid？* 普通语言（如Python）只能计算数据，无法自动追踪数据来源并支持用户交互（悬停查询）。Fluid特有的溯源运行时是“透明性”的技术保障。\n*   *为什么用LLM？* 只有LLM能理解复杂的学术语言并自动生成代码，从而降低作者的使用门槛。\n\n### 4. 实现策略：从“全自动”转向“人机协同”\n**工作流设计：**\n在具体实现路径上，作者没有追求完全自动化的“一键生成”，而是基于对LLM局限性的认知，设计了**人机协同**的迭代工作流。\n*   **思考逻辑：** LLM可能会产生幻觉或生成错误的代码。如果完全自动化，生成的文档将不可信。\n*   **Agent分工：**\n    *   **SuggestionAgent：** 充当“助手”，识别哪些文本片段是可以被数据化的（如数值、比较级）。\n    *   **InterpretationAgent：** 充当“翻译官”，尝试将文本片段编译为Fluid代码。\n*   **闭环验证机制：** 作者设计了一个“生成-验证-修正”的闭环。系统生成代码后，必须在Fluid环境中实际运行，检查输出字符串是否与原文完全匹配。如果不匹配，利用错误信息反馈给LLM进行重试。\n*   **人的角色：** 作者保留最终决定权。只有当作者在网页上交互验证（悬停查看数据）无误后，才会确认替换原文。这确保了科学严谨性。\n\n### 5. 评估视角：从“准确率”到“泛化性与鲁棒性”\n**验证逻辑的深化：**\n在评估方法时，作者不仅关注LLM能否“猜对”代码，更关注这种方法的**鲁棒性**。\n*   **思考：** 如果LLM只是死记硬背了数据，那么当数据发生变化时，生成的代码就会失效。\n*   **反事实测试：** 作者引入了反事实测试用例，故意修改底层数据，观察生成的代码是否能正确反映新的数据状态（例如，数据变了，文本是否自动从“增长”变为“下降”）。\n*   **意义：** 这证明了生成的代码不仅仅是字符串匹配，而是真正捕捉到了文本背后的**语义逻辑**。\n\n### 总结：思想演进脉络\n1.  **发现问题：** 学术文本是静态的，缺乏数据溯源，难以验证。\n2.  **提出愿景：** 让文本像图表一样，成为数据的动态视图（透明文档）。\n3.  **技术选型：** 利用LLM解决“写代码难”的问题，利用Fluid解决“溯源交互”的问题。\n4.  **流程设计：** 采用人机协同的闭环生成，平衡自动化效率与科学准确性。\n5.  **价值验证：** 通过反事实测试，确保系统真正理解了语言与数据的逻辑关系，而非简单的文本替换。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 36,
            "papers": [
                {
                    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
                    "arxiv_id": "2601.06021",
                    "authors": "Jiajie Zhang, Xin Lv, Ling Feng, Lei Hou, Juanzi Li",
                    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究基于LLM的深度搜索智能体，提出了一种强化学习框架来训练智能体进行证据链构建和推理，属于单智能体的工具使用与自我演化范畴，且不涉及被排除的纯应用或纯推理内容。",
                    "summary2": "本文旨在解决深度搜索智能体在强化学习中因依赖二元结果奖励而导致的捷径利用和幻觉问题。针对多跳问答场景，我们提出了一种Citation-aware Rubric Rewards (CaRR) 框架及C-GRPO算法，通过细粒度的引用感知规则奖励来评估推理的全面性和事实性。在BrowseComp、GAIA等多个深度搜索基准上，通过准确率验证了该方法能有效提升智能体的鲁棒性和推理质量。",
                    "summary_translation": "强化学习 (Reinforcement Learning, RL) 已成为提升基于大语言模型 (Large Language Model, LLM) 的深度搜索智能体性能的关键技术。然而，现有方法主要依赖二元结果奖励，这无法捕捉智能体推理过程的全面性和事实性，且往往导致捷径利用和幻觉等不良行为。为解决这些局限性，我们提出了 **引文感知评分标准奖励**，这是一种针对深度搜索智能体的细粒度奖励框架，强调推理的全面性、事实依据以及证据的连接性。CaRR 将复杂问题分解为可验证的单跳评分标准，并要求智能体通过显式识别隐藏实体、利用正确引文予以支持，以及构建连接至预测答案的完整证据链来满足这些标准。我们进一步引入了 **引文感知组相对策略优化**，该方法结合了 CaRR 和结果奖励，用于训练稳健的深度搜索智能体。实验结果表明，在多个深度搜索基准测试中，C-GRPO 始终优于标准的基于结果的 RL 基线模型。我们的分析还验证了 C-GRPO 能够有效抑制捷径利用，促进全面且基于证据的推理，并在开放式深度研究任务中表现出强大的泛化能力。我们的代码和数据可在 https://github.com/THUDM/CaRR 获取。",
                    "inspiration_trace": "基于论文《Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 1. 宏观背景与痛点观察\n**逻辑起点：** 深度搜索代理的兴起与强化学习（RL）的引入。\n*   **现状观察：** 当前主流的深度搜索代理训练主要依赖RL，且普遍使用**二元结果奖励**——即只看最终答案是否与Ground Truth匹配。\n*   **问题发现：** 作者敏锐地观察到这种“唯结果论”的奖励机制存在严重缺陷。在长链路、多跳的复杂推理任务中，代理可以通过“走捷径”（只关注最后几跳，忽略题目中的其他约束）或“幸运的幻觉”（猜对答案但推理过程错误）来获得高分。\n*   **核心矛盾：** 我们的目标是训练一个**鲁棒**、**诚实**且**全面**的推理代理，但现有的奖励信号却在鼓励“投机取巧”的行为。这种错位导致了模型在面对长上下文或更复杂任务时泛化能力差。\n\n### 2. 核心假设与切入点\n**思维转折：** 从关注“答案对不对”转向关注“过程好不好”。\n*   **灵感来源：** 作者注意到现有的合成多跳QA数据集具有天然的**结构化特征**。一个复杂问题可以被拆解为多个中间步骤，每个步骤都包含特定的“隐藏实体”。\n*   **核心假设：** 如果我们将这些中间步骤视为必须通过的“检查点”，那么一个完美的推理轨迹应该满足所有检查点，而不仅仅是终点。\n*   **切入点定义：** 引入**细粒度奖励**来评估推理过程。这个奖励必须包含三个维度：\n    1.  **全面性：** 是否覆盖了所有必要的中间实体？\n    2.  **事实性：** 每个结论是否有引用来源支持？\n    3.  **连通性：** 这些证据是否逻辑相连，最终指向答案？\n\n### 3. 方法论构建：从“结果”到“证据链”\n**逻辑展开：** 如何将上述假设转化为可执行的评估框架？\n*   **第一步：分解。**\n    *   既然问题是由多跳构成的，那么在训练前，先利用LLM将复杂问题拆解为一系列原子化的**单跳规则**。每个规则对应一个需要被发现的隐藏实体。\n*   **第二步：验证。**\n    *   仅仅找到实体还不够，代理必须证明它。作者引入了**引用感知**机制。\n    *   **实体识别：** 检查代理的最终回答中是否明确指出了这些隐藏实体。\n    *   **引用校验：** 检查这些实体的描述是否有对应的网页内容支持，防止幻觉。\n*   **第三步：连通。**\n    *   为了防止代理堆砌无关的正确事实，作者引入了**证据连通性检查**。通过构建图结构，确保被满足的规则能够通过实体关系最终连接到答案节点，形成一条完整的证据链。\n*   **产出：** 形成了**Citation-aware Rubric Rewards (CaRR)** 框架，将原本模糊的“推理质量”量化为“被满足规则的比率”。\n\n### 4. 算法落地：混合奖励机制\n**逻辑闭环：** 如何将新的评估框架融入现有的RL训练流程？\n*   **权衡思考：** 如果完全抛弃结果奖励，只看过程奖励，可能会导致模型陷入“为了找证据而找证据”的误区，偏离“回答问题”的最终目标。\n*   **策略设计：** 提出了**Citation-aware Group Relative Policy Optimization (C-GRPO)**。\n    *   **混合策略：** 保留结果奖励作为基础（保证答案正确），但在答案正确的前提下，叠加过程奖励（鼓励推理更好）。\n    *   **加权机制：** 仅对那些答案正确的轨迹给予额外的过程奖励加权。这样既锁定了正确方向，又激励了更优的路径。\n*   **优化目标：** 引导模型从“只要对就行”进化到“既要对，又要证据确凿、逻辑严密”。\n\n### 5. 验证与反思\n**逻辑验证：** 这种方法真的有效吗？\n*   **实验预期：** 作者预期C-GRPO训练出的模型在长上下文（128k）下表现更好，因为它学会了彻底验证而非走捷径。\n*   **结果分析：** 实验数据证实了这一点。相比标准GRPO，C-GRPO在长上下文下性能提升显著，且在开放性研究任务中展现了更强的泛化能力。\n*   **最终结论：** 通过引入细粒度的、基于引用的规则奖励，成功解决了深度搜索代理中的“捷径利用”和“幻觉”问题，实现了从“投机性智能”向“鲁棒性智能”的转变。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“发现问题 -> 提出假设 -> 构建框架 -> 算法融合 -> 实验验证”**的学术逻辑链条。其核心创新在于**利用合成数据的结构化特征，将不可见的“推理过程”转化为可见的、可验证的“证据链”**，从而解决了RL训练中奖励信号稀疏且误导的痛点。"
                },
                {
                    "title": "Distilling Feedback into Memory-as-a-Tool",
                    "arxiv_id": "2601.05960",
                    "authors": "Víctor Gallego",
                    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了包含“基于文件的记忆系统”和“智能体控制的工具调用”的框架，涉及单智能体的记忆机制、工具使用以及通过反馈进行自我完善，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决推理时自修正计算成本高且无法持久化的问题。针对基于rubric的反馈学习场景，我们提出了一种Memory-as-a-Tool框架，利用文件系统工具调用将瞬时反馈转化为可检索的抽象指南。在Rubric Feedback Bench数据集上，通过Judge scores和成本分析验证了其有效性。实验表明该方法能快速匹配Self-Critique性能并大幅降低推理成本。",
                    "summary_translation": "我们提出了一种框架，通过基于文件的内存系统和代理控制的工具调用，将瞬态批评转化为可检索的指南，从而摊销 inference-time reasoning (推理时推理) 的成本。我们在 Rubric Feedback Bench (基于量规的反馈基准) 上对该方法进行了评估，这是一个用于基于量规学习的新颖数据集。实验表明，我们的增强型 LLMs 能够迅速达到 test-time refinement pipelines (测试时细化流水线) 的性能水平，同时大幅降低推理成本。",
                    "inspiration_trace": "基于论文《Distilling Feedback into Memory-as-a-Tool》，以下是对作者核心方法论形成逻辑链的系统性推演：\n\n### 1. 宏观观察：System 2 的繁荣与代价\n**起点：** 作者首先关注到了当前大模型（LLM）领域的一个核心趋势——“System 2”扩展（如思维链、自我修正、搜索）。\n**现象：** 通过在推理时增加计算量，模型能够显著超越零样本表现，展现出强大的逻辑和生成能力。\n**痛点：** 这种高性能的代价极其昂贵。每次推理都需要重新进行“思考”过程，且这种思考是“片段式”的——一旦上下文窗口关闭，模型就会“忘记”刚才的推理过程。面对新任务时，它必须从头开始推导相同的结论，造成了巨大的计算冗余。\n\n### 2. 核心矛盾：持久性与灵活性的两难\n**深入分析：** 作者对比了现有的两种解决方案，发现了中间的空白地带：\n*   **推理时修正：** 灵活性高，能适应特定任务，但计算成本高，且无法持久化知识。\n*   **微调：** 能持久化知识，推理成本低，但训练成本高，且缺乏快速适应新用户自定义规则（如特定写作风格）的灵活性。\n**问题聚焦：** 如何既保留推理时修正的**灵活适应性**，又能像微调一样实现**低成本的持久化**？\n\n### 3. 核心假设：将“反馈”转化为“记忆”以摊销成本\n**逻辑跃迁：** 作者意识到，在自我修正循环中，模型生成的“批评”或“反馈”本质上是一个高价值的学习信号。\n*   **传统视角：** 反馈仅用于修正当前的输出，用完即弃。\n*   **作者视角：** 反馈应当被蒸馏并存储。\n**假设提出：** 如果能将这种短暂的“批评”转化为持久的、可检索的“指南”，那么在未来的任务中，模型就可以直接调用这些指南，而无需重复昂贵的自我修正循环。这就是**“摊销推理成本”**的核心思想。\n\n### 4. 机制设计：从“被动存储”到“主动工具”\n**实现挑战：** 如何存储这些知识？传统的向量数据库（RAG）通常存储原始数据，缺乏抽象能力，且检索过程是被动的。\n**设计思路：** 作者提出了一种**“记忆即工具”**的范式，强调记忆的主动性和语义性：\n*   **抽象化：** 记忆不应是原始的对话日志，而应是“经验教训”。模型需要将具体的反馈（如“第2段缺乏通感语言”）抽象为通用的原则（如“优先使用通感修辞”）。\n*   **工具化交互：** 不使用黑盒的向量检索，而是将文件系统作为工具。模型必须通过 `ls`（列举）、`read`（读取）、`write`（写入）等工具调用来管理记忆。\n    *   *逻辑：* 这迫使模型在写入时进行**语义命名**（为了以后能找到），在读取时进行**主动推理**（判断哪个文件相关）。这模拟了人类整理笔记的过程。\n\n### 5. 验证与闭环：构建基准测试\n**验证需求：** 为了证明这种方法不仅省钱，还能保持高性能，作者需要一个能测试“从反馈中学习”的环境。\n**方案：** 构建了“Rubric Feedback Bench”。\n*   **逻辑：** 该基准包含复杂的、多维度的评分标准，迫使模型必须通过反馈学习特定的风格或规则（如“混乱写作风格”或“义务论伦理框架”）。\n*   **预期结果：** 实验应证明，经过几轮反馈后，使用记忆的模型在后续任务中能直接生成高质量答案，其性能接近每次都做自我修正的模型，但成本大幅降低。\n\n### 总结：思想演进脉络\n作者从**“System 2 推理的高冗余”**这一宏观问题出发，通过**“摊销计算成本”**的经济学视角，提出了**“将反馈蒸馏为持久记忆”**的解决方案。为了实现这一点，作者摒弃了传统的被动检索，转而采用**基于文件系统的主动工具调用**，强迫模型进行知识的抽象与结构化，最终在**性能与成本**之间找到了帕累托最优的平衡点。"
                },
                {
                    "title": "Can We Predict Before Executing Machine Learning Agents?",
                    "arxiv_id": "2601.05930",
                    "authors": "Jingsheng Zheng, Jintian Zhang, Yujie Luo, Yuren Mao, Yunjun Gao, Lun Du, Huajun Chen, Ningyu Zhang",
                    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了名为 \"FOREAGENT\" 的智能体，旨在解决自主机器学习智能体中的“执行瓶颈”问题。该研究通过引入“预测-验证”循环来优化智能体的工作流，属于单智能体机制（规划与执行优化）的研究范畴。",
                    "summary2": "本文旨在解决自主ML智能体面临的“执行瓶颈”，将数小时的物理执行压缩为秒级的逻辑推理。针对机器学习任务中的算法解决方案选择场景，我们提出了一种基于“隐式世界模型”的预测框架，利用“Verified Data Analysis Report”使LLM在不执行代码的情况下预测方案优劣，并构建了FORE AGENT采用“Predict-then-Verify”循环。在包含18,438对比较的自建语料库及MLE-bench上，通过Pairwise Accuracy、Beat Ratio和Speedup等指标验证了其有效性，实现了6倍加速及6%的性能提升。",
                    "summary_translation": "自主机器学习智能体彻底变革了科学发现，但它们仍受限于 Generate-Execute-Feedback paradigm（生成-执行-反馈范式）。先前的方法面临严重的 Execution Bottleneck（执行瓶颈），因为假设评估严格依赖于昂贵的物理执行。为了绕过这些物理约束，我们借鉴 World Models（世界模型）的思想，内化 execution priors（执行先验），用即时预测推理替代昂贵的运行时检查。在这项工作中，我们形式化定义了 Data-centric Solution Preference（以数据为中心的解偏好）任务，并构建了一个包含 18,438 个成对比较的综合语料库。我们证明，当以 Verified Data Analysis Report（验证过的数据分析报告）为提示时，LLMs（大语言模型）表现出显著的预测能力，达到了 61.5% 的准确率和鲁棒的置信度校准。最后，我们在 FOREAGENT 中实例化了该框架，这是一个采用 Predict-then-Verify loop（预测-验证循环）的智能体，实现了 6 倍的收敛加速，同时性能超过基于执行的基线 6%。我们的代码和数据集将很快在 https://github.com/zjunlp/predict-before-execute 公开。",
                    "inspiration_trace": "基于论文《Can We Predict Before Executing Machine Learning Agents?》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**——从“试错法”的效率困境出发**\n\n1.  **观察现状**：\n    *   当前的自主机器学习智能体（如 AIDE, AutoMind）在解决科学发现等复杂任务时，普遍遵循“生成-执行-反馈”的范式。\n    *   这种范式本质上是**暴力试错**：智能体生成代码 -> 在物理环境（如GPU集群）中运行数小时 -> 获得结果 -> 迭代。\n\n2.  **锁定核心瓶颈**：\n    *   **执行瓶颈**：物理执行极其昂贵且缓慢（例如在 MLE-Bench 上单次运行需9小时）。\n    *   **探索受限**：由于时间成本高昂，智能体无法广泛探索多样化的解决方案，只能进行有限的线性尝试。\n\n3.  **提出根本性问题**：\n    *   人类专家在编写代码前，会先在脑海中“模拟”算法的适用性，剔除明显不合适的方案。\n    *   **核心追问**：我们能否将“数小时的物理执行”压缩为“数秒的逻辑推理”？即，智能体能否在运行代码之前，就预测出哪个方案更好？\n\n---\n\n### 第二阶段：概念迁移与假设提出\n**——引入“世界模型”思想**\n\n1.  **跨域灵感**：\n    *   借鉴强化学习中的**世界模型**概念：智能体通过内部模拟环境动力学来预测行动结果，而非依赖外部真实交互。\n\n2.  **形成核心假设**：\n    *   大语言模型（LLM）是否可以充当机器学习任务中的**隐式世界模型**？\n    *   **假设**：LLM 不需要实际运行代码，仅通过“阅读”任务描述、数据特征和代码逻辑，就能通过推理预测出两个解决方案的相对优劣。\n\n---\n\n### 第三阶段：关键挑战与认知鸿沟\n**——发现“数据理解”的缺失**\n\n1.  **识别障碍**：\n    *   要预测代码好坏，光看代码逻辑是不够的，必须看**数据**（Data-centric）。\n    *   例如：一个复杂的深度学习模型在小样本数据上会过拟合，而简单的树模型可能表现更好。这种判断依赖于对数据分布的理解。\n\n2.  **直面 LLM 的局限性**：\n    *   LLM 存在**数值盲区**：直接将成千上万行的原始数据或统计日志喂给 LLM，它们无法有效处理，且容易产生幻觉。\n    *   **鸿沟**：原始数据的“数值空间”与 LLM 擅长的“语义空间”之间存在断层。\n\n---\n\n### 第四阶段：方法论构建与语义桥接\n**——从“数值”到“语义”的转化**\n\n1.  **任务形式化**：\n    *   将问题定义为**以数据为中心的解决方案偏好**任务。\n    *   不要求预测具体的准确率数值（太难），而是进行**成对比较**：给定方案 A 和方案 B，判断谁更好。\n\n2.  **核心创新：验证式数据分析报告**：\n    *   为了解决 LLM 看不懂数据的痛点，作者提出了一种**“语义化”策略**。\n    *   **逻辑**：不直接给数据，而是让 LLM 生成一段脚本来分析数据，然后将分析结果（统计特征）转化为**自然语言描述**。\n    *   **转化示例**：将“数据集包含5000条样本，类别分布极度不均”转化为一段关于“小样本与类别不平衡风险”的语义叙述。\n    *   **作用**：这相当于给 LLM 提供了一个“数据说明书”，使其能够基于数据特性进行逻辑推理，而非仅仅依赖代码复杂度（如“模型越大越好”的偏见）。\n\n3.  **构建验证基准**：\n    *   收集真实智能体的执行轨迹，构建包含 18,438 对比较的大规模数据集，用于验证上述假设。\n\n---\n\n### 第五阶段：系统验证与机制洞察\n**——证明“推理”替代“执行”的可行性**\n\n1.  **实验验证**：\n    *   实验证明，DeepSeek-V3.2 等推理能力强的模型在阅读了“数据报告”后，预测准确率达到 61.5%，显著优于随机猜测和基于代码复杂度的启发式规则。\n    *   **结论**：LLM 确实具备隐式世界模型的能力，能够通过语义理解捕捉算法与数据的匹配度。\n\n2.  **机制分析**：\n    *   发现**语义报告**是关键：仅提供代码或原始数字效果不佳，只有转化为语义叙述，LLM 的推理能力才被激活。\n    *   发现**置信度校准**：模型对自己判断的信心与实际准确率高度相关，这意味着可以用它来做“过滤器”。\n\n---\n\n### 第六阶段：最终应用与范式革新\n**——从“预测”到“智能体加速”**\n\n1.  **闭环整合**：\n    *   既然预测有效，就将其嵌入到智能体的工作流中。\n    *   提出 **FORE AGENT** 框架，将传统的“生成-执行-反馈”改造为**“预测-验证”循环**。\n\n2.  **逻辑流程**：\n    *   **并行生成**：一次性生成多个候选方案（不执行）。\n    *   **预测筛选**：利用上述的“隐式世界模型”在几秒钟内对所有方案进行推理打分，剔除低置信度的方案。\n    *   **物理验证**：仅对筛选出的 Top-k 方案进行昂贵的物理执行。\n\n3.  **最终收益**：\n    *   **解耦探索与执行**：用低成本的推理（秒级）替代高成本的执行（小时级）。\n    *   **结果**：实现了 6 倍的收敛加速，并在相同时间内探索了更广的搜索空间，最终性能提升了 +6%。\n\n---\n\n### 总结：逻辑演进全貌\n\n1.  **痛点**：物理执行太慢，限制了智能体的探索效率。\n2.  **灵感**：用 LLM 做“世界模型”，以推理代替执行。\n3.  **障碍**：LLM 读不懂原始数据，无法判断算法与数据的适配性。\n4.  **突破**：将数据统计特征转化为**语义报告**，激活 LLM 的逻辑推理能力。\n5.  **验证**：证明了 LLM 能基于语义报告准确预测方案优劣。\n6.  **落地**：构建“预测-验证”循环，用推理做过滤器，大幅提升智能体效率。"
                },
                {
                    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
                    "arxiv_id": "2601.05808",
                    "authors": "Xiaoshuai Song, Haofei Chang, Guanting Dong, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen",
                    "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于构建用于LLM智能体的工具交互环境，旨在通过程序化合成生成多样化的环境来训练智能体，提升其在复杂场景下的多轮、多工具交互能力，属于单智能体中的“工具使用”研究范畴。",
                    "summary2": "本文旨在解决LLM智能体训练中缺乏可扩展、高质量工具交互环境的问题。针对受限的真实系统和不可靠的模拟环境场景，我们提出了EnvScaler框架，通过SkelBuilder构建环境骨架，利用ScenGenerator生成任务场景及基于规则的验证函数。在Qwen3系列模型上应用SFT和RL训练后，于BFCL-v3 Multi-Turn、Tau-Bench和ACEBench-Agent基准上，通过Overall Score等指标验证了其显著提升模型解决复杂多轮多工具交互任务能力的有效性。",
                    "summary_translation": "人们期望将大语言模型 (LLMs) 训练为在各种现实世界环境中运作的智能体，但这一过程依赖于丰富多样的工具交互沙箱。然而，获取真实系统的权限通常受限；由 LLM 模拟的环境容易出现幻觉和不一致性问题；而手动构建的沙箱难以扩展规模。在本文中，我们提出了 EnvScaler，这是一个利用程序合成技术实现可扩展工具交互环境的自动化框架。EnvScaler 由两个组件组成。首先，SkelBuilder 通过主题挖掘、逻辑建模和质量评估构建多样化的环境骨架。随后，ScenGenerator 为每个环境生成多个任务场景以及基于规则的轨迹验证函数。借助 EnvScaler，我们合成了 191 个环境和约 7K 个场景，并将其应用于 Qwen3 系列模型的监督微调 (SFT) 和强化学习 (RL)。三个基准测试的结果表明，EnvScaler 显著提升了 LLM 在涉及多轮、多工具交互的复杂环境中解决任务的能力。我们在 https://github.com/RUC-NLPIR/EnvScaler 发布了我们的代码和数据。",
                    "inspiration_trace": "基于论文《EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体解决方案产出的思考过程。\n\n---\n\n### 1. 宏观观察：智能体的“数据饥渴”与环境瓶颈\n**思考起点：**\n作者首先观察到LLM智能体的发展趋势——从单纯的对话者转向能够执行复杂任务的行动者（如电商后台操作、航班改签等）。\n**核心矛盾：**\n要训练这样的智能体，必须让其在大量的、多样化的环境中进行交互学习（无论是模仿学习SFT还是强化学习RL）。\n**现实困境：**\n*   **真实环境：** 访问受限，隐私风险高，无法大规模获取。\n*   **模拟环境：** 现有的LLM模拟环境容易产生幻觉，状态不一致（即“前脚说有这个文件，后脚找不到了”）。\n*   **人工构建：** 虽然稳定，但成本极高，无法扩展。\n\n**初步结论：** 现有的环境供给方式无法满足智能体训练对“大规模、高质量、多样化”环境的需求。\n\n---\n\n### 2. 深入分析：寻找“一致性”与“可扩展性”的平衡点\n**对比分析：**\n作者对比了三种环境类型（Table 1），发现“程序化构建”是唯一能同时满足“可扩展、一致、可控、稳定”的路径。\n**现有方案的缺陷：**\n*   现有的程序化工作大多依赖于**先验知识**（如已有的API文档、已有的轨迹数据）。\n*   这意味着它们只能“复现”或“重组”已知的世界，无法创造全新的、未见过的环境，限制了智能体的泛化能力。\n\n**关键假设：** 如果我们能自动化地**从零合成**可执行的环境代码，而不是依赖现有的API或轨迹，就能打破数据来源的限制，实现真正的环境扩展。\n\n---\n\n### 3. 范式转移：从“LLM作为模拟器”到“LLM作为程序员”\n**逻辑跃迁：**\n既然LLM直接模拟环境状态容易产生幻觉（不可控），那么不如利用LLM强大的代码生成能力，让它编写**环境的逻辑代码**。\n**核心洞察：**\n*   **代码即规则：** Python代码是确定性的，执行逻辑是严谨的，这天然解决了LLM模拟时的“状态不一致”问题。\n*   **LLM作为架构师：** 让LLM去设计环境的状态空间、工具接口和业务逻辑，而不是直接模拟每一次交互的反馈。\n\n**方法论雏形：** 构建一个自动化流水线，输入是文本描述，输出是可执行的Python环境类。\n\n---\n\n### 4. 质量控制：解决“代码生成不可靠”的挑战\n**新问题：**\n虽然代码比文本模拟更严谨，但LLM生成的代码可能包含逻辑错误或Bug。如果环境本身是错的，智能体就会学到错误的策略。\n**解决方案构思：**\n需要一个自动化的“测试-验收”机制。\n**双智能体闭环：**\n*   **测试智能体：** 扮演“黑盒测试者”，随机或针对性地调用工具，试图找出环境漏洞（如输入非法参数、调用不存在的ID）。\n*   **检查智能体：** 扮演“代码审查员”，检查源代码、执行结果和状态变化，判断是否符合预期逻辑。\n\n**逻辑演进：** 通过这种对抗性的闭环测试，只有通过率高的环境才会被保留，从而保证了合成环境的质量。\n\n---\n\n### 5. 场景构建：从“空壳”到“实战”\n**进一步思考：**\n仅有环境代码（骨架）是不够的，智能体需要具体的任务和数据来训练。\n**数据生成的逻辑：**\n*   **状态先行：** 任务必须依赖于环境的具体状态。例如，不能“取消一个不存在的订单”。因此，必须先生成环境的初始状态数据。\n*   **任务反推：** 基于生成的初始状态和可用工具，设计出具有挑战性且可解的任务。\n*   **评估革新：** 传统的评估往往依赖与标准轨迹的匹配（死板）。作者提出基于**最终状态**的规则验证。只要最终环境状态符合规则（如订单状态变为“已取消”），无论中间用了什么工具，都算成功。这更符合真实世界的多解性。\n\n---\n\n### 6. 最终方法论形成：EnvScaler\n**逻辑闭环：**\n将上述思考整合为一个完整的自动化框架：\n1.  **SkelBuilder（骨架构建）：**\n    *   *挖掘：* 从现有任务中反推环境主题（解决“灵感来源”）。\n    *   *建模：* LLM编写环境代码（解决“一致性”）。\n    *   *评估：* 双智能体测试（解决“质量”）。\n2.  **ScenGenerator（场景生成）：**\n    *   *生成：* 生成初始状态和任务（解决“训练数据”）。\n    *   *验证：* 生成基于状态的检查函数（解决“评估灵活性”）。\n\n**总结：**\n作者的思考路径是从**“缺乏训练环境”**这一痛点出发，通过**“程序化合成”**解决一致性问题，通过**“双智能体测试”**解决代码质量问题，最后通过**“状态驱动”**的任务生成解决训练数据的实用性，最终形成了一套无需依赖真实系统即可无限扩展高质量训练环境的自动化方案。"
                },
                {
                    "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat",
                    "arxiv_id": "2601.05657",
                    "authors": "Hao Yang, Hongyuan Lu, Dingkang Yang, Wenliang Yang, Peng Sun, Xiaochuan Zhang, Jun Xiao, Kefan He, Wai Lam, Yang Liu, Xinhua Zeng",
                    "summary": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个具有分步决策能力的对话智能体，能够主动决定发送消息还是等待，并模拟思考时间，涉及单智能体的决策机制以及双智能体系统的交互，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有逐步式AI聊天系统缺乏主动等待机制及消息节奏不自然的问题。针对即时通讯社交聊天场景，我们提出了一种名为Stephanie2的逐步决策对话智能体，引入主动等待和消息节奏适应机制，将延迟建模为思考时间与打字时间之和。我们在基于Persona-Chat生成的伪对话数据上，通过自然度、参与度等指标及角色识别测试验证了其有效性。",
                    "summary_translation": "即时通讯中的人类社交聊天通常通过一系列短消息序列进行。现有的 step-by-step AI chatting systems（逐步式 AI 聊天系统）通常将 one-shot generation（一次性生成）拆分为多条消息并顺序发送，但它们缺乏 active waiting mechanism（主动等待机制），且表现出不自然的 message pacing（消息节奏）。为了解决这些问题，我们提出了 Stephanie2，一种新颖的下一代 step-wise decision-making dialogue agent（逐步决策对话代理）。通过 active waiting（主动等待）和 message-pace adaptation（消息节奏适应），Stephanie2 在每一步明确决定是发送还是等待，并将 latency（延迟）建模为 thinking time（思考时间）和 typing time（打字时间）的总和，从而实现更自然的节奏。我们进一步引入了一种 time-window-based dual-agent dialogue system（基于时间窗口的双代理对话系统），用于生成 pseudo dialogue histories（伪对话历史）以进行人工和自动评估。实验结果表明，Stephanie2 在 naturalness（自然度）和 engagement（参与度）等指标上明显优于 Stephanie1，并且在 role identification Turing test（角色识别图灵测试）的人工评估中获得了更高的通过率。",
                    "inspiration_trace": "基于论文《Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat》，以下是对作者核心方法产出逻辑链的系统推演：\n\n### 1. 宏观观察：从“单步生成”到“分步交互”的范式错位\n*   **现象**：现有的主流LLM对话系统遵循“单步范式”，即用户输入一句，AI回复一大段长文本。\n*   **现实**：人类的即时通讯（IM）社交是“分步范式”的——人们倾向于将一个想法拆解为多条短消息发送，并根据对方的反应实时调整措辞或话题。\n*   **初步结论**：为了模拟真实社交体验，AI必须从“生成一段长文本”转变为“生成一系列连续的短消息”。（这是前作Stephanie1的基础，也是本文的起点）。\n\n### 2. 问题诊断：Stephanie1的机械性与“失聪”\n作者在肯定前作Stephanie1（通过分隔符生成多段消息）的基础上，敏锐地发现了其依然存在的两个核心缺陷，这构成了本文的突破口：\n\n*   **缺陷一：缺乏“主动等待”机制**\n    *   *观察*：Stephanie1虽然把消息切短了，但它倾向于一股脑地把所有切好的消息发出去。\n    *   *后果*：当用户正在连续表达（如倾诉情绪、补充细节）时，AI往往会因为急于输出而打断用户，破坏了对话的自然流和情感连贯性。\n    *   *本质*：AI不懂“倾听”，它只知道“输出”。\n\n*   **缺陷二：消息节奏的建模过于简化**\n    *   *观察*：现有的分步系统通常仅根据消息长度（模拟打字速度）来计算发送延迟。\n    *   *后果*：短消息回得太快（显得轻率、像机器），长消息回得太慢（破坏对话流）。\n    *   *本质*：忽略了人类交流中的“思考时间”。在真实对话中，停顿往往代表思考，而不仅仅是打字耗时。\n\n### 3. 核心假设：对话即“决策”而非单纯的“生成”\n基于上述诊断，作者的思想发生了质的飞跃：**对话不应被视为文本生成的任务，而应被视为一系列微观决策的序列。**\n\n*   **假设**：一个拟人化的AI在每一步都应该面临一个二元选择：**“现在发送”** 还是 **“继续等待”**。\n*   **推论**：为了做出正确的选择，AI必须具备“认知”能力，即显式地思考当前的语境（对方说完了吗？我表达完整了吗？）。\n*   **延展**：既然引入了“思考”，那么“思考”本身应当消耗时间。因此，**延迟 = 思考时间 + 打字时间**，这样才能还原真实的对话节奏。\n\n### 4. 方法构建：Stephanie2的“思考-决策”闭环\n为了验证上述假设，作者构建了Stephanie2系统，其逻辑演进如下：\n\n*   **第一步：显式思维链**\n    *   强制模型在输出内容前，先输出一段 `"
                },
                {
                    "title": "GIFT: Games as Informal Training for Generalizable LLMs",
                    "arxiv_id": "2601.05633",
                    "authors": "Nuoyan Lyu, Bingbing Xu, Weihao Meng, Yige Yuan, Yang Zhang, Zhiyong Huang, Tat-Seng Chua, Huawei Shen",
                    "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "论文利用游戏（如矩阵博弈、井字棋、谁是卧底）作为环境，通过强化学习（GRPO）训练LLM的战略创造力和社会推理能力，涉及多智能体博弈与通过反馈自我完善，符合“多智能体：博弈”及“自我演化”的研究范围。",
                    "summary2": "本文旨在解决LLMs缺乏实践智慧及多任务训练中的性能退化问题。针对数学和游戏环境，我们提出了一种Nested Training Framework，将游戏作为非正式学习环境，通过顺序组合子任务将优化目标从“OR”转变为“AND”。我们在Qwen2.5模型上，通过MATH500、MMLU、CommonGen等基准验证了其有效性，显著提升了模型的泛化能力。",
                    "summary_translation": "虽然 Large Language Models (LLMs，大型语言模型) 在数学和代码生成等 formal learning tasks (形式化学习任务) 中取得了显著成就，但在人类认知所特有的“practical wisdom” (实践智慧) 和 generalizable intelligence (可泛化智能) —— 例如 strategic creativity (战略创造力) 和 social reasoning (社会推理) —— 方面仍面临挑战。这种差距源于缺乏 informal learning (非正式学习)，后者依赖于 interactive feedback (交互式反馈) 而非 goal-oriented instruction (目标导向型指令)。本文提出将 Games (游戏) 作为 LLM informal learning (非正式学习) 的主要环境，利用其 intrinsic reward signals (内在奖励信号) 和 abstracted complexity (抽象复杂性) 来培养多样化的 competencies (能力)。为解决 multi-task learning (多任务学习) 中观察到的 performance degradation (性能下降) 问题，我们引入了一种 Nested Training Framework (嵌套训练框架)。与优化隐式“OR”目标的 naive task mixing (朴素任务混合) 不同，我们的框架采用 sequential task composition (顺序任务组合) 来强制执行显式“AND”目标，迫使模型同时掌握多种能力以获得最大奖励。通过在 Matrix Games (矩阵博弈)、TicTacToe (井字棋) 和 Who's the Spy (谁是卧底) 游戏中使用基于 GRPO 的 reinforcement learning (强化学习)，我们证明了整合 game-based informal learning (基于游戏的非正式学习) 不仅能够防止 task interference (任务干扰)，还能显著增强模型在广泛的 ability-oriented benchmarks (能力导向型基准) 上的 generalization (泛化能力)。该框架和实现代码已公开可用。",
                    "inspiration_trace": "基于论文《GIFT: Games as Informal Training for Generalizable LLMs》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 1. 宏观观察：LLM 的“偏科”现象\n**思考起点：** 现有的 LLM 在数学、代码等“正式学习”任务上表现卓越，但在策略创造力、社会推理等体现“实践智慧”的通用智能上仍显不足。\n**核心洞察：** 人类智能源于“正式学习”与“非正式学习”的互补。正式学习侧重结构化知识，而非正式学习侧重在交互中通过反馈获取隐性知识。LLM 的短板在于缺乏后者——即缺乏在非结构化、交互式环境中通过试错来积累经验的能力。\n\n### 2. 假设提出：寻找 LLM 的“非正式学习”环境\n**问题转化：** 既然 LLM 缺乏非正式学习，那么什么环境适合作为 LLM 的非正式学习场所？\n**假设形成：** **游戏** 是最佳载体。\n**逻辑支撑：**\n*   **交互性：** 游戏通过内在规则和奖励信号提供反馈，无需人工标注数据。\n*   **抽象性：** 游戏是现实世界复杂交互的高度抽象（如博弈、规划、社交）。\n*   **多样性：** 不同类型的游戏可以对应不同的认知能力（如矩阵游戏对应抽象推理，多回合游戏对应规划，多人游戏对应心智理论）。\n**初步方案：** 将数学任务作为“正式学习”环境，将多种游戏作为“非正式学习”环境，结合训练。\n\n### 3. 问题识别：朴素多任务学习的“陷阱”\n**尝试与失败：** 作者尝试将数学与游戏任务进行简单的混合训练。\n**观察到的现象：** 这种“朴素混合”导致了性能退化，模型往往顾此失彼。\n**深度归因：**\n*   **优化视角：** 朴素混合实际上是在优化一个隐式的 **“OR” 目标**。只要模型在任意一个子任务上表现好，总奖励就会增加。\n*   **后果：** 模型会倾向于“偷懒”，专注于优化容易获得高奖励的单一任务，而忽略其他任务。这导致梯度信号被主导任务垄断，其他任务无法得到有效学习，最终损害了泛化能力。\n\n### 4. 方法创新：从“OR”到“AND”的逻辑重构\n**核心突破：** 如何强迫模型必须同时掌握所有能力，而不是只掌握其中之一？\n**概念转换：** 将优化目标从隐式的 **“OR”** 转换为显式的 **“AND”**。\n**具体方案：** 提出 **嵌套训练框架**。\n*   **机制：** 不再随机混合任务，而是将多个子任务按顺序串联成一个复合任务。\n*   **约束：** 模型只有在连续完成所有子任务（如：先解出数学题，再赢得游戏）时，才能获得最大奖励。\n*   **效果：** 这种结构迫使模型在一个轨迹中必须同时调用多种能力。部分成功无法满足目标，从而保持了更高的探索熵和梯度的稳定性，避免了单一任务的梯度主导。\n\n### 5. 逻辑闭环：通用智能的涌现\n**最终验证：** 通过这种“正式+非正式”结合且强制“AND”逻辑的嵌套训练，模型不仅在游戏和数学任务上表现良好，更重要的是在 MMLU、SocialIQA 等通用能力基准上取得了显著提升。\n**结论：** 游戏作为非正式学习环境是有效的，而嵌套训练框架解决了多任务干扰问题，二者结合成功赋予了 LLM 更强的泛化智能。"
                },
                {
                    "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation",
                    "arxiv_id": "2601.05548",
                    "authors": "Jeonghyun Kang, Hongjin Kim, Harksoo Kim",
                    "summary": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.",
                    "category": "cs.CL",
                    "filter_reason": "论文专注于长期对话系统中的记忆更新机制，提出了动态生成整合记忆的方法。记忆是LLM智能体的核心组件之一（属于单智能体研究范围中的“记忆”），该研究旨在提升智能体在长期交互中跟踪用户状态和情感语境的能力，符合筛选条件。",
                    "summary2": "本文旨在解决长期对话中记忆更新导致的信息冲突及缺乏情感理解的问题。针对多会话对话场景，我们提出了一种基于生成且融合情感与因果关系的KEEM数据集，通过动态生成整合性记忆来替代传统的操作式更新。我们在KEEM数据集上通过人工评估、关键词召回率及多种长期对话模型的Perplexity指标验证了其有效性。",
                    "summary_translation": "本文介绍了 Keep Emotional and Essential Memory (KEEM) 数据集，这是一个新颖的 generation-based dataset (基于生成的数据集)，旨在增强 long-term conversational systems (长期对话系统) 中的 memory updates (记忆更新)。与现有的依赖简单累积或 operation-based methods (基于操作的方法) 不同——这些方法往往导致 information conflicts (信息冲突) 并难以准确 tracking a user's current state (跟踪用户当前状态)——KEEM 能够动态生成 integrative memories (整合记忆)。这一过程不仅保留了 essential factual information (基本事实信息)，还融入了 emotional context (情感语境) 和 causal relationships (因果关系)，从而实现了对 user interactions (用户交互) 更 nuanced understanding (细致入微的理解)。通过利用 emotional and essential data (情感与基本数据) 无缝更新系统记忆，我们的方法促进了 deeper empathy (深层共情)，并增强了系统在 open-domain conversations (开放域对话) 中做出 meaningful response (有意义回应) 的能力。",
                    "inspiration_trace": "基于对论文《Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation》的深入分析，以下是作者产出该文章核心思想的逻辑演进过程推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“长期对话”的需求到“记忆管理”的瓶颈）**\n\n1.  **观察现象**：随着开放域对话系统的发展，多轮、跨会话的长期对话成为趋势。在真实的人类交互中，用户的状态（如健康状况、位置、情绪）是随时间动态变化的。\n2.  **识别核心矛盾**：现有的长期对话系统大多关注如何生成回复，却忽视了**记忆管理**。系统往往只是简单地累积信息，导致记忆库臃肿且充满过时信息，无法准确反映用户的“当前状态”。\n3.  **初步聚焦**：作者意识到，要实现真正像人类一样的长期对话，关键不在于“记住更多”，而在于“如何有效地更新记忆”。\n\n### 第二阶段：对现有范式的批判性分析\n**（从“简单操作”到“信息丢失”的反思）**\n\n1.  **批判“累积法”**：传统的做法是将新会话的摘要直接追加到旧记忆中。\n    *   *逻辑漏洞*：这会导致信息冲突（例如：记忆中既有“我在欧洲”，又有“我回韩国了”），且无法区分历史事实与当前状态。\n2.  **批判“操作法”**：以 CareCallmem 为代表的方法引入了 PASS, APPEND, DELETE, REPLACE 四种操作来处理新旧记忆的关系。\n    *   *逻辑漏洞*：作者发现这种非黑即白的操作会导致**关键信息的语义丢失**。\n        *   *案例反思*：当用户从“我在欧洲”变为“我回韩国了”，REPLACE 操作会删除“我在欧洲”这一历史事实；当用户从“感冒”变为“痊愈”，DELETE 操作会让系统彻底忘记用户曾生过病。\n3.  **形成假设**：记忆更新不应是简单的“选择”或“删除”，而应是**信息的融合与重构**。我们需要一种能保留“历史本质”同时反映“当前状态”的更新机制。\n\n### 第三阶段：概念跃迁——从“操作式”到“生成式”\n**（提出“生成式更新”的核心思想）**\n\n1.  **思想转变**：作者提出放弃基于标签的操作（如 REPLACE），转而采用**生成式**的方法。\n2.  **逻辑推演**：面对新旧记忆冲突，系统应具备理解能力，生成一个新的句子来整合两者。\n    *   *例子*：旧记忆“我在欧洲” + 新信息“我回韩国了” -> 生成新记忆“我之前去了欧洲，现在已经回韩国了”。\n3.  **确立核心优势**：这种方法既能消除冲突，又能保留用户经历的时间线完整性，从而更准确地追踪用户状态。\n\n### 第四阶段：维度的深化——引入“情感与因果”\n**（从“事实记忆”到“共情记忆”的扩展）**\n\n1.  **发现新缺口**：在分析现有数据集（如 MSC, CareCallmem）时，作者发现记忆内容多局限于客观事实摘要，缺乏情感维度。\n2.  **逻辑推演**：人类对话中的共情不仅需要知道用户“是什么情绪”，更需要知道“为什么产生这种情绪”。\n    *   *假设*：如果记忆中只包含“我很伤心”，系统只能给予泛泛的安慰；如果记忆包含“因为工作失误而感到羞愧”，系统就能提供更有针对性的建议。\n3.  **整合目标**：理想的记忆更新必须同时包含**情感**及其**因果原因**，以支持深度的认知共情。\n\n### 第五阶段：方法论构建与验证\n**（利用 LLM 构建 KEEM 数据集以验证假设）**\n\n1.  **数据策略**：由于缺乏现成的、包含情感因果且支持生成式更新的数据集，作者决定利用 ChatGPT-4 对现有的 KMSC 数据集进行重构。\n2.  **实施逻辑**：\n    *   **步骤一（情感注入）**：指令 LLM 从对话中提取情感及其原因，重写摘要，解决“情感缺失”问题。\n    *   **步骤二（生成式更新）**：指令 LLM 对比旧记忆与新摘要，生成整合后的新记忆，解决“操作式丢失”问题。\n3.  **闭环验证**：通过人工评估和模型下游任务测试（如 Perplexity、冲突率分析），证明这种“生成式+情感反思”的记忆更新方法，在信息保留量、冲突减少率和对话质量上均优于传统的累积法和操作法。\n\n---\n\n**总结：**\n作者的思考路径是从**长期对话的动态性需求**出发，通过批判现有方法导致的信息冲突与丢失，提出了**生成式更新**的范式转变；进而为了实现更深层次的共情，引入了**情感与因果**维度，最终通过构建 KEEM 数据集将这一方法论落地并验证其有效性。"
                },
                {
                    "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems",
                    "arxiv_id": "2601.05520",
                    "authors": "Xuemei Tang, Chengxi Yan, Jinghang Gu, Chu-Ren Huang",
                    "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个名为CHisAgent的多智能体LLM框架，包含Inducer、Expander和Enricher三个具有特定角色的智能体，它们通过协作（自底向上、自顶向下、证据引导）来完成分类法构建任务。这完全符合“多智能体：协作”的研究范围，且核心贡献在于智能体框架本身而非单纯的历史领域应用。",
                    "summary2": "本文旨在解决LLMs在非英语历史文化推理中能力有限及手动构建分类法成本高昂的问题。针对中国古代文化系统，我们提出了一种名为CHisAgent的多智能体LLM框架，通过Inducer、Expander和Enricher三个阶段协同构建事件分类法。我们在二十四史数据集上，通过Path Granularity、CSC、Coverage Rate及Node Recall等指标验证了其有效性，并展示了其在跨文化对齐中的优越性。",
                    "summary_translation": "尽管在众多任务上表现出色，但大型语言模型在历史与文化推理方面的能力仍然有限，特别是在中国历史等非英语语境中。分类体系结构为组织历史知识和提升理解提供了有效机制。然而，人工构建分类体系成本高昂且难以扩展。因此，我们提出了 \\textbf{CHisAgent}，一个面向中国古代语境历史分类体系构建的多智能体 LLM 框架。CHisAgent 将分类体系构建分解为三个角色专门化的阶段：自下而上的 *Inducer*（归纳器），负责从原始历史语料库中推导出初始层次结构；自上而下的 *Expander*（扩展器），利用 LLM 的世界知识引入缺失的中间概念；以及证据引导的 *Enricher*（丰富器），通过整合外部结构化历史资源来确保内容的忠实性。基于《二十四史》，我们构建了一个大规模的、领域感知的事件分类体系，涵盖了中国古代的政治、军事、外交和社会生活。广泛的无参考和有参考评估表明，该方法在结构连贯性和覆盖范围上均有提升，进一步分析显示，生成的分类体系能够支持跨文化对齐。",
                    "inspiration_trace": "基于论文《CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到方法论构建的思考过程：\n\n---\n\n### 1. 宏观问题观察：LLM的文化“失语”与知识结构化需求\n**思考起点：**\n作者首先观察到一个核心矛盾：尽管大语言模型（LLMs）在通用任务上表现优异，但在**历史与文化推理**方面存在显著局限，特别是在**非英语语境**（如中国古代史）中。\n*   **现象：** LLMs往往只能捕捉文本表面的模式，无法深入理解深层的文化结构和历史逻辑。\n*   **推论：** 单纯的文本建模不足以支撑历史理解。历史知识需要显性的结构化组织，而**分类体系**正是组织这种知识、提升模型理解能力的有效机制。\n\n### 2. 现实瓶颈：人工构建的不可行性与现有方法的局限\n**问题聚焦：**\n既然分类体系如此重要，为何目前缺乏大规模、系统化的中国古代历史分类法？\n*   **障碍：** 传统的分类体系构建高度依赖专家，成本高昂且难以扩展。\n*   **现有技术缺陷：** 虽然已有利用LLM自动构建分类法的研究，但作者发现它们存在两极分化的问题：\n    *   **纯自底向上：** 仅依赖语料聚类，虽然忠实于文本，但往往缺乏抽象的中间概念，结构松散。\n    *   **纯自顶向下：** 依赖模型先验知识生成，虽然结构连贯，但容易产生幻觉，且对特定历史语料的覆盖率不足。\n\n### 3. 核心假设：混合策略与多智能体协作\n**逻辑转折：**\n为了解决“忠实度”与“结构完整性”之间的矛盾，作者提出了一种**辩证的构建思路**：\n*   **假设：** 一个完美的历史分类体系应当同时具备**数据驱动的颗粒度**（来自原始文献）和**知识驱动的逻辑性**（来自专家/模型先验），并最终通过**外部证据**进行校验。\n*   **方法论选择：** 单一模型难以同时胜任这些相互冲突的任务。因此，必须采用**多智能体框架**，将复杂的构建任务分解为不同角色的协作流程。\n\n### 4. 方法论构建：三阶段逻辑闭环\n基于上述假设，作者设计了一个包含三个专门化阶段的演进逻辑，形成了CHisAgent框架：\n\n#### 第一阶段：归纳者—— 数据驱动的“基石”\n*   **思考：** 必须先从最原始的史料（《二十四史》）中挖掘真实存在的实体。\n*   **逻辑：** 采用**自底向上**策略。从海量历史文本中提取事件实例，通过聚类形成初步的层级。\n*   **目的：** 确保分类体系扎根于真实的历史语料，解决“空对空”的问题。\n\n#### 第二阶段：扩展者—— 知识驱动的“骨架”\n*   **思考：** 仅靠数据挖掘的体系往往存在“断层”，缺乏人类专家眼中的中间抽象概念（如从“战争”直接跳到“具体战役”，缺失了“战术”或“战略”等中间层）。\n*   **逻辑：** 引入**自顶向下**策略。利用LLM的世界知识和专家角色，识别并填补缺失的中间节点，修正层级结构。\n*   **目的：** 提升分类体系的结构连贯性和逻辑完整性。\n\n#### 第三阶段：丰富者—— 证据导向的“校验”\n*   **思考：** 扩展阶段虽然补全了结构，但可能引入了不符合历史事实的节点，或者遗漏了语料中隐含的重要事件。\n*   **逻辑：** 引入外部结构化知识（如CBDB人物数据库）和主题模型作为“证据源”。将高频事件、潜在主题和外部关系映射回分类树中。\n*   **目的：** 确保最终结果的**历史忠实度**和覆盖广度。\n\n### 5. 总结：从“单点突破”到“系统演进”\n**最终产出：**\n作者的思考过程并非简单的技术堆砌，而是针对历史领域特殊性（古汉语、文化特异性、时间跨度大）的定制化演进：\n1.  **发现问题：** LLM不懂历史深层逻辑。\n2.  **寻找抓手：** 用分类体系结构化知识。\n3.  **克服困难：** 人工太慢，单一AI方法太偏（要么太散，要么太假）。\n4.  **提出方案：** 用多智能体模拟人类专家的工作流——先**归纳**事实，再**演绎**逻辑，最后**考证**证据。\n\n这一逻辑链体现了作者将**数据挖掘、知识推理与事实校验**有机结合的系统性思维。"
                },
                {
                    "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse",
                    "arxiv_id": "2601.05505",
                    "authors": "Yubo Hou, Zhisheng Chen, Tao Wan, Zengchang Qin",
                    "summary": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了FlashMem框架，旨在解决LLM智能体在长期自主任务中缺乏动态上下文保存机制的问题，属于单智能体研究中的“记忆”范畴。虽然涉及推理延迟优化，但其核心在于通过计算重用提取内在记忆以增强智能体的持久认知能力，而非单纯的基础设施部署优化。",
                    "summary2": "本文旨在解决LLM无状态架构导致的历史信息冗余处理及现有潜在记忆方法的架构分离问题。针对长时程自主代理任务，我们提出了一种FlashMem框架，利用Shared-KV Consolidator直接复用主干网络的冻结缓存提取记忆，并通过基于注意熵的Cognitive Monitor自适应触发记忆整合。我们在GSM8K、MATH等六个基准数据集上，通过准确率、ROUGE-1及推理延迟验证了其有效性，结果显示其在匹配重型基线性能的同时，将推理延迟降低了5倍。",
                    "summary_translation": "大型语言模型 的 stateless architecture (无状态架构) 本质上缺乏保存动态上下文的机制，迫使智能体 冗余地重新处理历史记录以维持 long-horizon autonomy (长期自主性)。尽管 latent memory (潜在记忆) 提供了一种解决方案，但当前方法受限于 architectural segregation (架构分离)，依赖于将记忆与 reasoning backbone (推理骨干网络) 解耦的 auxiliary encoders (辅助编码器)。我们提出了 FlashMem，这是一个通过 computation reuse (计算复用) 直接从 transient reasoning states (瞬时推理状态) 中蒸馏 intrinsic memory (内在记忆) 的框架。利用 internal representations (内部表示) 唯一编码 input trajectories (输入轨迹) 的特性，FlashMem 将 last hidden state (最后一个隐藏状态) 识别为交互历史的 sufficient statistic (充分统计量)。这使得 Shared-KV Consolidator (共享键值整合器) 能够通过直接关注 backbone's frozen cache (骨干网络的冻结缓存) 来合成记忆，从而消除 redundant re-parameterization (冗余的重新参数化)。此外，一个无参数的 Cognitive Monitor (认知监视器) 利用 attention entropy (注意力熵)，仅在检测到高 epistemic uncertainty (认知不确定性) 时自适应地触发 consolidation (整合)。实验表明，FlashMem 在将 inference latency (推理延迟) 降低 5 倍的同时，达到了 heavy baselines (重型基线) 的性能，有效地弥合了效率与 persistent cognition (持久认知) 之间的差距。",
                    "inspiration_trace": "基于论文《FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观困境与现状反思\n**——从“无状态”架构的局限出发**\n\n1.  **观察现象**：现有的LLM本质上是“无状态”的，它们将输入映射到输出，但在交互之间不保留持久的内部状态。\n2.  **面临挑战**：对于需要长期自主性的智能体，这种无状态性导致了一个严重的瓶颈——为了维持上下文连贯性，智能体必须在每一步推理中**冗余地重新处理历史信息**。\n3.  **现有方案的局限**：虽然“潜在记忆”被提出作为解决方案（将上下文压缩为密集向量），但作者发现现有方法存在根本性的**结构低效**。它们通常采用“分离式架构”，即依赖独立的编码器或适配器来生成记忆，这与主推理骨干是割裂的。\n\n### 第二阶段：痛点诊断与核心假设\n**——识别“计算冗余”与“架构隔离”的根源**\n\n1.  **深入分析**：为什么现有的潜在记忆方案效率低下？作者意识到，这是因为它们引入了**辅助参数**来重新编码历史。\n2.  **逻辑推演**：\n    *   LLM在推理过程中已经计算过一次历史信息，这些信息蕴含在内部的KV Cache（键值缓存）和隐藏状态中。\n    *   现有方法却丢弃这些现成的计算结果，转而使用另一个独立的模块从头开始处理原始文本。这不仅是存储上的浪费，更是**计算上的重复**。\n3.  **提出核心假设**：**“内在性”假设**。LLM的内部表示（特别是最后一层的隐藏状态）已经唯一且充分地编码了输入轨迹。因此，我们不需要外部编码器，可以直接从模型现有的推理状态中“蒸馏”出记忆。\n\n### 第三阶段：范式转移——从“分离”到“内在”\n**——确立“计算复用”的设计哲学**\n\n1.  **思维转变**：从“如何设计一个更好的外部记忆编码器”转变为“如何直接复用骨干网络的计算成果”。\n2.  **理论支撑**：利用LLM表示的**单射性**，即输入轨迹与内部表示是一一对应的。这意味着**最后一个隐藏状态是交互历史的充分统计量**。\n3.  **方法论雏形**：提出**计算复用**的概念。记忆生成过程不应是一个独立的编码Pass，而应是一个直接读取骨干网络冻结KV Cache的“读取”操作。\n\n### 第四阶段：机制细化——何时记忆与如何记忆\n**——解决动态触发与轻量化实现的矛盾**\n\n1.  **子问题一：何时生成记忆？（动态触发）**\n    *   **思考**：并非每一步推理都需要记忆，频繁生成会带来巨大开销。我们需要一个“认知监控器”。\n    *   **洞察**：模型的不确定性与注意力机制的熵高度相关。当模型困惑时，注意力分布趋于分散（高熵）。\n    *   **方案**：设计一个**无参数的认知监控器**，基于注意力熵来实时检测模型的“认知困惑”。只有当熵超过阈值（即模型不确定时）才触发记忆固化，避免在简单问题上浪费算力。\n\n2.  **子问题二：如何高效生成记忆？（轻量化读取）**\n    *   **思考**：既然要复用KV Cache，那么记忆生成模块就不应该有庞大的参数。\n    *   **方案**：设计**共享KV整合器**。\n        *   **输入**：直接使用骨干网络当前的隐藏状态作为初始Query。\n        *   **操作**：通过交叉注意力机制，直接对骨干网络的冻结KV Cache进行查询。\n        *   **去重**：摒弃传统的Key/Value投影矩阵，只保留Query的投影，实现极低的参数开销。\n\n### 第五阶段：逻辑闭环与系统成型\n**——FlashMem框架的最终确立**\n\n1.  **整合逻辑**：\n    *   **感知层**：利用注意力熵监控模型的不确定性，决定“何时”介入。\n    *   **提取层**：利用Shared-KV Consolidator，直接从骨干网络的现有状态中提取信息，解决“如何”高效提取。\n    *   **反馈层**：生成的潜在记忆向量被软注入回骨干网络的输入流，作为高密度的认知线索。\n2.  **最终愿景**：FlashMem不再是一个外挂的辅助系统，而是一个与骨干网络深度耦合的**内在记忆机制**。它消除了架构隔离，通过复用计算资源，在保持高性能推理的同时，实现了极低的推理延迟（5倍提升）。\n\n---\n\n**总结**：作者的思考路径是从**“无状态架构的缺陷”**出发，通过批判**“现有分离式架构的冗余”**，提出了**“内在记忆与计算复用”**的核心假设，并最终通过**“熵触发机制”**和**“共享KV设计”**将这一假设落地为一个高效、轻量的智能体记忆框架。"
                },
                {
                    "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards",
                    "arxiv_id": "2601.05488",
                    "authors": "Zhiyu Shen, Ziming Wu, Fuming Lai, Shaobing Lian, Yanghui Rao",
                    "summary": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的核心组件——长期记忆构建。它提出利用强化学习框架来训练模型构建多维记忆，属于单智能体研究中的“记忆”范畴，且涉及通过反馈进行自我完善，符合筛选标准。",
                    "summary2": "本文旨在解决LLM在长期对话中保持一致性的挑战。针对长期对话场景，我们提出MemBuilder强化学习框架，利用Attributed Dense Rewards Policy Optimization (ADRPO) 优化多维记忆构建。我们在LoCoMo、LongMemEval和PerLTQA数据集上通过QA准确率验证了其有效性，使4B参数模型超越了SOTA闭源基线。",
                    "summary_translation": "在长期对话中保持一致性仍是 LLMs (Large Language Models，大语言模型) 面临的一项基本挑战，因为标准的 retrieval mechanisms (检索机制) 往往无法捕捉 historical states (历史状态) 的 temporal evolution (时间演变)。尽管 memory-augmented frameworks (记忆增强框架) 提供了一种结构化的替代方案，但现有系统要么依赖于对 closed-source models (闭源模型) 的 static prompting (静态提示)，要么受困于 sparse rewards (稀疏奖励) 导致的低效 training paradigms (训练范式)。我们提出了 MemBuilder，这是一个 reinforcement learning (强化学习) 框架，旨在训练模型利用 attributed dense rewards (归因密集奖励) 来 orchestrate (编排) multi-dimensional memory construction (多维记忆构建)。MemBuilder 解决了两个关键挑战：(1) Sparse Trajectory-Level Rewards (稀疏轨迹级奖励)：我们采用 synthetic session-level question generation (合成会话级问题生成) 来在 extended trajectories (扩展轨迹) 中提供 dense intermediate rewards (密集中间奖励)；(2) Multi-Dimensional Memory Attribution (多维记忆归因)：我们引入了 contribution-aware gradient weighting (贡献感知梯度加权)，根据每个组件的 downstream impact (下游影响) 来缩放 policy updates (策略更新)。实验结果表明，MemBuilder 能够使一个 4B-parameter model (40亿参数模型) 的性能超越 state-of-the-art (SOTA，最先进的) closed-source baselines (闭源基线)，并在 long-term dialogue benchmarks (长期对话基准) 上展现出强大的 generalization (泛化) 能力。",
                    "inspiration_trace": "基于论文《MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards》，以下是对作者核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察：现有记忆机制的“盲点”\n**起点：** 长期对话的一致性是LLM的核心痛点。\n**观察：**\n*   **RAG的局限：** 传统的检索增强生成（RAG）将信息视为静态、独立的切片，无法捕捉信息的“时间演化”（例如用户喜好的改变）。\n*   **Prompting框架的局限：** 现有的记忆增强框架（如MemGPT, Mem0）主要依赖静态提示词和昂贵的闭源模型。它们处于“开环”状态——只管写入记忆，却不知道这些记忆是否真的对下游任务有帮助。\n\n### 2. 问题聚焦：从“开环”到“闭环”的挑战\n**假设：** 能否训练一个轻量级模型，通过直接监督来构建记忆，从而替代昂贵的闭源模型？\n**现状分析：** 虽然已有尝试（如Memory-R1, Mem-α）使用强化学习（RL）来训练记忆构建，但效果不佳。作者诊断出两个核心瓶颈：\n*   **瓶颈一：奖励过于稀疏。** 在跨越数十个会话的长期对话中，仅在轨迹末端给出一个奖励，模型无法分辨是哪一个会话的记忆操作导致了最终的成功或失败，导致梯度更新噪声极大。\n*   **瓶颈二：归因过于粗糙。** 记忆通常是多维度的（如核心记忆、情景记忆）。现有方法对所有维度的记忆操作共享一个全局奖励，无法区分是哪一类记忆对回答问题做出了实际贡献。\n\n### 3. 核心思路：将“模糊反馈”转化为“精准信号”\n为了解决上述瓶颈，作者提出了**“归因化密集奖励”**的思路，逻辑演进如下：\n\n#### 3.1 解决稀疏性：从“终点奖励”到“过程奖励”\n**思考：** 既然无法在真实对话中获得每一步的反馈，能否“模拟”反馈？\n**方案：** 引入**合成会话级问答**。\n*   在每个会话结束后，利用专家模型基于当前会话和历史记忆生成一组QA对。\n*   立即用这些QA对测试当前构建的记忆质量。\n*   **逻辑：** 这样就将原本在对话结束才给出的稀疏奖励，变成了每个会话都能获得的密集奖励，极大加快了学习收敛速度。\n\n#### 3.2 解决归因性：从“全局平均”到“贡献加权”\n**思考：** 既然不同类型的记忆（如情景记忆 vs 语义记忆）在回答问题时的作用不同，奖励信号也应有所区分。\n**方案：** 引入**贡献感知的梯度加权**。\n*   在计算奖励时，记录回答问题过程中检索到了哪一类记忆。\n*   如果某类记忆被频繁检索并用于正确回答，那么该类记忆对应的操作策略应获得更强的梯度更新。\n*   **逻辑：** 这解决了“多任务共享奖励”时的信用分配难题，让模型学会优先优化那些真正有用的记忆维度。\n\n### 4. 方法论落地：ADRPO框架\n基于上述思考，作者构建了完整的训练流程：\n\n1.  **架构设计：** 采用多维记忆架构（Core, Episodic, Semantic, Procedural），为精细化的归因提供物理基础。\n2.  **冷启动（SFT）：** 利用专家模型（Claude）收集轨迹进行监督微调，先教会模型“怎么写”（格式正确），解决RL探索初期的无效动作问题。\n3.  **强化学习（ADRPO）：**\n    *   利用**合成QA**提供密集的会话级奖励。\n    *   利用**检索计数**对梯度进行加权，实现归因化更新。\n    *   最终目标：让模型学会构建能够最大化下游QA效用的记忆。\n\n### 5. 总结：逻辑链全景\n**发现问题**（现有记忆系统昂贵且盲目） $\\rightarrow$ **提出假设**（用RL训练小模型构建记忆） $\\rightarrow$ **诊断失败**（现有RL奖励太稀疏、归因太粗糙） $\\rightarrow$ **提出解法**（合成QA实现密集奖励 + 检索统计实现归因加权） $\\rightarrow$ **验证效果**（小模型超越大模型Prompting方案）。\n\n这就是作者从观察到方法论的完整思考路径。"
                },
                {
                    "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models",
                    "arxiv_id": "2601.05366",
                    "authors": "Zheng Luo, T Pranav Kutralingam, Ogochukwu N Okoani, Wanpeng Xu, Hua Wei, Xiyang Hu",
                    "summary": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.",
                    "category": "cs.CL",
                    "filter_reason": "该论文研究了大语言模型作为智能体调用外部工具的能力，属于单智能体研究中的“工具使用”范畴。虽然涉及多语言鲁棒性，但其核心是评估和改进智能体的工具调用能力，而非纯应用或纯推理。",
                    "summary2": "本文旨在解决大型语言模型在多语言场景下工具调用的鲁棒性问题。针对多语言用户查询与英语执行接口的冲突，我们提出了MLCL诊断基准，通过控制查询语言组成和语义扰动来隔离执行级错误。我们在MLCL数据集上，通过细粒度错误分类法验证了参数值语言不匹配是主要失败模式，并评估了推理时缓解策略的有效性。",
                    "summary_translation": "大语言模型正日益被部署为智能体，通过结构化函数调用（structured function calls）来调用外部工具。尽管近期的研究报告显示，在以英语为中心的标准评估中，模型表现出了强大的工具调用（tool-calling）能力，但在多语言用户交互场景下，工具调用的鲁棒性（robustness）仍有待探索。在本研究中，我们引入了 MLCL（一个诊断基准），并对中文、印地语以及低资源语言伊博语（Igbo）的多语言工具调用进行了系统性评估。通过细粒度的错误分析，我们发现即便模型正确理解了意图并选择了工具，仍会出现许多失败情况。我们将参数值语言不匹配（parameter value language mismatch）确定为主要的一种失败模式，即模型虽然生成了语义上恰当的参数值，但这些值使用的是用户的语言，从而违反了语言不变的执行约定（language-invariant execution conventions）。我们进一步评估了几种推理时系统策略（inference-time system strategies），结果发现，尽管这些策略显著减少了由语言引起的执行错误，但没有任何一种策略能够完全恢复到英语水平的性能。",
                    "inspiration_trace": "基于论文《Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models》的内容，以下是对作者产出该文章核心方法论的逻辑链推演：\n\n### 1. 宏观背景与问题切入\n**逻辑起点：** LLM正在从单纯的对话系统演变为能够调用外部工具的智能体。\n**观察现状：** 现有的工具调用评估主要在英语环境下进行，且表现优异。然而，现实世界的用户是多语言的，而底层的工具接口（API）通常是语言无关的（通常是英语）。\n**提出问题：** 当用户使用非英语与LLM交互时，LLM的工具调用能力是否依然稳健？这种跨语言的交互是否会引入新的、未被现有基准测试发现的失败模式？\n\n### 2. 现象观察与初步假设\n**深入观察：** 作者通过初步测试发现，当用户输入中文、印地语或伊博语时，模型经常生成“语义正确但无法执行”的工具调用。\n**典型案例：** 用户问“纽约的天气”，模型理解了意图，选择了正确的函数，但在参数`location`中填入了“纽约市”（中文）而非“New York City”。\n**形成假设：** 这种失败并非源于模型“没听懂”（语义理解失败），而是源于模型“说错了话”（执行层面的语言不匹配）。模型倾向于模仿用户的语言来生成参数值，从而违反了底层代码要求英语参数的硬性约束。\n\n### 3. 核心概念定义\n**概念提炼：** 作者将这种失败模式定义为**“参数值语言不匹配”**。\n**逻辑推演：** 如果假设成立，那么只要解决了语言格式问题，模型的工具调用表现应该就能恢复。这意味着，多语言工具调用的瓶颈可能不在于模型的跨语言推理能力，而在于自然语言与程序化接口之间的对齐问题。\n\n### 4. 诊断性基准的设计\n为了验证上述假设，作者需要剥离干扰变量，设计一个受控的实验环境（即MLCL基准）。\n\n*   **变量控制：** 保持工具接口（函数名、参数定义）严格为英语，仅改变用户查询的语言。\n*   **维度一：语言构成**\n    *   *设计思路：* 为了区分“理解能力”和“格式习惯”，作者引入了“部分翻译”。\n    *   *逻辑：* 如果在部分翻译（保留关键参数为英文）的情况下，模型表现恢复，就证明模型理解了非英语指令，只是在全翻译环境下习惯性地复制了用户的语言。\n*   **维度二：语义扰动**\n    *   *设计思路：* 引入改写和同义词替换。\n    *   *逻辑：* 测试模型对表面形式变化的鲁棒性，观察在严格匹配要求下，语义噪声是否会加剧执行错误。\n*   **语言选择：** 选取中文（高资源）、印地语（中资源）、伊博语（低资源），以探究不同语系和资源条件下的表现差异。\n\n### 5. 实验验证与归因分析\n**执行验证：** 在MLCL基准上测试多个主流模型。\n**结果分析：**\n*   **全翻译（FT）导致错误激增：** 证实了“参数值语言不匹配”是主导错误模式。\n*   **部分翻译（PAR）显著改善：** 证实了模型确实具备跨语言意图理解能力，失败主要发生在“语言-执行”边界。\n*   **低资源语言（如伊博语）的特殊性：** 发现模型较少直接复制低资源语言词汇（可能因为训练数据少），因此语言不匹配错误少，但语义理解错误多。这进一步细化了结论：高资源语言的失败主要是“接口规范”问题，低资源语言则包含“理解能力”问题。\n\n### 6. 解决方案探索与反思\n**尝试修复：** 既然问题是语言不匹配，能否通过简单的推理时策略解决？\n**策略测试：**\n*   **提示词干预：** 明确要求输出英语参数。\n*   **预翻译：** 先把用户问话翻译成英语再调用工具。\n*   **后翻译：** 生成参数后再翻译回英语。\n**发现局限：** 虽然这些策略能减少语言不匹配，但无法完全恢复到英语水平。原因在于翻译过程会引入“语义漂移”（如“Queen-size bed”被翻译成“King-size bed”），导致新的执行错误。\n\n### 7. 最终结论与贡献\n**逻辑闭环：** 作者得出结论，多语言工具调用的鲁棒性不仅仅是一个模型训练问题，更是一个**系统级设计挑战**。\n**核心产出：**\n1.  揭示了“参数值语言不匹配”这一被忽视的失败模式。\n2.  提出了MLCL这一诊断性基准，将语义理解错误与执行接口错误解耦。\n3.  指出现有的轻量级修复方案存在权衡，未来的Agent系统需要在自然语言交互与代码执行规范之间做更深层的对齐。"
                },
                {
                    "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization",
                    "arxiv_id": "2601.05475",
                    "authors": "Jiefu Ou, Sapana Chaudhary, Kaj Bostrom, Nathaniel Weir, Shuai Zhang, Huzefa Rangwala, George Karypis",
                    "summary": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于强化学习搜索框架的代码优化方法，核心在于LLM通过执行反馈进行迭代细化和自我完善。其中集成了自然语言批判模型进行自我反思，并利用推理时搜索算法进行规划，符合单智能体中关于工具使用、自我反思和自我演化的定义。",
                    "summary2": "本文旨在解决LLM在代码优化中面临的复杂性与性能指标解读难题。针对CUDA和C++代码优化场景，我们提出了一种MaxCode最大奖励强化学习框架，引入自然语言评论模型和最佳折扣奖励以增强观察空间，并在KernelBench和PIE基准上通过绝对加速比和相对加速排名验证了其有效性。",
                    "summary_translation": "大型语言模型在通用编码任务中展现出强大的能力，但在代码优化方面面临两个关键挑战：（i）编写优化代码（例如高性能 CUDA 内核和竞赛级 CPU 代码）的复杂性，需要具备系统、算法及特定语言的专业知识；（ii）除了二进制正确性之外，还需要对执行时间和设备利用率等性能指标进行解读。在本研究中，我们探索了推理时搜索算法，该算法引导 LLM 基于执行反馈通过迭代改进来发现更优的解决方案。我们的方法 MaxCode 将现有的搜索方法统一在最大奖励强化学习框架下，使得观测函数和动作价值函数模块化，便于进行修改。为了增强观测空间，我们集成了一个自然语言评论模型，将原始执行反馈转化为关于错误和性能瓶颈的诊断性洞察，以及迄今为止观测到的最佳折扣奖励。这些信息共同为代码提议函数提供了更丰富的输入。为了改善搜索过程中的探索能力，我们利用推演产生的动作值训练了一个生成性回报模型，以对潜在解决方案进行重排序。在 KernelBench (CUDA) 和 PIE (C++) 优化基准上的测试表明，与基线相比，MaxCode 提升了优化代码的性能，在绝对加速比和相对加速排名上分别实现了 20.3% 和 10.1% 的相对提升。",
                    "inspiration_trace": "基于论文《MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization》，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观视角：代码优化的特殊性\n**起点：** 作者首先观察到，虽然大语言模型（LLM）在通用代码生成上表现出色，但在**代码优化**（如编写高性能CUDA内核或竞赛级C++代码）这一特定任务上仍面临巨大挑战。\n**核心矛盾：**\n*   **高门槛：** 优化代码需要深厚的系统级知识（算法权衡、内存访问模式、硬件架构），这超出了通用LLM的常规训练分布。\n*   **模糊的反馈：** 与“代码能否运行”这种二元反馈不同，优化任务关注的是“运行时间”或“资源利用率”。原始的性能指标（如“慢了20%”）是非诊断性的，它无法告诉LLM*为什么*慢以及*如何*改进。\n\n### 2. 痛点分析：现有搜索方法的局限\n**观察：** 现有的基于推理时搜索的方法（如迭代优化、基于执行反馈的修正）虽然有效，但存在两个根本性的逻辑缺陷：\n*   **目标错位：** 传统的强化学习（RL）通常最大化“累积奖励”。但在代码优化中，我们只关心**最终找到的那个最好解**（Max Reward），中间过程的累积性能毫无意义。如果找到了一个极快的解，即使之前尝试了很多次失败的解，结果也是成功的。\n*   **信息利用率低：** 现有的搜索算法往往只将上一步的执行结果作为输入，缺乏对历史轨迹的深度利用，且难以理解复杂的性能指标。\n\n### 3. 理论重构：从“累积奖励”到“最大奖励”\n**假设：** 如果将代码优化过程重新定义为一个**最大奖励强化学习**问题，而不是标准RL，就能更准确地匹配优化任务的目标。\n**逻辑推演：**\n*   在标准RL中，Agent试图最大化长期回报的总和。\n*   在代码优化中，Agent应该最大化**轨迹中出现的最大奖励**。\n*   **推论：** 为了保持马尔可夫性质，状态空间必须显式包含一个辅助变量 $u$，代表**“迄今为止见到的最大折扣奖励”**。这样，LLM在生成新代码时，就能明确知道“目前的最好成绩是多少”，从而以此为基准进行超越。\n\n### 4. 信息增强：从“数值反馈”到“自然语言诊断”\n**问题：** 即使有了“最大奖励”的概念，LLM面对原始的执行反馈（如编译错误日志或具体的运行时间）仍然难以直接转化为有效的代码修改策略。\n**灵感：** 借鉴“自我反思”和“自然语言批评”的研究。\n**解决方案：** 引入一个**批评模型**。\n*   **作用：** 将原始的执行反馈（数字、错误码）转化为**自然语言的诊断洞察**（例如：“内存带宽是瓶颈”或“存在线程同步问题”）。\n*   **逻辑：** 这种“翻译”过程极大地丰富了观察空间，将冷冰冰的指标变成了LLM可以理解并据此采取行动的“建议”。\n\n### 5. 效率优化：引入价值预测模型\n**新挑战：** 代码优化的评估成本极高（需要编译、运行、测试）。在有限的计算预算下，无法无限制地探索所有可能的代码变体。\n**假设：** 如果能训练一个模型来预测某个搜索路径的“潜力”，就可以提前剪枝，避免在无希望的分支上浪费计算资源。\n**方法：** 训练一个**生成式价值/奖励预测模型**。\n*   **逻辑：** 该模型预测在当前状态下，未来能获得的最大奖励是多少。\n*   **应用：** 在搜索过程中，先生成多个候选代码，先用价值模型筛选出最有希望的几个，再送去执行环境评估。这实现了“以小博大”，提高了搜索效率。\n\n### 6. 最终框架：MaxCode 的统一\n**综合：** 作者将上述思考整合为一个统一的框架——MaxCode。\n*   **形式化：** 定义了包含初始代码、当前代码、执行反馈、自然语言批评以及历史最佳奖励的MDP。\n*   **算子化：** 提出了“最大奖励推理算子”，将现有的搜索方法（如Effi-Learner, CUDA-LLM）统一在这个框架下进行重写。\n*   **闭环：** 通过“批评”增强理解，通过“最大奖励”明确目标，通过“价值模型”提升效率。\n\n**总结：**\n作者的思考路径是从**任务的特殊性**（优化难、反馈模糊）出发，通过**理论视角的转换**（Max-Reward RL）重新定义目标，利用**自然语言作为中间媒介**解决理解难题，最后引入**学习型价值函数**解决计算成本问题，从而构建出一套完整的代码优化方法论。"
                },
                {
                    "title": "Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring",
                    "arxiv_id": "2601.05256",
                    "authors": "Eirini Baltzi, Tilemachos Moumouris, Athena Psalta, Vasileios Tsironis, Konstantinos Karantzalos",
                    "summary": "Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了 NAIAD 系统，明确描述了其作为“agentic AI assistant”的架构，涉及 LLM 推理、外部工具编排和智能体反思，符合单智能体中“工具使用”和“自我反思”的研究范围。尽管应用于内陆水监测领域，但其核心贡献在于智能体系统的设计与实现，而非单纯的应用效果展示。",
                    "summary2": "本文旨在解决内陆水域监测中现有方法缺乏集成且对非专家不友好的问题。针对内陆水域质量监测场景，我们提出了一种名为NAIAD的Agentic AI系统，利用LLMs和RAG技术，通过动态构建DAGs来编排外部工具。在希腊三个湖泊构建的数据集上，通过正确率和输出相关性验证了其有效性，结果显示Qwen2.5模型表现最佳。",
                    "summary_translation": "内陆水体监测对于保障公共健康和生态系统至关重要，能够实现及时干预以降低风险。现有方法通常分别解决孤立的子问题，如蓝藻、叶绿素或其他水质指标。NAIAD 提出了一种智能体 AI 助手，利用大语言模型和外部分析工具，基于对地观测数据为内陆水体监测提供整体解决方案。NAIAD 专为专家和非专家设计，提供了一个单指令界面，能够将自然语言查询转化为可执行的洞察。通过检索增强生成、大语言模型推理、外部工具编排、计算图执行和智能体反思，该系统从精选数据源中检索并综合知识，以生成定制化报告。该系统集成了多种工具，用于处理气象数据、Sentinel-2 影像、遥感指数计算（如 NDCI）、叶绿素a 估算，并集成了 CyFi 等成熟平台。性能评估使用了正确性和相关性指标，在涵盖多种用户专业水平的专用基准测试中，分别达到了 77% 和 85% 以上。初步结果显示了该系统在不同查询类型下具有很强的适应性和鲁棒性。针对大语言模型基座的消融实验进一步表明，Gemma 3 (27B) 和 Qwen 2.5 (14B) 在计算效率和推理性能之间提供了最佳平衡。",
                    "inspiration_trace": "基于论文《NAIAD: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring》的内容，以下是对作者构建该系统核心思想的逻辑链推演：\n\n### 1. 宏观观察与问题定义\n**逻辑起点：环境监测的紧迫性与技术落地的脱节**\n*   **观察**：内陆水体（湖泊、河流）对生态系统和公共健康至关重要，面临富营养化、藻华等威胁。\n*   **现状**：虽然已有大量技术手段（如卫星遥感、原位传感器、AI模型），但它们通常是**碎片化**的。\n*   **核心矛盾**：现有的AI解决方案多针对单一任务（如只测叶绿素、只测浊度）。要获得全面的水质评估，专家必须手动整合多个孤立工具的结果。\n*   **痛点**：对于非专家（环境从业者、决策者），技术门槛过高，缺乏一个能够将复杂EO（对地观测）数据转化为直观决策支持的统一入口。\n\n### 2. 假设提出与范式转移\n**逻辑推演：从“专用工具”到“通用智能助手”**\n*   **假设**：如果能够利用大语言模型（LLM）强大的推理与自然语言理解能力，是否可以构建一个“中间人”或“代理人”，自动理解用户意图并调度这些碎片化工具？\n*   **范式转移**：从传统的“用户手动操作特定软件”转向“用户自然语言提问，系统自动执行复杂工作流”。\n*   **目标定位**：构建一个**通用目的**的内陆水监测系统，而非单一功能的模型。\n\n### 3. 关键挑战的识别\n**逻辑深化：如何让LLM“靠谱”地执行科学任务？**\n*   **挑战一：领域知识缺失**。通用LLM不懂遥感指数（如NDCI）或特定水体的生化特性。\n    *   *应对思路*：引入**检索增强生成（RAG）**，将专业知识库注入LLM，确保其理解的专业性。\n*   **挑战二：工具调度的逻辑性**。水质分析往往涉及多步骤依赖（例如：必须先下载卫星图像 -> 计算光谱指数 -> 估算叶绿素 -> 结合气象数据 -> 生成报告）。简单的线性推理容易出错。\n    *   *应对思路*：需要一种结构化的方式来表示任务流程，确保步骤之间的依赖关系清晰。\n\n### 4. 核心创新点的诞生\n**逻辑突破：动态计算图（DAG）的引入**\n*   **灵感来源**：现有的Agent框架要么是多智能体协作（过于复杂），要么是简单的链式调用（缺乏灵活性）。\n*   **核心思想**：将用户的查询转化为一个**有向无环图（DAG）**。\n    *   **节点**：代表具体的工具（如Sentinel-2下载、NDCI计算、天气API）。\n    *   **边**：代表数据流向和依赖关系（例如，NDCI计算节点的输入必须来自图像下载节点的输出）。\n*   **优势**：LLM不再只是生成文本，而是充当“编译器”，根据查询动态“编译”出一张执行蓝图。这不仅保证了逻辑的正确性，还提供了可解释性和容错能力（如节点重试）。\n\n### 5. 方法论的系统化构建\n**逻辑整合：单智能体架构与反思机制**\n*   **架构决策**：选择**单智能体**架构而非多智能体。作者认为，对于内陆水监测这一特定垂直领域，一个具备强大工具编排能力的单智能体比多智能体系统更高效、更易于部署和维护。\n*   **流程闭环**：\n    1.  **理解**：用户输入自然语言 -> LLM重写查询。\n    2.  **规划**：利用RAG获取背景知识 -> LLM构建DAG（规划工作流）。\n    3.  **执行**：按DAG顺序调用外部工具（卫星、气象、模型）。\n    4.  **反思**：引入**Agentic Reflection**机制，系统自我审查输出结果，修正偏差，确保报告的相关性和准确性。\n*   **工具生态**：集成异构工具（Sentinel-2数据、气象API、CyFi预测平台），通过统一的元数据描述，使LLM能够灵活调用。\n\n### 6. 验证与迭代\n**逻辑验证：从模拟到现实的跨越**\n*   **评估策略**：不同于以往仅做模拟，作者强调在真实地理环境（希腊三个湖泊）上进行测试。\n*   **模型选择**：通过消融研究，在开源模型（如Qwen2.5, Gemma 3）中寻找“推理能力”与“计算成本”的最佳平衡点，证明了该架构不依赖于昂贵的闭源模型（如GPT-4），具有实际部署的可行性。\n\n---\n\n**总结：作者的思考路径**\n从**“监测手段碎片化”**的现实痛点出发，提出**“LLM作为统一调度中枢”**的假设。为了解决科学计算中复杂的**逻辑依赖问题**，创新性地引入**动态DAG构建机制**，并结合**RAG**与**反思机制**确保专业性与准确性，最终形成了一个**垂直领域、可落地、高鲁棒性**的智能体系统。"
                },
                {
                    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
                    "arxiv_id": "2601.07782",
                    "authors": "Wei Fang, James Glass",
                    "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的**工具使用**和**规划**能力。它提出了TOOLQP框架，通过将指令分解为子任务并动态生成查询来改进工具检索，直接属于单智能体的研究范围。",
                    "summary2": "本文旨在解决大规模动态工具库中复杂请求检索困难的问题。针对用户意图与工具文档间的语义鸿沟及组合性挑战，我们提出了一种TOOL QP框架，将检索建模为迭代查询规划过程，通过任务分解和动态查询生成与检索器交互。在ToolRet基准测试上，通过nDCG@K和Completeness@K等指标验证了其有效性，显著提升了检索精度和下游执行成功率。",
                    "summary_translation": "运行于大规模、动态工具库之上的 LLM agents（大语言模型智能体）依赖于有效检索，然而标准的 single-shot dense retrievers（单次密集检索器）在应对复杂请求时往往力不从心。这些检索失败主要归因于抽象用户目标与技术文档之间的脱节，以及固定大小 embeddings（嵌入向量）在建模组合式工具组合方面的能力局限。为应对上述挑战，我们提出了 TOOLQP，这是一个将检索过程建模为 iterative query planning（迭代式查询规划）的轻量级框架。不同于单次匹配，TOOLQP 将指令分解为若干子任务，并动态生成查询与检索器进行交互；通过针对组合所需的具体子任务，该方法有效地弥合了语义鸿沟。我们利用合成查询轨迹对 TOOLQP 进行训练，随后通过 Reinforcement Learning with Verifiable Rewards (RLVR，可验证奖励强化学习) 进行优化。实验结果表明，TOOLQP 实现了 state-of-the-art（最先进）的性能，展现出卓越的 zero-shot generalization（零样本泛化）能力、在不同检索器间的鲁棒性，以及在 downstream agentic execution（下游智能体执行）方面的显著提升。",
                    "inspiration_trace": "基于论文《Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning》，以下是对作者产出核心方法 **TOOL QP** 的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM智能体”到“工具检索的必要性”）**\n\n1.  **观察趋势**：随着大语言模型（LLM）向智能体演进，解决复杂任务（如数学、编程、推理）越来越依赖于外部工具（API、数据库）。\n2.  **现实瓶颈**：工具库的规模正在从几十个手工挑选的函数，爆炸式增长到数万个动态API。\n3.  **核心冲突**：由于上下文窗口的限制，无法将所有工具的文档和说明一次性塞入LLM。因此，**工具检索**成为了连接用户意图与海量工具库的必经之路。\n\n### 第二阶段：现有方案的深度诊断\n**（为什么传统的“单次检索”会失效？）**\n\n作者首先审视了当前主流的解决方案——即直接套用标准信息检索（IR）技术，使用密集嵌入进行单次语义匹配。通过分析，作者发现了三个根本性的结构性缺陷：\n\n1.  **语义鸿沟**：\n    *   *现象*：用户的表达通常是抽象的、高层的（如“让这段录音音质变好”），而工具文档是技术的、底层的（如“IIR滤波器参数”）。\n    *   *诊断*：单次嵌入试图在一个向量空间内强行对齐这两种完全不同的语言体系，往往导致匹配失败。\n\n2.  **组合性瓶颈**：\n    *   *现象*：现实任务是组合性的，往往需要同时调用多个不同的工具（如“分析降雨如何影响零售销量”需要天气API+股票数据库）。\n    *   *诊断*：单次查询生成的固定维度向量，本质上是一个“词袋”，缺乏表达“多个离散工具组合”的容量。它无法编码工具之间的逻辑关系和组合多样性。\n\n3.  **缺乏交互性**：\n    *   *现象*：工具之间存在依赖关系（如工具A需要工具B的输出作为参数），且工具库是动态变化的。\n    *   *诊断*：传统检索将工具库视为静态数据库，只能“查一次”，无法像人类一样通过“试错”或“反馈”来发现隐含的依赖关系。\n\n### 第三阶段：范式转换与核心假设\n**（从“静态匹配”转向“动态规划”）**\n\n基于上述诊断，作者意识到问题的根源在于**试图用一次性的静态映射来解决动态的、多步骤的推理问题**。\n\n*   **思维跃迁**：如果人类面对复杂任务时会先“制定计划”，再分步执行，为什么不让检索器也这样做？\n*   **核心假设**：工具检索不应是“Query -> Result”的单跳匹配，而应是一个“Goal -> Plan -> Sub-goals -> Queries -> Results”的**迭代规划过程**。\n*   **新视角**：将底层的检索器视为一个可交互的“环境”，而不是一个静态的索引库。\n\n### 第四阶段：方法论构建\n**（如何实现“查询规划”？）**\n\n为了验证上述假设，作者设计了 **TOOL QP** 框架，将检索过程拆解为三个逻辑阶段：\n\n1.  **任务分解**：\n    *   *逻辑*：为了解决语义鸿沟，不能直接用用户原始查询去检索。\n    *   *方案*：先将复杂的用户指令拆解为一系列逻辑上的子任务。这相当于在抽象意图和具体工具之间架设了一座“中间层桥梁”。\n\n2.  **交互式查询生成**：\n    *   *逻辑*：为了解决组合性和依赖性问题，需要分步检索。\n    *   *方案*：针对每个子任务生成特定的搜索查询。关键在于引入**反馈机制**——每一步检索后，模型会观察结果，并动态调整下一步的查询策略（例如，发现缺少某个前置工具，下一步就去专门搜那个工具）。\n\n3.  **检索聚合**：\n    *   *逻辑*：多步检索会产生多个列表，如何合并？\n    *   *方案*：放弃复杂的加权融合，采用“峰值排名”策略——即取每个工具在所有检索步骤中获得的最高排名。这避免了某些子任务因为查询次数多而主导最终结果的偏差。\n\n### 第五阶段：训练策略的演进\n**（如何在没有标注数据的情况下训练规划器？））\n\n框架设计好了，但面临一个现实难题：现有的数据集只有（用户查询，相关工具），没有中间的“规划轨迹”或“子任务标注”。\n\n1.  **数据合成**：\n    *   *思路*：利用强模型（如GPT-4）作为“教师”，反向合成数据。\n    *   *过程*：让教师模型根据最终的正确工具，反推并生成能够找到这些工具的“规划路径”和“中间查询”。这为模型提供了模仿学习的样本。\n\n2.  **强化学习优化（RLVR）**：\n    *   *思路*：单纯的模仿学习（SFT）只能学会教师的风格，不一定能最大化检索成功率。\n    *   *过程*：引入强化学习（RLVR），直接以检索指标（如nDCG、Recall）作为奖励信号。这迫使模型跳出模仿的局限，自主探索能真正提高检索准确率的查询策略。\n\n---\n\n### 总结：作者的思考路径图\n\n1.  **起点**：LLM智能体需要处理海量工具库 -> 必须检索。\n2.  **痛点**：单次密集检索在复杂任务上表现糟糕。\n3.  **归因**：语义错位、组合性限制、缺乏交互反馈。\n4.  **顿悟**：检索应该是一个**规划**过程，而非简单的匹配。\n5.  **方案**：分解任务 -> 迭代查询 -> 动态反馈 -> 结果聚合。\n6.  **落地**：利用合成数据教模型“怎么想”，利用强化教模型“怎么做得更好”。\n\n这一逻辑链条清晰地展示了作者如何从对现有技术缺陷的敏锐观察，上升到对问题本质的重新定义（从IR到Planning），最终构建出一套完整的解决方案。"
                },
                {
                    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
                    "arxiv_id": "2601.07711",
                    "authors": "Pietro Ferrazzi, Milica Cvjeticanin, Alessio Piraccini, Davide Giannuzzi",
                    "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了“Agentic RAG”，其中LLM作为智能体自主编排整个过程（决定动作、时机、迭代），这直接涉及单智能体的规划、工具使用和自我反思能力，符合研究范围。",
                    "summary2": "本文旨在比较Enhanced RAG与Agentic RAG的性能与成本权衡。针对FIQA、NQ、FEVER和CQADupStack-English数据集，我们对比了基于固定模块的Enhanced RAG与LLM自主编排的Agentic RAG。通过F1、NDCG@10及LLM-as-a-judge等指标验证，发现Agentic RAG在查询重写上表现更优，而Enhanced RAG在文档重排和成本控制上更具优势。",
                    "summary_translation": "检索增强生成 (RAG) 系统通常定义为生成器与检索组件的组合，其中检索组件负责从知识库中提取文本上下文，以回答用户查询。然而，此类基础实现存在若干局限性，包括检索结果存在噪声或非最优、对超出范围的查询误用检索机制、查询与文档匹配度低，以及生成器带来的波动性或成本问题。这些缺陷推动了“增强型” RAG 的发展，即在流程中引入专用模块以解决特定的薄弱环节。近期，大型语言模型日益增强的自我反思能力催生了一种新范式，我们将其称为“代理型” RAG。在该方法中，LLM 统筹整个流程——决定执行何种操作、何时执行以及是否进行迭代——从而减少对固定的人工设计模块的依赖。尽管这两种范式已得到快速普及，但在何种条件下哪种方法更具优势尚不明确。在本研究中，我们针对增强型和代理型 RAG，在多种场景和维度上进行了广泛的实证驱动评估。我们的研究结果揭示了这两种范式之间的权衡关系，并综合考虑成本与性能，为在现实应用中选择最有效的 RAG 设计提供了指导。",
                    "inspiration_trace": "基于论文《Is Agentic RAG worth it? An experimental comparison of RAG approaches》，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：RAG 范式的分化与演进\n**思考起点：** 作者首先观察到 RAG（检索增强生成）技术已从最初的“朴素 RAG”（Naïve RAG，即简单的检索+生成）演进出两条截然不同的发展路径：\n*   **增强型 RAG (Enhanced RAG)：** 传统的工程化优化路径。通过在固定流程中增加特定模块（如路由器、查询重写、重排序器）来修补朴素 RAG 的已知缺陷。这是一种“流水线”式的思维。\n*   **代理型 RAG (Agentic RAG)：** 新兴的智能化路径。利用大语言模型（LLM）的反思和规划能力，让 LLM 作为“大脑”自主决定何时检索、如何重写、是否迭代。这是一种“动态循环”式的思维。\n\n**核心冲突：** 社区和业界对 Agentic RAG 的热情高涨，认为其灵活性代表了未来。然而，这种灵活性是否真的带来了性能的提升？还是仅仅增加了不必要的复杂度和成本？\n\n### 2. 问题聚焦：实证缺失与决策困境\n**痛点识别：** 尽管已有理论上的定义和分类，但缺乏严格的**实证对比**。现有的文献多停留在概念探讨或单一架构的优化上。\n**核心问题：** 在实际应用中，Agentic RAG 相比于精心设计的 Enhanced RAG，究竟是“物有所值”还是“徒增开销”？在什么场景下应该选择哪一种？\n\n### 3. 假设提出：基于“缺陷修复”的维度拆解\n**逻辑推演：** 为了公平对比，不能笼统地比较“整体好坏”，而应该回到 RAG 的根本痛点上。作者假设：Agentic 和 Enhanced 两种范式在解决 RAG 的不同缺陷时，可能各有优劣。\n**维度构建：** 作者将朴素 RAG 的缺陷拆解为四个核心维度，并针对每个维度提出了对比假设：\n1.  **用户意图处理：** Enhanced RAG 依赖显式的分类器（路由），而 Agentic RAG 依赖 LLM 的自主判断。假设：在复杂意图下，Agentic 可能更灵活，但在简单任务上可能过度思考。\n2.  **查询-文档对齐：** Enhanced RAG 使用固定的重写技术（如 Hyde），而 Agentic RAG 可以动态调整查询。假设：Agentic 的动态重写可能更能适应不同文档格式。\n3.  **检索结果精炼：** Enhanced RAG 使用专门的重排序模型，而 Agentic RAG 通过多次迭代检索来优化。假设：专门的重排序模型可能比 LLM 的迭代检索更精准。\n4.  **底层模型质量的影响：** 两种架构对 LLM 能力的敏感度是否不同？\n\n### 4. 方法论设计：控制变量的“擂台赛”\n**实验设计思路：** 为了验证上述假设，作者设计了一个“头对头”的对比实验框架。\n*   **数据选择：** 选取了涵盖问答（QA）和信息检索/提取（IR/E）的四个数据集（FIQA, NQ, FEVER, CQADupStack），以覆盖不同领域和任务类型。\n*   **架构对齐：**\n    *   **Enhanced 端：** 选用当前最先进的 SOTA 组件（如 Semantic Router, Hyde 查询重写, Cross-encoder 重排序）构建最强流水线。\n    *   **Agentic 端：** 构建一个基于图的最小化代理框架，赋予其调用 RAG 工具、重写查询和迭代的能力，但不预设固定步骤。\n*   **评估指标：** 除了传统的性能指标（F1, NDCG），作者特别引入了**成本分析**（Token 消耗、端到端延迟），因为“Is it worth it”的核心在于性价比。\n\n### 5. 结果分析与洞察：打破迷思\n**逻辑推演与发现：** 通过实验数据，作者得出了反直觉或精细化的结论，修正了最初的假设：\n*   **关于意图：** Agentic RAG 在狭窄领域（如金融）表现出色，但在广泛或嘈杂领域（如 FEVER）反而不如 Enhanced RAG 的显式路由器可靠。这表明 LLM 的自主判断在边界模糊时容易失效。\n*   **关于重写：** Agentic RAG 确实表现更好，证明了动态适应性的价值。\n*   **关于精炼：** 专门的重排序模型显著优于 Agentic 的迭代检索。这揭示了 LLM 在“从一堆文档中挑出最好的”这一具体任务上，不如专门的微调模型。\n*   **关于成本：** Agentic RAG 的成本高出数倍（最高 3.6 倍），且延迟更高。\n\n### 6. 结论形成：权衡与指导\n**最终思考：** 并不存在“银弹”。\n*   **Agentic RAG 的价值：** 在于处理模糊的意图和需要动态适应查询格式的场景。\n*   **Enhanced RAG 的价值：** 在于处理需要高精度检索（重排序）和高效率的场景。\n*   **核心建议：** 不要盲目追求 Agentic 的新颖性。如果业务场景对成本敏感、对检索精度要求极高，经过优化的 Enhanced RAG 依然是更优选择；只有在需要高度灵活性和自主决策的复杂场景下，Agentic RAG 的额外成本才“值得”。"
                },
                {
                    "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
                    "arxiv_id": "2601.07696",
                    "authors": "Nick Ferguson, Alan Bundy, Kwabena Nuamah",
                    "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了LLM在多跳问答任务中的工具使用和规划能力（将问题分解为步骤），重点分析了工具选择和工具调用输出，符合单智能体中“工具使用”和“规划”的研究范围。",
                    "summary2": "本文旨在探索大语言模型的元级推理能力。针对多跳表格问答任务，我们提出了一种基于工具的评估框架，通过比较模型工具调用与预设的“essential actions”来分析推理过程。我们在基于世界银行数据的自定义数据集上，通过最终答案准确率、精确率和召回率验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）的最新进展日益聚焦于“推理”能力，这一概念在LLM相关讨论中存在诸多重叠的定义。我们采用一种更具结构化的方法，将元级推理（meta-level reasoning，指代为解决任务所需的中间步骤进行推理的过程）与对象级推理（object-level reasoning，涉及上述步骤的底层执行）区分开来。我们设计了一项新颖的问答任务，该任务基于不同国家在不同年份的地缘政治指标数值。这些问题需要分解为中间步骤、进行数据检索以及对检索到的数据执行数学运算。我们通过考察模型为回答问题而选择合适工具的情况，来分析LLMs的元级推理能力。为了超越单纯的最终答案准确率，对LLMs进行更深入的分析，我们的任务中包含了“必要动作”，通过将LLMs的工具调用输出与这些动作进行比对，从而推断其推理能力的强弱。我们发现，LLMs在我们的任务中表现出了良好的元级推理能力，但在任务理解的某些方面仍存在缺陷。研究发现，n-shot提示（n-shot prompting）对准确率影响甚微；遇到的错误信息通常不会导致性能下降；此外，我们还提供了LLMs数理能力低下的进一步证据。最后，我们讨论了这些发现在其他任务领域的泛化性及其局限性。",
                    "inspiration_trace": "基于对论文《Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：现有“推理”评估的模糊性\n**思考起点：** 当前学术界对大语言模型（LLM）能力的讨论高度集中在“推理”这一概念上。然而，作者观察到“推理”一词在LLM语境下定义重叠且模糊（如数学推理、常识推理等）。\n**核心痛点：** 现有的基准测试（如GSM8K, MATH）大多仅关注**最终答案的准确性**。这种“黑盒”评估方式存在严重缺陷：如果模型答错了，我们无法区分是模型**没想对步骤**（规划失败），还是**算错了数**（执行失败）。\n**初步假设：** 为了真正理解LLM的推理能力，必须将“规划做什么”与“实际去做”这两个层面解耦。\n\n### 2. 理论引入：经典AI视角的二元划分\n**理论溯源：** 为了解决上述模糊性，作者回顾了符号AI和自动定理证明领域的经典理论（特别是Bundy, 1983的工作）。\n**概念界定：** 引入**元级推理**与**对象级推理**的严格区分：\n*   **元级推理：** 关于“如何解决问题”的思考，即高层规划、任务分解、步骤选择。\n*   **对象级推理：** 具体执行上述步骤的过程，如数据检索、算术计算、符号操作。\n**逻辑演进：** 作者意识到，将这一经典框架应用于LLM评估，可以将原本混在一起的“推理能力”拆解为两个可独立分析的维度，从而提供比单纯准确率更深层的诊断。\n\n### 3. 方法论构建：将思维过程“外化”\n**关键挑战：** LLM的推理过程通常隐藏在模型内部的隐状态或生成的自然语言中，难以量化评估。如何让“元级推理”变得可观测？\n**解决方案：** 利用**工具使用**范式。\n*   **逻辑支点：** 当LLM调用一个工具（如`search_indicator`或`calculate_mean`）时，它实际上是在显式地展示其“计划”。工具调用序列就是元级推理的**中间表征**。\n*   **任务设计：** 选择**多跳表格QA任务**（基于世界银行数据）。该任务天然需要将复杂问题分解为“检索数据”和“数学运算”两个子步骤，完美契合元级（规划检索与运算顺序）与对象级（实际检索与计算）的二元框架。\n\n### 4. 评估创新：从“结果导向”转向“过程导向”\n**评估困境：** 传统的QA评估只有“对/错”两种状态。但在工具使用场景下，模型可能选对了工具（元级强），但工具参数填错或计算出错（对象级弱）。\n**核心创新：** 提出**“必要动作”**的概念。\n*   **定义：** 针对每个问题，定义一组必须执行的工具调用集合。这不是唯一的“黄金路径”，而是解决问题的核心动作集。\n*   **指标构建：** 不再只看Final Answer，而是将模型生成的工具调用序列与“必要动作”进行对比，计算**精确率**和**召回率**。\n    *   **高精确率：** 模型知道该用什么工具（元级推理强）。\n    *   **低召回率：** 模型遗漏了必要步骤（规划有漏洞）。\n    *   **最终答案错误：** 可能是对象级计算错误，而非元级规划错误。\n\n### 5. 实验验证与发现：诊断模型的能力边界\n**实验设计意图：** 作者并不旨在设计一个让模型得高分的系统，而是利用这个环境作为“显微镜”来观察模型。\n**逻辑推演与验证：**\n*   **验证元级能力：** 实验发现模型在工具选择的精确率上表现良好，证明LLM具备较强的**高层规划能力**（即知道“先做什么后做什么”）。\n*   **验证对象级缺陷：** 当移除数学工具，强制模型自己计算时，性能大幅下降。这证实了LLM在**底层执行（特别是算术）**上的固有缺陷。\n*   **验证鲁棒性：** 通过引入错误信息，观察模型是否能自我修正。这进一步测试了元级推理中的“动态调整”能力。\n\n### 6. 总结：逻辑链的闭环\n作者的思考过程完成了一个闭环：\n从**现象**（LLM推理定义模糊、评估单一）出发 -> 引入**理论**（元级/对象级二分法） -> 寻找**载体**（工具调用作为思维外化的接口） -> 设计**度量**（基于必要动作的过程评估） -> 最终得出**结论**（LLM是优秀的“规划者”，但仍是蹩脚的“计算器”）。\n\n这篇文章的本质不仅仅是发布了一个数据集，而是提供了一套**解剖LLM推理能力的思维框架和手术刀（工具调用评估）**。"
                },
                {
                    "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments",
                    "arxiv_id": "2601.07606",
                    "authors": "Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",
                    "summary": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了一个用于评估“基于智能体的研究判断”的基准，重点比较了“使用工具的智能体”与非智能体基线，涉及工具使用和交互预算，符合单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在评估模型对科学思想的判断能力及预测其未来影响力。针对科学思想评估缺乏可扩展验证方法的问题，我们提出了一种名为 Proof of Time (PoT) 的半可验证基准框架，通过冻结截止时间前的证据并在离线沙箱中预测未来结果。我们在涵盖四个领域的 30K+ 实例上，通过准确率和测试时计算缩放分析验证了其有效性，发现增加交互预算能提升智能体性能，且工具使用的效果高度依赖于任务类型。",
                    "summary_translation": "大语言模型正日益被用于评估和预测研究思路，然而，我们目前缺乏可扩展的方法来衡量模型对这些科学想法的判断质量。为实现这一目标，我们提出了 PoT，这是一个半可验证的基准测试框架，它将科学想法的判断与随后可观察到的下游信号（例如引用和研究人员议程的转变）联系起来。PoT 在离线沙箱中冻结截止前的证据快照，并要求模型预测截止后的结果，这使得在真实情况出现时能够进行可验证的评估，在无需详尽专家标注的情况下实现可扩展的基准测试，并能够针对同行评审奖项等信号分析人类与模型之间的不一致性。此外，PoT 为评估科学想法的基于智能体的研究判断提供了一个受控测试平台，能够在提示消融和预算缩放的条件下，对比使用工具的智能体与非智能体基线。在跨越四个基准领域的 30,000 多个实例中，我们发现，与非智能体基线相比，更高的交互预算通常能提升智能体的性能，而使用工具的收益则高度依赖于具体任务。通过将按时间划分的、未来可验证的目标与用于工具使用的离线沙箱相结合，PoT 支持对面向未来的科学想法判断任务中的智能体进行可扩展评估。",
                    "inspiration_trace": "基于论文《Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法设计的思考过程：\n\n### 第一阶段：宏观观察与核心矛盾\n**思考起点：科学评价的“时效性错位”**\n1.  **现象观察**：科学界评价研究创意（Idea）的主要机制（同行评审、静态基准测试）通常发生在“当下”，且高度依赖主观判断。\n2.  **核心矛盾**：真正衡量一个科学创意价值的标准是“时间的检验”（如引用量、奖项、后续研究方向的改变），但这些信号具有滞后性，无法在决策当下立即获取。\n3.  **现有困境**：大语言模型（LLM）正被用于辅助科研评价，但我们缺乏一种可扩展、客观的方法来评估模型判断“未来影响力”的能力。如果仅用静态数据集评估，无法反映模型对科学演进的预测能力。\n\n### 第二阶段：概念突破——将“时间”转化为验证机制\n**核心假设：利用历史数据模拟未来预测**\n1.  **思维转换**：既然无法真的等待未来，不如利用“过去”来模拟“未来”。如果我们将时间轴切分，设定一个截止点 $t_0$，那么对于 $t_0$ 之后的 $t_1$ 时刻，其结果在当下已经是已知的客观事实。\n2.  **方法论雏形**：\n    *   **冻结证据**：只给模型提供 $t_0$ 时刻之前的“快照”信息（如论文摘要、作者历史）。\n    *   **预测未来**：要求模型预测 $t_1$ 时刻才会发生的信号（如 $t_1$ 时刻的引用数、获奖情况）。\n    *   **事后验证**：利用现实中已经发生的 $t_1$ 结果作为“金标准”进行评分。\n3.  **优势确立**：这种方法解决了“可验证性”（标签是客观事实而非主观打分）和“可扩展性”（无需专家人工标注，数据可自动更新）的问题。\n\n### 第三阶段：控制变量——解决“智能体”评估的污染问题\n**进阶思考：如何公平地评估工具使用能力？**\n1.  **新挑战**：当前流行使用“工具调用智能体”来处理复杂任务。但现有评估往往混淆了“推理能力”与“信息获取能力”。如果允许智能体联网，它可能只是直接查到了答案，而非基于证据进行了判断。\n2.  **隔离设计**：为了纯粹测试模型基于有限证据进行推理和判断的能力，作者引入了**“离线沙盒”**概念。\n3.  **逻辑闭环**：\n    *   将智能体关在一个“断网”的房间里。\n    *   房间里只有 $t_0$ 时刻的冻结证据和本地工具（如Python、文本编辑器）。\n    *   智能体表现出的任何提升，必须归因于其对有限证据的挖掘和推理能力，而非外部信息检索。\n\n### 第四阶段：维度拆解——定义“科学创意判断”的具体内涵\n**操作化定义：从抽象概念到具体任务**\n1.  **问题细化**：“科学创意判断”是一个抽象概念，需要将其拆解为可量化的具体维度。\n2.  **四个维度的构建**：\n    *   **影响力预测**：预测未来的引用量（量化指标）。\n    *   **价值评估**：预测同行评审奖项（定性共识）。\n    *   **研究演进**：预测教授未来的研究方向（连续性与漂移）。\n    *   **技术前沿**：预测基准测试的SOTA轨迹（技术极限）。\n3.  **任务设计逻辑**：这些任务覆盖了从个人（教授）、群体（会议奖项）到领域（SOTA）不同层面的科学判断，且均符合“时间可验证”原则。\n\n### 第五阶段：实验假设与验证——探索“智能体”的边际效应\n**实证探究：智能体何时才值得？**\n1.  **对比基准**：设置“零样本”与“智能体”模式的对比，旨在验证增加工具和推理步骤是否真的有效。\n2.  **成本-收益分析**：引入“消息预算”概念，模拟测试时的计算成本。\n3.  **假设验证**：\n    *   智能体并非在所有任务上都优于直接生成。\n    *   在需要深度证据挖掘的任务（如Faculty任务）上，智能体优势明显。\n    *   在结构化预测或简单任务上，增加智能体步骤可能只是浪费算力。\n4.  **结论导向**：通过实验揭示模型在处理“未来导向”任务时的失败模式（如检索失败、推理循环），为未来改进提供方向。\n\n---\n\n**总结：作者的逻辑演进路径**\n从**“科学评价需要时间检验”**的哲学观察出发，通过**“时间切片”**的技术手段将未来预测转化为离线验证，进而引入**“离线沙盒”**以排除信息干扰，纯粹考察模型的**“证据推理能力”**，最终构建了一个多维度、可扩展的基准，回答了“AI能否判断科学创意的未来价值”这一核心问题。"
                },
                {
                    "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents",
                    "arxiv_id": "2601.07582",
                    "authors": "Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",
                    "summary": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了长期对话智能体的记忆机制，提出了基于事件分割的分层记忆架构，属于单智能体研究范围中的“记忆”方向。",
                    "summary2": "本文旨在解决长期对话智能体中记忆粒度僵化及检索缺乏结构感知的问题。针对长期交互场景，我们提出了一种基于Event Segmentation Theory的ES-Mem框架，结合动态事件分割与边界锚定的分层记忆架构。我们在LoCoMo和LongMemEval-S基准上通过F1、BLEU-1和Accuracy等指标验证了其有效性。",
                    "summary_translation": "记忆对于对话代理在长期交互中维持连贯性并实现持续适应至关重要。尽管现有的记忆机制具备基本的存储与检索能力，但它们主要受限于两个方面：(1) 僵化的记忆粒度往往破坏语义完整性，导致记忆单元碎片化且缺乏连贯性；(2) 主流的扁平化检索范式仅依赖于表层语义相似度，忽视了在导航和定位特定情景语境时所必需的话语结构线索。为克服上述局限，受事件分割理论的启发，我们提出了ES-Mem框架，该框架包含两个核心组件：(1) 动态事件分割模块，用于将长期交互划分为具有清晰边界的语义连贯事件；(2) 分层记忆架构，通过构建多层记忆并利用边界语义来锚定特定的情景记忆，从而实现精确的语境定位。在两个记忆基准测试上的评估表明，ES-Mem相较于基线方法取得了持续的性能提升。此外，所提出的事件分割模块在对话分割数据集上也展现出了稳健的适用性。",
                    "inspiration_trace": "基于论文《ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents》的内容，以下是对作者产出该文章核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法论构建的思考过程。\n\n---\n\n### 1. 宏观问题：长程对话中的“记忆断层”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在对话智能体应用中的一个根本性矛盾：虽然LLM生成能力极强，但其固有的“上下文窗口”限制了处理超长对话的能力。\n**核心挑战：**\n为了实现真正的个性化与连续适应，智能体必须具备“长期记忆”。然而，现有的记忆机制（如简单的RAG或向量数据库）在面对成百上千轮的复杂对话时，往往表现不佳，导致智能体“遗忘”或“胡言乱语”。\n\n### 2. 深入观察：现有记忆机制的两大“结构性缺陷”\n作者对现有的主流记忆方法（如MemGPT, MemoryBank等）进行了深入剖析，发现它们普遍存在两个深层次的逻辑漏洞，这构成了文章的切入点：\n\n*   **缺陷一：记忆粒度的“机械性”。**\n    *   *观察：* 现有方法大多采用固定粒度（如按“轮次”Turn或固定Token数）切分对话。\n    *   *问题：* 真实的对话流是语义连续的。机械切分往往会打断一个完整的语义事件（例如，讨论“生日礼物”的想法跨越了3轮，却被切成了两半）。这导致存储的记忆单元本身是“碎片化”和“语义不完整”的。\n*   **缺陷二：检索范式的“扁平化”。**\n    *   *观察：* 现有检索大多基于向量相似度的“扁平检索”，把所有记忆块当作孤立的文本片段进行匹配。\n    *   *问题：* 这种方式忽略了对话的“篇章结构”。当用户问“为什么我们后来放弃了那个园艺工具？”时，关键信息不在于“园艺工具”这个词本身，而在于话题**转换**的那个瞬间。扁平检索很难定位这种结构性的转折点。\n\n### 3. 跨学科灵感：引入认知心理学中的“事件分割理论”\n**思考转折：**\n为了解决上述“语义碎片”和“结构缺失”的问题，作者跳出纯计算机科学的视角，转向认知心理学寻找答案。\n**理论引入：**\n作者引入了**事件分割理论**。该理论指出，人类并非连续地感知世界，而是将经验流解析为离散的、有意义的事件单元。\n**关键洞察：**\n人类记忆中，**事件边界**尤为重要。边界处是注意力最集中的时刻，起到了“认知锚点”的作用，帮助人类高效地索引和回忆过去的经历。\n*假设：* 如果让AI像人类一样，按“事件”来组织记忆，并利用“边界”作为检索的锚点，就能解决现有方法的痛点。\n\n### 4. 核心假设形成：从“存储文本”转向“结构化事件”\n基于EST理论，作者提出了核心假设：\n*   **关于存储：** 记忆的粒度不应是固定的轮次，而应是动态的“语义事件”。\n*   **关于检索：** 检索不应是全局的文本匹配，而应是先定位“边界锚点”，再展开细节的“由粗到细”过程。\n\n### 5. 方法论构建：ES-Mem框架的逻辑落地\n为了验证上述假设，作者设计了ES-Mem框架，其逻辑演进分为三个步骤：\n\n**第一步：如何定义“事件”？（动态分割模块）**\n*   *思考：* 机器如何知道一个话题结束了？\n*   *策略：* 采用“统计信号+语义验证”的两阶段法。\n    1.  **粗筛：** 利用互信息计算话题连贯性，当语义连贯性骤降时，标记为潜在边界。\n    2.  **精修：** 引入意图识别，判断这是话题的“转换”还是内容的“细化”。只有真正的意图转换才被确认为边界。\n\n**第二步：如何利用“边界”？（分层记忆架构）**\n*   *思考：* 既然边界是锚点，那么记忆的结构就不能是扁平的。\n*   *策略：* 构建三层金字塔结构。\n    *   **Level 1（顶层）：精炼边界。** 专门描述“话题A是如何转换到话题B的”。这是检索时的“路标”。\n    *   **Level 2（中层）：事件摘要。** 用于快速匹配内容。\n    *   **Level 3（底层）：原始上下文。** 用于最终生成细节。\n    *   *创新点：* 显式地将“边界”建模为一种可检索的信息索引，而不仅仅是切分点。\n\n**第三步：如何模拟人类回忆？（由粗到细检索）**\n*   *思考：* 人类回忆时，先想“那是哪段时间的事？”，再想“具体说了什么？”。\n*   *策略：* 模仿这一认知过程。\n    1.  **边界扫描：** 先在Level 1（边界层）搜索，找到最相关的“话题转换时刻”。\n    2.  **区间扩展：** 以该边界为中心，向前后扩展一个时间窗口，锁定相关的记忆区间。\n    3.  **摘要重排：** 在锁定的区间内，利用Level 2（摘要）进行精细打分，选出最准确的上下文。\n\n### 6. 逻辑闭环与验证\n**最终产出：**\n作者通过这一系列思考，将传统的“静态、扁平”的记忆系统，重构为“动态、结构化、认知驱动”的记忆框架。\n**验证逻辑：**\n在实验中，作者不仅验证了ES-Mem在长程记忆任务上的性能提升，还专门验证了“事件分割”模块本身的鲁棒性。这证明了：**模仿人类认知结构（事件分割+边界锚定）确实是解决长程对话记忆难题的有效路径。**\n\n---\n\n**总结：**\n作者的思考路径是从**“现有技术无法处理长程语义连贯性”**这一工程问题出发，通过**“认知心理学的事件分割理论”**获得理论指引，最终通过**“动态分割+分层存储+锚点检索”**的技术手段，实现了对人类记忆机制的工程化复现。"
                },
                {
                    "title": "GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap",
                    "arxiv_id": "2601.07375",
                    "authors": "Farzad Shami, Subhrasankha Dey, Nico Van de Weghe, Henrikki Tenkanen",
                    "summary": "The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent's execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at https://anonymous.4open.science/r/groke.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于LLM的分层框架（GROKE），用于执行导航指令评估。它涉及单智能体的核心能力——规划（子指令规划）和导航（拓扑图导航），符合“单智能体：规划”的研究范围。论文明确为“无视觉”，避开了多模态/视觉的排除项，且重点在于智能体的架构设计与执行能力，而非纯应用或基础设施优化。",
                    "summary2": "本文旨在解决视觉依赖评估中的成本与感知误差问题。针对 Map2Seq 数据集，我们提出了一种基于 OpenStreetMap 的无视觉分层 LLM 框架 GROKE，结合子指令规划与结构化 JSON 表示进行图推理。我们在 Map2Seq 上通过 Navigation Error (NE)、Success Rate (SR) 等指标验证了其有效性，导航误差降低了 68.5%。",
                    "summary_translation": "导航指令的评估仍然是视觉语言导航 (VLN) 研究中一个长期存在的挑战。传统的基于参考的指标，如 BLEU 和 ROUGE，无法捕捉空间指令的功能效用，特别是无法衡量指令是否成功引导导航者到达预定目的地。尽管现有的 VLN 智能体可以作为评估器，但它们对高保真视觉模拟器的依赖带来了许可限制和计算成本，且感知误差进一步干扰了语言质量评估。本文介绍了 GROKE (Graph-based Reasoning over OSM Knowledge for instruction Evaluation)，这是一个基于分层大语言模型 (LLM) 的无视觉、无需训练的框架，用于利用 OpenStreetMap 数据评估导航指令。通过系统的消融实验，我们证明了空间信息的结构化 JSON 和文本格式显著优于基于网格和视觉图的表示。我们的分层架构结合了子指令规划与拓扑图导航，在 Map2Seq 数据集上，与启发式和采样基线相比，将导航误差降低了 68.5%。智能体的执行成功率、轨迹保真度和决策模式作为功能可导航性的代理指标（基于 OSM 可见地标和拓扑结构），建立了一种无视觉依赖的可扩展且可解释的评估范式。代码和数据可在 https://anonymous.4open.science/r/groke 获取。",
                    "inspiration_trace": "基于对论文《GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观问题：如何准确评估导航指令的“功能性”质量？\n**思考起点：**\n在视觉语言导航（VLN）领域，传统的评估指标（如BLEU、ROUGE）存在根本性缺陷。这些指标基于文本相似度（n-gram重叠），无法捕捉导航指令的核心价值——即“能否引导用户到达目的地”。\n*   **反例：** “在银行左转”与“在银行右转”文本相似度极高，但功能截然相反。\n*   **结论：** 评估必须从“文本相似度”转向“功能效用性”。\n\n### 2. 现状批判：现有“务实评估”路径的痛点\n**演进逻辑：**\n为了解决上述问题，学术界引入了“Agent-as-Judge”范式，即训练一个智能体在模拟器中执行指令，通过成功率来反推指令质量。\n**观察到的瓶颈：**\n这种方法严重依赖高保真的视觉模拟器（如Matterport3D、Google Street View），引入了新的噪声：\n1.  **混淆变量：** 智能体失败可能是因为视觉识别能力差（看不清“红砖墙”），而非指令本身写得不好。这导致评估结果混杂了视觉感知误差。\n2.  **成本与壁垒：** 视觉数据昂贵、版权受限、计算量大，限制了评估的可扩展性。\n\n### 3. 核心假设：能否剥离视觉，仅基于“语义与拓扑”进行评估？\n**思维跃迁：**\n导航的本质是空间推理，而非像素识别。人类在阅读导航指南（如地图）时，依赖的是地标（POI）、方向和拓扑连接，而非实景照片。\n**假设提出：**\n如果我们将环境抽象为符号化的地图数据（如OpenStreetMap），构建一个“无视觉”的评估智能体，是否既能保留功能性评估的优势，又能消除视觉噪声和成本问题？\n*   **数据基础：** Map2Seq数据集提供了OSM数据（节点、边、POI），为这一假设提供了实验土壤。\n\n### 4. 方法论探索：如何让大语言模型（LLM）“看懂”地图？\n**技术挑战：**\n既然决定使用LLM作为推理核心，如何将图结构的空间数据转化为LLM能高效理解的输入？\n**实验与试错（Ablation Studies驱动的设计）：**\n作者对比了四种空间表征形式，试图寻找最优解：\n1.  **网格/矩阵：** 模仿视觉像素。结果发现LLM难以解析这种高密度的ASCII字符，效果最差。\n2.  **可视化图：** 使用Graphviz风格。虽然直观，但LLM处理箭头和图形符号的推理能力不如处理结构化数据。\n3.  **纯文本描述：** 自然语言描述连接关系。效果尚可，但在复杂路径上信息密度不足，导致认知负荷过高。\n4.  **结构化JSON（最终选择）：** 将节点、边、POI组织为层级化的JSON。\n    *   **逻辑判断：** JSON格式既保留了机器可读的结构，又符合LLM预训练数据中的代码/结构化文本模式，能显著提升推理效率和准确性。\n\n### 5. 架构优化：如何处理长程导航的复杂性？\n**问题分解：**\n直接让LLM根据整段长指令在地图上一步步走，容易迷失目标或产生累积误差。\n**灵感来源：** 人类认知习惯——将复杂任务拆解为子目标。\n**架构设计：**\n提出**分层架构**：\n1.  **子指令代理：** 负责高层规划，将长指令拆解为原子动作（如“直走”、“左转”）并提取关键地标。\n2.  **导航代理：** 负责底层执行，仅关注当前子目标在局部地图（可见区域）内的实现。\n*   **逻辑优势：** 这种解耦降低了单次推理的复杂度，使得智能体能更专注于当前的局部决策，同时保持全局目标的一致性。\n\n### 6. 最终验证：这种“无视觉”评估是否有效？\n**闭环思考：**\n如果智能体没有眼睛，它的成功是否真的代表了指令的质量？\n**验证逻辑：**\n通过相关性分析，将GROKE的导航指标（如导航误差NE、成功率SR）与人类对指令清晰度的评分进行对比。\n*   **结果：** 两者呈现显著相关性。证明了一个基于逻辑和拓扑的智能体，足以作为指令质量的可靠代理指标，从而建立了一种**可扩展、可解释且无视觉依赖**的评估新范式。\n\n---\n\n**总结：**\n作者的思考路径是从**评估指标的失效**出发，批判了**视觉依赖的局限性**，提出了**基于OSM图推理的“无视觉”假设**，并通过**对比实验确定了JSON作为最优的空间表征**，最终利用**分层代理架构**实现了高效、准确的指令评估。"
                },
                {
                    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
                    "arxiv_id": "2601.07348",
                    "authors": "Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang",
                    "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了“受控自我演化”（CSE）框架，通过迭代“生成-验证-优化”循环实现自我完善，符合“自我演化”的研究范围。同时，文中包含“多样化规划初始化”和“分层进化记忆”等组件，涉及单智能体的规划与记忆机制。",
                    "summary2": "本文旨在解决现有自进化方法在算法代码优化中探索效率低下的问题。针对代码生成任务，我们提出了一种Controlled Self-Evolution (CSE)框架，通过多样化规划初始化、遗传进化及分层进化记忆提升搜索效率。在EffiBench-X基准上，通过Execution-Time ratio (ET)、Memory-Peak ratio (MP)和Memory-Integral ratio (MI)指标验证了其有效性，CSE在多种LLM主干网络上均表现出更优的算法优化能力。",
                    "summary_translation": "自进化方法通过迭代的“生成-验证-优化”循环来增强代码生成，然而现有方法存在探索效率低下的问题，无法在有限的预算内发现具有更优复杂度的解决方案。这种低效性源于初始化偏差导致进化陷入劣质解区域、缺乏反馈引导的不可控随机操作，以及跨任务经验利用不足。为解决这些瓶颈，我们提出了受控自进化，该方法包含三个关键组件。多样化规划初始化生成结构各异的算法策略，以实现广泛的解空间覆盖。遗传进化用反馈引导机制替代随机操作，从而实现定向突变和组合交叉。分层进化记忆在任务间和任务内层面捕获成功与失败的经验。在 EffiBench-X 上的实验表明，CSE 在各种 LLM backbones (大语言模型骨干) 上均持续优于所有 baselines (基线模型)。此外，CSE 在早期代即展现出更高的效率，并在整个进化过程中保持持续改进。我们的代码已在 https://github.com/QuantaAlpha/EvoControl 公开。",
                    "inspiration_trace": "基于论文《Controlled Self-Evolution for Algorithmic Code Optimization》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：从“功能正确”到“算法最优”的鸿沟\n*   **现象**：现有的LLM在代码生成任务上已经表现出色，能够通过单次生成解决许多编程问题，实现“功能正确”。\n*   **问题**：在算法竞赛或高性能计算场景下，仅仅“正确”是不够的。代码的执行效率（时间复杂度、空间复杂度）至关重要。现有的模型往往生成的是“正确但低效”的代码。\n*   **初步思考**：如何让模型不仅能写出能跑通的代码，还能像人类专家一样不断优化代码，逼近算法的最优解？\n\n### 2. 现有范式分析：自进化的潜力与瓶颈\n*   **现有方案**：学术界引入了“自进化”范式，即通过“生成-验证-修正”的迭代循环来优化代码。\n*   **深入观察**：虽然理论上只要迭代次数足够多，随机搜索总能找到最优解，但在实际应用中，计算资源和推理预算是有限的。\n*   **核心矛盾**：**探索效率低下**。现有的自进化方法在有限的预算内，很难跳出局部最优，发现具有更优复杂度的解。它们浪费了大量的预算在低质量的解空间中。\n\n### 3. 病因诊断：效率低下的三大根源\n作者深入剖析了为什么现有的自进化方法“瞎忙活”，归纳出三个核心痛点：\n\n*   **痛点一：初始化偏差**\n    *   *观察*：传统方法通常从一个或少数几个初始解开始进化。如果初始解处于解空间的“贫瘠区域”（例如算法思路本身是低效的），后续的微调很难从根本上改变算法结构，导致进化陷入局部最优。\n    *   *思考*：起点决定了起跑线，如果起跑线就选错了，后面跑得再快也没用。我们需要在起跑时就覆盖不同的算法思路。\n\n*   **痛点二：无控制的随机进化**\n    *   *观察*：现有的变异和交叉操作往往是随机的、黑盒的。模型不知道哪里错了，只是盲目地修改代码或拼接文本。这种“无向探索”导致生成的变体大多无效，无法利用验证反馈来指导搜索方向。\n    *   *思考*：进化不能靠“猜”，必须靠“反馈”。我们需要一种机制，能精准定位代码中的“病灶”，并进行“手术式”的修复，而不是盲目重写。\n\n*   **痛点三：进化经验的浪费**\n    *   *观察*：模型在进化过程中经常重复犯同样的错误（无论是同一个任务内的重复失败，还是不同任务间忽略了通用的优化技巧）。现有的方法缺乏记忆机制，无法积累和复用成功的经验。\n    *   *思考*：人类专家之所以强，是因为他们记住了之前的教训和套路。我们需要给模型装上“短期记忆”（避免重蹈覆辙）和“长期记忆”（复用通用优化策略）。\n\n### 4. 核心假设：从“随机搜索”转向“受控进化”\n*   **假设提出**：如果我们将进化过程从“无控制的随机操作”转变为“受反馈引导的精细化操作”，并辅以多样化的起点和记忆机制，就能大幅提升探索效率。\n*   **方法论构建**：基于上述三个痛点，提出 **Controlled Self-Evolution (CSE)** 框架，对应设计三个关键组件来逐一击破。\n\n### 5. 方法论演进：三大组件的逻辑构建\n\n*   **针对痛点一（初始化偏差） -> 多样化规划初始化**\n    *   *设计思路*：不要直接生成代码，先生成“策略草图”。强制模型在生成具体代码前，先规划出多种结构上截然不同的算法策略（如贪心 vs 动态规划 vs 位运算）。\n    *   *逻辑*：通过策略层面的多样性，确保初始种群覆盖了解空间中多个有潜力的区域，降低了陷入局部最优的风险。\n\n*   **针对痛点二（无控制进化） -> 遗传进化机制**\n    *   *设计思路*：引入“功能分解”概念。将代码拆解为独立的功能模块（如I/O、核心逻辑、边界处理）。\n    *   *受控变异*：利用反馈定位导致性能低下的具体模块，只对该模块进行“靶向再生”，保留表现良好的部分。\n    *   *组合交叉*：模仿人类专家，从不同父代中提取优势模块（如A的算法核心 + B的优化技巧），在逻辑层面进行结构化重组，而非简单的文本拼接。\n\n*   **针对痛点三（经验浪费） -> 分层进化记忆**\n    *   *设计思路*：建立双层记忆系统。\n    *   *局部记忆（任务内）*：记录当前任务中哪些修改带来了提升（成功经验），哪些导致了倒退（失败教训），实时指导后续迭代，避免走回头路。\n    *   *全局记忆（跨任务）*：将不同任务中的通用优化模式（如特定的I/O加速技巧、数据结构替换规则）提炼出来，存入向量数据库。遇到新任务时，主动检索相关经验作为先验知识。\n\n### 6. 逻辑闭环与验证\n*   **综合**：将“多样化起点”作为基础，通过“受控的遗传操作”在解空间中高效导航，并利用“分层记忆”作为导航的指南针。\n*   **预期结果**：这种方法不仅能更快地找到高质量解（早期效率高），而且能在整个进化过程中持续改进（持续优化），不会过早陷入停滞。\n*   **实验验证**：在EffiBench-X上的实验结果证实了CSE在不同LLM骨干网络上均优于基线方法，且消融实验证明了三个组件缺一不可，形成了协同效应。\n\n---\n\n**总结**：作者的思考路径是从**发现LLM代码效率不足**这一现象出发，通过分析现有自进化方法**“盲目搜索”**的本质缺陷，提出了**“受控引导”**的核心思想，并最终通过**策略多样化、操作精细化、经验分层化**三个维度的创新，构建了一套完整的算法代码优化方法论。"
                },
                {
                    "title": "Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation",
                    "arxiv_id": "2601.07338",
                    "authors": "Yanzhi Tian, Cunxiang Wang, Zeming Liu, Heyan Huang, Wenbo Yu, Dawei Song, Jie Tang, Yuhang Guo",
                    "summary": "Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了 RATE，一个基于智能体的翻译评估框架，其中包含一个具有自我反思能力的核心智能体，并能动态调用专门的子智能体。这涉及了单智能体的自我反思、工具使用以及多智能体协作，符合 LLM 智能体的研究范围。",
                    "summary2": "本文旨在解决非字面翻译评估中现有指标失效的问题。针对包含俚语、隐喻等复杂语言现象的场景，我们提出了一种名为 RATE 的 Reflective Agentic Translation Evaluation 框架，通过 Core Agent 动态调用子代理获取外部知识并校准分数。我们在构建的 MENT 数据集上通过 Meta Score 验证了其有效性，结果显示 RATE 显著优于现有指标。",
                    "summary_translation": "大语言模型显著推动了机器翻译的发展，并将其应用于语言复杂的领域——如社交网络服务、文学等。在这些场景中，翻译往往需要处理非字面表达，从而导致机器翻译指标的不准确。为了系统地研究机器翻译指标的可靠性，我们首先构建了一个专注于非字面翻译的元评估数据集，即 MENT。MENT 涵盖了四个非字面翻译领域，包含源句子与来自不同机器翻译系统的译文配对，并附带 7,530 个人工标注的翻译质量分数。实验结果揭示了传统机器翻译指标的不准确性，以及大语言模型作为评判者的局限性，特别是知识截止和评分不一致的问题。为了缓解这些局限性，我们提出了 RATE，这是一种新颖的基于智能体的翻译评估框架，其核心是一个能够动态调用专门子智能体的反思性核心智能体。实验结果表明了 RATE 的有效性，与当前指标相比，其元分数至少提升了 3.2 分。进一步的实验证明，RATE 在通用领域的机器翻译评估中也具有鲁棒性。代码和数据集可在以下地址获取：https://github.com/BITHLP/RATE。",
                    "inspiration_trace": "基于论文《Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation》，以下是对作者产出该文章核心方法（RATE）的逻辑链推演与思考过程还原：\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM很强大”到“评估标准失效”）**\n\n1.  **现象观察**：作者首先注意到大语言模型（LLMs）极大地推动了机器翻译（MT）的发展，使其应用场景从传统的新闻领域扩展到了社交媒体（SNS）、文学、跨文化内容等复杂领域。\n2.  **核心矛盾**：在这些新场景中，翻译的核心难点不再是字面意义的转换，而是对**非字面表达**（如网络俚语、文化隐喻、诗歌意象）的处理。\n3.  **现有工具的失效**：作者发现，传统的MT评估指标（如BLEU、COMET）依赖于字面重叠或形式化文本匹配，无法理解深层语义，导致评估结果与人类判断严重错位。\n4.  **初步假设**：现有的评估体系已经无法适应LLM时代的非字面翻译需求，必须重新审视评估的可靠性。\n\n### 第二阶段：假设验证与基准构建\n**（从“怀疑指标”到“量化失效”）**\n\n1.  **验证策略**：为了系统性地证明“现有指标不可靠”，作者需要一个专门的测试集。然而，现有的Meta-evaluation数据集（如WMT）多基于新闻或维基百科，缺乏非字面内容。\n2.  **构建MENT数据集**：作者决定构建一个专注于非字面翻译的Meta-evaluation数据集（MENT）。\n    *   **覆盖范围**：选取了四个最具代表性的非字面领域（SNS、跨文化、诗歌、文学）。\n    *   **数据质量**：通过严格的筛选和人工标注，确保数据集包含高难度的语言学挑战。\n3.  **实验验证**：在MENT上测试传统指标和新兴的“LLM-as-a-Judge”方法。\n4.  **发现新问题**：实验证实了传统指标确实失效。虽然“LLM-as-a-Judge”表现较好，但作者敏锐地发现了其两个致命缺陷：\n    *   **知识截止**：LLM无法理解最新的网络流行语或生僻的文化典故。\n    *   **评分不一致**：LLM在打分时存在主观性和波动性，缺乏校准机制。\n\n### 第三阶段：根因分析与思维转向\n**（从“静态评估”到“动态反思”）**\n\n1.  **根因诊断**：作者意识到，单纯依赖LLM的内部参数知识（静态）和单次Prompt（被动）是无法解决上述问题的。\n    *   针对“知识截止”，评估者必须具备**外部检索能力**。\n    *   针对“评分不一致”，评估者必须具备**自我反思与校准能力**。\n2.  **思维跃迁**：作者不再将评估视为一个简单的“输入文本-输出分数”的函数，而是将其建模为一个**需要多步推理、工具调用和决策的智能过程**。这自然引出了“Agent（智能体）”的概念。\n\n### 第四阶段：方法论设计\n**（从“单一模型”到“多智能体协作框架 RATE”）**\n\n为了解决上述根因，作者设计了 **RATE (Reflective Agentic Translation Evaluation)** 框架，其设计逻辑遵循“分而治之”与“动态编排”：\n\n1.  **核心架构设计**：需要一个“大脑”来统筹全局，而不是固定的流水线。因此设计了 **Core Agent（核心智能体）**，采用OODA（观察-调整-决策-行动）循环，根据当前状态动态决定下一步动作。\n2.  **解决“知识截止” -> Search Agent**：\n    *   *思考*：当Core Agent遇到未知俚语或文化背景时，不应瞎猜，而应去查。\n    *   *实现*：设计 **Search Agent**，负责调用搜索引擎获取实时外部知识，并将背景信息回传给Core Agent。\n3.  **解决“评分不一致” -> Comparison Agent**：\n    *   *思考*：绝对分数（如3.5分）很难把握，但相对好坏（A比B好）更容易判断。\n    *   *实现*：设计 **Comparison Agent**，通过将当前译文与历史锚点进行成对比较，来校准分数，将主观判断转化为相对排序。\n4.  **基础评估 -> Evaluation Agent**：\n    *   *思考*：仍需要一个基础模块来执行具体的打分任务。\n    *   *实现*：设计 **Evaluation Agent**，结合Core Agent提供的背景知识，进行初步打分并标记置信度。\n\n### 第五阶段：验证与泛化\n**（从“特定领域”到“通用鲁棒性”）**\n\n1.  **闭环验证**：在MENT数据集上测试RATE。逻辑是：如果RATE确实解决了知识截止和评分不一致，那么它在非字面翻译上的表现应显著优于所有Baseline。实验结果证实了这一点（Meta score提升至少3.2）。\n2.  **鲁棒性检验**：作者进一步思考：这种复杂的Agent框架是否只适用于刁钻的非字面场景？在通用领域（如WMT23）是否会“杀鸡用牛刀”甚至性能下降？\n3.  **结论**：实验证明，由于Core Agent的动态调度能力，RATE在通用领域也能保持与SOTA相当的性能，证明了该方法的普适性和鲁棒性。\n\n---\n\n**总结：作者的思考路径**\n从**发现LLM应用场景下沉带来的评估错位**出发，通过**构建MENT数据集量化了传统方法和静态LLM的缺陷**，进而**诊断出“知识缺失”和“主观波动”两大痛点**，最终**跳出单一模型的思维定式，利用Agent技术构建了一个具备反思、检索和校准能力的动态评估框架（RATE）**，完成了从问题发现到方法创新的全逻辑闭环。"
                },
                {
                    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
                    "arxiv_id": "2601.07264",
                    "authors": "Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya",
                    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究基于LLM的“tool-use agents”（工具使用智能体），分析了工具集成智能体工作流中的校准问题，并提出了通过强化学习微调来构建具有自我意识的智能体。这完全符合“单智能体：工具使用”的研究范围。",
                    "summary2": "本文旨在解决Tool-use agents中的miscalibration问题。针对Evidence tools导致overconfidence的场景，我们提出了Calibration Agentic RL (CAR)框架，利用Margin-Separated Calibration Reward (MSCR)联合优化任务准确性与校准。我们在NQ、HotpotQA、SimpleQA-verified及AIME、MATH-500数据集上，通过Accuracy、ECE、Brier Score和AUROC验证了其有效性，显著提升了模型的校准能力与泛化性。",
                    "summary_translation": "基于大语言模型 (LLMs) 的自主代理正在快速发展以处理多轮任务，但确保其可信度仍然是一个关键挑战。这种可信度的一个基本支柱是校准，它指的是代理表达能够可靠反映其实际性能的置信度的能力。尽管校准在静态模型中已有深入研究，但其在集成工具的代理工作流中的动态变化仍未被充分探索。在这项工作中，我们系统地调查了工具使用代理中的语言化校准，揭示了由工具类型驱动的基本置信度二分法。具体而言，我们的试点研究表明，证据工具（如 web search）由于检索信息中固有的噪声，会系统性地导致严重的过度自信，而验证工具（如 code interpreters）可以通过确定性反馈来锚定推理并减轻校准偏差。为了在不同工具类型间稳健地提升校准性能，我们提出了一种强化学习 (RL) 微调框架，该框架联合优化任务准确性和校准性能，并得到了全面的奖励设计基准的支持。我们证明，经过训练的代理不仅实现了卓越的校准性能，而且表现出从本地训练环境到嘈杂的网络设置以及数学推理等不同领域的稳健泛化能力。我们的结果强调了针对工具使用代理采用特定领域校准策略的必要性。更广泛地说，这项工作为构建能够在高风险的现实世界部署中可靠地传达不确定性的自我感知代理奠定了基础。",
                    "inspiration_trace": "基于论文《The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents》，以下是对作者核心方法论产出逻辑链的系统推演：\n\n### 1. 宏观背景与核心矛盾：从“能力”到“可信度”\n**思考起点：**\n随着大语言模型（LLM）向智能体演进，工具使用（如搜索、代码解释器）极大地扩展了模型的能力边界。然而，作者敏锐地捕捉到了一个被忽视的关键问题：**信任危机**。\n**逻辑推演：**\n*   现有研究多关注智能体“能不能做”，而忽略了“知不知道自己能不能做”。\n*   在高风险场景下，智能体的**校准**能力——即其表达的置信度与实际表现的一致性——是可信度的基石。\n*   **初步观察：** 现有文献指出，引入工具后，智能体往往比静态模型表现出更严重的过度自信。这引发了一个根本性的疑问：**工具使用本身是否就是导致校准失效的元凶？**\n\n### 2. 深入探究与假设提出：打破“工具”的刻板印象\n**思考转折：**\n作者没有接受“工具导致过度自信”这一笼统结论，而是试图解构“工具”这一概念。\n**逻辑推演：**\n*   **假设：** 并非所有工具都对校准产生相同影响。工具的**性质**（反馈机制、输出确定性）可能决定了其对置信度的不同影响。\n*   **分类维度：** 作者将工具划分为两类典型范式：\n    1.  **证据工具：** 如网络搜索。特征是输出开放、充满噪声、缺乏明确的负向反馈（搜索总是有结果的，无论是否相关）。\n    2.  **验证工具：** 如代码解释器。特征是输出确定、提供执行反馈（代码会报错），能提供逻辑上的“落地”。\n\n### 3. 验证与发现：揭示“置信度二分法”\n**思考过程：**\n通过设计对比实验（直接提示 vs. 工具使用 vs. RL微调），作者验证了上述假设，发现了核心现象——**置信度二分法**。\n**逻辑推演：**\n*   **证据工具的陷阱：** 在使用网络搜索时，智能体表现出严重的过度自信。原因在于“检索行为”本身被模型误认为是“尽职调查”，且检索到的噪声信息被误认为确凿证据，导致虚假的确定性。\n*   **验证工具的锚定：** 在使用代码解释器时，智能体的校准度反而提升。因为确定性的执行反馈（如报错信息）为推理过程提供了现实约束，抑制了盲目的自信。\n*   **结论：** 校准失效并非工具使用的普遍后果，而是特定于**证据工具**带来的噪声干扰。这指明了后续研究的靶心：**如何修复证据工具导致的过度自信？**\n\n### 4. 方法论构建：从“提示工程”到“内在校准”\n**思考转折：**\n既然证据工具的噪声无法完全消除，且简单的提示工程无法解决根本问题（实验表明Prompting-based策略依然失效），作者转向通过训练来改变模型的内在置信度生成机制。\n**逻辑推演：**\n*   **技术选型：** 采用强化学习（RL）进行微调，因为智能体本身就是通过RL训练来使用工具的，这能保持任务能力的连贯性。\n*   **核心挑战：** 如何设计奖励函数？传统的奖励仅关注任务准确性，这往往鼓励模型“瞎猜”或过度自信。引入校准项（如Brier Score）虽然能惩罚置信度偏差，但存在一个隐患：**激励重叠**。\n\n### 5. 核心创新：解决“安全失败”的激励冲突\n**思考深化：**\n作者深入分析了现有校准奖励（如RLCR）的缺陷，发现了一个逻辑漏洞：如果对“低置信度的错误回答”给予过高的奖励（因为它诚实），模型可能会学会“安全失败”——即为了获得校准分而故意降低置信度，甚至放弃尝试正确回答。\n**逻辑推演：**\n*   **设计原则：** 必须建立严格的优先级。**“做对”必须永远优于“做错”**，无论置信度如何。\n*   **方案提出：** **边际分离校准奖励**。\n    *   **机制：** 强制将奖励空间划分为两个互不重叠的区域。所有正确答案的奖励下限，必须高于所有错误答案的奖励上限。\n    *   **效果：** 这消除了模型通过“诚实但错误”来投机取巧的动机，迫使模型在追求正确性的前提下，再去优化置信度的表达。\n\n### 6. 验证与泛化：从实验室到现实世界\n**思考闭环：**\n为了证明CAR框架不仅仅是过拟合训练数据，作者设计了更具挑战性的验证场景。\n**逻辑推演：**\n*   **环境泛化：** 从干净的本地检索环境迁移到充满噪声的真实API环境（如Serper API）。结果证明，模型学到的不是死记硬背的特定置信度值，而是一种对不确定性的感知能力。\n*   **领域泛化：** 将该方法应用于数学推理（验证工具场景）。虽然验证工具本身有助于校准，但CAR框架依然能带来额外提升，证明了该方法的通用性。\n\n### 总结：思想演进脉络\n1.  **观察：** 智能体越强，越容易盲目自信（可信度危机）。\n2.  **质疑：** 是所有工具都导致盲目自信吗？\n3.  **发现：** 只有“证据工具”（如搜索）因噪声导致过度自信，而“验证工具”（如代码）反而能锚定置信度（二分法）。\n4.  **定位：** 重点解决证据工具场景下的校准问题。\n5.  **洞察：** 现有的校准训练方法存在“安全失败”的漏洞，可能鼓励模型“躺平”。\n6.  **解决：** 提出CAR框架与MSCR奖励，通过严格分离正确与错误的奖励边界，迫使模型在追求准确的同时学会表达不确定性。"
                },
                {
                    "title": "ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity A REM-Inspired System Design for Emergent Creative Ideation",
                    "arxiv_id": "2601.07121",
                    "authors": "Makoto Sato",
                    "summary": "Large language models (LLMs) are used not only for problem solving but also for creative ideation; however, eliciting serendipitous insights that are both novel and internally coherent remains difficult. While stochastic sampling promotes novelty, it often degrades consistency. Here, we propose ReMIND, a REM-inspired modular framework for ideation. ReMIND consists of four stages: wake, which generates a stable low-temperature semantic baseline; dream, which performs high-temperature exploratory generation; judge, which applies coarse evaluation to filter incoherent outputs and extract candidate ideas; and re-wake, which re-articulates selected ideas into coherent final outputs. By instantiating each stage as an independent LLM, ReMIND enables functional separation between exploration and consolidation. Parameter sweeps show that ReMIND reliably induces semantic exploration while preserving downstream stability. Embedding-based analyses confirm substantial semantic displacement during the dream phase, whereas external evaluations reveal that high-quality ideas emerge sporadically rather than as extrema along any single metric. These results suggest that serendipitous ideation in LLMs is a rare-event process best approached through system level design that shapes the conditions under which valuable ideas can emerge and be stabilized. ReMIND provides a general framework for studying the computational basis of serendipity and illustrates how modular LLM orchestration can bridge exploration and stabilization.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了ReMIND框架，通过将四个独立的LLM实例化为不同的模块（Wake, Dream, Judge, Re-wake）来协同完成创意构思任务。这属于多智能体协作（角色分工）和自我反思（Judge阶段评估与过滤）的研究范畴，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决LLM难以生成兼具新颖性与连贯性的Serendipitous insights的问题。针对Creative ideation场景，我们提出了一种受REM睡眠启发的模块化框架ReMIND，通过Wake、Dream、Judge和Re-wake四个阶段实现探索与巩固的功能分离。在多种Conceptual pair prompts上，通过外部LLM评估和Embedding-based similarity analysis验证了其有效性。",
                    "summary_translation": "大语言模型不仅用于问题解决，还用于创造性构思；然而，要诱导出既新颖又内部连贯的意外洞察仍然困难重重。虽然随机采样有助于提升新颖性，但往往会损害一致性。在此，我们提出了 ReMIND，这是一种受 REM (快速眼动睡眠) 启发的模块化构思框架。ReMIND 包含四个阶段：wake (清醒)，用于生成稳定的低温度语义基线；dream (做梦)，用于执行高温度的探索性生成；judge (评判)，用于应用粗粒度评估以过滤不连贯的输出并提取候选想法；以及 re-wake (再清醒)，用于将选定的想法重新阐述为连贯的最终输出。通过将每个阶段实例化为一个独立的 LLM (大语言模型)，ReMIND 实现了探索与巩固之间的功能分离。参数扫描表明，ReMIND 能够可靠地诱导语义探索，同时保持下游稳定性。基于嵌入的分析证实，在 dream (做梦) 阶段存在显著的语义位移，而外部评估显示，高质量想法是零星出现的，而非作为任何单一指标的极值而存在。这些结果表明，LLM 中的意外构思是一个稀有事件过程，最好通过系统级设计来应对，这种设计塑造了有价值想法涌现并得以稳定的条件。ReMIND 为研究机缘巧合的计算基础提供了一个通用框架，并阐明了模块化 LLM (大语言模型) 编排如何桥接探索与稳定。",
                    "inspiration_trace": "基于对论文《ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：问题识别与核心矛盾\n**（从宏观现象出发）**\n\n1.  **观察现状**：大语言模型（LLMs）已被广泛应用于创意生成，但面临一个根本性瓶颈——难以同时兼顾“新颖性”与“连贯性”。\n2.  **剖析矛盾**：\n    *   **高随机性**：提高采样温度虽然能增加新颖性，但往往导致输出语无伦次、事实错误。\n    *   **低随机性**：降低温度保证了逻辑连贯，但输出多为陈词滥调，缺乏真正的洞察力。\n3.  **现有方法的局限**：目前的创意生成主要依赖“单次生成”，试图在同一个模型实例中平衡探索与约束。作者意识到，这种“单体模型”的范式本质上将创造力视为一种参数调优的权衡，而非一种可被系统化设计的认知过程。\n\n### 第二阶段：跨学科启发与理论假设\n**（引入生物学视角）**\n\n1.  **寻找灵感**：作者将目光转向人类认知科学，特别是关于“顿悟”产生机制的研究。\n2.  **关键隐喻——REM睡眠**：心理学和神经科学研究表明，人类的创造性洞察常发生在快速眼动（REM）睡眠期间。\n    *   **机制**：在REM阶段，大脑的海马体进行广泛的联想探索（记忆重组、概念松绑），而负责逻辑判断的前额叶皮层活动减弱（去抑制）。\n    *   **后续**：这种不受约束的探索之后，必须经历一个“稳定化”阶段，将碎片化的洞察重新整合进清醒时的逻辑框架中。\n3.  **提出假设**：如果人工系统的创造力也遵循这一机制，那么解决LLM创意瓶颈的关键不在于优化单一模型的参数，而在于**功能解耦**——将“探索”与“巩固”在时间和计算上分离开来。\n\n### 第三阶段：方法论构建与架构设计\n**（从理论到系统设计）**\n\n1.  **设计原则**：构建一个受REM启发的模块化框架，明确划分“探索”、“评估”和“巩固”三个阶段。\n2.  **模块定义与功能映射**：\n    *   **Wake（清醒/锚点）**：使用低温度采样，生成一个稳定、符合逻辑的基线输出。其作用不是产生创意，而是作为语义锚点，确保后续生成不偏离主题。\n    *   **Dream（做梦/探索）**：使用高温度采样，故意放松逻辑约束，进行疯狂的语义跳跃和概念重组。这一阶段模拟REM睡眠，允许产生看似荒谬但可能蕴含潜力的组合。\n    *   **Judge（评判/筛选）**：作为一个独立的过滤器，不参与生成，仅评估Dream输出的连贯性，并提取出有潜力的“候选想法”。这模拟了大脑对记忆痕迹的初步筛选。\n    *   **Re-wake（再清醒/巩固）**：这是最关键的一步。重新调用Wake模型，将Judge筛选出的“碎片化创意”进行重述和润色。\n    *   **逻辑闭环**：通过Re-wake，系统利用低温度模型的逻辑能力，将高温度探索出的“狂野想法”驯化为人类可理解的、连贯的最终输出。\n3.  **核心创新点**：作者意识到，**同一个模型在不同阶段扮演不同角色**。Wake在第一阶段是“锚点”，在最后阶段变成了“稳定器/压缩器”。\n\n### 第四阶段：实验验证与现象洞察\n**（通过实证修正认知）**\n\n1.  **验证策略**：如何证明这种方法真的产生了“有意义的意外”？\n    *   **量化语义位移**：使用嵌入向量计算Wake输出与Dream输出的余弦相似度。数据证实，Dream阶段确实导致了显著的语义漂移（探索发生）。\n    *   **外部评估**：使用更强的外部模型（如GPT-5.2）对最终输出进行评分。\n2.  **关键发现**：\n    *   高质量创意并非均匀分布，而是**稀疏出现的**。\n    *   即使在相同参数下，也只有部分运行产生了极具价值的洞察。\n3.  **理论修正**：这一发现促使作者将“意外创意”重新定义为一种**“稀有事件过程”**。这意味着我们无法通过确定性算法“制造”创意，但可以通过系统设计**提高其涌现的概率**。\n\n### 第五阶段：哲学升华与范式转移\n**（最终结论）**\n\n1.  **总结范式**：作者最终提出，通往人工创造力的路径不是单纯扩大模型规模或增加数据量，而是**“思维的功能编排”**。\n2.  **系统观**：ReMIND不仅仅是一个提示技巧，它代表了一种新的系统设计哲学——**BiMoLLM（脑启发模块化LLM）**。即通过模块间的交互涌现出高阶智能，而非依赖单一模型的万能性。\n3.  **最终产出**：文章产出了一套可复现的、将生物学认知过程转化为计算架构的工程框架，证明了通过分离探索与巩固，可以在保持逻辑连贯性的同时，显著提升LLM产生意外洞察的能力。"
                },
                {
                    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
                    "arxiv_id": "2601.06966",
                    "authors": "Haonan Bian, Zhiyuan Yao, Sen Hu, Zishan Xu, Shaolei Zhang, Yifu Guo, Ziliang Yang, Xueran Han, Huacan Wang, Ronghao Chen",
                    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究LLM作为自主通用智能体的记忆机制（属于单智能体核心能力），并在数据构建中使用了多智能体对话生成，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有LLM记忆基准难以评估长期项目导向交互的问题。针对现实世界中动态演进的项目场景，我们提出了RealMem基准及其包含项目基础构建、多智能体对话生成及记忆日程管理的三阶段合成管道。我们在包含11个场景、2000+跨会话对话的RealMem数据集上，通过Recall@k、NDCG@k及QA Score等指标验证了其有效性，揭示了现有系统在动态状态管理上的不足。",
                    "summary_translation": "随着 Large Language Models (LLMs，大语言模型) 从静态对话接口演变为 autonomous general agents (自主通用智能体)，有效的 memory (记忆) 对于确保 long-term consistency (长期一致性) 至关重要。然而，现有的 benchmarks (基准测试) 主要关注 casual conversation (日常闲聊) 或 task-oriented dialogue (任务导向对话)，未能捕捉到 **“long-term project-oriented” (长期项目导向)** 的交互，在此类交互中，agents (智能体) 必须跟踪 evolving goals (不断演进的目标)。为了弥合这一差距，我们介绍了 **RealMem**，这是首个 grounded in realistic project scenarios (基于现实项目场景) 的 benchmark (基准测试)。RealMem 包含跨越 11 个场景的 2,000 多个 cross-session dialogues (跨会话对话)，并利用 natural user queries (自然用户查询) 进行评估。我们提出了一个 synthesis pipeline (合成流程)，该流程整合了 Project Foundation Construction (项目基础构建)、Multi-Agent Dialogue Generation (多智能体对话生成) 以及 Memory and Schedule Management (记忆与日程管理)，以模拟 memory (记忆) 的 dynamic evolution (动态演变)。实验表明，当前的 memory systems (记忆系统) 在管理现实世界项目中固有的 long-term project states (长期项目状态) 和 dynamic context dependencies (动态上下文依赖) 方面面临重大挑战。我们的代码和数据集可在 [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench) 获取。",
                    "inspiration_trace": "基于论文《RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体产出的思考过程：\n\n### 1. 宏观观察：从“对话”到“代理”的范式转变\n**思考起点**：作者首先注意到大语言模型（LLMs）的发展趋势正在发生质变。模型不再仅仅是简单的“聊天机器人”，而是正在演变为能够长期协作的“自主智能体”。\n**核心洞察**：在这种新范式下，**“记忆”** 成为了决定性因素。没有有效的记忆，智能体就无法在长期交互中保持一致性，也无法实现真正的个性化与通用人工智能（AGI）。\n\n### 2. 问题聚焦：现有基准的“静态”局限\n**观察现状**：作者审视了现有的记忆评估基准（如 LoCoMo, LongMemEval, HaluMem），发现它们大多存在一个共同的缺陷：**过于“静态”和“孤立”**。\n*   **LoCoMo** 仅关注社交闲聊。\n*   **LongMemEval** 类似于“大海捞针”测试，侧重于孤立的事实检索。\n**提出质疑**：这些基准测试的是“你记住了某个事实吗？”，而不是“你能否在复杂、变化的环境中利用记忆推进项目？”。它们无法反映真实世界中**长期、跨会话、目标导向**的交互逻辑。\n\n### 3. 核心假设：真实交互是“项目导向”的\n**定义新范式**：作者提出，真实世界的记忆驱动交互应当属于第三种范式——**“长期项目导向交互”**。\n**提炼特征**：为了构建这一新范式，作者抽象出了四个关键特征，这也是后续方法设计的指导原则：\n1.  **内生性查询**：问题源于任务进展，而非孤立的事实核查。\n2.  **交错分布**：对话在多个项目间穿插（如健身与旅行计划交替进行）。\n3.  **动态状态演化**：环境非静止，记忆需随状态（如受伤、计划变更）同步更新。\n4.  **主动上下文对齐**：智能体需利用记忆主动推断模糊意图，而非被动应答。\n\n### 4. 方法构建：如何模拟“动态演化”？\n**面临的挑战**：如何获取包含数千次跨会话对话、且具有复杂逻辑一致性的真实数据？显然，人工标注不现实，现有数据集也不存在。\n**解决思路**：作者决定采用**合成数据**的方法，但必须解决“长期生成容易逻辑崩塌”的问题。为此，设计了一个**三阶段合成流水线**：\n\n*   **阶段一：项目基础构建**\n    *   *思考*：先搭骨架，再填血肉。\n    *   *逻辑*：先定义用户画像和项目目标，再生成“蓝图”和“事件列表”。这确保了全局逻辑的连贯性，防止后续对话跑偏。\n\n*   **阶段二：多智能体对话生成**\n    *   *思考*：模拟真实博弈，而非单向生成。\n    *   *逻辑*：引入“用户智能体”和“助手智能体”。用户智能体只能看到当前会话摘要（模拟人类遗忘），助手智能体拥有完整记忆。这种不对称信息设置迫使模型必须依赖记忆机制来维持对话。\n\n*   **阶段三：记忆与日程管理**\n    *   *思考*：形成闭环反馈，确保记忆“活着”。\n    *   *逻辑*：对话生成后，通过专门的代理提取记忆点、更新日程表、去重。这些更新后的记忆又会作为下一轮对话的上下文输入。这模拟了记忆随时间动态演化的过程。\n\n### 5. 评估洞察：从“检索”到“状态管理”\n**重新定义评估标准**：作者意识到，传统的检索指标（如 Recall）不足以衡量项目导向任务。\n**逻辑推演**：在复杂项目中，**精确度**比**召回率**更重要。如果检索到了大量相关但充满噪音的信息，反而会干扰模型决策。\n**新指标设计**：因此，作者引入了基于 LLM 的语义评估（如 Mem Recall, Mem Helpful）和 QA Score，重点考察模型是否正确利用了**动态状态**，而不仅仅是生成了流畅的文本。\n\n### 6. 最终产出：RealMem 的诞生\n**结论验证**：通过实验，作者发现现有的 SOTA 记忆系统（如 Mem0, MemoryOS）在处理动态更新和主动对齐时依然表现不佳，证明了该基准的有效性和挑战性。\n**价值定位**：RealMem 不仅仅是一个数据集，它是一个**诊断工具**，揭示了当前智能体在处理长期、复杂、动态项目时的核心瓶颈，迫使社区从“静态知识库”向“动态状态管理器”转变。\n\n---\n\n**总结**：作者的思考路径是从**“智能体需要长期记忆”**这一宏观趋势出发，通过批判现有基准的**“静态性”**，提出了**“项目导向”**的动态交互假设，进而通过**“分层合成+闭环反馈”**的方法论解决了数据构建难题，最终建立了一套能够真实反映智能体动态记忆管理能力的评估体系。"
                },
                {
                    "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
                    "arxiv_id": "2601.06922",
                    "authors": "Tianhua Zhang, Kun Li, Junan Li, Yunxiang Li, Hongyin Luo, Xixin Wu, James Glass, Helen Meng",
                    "summary": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“Agentic RAG”，将问答视为推理与信息检索（工具使用）之间的多步交互。它提出了一种基于树的强化学习框架来优化智能体的决策过程，属于单智能体和自我演化的研究范畴。",
                    "summary2": "本文旨在解决Agentic RAG中仅结果监督的强化学习面临的信用分配难题。针对多步推理与检索交互场景，我们提出了一种名为TREE PS-RAG的在线树结构强化学习框架，该方法将推理过程建模为树，利用蒙特卡洛估计从后代结果中推导步骤优势，无需中间标注。在七个QA基准上通过Exact Match指标验证，其性能显著优于现有基线。",
                    "summary_translation": "代理式检索增强生成将问答任务构建为推理与信息检索之间的多步交互过程，并近期通过基于结果的监督强化学习得到了推进。尽管行之有效，但仅依赖稀疏的最终奖励限制了逐步信用分配，且对中间推理和动作的指导作用较弱。近期的研究探索了过程级监督，但通常依赖于离线构建的训练数据（存在分布偏移的风险），或者需要高昂成本的中间标注。本文提出了 TreePS-RAG，这是一种用于代理式 RAG 的在线、基于树的强化学习框架，能够在保留标准仅基于结果奖励的同时实现逐步信用分配。我们的核心思想是将代理式 RAG 的推理过程建模为一棵推演树，其中每个推理步骤自然地映射为一个节点。这种树结构允许通过对其后代结果进行蒙特卡洛估计来估算步骤效用，从而在无需中间标签的情况下获得细粒度的过程优势。为了使该范式具有实用性，我们引入了一种高效的在线树构建策略，能够在受限的计算预算下保持探索多样性。在与 Search-R1 等强基线相当的推演成本下，在多个模型规模的七个多跳和通用问答基准上进行的实验表明，TreePS-RAG 始终显著优于基于结果监督和领先的过程监督强化学习方法。",
                    "inspiration_trace": "基于论文《TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG》的内容，以下是对作者核心方法论产出过程的逻辑推演与还原：\n\n### 第一阶段：问题锚定——从“结果导向”到“过程黑箱”的困境\n\n**1. 宏观观察：**\n作者首先关注到 Agentic RAG（智能体检索增强生成）已成为解决复杂多跳问答的主流范式。为了优化这一过程，学术界引入了强化学习（RL），特别是像 Search-R1 这样的方法，利用最终答案的正确性作为奖励信号来训练智能体。\n\n**2. 痛点识别：**\n然而，作者敏锐地发现这种仅依赖“结果监督”的方法存在一个核心缺陷：**信用分配难题**。\n*   **逻辑推演：** 在一个多步推理的轨迹中，如果最终答案错误，RL 算法通常会对所有步骤进行“连坐”惩罚。但实际上，可能只有中间某一步的检索或推理是致命的，其他步骤可能是正确的。\n*   **结论：** 稀疏且滞后的最终奖励无法提供细粒度的指导，限制了智能体学习高效搜索和推理策略的能力。\n\n### 第二阶段：路径探索——理想与现实的博弈\n\n**1. 提出假设：**\n既然结果监督太粗糙，那么“过程监督”显然是更好的选择。即，对每一个中间步骤（如搜索查询、推理片段）都给出一个即时奖励。\n\n**2. 现实阻碍：**\n作者审视了现有的过程监督方案，发现了两个不可忽视的障碍：\n*   **标注成本高：** 获取高质量的中间步骤标注（如每一步的子问题是否正确）极其昂贵。\n*   **分布偏移：** 许多方法（如 ReasonRAG）依赖离线构建的数据集进行训练。这意味着智能体是在“静态”的过去数据上学习，而非在“动态”的在线交互中学习，导致模型在面对新环境时泛化能力下降。\n\n**3. 核心矛盾：**\n我们需要**细粒度的过程信号**，但我们必须在**无中间标注**且**在线**的约束下获得它。\n\n### 第三阶段：核心洞察——将“树”转化为“自监督工具”\n\n**1. 思维跃迁：**\n如何在不依赖外部标注者的情况下评估一个中间步骤的好坏？作者借鉴了蒙特卡洛树搜索（MCTS）的思想，提出了一个关键假设：\n*   **假设：** 如果一个中间步骤（节点）是好的，那么从该步骤出发，通过多次随机探索（rollout），最终得到正确答案的概率应该很高。\n\n**2. 结构化建模：**\n基于上述假设，作者将 Agentic RAG 的推理过程重新定义为**树结构**，而非线性的轨迹。\n*   **逻辑映射：** 每一个推理步骤对应树上的一个节点。从根节点到叶节点的路径代表一条完整的推理轨迹。\n*   **价值反推：** 不需要人为给中间步骤打分。只需看该节点下的所有“子孙”叶节点（最终结果）的平均奖励。如果后代大多答对了，那么这个中间节点的价值就高。\n\n**3. 解决矛盾：**\n这种方法巧妙地绕过了“标注”和“离线”的障碍：\n*   **无标注：** 价值估计完全基于易于获取的最终答案。\n*   **在线性：** 树是在训练过程中实时构建和探索的，完全符合在线 RL 的范式。\n\n### 第四阶段：工程落地——在有限预算下驯服“指数爆炸”\n\n**1. 新的挑战：**\n虽然树结构在理论上完美，但在实际计算中，随着深度增加，节点数量会呈指数级爆炸。如果无限制地展开树，计算成本将不可接受。\n\n**2. 约束设定：**\n作者设定了一个硬性约束：**计算成本必须与传统的线性采样方法（如 Search-R1）相当**。即，总采样节点数 $N$ 必须固定。\n\n**3. 策略优化：**\n为了在固定预算 $N$ 下最大化树的效用，作者引入了两个关键机制：\n*   **动态分支控制：** 不再平均用力，而是根据当前层的节点数量动态分配下一层的分支数，确保总节点数维持在预算 $N$ 附近。\n*   **语义剪枝：** 作者意识到，如果两个搜索步骤检索到的文档高度重合，那么它们就是冗余的。为了在有限预算下探索更多可能性，必须去除冗余。\n    *   **逻辑：** 利用检索文档的 Jaccard 相似度来衡量节点间的语义距离，通过聚类保留多样化的路径，剔除重复探索。\n\n### 第五阶段：方法闭环——从树结构到 RL 优化\n\n**1. 信号生成：**\n通过上述构建的树，作者计算出了每个节点的“过程优势”。\n*   **全局优势：** 当前步骤相对于根节点（整体平均水平）的提升。\n*   **局部优势：** 当前步骤相对于其父节点（上一步）的提升。\n\n**2. 训练整合：**\n最后，将这些树结构推导出的细粒度优势值，无缝集成到标准的策略梯度算法（如 GRPO/PPO）中。\n*   **逻辑：** 模型不再只对最终答案负责，而是对树中每一个经过的推理步骤负责。这使得模型能够精确地学习到“哪一步检索是关键的”、“哪一步推理是多余的”。\n\n---\n\n**总结：**\n作者的思考路径是从**发现稀疏奖励的局限性**出发，试图引入过程监督但受限于**标注成本和分布偏移**，最终通过**树结构建模**将“最终结果”转化为“中间步骤的价值估计”，并利用**剪枝策略**解决了计算复杂度问题，从而实现了一种无需标注、在线且高效的 Agentic RAG 训练框架。"
                },
                {
                    "title": "AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents",
                    "arxiv_id": "2601.06818",
                    "authors": "Xuannan Liu, Xiao Yang, Zekun Li, Peipei Li, Ran He",
                    "summary": "As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体，提出了针对智能体工作流中幻觉归因的基准测试。研究内容明确涉及智能体的核心能力，如规划、工具使用和多步推理，属于单智能体研究范畴。虽然涉及幻觉（可靠性），但重点在于评估智能体轨迹而非被排除的安全对齐或纯推理问题。",
                    "summary2": "本文旨在解决LLM-based agents在多步工作流中难以定位和解释幻觉起源的问题。针对多步Agent轨迹，我们提出了一种**automated hallucination attribution**新任务，并构建了包含693条高质量轨迹及系统化分类法的**AgentHallu** benchmark。我们在该数据集上评估了13个主流LLM，通过**step localization accuracy**和**G-EVAL scores**验证了其有效性。实验表明，即使是顶尖模型（如Gemini-2.5-Pro）在步骤定位上也仅达到41.1%的准确率，凸显了该任务的挑战性。",
                    "summary_translation": "由于基于大语言模型的智能体在执行顺序多步推理时，中间步骤产生的幻觉存在沿轨迹传播的风险，从而降低整体可靠性。与单轮响应中的幻觉检测不同，诊断多步工作流中的幻觉需要识别出导致初始偏差的具体步骤。为填补这一空白，我们提出了一项新的研究任务——基于大语言模型的智能体的自动幻觉归因，旨在识别导致幻觉的步骤并解释其原因。为支持该任务，我们引入了 AgentHallu，这是一个综合基准，包含：(1) 693 条涵盖 7 个智能体框架和 5 个领域的高质量轨迹；(2) 一个包含 5 个大类（规划 Planning、检索 Retrieval、推理 Reasoning、人机交互 Human-Interaction 和工具使用 Tool-Use）及 14 个子类别的幻觉分类体系；(3) 涵盖二分类标签、致幻步骤及因果解释的人工策划多级标注。我们评估了 13 个领先模型，结果表明，即使是顶级模型（如 GPT-5、Gemini-2.5-Pro），该任务也极具挑战性。表现最佳的模型仅实现了 41.1% 的步骤定位准确率，其中工具使用幻觉最为困难，准确率仅为 11.6%。我们相信 AgentHallu 将促进未来关于开发鲁棒、透明且可靠的智能体系统的研究。",
                    "inspiration_trace": "基于对论文《AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：从“单轮对话”到“多步智能体”的范式转移\n**思考起点：**\n随着大语言模型（LLM）的发展，研究热点已从简单的单轮问答转向了复杂的**LLM-based Agents（智能体）**。智能体具备规划、检索、工具调用、多步推理等能力，能够解决长链路任务。\n\n**核心洞察：**\n在单轮对话中，幻觉通常表现为“生成内容与事实不符”。但在智能体的**多步工作流**中，问题变得复杂：中间某一步的错误（如规划失误、检索错误）会像滚雪球一样**向后传播**，导致最终结果错误。\n*   **关键矛盾：** 仅仅判断“最终答案是否错误”（二分类）对于智能体来说远远不够。如果不知道错误是在哪一步产生的，就无法修复智能体，也无法提升其可靠性。\n\n### 2. 问题聚焦：从“检测”到“归因”的认知升级\n**现有局限：**\n作者回顾了现有的幻觉检测基准（如HaluEval, FELM等），发现它们都局限于**单轮响应**的**二分类判断**（是/非幻觉）。这些基准无法回答两个关键问题：\n1.  **Where（在哪里）：** 错误最早出现在轨迹的哪一步？\n2.  **Why（为什么）：** 这一步为什么会出错？\n\n**假设提出：**\n为了构建可靠的智能体系统，必须提出一个新的研究任务——**自动化幻觉归因**。这个任务的目标不仅仅是发现错误，而是要像调试程序一样，**定位导致错误的“源代码行”（步骤）并解释原因**。\n\n### 3. 方法论构建：如何定义和量化“归因”？\n**思考难点：**\n在多步轨迹中，错误往往具有连锁反应。例如，第1步规划错了，导致第3步工具调用错了，最后第5步答案错了。究竟哪一步才是“负责”的？\n\n**逻辑定义（因果对齐）：**\n作者引入了因果推断的思想来定义“负责步骤”：\n*   **反事实推理：** 如果修正了某一步 $u_t$，并重新执行后续步骤，最终答案变正确了，那么 $u_t$ 就是幻觉的根源。\n*   **最小化原则：** 如果有多个步骤都满足上述条件，取最早的那一步（即错误的源头）。\n\n### 4. 数据构建：如何设计基准以覆盖智能体的复杂性？\n**思考路径：**\n既然要评估“归因”，数据集就不能只有问答对，必须包含完整的**思维-行动-观察**轨迹。同时，智能体的幻觉类型是多样的，不能一概而论。\n\n**分类学构建：**\n作者没有凭空想象类别，而是通过**扎根理论**分析数据，归纳出智能体特有的5大幻觉类别，对应智能体的核心能力模块：\n1.  **Planning（规划）：** 目标分解错误。\n2.  **Retrieval（检索）：** 查询或上下文错误。\n3.  **Reasoning（推理）：** 逻辑或计算错误。\n4.  **Human-Interaction（人机交互）：** 误解人类反馈。\n5.  **Tool-Use（工具使用）：** 工具参数或调用错误。\n\n**数据筛选策略：**\n为了保证基准的挑战性，作者制定了严格的过滤标准：\n*   **排除非欺骗性失败：** 剔除那些直接报错、崩溃的简单案例（太容易检测）。\n*   **保留“ plausible but wrong”：** 专注于那些看起来逻辑通顺、但结果错误的轨迹，这才是归因的难点所在。\n\n### 5. 评估验证：证明任务的必要性与难度\n**逻辑闭环：**\n如果现有的顶尖模型（如GPT-5, Gemini-2.5-Pro）能轻松完成这个任务，那么这个基准就没有价值。\n\n**实验设计：**\n作者在这些模型上测试了两种Prompting策略（标准Prompt vs. 逐步Prompt）。\n*   **预期结果：** 即使是最强的模型，在步骤定位上的准确率也很低（约41%），特别是在工具使用幻觉上（仅11.6%）。\n*   **结论：** 这证实了“幻觉归因”确实是一个尚未解决的难题，从而确立了AgentHallu基准的学术价值——它为未来的研究指明了方向（即如何让模型具备自我诊断和因果解释的能力）。\n\n---\n\n**总结：作者的思考链条**\n1.  **观察现象：** 智能体的多步特性导致错误传播，单轮检测失效。\n2.  **提出假设：** 需要从“判断对错”升级为“定位源头+解释原因”。\n3.  **形式化定义：** 利用反事实推理定义“负责步骤”。\n4.  **工程实现：** 构建包含多维度分类和细粒度标注的AgentHallu数据集。\n5.  **验证价值：** 通过实验证明现有SOTA模型在此任务上的不足，确立研究基准。"
                },
                {
                    "title": "IDRBench: Interactive Deep Research Benchmark",
                    "arxiv_id": "2601.06676",
                    "authors": "Yingchaojie Feng, Qiang Huang, Xiaoya Xie, Zhaorui Yang, Jun Yu, Wei Chen, Anthony K. H. Tung",
                    "summary": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了针对LLM驱动的深度研究智能体的基准，涉及多智能体框架、Web探索（工具使用）以及通过交互反馈进行动态调整（自我反思/演化），符合多智能体协作及单智能体工具使用的研究范围。",
                    "summary2": "本文旨在解决现有深度研究基准忽略交互动态评估的问题。针对未明确指定的查询场景，我们提出了IDRBench，包含交互式多代理框架、基于参考的User Simulator及交互感知评估套件。我们在引入模糊性注入的数据集上，通过Report Similarity、LLM-ACS及Interaction Turns等指标，验证了交互能显著提升研究质量与鲁棒性。",
                    "summary_translation": "由大语言模型驱动的深度研究代理能够执行多步推理、网络探索和长篇报告生成。然而，大多数现有系统以自主模式运行，假设用户意图已完全明确，且仅评估最终输出。在实践中，研究目标往往定义不足且在探索过程中不断演变，因此持续的交互对于实现鲁棒对齐至关重要。尽管交互至关重要，但现有的深度研究基准大多未将其纳入考量，既未对动态用户反馈进行建模，也未量化交互成本。我们介绍了IDRBench，这是首个用于系统性评估交互式深度研究的基准。IDRBench结合了具备按需交互功能的模块化多代理研究框架、可扩展的基于参考的用户模拟器，以及一个交互感知评估套件；该套件能够联合衡量交互收益（质量与对齐度）和交互成本（交互轮次与令牌数）。针对七个最先进大语言模型的实验表明，交互能够持续提升研究质量和鲁棒性，其效果往往能超越模型能力差异带来的影响，同时也揭示了在交互效率方面存在显著的权衡。",
                    "inspiration_trace": "基于论文《IDRBench: Interactive Deep Research Benchmark》的内容，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观趋势观察到具体痛点识别，再到方法论创新与验证的完整思考路径。\n\n---\n\n### 1. 宏观观察与趋势捕捉\n**思考起点：** 作者首先关注到大语言模型（LLM）在信息获取领域的演进。\n*   **现象：** LLM的能力已从单轮问答进化为能够进行多步推理、网页探索和长报告生成的“深度研究智能体”。\n*   **现状：** 现有的主流系统（如DeepResearcher等）大多采用**自主模式**，即用户给出初始指令，系统独立完成全过程，最后仅评估生成的报告质量。\n\n### 2. 现实痛点与核心假设\n**深入思考：** 作者敏锐地发现了“自主模式”与“真实研究场景”之间的巨大鸿沟。\n*   **问题识别：**\n    1.  **意图模糊性：** 现实中的用户需求往往是未充分定义的，用户在研究开始时并不清楚自己到底想要什么。\n    2.  **意图漂移：** 在长周期的推理过程中，智能体容易偏离用户初衷，产生幻觉或跑题，缺乏纠偏机制。\n*   **核心假设：** 深度研究不应是“独角戏”，而应是**“交互式协作”**。引入用户反馈可以显著提升研究质量和对齐度，甚至可能弥补模型本身能力的不足。\n\n### 3. 评估盲区的发现\n**关键转折：** 作者意识到，虽然“交互”很重要，但现有的评估体系完全忽略了这一点。\n*   **盲区分析：**\n    *   现有的基准测试都是**静态**的（Query + Reference Document），只看最终结果，不看中间过程。\n    *   这种评估方式无法区分“运气好答对”和“通过交互修正错误”的智能体。\n    *   更重要的是，它们忽略了交互的**成本**（打扰用户的次数、Token消耗）。\n*   **推论：** 要推动交互式研究的发展，必须建立一套能够量化“交互收益”与“交互成本”的新型基准。\n\n### 4. 方法论构建：从概念到落地\n为了验证上述假设并填补评估盲区，作者设计了IDRBench，其构建逻辑遵循以下步骤：\n\n#### A. 数据构建：如何逼真地模拟“需要交互”的场景？\n*   **挑战：** 现有的高质量数据集（如DeepResearch Bench）中的Query往往非常详细，智能体直接执行即可，不需要交互。\n*   **创新思路（模糊性注入）：** 作者决定人为制造“信息差”。通过LLM将原本详细的Query进行压缩（摘要化），保留核心意图但移除具体细节。\n*   **逻辑：** 只有当任务变得“模糊”时，智能体才被迫主动提问，从而触发交互行为。\n\n#### B. 用户模拟：如何实现大规模、可重复的评估？\n*   **挑战：** 真实的人类交互成本高昂且不可控（主观性强、不一致），无法作为大规模Benchmark的组件。\n*   **创新思路（基于参考的模拟器）：** 构建一个基于参考文档的“用户模拟器”。\n*   **逻辑：** 将参考文档视为“上帝视角”的真理。模拟器被设定为：像人类一样简洁回答，提供宏观指导，且拒绝错误选项。这样既保证了反馈的合理性，又实现了评估的标准化。\n\n#### C. 评估体系：如何定义“好的交互”？\n*   **思路：** 交互是一把双刃剑，必须建立多维度的评估指标。\n*   **维度拆解：**\n    1.  **收益：** 交互是否提升了质量？（语义相似度、结构覆盖度、意图满足度）。\n    2.  **成本：** 交互是否太烦人？（交互轮数、消耗的Token数）。\n*   **逻辑：** 只有同时考察这两个维度，才能判断一个智能体是否具备高效的“交互智能”。\n\n#### D. 框架设计：如何让智能体具备交互能力？\n*   **思路：** 基于现有的多智能体架构（规划、研究、生成），嵌入“交互模块”。\n*   **机制设计：**\n    *   **评估器：** 决定“何时”提问（权衡信息增益与打扰成本）。\n    *   **提问器：** 决定“问什么”（生成针对性的澄清问题）。\n*   **逻辑：** 交互不应是随机的，而应是基于当前上下文不确定性的理性决策。\n\n### 5. 实验验证与洞察提炼\n**最终验证：** 通过在多个SOTA模型上的实验，作者验证了最初的假设并发现了更深层的规律。\n*   **发现一：** 交互确实能普遍提升质量，且**交互能力有时比模型本身的原始智力更重要**（例如，开启交互的弱模型可能超过自主运行的强模型）。\n*   **发现二：** 存在**边际递减效应**。强模型通过交互获得的提升较小，而弱模型提升巨大。\n*   **发现三：** 交互策略存在差异。有的模型倾向于“频繁短问”，有的倾向于“少量长问”，这揭示了不同模型在交互效率上的权衡。\n\n### 总结\n作者的思考路径是一个典型的**“观察现象 -> 识别缺陷 -> 提出假设 -> 构建工具（Benchmark） -> 验证假设”**的学术闭环。\n\n其核心贡献不在于发明了一个新的聊天机器人，而在于**重新定义了深度研究的评估范式**——从“静态的结果导向”转向了“动态的过程导向”，并巧妙地通过“模糊性注入”和“用户模拟”解决了交互式系统难以量化评估的难题。"
                },
                {
                    "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
                    "arxiv_id": "2601.06636",
                    "authors": "Wenting Chen, Zhongrui Zhu, Guolin Huang, Wenxuan Wang",
                    "summary": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了名为 ECR-Agent 的智能体架构，该架构包含“Critic-Driven Graph and Memory Evolution (CGME)”组件，涉及记忆存储和迭代完善，符合“单智能体”中的记忆机制以及“自我演化”的研究范围。尽管论文应用于医疗领域，但其核心贡献在于智能体的架构设计（动态因果推理、记忆演化），而非单纯的应用部署。",
                    "summary2": "本文旨在解决医疗大模型在临床诊断中因依赖统计捷径而产生的Einstellung Effect（思维定势效应）。针对医疗诊断场景，我们提出了MedEinst基准测试及ECR-Agent框架。该框架通过Dynamic Causal Inference (DCI)和Critic-Driven Graph & Memory Evolution (CGME)实现基于循证医学的结构化因果推理。我们在MedEinst数据集上通过Bias Trap Rate和Robust Accuracy等指标验证了其有效性，显著降低了模型的误判率。",
                    "summary_translation": "尽管在医学基准测试中取得了高准确率，LLMs（大语言模型）在临床诊断中表现出 Einstellung Effect（定势效应）——即依赖统计捷径而非患者特异性证据，导致在非典型病例中出现误诊。现有的基准测试未能检测到这种关键的失效模式。我们提出了 MedEinst，这是一个包含 49 种疾病共 5,383 对临床病例的反事实基准。每一对病例包含一个对照病例和一个“陷阱”病例，后者通过改变鉴别性证据从而翻转诊断结果。我们通过 Bias Trap Rate（偏差陷阱率）来衡量易感性——即在正确诊断对照病例的情况下误诊陷阱病例的概率。对 17 个 LLMs 的广泛评估表明，前沿模型虽然达到了很高的基线准确率，但存在严重的偏差陷阱率。因此，我们提出了 ECR-Agent，通过两个组件将 LLM 推理与 Evidence-Based Medicine（循证医学）标准对齐：(1) Dynamic Causal Inference (DCI)（动态因果推理）通过双通路感知、跨越三个层次（关联、干预、反事实）的动态因果图推理以及用于最终诊断的证据审计来执行结构化推理；(2) Critic-Driven Graph and Memory Evolution (CGME)（批评驱动的图与记忆演化）通过将验证过的推理路径存储在范例库中并将疾病特异性知识整合到演化的疾病图中，来迭代地优化系统。源代码即将发布。",
                    "inspiration_trace": "基于论文《MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis》，以下是对作者产出该文章核心方法的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定义（从“高分低能”现象切入）\n\n**1. 观察现象：基准测试成绩与临床实战能力的错位**\n作者首先观察到一个矛盾现象：尽管当前的医学大语言模型在USMLE等标准医学基准测试上取得了极高的准确率，但在处理非典型或复杂的临床病例时，仍频繁误诊。\n*   **思考：** 为什么模型通过了“考试”，却在“看病”时失败？\n\n**2. 归因分析：定势效应的发现**\n作者将这种失败归因为心理学中的“定势效应”。即模型倾向于依赖统计捷径——即训练数据中最常见的症状与疾病的关联模式，而不是针对患者特异性证据进行逻辑推理。\n*   **核心洞察：** 模型在做“概率匹配”而非“因果诊断”。当遇到表面症状符合常见病（如流感），但关键细节指向罕见病（如肺栓塞）的病例时，模型会被先验概率“绑架”，忽略关键的反证证据。\n\n---\n\n### 第二阶段：评估工具的缺失与重构（从“静态知识”到“反事实推理”）\n\n**3. 现有工具的局限性分析**\n作者审视了现有的医学基准（如MedQA, DDXPlus），发现它们大多基于独立同分布（I.I.D.）的样本或典型病例。\n*   **逻辑推演：** 在这些数据集上，统计捷径往往能带来正确答案。因此，现有基准无法检测出模型是否真正具备“推翻直觉、依据证据下结论”的能力。我们需要一种能“诱骗”模型暴露其认知偏见的测试工具。\n\n**4. 构建新基准的假设：MedEinst的设计逻辑**\n为了捕捉定势效应，作者提出必须引入“反事实”思维。\n*   **设计思路：** 构建“对照组”与“陷阱组”病例对。\n    *   **对照组：** 典型病例，符合统计直觉。\n    *   **陷阱组：** 在对照组基础上进行最小化修改，仅替换关键的鉴别证据，使得诊断翻转。\n*   **核心指标：** 提出“偏差陷阱率”。即模型能做对对照组（证明有基础能力），却在陷阱组中坚持对照组诊断（证明被偏见误导）的概率。这成功将“推理能力”与“记忆力”解耦。\n\n---\n\n### 第三阶段：深层原因探究（从“概率拟合”到“循证医学”）\n\n**5. 失败模式的微观剖析**\n通过实验，作者发现即便是GPT-5等前沿模型，在陷阱病例上也表现出极高的错误率。进一步分析发现，模型的思维链存在三种缺陷：盲目（忽略关键证据）、思考不足（未深入分析）和过度思考（为错误结论找借口）。\n*   **思考：** 现有的“思维链”只是线性地合理化直觉，而非真正的验证过程。模型缺乏医生临床决策中的核心框架——循证医学（EBM）。\n\n**6. 理论对标：从相关性到因果性**\n作者意识到，要解决定势效应，必须让模型从Pearl因果层级的第一层（关联/Association）上升到第二层（干预/Intervention）和第三层（反事实/Counterfactual）。\n*   **逻辑演进：** 医生的诊断不是简单的“症状->诊断”映射，而是“症状->证据验证->诊断”的结构化过程。因此，新的方法论必须强制模型执行显式的证据鉴别。\n\n---\n\n### 第四阶段：方法论构建（ECR-Agent的诞生）\n\n**7. 架构设计：模拟EBM认知流程**\n基于上述分析，作者提出了ECR-Agent，旨在将LLM的推理过程与EBM标准对齐。其设计逻辑包含两个核心模块：\n\n*   **模块一：动态因果推理（DCI）—— 解决“怎么想”的问题**\n    *   **双通道感知：** 强制分离“直觉通道”（生成假设）和“分析通道”（提取客观事实），防止直觉过早封闭分析路径。\n    *   **三层因果图推理：**\n        *   *关联层：* 建立初步假设。\n        *   *干预层：* 主动检索鉴别证据，模拟“如果我去检查这个指标会怎样”。\n        *   *反事实层：* 引入“影子节点”，检查“如果这个诊断成立，应该有哪些证据缺失了？”，以此惩罚不完整的推理。\n\n*   **模块二：评论驱动的图与记忆演化（CGME）—— 解决“怎么学”的问题**\n    *   仅仅推理是不够的，系统需要像医生一样积累经验。通过评论模型反馈，将验证过的推理路径存储为范例，并将疾病知识固化为不断进化的疾病图谱。\n\n---\n\n### 第五阶段：验证与结论（从“规模定律”到“结构变革”）\n\n**8. 实验验证与反直觉发现**\n作者在MedEinst上测试了多种模型，结果证实：模型规模的扩大（Scaling Laws）并没有降低偏差陷阱率，甚至更强的模型因为更自信于统计先验，反而更容易掉进陷阱（“更强的先验，更强的盲目”）。\n*   **最终结论：** 解决医学LLM的定势效应，不能仅靠扩大参数规模，必须进行架构层面的范式转移——从基于统计的概率生成，转向基于证据的因果验证。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“现象观察（高分低能）→ 问题定性（定势效应）→ 工具创新（反事实基准MedEinst）→ 机制归因（缺乏EBM因果推理）→ 方法构建（ECR-Agent结构化验证）”** 的完整闭环。其核心贡献在于指出了LLM在医疗领域“概率拟合”的局限性，并引入因果推理框架作为破局的关键。"
                },
                {
                    "title": "Structured Episodic Event Memory",
                    "arxiv_id": "2601.06411",
                    "authors": "Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang, Baotian Hu",
                    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了针对自主智能体的结构化情景事件记忆（SEEM）框架，旨在解决智能体在长期交互中的记忆组织和动态关联问题，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在解决LLM在长期交互中因静态RAG导致的检索分散和缺乏结构依赖的问题。针对连续的交互流，我们提出了一种名为SEEM的分层框架，该框架协同了用于关系事实的Graph Memory Layer和用于叙事进展的Episodic Memory Layer，并引入了Episodic Event Frames (EEFs) 和Reverse Provenance Expansion (RPE) 机制。我们在LoCoMo和LongMemEval benchmarks上通过F1、BLEU-1和Accuracy等指标验证了其有效性，结果显示SEEM显著优于现有基线。",
                    "summary_translation": "目前，大型语言模型中的记忆方法主要依赖于静态的检索增强生成（RAG），这种方法往往导致检索结果零散，且无法捕捉复杂推理所需的结构依赖关系。对于自主代理而言，这些被动且扁平的架构缺乏必要的认知组织能力，难以对长期交互的动态性和联想性进行建模。为解决这一问题，我们提出了结构化情节事件记忆（SEEM），这是一个分层框架，协同整合了用于存储关系事实的图记忆层和用于处理叙事进展的动态情节记忆层。基于认知框架理论，SEEM 将交互流转化为结构化的情节事件框架（EEFs），并通过精确的溯源指针进行锚定。此外，我们引入了一种代理式联想融合机制和反向溯源扩展（RPE）机制，旨在从碎片化证据中重构连贯的叙事语境。在 LoCoMo 和 LongMemEval 基准测试上的实验结果表明，SEEM 显著优于基线模型，使代理能够保持卓越的叙事连贯性和逻辑一致性。",
                    "inspiration_trace": "基于论文《Structured Episodic Event Memory (SEEM)》，以下是对作者构建该方法论的逻辑链推演，旨在还原其从宏观问题观察到微观机制设计的思考过程：\n\n### 1. 宏观问题：智能体的“失忆”与“碎片化”困境\n**观察起点：**\n随着大语言模型（LLM）向自主智能体演进，它们需要处理长期的、动态的交互。然而，LLM 受限于有限的上下文窗口，且缺乏稳定的外部长期记忆系统。\n\n**现有方案的缺陷（痛点）：**\n为了解决记忆问题，业界普遍采用检索增强生成（RAG）。但作者敏锐地观察到，现有的 RAG 系统（无论是基于向量的还是基于图谱的）存在一个核心缺陷——**“碎片化检索”**。\n*   **现象：** 当智能体需要回答复杂问题时，检索到的往往是零散的文本片段或孤立的事实节点。\n*   **后果：** 这些片段缺乏上下文连贯性，无法支撑需要理解事件全貌、时间顺序和因果关系的复杂推理。智能体“只见树木，不见森林”，难以维持叙事的一致性。\n\n### 2. 认知科学假设：模拟人脑的双重记忆机制\n**思维转折：**\n为了解决“碎片化”问题，作者跳出纯计算机视角，转向认知心理学寻找灵感。人脑在处理记忆时并非单一存储，而是存在明确的分工：\n*   **语义记忆：** 存储客观事实、概念和关系（如“巴黎是法国首都”）。\n*   **情景记忆：** 存储特定时间、地点下的个人经历和事件流（如“去年夏天我在巴黎做了什么”）。\n\n**核心假设：**\n如果让智能体也具备这种分层记忆结构——即用**静态的关系图谱**来存储事实，用**动态的情景结构**来存储叙事流——就能从根本上解决上下文断裂的问题。\n\n### 3. 结构化创新：从“文本片段”到“认知框架”\n**具体化挑战：**\n虽然有了分层假设，但如何具体实现“情景记忆”？直接存储原始对话记录依然混乱。作者引入了认知框架理论。\n\n**方法论构建：**\n作者提出将连续的交互流转化为结构化的**情景事件框架**。\n*   **逻辑：** 一个事件不仅仅是文本，它包含参与者、动作、时间、地点、原因等多维属性。\n*   **设计：** 将非结构化的文本解析为具有明确语义槽位的结构化单元（EEF）。这就像把散乱的文字变成了填好的“案件调查表”，使得机器能像人类一样理解事件的要素。\n\n**动态融合机制：**\n现实中的对话是断续的（例如：A问了一半，B回答，A补充）。为了防止记忆碎片化，作者设计了**“联想融合”**机制。如果新的事件与旧的事件在语义上相关（如同一话题的不同轮次），系统会将它们合并为一个连贯的“场景”。这模拟了人类记忆中会将相关经历整合的心理过程。\n\n### 4. 检索机制革新：逆向溯源与上下文重构\n**解决“检索断层”：**\n即使有了结构化的记忆，如何确保检索时不漏掉关键信息？传统的检索是基于关键词匹配的，容易遗漏那些没有直接关键词但属于同一事件上下文的信息。\n\n**逻辑闭环：**\n作者提出了**逆向溯源扩展（RPE）**机制。\n*   **思考路径：** 当用户提问时，系统首先在“图谱层”找到相关的静态事实节点。但这只是线索。\n*   **关键动作：** 利用这些节点作为锚点，反向追踪到它们所属的“情景事件框架（EEF）”。\n*   **最终效果：** 一旦激活了某个事件框架，系统就会把该框架下关联的所有原始文本片段（通过溯源指针）全部召回。这确保了智能体看到的不是孤立的句子，而是整个事件的完整起承转合。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径遵循了从**现象观察**（RAG的碎片化） -> **理论借鉴**（认知心理学的双重记忆） -> **结构化建模**（EEF与分层架构） -> **机制完善**（联想融合与逆向溯源）的完整闭环。\n\n**核心思想演进：**\n1.  **发现问题：** 现有记忆是平面的、静态的，导致推理断裂。\n2.  **提出假设：** 记忆需要分层，区分“事实”与“故事”。\n3.  **构建模型：** 用图谱存事实，用框架存故事，并用指针连接两者。\n4.  **优化检索：** 从“找相似文本”转变为“找事件线索，再还原全貌”。\n\n这一过程体现了作者试图赋予 AI 智能体类似人类的“叙事能力”和“长期连贯性”的深层动机。"
                },
                {
                    "title": "Value of Information: A Framework for Human-Agent Communication",
                    "arxiv_id": "2601.06407",
                    "authors": "Yijiang River Dong, Tiancheng Hu, Zheng Hui, Caiqi Zhang, Ivan Vulić, Andreea Bobu, Nigel Collier",
                    "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于信息价值（VoI）的决策理论框架，用于解决LLM智能体在信息不足时是直接行动还是向用户提问的决策问题。这属于单智能体的决策与交互机制研究，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决LLM智能体在处理未指定请求时，如何在行动与提问间取得平衡的问题。针对未指定的用户查询，我们提出了一种基于决策论中Value of Information (VoI)的框架，动态权衡信息增益与用户认知成本。在20 Questions、Flight Recommendation和Ambiguous WebShop等四个领域上，通过总效用验证了其有效性，该方法无需超参数调整即可达到最优性能。",
                    "summary_translation": "部署用于现实世界任务的 Large Language Model (LLM) agents（大语言模型智能体）面临一个根本性的两难困境：用户的请求往往是信息不足的，但智能体必须决定是依据不完整的信息采取行动，还是打断用户以寻求澄清。现有方法要么依赖于需要针对特定任务进行调优的脆弱 confidence thresholds（置信度阈值），要么未能考虑到不同决策所涉及的不同利害关系。我们引入了一个 decision-theoretic framework（决策理论框架），该框架通过 Value of Information (VoI)（信息价值）解决了这一权衡问题，使智能体能够动态地权衡提问带来的 expected utility gain（预期效用增益）与给用户带来的 cognitive cost（认知成本）。我们的 inference-time（推理时）方法无需 hyperparameter tuning（超参数调优），并且能够跨场景无缝适应——从休闲游戏到医疗诊断。在四个不同领域（20 Questions、医疗诊断、航班预订和电子商务）的实验表明，VoI 始终匹配或超过最佳手动调优的 baselines（基线），在高成本设置中实现了高达 1.36 的效用点提升。这项工作提供了一个用于 adaptive agent communication（自适应智能体通信）的 parameter-free（无参数）框架，该框架明确平衡了 task risk（任务风险）、query ambiguity（查询歧义）和 user effort（用户努力）。",
                    "inspiration_trace": "基于论文《Value of Information: A Framework for Human-Agent Communication》，以下是对作者核心方法论产出逻辑链的系统性推演。这一过程展现了作者如何从现实痛点出发，通过批判性分析现有技术，最终引入决策理论解决人机交互中的根本矛盾。\n\n---\n\n### 第一阶段：宏观困境的识别——“模糊性”与“两难”\n**（观察与问题定义）**\n\n作者的思考始于对现实世界LLM智能体应用场景的观察。作者发现，尽管LLM在执行任务上能力强大，但在面对真实用户时存在一个根本性的**“信息缺口”**：\n*   **用户请求的天然模糊性**：用户的指令往往是欠规范的（如“订一张去伦敦的机票”），隐含了未知的偏好（预算、时间、转机容忍度）。\n*   **智能体的两难困境**：\n    *   **行动**：在信息不全时直接行动，可能导致结果与用户意图不符（任务失败风险）。\n    *   **询问**：通过提问澄清信息，但会打断用户，增加认知负担（用户流失风险）。\n\n**核心思考**：现有的智能体大多假设指令是清晰的，或者仅仅关注“如何执行”，而忽略了“何时该沟通”这一前置决策。作者意识到，**解决这一两难困境是智能体从“工具”进化为“合作伙伴”的关键。**\n\n---\n\n### 第二阶段：对现有范式的批判——“置信度”的失效\n**（假设验证与否定）**\n\n在寻找解决方案时，作者首先审视了学术界和工业界的主流做法，并发现了其逻辑漏洞：\n1.  **固定轮次策略**：无论任务难易都问固定数量的问题。这显然是愚蠢的，因为它忽略了上下文。\n2.  **基于置信度的阈值**：这是目前最先进的自适应方法。当模型对答案的“自信度”低于某个阈值（如0.9）时，就提问。\n\n**作者的批判性洞察**：\n*   **置信度 $\\neq$ 价值**：模型对“猜动物”有90%的把握，和对“诊断癌症”有90%的把握，其含义截然不同。\n*   **缺乏风险感知**：置信度方法只关注“我知道多少”（信息论视角），却忽略了“如果错了后果有多严重”（决策论视角）。在低风险任务（猜动物）中，90%的置信度可能已经足够；但在高风险任务（医疗诊断）中，90%可能意味着致命风险，必须继续提问。\n\n**结论**：单纯依赖模型内部的不确定性估计是片面的，必须引入对**任务风险**和**决策后果**的考量。\n\n---\n\n### 第三阶段：理论视角的转换——从“信息获取”到“理性决策”\n**（理论引入与框架构建）**\n\n为了解决上述缺陷，作者将视角从计算机科学转向了认知科学与决策理论，提出了核心假设：\n*   **沟通即决策**：提问不应仅仅是为了获取信息，而应被视为一种“行动”。这种行动有成本（认知负荷），也有收益（提升决策质量）。\n*   **理性言语行为**：借鉴RSA框架，智能体应当是“理性”的，即只有当提问带来的**预期效用提升**大于**提问成本**时，才应该进行沟通。\n\n**逻辑推演**：\n我们需要一个数学工具来量化“提问到底值不值”。作者引入了经典的**信息价值**理论。\n*   **定义**：VoI = (获得信息后的预期效用) - (当前信息下的预期效用)。\n*   **决策规则**：如果 $VoI > \\text{提问成本}$，则提问；否则，直接行动。\n\n这一转换将问题从“我不确定吗？”（模糊逻辑）变成了“值得去弄清楚吗？”（经济逻辑）。\n\n---\n\n### 第四阶段：方法论的落地——LLM驱动的贝叶斯模拟\n**（从理论到实践的映射）**\n\n有了VoI理论框架，接下来的挑战是如何让LLM在推理时计算出这个值。作者设计了一套无需训练的推理时算法：\n\n1.  **信念分布**：\n    *   *思考*：LLM通常只输出一个确定答案，但计算VoI需要概率。\n    *   *方案*：强制LLM输出对用户潜在意图（如偏好、疾病类别）的概率分布 $b(\\theta)$。\n\n2.  **前瞻性模拟**：\n    *   *思考*：在问出问题前，智能体需要预判“如果我问了，用户可能怎么答，以及回答后我的效用会变多少”。\n    *   *方案*：利用LLM的生成能力进行“反事实模拟”。针对候选问题 $q$，枚举可能的回答 $y$，模拟更新信念分布 $b(\\theta|y)$，并计算对应的效用。\n\n3.  **动态权衡**：\n    *   *思考*：如何整合风险和成本？\n    *   *方案*：在VoI公式中显式引入任务风险（通过效用函数 $U(\\theta, a)$ 的量级体现）和认知成本（常数 $c$）。\n\n---\n\n### 第五阶段：验证逻辑——自适应性与零参数优势\n**（实验设计与预期验证）**\n\n最后，作者通过实验验证这一逻辑链条的有效性，其验证逻辑紧扣之前的批判：\n*   **跨场景泛化**：选择“猜动物”（低风险）、“医疗诊断”（高风险）、“订票”（多属性偏好）等不同场景，证明VoI能自动适应不同的风险等级。\n*   **对比基线**：专门对比“置信度阈值”方法。\n    *   *预期结果*：置信度方法需要针对每个任务手动调整阈值（脆弱），而VoI方法无需调参（鲁棒）。\n    *   *逻辑闭环*：在高风险场景下，VoI会因为潜在收益巨大而倾向于多问；在低风险或高沟通成本场景下，VoI会自动停止提问。\n\n---\n\n### 总结：作者的思维演进图谱\n\n1.  **痛点**：用户指令模糊，智能体在“瞎猜”和“烦人”之间进退维谷。\n2.  **反思**：现有的“自信度”机制只看不确定性，不看后果，无法区分“猜错猫”和“误诊癌症”的区别。\n3.  **升维**：引入决策论，将沟通视为一种投资，必须计算ROI（投资回报率）。\n4.  **工具**：采用**信息价值**作为核心指标，量化“提问”带来的预期收益。\n5.  **实现**：利用LLM自身的推理能力进行信念估计和未来模拟，实现无需训练的动态决策。\n\n这一逻辑链条展示了作者如何从具体的交互体验出发，通过跨学科的理论融合，最终构建出一个既符合人类直觉又具备数学严谨性的通用框架。"
                },
                {
                    "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning",
                    "arxiv_id": "2601.06282",
                    "authors": "Yue Zhou, Xiaobo Guo, Belhassen Bayar, Srinivasan H. Sengamedu",
                    "summary": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于解决长期对话智能体的记忆问题，提出了通过智能体推理构建结构化记忆（情景记忆和语义记忆）的框架。这属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决长期对话代理中现有记忆框架缺乏连贯性且计算成本高昂的问题。针对长对话场景，我们提出了一种名为Amory的工作记忆框架，通过主动构建情景叙事、动量感知整合和语义化外围事实来组织记忆，并在LOCOMO benchmark上通过LLM-as-a-Judge分数和响应延迟验证了其有效性。",
                    "summary_translation": "随着交互时间的延长，长期对话代理面临着一个根本的可扩展性挑战：重复处理整个对话历史在计算上变得不可行。目前的解决方案试图通过记忆框架来解决这一问题，这些框架主要将对话分割为孤立的 embeddings（嵌入向量）或 graph representations（图表示），并以 RAG（检索增强生成）的方式检索相关信息。尽管这些方法在计算上效率较高，但它们往往对记忆形成过程的处理过于简单，无法捕捉人类记忆的微妙之处和连贯性。我们提出了 Amory，这是一个 working memory（工作记忆）框架，它通过在 offline time（离线时间）期间增强 agentic reasoning（智能体推理）来主动构建结构化的记忆表示。Amory 将对话片段组织成 episodic narratives（情景叙事），利用 momentum（动量）巩固记忆，并将 peripheral facts（外围事实）转化为 semantic memory（语义记忆）。在检索阶段，系统在叙事结构上采用 coherence-driven reasoning（连贯性驱动推理）。在针对长期推理的 LOCOMO benchmark（基准测试）上的评估表明，Amory 相比于先前的 state-of-the-art（最先进水平）取得了显著改进，其性能与 full context reasoning（全上下文推理）相当，同时将响应时间缩短了 50%。分析表明，momentum-aware consolidation（动量感知巩固）显著提升了响应质量，而 coherence-driven retrieval（连贯性驱动检索）相比基于 embeddings 的方法提供了更优越的 memory coverage（记忆覆盖率）。",
                    "inspiration_trace": "基于论文《Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 1. 宏观问题：长对话中的“质量-效率”悖论\n**观察起点：**\n随着大语言模型（LLM）在长对话场景中的应用，一个根本性的瓶颈浮出水面：**上下文窗口的有限性与对话历史无限增长之间的矛盾**。\n*   **现状：** 如果每次都处理全量历史，计算成本极高且响应缓慢。\n*   **现有解法（RAG范式）：** 为了解决效率问题，主流方法采用检索增强生成（RAG），将对话切片存入向量数据库或图结构，按需检索。\n*   **痛点识别：** 作者发现，这种“碎片化”的存储方式虽然快，但丢失了对话的**上下文连贯性**。它将记忆视为孤立的“数据点”，而非有逻辑的“体验”，导致模型难以进行复杂的推理（如多跳问题或时间推理）。\n\n### 2. 认知视角的引入：从“存储”转向“体验”\n**理论假设：**\n为了解决碎片化问题，作者将目光转向认知科学，试图寻找人类记忆的运作机制作为灵感。\n*   **核心洞察：** 人类记忆不是简单的关键词索引，而是基于**叙事**的。\n    *   **情景记忆：** 记住的是“故事”，包含情节、人物和因果链条。\n    *   **语义记忆：** 提取出的去语境化事实。\n    *   **记忆巩固：** 记忆不是静态的，而是随着时间推移从不稳定状态重组为稳定结构。\n*   **推论：** 如果AI能像人类一样，将对话碎片组织成连贯的“故事”，并在非活跃时间进行“巩固”，就能在保持效率的同时，大幅提升记忆的可用性和推理深度。\n\n### 3. 关键转折：利用“离线智能体推理”构建结构\n**技术难点：**\n要构建复杂的叙事结构，需要LLM进行深度的逻辑推理。然而，在用户提问的“在线”阶段进行这种推理会带来不可接受的延迟。\n*   **策略选择：** 作者提出了一个关键的时间维度分离策略——**“离线构建，在线检索”**。\n*   **核心假设：** 利用对话的自然间隙（离线时间），让智能体主动去“思考”和“整理”记忆。这样既利用了LLM的推理能力，又不影响实时响应速度。\n\n### 4. 方法论构建：动态演进的叙事记忆\n基于上述假设，作者设计了一套动态的记忆构建流程，模拟人类认知的三个阶段：\n\n*   **阶段一：叙事化组织**\n    *   *思考：* 对话不是杂乱无章的，而是围绕特定主题展开的。\n    *   *设计：* 将对话片段绑定到“情景记忆”中，形成层级结构（主情节 -> 子情节 -> 片段）。这解决了碎片化问题，赋予了记忆骨架。\n\n*   **阶段二：动量感知的巩固**\n    *   *思考：* 对话有“热度”。当一个话题被反复讨论时（活跃态），不应急于总结；当话题转移后（非活跃态），才是重组记忆的最佳时机。\n    *   *设计：* 引入“对话动量”概念。仅在记忆进入非活跃状态时，触发LLM对情节进行重组和概括（Consolidation）。这模拟了人类在事后反思并固化记忆的过程。\n\n*   **阶段三：语义化剥离**\n    *   *思考：* 并非所有信息都属于故事。有些是琐碎的事实（如“某人住在哪”），它们不需要上下文即可被理解。\n    *   *设计：* 将与主情节逻辑关联不大的边缘事实，提取为结构化的三元组存入“语义记忆”。这实现了叙事与事实的分离，提高了检索的精准度。\n\n### 5. 检索范式革新：连贯性驱动\n**最后一步：**\n既然记忆是结构化的叙事，检索方式也必须升级。\n*   *批判：* 传统的向量相似度检索无法理解逻辑关系（例如，用户问“John为什么喜欢篮球？”，向量检索可能只匹配到“篮球”这个词，而忽略了“职业发展”这个潜在情节）。\n*   *设计：* 采用**连贯性推理检索**。让LLM基于情节标题和人物关系进行逻辑判断，而非简单的向量匹配。这确保了检索到的不仅是“相似”的内容，更是“逻辑相关”的上下文。\n\n### 6. 总结：逻辑链的闭环\n作者的思考路径完成了一个闭环：\n1.  **发现问题：** 现有RAG方法虽然快，但记忆太碎，推理能力差。\n2.  **寻找灵感：** 人类通过叙事和巩固来形成高质量记忆。\n3.  **提出假设：** 利用LLM的推理能力，在离线阶段主动构建叙事结构。\n4.  **细化机制：** 通过“绑定-巩固-语义化”三步走，动态管理记忆的演进。\n5.  **验证效果：** 实验证明，这种方法在保持低延迟的同时，显著提升了长对话中的推理质量，接近全量上下文的效果。\n\n这一过程体现了作者从**工程痛点**出发，借鉴**认知科学理论**，最终通过**巧妙的时空分离设计（离线推理/在线检索）**实现了方法论落地的完整逻辑演进。"
                },
                {
                    "title": "Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms",
                    "arxiv_id": "2601.06039",
                    "authors": "Yueze Liu, Ajay Nagi Reddy Kumdam, Ronit Kanjilal, Hao Yang, Yichi Zhang",
                    "summary": "Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确提出了VEJA框架，旨在通过改进数据策览来增强角色扮演智能体的内部状态（价值观、经历、判断、能力）和推理能力。这属于单智能体研究范畴，涉及智能体的记忆、自我反思和行为建模，旨在提升智能体的深度和叙事连续性。",
                    "summary2": "本文旨在解决现代角色扮演模型缺乏真实角色深度及内心冲突推理的问题。针对现有训练范式的局限性，我们提出了一种VEJA框架（Values, Experiences, Judgments, Abilities），用于指导数据策划。我们在基于角色Makise Kurisu的数据集上，通过LLM-as-judge的盲A/B测试验证了其有效性，结果显示VEJA策划的数据在角色一致性和叙事连续性上显著优于合成基线。",
                    "summary_translation": "现代角色扮演模型日益精密，但始终难以捕捉可信且引人入胜角色的本质。我们认为，这一缺陷归因于训练范式忽视了角色内心世界的动态相互作用。当前的方法，包括检索增强生成、基于事实的提示、基于文学的学习以及合成数据生成，在建模定义人类互动的深思熟虑且充满价值冲突的推理方面，均表现出反复出现的局限性。在本文中，我们确定了对于角色真实性至关重要的四个核心概念：价值观、经历、判断和能力。我们提出 VEJA 框架作为一种新的数据策展范式，旨在解决这些系统性局限。为了展示本框架所能达到的质量上限，我们进行了一项试点研究，将人工策展的基于 VEJA 的数据集与最先进的合成基线进行了比较。利用大语言模型评判法，我们的研究结果显示出显著的质量差距，这表明转向以概念为基础的数据策展（如 VEJA 所体现的那样），对于创建具有真正深度和叙事连贯性的角色扮演智能体是必要的。完整数据集可在 https://github.com/HyouinKyoumaIRL/Operation-Veja 获取。",
                    "inspiration_trace": "基于论文《Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms》，以下是对作者产出该文章核心方法（VEJA框架）的逻辑链推演：\n\n### 第一阶段：现象观察与核心痛点识别\n**（从“模型能说话”到“模型没有灵魂”）**\n\n1.  **宏观观察**：作者发现，尽管现代角色扮演模型越来越复杂，能够生成流畅的对话，但它们始终缺乏“令人信服的、引人入胜的角色本质”。\n2.  **具体案例触发**：作者在尝试构建高保真角色（如《命运石之门》中的牧濑红莉栖）时发现，现有模型无法复刻其核心特质——即“求知欲”与“社交戒备心”之间的冲突。模型的反应仅仅是条件反射式的，缺乏内在的驱动力。\n3.  **核心假设提出**：作者认为，问题的根源不在于模型参数不够大，而在于**训练范式**忽视了角色内心世界的动态相互作用。人类互动不是检索“正确”答案，而是**冲突价值的协商**。\n\n### 第二阶段：对现有范式的批判性解构\n**（为什么当前主流方法都失效了？）**\n\n为了验证假设，作者系统性地解剖了当时四种主流的角色建模方法，试图找出它们共同的缺陷：\n\n1.  **检索增强生成（RAG）的局限**：RAG擅长处理事实，但人类的价值体系是组合爆炸的。试图用检索列表来穷举一个角色在所有情境下的价值判断是不可能的。\n2.  **基于事实的价值预设的局限**：为了通过基准测试，现有方法倾向于将价值简化为孤立的公式（如“对陌生人开放”）。这导致模型在对话中过度索引单一特征，忽略了语境和平衡，显得机械且缺乏分寸。\n3.  **基于文学生成的局限**：文学名著虽然包含深度，但对话只“暗示”了思维过程，而非“显式”展示。模型无法从对话文本中反向推导出角色复杂的内心 deliberation（ deliberative reasoning）。此外，文学中的“经验”通常通过旁白而非对话传递，导致模型难以学会“以史为鉴”。\n4.  **合成数据生成的死循环**：这是最致命的陷阱。试图用现有的强模型（如GPT-4）生成高质量角色数据是行不通的，因为**生成者本身就不具备处理复杂价值冲突的能力**。这导致了一个递归的质量天花板。\n\n### 第三阶段：理论重构与VEJA框架的诞生\n**（回归戏剧艺术，重建角色的“因果逻辑”）**\n\n在否定了现有技术路径后，作者转向经典戏剧理论（如斯坦尼斯拉夫斯基体系），试图从第一性原理出发定义什么是“真实的角色”。\n\n1.  **寻找基本单元**：作者认为，要模拟角色的深度，必须显式地建模其内心逻辑。通过数据整理过程中的观察，作者提炼出四个核心概念：\n    *   **Values (价值观)**：行为的根本动机（Why）。\n    *   **Experiences (经历)**：塑造价值观和判断的过去事件（Evidence）。\n    *   **Judgments (判断)**：价值观经过经历过滤后形成的具体观点（Output）。\n    *   **Abilities (能力)**：表达上述特质的知识和技能工具（Toolkit）。\n\n2.  **建立因果链条**：这四个要素不是孤立的标签，而是一个严密的**因果闭环**：\n    *   经历塑造价值观；\n    *   价值观与经历共同产出判断；\n    *   判断通过能力表达出来。\n    *   *逻辑演进点*：作者意识到，只有强制数据遵循这个因果链，才能让模型学会“像人一样思考”，即基于过去（E）和动机（V）来形成当下的观点（J），而不仅仅是模仿语气。\n\n### 第四阶段：验证与范式转移\n**（证明“人+框架”优于“纯模型合成”）**\n\n1.  **实验设计的逻辑**：既然现有模型无法生成高质量数据，那么“人类作者”是否就是答案？为了验证这一点，作者设计了一个对比实验：**纯模型生成** vs. **VEJA框架指导的人类写作**。\n2.  **结果解读**：实验结果显示，VEJA指导的人类数据显著优于SOTA合成数据。这证明了作者的核心论点：**当前的技术瓶颈不在于算力，而在于数据的“概念深度”**。\n3.  **最终结论**：作者提出，社区需要从“构建更好的鹦鹉”（模仿表面）转向“创造真正的数字心智”（模拟内在）。VEJA不仅仅是一个数据标注框架，更是一种新的训练范式，它要求我们在数据构建阶段就必须显式地包含角色的内心冲突和推理过程。\n\n---\n\n**总结：**\n作者的思考路径是从**“体验到的肤浅感”**出发，经过**“对技术路径的证伪”**，回归**“对人性和戏剧艺术的本体论思考”**，最终提炼出**“VEJA因果模型”**，并通过实验确立了**“概念驱动数据”**优于**“纯合成数据”**的新范式。"
                },
                {
                    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
                    "arxiv_id": "2601.07779",
                    "authors": "Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",
                    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个名为 OS-Symphony 的计算机使用智能体框架，核心研究内容包括智能体的记忆机制、自我反思、工具使用以及多智能体协作，完全符合 LLM 智能体的研究范围。虽然涉及视觉模型，但重点在于智能体架构而非视觉模型本身。",
                    "summary2": "本文旨在解决计算机使用代理在长时程任务中鲁棒性不足及新领域泛化能力差的问题。针对复杂的桌面自动化场景，我们提出了一种名为OS-Symphony的整体框架，该框架集成了利用里程碑驱动长期记忆的Reflection-Memory Agent和采用See-Act范式的Multimodal Searcher。并在OSWorld、WindowsAgentArena和MacOSArena基准上通过Step Success Rate验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "尽管 Vision-Language Models (VLMs，视觉语言模型) 显著推动了 Computer-Using Agents (CUAs，计算机使用代理) 的发展，但现有框架在长时程工作流的鲁棒性以及在新领域的泛化能力方面仍面临挑战。这些局限性主要归因于对历史视觉上下文筛选缺乏细粒度控制，以及缺乏视觉感知的教程检索机制。为弥合这些差距，我们提出了 OS-Symphony，这是一个包含 Orchestrator (编排器) 的整体框架，该编排器协调两项关键创新以实现鲁棒的自动化：(1) Reflection-Memory Agent (反思记忆代理)，利用里程碑驱动的长期记忆实现轨迹级自我修正，有效缓解长时程任务中的视觉上下文丢失；(2) Versatile Tool Agents (多功能工具代理)，其特色在于包含一个 Multimodal Searcher (多模态搜索器)，该搜索器采用 SeeAct 范式在基于浏览器的沙箱中进行导航，以合成实时的、视觉对齐的教程，从而解决未见场景中的保真度问题。实验结果表明，OS-Symphony 在不同模型规模下均带来了显著的性能提升，在三个在线基准测试中确立了新的 State-of-the-Art (SOTA，最先进) 结果，尤其在 OSWorld 上达到了 65.84% 的成绩。",
                    "inspiration_trace": "基于论文《OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方案构建的思考过程：\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**思考起点：** 尽管视觉语言模型（VLMs）推动了计算机代理（CUA）的发展，但现有的代理框架在实际应用中仍存在显著的脆弱性。\n**核心矛盾：** 当前的CUA在两个关键维度上表现不佳：\n1.  **长时程任务的鲁棒性：** 在需要多步骤、跨应用的复杂工作流中，代理容易迷失方向或陷入死循环。\n2.  **新领域的泛化能力：** 面对未见过的软件或环境（OOD场景），代理缺乏必要的知识储备，无法有效执行任务。\n\n### 第二阶段：微观诊断与归因分析\n**思考深入：** 为什么现有的模块化或端到端框架无法解决上述矛盾？作者通过分析现有架构的局限性，识别出两个具体的“技术断层”：\n\n**断层一：视觉上下文的“失忆”**\n*   **观察：** 现有的记忆机制（如简单的滑动窗口或文本摘要）缺乏对历史视觉信息的精细化管理。\n*   **推论：** 在长任务中，屏幕截图包含大量冗余信息，直接存储会撑爆上下文窗口，而简单丢弃又会丢失关键状态。这种“视觉上下文丢失”导致代理无法回溯历史，从而无法识别意图漂移或循环行为等错误，失去了自我纠错的基础。\n\n**断层二：检索增强的“视觉盲区”**\n*   **观察：** 为了解决泛化问题，现有方法引入了检索增强生成（RAG）。但这些方法多依赖纯文本检索或静态知识库。\n*   **推论：** GUI任务本质上是视觉的。纯文本检索无法捕捉界面布局、图标样式等视觉语义，导致检索到的教程与当前屏幕状态不匹配（保真度低）。此外，静态知识库更新成本高，难以适应新软件的快速迭代。\n\n### 第三阶段：核心假设与策略提出\n**思考转折：** 要解决上述断层，必须从“被动处理”转向“主动感知与压缩”。作者提出了两个核心假设：\n\n1.  **关于记忆的假设：** 如果能设计一种机制，只保留具有里程碑意义的关键截图，并基于这些视觉证据生成轨迹级的反思，就能在压缩上下文的同时保留纠错能力。\n2.  **关于泛化的假设：** 如果代理能像人类一样，在遇到不懂的操作时主动打开浏览器进行“视觉搜索”，通过实际浏览网页来合成与当前环境视觉对齐的教程，就能解决静态知识库的滞后和文本检索的盲区。\n\n### 第四阶段：方法论构建与系统设计\n**思考落地：** 基于上述假设，作者构建了 **OS-Symphony** 这一整体框架，其逻辑架构体现了“分工协作”的思想：\n\n**1. 设计“指挥官”：**\n*   **逻辑：** 系统需要一个核心大脑来负责任务理解和动作调度，同时协调其他模块。\n*   **角色：** Orchestrator（编排器）。它只关注短期记忆（最近K步）和来自其他模块的高级指令，保持决策的敏捷性。\n\n**2. 构建“反思者与记忆库”：**\n*   **逻辑：** 针对“视觉上下文丢失”，需要一个专门的模块来管理长期记忆和进行错误审计。\n*   **方案：** **Reflection-Memory Agent (RMA)**。\n    *   **里程碑机制：** 不存储所有截图，而是通过算法判断哪些步骤是“里程碑”（如状态发生重大改变），只保留这些关键帧。\n    *   **结构化反思：** RMA 审计历史轨迹，通过结构化的消息协议向 Orchestrator 反馈状态（如：On-track, Off-track, GUI Error, Lack of Tutorial），从而实现轨迹级的自我纠正。\n\n**3. 打造“全能工具箱”：**\n*   **逻辑：** 针对“视觉盲区”和执行效率问题，需要专门的工具来处理特定类型的任务。\n*   **方案：** **Versatile Tool Agents**。\n    *   **多模态搜索者：** 这是一个核心创新。它采用“See-Act”范式，在一个隔离的浏览器沙箱中自主导航，阅读网页并合成包含视觉描述的教程。这解决了传统RAG缺乏视觉感知的问题。\n    *   **定位器与编码器：** 分别负责UI元素的精确定位和系统级的代码操作，弥补纯GUI操作的不足。\n\n### 第五阶段：逻辑闭环与验证\n**思考总结：** 整个框架形成了一个闭环：\n*   Orchestrator 执行任务；\n*   遇到困难或错误时，RMA 通过视觉审计发现并反馈；\n*   如果是知识缺失，Searcher 主动上网寻找视觉教程；\n*   最终完成任务并更新记忆。\n\n**结论：** 这种设计通过**精细化的视觉记忆管理**解决了长时程任务的鲁棒性问题，通过**主动的视觉搜索**解决了新领域的泛化问题，从而在多个基准测试中实现了SOTA性能。\n\n---\n\n**总结：** 作者的思考路径是从**宏观的能力缺失**（鲁棒性与泛化性）出发，深入到**微观的信息处理缺陷**（视觉记忆丢失与检索视觉盲区），进而提出**主动化与结构化**的解决策略（里程碑记忆与视觉搜索），最终通过**多智能体协作**的架构实现了逻辑落地。"
                },
                {
                    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
                    "arxiv_id": "2601.07641",
                    "authors": "Jiaxuan Lu, Ziyu Kong, Yemin Wang, Rong Fu, Haiyuan Wan, Cheng Yang, Wenjie Lou, Haoran Sun, Lilong Wang, Yankai Jiang, Xiaosong Wang, Xiao Sun, Dongzhan Zhou",
                    "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了Test-Time Tool Evolution (TTE)框架，旨在解决LLM智能体在科学推理任务中依赖静态工具的问题。它属于单智能体研究范畴，重点探讨了智能体的工具使用和自我演化（工具演化）能力，符合筛选条件中关于单智能体和自我演化的定义。",
                    "summary2": "本文旨在解决静态工具库在科学推理中覆盖不足和适应性差的问题。针对开放式科学计算任务，我们提出了一种Test-Time Tool Evolution (TTE)框架，通过动态合成、验证和演化可执行工具来增强智能体能力。我们在SciEvo、SciBench等benchmark上通过Accuracy和Tool Reuse Rate验证了其有效性，显著提升了推理准确率和工具复用效率。",
                    "summary_translation": "AI for Science（科学智能）的核心挑战不仅在于单纯的推理，更在于在开放式的科学世界中创造计算方法的能力。现有的 LLM-based agents（基于大语言模型的智能体）依赖于静态的、预定义的工具库，这种范式在工具稀疏、异构且本质上不完整的科学领域中根本无法奏效。在本文中，我们提出了 Test-Time Tool Evolution (TTE，测试时工具演化)，这是一种新的范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，TTE 克服了静态工具库的僵化性和长尾局限性。为了进行严格的评估，我们引入了 SciEvo，这是一个包含 1,590 个科学推理任务的 benchmark（基准），并由 925 个自动演化的工具提供支持。大量实验表明，TTE 在准确率和工具效率方面均达到了最先进的性能，同时实现了计算工具的有效 cross-domain adaptation（跨域适应）。代码和 benchmark 已在 https://github.com/lujiaxuan0520/Test-Time-Tool-Evol 发布。",
                    "inspiration_trace": "基于论文《Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程：\n\n### 1. 宏观观察：AI for Science 的“严谨性鸿沟”\n**思考起点：** 作者首先审视了当前大语言模型（LLM）在科学领域的应用现状。\n*   **现象：** LLM 拥有强大的推理能力，但在处理科学问题时，其概率性的本质往往导致“幻觉”，缺乏科学研究所必须的精确计算和严谨逻辑。\n*   **现有解法：** 业界通用的做法是给 LLM 配备外部工具（如计算器、API），即“工具增强”。\n*   **初步质疑：** 这种“LLM + 工具”的模式虽然解决了通用领域的部分问题，但在真正的科学研究中，是否足够？\n\n### 2. 问题诊断：静态工具库的“长尾困境”\n**深入分析：** 作者进一步剖析了现有工具增强范式在科学领域的根本缺陷。\n*   **核心矛盾：** 科学世界是开放、无边界的，而现有的工具库是**静态**且**预定义**的。\n*   **两大瓶颈：**\n    1.  **稀疏性与异构性：** 科学计算工具分散且非标准化，无法像通用 API 那样通过爬取构建一个“全知”的静态库。\n    2.  **不可预知性：** 科学探索往往涉及新颖的问题，需要全新的计算原语。静态库无法包含尚未被定义的工具。\n*   **结论：** 依赖静态工具库，本质上将 AI 限制在“被动选择者”的角色，无法应对开放的科学问题。这是一个**范式层面**的局限，而非工程细节问题。\n\n### 3. 核心假设：从“工具检索”到“工具进化”\n**范式转换：** 为了解决上述矛盾，作者提出了一个颠覆性的假设。\n*   **假设：** 一个真正的科学智能体，不应该只是从仓库里拿工具，而应该具备**在推理过程中即时创造和演化工具**的能力。\n*   **核心概念：** **Test-Time Tool Evolution (TTE，测试时工具进化)**。\n*   **逻辑推演：** 如果工具库是不完整的，那么它就不应该是固定的资源，而应该是**问题驱动的产物**。工具应该在解决问题的过程中被动态合成、验证并积累。\n\n### 4. 方法论构建：闭环进化机制\n**具体化思考：** 如何实现“工具进化”？作者构建了一个闭环逻辑，将科学方法论的迭代性引入 AI 系统。\n*   **第一步：结构化分解。** 面对复杂问题，不能直接生成代码，而应先将其拆解为原子化的子目标。这是为了精准定位需要什么样的工具。\n*   **第二步：动态检索与合成。** 先看库里有没有，没有就现场写一个。这里的关键是**“按需合成”**。\n*   **第三步：验证与原子化。** 生成的工具不能直接入库，必须经过严格的验证（语法、执行、领域逻辑）。更重要的是，要将复杂的工具拆解为**原子工具**。\n    *   *思考逻辑：* 只有原子化的工具才能被未来不同的问题复用，避免生成大量“一次性脚本”。\n*   **第四步：更新与修剪。** 库不能无限膨胀，需要基于使用频率进行优胜劣汰，保持工具库的高效和紧凑。\n\n### 5. 验证与拓展：零起点与跨域适应\n**场景推演：** 为了证明 TTE 的普适性，作者设定了两个极端的验证场景。\n*   **场景一：TTE-Zero（白板起家）。** 模拟人类科学家从零开始探索。初始工具库为空，看智能体能否在解决问题的过程中，自我演化出一套完整的科学计算工具集。\n*   **场景二：TTE-Adapt（跨域迁移）。** 模拟知识迁移。给智能体一个“材料科学”的工具库，让它去解决“化学”问题。看它能否通过进化，保留通用工具，淘汰不适用工具，并生成新领域的专用工具。\n*   **预期结果：** 如果 TTE 成立，它不仅能解决问题，还能演化出高复用率的核心科学原语。\n\n### 6. 最终愿景：定义“科学智能体”的新标准\n**思想升华：** 作者的思考最终落脚于对 AI 智能体的重新定义。\n*   **总结：** 科学推理的核心不在于参数知识的多寡，而在于**创造计算方法的能力**。\n*   **产出：** 这篇文章不仅仅是提出了一个算法框架，更是确立了“动态工具进化”作为下一代科学 AI 的核心范式。智能体从被动的工具使用者，进化为了主动的方法创造者。\n\n---\n\n**逻辑链总结：**\n**严谨性鸿沟** $\\rightarrow$ **静态工具库的局限性** $\\rightarrow$ **提出“测试时进化”假设** $\\rightarrow$ **构建“分解-合成-验证-原子化”闭环** $\\rightarrow$ **验证零起点与跨域能力** $\\rightarrow$ **确立主动创造的科学智能体范式**。"
                },
                {
                    "title": "LRAS: Advanced Legal Reasoning with Agentic Search",
                    "arxiv_id": "2601.07296",
                    "authors": "Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",
                    "summary": "While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on \"closed-loop reasoning\" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric \"closed-loop thinking\" to dynamic and interactive \"Active Inquiry\". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了“Legal Reasoning with Agentic Search (LRAS)”框架，旨在将模型从静态推理转变为动态的“Active Inquiry”（主动询问）。这涉及智能体行为（搜索/询问）和自我反思（Introspective Imitation Learning），符合单智能体（工具使用、自我反思）的研究范围，尽管应用于法律领域，但其核心贡献在于智能体框架而非单纯的应用。",
                    "summary2": "本文旨在解决现有法律大模型因缺乏知识边界自省而在复杂场景下推理脆弱的问题。针对法律推理任务，我们提出了一种LRAS框架，通过Introspective Imitation Learning和Difficulty-aware Reinforcement Learning实现从“闭环思维”到“主动探究”的转变。我们在LexEval、LawBench等基准上通过准确率验证了其有效性，性能提升达8.2%-32%。",
                    "summary_translation": "尽管 Large Reasoning Models (LRMs，大型推理模型) 在数学领域展现了卓越的逻辑能力，但其在法律领域的应用仍受限于程序严谨性及遵循法律逻辑的严格要求。现有的 legal LLMs（法律大语言模型）依赖于仅源自内部参数化知识的“closed-loop reasoning”（闭环推理），往往缺乏对自身知识边界的认知，从而导致“自信但错误”的结论。为应对这一挑战，我们提出了 Legal Reasoning with Agentic Search (LRAS，基于智能体搜索的法律推理)，这是首个旨在将 legal LLMs 从静态且参数化的“closed-loop thinking”（闭环思维）转变为动态且交互式的“Active Inquiry”（主动探究）的框架。通过整合 Introspective Imitation Learning（内省模仿学习）和 Difficulty-aware Reinforcement Learning（难度感知强化学习），LRAS 赋能 LRMs 识别知识边界并应对法律推理的复杂性。实证结果表明，LRAS 的性能超越 state-of-the-art baselines（最先进的基线模型）8.2-32%，其中在需要基于可靠知识进行深度推理的任务中，提升幅度最为显著。我们将很快公开发布我们的数据和模型，以供进一步探索。",
                    "inspiration_trace": "基于论文《LRAS: Advanced Legal Reasoning with Agentic Search》的内容，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与问题界定\n**起点：** 大型推理模型（LRMs）在数学和符号逻辑领域表现出色，但在法律领域却遭遇瓶颈。\n**思考：** 法律领域不同于数学，它不仅需要逻辑，更要求极高的程序严谨性和对法律逻辑的严格遵循。现有的法律大模型大多依赖“闭环推理”，即仅利用模型内部的参数知识进行推断。\n**核心痛点：** 这种“闭卷考试”式的思维模式导致模型缺乏对自身知识边界的认知，经常在不知道答案时依然自信地输出错误结论（即“幻觉”），这在容错率极低的法律场景中是不可接受的。\n\n### 2. 深度诊断与假设验证\n**假设：** 既然模型内部知识不足，引入外部检索（RAG）是否就能解决问题？\n**实验观察：**\n*   **现象一（内省缺失）：** 在模型出错的案例中，虽然有搜索工具可用，但超过70%的情况下模型并未触发搜索。这说明主要问题不在于“缺乏知识”，而在于“缺乏自知之明”——模型不知道自己什么时候该去查资料。\n*   **现象二（复杂场景下的脆弱性）：** 在简单的法律任务上，静态检索（Full RAG）有效；但在需要深度推理的复杂任务上，静态检索的提升非常有限。这表明面对复杂案情，被动地接收检索结果是不够的，模型需要具备主动规划和多步探索的能力。\n\n**结论：** 仅仅给模型“喂”更多数据或简单的检索工具是不够的。必须从根本上改变模型的思维范式，从静态的“闭环思考”转向动态的“主动探究”。\n\n### 3. 范式转移与核心思路\n**核心思想：** 构建一个具有“代理搜索”能力的法律推理框架（LRAS），让模型像人类律师一样：先思考，发现知识盲区，主动检索，验证，再思考。\n**逻辑拆解：** 为了实现这一范式转移，需要解决两个递进的核心问题：\n1.  **“是否搜索”：** 解决内省缺失，让模型学会识别知识边界。\n2.  **“如何搜索”：** 解决复杂场景下的脆弱性，让模型学会在难题中自主规划多步搜索策略。\n\n### 4. 方法论构建\n基于上述逻辑，作者设计了双机制学习架构：\n\n*   **第一阶段：内省式模仿学习**\n    *   **目标：** 解决“是否搜索”的问题。\n    *   **逻辑：** 既然模型不知道自己不知道，那就通过专家示范来教它。通过合成包含“思考-搜索-验证”轨迹的高质量数据，训练模型模仿专家的行为——只有在遇到模糊或关键法律内容时，才主动触发搜索。这赋予了模型“内省”能力。\n\n*   **第二阶段：难度感知强化学习**\n    *   **目标：** 解决“如何搜索”的问题。\n    *   **逻辑：** 模仿学习只能教会基本的动作模式，但在模型依然做不出来的“硬骨头”案例上，需要更强的自主探索能力。作者筛选出SFT模型通过率低的困难样本，利用强化学习（GRPO）进行训练。通过奖励机制，鼓励模型在复杂场景下进行多轮探索和证据综合，从而从被动的“事实核查”进化为主动的“深度推理”。\n\n### 5. 逻辑闭环与验证\n**最终产出：** LRAS框架。\n**验证逻辑：** 实验结果显示，LRAS在需要深度推理的任务上提升显著（8.2%-32%），且在复杂案例中能主动进行多轮搜索并准确区分细微的法律概念（如“负责”与“报告工作”的区别）。这证明了从“被动接收”到“主动探究”的范式转移是提升法律AI推理能力的关键路径。"
                },
                {
                    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
                    "arxiv_id": "2601.07226",
                    "authors": "Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",
                    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了智能体AI系统和智能体工作流，重点评估了工具使用任务和RAG场景下的鲁棒性。它分析了智能体如何处理噪声工具输出，并旨在构建鲁棒的、具备推理能力的智能体，符合单智能体（工具使用、推理）的研究范围。",
                    "summary2": "本文旨在解决推理模型在噪声环境下的鲁棒性问题。针对包含随机文档、无关聊天记录和困难负例的噪声上下文，我们提出了NoisyBench基准和RARE（Rationale-Aware Reward）奖励函数，并在NoisyBench的11个数据集上通过准确率验证了其有效性。",
                    "summary_translation": "推理模型和智能体 AI 系统的最新进展导致了对多样化外部信息的依赖日益增加。然而，这种转变引入了本质上包含噪声的输入上下文，而当前经过净化的基准未能捕捉到这一现实。我们介绍了 NoisyBench，这是一个综合基准，针对包括随机文档、无关聊天历史和困难负样本干扰项在内的多种噪声类型，系统地评估了模型在 RAG (检索增强生成)、推理、对齐和工具使用任务中跨越 11 个数据集的鲁棒性。我们的评估显示，当面临上下文干扰项时，最先进的模型会出现高达 80% 的灾难性性能下降。关键在于，我们发现智能体工作流通常通过过度信任含噪工具输出来放大这些错误，并且即使没有对抗性意图，干扰项也能触发涌现性不对齐。我们发现提示工程、上下文工程、SFT (监督微调) 和仅基于结果奖励的 RL (强化学习) 都无法确保鲁棒性；相比之下，我们提出的 Rationale-Aware Reward (RARE) (理由感知奖励) 通过激励识别噪声中的有用信息，显著增强了韧性。最后，我们发现了一种逆向缩放趋势，即在噪声环境中，增加测试时计算会导致性能下降，并通过注意力可视化证明模型过度关注干扰项标记，这为构建下一代鲁棒且具备推理能力的智能体提供了重要见解。",
                    "inspiration_trace": "基于论文《Lost in the Noise: How Reasoning Models Fail with Contextual Distractors》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定义（从“理想”到“现实”）\n\n1.  **观察现状**：\n    *   作者注意到AI范式正从单纯的“对话模型”转向“智能体系统”。这些系统严重依赖外部工具（如RAG、搜索、计算器）和长上下文来解决复杂任务。\n2.  **发现缺口**：\n    *   **理想 vs. 现实**：现有的学术基准测试大多是在“无菌”的清洁数据下进行的。然而，现实世界中的智能体面临的是充满噪声的环境（错误的检索结果、无关的聊天历史、工具输出错误）。\n    *   **假设**：当前SOTA模型在清洁环境下的高分可能掩盖了其在真实噪声环境下的脆弱性。这种脆弱性可能不仅仅是效率问题，而是根本性的推理崩溃。\n\n### 第二阶段：现象验证与深度剖析（“有多脆弱？”）\n\n1.  **构建验证工具**：\n    *   为了验证假设，作者构建了 **NoisyBench**。这是一个系统性的基准，涵盖了RAG、推理、对齐和工具使用四大类任务，并人为注入了三种噪声：随机文档、无关聊天历史、硬负样本。\n2.  **关键发现**：\n    *   **灾难性下降**：即使是顶尖模型（如Gemini-2.5-Pro），在面对噪声时性能也出现了高达80%的断崖式下跌。这证明了“清洁性能强 $\\neq$ 抗噪能力强”。\n    *   **智能体的悖论**：引入智能体工作流（使用工具）在清洁环境下能提升性能，但在噪声环境下反而**放大**了错误。这是因为智能体倾向于“过度信任”工具输出和上下文，导致错误在多步规划中累积。\n    *   **反向缩放定律**：在噪声环境下，增加推理步骤（测试时计算）反而导致性能下降。模型花费更多token去“思考”噪声，结果越想越错。\n\n### 第三阶段：解决方案的试错与迭代（“常规方法为何失效？”）\n\n在确认问题严重性后，作者尝试了现有的主流修复方案，但均遭遇失败：\n\n1.  **尝试一：提示工程与上下文工程**\n    *   **逻辑**：通过优化Prompt或重新组织上下文来引导模型忽略噪声。\n    *   **结果**：**失败**。模型无法通过简单的指令区分信号与噪声，上下文工程本身也容易受到噪声干扰。\n2.  **尝试二：监督微调（SFT）**\n    *   **逻辑**：在包含噪声的数据集（NoisyInstruct）上进行训练，让模型适应噪声。\n    *   **结果**：**失败**。导致了“灾难性遗忘”，模型失去了原有的推理能力，且并未真正学会抗噪。\n3.  **尝试三：基于结果的强化学习（Outcome-based RL）**\n    *   **逻辑**：只对最终答案的正确性进行奖励，让模型自己探索如何在噪声中得出正确答案。\n    *   **结果**：**部分有效但局限**。虽然比SFT好，但模型往往通过“作弊”或依赖内部记忆来得分，而不是真正学会从噪声中提取信息。它无法区分“答对了”是因为“抗噪成功”还是“碰巧蒙对”。\n\n### 第四阶段：核心洞察与方法论形成（从“结果导向”转向“过程导向”）\n\n1.  **核心洞察**：\n    *   作者意识到，单纯奖励“最终答案”是不够的。模型失败的根本原因在于**推理过程**被噪声劫持（注意力机制分析显示模型在错误预测时过度关注干扰项）。\n    *   因此，必须显式地奖励模型在推理过程中**识别并锚定有用信息**的行为，而不仅仅是奖励最终结果。\n2.  **方法论提出：RARE (Rationale-Aware Reward)**\n    *   **逻辑转变**：从“Reward the Outcome”转变为“Reward the Process”。\n    *   **具体机制**：设计一个新的奖励函数，不仅检查最终答案，还检查模型的思维链中是否正确引用或提取了上下文中的**有效参考信息**。\n    *   **作用原理**：通过奖励模型在噪声中“抓取”正确线索的行为，强迫模型学会过滤干扰项。这就像训练学生不仅要写出正确答案，还要在草稿纸上圈出解题依据。\n3.  **最终验证**：\n    *   实验证明，RARE 显著降低了模型被干扰的比例，同时提高了最终准确率。更重要的是，这种方法不仅提升了抗噪性，在清洁环境下也没有性能损失，实现了鲁棒性与通用性的双赢。\n\n---\n\n**总结**：\n作者的思考路径是从**现实应用场景的落差**出发，通过**基准测试量化了“噪声脆弱性”这一现象**，在排除了**提示工程和传统训练方法**的无效性后，抓住了**“推理过程被干扰”这一本质原因**，最终通过**引入过程级奖励（RARE）**，成功引导模型学会了在噪声中“去伪存真”的推理能力。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 7,
            "papers": [
                {
                    "title": "Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting",
                    "arxiv_id": "2601.05606",
                    "authors": "Chen Han, Jin Tan, Bohan Yu, Wenzhen Zheng, Xijin Tang",
                    "summary": "Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.",
                    "category": "cs.MA",
                    "filter_reason": "该论文明确研究了LLM多智能体系统（MAS）中的社会交互、从众动态、网络拓扑以及决策范式（集中式与分布式），属于多智能体协作与通信的研究范畴。",
                    "summary2": "本文旨在研究网络拓扑和自我-社会权重如何影响LLM多智能体系统中的从众动态。针对虚假信息检测任务，我们提出了一种置信度归一化池化规则，通过参数$\\alpha$平衡自我依赖与社会影响，并在Snopes25数据集上通过Central Accuracy、Final Accuracy等指标验证了其有效性。",
                    "summary_translation": "Large Language Models (LLMs，大语言模型) 越来越多地被实例化为 multi-agent systems (MAS，多智能体系统) 中的交互智能体，其中集体决策是通过社会互动而非独立推理产生的。这一过程中一个基本但尚未被充分探索的机制是 conformity (从众)，即智能体将其判断与主流群体意见保持一致的倾向。本文通过一个 misinformation detection task (虚假信息检测任务)，系统研究了 network topology (网络拓扑结构) 如何塑造 LLM-based MAS (基于LLM的多智能体系统) 中的 conformity dynamics (从众动态)。我们引入了一种 confidence-normalized pooling rule (置信度归一化池化规则)，用于控制 self-reliance (自主性) 与 social influence (社会影响) 之间的权衡，从而能够对两种典型的决策范式进行比较：Centralized Aggregation (集中式聚合) 和 Distributed Consensus (分布式共识)。实验结果表明，network topology (网络拓扑结构) 关键性地决定了 collective judgments (集体判断) 的 efficiency (效率) 和 robustness (鲁棒性)。Centralized structures (集中式结构) 能够实现即时决策，但对 hub competence (枢纽节点能力) 敏感，并且表现出 same-model alignment biases (同模型对齐偏差)。相比之下，distributed structures (分布式结构) 促进了更稳健的共识，而 network connectivity (网络连接性) 的增加虽然加快了 convergence (收敛) 速度，但也加剧了 wrong-but-sure cascades (错误但确定的级联) 的风险，即智能体以高置信度收敛于错误决策。这些发现刻画了 LLM-based MAS (基于LLM的多智能体系统) 中的 conformity dynamics (从众动态)，阐明了 network topology (网络拓扑结构) 和 self-social weighting (自我-社会权重) 如何共同塑造集体决策的 efficiency (效率)、robustness (鲁棒性) 和 failure modes (失效模式)。",
                    "inspiration_trace": "基于论文《Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting》，以下是对作者核心方法论产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题界定\n**从“单体智能”到“群体涌现”的视角转变**\n\n1.  **现象观察**：随着LLM的发展，研究热点正从单一模型的推理能力转向多智能体系统（MAS）的协作。现有的MAS研究多关注任务完成的效率（如如何通过辩论提升准确率），却忽略了其中的**社会动力学机制**。\n2.  **核心痛点**：人类群体决策中存在“从众心理”，即个体倾向于调整判断以符合群体意见。在LLM构成的MAS中，这种“从众”是如何发生的？它是有助于消除噪声，还是会引发错误的信息级联？\n3.  **研究问题确立**：作者不再将LLM视为孤立的推理器，而是将其视为社会网络中的节点。核心问题转化为：**网络拓扑结构（谁和谁连接）与自我-社会权重（听自己的还是听别人的）如何共同塑造LLM群体的从众动态？**\n\n### 第二阶段：理论假设与机制抽象\n**将社会心理学概念转化为可计算模型**\n\n1.  **借鉴经典理论**：作者回顾了经典的舆论动力学模型（如DeGroot模型），但指出这些模型缺乏对LLM语义推理能力的刻画。\n2.  **关键变量提取**：\n    *   **置信度**：LLM不仅能给出判断（真/假），还能给出置信度。作者认为置信度是量化影响力的天然指标——越自信的智能体，对邻居的影响应越大。\n    *   **自我-社会权衡**：智能体在更新观点时，面临两难选择：是坚持己见（自我依赖），还是采纳邻居意见（社会影响）。\n3.  **方法论创新（核心公式）**：为了量化这一过程，作者提出了**置信度归一化池化规则**。\n    *   *逻辑推演*：需要一个参数 $\\alpha$ 来控制“自我”与“社会”的比重。同时，为了避免数值不稳定并模拟真实的信念更新，必须利用置信度 $p$ 对邻居的判断进行加权。\n    *   *结果*：这构建了一个通用的更新机制，使得从众行为不再是黑盒，而是可调节、可观测的数学过程。\n\n### 第三阶段：实验设计与拓扑解构\n**通过结构对比隔离变量**\n\n1.  **拓扑作为控制变量**：为了探究结构的影响，作者选取了两种极端的决策范式进行对比：\n    *   **中心化聚合**：模拟“独裁”或“专家咨询”模式（如星型网络）。假设是：决策快，但极度依赖中心节点的能力。\n    *   **分布式共识**：模拟“民主”或“去中心化”模式（如环状到全连接网络）。假设是：决策慢，但通过多轮交互可能达成更稳健的共识。\n2.  **任务选择**：选择**二分类虚假信息检测**任务。原因在于该任务有明确的真伪标准，便于量化群体决策的准确性，且容易触发“少数服从多数”的从众现象。\n\n### 第四阶段：实证发现与逻辑修正\n**从“效率-鲁棒性”权衡到“错误级联”的发现**\n\n1.  **验证假设**：实验证实了网络结构的关键作用。中心化结构下，Hub的能力决定了上限；分布式结构下，连接越紧密，收敛越快。\n2.  **意外发现（深层洞察）**：作者发现从众是一把双刃剑。\n    *   *正面*：适度的从众（$\\alpha=0.75$）能有效过滤个别智能体的噪声，提升整体准确率。\n    *   *反面*：在高连接度（全连接网络）且初始信号错误的情况下，群体会迅速达成**“错误但确信”的共识**。这揭示了LLM MAS的一个致命弱点：**回声室效应**。\n3.  **异质性分析**：进一步引入模型异质性（如GPT-4o与GPT-3.5混合），发现中心节点倾向于听取与其同源的模型意见（同源偏差），这进一步丰富了从众动态的内涵。\n\n### 第五阶段：理论升华\n**构建LLM MAS的设计原则**\n\n1.  **总结规律**：作者将实验现象上升为理论——LLM MAS中的从众动力学受拓扑和权重的联合调控。\n2.  **指导意义**：研究最终落脚于系统设计建议。没有绝对完美的结构，设计者必须在**收敛速度（效率）**与**抗级联能力（鲁棒性）**之间做权衡。\n3.  **逻辑闭环**：从最初的社会学观察（从众），到数学建模（置信度池化），再到实验验证（拓扑效应），最终回归到工程实践（如何设计更可靠的MAS），形成了一个完整的学术闭环。\n\n---\n\n**总结**：作者的思考路径是从**社会学的直觉**出发，利用**控制论的方法**（更新规则）进行建模，通过**网络科学的视角**（拓扑结构）进行实验剖析，最终揭示了LLM群体智能中**“盲目共识”的风险**，为构建更可靠的多智能体系统提供了理论依据。"
                },
                {
                    "title": "EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting",
                    "arxiv_id": "2601.05487",
                    "authors": "Huanxiang Lin, Qianyue Wang, Jinwu Hu, Bailin Chen, Qing Du, Mingkui Tan",
                    "summary": "Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \\textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \\textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \\textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了EvidFuse，这是一个用于文本-图表生成的多智能体框架。它涉及两个协作组件（智能体）：数据增强分析智能体和实时证据构建编写器，展示了智能体协作、规划（大纲规划）和工具使用（访问原始表格）等核心智能体特征，符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决现有数据驱动报告中图表与文本不一致及洞察冻结的问题。针对多数据表和用户分析请求，我们提出了一种名为EvidFuse的训练无关多智能体框架，通过Data-Augmented Analysis Agent和Real-Time Evidence Construction Writer实现写作时按需构建视觉证据。我们在Tableau、OWID和USAFacts数据集上，通过LLM-as-a-judge和人工评估验证了其在图表质量、文本图表对齐和报告有用性方面的有效性。",
                    "summary_translation": "数据驱动报告通过将叙述文本与基于底层表格的图表紧密交织，从而传达决策相关的见解。然而，当前的基于大语言模型的系统通常在分阶段流水线中生成叙述和可视化内容，遵循“先文本后图表”或“先图表后文本”的范式。这些设计往往导致图文不一致和见解冻结，即中间证据空间变得固定，模型无法随着叙述的演变检索或构建新的视觉证据，从而导致分析浅显且流于预设。为了解决这些局限性，我们提出了 **EvidFuse**，这是一个免训练的多智能体框架，能够在数据驱动报告的写作过程中实现文本与图表的交织生成。EvidFuse 通过两个协作组件将可视化分析与长文本撰写解耦：一个是配备了探索性数据分析（EDA）衍生知识并拥有原始表格访问权限的 **数据增强分析智能体**，另一个是负责规划大纲并起草报告，同时间歇性发出细粒度分析请求的 **实时证据构建编写器**。这种设计允许在叙述需要的确切时刻构建并整合视觉证据，直接约束后续论点，并实现证据空间的按需扩展。实验表明，在图表质量、图文对齐以及报告级实用性方面，EvidFuse 在大模型评判和人类评估中均获得了最高排名。",
                    "inspiration_trace": "基于论文《EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting》的内容，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察：数据报告生成的核心矛盾\n作者首先关注到一个宏观问题：高质量的数据驱动报告（如商业分析、政策报告）不仅仅是文本，而是**叙事文本与可视化图表的紧密交织**。\n*   **现状**：虽然大语言模型（LLM）在长文本生成上表现优异，但在生成这种“文图交织”的报告时，往往面临**一致性**（文本说的和图表画的不一样）和**深度**（仅停留在表面描述，缺乏决策洞察）的双重挑战。\n\n### 2. 问题诊断：现有范式的“时空错位”\n作者深入分析了现有的解决方案，发现它们大多遵循**分阶段流水线**，并指出了其根本缺陷：\n*   **范式 A：先文后图**。先写完故事，再插入图表。\n    *   *缺陷*：文本生成时并没有看到真实的图表，导致文本往往是“幻觉”或与最终生成的图表不匹配。\n*   **范式 B：先图后文**。先生成一组图表，再基于图表写故事。\n    *   *缺陷*：叙事被限制在预先生成的固定图表集合中。随着故事的发展，如果需要新的证据视角，模型无法回溯去生成新图。\n*   **核心症结**：作者将这一现象抽象为**“证据空间冻结”**。无论是先文还是先图，证据（图表）和叙事（文本）在时间上是分离的，导致两者无法在生成过程中相互动态约束。\n\n### 3. 假设提出：从“分阶段”到“写作时交织”\n为了解决“证据空间冻结”的问题，作者提出了一个核心假设：**证据的构建应该发生在写作的过程中，而不是写作之前或之后。**\n*   **新范式**：写作时证据构建。\n*   **逻辑推演**：如果模型在写到一个需要数据支撑的观点时，能够暂停，去生成一个精确的图表，拿到图表后再继续写接下来的文字，那么：\n    1.  文本将严格基于刚生成的真实图表（解决一致性问题）。\n    2.  叙事可以随时触发新的图表生成，不再受限于预设集合（解决深度和灵活性问题）。\n\n### 4. 方法设计：解耦与协作的双智能体架构\n为了实现上述“写作时交织”的理想状态，作者意识到让一个模型同时处理“复杂的数据分析/绘图”和“连贯的长文写作”会导致认知过载和上下文混乱。因此，逻辑演进转向了**任务解耦**：\n\n*   **角色一：数据增强分析代理**\n    *   *职责*：专门负责脏活累活。它需要懂探索性数据分析（EDA），能访问原始表格，能写代码画图。\n    *   *作用*：作为一个“工具”，随时响应具体的分析请求，产出带标注的图表。\n*   **角色二：实时证据构建写手**\n    *   *职责*：专门负责讲故事。它先规划大纲，然后分段写作。\n    *   *关键机制*：它具备“元认知”能力，知道何时需要证据。当它写到需要图表支撑的地方时，会发出一个特定的请求信号（如 `<visualization>`），然后**暂停生成**，等待分析代理返回图表结果，将图表注入上下文后，再恢复写作。\n\n### 5. 逻辑闭环：动态演进的证据空间\n通过上述设计，作者构建了一个动态闭环：\n*   **Writer** 发起请求 -> **Agent** 生成图表 -> **Writer** 基于图表继续写 -> 触发新请求...\n*   这种设计使得证据空间不再是静态的，而是随着叙事的深入不断**按需扩展**。文本约束了图表的内容（通过请求），图表约束了文本的描述（通过上下文注入），从而实现了真正的“文图一致”和“深度洞察”。\n\n### 总结\n作者的思考路径是从**发现现有方法“时空分离”导致的不一致性**出发，提出**“写作时构建证据”的范式转变**，进而通过**双智能体分工（Writer负责叙事流，Agent负责数据流）**来落地这一想法，最终实现了一个能够动态、按需生成高质量数据报告的框架。"
                },
                {
                    "title": "DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems",
                    "arxiv_id": "2601.07248",
                    "authors": "Shuyu Zhang, Yujie Liu, Xinru Wang, Cheng Zhang, Yanmin Zhu, Bin Li",
                    "summary": "Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了DarwinTOD框架，核心研究内容包括LLM驱动的终身自我演化（符合自我演化标准）以及在线多智能体对话执行与同行评审机制（符合多智能体标准），属于Agentic AI的研究范畴。",
                    "summary2": "本文旨在解决任务导向对话系统无法在部署后实现终身自主进化和适应新领域的问题。针对动态环境下的对话交互，我们提出了一种名为DarwinTOD的终身自进化框架，该框架集成了进化计算与LLM驱动的策略优化，通过维护可进化策略库（ESB）及双循环机制（在线多智能体执行与离线结构化进化）实现无人工干预的持续优化。我们在MultiWOZ和SGD数据集上通过Inform、Success、BLEU及Combine指标验证了其有效性。",
                    "summary_translation": "传统的任务型对话系统无法从持续的交互中进行演化，也无法在部署后适应新领域，这是其在现实世界动态环境中的一个关键局限。持续学习方法依赖于基于人工策划数据的阶段性重训练，未能实现自主的终身改进。尽管进化计算和 LLM (Large Language Model, 大语言模型) 驱动的自我改进为对话优化提供了有前景的机制，但它们缺乏一个用于全面、迭代策略优化的统一框架。为了弥合这一差距，我们提出了 DarwinTOD，一个终身自演化对话框架，该框架系统性整合了这两种范式，从而能够在无需特定任务微调的情况下，从零样本基础开始实现持续的策略优化。DarwinTOD 维护一个 Evolvable Strategy Bank (可演化策略库)，并通过双环过程运行：包含同伴评议的在线多智能体对话执行，以及利用累积反馈优化策略库的离线结构化进化操作。这种闭环设计使得无需人工干预即可实现自主的持续改进。大量实验表明，DarwinTOD 优于以往最先进的方法，并在整个演化过程中展现出持续的性能提升。我们的工作为构建具有终身自演化能力的对话系统提供了一个新颖的框架。",
                    "inspiration_trace": "基于对论文《DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems》的深度分析，以下是作者产出该核心方法的逻辑推演过程，还原了从宏观观察到具体方法论的思考链条：\n\n### 第一阶段：宏观问题识别——从“静态系统”到“动态世界”的矛盾\n\n**1. 现实观察：**\n作者首先观察到现实世界是动态变化的。用户的偏好、对话的领域以及任务的目标都在不断演进。然而，现有的任务型对话系统（TOD）在部署后本质上是“静态”的——一旦训练完成，其能力就被冻结，无法从后续的交互中学习或适应新领域。\n\n**2. 核心痛点提炼：**\n这导致了“研究原型”与“可部署系统”之间的巨大鸿沟。学术界通常在静态基准上评估模型，而工业界需要的是一个能在开放、动态环境中长期运行的智能体。因此，核心问题不再是“如何让模型在测试集上表现好”，而是**“如何让系统具备终身自我进化的能力，实现完全自主的持续改进？”**\n\n### 第二阶段：现有范式的批判与局限分析\n\n**1. 审视传统方案：**\n作者逐一分析了现有技术路线，发现它们都无法解决上述核心问题：\n*   **流水线架构：** 虽然模块化，但存在级联错误传播，且难以适应新领域，缺乏灵活性。\n*   **端到端大模型（LLM）：** 虽然泛化能力强，但本质上仍是基于初始指令的静态执行，缺乏从经验中学习的机制。\n*   **持续学习：** 虽然试图增量更新，但严重依赖人工整理的数据和周期性的重训练，无法实现真正的“自主”和“终身”进化。\n\n**2. 寻找突破口：**\n作者意识到，要实现真正的自主进化，必须摆脱对“人工标注数据”和“模型参数微调”的依赖，转而寻找一种能够利用系统自身交互经验进行自我优化的机制。\n\n### 第三阶段：理论融合——进化计算与大模型的互补性思考\n\n**1. 两个孤立的方向：**\n作者注意到了两个有潜力但各自为政的研究方向：\n*   **进化计算：** 擅长基于种群的优化，能通过选择、变异等机制寻找最优解，但缺乏语义理解能力，通常只用于优化孤立的提示词。\n*   **LLM驱动的自我改进：** 擅长推理和反思，能通过多智能体协作解决问题，但往往缺乏结构化的长期策略管理机制，容易陷入单轮优化的局部视角。\n\n**2. 逻辑跃迁（核心假设）：**\n**“如果将LLM作为进化算法的‘大脑’，利用其强大的语义理解和推理能力来驱动对话策略的进化，会发生什么？”**\n作者认为，LLM可以作为智能的“进化算子”，而进化算法提供了结构化的“优化框架”。两者的结合可以解决各自的短板：进化算法提供了终身迭代的框架，LLM提供了语义层面的策略生成与评估能力。\n\n### 第四阶段：方法论构建——从“单点优化”到“种群进化”\n\n**1. 核心概念定义：**\n基于上述假设，作者提出了**“可进化策略库”**的概念。\n*   **思维转变：** 传统的Prompt Engineering是在寻找一个“最好的”提示词。而DarwinTOD转向维护一个“多样化的策略种群”。这些策略在交互中竞争、优胜劣汰。\n\n**2. 闭环机制设计：**\n为了实现终身进化，作者设计了一个**“双循环”架构**，将理论落地：\n*   **在线执行循环：** 模拟真实环境。作者没有使用单一的端到端Agent，而是保留了**多智能体流水线（DST, DP, NLG）**。为什么？因为模块化不仅能防止错误级联，更重要的是，它允许每个模块拥有独立的策略，从而实现更细粒度的进化。\n*   **引入“同伴批判”：** 为了获得比单纯的“任务成功/失败”更密集的反馈信号，作者让智能体之间互相批判。这不仅能实时纠错，还能为离线进化提供高质量的反思数据。\n\n**3. 离线进化循环：**\n这是系统的“大脑”部分。作者设计了四种受进化论启发的操作算子，直接作用于策略库：\n*   **Genesis（创生）：** 针对新领域，利用LLM的零样本能力从无到有生成策略。\n*   **Mutation（变异）：** 针对失败的对话，利用LLM分析失败原因并修改策略。\n*   **Consolidation（整合）：** 利用LLM合并相似的策略，保持种群精简。\n*   **Pruning（剪枝）：** 淘汰低适应度的策略，控制计算成本。\n\n### 第五阶段：鲁棒性思考——应对噪声与不确定性\n\n**1. 潜在风险识别：**\n作者意识到，LLM生成的批判和变异可能包含噪声或偏见。如果系统盲目信任每一次反馈，可能会导致策略退化。\n\n**2. 解决方案设计：**\n为了解决这个问题，作者引入了**“适应度函数”**和**“玻尔兹曼选择”**机制。\n*   **长期统计：** 不依赖单次反馈，而是基于长期的历史表现（正负反馈计数）来计算策略的适应度。\n*   **概率选择：** 即使策略当前适应度低，也有一定概率被选中（探索），防止过早收敛到局部最优。\n*   **逻辑闭环：** 这种设计使得系统具有“抗噪性”，即使偶尔有错误的批判，长期的大数定律和种群选择机制也能过滤掉噪声，确保进化方向是向上的。\n\n### 总结：逻辑演进的全景图\n\n作者的思考路径是从**现实世界的动态需求**出发，批判了现有技术的静态本质，通过**融合进化计算的结构化优势与LLM的语义优势**，创造性地提出了**基于种群策略进化的新范式**。最终，通过**双循环架构**和**抗噪的进化机制**，将这一理论转化为一个无需人工干预、能够终身自我进化的对话系统。"
                },
                {
                    "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
                    "arxiv_id": "2601.07152",
                    "authors": "Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan",
                    "summary": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了一个名为“Agents of Diffusion”的框架，明确采用了多智能体架构，包含“提示优化智能体”和“判别智能体”。这些智能体通过协作和自然语言反馈来引导生成过程，符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决生成高质量结构化数据时难以兼顾语义丰富性与严格模式一致性的挑战。针对结构化文本生成任务，我们提出了Agents of Diffusion (AoD)框架，利用多智能体强化学习通过自然语言反馈迭代引导冻结的Diffusion Language Models (DLMs)。在MultiWOZ、Super-NaturalInstructions等数据集上，通过Task Success Rate (TSR)和Field Overlap等指标验证了其有效性，实现了优于基线模型的结构保真度与语义多样性平衡。",
                    "summary_translation": "生成高质量的结构化数据（例如 JSON 记录）仍然是大语言模型面临的一项基本挑战，尤其是在必须兼顾语义丰富性与严格模式遵守的情况下。尽管自回归大语言模型具备强大的结构一致性，但它们往往难以应对语义变化和输出多样性方面的要求。相比之下，扩散语言模型引入了实现语义丰富性和双向解码的强大机制，却缺乏可靠保持结构所需的归纳偏置。我们提出了 Agents of Diffusion (AoD)，这是一个新颖的框架，通过语言介导的强化学习，将扩散语言模型的生成灵活性与自回归模型的推理能力统一起来。AoD 将结构化文本生成构建为一个多智能体对齐过程，其中提示优化智能体与评判智能体协作，利用自然语言反馈迭代指导扩散语言模型。这种方法实现了可控且符合模式规范的生成，而无需修改模型参数或依赖人工设计的约束。AoD 证明了扩散模型在协作智能体的监督下能够同时实现高语义新颖性和结构保真度，从而推进了可控生成领域的发展。在多个结构化数据基准测试中，AoD 始终优于扩散模型和自回归模型的基线，为结构感知且多样性增强的文本合成开辟了一条新路径。",
                    "inspiration_trace": "基于论文《Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法创新的思考过程。\n\n---\n\n### 1. 宏观问题与现状观察：结构化生成的“两难困境”\n**思考起点：** 作者首先关注到生成高质量结构化数据（如JSON）是当前LLM应用的一个核心痛点。\n*   **观察现象：** 现有的两大生成范式存在明显的优缺点互补，但无法兼得。\n    *   **自回归模型（AR-LLMs）：** 具有极强的结构一致性和因果逻辑（因为是从左到右生成），容易符合Schema。但缺点是语义单一、容易陷入重复模式，缺乏多样性。\n    *   **扩散语言模型：** 具有双向去噪机制，语义丰富、多样性高。但缺乏位置先验，很难严格保持复杂的嵌套结构（如JSON的括号匹配、字段完整性）。\n*   **核心矛盾：** 我们既想要扩散模型的“语义多样性”，又想要自回归模型的“结构严谨性”。现有的方法要么微调模型（成本高），要么使用硬性规则（缺乏灵活性）。\n\n### 2. 核心假设：用“推理”驾驭“生成”\n**思考转折：** 既然重新训练一个完美的模型很难，能否通过“外部控制”来弥补内部缺陷？\n*   **假设提出：** 能否利用自回归模型强大的逻辑推理能力，来“监督”或“引导”扩散模型的生成过程？\n*   **关键洞察：** 不需要修改扩散模型的参数（保持其生成多样性），而是通过改变其输入条件来控制输出。\n*   **控制接口：** 最直接的控制接口就是**提示词**。如果能让提示词动态进化，就能在不改动模型权重的情况下，引导模型生成符合结构要求的内容。\n\n### 3. 方法论演进：从静态提示到动态强化学习\n**思考深化：** 传统的提示工程是静态的（写一次，固定用），无法应对生成过程中的随机性和错误。如何实现动态控制？\n*   **机制选择：** 引入**强化学习（RL）**。将“提示词的修改”看作是一个动作，将“生成结果的质量”看作是奖励。\n*   **反馈信号的困境：** 传统的RL通常使用标量奖励（如一个分数）。但在结构化生成中，一个分数很难解释具体的错误（例如：“缺少字段”还是“格式错误”）。\n*   **创新点：** **自然语言反馈**。既然是语言模型，为什么不直接用语言来作为奖励信号？语言反馈比标量数字包含更丰富的信息，且更容易被LLM理解和执行。\n\n### 4. 架构构建：多智能体分工协作\n**具体化：** 如何将上述理论落地？作者设计了一个基于角色的多智能体系统，将任务拆解。\n*   **角色分工：**\n    *   **生成者：** 冻结的扩散模型。负责提供多样化的候选内容（探索者）。\n    *   **评判者：** 自回归LLM。负责检查生成内容的结构完整性和语义准确性，并输出自然语言反馈（批评家）。\n    *   **优化者：** 另一个自回归LLM。负责根据评判者的反馈，修改提示词（决策者）。\n*   **闭环逻辑：**\n    1.  优化者给出初始提示。\n    2.  扩散模型根据提示生成JSON。\n    3.  评判者检查JSON，给出具体建议（如：“缺少date字段，请修正”）。\n    4.  优化者根据建议修改提示词（如：“确保包含YYYY-MM-DD格式的date字段”）。\n    5.  循环往复，直到生成完美结果。\n\n### 5. 理论保障与最终形态\n**逻辑闭环：** 为什么这个系统是稳定且有效的？\n*   **解决“漂移”问题：** 多智能体系统常面临对话发散的问题。作者通过将扩散模型作为“环境锚点”，所有智能体的交互都围绕具体的生成样本展开，从而保证了交互的稳定性。\n*   **无参数化优势：** 整个过程不需要梯度回传更新扩散模型，完全通过语言层面的交互实现优化。这使得该方法可以即插即用于各种开源或闭源模型。\n\n---\n\n**总结：作者的思考路径**\n从**“结构 vs 多样性”**的矛盾出发 $\\rightarrow$ 提出**“用AR推理控制DLM生成”**的假设 $\\rightarrow$ 选择**“提示词”**作为控制抓手 $\\rightarrow$ 引入**“强化学习+自然语言反馈”**实现动态优化 $\\rightarrow$ 最终构建**“生成-评判-优化”**的多智能体协作闭环。"
                },
                {
                    "title": "SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models",
                    "arxiv_id": "2601.07252",
                    "authors": "Chunwei Yang, Yankai Wang, Jianxiang Tang, Haojie Qu, Ziqiang Zou, YuLiu, Chunrui Deng, Zhifang Qiu, Ming Ding",
                    "summary": "Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了一个基于多种大语言模型的多智能体系统，专注于智能体之间的协作、智能纠错（自我反思）以及检索增强生成（RAG），符合研究范围中关于“多智能体：协作”和“工具使用”的定义。尽管论文涉及CFD领域应用和多模态输入，但其核心贡献在于构建智能体框架而非单纯的应用落地或多模态模型开发。",
                    "summary2": "本文旨在解决现有CFD多智能体系统在处理复杂几何形状时的局限性。针对包含图像和文本的多模态输入场景，我们提出了一种名为SwarmFoam的多智能体框架，集成了多模态感知、首错优先智能纠错及RAG机制。我们在25个涵盖多种物理问题的测试用例上，通过Pass Rate、Token Usage等指标验证了其有效性，总体通过率达到84%。",
                    "summary_translation": "数值模拟是科学研究中的主流方法之一，通常由专业工程师执行。随着多智能体技术的进步，利用协作智能体模拟人类行为，在智能计算流体力学 (CFD) 模拟方面展现出巨大潜力。目前已提出了一些基于大语言模型的多智能体系统。然而，在处理复杂几何结构时，这些系统表现出显著的局限性。本文介绍了一种新的多智能体模拟框架——SwarmFoam。SwarmFoam 集成了 Multi-modal perception (多模态感知)、Intelligent error correction (智能纠错) 和 Retrieval-Augmented Generation (检索增强生成) 等功能，旨在通过对图像和高级指令的双重解析来实现更复杂的模拟。实验结果表明，SwarmFoam 对不同模态的模拟输入具有良好的适应性。在 25 个测试用例中，整体通过率为 84%，其中自然语言输入和多模态输入用例的通过率分别为 80% 和 86.7%。SwarmFoam 所展示的工作将进一步推动 CFD 智能体方法的发展。",
                    "inspiration_trace": "基于论文《SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models》的内容，以下是对作者产出该核心方法的逻辑链推演，旨在还原其从宏观观察到微观实现的思考过程：\n\n---\n\n### 第一阶段：宏观观察与趋势研判\n**（从“AI for Science”到“智能体自动化”的范式转移）**\n\n1.  **背景洞察**：作者首先观察到计算流体力学（CFD）是科研和工程的核心工具，但高度依赖专业工程师，门槛高、流程繁琐。\n2.  **技术趋势**：随着生成式AI的爆发，利用大语言模型（LLM）和多智能体系统来模拟人类专家行为、自动化执行复杂任务已成为新趋势。\n3.  **现状评估**：作者回顾了2024-2025年的前沿工作（如MetaOpenFOAM、OpenFOAMGPT、Foam-Agent），确认了“基于LLM的智能体自动化OpenFOAM仿真”这一技术路线的可行性。\n4.  **核心痛点识别**：尽管现有系统（如Foam-Agent）已经能通过自然语言驱动仿真，但作者敏锐地发现了一个关键局限——**“模态缺失”**。在真实的工程场景中，几何结构和物理条件往往通过图纸、图像传递，仅靠自然语言描述复杂几何极其困难且不准确。\n\n### 第二阶段：问题聚焦与假设提出\n**（突破“文本单一性”与“纠错低效性”的双重瓶颈）**\n\n1.  **针对输入模态的假设**：\n    *   *思考*：如果给智能体加上“眼睛”，让它能像人类工程师一样看懂几何图纸，是否能大幅提升仿真的准确性和适用范围？\n    *   *假设*：引入多模态感知能力，结合图像和文本输入，能解决复杂几何描述不清的问题。\n\n2.  **针对多模态处理策略的假设**：\n    *   *思考*：有了图像能力后，是直接把图像丢给写配置文件的智能体，还是先专门解析？\n    *   *假设*：将“图像理解”与“网格生成”解耦。先由一个专门的智能体把图像信息转化为结构化的几何/物理文本描述，再传递给后续智能体，比直接端到端生成效果更好（避免信息丢失）。\n\n3.  **针对纠错机制的假设**：\n    *   *思考*：现有系统（如Foam-Agent）在仿真失败时，倾向于一次性分析所有错误日志并批量修改。这既浪费Token，又可能因为修改了由“根源错误”导致的“衍生错误”而陷入混乱。\n    *   *假设*：采用“首错优先”策略。假设第一个报错是根源，只修复它，然后重试。这种迭代方式虽然可能增加轮次，但能大幅降低单次推理成本，并避免无效修改。\n\n### 第三阶段：方法论构建与架构设计\n**（从“假设”到“SwarmFoam”系统架构的落地）**\n\n1.  **引入“观察者”**：\n    *   为了验证多模态假设，作者设计了**Observer Agent**。它的核心任务是“看图说话”，将用户输入的图像和自然语言，解析为标准的几何参数（如顶点坐标）和物理条件（如边界类型）。\n\n2.  **构建协作流水线**：\n    *   为了实现全流程自动化，作者将人类工程师的仿真工作流拆解为六个角色，形成流水线：\n        *   **Observer**（感知）：解析图文。\n        *   **Architect**（规划）：决定需要哪些文件，规划目录结构。\n        *   **InputWriter**（执行）：利用RAG（检索增强生成）技术，参考历史案例编写具体的配置文件。\n        *   **Runner**（运行）：执行OpenFOAM命令。\n        *   **Reviewer**（纠错）：实施“首错优先”策略，定位并反馈第一个错误。\n        *   **ParaMaster**（后处理）：自动调用ParaView生成可视化结果。\n\n3.  **强化知识支撑（RAG）**：\n    *   考虑到LLM可能产生幻觉或不懂OpenFOAM的特定语法，作者引入了RAG系统，将官方文档、参考案例等作为“外挂大脑”，确保生成的配置文件符合规范。\n\n### 第四阶段：验证与迭代优化\n**（通过消融实验验证核心假设）**\n\n1.  **验证多模态策略**：\n    *   *实验设计*：对比“预解析图像”（Method 1）与“直接利用图像”（Method 2）。\n    *   *结果反馈*：实验发现直接利用图像会导致大量几何信息丢失，通过率大幅下降。这证实了作者在第二阶段的判断——**解耦图像解析与文件生成是必要的**。\n\n2.  **验证纠错策略**：\n    *   *实验设计*：对比开启/关闭智能纠错机制，以及与基线模型（Foam-Agent）的Token消耗对比。\n    *   *结果反馈*：虽然“首错优先”可能增加迭代次数，但显著降低了Token使用量（-83.85%），证明了其在经济性和逻辑清晰度上的优势。\n\n3.  **综合评估**：\n    *   通过25个涵盖单相流、多相流、燃烧等不同物理问题的测试用例，最终验证了SwarmFoam在多模态输入下的高通过率（86.7%），确立了其作为新一代智能CFD仿真系统的有效性。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“观察-假设-设计-验证”**科研闭环：\n从**“现有智能体看不懂图纸”**这一具体痛点出发，提出**“引入多模态感知”**和**“首错优先纠错”**的创新假设，进而通过精细化的**Agent角色分工**和**RAG技术**构建了SwarmFoam系统，最后通过严格的**消融实验**证实了其设计思路的正确性。"
                },
                {
                    "title": "Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents",
                    "arxiv_id": "2601.06490",
                    "authors": "Wenyu Mao, Haosong Tan, Shuchang Liu, Haoyang Liu, Yifan Xu, Huaxiang Ji, Xiang Wang",
                    "summary": "Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了一个名为Bi-Mem的智能体框架，利用归纳智能体和反思智能体来构建和校准分层记忆。这属于单智能体研究范畴中的“记忆”和“自我反思”能力，符合筛选条件。",
                    "summary2": "本文旨在解决个性化LLM分层记忆中因噪声和幻觉导致的局部记忆与全局画像不一致问题。针对长期个性化对话场景，我们提出了一种名为Bi-Mem的智能体框架，通过归纳-反思双向构建机制校准记忆，并引入联想检索。我们在LoCoMo数据集上通过F1和BLEU-1指标验证了其有效性。",
                    "summary_translation": "从用户的长期对话中构建记忆，能够克服大语言模型的上下文限制，从而实现个性化交互。近期的研究侧重于层次化记忆，旨在通过聚类和聚合历史对话来建模用户的多粒度行为模式。然而，对话噪声和记忆幻觉可能在聚类过程中被放大，导致局部聚合记忆与用户的全局人设不一致。为缓解这一问题，我们提出了Bi-Mem，这是一个通过双向构建来确保层次化记忆保真度的智能体框架。具体而言，我们部署了一个归纳智能体来构建层次化记忆：该智能体从原始对话中提取事实信息以形成事实级记忆，利用图聚类将其聚合为主题场景（即局部场景级记忆），并推断用户画像作为全局人设级记忆。同时，我们设计了一个反思智能体，利用从人设级记忆中导出的全局约束来校准局部场景级记忆，从而强化全局-局部对齐。为实现连贯的记忆回忆，我们提出了一种联想检索机制：除了初始的层次搜索外，扩散激活过程允许事实激活上下文场景，而场景级匹配则检索显著的支撑性事实信息。实证评估表明，Bi-Mem在长期个性化对话任务的问答性能方面取得了显著提升。",
                    "inspiration_trace": "基于论文《Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观背景与现有范式的演进\n**（从“个性化需求”到“分层记忆”的必然性）**\n\n1.  **观察起点：** LLM的上下文窗口有限，无法容纳用户的长期对话历史。为了实现个性化交互（如记住用户偏好、回忆共同经历），必须引入外部记忆机制。\n2.  **现有方案的局限：** 早期的“扁平化记忆”仅存储孤立的事实或摘要，缺乏对事实间关联和用户高层行为模式的捕捉能力。\n3.  **趋势演进：** 研究自然转向了“分层记忆”，即通过聚类将原子事实聚合为场景，再提炼为用户画像。这种结构模仿了人类认知，从细粒度到粗粒度，看似完美解决了信息碎片化问题。\n\n### 第二阶段：关键问题的发现与诊断\n**（从“单向聚合”到“级联错误”的洞察）**\n\n1.  **深入审视：** 作者观察到，现有的分层记忆构建过程大多是**单向的**，即纯粹的自底向上聚合。\n2.  **核心痛点：** 在自底向上的过程中，原始对话中的“噪声”（如无关闲聊）和提取过程中的“幻觉”会被聚类算法放大。\n3.  **逻辑矛盾：** 这种放大的噪声会导致局部聚合的场景记忆与用户的全局画像发生冲突。\n    *   *案例思考：* 用户平时口味清淡（全局画像），但偶尔陪朋友吃了一次辣（局部场景）。单纯的聚类会错误地将“吃辣”归纳为用户的局部习惯，导致后续推荐出错。\n4.  **归纳假设：** 问题的根源在于缺乏“全局约束”。局部记忆的生成缺乏对全局一致性的校验，导致了“级联错误”的积累。\n\n### 第三阶段：核心假设与方法论的提出\n**（从“单向构建”到“双向闭环”的突破）**\n\n1.  **解决思路：** 为了解决局部与全局的冲突，记忆构建不能只是单向的归纳，必须引入反向的反思机制。\n2.  **框架设计：** 提出 **Bi-Mem** 框架，将记忆构建过程拆解为两个互补的智能体：\n    *   **归纳智能体：** 负责传统的自底向上构建（事实 -> 场景 -> 画像）。这是为了从数据中提取信息。\n    *   **反思智能体：** 负责自顶向下的校准。利用生成的全局画像作为“约束条件”，去检查和修正下层的场景记忆。\n3.  **逻辑闭环：** 通过这种“双向构建”，确保了局部细节（场景）始终服务于并服从于全局特征（画像），消除了记忆中的逻辑矛盾。\n\n### 第四阶段：记忆利用机制的优化\n**（从“静态检索”到“动态关联”的完善）**\n\n1.  **新问题：** 虽然记忆结构被修正了，但在检索时，如果仅按层级独立检索（如只查场景或只查画像），可能会割裂事实与上下文的联系。\n2.  **联想机制：** 作者引入了心理学中的“扩散激活”概念。\n3.  **检索逻辑：** 检索不应是孤立的。\n    *   检索到一个“事实”时，应自动激活其所属的“场景”。\n    *   检索到一个“场景”时，应回溯其包含的关键“事实”。\n4.  **最终形态：** 形成了**联想检索机制**，在初始检索后进行跨层级的扩散，确保模型在生成回答时能同时获得宏观的上下文和微观的证据支持。\n\n---\n\n### 总结：作者的思维演进路径\n\n1.  **宏观需求：** LLM需要长期记忆来实现个性化。\n2.  **技术选型：** 分层记忆优于扁平记忆。\n3.  **批判性观察：** 现有的分层记忆是单向的，容易因噪声放大导致“局部-全局”不一致。\n4.  **核心创新：** 引入“反思”机制，构建双向闭环（归纳+校准），用全局画像约束局部场景。\n5.  **应用落地：** 设计联想检索，打通层级间的壁垒，实现连贯的记忆召回。"
                },
                {
                    "title": "DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation",
                    "arxiv_id": "2601.06373",
                    "authors": "Yutong Song, Jiang Wu, Kazi Sharif, Honghui Xu, Nikil Dutt, Amir Rahmani",
                    "summary": "Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了 DemMA，一个用于模拟痴呆症患者的单智能体系统。它涉及智能体核心能力，如记忆（构建人格）和行动（模拟非语言行为），并提出了特定的智能体训练框架（CoT 蒸馏），而不仅仅是将现有智能体应用于医疗任务。",
                    "summary2": "本文旨在解决痴呆症模拟中数据稀缺及缺乏医学严谨性的挑战。针对多轮对话场景，我们提出了一种名为DemMA的专家引导推理与动作模拟框架。该方法通过临床人格构建和多智能体工作流生成数据，并利用CoT蒸馏技术将推理、语言和动作生成整合到单个LLM中。我们在DemMA-Dialogue数据集上，通过人格一致性、医学一致性等指标验证了其有效性，显著优于基线模型。",
                    "summary_translation": "利用大语言模型模拟 dementia patients (痴呆症患者) 具有挑战性，因为需要在长对话过程中对 cognitive impairment (认知障碍)、emotional dynamics (情绪动态) 和 nonverbal behaviors (非语言行为) 进行联合建模。我们提出了 DemMA，这是一个专家引导的 dementia dialogue agent (痴呆症对话智能体)，旨在实现高保真的 multi-turn patient simulation (多轮患者模拟)。DemMA 通过整合病理信息、人格特质以及由临床专家指导的特定亚型 memory-status personas (记忆状态人格)，构建了基于临床的 dementia personas (痴呆症人格)。为了突破纯文本模拟的局限，DemMA 对 nonverbal behaviors (非语言行为)（包括动作、面部表情和声音线索）进行了显式建模。我们进一步引入了一个 Chain-of-Thought (思维链) 蒸馏框架，该框架训练单个 LLM 在一次前向传播中联合生成 reasoning traces (推理轨迹)、患者话语以及对齐的行为动作，从而无需 multi-agent inference (多智能体推理) 即可实现高效部署。与专家、医学生及 LLM 评判者进行的广泛评估表明，DemMA 在多项指标上均显著优于现有的强 baselines (基线模型)。",
                    "inspiration_trace": "基于论文《DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体技术方案产出的思考过程：\n\n---\n\n### 第一阶段：宏观问题与痛点观察\n**思考起点：数据荒漠与模拟困境**\n作者首先观察到痴呆症研究和护理培训领域存在一个根本性的结构瓶颈：**高质量互动数据的极度稀缺**。\n*   **现实约束**：由于隐私和伦理限制，真实的患者数据（尤其是包含面部表情、语音语调等多模态信息的数据）几乎无法获取。\n*   **现有缺陷**：现有的模拟手段要么依赖僵化的脚本，无法捕捉真实互动的异质性；要么直接使用通用的对话模型，但这在医疗场景下极不可靠。\n\n### 第二阶段：深入剖析与假设提出\n**思考深入：通用大模型为何失效？**\n作者意识到，直接将LLM作为痴呆症患者模拟器存在三个核心矛盾，这构成了后续方法设计的假设前提：\n1.  **医学严谨性缺失**：通用模型缺乏临床依据，可能生成不安全或医学上不准确的建议。\n2.  **“过度完美”悖论**：LLM倾向于生成流畅、礼貌的回复，但这恰恰掩盖了痴呆症患者特有的认知衰退标志（如重复、犹豫、逻辑断裂）。这种“人格漂移”会导致模拟失真。\n3.  **模态缺失**：痴呆症沟通是多通道的（语言、情感、行为），纯文本模型丢失了非语言线索（如动作、神态），而这些随着语言能力下降变得愈发重要。\n\n**核心假设**：要实现高保真模拟，必须从**“通用对话生成”**转向**“临床病理驱动的行为建模”**。\n\n### 第三阶段：方法论演进与逻辑构建\n为了验证上述假设，作者分三个步骤构建了解决方案：\n\n#### 步骤一：构建临床锚点——从“角色扮演”到“病理分层”\n**思考**：如何防止模型生成随机的“疯言疯语”，而是生成符合特定痴呆症亚型的症状？\n**逻辑推演**：患者的人格不应是随机的，而应是病理学的产物。\n*   **创新点**：提出了**分层人格构建范式**。\n    *   不再使用单一的Prompt，而是将患者解构为三个依赖层：**背景层**（人口统计学+亚型病理）、**性格层**（基于ICF标准的心理功能）、**记忆层**（长/短期记忆状态）。\n    *   **目的**：通过这种结构化约束，确保生成的“遗忘”或“混乱”是特定病理（如阿尔茨海默症 vs. 额颞叶痴呆）的临床表现，而非模型的随机幻觉。\n\n#### 步骤二：解决数据与质量控制——多智能体流水线\n**思考**：既然没有真实数据，如何合成高质量数据？同时，如何解决长对话中的逻辑一致性问题？\n**逻辑推演**：单一模型难以同时兼顾记忆分析、对话规划和动作生成。需要“分而治之”。\n*   **创新点**：设计了**多智能体LLM工作流**。\n    *   引入专门的**记忆分析智能体**（判断当前哪些记忆可访问）、**对话规划智能体**（决定情感轨迹和内容）、**生成智能体**（产出语言）、**动作标注智能体**（补充非语言行为）以及**验证智能体**。\n    *   **目的**：通过将推理过程外显化，不仅生成了首个合成数据集，还确保了每一步都有临床逻辑支撑，解决了长对话的一致性问题。\n\n#### 步骤三：解决落地效率——思维链蒸馏\n**思考**：多智能体系统虽然质量高，但推理延迟大，无法满足实时护理培训的需求。如何保留“专家级推理”的同时，实现“单模型高效推理”？\n**逻辑推演**：多智能体的过程本质上是生成了丰富的“思维链”。如果能让一个模型学会这些思维过程，就不需要在推理时调用多个模型。\n*   **创新点**：提出了**CoT蒸馏多任务训练框架**。\n    *   将多智能体流水线产生的推理轨迹作为中间监督信号，训练一个单一模型同时完成“推理（规划）+ 说话（文本）+ 行动（多模态标签）”。\n    *   **目的**：将复杂的系统级逻辑内化为单模型的参数，实现了低延迟下的高保真模拟。\n\n### 第四阶段：最终方案合成\n**思考总结**：DemMA不仅仅是一个聊天机器人，而是一个**“临床 grounded 的多模态行为模拟器”**。\n\n**逻辑闭环**：\n1.  **输入端**：通过分层人格模块注入临床病理知识。\n2.  **训练端**：利用多智能体生成的高质量合成数据，通过CoT蒸馏，教会单模型如何像专家一样分析记忆状态、规划对话并匹配非语言行为。\n3.  **输出端**：在一个前向传播中，同时输出符合病理特征的语言、显式的推理逻辑以及对应的动作标签（Motion/Face/Sound），从而在文本界面中补偿了非语言信息的缺失。\n\n---\n\n**总结**：作者的思考路径是从**“数据稀缺”**的现实出发，识别出**“通用模型不适用”**的本质矛盾，进而通过**“结构化病理建模”**确立内容准确性，利用**“多智能体外显推理”**保证数据质量，最后通过**“知识蒸馏”**解决工程效率问题，最终实现了DemMA这一高保真、可落地的痴呆症模拟系统。"
                },
            ]
        },
    ],
    "2026-01-09": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                    "arxiv_id": "2601.04861",
                    "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
                    "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 OI-MAS 的新型多智能体框架。它不是将现有的智能体简单应用到某个垂直领域，而是针对多智能体系统（MAS）中存在的计算效率低下问题，提出了改进的架构和方法论。这符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确属于 `Multi-Agent Systems (MAS)`，关注多智能体协作。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并提出了动态选择智能体角色和模型尺度的机制，这是对多智能体协作机制的直接改进。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   虽然论文提到了“计算效率”和“成本降低”，但这属于智能体框架的优化和改进，而非底层的基础设施（如硬件加速）或单纯的部署优化，因此不应被排除。 综上所述，该论文通过引入置信度感知路由和自适应模型选择策略，实质性地改进了多智能体协作的效率和性能，是关于 LLM 智能体架构优化的高质量研究。",
                    "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。",
                    "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。"
                },
            ]
        },
    ],
    "2026-01-08": [
        {
            "name": "Artificial Intelligence",
            "count": 9,
            "papers": [
                {
                    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
                    "arxiv_id": "2601.05187",
                    "authors": "Yanchang Liang, Xiaowei Zhao",
                    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，主要基于以下判断： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文的核心不仅仅是将LLM应用于Simulink这一特定领域，而是提出了一种新的智能体架构和训练方法。具体而言，它提出了 **Reflection-GRPO (ReGRPO)**，这是一种结合了**自我反思**机制的强化学习算法。该算法通过自我反思痕迹提供中间反馈，解决长视距任务中的稀疏奖励问题，从而加速收敛并提高鲁棒性。这直接对应了筛选标准中的“自我演化”和“自我反思”机制。 2.  **具备明确的Agentic特征**： 论文描述了一个轻量级的 **“计划-执行”架构**，并明确提到该架构赋予智能体低级工具技能和高级设计推理能力。这符合筛选标准中关于“规划”和“工具使用”的正面指标。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是Simulink建模（特定工程领域），但根据筛选标准第四步第2点，如果论文的核心是提出一种新的“自我演化”机制（即ReGRPO和两阶段课程学习），即使它被应用在特定领域，也应该保留。本文的重点在于智能体如何通过反思和强化学习进行自我完善和迭代，而非单纯的应用。 综上所述，该论文在构建智能体架构、引入自我反思机制以及通过强化学习实现自我演化方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决LLM在图形化Simulink建模中的应用难题。针对Simulink建模任务，我们提出了SimuAgent框架，采用轻量级Python字典表示模型，并引入Reflection-GRPO (ReGRPO)算法结合两阶段训练策略。我们在新发布的SimuBench数据集（5300个任务）上进行了实验，通过建模准确率验证了其有效性。结果显示，SimuAgent比标准RL收敛更快，且超越了GPT-4o。",
                    "summary_translation": "大语言模型彻底变革了基于文本的代码自动化，但它们在面向图形的工程工作流中的潜力仍未得到充分探索。我们介绍了SimuAgent，这是一个专为Simulink定制的、由LLMs（大语言模型）驱动的建模与仿真智能体。SimuAgent用简洁的字典风格Python表示法取代了冗长的XML（可扩展标记语言），大幅减少了token（词元）数量，提高了可解释性，并实现了快速的进程内仿真。一种轻量级的plan-execute（规划-执行）架构，分两个阶段进行训练，使智能体既具备低级工具技能，又具备高级设计推理能力。为了解决长视界任务中的稀疏奖励问题，我们提出了Reflection-GRPO (ReGRPO)，它通过自我反思轨迹增强了Group Relative Policy Optimization (GRPO)（群体相对策略优化），这些轨迹提供了丰富的中间反馈，从而加速收敛并提高鲁棒性。在我们新发布的包含5300个多领域建模任务的基准SimuBench上的实验表明，使用SimuAgent微调的Qwen2.5-7B模型比标准RL（强化学习）基线收敛更快，建模精度更高，并且在同一基准上使用few-shot prompting（少样本提示）进行评估时，甚至超越了GPT-4o。消融实验证实，两阶段课程和抽象-重构数据增强进一步提高了泛化能力。SimuAgent完全在本地使用适度的硬件进行训练和运行，为工业模型驱动工程提供了一个保护隐私且具有成本效益的解决方案。SimuAgent弥合了LLMs与图形建模环境之间的差距，为工业环境下的AI辅助工程设计提供了实用的解决方案。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
                    "arxiv_id": "2601.05107",
                    "authors": "Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng",
                    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向下的核心研究。 1.  **核心贡献判断 (第一步)**: *   论文的核心贡献是提出了 **SteeM (Steerable Memory Agent)**，这是一个用于LLM智能体的新框架。 *   它旨在解决长期人机交互中智能体如何使用记忆的问题，即如何平衡“记忆锚定”与“创新”。这属于对LLM智能体核心组件（记忆机制）的构建和改进，而非简单的应用或基础设施研究。 2.  **符合核心关注点 (第二步)**: *   论文直接对应我的研究焦点中的 **“单智能体”** 类别。 *   具体而言，它涉及智能体的关键能力指标：**`Memory` (记忆)**。论文探讨了如何量化记忆依赖度以及如何动态调节记忆的使用，这是提升智能体在长期任务中表现的关键技术。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了“个性化人机协作”，但这只是应用场景，其核心在于提出一种可控的记忆机制框架，而非单纯解决特定领域的业务问题。 综上所述，该论文致力于改进LLM智能体的记忆机制，属于构建和演化LLM智能体的核心方法论研究，因此予以保留。",
                    "summary2": "本文旨在解决长期人机交互中智能体过度依赖历史记忆导致的“Memory Anchoring”问题，实现用户对记忆依赖度的动态控制。针对Research和Tutoring场景，我们提出了一种名为SteeM的框架，通过偏好对齐的数据生成、SFT和GRPO训练，使智能体能根据用户偏好动态调整记忆使用程度。我们在合成的长期交互数据集上，通过alignment error和reward score验证了其有效性，结果显示SteeM优于传统提示方法和记忆掩码策略。",
                    "summary_translation": "随着 LLM-based agents (基于大语言模型的智能体) 在长期交互中的应用日益广泛，cumulative memory (累积记忆) 对于实现个性化以及保持风格一致性至关重要。然而，大多数现有系统在记忆使用上采取了一种 all-or-nothing approach (全有或全无的方法)：纳入所有相关的过往信息可能导致 Memory Anchoring (记忆锚定)，即智能体受困于过去的交互；而完全排除记忆则会导致 under-utilization (利用不足) 以及重要交互历史的丢失。我们表明，智能体的 reliance on memory (记忆依赖) 可以被建模为一个显式的且用户可控的维度。我们首先引入了一种 memory dependence (记忆依赖度) 的 behavioral metric (行为指标)，用于量化过往交互对当前输出的影响。随后，我们提出了 **Stee**rable **M**emory Agent (可操控记忆智能体)，即 \\texttt{SteeM}，这是一个允许用户动态调节 reliance on memory (记忆依赖) 的框架，其调节范围涵盖促进创新的 fresh-start mode (全新开始模式) 到紧密遵循交互历史的 high-fidelity mode (高保真模式)。在不同场景下的实验表明，我们的方法始终优于 conventional prompting (传统提示) 和 rigid memory masking strategies (僵化的记忆掩码策略)，为个性化的人机协作提供了更为 nuanced (细致的) 且有效的控制手段。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
                    "arxiv_id": "2601.04888",
                    "authors": "Tongyu Wen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心判断**: *   论文的核心贡献是提出了 **SmartSearch**，这是一个专门用于改进 **LLM-based search agents**（基于LLM的搜索智能体）的新框架。 *   它不是简单地将智能体作为工具应用到特定垂直领域（如医疗或金融），而是专注于解决智能体本身在执行搜索任务时的核心缺陷（即中间搜索查询质量不高），属于对智能体能力的底层构建和改进。 2.  **符合研究焦点**: *   **单智能体**: 论文明确针对搜索智能体的架构进行优化，涉及智能体的 **Tool Use**（使用搜索工具）和 **Planning**（生成查询的推理过程）。 *   **自我演化**: 论文引入了“过程奖励”和“查询优化”机制，并设计了一个三阶段的课程学习框架（模仿 -> 对齐 -> 泛化）。这种机制旨在让智能体通过反馈信号，逐步内化并自我改进其生成查询的能力。这符合“自我演化”中通过环境反馈进行自我完善和迭代的标准。 3.  **排除标准检查**: *   论文不涉及安全、对齐、可解释性或水印等排除主题。 *   论文不涉及多模态或视觉技术。 *   论文不涉及知识图谱或图神经网络。 综上所述，该论文通过提出新的方法论来提升LLM智能体的工具使用效率和自我优化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决基于LLM的搜索代理中中间搜索查询质量低下的问题。针对知识密集型任务，我们提出了一种结合过程奖励和查询优化的SmartSearch框架，并设计了三阶段课程学习策略。我们在2WikiMQA、HotpotQA等六个基准上，通过EM和F1分数验证了其有效性，显著提升了搜索效率和查询质量。",
                    "summary_translation": "基于大语言模型 (LLM) 的搜索智能体通过结合信息检索能力，在解决知识密集型问题方面展现出了巨大的潜力。现有工作主要集中在优化搜索智能体的推理范式，然而推理过程中中间搜索查询的质量却往往被忽视。结果是，生成的查询往往不准确，导致检索结果不理想，最终限制了搜索智能体的整体效能。为了缓解这一问题，我们提出了 SmartSearch，这是一个基于两个关键机制的框架：(1) 过程奖励，它通过双层信用评估为每个中间搜索查询的质量提供细粒度的监督；(2) 查询精炼，通过选择性精炼低质量搜索查询并基于这些精炼结果重新生成后续搜索轮次，从而促进查询生成的优化。为了使搜索智能体能够在过程奖励的指导下逐步内化提高查询质量的能力，我们设计了一个三阶段的课程学习框架。该框架引导智能体经历从模仿，到对齐，最终到泛化的递进过程。实验结果表明，SmartSearch 始终优于现有基线，额外的定量分析进一步证实了其在搜索效率和查询质量方面的显著提升。代码可在 https://github.com/MYVAE/SmartSearch 获取。",
                    "inspiration_trace": "基于对论文《SmartSearch: Process Reward-Guided Query Refinement for Search Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：搜索智能体的“阿喀琉斯之踵”\n**起点：** 作者首先关注到 LLM 搜索智能体在解决知识密集型任务时的巨大潜力。现有的研究范式（如 ReAct）已经证明了通过迭代调用搜索工具可以弥补模型的知识盲区。\n**发现盲区：** 尽管学术界在优化智能体的“推理范式”（如思维链设计、强化学习策略）上投入巨大，但作者敏锐地发现了一个被忽视的环节：**中间搜索查询的质量**。\n**直觉判断：** 智能体的推理能力再强，如果它向搜索引擎发出的指令是模糊、错误或冗余的，那么检索回来的信息就是无用的，后续的推理自然建立在沙堆之上。\n\n### 2. 问题聚焦：从“结果导向”到“过程诊断”\n**现象分析：** 作者通过案例分析（如文中提到的 Kevin McCarthy 演员与政治家混淆的例子）发现，一个微小的查询偏差（如漏掉“Actor”关键词）会导致检索结果完全跑偏，进而导致整个推理轨迹崩塌。\n**现有方法的局限：** 传统的强化学习训练通常只关注“结果奖励”，即最终答案是否正确。这种反馈是稀疏且滞后的。模型即使最终答对了，中间可能走了很多弯路；或者答错了，模型也不知道具体是哪一步的查询出了问题。\n**核心假设：** 如果能对每一个中间步骤的搜索动作进行细粒度的评估和反馈，就能从根本上提升智能体的信息获取能力。\n\n### 3. 机制设计一：构建“过程奖励”作为质检员\n**思考：** 如何定义一个“好的搜索查询”？作者意识到这不能仅靠单一指标。\n**逻辑拆解：**\n*   **新颖性：** 查询不应重复。如果这一步搜到的内容和上一步完全一样，那就是浪费算力。这可以通过规则（文档重叠度）低成本解决。\n*   **有用性：** 查询必须服务于最终答案。这需要语义理解。这一步的意图是否必要？检索结果是否包含预期信息？这需要模型来判断。\n**方案形成：** 结合两者，提出了“双层信用评估”。规则层负责查重（效率），模型层负责语义评估（质量），从而输出具体的分数和文本反馈。\n\n### 4. 机制设计二：引入“查询重写”作为修正员\n**思考：** 仅仅给低分查询打分是不够的，模型需要知道“什么是更好的”。\n**灵感来源：** 类似于人类写作时的修改过程。如果发现某一步查询质量低，为什么不直接修改它，然后基于修改后的查询重新跑一遍后续流程？\n**逻辑闭环：**\n1.  生成原始轨迹。\n2.  利用过程奖励找出“坏”的查询步骤。\n3.  基于反馈文本，利用模型重写该查询。\n4.  从重写点开始重新生成后续步骤，形成一条“修正后的轨迹”。\n**价值：** 这不仅生成了更好的数据，还天然构成了用于对比学习的成对数据。\n\n### 5. 训练策略：三阶段课程学习\n**思考：** 直接让模型学会上述所有能力太难，需要一个循序渐进的过程。\n**逻辑演进：**\n*   **阶段一（模仿）：** 先让模型看“好榜样”。利用过程奖励筛选出那些不仅答案正确、且中间查询也高质量的轨迹进行监督微调（SFT）。这确立了基准能力。\n*   **阶段二（对齐）：** 让模型学会“辨别好坏”。利用上一阶段的“查询重写”机制，构造“原始轨迹”和“修正轨迹”的对比对。通过 DPO（直接偏好优化），让模型偏好那些查询质量更高的路径。\n*   **阶段三（泛化）：** 让模型在实战中“探索”。在强化学习阶段，将查询重写机制融入 Rollout 过程。如果模型生成了坏查询，系统会自动修正并继续，让模型在探索中不断内化“如何写出好查询”的策略。\n\n### 6. 总结：逻辑链的终点\n作者最终构建的 SmartSearch 框架，其核心思想并非单纯堆砌算力或模型参数，而是通过**引入细粒度的过程监督**和**主动的轨迹修正机制**，解决了搜索智能体中“工具使用不当”这一核心瓶颈。从发现中间查询的重要性，到设计评估标准，再到利用修正数据驱动训练，形成了一个完整的逻辑闭环。"
                },
                {
                    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                    "arxiv_id": "2601.04861",
                    "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
                    "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（符合）**：论文的本质是构建和改进 LLM 智能体框架。作者提出了 OI-MAS（Orchestrating Intelligence），这是一个新颖的多智能体系统框架。它并非将智能体作为工具应用于特定垂直领域（如医疗或金融），而是针对多智能体系统本身的“计算效率低下”这一痛点，提出了新的架构解决方案（自适应模型选择策略和状态依赖路由机制）。这属于对智能体框架的改进和优化。 2.  **正面指标（强匹配）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并引入了机制来动态选择智能体角色和模型尺度。 *   **优化机制**：虽然涉及效率，但其核心是通过智能体层面的“路由”和“置信度感知”来实现的，这是提升多智能体系统性能的关键技术，属于构建智能体方法论的一部分。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉或图技术，也不是单纯的基础设施研究。 综上所述，该论文致力于解决多智能体协作中的效率与资源分配问题，是对 LLM 智能体架构的直接改进，符合“构建、改进或演化 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。",
                    "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。"
                },
                {
                    "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search",
                    "arxiv_id": "2601.04703",
                    "authors": "Yiqun Chen, Lingyong Yan, Zixuan Yang, Erhan Zhang, Jiashu Zhao, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao",
                    "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **M-ASK (Multi-Agent Search and Knowledge)**，这是一个新的**多智能体框架**。 *   它旨在解决现有单体智能体在 Agentic Search 任务中的结构性瓶颈，而非仅仅将现有模型应用到一个特定领域。因此，它属于构建和改进 LLM 智能体的方法论研究，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **多智能体**: 论文将智能体解耦为两个互补的角色：`Search Behavior Agents`（负责规划和执行搜索动作）和 `Knowledge Management Agents`（负责聚合、过滤和维护内部上下文）。这体现了智能体间的协作与分工。 *   **智能体能力**: 涉及 `Planning`（规划搜索动作）、`Tool Use`（工具使用）以及 `Memory`（通过知识管理智能体维护内部上下文）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文在 Multi-hop QA（多跳问答）基准测试上进行了实验，但这只是为了验证框架的有效性，其核心在于提出了一种新的多智能体协作架构来优化 Agentic Search，而非单纯的应用型研究。 综上所述，该论文通过构建多智能体系统来改进 LLM 智能体的搜索与推理能力，精准契合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在解决单体 Agentic Search 架构因无约束输出、稀疏奖励和搜索噪声导致的训练不稳定及信用分配难题。针对复杂的多跳问答场景，我们提出了一种 M-ASK 多智能体框架，通过解耦 Search Behavior Agents 和 Knowledge Management Agents，并利用 turn-level dense rewards 实现联合优化。在 HotpotQA、2Wiki、Musique 等多跳问答基准上，通过 F1 Score 验证了其有效性和稳定性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题定位\n**——从“单体智能”的繁荣中看到结构性隐患**\n\n1.  **观察现象**：\n    *   随着 Agentic Search（智能体搜索）的兴起，LLM 被赋予了使用工具（如搜索引擎）进行迭代推理的能力。\n    *   现有的 SOTA 方法（如 Search-r1）大多采用**单体架构**，即由一个单一的 LLM 承担所有任务：规划、搜索、信息整合和最终回答。\n\n2.  **发现问题**：\n    *   作者发现这种“全能型”单体智能体在训练中表现出极不稳定的特征，容易崩溃。\n    *   虽然端到端的强化学习（RL）能提升性能，但在复杂的多跳问答任务中，单体模型往往难以收敛。\n\n### 第二阶段：深度诊断与归因分析\n**——解构“长视界信用分配”的死结**\n\n作者并未止步于“不稳定”的现象，而是深入剖析了单体架构背后的三个互为因果的致命缺陷，构成了一个**“毒性三角”**：\n\n1.  **无约束的输出长度**：\n    *   单体模型倾向于生成冗长的推理链。这不仅增加了计算成本，更重要的是拉长了决策轨迹，使得“最终结果”与“早期决策”之间的距离过远。\n2.  **稀疏的奖励信号**：\n    *   通常只有在任务结束时（回答正确与否）才有反馈。在长达数十步的推理中，模型无法知道哪一步是对的，哪一步是错的。\n3.  **搜索噪声**：\n    *   外部工具（搜索引擎）会引入无关或错误的信息。在单体架构中，这些噪声会直接累积在上下文中，干扰后续推理。\n\n**核心洞察**：\n上述三者共同导致了**长视界信用分配难题**。当一条冗长、充满噪声的轨迹最终只得到一个简单的对错反馈时，优化算法根本无法将奖励归因到具体的某个 Token 或动作上，导致训练梯度发散，模型崩溃。\n\n### 第三阶段：战略假设与范式转移\n**——从“分身乏术”到“术业专攻”**\n\n为了打破上述死结，作者提出了一个核心假设：**如果将“搜索行为”与“知识管理”解耦，就能从根本上隔离噪声并压缩轨迹。**\n\n1.  **角色解耦**：\n    *   不再让一个大脑既负责“找信息”又负责“记信息”。\n    *   **搜索行为代理**：只负责决策（搜什么、何时停）。\n    *   **知识管理代理**：只负责记忆（过滤噪声、更新状态）。\n2.  **预期效果**：\n    *   通过 KMA 的过滤，进入上下文的信息是高密度的，解决了“噪声”问题。\n    *   通过 SBA 的专注，每次生成的动作更短更精准，解决了“输出长度”问题。\n\n### 第四阶段：机制设计与优化逻辑\n**——构建“协作-反馈”闭环**\n\n有了架构假设，作者进一步思考：如何让这两个独立的团队协同工作并稳定训练？\n\n1.  **通信机制设计**：\n    *   设计了一个**结构化知识状态**。这不仅是共享内存，更是两个团队交互的唯一接口。SBA 写入查询，KMA 更新状态，双方通过这个紧凑的状态进行异步协作。\n\n2.  **解决信用分配难题（关键创新）**：\n    *   既然全局奖励太稀疏，那就引入**Turn-level Dense Rewards（轮级密集奖励）**。\n    *   **状态奖励**：给负责输出的 Agent（如 Answer Agent）基于当前答案质量的绝对分数。\n    *   **边际奖励**：这是最精妙的一笔。给负责迭代的 Agent（Search, Summary, Update）分配“边际增益”（$F1_{current} - F1_{previous}$）。\n    *   **逻辑**：如果这一轮搜索和更新让答案变好了，大家都有奖；如果没变好或变差了，大家都要负责。这迫使搜索团队必须找对信息，知识团队必须滤对噪声。\n\n### 第五阶段：验证与理论闭环\n**——从“假设”到“定律”**\n\n最后，作者通过实验验证了这一逻辑链条的完整性：\n*   **消融实验**：移除知识管理模块（KMA）导致性能下降，证明了“过滤噪声”的必要性；移除轮级奖励导致多跳任务崩溃，证明了“密集反馈”在长链路中的关键作用。\n*   **稳定性分析**：对比单体架构 Search-r1 的高崩溃率，M-ASK 实现了 0% 崩溃，证实了“解耦”确实解决了长视界训练的不稳定性。\n\n---\n\n**总结：作者的思考路径**\n从**单体架构的不稳定性**出发 $\\rightarrow$ 诊断出**长视界信用分配**是核心病灶 $\\rightarrow$ 提出通过**角色解耦**来切断噪声与长度的累积 $\\rightarrow$ 利用**边际奖励机制**将全局反馈转化为局部指导 $\\rightarrow$ 最终构建出一个既分工明确又利益绑定的多智能体协作系统。"
                },
                {
                    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
                    "arxiv_id": "2601.04620",
                    "authors": "Di Zhang",
                    "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 **AgentDevel**，这是一个将 LLM 智能体的自我改进过程重构为“发布工程”的新框架。它通过迭代运行智能体、生成质量信号、诊断故障并合成新版本，实现了智能体的自我演化。这完全符合“构建、改进或演化 LLM 智能体”的目标，不属于非演化型应用、非Agentic的基础推理或基础设施研究。 2.  **研究焦点匹配**: 论文直接对应研究焦点的第三点 **“自我演化”**。它提出了一种新的演化机制，即通过外部化的回归感知发布管道来迭代改进智能体，这与传统的内部自我反思或基于种群的搜索不同，强调了演化的稳定性和可审计性。 3.  **正面指标**: 论文明确涉及 `Self-Evolving`、`Self-Improvement`、`Iterative Improvement`、`Agentic AI` 以及 `LLM-based Agents` 等核心范式和能力。 4.  **排除标准**: 论文不涉及安全对齐、多模态技术或特定领域的垂直应用（如医疗、金融），也不属于基础设施优化。 5.  **结论**: 该论文为 LLM 智能体的演化提供了新的方法论视角，属于前沿研究，应予以保留。",
                    "summary2": "本文旨在解决自进化 LLM Agent 改进过程不稳定且难以审计的问题。针对执行密集型任务，我们提出了一种名为 AgentDevel 的发布工程管道，该管道包含实现无关 Critic、可执行诊断及翻转中心 Gating 机制。我们在 SWE-bench、WebArena 和 StableToolBench 上通过 Resolved rate、Success rate 及回归率等指标验证了其有效性，实现了性能显著提升且回归更少的稳定改进。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
                    "arxiv_id": "2601.04544",
                    "authors": "Jiuzhou Zhao, Chunrong Chen, Chenqi Qiao, Lebin Zheng, Minqi Han, Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang",
                    "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **TCAndon-Router (TCAR)**，这是一个用于多智能体协作的自适应推理路由器。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统（MAS）中的关键组件——路由器——进行了构建和改进。 *   论文设计了一个“协作执行流水线”，其中包括一个专门的“Refining Agent”来聚合和优化响应，这属于构建新的多智能体交互机制。 2.  **正面指标（第二步）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体特性**：涉及 `Collaboration`（协作执行流水线）、`Communication`（通过路由器进行任务分配）以及 `Agent Society`（支持动态智能体接入）。 *   **智能体能力**：利用 `Reasoning`（生成自然语言推理链）来辅助路由决策。 3.  **排除标准（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **综合结论**： 该论文致力于解决多智能体系统中的任务分配与协作效率问题，提出了新的路由机制和协作框架，直接对应研究课题中的“多智能体”方向，因此应当保留。",
                    "summary2": "本文旨在解决多智能体系统中现有路由策略难以动态接入新智能体及处理能力重叠导致的路由冲突问题。针对企业级应用中模糊或跨域的查询场景，我们提出了一种名为TCAndon-Router (TCAR) 的自适应推理路由器，通过生成自然语言推理链选择候选智能体，并利用Refining Agent聚合多智能体响应。在CLINC150、HWU64等公共数据集及腾讯云私有数据集上，通过Accuracy和F1等指标验证了其有效性。",
                    "summary_translation": "多智能体系统已成为构建高性能智能应用的有力范式。在这些系统中，负责确定哪些专家智能体应处理给定查询的路由器在整体性能中起着至关重要的作用。现有的路由策略通常分为两类：性能路由，它在不同规模的模型之间平衡延迟和成本；以及任务路由，它将查询分配给特定领域的专家以提高准确性。在现实世界的应用程序中，任务路由更为适用；然而，大多数现有方法依赖于静态单标签决策，这引入了两个主要局限性：随着业务领域的扩展，难以无缝集成新的智能体；以及由智能体能力重叠引起的路由冲突，最终降低了准确性和鲁棒性。\n\n为了解决这些挑战，我们提出了 TCAndon-Router (TCAR)：一种用于多智能体协作的自适应推理路由器。与传统路由器不同，TCAR 支持动态智能体接入，并在预测能够处理查询的候选智能体集合之前，首先生成一条自然语言推理链。此外，我们设计了一个协作执行管道，其中选定的智能体独立生成响应，随后由专门的精炼智能体将这些响应聚合并精炼为单一的高质量响应。\n\n在公共数据集和真实企业数据上的实验表明，TCAR 显著提高了路由准确性，减少了路由冲突，并在模糊场景中保持了鲁棒性。我们已在 https://huggingface.co/tencent/TCAndon-Router 发布了 TCAR，以支持未来关于可解释和协作多智能体路由的研究。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery",
                    "arxiv_id": "2601.04500",
                    "authors": "Yifei Gao, Jiang Wu, Xiaoyi Chen, Yifan Yang, Zhe Cui, Tianyi Ma, Jiaming Zhang, Jitao Sang",
                    "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **GUITester**，这是一个**多智能体框架**。它不仅仅是将现有的LLM智能体作为工具应用，而是构建了一个新的架构来解决特定问题。 *   该框架包含两个关键模块：**规划-执行模块 (PEM)** 和 **层次化反思模块 (HRM)**。这直接对应了研究焦点中的 **\"Agentic\" (规划)** 和 **\"自我演化\" (自我反思 Self-Reflection / 自我修正 Self-Correction)** 方向。 2.  **涉及多智能体与自我反思机制 (第二步 & 第四步)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   论文提出的 **HRM (Hierarchical Reflection Module)** 通过分析交互历史来解决归因歧义，这是一种典型的 **自我反思** 和 **自我修正** 机制。虽然它应用于GUI测试，但其核心在于改进智能体处理反馈和自我完善的能力，符合“自我演化”中通过反思进行迭代的定义。 3.  **特殊情况的正确处理 (第三步 & 第四步)**: *   **应用 vs. 方法论**: 虽然论文的应用场景是GUI测试（特定领域），但其核心在于提出了解决“目标导向掩蔽”和“执行偏差归因”的**新方法论**（即解耦导航与验证的框架），而非单纯的应用。 *   **多模态**: 论文虽然使用了多模态大模型 (MLLM) 来感知GUI界面，但视觉仅作为智能体感知环境的工具，研究的核心并非视觉模型的改进，而是智能体的决策与反思框架，因此符合“除非它们被用作智能体感知环境的工具”的例外条款。 综上所述，该论文在构建多智能体框架、引入规划机制以及实现自我反思方面做出了实质性贡献，完全符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有GUI智能体在探索性测试中难以自主发现缺陷的问题。针对目标导向掩蔽和执行偏差归因两大挑战，我们提出了一种名为GUITester的多智能体框架，通过规划执行模块（PEM）主动探测缺陷，并利用分层反思模块（HRM）解决归因歧义。我们在首个交互式benchmark GUITestBench上通过F1-score（Pass@3）验证了其有效性，结果显示GUITester达到48.90%，显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements",
                    "arxiv_id": "2601.04235",
                    "authors": "Hong Su",
                    "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种“主动获取反馈”的模型。这不仅仅是应用现有智能体解决具体问题，而是构建了一种新的智能体交互与评估机制，使智能体能够在没有预设测量的情况下，通过主动与环境交互来获取反馈。这属于构建和改进LLM智能体方法论的研究。 2.  **高度匹配核心关注点 (第二步)**: *   **单智能体**: 论文详细探讨了智能体如何进行“自主行动评估”、“与环境交互”以及“自主规划和调整行动”，这直接对应了Agentic AI中的规划、工具使用和环境交互能力。 *   **自我演化**: 论文引入了“自触发机制”，允许智能体根据内部目标（如准确性、效率）自主调整行动。这种基于反馈的自我调整和迭代优化，正是“自我演化”和“自我修正”的关键体现。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉核心或图技术等排除项。 *   它不是非演化型应用，而是提出了一种通用的智能体能力增强机制。 综上所述，该论文通过解决智能体如何在开放环境中自主获取反馈并自我完善的问题，为Agentic AI的演化和能力提升提供了新的方法论，因此应当保留。",
                    "summary2": "本文旨在解决AI智能体在开放环境中依赖预定义测量评估行动的问题。针对无预设测量的动态环境，我们提出了一种Actively Feedback Getting model，利用action-induced environmental differences进行反馈检测与主动干预。在文本场景和模拟环境上，通过语义相似度和LLM查询数量验证了其有效性。",
                    "summary_translation": "从环境中获取可靠反馈是 Intelligent agents (智能体) 评估其行为正确性并积累可复用知识的基本能力。然而，大多数现有方法依赖于 predefined measurements (预定义度量) 或 fixed reward signals (固定奖励信号)，这限制了它们在开放式和动态环境中的适用性，因为在这些环境中，新的行为可能需要以前未知的反馈形式。为了解决这些局限性，本文提出了一种 Actively Feedback Getting model (主动反馈获取模型)，其中 AI agent 主动与环境交互以发现、筛选和验证反馈，而无需依赖 predefined measurements (预定义度量)。该方法不假设显式的反馈定义，而是利用 action-induced environmental differences (动作引起的环境差异) 来识别未预先指定的目标反馈，这是基于动作不可避免地在环境中产生可测量变化这一观察。此外，本文引入了一种由提高准确度、精确度和效率等 internal objectives (内部目标) 驱动的 self-triggering mechanism (自触发机制)，以自主规划和调整行为，从而在没有 external commands (外部指令) 的情况下实现更快、更聚焦的反馈获取。实验结果表明，所提出的主动方法显著提高了 factor identification (因子识别) 的效率和鲁棒性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation",
                    "arxiv_id": "2601.04516",
                    "authors": "Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu",
                    "summary": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Multi-Agent (多智能体)** 方向的前沿研究。具体判断依据如下： 1.  **核心判断 (符合)**: *   论文的核心贡献是提出了一种名为 **LinguaGame** 的新范式，这是一种基于博弈论的多智能体对话生成框架。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统的 **交互过程** 本身进行了改进，旨在通过建模意图和策略来提升智能体间的通信效率。这符合“构建、改进 LLM智能体”的核心目标。 2.  **研究焦点匹配 (多智能体)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   它的核心关注点是多智能体之间的 **Communication (通信)** 和 **Interaction (交互)**。 *   论文涉及智能体如何推断彼此的意图和策略，这属于多智能体协作与博弈中的高级认知能力。 3.  **排除标准检查 (通过)**: *   **非单纯应用**: 虽然论文在模拟法庭和辩论中进行了评估，但其核心贡献是通用的交互机制，而非解决特定领域的业务问题。 *   **非安全/对齐/多模态**: 论文不涉及安全对齐、视觉或多模态内容，纯粹关注语言层面的智能体交互逻辑。 4.  **正面指标**: *   包含核心范式关键词：`Multi-Agent Systems (MAS)`, `Game-Theoretic`。 *   包含多智能体能力关键词：`Communication`, `Negotiation` (隐含在辩论场景中), `Agent Society` (隐含在多智能体环境中)。 综上所述，该论文提出了一种改进多智能体通信机制的新方法，属于 Agentic AI 中多智能体交互的重要进展，因此予以保留。",
                    "summary2": "本文旨在解决基于LLM的Multi-Agent Systems中沟通效率低下的问题。针对模拟法庭和辩论场景，我们提出了一种名为LinguaGame的基于语言学的博弈论范式。该方法将对话建模为关于交际意图和策略的Signalling Game，并利用无训练的均衡近似算法在推理时优化决策。在模拟法庭和辩论数据集上，通过人工专家评估（涵盖清晰度、简洁性、论证和策略等指标），验证了该方法能显著提升对话质量和沟通效率。",
                    "summary_translation": "大语言模型推动了多智能体系统的发展，在该系统中，智能体通过自然语言进行交互，以解决复杂任务或模拟多方对话。现有关于基于大语言模型的多智能体系统的研究主要集中在架构设计方面，例如角色分配和工作流编排。与之不同，本文聚焦于交互过程本身，旨在通过帮助智能体利用语言更有效地传达其预期含义，从而提升通信效率。为此，我们提出了 LinguaGame，这是一种基于语言学的博弈论范式，用于多智能体对话生成。我们的方法将对话建模为基于交际意图和策略的信号博弈，并采用一种无训练的均衡近似算法进行求解，以实现推理时的决策调整。与先前的博弈论多智能体系统不同，后者的博弈设计往往与特定任务目标紧密耦合，而我们的框架依赖于基于语言学的推理，仅与特定任务存在最小程度的耦合。具体而言，该框架将对话视为一种意图性和策略性的通信，要求智能体推断他人旨在实现的目标（意图）以及其追求这些目标的方式（策略）。我们在模拟法庭庭审和辩论场景中对该框架进行了评估，人类专家的评估结果显示通信效率得到了显著提升。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-07": [
        {
            "name": "Artificial Intelligence",
            "count": 6,
            "papers": [
                {
                    "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
                    "arxiv_id": "2601.04035",
                    "authors": "Yilin Cao, Yufeng Zhong, Zhixiong Zeng, Liming Zheng, Jing Huang, Haibo Qiu, Peng Shi, Wenji Mao, Wan Guanglu",
                    "summary": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献在于提出了 **MobileDreamer**，这是一个基于世界模型的前瞻框架，旨在装备和增强移动 GUI 智能体。它不仅仅是将现有的智能体框架应用到一个新领域，而是针对现有 GUI 智能体“反应式”的局限性，提出了一种新的架构（包含文本草图世界模型和推演想象策略），以提升智能体的决策能力。这属于构建和改进 LLM 智能体的范畴。 2.  **正面指标（符合）**： *   **Agentic AI**: 论文明确聚焦于 GUI Agent 的构建。 *   **Planning**: 论文的核心在于通过世界模型预测行动结果，从而支持智能体进行更好的决策制定和行动选择，这属于智能体规划能力的增强。 3.  **排除标准（未触发）**： *   **多模态与视觉**: 虽然论文涉及处理屏幕图像和生成草图，但这属于“智能体感知环境的工具”。论文的核心贡献不是提出一种新的视觉算法或图像生成模型，而是利用视觉转换来构建辅助智能体规划的“世界模型”。因此，符合“除非它们被用作智能体感知环境的工具”这一例外条款。 4.  **特殊与模糊情况（符合）**： *   **推理/规划**: 论文通过引入世界模型，让智能体能够进行“前瞻”和“想象”，这属于智能体在复杂任务中进行多步推理和规划的高级形式，符合保留条件。 综上所述，该论文通过引入世界模型机制显著增强了单智能体的规划与决策能力，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决移动GUI代理在长视界任务中缺乏前瞻性规划的问题。针对移动设备屏幕交互场景，我们提出了一种基于文本草图世界模型的高效前瞻框架MobileDreamer。该方法通过文本草图世界模型预测未来状态，并利用推演想象策略构建预测树以优化动作选择。在Android World基准测试上，通过任务成功率（SR）验证了其有效性，显著提升了代理的性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
                    "arxiv_id": "2601.03555",
                    "authors": "Yuxuan Jiang, Francis Ferraro",
                    "summary": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance. Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions. Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (第一步)**: *   该论文的核心贡献是提出了 **SCRIBE**，这是一个用于训练**工具增强型智能体** 的强化学习框架。 *   论文明确指出其目标是解决训练可靠智能体时的“信用分配”难题，并致力于提升智能体的“高层规划”和“低层执行”能力。 *   这完全符合“构建、改进或演化 LLM智能体”的核心目标，属于 Agentic AI 的范畴，因此应予以保留。 2.  **正面指标匹配 (第二步)**: *   **智能体能力**: 论文重点涉及 `Tool Use` (工具使用) 和 `Planning` (规划)，特别是区分了高层规划与低层执行。 *   **演化机制**: 摘要中明确提到了“co-evolution across abstraction levels”（跨抽象层的协同演化）以及“emergence of effective high-level planning behaviors”（有效高层规划行为的涌现），这与研究焦点中的“自我演化”高度契合。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文虽然涉及推理，但它是通过强化学习框架来训练智能体在多步任务中进行规划和工具使用，属于智能体的架构与训练方法，而非单纯提升LLM基础Token预测能力的数学或逻辑微调，因此符合保留条件。 **结论**: 该论文提出了一种新的训练框架来改进LLM智能体的工具使用和规划能力，并探讨了技能与规划之间的协同演化机制，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决工具增强模型在多步推理中因信用分配困难导致的训练不可靠问题。针对多步推理和复杂工具交互场景，我们提出了一种SCRIBE框架，利用Skill Prototypes库进行中间层抽象，将开放式LLM评估转化为基于原型的约束验证任务。并在MATH、AIME25和BFCL V4基准上通过准确率和成功率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
                    "arxiv_id": "2601.04060",
                    "authors": "Jinwei Su, Qizhen Lan, Zeyu Wang, Yinghui Xia, Hairu Wen, Yiqun Duan, Xi Xiao, Tianyu Shi, Yang Jingsong, Lewei He",
                    "summary": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，核心依据如下： 1.  **核心贡献符合 Agentic AI 定义 (第一步 & 第二步)**: *   论文明确提出了 \"ComfySearch, an **agentic framework**\"（一个智能体框架）。这不仅仅是将现有的LLM作为工具简单应用，而是构建了一个新的智能体架构来解决特定问题。 *   该框架的核心能力涉及 **Autonomous Exploration**（自主探索）和 **Reasoning**（推理），这直接对应了筛选标准中的“单智能体”方向，特别是 **Planning**（规划）和 **Tool Use**（工具使用，即操作ComfyUI组件）。 2.  **属于智能体规划与构建范畴 (第四步)**: *   论文解决的问题是如何在复杂的图约束下构建长视距的工作流。这属于智能体如何在复杂任务中进行多步规划和决策的研究范畴，而非单纯的LLM基础推理能力提升（如数学或逻辑题）。它关注的是智能体如何通过 \"validation-guided workflow construction\"（验证引导的工作流构建）来完成任务，这是一种典型的 Agentic 行为模式。 3.  **非简单的应用型论文 (第一步 & 第四步)**: *   虽然论文的应用场景是 ComfyUI（一个视觉生成平台），但论文的核心贡献在于**提出了一种能够自主探索组件空间并生成功能性工作流的智能体机制**，而不是仅仅展示LLM在生成图片上的效果。根据第四步的规则，只要核心是提出新的智能体机制（在此处是探索和构建机制），即使应用在特定领域，也应保留。 4.  **排除标准检查 (第三步)**: *   虽然涉及视觉内容生成，但论文的研究焦点不在于改进视觉模型本身，而在于控制生成流程的智能体框架，因此不属于被排除的“多模态与视觉”核心研究。 *   不涉及安全、对齐或基础设施问题。 综上所述，该论文提出了一种新的智能体框架来解决复杂的规划和构建问题，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决ComfyUI工作流生成中因组件复杂和图约束严格导致的低通过率和质量问题。针对ComfyUI工作流构建场景，我们提出了一种名为ComfySearch的智能体框架，采用Reasoning-as-action范式，结合State-aware validation和In-place repair确保结构正确性，并引入Entropy-adaptive branching处理长视距不确定性。我们在ComfyBench和GenEval上通过%Pass、%Resolve及GenEval分数验证了其有效性。",
                    "summary_translation": "AI生成内容已从单体模型演进至模块化工作流，特别是在 ComfyUI 等平台上，使用户能够定制复杂的创意流水线。然而，ComfyUI 中组件数量庞大，且在严格的图约束下难以保持长视距结构一致性，这往往导致通过率低下且生成的工作流质量有限。为解决上述局限性，我们提出了 ComfySearch，这是一个智能体框架，能够通过验证引导的工作流构建，有效探索组件空间并生成功能完备的 ComfyUI 流水线。实验表明，在复杂和创意任务中，ComfySearch 的性能显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决率以及更强的泛化能力。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Architecting Agentic Communities using Design Patterns",
                    "arxiv_id": "2601.03624",
                    "authors": "Zoran Milosevic, Fethi Rabhi",
                    "summary": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献在于提出了一种用于构建“Agentic Communities”（智能体社区）的架构方法和设计模式。这不仅仅是将LLM作为工具应用，而是构建了一个包含LLM智能体、Agentic AI实体和人类的复杂多智能体系统框架。它定义了智能体社区的组织结构、角色和协议，属于构建和改进LLM智能体系统的方法论。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确涉及 `Agentic AI` 和 `Multi-Agent Systems (MAS)`，特别是提出了“Agentic Communities”这一概念。 *   **多智能体能力**：摘要中详细讨论了智能体之间以及智能体与人类之间的 `Coordination`（协调）、`Communication`（通信）、`Negotiation`（谈判）以及 `Governance`（治理）机制。这些都是多智能体系统研究中的核心议题。 3.  **排除标准检查（第三步）**： *   虽然论文提到了“法律和伦理规则”以及“问责机制”，但其主要贡献是**架构框架**，而非单纯的安全算法或对齐技术研究。 *   虽然使用了“临床试验匹配”作为案例研究，但这仅用于验证框架的有效性，论文本质并非解决医疗领域的特定问题，而是提供通用的系统架构指导。 综上所述，该论文为构建复杂的多智能体协作系统提供了新的架构视角和设计模式，直接契合研究课题中关于“多智能体”的焦点，因此予以保留。",
                    "summary2": "本文旨在为构建生产级Agentic AI系统提供系统的架构指导。针对多智能体协调与治理的挑战，我们提出了一种基于ODP Enterprise Language (ODP-EL) 形式主义的设计模式框架，涵盖LLM Agents、Agentic AI及Agentic Communities三类模式。我们在临床试验匹配案例中验证了其有效性，实现了可验证的治理属性与问责机制。",
                    "summary_translation": "大语言模型 (Large Language Models, LLM) 及随后的 Agentic AI（智能体 AI）技术的快速演进，为构建复杂的、生产级系统提出了系统化架构指导的需求。本文提出了一种架构此类系统的方法，该方法采用了源于企业分布式系统标准、形式化方法和行业实践的设计模式。我们将这些模式划分为三个层级：LLM Agents（大语言模型智能体，即特定任务的自动化）、Agentic AI（智能体 AI，即自适应的目标寻求者）以及 Agentic Communities（智能体社区，即 AI 智能体和人类参与者通过正式角色、协议和治理结构进行协调的组织框架）。我们重点关注 Agentic Communities——这是一种包含 LLM Agents、Agentic AI 实体和人类的协调框架——其与企业及工业应用最为相关。借鉴分布式系统中既定的协调原则，我们将这些模式建立在一个形式框架之上，该框架明确了协作协议，规定了 AI 智能体和人类如何在受治理的生态系统中承担特定角色。该方法不仅提供了实用指导，还具备形式化验证能力，能够通过问责机制来表达组织、法律和伦理规则，从而确保对智能体间通信、谈判和意图建模进行可操作且可验证的治理。我们通过一项临床试验匹配的案例研究验证了该框架。我们的目标是为从业者提供切实可行的指导，同时保持企业部署在动态多智能体生态系统中所必需的形式严谨性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization",
                    "arxiv_id": "2601.03359",
                    "authors": "Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner",
                    "summary": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种“新颖的多智能体工作流”，用于优化提示词指令。这属于构建和改进 LLM 智能体系统的方法论研究，而非将现有智能体简单应用到特定垂直领域（如医疗、金融）。 2.  **正面指标匹配**： *   **多智能体**：标题和摘要中明确提到了“Multi-Agentic Workflow”，涉及多个智能体协同工作。 *   **自我演化/自我完善**：论文描述了使用“定量分数作为反馈来迭代地重写和改进”提示词，这符合自我演化中的“迭代改进”和“自我修正”机制。 *   **智能体能力**：该工作流体现了智能体的规划（解耦任务描述与约束）和工具使用（利用评估分数）能力。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然目标是“指令遵循”，但其手段是构建一个多智能体系统，这属于 Agentic AI 的范畴，而非单纯的非 Agentic 推理或数据集构建。 综上所述，该论文通过构建多智能体协作框架来实现任务的自我迭代优化，精准契合我对“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决 LLMs 难以严格遵守输出约束的问题。针对 InfoBench 数据集，我们提出了一种 evaluation-driven multi-agentic workflow，将任务描述与约束解耦，并利用定量反馈迭代优化约束。我们在 Llama 3.1 8B 和 Mixtral-8x 7B 上通过 compliance scores 验证了其有效性，显著提升了模型的指令遵循能力。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 虽然常能生成实质相关的内容，但往往未能遵守形式约束 (formal constraints)，从而导致输出结果在概念上正确，但在程序上存在缺陷。传统的提示词优化 (prompt refinement) 方法主要侧重于改写大语言模型需执行的主要任务描述，而忽视了那些作为响应验收标准 (acceptance criteria) 的细粒度约束 (granular constraints)。我们提出了一种新颖的多智能体工作流 (multi-agentic workflow)，该工作流将主要任务描述的优化与其约束解耦，并利用定量分数 (quantitative scores) 作为反馈，对二者进行迭代重写和改进。我们的评估表明，该方法生成的修订提示词能够使 Llama 3.1 8B 和 Mixtral-8x 7B 等模型产生显著更高的合规分数 (compliance scores)。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Evolving Programmatic Skill Networks",
                    "arxiv_id": "2601.03509",
                    "authors": "Haochen Shi, Xingdi Yuan, Bang Liu",
                    "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合“自我演化”**: 论文提出了 Programmatic Skill Network (PSN) 框架，其核心在于智能体如何通过经验构建、细化和重用技能库。摘要中明确指出技能库是“evolves through experience”（通过经验演化），并包含“progressive optimization”（渐进式优化）和“structural refactoring”（结构重构）等机制，这直接对应了筛选标准中的“Self-Evolving”、“Self-Improvement”和“Iterative Improvement”。 2.  **具备核心智能体能力**: 论文详细描述了智能体的自我反思和自我修正能力。具体而言，它利用 LLM 实例化了“REFLECT”机制进行结构化故障定位，以及“成熟度感知更新门控”来平衡技能的稳定性与可塑性。这符合筛选标准中的“Self-Correction”、“Self-Reflection”和“Memory”（技能库作为长期记忆）。 3.  **符合特殊处理规则**: 尽管论文是在 MineDojo 和 Crafter（具身/游戏环境）中进行实验，但根据第四步的规则，只要论文的核心贡献是提出一种新的“自我演化”机制（即 PSN 框架），即使应用在特定领域，也应予以保留。该论文并非单纯将 LLM 应用于游戏，而是提出了一套通用的技能演化算法。 综上所述，该论文不仅涉及 LLM 智能体的构建，更深入探讨了智能体如何通过反思和反馈实现自我演化和技能积累，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决开放具身环境中智能体持续获取、细化和重用技能的问题。针对开放任务流，我们提出了一种Programmatic Skill Network (PSN)框架，通过REFLECT机制进行结构化故障定位、成熟度感知更新门控及规范化结构重构，实现技能网络的持续演化。并在MineDojo和Crafter环境上通过技术树掌握迭代次数、累积奖励及技能保持率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
                    "arxiv_id": "2601.03785",
                    "authors": "Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang",
                    "summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为 \"Membox\" 的分层记忆架构，旨在解决 LLM 智能体在长程记忆中保持主题连续性的问题。这属于对 LLM 智能体核心组件（记忆系统）的构建与改进，而非将智能体作为工具应用到特定领域（非演化型应用），也非基础设施或基础模型推理能力的提升。 2.  **正面指标匹配（第二步）：** *   **核心范式：** 论文明确聚焦于 `LLM-based Agents`。 *   **智能体能力：** 论文的核心贡献点在于 `Memory`（记忆）。它通过引入 \"Topic Loom\" 和 \"Trace Weaver\" 等机制，优化了智能体存储和检索对话历史的能力，这直接对应了筛选标准中单智能体方向下的“记忆”子方向。 3.  **排除标准检查（第三步）：** 论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此未触犯任何排除规则。 4.  **总结：** 该论文致力于通过改进记忆机制来增强 LLM 智能体的连贯性和效率，属于单智能体研究中对基础能力（记忆）的深化，完全符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有LLM Agent记忆系统因碎片化存储导致的话题连续性丢失问题。针对长对话场景，我们提出了一种名为Membox的分层记忆架构，利用Topic Loom将连续同话题对话分组为“记忆盒”，并通过Trace Weaver链接宏观话题以恢复事件时间线。我们在LoCoMo数据集上通过F1和BLEU指标验证了其有效性，在Temporal Reasoning任务上实现了高达68%的F1提升，且显著降低了上下文Token消耗。",
                    "summary_translation": "人机对话通常表现出话题连续性——一种通过时间上相邻的交互演变的稳定主题框架——然而大多数大语言模型 (LLM) 智能体记忆系统未能保留这一特性。现有设计遵循一种碎片化-补偿范式：它们首先将对话流分解为孤立的话语进行存储，然后试图通过基于嵌入的检索来重建连贯性。这一过程不可逆地破坏了叙事和因果流，同时导致检索偏向于词汇相似性。我们介绍了 membox，这是一种以 Topic Loom (话题编织器) 为中心的分层记忆架构，它以滑动窗口的方式持续监控对话，在存储时将连续的同一话题轮次分组为连贯的“记忆盒”。封装后的记忆盒随后由 Trace Weaver (轨迹编织器) 链接成长程事件时间轴轨迹，从而跨越对话间隔恢复宏观话题的重现。在 LoCoMo 数据集上的实验表明，Membox 在时序推理任务上实现了高达 68% 的 F1 值提升，优于竞争性基线模型（如 Mem0, A-MEM）。值得注意的是，Membox 在仅使用现有方法所需的一小部分上下文 token 的情况下取得了这些提升，突显了其在效率与有效性之间取得了卓越的平衡。通过显式建模话题连续性，Membox 为增强 LLM 智能体的连贯性和效率提供了一种受认知启发的机制。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
                    "arxiv_id": "2601.03641",
                    "authors": "Zheng Wu, Xingyu Lou, Xinbei Ma, Yansi Li, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang",
                    "summary": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心论文。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 **Agent-Dice**，这是一个专门用于解决 **LLM智能体持续学习** 问题的参数融合框架。它旨在解决智能体在动态环境中学习新任务时遇到的“灾难性遗忘”和“稳定性-可塑性困境”。这直接对应了我研究目标中的“自我演化”，即智能体通过经验、反思或环境反馈进行自我完善和迭代。这不是将智能体作为工具的应用，而是对智能体底层学习机制的改进。 2.  **符合正面指标 (第二步)**: *   **核心范式**: 论文明确关注 `LLM-based Agents`。 *   **演化机制**: 论文的核心主题是 `Continual Learning`（持续学习），这是 `Self-Evolving`（自我演化）的重要组成部分。它涉及 `Knowledge Updates`（知识更新）和 `Iterative Improvement`（迭代改进）。 *   **智能体能力**: 论文在 `GUI agents` 和 `tool-use agents` 领域进行了验证，体现了智能体与环境的交互能力。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐或水印问题。 *   虽然提到了 GUI agents（可能涉及视觉），但视觉仅作为智能体感知环境的工具，而非研究的核心（核心是参数融合和持续学习算法），因此不违反多模态排除规则。 *   不涉及图技术。 4.  **特殊情况处理 (第四步)**: 论文提出了一种新的“自我演化”机制（通过几何共识和曲率加权实现持续学习），即使它在特定的 GUI 或工具使用领域进行了实验，根据规则“如果论文的核心是提出一种新的‘自我演化’机制……也应该保留”，这篇论文应当被保留。 综上所述，Agent-Dice 通过改进智能体的知识更新机制，实现了智能体在连续任务中的自我演化，精准契合“LLM智能体及其演化”这一课题。",
                    "summary2": "本文旨在解决LLM智能体持续学习中的稳定性-可塑性困境。针对GUI和工具使用智能体场景，我们提出了一种基于方向一致性评估的参数融合框架Agent-Dice，通过几何一致性过滤和基于曲率的重要性加权解耦知识更新。在AITZ、AndroidControl、GUI-Odyssey和ToolACE数据集上，通过平均Z-score等指标验证了其有效性。",
                    "summary_translation": "基于 Large Language Model (LLM) 的 agents（智能体）通过与动态环境交互，显著扩展了 LLMs 的效用。然而，使 agents 能够在不发生 catastrophic forgetting（灾难性遗忘）的情况下持续学习新任务，仍然是一个关键挑战，即所谓的 stability-plasticity dilemma（稳定性-可塑性困境）。在这项工作中，我们认为这一困境根本源于未能明确区分跨任务共享的 common knowledge（通用知识）与由 task-specific interference（任务特定干扰）引入的 conflicting knowledge（冲突知识）。为解决这一问题，我们提出了 Agent-Dice，这是一种基于 directional consensus evaluation（方向一致性评估）的 parameter fusion framework（参数融合框架）。具体而言，Agent-Dice 通过两阶段过程解耦知识更新：利用 geometric consensus filtering（几何一致性过滤）剪枝 conflicting gradients（冲突梯度），并利用 curvature-based importance weighting（基于曲率的重要性加权）增强 shared semantics（共享语义）。我们提供了严格的理论分析，确立了所提出的 fusion scheme（融合方案）的有效性，并深入探讨了 stability-plasticity dilemma（稳定性-可塑性困境）的起源。在 GUI agents（图形用户界面智能体）和 tool-use agent（工具使用智能体）领域的大量实验表明，Agent-Dice 展现出了卓越的 continual learning performance（持续学习性能），且仅需极少的 computational overhead（计算开销）和 parameter updates（参数更新）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing",
                    "arxiv_id": "2601.03261",
                    "authors": "Shuo Lu, Yinuo Xu, Jianjie Cheng, Lingxiao He, Meng Wang, Jian Liang",
                    "summary": "Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 论文明确以 \"Deep Research agents\"（深度研究智能体）为研究对象，提出了 DeepResearch-Slice 这一神经符号框架。这属于构建和改进 LLM 智能体的方法论，符合“单智能体”的研究范畴。 2.  **正面指标**: 论文涉及智能体的核心能力，特别是工具使用后的信息处理。它解决了智能体在嘈杂环境中利用检索证据的瓶颈，改进了智能体的推理鲁棒性，属于对智能体内部机制的优化。 3.  **排除标准**: 论文不涉及安全、对齐、多模态或图技术；也不是针对特定垂直领域（如生物、金融）的单纯应用，而是提出了通用的改进框架。 4.  **特殊处理**: 虽然涉及推理，但这是在智能体框架下对“检索-利用”过程的改进，而非单纯提升模型的基础数学或逻辑预测能力，因此不属于“非Agentic的推理”。",
                    "summary2": "本文旨在解决Deep Research中模型因上下文盲区无法利用已检索证据的Retrieval-Utilization Gap问题。针对噪声环境下的检索文档，我们提出了一种名为DeepResearch-Slice的神经符号框架，通过预测精确的span索引执行显式文本切片和硬过滤。在六个基准测试上，通过任务准确率验证了其有效性，在冻结骨干网络上实现了73%的相对性能提升。",
                    "summary_translation": "Deep Research agents (深度研究智能体) 主要致力于优化 search policies (搜索策略)，以最大化 retrieval probability (检索概率)。然而，我们发现了一个关键瓶颈：retrieval-utilization gap (检索-利用差距)，即由于 noisy environments (噪声环境) 中的 context blindness (上下文盲区)，模型即使在成功检索到 gold evidence (黄金证据) 后，仍无法有效利用这些证据。为弥合这一差距，我们提出了 DeepResearch-Slice，这是一个简单而有效的 neuro-symbolic framework (神经符号框架)。与 implicit attention (隐式注意力) 机制不同，我们的方法通过预测精确的 span indices (片段索引)，在 reasoning (推理) 过程之前执行 deterministic hard filter (确定性硬过滤)。在六个 benchmarks (基准测试) 上进行的广泛评估表明，该方法带来了显著的 robustness gains (鲁棒性提升)。将该方法应用于 frozen backbones (冻结骨干网络) 时，实现了 73% 的相对性能提升（从 19.1% 提升至 33.0%），有效缓解了噪声干扰，且无需对 reasoning model (推理模型) 进行 parameter updates (参数更新)。这些结果凸显了在 open-ended research (开放式研究) 中引入 explicit grounding mechanisms (显式定位机制) 的必要性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
                    "arxiv_id": "2601.03905",
                    "authors": "Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, Heng Ji, Heng Ji",
                    "summary": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献在于实证研究LLM智能体如何利用“世界模型”作为外部工具来进行“前瞻”规划。这直接属于“单智能体”研究范畴中的“工具使用”和“规划”能力。虽然论文主要揭示了当前智能体的局限性（即未能有效利用），但其目的是为了指导未来构建具有更强认知能力的智能体系统，属于对智能体构建和改进机制的探索，而非单纯的应用。 2.  **正面指标匹配（第二步）**：论文高度符合核心关注点。 *   **Agentic AI**: 明确以“Agents”为研究对象。 *   **Tool Use / Tool Augmentation**: 研究的核心是智能体如何将生成式世界模型作为“外部模拟器”工具来使用。 *   **Planning**: 论文聚焦于“anticipating future states”（前瞻）和“foresight”，这是智能体规划能力的高级形式。 3.  **排除标准与特殊情况处理（第三、四步）**： *   **多模态问题**: 尽管摘要提到了“vision-language models”和“visual question answering”，但视觉内容在此处是智能体感知的环境或工具（世界模型）的一部分，而非论文的研究核心。论文的核心在于智能体与该工具的交互机制，而非改进视觉模型本身，因此符合“作为工具使用”的例外情况。 *   **推理/规划**: 论文讨论的是智能体如何通过模拟进行多步推理和决策，属于Agentic层面的规划，而非单纯的LLM内部逻辑推理优化。 综上所述，该论文深入探讨了单智能体在工具使用和前瞻规划方面的关键问题，对构建和改进LLM智能体具有直接的指导意义，完全符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在探究当前智能体利用世界模型进行前瞻认知的能力。针对多样化的智能体决策和视觉问答任务，我们提出了一种“世界模型即工具”的评估框架，并在FrozenLake、Navigation等任务及VQA基准上通过成功率、准确率和调用率验证了其有效性。研究发现当前智能体很少调用世界模型，且强制使用会降低性能，核心瓶颈在于前瞻治理机制。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "LLM-Enabled Multi-Agent Systems: Empirical Evaluation and Insights into Emerging Design Patterns & Paradigms",
                    "arxiv_id": "2601.03328",
                    "authors": "Harri Renney, Maxim N Nethercott, Nathan Renney, Peter Hayes",
                    "summary": "This paper formalises the literature on emerging design patterns and paradigms for Large Language Model (LLM)-enabled multi-agent systems (MAS), evaluating their practical utility across various domains. We define key architectural components, including agent orchestration, communication mechanisms, and control-flow strategies, and demonstrate how these enable rapid development of modular, domain-adaptive solutions. Three real-world case studies are tested in controlled, containerised pilots in telecommunications security, national heritage asset management, and utilities customer service automation. Initial empirical results show that, for these case studies, prototypes were delivered within two weeks and pilot-ready solutions within one month, suggesting reduced development overhead compared to conventional approaches and improved user accessibility. However, findings also reinforce limitations documented in the literature, including variability in LLM behaviour that leads to challenges in transitioning from prototype to production maturity. We conclude by outlining critical research directions for improving reliability, scalability, and governance in MAS architectures and the further work needed to mature MAS design patterns to mitigate the inherent challenges.",
                    "category": "cs.MA",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心贡献在于形式化了LLM驱动的多智能体系统（MAS）的设计模式和范式，并定义了包括智能体编排、通信机制和控制流策略在内的关键架构组件。这属于“构建”和“改进”LLM智能体（特别是多智能体系统）的方法论研究，符合第一步中关于保留“构建LLM智能体”或“多智能体系统”方法论的要求。 2.  **正面指标 (匹配)**: 论文明确涉及 `Multi-Agent Systems (MAS)` 这一核心范式，并深入探讨了 `Agent Orchestration`（智能体编排）和 `Communication mechanisms`（通信机制），这些都是多智能体研究中的关键能力和正面指标。 3.  **排除标准 (未触发)**: *   **非演化型应用**: 尽管论文使用了电信、遗产管理等领域的案例研究，但其主要目的不是为了解决这些领域的具体业务问题，而是为了评估MAS架构和设计模式的实用性及开发效率。因此，它不属于仅将LLM作为工具应用到特定领域的“非演化型应用”。 *   **安全与对齐**: 论文虽然提到了治理和可靠性，但这并非其核心贡献，核心在于系统架构和设计模式。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 4.  **综合结论**: 该论文提供了关于如何构建、设计和评估多智能体系统的实证见解和架构框架，直接服务于“LLM智能体及其演化”中关于多智能体方向的研究目标。",
                    "summary2": "本文旨在形式化LLM赋能的多智能体系统（MAS）的设计模式并评估其实用性。针对电信安全、国家遗产资产管理和公用事业客户服务等真实场景，我们提出了一种基于ReAct智能体和动态编排的MAS设计范式，并在三个受控的容器化试点项目中通过开发周期、利益相关者反馈及UAT评分验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "When Numbers Start Talking: Implicit Numerical Coordination Among LLM-Based Agents",
                    "arxiv_id": "2601.03846",
                    "authors": "Alessio Buscemi, Daniele Proverbio, Alessandro Di Stefano, The Anh Han, German Castignani, Pietro Liò",
                    "summary": "LLMs-based agents increasingly operate in multi-agent environments where strategic interaction and coordination are required. While existing work has largely focused on individual agents or on interacting agents sharing explicit communication, less is known about how interacting agents coordinate implicitly. In particular, agents may engage in covert communication, relying on indirect or non-linguistic signals embedded in their actions rather than on explicit messages. This paper presents a game-theoretic study of covert communication in LLM-driven multi-agent systems. We analyse interactions across four canonical game-theoretic settings under different communication regimes, including explicit, restricted, and absent communication. Considering heterogeneous agent personalities and both one-shot and repeated games, we characterise when covert signals emerge and how they shape coordination and strategic outcomes.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的本质是研究 LLM 智能体在多智能体环境中的行为机制。它不是将智能体作为工具去解决生物、医疗等特定领域的应用问题，而是深入探讨智能体之间如何进行“隐式协调”和“隐蔽通信”。这属于构建和理解多智能体系统（Multi-Agent Systems）的方法论研究，因此应予以保留。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Communication`（特别是非显式的、隐蔽的通信）和 `Collaboration`（协调）。 *   **博弈与互动**：通过博弈论设置分析智能体间的战略互动，这属于多智能体研究中的社会行为和博弈范畴。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图技术等排除项。 **总结**：该论文的核心贡献在于揭示了 LLM 智能体在缺乏显式语言通道时，如何通过行动（如数字信号）进行隐式协调。这直接拓展了对于多智能体通信机制和协作行为的理解，符合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在探究LLM智能体在多智能体环境中的隐式通信与协调机制。针对四种经典博弈论场景（Prisoner's Dilemma等），我们提出了一种基于数值序列的隐式通信机制，并在FAIRGAME框架下通过GPT-4o智能体进行了实验。通过分析不同通信条件下的合作水平与熵值，验证了隐式信号能产生结构化模式并有效影响战略结果。",
                    "summary_translation": "基于LLMs（大语言模型）的智能体越来越多地在需要strategic interaction（策略互动）和协调的multi-agent environments（多智能体环境）中运行。尽管现有工作主要集中在个体智能体或共享explicit communication（显式通信）的交互智能体上，但对于交互智能体如何进行隐式协调的研究尚不充分。特别是，智能体可能会进行covert communication（隐蔽通信），即依赖于嵌入在其行动中的间接或非语言信号，而非显式消息。本文对LLM驱动的多智能体系统中的隐蔽通信进行了game-theoretic（博弈论）研究。我们分析了在不同communication regimes（通信机制）下，包括显式、受限和缺失通信，四种典型博弈论设定中的互动。考虑到heterogeneous agent personalities（异构智能体人格）以及one-shot games（单次博弈）和repeated games（重复博弈），我们刻画了隐蔽信号何时涌现，以及它们如何塑造协调和策略结果。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-06": [
        {
            "name": "Artificial Intelligence",
            "count": 7,
            "papers": [
                {
                    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
                    "arxiv_id": "2601.03236",
                    "authors": "Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li",
                    "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 MAGMA 的“智能体记忆架构”。这属于构建和改进 LLM 智能体的方法论范畴，旨在解决智能体在长视界推理中的记忆检索和表示问题。它不是将现有智能体简单应用于特定领域，也不是单纯的基础设施优化，而是直接针对智能体核心组件（记忆）的架构创新。 2.  **包含核心关注点 (第二步)**: 论文明确涉及 `Agentic AI` 和 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它探讨了如何通过多图结构（语义、时间、因果、实体）来优化智能体的记忆存储和检索，从而提升推理能力。这完全符合单智能体方向中关于“记忆”机制的子方向。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 虽然摘要提到了“alignment between query intent and retrieved evidence”（查询意图与检索证据的对齐），但这指的是信息检索层面的语义匹配，而非 AI 安全领域的“对齐”。论文主要贡献不在于 Safety 或 Alignment。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 *   **图**: 尽管论文使用了“Multi-Graph”（多图）技术，但这是作为实现智能体记忆架构的**手段**，而非研究图算法本身。论文的主题是“Agentic Memory Architecture”，属于智能体研究，因此不应被排除。 综上所述，这篇论文通过改进智能体的记忆机制来提升其性能，属于构建和演化 LLM 智能体的核心研究范围，符合筛选标准。",
                    "summary2": "本文旨在解决现有Memory-Augmented Generation系统因依赖单一语义相似度而限制长时程推理准确性和可解释性的问题。针对长上下文推理场景，我们提出了一种名为MAGMA的多图智能体记忆架构，通过语义、时间、因果和实体四个正交关系图解耦记忆表示，并采用策略引导的图遍历进行检索。在LoCoMo和LongMemEval数据集上通过LLM-as-a-Judge、F1及系统延迟等指标验证了其有效性。",
                    "summary_translation": "Memory-Augmented Generation (MAG，记忆增强生成) 通过引入外部记忆扩展了大语言模型，以支持长上下文推理。然而，现有方法主要依赖于单一整体记忆存储上的语义相似度，导致时间、因果和实体信息相互纠缠。这种设计限制了可解释性以及查询意图与检索证据之间的对齐，从而导致推理准确率不理想。在本文中，我们提出了 MAGMA，这是一种多图智能体记忆架构，它在正交的语义、时间、因果和实体图中表示每个记忆项。MAGMA 将检索过程构建为在这些关系视图上的策略引导遍历，从而实现查询自适应的选择和结构化上下文构建。通过将记忆表示与检索逻辑解耦，MAGMA 提供了透明的推理路径以及对检索过程的细粒度控制。在 LoCoMo 和 LongMemEval 数据集上的实验表明，MAGMA 在长视界推理任务中始终优于最先进的智能体记忆系统。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
                    "arxiv_id": "2601.02950",
                    "authors": "Xuan Yang, Furong Jia, Roy Xie, Xiong Xi, Hengwei Bian, Jian Li, Monica Agrawal",
                    "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 范畴）**： 虽然论文标题侧重于“推理”，但其核心贡献不仅仅是提出一种新的思维链变体，而是构建了一个名为 **BoT-R (Batch-of-Thought with Reflection)** 的架构。摘要明确指出，该方法是在 **\"multi-agent reflection architecture\"（多智能体反思架构）** 中实例化的。这表明论文的本质是构建了一个包含特定角色（Reflector 智能体）的智能体系统，而非单纯的模型推理能力提升。 2.  **符合正面指标（多智能体与自我反思）**： *   **多智能体**：论文明确提到了 \"multi-agent reflection architecture\"，涉及智能体之间的协作与信息交互（Reflector 执行联合评估以解锁互信息增益）。 *   **自我反思/修正**：论文的核心机制涉及 \"reflection\"（反思）和 \"detects errors\"（检测错误），这属于智能体的自我反思和自我修正能力范畴。 *   **Agentic 框架**：BoT-R 是一种新的 Agentic 框架，用于处理复杂任务，符合“构建、改进 LLM 智能体”的目标。 3.  **排除标准检查**： *   该论文不属于特定领域的非演化型应用（如医疗、法律），而是一种通用的推理架构改进。 *   虽然涉及推理，但它通过多智能体架构来实现，不属于“非 Agentic 的推理”排除项。 *   不涉及安全、对齐、多模态或图等排除领域。 综上所述，该论文通过引入多智能体反思架构来增强 LLM 的推理能力，属于 Agentic AI 和 Multi-Agent Systems 的研究范畴，符合筛选要求。",
                    "summary2": "本文旨在解决现有LLM推理系统独立处理查询导致跨实例信号丢失的问题。针对相关查询批次，我们提出了一种名为Batch-of-Thought (BoT) 的免训练方法，利用Reflector进行联合评估以实现跨实例学习。在六个基准和三个模型家族上，通过准确率、置信度校准和Token成本验证了其有效性，实现了性能提升和高达61%的成本降低。",
                    "summary_translation": "当前的 Large Language Model (大语言模型) 推理系统独立处理查询，忽略了诸如共享推理模式和一致性约束等有价值的 cross-instance signals (跨实例信号)。我们提出了 Batch-of-Thought (BoT)，这是一种 training-free (无需训练) 的方法，通过联合处理相关查询来实现 cross-instance learning (跨实例学习)。通过在批次间执行 comparative analysis (比较分析)，BoT 能够识别高质量的 reasoning templates (推理模板)，通过 consistency checks (一致性检查) 检测错误，并分摊 computational costs (计算成本)。我们在 multi-agent reflection architecture (多智能体反思架构) 中实例化了 BoT (BoT-R)，其中 Reflector (反思者) 执行 joint evaluation (联合评估)，以释放 isolated processing (孤立处理) 中无法获得的 mutual information gain (互信息增益)。在三个模型系列和六个基准测试上的实验表明，BoT-R 在将 inference costs (推理成本) 降低高达 61% 的同时，持续提高了 accuracy (准确性) 和 confidence calibration (置信度校准)。我们的理论和实验分析揭示了 batch-aware reasoning (批感知推理) 在何时以及为何能使 Large Language Model (大语言模型) 系统受益。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
                    "arxiv_id": "2601.02702",
                    "authors": "Shuhaib Mehri, Priyanka Kargupta, Tal August, Dilek Hakkani-Tür",
                    "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”与“自我演化”的交叉领域。 1.  **核心贡献符合要求**：论文的核心在于构建了一种“长期协作智能体”，并提出了相应的基准测试。这不仅仅是应用现有模型，而是提出了一种新的智能体框架，旨在解决智能体如何适应环境（用户）的问题。 2.  **符合“单智能体”方向**：论文明确聚焦于智能体的关键能力——**记忆**和**自我反思**。它详细描述了如何通过持久化的记忆来存储用户偏好，以及如何通过反思机制来更新记忆，这直接对应筛选标准中的“Agentic: 记忆、自我反思”。 3.  **符合“自我演化”方向**：论文强调智能体随着交互经验的积累，能够不断细化用户偏好并改进协作质量。这种通过经验反馈进行迭代改进和自我完善的过程，正是“自我演化”的核心体现。 4.  **排除标准检查**：论文不属于特定领域的非演化型应用（其核心是智能体机制的通用改进），也不涉及安全对齐、多模态或图技术等排除领域。 综上所述，该论文在构建具备记忆和自我演化能力的LLM智能体方面做出了实质性贡献，应予保留。",
                    "summary2": "本文旨在解决对话代理在多会话协作中学习并利用用户偏好以提升协作质量的问题。针对多会话协作问题求解场景，我们提出了一种配备持久化记忆的长期协作代理及基于强化学习的记忆更新框架，并在MULTI SESSION COLLAB基准上通过Task Success、User Effort和Conversation Length验证了其有效性。",
                    "summary_translation": "随着 conversational agents (对话智能体) 积累与用户协作的经验，适应用户偏好对于建立长期关系以及随着时间的推移提升协作质量至关重要。我们介绍了 MultiSessionCollab，这是一个 benchmark (基准测试)，旨在评估智能体在跨多个会话的过程中学习用户偏好并利用这些偏好提升协作质量的能力。为了开发能够在此类场景中取得成功的智能体，我们提出了 long-term collaborative agents (长期协作智能体)，该智能体配备了 memory (记忆)，能够随着交互经验的积累而持久保存并细化用户偏好。此外，我们证明了可以从 MultiSessionCollab 中的 user simulator (用户模拟器) 行为中提取 learning signals (学习信号)，从而训练智能体生成更全面的 reflections (反思内容) 并更有效地更新其 memory (记忆)。大量实验表明，为智能体配备 memory (记忆) 能够改善长期协作效果，从而带来更高的任务成功率、更高效的交互以及更低的用户负担。最后，我们进行了一项人类用户研究，结果表明 memory (记忆) 有助于在真实场景中提升用户体验。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents",
                    "arxiv_id": "2601.02643",
                    "authors": "Mehmet Kurmaz",
                    "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中的“工具使用”和“规划”子方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献在于提出了一种新的方法论（偏好感知查询修复）和一个基准（AWARE-US），旨在解决“工具调用智能体”在查询结构化数据库时遇到的“不可行性”问题。 *   这不是将智能体简单应用于特定领域（如医疗或金融）的应用型论文，而是针对智能体在工具使用过程中的通用能力缺陷（查询失败处理）进行改进和构建。 *   它不属于基础设施优化，也不是非Agentic的基础推理提升。 2.  **正面指标匹配 (第二步)**: *   论文明确涉及 `LLM-based Agents` 和 `Tool Use / Tool Augmentation`。 *   它关注智能体如何根据对话上下文推断用户意图，并动态调整查询策略（放松约束），这属于智能体的 `Planning` 和决策能力范畴。 3.  **排除标准检查 (第三步)**: *   虽然论文提到了“preference alignment”（偏好对齐），但这里的对齐是指智能体在执行任务时对用户具体约束条件的偏好（例如用户更看重价格还是速度），而非AI安全、伦理或价值观层面的“对齐”。因此，不应被排除。 *   论文不涉及多模态视觉、图技术或安全防御机制。 4.  **特殊处理 (第四步)**: *   论文关于智能体如何处理工具调用失败并进行自我修正（查询修复），属于Agentic的推理与规划范畴，符合保留条件。 综上所述，该论文致力于改进LLM智能体的工具使用鲁棒性和交互规划能力，符合“构建、改进LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决Tool-calling agents在查询结构化数据库时面临的infeasibility问题。针对基于personas的对话场景，我们提出了一种Preference-Aware Resolution框架，包含local weighting、global one-shot weighting和pairwise ranking三种约束重要性推断方法。我们在AWARE-US benchmark上通过Relax match和Car match等指标验证了其有效性，结果显示Local weighting方法在汽车推荐中与用户偏好一致性达到48%。",
                    "summary_translation": "调用工具查询结构化数据库的对话代理通常面临两个相互关联的问题：underspecification（约束不足，即缺乏运行精确查询所需的约束）和 infeasibility（不可行性，即完全指定的查询返回空集，因为没有项目满足所有约束）。现有研究通常以“无结果”作为回应，或利用 ad hoc rules（特设规则）放宽约束，这可能会因丢弃用户最关心的需求而违背用户意图。我们将 infeasibility handling（不可行性处理）构建为一个 preference-aware query repair（感知偏好的查询修复）问题：当查询不可满足时，代理应当放宽对用户而言重要性最低的约束。我们提出了三种基于 LLM（大语言模型）的方法，用于从对话中推断相对约束重要性：(1) local weighting（局部加权），(2) global one-shot weighting（全局一次性加权），以及 (3) pairwise ranking（成对排序）。实验结果表明，local weighting（局部加权）实现了最佳的 preference alignment（偏好对齐），而 global weighting（全局加权）在正确的 constraint relaxation（约束放宽）方面表现最佳。我们还介绍了 AWARE-US，这是一个包含 persona-grounded queries（基于人设的查询）的基准数据集，要求代理通过对话消除请求歧义，并以与 persona-implied preferences（人设隐含偏好）一致的方式解决 infeasibility（不可行性）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
                    "arxiv_id": "2601.02553",
                    "authors": "Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao",
                    "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **SimpleMem**，这是一个专门为 **LLM智能体** 设计的高效记忆框架。 *   它属于 **构建/改进 LLM智能体** 的范畴，旨在解决智能体在长期交互中的记忆管理问题，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或硬件加速的研究。 2.  **正面指标匹配（第二步）**： *   论文直接涉及 **Agentic AI** 和 **LLM-based Agents** 的核心范式。 *   论文重点解决了智能体的 **Memory（记忆）** 能力，这是单智能体方向的关键子方向之一。摘要中提到的“管理历史经验”、“语义结构化压缩”和“递归记忆整合”都是为了增强智能体的记忆机制，使其能更好地支持长期交互。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，该论文致力于改进LLM智能体的核心组件（记忆系统），属于单智能体研究范畴，符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM Agent在长期交互中因上下文限制和冗余信息导致的记忆管理低效问题。针对复杂环境下的长上下文交互场景，我们提出了一种基于语义无损压缩的SimpleMem框架，包含语义结构化压缩、递归记忆整合和自适应查询感知检索三阶段流程。我们在LoCoMo benchmark上通过F1 score和Token Cost验证了其有效性，实现了平均F1提升26.4%且推理Token消耗降低30倍。",
                    "summary_translation": "为了在复杂环境中支持可靠的长期交互，LLM agents（大语言模型智能体）需要能够高效管理历史经验的 memory systems（记忆系统）。现有方法要么通过 passive context extension（被动上下文扩展）保留完整的交互历史，导致大量冗余，要么依赖 iterative reasoning（迭代推理）来过滤噪声，从而产生高昂的 token costs（Token 成本）。为了应对这一挑战，我们提出了 SimpleMem，这是一种基于 semantic lossless compression（语义无损压缩）的高效 memory framework（记忆框架）。我们提出了一个旨在最大化 information density（信息密度）和 token utilization（Token 利用率）的 three-stage pipeline（三阶段流水线）：(1) \\textit{Semantic Structured Compression}（语义结构化压缩），该阶段应用 entropy-aware filtering（熵感知过滤）将非结构化交互提炼为紧凑的 multi-view indexed memory units（多视图索引记忆单元）；(2) \\textit{Recursive Memory Consolidation}（递归记忆整合），这是一个异步过程，将相关单元整合为 higher-level abstract representations（高层抽象表示）以减少冗余；以及 (3) \\textit{Adaptive Query-Aware Retrieval}（自适应查询感知检索），该阶段根据 query complexity（查询复杂度）动态调整 retrieval scope（检索范围），以高效构建精确的上下文。在 benchmark datasets（基准数据集）上的实验表明，我们的方法在准确性、retrieval efficiency（检索效率）和 inference cost（推理成本）方面始终优于 baseline approaches（基线方法），实现了平均 26.4% 的 F1 提升，同时将 inference-time token consumption（推理时 Token 消耗）减少了多达 30 倍，展示了性能与效率之间的卓越平衡。代码可在 https://github.com/aiming-lab/SimpleMem 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices",
                    "arxiv_id": "2601.02732",
                    "authors": "Lingzhe Zhang, Tong Jia, Yunpeng Zhai, Leyi Pan, Chiming Duan, Minghua He, Mengxi Jia, Ying Li",
                    "summary": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建LLM智能体”的目标**： 尽管论文的应用场景是微服务中的根因定位（特定领域），但其核心贡献并非简单地将现有LLM作为工具应用，而是提出了一个新的框架 **AMER-RCL**。该框架包含两个关键的智能体组件： *   **Recursive Reasoning RCL engine**：明确被定义为一个**多智能体框架**，用于执行递归推理。 *   **Agentic Memory**：一种智能体记忆机制，用于增量累积和重用推理过程。 2.  **高度匹配“正面指标”**： *   **多智能体**：论文明确提出了多智能体框架来处理告警和细化候选原因。 *   **智能体能力**：涉及 `Memory`（智能体记忆）、`Reasoning`（递归推理）以及 `Multi-Agent` 协作。 *   **核心范式**：论文标题和摘要中多次强调 \"Agentic Memory\" 和 \"Agentic\"，完全符合 Agentic AI 的研究焦点。 3.  **通过“排除标准”和“特殊情况”检查**： *   虽然涉及特定领域（微服务），但根据第四步的“自我演化的应用”逻辑（此处虽非演化，但同理），如果论文的核心是提出一种新的智能体机制（如这里的递归推理+记忆机制），即使应用在特定领域，也应保留。这区别于仅仅调用API解决领域问题的“非演化型应用”。 *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除内容。 综上所述，该论文在构建多智能体框架和智能体记忆机制方面做出了实质性贡献，属于 Agentic AI 和 Multi-Agent Systems 的前沿研究。",
                    "summary2": "本文旨在解决现有LLM方法在微服务根因定位中推理浅显及缺乏跨告警复用的问题。针对微服务系统中的多告警场景，我们提出了一种名为AMER-RCL的智能体记忆增强递归推理框架。该框架利用递归推理引擎进行深度分析，并通过智能体记忆复用历史推理结果。在AIOPS 2022、Train-Ticket和FAMOS-Mall数据集上，通过Recall@k、MRR及推理延迟验证了其有效性，显著优于现有方法。",
                    "summary_translation": "随着当代微服务系统日益普及且日趋复杂——通常由数百甚至数千个细粒度、相互依赖的子系统组成——其故障发生频率也随之增加。因此，确保系统可靠性需要准确的根因定位。尽管针对该任务已探索了许多传统的基于图和深度学习的方法，但它们往往严重依赖预定义模式，难以适应不断演变的运行环境。因此，近期提出了多种基于大语言模型的方法。然而，这些方法仍面临两大主要局限：一是浅层的、以症状为中心的推理损害了准确性；二是缺乏跨告警重用，导致推理冗余和高延迟。本文对站点可靠性工程师如何定位故障根因进行了全面研究，汲取了来自多个组织的专业人士的见解。调查结果显示，专家根因分析表现出三个关键特征：递归性、多维扩展和跨模态推理。基于这些发现，我们提出了 AMER-RCL，一种用于微服务根因定位的智能体记忆增强递归推理框架。AMER-RCL 采用了递归推理 RCL 引擎，这是一个多智能体框架，通过对每个告警执行递归推理来逐步细化候选原因；同时，智能体记忆增量地积累并重用时间窗口内先前告警的推理结果，以减少冗余探索并降低推理延迟。实验结果表明，AMER-RCL 在定位准确性和推理效率方面均持续优于最先进的方法。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance",
                    "arxiv_id": "2601.02454",
                    "authors": "Saba Naqvi, Mohammad Baqar, Nawaz Ali Mohammad",
                    "summary": "Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”与“自我演化”方向**： 论文的核心贡献是提出了一种“智能体多模型测试框架”。这不仅仅是一个应用，而是一个新的**多智能体系统（MAS）**架构。该系统包含三个具有不同角色的智能体（测试生成、执行与分析、审查与优化），它们通过协作完成任务，这直接对应了筛选标准中的“多智能体”方向。 2.  **具备明确的“自我演化”机制**： 摘要中明确提到这是一个“闭环、自我修正系统”，利用“沙箱执行”和“强化信号”进行“迭代再生或修补”。这种通过环境反馈（执行结果、覆盖率指标）来引导智能体自主改进和迭代的过程，完全符合“自我演化”中关于自我完善、自我修正和迭代改进的定义。 3.  **属于Agentic AI的构建而非单纯应用**： 虽然论文的应用场景是软件测试（特定领域），但根据筛选标准第四步的“自我演化的应用”例外规则，只要论文的核心是提出一种新的“自我演化”机制或Agentic框架，即使应用在特定领域也应保留。本文重点在于构建了一个能够自主规划、协作和反思的智能体框架，而非简单地将现有LLM作为工具生成测试代码。 综上所述，该论文在多智能体协作、自我修正机制以及闭环演化方面具有明确的方法论贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有AI测试生成缺乏执行反馈导致测试无效的问题。针对微服务应用，我们提出了一种Agentic Testing Architecture (ATA)，即包含Test Generation Agent、Execution and Analysis Agent和Review and Optimization Agent的多智能体闭环协作框架。在开源及企业应用数据集上，通过代码覆盖率、无效测试率和人工工作量等指标验证了其有效性，实现了无效测试减少60%及覆盖率提升30%。",
                    "summary_translation": "软件测试正迈向智能自动化，但现有的 AI-based test generators（基于AI的测试生成器）仍受限于静态、单次输出的模式。由于缺乏 execution aware feedback（执行感知反馈），这些工具常生成无效、冗余或不可执行的测试用例。本文提出了一种 agentic multi-model testing framework（代理多模型测试框架），这是一个闭环、自纠正系统。在该系统中，Test Generation Agent（测试生成代理）、Execution and Analysis Agent（执行与分析代理）以及 Review and Optimization Agent（审查与优化代理）协同工作，生成、执行、分析并完善测试，直至达到收敛状态。通过利用 sandboxed execution（沙箱执行）、详细的 failure reporting（失败报告）以及对失败测试的 iterative regeneration（迭代重新生成）或 patching（修补），该框架能够自主提升测试质量并扩大覆盖率。该框架集成于 CI/CD-compatible pipeline（兼容CI/CD的流水线）中，利用来自 coverage metrics（覆盖率指标）和 execution outcomes（执行结果）的 reinforcement signals（强化信号）来指导测试的完善过程。针对 microservice based applications（基于微服务的应用程序）进行的实证评估表明，与 single-model baselines（单模型基线）相比，该框架将无效测试减少了高达 60%，覆盖率提升了 30%，并显著降低了人工投入。这证明了 multi-agent（多代理）、feedback-driven loops（反馈驱动循环）能够将软件测试演变为一个自主的、持续学习的 quality assurance ecosystem（质量保证生态系统），以支持 self-healing（自愈）和 high-reliability codebases（高可靠性代码库）。",
                    "inspiration_trace": "基于论文《The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance》，以下是对作者产出该核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观实现的思考过程：\n\n### 1. 宏观背景：软件复杂性与自动化瓶颈\n**观察：**\n现代软件工程已转向云原生、微服务架构和DevOps模式，代码迭代速度极快，依赖关系复杂。传统的手工测试或基于规则的自动化脚本已无法跟上这种交付节奏，成为效率瓶颈。\n\n**思考：**\n必须引入更高阶的智能化手段。近年来，大语言模型（LLMs）在代码理解和生成方面表现出色，似乎为解决这一瓶颈提供了契机。学术界和工业界开始尝试利用LLMs自动生成测试用例。\n\n### 2. 核心痛点：LLM的“静态生成”与“执行盲区”\n**深入观察：**\n尽管现有的基于LLM的测试生成工具（如Codex, GPT-4）能够快速产出代码，但作者发现了一个关键问题：**这些生成是“一次性”的**。LLM像是在真空中写作，它不知道生成的代码在真实环境中能否运行。\n\n**逻辑断层：**\n实证数据显示，超过40%的LLM生成的测试用例在首次执行时就会失败（如缺少依赖、语法错误、逻辑不匹配）。这是因为现有方法缺乏“执行感知”的反馈机制。LLM无法从自己的错误中学习，导致产生了大量无效、冗余的垃圾代码，反而增加了人工清理的成本。\n\n**假设提出：**\n如果能让测试生成系统具备“自我反省”和“自我修正”的能力，即像人类测试人员一样——写代码 -> 运行 -> 报错 -> 修改 -> 再运行，那么测试的质量和有效性将大幅提升。\n\n### 3. 概念跃迁：从“单点工具”到“多智能体协作”\n**灵感来源：**\n作者观察到AI领域正在兴起“Agentic AI”（智能体AI）和多智能体系统（如AutoGen, SWE-Agent）。这些系统通过让多个专门的AI角色相互协作、对话来解决复杂任务，而非依赖单一模型。\n\n**类比推理：**\n软件测试本身就是一个团队协作过程：有人写测试，有人执行测试，有人分析结果。为什么不让AI也模仿这种社会分工？\n\n**方法论雏形：**\n不再使用一个单一的LLM模型完成所有工作，而是设计一个**多智能体系统**。将测试流程拆解为不同的专业角色，让它们各司其职，形成一个流水线。\n\n### 4. 方法论构建：闭环反馈与收敛机制\n**架构设计：**\n为了实现上述假设，作者构建了三个核心智能体，形成了一个闭环：\n1.  **生成者：** 负责根据需求编写初始测试。\n2.  **执行者：** 负责在沙盒环境中运行测试，并收集覆盖率数据和报错信息。\n3.  **审查与优化者：** 负责分析执行结果，诊断失败原因，并指导生成者进行修复。\n\n**逻辑核心：**\n这个系统的核心不在于单个智能体的能力，而在于它们之间的**交互循环**。作者引入了“收敛”的概念：系统不是无限循环，而是设定了明确的停止条件（如覆盖率>95%，失败率<2%）。这将其从简单的“尝试”转变为一种“优化过程”。\n\n**关键创新点：**\n作者意识到，只有当“执行反馈”被转化为“自然语言指令”并重新输入给LLM时，真正的自我修正才发生。因此，必须建立一个共享的记忆库，让智能体能记住之前的错误，避免重蹈覆辙。\n\n### 5. 最终愿景：迈向自主演进的QA生态系统\n**验证与结论：**\n通过实验，作者验证了这种多智能体闭环模式确实能显著降低无效测试比例，提高覆盖率。这证明了“反馈驱动”优于“静态生成”。\n\n**思想升华：**\n最终，作者将这一方法定义为“Agentic Testing”。这不仅仅是一个工具，而是一个**自主的质量保证生态系统**。它标志着软件测试从“人类辅助AI”转向了“AI自主协作”，测试系统具备了类似生物的“自愈”和“适应”能力，能够随着代码库的演变而自动演进。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现问题（静态生成无效） -> 寻找类比（人类协作流程） -> 引入范式（多智能体系统） -> 构建机制（闭环反馈与收敛） -> 实现愿景（自主QA）”** 的完整逻辑链条。其核心洞察在于：**没有反馈的生成是盲目的，只有引入执行反馈和多智能体协作，AI测试才能真正落地。**"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
                    "arxiv_id": "2601.03192",
                    "authors": "Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen, Zhiyu Li, Feiyu Xiong, Yutao Qi, Bo Tang, Muning Wen",
                    "summary": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **MemRL**，这是一个旨在实现 **Self-Evolving Agents（自我演化智能体）** 的框架。它解决的是智能体如何通过经验进行自我完善和迭代的问题，而非仅仅是将LLM作为工具应用到特定领域。这直接对应了研究目标中的“自我演化”方向。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文标题和摘要中明确提到了 `Self-Evolving Agents` 和 `Reinforcement Learning`，属于核心关注点。 *   **智能体能力**：论文重点研究了 `Memory`（情景记忆 Episodic Memory）和 `Self-Improvement`（通过环境反馈持续改进）。 *   **演化机制**：论文提出了一种非参数强化学习机制，通过环境反馈在运行时迭代改进 Q-values（效用），从而实现智能体的持续演化，符合 `Iterative Improvement` 和 `Self-Refine` 的定义。 3.  **排除标准（未触发）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然论文在 HLE、BigCodeBench 等基准上进行了实验，但这些是用于验证智能体能力的通用任务，不属于“非演化型应用”的排除范畴。 4.  **特殊与模糊情况处理**： *   论文属于典型的 **自我演化** 机制研究。它提出了一种新的“在情景记忆上进行运行时强化学习”的方法，使智能体能够在不更新模型权重的情况下解决稳定性-可塑性困境并实现持续改进。这完全符合“保留：如果论文的核心是提出一种新的‘自我演化’机制”的规则。 综上所述，该论文专注于构建新的框架以实现LLM智能体的自我演化和记忆优化，是“LLM智能体及其演化”课题下的高质量相关论文。",
                    "summary2": "本文旨在解决LLM在部署后难以持续自我进化及避免灾难性遗忘的问题。针对需要Runtime Continuous Learning的场景，我们提出了一种MemRL框架，通过非参数强化学习优化情节记忆，采用基于Q值的Two-Phase Retrieval机制区分高价值策略与噪声。并在HLE、BigCodeBench、ALFWorld及Lifelong Agent Bench上通过Last Epoch Accuracy和Cumulative Success Rate验证了其有效性。",
                    "summary_translation": "人类智能的显著特征在于通过 Constructive Episodic Simulation（建设性情景模拟）掌握新技能的能力——即检索过往经验以综合解决新颖任务的方案。尽管 Large Language Models（大语言模型）拥有强大的推理能力，但它们难以模拟这种自我进化：Fine-tuning（微调）计算成本高昂且容易导致 Catastrophic Forgetting（灾难性遗忘），而现有的基于记忆的方法依赖于被动语义匹配，往往会检索到噪声。为了应对这些挑战，我们提出了 MemRL，这是一个通过在 Episodic Memory（情景记忆）上进行 Non-parametric Reinforcement Learning（非参数强化学习）来使智能体实现自我进化的框架。MemRL 明确地将冻结 LLM 的稳定推理与具有可塑性且不断进化的记忆分离开来。与传统方法不同，MemRL 采用了一种 Two-Phase Retrieval（两阶段检索）机制，首先根据语义相关性过滤候选，然后基于学习到的 Q-values（效用）进行选择。这些效用通过环境反馈以试错的方式不断优化，使智能体能够区分高价值策略与相似的噪声。在 HLE、BigCodeBench、ALFWorld 和 Lifelong Agent Bench 上进行的广泛实验表明，MemRL 显著优于 State-of-the-art（最先进的）基线。我们的分析实验证实，MemRL 有效调和了 Stability-Plasticity Dilemma（稳定性-可塑性困境），能够在不进行权重更新的情况下实现持续的运行时改进。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents",
                    "arxiv_id": "2601.02845",
                    "authors": "Kai Li, Xuanqing Yu, Ziyi Ni, Yi Zeng, Yao Xu, Zheqing Zhang, Xin Li, Jitao Sang, Xiaogang Duan, Xuelei Wang, Chengbao Liu, Jie Tan",
                    "summary": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心贡献符合第一步判断**：论文的核心是提出了一种名为 TiMem 的时间-分层记忆框架，旨在解决长视界对话智能体在管理不断增长的交互历史时面临的上下文窗口限制问题。这属于“构建、改进或演化 LLM智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等）的非演化型应用。 2.  **精准命中第二步正面指标**：我的研究焦点明确包含“单智能体”方向下的“记忆”能力。该论文专门针对智能体的“记忆”机制进行了创新，提出了时间记忆树（TMT）和语义引导的整合机制，这正是对智能体核心组件的改进。 3.  **不触犯第三步排除标准**：论文主要关注智能体的记忆架构，不涉及安全与对齐、多模态视觉技术或知识图谱等被明确排除的领域。 综上所述，该论文致力于改进LLM智能体的记忆能力，属于单智能体研究的关键子方向，符合筛选要求。",
                    "summary2": "本文旨在解决长周期对话代理中记忆碎片化及个性化不稳定的问题。针对不断增长的交互历史，我们提出了一种TiMem框架，该框架利用Temporal Memory Tree实现时间分层记忆整合与复杂度感知检索，并在LoCoMo和LongMemEval-S基准上通过准确率和记忆长度验证了其有效性。",
                    "summary_translation": "长程对话智能体必须管理不断增长的交互历史，这些历史很快就会超过大语言模型（large language models, LLMs，大语言模型）的有限上下文窗口。现有的记忆框架对跨层级的时间结构化信息支持有限，往往导致记忆碎片化和不稳定的长程个性化。我们提出了 TiMem，一个时间-层级记忆框架，它通过时间记忆树（Temporal Memory Tree, TMT，时间记忆树）组织对话，实现了从原始对话观察到逐步抽象的人设表征的系统性记忆整合。TiMem 具有三个核心特征：（1）通过 TMT 进行的时间-层级组织；（2）语义引导的整合，能够在无需微调的情况下实现跨层级的记忆整合；（3）复杂度感知的记忆召回，在不同复杂度的查询中平衡精确度和效率。在一致的评估设置下，TiMem 在两个基准测试中都达到了最先进的准确率，在 LoCoMo 上达到 75.30%，在 LongMemEval-S 上达到 76.88%。它优于所有评估的基线模型，同时在 LoCoMo 上将召回记忆长度减少了 52.20%。流形分析表明，在 LoCoMo 上存在清晰的人设分离，在 LongMemEval-S 上离散度有所降低。总的来说，TiMem 将时间连续性视为对话智能体长程记忆的首要组织原则。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
                    "arxiv_id": "2601.02744",
                    "authors": "Hanqi Jiang, Junhao Chen, Yi Pan, Ling Chen, Weihang You, Yifan Zhou, Ruidong Zhang, Yohannes Abate, Tianming Liu",
                    "summary": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断 (第一步)**: 该论文的核心贡献是提出了一种名为 Synapse 的统一记忆架构，旨在解决 LLM 智能体在长期记忆中的“不连贯”问题。这属于对 LLM 智能体核心组件（记忆）的构建与改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型推理能力的提升。因此，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: 论文直接涉及 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它引入了基于认知科学的扩散激活机制来优化智能体的记忆检索，这直接提升了智能体处理复杂时序和多跳推理任务的能力，属于单智能体方向的关键技术突破。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 论文不涉及安全、对齐或可解释性问题。 *   **多模态**: 论文专注于文本记忆架构，不涉及视觉或多模态内容。 *   **图技术**: 尽管论文中提到了“动态图”和“图遍历”，但其核心目的是构建智能体的记忆系统，而非研究图神经网络（GNN）或知识图谱算法本身。图在这里是作为智能体内部存储和检索信息的机制，服务于智能体的功能，因此不应被排除。 4.  **综合结论**: 该论文通过改进智能体的记忆机制，直接增强了 LLM 智能体的自主性和任务处理能力，完全符合“单智能体”方向中关于“记忆”的研究焦点。",
                    "summary2": "本文旨在解决LLM智能体长期记忆中的上下文隔离问题。针对长对话中的复杂推理场景，我们提出了一种基于Spreading Activation的SYNAPSE架构，构建Unified Episodic-Semantic Graph并结合Lateral Inhibition实现动态检索。我们在LoCoMo benchmark上通过F1 Score和Token消耗验证了其有效性，显著提升了多跳推理精度并降低了95%的Token使用量。",
                    "summary_translation": "尽管 Large Language Models (LLMs，大型语言模型) 在 generalized reasoning (泛化推理) 方面表现出色，但标准的 retrieval-augmented approaches (检索增强方法) 未能解决 long-term agentic memory (长期智能体记忆) 的割裂特性。为弥合这一差距，我们提出了 Synapse (Synergistic Associative Processing Semantic Encoding，协同联想处理语义编码)，这是一种超越 static vector similarity (静态向量相似度) 的 unified memory architecture (统一记忆架构)。借鉴 cognitive science (认知科学)，Synapse 将记忆建模为一个 dynamic graph (动态图)，其中相关性源于 spreading activation (扩散激活) 而非 pre-computed links (预计算链接)。通过整合 lateral inhibition (侧抑制) 和 temporal decay (时间衰减)，该系统能够动态突显相关的 sub-graphs (子图)，同时过滤干扰信息。我们实施了一种 Triple Hybrid Retrieval strategy (三重混合检索策略)，该策略将 geometric embeddings (几何嵌入) 与 activation-based graph traversal (基于激活的图遍历) 相融合。在 LoCoMo benchmark (LoCoMo 基准测试) 上的全面评估表明，Synapse 在复杂的 temporal and multi-hop reasoning tasks (时序和多跳推理任务) 中显著优于 state-of-the-art methods (最先进方法)，为 \"Contextual Tunneling\" (上下文隧道) 问题提供了稳健的解决方案。我们的代码和数据将在论文录用后公开发布。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
                    "arxiv_id": "2601.02880",
                    "authors": "Abhishek HS, Pavan C Shekar, Arpit Jain, Ashwanth Krishnan",
                    "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心判断**: *   论文的核心贡献是提出了 **ReTreVal** 这一混合框架，旨在构建和改进 LLM 智能体的多步推理能力。 *   它不是简单的应用型论文，而是提出了一种新的方法论，结合了 Tree-of-Thoughts (ToT)、自我细化和 Reflexion 记忆机制。 *   它不属于非演化型应用，也不属于基础设施或基础模型推理能力的微调，而是专注于智能体的架构设计。 2.  **符合研究焦点**: *   **单智能体**: 论文深入探讨了智能体的核心能力，包括 **Planning** (通过构建推理树进行结构化探索)、**Memory** (Reflexion memory buffer 用于持久化存储见解) 和 **Self-Reflection** (self-critique and refinement)。 *   **自我演化**: 论文明确提到了 \"persistent learning across problems\"（跨问题的持久学习）和 \"cross-problem memory\"（跨问题记忆）。智能体通过存储成功路径和失败模式的见解，利用这些经验在后续问题中进行改进，这完全符合“通过经验、反思进行自我完善”的自我演化定义。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   虽然在数学和创意写作任务上进行了评估，但这只是验证框架有效性的实验场景，论文的核心在于框架本身的机制（验证、剪枝、记忆），而非特定领域的应用。 4.  **特殊规则处理**: *   根据“推理/规划”规则，该论文属于保留范畴。它不仅仅是提出一种新的 CoT 变体，而是构建了一个包含验证、记忆和树搜索的完整 **Agentic 框架**，与 ReAct 和 Reflexion 等经典 Agentic 方法进行对比和改进。 综上所述，该论文通过引入记忆机制和结构化探索，增强了 LLM 智能体的规划和自我演化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决大语言模型在复杂多步推理任务中的局限性。针对数学和创意写作场景，我们提出了一种名为 ReTreVal 的混合框架，该框架集成了 Tree-of-Thoughts 探索、Self-Refinement、LLM-based critique scoring 和 reflexion memory。我们在 500 个数学问题和创意写作任务上，基于 Qwen 2.5 7B 模型，通过准确性和 GPT-4o mini 评分验证了其有效性。",
                    "summary_translation": "多步推理仍然是大型语言模型面临的一个关键挑战，特别是在数学和创意写作等复杂领域。尽管包括 ReAct、Reflexion 和 Self-Refine 在内的近期方法通过迭代优化和反思改进了推理能力，但它们往往缺乏对替代解题路径的结构化探索以及跨问题的持久学习能力。我们提出了 ReTreVal (Reasoning Tree with Validation，带验证的推理树)，这是一个集成了思维树探索、自我优化、基于 LLM 的批评评分以及反思记忆的混合框架，旨在实现有界且经过验证的多步推理。ReTreVal 构建了一个具有基于问题复杂度的自适应深度的结构化推理树，其中每个节点都在明确的 LLM 生成反馈指导下，经历迭代的自我批评和优化。双重验证机制评估每个节点的推理质量、连贯性和正确性，同时将来自成功推理路径的见解和失败模式持久地存储在反思记忆缓冲区中，从而实现跨问题学习。基于批评的剪枝策略在每一层仅保留得分最高的 top-k 个节点，在控制计算成本的同时保留了高质量的解题路径。我们使用 Qwen 2.5 7B 作为底层 LLM，在 500 个数学问题和创意写作任务上对 ReTreVal 与 ReAct、Reflexion 和 Self-Refine 进行了评估，结果表明 ReTreVal 通过结合结构化探索、批评驱动的优化以及跨问题记忆，始终优于现有方法，使其在需要探索性推理、严格验证和知识迁移的任务中尤为有效。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
                    "arxiv_id": "2601.02997",
                    "authors": "Waleed Khalid, Dmitry Ignatov, Radu Timofte",
                    "summary": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“Agentic AI”的核心研究范畴。 1.  **核心判断（符合自我演化机制）**： 论文的核心贡献在于构建了一个“闭环合成框架”，让LLM能够通过环境反馈（执行结果和性能评估）进行自我完善。论文详细描述了LLM经过22个监督微调周期的演化过程，通过迭代微调，模型内化了经验性的架构先验，从而提升了生成质量和性能。这完全符合“自我演化”中定义的“智能体通过经验、反思或环境反馈进行自我完善和迭代”。 2.  **符合Agentic AI特征**： 论文将LLM定位为一个“自主的、性能驱动的神经设计者”。它不仅仅是生成代码，而是通过“验证-评估-过滤-更新”的循环，展示了智能体利用工具（代码执行）和反馈机制来优化自身输出的能力，这属于Agentic AI的高级形态。 3.  **特殊情况处理（自我演化的应用）**： 虽然论文的应用场景是“神经架构设计”（NAS），属于特定领域的应用，但根据第四步的规则，只要论文的核心是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。本文的重点在于证明LLM如何通过反馈循环“从记忆走向创造”，其演化机制具有通用性和研究价值，而非单纯的应用落地。 综上所述，该论文在自我演化和智能体框架构建上具有显著贡献，符合研究课题要求。",
                    "summary2": "本文旨在探索LLM能否通过迭代微调成为自主的神经网络架构设计师。针对CIFAR-10图像分类任务，我们提出了一种基于闭环合成框架的方法，结合了MinHash-Jaccard新颖性过滤和低保真性能信号（单轮准确率）。在22个微调周期中，我们使用DeepSeek-Coder-7B-Instruct-v1.5模型，通过有效生成率、首轮准确率和结构新颖性等指标验证了其有效性。结果显示模型性能显著提升，且保持了结构多样性。",
                    "summary_translation": "大型语言模型在程序合成方面表现出色，但其在自主导航神经架构设计——即在句法可靠性、性能和结构新颖性之间取得平衡——方面的能力仍有待探索。我们通过将面向代码的LLM置于闭环合成框架中来解决这一问题，并分析了其在22个监督微调周期中的演变过程。该模型合成PyTorch卷积网络，这些网络经过验证，通过低保真性能信号（单轮准确率）进行评估，并使用MinHash-Jaccard标准进行过滤以防止结构冗余。高性能且新颖的架构被转换为提示-代码对，用于通过参数高效的LoRA适配进行迭代微调，该过程初始化自LEMUR数据集。在各个周期中，LLM内化了经验性架构先验，成为了一个鲁棒生成器。有效生成率稳定在50.6%（峰值达到74.5%），而平均首轮准确率从28.06%上升到50.99%，准确率超过40%的候选者比例从2.04%增长到96.81%。分析证实，该模型超越了复制现有模式的阶段，合成了455个原始语料库中不存在的高性能架构。通过将代码合成建立在执行反馈的基础上，这项工作提供了一个可扩展蓝图，用于将随机生成器转化为自主的、性能驱动的神经设计器，确立了LLM能够内化经验性的、非文本奖励从而超越其训练数据的结论。",
                    "inspiration_trace": "基于论文《From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures》，以下是对作者核心方法论产出过程的逻辑推演：\n\n### 第一阶段：宏观观察与问题定位\n**逻辑起点：LLM的代码能力与NAS的高昂成本之间的矛盾。**\n1.  **观察现状**：传统神经架构搜索（NAS）虽然能自动化设计网络，但依赖强化学习或进化算法，计算成本极高（通常需要训练大量模型）。\n2.  **技术契机**：大型语言模型（LLM）在代码合成方面表现出色，能够生成复杂的PyTorch代码。\n3.  **提出核心问题**：现有的LLM辅助NAS研究多将LLM视为静态的“代码生成器”，仅关注最终搜索结果的精度。作者试图转换视角——**如果将LLM视为一个可进化的“智能体”，通过自我迭代，它能否从单纯的代码生成者转变为具备设计直觉的“架构设计师”？**\n\n### 第二阶段：假设形成与关键挑战\n**逻辑推演：从“死记硬背”到“举一反三”的可能性。**\n1.  **核心假设**：如果让LLM在一个闭环中不断生成架构、接收反馈并基于自身的高质量产出进行微调，它将逐渐内化出一种“经验性的架构先验”，而不仅仅是记忆训练数据中的代码片段。\n2.  **面临的挑战**：\n    *   **可靠性**：生成的代码往往无法运行（语法错误、维度不匹配）。\n    *   **性能评估**：如果对每个生成的模型都进行完整训练，计算成本将不可接受。\n    *   **模式崩塌**：模型可能会陷入局部最优，反复生成相似的“高分”架构，失去探索新结构的能力。\n\n### 第三阶段：策略性简化与代理指标设计\n**逻辑转折：为了可行性，必须降低评估成本。**\n1.  **引入低保真代理**：为了解决计算成本问题，作者决定不追求模型的最终收敛精度，而是采用**“单轮训练后的准确率”**作为性能代理。\n    *   *思考逻辑*：一个优秀的架构通常在训练初期就能快速学习。这个指标既便宜又能反映架构的“学习潜力”。\n2.  **定义目标函数**：将优化目标从“最终精度”转化为“初始学习速度”与“语法有效性”的结合。\n\n### 第四阶段：闭环机制与多样性约束\n**逻辑构建：如何确保“进化”而非“退化”？**\n1.  **构建闭环**：设计了一个“生成-评估-筛选-微调”的迭代循环。\n    *   *生成*：LLM产出PyTorch代码。\n    *   *评估*：检查代码可执行性，并跑一轮训练获取代理分数。\n    *   *筛选*：这是关键步骤，必须同时满足三个条件：**代码可运行**、**代理分数达标**、**结构新颖**。\n2.  **引入新颖性过滤**：为了防止模型陷入重复生成已知好架构的陷阱（即过拟合），作者引入了**MinHash-Jaccard相似度**作为去重机制。\n    *   *思考逻辑*：只有那些“既好用又没见过”的架构，才有资格被加入下一轮的微调数据集。这迫使LLM不断探索设计空间的新区域，而不是在舒适区里打转。\n\n### 第五阶段：验证与结论\n**逻辑闭环：从假设到现实的映射。**\n1.  **实验验证**：通过22个周期的迭代，观察LLM的行为变化。\n2.  **结果解读**：\n    *   **可靠性提升**：有效生成率从低位稳定在50%以上，说明LLM学会了如何写“能跑的代码”。\n    *   **性能提升**：单轮准确率显著提高，说明LLM学会了什么样的结构更容易学习。\n    *   **创造力涌现**：发现了大量不在原始数据集中的高性能架构，证明了模型超越了简单的记忆，具备了设计能力。\n\n**总结：**\n作者的思考路径是从**利用LLM写代码**（工具属性），进化到**利用反馈机制训练LLM**（进化属性），最后通过**约束多样性**（探索属性），成功将一个通用的代码模型重塑为一个专用的、具备创造力的神经网络设计专家。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
                    "arxiv_id": "2601.02695",
                    "authors": "Guibin Zhang, Haiyang Yu, Kaiming Yang, Bingli Wu, Fei Huang, Yongbin Li, Shuicheng Yan",
                    "summary": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”方向。 1.  **核心贡献符合第一步判断**: 论文的核心贡献是提出了 **EvoRoute**，这是一种**自我演化**的模型路由范式。它不是将现有的智能体简单应用到某个垂直领域（非演化型应用），也不是单纯的基础设施优化，而是针对LLM智能体系统本身提出了一种新的架构和演化机制，旨在解决智能体系统中的性能、成本和延迟之间的权衡问题。 2.  **精准命中“自我演化”与“Agentic AI”焦点**: *   **自我演化**: 论文明确提到这是一个“self-evolving”范式，并且详细描述了其机制：利用不断扩大的先验经验知识库，通过环境反馈持续完善其自身的选择策略。这完全符合筛选标准中关于“智能体通过经验、反思或环境反馈进行自我完善和迭代”的定义。 *   **Agentic AI**: 论文的研究对象是“Complex agentic AI systems”，涉及LLM、工具和记忆模块的协调集成，这直接对应了研究课题中的LLM智能体构建。 3.  **符合正面指标**: 论文包含了大量核心关键词，如 `Self-Evolving`、`Agentic AI`、`LLM-based Agents`、`Iterative Improvement`（通过反馈持续完善）以及 `Tool and memory modules`。 4.  **排除标准检查**: 论文不涉及安全对齐、多模态视觉核心研究或图技术。虽然它关注了成本和延迟（通常属于基础设施范畴），但其解决手段是通过智能体的自我演化策略来实现的，因此属于智能体架构的改进，而非单纯的底层硬件或部署优化。 综上所述，该论文提出了一种新的智能体自我演化机制，属于构建和改进LLM智能体的前沿研究，应予以保留。",
                    "summary2": "本文旨在解决Agent System Trilemma，即性能、成本与效率之间的权衡问题。针对复杂的多轮LLM Agent系统，我们提出了一种名为EvoRoute的自进化模型路由范式，利用多方面检索和帕累托最优过滤动态选择LLM。我们在GAIA和BrowseComp+等基准上通过性能、成本和延迟指标验证了其有效性，在维持高性能的同时将成本降低了80%，延迟降低了70%。",
                    "summary_translation": "由大语言模型、工具和记忆模块的协调集成所驱动的复杂智能体 AI 系统，在复杂的多轮任务中展现出了卓越的能力。然而，这一成就伴随着高昂的经济成本和严重的延迟，从而揭示了一个关键但尚未被充分探索的权衡问题。我们将这一挑战形式化为 **Agent System Trilemma**（智能体系统三难困境）：即在实现最先进的性能、最小化货币成本以及确保快速任务完成这三者之间存在的内在张力。为了打破这一三难困境，我们提出了 EvoRoute，这是一种超越静态、预定义模型分配的自进化模型路由范式。EvoRoute 利用不断扩展的先验经验知识库，在每一步动态选择 Pareto-optimal（帕累托最优）的 LLM backbones（大语言模型骨干网络），在准确性、效率和资源使用之间取得平衡，同时通过环境反馈持续优化其自身的选择策略。在 GAIA 和 BrowseComp+ 等具有挑战性的智能体基准上进行的实验表明，当将 EvoRoute 集成到现成的智能体系统中时，它不仅保持甚至提升了系统性能，还将执行成本降低了高达 80%，延迟降低了超过 70%。",
                    "inspiration_trace": ""
                },
                {
                    "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
                    "arxiv_id": "2601.03204",
                    "authors": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
                    "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **InfiAgent**，这是一个用于通用自主智能体的**新框架**。 *   它解决的是 LLM 智能体在长视界任务中面临的上下文增长和误差累积问题，属于**构建和改进 LLM 智能体**的方法论研究。 *   它不是将现有智能体简单应用到特定领域（如医疗、金融），而是提出了底层的架构改进（状态外部化），因此不属于“非演化型应用”。 *   它关注的是智能体的系统架构和状态管理，而非模型的基础设施或硬件加速。 2.  **正面指标匹配（第二步）：** *   **核心范式**：明确属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文重点解决了智能体的 **`Memory`**（通过文件中心的状态抽象外部化持久状态）和 **`Planning`**（在长视界任务中保持推理稳定性）能力。这是单智能体研究中的关键子方向。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态或图技术，未触发任何排除标准。 4.  **特殊情况处理（第四步）：** *   论文关于长视界任务中的推理稳定性，属于智能体如何在复杂任务中进行规划和多步推理的范畴，符合保留条件。 **总结**：该论文通过引入显式的状态外部化机制，改进了 LLM 智能体处理长视界任务的能力，是对智能体架构和记忆机制的重要创新，直接契合“单智能体”方向中的规划与记忆研究焦点。",
                    "summary2": "本文旨在解决LLM agents在长视界任务中因上下文无界增长导致的稳定性问题。针对长视界推理场景，我们提出了一种InfiAgent框架，采用File-Centric State Abstraction将持久状态外部化，并结合分层代理架构保持上下文有界。在DeepResearch benchmark和80篇论文综述任务上，通过综合评分和覆盖率指标验证了其有效性，证明其能以20B开源模型达到与大型专有系统相当的性能。",
                    "summary_translation": "LLM agents (大语言模型智能体) 虽然具备推理和使用工具的能力，但在长视界任务中，往往因上下文的无界增长和误差累积而失效。常见的解决方案，如上下文压缩或 retrieval-augmented prompting (检索增强提示)，往往在信息保真度与推理稳定性之间引入了权衡。我们提出了 InfiAgent，这是一个通用框架，通过将持久状态外部化到 file-centric state abstraction (以文件为中心的状态抽象) 中，无论任务持续时间长短，都能将智能体的推理上下文严格限制在固定范围内。在每一步中，智能体通过结合 workspace state snapshot (工作区状态快照) 和近期操作的固定窗口来重构上下文。在 DeepResearch 数据集和一项包含 80 篇论文的文献综述任务上的实验表明，无需进行 task-specific fine-tuning (针对特定任务的微调)，配备 20B 开源模型的 InfiAgent 的性能可与更大的专有系统相媲美，并且相比 context-centric baselines (以上下文为中心的基线方法)，其长视界覆盖率显著更高。这些结果验证了 explicit state externalization (显式状态外部化) 作为构建稳定长视界智能体的实用基础的可行性。\n\nGithub 代码库：https://github.com/ChenglinPoly/infiAgent",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-05": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
                    "arxiv_id": "2601.01569",
                    "authors": "Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sihui Han, Bo An, Yike Guo, Jun Song",
                    "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断**: *   论文的核心贡献是提出了 **CaveAgent**，这是一个新的 **LLM智能体框架**。它旨在解决现有智能体系统在处理长时程任务时面临的上下文漂移和脆弱依赖问题。 *   这不是一篇将现有智能体简单应用到特定领域的应用论文，而是对智能体底层架构和运行机制的改进。 *   它不属于基础设施优化或非Agentic的基础推理研究。 2.  **正面指标匹配**: *   **Agentic AI / LLM-based Agents**: 论文明确致力于改进基于LLM的智能体系统。 *   **Memory (记忆)**: 论文的核心创新点之一是 **Stateful Runtime Management**（有状态运行时管理）。通过注入、操作和检索持久的Python对象（如DataFrames），它构建了一个高保真的 **外部记忆** 机制，以消除上下文漂移和灾难性遗忘。这直接对应了单智能体研究中的“记忆”能力。 *   **Tool Use / Tool Augmentation**: 框架利用代码生成作为工具，通过Python Runtime流来执行任务，超越了传统的JSON函数调用，属于工具使用能力的增强。 *   **Planning (规划)**: 论文提到利用代码生成高效解决相互依赖的子任务（如循环、条件判断），这涉及智能体在复杂任务中的多步规划和执行能力。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 **结论**: 该论文通过引入双流上下文架构和有状态运行时管理，显著增强了LLM智能体的记忆保持和任务执行能力，是对单智能体架构的重要改进，完全符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决传统基于JSON的LLM Agent在长周期任务中面临的上下文漂移和效率低下问题。针对复杂的工具调用和数据处理场景，我们提出了一种名为CaveAgent的框架，采用双流上下文架构和有状态运行时管理，实现对象级交互。我们在Tau 2-bench、BFCL及数据密集型任务上通过成功率和Token消耗验证了其有效性，显著提升了任务表现并降低了成本。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
                    "arxiv_id": "2601.01885",
                    "authors": "Yi Yu, Liuyi Yao, Yuexiang Xie, Qingquan Tan, Jiaqi Feng, Yaliang Li, Libing Wu",
                    "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 \"Agentic Memory (AgeMem)\" 的新框架，旨在解决LLM智能体在长程推理中的记忆管理问题。这属于对LLM智能体内部机制的构建和改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型能力的提升。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确属于 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**：论文的核心焦点是 `Memory`（记忆），这是智能体的关键能力之一。同时，它将记忆操作（存储、检索、更新等）定义为 `Tool Use / Tool Augmentation`（基于工具的动作），使智能体能自主决策。 *   **演化机制**：论文提出了一种三阶段渐进式强化学习（RL）策略来训练智能体的记忆管理策略，这涉及智能体通过环境反馈进行 `Self-Improvement`（自我完善）和策略优化。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉核心或图技术，因此不在排除之列。 综上所述，该论文通过引入统一的记忆管理框架和强化学习训练方法，显著增强了LLM智能体的自主性和适应性，是对Agentic AI核心能力的直接贡献，因此予以保留。",
                    "summary2": "本文旨在解决LLM智能体在长视界推理中因有限上下文窗口导致的记忆管理限制问题。针对LTM和STM分离且依赖启发式规则的现状，我们提出了一种名为Agentic Memory (AgeMem)的统一框架，通过基于工具的动作接口实现端到端的记忆管理，并采用三阶段渐进式RL策略进行训练。在ALFWorld、SciWorld等五个长视界基准测试上，通过Success Rate和Memory Quality等指标验证了其有效性，显著提升了任务性能和记忆质量。",
                    "summary_translation": "大语言模型智能体由于上下文窗口有限，在长程推理方面面临根本性局限，这使得有效的记忆管理变得至关重要。现有方法通常将长期记忆和短期记忆视为独立的组件进行处理，并依赖于启发式方法或辅助控制器，这限制了系统的适应性和端到端优化能力。在本文中，我们提出了 Agentic Memory (AgeMem，智能体记忆)，这是一个将长期记忆和短期记忆管理直接整合到智能体策略中的统一框架。AgeMem 将记忆操作呈现为基于工具的动作，使大语言模型智能体能够自主决定存储、检索、更新、总结或丢弃何种信息以及何时执行这些操作。为了训练这种统一行为，我们提出了一种三阶段渐进式强化学习策略，并设计了分步 GRPO，以解决由记忆操作引起的奖励稀疏和不连续问题。在五个长程基准测试上的实验表明，AgeMem 在多种大语言模型骨干网络上始终优于强大的记忆增强基线模型，实现了更优的任务性能、更高质量的长期记忆以及更高效的上下文利用率。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-02": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "An Agentic Framework for Neuro-Symbolic Programming",
                    "arxiv_id": "2601.00743",
                    "authors": "Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi",
                    "summary": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合 Agentic AI 范畴**：论文明确提出了一个名为 \"AgenticDomiKnowS (ADS)\" 的框架，其核心在于构建一个基于 LLM 的智能体工作流。这完全符合第一步中关于“构建 LLM 智能体（Agentic LLM）的方法论或新框架”的保留标准。 2.  **具备单智能体核心能力**：论文描述的智能体工作流涉及将自由形式的任务描述分解并转化为具体的程序组件，这体现了智能体的**规划**能力；同时，工作流包含“创建和测试”组件的过程，体现了**工具使用**和执行能力。这些均属于第二步正面指标中的“单智能体”核心关注点。 3.  **不属于排除项**： *   虽然论文涉及“神经符号编程”这一特定领域，但其核心贡献并非仅仅是应用 LLM 解决该领域问题，而是提出了一种通用的智能体框架来辅助编程，因此不属于“非演化型应用”。 *   尽管神经符号编程可能涉及逻辑或图结构，但论文的研究焦点是智能体框架本身，而非图神经网络或知识图谱算法的改进，因此不触犯“图”相关的排除标准。 *   论文不涉及安全、对齐或多模态视觉等排除领域。 综上所述，该论文的核心在于提出一个新的 Agentic 框架来解决复杂的编程构建任务，符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在解决神经符号编程门槛高、编写困难的问题。针对自然语言任务描述，我们提出了一种 AgenticDomiKnowS (ADS) 智能体框架，采用多智能体协作与RAG检索的分阶段工作流生成代码。在涵盖NLP、视觉和CSP的12个数据集及人类评估实验中，通过图生成正确率和开发时间验证了其有效性，显著提升了开发效率。",
                    "summary_translation": "将 symbolic constraints（符号约束）集成到 deep learning models（深度学习模型）中，能够提升模型的鲁棒性、可解释性以及数据效率。然而，这一过程依然耗时且充满挑战。现有框架（如 DomiKnowS）通过提供 high-level declarative programming interface（高级声明式编程接口）辅助这一集成过程，但它们仍预设用户已熟练掌握该库的特定语法。为消除这一依赖，我们提出了 AgenticDomiKnowS (ADS)。ADS 利用一种 agentic workflow（智能体工作流），将 free-form task descriptions（自由形式的任务描述）转化为完整的 DomiKnowS 程序，该工作流会对每个 DomiKnowS component（组件）分别进行创建和测试。该工作流支持可选的 human-in-the-loop intervention（人在回路干预），使熟悉 DomiKnowS 的用户能够优化 intermediate outputs（中间输出）。我们展示了 ADS 如何使经验丰富的 DomiKnowS 用户及非用户均能快速构建 neuro-symbolic programs（神经符号程序），将开发时间从数小时缩短至 10-15 分钟。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-31": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
                    "arxiv_id": "2512.24613",
                    "authors": "Zheyu Shi, Dong Qiu, Shanlong Yu",
                    "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心贡献符合多智能体系统 (Multi-Agent)**: 论文的核心是提出了一种“面向群体商议的多智能体对话模型”。它构建了一个包含三个不同角色的智能体架构：意见生成智能体、证据验证智能体和一致性仲裁智能体。这直接对应了我筛选标准中的“多智能体”方向，特别是智能体间的协作、通信和角色分工。 2.  **包含自我演化机制**: 论文中明确引入了“自我博弈机制”来扩展多路径推理轨迹，并设计了复合奖励函数结合改进的近端策略优化（PPO）策略进行协作训练。这种通过反馈和强化学习进行迭代优化的过程，符合“自我演化”中关于智能体通过环境反馈进行自我完善和迭代的标准。 3.  **具备智能体关键能力**: 论文模型包含检索增强模块，体现了“工具使用”能力；同时，其通过多智能体商议解决复杂推理任务的过程，涉及了复杂的“规划”和“多步推理”。 4.  **排除标准检查**: 该论文不涉及安全对齐、多模态视觉或图技术等排除项。虽然论文在HotpotQA等数据集上进行实验，但其核心贡献在于提出了新的智能体框架和训练机制，而非单纯将现有模型应用于特定领域。 综上所述，该论文在构建多智能体协作框架及引入自我博弈演化机制方面做出了实质性贡献，符合筛选要求。",
                    "summary2": "本文旨在优化单一语言模型在复杂推理任务中的局限性。针对多跳问答等场景，我们提出了一种Group Deliberation Multi-Agent Dialogue Model，构建“Generation-Verification-Integration”三级角色分工架构，引入Self-game mechanism和Retrieval Enhancement Module。在HotpotQA、2WikiMultihopQA和MeetingBank数据集上，通过Multi-hop reasoning accuracy和Consistency index验证了其有效性，显著提升了推理准确性与一致性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory",
                    "arxiv_id": "2512.24684",
                    "authors": "Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu",
                    "summary": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**：论文明确提出了 R-Debater，这是一个用于生成多轮辩论的 **\"agentic framework\"**（智能体框架）。其核心贡献在于构建了一个结合检索增强和论证记忆的智能体架构，而非仅仅将现有模型作为工具简单应用于辩论领域。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合研究焦点**：该研究完全符合 **单智能体** 方向。 *   **记忆**：论文的核心创新点是 \"Argumentative Memory\"（论证记忆），使智能体能够回忆和调整先前的论点。 *   **工具使用**：集成了辩论知识库进行检索，作为智能体获取证据的工具。 *   **规划**：摘要中提到 \"structured planning\"（结构化规划），用于在多轮对话中保持立场一致性和连贯性。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然应用场景是辩论，但其重点在于智能体如何通过记忆和规划机制来完成任务，属于智能体能力的构建，而非单纯的非演化型应用。 综上所述，该论文提出了新的智能体框架并深入探讨了记忆与规划机制，符合筛选要求。",
                    "summary2": "本文旨在解决LLM在多轮辩论生成中缺乏事实依据、立场一致性差及跨轮次连贯性弱的问题。针对ORCHID数据集上的多轮辩论场景，我们提出了一种名为R-Debater的框架，该框架集成了检索增强生成与基于角色的智能体，通过检索和重构论证记忆来生成高质量辩论。我们在ORCHID数据集上通过InspireScore和Debatrix指标验证了其有效性，实验表明其在逻辑连贯性和事实准确性上显著优于强基线模型。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-29": [
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
                    "arxiv_id": "2512.22733",
                    "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
                    "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心方法论研究。 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **FoldAct** 这一新框架，旨在解决 **Long-horizon search agents**（长视界搜索智能体）在强化学习训练中面临的上下文无限增长和非平稳观测分布问题。 *   这属于 **构建和改进 LLM 智能体** 的范畴。它关注的是智能体如何处理记忆（上下文折叠）以及如何在长任务中保持稳定性和效率，这是智能体架构和训练机制的关键部分，而非简单的应用或基础设施优化。 2.  **正面指标匹配（第二步）：** *   **核心范式**：论文明确针对 **LLM-based Agents**（特别是搜索智能体）。 *   **智能体能力**：论文重点解决了智能体的 **Memory**（通过上下文折叠压缩交互历史）和 **Planning/Search**（长视界搜索）能力。它提出的“分离损失计算”和“全上下文一致性损失”是为了优化智能体的训练过程，使其具备更好的长程任务处理能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了计算效率（5.19倍加速），但这属于算法层面的优化（选择性片段训练），而非硬件或部署基础设施的研究。 4.  **特殊情况处理（第四步）：** *   论文涉及 **Reasoning/Planning**（长视界搜索），且其核心是改进智能体在执行过程中的状态管理和训练稳定性，符合保留条件。 综上所述，FoldAct 提出了一种改进智能体记忆机制和训练稳定性的新方法，直接贡献于 LLM 智能体的构建与优化，因此被保留。",
                    "summary2": "本文旨在解决长视距强化学习中上下文折叠导致的非平稳观测分布及训练不稳定问题。针对长视距搜索代理场景，我们提出了一种FoldAct框架，通过分离损失计算、全上下文一致性损失和选择性片段训练来优化上下文折叠。在Local RAG和Web Search基准上，通过F1、EM及Pass@1等指标验证了其有效性，并实现了5.19倍的训练加速。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Memento-II: Learning by Stateful Reflective Memory",
                    "arxiv_id": "2512.22716",
                    "authors": "Jun Wang",
                    "summary": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的核心研究。 1.  **核心贡献符合第一步判断**： 论文的核心贡献是提出了一种名为“Memento-II”的理论框架，旨在解决LLM智能体的持续学习和经验学习问题。这直接对应了您筛选条件中的“构建、改进或演化 LLM智能体”以及“自我演化”的方法论。它不是将智能体作为工具应用到特定领域，而是研究智能体本身如何学习和适应的底层机制。 2.  **高度匹配正面指标**： *   **自我演化**：论文明确提出了“continual and experiential learning”（持续和经验学习），并强调智能体可以在“without back propagation or model fine tuning”（无需反向传播或模型微调）的情况下通过交互进行适应。这是典型的自我演化特征，即通过经验而非参数更新来迭代改进。 *   **智能体能力**：论文重点研究了“Memory”（情景记忆）和“Self-Reflection”（反思），将其作为智能体适应环境的关键机制。这直接命中了您关注的核心能力列表。 *   **核心范式**：该研究属于“LLM-based Agents”和“Agentic AI”范畴，特别是关于“Retrieval-based language model agents”（基于检索的语言模型智能体）。 3.  **不涉及排除标准**： 论文主要关注智能体的学习机制和理论框架，不涉及安全对齐、多模态视觉处理或图神经网络等排除领域。 综上所述，该论文通过引入“有状态反思决策过程”和结合强化学习，为LLM智能体在不更新模型参数的情况下实现自我完善和持续适应提供了理论基础，精准契合您关于“自我演化”和“单智能体”的研究目标。",
                    "summary2": "本文旨在解决LLM智能体无需微调即可实现持续学习的问题。针对开放式长视野任务，我们提出了一种Stateful Reflective Decision Process (SRDP) 框架，通过Read-Write Reflective Learning机制将 episodic memory 与 soft policy iteration 相结合。我们在软件测试、自动化数据科学和深度研究代理等场景上，通过理论收敛性证明和实证表现验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
                    "arxiv_id": "2512.22322",
                    "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
                    "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心贡献符合构建与改进 LLM 智能体的目标**： 论文提出了 **SmartSnap** 这一新范式，并定义了一种新型的 **Self-Verifying Agent（自验证智能体）**。这不仅仅是应用现有智能体去解决特定问题，而是对智能体架构和机制的实质性改进。它解决了 Agentic RL 中任务验证的瓶颈问题，将验证从“被动、事后”转变为“主动、原位”。 2.  **符合“单智能体”与“自我反思”的核心关注点**： 论文的核心在于赋予智能体双重使命：完成任务 + 证明完成。这直接对应筛选标准中的 **自我反思** 和 **自我修正** 能力。智能体通过主动寻找证据来验证自身行为，这是一种高级的元认知能力，属于 Agentic AI 的关键能力范畴。 3.  **不属于排除项**： *   虽然论文在 GUI 和移动任务上进行了实验，但其核心贡献是通用的“自验证”机制，而非针对特定领域的应用解决方案，因此不属于“非演化型应用”。 *   虽然涉及 GUI，但视觉仅作为智能体感知环境的一部分，并非研究视觉模型本身，因此不违反多模态排除规则。 *   论文关注的是智能体的任务完成验证机制，而非安全对齐或基础设施优化。 综上所述，该论文通过提出新的智能体框架来增强智能体的自主性和可靠性，是关于 LLM 智能体构建与演化的高质量研究。",
                    "summary2": "本文旨在解决Agentic RL中被动验证成本高且不可靠的瓶颈。针对复杂的GUI任务，我们提出了一种SmartSnap范式，引入了基于3C原则的Self-Verifying Agent，使其主动收集并提交关键快照证据。我们在AndroidLab基准上通过Success Rate等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-26": [
        {
            "name": "Computation and Language",
            "count": 2,
            "papers": [
                {
                    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
                    "arxiv_id": "2512.21919",
                    "authors": "KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He",
                    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合第一步（核心判断）**： *   论文的核心贡献是提出了 **SWE-RM**，一种用于软件工程智能体的奖励模型。 *   它不是简单地将智能体作为工具应用，而是致力于**改进智能体本身**的反馈机制。论文旨在解决现有基于执行的反馈（如单元测试）存在的稀疏性和依赖性问题，提出了一种免执行的反馈机制。 2.  **高度契合第二步（正面指标）与第三步（自我演化）**： *   **自我演化机制**：论文明确指出该模型旨在支持 **Reinforcement Learning (RL)** 和 **Test-time scaling (TTS)**。RL 是智能体通过环境反馈进行自我完善和迭代的核心技术，属于典型的“自我演化”范畴。TTS 则涉及智能体在推理时的搜索和规划。 *   **智能体能力**：论文讨论了如何通过奖励模型提供更细粒度的信号，帮助智能体进行 **Self-Correction**（自我修正）和 **Trajectory Selection**（轨迹选择，即规划的一部分）。 3.  **符合第四步（特殊和模糊情况）**： *   虽然论文的应用场景是软件工程（SWE），但其核心在于提出一种新的“反馈/奖励机制”，这种机制是智能体实现自我演化和改进的关键组件。根据第四步中关于“自我演化的应用”的例外规则，只要核心是提出新的演化/改进机制，即使应用在特定领域，也应保留。 综上所述，SWE-RM 提供了一种让智能体通过更高质量的反馈进行自我学习和优化的新方法，直接贡献于 LLM 智能体的演化能力，因此予以保留。",
                    "summary2": "本文旨在解决软件工程代理中基于执行的反馈稀疏且不可靠的问题。针对SWE任务中的多轮轨迹，我们提出了一种名为SWE-RM的免执行奖励模型，该模型采用混合专家架构，并强调AUC和校准能力。我们在SWE-Bench Verified数据集上通过Resolve Rate、AUC和ECE验证了其有效性。实验表明，SWE-RM在测试时扩展和强化学习中均显著提升了代理性能，达到了开源模型中的SOTA水平。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management",
                    "arxiv_id": "2512.21567",
                    "authors": "Changzhi Sun, Xiangyu Chen, Jixiang Luo, Dell Zhang, Xuelong Li",
                    "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 DAM (Decision-theoretic Agent Memory) 的框架，用于解决 LLM 智能体中的“记忆管理”问题。这直接属于“构建、改进 LLM 智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或基础模型推理能力的提升。 2.  **命中核心关注点 (第二步)**: *   **单智能体能力**: 论文明确聚焦于智能体的 **Memory (记忆)** 机制。记忆是您列出的单智能体核心能力之一（规划、记忆、工具使用、自我反思）。 *   **Agentic AI**: 论文将记忆管理视为“不确定性下的序列决策问题”，这属于 Agentic AI 的核心方法论，旨在提升智能体在长期交互中的表现。 3.  **无排除项 (第三步)**: 论文不涉及安全与对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **综合结论**: 该论文致力于改进 LLM 智能体的关键组件（记忆），提出了新的决策理论框架来替代传统的启发式方法，从而提升智能体的长期交互能力。这完全符合您的研究课题中关于“单智能体”及其能力演化的方向。",
                    "summary2": "总结生成失败",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-24": [
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "Reflection-Driven Self-Optimization 6G Agentic AI RAN via Simulation-in-the-Loop Workflows",
                    "arxiv_id": "2512.20640",
                    "authors": "Yunhao Hu, Xinchen Lyu, Chenshan Ren, Keda Chen, Qimei Cui, Xiaofeng Tao",
                    "summary": "The escalating complexity of sixth-generation (6G) networks demands unprecedented levels of autonomy beyond the capabilities of traditional optimization-based and current AI-based resource management approaches. While agentic AI has emerged as a promising paradigm for autonomous RAN, current frameworks provide sophisticated reasoning capabilities but lack mechanisms for empirical validation and self-improvement. This article identifies simulation-in-the-loop validation as a critical enabler for truly autonomous networks, where AI agents can empirically verify decisions and learn from outcomes. We present the first reflection-driven self-optimization framework that integrates agentic AI with high-fidelity network simulation in a closed-loop architecture. Our system orchestrates four specialized agents, including scenario, solver, simulation, and reflector agents, working in concert to transform agentic AI into a self-correcting system capable of escaping local optima, recognizing implicit user intent, and adapting to dynamic network conditions. Extensive experiments validate significant performance improvements over non-agentic approaches: 17.1\\% higher throughput in interference optimization, 67\\% improved user QoS satisfaction through intent recognition, and 25\\% reduced resource utilization during low-traffic periods while maintaining service quality.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“多智能体”方向**： 论文的核心贡献是提出了一种“反思驱动的自我优化框架”，这直接对应研究焦点中的“自我演化”和“自我反思”。论文构建了一个包含四个专门智能体（场景、求解器、模拟、反思智能体）的闭环架构，旨在通过经验验证和反思机制实现系统的自我修正和迭代改进，这属于构建新的Agentic框架和方法论。 2.  **符合“自我演化的应用”例外规则**： 根据筛选标准第四步第2点，虽然论文的应用领域是特定的6G网络（RAN），但其核心在于提出了一种新的“自我演化”机制（Simulation-in-the-Loop + Reflection-driven），而不仅仅是将现有智能体作为工具应用。因此，符合“保留”的例外条件。 3.  **包含关键正面指标**： 论文明确涉及了 `Multi-Agent Systems`（多智能体协作）、`Self-Reflection`（反思智能体）、`Self-Correction`（自我修正）和 `Self-Optimization`（自我优化）等核心关键词。 综上所述，该论文在特定领域（6G）中验证了具有自我演化能力的多智能体框架，符合关于“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决6G网络中现有Agentic AI缺乏经验验证与自我改进机制的问题。针对多小区RAN场景，我们提出了一种Reflection-Driven Self-Optimization Framework，通过Simulation-in-the-Loop工作流协调Scenario、Solver、Simulation和Reflector四个Agent。我们在Sionna平台上通过吞吐量、QoS满意度和资源利用率等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-23": [
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
                    "arxiv_id": "2512.20111",
                    "authors": "Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr",
                    "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”和“自我演化”方向的交叉研究。具体判断依据如下： 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种名为 ABBEL 的新框架，旨在解决 LLM 智能体在长序列决策任务中的上下文管理问题。这属于构建和改进 LLM 智能体方法论的范畴，而非简单的应用或基础设施研究。 2.  **高度契合核心关注点 (第二步)**: *   **Agentic AI & 单智能体**: 论文明确研究 LLM Agents，涉及智能体如何通过“信念状态”来处理多步交互、环境观察和行动选择，这是典型的智能体规划与记忆机制研究。 *   **自我演化**: 论文不仅提出了框架，还引入了强化学习（RL）后训练机制来改进智能体生成信念和采取行动的能力。这种通过反馈（RL奖励）进行迭代优化和自我完善的过程，符合“自我演化”的定义。 3.  **不涉及排除项 (第三步)**: 论文主要关注智能体的架构优化和性能提升，不涉及安全对齐、多模态视觉核心研究或图神经网络等排除领域。 综上所述，ABBEL 提出了一种改进智能体记忆和决策机制的方法，并结合 RL 进行自我优化，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决长序列决策任务中上下文长度超出限制的问题。针对长视距交互场景，我们提出了一种基于自然语言信念瓶颈的ABBEL框架，通过维护简洁的信念状态替代完整交互历史，并利用强化学习进行后训练优化。我们在Wordle、ColBench等六个多步环境中通过Success Rate和Peak Token Usage验证了其有效性，实现了在保持高性能的同时显著降低内存使用。",
                    "summary_translation": "随着序列决策任务长度的增加，在上下文中保留完整的交互历史在计算上变得不可行。我们介绍了一个通用框架，用于 LLM（大语言模型）智能体通过多步交互保持简洁的上下文：Acting through Belief Bottlenecks Expressed in Language (ABBEL，基于语言表达的信念瓶颈行动)，以及利用 RL post-training（强化学习后训练）进一步改进 ABBEL 智能体的方法。ABBEL 用 belief state（信念状态）替换了冗长的多步交互历史，即关于任务相关未知因素已发现内容的自然语言摘要。在 ABBEL 框架下，智能体在每一步首先利用来自环境的最新观测更新 prior belief（先验信念）以形成 posterior belief（后验信念），然后仅使用后验信念来选择动作。我们在六个不同的多步环境中系统地评估了 ABBEL 下的 frontier models（前沿模型），发现 ABBEL 支持生成可解释的信念，同时在交互步骤中保持近乎恒定的内存使用。然而，bottleneck approaches（瓶颈方法）通常容易受到 error propagation（错误传播）的影响，我们观察到由于信念更新中的错误，这导致与 full context setting（全上下文设置）相比性能较差。因此，我们通过 reinforcement learning (RL，强化学习) 训练 LLM 在 ABBEL 框架内生成信念并基于信念行动。我们尝试了 belief grading（信念评分）以奖励更高质量的信念，以及 belief length penalties（信念长度惩罚）以奖励更压缩的信念。我们的实验证明了 RL 将 ABBEL 的性能提升至超越全上下文设置的能力，同时使用的内存少于 contemporaneous approaches（同期方法）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-22": [
        {
            "name": "Artificial Intelligence",
            "count": 2,
            "papers": [
                {
                    "title": "Sophia: A Persistent Agent Framework of Artificial Life",
                    "arxiv_id": "2512.18202",
                    "authors": "Mingyang Sun, Feng Hong, Weinan Zhang",
                    "summary": "The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a \"Persistent Agent\" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心判断（第一步）：** 论文的核心贡献是构建了一个名为 Sophia 的“持久智能体框架”，并提出了“System 3”这一新的架构层级。这属于构建和改进 LLM 智能体的方法论，而非将现有智能体简单应用于特定领域（如医疗、金融），因此符合保留条件。 2.  **正面指标匹配（第二步）：** *   **Agentic AI / 单智能体：** 论文详细探讨了智能体的规划、记忆（Narrative Memory）、自我建模以及长期适应性，这些都是单智能体研究的核心能力。 *   **自我演化：** 摘要中明确提到该框架包含“continuous self-improvement loop”（连续自我改进循环）和“long-horizon adaptation”（长期适应性），这直接对应了研究焦点中的“自我演化”机制。 *   **核心范式：** 论文涉及 `Self-Reflection`（通过 System 3 验证推理）、`Memory` 和 `Iterative Improvement`。 3.  **排除标准检查（第三步）：** 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此未被排除。 4.  **特殊处理（第四步）：** 论文虽然涉及推理，但其重点在于通过智能体架构（System 3）来实现持久性和自我驱动的推理过程，而非单纯优化 LLM 的基础 Token 预测能力或数学逻辑，因此属于 Agentic 的推理范畴，应予保留。 综上所述，该论文提出了一个新的智能体框架来解决智能体的身份持久性和自我完善问题，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有AI代理缺乏长期适应性和自我改进能力的问题。针对动态网络环境，我们提出了一种名为Sophia的持久化代理框架，该框架引入了包含元认知、心智理论等模块的System 3元认知层。在36小时的浏览器沙盒实验环境中，通过复杂任务成功率提升及重复操作推理步骤减少80%等指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Propose, Solve, Verify: Self-Play Through Formal Verification",
                    "arxiv_id": "2512.18160",
                    "authors": "Alex Wilf, Pranjal Aggarwal, Bryan Parno, Daniel Fried, Louis-Philippe Morency, Paul Pu Liang, Sean Welleck",
                    "summary": "Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“多智能体”的核心研究范畴。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 \"Propose, Solve, Verify\" (PSV) 框架。这是一个基于自我博弈的训练框架，旨在通过形式验证信号来训练模型。这不仅仅是将LLM作为工具应用，而是提出了一种新的**训练和演化方法论**。 2.  **符合核心关注点 (第二步)**: *   **自我演化**: 论文明确使用了 \"Self-Play\"（自我博弈）、\"Expert Iteration\"（专家迭代）和 \"Generational Evolution\"（隐含在迭代训练中）的概念。模型通过生成问题、解决问题和验证结果的循环，实现了性能的自我提升和迭代。 *   **多智能体**: 框架中包含两个核心角色——\"Proposer\"（提议者，负责生成合成问题）和 \"Solver\"（求解器，负责解决问题）。这两个角色通过交互和对抗/协作（生成难题 vs 解决难题）共同进化，符合多智能体系统的定义。 3.  **特殊与模糊情况处理 (第四步)**: 虽然论文的应用场景是代码生成，但其核心在于提出了一种新的“自我演化”机制（Self-Play through Formal Verification）。根据筛选标准第四步中的“自我演化的应用”规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。因此，尽管涉及代码领域，由于其核心贡献在于演化框架本身，应当保留。 综上所述，该论文聚焦于通过自我博弈和专家迭代机制实现LLM的自我演化，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决代码生成中自博弈奖励信号不可靠的问题。针对形式化验证代码生成场景，我们提出了一种名为PSV的自博弈框架，利用形式化验证器构建包含难度感知提议器的自博弈循环，并在Dafny2Verus、MBPP-Verified和HumanEval-Verified数据集上通过Pass@k指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 2,
            "papers": [
                {
                    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
                    "arxiv_id": "2512.19682",
                    "authors": "Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang",
                    "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **GenEnv** 这一新框架，旨在解决训练LLM智能体时数据成本高且静态的问题。其本质是构建一种**智能体与环境模拟器之间的协同演化机制**，这直接属于“构建、改进或演化 LLM智能体”的方法论研究，而非将现有智能体简单应用到特定领域。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文明确涉及 `LLM-based Agents` 和 `Self-Evolving`（特别是 `Co-Evolution` 协同演化）。 *   **演化机制**：GenEnv 建立了一个动态的课程策略，通过 `Co-Evolutionary Game`（协同演化博弈）让环境根据智能体的能力生成任务，这完全符合“自我演化”和“迭代改进”的研究焦点。 *   **智能体能力**：虽然侧重于训练数据生成，但其目的是提升智能体在 `ALFWorld`、`API-Bank` 等需要 `Planning`（规划）和 `Tool Use`（工具使用）的基准测试上的表现。 3.  **排除标准（无冲突）**： 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此不触犯任何排除规则。 4.  **特殊与模糊情况（符合）**： 论文提出的“协同演化”机制属于典型的自我演化范畴。尽管它在多个基准（如TravelPlanner）上进行了评估，但这些仅用于验证智能体能力的提升，论文的核心在于提出了一种新的**演化训练框架**，而非单纯的应用。 综上所述，该论文通过提出智能体与环境的协同演化框架来提升智能体能力，精准契合“LLM智能体及其演化”这一研究课题。",
                    "summary2": "本文旨在解决LLM智能体训练中真实交互数据成本高且静态分布导致的瓶颈。针对智能体训练场景，我们提出了一种名为GenEnv的难度对齐协同进化框架，通过$\\alpha$-Curriculum Reward引导环境模拟器动态生成适配智能体当前能力的任务。我们在API-Bank、ALFWorld等五个基准测试上通过成功率等指标验证了其有效性，该方法显著提升了7B基线模型性能，并展现出优于静态数据增强的高数据效率。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
                    "arxiv_id": "2512.18552",
                    "authors": "Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang",
                    "summary": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文提出了“Self-play SWE-RL (SSR)”这一新框架，其核心在于通过**自我对弈**和**强化学习**来训练软件智能体。这直接对应了研究课题中的“自我演化”方向，特别是智能体通过环境反馈（测试补丁）进行自我完善和迭代改进的能力。 2.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是软件工程，但根据筛选标准第四步，只要论文的核心贡献是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。本文的核心不在于单纯解决某个具体的编程问题，而在于提出了一种让智能体**自主收集经验**、**自我注入并修复Bug**的训练范式，这属于智能体演化机制的创新。 3.  **具备明确的Agentic特征**： 论文明确研究对象是“Software Agents”，并使用了“Agentic reinforcement learning”方法。智能体在沙盒环境中进行多步交互（注入Bug、修复Bug），体现了智能体的自主性和工具使用能力。 综上所述，该论文不仅涉及LLM智能体的构建，更重点解决了智能体如何通过自我对弈实现能力演化和自我提升，高度契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在突破软件智能体依赖人类知识数据的限制，迈向超智能系统。针对仅需沙箱化代码库的最小数据假设场景，我们提出了一种 Self-play SWE-RL (SSR) 方法，通过自我博弈让同一 LLM 智能体交替扮演 Bug 注入者和求解者以迭代生成和修复复杂 Bug。我们在 SWE-bench Verified 和 SWE-Bench Pro 上通过 Resolve rate 验证了其有效性，结果表明该方法能实现持续自我改进并优于人类数据基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
                    "arxiv_id": "2512.18950",
                    "authors": "Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili",
                    "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心判断**: *   论文提出了 **MACLA** 框架，其核心贡献在于构建了一种新的 **LLM智能体** 架构。该架构通过解耦推理与学习，利用外部分层程序记忆来增强智能体的能力。 *   这不是简单的应用型论文，而是提出了关于智能体如何存储记忆、如何从经验中学习以及如何自我完善的方法论。 2.  **符合核心关注点**: *   **单智能体**: 论文专注于提升单个智能体的能力，特别是 **Memory (记忆)** 机制的构建。它提出了“分层程序记忆”，这是智能体研究中的关键组件。 *   **自我演化**: 论文明确提到了“continually improving agents”（持续改进的智能体）。其核心机制包括从轨迹中提取可复用程序、通过 **Contrastive Refinement (对比精炼)** 来修正程序，以及利用贝叶斯选择跟踪可靠性。这完全符合“自我演化”中定义的“通过经验、反思或环境反馈进行自我完善和迭代”的标准。 *   **正面指标匹配**: 涉及 `Memory`, `Self-Correction` (通过对比精炼实现), `Iterative Improvement`。 3.  **排除标准检查**: *   论文不涉及安全对齐、多模态视觉核心研究或图技术。 *   虽然在 ALFWorld 等基准上测试，但这些是通用的智能体决策基准，而非生物、医疗等特定垂直领域的非演化型应用。 综上所述，该论文致力于解决LLM智能体的记忆构建和自我迭代学习问题，属于构建和演化LLM智能体的前沿研究，应予以保留。",
                    "summary2": "本文旨在解决LLM智能体在复杂任务中缺乏高效、可解释且持续适应的学习机制问题。针对多步交互任务场景，我们提出了一种名为MACLA的框架，通过冻结LLM并结合外部分层程序化记忆，利用贝叶斯选择和对比细化机制实现推理与学习的解耦。在ALFWorld、WebShop等四个基准测试上，通过任务成功率和训练效率等指标验证了其有效性，实现了78.1%的平均性能，且构建速度比SOTA快2800倍。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
                    "arxiv_id": "2512.18622",
                    "authors": "Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen",
                    "summary": "Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 \"Multi-Agent\" 方向**：论文的核心是提出了一种名为 MATS 的新型框架，明确使用了 \"Multi-agent mechanism\"（多智能体机制）。该机制通过为辅助智能体分配专门角色来减少工作量并促进交互，这直接对应了研究焦点中的“多智能体”及其子方向（协作、通信）。 2.  **包含 \"Self-Evolving\" 机制**：论文提出了一种基于强化学习的训练方案，利用 \"execution feedback\"（执行反馈）来对齐智能体。这符合筛选标准中的“自我演化”定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。这种通过反馈循环来改进智能体性能的方法是 Agentic AI 的关键特征。 3.  **属于构建新框架而非单纯应用**：虽然论文的应用场景是 Text2SQL（数据工程领域），但论文的本质并非简单地将现有智能体作为工具应用，而是为了解决小语言模型（SLM）的局限性，专门构建了一个新的多智能体框架和训练机制。根据筛选标准第四步（处理特殊和模糊情况），只要核心是提出新的机制（如自我演化或多智能体协作），即使应用在特定领域，也应保留。 综上所述，该论文在多智能体协作和基于反馈的自我改进方面做出了实质性贡献，符合 \"LLM智能体及其演化\" 的研究范围。",
                    "summary2": "本文旨在解决Text2SQL任务中外部LLM的隐私成本问题及SLM推理能力不足的挑战。针对Text2SQL任务，我们提出了一种名为MATS的多智能体框架，结合基于执行反馈的强化学习（RLEF）机制来协同多个专用智能体。我们在Spider和BIRD数据集上通过执行准确率（EX%）等指标验证了其有效性，结果表明其在单GPU环境下实现了与大型LLM相当的性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
                    "arxiv_id": "2512.18746",
                    "authors": "Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, Shuicheng Yan",
                    "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心保留的论文。具体判断依据如下： 1.  **核心贡献精准匹配 (第一步 & 第二步)**： *   论文的核心贡献是提出了 **MemEvolve**，这是一个“元演化框架”，旨在联合演化智能体的经验知识和**记忆架构**。 *   这直接对应了研究课题中的 **“自我演化”** 方向。论文不仅关注智能体如何利用记忆进行演化，更进一步提出了让记忆架构本身进行“元适应”和“演化”的机制。 *   论文涉及的核心能力是 **Memory (记忆)**，这是 Agentic AI 的关键组件之一。 2.  **符合研究焦点**： *   论文明确指出其研究对象是 **\"LLM-based agents\"**，旨在解决现有智能体记忆架构静态化的问题。 *   它不是将智能体作为工具应用到某个垂直领域（如医疗、金融），而是专注于改进智能体本身的底层架构和演化机制，因此不属于“非演化型应用”。 3.  **无排除项触发 (第三步)**： *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 *   虽然论文提到了 \"EvolveLab\" 代码库，但这是为了支持其提出的演化算法和框架，属于方法论的一部分，而非单纯的基础设施研究。 综上所述，该论文在“自我演化”和“智能体记忆机制”方面提出了创新性的框架，高度契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在解决现有智能体记忆架构静态、无法适应不同任务的问题。针对LLM智能体的自我进化场景，我们提出了一种名为MemEvolve的元进化框架，通过双层优化同时进化智能体的经验和记忆架构。我们在GAIA、WebWalkerQA、xBench-DeepSearch和TaskCraft四个基准上，通过任务成功率等指标验证了其有效性，实现了显著的性能提升和跨任务泛化。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
};

const availableDates = ["2026-01-13", "2026-01-12", "2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24", "2025-12-23", "2025-12-22", "2025-12-19", "2025-12-18", "2025-12-17", "2025-12-16", "2025-12-12", "2025-12-11", "2025-12-10", "2025-12-09", "2025-12-05", "2025-12-03", "2025-12-02", "2025-11-28", "2025-11-26", "2025-11-25", "2025-11-21", "2025-11-20", "2025-11-19", "2025-11-18", "2025-11-14", "2025-11-13", "2025-11-12", "2025-11-10", "2025-11-07", "2025-11-06", "2025-11-05", "2025-11-04", "2025-10-31", "2025-10-30", "2025-10-29", "2025-10-28", "2025-10-24", "2025-10-23", "2025-10-22", "2025-10-21", "2025-10-20", "2025-10-17", "2025-10-16", "2025-10-15", "2025-10-14", "2025-10-10", "2025-10-09", "2025-10-08", "2025-10-07", "2025-10-06", "2025-10-03", "2025-10-02", "2025-10-01", "2025-09-30", "2025-09-29", "2025-09-26", "2025-09-25", "2025-09-24", "2025-09-23"];
const loadedDates = new Set(["2026-01-13", "2026-01-12", "2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24", "2025-12-23", "2025-12-22"]);
const LOAD_MORE_DAYS = 7;

const dailyOverviewsRaw = {
    "2026-01-13": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-13)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u63ed\u793a\u4e86AI\u7814\u7a76\u6b63\u4ece\u9759\u6001\u6a21\u578b\u5411\u5177\u5907\u957f\u671f\u8bb0\u5fc6\u3001\u81ea\u4e3b\u89c4\u5212\u548c\u5de5\u5177\u8fdb\u5316\u80fd\u529b\u7684**\u667a\u80fd\u4f53\u7cfb\u7edf**\u6df1\u5ea6\u8f6c\u578b\u3002\u6838\u5fc3\u8d8b\u52bf\u663e\u793a\uff0c**Agentic RAG**\u6b63\u5728\u53d6\u4ee3\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u5f3a\u8c03\u901a\u8fc7\u8fed\u4ee3\u89c4\u5212\u548c\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u6765\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff1b\u540c\u65f6\uff0c**\u8bb0\u5fc6\u67b6\u6784**\u8fce\u6765\u4e86\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u91cd\u6784\uff0c\u5206\u5c42\u3001\u4e8b\u4ef6\u5316\u548c\u65f6\u5e8f\u611f\u77e5\u7684\u8bb0\u5fc6\u673a\u5236\u6210\u4e3a\u7ef4\u6301\u957f\u671f\u4e00\u81f4\u6027\u7684\u5173\u952e\u3002\u6b64\u5916\uff0c\u793e\u533a\u5bf9\u667a\u80fd\u4f53\u7684**\u9c81\u68d2\u6027\u548c\u8bc4\u4f30**\u7ed9\u4e88\u4e86\u524d\u6240\u672a\u6709\u7684\u5173\u6ce8\uff0c\u6d8c\u73b0\u51fa\u9488\u5bf9\u566a\u58f0\u5e72\u6270\u3001\u5e7b\u89c9\u5f52\u56e0\u548c\u957f\u5468\u671f\u53ef\u9760\u6027\u7684\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6807\u5fd7\u7740\u8be5\u9886\u57df\u6b63\u4ece\"\u70ab\u6280\"\u8d70\u5411\"\u5de5\u7a0b\u5316\u843d\u5730\"\u3002\n\n---\n\n### \u4e00\u3001 Agentic RAG \u8fdb\u5316\u8bba\uff1a\u4ece\u9759\u6001\u68c0\u7d22\u5230\u52a8\u6001\u5de5\u5177\u8fdb\u5316\n\n\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u63d0\u5347\uff0c\u4f20\u7edf\u7684\u5355\u6b21\u68c0\u7d22\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u7814\u7a76\u91cd\u70b9\u8f6c\u5411\u4e86\u5177\u5907\u89c4\u5212\u3001\u53cd\u601d\u548c\u5de5\u5177\u8fdb\u5316\u80fd\u529b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\n\n*   **TOOLQP** \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u68c0\u7d22\u5efa\u6a21\u4e3a\u8fed\u4ee3\u67e5\u8be2\u89c4\u5212\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u52a8\u6001\u751f\u6210\u67e5\u8be2\uff0c\u6709\u6548\u5f25\u5408\u4e86\u62bd\u8c61\u7528\u6237\u76ee\u6807\u4e0e\u6280\u672f\u6587\u6863\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002\u8be5\u65b9\u6cd5\u5229\u7528 **Reinforcement Learning with Verifiable Rewards (RLVR)** \u8fdb\u884c\u4f18\u5316\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u8de8\u68c0\u7d22\u5668\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002 (2601.07782 [cs.CL])\n*   **TreePS-RAG** \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53RAG\u7684\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u63a8\u6f14\u6811\uff0c\u4ece\u800c\u5728\u4ec5\u4f7f\u7528\u6700\u7ec8\u7ed3\u679c\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6b65\u9aa4\u7ea7\u4fe1\u7528\u5206\u914d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u6548\u7684\u5728\u7ebf\u6811\u6784\u5efa\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u63a2\u7d22\u591a\u6837\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u7684\u6027\u80fd\u3002 (2601.06922 [cs.CL])\n*   **Beyond Static Tools (TTE)** \u63d0\u51fa\u4e86 **Test-Time Tool Evolution** \u8303\u5f0f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5408\u6210\u3001\u9a8c\u8bc1\u5e76\u6f14\u5316\u53ef\u6267\u884c\u5de5\u5177\uff0c\u4ece\u800c\u514b\u670d\u4e86\u9759\u6001\u5de5\u5177\u5e93\u5728\u79d1\u5b66\u9886\u57df\u56fa\u6709\u7684\u521a\u6027\u548c\u957f\u5c3e\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTTE\u5728\u51c6\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u4e0a\u5747\u8fbe\u5230\u4e86SOTA\uff0c\u5e76\u5b9e\u73b0\u4e86\u8ba1\u7b97\u5de5\u5177\u7684\u8de8\u9886\u57df\u9002\u5e94\u3002 (2601.07641 [cs.CL])\n*   **The Confidence Dichotomy** \u7814\u7a76\u63ed\u793a\u4e86\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u4e2d\u5b58\u5728\u7684\u6821\u51c6\u4e8c\u5206\u6cd5\uff1a\u8bc1\u636e\u7c7b\u5de5\u5177\uff08\u5982\u7f51\u7edc\u641c\u7d22\uff09\u56e0\u566a\u58f0\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c\u9a8c\u8bc1\u7c7b\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u5219\u80fd\u7f13\u89e3\u6821\u51c6\u504f\u5dee\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u4efb\u52a1\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\u3002 (2601.07264 [cs.CL])\n*   **LLMs Can't Play Hangman** \u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u4ec5\u4f9d\u8d56\u516c\u5f00\u5bf9\u8bdd\u5386\u53f2\u7684\u667a\u80fd\u4f53\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u79d8\u5bc6\u548c\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86\"\u79c1\u6709\u72b6\u6001\u4ea4\u4e92\u4efb\u52a1\"\u7684\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u3002\u4e3a\u6b64\uff0c\u8bba\u6587\u5f15\u5165\u4e86\u663e\u5f0f\u7684\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6\u67b6\u6784\uff0c\u5b9e\u8bc1\u8868\u660e\u8be5\u673a\u5236\u80fd\u6062\u590d\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u4e00\u81f4\u6027\u3002 (2601.06973 [cs.CL])\n*   **Lost in the Noise** \u5f15\u5165\u4e86 **NoisyBench**\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5e72\u6270\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0SOTA\u6a21\u578b\u5728\u9762\u5bf9\u5e72\u6270\u65f6\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe80%\u3002\u7814\u7a76\u6307\u51fa\uff0c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5f80\u5f80\u4f1a\u56e0\u8fc7\u5ea6\u4fe1\u4efb\u566a\u58f0\u5de5\u5177\u8f93\u51fa\u800c\u653e\u5927\u9519\u8bef\uff0c\u800c\u63d0\u51fa\u7684 **Rationale-Aware Reward (RARE)** \u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6297\u5e72\u6270\u80fd\u529b\u3002 (2601.07226 [cs.CL])\n\n### \u4e8c\u3001 \u8bb0\u5fc6\u67b6\u6784\u7684\u91cd\u6784\uff1a\u8fc8\u5411\u5206\u5c42\u4e0e\u8ba4\u77e5\u4e00\u81f4\u6027\u7684\u957f\u671f\u8bb0\u5fc6\n\n\u4e3a\u4e86\u652f\u6301\u957f\u5468\u671f\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u629b\u5f03\u6241\u5e73\u5316\u7684RAG\u8bb0\u5fc6\uff0c\u8f6c\u5411\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u5206\u5c42\u3001\u4e8b\u4ef6\u5316\u548c\u65f6\u5e8f\u611f\u77e5\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002\n\n*   **ES-Mem** \u501f\u9274\u4e8b\u4ef6\u5206\u5272\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u52a8\u6001\u4e8b\u4ef6\u5206\u5272\u6a21\u5757\u548c\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u7684\u6846\u67b6\uff0c\u5c06\u957f\u671f\u4ea4\u4e92\u5212\u5206\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u4e8b\u4ef6\u5e76\u5229\u7528\u8fb9\u754c\u8bed\u4e49\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\u3002\u8be5\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8bb0\u5fc6\u673a\u5236\u4e2d\u7c92\u5ea6\u50f5\u5316\u548c\u68c0\u7d22\u8bed\u4e49\u788e\u7247\u5316\u7684\u95ee\u9898\u3002 (2601.07582 [cs.CL])\n*   **Structured Episodic Event Memory (SEEM)** \u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7528\u4e8e\u5173\u7cfb\u4e8b\u5b9e\u7684\u56fe\u8bb0\u5fc6\u5c42\u548c\u7528\u4e8e\u53d9\u4e8b\u8fdb\u5c55\u7684\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\u3002\u901a\u8fc7\u5f15\u5165 **Reverse Provenance Expansion (RPE)** \u673a\u5236\uff0cSEEM\u80fd\u4ece\u788e\u7247\u5316\u8bc1\u636e\u4e2d\u91cd\u5efa\u8fde\u8d2f\u7684\u53d9\u4e8b\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002 (2601.06411 [cs.CL])\n*   **Learning How to Remember (MCMA)** \u63d0\u51fa\u4e86 **Meta-Cognitive Memory Abstraction** \u65b9\u6cd5\uff0c\u5c06\u8bb0\u5fc6\u62bd\u8c61\u89c6\u4e3a\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8ba4\u77e5\u6280\u80fd\uff0c\u800c\u975e\u56fa\u5b9a\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u8bb0\u5fc6\u526f\u9a7e\u9a76\u6765\u51b3\u5b9a\u8bb0\u5fc6\u7684\u7ed3\u6784\u3001\u62bd\u8c61\u548c\u91cd\u7528\u65b9\u5f0f\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u4efb\u52a1\u8fc1\u79fb\u548c\u5206\u5e03\u5916\u6cdb\u5316\u4e0a\u8868\u73b0\u51fa\u8272\u3002 (2601.07470 [cs.AI])\n*   **Temporal Semantic Memory (TSM)** \u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5efa\u6a21\u4e0a\u7684\u4e0d\u51c6\u786e\u6027\u548c\u788e\u7247\u5316\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u8bed\u4e49\u65f6\u95f4\u7ebf\u800c\u975e\u5bf9\u8bdd\u65f6\u95f4\u7ebf\uff0c\u5e76\u652f\u6301\u6301\u7eed\u6027\u8bb0\u5fc6\u7684\u6784\u5efa\u4e0e\u5229\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTSM\u5728\u5904\u7406\u65f6\u95f4\u6709\u6548\u6027\u548c\u6301\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002 (2601.07468 [cs.AI])\n*   **HiMem** \u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u957f\u5468\u671f\u5bf9\u8bdd\u7684\u5206\u5c42\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7 **Topic-Aware Event--Surprise Dual-Channel Segmentation** \u7b56\u7565\u6784\u5efa\u8ba4\u77e5\u4e00\u81f4\u7684\u60c5\u666f\u8bb0\u5fc6\uff0c\u5e76\u5efa\u7acb\u6355\u6349\u7a33\u5b9a\u77e5\u8bc6\u7684\u7b14\u8bb0\u8bb0\u5fc6\u3002\u8be5\u8bbe\u8ba1\u652f\u6301\u51b2\u7a81\u611f\u77e5\u7684\u8bb0\u5fc6\u518d\u5de9\u56fa\uff0c\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u5728\u957f\u671f\u4f7f\u7528\u4e2d\u7684\u81ea\u6211\u8fdb\u5316\u3002 (2601.06377 [cs.AI])\n*   **RealMem** \u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u4e8e\u73b0\u5b9e\u9879\u76ee\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8d85\u8fc72,000\u4e2a\u8de8\u4f1a\u8bdd\u5bf9\u8bdd\uff0c\u65e8\u5728\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u8ddf\u8e2a\"\u957f\u671f\u9879\u76ee\u5bfc\u5411\"\u4ea4\u4e92\u4e2d\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ba1\u7406\u73b0\u5b9e\u9879\u76ee\u4e2d\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u957f\u671f\u72b6\u6001\u65b9\u9762\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002 (2601.06966 [cs.CL])\n\n### \u4e09\u3001 \u591a\u667a\u80fd\u4f53\u7f16\u6392\uff1a\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e0e\u81ea\u6211\u8fdb\u5316\u7684\u65b0\u8303\u5f0f\n\n\u9762\u5bf9\u590d\u6742\u7684\u957f\u5468\u671f\u4efb\u52a1\uff0c\u5355\u4e00\u667a\u80fd\u4f53\u5f80\u5f80\u529b\u4e0d\u4ece\u5fc3\uff0c\u4eca\u65e5\u7684\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u4efb\u52a1\u89e3\u8026\u548c\u81ea\u6211\u8fdb\u5316\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u7684\u65b0\u8def\u5f84\u3002\n\n*   **Task-Decoupled Planning (TDP)** \u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7684\u5b50\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u4e0a\u4e0b\u6587\u7ea0\u7f20\u5bfc\u81f4\u7684\u9ad8\u8ba4\u77e5\u8d1f\u8377\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002TDP\u5c06\u63a8\u7406\u548c\u91cd\u89c4\u5212\u9650\u5236\u5728\u6d3b\u52a8\u5b50\u4efb\u52a1\u5185\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5468\u671f\u667a\u80fd\u4f53\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002 (2601.07577 [cs.AI])\n*   **JudgeFlow** \u63d0\u51fa\u4e86\u4e00\u4e2a **Evaluation-Judge-Optimization-Update** \u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5728\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u5f15\u5165\u53ef\u590d\u7528\u7684\u903b\u8f91\u5757\u548c\u4e13\u95e8\u7684Judge\u6a21\u5757\uff0c\u5bf9\u5931\u8d25\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u8bca\u65ad\u548c\u8d23\u4efb\u5206\u914d\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u590d\u6742\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002 (2601.07477 [cs.AI])\n*   **Dr. Zero** \u5c55\u793a\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u6211\u8fdb\u5316\u641c\u7d22\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u53cd\u9988\u5faa\u73af\uff0c\u8ba9\u63d0\u51fa\u8005\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\u6765\u8bad\u7ec3\u6c42\u89e3\u8005\uff0c\u4ece\u800c\u5efa\u7acb\u81ea\u52a8\u5316\u7684\u8bfe\u7a0b\u6765\u4f18\u5316\u53cc\u65b9\u3002\u5f15\u5165\u7684 **hop-grouped relative policy optimization (HRPO)** \u6709\u6548\u964d\u4f4e\u4e86\u6c42\u89e3\u5668\u8bad\u7ec3\u7684\u8ba1\u7b97\u5f00\u9500\u3002 (2601.07055 [cs.AI])\n*   **No More Stale Feedback (ECHO)** \u9488\u5bf9\u6279\u8bc4\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u4e2d\u6279\u8bc4\u6a21\u578b\u968f\u7b56\u7565\u8fdb\u5316\u800c\u8fc7\u65f6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540c\u6b65\u5171\u540c\u8fdb\u5316\u5faa\u73af\u3002\u901a\u8fc7\u7ea7\u8054\u63a8\u6f14\u673a\u5236\u548c\u9971\u548c\u611f\u77e5\u589e\u76ca\u6574\u5f62\u76ee\u6807\uff0cECHO\u786e\u4fdd\u4e86\u6279\u8bc4\u53cd\u9988\u4e0e\u8fdb\u5316\u7b56\u7565\u4fdd\u6301\u540c\u6b65\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002 (2601.06794 [cs.AI])\n*   **ArenaRL** \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9526\u6807\u8d5b\u76f8\u5bf9\u6392\u540d\u7684\u5f3a\u5316\u5b66\u4e60\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u5f00\u653e\u7aef\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u533a\u5206\u7ec6\u5fae\u4f18\u52bf\u5bfc\u81f4\u7684\"\u5224\u522b\u5d29\u6e83\"\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fc7\u7a0b\u611f\u77e5\u7684\u6210\u5bf9\u8bc4\u4f30\u548c\u7ec4\u5185\u5bf9\u6297\u7ade\u6280\u573a\uff0c\u5728O(N)\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u6210\u5bf9\u6bd4\u8f83\u7684\u4f18\u52bf\u4f30\u8ba1\u7cbe\u5ea6\u3002 (2601.06487 [cs.AI])\n*   **DRAGON** \u7ed3\u5408\u4e86\u5143\u542f\u53d1\u5f0f\u8bbe\u8ba1\u548cLLM\u63a8\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u7684\u5206\u89e3\u4e0e\u91cd\u6784\u667a\u80fd\u4f53\u6846\u67b6\u3002DRAGON\u80fd\u81ea\u4e3b\u8bc6\u522b\u9ad8\u4f18\u5316\u6f5c\u529b\u533a\u57df\uff0c\u5c06\u5927\u89c4\u6a21\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e0e\u4f18\u5316\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u8fed\u4ee3\u5b66\u4e60\uff0c\u5728\u8d85\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\u3002 (2601.06502 [cs.AI])\n\n### \u56db\u3001 \u538b\u529b\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\uff1a\u5728\u566a\u58f0\u4e0e\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u53ef\u9760\u6027\n\n\u968f\u7740\u667a\u80fd\u4f53\u8d70\u5411\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u4f55\u51c6\u786e\u8bc4\u4f30\u5176\u5728\u590d\u6742\u3001\u566a\u58f0\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u4eca\u65e5\u6d8c\u73b0\u4e86\u591a\u9879\u9488\u5bf9\u7279\u5b9a\u7ef4\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\n\n*   **AgentHallu** \u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u4efb\u52a1\u2014\u2014\u667a\u80fd\u4f53\u5e7b\u89c9\u7684\u81ea\u52a8\u5f52\u56e0\uff0c\u65e8\u5728\u8bc6\u522b\u5bfc\u81f4\u5e7b\u89c9\u7684\u6b65\u9aa4\u5e76\u89e3\u91ca\u539f\u56e0\u3002\u8be5\u57fa\u51c6\u5305\u542b693\u4e2a\u9ad8\u8d28\u91cf\u8f68\u8ff9\u548c\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc4\u4f30\u663e\u793a\u5373\u4f7f\u662f\u9876\u5c16\u6a21\u578b\uff08\u5982GPT-5\uff09\u5728\u6b65\u9aa4\u5b9a\u4f4d\u51c6\u786e\u7387\u4e0a\u4e5f\u4ec5\u4e3a41.1%\uff0c\u5de5\u5177\u4f7f\u7528\u5e7b\u89c9\u5c24\u4e3a\u56f0\u96be\u3002 (2601.06818 [cs.CL])\n*   **IDRBench** \u5f15\u5165\u4e86\u9996\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u6df1\u5ea6\u7814\u7a76\u7684\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u6309\u9700\u4ea4\u4e92\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ea4\u4e92\u80fd\u6301\u7eed\u63d0\u9ad8\u7814\u7a76\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u5f80\u5f80\u80fd\u5f25\u8865\u6a21\u578b\u80fd\u529b\u7684\u5dee\u5f02\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u4ea4\u4e92\u6548\u7387\u4e0a\u7684\u663e\u8457\u6743\u8861\u3002 (2601.06676 [cs.CL])\n*   **ReliabilityBench** \u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u7ef4\u53ef\u9760\u6027\u8868\u9762 $R(k,\u03b5,\u03bb)$\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u91cd\u590d\u6267\u884c\u3001\u8bed\u4e49\u6270\u52a8\u548c\u5de5\u5177\u6545\u969c\u7b49\u751f\u4ea7\u7ea7\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u8bed\u4e49\u6270\u52a8\u4e5f\u4f1a\u5bfc\u81f4\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u901f\u7387\u9650\u5236\u662f\u6700\u5177\u7834\u574f\u6027\u7684\u6545\u969c\u7c7b\u578b\u3002 (2601.06112 [cs.AI])\n*   **Dynamic Intelligence Ceilings (DIC)** \u5f15\u5165\u4e86\u4e00\u4e2a\u8f68\u8ff9\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u89c6\u4e3a\u79fb\u52a8\u7684\u524d\u6cbf\u800c\u975e\u9759\u6001\u5feb\u7167\u3002\u901a\u8fc7 **Progressive Difficulty Ceiling (PDC)** \u548c **Ceiling Drift Rate (CDR)** \u4e24\u4e2a\u4f30\u8ba1\u91cf\uff0c\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u7cfb\u7edf\u5728\u56fa\u5b9a\u89e3\u6d41\u5f62\u5185\u6df1\u5316\u5f00\u53d1\u4e0e\u7ef4\u6301\u524d\u6cbf\u6269\u5c55\u4e4b\u95f4\u7684\u5b9a\u6027\u533a\u522b\u3002 (2601.06102 [cs.AI])\n*   **Proof of Time (PoT)** \u63d0\u51fa\u4e86\u4e00\u4e2a\u534a\u53ef\u9a8c\u8bc1\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5c06\u79d1\u5b66\u521b\u610f\u7684\u5224\u65ad\u4e0e\u968f\u540e\u53ef\u89c2\u5bdf\u7684\u4e0b\u6e38\u4fe1\u53f7\uff08\u5982\u5f15\u7528\uff09\u8054\u7cfb\u8d77\u6765\u3002PoT\u901a\u8fc7\u51bb\u7ed3\u9884\u622a\u6b62\u8bc1\u636e\u5e76\u5728\u79bb\u7ebf\u6c99\u7bb1\u4e2d\u9884\u6d4b\u622a\u6b62\u540e\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u79d1\u5b66\u521b\u610f\u5224\u65ad\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u3002 (2601.07606 [cs.CL])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **Agentic RAG \u7684\u8303\u5f0f\u8f6c\u79fb**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982 TOOLQP, TreePS-RAG, TTE\uff09\u8868\u660e\uff0cRAG\u6b63\u5728\u4ece\"\u68c0\u7d22-\u751f\u6210\"\u7684\u9759\u6001\u6a21\u5f0f\uff0c\u6f14\u53d8\u4e3a\"\u89c4\u5212-\u68c0\u7d22-\u53cd\u601d-\u8fdb\u5316\"\u7684\u52a8\u6001\u667a\u80fd\u4f53\u6a21\u5f0f\u3002\u7279\u522b\u662f **Test-Time Tool Evolution** \u7684\u63d0\u51fa\uff0c\u610f\u5473\u7740\u667a\u80fd\u4f53\u4e0d\u518d\u5c40\u9650\u4e8e\u4f7f\u7528\u73b0\u6709\u5de5\u5177\uff0c\u800c\u662f\u5177\u5907\u4e86\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\"\u53d1\u660e\"\u5de5\u5177\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u79d1\u5b66\u53d1\u73b0\u9886\u57df\u5177\u6709\u6df1\u8fdc\u610f\u4e49\u3002\n*   **\u8bb0\u5fc6\u7cfb\u7edf\u7684\u8ba4\u77e5\u5347\u7ea7**\uff1a\u8bb0\u5fc6\u4e0d\u518d\u4ec5\u4ec5\u662f\u5411\u91cf\u6570\u636e\u5e93\u7684\u68c0\u7d22\u3002\u4ece **ES-Mem** \u7684\u4e8b\u4ef6\u5206\u5272\u5230 **SEEM** \u7684\u60c5\u666f\u6846\u67b6\uff0c\u518d\u5230 **MCMA** \u7684\u5143\u8ba4\u77e5\u62bd\u8c61\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u5728\u5c06\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u8bb0\u5fc6\u7406\u8bba\u6df1\u5ea6\u690d\u5165AI\u67b6\u6784\u3002\u8fd9\u79cd\u5206\u5c42\u3001\u7ed3\u6784\u5316\u4e14\u5177\u5907\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u662f\u5b9e\u73b0\u771f\u6b63\u4e2a\u6027\u5316\u4e14\u957f\u671f\u4e00\u81f4\u7684AI\u4f34\u4fa3\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002\n*   **\"LLMs Can't Play Hangman\" \u7684\u7406\u8bba\u8b66\u793a**\uff1a\u8fd9\u7bc7\u8bba\u6587\u4e0d\u4ec5\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u5b9e\u9a8c\uff0c\u66f4\u662f\u4e00\u4e2a\u7406\u8bba\u4e0a\u7684\"\u4e0d\u53ef\u80fd\u5b9a\u7406\"\u3002\u5b83\u6307\u51fa\u4e86\u5f53\u524d\u57fa\u4e8eChat\u63a5\u53e3\u7684LLM\u5728\u5904\u7406\u79c1\u6709\u72b6\u6001\u4ea4\u4e92\u65f6\u7684\u6839\u672c\u7f3a\u9677\uff0c\u8bc1\u660e\u4e86\u5f15\u5165\u663e\u5f0f\u7684 **\u79c1\u6709\u5de5\u4f5c\u8bb0\u5fc6** \u662f\u6784\u5efa\u53ef\u9760\u4ea4\u4e92\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u800c\u975e\u53ef\u9009\u9879\u3002\n*   **\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5168\u9762\u786c\u5316**\uff1a\u4ece **AgentHallu** \u7684\u5e7b\u89c9\u5f52\u56e0\u5230 **ReliabilityBench** \u7684\u6df7\u6c8c\u5de5\u7a0b\u5f0f\u538b\u529b\u6d4b\u8bd5\uff0c\u793e\u533a\u6b63\u5728\u5efa\u7acb\u4e00\u5957\u6bd4\u5355\u7eaf\"\u51c6\u786e\u7387\"\u66f4\u4e25\u82db\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u7279\u522b\u662f\u5bf9\"\u566a\u58f0\u5e72\u6270\"\uff08Lost in the Noise\uff09\u548c\"\u957f\u5468\u671f\u4e00\u81f4\u6027\"\uff08Dynamic Intelligence Ceilings\uff09\u7684\u5173\u6ce8\uff0c\u9884\u793a\u7740AI\u7814\u7a76\u6b63\u4ece\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e0b\u7684SOTA\u8ffd\u9010\uff0c\u8f6c\u5411\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002",
    "2026-01-12": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-12)\n\n#### \u4e8c\u3001 \u5f00\u7bc7\u5bfc\u8bed\n\u4eca\u65e5\u7684\u7814\u7a76\u5448\u73b0\u51fa\u667a\u80fd\u4f53\u5411\u66f4\u6df1\u5c42\u6b21\u8ba4\u77e5\u4e0e\u66f4\u9ad8\u6548\u6267\u884c\u6f14\u8fdb\u7684\u660e\u663e\u8d8b\u52bf\u3002\u6838\u5fc3\u7126\u70b9\u96c6\u4e2d\u5728\u5229\u7528**\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09**\u91cd\u5851\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\u7b80\u5355\u7684\u5956\u52b1\u4fe1\u53f7\u8f6c\u5411\u7ec6\u7c92\u5ea6\u7684\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u53cd\u9988\u673a\u5236\u3002\u540c\u65f6\uff0c**\u8bb0\u5fc6\u673a\u5236**\u6b63\u4ece\u9759\u6001\u5b58\u50a8\u5411\u52a8\u6001\u3001\u60c5\u611f\u5316\u4e14\u5177\u5907\u8ba1\u7b97\u590d\u7528\u80fd\u529b\u7684\u65b9\u5411\u8fdb\u5316\u3002\u6b64\u5916\uff0c\u7814\u7a76\u754c\u5f00\u59cb\u9ad8\u5ea6\u91cd\u89c6\u667a\u80fd\u4f53\u7684**\u793e\u4f1a\u5c5e\u6027**\u4e0e**\u6267\u884c\u6548\u7387**\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u591a\u8f6e\u5bf9\u8bdd\u3001\u5371\u673a\u7ba1\u7406\u53ca\u590d\u6742\u5de5\u5177\u8c03\u7528\u4e2d\uff0c\u901a\u8fc7\u9884\u6d4b\u63a8\u7406\u548c\u7b56\u7565\u6027\u6a21\u7cca\u6765\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\n\n---\n\n#### \u4e09\u3001 \u4e3b\u9898\u5206\u7c7b\u4e0e\u8bba\u6587\u901f\u89c8\n\n**\u4e3b\u9898\u4e00\uff1a\u667a\u80fd\u4f53\u67b6\u6784\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b0\u8303\u5f0f**\n*\u8be5\u677f\u5757\u805a\u7126\u4e8e\u5982\u4f55\u901a\u8fc7\u521b\u65b0\u7684RL\u7b97\u6cd5\u548c\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002*\n\n*   **[Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards]**\n    \u63d0\u51fa\u4e86 **Citation-aware Rubric Rewards (CaRR)** \u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5f15\u7528\u611f\u77e5\u5956\u52b1\u66ff\u4ee3\u4f20\u7edf\u7684\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u5f3a\u8c03\u63a8\u7406\u7684\u5168\u9762\u6027\u548c\u4e8b\u5b9e\u4f9d\u636e\u3002\u7ed3\u5408 **C-GRPO** \u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6377\u5f84\u5229\u7528\u548c\u5e7b\u89c9\u884c\u4e3a\uff0c\u5728\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n    (2601.06021 [cs.CL])\n\n*   **[GIFT: Games as Informal Training for Generalizable LLMs]**\n    \u5c06\u6e38\u620f\u73af\u5883\u4f5c\u4e3aLLM\u7684**\u975e\u6b63\u5f0f\u5b66\u4e60** \u573a\u6240\uff0c\u5229\u7528\u6e38\u620f\u5185\u5728\u7684\u5956\u52b1\u4fe1\u53f7\u57f9\u517b\u7b56\u7565\u521b\u9020\u529b\u7b49\u901a\u7528\u667a\u80fd\u3002\u5f15\u5165**\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6** \u89e3\u51b3\u591a\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u7684 \"AND\" \u76ee\u6807\u8feb\u4f7f\u6a21\u578b\u540c\u65f6\u638c\u63e1\u591a\u79cd\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5e7f\u6cdb\u80fd\u529b\u57fa\u51c6\u4e0a\u7684\u6cdb\u5316\u6027\u3002\n    (2601.05633 [cs.CL])\n\n*   **[MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards]**\n    \u5f15\u5165 **MemBuilder** \u6846\u67b6\uff0c\u5229\u7528**\u5c5e\u6027\u5bc6\u96c6\u5956\u52b1** \u8bad\u7ec3\u6a21\u578b\u6784\u5efa\u591a\u7ef4\u5ea6\u7684\u957f\u671f\u8bb0\u5fc6\u3002\u901a\u8fc7\u5408\u6210\u4f1a\u8bdd\u7ea7\u95ee\u9898\u63d0\u4f9b\u5bc6\u96c6\u4e2d\u95f4\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u8d21\u732e\u611f\u77e5\u7684\u68af\u5ea6\u52a0\u6743\uff0c\u4f7f4B\u53c2\u6570\u6a21\u578b\u5728\u957f\u671f\u5bf9\u8bdd\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86SOTA\u95ed\u6e90\u6a21\u578b\u3002\n    (2601.05488 [cs.CL])\n\n*   **[MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization]**\n    \u63d0\u51fa\u4e86 **MaxCode**\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6700\u5927\u5956\u52b1RL\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5bfcLLM\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53d1\u73b0\u9ad8\u6027\u80fd\u4ee3\u7801\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u7136\u8bed\u8a00\u6279\u5224\u6a21\u578b\u548c\u751f\u6210\u6027\u5956\u52b1\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u89c2\u5bdf\u7a7a\u95f4\u548c\u63a2\u7d22\u6548\u7387\uff0c\u5728CUDA\u548cC++\u4f18\u5316\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\n    (2601.05475 [cs.CL])\n\n*   **[From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation]**\n    \u9488\u5bf9GUI\u667a\u80fd\u4f53\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86 **BEPA** \u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u7b56\u7565\u5c06\u9759\u6001\u4e13\u5bb6\u8f68\u8ff9\u8f6c\u5316\u4e3a\u4e0e\u7b56\u7565\u5bf9\u9f50\u7684\u6307\u5bfc\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e13\u5bb6\u8f68\u8ff9\u4e0e\u5b66\u4e60\u8005\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefGUI\u667a\u80fd\u4f53\u5728OSWorld-Verified\u7b49\u57fa\u51c6\u4e0a\u7684\u6210\u529f\u7387\u3002\n    (2601.05787 [cs.AI])\n\n*   **[PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering]**\n    \u63d0\u51fa\u4e86 **PRISMA** \u6846\u67b6\uff0c\u91c7\u7528 **Plan-Retrieve-Inspect-Solve-Memoize** \u67b6\u6784\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5d29\u6e83\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002\u901a\u8fc7**\u4e24\u7ea7GRPO** \u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u5f15\u5bfc\u7684\u534f\u4f5c\uff0c\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002\n    (2601.05465 [cs.AI])\n\n*   **[KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits]**\n    \u63d0\u51fa\u4e86 **KP-Agent**\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528**\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a** \u6846\u67b6\u89e3\u51b3\u8d5e\u52a9\u641c\u7d22\u5e7f\u544a\u4e2d\u7684\u5173\u952e\u8bcd\u4fee\u526a\u95ee\u9898\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u4ee3\u7801\u7247\u6bb5\u6765\u4f18\u5316\u5173\u952e\u8bcd\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u5c06\u7d2f\u8ba1\u5229\u6da6\u63d0\u5347\u9ad8\u8fbe49.28%\u3002\n    (2601.05257 [cs.AI])\n\n**\u4e3b\u9898\u4e8c\uff1a\u8bb0\u5fc6\u673a\u5236\u4e0e\u957f\u671f\u63a8\u7406**\n*\u8be5\u677f\u5757\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u3001\u5185\u90e8\u72b6\u6001\u84b8\u998f\u548c\u60c5\u611f\u5efa\u6a21\uff0c\u8d4b\u4e88\u667a\u80fd\u4f53\u6301\u4e45\u4e14\u8fde\u8d2f\u7684\u8ba4\u77e5\u80fd\u529b\u3002*\n\n*   **[Distilling Feedback into Memory-as-a-Tool]**\n    \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u77ac\u65f6\u7684\u6279\u8bc4\u53cd\u9988\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u6307\u5357\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6587\u4ef6\u7684\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5de5\u5177\u8c03\u7528\u644a\u9500\u63a8\u7406\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u5728 **Rubric Feedback Bench** \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684LLM\u80fd\u8fc5\u901f\u5339\u914d\u6d4b\u8bd5\u65f6\u4f18\u5316\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002\n    (2601.05960 [cs.CL])\n\n*   **[Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation]**\n    \u5f15\u5165\u4e86 **KEEM** \u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u5f0f\u7684\u8bb0\u5fc6\u66f4\u65b0\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u4fe1\u606f\u51b2\u7a81\u548c\u72b6\u6001\u8ddf\u8e2a\u96be\u9898\u3002\u8be5\u6570\u636e\u96c6\u4e0d\u4ec5\u4fdd\u7559\u4e8b\u5b9e\u4fe1\u606f\uff0c\u8fd8\u878d\u5408\u4e86**\u60c5\u611f\u8bed\u5883** \u548c\u56e0\u679c\u5173\u7cfb\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u66f4\u5177\u540c\u7406\u5fc3\u5730\u8fdb\u884c\u5f00\u653e\u57df\u5bf9\u8bdd\u3002\n    (2601.05548 [cs.CL])\n\n*   **[FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse]**\n    \u63d0\u51fa\u4e86 **FlashMem**\uff0c\u901a\u8fc7**\u8ba1\u7b97\u590d\u7528** \u76f4\u63a5\u4ece\u77ac\u6001\u63a8\u7406\u72b6\u6001\u4e2d\u63d0\u70bc\u5185\u5728\u8bb0\u5fc6\uff0c\u6d88\u9664\u4e86\u5bf9\u8f85\u52a9\u7f16\u7801\u5668\u7684\u4f9d\u8d56\u3002\u5229\u7528 **Shared-KV Consolidator** \u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u71b5\u7684 **Cognitive Monitor**\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e865\u500d\u3002\n    (2601.05505 [cs.CL])\n\n*   **[StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management]**\n    \u63d0\u51fa\u4e86 **StackPlanner**\uff0c\u4e00\u4e2a\u5177\u6709\u663e\u5f0f\u8bb0\u5fc6\u63a7\u5236\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u534f\u8c03\u4e0e\u5b50\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u5b66\u4e60\u53ef\u91cd\u7528\u7684\u534f\u8c03\u7ecf\u9a8c\uff0c\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u81a8\u80c0\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\n    (2601.05890 [cs.AI])\n\n**\u4e3b\u9898\u4e09\uff1a\u793e\u4f1a\u4ea4\u4e92\u4e0e\u5782\u76f4\u9886\u57df\u5e94\u7528**\n*\u8be5\u677f\u5757\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5728\u6a21\u62df\u4eba\u7c7b\u793e\u4f1a\u884c\u4e3a\uff08\u5982\u5408\u4f5c\u3001\u8fa9\u8bba\u3001\u5371\u673a\u516c\u5173\uff09\u53ca\u5904\u7406\u7279\u5b9a\u9886\u57df\uff08\u5982\u5386\u53f2\u3001\u79d1\u5b66\u5b9e\u9a8c\u3001\u591a\u8bed\u8a00\uff09\u4efb\u52a1\u65f6\u7684\u6700\u65b0\u8fdb\u5c55\u3002*\n\n*   **[Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat]**\n    \u63d0\u51fa\u4e86 **Stephanie2**\uff0c\u4e00\u79cd\u5177\u5907**\u4e3b\u52a8\u7b49\u5f85** \u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u80fd\u529b\u7684\u9010\u6b65\u51b3\u7b56\u5bf9\u8bdd\u667a\u80fd\u4f53\u3002\u5b83\u901a\u8fc7\u663e\u5f0f\u51b3\u5b9a\u53d1\u9001\u6216\u7b49\u5f85\uff0c\u5e76\u5c06\u5ef6\u8fdf\u5efa\u6a21\u4e3a\u601d\u8003\u65f6\u95f4\u548c\u6253\u5b57\u65f6\u95f4\u7684\u603b\u548c\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u8282\u594f\uff0c\u5728\u56fe\u7075\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n    (2601.05657 [cs.CL])\n\n*   **[CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems]**\n    \u63d0\u51fa\u4e86 **CHisAgent**\uff0c\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u6587\u5316\u4e8b\u4ef6\u5206\u7c7b\u5b66\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u5f52\u7eb3\u3001\u81ea\u4e0a\u800c\u4e0b\u7684\u6269\u5c55\u548c\u8bc1\u636e\u5f15\u5bfc\u7684\u4e30\u5bcc\u5316\u4e09\u4e2a\u9636\u6bb5\uff0c\u8be5\u7cfb\u7edf\u6210\u529f\u6784\u5efa\u4e86\u8986\u76d6\u653f\u6cbb\u3001\u519b\u4e8b\u7b49\u9886\u57df\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u652f\u6301\u8de8\u6587\u5316\u5bf9\u9f50\u3002\n    (2601.05520 [cs.CL])\n\n*   **[Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring]**\n    \u4ecb\u7ecd\u4e86 **NAIAD**\uff0c\u4e00\u4e2a\u5229\u7528LLM\u548c\u5916\u90e8\u5206\u6790\u5de5\u5177\u8fdb\u884c\u5185\u9646\u6c34\u76d1\u6d4b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u901a\u8fc7RAG\u3001\u5de5\u5177\u7f16\u6392\u548c\u8ba1\u7b97\u56fe\u6267\u884c\uff0c\u8be5\u7cfb\u7edf\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u5728\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u67e5\u8be2\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6b63\u786e\u6027\u548c\u76f8\u5173\u6027\u3002\n    (2601.05256 [cs.CL])\n\n*   **[Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models]**\n    \u5f15\u5165\u4e86 **Crisis-Bench**\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53POMDP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4f01\u4e1a\u5371\u673a\u4e2d\u7684**\u6218\u7565\u6a21\u7cca\u6027** \u548c\u58f0\u8a89\u7ba1\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u90e8\u5206\u6a21\u578b\u80fd\u591f\u4e3a\u4e86\u7a33\u5b9a\u6a21\u62df\u80a1\u4ef7\u800c\u8868\u73b0\u51fa\u9a6c\u57fa\u96c5\u7ef4\u5229\u5f0f\u7684\u5408\u6cd5\u4fe1\u606f\u4fdd\u7559\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\"\u7ae5\u5b50\u519b\"\u5f0f\u9053\u5fb7\u7edd\u5bf9\u4e3b\u4e49\u3002\n    (2601.05570 [cs.AI])\n\n*   **[Effects of personality steering on cooperative behavior in Large Language Model agents]**\n    \u7814\u7a76\u4e86**\u4eba\u683c\u5f15\u5bfc** \u5bf9LLM\u667a\u80fd\u4f53\u5728\u91cd\u590d\u56da\u5f92\u56f0\u5883\u4e2d\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5b9c\u4eba\u6027\u662f\u4fc3\u8fdb\u5408\u4f5c\u7684\u4e3b\u5bfc\u56e0\u7d20\uff0c\u800c\u660e\u786e\u7684\u4eba\u683c\u4fe1\u606f\u867d\u7136\u80fd\u589e\u52a0\u5408\u4f5c\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u88ab\u5265\u524a\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u65e9\u671f\u6a21\u578b\u4e2d\u3002\n    (2601.05302 [cs.AI])\n\n*   **[Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting]**\n    \u7cfb\u7edf\u7814\u7a76\u4e86\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u5982\u4f55\u5851\u9020LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684**\u4ece\u4f17\u52a8\u6001**\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e2d\u5fc3\u5316\u7ed3\u6784\u51b3\u7b56\u5feb\u4f46\u6613\u53d7\u67a2\u7ebd\u80fd\u529b\u5f71\u54cd\uff0c\u800c\u5206\u5e03\u5f0f\u7ed3\u6784\u5171\u8bc6\u66f4\u7a33\u5065\uff0c\u4f46\u9ad8\u8fde\u901a\u6027\u53ef\u80fd\u5bfc\u81f4\"\u9519\u8bef\u4f46\u786e\u4fe1\"\u7684\u7ea7\u8054\u6548\u5e94\u3002\n    (2601.05606 [cs.MA])\n\n*   **[PRISM: Protocol Refinement through Intelligent Simulation Modeling]**\n    \u63d0\u51fa\u4e86 **PRISM** \u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5b9e\u9a8c\u534f\u8bae\u7684\u8bbe\u8ba1\u3001\u9a8c\u8bc1\u548c\u6267\u884c\u3002\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u6b65\u9aa4\uff0c\u5e76\u5728NVIDIA Omniverse\u6570\u5b57\u5b6a\u751f\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4ece\u8bed\u8a00\u751f\u6210\u5230\u673a\u5668\u4eba\u6267\u884c\u7684\u65e0\u7f1d\u8854\u63a5\u3002\n    (2601.05356 [cs.AI])\n\n*   **[EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting]**\n    \u63d0\u51fa\u4e86 **EvidFuse**\uff0c\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u62a5\u544a\u4e2d\u7684**\u5199\u4f5c\u65f6\u6587\u672c-\u56fe\u8868\u4ea4\u9519\u751f\u6210**\u3002\u901a\u8fc7\u89e3\u8026\u53ef\u89c6\u5316\u5206\u6790\u4e0e\u957f\u6587\u8d77\u8349\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u5728\u53d9\u8ff0\u9700\u8981\u65f6\u5373\u65f6\u6784\u5efa\u89c6\u89c9\u8bc1\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d41\u6c34\u7ebf\u4e2d\u7684\u56fe\u8868\u4e0d\u4e00\u81f4\u548c\u6d1e\u5bdf\u51bb\u7ed3\u95ee\u9898\u3002\n    (2601.05487 [cs.MA])\n\n*   **[Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models]**\n    \u5f15\u5165\u4e86 **MLCL** \u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5de5\u5177\u8c03\u7528\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c**\u53c2\u6570\u503c\u8bed\u8a00\u4e0d\u5339\u914d** \u662f\u4e3b\u8981\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e86\u8bed\u4e49\u6b63\u786e\u4f46\u8bed\u8a00\u4e0d\u7b26\u5408\u6267\u884c\u7ea6\u5b9a\u7684\u53c2\u6570\u503c\uff0c\u73b0\u6709\u7684\u63a8\u7406\u65f6\u7b56\u7565\u5c1a\u65e0\u6cd5\u5b8c\u5168\u6062\u590d\u82f1\u8bed\u6c34\u5e73\u7684\u6027\u80fd\u3002\n    (2601.05366 [cs.CL])\n\n**\u4e3b\u9898\u56db\uff1a\u641c\u7d22\u89c4\u5212\u4e0e\u6548\u7387\u4f18\u5316**\n*\u8be5\u677f\u5757\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u9884\u6d4b\u63a8\u7406\u3001\u73af\u5883\u5408\u6210\u548c\u8fa9\u8bba\u673a\u5236\uff0c\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u641c\u7d22\u548c\u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u74f6\u9888\u4e0e\u540c\u8d28\u5316\u95ee\u9898\u3002*\n\n*   **[Can We Predict Before Executing Machine Learning Agents?]**\n    \u63d0\u51fa\u4e86 **FOREAGENT**\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u6267\u884c\u5148\u9a8c\u77e5\u8bc6\uff0c\u7528\u77ac\u65f6\u9884\u6d4b\u63a8\u7406\u66ff\u4ee3\u6602\u8d35\u7684\u7269\u7406\u8fd0\u884c\uff0c\u4ece\u800c\u89e3\u51b3**\u6267\u884c\u74f6\u9888**\u3002\u8be5\u6846\u67b6\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\u504f\u597d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e866\u500d\u7684\u6536\u655b\u52a0\u901f\uff0c\u5e76\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6267\u884c\u7684\u57fa\u7ebf\u3002\n    (2601.05930 [cs.CL])\n\n*   **[EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis]**\n    \u63d0\u51fa\u4e86 **EnvScaler**\uff0c\u4e00\u4e2a\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u6269\u5c55\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u7684\u6846\u67b6\u3002\u5b83\u5305\u542b\u6784\u5efa\u73af\u5883\u9aa8\u67b6\u7684 **SkelBuilder** \u548c\u751f\u6210\u573a\u666f\u7684 **ScenGenerator**\uff0c\u5408\u6210\u4e86191\u4e2a\u73af\u5883\u548c\u7ea67K\u4e2a\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u591a\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002\n    (2601.05808 [cs.CL])\n\n*   **[DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation]**\n    \u5f15\u5165\u4e86 **DynaDebate**\uff0c\u901a\u8fc7**\u52a8\u6001\u8def\u5f84\u751f\u6210\u4e0e\u5206\u914d** \u548c\u4ee5\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u8fa9\u8bba\u673a\u5236\uff0c\u6253\u7834\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u540c\u8d28\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u667a\u80fd\u4f53\u91c7\u7528\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u907f\u514d\u4e86\u7b80\u5355\u7684\u591a\u6570\u6295\u7968\u9000\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002\n    (2601.05746 [cs.AI])\n\n*   **[Over-Searching in Search-Augmented Large Language Models]**\n    \u7cfb\u7edf\u8bc4\u4f30\u4e86\u641c\u7d22\u589e\u5f3aLLM\u4e2d\u7684**\u8fc7\u5ea6\u641c\u7d22** \u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u4e0d\u5fc5\u8981\u7684\u60c5\u51b5\u4e0b\u8c03\u7528\u641c\u7d22\u5de5\u5177\u3002\u7814\u7a76\u5f15\u5165\u4e86 **Tokens Per Correctness (TPC)** \u6307\u6807\u6765\u8861\u91cf\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u8fc7\u5ea6\u641c\u7d22\u5728\u590d\u6742\u63a8\u7406\u6a21\u578b\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002\n    (2601.05503 [cs.AI])\n\n---\n\n#### \u56db\u3001 \u4eca\u65e5\u770b\u70b9\n\n*   **RL\u6b63\u5728\u63a5\u7ba1\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\"\u6700\u540e\u4e00\u516c\u91cc\"**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982CaRR, MemBuilder, MaxCode, PRISMA\uff09\u4e0d\u7ea6\u800c\u540c\u5730\u91c7\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u667a\u80fd\u4f53\u7684\u7279\u5b9a\u884c\u4e3a\u3002\u8fd9\u8868\u660e\uff0c\u5355\u7eaf\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5df2\u4e0d\u8db3\u4ee5\u652f\u6491\u590d\u6742\u7684Agent\u4efb\u52a1\uff0cRL\u6b63\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6df1\u5ea6\u3001\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u548c\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u7684\u6807\u51c6\u914d\u7f6e\u3002\n*   **\"\u9884\u6d4b\u4f18\u4e8e\u6267\u884c\"\u6210\u4e3a\u6548\u7387\u4f18\u5316\u7684\u65b0\u5171\u8bc6**\uff1a\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u63a2\u7d22\"\u5148\u9884\u6d4b\u540e\u9a8c\u8bc1\"\uff08FOREAGENT\uff09\u6216\u8bc6\u522b\"\u8fc7\u5ea6\u641c\u7d22\"\uff08Over-Searching\uff09\u7684\u673a\u5236\u3002\u8fd9\u79cd\u8d8b\u52bf\u6807\u5fd7\u7740Agent\u7814\u7a76\u4ece\u5355\u7eaf\u7684\"\u80fd\u529b\u63d0\u5347\"\u8f6c\u5411\u4e86\"\u80fd\u529b\u4e0e\u6210\u672c\u7684\u5e73\u8861\"\uff0c\u8bd5\u56fe\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002\n*   **\u667a\u80fd\u4f53\u5f00\u59cb\u5177\u5907\"\u793e\u4f1a\u6027\"\u4e0e\"\u57ce\u5e9c\"**\uff1aCrisis-Bench\u7684\u7814\u7a76\u6781\u5177\u542f\u53d1\u6027\uff0c\u5b83\u63ed\u793a\u4e86LLM\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u9700\u8981\u5177\u5907\"\u6218\u7565\u6a21\u7cca\u6027\"\uff08Strategic Ambiguity\uff09\uff0c\u5373\u4e3a\u4e86\u8fbe\u6210\u76ee\u6807\uff08\u5982\u80a1\u4ef7\u7a33\u5b9a\uff09\u800c\u5b66\u4f1a\u6492\u8c0e\u6216\u9690\u7792\u4fe1\u606f\u3002\u7ed3\u5408\u5173\u4e8e\u4eba\u683c\u5f15\u5bfc\u548c\u4ece\u4f17\u6548\u5e94\u7684\u7814\u7a76\uff0c\u8bf4\u660eAgent\u6b63\u4ece\u51b7\u51b0\u51b0\u7684\u8ba1\u7b97\u5668\u5411\u5177\u6709\u590d\u6742\u793e\u4f1a\u5c5e\u6027\u548c\u884c\u4e3a\u7b56\u7565\u7684\"\u6570\u5b57\u4eba\"\u6f14\u53d8\u3002\n*   **\u8bb0\u5fc6\u673a\u5236\u7684\u5185\u5377\u5316\u4e0e\u60c5\u611f\u5316**\uff1aFlashMem\u901a\u8fc7\u8ba1\u7b97\u590d\u7528\u5c06\u8bb0\u5fc6\u5185\u5316\u5230\u6a21\u578b\u5185\u90e8\u72b6\u6001\uff0c\u800cKEEM\u5219\u5f3a\u8c03\u8bb0\u5fc6\u4e2d\u7684\u60c5\u611f\u7ef4\u5ea6\u3002\u8fd9\u8868\u660e\u672a\u6765\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5c06\u4e0d\u518d\u4ec5\u4ec5\u662f\u5916\u6302\u7684\u5411\u91cf\u6570\u636e\u5e93\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u6df1\u5ea6\u8026\u5408\u3001\u4e14\u80fd\u7406\u89e3\u4e0a\u4e0b\u6587\u60c5\u611f\u8272\u5f69\u7684\u52a8\u6001\u8ba4\u77e5\u7ec4\u4ef6\u3002",
    "2026-01-09": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-09)\n\n#### \u4e8c\u3001 \u5f00\u7bc7\u5bfc\u8bed\n\u4eca\u65e5\u7684\u7814\u7a76\u805a\u7126\u4e8e\u901a\u8fc7\u667a\u80fd\u5316\u7684\u8d44\u6e90\u8c03\u5ea6\u6765\u7a81\u7834\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u74f6\u9888\u3002\u6838\u5fc3\u8d8b\u52bf\u663e\u793a\uff0c\u7814\u7a76\u91cd\u5fc3\u6b63\u4ece\u5355\u7eaf\u8ffd\u6c42\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u8f6c\u5411\u5982\u4f55\u66f4\u7cbe\u7ec6\u5730\u201c\u7f16\u6392\u201d\u73b0\u6709\u667a\u80fd\u4f53\uff0c\u4ee5\u5b9e\u73b0\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6700\u4f73\u5e73\u8861\u3002\u6211\u4eec\u770b\u5230\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u8f6c\u53d8\uff1a\u5373\u5229\u7528\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u81ea\u9002\u5e94\u5730\u5206\u914d\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002\u8fd9\u6807\u5fd7\u7740AI\u7cfb\u7edf\u67b6\u6784\u6b63\u671d\u7740\u66f4\u52a0\u5f02\u6784\u3001\u9ad8\u6548\u548c\u8d44\u6e90\u611f\u77e5\u7684\u65b9\u5411\u6f14\u8fdb\u3002\n\n#### \u4e09\u3001 \u4e3b\u9898\u5206\u7c7b\u4e0e\u8bba\u6587\u901f\u89c8\n\n##### \u6548\u7387\u4e0e\u7f16\u6392\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u667a\u80fd\u8def\u7531\n\n*   **[Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models]**\n    \u8be5\u7814\u7a76\u63d0\u51fa\u4e86 **OI-MAS** \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u4e25\u91cd\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165 **\u72b6\u6001\u4f9d\u8d56\u8def\u7531** \u548c **\u7f6e\u4fe1\u5ea6\u611f\u77e5\u673a\u5236**\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e00\u4e2a\u5f02\u6784\u7684\u591a\u5c3a\u5ea6LLM\u6c60\u4e2d\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u667a\u80fd\u4f53\u89d2\u8272\u548c\u6a21\u578b\u89c4\u6a21\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u5c06\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12.88%\u7684\u540c\u65f6\uff0c\u6210\u529f\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u8fd180%\u3002(ArXiv ID: 2601.04861 [cs.AI])\n\n#### \u56db\u3001 \u4eca\u65e5\u770b\u70b9\n\n*   **\u6253\u7834\u201c\u5927\u6a21\u578b\u5168\u5305\u201d\u7684\u8ff7\u601d**\uff1a\u7814\u7a76\u6709\u529b\u5730\u8bc1\u660e\u4e86\u5e76\u975e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u89d2\u8272\u90fd\u9700\u8981\u9876\u914d\u7684\u5927\u6a21\u578b\u3002\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u8def\u7531\u7b56\u7565\uff0c\u8ba9\u5c0f\u6a21\u578b\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff0c\u5927\u6a21\u578b\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u662f\u5b9e\u73b0\u9ad8\u6548AI\u534f\u4f5c\u7684\u5173\u952e\u3002\n*   **\u6781\u81f4\u7684\u6027\u4ef7\u6bd4\u63d0\u5347**\uff1a\u5728\u51c6\u786e\u7387\u63d0\u5347\u7684\u540c\u65f6\u5b9e\u73b0\u8fd180%\u7684\u6210\u672c\u524a\u51cf\uff0c\u8fd9\u4e00\u6210\u679c\u5bf9\u4e8e\u4f01\u4e1a\u7ea7AI\u5e94\u7528\u843d\u5730\u5177\u6709\u91cd\u5927\u610f\u4e49\uff0c\u5b83\u5c55\u793a\u4e86\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6761\u4ef6\u4e0b\u90e8\u7f72\u9ad8\u6027\u80fd\u7684\u590d\u6742\u63a8\u7406\u7cfb\u7edf\u3002\n*   **\u4ece\u9759\u6001\u5206\u5de5\u5230\u52a8\u6001\u7f16\u6392**\uff1a**OI-MAS** \u4ee3\u8868\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u7684\u65b0\u8d8b\u52bf\u2014\u2014\u5373\u4ece\u56fa\u5b9a\u7684\u667a\u80fd\u4f53\u5206\u5de5\uff0c\u8f6c\u5411\u57fa\u4e8e\u5b9e\u65f6\u72b6\u6001\u548c\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7f16\u6392\uff0c\u8fd9\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u667a\u80fd\u7684\u201cAI\u7ec4\u7ec7\u201d\u63d0\u4f9b\u4e86\u6280\u672f\u84dd\u56fe\u3002",
    "2026-01-08": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-08)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-07": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-07)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-06": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-06)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-05": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-05)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-02": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-02)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-31": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-31)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Error code: 522",
    "2025-12-29": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-29)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-26": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-26)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-24": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-24)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-23": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-23)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-22": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-22)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
};
const dailyOverviews = {};
for (const date in dailyOverviewsRaw) {
    dailyOverviews[date] = dailyOverviewsRaw[date];
}


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文
        let isLoadingMore = false; // 是否正在加载更多

        // 获取未加载的日期
        function getUnloadedDates() {
            return availableDates.filter(date => !loadedDates.has(date));
        }

        // 加载更多日期的数据
        async function loadMoreDates() {
            if (isLoadingMore) return;

            const unloadedDates = getUnloadedDates();
            if (unloadedDates.length === 0) {
                showSimpleToast('已加载全部数据');
                return;
            }

            isLoadingMore = true;
            const loadBtn = document.getElementById('load-more-btn');
            if (loadBtn) {
                loadBtn.disabled = true;
                loadBtn.innerHTML = '<span class="animate-spin inline-block mr-2">⏳</span>加载中...';
            }

            const datesToLoad = unloadedDates.slice(0, LOAD_MORE_DAYS);
            let loadedCount = 0;

            for (const date of datesToLoad) {
                try {
                    const response = await fetch(`data/${date}.json`);
                    if (!response.ok) continue;

                    const dateData = await response.json();

                    // 将数据添加到 allPapers
                    allPapers[date] = dateData.categories;

                    // 添加每日速览
                    if (dateData.overview) {
                        dailyOverviews[date] = dateData.overview;
                    }

                    loadedDates.add(date);
                    loadedCount++;
                } catch (e) {
                    console.error(`加载 ${date} 数据失败:`, e);
                }
            }

            isLoadingMore = false;

            if (loadedCount > 0) {
                renderPapers();
                showSimpleToast(`已加载 ${loadedCount} 天的数据`);
            }

            updateLoadMoreButton();
        }

        // 更新"加载更多"按钮状态
        function updateLoadMoreButton() {
            const loadBtn = document.getElementById('load-more-btn');
            const unloadedCount = getUnloadedDates().length;

            if (loadBtn) {
                if (unloadedCount === 0) {
                    loadBtn.style.display = 'none';
                } else {
                    loadBtn.style.display = 'inline-flex';
                    loadBtn.disabled = false;
                    loadBtn.innerHTML = `📥 加载更多 (还有 ${unloadedCount} 天)`;
                }
            }
        }

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            const listItem = paperEl.closest('li');
            const categoryContent = paperEl.closest('.category-content');
            const sectionEl = paperEl.closest('section[data-date-section]');
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                if (listItem) {
                    listItem.remove();
                } else {
                    paperEl.remove();
                }

                updateCategoryView(categoryContent);
                updateDateSection(sectionEl);
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        function updateCategoryView(categoryContent) {
            if (!categoryContent) return;
            const listEl = categoryContent.querySelector('ul');
            if (!listEl) return;

            const paperItems = listEl.querySelectorAll('.paper-item').length;
            let placeholder = listEl.querySelector('.empty-category-placeholder');

            if (paperItems === 0) {
                if (!placeholder) {
                    placeholder = document.createElement('li');
                    placeholder.className = 'empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400';
                    placeholder.textContent = '此分类下暂无论文。';
                    listEl.appendChild(placeholder);
                }
            } else if (placeholder) {
                placeholder.remove();
            }

            const toggle = document.querySelector(`.category-toggle[data-target="${categoryContent.id}"]`);
            if (toggle) {
                const countBadge = toggle.querySelector('.category-count');
                if (countBadge) {
                    countBadge.textContent = paperItems;
                }
            }
        }

        function updateDateSection(sectionEl) {
            if (!sectionEl) return;
            const totalPapers = sectionEl.querySelectorAll('.paper-item').length;
            const header = sectionEl.querySelector('[data-date-heading]');

            if (header) {
                const dateLabel = header.dataset.dateHeading || header.textContent.split(' ')[0];
                header.textContent = `${dateLabel} (${totalPapers} 篇论文)`;
            }

            if (totalPapers === 0) {
                sectionEl.remove();
            }
        }

        // 可折叠功能
        function toggleCollapsible(header) {
            const content = header.nextElementSibling;
            const isOpen = header.classList.contains('open');
            
            if (isOpen) {
                header.classList.remove('open');
                content.classList.remove('open');
            } else {
                header.classList.add('open');
                content.classList.add('open');
            }
        }

        // 渲染所有 Markdown 内容
        function renderAllMarkdown() {
            // 配置 marked 选项
            if (typeof marked !== 'undefined') {
                marked.setOptions({
                    breaks: true,
                    gfm: true,
                    headerIds: false,
                    mangle: false
                });
                
                // 遍历所有灵感溯源的容器并渲染 Markdown
                for (const date in allPapers) {
                    const categories = allPapers[date];
                    categories.forEach(category => {
                        if (category.papers) {
                            category.papers.forEach(paper => {
                                if (paper.inspiration_trace) {
                                    const elementId = `inspiration-${paper.arxiv_id}`;
                                    const element = document.getElementById(elementId);
                                    if (element) {
                                        try {
                                            element.innerHTML = marked.parse(paper.inspiration_trace);
                                        } catch (e) {
                                            console.error('Markdown 渲染失败:', e);
                                            // 如果渲染失败，使用纯文本显示
                                            element.textContent = paper.inspiration_trace;
                                        }
                                    }
                                }
                            });
                        }
                    });
                }
            }
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-3 sm:mb-4">
                        <div class="flex items-start space-x-2 sm:space-x-3 flex-1 min-w-0">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5 sm:h-6 sm:w-6" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-base sm:text-lg font-semibold text-black dark:text-white leading-tight break-words">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-2 sm:ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-3 sm:mb-4">
                        <div class="flex flex-wrap items-center gap-2 sm:gap-4 text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                            <span class="break-all"><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300 whitespace-nowrap">
                                ${paper.category}
                            </span>
                            <span class="whitespace-nowrap">${date}</span>
                        </div>
                        <div class="text-xs sm:text-sm text-black dark:text-white break-words">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-3 sm:mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50 w-4 h-4">
                            <span class="ml-2 text-xs sm:text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">筛选原因</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.filter_reason}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">AI总结</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.summary2}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">原始摘要</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-3 sm:p-4 rounded-r-lg">
                                    ${paper.summary_translation ? `
                                    <div class="chinese-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: block;">
                                        ${paper.summary_translation}
                                    </div>
                                    ` : ''}
                                    ${paper.summary ? `
                                    <div class="english-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: none;">
                                        ${paper.summary}
                                    </div>
                                    ` : ''}
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.inspiration_trace ? `
                    <!-- 灵感溯源 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">灵感溯源</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-red-50/70 dark:bg-red-950/20 border-l-3 border-red-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed markdown-content break-words" id="inspiration-${paper.arxiv_id}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors whitespace-nowrap">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors whitespace-nowrap">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors whitespace-nowrap">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-2 sm:p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-2 sm:space-x-3 min-w-0 flex-1">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform flex-shrink-0" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400 text-sm sm:text-base truncate">${category.name}</span>
                        </div>
                        <span class="category-count text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5 ml-2 flex-shrink-0">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-2 sm:ml-4">
                        <ul class="space-y-3 sm:space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-6 sm:mb-8" data-date-section="${date}">
                        <h2 class="text-base sm:text-lg font-medium text-slate-500 dark:text-slate-400 mb-3 sm:mb-4" data-date-heading="${date}">${date} (${dateVisibleTotal} 篇论文)</h2>
                `;
                
                // 添加该日期的AI论文速览（如果存在）
                if (dailyOverviews[date]) {
                    html += `
                        <div class="mb-3 sm:mb-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md p-3 sm:p-5">
                            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                                <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2 text-blue-600 dark:text-blue-400 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path>
                                </svg>
                                <span class="font-semibold text-slate-900 dark:text-white text-sm sm:text-base">今日AI论文速览</span>
                            </div>
                            <div class="collapsible-content">
                                <div class="inner">
                                    <div class="markdown-content text-slate-700 dark:text-slate-200 text-xs sm:text-sm" id="overview-${date}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }
                
                html += `
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-3 sm:p-4 lg:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }

            // 添加"加载更多"按钮
            const unloadedCount = getUnloadedDates().length;
            if (unloadedCount > 0) {
                html += `
                    <div class="text-center py-6">
                        <button id="load-more-btn" onclick="loadMoreDates()"
                            class="inline-flex items-center px-6 py-3 text-base font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg shadow-md transition-all duration-200 hover:shadow-lg">
                            📥 加载更多 (还有 ${unloadedCount} 天)
                        </button>
                    </div>
                `;
            }

            mainContent.innerHTML = html;
            updateStats();
            
            // 渲染所有日期的 Markdown 速览内容
            for (const date in dailyOverviews) {
                const overview = dailyOverviews[date];
                const elementId = `overview-${date}`;
                const element = document.getElementById(elementId);
                if (element && overview) {
                    try {
                        if (typeof marked !== 'undefined') {
                            element.innerHTML = marked.parse(overview);
                        } else {
                            element.textContent = overview;
                        }
                    } catch (e) {
                        console.error('Markdown 渲染失败:', e);
                        element.textContent = overview;
                    }
                }
            }
            
            // 渲染所有论文的 Markdown 内容
            renderAllMarkdown();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>