<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MyArxiv - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 hidden">
        <div class="flex items-center space-x-2">
            <span id="toast-message">已删除</span>
            <span id="countdown" class="text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-3/5 max-w-none p-4 sm:p-6">
        <!-- 头部导航栏 -->
        <header class="flex justify-between items-center mb-6">
            <h1 class="text-3xl font-bold text-slate-900 dark:text-white">MyArxiv</h1>
            <div class="flex items-center space-x-4">
                <!-- 统计信息 -->
                <div class="text-sm text-slate-600 dark:text-slate-400">
                    总计 <span id="total-papers">0</span> 篇论文
                </div>
                <!-- 中英文摘要切换按钮 -->
                <button id="summary-toggle" class="px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors">
                    中文摘要
                </button>
                <button id="theme-toggle" class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500">
                    <!-- 太阳图标 (浅色模式) -->
                    <svg id="theme-icon-light" class="h-6 w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                    </svg>
                    <!-- 月亮图标 (深色模式) -->
                    <svg id="theme-icon-dark" class="h-6 w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                    </svg>
                </button>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2025-09-24": [
        {
            "name": "Artificial Intelligence",
            "count": 5,
            "papers": [
                {
                    "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
                    "arxiv_id": "2509.20105",
                    "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
                    "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础推理能力，提出了一种量子启发的强化学习方法来增强LLM在连贯多步推理方面的表现。这直接对应了\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的核心标准。 从正面指标分析，论文明确包含以下关键要素： - 核心概念：直接关注\"Large Language Models (LLMs)\" - 能力方向：专注于\"coherent multi-step reasoning traces\"和\"structured logical flow\"，属于推理能力范畴 - 训练方法：采用强化学习方法（Proximal Policy Optimization），结合量子物理中的Projected Entangled Pair States (PEPS)概念 论文不涉及任何排除标准中的领域。它不是关于多模态与视觉研究，不是将LLM应用到特定领域，也不是关于模型可靠性在应用层面的研究。虽然论文在GSM8K、StrategyQA和EntailmentBank等数据集上进行了评估，但这些是评估通用推理能力的标准数据集，而非特定领域应用。 论文的核心贡献是提出了一种基于量子物理概念的强化学习方法，通过保真度奖励机制来提高LLM生成连贯推理痕迹的能力，这是一种从根本上提升模型推理能力的方法论创新，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)在保持连贯多步推理追踪方面的困难。针对需要结构化逻辑流程的任务，我们提出了一种量子启发的强化学习方法，使用Projected Entangled Pair States (PEPS)导出基于保真度的奖励并集成到Proximal Policy Optimization (PPO)中，并在GSM8K、StrategyQA和EntailmentBank数据集上通过MEC、WES、BERT和BLEURT等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在保持连贯的多步推理轨迹方面常常面临困难，特别是在需要结构化逻辑流程的任务中。本研究引入了一种量子启发（quantum-inspired）的方法，通过将基于投影纠缠对态（Projected Entangled Pair States, PEPS）的保真度奖励（fidelity-based reward）纳入近端策略优化（Proximal Policy Optimization）来应对这一挑战。与先前使用直接监督或对比目标的方法不同，所提出的方法通过结构一致性指导学习，为在生成的推理轨迹中强制执行全局连贯性提供了一种新途径。该框架在多个数据集上使用多种连贯性确定指标进行了评估，这些数据集包括GSM8K、StrategyQA和EntailmentBank，涵盖了算术、直观和基于蕴含（entailment-based）的推理类型。结果表明，所提出的量子启发方法相比监督、对比和预训练的基线方法有显著改进，凸显了量子启发的保真度作为提高大型语言模型中推理轨迹连贯性基础的有效性。"
                },
                {
                    "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
                    "arxiv_id": "2509.19762",
                    "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
                    "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是关于改进LLM的推理能力，提出了一种优化的推理工作流程(\\cepo)，通过协调内部模型训练和外部智能体编排来提高推理效率，使较小的开源模型能够超越比它们大得多的模型。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的标准。 其次，从正面指标看，论文明确涉及\"LLM reasoning\"这一核心概念和\"reasoning\"这一能力方向，同时提到了\"external agentic orchestration\"，与智能体(llm-based agents)这一新兴范式相关。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊情况下，论文提出的智能体编排框架是通用性的，旨在增强LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，该论文的核心贡献是提出了一种协同设计编排框架与底层模型能力的方法，以释放中小型模型的强大推理能力，这与研究目标高度一致。",
                    "summary2": "本文旨在 [解决现代LLM推理中因模型冗长和指令遵循不佳导致的计算浪费问题]。针对 [中小型开源模型的推理效率与性能问题]，我们提出了一种 [CODA（Conductor-driven Architecture）优化推理工作流，包含自适应规划、执行、自我反思和验证等关键组件]，并在 [AIME、GPQA、LiveCodeBench等数学和编码基准测试] 上通过 [准确率、Pass@k等指标] 验证了其有效性。",
                    "summary_translation": "现代大语言模型(LLM, Large Language Model)推理依赖于广泛的测试时计算(test-time computation)，这种计算由内部模型训练和外部智能体编排(agentic orchestration)共同驱动。然而，这种协同作用(synergy)往往效率低下，因为模型的冗长性(verbosity)和不良的指令遵循(instruction following)能力导致计算资源浪费(wasted compute)。我们分析了这种能力-成本权衡(capability-cost trade-off)，并提出了一种优化的推理工作流(optimized reasoning workflow) \\cepo，它使较小的开源模型能够超越规模大得多的模型。我们将开源(open-source)这一工作流，以促进进一步的研究。我们的工作展示了一条明确的路径，即通过协同设计(co-designing)编排框架(orchestration frameworks)与底层模型能力(underlying model capabilities)，来解锁中小型模型(small-to-medium sized models)的强大推理能力。"
                },
                {
                    "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
                    "arxiv_id": "2509.19681",
                    "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
                    "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出一种\"解释性验证器\"(Explanatory Verifier)来增强大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是通过强化学习(GRPO)训练的验证器来提升模型的自我评估能力，这属于增强LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标看，论文涉及多个相关主题： 1. 能力方向：明确聚焦于\"reasoning\"和\"problem-solving\"，这正是研究目标的核心 2. 训练方法：使用\"reinforcement learning (GRPO)\"进行训练，符合强化学习优化LLM能力的方向 3. 提到的\"self-reflection\"也与提升模型自主推理能力相关 从第三步排除标准看，论文不涉及任何多模态、视觉内容，也不针对医疗、化学、生物等特定应用领域，同时虽然涉及到模型可靠性，但目的是从根本上提升模型的推理能力而非仅作为应用层面的防御。 论文特别关注提高LLM的\"自我评估\"能力，这是通用推理能力的重要组成部分，通过校准的置信度分数和自然语言解释来增强模型的推理质量，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决推理模型在测试时计算策略中自我评估能力不足的问题。针对数学和编码问题求解场景，我们提出了一种基于强化学习(GRPO)训练的成对解释性验证器(Explanatory Verifier)，并在Numina Math、CodeForces和LeetCode数据集上通过准确性和计算效率指标验证了其有效性。",
                    "summary_translation": "先进的测试时计算（test-time computing）策略对于扩展推理模型至关重要，但其有效性受到模型自我评估（self-evaluation）能力不足的限制。我们提出了一种成对解释验证器（pairwise Explanatory Verifier），通过强化学习（GRPO）进行训练，可为生成的解决方案生成校准的置信度分数（calibrated confidence scores）及相关自然语言推理（natural language reasoning）。我们的验证器提高了最佳n选一（best-of-n）和自我反思（self-reflection）等测试时策略的准确性和效率。关键的是，它擅长识别具有挑战性的故障模式（failure modes），例如当两个候选解决方案都完全错误时，能够在多数投票（majority voting）等标准方法失败的情况下取得成功。"
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究基于Transformer的LLMs在图推理任务中的内部机制，通过circuit-tracer框架来解释decoder-only transformers的推理过程。论文识别了图推理中的两个核心机制：token merging和structural memorization，并提供了统一的可解释性框架来理解结构推理。这符合研究目标中\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的要求。论文关注的是LLM本身的推理能力机制，而不是将LLM作为工具应用到特定领域。虽然论文聚焦于图推理这一特定类型的推理，但其目标是提供\"统一的可解释性框架\"来理解结构推理，这属于通用推理能力的研究范畴。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文符合研究范围。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。"
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究线性注意力transformer（一种LLM架构）的基础能力。论文展示了模型通过训练能够隐式地发现统一的数值算法，这直接涉及LLM的内在能力提升，而非将LLM作为工具应用于特定领域。论文关注的是上下文学习(in-context learning)能力，这是一种基础能力的研究，与提高LLM的通用推理能力密切相关。 其次，从正面指标分析，论文符合以下关键点： - 核心概念：研究的是线性注意力transformer，属于LLM架构变体 - 能力方向：涉及数学推理(math reasoning)和问题解决(problem-solving)能力，模型通过学习解决矩阵补全问题，隐式发现了数值算法 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域 - 不涉及模型可靠性的应用层面研究 最后，论文的核心贡献是揭示了LLM能够通过训练隐式发现统一的、资源自适应的迭代求解器，这展示了LLM在算法发现和数学推理方面的强大能力，直接关系到通用推理能力的提升。论文研究的是LLM内在的能力机制，而非特定应用，因此完全符合研究目标。",
                    "summary2": "本文旨在探索训练线性Transformer是否能隐式发现统一的数值算法。针对低秩矩阵补全任务，我们提出了一种训练线性Transformer在masked-block completion任务上隐式学习数值算法的方法，并通过收敛速度和预测准确率验证了EAGLE算法的有效性。",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 17,
            "papers": [
                {
                    "title": "Language Models that Think, Chat Better",
                    "arxiv_id": "2509.20357",
                    "authors": "Adithya Bhaskar, Xi Ye, Danqi Chen",
                    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种名为\"RL with Model-rewarded Thinking (RLMT)\"的新训练范式，旨在增强大语言模型的通用推理能力。该方法要求模型在回答前生成长链思维(CoT)推理，并使用基于偏好的奖励模型进行在线强化学习优化，从而提升模型的规划和问题解决能力。 从筛选标准来看： 1. 第一步核心判断：论文本质是改进LLM的基础能力，提出新的训练范式(RLMT)，增强其推理能力，完全符合保留标准。 2. 第二步正面指标：论文包含多个正面指标，如大语言模型(Llama-3.1-8B和Qwen-2.5-7B)、推理能力(reasoning)、强化学习(RL)等。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性等排除领域。 4. 第四步特殊情况：论文专注于通过思维链推理和强化学习来提升模型本身的推理能力，而非将LLM作为工具应用到特定领域。 论文在多个聊天基准测试和任务上取得了显著改进，包括创意写作和一般知识，这表明其方法有效提升了模型的通用推理能力，而非仅限于特定领域。因此，这篇论文完全符合研究目标。",
                    "summary2": "本文旨在解决基于可验证奖励的强化学习(RLVR)在开放性任务上泛化能力有限的问题。针对通用聊天任务，我们提出了一种结合长链推理与偏好模型奖励的强化学习方法(RLMT)，并在Llama-3.1-8B和Qwen-2.5-7B模型上通过多种聊天基准测试(AlpacaEval2, WildBench, Arena-HardV2)验证了其有效性，实现了3-7点的性能提升，甚至超越了GPT-4o在聊天和创意写作方面的表现。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)通过在可验证领域(如数学和代码)中使用基于规则的奖励来改善语言模型推理能力。然而，RLVR在开放性任务(如撰写大纲论文或制定膳食计划)中导致有限的泛化能力，而这些任务正是人类日常推理的场景。本文表明RLVR范式在可验证领域之外同样有效，并提出了用于通用聊天能力的**基于模型奖励思维的强化学习**(**R**einforcement **L**earning with **M**odel-rewarded **T**hinking, **RLMT**)。\n\n使用多样化的真实世界提示，RLMT要求语言模型(LMs)在响应前生成长链思维(Chain of Thought, CoT)推理，并通过在线强化学习(online RL)对其进行优化，使用的奖励模型是基于偏好的，类似于RLHF(Reinforcement Learning from Human Feedback，人类反馈强化学习)中使用的模型。在Llama-3.1-8B和Qwen-2.5-7B(包括基础模型和指令模型)上进行的40次训练运行以及多种优化算法(DPO、PPO和GRPO)中，RLMT始终优于标准RLHF流程。这包括在三个聊天基准测试(AlpacaEval2、WildBench和ArenaHardV2)上获得3-7分的显著提升，以及在创意写作和常识等其他任务上1-3分的改进。\n\n我们最佳的8B模型在聊天和创意写作方面超越了GPT-4o，并与Claude-3.7-Sonnet (Thinking)相当。RLMT也可以直接应用于基础模型，无需SFT(Supervised Fine-Tuning，监督微调)阶段，类似于R1-Zero训练方式。值得注意的是，仅使用7K个提示，通过我们的RLMT方法训练的Llama-3.1-8B基础模型，其性能超过了经过复杂多阶段流程(使用2500万+示例)后训练的Llama-3.1-8B-Instruct模型。最后，我们对训练模型如何规划其响应进行了定性和定量分析。我们的结果重新思考了后训练流程，并呼吁未来的工作更广泛地理解和运用思维过程。"
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式\"Thinking augmented Pre-Training (TPT)\"，通过在预训练阶段增加思维轨迹来增强模型的推理能力。这直接关注提升LLM的通用推理能力，而非将其作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确关注大型语言模型(LLMs)，能力方向聚焦于推理能力(reasoning)，特别是通过\"step-by-step reasoning and decomposition\"来提升模型性能。实验结果也显示该方法在多个具有挑战性的推理基准上提高了模型性能。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献是提出了一种通用的预训练方法，通过自动生成的思维轨迹增强文本数据，使高质量token更易学习，从而提高LLM的数据效率和推理能力。这种方法不是针对特定领域，而是旨在从根本上提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"论文的目标。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。"
                },
                {
                    "title": "Causal Understanding by LLMs: The Role of Uncertainty",
                    "arxiv_id": "2509.20088",
                    "authors": "Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci",
                    "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究LLMs在因果关系理解方面的能力，属于LLM的基础推理能力研究，特别是逻辑推理能力的重要组成部分。论文通过多种模型测试，分析了LLMs在因果分类和记忆探测方面的表现，发现LLMs在因果理解上的失败源于缺乏结构化的因果表示，而非预训练中因果例子暴露不足。虽然论文使用了PubMed句子作为测试数据，但这只是为了评估LLM的通用因果理解能力，而不是将LLM应用于医疗领域。论文关注的是LLMs本身的推理能力缺陷，属于对LLM基础能力的探索，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在探究大型语言模型在因果理解任务中表现不佳的原因。针对预训练数据中因果例子暴露不足与表示能力缺陷的争议，我们提出了一种基于不确定性量化的评估方法，通过熵和校准误差等指标分析模型行为。在包含18,000多个PubMed句子的数据集上，通过因果类型分类和逐字记忆探测任务，验证了模型在已见和未见数据上表现无显著差异，输出分布接近随机，表明因果理解失败源于缺乏结构化因果表示而非数据暴露不足。",
                    "summary_translation": "最近的研究表明，大型语言模型（LLMs）在因果关系分类（causal relation classification）中实现接近随机的准确率，引发了关于此类失败是源于预训练（pretraining）中接触有限还是更深层次的表征缺口（representational gaps）的问题。我们在基于不确定性（uncertainty-based）的评估下对此进行了研究，测试了预训练中接触因果例子是否能够改善对超过18,000条PubMed句子的因果理解（causal understanding）——其中一半来自The Pile语料库，一半来自2024年之后的内容——涉及七个模型（Pythia-1.4B/7B/12B、GPT-J-6B、Dolly-7B/12B、Qwen-7B）。我们通过以下方式分析模型行为：（i）因果关系分类（causal classification），模型识别文本中的因果关系；以及（ii）逐字记忆探测（verbatim memorization probing），我们评估模型是否更偏好之前见过的因果陈述而非其释义（paraphrases）。模型执行四类分类（direct/conditional/correlational/no-relationship，直接/条件/相关/无关系）并在原始句子和生成的释义之间进行选择。结果显示，模型在已见/未见句子上的准确率几乎相同（p > 0.05），没有记忆偏差（memorization bias）（24.8%选择原始句子），且可能选项的输出分布几乎平坦，熵值（entropic values）接近最大值（1.35/1.39），证实了随机猜测。指令微调（Instruction-tuned）模型表现出严重的校准失调（miscalibration）（Qwen：> 95%的置信度，32.8%的准确率，ECE=0.49）。条件关系（Conditional relations）诱导出最高的熵（比直接关系高+11%）。这些发现表明，因果理解的失败源于缺乏结构化的因果表征（structured causal representation），而非预训练中接触因果例子不足。"
                },
                {
                    "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                    "arxiv_id": "2509.20317",
                    "authors": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin",
                    "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的推理能力，特别是针对Implicit Chain-of-Thought (CoT)方法提出了一种新的训练范式SIM-CoT，这直接属于改进LLM基础能力和通用推理能力的范畴，符合保留标准。 其次，论文包含多项正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于reasoning，特别是Chain-of-Thought推理；训练方法方面提出了创新的step-level supervision机制来增强模型训练过程。 第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文提出的辅助解码器增强了模型推理的可解释性，通过\"projecting each latent token onto an explicit reasoning vocabulary\"来提升模型的推理质量，这符合保留条件。 论文的核心贡献是解决了implicit CoT方法中的潜在不稳定性问题，通过step-level supervision稳定和丰富潜在推理空间，从而提升LLM的推理能力，这与研究目标\"致力于提高大语言模型的通用推理能力\"高度一致。",
                    "summary2": "本文旨在解决隐式思维链(Implicit CoT)方法中的潜在不稳定性问题。针对大语言模型在增加隐式推理令牌时训练崩溃的场景，我们提出了一种SIM-CoT方法，通过引入步骤级监督的辅助解码器来稳定隐式推理空间，并在GSM8K-Aug等多个数学推理数据集上通过准确率、令牌效率等指标验证了其有效性。",
                    "summary_translation": "Implicit Chain-of-Thought (CoT)（隐式思维链）方法为大型语言模型（Large Language Models, LLMs）中的显式思维链推理提供了一种有前景且高效的token（令牌）替代方案，但持续存在的性能差距限制了隐式CoT的应用。通过扩展隐式CoT方法的计算预算，我们发现了一个核心的潜在不稳定性问题：当我们增加隐式推理token的数量以提高性能时，训练过程常常变得不稳定并崩溃。我们的分析表明，这种不稳定性源于潜在表示变得同质化并失去其语义多样性，这是现有隐式CoT方法中步骤级别（step-level）监督不足导致的失败。\n\n为解决这一问题，我们提出了SIM-CoT，即插即用（plug-and-play）训练模块，它引入步骤级别监督以稳定并丰富潜在推理空间。具体而言，SIM-CoT在训练过程中采用辅助解码器（auxiliary decoder）将每个隐式token与其对应的显式推理步骤对齐，确保潜在状态捕获独特且有意义的信息。在推理过程中，所提出的辅助解码器被移除，保持了隐式CoT方法的计算效率，且不增加额外开销。此外，辅助解码器通过将每个潜在token投影到显式推理词汇表（explicit reasoning vocabulary）上，提供了隐式推理的可解释性，实现了每步语义角色和诊断的可视化。\n\nSIM-CoT显著提升了各种隐式CoT方法的域内（in-domain）准确性和域外（out-of-domain）稳定性，使GPT-2上的Coconut基线提高了+8.2%，LLaMA-3.1 8B上的CODI提高了+3.0%。展示了强大的可扩展性（scalability），SIM-CoT在GPT-2上以2.3倍的token效率超越了显式CoT基线2.1%，同时在更大模型如LLaMA-3.1 8B上显著缩小了性能差距。"
                },
                {
                    "title": "Can Constructions \"SCAN\" Compositionality ?",
                    "arxiv_id": "2509.20074",
                    "authors": "Ganesh Katrapati, Manish Shrivastava",
                    "summary": "Sequence to Sequence models struggle at compositionality and systematic generalisation even while they excel at many other tasks. We attribute this limitation to their failure to internalise constructions conventionalised form meaning pairings that license productive recombination. Building on these insights, we introduce an unsupervised procedure for mining pseudo-constructions: variable-slot templates automatically extracted from training data. When applied to the SCAN dataset, our method yields large gains out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on AROUND RIGHT without any architectural changes or additional supervision. The model also attains competitive performance with? 40% of the original training data, demonstrating strong data efAciency. Our findings highlight the promise of construction-aware preprocessing as an alternative to heavy architectural or training-regime interventions.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是关于改进序列到序列模型(Seq2Seq)的组合性和系统泛化能力，这些能力是大语言模型通用推理能力的重要组成部分。论文提出了一种无监督方法来挖掘\"伪构造\"(pseudo-constructions)，即从训练数据中自动提取的可变槽模板，这种方法在SCAN数据集上显著提高了模型在分布外分割上的准确性。这属于改进模型基础能力的方法论研究，而非将LLM应用于特定领域。组合性是逻辑推理和语言理解的基础，系统泛化则涉及到模型如何处理新的、未见过的组合，这些都是通用推理能力的关键方面。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决序列到序列模型在组合性和系统泛化方面的困难。针对SCAN数据集的分布外测试场景，我们提出了一种无监督的伪构造挖掘方法，并在SCAN数据集的ADD JUMP和AROUND RIGHT分割上通过准确率验证了其有效性。",
                    "summary_translation": "序列到序列（Sequence to Sequence）模型在组合性（compositionality）和系统性泛化（systematic generalisation）方面表现不佳，尽管它们在许多其他任务上表现出色。我们将这一局限性归因于它们未能内化构式（constructions）——即约定俗成的形式-意义配对，而这种配对能够许可生产性重组。基于这些见解，我们引入了一种无监督（unsupervised）程序来挖掘伪构式（pseudo-constructions）：即从训练数据中自动提取的可变槽位模板（variable-slot templates）。当应用于SCAN数据集时，我们的方法在分布外（out-of-distribution）分割上取得了显著提升：在无需任何架构变更或额外监督的情况下，ADD JUMP上的准确率提高到47.8%，AROUND RIGHT上提高到20.3%。该模型仅使用40%的原始训练数据就能达到竞争性性能，展示了强大的数据效率（efficiency）。我们的研究结果突显了构式感知（construction-aware）预处理作为一种替代方案的潜力，以替代繁重的架构或训练机制干预。"
                },
                {
                    "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
                    "arxiv_id": "2509.19893",
                    "authors": "Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo",
                    "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的数学推理能力，属于\"增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文提出了FPA（Future Policy Aware）方法，用于解决偏好学习在数学推理中的问题，这是直接提升LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标：核心概念上明确研究Large Language Models (LLMs)；能力方向上专注于mathematical reasoning（数学推理）；训练方法上涉及Direct Preference Optimization (DPO)等偏好学习方法，这些通常与强化学习相关。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然聚焦于数学推理，但数学推理被视为评估和提升LLM通用能力的重要方面，而非特定应用领域；也没有主要关注模型可靠性方面的应用问题。 论文的核心贡献是提出了一种新的偏好学习方法FPA，通过在正则化项中使用未来策略而非当前策略，解决了数学推理中偏好学习的过度惩罚问题，从而提升了LLM的数学推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决数学推理任务中偏好学习方法的梯度纠缠问题。针对数学推理轨迹中大量共享token导致的过度惩罚问题，我们提出了一种未来策略感知（FPA）偏好学习方法，通过轻量级logit空间外推估计未来策略进行主动正则化。在MATH和GSM8K基准测试上通过准确率验证了其有效性，FPA在SimPER算法上实现了高达5.75%的性能提升，同时支持更长的无退化训练。",
                    "summary_translation": "像直接偏好优化（Direct Preference Optimization, DPO，直接偏好优化）这样的偏好学习方法已成为大型语言模型（Large Language Model, LLM，大型语言模型）后训练的标准方法，但它们在数学推理方面往往效果不佳。一个关键挑战是偏好轨迹和非偏好轨迹之间存在大量的标记（token，标记）重叠；降低非偏好轨迹的概率同时也会降低共享有用标记的概率，导致过度惩罚和整体性能崩溃。作为一种缓解措施，现有算法将轨迹在当前策略下的概率作为正则化项（regularization term，正则化项）包含在内，当概率较低时，这会降低梯度的影响。然而，当这种效果开始显现时，有用的标记可能已经被过度惩罚，因为模型已经开始退化。\n\n为解决这个问题，我们提出了未来策略感知（Future Policy Aware, FPA，未来策略感知）偏好学习，它在正则化项中用未来策略替代当前策略。这种未来策略通过从参考模型到当前模型的轻量级logit空间（logit-space，logit空间）外推来估计。FPA通过预先规范化可能有问题的梯度，实现了更安全的训练。我们将FPA应用于DPO、RPO和SimPER，并在MATH和GSM8K基准测试上对它们进行评估。FPA带来了一致的性能提升，其中在SimPER上观察到最大的改进，实现了高达5.75%的提升。我们证明FPA提供了主动的正则化，同时保留了共享的有用数学标记的概率，并实现了更长时间的无退化训练，且计算开销可忽略不计。我们将在发表后公开发布我们的代码。"
                },
                {
                    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
                    "arxiv_id": "2509.19593",
                    "authors": "Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang",
                    "summary": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是大语言模型的通用推理能力。论文提出了GuessingGame协议，用于评估LLMs作为战略提问者的能力，本质上是在研究模型如何通过提问和获取信息来进行有效推理。这种开放式提问能力是一种通用推理能力，类似于思维链(CoT)等多步推理能力，而非将LLM应用于特定领域。论文提出的两种信息增益指标旨在衡量和提升LLMs的推理效率，结果显示信息增益与推理效率显著相关，这种研究直接针对提升LLM的基础推理能力。论文不涉及任何排除标准中的领域（如多模态、特定应用或模型基础设施），而是聚焦于提高LLM本身的通用推理能力，因此完全符合我的研究范围。",
                    "summary2": "本文旨在评估大型语言模型作为策略性提问者的能力。针对开放式、开放领域的问答场景，我们提出了GuessingGame协议，通过Guesser LLM向Oracle提问识别隐藏对象，并设计了两种信息增益(IG)度量方法：贝叶斯信念跟踪和基于ConceptNet的熵方法。在858个游戏实验中，通过成功率(SR)和平均问题数(ANQ)验证了方法有效性，证明IG与任务效率强相关，一个标准差IG增加可减少43%预期游戏长度。",
                    "summary_translation": "我们提出了GuessingGame（猜谜游戏）协议，用于评估大型语言模型（LLMs, Large Language Models）在开放式、开放域环境中作为战略提问者的表现。在该协议中，一个Guesser LLM（猜测者语言模型）通过向Oracle（预言者）提出自由形式的问题来识别一个隐藏对象，无需预设选项或候选列表。为衡量问题质量，我们提出了两种信息增益（IG, Information Gain）指标：一种贝叶斯方法，通过使用LLM评分的相关性来追踪对语义概念的信念更新；另一种基于熵的方法，通过ConceptNet（概念网络）过滤候选对象。这两种指标都是模型无关的（model-agnostic），并支持事后分析（post hoc analysis）。在涉及多个模型和提示策略的858场游戏中，更高的IG strongly predicts效率：IG的一个标准差增加使预期游戏长度减少43%。由IG指导的提示约束，如强制问题多样性，使较弱的模型能显著提高性能。这些结果表明，LLMs中的提问既是可测量的也是可改进的，并且对交互式推理（interactive reasoning）至关重要。"
                },
                {
                    "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
                    "arxiv_id": "2509.19569",
                    "authors": "Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva",
                    "summary": "This paper introduces a novel approach to position embeddings in transformer models, named \"Exact Positional Embeddings\" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是改进Transformer模型的位置编码方法，提出了\"精确位置嵌入\"(ExPE)，使模型能够处理比训练时更长的序列。这属于改进LLM基础架构的研究，旨在增强模型处理长序列的基础能力，而非将LLM作为工具应用到特定领域。因此，论文符合保留标准。 第二步正面指标：论文涉及\"Generative Transformer Models\"，属于LLM范畴。虽然未直接讨论推理、规划等能力，但处理长序列的能力是支持复杂推理任务的基础。例如，数学推理、逻辑推理和多步规划通常需要处理长序列的能力，因此这项工作间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及任何排除标准中的领域，包括多模态与视觉、特定应用领域以及模型可靠性的应用层面问题。它关注的是基础模型架构的改进。 第四步特殊和模糊情况：论文情况清晰，不涉及特殊或模糊情况。它明确关注的是Transformer模型的位置编码方法，属于基础模型架构的改进。 最终决策：虽然论文没有直接讨论推理、规划或问题解决能力，但改进模型处理长序列的能力是支持复杂推理任务的基础。因此，这篇致力于改进LLM基础架构能力的研究符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的核心研究目标。",
                    "summary2": "本文旨在解决Transformer模型在处理比训练序列更长时的位置编码外推问题。针对序列长度外推的场景，我们提出了一种精确位置编码（ExPE）方法，通过覆盖嵌入向量中的特定维度来编码精确位置信息，并在因果语言建模任务上通过困惑度（perplexity）指标验证了其有效性。",
                    "summary_translation": "本文介绍了一种在transformer models（Transformer模型）中进行position embeddings（位置嵌入）的新方法，名为\"Exact Positional Embeddings\"（精确位置嵌入，ExPE）。这是一种absolute positional embedding（绝对位置嵌入）方法，能够extrapolate（外推）到比训练时更长的序列。传统的transformer models（Transformer模型）依赖absolute或relative position embeddings（绝对或相对位置嵌入）将位置信息整合到token embeddings（词元嵌入）中，但这些方法通常难以extrapolate（外推）到比训练时更长的序列。我们提出的方法利用一种新的embedding strategy（嵌入策略），通过重写embedding vectors（嵌入向量）的特定维度来编码精确的位置信息，从而实现对词元位置的更精确表示。所提出的方法不仅保持了原始embeddings（嵌入）的原有特性，还增强了模型对更长序列的泛化能力。在causal language modeling（因果语言建模）中，当在比训练中使用的更长的序列上进行测试时，我们的ExPE embeddings（ExPE嵌入）与rotary and sinusoidal embeddings（旋转和正弦嵌入）相比，显著降低了perplexity（困惑度）。"
                },
                {
                    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
                    "arxiv_id": "2509.19371",
                    "authors": "Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng",
                    "summary": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，主要基于以下分析： 第一步核心判断：这篇论文的本质是关于改进LLM基础能力的研究，具体聚焦于预训练阶段的知识注入方法。论文提出了\"知识注入缩放定律\"，这是一种新的训练范式，旨在解决LLM在知识获取与保留方面的核心挑战。虽然论文提到了\"领域知识\"，但其核心贡献是通用的方法论，用于平衡知识注入与避免灾难性遗忘，这直接关系到提升LLM的基础能力，符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。 第二步正面指标：论文明确包含\"Large language models, LLMs\"这一核心概念。虽然论文没有直接讨论reasoning、planning等具体能力方向，但知识获取和保留是推理能力的基础，论文研究的是如何更有效地让模型获取并保留知识，这间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及多模态与视觉研究。虽然提到\"domain-specific data\"和\"specialized knowledge\"，但论文的核心是提出一种通用的知识注入缩放定律，而非专注于某个特定应用领域（如医疗、化学等）。论文提到\"hallucination\"问题，但是从知识注入角度研究如何减少幻觉，而非仅作为应用层面的防御。 第四步特殊和模糊情况处理：论文虽然涉及\"领域知识\"，但其核心贡献是通用的方法论，可以应用于各种领域知识的注入，而不是针对特定领域的应用研究。因此，它更符合\"改进LLM基础能力\"而非\"特定应用领域\"的特征。 综上所述，这篇论文的核心贡献是提出了一种通用的知识注入缩放定律，用于优化LLM预训练过程中的知识获取和保留，这属于提升LLM基础能力的研究范畴，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型预训练中如何高效注入领域知识的问题。针对不同规模LLMs在知识注入时出现的记忆崩溃现象，我们提出了一种知识注入扩展律(Knowledge Infusion Scaling Law)，通过分析较小模型预测大型模型的最佳知识注入量，并在从137M到3B参数的不同模型规模和高达100B训练token的实验环境中，通过记忆保留率(Memorization Rate)指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）因其令人印象深刻的跨多样化下游任务的通用能力而吸引了广泛关注。然而，在没有领域特定优化（domain-specific optimization）的情况下，它们在专业知识基准（specialized knowledge benchmarks）上表现不佳，甚至会产生幻觉（hallucination）。最近的研究表明，在预训练（pretraining）期间策略性地注入领域知识（domain knowledge）可以显著提高下游性能（downstream performance）。一个关键挑战在于平衡这种注入权衡（infusion trade-off）：注入过少的领域特定数据（domain-specific data）会导致专业化不足，而过量注入则会引发灾难性遗忘（catastrophic forgetting）先前获得的知识。在这项工作中，我们关注由过度注入（over-infusion）引起的记忆崩溃（memory collapse）现象。通过系统性实验，我们得出了两个关键观察结果，即1）关键崩溃点（Critical collapse point）：每个模型都表现出一个阈值，超过该阈值，其知识保留能力会急剧下降。2）规模相关性（Scale correlation）：这些崩溃点与模型规模（model's size）呈一致的比例关系。基于这些见解，我们提出了一种知识注入扩展定律（knowledge infusion scaling law），通过分析较小规模的对应模型来预测应注入大型语言模型（LLMs）的最佳领域知识量。在不同模型规模和预训练令牌预算（pertaining token budgets）上的大量实验验证了我们扩展定律的有效性和泛化性。"
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM的基础能力和提出新的训练范式。ShinkaEvolve框架利用LLMs作为变异操作符，通过进化机制增强模型生成解决方案的能力，这直接关注提升LLM的通用推理和问题解决能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：涉及\"mathematical reasoning\"和\"competitive programming solutions\"，属于通用推理能力范畴 - 训练方法：提出\"evolutionary agentic harnesses\"和\"bandit-based LLM ensemble selection strategy\"，属于进化学习方法 - 新兴范式：包含\"agentic harnesses\"，与LLM-based agents相关 第三步排除标准：论文不符合任何排除标准。虽然提到了圆打包、数学推理等应用场景，但这些是作为评估框架通用性的示例，而非论文的主要焦点。论文核心是提出通用的程序进化框架，而非针对特定领域应用。 第四步特殊情况处理：论文提出的\"evolutionary agentic harnesses\"是一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非针对特定领域的应用，因此符合保留条件。 综合分析，ShinkaEvolve的核心贡献是提出了一种新的进化框架，通过创新的采样和集成选择策略，提高LLM在程序进化方面的样本效率和解决方案质量，这直接服务于提升大语言模型的通用推理能力，符合研究目标。",
                    "summary2": "本文旨在解决当前LLM驱动的科学发现方法中存在的样本效率低下和闭源限制问题。针对程序进化任务，我们提出了ShinkaEvolve框架，通过三种关键创新（自适应父程序采样、代码新颖性拒绝采样和基于bandit的LLM集成选择策略）显著提升样本效率。在circle packing、AIME数学推理、ALE-Bench竞赛编程和MoE负载平衡损失设计等多样任务上，ShinkaEvolve以更少样本实现了state-of-the-art性能，并通过开源发布促进了广泛应用。",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。"
                },
                {
                    "title": "Pluralistic Off-policy Evaluation and Alignment",
                    "arxiv_id": "2509.19333",
                    "authors": "Chengkai Huang, Junda Wu, Zhouhang Xie, Yu Xia, Rui Wang, Tong Yu, Subrata Mitra, Julian McAuley, Lina Yao",
                    "summary": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了POPE（Pluralistic Off-Policy Evaluation）框架，用于解决LLM在多元人类偏好下的评估和对齐问题。从第一步判断来看，论文本质上是关于改进LLM的基础能力（偏好对齐），提出新的评估和优化框架，这符合保留标准。论文明确针对LLM的偏好对齐问题，并涉及到强化学习中的离线策略评估和优化概念，这与第二步中的正面指标部分吻合。论文不符合第三步中的排除标准，它不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接讨论推理、规划或问题解决能力，但它关注的是偏好对齐，这是LLM的一个重要基础能力，良好的偏好对齐是模型展现高质量推理能力的前提。论文提出的框架通过结合人类偏好信号和多样性组件来改进模型的基础能力，这种改进可能间接提升模型在推理和其他任务上的表现。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM中多元化偏好对齐的离线评估问题。针对记录在不同策略下的偏好数据，我们提出了POPE框架，结合协作效用和多样性奖励的统一函数，并通过可分解的反向倾向评分估计器进行评估。在Alpaca-GPT4、电影评论等数据集上通过PL-Score、Pluralistic Coverage等指标验证了其有效性。",
                    "summary_translation": "针对具有多样化人类偏好的大型语言模型(LLMs, 大型语言模型)的个性化偏好对齐(preference alignment, 偏好对齐)，需要能够捕捉多元性(pluralism, 多元性)的评估和对齐方法。大多数现有的偏好对齐数据集是在与被评估的LLMs显著不同的策略下记录的，而现有的离策略估计器(off-policy estimators, 离策略估计器)仅关注整体效用(utility, 效用)，却忽视了偏好多元性(preference pluralism, 偏好多元性)。因此，将离策略评估(Off-Policy Evaluation, OPE)扩展到多元偏好对齐(pluralistic preference alignment, 多元偏好对齐)仍然是一个开放性问题。为此，我们提出了多元离策略评估(Pluralistic Off-Policy Evaluation, POPE)，这是首个用于LLMs离线(offline, 离线)多元偏好评估和对齐的框架。POPE包含一个统一的奖励函数(reward function, 奖励函数)，该函数结合了(1)源自人类偏好信号（例如，点赞或相关性评分）的协作效用组件(collaborative utility component, 协作效用组件)，以及(2)受基于熵的覆盖度量(entropy-based coverage measures, 基于熵的覆盖度量)启发的多样性组件(diversity component, 多样性组件)，共同反映了多元对齐(pluralistic alignment, 多元对齐)。此外，为了从记录的交互中估计此奖励，我们推导出了可分解的反向倾向评分(inverse propensity scoring, IPS)估计器，该估计器分别评估相关性(relevance, 相关性)和多样性(diversity, 多样性)。理论上，我们证明了我们分解的IPS估计器为其方差建立了下界。通过离策略评估的值函数(value function, 值函数)，我们可以直接启用离策略优化(off-policy optimization, 离策略优化)，以进一步增强多元对齐。实证结果表明，POPE有效增强了多元响应生成(pluralistic response generation, 多元响应生成)，并保持了模型在下游任务(downstream tasks, 下游任务)上的通用能力。"
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究RLHF（Reinforcement Learning from Human Feedback）的优化问题，特别是分析了最大熵强化学习在在线RLHF设置中的失败模式，并探讨了SimPO在离线设置中成功的原因。RLHF是提升大语言模型通用能力的关键训练技术，论文研究的是如何改进这一训练方法，属于\"改进LLM的基础能力、提出新的训练范式\"的范畴。论文直接关注强化学习（RLHF）这一训练方法，符合正面指标。同时，论文不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面）。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。"
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围。首先，从核心判断来看，论文的本质是提升大语言模型的通用推理能力，特别是数学和编程推理能力。论文提出了PromptCoT 2.0框架，通过改进提示合成方法来增强LLM的推理能力，这属于\"改进LLM的基础能力和提出新的训练范式\"的范畴。 其次，论文包含多个正面指标：明确关注\"Large language models (LLMs)\"；核心能力方向是\"reasoning\"，特别是\"math reasoning\"和\"logical reasoning\"；提出了新的训练方法，包括\"Self-Play\"和\"Supervised Fine-Tuning (SFT)\"。 第三，论文不涉及任何排除标准中的领域：没有关注多模态与视觉问题；虽然涉及数学和编程，但这些是通用推理的基础领域而非特定应用领域；也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种可扩展的提示合成框架，通过迭代改进推理过程来生成更难、更多样化的问题，从而提升LLM的推理能力。这种方法从根本上增强了模型的基础推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）正在从对话系统演变为强大的推理工具，用于奥数竞赛和竞技编程等任务。尽管参数扩展（scaling parameters）和测试时计算（test-time computation）推动了进展，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高昂且有限，而现有的合成语料库（synthetic corpora）往往过于简单或范围狭窄。PromptCoT 1.0表明，在提示合成中注入推理过程（rationales）会增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（Expectation-Maximization, EM）循环替代手工设计的启发式方法，其中推理过程被迭代优化以指导提示构建。这产生的问题比之前的语料库更难且更多样化。\n\n这些合成提示支持两种后训练机制：（1）自我对弈（Self-Play），其中强模型通过可验证的反馈在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），其中弱模型从教师蒸馏的轨迹中学习。广泛的实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进结果，在AIME 24/25和HMMT 25上分别提升+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升+6.1和+5.0，在Codeforces上提升+35 Elo。在SFT中，仅使用合成提示训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布上不同的问题。这些结果将提示合成确立为扩展推理的新维度，并将PromptCoT 2.0定位为未来开源模型的可扩展基础。该实现在https://github.com/inclusionAI/PromptCoT上可用。"
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是提出VCRL，一种基于课程学习的强化学习框架，用于提高大语言模型的推理能力。论文本质上是关于改进LLM的基础能力，特别是数学推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文包含多个正面指标，如关注LLMs核心概念、数学推理能力方向以及强化学习训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文主要在数学推理任务上进行实验，但提出的方法是通用的课程学习强化学习框架，通过动态控制训练样本的难度来提高LLM对不同难度样本的学习能力，这与人类从易到难的认知过程一致，可以推广到其他需要推理能力的任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（Policy-based reinforcement learning）目前在提升大语言模型（LLMs）数学推理能力方面发挥着重要作用。然而，现有的基于展开的强化学习方法（rollout-based reinforcement learning methods）（如GRPO、DAPO、GSPO等）未能明确考虑大语言模型对不同难度样本的学习能力，这与人类从易到难的数学推理任务认知过程相悖。直观上，我们发现RLVR中展开组（rollout group）的奖励方差部分反映了当前样本对大语言模型的难度。过易或过难的样本具有较低的方差，而难度适中的样本具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准测试和两个模型上的实验揭示了VCRL相较于当前大语言模型强化学习基线方法的优势。"
                },
                {
                    "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
                    "arxiv_id": "2509.20175",
                    "authors": "Lorenzo Giusti, Ole Anton Werner, Riccardo Taiello, Matilde Carvalho Costa, Emre Tosun, Andrea Protani, Marc Molina, Rodrigo Lopes de Almeida, Paolo Cacace, Diogo Reis Santos, Luigi Serio",
                    "summary": "We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Federation of Agents (FoA)\"的分布式编排框架，用于实现大规模智能体AI的动态协作。从本质上看，论文属于\"智能体协作框架\"的研究范畴，符合筛选标准中的保留条件。论文提出的版本化能力向量(VCVs)、语义路由、动态任务分解和智能聚类等创新方法，都是为了提升智能体系统的通用协作和推理能力，而非将LLM作为工具应用到特定领域。 论文在正面指标上表现良好，涉及了\"multi-agent systems\"这一新兴范式，并明确提到该系统在\"complex reasoning tasks\"上表现出色，这与\"通用推理能力\"的研究目标直接相关。虽然论文没有直接提及\"Large language models\"，但智能体系统通常基于LLM构建，且论文关注的是通用能力的提升。 在排除标准方面，论文没有主要关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然评估中使用了HealthBench数据集，但这仅用于验证系统性能，论文本身并非针对医疗等特定领域的研究。 综合分析，这篇论文提出的是一种通用的智能体协作框架，旨在通过语义感知的通信机制增强智能体系统的协作和推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决大规模多智能体AI系统中能力发现和动态协调的问题。针对异构智能体协作场景，我们提出了一种Federation of Agents (FoA)语义感知通信框架，通过Versioned Capability Vectors实现能力驱动的动态编排，并在HealthBench数据集上通过任务完成质量指标验证了其有效性，相比单模型基线实现了13倍的性能提升。",
                    "summary_translation": "我们提出了代理联盟（Federation of Agents, FoA），这是一个分布式编排框架（distributed orchestration framework），能够将静态的多智能体协调转变为动态的、由能力驱动的协作。FoA引入了版本化能力向量（Versioned Capability Vectors, VCVs）：这是一种机器可读的配置文件，通过语义嵌入（semantic embeddings）使智能体能力可被搜索，使智能体能够宣传其能力、成本和局限性。我们的架构结合了三个关键创新：(1) 语义路由（semantic routing），它在分片的HNSW索引上将任务匹配到智能体，同时通过成本偏置优化（cost-biased optimization）强制执行操作约束；(2) 动态任务分解（dynamic task decomposition），其中兼容的智能体通过基于共识的合并（consensus-based merging）协作地将复杂任务分解为有向无环图（DAGs）的子任务；以及(3) 智能聚类（smart clustering），它将处理相似子任务的智能体分组到协作通道中，在综合之前进行k轮细化。FoA建立在MQTT的发布-订阅语义（publish-subscribe semantics）之上，以实现可扩展的消息传递，并通过分层能力匹配（hierarchical capability matching）和高效的索引维护（efficient index maintenance）实现了次线性复杂度（sub-linear complexity）。在HealthBench上的评估显示，与单模型基线相比有13倍的改进，其中聚类增强的协作（clustering-enhanced collaboration）对于需要多视角的复杂推理任务特别有效。该系统能够水平扩展（scales horizontally）同时保持一致的性能，表明具有结构化协作的语义编排（semantic orchestration）可以释放异构AI代理联盟（heterogeneous federations of AI agents）的集体智能。"
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是提出UserRL框架，通过强化学习训练用户中心的智能体，这属于\"智能体协作框架\"的范畴，是改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。其次，论文包含多个正面指标：明确使用了大语言模型(Qwen3)，采用了强化学习(RL)方法训练模型，研究了基于LLM的智能体(agentic models)，并关注动态多轮交互能力，这些都符合研究目标。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，虽然论文涉及智能体研究，但它提出的是通用的智能体训练框架，而非针对特定领域的应用，因此应予以保留。论文的核心贡献在于探索如何通过奖励塑造和用户模拟选择来提升智能体的通用交互能力，这与提高大语言模型通用推理能力的研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习 (Reinforcement learning, RL) 在训练智能体模型 (agentic models) 方面显示出潜力，这些模型能够超越静态基准测试 (static benchmarks)，进行动态、多轮交互 (dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这一场景中，用户交互的多样性和动态性带来了挑战。在这项工作中，我们提出了UserRL，这是一个通过标准化的gym环境 (gym environments) 配合模拟用户 (simulated users) 来训练和评估以用户为中心能力 (user-centric abilities) 的统一框架。我们系统性地改变轮级奖励分配 (turn-level reward assignment) 和轨迹级分数计算 (trajectory-level score calculation)，以分析不同表述形式如何影响GRPO算法 (GRPO algorithm) 下的学习效果。我们在Qwen3模型 (Qwen3 models) 上的实验揭示了三个关键发现：(i) SFT冷启动 (SFT cold start) 对于解锁初始交互能力和实现持续的RL改进至关重要；(ii) 精心设计的轨迹评分 (deliberate trajectory scoring) 能够产生更高效且有效的多轮交互；(iii) 虽然更强大的模拟用户（如GPT-4o）能够促进训练，但开源模拟器（如Qwen3-32B）仍然是一种经济高效且可迁移的选择。总体而言，这些结果强调，奖励塑造 (reward shaping) 和用户模拟选择 (user simulation choice) 的精心设计与模型规模 (model scale) 同样重要，并将UserRL确立为开发强大的以用户为中心的智能体模型 (robust user-centric agentic models) 的实用途径。所有代码和数据均公开，以供未来研究使用。"
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心贡献是研究大语言模型在认知负荷下的多跳推理能力限制。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文本质是研究LLM本身的推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估基准(ICE)来测试LLM在多跳推理任务上的表现，特别是在认知负荷条件下的性能变化。这不是将LLM应用于特定领域，而是直接研究LLM的基础推理能力，属于改进LLM通用能力的研究。 第二步：正面指标——论文包含关键正面指标：(1)核心概念：明确研究Large Language Models (LLMs)；(2)能力方向：聚焦于multi-hop reasoning（多跳推理），属于逻辑推理范畴。虽然未涉及训练方法和新兴范式，但这两个核心正面指标已足够表明论文与研究方向高度相关。 第三步：排除标准——论文不涉及任何需要排除的领域。它没有研究多模态与视觉问题，没有聚焦于特定应用领域（如医疗、化学等），也没有从应用层面研究模型可靠性。 第四步：特殊和模糊情况——论文提到\"hallucination-as-guessing under uncertainty\"，这是从认知机制角度解释幻觉现象，探讨其与推理能力的关系，而非仅进行社会学研究或应用层面讨论，这有助于理解LLM推理能力的本质限制。 综合来看，这篇论文通过研究认知负荷对LLM推理能力的影响，提出了新的评估方法和理论框架，直接服务于提升LLM通用推理能力的研究目标，完全符合筛选要求。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（LLMs）的扩展揭示了它们在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷（cognitive load）下推理的计算限制仍然知之甚少。在本研究中，我们提出了一个计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换造成的干扰（Attentional Residue，注意力残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令微调（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.2）表现出基线脆弱性（baseline brittleness），在这个高内在负荷（high-intrinsic-load）任务的所有条件下（包括干净的对照组）实现了0%的准确率（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性（partial resilience），在对照条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的性能下降（$\\beta = -0.003$每%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，动态的、具有认知意识的压力测试（cognitive-aware stress testing），如ICE基准测试所示，对于评估先进AI系统的真正韧性和安全性至关重要。"
                },
            ]
        },
    ],
    "2025-09-23": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
                    "arxiv_id": "2509.19236",
                    "authors": "Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang",
                    "summary": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
                    "category": "cs.AI",
                    "filter_reason": "我根据筛选标准对这篇论文进行了全面分析，认为它符合研究范围。以下是详细判断过程： 第一步核心判断：这篇论文的核心是提出AgentInit，一种用于初始化基于大语言模型的多智能体系统的方法。它通过多样性和专业性的协调来优化智能体团队的结构，从而提高系统协作效率和问题解决能力。这属于\"智能体协作框架\"的研究范畴，旨在增强大语言模型的通用能力，而非将其作为工具应用于特定领域。因此，根据第一步标准，应予以保留。 第二步正面指标：论文包含多个相关主题。首先，核心概念方面明确研究\"LLM-based Multi-Agent Systems\"；其次，在新兴范式方面，论文聚焦于多智能体系统的研究；虽然论文没有直接提及推理、规划或强化学习等术语，但其优化多智能体协作的目标本质上是为了提升系统的问题解决能力，这与通用推理能力相关。 第三步排除标准：论文不涉及任何排除领域。它没有关注多模态与视觉内容，不针对医疗、化学、生物等特定应用领域，也不讨论模型可靠性方面如水印、安全等问题。相反，论文强调其方法在\"各种框架和任务\"上的适用性，表明其通用性。 第四步特殊情况处理：论文研究的是智能体协作框架，但提出的是一种通用的初始化方法，而非针对特定领域的应用。它旨在通过优化智能体团队的初始化来增强LLM在多智能体环境下的通用协作能力，因此符合保留条件。 综上所述，这篇论文的核心贡献是提出一种通用的多智能体系统初始化方法，通过优化团队结构和协作机制来增强大语言模型的通用问题解决能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决多智能体系统(MAS)初始化过程中智能体协作效率低下的问题。针对现有方法未充分考虑智能体协作需求的问题，我们提出了一种AgentInit方法，通过标准化智能体生成和平衡团队选择两个模块，联合优化智能体多样性和任务相关性。在MMLU、GSM8K等多个基准测试上验证，AgentInit性能提升高达1.2和1.6，同时显著降低token消耗，展现出强大的迁移能力和适应性。",
                    "summary_translation": "合理的初始化对任何系统都至关重要，尤其是在多智能体系统（multi-agent systems, MAS）中，初始化在决定系统效率与有效性方面起着关键作用。然而，现有的MAS初始化方法未能充分考虑所生成智能体在后续阶段中的协作需求。受高效团队构成原则的启发，我们提出了AgentInit，旨在优化智能体团队的结构。具体而言，除了在智能体生成过程中引入多轮智能体间的交互与反思机制外，AgentInit还引入了一种自然语言到格式化输出的机制（Natural Language to Format mechanism），以确保输出的一致性与标准化。随后，采用基于帕累托原则（Pareto principles）的平衡团队选择策略，综合考虑智能体团队的多样性（diversity）与任务相关性（task relevance），以促进高效且有效的协作，提升系统整体性能。实验结果表明，AgentInit在多种框架和任务下均持续优于当前最先进的初始化方法和预定义策略，整体性能分别提升了最高达1.2和1.6倍，同时显著降低了token消耗。进一步分析验证了其在相似任务中的强可迁移性，并确认了其核心组件的有效性，展示了AgentInit作为一种可靠MAS初始化方法的卓越能力与适应性。源代码与模型可在 https://github.com/1737423697/AgentInit 获取。"
                },
                {
                    "title": "Code Driven Planning with Domain-Adaptive Critic",
                    "arxiv_id": "2509.19077",
                    "authors": "Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing Hu, Yunji Chen",
                    "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准，我进行了如下分析： 第一步核心判断：论文的本质是改进LLM的规划能力（planning），这属于通用推理能力的核心组成部分。论文提出CoPiC方法，通过让LLM生成高级规划程序并配合领域自适应批评者来提升规划质量，这是对LLM基础能力的增强，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注Large Language Models (LLMs)作为任务规划器 - 能力方向：重点研究planning能力，这是推理能力的重要组成部分 - 新兴范式：涉及LLM-based agents，将LLM用于AI智能体的序列决策问题 第三步排除标准：论文不聚焦于排除的领域： - 虽然在ALFWorld、NetHack和StarCraft II等特定环境中进行实验，但这些仅作为验证方法有效性的测试平台，论文核心是提出通用规划框架，而非针对特定领域的应用 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊情况处理：论文提出的是一种通用的智能体规划框架，旨在提升LLM的规划能力，减少查询成本并提高与长期奖励的一致性，这属于应保留的情况。 论文的核心贡献是CoPiC方法，它通过减少LLM查询频率并引入领域自适应批评者来评估候选计划，从而提升LLM的规划能力。这直接关系到提升LLM的通用推理能力，特别是规划和决策方面的能力，符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)作为任务规划器时通用知识与特定环境需求不匹配导致的高查询成本和长期规划问题。针对复杂决策环境，我们提出了一种Code Driven Planning with Domain-Adaptive Critic (CoPiC)框架，结合LLM生成的多样化规划程序和领域自适应评论器选择最优计划，并在ALFWorld、NetHack和StarCraft II Unit Building环境中通过成功率和token成本验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）因其丰富的世界知识，已被广泛用作人工智能代理在序贯决策问题中的任务规划器。然而，LLMs的通用知识与特定环境需求之间存在差距，常常导致生成的规划不准确。为应对这一问题，现有方法依赖频繁调用LLM，根据即时环境反馈迭代优化规划，但这种方式带来了高昂的查询成本。此外，此类优化通常仅依赖短期环境反馈，限制了LLM生成与长期奖励一致的规划能力。\n\n本文提出**基于代码驱动与领域自适应评判器的规划方法**（Code Driven Planning with Domain-Adaptive Critic, CoPiC）。与频繁查询LLM的方法不同，CoPiC利用LLM生成一组多样化的高层规划程序（high-level planning programs），这些程序可迭代地生成并优化候选规划。随后，一个经过训练的领域自适应评判器（domain-adaptive critic）对这些候选规划进行评估，并选择最符合长期奖励的方案予以执行。通过将高层规划程序作为规划器、领域自适应评判器作为评估器，CoPiC在显著降低查询成本的同时提升了规划质量。\n\n在ALFWorld、NetHack和StarCraft II Unit Building三个基准任务上的实验结果表明，CoPiC优于当前先进的基于LLM的基线方法AdaPlanner和Reflexion，平均实现了（1）23.33%的成功率提升，以及（2）91.27%的查询成本降低。"
                },
                {
                    "title": "LongCat-Flash-Thinking Technical Report",
                    "arxiv_id": "2509.18883",
                    "authors": "Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, Yang Liu, Yang Liu, Yang Zheng, Yaoming Wang, Yaqi Huo, Yerui Sun, Yifan Lu, Yiyang Li, Youshao Xiao, Yuanzhe Lei, Yuchen Xie, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunke Zhao, Yuqing Ding, Yuwei Jiang, Zhaohua Yang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhongda Su, Ziran Li, Ziwen Wang, Ziyuan Zhuang, Zongyu Wang, Zunyuan Yang",
                    "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，其核心贡献是提升大语言模型本身的通用推理能力。首先，论文的本质是关于改进LLM的基础推理能力，提出了新的训练范式，包括长思维链(CoT)数据冷启动和大规模强化学习(RL)方法，这直接符合筛选标准中的保留条件。其次，论文包含了多个正面指标：明确研究大语言模型(LLMs)的推理能力，涉及强化学习训练方法，并探讨了智能体推理(agentic reasoning)这一新兴范式。第三，论文没有聚焦于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。特别值得注意的是，虽然论文提到了STEM、代码和智能体等不同领域，但这是作为其领域并行训练方案的一部分，目的是增强模型的通用推理能力，而非将LLM应用于特定领域解决问题。论文的核心创新——领域并行训练方案和DORA系统——都是为了提升模型的基础推理能力，使其在复杂推理任务上达到最先进性能。因此，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在提升大型语言模型的推理能力。针对复杂推理任务场景，我们提出了LongCat-Flash-Thinking，一个5600亿参数的MoE推理模型，通过长链思维数据冷启动和大规模强化学习培养高级推理能力，核心创新包括领域并行训练方案和DORA系统。在HMMT-25、ARC-AGI、τ²-Bench等多个推理基准上通过Mean@32、Pass@1等指标验证了其有效性，在AIME-25上实现了64.5%的token消耗降低。",
                    "summary_translation": "我们提出了LongCat-Flash-Thinking，一个高效的5600亿参数开源专家混合（Mixture-of-Experts, MoE）推理模型。其先进能力是通过精心设计的训练过程培养的，从长思维链（Chain-of-Thought, CoT）数据冷启动开始，到大规模强化学习（Reinforcement Learning, RL）结束。我们首先采用精心设计的冷启动训练策略，显著提升了推理潜力，并使模型具备形式推理和智能体推理（agentic reasoning）的专业技能。然后，核心创新是我们的领域并行训练方案，该方案解耦了不同领域（如STEM、代码、智能体）的优化，随后将产生的专家模型融合成单一的接近帕累托最优（Pareto-optimal）模型。整个过程由我们的动态异步编排（Dynamic ORchestration for Asynchronous rollout, DORA）系统驱动，这是一个大规模强化学习框架，在数万个加速器上比同步方法实现了超过三倍的训练加速。因此，LongCat-Flash-Thinking在一系列复杂推理任务上实现了开源模型中的最先进性能。该模型在智能体推理方面表现出卓越的效率，在AIME-25上平均令牌消耗减少了64.5%（从19,653降至6,965），同时不降低任务准确性。我们发布LongCat-Flash-Thinking以促进推理系统和智能体人工智能（agentic AI）研究的进一步发展。"
                },
                {
                    "title": "MAPO: Mixed Advantage Policy Optimization",
                    "arxiv_id": "2509.18849",
                    "authors": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
                    "summary": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为MAPO（Mixed Advantage Policy Optimization）的新强化学习策略，用于改进GRPO（Group Relative Policy Optimization）方法，以解决其在推理任务中遇到的advantage reversion和advantage mirror问题。论文通过引入advantage percent deviation和动态重加权advantage function来优化不同查询样本间的advantage分配，从而提升基础模型在推理任务上的性能。 这完全符合研究目标中的\"改进LLM的基础能力\"和\"提出新的训练范式\"，特别是强化学习优化方面的研究。论文明确关注\"reasoning tasks\"，这是筛选标准中的核心能力方向之一。同时，论文属于强化学习（RL）训练方法的研究，这也是正面指标中明确提到的重要内容。 论文不属于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。它专注于提升基础模型在通用推理任务上的性能，而不是将模型应用到特定领域。因此，这篇论文应该被保留，它对提升大语言模型的通用推理能力有直接贡献。",
                    "summary2": "本文旨在解决GRPO中优势函数面临的\"优势反转\"和\"优势镜像\"问题。针对不同轨迹确定性的样本，我们提出了一种混合优势策略优化(MAPO)方法，通过引入优势百分比偏差处理高确定性轨迹，并基于轨迹确定性动态重新加权优势函数。在Geo3K和EmoSet等多个数据集上使用Qwen2.5-VL-7B架构，通过准确率等指标验证了MAPO能够有效提升基础模型在推理任务上的稳定性和准确性。",
                    "summary_translation": "基础模型（foundation models）在强化学习领域的最新进展，如群组相对策略优化（Group Relative Policy Optimization, GRPO），显著提升了基础模型在推理任务上的性能。值得注意的是，优势函数（advantage function）作为GRPO中的核心机制，用于排序轨迹（trajectory）的重要性。然而，现有探索同时遇到优势反转（advantage reversion）和优势镜像（advantage mirror）问题，这些问题阻碍了在不同查询样本（query samples）间进行合理的优势分配。在这项工作中，我们提出了一种简单但有效的GRPO策略，即混合优势策略优化（Mixed Advantage Policy Optimization, MAPO）。我们揭示了轨迹以不同的确定性（certainty）出现，并为具有高确定性轨迹的样本提出了优势百分比偏差（advantage percent deviation）。此外，我们对具有不同轨迹确定性的样本动态重新加权优势函数，从而自适应地配置优势函数以考虑样本特定特征。与相关最先进（state-of-the-art）方法的比较，以及对不同优势变体（advantage variants）的消融研究（ablation studies），验证了我们方法的有效性。"
                },
                {
                    "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
                    "arxiv_id": "2509.18868",
                    "authors": "Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang, Sheng Li",
                    "summary": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型的记忆机制，提出了统一的操作定义、四部分分类法（参数化、上下文、外部、程序性/情景性）和记忆四元组（位置、持久性、写入/访问路径、可控性）。虽然论文没有直接讨论推理能力，但记忆是推理的基础能力之一，模型需要有效记忆信息并在需要时检索使用，才能进行复杂的推理任务。论文链接了机制、评估和治理，提出了三设置协议和分层评估框架，以及更新和遗忘策略（如DMM Gov），这些都是为了增强LLM的基础能力，从而间接提升其通用推理能力。论文不符合排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在 [建立大语言模型记忆的统一分析框架，解决记忆概念边界模糊和评估碎片化等挑战]。针对 [LLM记忆全生命周期]，我们提出了一种 [四分类记忆类型（参数、上下文、外部和程序/情景记忆）和三设置并行评估协议]，并在 [多种公开数据集和benchmark] 上通过 [分层评估指标和DMM-Gov动态治理框架] 验证了其有效性。",
                    "summary_translation": "在统一的操作定义下，我们将LLM记忆（大语言模型记忆）定义为在预训练、微调或推理过程中写入的持久状态，该状态可被后续访问并稳定影响输出。我们提出了一个四部分分类法（参数型parametric、上下文型contextual、外部型external、程序型/情景型procedural/episodic）和一个记忆四元组（位置location、持久性persistence、写入/访问路径write/access path、可控性controllability）。我们通过写入->读取->抑制/更新的链条将机制、评估和治理联系起来。为避免在不同设置间产生扭曲的比较，我们采用了三设置协议（仅参数型parametric only、离线检索offline retrieval、在线检索online retrieval），该协议在同一数据和时间线上将能力与信息可用性解耦。在此基础上，我们构建了分层评估：参数型评估（闭卷召回closed-book recall、编辑差异edit differential、记忆化/隐私memorization/privacy）、上下文型评估（位置曲线和序列中段下降position curves and the mid-sequence drop）、外部型评估（答案正确性与片段归因/忠实性answer correctness vs snippet attribution/faithfulness），以及程序型/情景型评估（跨会话一致性和时间线重放cross-session consistency and timeline replay, E MARS+）。该框架整合了时间治理和泄漏审计（新鲜度命中freshness hits、过时答案outdated answers、拒绝切片refusal slices）以及通过评估者间一致性加上多重比较校正的配对测试进行的不确定性报告。针对更新和遗忘，我们提出了DMM Gov（动态记忆管理治理）：协调DAPT/TAPT（领域自适应预训练/任务自适应预训练）、PEFT（参数高效微调）、模型编辑（ROME, MEND, MEMIT, SERAC）和RAG（检索增强生成），形成一个可审计的循环，涵盖准入阈值、部署、监控、回滚和变更审计，并规定了及时性、冲突处理和长期一致性的规范。最后，我们提出了四个可测试的命题：最小可识别性；最小评估卡片；具有可验证遗忘的因果约束编辑；以及何时小窗口重放的检索优于超长上下文阅读。这产生了一个可重现、可比较和可治理的研究与部署坐标系。"
                },
                {
                    "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
                    "arxiv_id": "2509.18771",
                    "authors": "Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du",
                    "summary": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。在第一步核心判断中，论文本质上是提出一种名为\"经验缩放\"(Experience Scaling)的新框架，用于大语言模型部署后的持续进化，这直接符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。论文关注LLM通过自主与环境交互和协作共享积累的经验来提升自身能力，这是一种增强LLM通用推理能力的方法论研究。 在第二步正面指标分析中，论文明确包含核心概念\"Large Language Models (LLMs)\"；涉及训练方法中的\"evolution\"和\"self-evolve\"概念（标题中直接提及\"Post-Deployment Evolution\"）；同时提到\"autonomous interaction with the environment\"暗示了agent概念；论文强调\"对以前未见但相关任务的泛化\"，这间接涉及到推理和问题解决能力。 在第三步排除标准检查中，论文完全不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 在第四步特殊情况处理中，论文提到的\"autonomous interaction with the environment\"属于通用智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种让LLM通过部署后的持续学习和经验积累来提升自身通用能力的框架，这种方法超越了静态人类生成数据的限制，为提升LLM的通用推理能力提供了新的路径，完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型（LLM）在传统扩展方法达到饱和后的持续进化问题。针对部署后的LLM与环境交互数据，我们提出了一种Experience Scaling框架，通过捕获原始交互、提炼为可重用知识并定期优化存储内容，实现LLM的自主进化。在MMLU和SciQ基准上通过准确性、时间效率和泛化能力等指标验证了其有效性，结果表明该框架能提高准确性、维持长期性能并在新场景中保持收益。",
                    "summary_translation": "扩大模型规模、训练数据和计算能力推动了大型语言模型（large language models, LLMs）的进步，但随着人类生成文本的耗尽和进一步收益的减少，这些方法正达到饱和状态。我们提出了经验扩展（experience scaling）框架，这是一个通过与环境自主交互和协作共享积累经验，实现大型语言模型部署后持续进化的框架。该框架捕获原始交互，将其提炼为紧凑、可重用的知识，并定期优化存储内容以保持相关性和效率。我们在模拟的真实世界场景中验证了该框架，这些场景涉及对以前未见但相关任务的泛化、重复性查询和过度饱和的知识存储。在所有设置中，经验扩展（experience scaling）提高了准确性，随时间推移保持性能，并在应用于新情况时保持收益。这些结果表明，结构化的部署后学习可以将大型语言模型的能力扩展到静态人类生成数据的限制之外，为持续智能进步提供了一条可扩展的路径。"
                },
                {
                    "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
                    "arxiv_id": "2509.18565",
                    "authors": "Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita",
                    "summary": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，该论文的本质是提升LLM的数学推理能力，属于改进LLM基础能力和增强其逻辑、数学推理等通用能力的范畴。论文提出了一种结合方程生成和估计验证的新方法，通过外部符号方程求解器和LLM的估计能力相互验证，提高数学问题求解的准确性，这直接针对LLM在推理方面的能力提升。 其次，论文满足多个正面指标：核心概念明确涉及Large language models (LLMs)；能力方向专注于数学推理(math reasoning)，这正是通用推理能力的重要组成部分；同时采用了工具使用(tool use)的新兴范式，即使用外部符号方程求解器来辅助LLM解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学文字问题，但数学推理被视为基础能力而非特定应用领域；也没有涉及模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文中使用的外部符号方程求解器作为工具，是为了增强LLM的通用数学问题解决能力，而不是应用在特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种提升LLM数学推理能力的新方法，通过估计验证和方程生成相结合的方式，显著提高了LLM在解决数学文字问题上的表现，这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在提高大型语言模型解决数学应用题的准确性。针对数学应用题求解场景，我们提出了一种EVoSS方法，结合方程生成和估计验证技术，通过符号求解器获得精确解后，利用估计值进行验证，并在GSM8K、SVAMP和Algebra等数据集上通过精确匹配指标验证了其有效性，实现了平均近2%的性能提升，并创建了新的Trig300数据集用于三角函数问题测试。",
                    "summary_translation": "\n大型语言模型（Large Language Models, LLMs）在各种任务上表现出色，包括问题解决和问答。然而，大型语言模型通常发现数学应用题（Math Word Problems, MWPs）具有挑战性，因为解决这些问题需要一系列推理和数学能力，而这些能力似乎是大型语言模型所欠缺的。近来的努力通过改进提示词（prompts）帮助大型语言模型解决更复杂的数学应用题。本研究提出了一种新方法，首先提示大型语言模型从问题的分解中创建方程，然后使用外部符号方程求解器（symbolic equation solver）来产生答案。为了确保获得答案的准确性，受数学教师既定建议的启发，大型语言模型被指示第二次解决数学应用题，但这次的目标是估计正确答案，而不是精确求解。然后将估计值与生成的答案进行比较以进行验证。如果验证失败，则采用迭代修正过程（iterative rectification process）以确保最终找到正确答案。这种方法在先前发表的关于数值和代数数学应用题的研究所使用的数据集上取得了新的最先进结果，平均将之前的最佳结果提高了近两个百分点。此外，该方法在三角数学应用题（trigonometric MWPs）上取得了令人满意的结果，据作者所知，这是之前未曾尝试过的任务。本研究还介绍了两个新数据集SVAMPClean和Trig300，以进一步推进对大型语言模型推理能力的测试。"
                },
                {
                    "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?",
                    "arxiv_id": "2509.18383",
                    "authors": "Moran Feldman, Amin Karbasi",
                    "summary": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the Gödel Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the Gödel Test.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是评估和探讨大语言模型(GPT-5)的数学推理能力，特别是解决简单数学猜想的能力，这直接涉及LLM的基础推理能力提升，而非将LLM作为工具应用于特定领域。论文提出的\"Gödel Test\"旨在评估模型产生正确证明的能力，这属于通用推理能力的核心范畴。 其次，论文满足多个正面指标：明确以大型语言模型(LLMs)为研究对象；专注于推理能力(reasoning)，特别是数学推理(math reasoning)；涉及问题解决(problem-solving)等关键能力方向。 第三，论文不符合任何排除标准。虽然研究内容涉及数学领域，但数学在这里是作为评估LLM推理能力的测试场，而非作为特定应用领域。论文的核心不是解决数学问题本身，而是研究LLM的推理能力表现和局限。 论文的核心贡献在于提出了一种评估LLM高级推理能力的新方法(Gödel Test)，并通过实验揭示了当前模型在数学推理方面的进展、原创性闪光点以及局限性。这直接服务于提升LLM通用推理能力的研究目标，因此应该被保留。",
                    "summary2": "本文旨在评估大型语言模型解决简单数学猜想的能力。针对组合优化领域中的五个未解决猜想，我们提出了Gödel Test评估框架，并通过检查GPT-5生成的证明正确性验证了其性能。实验表明GPT-5在需要单一推理路径的问题上表现良好，但在需要跨论文综合推理的问题上存在明显局限。",
                    "summary_translation": "前沿AI模型实验室最近的公告强调了在高中和本科数学竞赛中取得的显著成果。然而，大语言模型（large language models）是否能够解决更高阶数学领域中的新颖简单猜想（conjectures），目前仍不清楚。我们提出了哥德尔测试（Gödel Test）：评估模型是否能够为非常简单但先前未解决的猜想（previously unsolved conjectures）产生正确的证明。为此，我们研究了GPT-5在组合优化（combinatorial optimization）领域中的五个猜想上的表现。对于每个问题，我们提供了一到两篇提出该猜想的源论文（source papers），保留了我们自己的猜想，然后详细评估了模型的推理过程。\n\n在三个较简单的问题上，GPT-5产生了几乎正确的解决方案；对于问题2，它甚至推导出了一个不同的近似保证（approximation guarantee），经过检查，这一保证反驳了我们的猜想，同时提供了一个有效的解决方案。该模型在问题4上失败了，该问题需要结合两篇论文的结果。在问题5上，这是一个没有已验证猜想（validated conjecture）的更难案例，GPT-5提出了与我们心中相同的算法（algorithm），但在分析（analysis）环节失败了，这表明证明比预期的更具挑战性。\n\n尽管我们的样本量很小，但结果表明在常规推理（routine reasoning）方面取得了有意义的进展，偶尔展现出创造性闪光（flashes of originality），而在需要跨论文综合（cross-paper synthesis）时则存在明显的局限性。GPT-5可能是前沿模型最终通过哥德尔测试（Gödel Test）的早期一步。"
                },
                {
                    "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
                    "arxiv_id": "2509.18382",
                    "authors": "Adarsha Balaji, Le Chen, Rajeev Thakur, Franck Cappello, Sandeep Madireddy",
                    "summary": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究如何在计算约束条件下提升大型推理模型的推理能力和安全性，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文主要探讨了通过思维链（CoT）和强化学习（LCPO方法）来优化模型的推理能力，这属于改进LLM基础能力和提出新训练范式的范畴。论文明确关注推理能力这一核心指标，并研究了如何通过强化学习方法来控制推理长度，同时保持模型的安全性。论文不是将LLM作为工具应用到特定领域，而是关注LLM本身的通用推理能力提升。虽然论文涉及安全性研究，但这是从提升模型内在推理质量和可靠性的角度出发，而非仅作为应用层面的防御。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决计算约束下大型推理模型（LRMs）的安全性与技能推理平衡问题。针对推理长度和计算资源限制场景，我们提出了两种计算约束策略：基于强化学习的长度控制策略优化（LCPO）和模型量化（GPTQ），并在科学、数学和安全推理基准（GPQA, MATH500, AIME, StrongReject）上通过pass@1和safe@1指标验证了其有效性。",
                    "summary_translation": "测试时计算扩展(test-time compute scaling)已证明能够通过生成更长的思维链(chain-of-thought, CoT)序列来提高推理语言模型(reasoning language models)的性能。然而，这种性能提升伴随着计算成本(computational cost)的显著增加。在本研究中，我们调查了两种计算约束策略(compute constraint strategies)：(1)推理长度约束(reasoning length constraint)和(2)模型量化(model quantization)，作为降低推理模型计算需求并研究其对安全性能(safety performance)影响的方法。具体而言，我们探索了两种将计算约束应用于推理模型的方法：(1)使用基于长度控制策略优化(length controlled policy optimization, LCPO)的强化学习(reinforcement learning)方法微调推理模型，以满足用户定义的CoT推理长度(user-defined CoT reasoning length)；(2)应用量化技术(quantization)以在用户定义的计算约束(user-defined compute constraint)内最大化CoT序列的生成。此外，我们还研究了模型的计算效率(computational efficiency)与安全性(safety)之间的权衡(trade-off)。"
                },
                {
                    "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
                    "arxiv_id": "2509.18851",
                    "authors": "Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu",
                    "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出NGRPO算法，一种改进的强化学习方法，用于增强大语言模型的推理能力。论文针对GRPO算法在处理同质响应组（全部正确或全部错误）时的局限性，提出了两个关键机制：Advantage Calibration和Asymmetric Clipping，使模型能够从同质错误中学习。这直接符合研究目标，因为：(1)论文本质上是改进LLM的基础推理能力，特别是数学推理能力；(2)它提出新的训练范式（强化学习优化）；(3)实验证明该方法能显著提升LLM在多个数学基准测试上的表现。论文不涉及特定应用领域、多模态研究或模型基础设施优化，完全符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决 GRPO 算法在处理同质响应组时无法学习的问题。针对同质错误组导致零梯度的问题，我们提出了一种 NGRPO 方法，通过 Advantage Calibration 和 Asymmetric Clipping 两个核心机制将同质错误转换为稳健学习信号，并在 Qwen2.5-Math-7B 模型上通过 MATH500、AMC23 和 AIME2025 数学基准测试的 Pass@k AUC 指标验证了其有效性。",
                    "summary_translation": "RLVR (强化学习与验证推理) 已增强大型语言模型 (LLMs) 在各种任务中的推理能力。然而，作为代表性RLVR算法的GRPO (组相对策略优化) 存在一个关键限制：当组内所有响应要么完全正确要么完全错误时，模型无法从这些同质响应中学习。对于同质错误组，这一问题尤为严重，因为GRPO的优势函数 (advantage function) 会产生零值，导致零梯度 (null gradients) 并损失有价值的学习信号。为克服这一问题，我们提出了NGRPO (负增强组相对策略优化)，这是一种旨在将同质错误转化为稳健学习信号的算法。首先，NGRPO引入了优势校准 (Advantage Calibration) 机制。该机制假设在优势计算过程中存在一个虚拟最大奖励样本 (virtual maximum-reward sample)，从而改变组内奖励的均值和方差，确保同质错误样本的优势不再为零。其次，NGRPO采用非对称裁剪 (Asymmetric Clipping)，放宽对正样本 (positive samples) 的更新幅度限制，同时对负样本 (negative samples) 施加更严格的约束。这有助于稳定由优势校准引入的探索压力 (exploration pressure)。我们在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试 (mathematical benchmarks) 上显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法 (baselines)。这些结果验证了NGRPO从同质错误中学习的能力，从而在数学推理方面实现了稳定且实质性的改进。我们的代码可在https://github.com/nangongrui-ngr/NGRPO获取。"
                },
                {
                    "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
                    "arxiv_id": "2509.18542",
                    "authors": "Qi Wang, Hanyang Peng, Yue Yu",
                    "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的Mixture-of-Experts (MoE)模型构建方法Symphony-MoE，该方法通过整合多个不同但架构相同的预训练模型（如Llama2-Chat和Code Llama）来提高整体性能。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM的基础架构和能力。它提出了一种两阶段框架来解决不同预训练模型在参数空间中的不协调问题，通过层感知融合策略和基于激活的功能对齐来协调这些模型，然后进行轻量级的路由器训练。论文提到其方法在\"多领域任务\"和\"分布外泛化\"方面取得了显著成果，这表明它关注的是模型的通用能力，而非特定领域应用。虽然论文没有直接提到推理、规划等具体能力，但它提出的架构改进本质上是为了提升LLM的通用能力，符合研究目标中\"改进LLM的基础能力、提出新的训练范式\"的要求。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决如何将多个架构相同但训练历史不同的预训练模型有效地整合成一个强大的MoE模型的问题。针对多个异构预训练模型，我们提出了一种Symphony-MoE两阶段框架，通过无训练的功能对齐和轻量级路由器训练来协调不同模型，并在MMLU、GSM8K、BBH、HumanEval、TruthfulQA和MedCQA等多个数据集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "混合专家(Mixture-of-Experts, MoE)模型通过稀疏激活大型参数集实现可扩展性能，同时最小化计算开销。为规避从头训练MoE模型的过高成本，近期工作采用升级改造(upcycling)方法，通过将单个预训练密集模型的前馈网络(feed-forward network, FFN)层复制成专家来重用该模型。然而，这种方法限制了专家多样性，因为所有专家都源自于单个预训练密集模型。本文通过使用来源于多个架构相同但不同的预训练模型（如Llama2-Chat和Code Llama）的专家来构建强大的MoE模型，从而解决这一限制。\n\n一个关键挑战在于，这些源模型在参数空间(parameter space)中占据不同且不协调的区域，使得直接升级改造容易导致严重性能下降。为克服这一问题，我们提出了Symphony-MoE，这是一个新颖的两阶段框架，旨在将这些模型协调成一个单一、连贯的专家混合体。首先，我们以无训练方式建立这种协调：我们通过层感知融合策略构建共享骨干网络，并且关键的是，使用基于激活的功能对齐(activation-based functional alignment)来缓解专家之间的参数不对齐问题。随后，一个轻量级的路由器(router)训练阶段协调整个架构。\n\n实验表明，我们的方法成功整合了来自异构源的专家，构建出的MoE模型在多领域任务和分布外(out-of-distribution)泛化方面显著超越基线模型。"
                },
                {
                    "title": "Self-Evolving LLMs via Continual Instruction Tuning",
                    "arxiv_id": "2509.18133",
                    "authors": "Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan Shi, Ting Bai",
                    "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出MoE-CL框架，一种用于大语言模型自进化的参数高效对抗性混合专家框架。论文直接针对LLM的基础能力——持续学习和自进化能力进行研究，提出通过双专家设计（每个任务的专用LoRA专家和共享LoRA专家）以及GAN中的任务感知判别器来解决持续学习中的灾难性遗忘问题。这明显符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为自进化能力是LLM通用推理能力的重要组成部分。论文虽然提到了在Tencent Video平台的应用案例，但这只是验证方法有效性的实验，论文的核心并不是针对特定应用领域的研究，而是提出了一种通用的训练范式来增强LLM的基础能力。论文明确涉及\"self-evolution\"这一正面指标，并且不涉及任何排除标准中的领域。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在持续学习中面临的灾难性遗忘问题，实现LLMs的自我进化能力。针对工业场景中多样化任务需求，我们提出了一种名为MoE-CL的对抗性LoRA专家混合框架，并在公共MTL5基准和工业Tencent3基准上通过Accuracy、Backward Transfer和Forward Transfer指标验证了其有效性。在腾讯视频平台的内容合规审查A/B测试中，MoE-CL将人工审核成本降低了15.3%。",
                    "summary_translation": "在真实工业环境中，大型语言模型（large language models, LLMs）必须持续学习以跟上多样化和不断发展的任务，需要在动态数据分布下进行自我进化（self-evolution）以完善知识。然而，现有的持续学习（continual learning, CL）方法，如回放（replay）和参数隔离（parameter isolation），常常遭受灾难性遗忘（catastrophic forgetting）：在新任务上的训练会因过度拟合新分布而降低在早期任务上的性能，并削弱泛化能力。\n\n我们提出了MoE-CL，一个参数高效的对抗性专家混合（adversarial mixture-of-experts）框架，用于工业规模、自我进化的LLM持续指令调优（continual instruction tuning）。MoE-CL采用双专家设计：（1）每个任务一个专用的LoRA专家，通过参数独立性保留任务特定知识，减轻遗忘；以及（2）一个共享的LoRA专家，以实现跨任务迁移。为防止通过共享路径传递任务无关噪声，我们在生成对抗网络（GAN）中集成了一个任务感知判别器（task-aware discriminator）。判别器鼓励共享专家在顺序训练过程中仅传递与任务对齐的信息。通过对抗学习（adversarial learning），共享专家获得模仿判别器的泛化表示，而专用专家保留任务特定细节，平衡知识保留和跨任务泛化，从而支持自我进化。\n\n在公开的MTL5基准测试和工业级Tencent3基准测试上的大量实验验证了MoE-CL在持续指令调优方面的有效性。在腾讯视频平台内容合规审查的真实A/B测试中，MoE-CL将人工审查成本降低了15.3%。这些结果表明，MoE-CL适用于大规模工业部署，特别是在持续适应和稳定迁移至关重要的场景中。"
                },
                {
                    "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
                    "arxiv_id": "2509.18116",
                    "authors": "Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary",
                    "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文提出了\"Amortized Latent Steering (ALS)\"方法，这是一种新的训练/推理范式，通过潜在空间引导来增强模型的推理能力，特别是数学推理能力。ALS将测试时优化的迭代过程转化为离线计算的向量，在推理过程中以恒定成本应用，从而在保持或提高推理性能的同时显著降低计算成本。这明确符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：论文虽然未在摘要中明确提到\"Large language models\"，但从上下文（如提到Chain-of-Thought, Self-Consistency等LLM相关技术）可以推断这是针对LLM的研究。 - 能力方向：论文明确在GSM8K和MATH-500这两个数学推理基准测试上评估方法，表明论文关注数学推理能力，并提到\"sophisticated reasoning techniques\"，进一步确认其与推理相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学、生物等）或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出了一种提高LLM数学推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决测试时优化计算成本过高的问题。针对数学推理任务的高计算开销场景，我们提出了一种摊销潜在转向（ALS）方法，通过离线计算成功与失败生成间的隐藏状态差异向量，并在推理时以恒定成本应用该向量校准模型表示。在GSM8K和MATH-500基准上通过效率-准确性权衡指标验证了其有效性，实现了2-5倍加速同时保持或超越基线性能。",
                    "summary_translation": "测试时优化（Test-time optimization）由于高昂的推理成本在大规模应用中仍然不切实际——诸如迭代细化（iterative refinement）和多步验证（multi-step verification）等技术可能需要比标准解码（standard decoding）多10-100倍的计算量。像LatentSeek这样的潜在空间测试时优化方法（Latent space test-time optimization methods）通过引导隐藏表示（steering hidden representations）提供了一种更直接的方法，但仍然需要昂贵的每次查询优化循环（per-query optimization loops）和多次反向传播（multiple backward passes）。我们提出了摊销潜在引导（Amortized Latent Steering, ALS），它将这种迭代优化折叠成一个在推理过程中以恒定成本应用的离线计算向量（offline-computed vector）。ALS计算成功生成与不成功生成之间的隐藏状态平均差异，然后使用这个方向来校准模型的隐藏表示（hidden representations）：当解码偏离成功流形（success manifold）时，ALS会将激活值（activations）推回该流形。在GSM8K和MATH-$500$基准测试中，ALS比迭代方法实现了2-5倍的加速，同时匹配或超过了贪心思维链（greedy Chain-of-Thought, CoT）和自我一致性（Self-Consistency）基线，使效率-准确率权衡（efficiency--accuracy trade-off）提高了高达101%。这些结果表明，潜在优化（latent optimization）的大部分好处可以离线捕获，使复杂的推理技术（sophisticated reasoning techniques）能够在生产部署中变得可行。代码可在~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}获取。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 13,
            "papers": [
                {
                    "title": "Reinforcement Learning on Pre-Training Data",
                    "arxiv_id": "2509.19249",
                    "authors": "Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang",
                    "summary": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心是提出一种名为RLPT（Reinforcement Learning on Pre-Training Data）的新训练范式，用于提升大语言模型的通用推理能力。从第一步核心判断来看，论文本质上是改进LLM的基础能力，提出新的强化学习训练方法，旨在增强模型的通用推理技能，而非将LLM应用于特定领域。论文明确提到RLPT\"鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更通用的推理技能\"，并在数学推理等通用能力基准测试中验证了有效性。 从第二步正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(特别是数学推理)；训练方法上提出了基于强化学习的新范式(RLPT)，并与RLHF、RLVR等方法进行了比较。 第三步排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容，没有任何排除因素。 综上所述，这篇论文的核心贡献是提出一种新的训练范式来增强LLM的通用推理能力，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决计算资源指数级增长与高质量文本数据有限增长之间的差距限制LLMs传统扩展方法的问题。针对预训练数据，我们提出了一种Reinforcement Learning on Pre-Training Data (RLPT)方法，采用next-segment reasoning目标，包括ASR和MSR任务，直接从预训练数据中获取奖励信号，并在通用领域和数学推理基准测试上通过准确率和Pass@k指标验证了其有效性。",
                    "summary_translation": "计算资源的指数级扩展与高质量文本数据的有限增长之间的日益扩大的差距，现在正限制着大型语言模型（LLMs）的传统扩展方法。为应对这一挑战，我们提出了预训练数据强化学习（Reinforcement Learning on Pre-Training data, RLPT），这是一种用于优化LLMs的新型训练时扩展范式。与先前主要通过监督学习（supervised learning）扩展训练的方法不同，RLPT使策略能够自主探索有意义的轨迹，从预训练数据中学习，并通过强化学习（reinforcement learning, RL）提升其能力。虽然现有的强化学习策略，如基于人类反馈的强化学习（reinforcement learning from human feedback, RLHF）和可验证奖励强化学习（reinforcement learning with verifiable rewards, RLVR）依赖于人类标注来构建奖励，但RLPT通过直接从预训练数据中导出奖励信号，消除了这种依赖性。具体而言，它采用下一段推理目标（next-segment reasoning objective），奖励策略基于前文上下文准确预测后续文本段。这种表述方式允许强化学习在预训练数据上进行扩展，鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更具泛化性的推理技能。在多个模型的通用领域和数学推理基准上进行的大量实验验证了RLPT的有效性。例如，当应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别实现了$3.0$、$5.1$、$8.1$、$6.0$、$6.6$和$5.3$的绝对提升。结果进一步展示了良好的扩展行为，表明随着更多计算资源的投入，持续提升的潜力巨大。此外，RLPT提供了坚实的基础，扩展了LLMs的推理边界，并增强了RLVR的性能。"
                },
                {
                    "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
                    "arxiv_id": "2509.19199",
                    "authors": "Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao",
                    "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments. However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging. Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare. We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels. In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective. These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop. Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training. Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA. Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training. Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究范围，核心贡献是提出一种新的强化学习方法(OPRL)来增强大语言模型作为智能体时的通用推理能力。 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力的研究。它提出了\"在线过程奖励学习\"(OPRL)这一新的训练范式，专注于解决LLM作为自主智能体在长期任务中的信用分配问题，这直接属于提升LLM通用推理能力的范畴。 从第二步正面指标来看，论文涵盖了所有关键主题：明确以大语言模型(LLMs)为核心研究对象；关注模型的推理能力(\"reason and act over long horizons\")；采用强化学习作为主要训练方法；并聚焦于基于LLM的智能体(\"agentic reinforcement learning\")研究。 从第三步排除标准来看，虽然论文在评估中使用了WebShop、VisualSokoban等特定环境，但这些只是用来验证通用方法的应用场景，论文本身并不专注于任何特定应用领域或多模态研究，其核心贡献是通用的强化学习方法。 从第四步特殊和模糊情况处理来看，论文提出的是一种通用的智能体学习方法，旨在增强LLM的通用问题解决和推理能力，而非针对特定领域的应用。 综上所述，这篇论文通过提出新的强化学习范式来提升LLM的长期推理和决策能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型作为智能体在强化学习中面临的稀疏奖励和信用分配挑战。针对长时程交互任务，我们提出了一种在线过程奖励学习(OPRL)方法，通过优化隐式过程奖励模型将轨迹偏好转换为步骤级奖励，并在WebShop、VisualSokoban和SOTOPIA三个基准测试上通过成功率、分数和目标完成分数等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）越来越多地通过强化学习（Reinforcement Learning, RL）进行训练，作为在交互环境中进行长期推理和行动的自主代理。然而，稀疏且有时不可验证的奖励使得时间信用分配（temporal credit assignment）极具挑战性。近期工作尝试将过程监督（process supervision）整合到代理学习中，但存在有偏标注、奖励劫持（reward hacking）、过度细粒度信号导致的高方差或状态重叠较少时的失败等问题。\n\n因此，我们提出了在线过程奖励学习（Online Process Reward Learning, OPRL），这是一种用于代理强化学习（agentic RL）的通用信用分配策略，可与标准在策略算法（on-policy algorithms）无缝集成，而无需依赖额外的推演（rollouts）或显式步骤标签。在OPRL中，我们交替优化隐式过程奖励模型（implicit process reward model, PRM）和代理策略，通过基于轨迹的DPO目标（trajectory-based DPO objective）将轨迹偏好转化为隐式步骤奖励。这些步骤奖励随后用于计算步骤级优势（step-level advantages），与来自结果奖励的回合级优势（episode-level advantages）相结合用于策略更新，形成一个自我强化循环（self-reinforcing loop）。\n\n理论研究保证，学习到的步骤奖励与轨迹偏好一致，并作为基于势能的塑形奖励（potential-based shaping rewards），提供有界梯度以稳定训练。在实证方面，我们在三个不同的代理基准测试（agent benchmarks）上评估了OPRL，包括WebShop和VisualSokoban，以及在SOTOPIA中具有不可验证奖励的开放式社交互动（open-ended social interactions）。关键的是，OPRL在各个领域都表现出优于前沿LLMs和强大RL基线的性能，以更高的样本效率（sample-efficiency）和更低的训练方差实现了最先进的结果。进一步分析还表明，OPRL通过使用更少的动作实现了高效探索（efficient exploration），强调了其在现实场景中代理学习的潜力。"
                },
                {
                    "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
                    "arxiv_id": "2509.19094",
                    "authors": "Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky",
                    "summary": "Personalization is essential for adapting question answering (QA) systems to user-specific information needs, thereby improving both accuracy and user satisfaction. However, personalized QA remains relatively underexplored due to challenges such as inferring preferences from long, noisy, and implicit contexts, and generating responses that are simultaneously correct, contextually appropriate, and aligned with user expectations and background knowledge. To address these challenges, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without requiring task-specific fine-tuning. The approach models the reasoning of an LLM as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark for personalized QA show that PoT consistently outperforms competitive baselines, achieving up to a 13.1% relative improvement. Human evaluation corroborates these results, with annotators preferring outputs from PoT in 66% of cases and reporting ties in only 15% of cases.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了\"Pathways of Thoughts (PoT)\"方法，一种用于增强大语言模型推理能力的通用框架。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。PoT方法将LLM的推理过程建模为迭代决策过程，使模型能够动态选择认知操作（推理、修订、个性化、澄清等），这直接涉及增强LLM的多步推理能力和逻辑推理能力，属于提升LLM通用推理能力的研究。 其次，从正面指标分析，论文明确包含多个关键主题：核心概念上聚焦于大语言模型(LLMs)；能力方向上专注于reasoning（推理过程）；方法上提出了一种新的推理范式，使模型能够探索多个推理轨迹并聚合结果。 第三，该论文不涉及任何排除标准领域。它没有关注多模态与视觉，没有针对特定应用领域（如医疗、化学等），也没有专注于模型基础设施或部署优化。 最后，关于特殊情况的考虑，虽然论文应用于个性化问答场景，但其提出的是一种通用的推理框架，可以应用于任何LLM而无需任务特定微调，因此不是将LLM作为工具应用于特定领域，而是致力于提升LLM本身的通用推理能力。 综合来看，这篇论文通过提出多方向思维的推理方法，直接增强了大语言模型的通用推理能力，完全符合研究课题的筛选要求。",
                    "summary2": "本文旨在解决个性化问答系统中的挑战，包括从长、嘈杂和隐式上下文中推断用户偏好，以及生成既正确又符合用户期望的响应。针对个性化问答场景，我们提出了一种Pathways of Thoughts (PoT)方法，将LLM的推理建模为迭代决策过程，探索多个推理轨迹并聚合响应，并在LaMP-QA基准测试上通过自动化评估和人工评估验证了其有效性，实现了高达13.1%的相对改进。",
                    "summary_translation": "个性化（Personalization）对于使问答系统（question answering, QA）适应用户特定的信息需求至关重要，从而同时提高准确性和用户满意度。然而，由于从冗长、嘈杂和隐含的上下文中推断用户偏好，以及生成同时满足正确性、上下文适当性并与用户期望和背景知识保持一致的响应等挑战，个性化问答（personalized QA）研究仍相对不足。为应对这些挑战，我们提出了\"思维路径\"（Pathways of Thoughts, PoT），这是一种适用于任何大型语言模型（large language model, LLM）的推理阶段（inference-stage）方法，无需进行任务特定的微调（fine-tuning）。该方法将大型语言模型的推理过程建模为一个迭代决策过程，模型在其中动态选择认知操作，如推理（reasoning）、修订（revision）、个性化（personalization）和澄清（clarification）。这使得能够探索多种推理轨迹，生成捕捉不同视角的多样化候选响应。然后，PoT根据推断的用户偏好对这些候选响应进行聚合和重新加权，产生最终的个性化响应，该响应受益于多样化推理路径的互补优势。在个性化问答的LaMP-QA基准测试（benchmark）上的实验表明，PoT始终优于竞争性基线（baselines），实现了高达13.1%的相对改进。人工评估（human evaluation）证实了这些结果，标注员（annotators）在66%的情况下更喜欢PoT的输出，仅在15%的情况下报告平局。"
                },
                {
                    "title": "Soft Tokens, Hard Truths",
                    "arxiv_id": "2509.19170",
                    "authors": "Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier",
                    "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种通过强化学习(RL)来学习连续思维链(CoTs)的新方法，以提高大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是思维链推理这一通用能力，而非将LLM作为工具应用到特定领域。论文使用了强化学习这一训练方法，聚焦于数学推理任务，符合第二步正面指标中的多个关键点：核心概念(LLMs)、能力方向(math reasoning)和训练方法(RL)。论文不涉及第三步排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊和模糊情况处理中，论文提出的是一种通用的推理增强方法，而非针对特定领域的应用。综合来看，这篇论文直接致力于提高LLM的通用推理能力，通过创新的\"soft tokens\"方法增强思维链推理，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在连续Chain-of-Thought推理中的训练难题。针对数学推理任务，我们提出了一种基于强化学习的soft tokens训练方法，通过在token嵌入中添加噪声实现有效探索，无需从真实离散CoT中蒸馏。在Llama和Qwen模型上，通过pass@1和pass@32指标验证了其有效性，表明该方法在保持pass@1性能的同时显著提高了pass@32的多样性，且在域外任务上更好地保留了基础模型性能。",
                    "summary_translation": "在大型语言模型(LLM)的Chain-of-Thought (CoT，思维链)推理阶段使用连续token(标记)而非离散token(标记)的做法最近引起了广泛关注，其基于一种直觉，即离散token(标记)的连续混合可以同时模拟多条推理路径的叠加。理论结果已经正式证明，连续token(标记)具有更强的表达能力，并能更高效地解决特定问题。\n\n然而，连续token(标记)的实际应用受到严重训练困难的限制：先前的研究要么仅在预训练的离散token(标记)模型上在推理时使用连续token(标记)，要么必须从真实的离散CoT中蒸馏出连续CoT，并面临计算成本的限制，导致CoT只能包含非常少的token(标记)。\n\n这是首个介绍通过强化学习(RL，Reinforcement Learning)学习连续CoT的可扩展方法的研究，无需从参考离散CoT中进行蒸馏。我们使用\"软\"token(标记)：token(标记)的混合与输入嵌入上的噪声相结合，以提供RL探索。计算开销极小，使我们能够学习包含数百个token(标记)的连续CoT。\n\n在高达8B参数的Llama和Qwen模型的数学推理基准测试中，使用连续CoT训练在pass@1指标上与离散token(标记) CoT相当，在pass@32指标上超越后者，显示出更大的CoT多样性。在系统比较中，表现最佳的场景是使用连续CoT token(标记)进行训练，然后在推理时使用离散token(标记)，这意味着\"软\"模型可以以标准方式部署。\n\n最后，我们表明连续CoT RL训练能更好地保留基础模型在域外任务(out-of-domain tasks)上的预测，从而为基础模型提供更柔和的调整。"
                },
                {
                    "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
                    "arxiv_id": "2509.18632",
                    "authors": "Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, Jordan Lee Boyd-Graber",
                    "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究LLM的规划(planning)能力，属于通用推理能力的重要组成部分。论文探讨了LLM如何生成步骤化计划来帮助用户完成复杂任务，并评估这些计划的实际有效性。这不是将LLM作为工具应用到特定领域，而是关注LLM本身的基础能力和改进方法，特别是当前对齐方法(如RLHF)的局限性。 其次，论文满足多个正面指标： 1) 核心概念：明确研究LLM生成的计划 2) 能力方向：聚焦于规划(planning)能力，这是推理和问题解决的关键部分 3) 训练方法：讨论了RLHF等对齐方法，并指出其局限性 4) 新兴范式：提到了智能体(agents)在模拟用户交互中的作用 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不是关于模型可靠性的应用层面研究(如水印、安全等)。 在特殊和模糊情况处理上，论文虽然提到了智能体，但这是作为研究方法的一部分，而非将智能体应用在特定领域。论文讨论的对齐问题是从提升模型基础能力的角度，而不是应用层面的防御技术。 论文的核心贡献在于揭示了当前LLM对齐方法(基于用户偏好)与实际帮助用户之间的差距，并提出需要基于真实用户交互反馈来改进对齐方法。这直接关系到如何提升LLM的通用推理能力和实际效用，完全符合研究目标。",
                    "summary2": "本文旨在解决LLM对齐方法中用户偏好与实际帮助性不一致的问题。针对多步骤问答场景，我们提出了Planorama界面，收集126用户对600个LLM计划的4388个执行结果和5584个偏好比较。通过Item Response Theory量化计划帮助性，我们发现用户偏好、reward模型和agent表现均无法准确预测真正帮助用户的计划，表明当前基于偏好的对齐方法与实际帮助性存在根本性错位。",
                    "summary_translation": "为协助用户完成复杂任务，大型语言模型（LLMs）会生成计划：即朝向目标的分步说明。尽管对齐方法（alignment methods）旨在确保LLM计划具有帮助性，但它们基于用户偏好进行训练（基于人类反馈的强化学习，RLHF）或评估（聊天机器人竞技场，ChatbotArena），假设这种偏好反映了什么对用户真正有帮助。我们通过Planorama（计划全景）界面对此进行测试：在该界面中，126名用户使用LLM计划回答300个多步骤问题。我们获得4388个计划执行和5584个比较数据，以衡量计划的有用性（问答成功率，QA success）和用户对计划的偏好，并在代理（agents）和奖励模型（reward models）中重建此设置，以观察它们是否能模拟或偏好真正帮助用户的方案。我们揭示了：1）用户/模型偏好与代理成功无法准确预测哪些计划真正帮助用户，因此常见的对齐反馈可能与有用性产生偏差；2）这种差距并非源于用户特定偏好，因为用户在使用他们偏好/不偏好的计划时成功率相似；3）简洁性和问题相似性等表面线索与偏好强烈相关，但此类偏见无法预测有用性。总之，我们认为对齐有用的LLMs需要来自真实用户交互的反馈，而不仅仅是对看起来有用的东西的偏好，因此我们讨论了自然语言处理（NLP）研究人员可执行的计划以解决此问题。"
                },
                {
                    "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering",
                    "arxiv_id": "2509.18655",
                    "authors": "Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li",
                    "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with new or corrected information without retraining or parameter adjustment. Recent PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE) capabilities to multi-hop question answering (MHQA). However, these methods often lack consistency, leading to knowledge contamination, unstable updates, and retrieval behaviors that fail to reflect the intended edits. Such inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures KG construction, update, and retrieval are always aligned with the requirements of the MHQA task, maintaining coherent reasoning over both unedited and edited knowledge. Extensive experiments on the MQuAKE benchmark show accuracy improvements in PPKE performance for MHQA, demonstrating the effectiveness of addressing consistency in PPKE.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断——这篇论文的本质是关于改进LLM在多跳问答(multi-hop question answering)任务中的推理能力。论文提出了CAPE-KG框架，解决参数保留知识编辑(PPKE)中的一致性问题，确保模型在更新知识后能够保持连贯的多跳推理能力。这属于增强LLM逻辑推理能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文包含\"reasoning\"这一核心能力方向，特别是\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分，涉及模型进行多步逻辑推理来连接不同知识片段以得出答案。 第三步：排除标准——论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。多跳问答是一种通用的推理任务，不属于特定应用领域。 第四步：特殊和模糊情况——论文提出了一种新方法来解决知识编辑中的不一致性问题，从而提升模型在多跳问答任务中的推理质量和可靠性。这符合\"提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量\"的情况，应保留。 综合以上分析，该论文的核心贡献是提出了一种一致性感知的知识编辑框架，旨在提升LLM在多跳推理任务中的表现，这与研究\"大语言模型通用推理能力\"的目标高度一致，因此最终判断为True。",
                    "summary2": "本文旨在 [解决参数保持知识编辑在多跳问答中缺乏一致性的问题]。针对 [多跳问答场景]，我们提出了一种 [一致性感知的参数保持知识编辑框架CAPE-KG]，并在 [MQuAKE基准测试] 上通过 [M-Acc和H-Acc指标] 验证了其有效性。",
                    "summary_translation": "参数保留知识编辑（Parameter-Preserving Knowledge Editing, PPKE）使模型能够在无需重新训练或参数调整的情况下，使用新的或经过校正的信息进行更新。近期基于知识图谱（Knowledge Graph, KG）的PPKE方法将知识编辑（Knowledge Editing, KE）能力扩展到多跳问题回答（Multi-hop Question Answering, MHQA）任务中。然而，这些方法常常缺乏一致性，导致知识污染、更新不稳定，以及检索行为无法反映预期的编辑效果。这种不一致性削弱了PPKE在多跳推理中的可靠性。我们提出了CAPE-KG（Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs，基于知识图谱的一致性感知参数保留编辑），这是一种用于MHQA任务中PPKE的新型一致性感知框架。CAPE-KG确保知识图谱的构建、更新和检索始终与MHQA任务的要求保持一致，从而在未编辑和已编辑的知识上维持连贯的推理。在MQuAKE基准测试上进行的大量实验表明，CAPE-KG在MHQA任务的PPKE性能方面提高了准确性，证明了解决PPKE中一致性问题的有效性。"
                },
                {
                    "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs",
                    "arxiv_id": "2509.18314",
                    "authors": "Hieu Tran, Zonghai Yao, Hong Yu",
                    "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes. Built on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated \\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \\emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心是关于提高大语言模型通用推理能力的研究。 首先，从本质上看，论文的核心贡献是提出了一种新的强化学习训练方法TEMPO，用于解决LLM在长序列推理任务中的信用分配问题。这直接针对LLM的基础能力改进，特别是提升其在推理任务中的表现，符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心研究对象；专注于reasoning能力（特别是math reasoning）；提出了新的强化学习训练方法(TEMPO)，改进了现有的PPO和GRPO算法。 第三，论文不涉及任何排除标准领域。虽然论文在实验中使用了医疗问答(MedQA, MMLU-Medical)作为评估基准，但这只是作为验证方法有效性的测试场景，而非论文的核心焦点。论文的核心是改进通用推理能力，而非专注于医疗应用。 最后，论文提出的方法具有通用性，可以应用于各种需要推理能力的任务，不仅限于特定领域。作者在多个数学推理基准(MATH, GSM-HARD, AMC23)和医疗问答基准上进行了验证，证明了其方法的通用性和有效性。 综上所述，这篇论文明确致力于提高LLM的通用推理能力，通过改进强化学习训练方法来解决信用分配问题，完全符合我的研究范围。",
                    "summary2": "本文旨在解决LLM强化训练中的token级别信用分配问题。针对长序列推理任务中奖励稀疏延迟的场景，我们提出了一种利用响应树结构的TEMPO算法，并在数学和医学问答数据集上通过准确率和收敛速度验证了其有效性。",
                    "summary_translation": "强化学习（reinforcement learning）改善了大型语言模型（LLM）的推理能力，但在长序列上的稀疏延迟奖励（sparse delayed reward）使得token级别的信用分配（token-level credit assignment）成为关键瓶颈。我们研究了可验证奖励（verifiable-reward）设置，其中最终答案是可检查的，并且每个提示（prompt）可以生成多个响应。数学和医学问答（medical QA）中的推理任务与这种设置一致，其中只有少数决策token（decision tokens）对结果有显著影响。PPO通过学习到的价值模型（learned value model）提供了token级别的优势，但同时训练actor和critic模型很复杂，并且不容易泛化，因为来自critic模型的token级别值可能导致训练容易过拟合。GRPO是无critic的（critic-free）且支持可验证奖励，但它将单个序列级别的回报（sequence-level return）分散到各个token上，并忽略了分支（branching）。\n\n我们提出了**Prefix-to-Tree (P2T)**，这是一个简单的过程，将一组响应转换为前缀树（prefix tree），并通过聚合后代结果（descendant outcomes）来计算*非参数*（nonparametric）前缀值\\(V(s)\\)。基于P2T，我们提出了**TEMPO**（***T**ree-**E**stimated **M**ean **P**refix Value for **P**olicy **O**ptimization*，树估计平均前缀值用于策略优化），这是一种无critic的算法，它用从树中派生的*分支门控*（branch-gated）时序差分（temporal-difference）修正来增强GRPO的组相对结果信号。在非分支token上，时序差分（TD）项为零，因此TEMPO简化为GRPO；在分支token上，它提供了精确的token级别信用，而无需学习的价值网络或额外的评判者/教师。在Qwen3-1.7B/4B模型上，TEMPO在分布内（MATH, MedQA）和分布外（GSM-HARD, AMC23, MedMCQA, MMLU-Medical）基准测试上均优于PPO和GRPO，并在大致相同的实际时间（wall-clock time）内达到更高的验证准确率。"
                },
                {
                    "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning",
                    "arxiv_id": "2509.18163",
                    "authors": "Haodong Zhao, Chenyan Zhao, Yansi Li, Zhuosheng Zhang, Gongshen Liu",
                    "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative \"thinking mode\" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models \"think\", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的核心是研究大语言模型(LLM)的推理能力，特别是探讨外部辅助信息对LLM推理过程的影响。论文关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文研究了模型的\"thinking mode\"（思维模式）如何影响其对不同类型信息的处理，这直接关系到LLM的通用推理能力。因此，从核心判断来看，这篇论文应该被保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：直接研究reasoning能力，特别是模型的step-by-step thinking capabilities - 论文探讨了LLM在面对不同类型信息时的推理过程，这与提升模型通用推理能力直接相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉研究 - 不是针对特定应用领域（如医疗、化学等）的研究 - 虽然涉及模型可靠性，但是从提升模型推理能力的根本角度出发，而非仅作为应用层面的防御 第四步：处理特殊和模糊情况 论文研究的是LLM的通用推理能力在面对不同类型信息时的表现，特别是\"thinking mode\"如何影响推理过程。这不是将LLM应用到特定领域，而是研究LLM本身的推理机制和脆弱性。论文提出的挑战是\"not merely to make models 'think', but to endow them with the critical faculty to evaluate the information upon which their reasoning is based\"，这直接指向提升LLM的通用推理能力。 第五步：最终决策 综合以上分析，这篇论文的核心贡献是研究外部辅助信息对LLM推理过程的影响，揭示了模型\"thinking mode\"的双面性，并提出了提升模型批判性评估信息能力的重要性。这直接关系到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True，应该保留这篇论文。",
                    "summary2": "本文旨在探究辅助信息对LLM推理过程的影响。针对不同类型辅助信息（有帮助、不相关、误导）的场景，我们提出了SciAux数据集，并在具有可切换思考模式的大型推理模型上通过准确率指标验证了推理过程的双刃剑效应。实验证明，思考模式虽在有帮助上下文中提升性能，但在误导信息面前会显著放大错误，揭示了当前推理模型在信息评估能力上的关键缺陷。",
                    "summary_translation": "Large Language Models (LLMs, 大型语言模型)的推理能力是其应用于复杂、知识密集型领域的基础。在现实场景中，LLMs常被辅以外部信息，这些信息可能是有帮助的、无关的，甚至是误导性的。本文研究了此类辅助信息对具有明确逐步思考能力的LLMs推理过程的因果影响。我们介绍了SciAux，一个源自ScienceQA的新数据集，用于系统测试模型对这些类型信息的鲁棒性(robustness)。我们的发现揭示了一个关键漏洞：模型的审慎\"思考模式\"(thinking mode)是一把双刃剑。有帮助的上下文提高准确性，而误导性信息导致性能灾难性下降，这种影响在思考过程中被放大。思考并未赋予鲁棒性，而是在提供错误信息(misinformation)时强化了错误程度。这表明挑战不仅仅是让模型\"思考\"，而是赋予它们评估其推理所依据信息的批判性能力(critical faculty)。SciAux数据集可在https://huggingface.co/datasets/billhdzhao/SciAux获取。"
                },
                {
                    "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs",
                    "arxiv_id": "2509.18113",
                    "authors": "Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu",
                    "summary": "This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种动态提示融合方法，用于改进大语言模型在多任务和跨领域设置下的泛化能力。论文通过引入提示池和任务感知调度策略，动态组合和对齐不同任务的提示，增强了模型捕捉跨任务语义差异的能力。从第一步核心判断来看，这明显属于改进LLM基础能力的研究，提出了新的训练范式来增强模型的通用能力，而非将LLM作为工具应用到特定领域。从第二步正面指标看，论文明确涉及大语言模型(LLMs)核心概念，并特别提到提高了\"知识推理任务\"(knowledge reasoning tasks)的性能，直接符合推理能力方向。论文不涉及第三步中的任何排除标准，如多模态视觉、特定应用领域或模型可靠性等应用层面的研究。综合分析，该论文致力于提升LLM的通用推理能力和泛化能力，完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多任务和跨领域设置中的泛化限制问题。针对异构任务和领域迁移场景，我们提出了一种基于动态提示调度的统一多任务学习框架，并在CrossFit数据集上通过SuperGLUE和MMLU Accuracy等指标验证了其有效性。",
                    "summary_translation": "本研究针对大语言模型（large language models）在多任务（multi-task）和跨域（cross-domain）设置中普遍存在的泛化限制（generalization limitations）问题。与先前如SPoT等依赖固定提示模板（prompt templates）的方法不同，我们引入了一种具有动态提示调度机制（dynamic prompt scheduling mechanism）的统一多任务学习框架。通过引入提示池（prompt pool）和任务感知调度策略（task-aware scheduling strategy），该方法动态组合和对齐不同任务的提示。这增强了模型捕捉任务间语义差异（semantic differences）的能力。在提示融合（prompt fusion）过程中，模型利用任务嵌入（task embeddings）和门控机制（gating mechanism）精细控制提示信号（prompt signals）。这确保了提示内容与任务特定需求之间的一致性，同时构建了跨任务的灵活共享路径。此外，所提出的优化目标（optimization objective）以联合多任务学习（joint multi-task learning）为中心，融合了调度权重（scheduling weights）的自动学习策略，有效缓解了任务干扰（task interference）和负迁移（negative transfer）问题。为评估该方法的有效性，我们进行了一系列敏感性实验（sensitivity experiments），这些实验考察了提示温度参数（prompt temperature parameters）和任务数量变化的影响。结果证实了所提机制在保持模型稳定性和增强迁移能力（transferability）方面的优势。实验发现表明，该提示调度方法在一系列语言理解（language understanding）和知识推理（knowledge reasoning）任务上显著提高了性能。这些结果充分证明了其在统一多任务建模（unified multi-task modeling）和跨域适应（cross-domain adaptation）中的适用性和有效性。"
                },
                {
                    "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization",
                    "arxiv_id": "2509.18158",
                    "authors": "Seungyoun Yi, Minsoo Khang, Sungrae Park",
                    "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，这篇论文的本质是提出ZERA框架，一种用于自动优化提示的新方法，通过联合优化系统提示和用户提示来提高大语言模型在各种任务上的性能。这不是将LLM作为工具应用到特定领域，而是提出一种通用的方法来增强LLM的基础能力，特别是在推理任务上的表现，因此符合保留条件。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)，并在五个不同LLM上进行了评估 - 能力方向：评估数据集包含推理(reasoning)任务，直接符合我们的研究方向 - 训练方法：标题中的\"Instruction Evolving\"和摘要中的\"principled, low-overhead refinement\"表明涉及进化或自我优化的方法 - 新兴范式：论文提出的ZERA是一种基于LLM的智能体(Refinement Agent)，用于优化提示 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域，评估的是通用能力（推理、摘要、代码生成） - 不主要关注模型可靠性的应用层面问题 最后，在特殊和模糊情况处理上，ZERA智能体是一种通用框架，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种新颖的、基于原则的提示优化框架，能够使用最少的示例和短的迭代周期快速收敛到高质量提示，从而提升LLM在推理等通用任务上的表现。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决自动提示优化(APO)方法依赖非结构化反馈、需要大量样本和长迭代周期的问题。针对多种LLMs和九个不同任务数据集，我们提出了一种ZERA框架，通过八个评估原则和自动推断权重共同优化系统和用户提示，并在推理、摘要和代码生成任务上通过准确率、ROUGE-L和pass@1等指标验证了其有效性。",
                    "summary_translation": "自动提示优化(Automatic Prompt Optimization, APO)通过针对特定任务优化提示来提高大型语言模型(large language model, LLM)的性能。然而，先前的APO方法通常只关注用户提示，依赖非结构化反馈，并且需要大量样本和长迭代周期——这使得它们成本高昂且脆弱。我们提出了ZERA(Zero-init Instruction Evolving Refinement Agent，零初始化指令进化优化代理)，一种新颖的框架，通过有原则的、低开销的优化来共同优化系统和用户提示。ZERA使用八个具有自动推断权重的可泛化标准对提示进行评分，并基于这些结构化批评修订提示。这使得能够使用最少的示例和短的迭代周期快速收敛到高质量的提示。我们在五个LLMs和九个涵盖推理、摘要和代码生成任务的多样化数据集上评估了ZERA。实验结果表明，与强大的基线相比，ZERA取得了持续的改进。进一步的消融研究(ablation studies)突显了每个组件对更有效提示构建的贡献。我们的实现包括所有提示，可在https://github.com/younatics/zera-agent公开获取。"
                },
                {
                    "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
                    "arxiv_id": "2509.19265",
                    "authors": "Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto",
                    "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是研究LLMs的常识推理能力在不同文化背景下的迁移效果，并提出了轻量级对齐方法（如上下文学习和基于演示的强化学习DITTO）来提升这种能力。常识推理是通用推理能力的重要组成部分，论文评估的训练方法也属于改进LLM基础能力的范畴。虽然论文涉及文化领域，但其焦点不是将LLM作为工具应用于特定领域，而是研究LLM本身的推理能力如何在不同文化背景下迁移和提升。论文提出的方法（使用12个文化特定示例即可提高其他地区性能10%）可以作为一种通用的方法来增强LLMs的推理能力，而不是仅限于解决特定领域的问题。因此，这篇论文符合我的研究目标，它探索了如何通过文化迁移来提升LLM的通用推理能力，特别是常识推理这一核心能力。",
                    "summary2": "本文旨在研究大型语言模型(LLMs)在阿拉伯世界中的跨文化常识推理迁移能力。针对阿拉伯国家间存在语言和历史相似性但又有本地文化差异的特点，我们提出了一种轻量级文化对齐方法，包括上下文学习(ICL)和基于演示的强化学习(DITTO)，并在ArabCulture数据集上通过准确率提升验证了其有效性。实验表明，仅需12个特定文化示例就能使多语言模型在其他国家的表现平均提升10%，且非阿拉伯文化(如印尼和美国)的演示也能达到或超过文化内对齐的效果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）常表现出以西方为中心的偏见（Western-centric biases），限制了其在多元文化语境中的有效性。尽管已有研究探索了文化对齐（cultural alignment）问题，但利用某一文化中的对齐成果来提升其他文化中模型表现的跨文化迁移（cross-cultural transfer）潜力仍鲜有研究。本文聚焦阿拉伯世界中的常识推理（commonsense reasoning）跨文化迁移问题，该区域在语言与历史方面具有高度相似性，同时又存在显著的本地文化差异。我们使用一个涵盖13个阿拉伯国家、基于文化情境构建的常识推理数据集，评估了多种轻量级对齐方法，包括上下文学习（in-context learning）和基于示范的强化方法（demonstration-based reinforcement, DITTO），并与监督微调（supervised fine-tuning）和直接偏好优化（direct preference optimization）等基线方法进行对比。实验结果表明，在多语言模型中，仅需来自某一国家的12个文化特异性示例，即可使其他阿拉伯国家的模型表现平均提升10%。此外，我们发现来自印度尼西亚和美国语境的“异文化”示范（out-of-culture demonstrations）在多项选择题（MCQ）推理任务中，其效果可媲美甚至超越“同文化”对齐方法，揭示了常识推理能力在阿拉伯世界之外的跨文化可迁移性。这些发现表明，高效的跨文化对齐是可行的，为将大语言模型适配至资源匮乏的文化环境提供了有前景的路径。"
                },
                {
                    "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
                    "arxiv_id": "2509.18847",
                    "authors": "Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu",
                    "summary": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是通过提出\"结构化反思\"(structured reflection)这一新训练范式来增强LLM的工具使用和错误恢复能力，这属于提升LLM通用推理能力的范畴。 论文包含多个正面指标：明确提及\"Large language models (LLMs)\"和\"Tool-augmented large language models\"等核心概念；虽然未直接使用\"reasoning\"一词，但\"reflection\"和\"error diagnosis\"本质上是一种推理过程；训练方法涉及强化学习(\"reinforcement learning\"和\"reward scheme\")；同时论文明确讨论了\"llm-based agents\"和\"tool use\"等新兴范式。 论文不符合任何排除标准：它不涉及多模态与视觉内容，不专注于特定应用领域，且讨论的\"reliability\"是从提升模型内在能力的角度而非应用层面的防御。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体反思框架来增强LLM的工具使用能力，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是通过结构化反思机制，使智能体能够从失败中学习并改进其推理过程，这直接提升了LLM的通用推理能力和问题解决能力，符合研究目标。",
                    "summary2": "本文旨在解决工具增强型大语言模型在多轮交互中错误恢复能力不足的问题。针对多轮工具调用场景，我们提出了一种结构化反思方法，将错误诊断和纠正转化为可训练的显式行动，并通过结合DAPO和GSPO的目标函数优化Reflect → Call → Final策略。我们在BFCL v3和Tool-Reflection-Bench上通过多轮工具调用成功率、错误恢复率和冗余调用减少率等指标验证了其有效性。",
                    "summary_translation": "工具增强型大语言模型（LLMs，Large Language Models）通常通过监督模仿或优化单次工具调用的粗粒度强化学习进行训练。当前的自省（self-reflection）实践依赖于启发式提示（heuristic prompts）或单向推理（one-way reasoning）：模型被鼓励\"多思考\"，而不是学习错误诊断和修复。这种方法在多轮交互（multi-turn interactions）中表现脆弱；失败后，模型往往会重复同样的错误。我们提出了结构化反思（structured reflection），它将从错误到修复的路径转变为一种明确、可控且可训练的行动。智能体（agent）生成简短而精确的反思：它利用前一步骤的证据诊断失败，然后提出一个正确的、可执行的后续调用。在训练方面，我们将DAPO（DAPO）和GSPO（GSPO）目标与针对工具使用量身定制的奖励方案相结合，优化\"先反思，后调用，最终确定\"的逐步策略。为进行评估，我们引入了Tool-Reflection-Bench（工具反思基准），这是一个轻量级基准（lightweight benchmark），可程序化地检查结构有效性、可执行性、参数正确性和结果一致性。任务被构建为错误调用、反思和纠正调用的小型轨迹（mini trajectories），训练集和测试集互不重叠。在BFCL v3（BFCL v3）和Tool-Reflection-Bench上的实验表明，多轮工具调用成功率和错误恢复能力大幅提升，冗余调用也有所减少。这些结果表明，将反思明确化并直接优化它，可以提高工具交互的可靠性，并为智能体提供从失败中学习的可复现路径。"
                },
                {
                    "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
                    "arxiv_id": "2509.18169",
                    "authors": "Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan He",
                    "summary": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出PiMoE（物理隔离的专家混合）架构，用于将高精度计算能力内生性地集成到大语言模型中，以增强其推理能力。从筛选标准来看： 第一步（核心判断）：论文本质上是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。PiMoE通过令牌级路由实现计算和推理的迭代交替，属于增强LLM通用推理能力的新范式，类似思维链(CoT)的扩展，因此应保留。 第二步（正面指标）：论文包含多个相关主题，包括核心概念\"large language models (LLMs)\"、能力方向\"reasoning\"，以及新兴范式\"multi-agent systems\"和\"tool use\"的讨论。 第三步（排除标准）：论文未主要聚焦于多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。虽然提及\"scientific or industrial intelligent systems\"，但这是潜在应用场景而非研究焦点。 第四步（特殊和模糊情况）：论文确实涉及智能体/工具使用，但不是将其应用于特定领域，而是提出一种通用的架构来增强LLM的通用问题解决能力，这与筛选标准中\"提出通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况相符。 综上所述，PiMoE论文直接致力于提高大语言模型本身的通用推理能力，通过新的架构设计实现计算与推理的融合，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型无法内在集成高精度数值计算的问题。针对科学计算与推理任务场景，我们提出了一种PiMoE (Physically-isolated Mixture of Experts)架构，通过token级别路由实现高精度计算与推理的集成。在电池容量预测和电池利润计算任务上，通过MSE、响应延迟、token使用量和GPU能耗等指标验证了其有效性，结果表明PiMoE不仅比直接微调LLMs准确性更高，还比多智能体方法在效率方面有显著改善。",
                    "summary_translation": "复杂系统（complex systems）通常依赖高精度数值计算（high-precision numerical computation）来支持决策，但当前的大型语言模型（large language models, LLMs）还无法在现有架构中将这种计算作为一种内在且可解释的能力（intrinsic and interpretable capability）加以整合。主流的多智能体方法（multi-agent approaches）可以利用外部专家（external experts），但不可避免地引入通信开销（communication overhead），并存在多模态涌现能力（multimodal emergent capability）效率低下和可扩展性（scalability）有限的问题。为此，我们提出了PiMoE（物理隔离的专家混合，Physically-isolated Mixture of Experts），一种用于整合计算和推理（computation and reasoning）的训练和推理架构（inference architecture）。不同于工具调用（tool invocation）的工作流范式（workflow paradigm），PiMoE在分别训练专家、文本到计算模块（text-to-computation module）和路由器（router）后，将计算能力内生整合（endogenously integrates）到神经网络中。在推理（inference）阶段，路由器在词元级别（token level）引导计算和推理，从而在单一思维链（chain of thought）内实现迭代交替（iterative alternation）。我们在两项推理-计算任务（reasoning-computation tasks）上评估了PiMoE，并与LLM微调（LLM finetuning）和多智能体系统方法（multi-agent system approaches）进行了比较。结果表明，PiMoE架构不仅比直接微调LLMs实现了更高的准确度（accuracy），而且与主流多智能体方法相比，在响应延迟（response latency）、词元使用量（token usage）和GPU能耗（GPU energy consumption）方面也有显著改进。PiMoE为下一代科学或工业智能系统（intelligent systems）提供了一种高效（efficient）、可解释（interpretable）且可扩展（scalable）的范式（paradigm）。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 17,
            "papers": [
                {
                    "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
                    "arxiv_id": "2509.19284",
                    "authors": "Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn",
                    "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是研究什么构成了有效的思维链(CoT)推理，并提出了一种新的方法来评估和改进CoT的质量。论文通过系统评估发现，更长的CoT并不总是更好的，并引入了失败步骤分数(FSF)这一指标来预测CoT的正确性。作者还设计了两种干预措施来测试因果关系，证明有效的CoT是那些\"失败较少\"的，并支持\"结构感知\"的测试时扩展。这直接关系到改进LLM的基础推理能力，特别是数学和科学推理能力，属于通用推理能力的范畴。论文不涉及任何需要排除的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文在数学和科学推理上进行了评估，但这些只是作为评估推理能力的基准领域，而非论文的应用目标。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在探究什么特征使思维链(CoT)推理有效。针对大型推理模型(LRMs)的长CoT推理场景，我们提出了一种基于图视图的CoT结构分析方法，定义了Failed-Step Fraction (FSF)作为核心指标，并在HARP和GPQA-Diamond数据集上通过条件相关分析、测试时选择和CoT编辑实验验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在测试阶段花费大量计算资源于长思维链（chain-of-thought, CoT）轨迹，但什么*特征*构成了有效的CoT仍不明确。尽管先前的研究报告称通过延长CoT和通过附加*等待*（wait）标记增加回顾（revisiting earlier steps）可以带来性能提升，但近期研究表明，更短的思考可能优于更长的轨迹。因此，我们在数学和科学推理任务上对十种LRM进行了系统评估。与\"越长越好\"的叙述相反，我们发现无论是简单的CoT延长还是增加回顾都与*更低*的准确率相关。随着CoT逐步展开，标记级别的指标可能会将冗长性与过程质量混为一谈。我们引入了CoT的图视图来提取结构，并识别出一个单一统计量——*失败步骤比例（Failed-Step Fraction, FSF）*，即被放弃分支中的步骤比例，该统计量在预测模型正确性方面始终优于长度和回顾比例。为了探究因果关系，我们设计了两种干预措施。首先，我们在测试时根据每个指标对候选CoT进行排序，其中FSF产生了最大的pass@1增益；其次，我们编辑CoT以移除失败的分支，这显著提高了准确率，表明失败的分支会影响后续推理。综合来看，这些结果表明有效的CoT是那些*失败更少*的CoT，并支持在测试阶段采用*结构感知*（structure-aware）的扩展策略，而非不加选择地生成长CoT。"
                },
                {
                    "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
                    "arxiv_id": "2509.19189",
                    "authors": "Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu",
                    "summary": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出\"功能缩放定律\"(FSL)框架，用于理解和优化大语言模型的训练动态，特别是学习率计划(LLM训练中的关键超参数)如何影响训练过程。虽然论文没有直接研究推理能力或解决问题等具体能力，但它关注的是LLM的基础训练机制，属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴。论文通过优化训练过程和学习率计划，为提高LLM的通用能力提供了理论基础和实践指导。论文明确研究LLM训练，不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它通过改进训练方法来提升模型的基础能力，而非将LLM作为工具应用到特定领域。",
                    "summary2": "本文旨在揭示学习率调度(LRS)在大型语言模型(LLM)训练中的影响机制。针对LLM预训练过程中的损失动态和缩放行为，我们提出了一种功能性缩放定律(FSL)，通过内禀时间随机微分方程(SDE)建模，明确刻画了LRS对训练过程的影响。在teacher-student核回归设置和0.1B到1B参数的LLM上，通过损失曲线拟合和预测验证了FSL的有效性，为LLM预训练提供了理论指导和实践洞见。",
                    "summary_translation": "缩放定律（scaling laws）在指导大语言模型（large language models, LLMs）训练中扮演着基石角色。然而，大多数关于缩放定律的现有研究主要关注最终步骤损失（final-step loss），忽视了训练过程中的损失动态，以及至关重要的学习率计划（learning rate schedule, LRS）的影响。在本文中，我们旨在通过研究通过在线随机梯度下降（online stochastic gradient descent, SGD）训练的教师-学生核回归（teacher-student kernel regression）设置来弥合这一差距。利用新颖的内在时间观点（intrinsic time viewpoint）和SGD的随机微分方程（stochastic differential equation, SDE）建模，我们引入了功能缩放定律（Functional Scaling Law, FSL），该定律表征了在一般LRS下训练过程中总体风险（population risk）的演变。值得注意的是，LRS的影响通过一个显式的卷积型功能项（convolution-type functional term）被捕获，使其效果完全可追踪。为了说明FSL的实用性，我们在数据有限（data-limited）和计算有限（compute-limited）两种情况下分析了三种广泛使用的LRS——恒定（constant）、指数衰减（exponential decay）和预热稳定衰减（warmup-stable-decay, WSD）。我们为LLMs预训练中广泛采用的实证实践提供了理论依据，例如：(i) 更高容量的模型在数据和计算利用上更高效；(ii) 学习率衰减可以提高训练效率；(iii) 类似WSD的计划可以优于直接衰减（direct-decay）计划。最后，我们探索了FSL作为LLMs预训练中损失曲线拟合、预测和优化的代理模型（surrogate model）的实际相关性，实验在参数规模从0.1B到1B的模型上进行。我们希望我们的FSL框架能够深化对LLMs预训练动态的理解，并为改进大规模模型训练提供见解。"
                },
                {
                    "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio",
                    "arxiv_id": "2509.19128",
                    "authors": "Alexandre Piché, Ehsan Kamaloo, Rafael Pardinas, Dzmitry Bahdanau",
                    "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的强化学习训练范式(PipelineRL)来增强LLM的推理能力。论文明确提到\"Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs)\"，表明其核心是提升LLM的推理能力这一基础能力。 其次，论文包含多个正面指标：核心概念方面直接涉及Large language models (LLMs)；能力方向上关注reasoning；训练方法上专注于reinforcement learning，这些都是研究目标中明确关注的重点。 第三，论文完全避开了排除标准中的所有领域，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文虽然不直接涉及智能体/工具使用或幻觉/可解释性/安全等问题，但通过提高训练效率和数据新鲜度(on-policyness)来改进RL训练质量，这从根本上提升了LLM的推理能力，而非仅作为应用层面的优化。 综上所述，PipelineRL论文的核心贡献是提出了一种更高效的RL训练方法来提升LLM的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决强化学习在长序列生成中面临的硬件效率与数据策略性之间的权衡问题。针对大型语言模型的长序列生成场景，我们提出了一种PipelineRL方法，通过并发异步数据生成和模型训练，并引入创新的in-flight weight updates机制，在128个H100 GPU上通过学习速度和任务成功率等指标验证了其有效性，实现了约2倍的学习速度提升。",
                    "summary_translation": "强化学习 (Reinforcement Learning, RL) 越来越多地被用于增强大语言模型 (Large Language Models, LLMs) 的推理能力。然而，有效扩展这些强化学习方法面临着重大挑战，主要源于在保持高AI加速器利用率的同时难以避免产生陈旧的、非策略数据 (off-policy data)，而这些数据会损害常见的强化学习算法。本文介绍了PipelineRL，这是一种旨在为大语言模型训练实现硬件效率与数据策略性 (data on-policyness) 之间更优权衡的方法。PipelineRL采用并发的异步数据生成和模型训练，其特点是通过新颖的飞行中权重更新 (in-flight weight updates) 机制。这一机制使大语言模型生成引擎能够在生成token序列期间以最小中断接收更新的模型权重，从而最大化加速器利用率和训练数据的新鲜度。在128个H100 GPU上进行的长格式推理任务实验表明，与传统的强化学习基线相比，PipelineRL实现了约2倍的学习速度提升，同时保持了高度策略性的训练数据。作为一项重要贡献，研究团队还发布了PipelineRL的可扩展、模块化开源实现。"
                },
                {
                    "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
                    "arxiv_id": "2509.19104",
                    "authors": "Sharan Sahu, Martin T. Wells",
                    "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种新的强化学习对齐方法(DRO-REBEL)，用于改进LLM与人类意图的对齐效果。论文核心是解决现有RLHF方法中的过优化问题，提出了一种分布鲁棒相对奖励回归的新训练范式。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了Large Language Models (LLMs) - 训练方法：重点研究了Reinforcement Learning with Human Feedback (RLHF)的改进，属于强化学习优化范畴 虽然论文没有直接提到reasoning、planning等能力方向，但RLHF作为提升LLM与人类意图对齐的方法，本质上可以增强模型的通用推理能力。 第三步：排除标准 论文不涉及任何排除领域： - 没有涉及多模态与视觉相关内容 - 没有针对特定应用领域（如医疗、化学、生物等） - 没有主要关注模型可靠性方面的应用层面研究（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用、幻觉/可解释性/安全等特殊或模糊情况。论文提出的DRO-REBEL方法是从根本上提升LLM与人类意图对齐的能力，属于基础能力改进。 最终决策 综合以上分析，这篇论文的核心贡献是提出了一种新的强化学习对齐方法，用于改进LLM的基础能力，使其更好地与人类意图对齐。这符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，因为更好的对齐可以提升模型在各种推理任务中的表现。因此，最终判断为True。",
                    "summary2": "",
                    "summary_translation": "强化学习与人类反馈(Reinforcement learning with human feedback, RLHF)已成为使大型语言模型(Large Language Models, LLMs)与人类意图对齐的关键技术。然而，现有的离线RLHF方法存在过度优化问题，即模型过度拟合于奖励误指定(reward misspecification)，并偏离训练过程中观察到的偏好行为。我们提出了DRO-REBEL，这是一个统一的鲁棒REBEL更新家族，包含p型Wasserstein、KL(Kullback-Leibler)和χ²(chi-square)模糊集。利用Fenchel对偶性(Fenchel duality)，每次更新都简化为简单的相对奖励回归(relative-reward regression)，保持了可扩展性，并避免了PPO(Proximal Policy Optimization)风格裁剪或辅助价值网络。在标准线性奖励和对数线性策略类以及数据覆盖条件下，我们建立了$O(n^{-1/4})$的估计界限，其常数比先前的DRO-DPO(Direct Preference Optimization)方法更紧，并通过局部化Rademacher复杂性分析(localized Rademacher complexity analysis)恢复了极小极大最优的$O(n^{-1/2})$速率。同样的分析填补了Wasserstein-DPO和KL-DPO的差距，表明两者也达到了最优参数速率。我们为所有三种散度推导出实用的SGD(Stochastic Gradient Descent)算法：梯度正则化(Wasserstein)、重要性加权(KL)和快速一维对偶求解(χ²)。在情感对齐(Emotion Alignment)、大规模ArmoRM多目标基准和HH-Alignment上的实验表明，该方法在未见过的偏好混合、模型大小和数据规模上均展现出强大的最坏情况鲁棒性，其中χ²-REBEL表现出一致强大的经验性能。一项受控的半径-覆盖研究验证了没有免费午餐的权衡(no-free-lunch trade-off)：比经验散度集中速率更快的收缩半径可实现极小极大最优参数速率，但会放弃覆盖，而保证覆盖的半径则会产生$O(n^{-1/4})$的速率。"
                },
                {
                    "title": "Reflect before Act: Proactive Error Correction in Language Models",
                    "arxiv_id": "2509.18607",
                    "authors": "Qiuhai Zeng, Sarvesh Rajkumar, Di Wang, Narendra Gyanchandani, Wenbo Yan",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in interactive decision-making tasks, but existing methods often struggle with error accumulation and lack robust self-correction mechanisms. We introduce \"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based decision-making by introducing a critical reflect step prior to taking the next action. This approach allows for immediate error correction, ensuring smooth action path and adaptibity to environment feedback. We evaluate REBACT on three diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results demonstrate that REBACT significantly outperforms strong baselines, improving success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld (achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's performance improvements are achieved with only a few modification steps, demonstrating its computational efficiency.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究目标，理由如下： 首先，从核心判断来看，论文本质上是提出一种名为\"Reflect before Act\" (REBACT)的新方法，通过在行动前引入反思步骤来增强语言模型的决策能力。这明显属于改进LLM基础能力和增强其通用推理能力的范畴，特别是通过自我纠错机制来提升模型的逻辑推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标分析，论文明确提到了\"Large Language Models (LLMs)\"这一核心概念，并且关注的是\"decision-making\"能力，这与推理和问题解决密切相关。虽然未直接提及reasoning，但反思和纠错机制本质上是在提升模型的推理质量。 第三，从排除标准看，论文不涉及多模态与视觉内容，也没有专注于特定应用领域（如医疗、化学等）。虽然提到了ALFWorld、WebShop和TextCraft三个评估环境，但这些是用于测试通用能力的基准环境，而非特定应用领域。 最后，在特殊和模糊情况处理上，REBACT是一种通用的反思机制，用于增强LLM的通用问题解决能力，而不是针对特定领域的应用。论文关注的是如何通过反思步骤提升模型的内在纠错能力，从而从根本上提高模型的推理质量，这完全符合\"提升LLM通用推理能力\"的研究目标。 综上所述，这篇论文的核心贡献是提出了一种通过前瞻性反思来增强LLM自我纠错能力的新方法，属于提升大语言模型通用推理能力的研究范畴，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在交互式决策任务中的错误累积和缺乏自我纠正机制的问题。针对交互式决策环境，我们提出了一种REBACT（Reflect before Act）方法，在执行下一个动作前先反思并修正之前动作中的错误，并在ALFWorld、WebShop和TextCraft三个数据集上通过成功率（success rate）验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已在交互式决策任务中展现出显著能力，但现有方法常常面临错误累积问题，且缺乏强大的自我纠正机制。我们提出了\"Reflect before Act\"（REBACT，行动前反思）方法，这是一种通过在采取下一步行动前引入关键反思步骤来增强基于LLM的决策的新方法。该方法能够实现即时错误纠正，确保行动路径的顺畅性以及对环境反馈的适应性。我们在三个多样化的交互环境中对REBACT进行了评估：ALFWorld、WebShop和TextCraft。我们的结果表明，使用Claude3.5-sonnet作为底层LLM，REBACT显著优于强基线方法，在WebShop上的成功率提高了24%（达到61%），在ALFWorld上提高了6.72%（达到98.51%），在TextCraft上提高了0.5%（达到99.5%）。进一步分析表明，REBACT的性能提升仅需少量修改步骤即可实现，这证明了其计算效率。"
                },
                {
                    "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
                    "arxiv_id": "2509.18164",
                    "authors": "Ranfei Chen, Ming Chen",
                    "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a comprehensive understanding of mathematical and logical patterns. We propose DSFT, a simple yet effective Diffusion SFT strategy, by adjusting the masking strategy and loss function, guiding models to understand mathematical and logical patterns. This strategy can be flexibly combined with pre-training, reinforcement learning, and other training methods. Validated on models such as LLaDA and Dream series, we prove that DSFT on small-scale data can achieve improvements of 5-10% and approximately 2% on mathematical and logical problems, respectively. This inspiring masking approach offers insights for future learning of specific patterns, which can be easily and efficiently combined with other training methods and applied to various dLLMs. Our code is publicly available at https://anonymous.4open.science/r/DSFT-0FFB/",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数来增强扩散大语言模型(dLLMs)对数学和逻辑模式的理解能力。论文明确关注LLM的通用推理能力，特别是数学推理和逻辑推理，这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是直接提升模型本身的推理能力，且不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面研究）。此外，论文提出的方法可以与预训练、强化学习等其他训练方法结合，显示出其作为提升LLM通用推理能力的普适性。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决扩散大语言模型在数学和逻辑模式理解方面的挑战。针对数值敏感的数学任务和顺序敏感的逻辑任务，我们提出了一种DSFT(Diffusion SFT)策略，通过调整掩码策略和损失函数指导模型理解数学和逻辑模式，并在LLaDA和Dream系列模型上通过数学(GSM8K, MATH, GPQA)、逻辑(BBH, ARC-C, WinoGrande)和一般任务基准验证了其有效性，实现了数学问题5-10%和逻辑问题约2%的准确率提升。",
                    "summary_translation": "扩散大型语言模型（Diffusion large language models, dLLMs）是继自回归模型（auto regressive models）之后出现的一种新架构。它们的去噪过程（denoising process）提供了强大的生成优势（generative advantage），但在学习和理解对数值敏感的数学任务（numerically sensitive mathematical tasks）和对顺序敏感的逻辑任务（order-sensitive logical tasks）方面面临重大挑战。当前的训练方法，包括预训练（pre-training）、微调（fine-tuning）和强化学习（reinforcement learning），主要侧重于提高一般知识保留（knowledge retention）和推理能力（reasoning abilities），但缺乏对数学和逻辑模式的全面理解。我们提出了DSFT，一种简单而有效的扩散SFT策略（Diffusion SFT strategy），通过调整掩码策略（masking strategy）和损失函数（loss function），引导模型理解数学和逻辑模式。该策略可以灵活地与预训练、强化学习和其他训练方法结合使用。在LLaDA和Dream系列等模型上验证，我们证明在小规模数据上使用DSFT可以在数学问题和逻辑问题上分别实现5-10%和约2%的改进。这种启发性的掩码方法为未来特定模式的学习提供了见解，可以轻松高效地与其他训练方法结合，并应用于各种dLLMs。我们的代码公开可获取，网址为https://anonymous.4open.science/r/DSFT-0FFB/"
                },
                {
                    "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
                    "arxiv_id": "2509.18389",
                    "authors": "Jiuqi Wang, Rohan Chandra, Shangtong Zhang",
                    "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步：核心判断 这篇论文的核心是关于强化学习(RL)预训练如何使大语言模型获得上下文强化学习(ICRL)能力的研究。论文探讨的是为什么RL预训练算法能够产生支持ICRL的网络参数，并提出了假设：能够进行ICRL的参数是预训练损失的最小值。这属于改进LLM基础能力和提出新训练范式的研究，特别是关注模型如何在不更新参数的情况下通过上下文信息进行学习和推理，这与通用推理能力直接相关。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文研究的是Transformer模型在强化学习中的表现，Transformer是大语言模型的基础架构 - 能力方向：论文关注的是推理能力，特别是上下文学习(in-context learning)和时序差分学习(temporal difference learning)，这些都是通用推理能力的重要组成部分 - 训练方法：论文探讨强化学习预训练方法，这与RLHF/RL相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉 - 不针对特定应用领域（如医疗、化学等） - 不主要关注模型基础设施或部署优化 第四步：处理特殊和模糊情况 论文研究的是一种通用的学习机制（上下文强化学习），而不是将其应用于特定领域。它探讨的是模型如何通过预训练获得在不更新参数的情况下适应新任务的能力，这属于提升模型内在推理能力的研究。 第五步：最终决策 综合分析，这篇论文的核心贡献是研究大语言模型如何通过预训练获得上下文强化学习能力，这是一种重要的通用推理能力。论文探讨的是模型的基础能力和训练范式，而不是将模型作为工具应用于特定领域。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决上下文强化学习（ICRL）中为何强化学习预训练能产生支持ICRL的网络参数的问题。针对策略评估任务，我们提出了一种基于线性Transformer的理论分析方法，证明了预训练损失的全局最小化器能实现上下文时序差分学习，并在Boyan's chain环境中通过MSVE指标验证了随着上下文长度增加，模型性能提升且收敛参数与理论预测一致。",
                    "summary_translation": "通常，现代强化学习（reinforcement learning, RL）智能体通过更新其神经网络参数来调整策略以解决任务。最近，人们观察到一些RL智能体在某个任务分布上进行预训练后，无需参数更新就能解决各种新的分布外（out-of-distribution）任务。在新任务中评估时，预训练的智能体不进行参数更新，而是将其策略基于称为上下文（context）的额外输入，例如智能体在新任务中的交互历史。随着上下文中信息的增加，智能体的性能会提高，而智能体的参数保持固定。这种现象通常被称为上下文强化学习（in-context RL, ICRL）。智能体网络的预训练参数使得显著的ICRL现象成为可能。然而，许多ICRL研究使用标准RL算法进行预训练。这引出了本文旨在解决的核心问题：为什么RL预训练算法能够生成使ICRL成为可能的网络参数？我们假设能够实现ICRL的参数是预训练损失的最小值点（minimizers）。本研究通过案例研究为这一假设提供了初步支持。具体而言，我们证明了当Transformer（一种神经网络架构）为策略评估（policy evaluation）进行预训练时，预训练损失的一个全局最小值点（global minimizers）可以实现上下文时序差分学习（in-context temporal difference learning）。"
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断 这篇论文的本质是研究Transformer-based LLMs在图推理任务中的内部机制和工作原理。论文使用了circuit-tracer框架来解释decoder-only transformers的推理过程，识别了图推理中的两个核心机制（token merging和structural memorization），并提供了一个统一的可解释性框架。这属于研究LLM基础能力和推理机制的范畴，而不是将LLM作为工具应用到特定领域，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了\"Transformer-based LLMs\" - 能力方向：聚焦于\"graph reasoning\"，属于推理能力的研究范畴 虽然论文没有提到强化学习等训练方法或智能体等新兴范式，但已经包含了两个重要的正面指标。 第三步：排除标准 论文没有主要聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉研究 - 不针对特定应用领域（如医疗、化学等），虽然研究图推理，但这是作为理解LLM通用推理能力的一个窗口 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文涉及可解释性研究，但这是为了理解LLM的推理机制，提供\"统一的可解释性框架来理解结构推理\"，从而提升对模型内部工作原理的理解，符合研究目标，应该保留。 综上所述，这篇论文的核心贡献是揭示和解释LLM在图推理任务中的内部机制，这直接关系到提高LLM的通用推理能力，符合研究目标。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。"
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于RLHF（Reinforcement Learning from Human Feedback）的研究，特别是最大熵RLHF的失败模式分析。RLHF是改进大语言模型基础能力的关键训练范式，属于\"强化学习优化\"这一类别，符合第一步的保留标准。论文虽然没有直接提出新方法，而是分析现有方法的问题，但这种分析有助于理解如何更好地训练LLM，从而提升其通用能力。其次，论文明确涉及强化学习（RLHF）这一训练方法，符合第二步中的正面指标。同时，论文不聚焦于多模态、特定应用领域或模型可靠性的应用层面，因此不触犯第三步的排除标准。总体而言，这篇论文对改进LLM的训练方法有贡献，与提升大语言模型通用推理能力的研究目标相符。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。"
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出PromptCoT 2.0框架，这是一种新的训练范式，旨在通过合成高质量提示词来增强LLM的基础推理能力。论文明确关注提升LLM在数学推理和编程推理等通用能力上的表现，而非将LLM作为工具应用于特定领域。 从正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于reasoning(特别是数学推理)、planning和problem-solving；训练方法上涉及self-play(类似强化学习的思想)和self-evolve(模型通过自我博弈自主改进)。 论文不涉及任何排除标准中的领域：没有关注多模态与视觉，没有将LLM应用于特定领域(虽然使用数学和编程作为评估任务，但这些是用于评估通用推理能力的基准)，也没有主要关注模型可靠性的应用层面。 在特殊情况下，论文提到的self-play可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域的智能体。 论文的核心贡献是提出了一种可扩展的框架，通过迭代改进提示词构建来生成更难且更多样化的问题，从而提升LLM的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）正在从对话系统演变为强大的推理器，用于处理奥林匹克数学和竞技编程等任务。虽然扩大参数规模和测试时计算（test-time computation）推动了进步，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高且有限，而现有的合成语料库（synthetic corpora）通常过于简单或过于狭窄。\n\nPromptCoT 1.0表明，将推理过程（rationales）注入提示合成（prompt synthesis）可以增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（expectation-maximization, EM）循环替代手工设计的启发式算法（hand-crafted heuristics），其中推理过程被迭代优化以指导提示构建。这产生的比先前语料库更难且更多样化的问题。\n\n这些合成提示支持两种后训练（post-training）机制：（1）自我对弈（Self-Play），即强模型通过可验证的反馈（verifiable feedback）在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），即弱模型从教师提炼的痕迹（teacher-distilled traces）中学习。\n\n大量实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进（state-of-the-art）结果，在AIME 24/25和HMMT 25上分别提升了+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升了+6.1和+5.0，在Codeforces上提升了+35 Elo。在SFT中，仅在合成提示上训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布不同（distributionally distinct）的问题。这些结果确立了提示合成作为扩展推理能力的新维度（new axis for scaling reasoning），并将PromptCoT 2.0定位为未来开源模型的可扩展基础。实现代码可在https://github.com/inclusionAI/PromptCoT获取。"
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断 这篇论文的本质是提出一种名为VCRL的课程强化学习框架，用于改进大语言模型的数学推理能力。论文核心关注的是改进LLM的基础推理能力，提出了一种新的训练范式（基于方差的课程强化学习），这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的推理能力。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning - 训练方法：提出了reinforcement learning的新方法(VCRL) 第三步：排除标准 论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不针对特定应用领域（如医疗、化学等），虽然实验在数学推理任务上进行，但数学推理被视为通用推理能力的基础组成部分 - 不关注模型基础设施、部署优化或硬件加速 第四步：特殊和模糊情况处理 虽然论文在数学推理任务上进行了实验评估，但这并不使其成为特定应用领域的研究。数学推理通常被视为评估和提升LLM通用推理能力的关键基准。论文提出的VCRL是一种通用的课程强化学习框架，其原理可以推广到其他需要逐步学习的推理任务上，因此应被视为对LLM通用推理能力的提升。 综上所述，这篇论文的核心贡献是提出了一种基于方差的课程强化学习方法，通过动态控制训练样本难度来提升LLM的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（policy-based reinforcement learning）目前在提高大型语言模型（LLMs）的数学推理任务能力方面发挥着重要作用。然而，现有的基于rollout的强化学习方法（如GRPO、DAPO、GSPO等）未能明确考虑LLMs对不同难度级别样本的学习能力，这与人类从易到难的数学推理认知过程相悖。直观上，我们发现RLVR中rollout组的奖励（reward）方差部分反映了当前样本对LLMs的难度。过于简单或过于困难的样本具有较低的方差，而中等难度的样本则具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准和两个模型上的实验揭示了VCRL相较于当前LLM强化学习基线方法的优势。"
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我认为这篇论文符合研究范围。以下是我的判断过程： 第一步：核心判断上，论文本质是研究线性注意力transformer如何通过训练隐式发现数值算法的能力。这属于改进LLM基础能力的研究，特别是增强其数学推理和问题解决能力。论文探索了模型如何通过矩阵补全任务自动学习数学计算规则，而非被明确编程这些规则，这符合\"提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。 第二步：正面指标方面，论文包含以下相关主题： - 核心概念：提及\"Linear Transformers\"，与LLMs直接相关 - 能力方向：涉及数值算法的发现，与数学推理和问题解决能力密切相关 - 论文展示了transformer的上下文学习能力，这是提升通用推理能力的关键 第三步：排除标准方面，论文不涉及任何排除领域： - 没有多模态与视觉内容 - 没有将LLM应用于特定领域（如医疗、化学等） - 没有聚焦于模型可靠性的应用层面（如水印、安全等） 第四步：论文不涉及特殊或模糊情况（如智能体/工具使用、幻觉/可解释性等），所以无需额外判断。 综合来看，这篇论文的核心贡献在于展示了transformer如何通过训练隐式发现统一的数值算法，这直接提升了LLM的数学推理和问题解决能力，属于\"大语言模型通用推理能力\"的研究范围。论文关注的是模型的基础能力提升，而非特定领域应用，因此符合研究目标。",
                    "summary2": "",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。"
                },
                {
                    "title": "Mamba Modulation: On the Length Generalization of Mamba",
                    "arxiv_id": "2509.19633",
                    "authors": "Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui",
                    "summary": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究Mamba架构（一种大语言模型架构）在长上下文泛化方面的局限性，并提出了一种改进方法。论文通过分析Mamba在处理比预训练时更长的上下文时性能下降的问题，将其归因于状态空间动态的参数化，特别是状态转移矩阵A的特性。作者提出了一种频谱缩放方法来改进预训练Mamba模型，以实现稳健的长上下文泛化。这符合我的研究目标，因为论文关注的是改进大语言模型的基础能力（特别是长上下文推理能力），而不是将LLM应用于特定领域。论文的贡献在于增强模型本身的通用推理能力，使其能够更好地处理长序列，这对于复杂的多步推理任务至关重要。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决Mamba模型在处理超出预训练长度的上下文时性能显著下降的问题。针对长上下文泛化场景，我们提出了一种基于状态转移矩阵A频谱缩放的方法，并在ProofPile、PG19等数据集以及Passkey检索和LongBench基准上通过困惑度和准确率等指标验证了其有效性。",
                    "summary_translation": "Transformer模型中注意力机制（attention mechanism）的二次复杂度（quadratic complexity）推动了具有次二次扩展（sub-quadratic scaling）的替代架构的发展，如状态空间模型（state-space models）。其中，Mamba已成为一种领先的架构，在一系列语言建模任务中取得了最先进（state-of-the-art）的成果。\n\n然而，当Mamba应用于比预训练期间所见更长的上下文时，其性能显著下降，显示出对上下文长度扩展的明显敏感性。通过详细分析，我们将这一限制归因于其状态空间动态（state-space dynamics）的分布外行为（out-of-distribution behaviour），特别是在状态转移矩阵（state transition matrix）$\\mathbf{A}$的参数化中。与近期将这种敏感性归因于离散时间步长（discretization time steps）$\\exp(-\\sum_{t=1}^N\\Delta_t)$的消失累积的研究不同，我们建立了当输入长度趋近于无穷大时状态收敛行为与转移矩阵$\\mathbf{A}$的谱（spectrum）之间的联系，为其在长度扩展中的作用提供了充分依据的解释。\n\n接下来，为了克服这一挑战，我们提出了一种将谱缩放（spectrum scaling）应用于预训练Mamba模型的方法，通过选择性调制（modulating）每一层中$\\mathbf{A}$矩阵的谱来实现稳健的长上下文泛化（long-context generalization）。我们表明，在仅仅调制$\\Delta_t$失效的情况下，这种方法可以显著提高性能，验证了我们的见解，并为具有结构化转移矩阵（structured transition matrices）的状态空间模型提供了更好的长度泛化（length generalization）途径。"
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是改进LLM的基础能力和通用推理能力。论文提出\"Thinking augmented Pre-Training (TPT)\"方法，通过在文本数据中增加思维轨迹来增强LLM的预训练过程。这是一种新的训练范式，旨在通过逐步推理和分解来提高模型的学习效率和推理能力，而不是将LLM应用于特定领域。 第二步正面指标：论文包含多个正面指标。它明确关注\"Large language models (LLMs)\"这一核心概念，并专注于\"reasoning\"能力方向（论文提到\"step-by-step reasoning\"和\"reasoning benchmarks\"）。实验结果也证明该方法在多个具有挑战性的推理基准上提升了模型性能。 第三步排除标准：论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，它直接关注通过思维轨迹增强来提升LLM的通用推理能力。 最终决策：论文的核心贡献是提出了一种通用的训练方法(TPT)来增强LLM的推理能力，提高数据效率，这与研究目标完全一致。实验表明该方法能显著提升模型在推理任务上的表现，因此应该被保留。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。"
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文提出了UserRL框架，通过强化学习训练交互式以用户为中心的智能体，核心是关于提升大语言模型的交互能力和多轮对话能力。从第一步核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（UserRL框架），并使用强化学习（GRPO算法）来优化模型，这符合\"改进LLM基础能力和提出新训练范式\"的保留标准。 从第二步正面指标看，论文包含多个相关主题：1)核心概念方面，论文明确基于Qwen3等大语言模型；2)训练方法方面，论文使用了强化学习(RL)方法；3)新兴范式方面，论文关注的是基于LLM的智能体(llm-based agents)的训练。 从第三步排除标准看，论文没有涉及多模态与视觉内容，没有聚焦于特定应用领域（如医疗、化学等），也没有主要关注模型可靠性的应用层面问题（如水印、安全性），因此不符合任何排除标准。 从第四步特殊和模糊情况处理看，论文提出的是通用的智能体训练框架，旨在增强LLM的通用交互能力，而不是将智能体应用在特定领域，因此应该保留。 综合分析，论文的核心贡献是提供了一种新的强化学习框架来提升LLM的交互式推理和多轮对话能力，这属于大语言模型通用推理能力的重要组成部分，与研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习(Reinforcement Learning, RL)在训练智能体模型(agentic models)方面展现出潜力，这些模型能够超越静态基准(static benchmarks)，进行动态、多轮交互(dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这种场景中，用户交互的多样性和动态性带来了挑战。\n\n在这项工作中，我们提出了UserRL，这是一个通过标准化gym环境(standardized gym environments)与模拟用户(simulated users)相结合，用于训练和评估以用户为中心能力(user-centric abilities)的统一框架。我们系统地改变轮级奖励分配(turn-level reward assignment)和轨迹级分数计算(trajectory-level score calculation)，以分析不同公式化方法如何影响GRPO算法(GRPO algorithm)下的学习效果。\n\n我们在Qwen3模型上的实验揭示了三个关键发现：(i) SFT冷启动(SFT cold start)对于解锁初始交互能力和实现持续的强化学习改进至关重要；(ii) 精心设计的轨迹评分(deliberate trajectory scoring)能够产生更高效、更有效的多轮交互；(iii) 尽管更强大的模拟用户(如GPT-4o)有助于训练，但开源模拟器(如Qwen3-32B)仍然是一种经济高效且可转移的选择。\n\n总的来说，这些结果强调，奖励塑形(reward shaping)和用户模拟选择的精心设计与模型规模同等重要，并将UserRL确立为开发稳健的以用户为中心的智能体模型的实用途径。所有代码和数据均公开，以供未来研究使用。"
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究大语言模型在认知负荷下的多跳推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估方法(ICE基准测试)，旨在理解和提升LLMs在复杂推理任务中的表现。这明显属于改进LLM基础推理能力的研究，特别是关注多跳推理这一通用推理能力的核心方面，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标：它明确关注\"Large Language Models (LLMs)\"这一核心概念，并深入研究\"multi-hop reasoning\"(多跳推理)这一关键推理能力方向。虽然论文不涉及训练方法和新兴范式，但对推理能力的深入研究已足够表明其与研究目标的高度相关性。 第三，论文不符合任何排除标准：它不涉及多模态与视觉研究，不聚焦于任何特定应用领域(如医疗、化学等)，也不主要关注模型可靠性的应用层面问题(如水印、安全性等)。虽然论文结尾提到了\"安全性\"评估，但这是作为评估模型推理能力的一个方面，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了新的理论框架和评估方法来理解和提升LLMs的通用推理能力，特别是在认知负荷条件下的多跳推理表现，完全符合研究目标。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的扩展已经揭示了其在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间存在的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷下推理的计算限制仍然知之甚少。在这项工作中，我们提出了计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换的干扰（Attentional Residue，注意残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令调优（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2）表现出基准脆弱性，在这个高内在负荷（high-intrinsic-load）任务中，包括清洁对照组在内的所有条件下准确率均达到0%（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性，在对照组条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的下降（$\\beta = -0.003$每增加1%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，以ICE基准测试为例的动态、认知感知的压力测试对于评估先进AI系统的真正韧性和安全性至关重要。"
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力和提出新的训练范式。论文提出的ShinkaEvolve框架利用LLMs作为变异算子，通过演化的方式增强其问题解决和推理能力。论文的核心贡献是提高LLM在程序演化方面的样本效率和解决方案质量，而不是将LLM作为工具应用于特定领域，因此符合保留标准。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)作为核心组件；关注推理能力，特别是在数学推理任务中的应用；采用演化(evolution)作为训练方法；并使用了基于LLM的智能体框架(evolutionary agentic harnesses)。 第三，论文不符合排除标准。虽然论文提到了一些应用案例(如圆包装、数学推理、竞争编程等)，但这些是作为评估框架性能的示例，而非论文的主要焦点。论文的核心是提出一种通用的程序演化框架，而非专注于多模态、特定应用领域或模型可靠性等排除领域。 最后，在特殊和模糊情况的处理上，论文提出的是一种通用的演化智能体框架，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用，因此应该保留。 综合来看，ShinkaEvolve论文致力于通过演化的方法增强LLM的通用推理和问题解决能力，与研究目标高度一致。",
                    "summary2": "",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。"
                },
            ]
        },
    ],
};


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            
            // 立即隐藏
            paperEl.classList.add('hidden-paper');
            deletedPapers.add(arxivId);
            saveState();
            updateStats();
            
            // 设置撤销计时器
            const timerId = setTimeout(() => {
                paperEl.remove();
                pendingDeletes.delete(arxivId);
                updateStats();
            }, 5000);
            
            pendingDeletes.set(arxivId, timerId);
            
            showUndoToast('已删除，5秒内可撤销', 5, () => {
                // 撤销删除
                paperEl.classList.remove('hidden-paper');
                deletedPapers.delete(arxivId);
                const t = pendingDeletes.get(arxivId);
                if (t) { 
                    clearTimeout(t); 
                    pendingDeletes.delete(arxivId); 
                }
                saveState();
                updateStats();
            });
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
            if (!starBtn) return;
            
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
                starBtn.classList.remove('starred');
            } else {
                starredPapers.add(arxivId);
                starBtn.classList.add('starred');
            }
            saveState();
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6 ${isDeleted ? 'hidden-paper' : ''}" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-4">
                        <div class="flex items-start space-x-3 flex-1">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-lg font-semibold text-black dark:text-white leading-tight">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-4 flex-shrink-0" onclick="deletePaper('${paper.arxiv_id}', '${paper.title.replace(/'/g, "\\'")}'))" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-4">
                        <div class="flex flex-wrap items-center gap-4 text-sm text-slate-600 dark:text-slate-400">
                            <span><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300">
                                ${paper.category}
                            </span>
                            <span>${date}</span>
                        </div>
                        <div class="text-sm text-black dark:text-white">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50">
                            <span class="ml-2 text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 -->
                    <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-4 mb-4 rounded-r-lg">
                        <div class="text-sm text-black dark:text-white leading-relaxed">
                            <strong class="font-medium text-blue-600 dark:text-blue-400">筛选原因:</strong> ${paper.filter_reason}
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 -->
                    <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-4 mb-4 rounded-r-lg">
                        <div class="text-sm text-black dark:text-white leading-relaxed">
                            <strong class="font-medium text-yellow-600 dark:text-yellow-400">AI总结:</strong> ${paper.summary2}
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 -->
                    <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-4 mb-4 rounded-r-lg">
                        ${paper.summary_translation ? `
                        <div class="chinese-summary text-sm text-black dark:text-white leading-relaxed" style="display: block;">
                            <strong class="font-medium text-green-600 dark:text-green-400">原始摘要:</strong> ${paper.summary_translation}
                        </div>
                        ` : ''}
                        ${paper.summary ? `
                        <div class="english-summary text-sm text-black dark:text-white leading-relaxed" style="display: none;">
                            <strong class="font-medium text-green-600 dark:text-green-400">Original Abstract:</strong> ${paper.summary}
                        </div>
                        ` : ''}
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    papersHTML += `
                        <li>
                            ${createPaperHTML(paper, date)}
                        </li>
                    `;
                });
            } else {
                papersHTML = '<li class="pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-3">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400">${category.name}</span>
                        </div>
                        <span class="text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5">${category.count}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-4">
                        <ul class="space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            loading.classList.add('hidden');
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                const dateTotal = categories.reduce((sum, cat) => sum + cat.count, 0);
                totalPapers += dateTotal;
                
                html += `
                    <section class="mb-8">
                        <h2 class="text-lg font-medium text-slate-500 dark:text-slate-400 mb-4">${date} (${dateTotal} 篇论文)</h2>
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6">
                            <ul class="space-y-2">
                `;
                
                categories.forEach(category => {
                    html += createCategoryHTML(category, date);
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }
            
            mainContent.innerHTML = html;
            updateStats();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            renderPapers();
        });
    </script>
</body>
</html>