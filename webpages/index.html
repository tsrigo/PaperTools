<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PaperTools - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 引入 Marked.js 用于 Markdown 渲染 -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        
        /* 移动端优化 */
        @media (max-width: 640px) {
            body {
                font-size: 14px;
            }
            
            /* 改善可点击区域 */
            button, a {
                min-height: 44px;
                min-width: 44px;
            }
            
            /* 优化间距 */
            .container {
                padding-left: 12px !important;
                padding-right: 12px !important;
            }
        }
        
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
        
        /* 可折叠部分样式 */
        .collapsible-header {
            cursor: pointer;
            display: flex;
            align-items: center;
            font-weight: 600;
            padding: 8px 0;
            user-select: none;
            color: #1e40af;
            transition: all 0.2s ease-in-out;
        }
        .dark .collapsible-header {
            color: #60a5fa;
        }
        .collapsible-header:hover {
            opacity: 0.8;
        }
        .collapsible-header::before {
            content: "▶";
            margin-right: 8px;
            transition: transform 0.3s ease;
            font-size: 0.8em;
        }
        .collapsible-header.open::before {
            transform: rotate(90deg);
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: none;
        }
        .collapsible-content .inner {
            padding-top: 8px;
        }
        
        /* Markdown 内容样式 */
        .markdown-content {
            line-height: 1.6;
        }
        .markdown-content h1 {
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 1em;
            margin-bottom: 0.5em;
            color: #1e40af;
        }
        .dark .markdown-content h1 {
            color: #60a5fa;
        }
        .markdown-content h2 {
            font-size: 1.3em;
            font-weight: bold;
            margin-top: 0.8em;
            margin-bottom: 0.4em;
            color: #1e40af;
        }
        .dark .markdown-content h2 {
            color: #60a5fa;
        }
        .markdown-content h3 {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 0.6em;
            margin-bottom: 0.3em;
            color: #1e40af;
        }
        .dark .markdown-content h3 {
            color: #60a5fa;
        }
        .markdown-content h4 {
            font-size: 1em;
            font-weight: bold;
            margin-top: 0.5em;
            margin-bottom: 0.25em;
            color: #2563eb;
        }
        .dark .markdown-content h4 {
            color: #93c5fd;
        }
        .markdown-content p {
            margin-bottom: 0.8em;
        }
        .markdown-content ul, .markdown-content ol {
            margin-left: 1.5em;
            margin-bottom: 0.8em;
        }
        .markdown-content ul {
            list-style-type: disc;
        }
        .markdown-content ol {
            list-style-type: decimal;
        }
        .markdown-content li {
            margin-bottom: 0.3em;
        }
        .markdown-content code {
            background-color: #f1f5f9;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
        .dark .markdown-content code {
            background-color: #334155;
        }
        .markdown-content pre {
            background-color: #f1f5f9;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 0.8em;
        }
        .dark .markdown-content pre {
            background-color: #334155;
        }
        .markdown-content pre code {
            background-color: transparent;
            padding: 0;
        }
        .markdown-content blockquote {
            border-left: 3px solid #cbd5e1;
            padding-left: 1em;
            margin-left: 0;
            margin-bottom: 0.8em;
            color: #64748b;
        }
        .dark .markdown-content blockquote {
            border-left-color: #475569;
            color: #94a3b8;
        }
        .markdown-content strong {
            font-weight: 600;
        }
        .markdown-content em {
            font-style: italic;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-3 sm:px-4 py-2 rounded-lg shadow-lg z-50 hidden max-w-xs sm:max-w-sm">
        <div class="flex items-center space-x-2">
            <span id="toast-message" class="text-sm sm:text-base">已删除</span>
            <span id="countdown" class="text-xs sm:text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-xs sm:text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-full lg:w-3/5 max-w-none p-3 sm:p-4 lg:p-6">
        <!-- 头部导航栏 -->
        <header class="mb-4 sm:mb-6">
            <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-3 sm:gap-4">
                <h1 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white">PaperTools</h1>
                <div class="flex flex-wrap items-center gap-2 sm:gap-3 w-full sm:w-auto">
                    <!-- 统计信息 -->
                    <div class="text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                        总计 <span id="total-papers">0</span> 篇论文
                    </div>
                    <!-- 筛选按钮 -->
                    <button id="filter-starred" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none transition-colors whitespace-nowrap">
                        显示全部
                    </button>
                    <!-- 中英文摘要切换按钮 -->
                    <button id="summary-toggle" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        中文摘要
                    </button>
                    <button id="theme-toggle" class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <!-- 太阳图标 (浅色模式) -->
                        <svg id="theme-icon-light" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                        </svg>
                        <!-- 月亮图标 (深色模式) -->
                        <svg id="theme-icon-dark" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                        </svg>
                    </button>
                    <!-- GitHub 图标按钮 -->
                    <a href="https://github.com/tsrigo/PaperTools" target="https://github.com/tsrigo/PaperTools" title="GitHub 项目主页"
                       class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <svg class="h-5 w-5 sm:h-6 sm:w-6 text-slate-700 dark:text-slate-200" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.021c0 4.428 2.865 8.184 6.839 9.504.5.092.682-.217.682-.483 0-.237-.009-.868-.014-1.703-2.782.605-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.004.07 1.532 1.032 1.532 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.339-2.221-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.254-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.025A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.295 2.748-1.025 2.748-1.025.546 1.378.202 2.396.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.847-2.337 4.695-4.566 4.944.359.309.678.919.678 1.852 0 1.336-.012 2.417-.012 2.747 0 .268.18.579.688.481C19.138 20.2 22 16.447 22 12.021 22 6.484 17.523 2 12 2z" clip-rule="evenodd"/>
                        </svg>
                    </a>
                </div>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-6 sm:space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2026-01-09": [
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse",
                    "arxiv_id": "2601.05505",
                    "authors": "Yubo Hou, Zhisheng Chen, Tao Wan, Zengchang Qin",
                    "summary": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 FlashMem 这一框架，旨在解决 LLM 智能体在长期自主任务中缺乏动态上下文保存机制的问题。这属于构建和改进 LLM 智能体架构的方法论研究，而非将智能体作为工具应用到特定领域的非演化型应用。 2.  **符合研究焦点**: 论文明确针对 \"Agents\" 的 \"Memory\"（记忆）能力。根据您的筛选标准，单智能体方向明确包含了 \"记忆\" 这一子方向。FlashMem 通过计算重用提取内在记忆，并引入认知监视器，直接增强了智能体的持久认知和长期自主能力。 3.  **排除标准检查**: 论文不涉及安全、对齐、多模态或图技术。尽管论文提到了 KV Cache 和推理延迟的优化，但其根本目的是为了实现智能体的 \"持久认知\" 和 \"长期自主\"，属于智能体核心能力的架构改进，而非单纯的模型基础设施或硬件加速研究。 4.  **结论**: 该论文属于单智能体方向中关于记忆机制的改进，符合您关于 \"构建、改进 LLM 智能体\" 的核心目标。",
                    "summary2": "本文旨在解决LLM无状态架构导致的历史信息冗余处理及现有潜在记忆方法的架构分离问题。针对长时程自主代理任务，我们提出了一种FlashMem框架，利用Shared-KV Consolidator直接复用主干网络的冻结缓存提取记忆，并通过基于注意熵的Cognitive Monitor自适应触发记忆整合。我们在GSM8K、MATH等六个基准数据集上，通过准确率、ROUGE-1及推理延迟验证了其有效性，结果显示其在匹配重型基线性能的同时，将推理延迟降低了5倍。",
                    "summary_translation": "大型语言模型 的 stateless architecture (无状态架构) 本质上缺乏保存动态上下文的机制，迫使智能体 冗余地重新处理历史记录以维持 long-horizon autonomy (长期自主性)。尽管 latent memory (潜在记忆) 提供了一种解决方案，但当前方法受限于 architectural segregation (架构分离)，依赖于将记忆与 reasoning backbone (推理骨干网络) 解耦的 auxiliary encoders (辅助编码器)。我们提出了 FlashMem，这是一个通过 computation reuse (计算复用) 直接从 transient reasoning states (瞬时推理状态) 中蒸馏 intrinsic memory (内在记忆) 的框架。利用 internal representations (内部表示) 唯一编码 input trajectories (输入轨迹) 的特性，FlashMem 将 last hidden state (最后一个隐藏状态) 识别为交互历史的 sufficient statistic (充分统计量)。这使得 Shared-KV Consolidator (共享键值整合器) 能够通过直接关注 backbone's frozen cache (骨干网络的冻结缓存) 来合成记忆，从而消除 redundant re-parameterization (冗余的重新参数化)。此外，一个无参数的 Cognitive Monitor (认知监视器) 利用 attention entropy (注意力熵)，仅在检测到高 epistemic uncertainty (认知不确定性) 时自适应地触发 consolidation (整合)。实验表明，FlashMem 在将 inference latency (推理延迟) 降低 5 倍的同时，达到了 heavy baselines (重型基线) 的性能，有效地弥合了效率与 persistent cognition (持久认知) 之间的差距。",
                    "inspiration_trace": "基于论文《FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观困境与现状反思\n**——从“无状态”架构的局限出发**\n\n1.  **观察现象**：现有的LLM本质上是“无状态”的，它们将输入映射到输出，但在交互之间不保留持久的内部状态。\n2.  **面临挑战**：对于需要长期自主性的智能体，这种无状态性导致了一个严重的瓶颈——为了维持上下文连贯性，智能体必须在每一步推理中**冗余地重新处理历史信息**。\n3.  **现有方案的局限**：虽然“潜在记忆”被提出作为解决方案（将上下文压缩为密集向量），但作者发现现有方法存在根本性的**结构低效**。它们通常采用“分离式架构”，即依赖独立的编码器或适配器来生成记忆，这与主推理骨干是割裂的。\n\n### 第二阶段：痛点诊断与核心假设\n**——识别“计算冗余”与“架构隔离”的根源**\n\n1.  **深入分析**：为什么现有的潜在记忆方案效率低下？作者意识到，这是因为它们引入了**辅助参数**来重新编码历史。\n2.  **逻辑推演**：\n    *   LLM在推理过程中已经计算过一次历史信息，这些信息蕴含在内部的KV Cache（键值缓存）和隐藏状态中。\n    *   现有方法却丢弃这些现成的计算结果，转而使用另一个独立的模块从头开始处理原始文本。这不仅是存储上的浪费，更是**计算上的重复**。\n3.  **提出核心假设**：**“内在性”假设**。LLM的内部表示（特别是最后一层的隐藏状态）已经唯一且充分地编码了输入轨迹。因此，我们不需要外部编码器，可以直接从模型现有的推理状态中“蒸馏”出记忆。\n\n### 第三阶段：范式转移——从“分离”到“内在”\n**——确立“计算复用”的设计哲学**\n\n1.  **思维转变**：从“如何设计一个更好的外部记忆编码器”转变为“如何直接复用骨干网络的计算成果”。\n2.  **理论支撑**：利用LLM表示的**单射性**，即输入轨迹与内部表示是一一对应的。这意味着**最后一个隐藏状态是交互历史的充分统计量**。\n3.  **方法论雏形**：提出**计算复用**的概念。记忆生成过程不应是一个独立的编码Pass，而应是一个直接读取骨干网络冻结KV Cache的“读取”操作。\n\n### 第四阶段：机制细化——何时记忆与如何记忆\n**——解决动态触发与轻量化实现的矛盾**\n\n1.  **子问题一：何时生成记忆？（动态触发）**\n    *   **思考**：并非每一步推理都需要记忆，频繁生成会带来巨大开销。我们需要一个“认知监控器”。\n    *   **洞察**：模型的不确定性与注意力机制的熵高度相关。当模型困惑时，注意力分布趋于分散（高熵）。\n    *   **方案**：设计一个**无参数的认知监控器**，基于注意力熵来实时检测模型的“认知困惑”。只有当熵超过阈值（即模型不确定时）才触发记忆固化，避免在简单问题上浪费算力。\n\n2.  **子问题二：如何高效生成记忆？（轻量化读取）**\n    *   **思考**：既然要复用KV Cache，那么记忆生成模块就不应该有庞大的参数。\n    *   **方案**：设计**共享KV整合器**。\n        *   **输入**：直接使用骨干网络当前的隐藏状态作为初始Query。\n        *   **操作**：通过交叉注意力机制，直接对骨干网络的冻结KV Cache进行查询。\n        *   **去重**：摒弃传统的Key/Value投影矩阵，只保留Query的投影，实现极低的参数开销。\n\n### 第五阶段：逻辑闭环与系统成型\n**——FlashMem框架的最终确立**\n\n1.  **整合逻辑**：\n    *   **感知层**：利用注意力熵监控模型的不确定性，决定“何时”介入。\n    *   **提取层**：利用Shared-KV Consolidator，直接从骨干网络的现有状态中提取信息，解决“如何”高效提取。\n    *   **反馈层**：生成的潜在记忆向量被软注入回骨干网络的输入流，作为高密度的认知线索。\n2.  **最终愿景**：FlashMem不再是一个外挂的辅助系统，而是一个与骨干网络深度耦合的**内在记忆机制**。它消除了架构隔离，通过复用计算资源，在保持高性能推理的同时，实现了极低的推理延迟（5倍提升）。\n\n---\n\n**总结**：作者的思考路径是从**“无状态架构的缺陷”**出发，通过批判**“现有分离式架构的冗余”**，提出了**“内在记忆与计算复用”**的核心假设，并最终通过**“熵触发机制”**和**“共享KV设计”**将这一假设落地为一个高效、轻量的智能体记忆框架。"
                },
            ]
        },
    ],
    "2026-01-08": [
        {
            "name": "Artificial Intelligence",
            "count": 9,
            "papers": [
                {
                    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
                    "arxiv_id": "2601.05187",
                    "authors": "Yanchang Liang, Xiaowei Zhao",
                    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，主要基于以下判断： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文的核心不仅仅是将LLM应用于Simulink这一特定领域，而是提出了一种新的智能体架构和训练方法。具体而言，它提出了 **Reflection-GRPO (ReGRPO)**，这是一种结合了**自我反思**机制的强化学习算法。该算法通过自我反思痕迹提供中间反馈，解决长视距任务中的稀疏奖励问题，从而加速收敛并提高鲁棒性。这直接对应了筛选标准中的“自我演化”和“自我反思”机制。 2.  **具备明确的Agentic特征**： 论文描述了一个轻量级的 **“计划-执行”架构**，并明确提到该架构赋予智能体低级工具技能和高级设计推理能力。这符合筛选标准中关于“规划”和“工具使用”的正面指标。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是Simulink建模（特定工程领域），但根据筛选标准第四步第2点，如果论文的核心是提出一种新的“自我演化”机制（即ReGRPO和两阶段课程学习），即使它被应用在特定领域，也应该保留。本文的重点在于智能体如何通过反思和强化学习进行自我完善和迭代，而非单纯的应用。 综上所述，该论文在构建智能体架构、引入自我反思机制以及通过强化学习实现自我演化方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决LLM在图形化Simulink建模中的应用难题。针对Simulink建模任务，我们提出了SimuAgent框架，采用轻量级Python字典表示模型，并引入Reflection-GRPO (ReGRPO)算法结合两阶段训练策略。我们在新发布的SimuBench数据集（5300个任务）上进行了实验，通过建模准确率验证了其有效性。结果显示，SimuAgent比标准RL收敛更快，且超越了GPT-4o。",
                    "summary_translation": "大语言模型彻底变革了基于文本的代码自动化，但它们在面向图形的工程工作流中的潜力仍未得到充分探索。我们介绍了SimuAgent，这是一个专为Simulink定制的、由LLMs（大语言模型）驱动的建模与仿真智能体。SimuAgent用简洁的字典风格Python表示法取代了冗长的XML（可扩展标记语言），大幅减少了token（词元）数量，提高了可解释性，并实现了快速的进程内仿真。一种轻量级的plan-execute（规划-执行）架构，分两个阶段进行训练，使智能体既具备低级工具技能，又具备高级设计推理能力。为了解决长视界任务中的稀疏奖励问题，我们提出了Reflection-GRPO (ReGRPO)，它通过自我反思轨迹增强了Group Relative Policy Optimization (GRPO)（群体相对策略优化），这些轨迹提供了丰富的中间反馈，从而加速收敛并提高鲁棒性。在我们新发布的包含5300个多领域建模任务的基准SimuBench上的实验表明，使用SimuAgent微调的Qwen2.5-7B模型比标准RL（强化学习）基线收敛更快，建模精度更高，并且在同一基准上使用few-shot prompting（少样本提示）进行评估时，甚至超越了GPT-4o。消融实验证实，两阶段课程和抽象-重构数据增强进一步提高了泛化能力。SimuAgent完全在本地使用适度的硬件进行训练和运行，为工业模型驱动工程提供了一个保护隐私且具有成本效益的解决方案。SimuAgent弥合了LLMs与图形建模环境之间的差距，为工业环境下的AI辅助工程设计提供了实用的解决方案。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
                    "arxiv_id": "2601.05107",
                    "authors": "Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng",
                    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向下的核心研究。 1.  **核心贡献判断 (第一步)**: *   论文的核心贡献是提出了 **SteeM (Steerable Memory Agent)**，这是一个用于LLM智能体的新框架。 *   它旨在解决长期人机交互中智能体如何使用记忆的问题，即如何平衡“记忆锚定”与“创新”。这属于对LLM智能体核心组件（记忆机制）的构建和改进，而非简单的应用或基础设施研究。 2.  **符合核心关注点 (第二步)**: *   论文直接对应我的研究焦点中的 **“单智能体”** 类别。 *   具体而言，它涉及智能体的关键能力指标：**`Memory` (记忆)**。论文探讨了如何量化记忆依赖度以及如何动态调节记忆的使用，这是提升智能体在长期任务中表现的关键技术。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了“个性化人机协作”，但这只是应用场景，其核心在于提出一种可控的记忆机制框架，而非单纯解决特定领域的业务问题。 综上所述，该论文致力于改进LLM智能体的记忆机制，属于构建和演化LLM智能体的核心方法论研究，因此予以保留。",
                    "summary2": "本文旨在解决长期人机交互中智能体过度依赖历史记忆导致的“Memory Anchoring”问题，实现用户对记忆依赖度的动态控制。针对Research和Tutoring场景，我们提出了一种名为SteeM的框架，通过偏好对齐的数据生成、SFT和GRPO训练，使智能体能根据用户偏好动态调整记忆使用程度。我们在合成的长期交互数据集上，通过alignment error和reward score验证了其有效性，结果显示SteeM优于传统提示方法和记忆掩码策略。",
                    "summary_translation": "随着 LLM-based agents (基于大语言模型的智能体) 在长期交互中的应用日益广泛，cumulative memory (累积记忆) 对于实现个性化以及保持风格一致性至关重要。然而，大多数现有系统在记忆使用上采取了一种 all-or-nothing approach (全有或全无的方法)：纳入所有相关的过往信息可能导致 Memory Anchoring (记忆锚定)，即智能体受困于过去的交互；而完全排除记忆则会导致 under-utilization (利用不足) 以及重要交互历史的丢失。我们表明，智能体的 reliance on memory (记忆依赖) 可以被建模为一个显式的且用户可控的维度。我们首先引入了一种 memory dependence (记忆依赖度) 的 behavioral metric (行为指标)，用于量化过往交互对当前输出的影响。随后，我们提出了 **Stee**rable **M**emory Agent (可操控记忆智能体)，即 \\texttt{SteeM}，这是一个允许用户动态调节 reliance on memory (记忆依赖) 的框架，其调节范围涵盖促进创新的 fresh-start mode (全新开始模式) 到紧密遵循交互历史的 high-fidelity mode (高保真模式)。在不同场景下的实验表明，我们的方法始终优于 conventional prompting (传统提示) 和 rigid memory masking strategies (僵化的记忆掩码策略)，为个性化的人机协作提供了更为 nuanced (细致的) 且有效的控制手段。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
                    "arxiv_id": "2601.04888",
                    "authors": "Tongyu Wen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心判断**: *   论文的核心贡献是提出了 **SmartSearch**，这是一个专门用于改进 **LLM-based search agents**（基于LLM的搜索智能体）的新框架。 *   它不是简单地将智能体作为工具应用到特定垂直领域（如医疗或金融），而是专注于解决智能体本身在执行搜索任务时的核心缺陷（即中间搜索查询质量不高），属于对智能体能力的底层构建和改进。 2.  **符合研究焦点**: *   **单智能体**: 论文明确针对搜索智能体的架构进行优化，涉及智能体的 **Tool Use**（使用搜索工具）和 **Planning**（生成查询的推理过程）。 *   **自我演化**: 论文引入了“过程奖励”和“查询优化”机制，并设计了一个三阶段的课程学习框架（模仿 -> 对齐 -> 泛化）。这种机制旨在让智能体通过反馈信号，逐步内化并自我改进其生成查询的能力。这符合“自我演化”中通过环境反馈进行自我完善和迭代的标准。 3.  **排除标准检查**: *   论文不涉及安全、对齐、可解释性或水印等排除主题。 *   论文不涉及多模态或视觉技术。 *   论文不涉及知识图谱或图神经网络。 综上所述，该论文通过提出新的方法论来提升LLM智能体的工具使用效率和自我优化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决基于LLM的搜索代理中中间搜索查询质量低下的问题。针对知识密集型任务，我们提出了一种结合过程奖励和查询优化的SmartSearch框架，并设计了三阶段课程学习策略。我们在2WikiMQA、HotpotQA等六个基准上，通过EM和F1分数验证了其有效性，显著提升了搜索效率和查询质量。",
                    "summary_translation": "基于大语言模型 (LLM) 的搜索智能体通过结合信息检索能力，在解决知识密集型问题方面展现出了巨大的潜力。现有工作主要集中在优化搜索智能体的推理范式，然而推理过程中中间搜索查询的质量却往往被忽视。结果是，生成的查询往往不准确，导致检索结果不理想，最终限制了搜索智能体的整体效能。为了缓解这一问题，我们提出了 SmartSearch，这是一个基于两个关键机制的框架：(1) 过程奖励，它通过双层信用评估为每个中间搜索查询的质量提供细粒度的监督；(2) 查询精炼，通过选择性精炼低质量搜索查询并基于这些精炼结果重新生成后续搜索轮次，从而促进查询生成的优化。为了使搜索智能体能够在过程奖励的指导下逐步内化提高查询质量的能力，我们设计了一个三阶段的课程学习框架。该框架引导智能体经历从模仿，到对齐，最终到泛化的递进过程。实验结果表明，SmartSearch 始终优于现有基线，额外的定量分析进一步证实了其在搜索效率和查询质量方面的显著提升。代码可在 https://github.com/MYVAE/SmartSearch 获取。",
                    "inspiration_trace": "基于对论文《SmartSearch: Process Reward-Guided Query Refinement for Search Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：搜索智能体的“阿喀琉斯之踵”\n**起点：** 作者首先关注到 LLM 搜索智能体在解决知识密集型任务时的巨大潜力。现有的研究范式（如 ReAct）已经证明了通过迭代调用搜索工具可以弥补模型的知识盲区。\n**发现盲区：** 尽管学术界在优化智能体的“推理范式”（如思维链设计、强化学习策略）上投入巨大，但作者敏锐地发现了一个被忽视的环节：**中间搜索查询的质量**。\n**直觉判断：** 智能体的推理能力再强，如果它向搜索引擎发出的指令是模糊、错误或冗余的，那么检索回来的信息就是无用的，后续的推理自然建立在沙堆之上。\n\n### 2. 问题聚焦：从“结果导向”到“过程诊断”\n**现象分析：** 作者通过案例分析（如文中提到的 Kevin McCarthy 演员与政治家混淆的例子）发现，一个微小的查询偏差（如漏掉“Actor”关键词）会导致检索结果完全跑偏，进而导致整个推理轨迹崩塌。\n**现有方法的局限：** 传统的强化学习训练通常只关注“结果奖励”，即最终答案是否正确。这种反馈是稀疏且滞后的。模型即使最终答对了，中间可能走了很多弯路；或者答错了，模型也不知道具体是哪一步的查询出了问题。\n**核心假设：** 如果能对每一个中间步骤的搜索动作进行细粒度的评估和反馈，就能从根本上提升智能体的信息获取能力。\n\n### 3. 机制设计一：构建“过程奖励”作为质检员\n**思考：** 如何定义一个“好的搜索查询”？作者意识到这不能仅靠单一指标。\n**逻辑拆解：**\n*   **新颖性：** 查询不应重复。如果这一步搜到的内容和上一步完全一样，那就是浪费算力。这可以通过规则（文档重叠度）低成本解决。\n*   **有用性：** 查询必须服务于最终答案。这需要语义理解。这一步的意图是否必要？检索结果是否包含预期信息？这需要模型来判断。\n**方案形成：** 结合两者，提出了“双层信用评估”。规则层负责查重（效率），模型层负责语义评估（质量），从而输出具体的分数和文本反馈。\n\n### 4. 机制设计二：引入“查询重写”作为修正员\n**思考：** 仅仅给低分查询打分是不够的，模型需要知道“什么是更好的”。\n**灵感来源：** 类似于人类写作时的修改过程。如果发现某一步查询质量低，为什么不直接修改它，然后基于修改后的查询重新跑一遍后续流程？\n**逻辑闭环：**\n1.  生成原始轨迹。\n2.  利用过程奖励找出“坏”的查询步骤。\n3.  基于反馈文本，利用模型重写该查询。\n4.  从重写点开始重新生成后续步骤，形成一条“修正后的轨迹”。\n**价值：** 这不仅生成了更好的数据，还天然构成了用于对比学习的成对数据。\n\n### 5. 训练策略：三阶段课程学习\n**思考：** 直接让模型学会上述所有能力太难，需要一个循序渐进的过程。\n**逻辑演进：**\n*   **阶段一（模仿）：** 先让模型看“好榜样”。利用过程奖励筛选出那些不仅答案正确、且中间查询也高质量的轨迹进行监督微调（SFT）。这确立了基准能力。\n*   **阶段二（对齐）：** 让模型学会“辨别好坏”。利用上一阶段的“查询重写”机制，构造“原始轨迹”和“修正轨迹”的对比对。通过 DPO（直接偏好优化），让模型偏好那些查询质量更高的路径。\n*   **阶段三（泛化）：** 让模型在实战中“探索”。在强化学习阶段，将查询重写机制融入 Rollout 过程。如果模型生成了坏查询，系统会自动修正并继续，让模型在探索中不断内化“如何写出好查询”的策略。\n\n### 6. 总结：逻辑链的终点\n作者最终构建的 SmartSearch 框架，其核心思想并非单纯堆砌算力或模型参数，而是通过**引入细粒度的过程监督**和**主动的轨迹修正机制**，解决了搜索智能体中“工具使用不当”这一核心瓶颈。从发现中间查询的重要性，到设计评估标准，再到利用修正数据驱动训练，形成了一个完整的逻辑闭环。"
                },
                {
                    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                    "arxiv_id": "2601.04861",
                    "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
                    "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（符合）**：论文的本质是构建和改进 LLM 智能体框架。作者提出了 OI-MAS（Orchestrating Intelligence），这是一个新颖的多智能体系统框架。它并非将智能体作为工具应用于特定垂直领域（如医疗或金融），而是针对多智能体系统本身的“计算效率低下”这一痛点，提出了新的架构解决方案（自适应模型选择策略和状态依赖路由机制）。这属于对智能体框架的改进和优化。 2.  **正面指标（强匹配）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并引入了机制来动态选择智能体角色和模型尺度。 *   **优化机制**：虽然涉及效率，但其核心是通过智能体层面的“路由”和“置信度感知”来实现的，这是提升多智能体系统性能的关键技术，属于构建智能体方法论的一部分。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉或图技术，也不是单纯的基础设施研究。 综上所述，该论文致力于解决多智能体协作中的效率与资源分配问题，是对 LLM 智能体架构的直接改进，符合“构建、改进或演化 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。",
                    "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。"
                },
                {
                    "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search",
                    "arxiv_id": "2601.04703",
                    "authors": "Yiqun Chen, Lingyong Yan, Zixuan Yang, Erhan Zhang, Jiashu Zhao, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao",
                    "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **M-ASK (Multi-Agent Search and Knowledge)**，这是一个新的**多智能体框架**。 *   它旨在解决现有单体智能体在 Agentic Search 任务中的结构性瓶颈，而非仅仅将现有模型应用到一个特定领域。因此，它属于构建和改进 LLM 智能体的方法论研究，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **多智能体**: 论文将智能体解耦为两个互补的角色：`Search Behavior Agents`（负责规划和执行搜索动作）和 `Knowledge Management Agents`（负责聚合、过滤和维护内部上下文）。这体现了智能体间的协作与分工。 *   **智能体能力**: 涉及 `Planning`（规划搜索动作）、`Tool Use`（工具使用）以及 `Memory`（通过知识管理智能体维护内部上下文）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文在 Multi-hop QA（多跳问答）基准测试上进行了实验，但这只是为了验证框架的有效性，其核心在于提出了一种新的多智能体协作架构来优化 Agentic Search，而非单纯的应用型研究。 综上所述，该论文通过构建多智能体系统来改进 LLM 智能体的搜索与推理能力，精准契合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在解决单体 Agentic Search 架构因无约束输出、稀疏奖励和搜索噪声导致的训练不稳定及信用分配难题。针对复杂的多跳问答场景，我们提出了一种 M-ASK 多智能体框架，通过解耦 Search Behavior Agents 和 Knowledge Management Agents，并利用 turn-level dense rewards 实现联合优化。在 HotpotQA、2Wiki、Musique 等多跳问答基准上，通过 F1 Score 验证了其有效性和稳定性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题定位\n**——从“单体智能”的繁荣中看到结构性隐患**\n\n1.  **观察现象**：\n    *   随着 Agentic Search（智能体搜索）的兴起，LLM 被赋予了使用工具（如搜索引擎）进行迭代推理的能力。\n    *   现有的 SOTA 方法（如 Search-r1）大多采用**单体架构**，即由一个单一的 LLM 承担所有任务：规划、搜索、信息整合和最终回答。\n\n2.  **发现问题**：\n    *   作者发现这种“全能型”单体智能体在训练中表现出极不稳定的特征，容易崩溃。\n    *   虽然端到端的强化学习（RL）能提升性能，但在复杂的多跳问答任务中，单体模型往往难以收敛。\n\n### 第二阶段：深度诊断与归因分析\n**——解构“长视界信用分配”的死结**\n\n作者并未止步于“不稳定”的现象，而是深入剖析了单体架构背后的三个互为因果的致命缺陷，构成了一个**“毒性三角”**：\n\n1.  **无约束的输出长度**：\n    *   单体模型倾向于生成冗长的推理链。这不仅增加了计算成本，更重要的是拉长了决策轨迹，使得“最终结果”与“早期决策”之间的距离过远。\n2.  **稀疏的奖励信号**：\n    *   通常只有在任务结束时（回答正确与否）才有反馈。在长达数十步的推理中，模型无法知道哪一步是对的，哪一步是错的。\n3.  **搜索噪声**：\n    *   外部工具（搜索引擎）会引入无关或错误的信息。在单体架构中，这些噪声会直接累积在上下文中，干扰后续推理。\n\n**核心洞察**：\n上述三者共同导致了**长视界信用分配难题**。当一条冗长、充满噪声的轨迹最终只得到一个简单的对错反馈时，优化算法根本无法将奖励归因到具体的某个 Token 或动作上，导致训练梯度发散，模型崩溃。\n\n### 第三阶段：战略假设与范式转移\n**——从“分身乏术”到“术业专攻”**\n\n为了打破上述死结，作者提出了一个核心假设：**如果将“搜索行为”与“知识管理”解耦，就能从根本上隔离噪声并压缩轨迹。**\n\n1.  **角色解耦**：\n    *   不再让一个大脑既负责“找信息”又负责“记信息”。\n    *   **搜索行为代理**：只负责决策（搜什么、何时停）。\n    *   **知识管理代理**：只负责记忆（过滤噪声、更新状态）。\n2.  **预期效果**：\n    *   通过 KMA 的过滤，进入上下文的信息是高密度的，解决了“噪声”问题。\n    *   通过 SBA 的专注，每次生成的动作更短更精准，解决了“输出长度”问题。\n\n### 第四阶段：机制设计与优化逻辑\n**——构建“协作-反馈”闭环**\n\n有了架构假设，作者进一步思考：如何让这两个独立的团队协同工作并稳定训练？\n\n1.  **通信机制设计**：\n    *   设计了一个**结构化知识状态**。这不仅是共享内存，更是两个团队交互的唯一接口。SBA 写入查询，KMA 更新状态，双方通过这个紧凑的状态进行异步协作。\n\n2.  **解决信用分配难题（关键创新）**：\n    *   既然全局奖励太稀疏，那就引入**Turn-level Dense Rewards（轮级密集奖励）**。\n    *   **状态奖励**：给负责输出的 Agent（如 Answer Agent）基于当前答案质量的绝对分数。\n    *   **边际奖励**：这是最精妙的一笔。给负责迭代的 Agent（Search, Summary, Update）分配“边际增益”（$F1_{current} - F1_{previous}$）。\n    *   **逻辑**：如果这一轮搜索和更新让答案变好了，大家都有奖；如果没变好或变差了，大家都要负责。这迫使搜索团队必须找对信息，知识团队必须滤对噪声。\n\n### 第五阶段：验证与理论闭环\n**——从“假设”到“定律”**\n\n最后，作者通过实验验证了这一逻辑链条的完整性：\n*   **消融实验**：移除知识管理模块（KMA）导致性能下降，证明了“过滤噪声”的必要性；移除轮级奖励导致多跳任务崩溃，证明了“密集反馈”在长链路中的关键作用。\n*   **稳定性分析**：对比单体架构 Search-r1 的高崩溃率，M-ASK 实现了 0% 崩溃，证实了“解耦”确实解决了长视界训练的不稳定性。\n\n---\n\n**总结：作者的思考路径**\n从**单体架构的不稳定性**出发 $\\rightarrow$ 诊断出**长视界信用分配**是核心病灶 $\\rightarrow$ 提出通过**角色解耦**来切断噪声与长度的累积 $\\rightarrow$ 利用**边际奖励机制**将全局反馈转化为局部指导 $\\rightarrow$ 最终构建出一个既分工明确又利益绑定的多智能体协作系统。"
                },
                {
                    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
                    "arxiv_id": "2601.04620",
                    "authors": "Di Zhang",
                    "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 **AgentDevel**，这是一个将 LLM 智能体的自我改进过程重构为“发布工程”的新框架。它通过迭代运行智能体、生成质量信号、诊断故障并合成新版本，实现了智能体的自我演化。这完全符合“构建、改进或演化 LLM 智能体”的目标，不属于非演化型应用、非Agentic的基础推理或基础设施研究。 2.  **研究焦点匹配**: 论文直接对应研究焦点的第三点 **“自我演化”**。它提出了一种新的演化机制，即通过外部化的回归感知发布管道来迭代改进智能体，这与传统的内部自我反思或基于种群的搜索不同，强调了演化的稳定性和可审计性。 3.  **正面指标**: 论文明确涉及 `Self-Evolving`、`Self-Improvement`、`Iterative Improvement`、`Agentic AI` 以及 `LLM-based Agents` 等核心范式和能力。 4.  **排除标准**: 论文不涉及安全对齐、多模态技术或特定领域的垂直应用（如医疗、金融），也不属于基础设施优化。 5.  **结论**: 该论文为 LLM 智能体的演化提供了新的方法论视角，属于前沿研究，应予以保留。",
                    "summary2": "本文旨在解决自进化 LLM Agent 改进过程不稳定且难以审计的问题。针对执行密集型任务，我们提出了一种名为 AgentDevel 的发布工程管道，该管道包含实现无关 Critic、可执行诊断及翻转中心 Gating 机制。我们在 SWE-bench、WebArena 和 StableToolBench 上通过 Resolved rate、Success rate 及回归率等指标验证了其有效性，实现了性能显著提升且回归更少的稳定改进。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
                    "arxiv_id": "2601.04544",
                    "authors": "Jiuzhou Zhao, Chunrong Chen, Chenqi Qiao, Lebin Zheng, Minqi Han, Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang",
                    "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **TCAndon-Router (TCAR)**，这是一个用于多智能体协作的自适应推理路由器。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统（MAS）中的关键组件——路由器——进行了构建和改进。 *   论文设计了一个“协作执行流水线”，其中包括一个专门的“Refining Agent”来聚合和优化响应，这属于构建新的多智能体交互机制。 2.  **正面指标（第二步）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体特性**：涉及 `Collaboration`（协作执行流水线）、`Communication`（通过路由器进行任务分配）以及 `Agent Society`（支持动态智能体接入）。 *   **智能体能力**：利用 `Reasoning`（生成自然语言推理链）来辅助路由决策。 3.  **排除标准（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **综合结论**： 该论文致力于解决多智能体系统中的任务分配与协作效率问题，提出了新的路由机制和协作框架，直接对应研究课题中的“多智能体”方向，因此应当保留。",
                    "summary2": "本文旨在解决多智能体系统中现有路由策略难以动态接入新智能体及处理能力重叠导致的路由冲突问题。针对企业级应用中模糊或跨域的查询场景，我们提出了一种名为TCAndon-Router (TCAR) 的自适应推理路由器，通过生成自然语言推理链选择候选智能体，并利用Refining Agent聚合多智能体响应。在CLINC150、HWU64等公共数据集及腾讯云私有数据集上，通过Accuracy和F1等指标验证了其有效性。",
                    "summary_translation": "多智能体系统已成为构建高性能智能应用的有力范式。在这些系统中，负责确定哪些专家智能体应处理给定查询的路由器在整体性能中起着至关重要的作用。现有的路由策略通常分为两类：性能路由，它在不同规模的模型之间平衡延迟和成本；以及任务路由，它将查询分配给特定领域的专家以提高准确性。在现实世界的应用程序中，任务路由更为适用；然而，大多数现有方法依赖于静态单标签决策，这引入了两个主要局限性：随着业务领域的扩展，难以无缝集成新的智能体；以及由智能体能力重叠引起的路由冲突，最终降低了准确性和鲁棒性。\n\n为了解决这些挑战，我们提出了 TCAndon-Router (TCAR)：一种用于多智能体协作的自适应推理路由器。与传统路由器不同，TCAR 支持动态智能体接入，并在预测能够处理查询的候选智能体集合之前，首先生成一条自然语言推理链。此外，我们设计了一个协作执行管道，其中选定的智能体独立生成响应，随后由专门的精炼智能体将这些响应聚合并精炼为单一的高质量响应。\n\n在公共数据集和真实企业数据上的实验表明，TCAR 显著提高了路由准确性，减少了路由冲突，并在模糊场景中保持了鲁棒性。我们已在 https://huggingface.co/tencent/TCAndon-Router 发布了 TCAR，以支持未来关于可解释和协作多智能体路由的研究。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery",
                    "arxiv_id": "2601.04500",
                    "authors": "Yifei Gao, Jiang Wu, Xiaoyi Chen, Yifan Yang, Zhe Cui, Tianyi Ma, Jiaming Zhang, Jitao Sang",
                    "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **GUITester**，这是一个**多智能体框架**。它不仅仅是将现有的LLM智能体作为工具应用，而是构建了一个新的架构来解决特定问题。 *   该框架包含两个关键模块：**规划-执行模块 (PEM)** 和 **层次化反思模块 (HRM)**。这直接对应了研究焦点中的 **\"Agentic\" (规划)** 和 **\"自我演化\" (自我反思 Self-Reflection / 自我修正 Self-Correction)** 方向。 2.  **涉及多智能体与自我反思机制 (第二步 & 第四步)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   论文提出的 **HRM (Hierarchical Reflection Module)** 通过分析交互历史来解决归因歧义，这是一种典型的 **自我反思** 和 **自我修正** 机制。虽然它应用于GUI测试，但其核心在于改进智能体处理反馈和自我完善的能力，符合“自我演化”中通过反思进行迭代的定义。 3.  **特殊情况的正确处理 (第三步 & 第四步)**: *   **应用 vs. 方法论**: 虽然论文的应用场景是GUI测试（特定领域），但其核心在于提出了解决“目标导向掩蔽”和“执行偏差归因”的**新方法论**（即解耦导航与验证的框架），而非单纯的应用。 *   **多模态**: 论文虽然使用了多模态大模型 (MLLM) 来感知GUI界面，但视觉仅作为智能体感知环境的工具，研究的核心并非视觉模型的改进，而是智能体的决策与反思框架，因此符合“除非它们被用作智能体感知环境的工具”的例外条款。 综上所述，该论文在构建多智能体框架、引入规划机制以及实现自我反思方面做出了实质性贡献，完全符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有GUI智能体在探索性测试中难以自主发现缺陷的问题。针对目标导向掩蔽和执行偏差归因两大挑战，我们提出了一种名为GUITester的多智能体框架，通过规划执行模块（PEM）主动探测缺陷，并利用分层反思模块（HRM）解决归因歧义。我们在首个交互式benchmark GUITestBench上通过F1-score（Pass@3）验证了其有效性，结果显示GUITester达到48.90%，显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements",
                    "arxiv_id": "2601.04235",
                    "authors": "Hong Su",
                    "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种“主动获取反馈”的模型。这不仅仅是应用现有智能体解决具体问题，而是构建了一种新的智能体交互与评估机制，使智能体能够在没有预设测量的情况下，通过主动与环境交互来获取反馈。这属于构建和改进LLM智能体方法论的研究。 2.  **高度匹配核心关注点 (第二步)**: *   **单智能体**: 论文详细探讨了智能体如何进行“自主行动评估”、“与环境交互”以及“自主规划和调整行动”，这直接对应了Agentic AI中的规划、工具使用和环境交互能力。 *   **自我演化**: 论文引入了“自触发机制”，允许智能体根据内部目标（如准确性、效率）自主调整行动。这种基于反馈的自我调整和迭代优化，正是“自我演化”和“自我修正”的关键体现。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉核心或图技术等排除项。 *   它不是非演化型应用，而是提出了一种通用的智能体能力增强机制。 综上所述，该论文通过解决智能体如何在开放环境中自主获取反馈并自我完善的问题，为Agentic AI的演化和能力提升提供了新的方法论，因此应当保留。",
                    "summary2": "本文旨在解决AI智能体在开放环境中依赖预定义测量评估行动的问题。针对无预设测量的动态环境，我们提出了一种Actively Feedback Getting model，利用action-induced environmental differences进行反馈检测与主动干预。在文本场景和模拟环境上，通过语义相似度和LLM查询数量验证了其有效性。",
                    "summary_translation": "从环境中获取可靠反馈是 Intelligent agents (智能体) 评估其行为正确性并积累可复用知识的基本能力。然而，大多数现有方法依赖于 predefined measurements (预定义度量) 或 fixed reward signals (固定奖励信号)，这限制了它们在开放式和动态环境中的适用性，因为在这些环境中，新的行为可能需要以前未知的反馈形式。为了解决这些局限性，本文提出了一种 Actively Feedback Getting model (主动反馈获取模型)，其中 AI agent 主动与环境交互以发现、筛选和验证反馈，而无需依赖 predefined measurements (预定义度量)。该方法不假设显式的反馈定义，而是利用 action-induced environmental differences (动作引起的环境差异) 来识别未预先指定的目标反馈，这是基于动作不可避免地在环境中产生可测量变化这一观察。此外，本文引入了一种由提高准确度、精确度和效率等 internal objectives (内部目标) 驱动的 self-triggering mechanism (自触发机制)，以自主规划和调整行为，从而在没有 external commands (外部指令) 的情况下实现更快、更聚焦的反馈获取。实验结果表明，所提出的主动方法显著提高了 factor identification (因子识别) 的效率和鲁棒性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation",
                    "arxiv_id": "2601.04516",
                    "authors": "Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu",
                    "summary": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Multi-Agent (多智能体)** 方向的前沿研究。具体判断依据如下： 1.  **核心判断 (符合)**: *   论文的核心贡献是提出了一种名为 **LinguaGame** 的新范式，这是一种基于博弈论的多智能体对话生成框架。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统的 **交互过程** 本身进行了改进，旨在通过建模意图和策略来提升智能体间的通信效率。这符合“构建、改进 LLM智能体”的核心目标。 2.  **研究焦点匹配 (多智能体)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   它的核心关注点是多智能体之间的 **Communication (通信)** 和 **Interaction (交互)**。 *   论文涉及智能体如何推断彼此的意图和策略，这属于多智能体协作与博弈中的高级认知能力。 3.  **排除标准检查 (通过)**: *   **非单纯应用**: 虽然论文在模拟法庭和辩论中进行了评估，但其核心贡献是通用的交互机制，而非解决特定领域的业务问题。 *   **非安全/对齐/多模态**: 论文不涉及安全对齐、视觉或多模态内容，纯粹关注语言层面的智能体交互逻辑。 4.  **正面指标**: *   包含核心范式关键词：`Multi-Agent Systems (MAS)`, `Game-Theoretic`。 *   包含多智能体能力关键词：`Communication`, `Negotiation` (隐含在辩论场景中), `Agent Society` (隐含在多智能体环境中)。 综上所述，该论文提出了一种改进多智能体通信机制的新方法，属于 Agentic AI 中多智能体交互的重要进展，因此予以保留。",
                    "summary2": "本文旨在解决基于LLM的Multi-Agent Systems中沟通效率低下的问题。针对模拟法庭和辩论场景，我们提出了一种名为LinguaGame的基于语言学的博弈论范式。该方法将对话建模为关于交际意图和策略的Signalling Game，并利用无训练的均衡近似算法在推理时优化决策。在模拟法庭和辩论数据集上，通过人工专家评估（涵盖清晰度、简洁性、论证和策略等指标），验证了该方法能显著提升对话质量和沟通效率。",
                    "summary_translation": "大语言模型推动了多智能体系统的发展，在该系统中，智能体通过自然语言进行交互，以解决复杂任务或模拟多方对话。现有关于基于大语言模型的多智能体系统的研究主要集中在架构设计方面，例如角色分配和工作流编排。与之不同，本文聚焦于交互过程本身，旨在通过帮助智能体利用语言更有效地传达其预期含义，从而提升通信效率。为此，我们提出了 LinguaGame，这是一种基于语言学的博弈论范式，用于多智能体对话生成。我们的方法将对话建模为基于交际意图和策略的信号博弈，并采用一种无训练的均衡近似算法进行求解，以实现推理时的决策调整。与先前的博弈论多智能体系统不同，后者的博弈设计往往与特定任务目标紧密耦合，而我们的框架依赖于基于语言学的推理，仅与特定任务存在最小程度的耦合。具体而言，该框架将对话视为一种意图性和策略性的通信，要求智能体推断他人旨在实现的目标（意图）以及其追求这些目标的方式（策略）。我们在模拟法庭庭审和辩论场景中对该框架进行了评估，人类专家的评估结果显示通信效率得到了显著提升。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-07": [
        {
            "name": "Artificial Intelligence",
            "count": 6,
            "papers": [
                {
                    "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
                    "arxiv_id": "2601.04035",
                    "authors": "Yilin Cao, Yufeng Zhong, Zhixiong Zeng, Liming Zheng, Jing Huang, Haibo Qiu, Peng Shi, Wenji Mao, Wan Guanglu",
                    "summary": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献在于提出了 **MobileDreamer**，这是一个基于世界模型的前瞻框架，旨在装备和增强移动 GUI 智能体。它不仅仅是将现有的智能体框架应用到一个新领域，而是针对现有 GUI 智能体“反应式”的局限性，提出了一种新的架构（包含文本草图世界模型和推演想象策略），以提升智能体的决策能力。这属于构建和改进 LLM 智能体的范畴。 2.  **正面指标（符合）**： *   **Agentic AI**: 论文明确聚焦于 GUI Agent 的构建。 *   **Planning**: 论文的核心在于通过世界模型预测行动结果，从而支持智能体进行更好的决策制定和行动选择，这属于智能体规划能力的增强。 3.  **排除标准（未触发）**： *   **多模态与视觉**: 虽然论文涉及处理屏幕图像和生成草图，但这属于“智能体感知环境的工具”。论文的核心贡献不是提出一种新的视觉算法或图像生成模型，而是利用视觉转换来构建辅助智能体规划的“世界模型”。因此，符合“除非它们被用作智能体感知环境的工具”这一例外条款。 4.  **特殊与模糊情况（符合）**： *   **推理/规划**: 论文通过引入世界模型，让智能体能够进行“前瞻”和“想象”，这属于智能体在复杂任务中进行多步推理和规划的高级形式，符合保留条件。 综上所述，该论文通过引入世界模型机制显著增强了单智能体的规划与决策能力，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决移动GUI代理在长视界任务中缺乏前瞻性规划的问题。针对移动设备屏幕交互场景，我们提出了一种基于文本草图世界模型的高效前瞻框架MobileDreamer。该方法通过文本草图世界模型预测未来状态，并利用推演想象策略构建预测树以优化动作选择。在Android World基准测试上，通过任务成功率（SR）验证了其有效性，显著提升了代理的性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
                    "arxiv_id": "2601.03555",
                    "authors": "Yuxuan Jiang, Francis Ferraro",
                    "summary": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance. Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions. Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (第一步)**: *   该论文的核心贡献是提出了 **SCRIBE**，这是一个用于训练**工具增强型智能体** 的强化学习框架。 *   论文明确指出其目标是解决训练可靠智能体时的“信用分配”难题，并致力于提升智能体的“高层规划”和“低层执行”能力。 *   这完全符合“构建、改进或演化 LLM智能体”的核心目标，属于 Agentic AI 的范畴，因此应予以保留。 2.  **正面指标匹配 (第二步)**: *   **智能体能力**: 论文重点涉及 `Tool Use` (工具使用) 和 `Planning` (规划)，特别是区分了高层规划与低层执行。 *   **演化机制**: 摘要中明确提到了“co-evolution across abstraction levels”（跨抽象层的协同演化）以及“emergence of effective high-level planning behaviors”（有效高层规划行为的涌现），这与研究焦点中的“自我演化”高度契合。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文虽然涉及推理，但它是通过强化学习框架来训练智能体在多步任务中进行规划和工具使用，属于智能体的架构与训练方法，而非单纯提升LLM基础Token预测能力的数学或逻辑微调，因此符合保留条件。 **结论**: 该论文提出了一种新的训练框架来改进LLM智能体的工具使用和规划能力，并探讨了技能与规划之间的协同演化机制，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决工具增强模型在多步推理中因信用分配困难导致的训练不可靠问题。针对多步推理和复杂工具交互场景，我们提出了一种SCRIBE框架，利用Skill Prototypes库进行中间层抽象，将开放式LLM评估转化为基于原型的约束验证任务。并在MATH、AIME25和BFCL V4基准上通过准确率和成功率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
                    "arxiv_id": "2601.04060",
                    "authors": "Jinwei Su, Qizhen Lan, Zeyu Wang, Yinghui Xia, Hairu Wen, Yiqun Duan, Xi Xiao, Tianyu Shi, Yang Jingsong, Lewei He",
                    "summary": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，核心依据如下： 1.  **核心贡献符合 Agentic AI 定义 (第一步 & 第二步)**: *   论文明确提出了 \"ComfySearch, an **agentic framework**\"（一个智能体框架）。这不仅仅是将现有的LLM作为工具简单应用，而是构建了一个新的智能体架构来解决特定问题。 *   该框架的核心能力涉及 **Autonomous Exploration**（自主探索）和 **Reasoning**（推理），这直接对应了筛选标准中的“单智能体”方向，特别是 **Planning**（规划）和 **Tool Use**（工具使用，即操作ComfyUI组件）。 2.  **属于智能体规划与构建范畴 (第四步)**: *   论文解决的问题是如何在复杂的图约束下构建长视距的工作流。这属于智能体如何在复杂任务中进行多步规划和决策的研究范畴，而非单纯的LLM基础推理能力提升（如数学或逻辑题）。它关注的是智能体如何通过 \"validation-guided workflow construction\"（验证引导的工作流构建）来完成任务，这是一种典型的 Agentic 行为模式。 3.  **非简单的应用型论文 (第一步 & 第四步)**: *   虽然论文的应用场景是 ComfyUI（一个视觉生成平台），但论文的核心贡献在于**提出了一种能够自主探索组件空间并生成功能性工作流的智能体机制**，而不是仅仅展示LLM在生成图片上的效果。根据第四步的规则，只要核心是提出新的智能体机制（在此处是探索和构建机制），即使应用在特定领域，也应保留。 4.  **排除标准检查 (第三步)**: *   虽然涉及视觉内容生成，但论文的研究焦点不在于改进视觉模型本身，而在于控制生成流程的智能体框架，因此不属于被排除的“多模态与视觉”核心研究。 *   不涉及安全、对齐或基础设施问题。 综上所述，该论文提出了一种新的智能体框架来解决复杂的规划和构建问题，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决ComfyUI工作流生成中因组件复杂和图约束严格导致的低通过率和质量问题。针对ComfyUI工作流构建场景，我们提出了一种名为ComfySearch的智能体框架，采用Reasoning-as-action范式，结合State-aware validation和In-place repair确保结构正确性，并引入Entropy-adaptive branching处理长视距不确定性。我们在ComfyBench和GenEval上通过%Pass、%Resolve及GenEval分数验证了其有效性。",
                    "summary_translation": "AI生成内容已从单体模型演进至模块化工作流，特别是在 ComfyUI 等平台上，使用户能够定制复杂的创意流水线。然而，ComfyUI 中组件数量庞大，且在严格的图约束下难以保持长视距结构一致性，这往往导致通过率低下且生成的工作流质量有限。为解决上述局限性，我们提出了 ComfySearch，这是一个智能体框架，能够通过验证引导的工作流构建，有效探索组件空间并生成功能完备的 ComfyUI 流水线。实验表明，在复杂和创意任务中，ComfySearch 的性能显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决率以及更强的泛化能力。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Architecting Agentic Communities using Design Patterns",
                    "arxiv_id": "2601.03624",
                    "authors": "Zoran Milosevic, Fethi Rabhi",
                    "summary": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献在于提出了一种用于构建“Agentic Communities”（智能体社区）的架构方法和设计模式。这不仅仅是将LLM作为工具应用，而是构建了一个包含LLM智能体、Agentic AI实体和人类的复杂多智能体系统框架。它定义了智能体社区的组织结构、角色和协议，属于构建和改进LLM智能体系统的方法论。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确涉及 `Agentic AI` 和 `Multi-Agent Systems (MAS)`，特别是提出了“Agentic Communities”这一概念。 *   **多智能体能力**：摘要中详细讨论了智能体之间以及智能体与人类之间的 `Coordination`（协调）、`Communication`（通信）、`Negotiation`（谈判）以及 `Governance`（治理）机制。这些都是多智能体系统研究中的核心议题。 3.  **排除标准检查（第三步）**： *   虽然论文提到了“法律和伦理规则”以及“问责机制”，但其主要贡献是**架构框架**，而非单纯的安全算法或对齐技术研究。 *   虽然使用了“临床试验匹配”作为案例研究，但这仅用于验证框架的有效性，论文本质并非解决医疗领域的特定问题，而是提供通用的系统架构指导。 综上所述，该论文为构建复杂的多智能体协作系统提供了新的架构视角和设计模式，直接契合研究课题中关于“多智能体”的焦点，因此予以保留。",
                    "summary2": "本文旨在为构建生产级Agentic AI系统提供系统的架构指导。针对多智能体协调与治理的挑战，我们提出了一种基于ODP Enterprise Language (ODP-EL) 形式主义的设计模式框架，涵盖LLM Agents、Agentic AI及Agentic Communities三类模式。我们在临床试验匹配案例中验证了其有效性，实现了可验证的治理属性与问责机制。",
                    "summary_translation": "大语言模型 (Large Language Models, LLM) 及随后的 Agentic AI（智能体 AI）技术的快速演进，为构建复杂的、生产级系统提出了系统化架构指导的需求。本文提出了一种架构此类系统的方法，该方法采用了源于企业分布式系统标准、形式化方法和行业实践的设计模式。我们将这些模式划分为三个层级：LLM Agents（大语言模型智能体，即特定任务的自动化）、Agentic AI（智能体 AI，即自适应的目标寻求者）以及 Agentic Communities（智能体社区，即 AI 智能体和人类参与者通过正式角色、协议和治理结构进行协调的组织框架）。我们重点关注 Agentic Communities——这是一种包含 LLM Agents、Agentic AI 实体和人类的协调框架——其与企业及工业应用最为相关。借鉴分布式系统中既定的协调原则，我们将这些模式建立在一个形式框架之上，该框架明确了协作协议，规定了 AI 智能体和人类如何在受治理的生态系统中承担特定角色。该方法不仅提供了实用指导，还具备形式化验证能力，能够通过问责机制来表达组织、法律和伦理规则，从而确保对智能体间通信、谈判和意图建模进行可操作且可验证的治理。我们通过一项临床试验匹配的案例研究验证了该框架。我们的目标是为从业者提供切实可行的指导，同时保持企业部署在动态多智能体生态系统中所必需的形式严谨性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization",
                    "arxiv_id": "2601.03359",
                    "authors": "Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner",
                    "summary": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种“新颖的多智能体工作流”，用于优化提示词指令。这属于构建和改进 LLM 智能体系统的方法论研究，而非将现有智能体简单应用到特定垂直领域（如医疗、金融）。 2.  **正面指标匹配**： *   **多智能体**：标题和摘要中明确提到了“Multi-Agentic Workflow”，涉及多个智能体协同工作。 *   **自我演化/自我完善**：论文描述了使用“定量分数作为反馈来迭代地重写和改进”提示词，这符合自我演化中的“迭代改进”和“自我修正”机制。 *   **智能体能力**：该工作流体现了智能体的规划（解耦任务描述与约束）和工具使用（利用评估分数）能力。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然目标是“指令遵循”，但其手段是构建一个多智能体系统，这属于 Agentic AI 的范畴，而非单纯的非 Agentic 推理或数据集构建。 综上所述，该论文通过构建多智能体协作框架来实现任务的自我迭代优化，精准契合我对“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决 LLMs 难以严格遵守输出约束的问题。针对 InfoBench 数据集，我们提出了一种 evaluation-driven multi-agentic workflow，将任务描述与约束解耦，并利用定量反馈迭代优化约束。我们在 Llama 3.1 8B 和 Mixtral-8x 7B 上通过 compliance scores 验证了其有效性，显著提升了模型的指令遵循能力。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 虽然常能生成实质相关的内容，但往往未能遵守形式约束 (formal constraints)，从而导致输出结果在概念上正确，但在程序上存在缺陷。传统的提示词优化 (prompt refinement) 方法主要侧重于改写大语言模型需执行的主要任务描述，而忽视了那些作为响应验收标准 (acceptance criteria) 的细粒度约束 (granular constraints)。我们提出了一种新颖的多智能体工作流 (multi-agentic workflow)，该工作流将主要任务描述的优化与其约束解耦，并利用定量分数 (quantitative scores) 作为反馈，对二者进行迭代重写和改进。我们的评估表明，该方法生成的修订提示词能够使 Llama 3.1 8B 和 Mixtral-8x 7B 等模型产生显著更高的合规分数 (compliance scores)。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Evolving Programmatic Skill Networks",
                    "arxiv_id": "2601.03509",
                    "authors": "Haochen Shi, Xingdi Yuan, Bang Liu",
                    "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合“自我演化”**: 论文提出了 Programmatic Skill Network (PSN) 框架，其核心在于智能体如何通过经验构建、细化和重用技能库。摘要中明确指出技能库是“evolves through experience”（通过经验演化），并包含“progressive optimization”（渐进式优化）和“structural refactoring”（结构重构）等机制，这直接对应了筛选标准中的“Self-Evolving”、“Self-Improvement”和“Iterative Improvement”。 2.  **具备核心智能体能力**: 论文详细描述了智能体的自我反思和自我修正能力。具体而言，它利用 LLM 实例化了“REFLECT”机制进行结构化故障定位，以及“成熟度感知更新门控”来平衡技能的稳定性与可塑性。这符合筛选标准中的“Self-Correction”、“Self-Reflection”和“Memory”（技能库作为长期记忆）。 3.  **符合特殊处理规则**: 尽管论文是在 MineDojo 和 Crafter（具身/游戏环境）中进行实验，但根据第四步的规则，只要论文的核心贡献是提出一种新的“自我演化”机制（即 PSN 框架），即使应用在特定领域，也应予以保留。该论文并非单纯将 LLM 应用于游戏，而是提出了一套通用的技能演化算法。 综上所述，该论文不仅涉及 LLM 智能体的构建，更深入探讨了智能体如何通过反思和反馈实现自我演化和技能积累，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决开放具身环境中智能体持续获取、细化和重用技能的问题。针对开放任务流，我们提出了一种Programmatic Skill Network (PSN)框架，通过REFLECT机制进行结构化故障定位、成熟度感知更新门控及规范化结构重构，实现技能网络的持续演化。并在MineDojo和Crafter环境上通过技术树掌握迭代次数、累积奖励及技能保持率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
                    "arxiv_id": "2601.03785",
                    "authors": "Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang",
                    "summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为 \"Membox\" 的分层记忆架构，旨在解决 LLM 智能体在长程记忆中保持主题连续性的问题。这属于对 LLM 智能体核心组件（记忆系统）的构建与改进，而非将智能体作为工具应用到特定领域（非演化型应用），也非基础设施或基础模型推理能力的提升。 2.  **正面指标匹配（第二步）：** *   **核心范式：** 论文明确聚焦于 `LLM-based Agents`。 *   **智能体能力：** 论文的核心贡献点在于 `Memory`（记忆）。它通过引入 \"Topic Loom\" 和 \"Trace Weaver\" 等机制，优化了智能体存储和检索对话历史的能力，这直接对应了筛选标准中单智能体方向下的“记忆”子方向。 3.  **排除标准检查（第三步）：** 论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此未触犯任何排除规则。 4.  **总结：** 该论文致力于通过改进记忆机制来增强 LLM 智能体的连贯性和效率，属于单智能体研究中对基础能力（记忆）的深化，完全符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有LLM Agent记忆系统因碎片化存储导致的话题连续性丢失问题。针对长对话场景，我们提出了一种名为Membox的分层记忆架构，利用Topic Loom将连续同话题对话分组为“记忆盒”，并通过Trace Weaver链接宏观话题以恢复事件时间线。我们在LoCoMo数据集上通过F1和BLEU指标验证了其有效性，在Temporal Reasoning任务上实现了高达68%的F1提升，且显著降低了上下文Token消耗。",
                    "summary_translation": "人机对话通常表现出话题连续性——一种通过时间上相邻的交互演变的稳定主题框架——然而大多数大语言模型 (LLM) 智能体记忆系统未能保留这一特性。现有设计遵循一种碎片化-补偿范式：它们首先将对话流分解为孤立的话语进行存储，然后试图通过基于嵌入的检索来重建连贯性。这一过程不可逆地破坏了叙事和因果流，同时导致检索偏向于词汇相似性。我们介绍了 membox，这是一种以 Topic Loom (话题编织器) 为中心的分层记忆架构，它以滑动窗口的方式持续监控对话，在存储时将连续的同一话题轮次分组为连贯的“记忆盒”。封装后的记忆盒随后由 Trace Weaver (轨迹编织器) 链接成长程事件时间轴轨迹，从而跨越对话间隔恢复宏观话题的重现。在 LoCoMo 数据集上的实验表明，Membox 在时序推理任务上实现了高达 68% 的 F1 值提升，优于竞争性基线模型（如 Mem0, A-MEM）。值得注意的是，Membox 在仅使用现有方法所需的一小部分上下文 token 的情况下取得了这些提升，突显了其在效率与有效性之间取得了卓越的平衡。通过显式建模话题连续性，Membox 为增强 LLM 智能体的连贯性和效率提供了一种受认知启发的机制。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
                    "arxiv_id": "2601.03641",
                    "authors": "Zheng Wu, Xingyu Lou, Xinbei Ma, Yansi Li, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang",
                    "summary": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心论文。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 **Agent-Dice**，这是一个专门用于解决 **LLM智能体持续学习** 问题的参数融合框架。它旨在解决智能体在动态环境中学习新任务时遇到的“灾难性遗忘”和“稳定性-可塑性困境”。这直接对应了我研究目标中的“自我演化”，即智能体通过经验、反思或环境反馈进行自我完善和迭代。这不是将智能体作为工具的应用，而是对智能体底层学习机制的改进。 2.  **符合正面指标 (第二步)**: *   **核心范式**: 论文明确关注 `LLM-based Agents`。 *   **演化机制**: 论文的核心主题是 `Continual Learning`（持续学习），这是 `Self-Evolving`（自我演化）的重要组成部分。它涉及 `Knowledge Updates`（知识更新）和 `Iterative Improvement`（迭代改进）。 *   **智能体能力**: 论文在 `GUI agents` 和 `tool-use agents` 领域进行了验证，体现了智能体与环境的交互能力。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐或水印问题。 *   虽然提到了 GUI agents（可能涉及视觉），但视觉仅作为智能体感知环境的工具，而非研究的核心（核心是参数融合和持续学习算法），因此不违反多模态排除规则。 *   不涉及图技术。 4.  **特殊情况处理 (第四步)**: 论文提出了一种新的“自我演化”机制（通过几何共识和曲率加权实现持续学习），即使它在特定的 GUI 或工具使用领域进行了实验，根据规则“如果论文的核心是提出一种新的‘自我演化’机制……也应该保留”，这篇论文应当被保留。 综上所述，Agent-Dice 通过改进智能体的知识更新机制，实现了智能体在连续任务中的自我演化，精准契合“LLM智能体及其演化”这一课题。",
                    "summary2": "本文旨在解决LLM智能体持续学习中的稳定性-可塑性困境。针对GUI和工具使用智能体场景，我们提出了一种基于方向一致性评估的参数融合框架Agent-Dice，通过几何一致性过滤和基于曲率的重要性加权解耦知识更新。在AITZ、AndroidControl、GUI-Odyssey和ToolACE数据集上，通过平均Z-score等指标验证了其有效性。",
                    "summary_translation": "基于 Large Language Model (LLM) 的 agents（智能体）通过与动态环境交互，显著扩展了 LLMs 的效用。然而，使 agents 能够在不发生 catastrophic forgetting（灾难性遗忘）的情况下持续学习新任务，仍然是一个关键挑战，即所谓的 stability-plasticity dilemma（稳定性-可塑性困境）。在这项工作中，我们认为这一困境根本源于未能明确区分跨任务共享的 common knowledge（通用知识）与由 task-specific interference（任务特定干扰）引入的 conflicting knowledge（冲突知识）。为解决这一问题，我们提出了 Agent-Dice，这是一种基于 directional consensus evaluation（方向一致性评估）的 parameter fusion framework（参数融合框架）。具体而言，Agent-Dice 通过两阶段过程解耦知识更新：利用 geometric consensus filtering（几何一致性过滤）剪枝 conflicting gradients（冲突梯度），并利用 curvature-based importance weighting（基于曲率的重要性加权）增强 shared semantics（共享语义）。我们提供了严格的理论分析，确立了所提出的 fusion scheme（融合方案）的有效性，并深入探讨了 stability-plasticity dilemma（稳定性-可塑性困境）的起源。在 GUI agents（图形用户界面智能体）和 tool-use agent（工具使用智能体）领域的大量实验表明，Agent-Dice 展现出了卓越的 continual learning performance（持续学习性能），且仅需极少的 computational overhead（计算开销）和 parameter updates（参数更新）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing",
                    "arxiv_id": "2601.03261",
                    "authors": "Shuo Lu, Yinuo Xu, Jianjie Cheng, Lingxiao He, Meng Wang, Jian Liang",
                    "summary": "Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 论文明确以 \"Deep Research agents\"（深度研究智能体）为研究对象，提出了 DeepResearch-Slice 这一神经符号框架。这属于构建和改进 LLM 智能体的方法论，符合“单智能体”的研究范畴。 2.  **正面指标**: 论文涉及智能体的核心能力，特别是工具使用后的信息处理。它解决了智能体在嘈杂环境中利用检索证据的瓶颈，改进了智能体的推理鲁棒性，属于对智能体内部机制的优化。 3.  **排除标准**: 论文不涉及安全、对齐、多模态或图技术；也不是针对特定垂直领域（如生物、金融）的单纯应用，而是提出了通用的改进框架。 4.  **特殊处理**: 虽然涉及推理，但这是在智能体框架下对“检索-利用”过程的改进，而非单纯提升模型的基础数学或逻辑预测能力，因此不属于“非Agentic的推理”。",
                    "summary2": "本文旨在解决Deep Research中模型因上下文盲区无法利用已检索证据的Retrieval-Utilization Gap问题。针对噪声环境下的检索文档，我们提出了一种名为DeepResearch-Slice的神经符号框架，通过预测精确的span索引执行显式文本切片和硬过滤。在六个基准测试上，通过任务准确率验证了其有效性，在冻结骨干网络上实现了73%的相对性能提升。",
                    "summary_translation": "Deep Research agents (深度研究智能体) 主要致力于优化 search policies (搜索策略)，以最大化 retrieval probability (检索概率)。然而，我们发现了一个关键瓶颈：retrieval-utilization gap (检索-利用差距)，即由于 noisy environments (噪声环境) 中的 context blindness (上下文盲区)，模型即使在成功检索到 gold evidence (黄金证据) 后，仍无法有效利用这些证据。为弥合这一差距，我们提出了 DeepResearch-Slice，这是一个简单而有效的 neuro-symbolic framework (神经符号框架)。与 implicit attention (隐式注意力) 机制不同，我们的方法通过预测精确的 span indices (片段索引)，在 reasoning (推理) 过程之前执行 deterministic hard filter (确定性硬过滤)。在六个 benchmarks (基准测试) 上进行的广泛评估表明，该方法带来了显著的 robustness gains (鲁棒性提升)。将该方法应用于 frozen backbones (冻结骨干网络) 时，实现了 73% 的相对性能提升（从 19.1% 提升至 33.0%），有效缓解了噪声干扰，且无需对 reasoning model (推理模型) 进行 parameter updates (参数更新)。这些结果凸显了在 open-ended research (开放式研究) 中引入 explicit grounding mechanisms (显式定位机制) 的必要性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
                    "arxiv_id": "2601.03905",
                    "authors": "Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, Heng Ji, Heng Ji",
                    "summary": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献在于实证研究LLM智能体如何利用“世界模型”作为外部工具来进行“前瞻”规划。这直接属于“单智能体”研究范畴中的“工具使用”和“规划”能力。虽然论文主要揭示了当前智能体的局限性（即未能有效利用），但其目的是为了指导未来构建具有更强认知能力的智能体系统，属于对智能体构建和改进机制的探索，而非单纯的应用。 2.  **正面指标匹配（第二步）**：论文高度符合核心关注点。 *   **Agentic AI**: 明确以“Agents”为研究对象。 *   **Tool Use / Tool Augmentation**: 研究的核心是智能体如何将生成式世界模型作为“外部模拟器”工具来使用。 *   **Planning**: 论文聚焦于“anticipating future states”（前瞻）和“foresight”，这是智能体规划能力的高级形式。 3.  **排除标准与特殊情况处理（第三、四步）**： *   **多模态问题**: 尽管摘要提到了“vision-language models”和“visual question answering”，但视觉内容在此处是智能体感知的环境或工具（世界模型）的一部分，而非论文的研究核心。论文的核心在于智能体与该工具的交互机制，而非改进视觉模型本身，因此符合“作为工具使用”的例外情况。 *   **推理/规划**: 论文讨论的是智能体如何通过模拟进行多步推理和决策，属于Agentic层面的规划，而非单纯的LLM内部逻辑推理优化。 综上所述，该论文深入探讨了单智能体在工具使用和前瞻规划方面的关键问题，对构建和改进LLM智能体具有直接的指导意义，完全符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在探究当前智能体利用世界模型进行前瞻认知的能力。针对多样化的智能体决策和视觉问答任务，我们提出了一种“世界模型即工具”的评估框架，并在FrozenLake、Navigation等任务及VQA基准上通过成功率、准确率和调用率验证了其有效性。研究发现当前智能体很少调用世界模型，且强制使用会降低性能，核心瓶颈在于前瞻治理机制。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "LLM-Enabled Multi-Agent Systems: Empirical Evaluation and Insights into Emerging Design Patterns & Paradigms",
                    "arxiv_id": "2601.03328",
                    "authors": "Harri Renney, Maxim N Nethercott, Nathan Renney, Peter Hayes",
                    "summary": "This paper formalises the literature on emerging design patterns and paradigms for Large Language Model (LLM)-enabled multi-agent systems (MAS), evaluating their practical utility across various domains. We define key architectural components, including agent orchestration, communication mechanisms, and control-flow strategies, and demonstrate how these enable rapid development of modular, domain-adaptive solutions. Three real-world case studies are tested in controlled, containerised pilots in telecommunications security, national heritage asset management, and utilities customer service automation. Initial empirical results show that, for these case studies, prototypes were delivered within two weeks and pilot-ready solutions within one month, suggesting reduced development overhead compared to conventional approaches and improved user accessibility. However, findings also reinforce limitations documented in the literature, including variability in LLM behaviour that leads to challenges in transitioning from prototype to production maturity. We conclude by outlining critical research directions for improving reliability, scalability, and governance in MAS architectures and the further work needed to mature MAS design patterns to mitigate the inherent challenges.",
                    "category": "cs.MA",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心贡献在于形式化了LLM驱动的多智能体系统（MAS）的设计模式和范式，并定义了包括智能体编排、通信机制和控制流策略在内的关键架构组件。这属于“构建”和“改进”LLM智能体（特别是多智能体系统）的方法论研究，符合第一步中关于保留“构建LLM智能体”或“多智能体系统”方法论的要求。 2.  **正面指标 (匹配)**: 论文明确涉及 `Multi-Agent Systems (MAS)` 这一核心范式，并深入探讨了 `Agent Orchestration`（智能体编排）和 `Communication mechanisms`（通信机制），这些都是多智能体研究中的关键能力和正面指标。 3.  **排除标准 (未触发)**: *   **非演化型应用**: 尽管论文使用了电信、遗产管理等领域的案例研究，但其主要目的不是为了解决这些领域的具体业务问题，而是为了评估MAS架构和设计模式的实用性及开发效率。因此，它不属于仅将LLM作为工具应用到特定领域的“非演化型应用”。 *   **安全与对齐**: 论文虽然提到了治理和可靠性，但这并非其核心贡献，核心在于系统架构和设计模式。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 4.  **综合结论**: 该论文提供了关于如何构建、设计和评估多智能体系统的实证见解和架构框架，直接服务于“LLM智能体及其演化”中关于多智能体方向的研究目标。",
                    "summary2": "本文旨在形式化LLM赋能的多智能体系统（MAS）的设计模式并评估其实用性。针对电信安全、国家遗产资产管理和公用事业客户服务等真实场景，我们提出了一种基于ReAct智能体和动态编排的MAS设计范式，并在三个受控的容器化试点项目中通过开发周期、利益相关者反馈及UAT评分验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "When Numbers Start Talking: Implicit Numerical Coordination Among LLM-Based Agents",
                    "arxiv_id": "2601.03846",
                    "authors": "Alessio Buscemi, Daniele Proverbio, Alessandro Di Stefano, The Anh Han, German Castignani, Pietro Liò",
                    "summary": "LLMs-based agents increasingly operate in multi-agent environments where strategic interaction and coordination are required. While existing work has largely focused on individual agents or on interacting agents sharing explicit communication, less is known about how interacting agents coordinate implicitly. In particular, agents may engage in covert communication, relying on indirect or non-linguistic signals embedded in their actions rather than on explicit messages. This paper presents a game-theoretic study of covert communication in LLM-driven multi-agent systems. We analyse interactions across four canonical game-theoretic settings under different communication regimes, including explicit, restricted, and absent communication. Considering heterogeneous agent personalities and both one-shot and repeated games, we characterise when covert signals emerge and how they shape coordination and strategic outcomes.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的本质是研究 LLM 智能体在多智能体环境中的行为机制。它不是将智能体作为工具去解决生物、医疗等特定领域的应用问题，而是深入探讨智能体之间如何进行“隐式协调”和“隐蔽通信”。这属于构建和理解多智能体系统（Multi-Agent Systems）的方法论研究，因此应予以保留。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Communication`（特别是非显式的、隐蔽的通信）和 `Collaboration`（协调）。 *   **博弈与互动**：通过博弈论设置分析智能体间的战略互动，这属于多智能体研究中的社会行为和博弈范畴。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图技术等排除项。 **总结**：该论文的核心贡献在于揭示了 LLM 智能体在缺乏显式语言通道时，如何通过行动（如数字信号）进行隐式协调。这直接拓展了对于多智能体通信机制和协作行为的理解，符合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在探究LLM智能体在多智能体环境中的隐式通信与协调机制。针对四种经典博弈论场景（Prisoner's Dilemma等），我们提出了一种基于数值序列的隐式通信机制，并在FAIRGAME框架下通过GPT-4o智能体进行了实验。通过分析不同通信条件下的合作水平与熵值，验证了隐式信号能产生结构化模式并有效影响战略结果。",
                    "summary_translation": "基于LLMs（大语言模型）的智能体越来越多地在需要strategic interaction（策略互动）和协调的multi-agent environments（多智能体环境）中运行。尽管现有工作主要集中在个体智能体或共享explicit communication（显式通信）的交互智能体上，但对于交互智能体如何进行隐式协调的研究尚不充分。特别是，智能体可能会进行covert communication（隐蔽通信），即依赖于嵌入在其行动中的间接或非语言信号，而非显式消息。本文对LLM驱动的多智能体系统中的隐蔽通信进行了game-theoretic（博弈论）研究。我们分析了在不同communication regimes（通信机制）下，包括显式、受限和缺失通信，四种典型博弈论设定中的互动。考虑到heterogeneous agent personalities（异构智能体人格）以及one-shot games（单次博弈）和repeated games（重复博弈），我们刻画了隐蔽信号何时涌现，以及它们如何塑造协调和策略结果。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-06": [
        {
            "name": "Artificial Intelligence",
            "count": 7,
            "papers": [
                {
                    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
                    "arxiv_id": "2601.03236",
                    "authors": "Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li",
                    "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 MAGMA 的“智能体记忆架构”。这属于构建和改进 LLM 智能体的方法论范畴，旨在解决智能体在长视界推理中的记忆检索和表示问题。它不是将现有智能体简单应用于特定领域，也不是单纯的基础设施优化，而是直接针对智能体核心组件（记忆）的架构创新。 2.  **包含核心关注点 (第二步)**: 论文明确涉及 `Agentic AI` 和 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它探讨了如何通过多图结构（语义、时间、因果、实体）来优化智能体的记忆存储和检索，从而提升推理能力。这完全符合单智能体方向中关于“记忆”机制的子方向。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 虽然摘要提到了“alignment between query intent and retrieved evidence”（查询意图与检索证据的对齐），但这指的是信息检索层面的语义匹配，而非 AI 安全领域的“对齐”。论文主要贡献不在于 Safety 或 Alignment。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 *   **图**: 尽管论文使用了“Multi-Graph”（多图）技术，但这是作为实现智能体记忆架构的**手段**，而非研究图算法本身。论文的主题是“Agentic Memory Architecture”，属于智能体研究，因此不应被排除。 综上所述，这篇论文通过改进智能体的记忆机制来提升其性能，属于构建和演化 LLM 智能体的核心研究范围，符合筛选标准。",
                    "summary2": "本文旨在解决现有Memory-Augmented Generation系统因依赖单一语义相似度而限制长时程推理准确性和可解释性的问题。针对长上下文推理场景，我们提出了一种名为MAGMA的多图智能体记忆架构，通过语义、时间、因果和实体四个正交关系图解耦记忆表示，并采用策略引导的图遍历进行检索。在LoCoMo和LongMemEval数据集上通过LLM-as-a-Judge、F1及系统延迟等指标验证了其有效性。",
                    "summary_translation": "Memory-Augmented Generation (MAG，记忆增强生成) 通过引入外部记忆扩展了大语言模型，以支持长上下文推理。然而，现有方法主要依赖于单一整体记忆存储上的语义相似度，导致时间、因果和实体信息相互纠缠。这种设计限制了可解释性以及查询意图与检索证据之间的对齐，从而导致推理准确率不理想。在本文中，我们提出了 MAGMA，这是一种多图智能体记忆架构，它在正交的语义、时间、因果和实体图中表示每个记忆项。MAGMA 将检索过程构建为在这些关系视图上的策略引导遍历，从而实现查询自适应的选择和结构化上下文构建。通过将记忆表示与检索逻辑解耦，MAGMA 提供了透明的推理路径以及对检索过程的细粒度控制。在 LoCoMo 和 LongMemEval 数据集上的实验表明，MAGMA 在长视界推理任务中始终优于最先进的智能体记忆系统。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
                    "arxiv_id": "2601.02950",
                    "authors": "Xuan Yang, Furong Jia, Roy Xie, Xiong Xi, Hengwei Bian, Jian Li, Monica Agrawal",
                    "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 范畴）**： 虽然论文标题侧重于“推理”，但其核心贡献不仅仅是提出一种新的思维链变体，而是构建了一个名为 **BoT-R (Batch-of-Thought with Reflection)** 的架构。摘要明确指出，该方法是在 **\"multi-agent reflection architecture\"（多智能体反思架构）** 中实例化的。这表明论文的本质是构建了一个包含特定角色（Reflector 智能体）的智能体系统，而非单纯的模型推理能力提升。 2.  **符合正面指标（多智能体与自我反思）**： *   **多智能体**：论文明确提到了 \"multi-agent reflection architecture\"，涉及智能体之间的协作与信息交互（Reflector 执行联合评估以解锁互信息增益）。 *   **自我反思/修正**：论文的核心机制涉及 \"reflection\"（反思）和 \"detects errors\"（检测错误），这属于智能体的自我反思和自我修正能力范畴。 *   **Agentic 框架**：BoT-R 是一种新的 Agentic 框架，用于处理复杂任务，符合“构建、改进 LLM 智能体”的目标。 3.  **排除标准检查**： *   该论文不属于特定领域的非演化型应用（如医疗、法律），而是一种通用的推理架构改进。 *   虽然涉及推理，但它通过多智能体架构来实现，不属于“非 Agentic 的推理”排除项。 *   不涉及安全、对齐、多模态或图等排除领域。 综上所述，该论文通过引入多智能体反思架构来增强 LLM 的推理能力，属于 Agentic AI 和 Multi-Agent Systems 的研究范畴，符合筛选要求。",
                    "summary2": "本文旨在解决现有LLM推理系统独立处理查询导致跨实例信号丢失的问题。针对相关查询批次，我们提出了一种名为Batch-of-Thought (BoT) 的免训练方法，利用Reflector进行联合评估以实现跨实例学习。在六个基准和三个模型家族上，通过准确率、置信度校准和Token成本验证了其有效性，实现了性能提升和高达61%的成本降低。",
                    "summary_translation": "当前的 Large Language Model (大语言模型) 推理系统独立处理查询，忽略了诸如共享推理模式和一致性约束等有价值的 cross-instance signals (跨实例信号)。我们提出了 Batch-of-Thought (BoT)，这是一种 training-free (无需训练) 的方法，通过联合处理相关查询来实现 cross-instance learning (跨实例学习)。通过在批次间执行 comparative analysis (比较分析)，BoT 能够识别高质量的 reasoning templates (推理模板)，通过 consistency checks (一致性检查) 检测错误，并分摊 computational costs (计算成本)。我们在 multi-agent reflection architecture (多智能体反思架构) 中实例化了 BoT (BoT-R)，其中 Reflector (反思者) 执行 joint evaluation (联合评估)，以释放 isolated processing (孤立处理) 中无法获得的 mutual information gain (互信息增益)。在三个模型系列和六个基准测试上的实验表明，BoT-R 在将 inference costs (推理成本) 降低高达 61% 的同时，持续提高了 accuracy (准确性) 和 confidence calibration (置信度校准)。我们的理论和实验分析揭示了 batch-aware reasoning (批感知推理) 在何时以及为何能使 Large Language Model (大语言模型) 系统受益。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
                    "arxiv_id": "2601.02702",
                    "authors": "Shuhaib Mehri, Priyanka Kargupta, Tal August, Dilek Hakkani-Tür",
                    "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”与“自我演化”的交叉领域。 1.  **核心贡献符合要求**：论文的核心在于构建了一种“长期协作智能体”，并提出了相应的基准测试。这不仅仅是应用现有模型，而是提出了一种新的智能体框架，旨在解决智能体如何适应环境（用户）的问题。 2.  **符合“单智能体”方向**：论文明确聚焦于智能体的关键能力——**记忆**和**自我反思**。它详细描述了如何通过持久化的记忆来存储用户偏好，以及如何通过反思机制来更新记忆，这直接对应筛选标准中的“Agentic: 记忆、自我反思”。 3.  **符合“自我演化”方向**：论文强调智能体随着交互经验的积累，能够不断细化用户偏好并改进协作质量。这种通过经验反馈进行迭代改进和自我完善的过程，正是“自我演化”的核心体现。 4.  **排除标准检查**：论文不属于特定领域的非演化型应用（其核心是智能体机制的通用改进），也不涉及安全对齐、多模态或图技术等排除领域。 综上所述，该论文在构建具备记忆和自我演化能力的LLM智能体方面做出了实质性贡献，应予保留。",
                    "summary2": "本文旨在解决对话代理在多会话协作中学习并利用用户偏好以提升协作质量的问题。针对多会话协作问题求解场景，我们提出了一种配备持久化记忆的长期协作代理及基于强化学习的记忆更新框架，并在MULTI SESSION COLLAB基准上通过Task Success、User Effort和Conversation Length验证了其有效性。",
                    "summary_translation": "随着 conversational agents (对话智能体) 积累与用户协作的经验，适应用户偏好对于建立长期关系以及随着时间的推移提升协作质量至关重要。我们介绍了 MultiSessionCollab，这是一个 benchmark (基准测试)，旨在评估智能体在跨多个会话的过程中学习用户偏好并利用这些偏好提升协作质量的能力。为了开发能够在此类场景中取得成功的智能体，我们提出了 long-term collaborative agents (长期协作智能体)，该智能体配备了 memory (记忆)，能够随着交互经验的积累而持久保存并细化用户偏好。此外，我们证明了可以从 MultiSessionCollab 中的 user simulator (用户模拟器) 行为中提取 learning signals (学习信号)，从而训练智能体生成更全面的 reflections (反思内容) 并更有效地更新其 memory (记忆)。大量实验表明，为智能体配备 memory (记忆) 能够改善长期协作效果，从而带来更高的任务成功率、更高效的交互以及更低的用户负担。最后，我们进行了一项人类用户研究，结果表明 memory (记忆) 有助于在真实场景中提升用户体验。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents",
                    "arxiv_id": "2601.02643",
                    "authors": "Mehmet Kurmaz",
                    "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中的“工具使用”和“规划”子方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献在于提出了一种新的方法论（偏好感知查询修复）和一个基准（AWARE-US），旨在解决“工具调用智能体”在查询结构化数据库时遇到的“不可行性”问题。 *   这不是将智能体简单应用于特定领域（如医疗或金融）的应用型论文，而是针对智能体在工具使用过程中的通用能力缺陷（查询失败处理）进行改进和构建。 *   它不属于基础设施优化，也不是非Agentic的基础推理提升。 2.  **正面指标匹配 (第二步)**: *   论文明确涉及 `LLM-based Agents` 和 `Tool Use / Tool Augmentation`。 *   它关注智能体如何根据对话上下文推断用户意图，并动态调整查询策略（放松约束），这属于智能体的 `Planning` 和决策能力范畴。 3.  **排除标准检查 (第三步)**: *   虽然论文提到了“preference alignment”（偏好对齐），但这里的对齐是指智能体在执行任务时对用户具体约束条件的偏好（例如用户更看重价格还是速度），而非AI安全、伦理或价值观层面的“对齐”。因此，不应被排除。 *   论文不涉及多模态视觉、图技术或安全防御机制。 4.  **特殊处理 (第四步)**: *   论文关于智能体如何处理工具调用失败并进行自我修正（查询修复），属于Agentic的推理与规划范畴，符合保留条件。 综上所述，该论文致力于改进LLM智能体的工具使用鲁棒性和交互规划能力，符合“构建、改进LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决Tool-calling agents在查询结构化数据库时面临的infeasibility问题。针对基于personas的对话场景，我们提出了一种Preference-Aware Resolution框架，包含local weighting、global one-shot weighting和pairwise ranking三种约束重要性推断方法。我们在AWARE-US benchmark上通过Relax match和Car match等指标验证了其有效性，结果显示Local weighting方法在汽车推荐中与用户偏好一致性达到48%。",
                    "summary_translation": "调用工具查询结构化数据库的对话代理通常面临两个相互关联的问题：underspecification（约束不足，即缺乏运行精确查询所需的约束）和 infeasibility（不可行性，即完全指定的查询返回空集，因为没有项目满足所有约束）。现有研究通常以“无结果”作为回应，或利用 ad hoc rules（特设规则）放宽约束，这可能会因丢弃用户最关心的需求而违背用户意图。我们将 infeasibility handling（不可行性处理）构建为一个 preference-aware query repair（感知偏好的查询修复）问题：当查询不可满足时，代理应当放宽对用户而言重要性最低的约束。我们提出了三种基于 LLM（大语言模型）的方法，用于从对话中推断相对约束重要性：(1) local weighting（局部加权），(2) global one-shot weighting（全局一次性加权），以及 (3) pairwise ranking（成对排序）。实验结果表明，local weighting（局部加权）实现了最佳的 preference alignment（偏好对齐），而 global weighting（全局加权）在正确的 constraint relaxation（约束放宽）方面表现最佳。我们还介绍了 AWARE-US，这是一个包含 persona-grounded queries（基于人设的查询）的基准数据集，要求代理通过对话消除请求歧义，并以与 persona-implied preferences（人设隐含偏好）一致的方式解决 infeasibility（不可行性）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
                    "arxiv_id": "2601.02553",
                    "authors": "Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao",
                    "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **SimpleMem**，这是一个专门为 **LLM智能体** 设计的高效记忆框架。 *   它属于 **构建/改进 LLM智能体** 的范畴，旨在解决智能体在长期交互中的记忆管理问题，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或硬件加速的研究。 2.  **正面指标匹配（第二步）**： *   论文直接涉及 **Agentic AI** 和 **LLM-based Agents** 的核心范式。 *   论文重点解决了智能体的 **Memory（记忆）** 能力，这是单智能体方向的关键子方向之一。摘要中提到的“管理历史经验”、“语义结构化压缩”和“递归记忆整合”都是为了增强智能体的记忆机制，使其能更好地支持长期交互。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，该论文致力于改进LLM智能体的核心组件（记忆系统），属于单智能体研究范畴，符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM Agent在长期交互中因上下文限制和冗余信息导致的记忆管理低效问题。针对复杂环境下的长上下文交互场景，我们提出了一种基于语义无损压缩的SimpleMem框架，包含语义结构化压缩、递归记忆整合和自适应查询感知检索三阶段流程。我们在LoCoMo benchmark上通过F1 score和Token Cost验证了其有效性，实现了平均F1提升26.4%且推理Token消耗降低30倍。",
                    "summary_translation": "为了在复杂环境中支持可靠的长期交互，LLM agents（大语言模型智能体）需要能够高效管理历史经验的 memory systems（记忆系统）。现有方法要么通过 passive context extension（被动上下文扩展）保留完整的交互历史，导致大量冗余，要么依赖 iterative reasoning（迭代推理）来过滤噪声，从而产生高昂的 token costs（Token 成本）。为了应对这一挑战，我们提出了 SimpleMem，这是一种基于 semantic lossless compression（语义无损压缩）的高效 memory framework（记忆框架）。我们提出了一个旨在最大化 information density（信息密度）和 token utilization（Token 利用率）的 three-stage pipeline（三阶段流水线）：(1) \\textit{Semantic Structured Compression}（语义结构化压缩），该阶段应用 entropy-aware filtering（熵感知过滤）将非结构化交互提炼为紧凑的 multi-view indexed memory units（多视图索引记忆单元）；(2) \\textit{Recursive Memory Consolidation}（递归记忆整合），这是一个异步过程，将相关单元整合为 higher-level abstract representations（高层抽象表示）以减少冗余；以及 (3) \\textit{Adaptive Query-Aware Retrieval}（自适应查询感知检索），该阶段根据 query complexity（查询复杂度）动态调整 retrieval scope（检索范围），以高效构建精确的上下文。在 benchmark datasets（基准数据集）上的实验表明，我们的方法在准确性、retrieval efficiency（检索效率）和 inference cost（推理成本）方面始终优于 baseline approaches（基线方法），实现了平均 26.4% 的 F1 提升，同时将 inference-time token consumption（推理时 Token 消耗）减少了多达 30 倍，展示了性能与效率之间的卓越平衡。代码可在 https://github.com/aiming-lab/SimpleMem 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices",
                    "arxiv_id": "2601.02732",
                    "authors": "Lingzhe Zhang, Tong Jia, Yunpeng Zhai, Leyi Pan, Chiming Duan, Minghua He, Mengxi Jia, Ying Li",
                    "summary": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建LLM智能体”的目标**： 尽管论文的应用场景是微服务中的根因定位（特定领域），但其核心贡献并非简单地将现有LLM作为工具应用，而是提出了一个新的框架 **AMER-RCL**。该框架包含两个关键的智能体组件： *   **Recursive Reasoning RCL engine**：明确被定义为一个**多智能体框架**，用于执行递归推理。 *   **Agentic Memory**：一种智能体记忆机制，用于增量累积和重用推理过程。 2.  **高度匹配“正面指标”**： *   **多智能体**：论文明确提出了多智能体框架来处理告警和细化候选原因。 *   **智能体能力**：涉及 `Memory`（智能体记忆）、`Reasoning`（递归推理）以及 `Multi-Agent` 协作。 *   **核心范式**：论文标题和摘要中多次强调 \"Agentic Memory\" 和 \"Agentic\"，完全符合 Agentic AI 的研究焦点。 3.  **通过“排除标准”和“特殊情况”检查**： *   虽然涉及特定领域（微服务），但根据第四步的“自我演化的应用”逻辑（此处虽非演化，但同理），如果论文的核心是提出一种新的智能体机制（如这里的递归推理+记忆机制），即使应用在特定领域，也应保留。这区别于仅仅调用API解决领域问题的“非演化型应用”。 *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除内容。 综上所述，该论文在构建多智能体框架和智能体记忆机制方面做出了实质性贡献，属于 Agentic AI 和 Multi-Agent Systems 的前沿研究。",
                    "summary2": "本文旨在解决现有LLM方法在微服务根因定位中推理浅显及缺乏跨告警复用的问题。针对微服务系统中的多告警场景，我们提出了一种名为AMER-RCL的智能体记忆增强递归推理框架。该框架利用递归推理引擎进行深度分析，并通过智能体记忆复用历史推理结果。在AIOPS 2022、Train-Ticket和FAMOS-Mall数据集上，通过Recall@k、MRR及推理延迟验证了其有效性，显著优于现有方法。",
                    "summary_translation": "随着当代微服务系统日益普及且日趋复杂——通常由数百甚至数千个细粒度、相互依赖的子系统组成——其故障发生频率也随之增加。因此，确保系统可靠性需要准确的根因定位。尽管针对该任务已探索了许多传统的基于图和深度学习的方法，但它们往往严重依赖预定义模式，难以适应不断演变的运行环境。因此，近期提出了多种基于大语言模型的方法。然而，这些方法仍面临两大主要局限：一是浅层的、以症状为中心的推理损害了准确性；二是缺乏跨告警重用，导致推理冗余和高延迟。本文对站点可靠性工程师如何定位故障根因进行了全面研究，汲取了来自多个组织的专业人士的见解。调查结果显示，专家根因分析表现出三个关键特征：递归性、多维扩展和跨模态推理。基于这些发现，我们提出了 AMER-RCL，一种用于微服务根因定位的智能体记忆增强递归推理框架。AMER-RCL 采用了递归推理 RCL 引擎，这是一个多智能体框架，通过对每个告警执行递归推理来逐步细化候选原因；同时，智能体记忆增量地积累并重用时间窗口内先前告警的推理结果，以减少冗余探索并降低推理延迟。实验结果表明，AMER-RCL 在定位准确性和推理效率方面均持续优于最先进的方法。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance",
                    "arxiv_id": "2601.02454",
                    "authors": "Saba Naqvi, Mohammad Baqar, Nawaz Ali Mohammad",
                    "summary": "Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”与“自我演化”方向**： 论文的核心贡献是提出了一种“智能体多模型测试框架”。这不仅仅是一个应用，而是一个新的**多智能体系统（MAS）**架构。该系统包含三个具有不同角色的智能体（测试生成、执行与分析、审查与优化），它们通过协作完成任务，这直接对应了筛选标准中的“多智能体”方向。 2.  **具备明确的“自我演化”机制**： 摘要中明确提到这是一个“闭环、自我修正系统”，利用“沙箱执行”和“强化信号”进行“迭代再生或修补”。这种通过环境反馈（执行结果、覆盖率指标）来引导智能体自主改进和迭代的过程，完全符合“自我演化”中关于自我完善、自我修正和迭代改进的定义。 3.  **属于Agentic AI的构建而非单纯应用**： 虽然论文的应用场景是软件测试（特定领域），但根据筛选标准第四步的“自我演化的应用”例外规则，只要论文的核心是提出一种新的“自我演化”机制或Agentic框架，即使应用在特定领域也应保留。本文重点在于构建了一个能够自主规划、协作和反思的智能体框架，而非简单地将现有LLM作为工具生成测试代码。 综上所述，该论文在多智能体协作、自我修正机制以及闭环演化方面具有明确的方法论贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有AI测试生成缺乏执行反馈导致测试无效的问题。针对微服务应用，我们提出了一种Agentic Testing Architecture (ATA)，即包含Test Generation Agent、Execution and Analysis Agent和Review and Optimization Agent的多智能体闭环协作框架。在开源及企业应用数据集上，通过代码覆盖率、无效测试率和人工工作量等指标验证了其有效性，实现了无效测试减少60%及覆盖率提升30%。",
                    "summary_translation": "软件测试正迈向智能自动化，但现有的 AI-based test generators（基于AI的测试生成器）仍受限于静态、单次输出的模式。由于缺乏 execution aware feedback（执行感知反馈），这些工具常生成无效、冗余或不可执行的测试用例。本文提出了一种 agentic multi-model testing framework（代理多模型测试框架），这是一个闭环、自纠正系统。在该系统中，Test Generation Agent（测试生成代理）、Execution and Analysis Agent（执行与分析代理）以及 Review and Optimization Agent（审查与优化代理）协同工作，生成、执行、分析并完善测试，直至达到收敛状态。通过利用 sandboxed execution（沙箱执行）、详细的 failure reporting（失败报告）以及对失败测试的 iterative regeneration（迭代重新生成）或 patching（修补），该框架能够自主提升测试质量并扩大覆盖率。该框架集成于 CI/CD-compatible pipeline（兼容CI/CD的流水线）中，利用来自 coverage metrics（覆盖率指标）和 execution outcomes（执行结果）的 reinforcement signals（强化信号）来指导测试的完善过程。针对 microservice based applications（基于微服务的应用程序）进行的实证评估表明，与 single-model baselines（单模型基线）相比，该框架将无效测试减少了高达 60%，覆盖率提升了 30%，并显著降低了人工投入。这证明了 multi-agent（多代理）、feedback-driven loops（反馈驱动循环）能够将软件测试演变为一个自主的、持续学习的 quality assurance ecosystem（质量保证生态系统），以支持 self-healing（自愈）和 high-reliability codebases（高可靠性代码库）。",
                    "inspiration_trace": "基于论文《The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance》，以下是对作者产出该核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观实现的思考过程：\n\n### 1. 宏观背景：软件复杂性与自动化瓶颈\n**观察：**\n现代软件工程已转向云原生、微服务架构和DevOps模式，代码迭代速度极快，依赖关系复杂。传统的手工测试或基于规则的自动化脚本已无法跟上这种交付节奏，成为效率瓶颈。\n\n**思考：**\n必须引入更高阶的智能化手段。近年来，大语言模型（LLMs）在代码理解和生成方面表现出色，似乎为解决这一瓶颈提供了契机。学术界和工业界开始尝试利用LLMs自动生成测试用例。\n\n### 2. 核心痛点：LLM的“静态生成”与“执行盲区”\n**深入观察：**\n尽管现有的基于LLM的测试生成工具（如Codex, GPT-4）能够快速产出代码，但作者发现了一个关键问题：**这些生成是“一次性”的**。LLM像是在真空中写作，它不知道生成的代码在真实环境中能否运行。\n\n**逻辑断层：**\n实证数据显示，超过40%的LLM生成的测试用例在首次执行时就会失败（如缺少依赖、语法错误、逻辑不匹配）。这是因为现有方法缺乏“执行感知”的反馈机制。LLM无法从自己的错误中学习，导致产生了大量无效、冗余的垃圾代码，反而增加了人工清理的成本。\n\n**假设提出：**\n如果能让测试生成系统具备“自我反省”和“自我修正”的能力，即像人类测试人员一样——写代码 -> 运行 -> 报错 -> 修改 -> 再运行，那么测试的质量和有效性将大幅提升。\n\n### 3. 概念跃迁：从“单点工具”到“多智能体协作”\n**灵感来源：**\n作者观察到AI领域正在兴起“Agentic AI”（智能体AI）和多智能体系统（如AutoGen, SWE-Agent）。这些系统通过让多个专门的AI角色相互协作、对话来解决复杂任务，而非依赖单一模型。\n\n**类比推理：**\n软件测试本身就是一个团队协作过程：有人写测试，有人执行测试，有人分析结果。为什么不让AI也模仿这种社会分工？\n\n**方法论雏形：**\n不再使用一个单一的LLM模型完成所有工作，而是设计一个**多智能体系统**。将测试流程拆解为不同的专业角色，让它们各司其职，形成一个流水线。\n\n### 4. 方法论构建：闭环反馈与收敛机制\n**架构设计：**\n为了实现上述假设，作者构建了三个核心智能体，形成了一个闭环：\n1.  **生成者：** 负责根据需求编写初始测试。\n2.  **执行者：** 负责在沙盒环境中运行测试，并收集覆盖率数据和报错信息。\n3.  **审查与优化者：** 负责分析执行结果，诊断失败原因，并指导生成者进行修复。\n\n**逻辑核心：**\n这个系统的核心不在于单个智能体的能力，而在于它们之间的**交互循环**。作者引入了“收敛”的概念：系统不是无限循环，而是设定了明确的停止条件（如覆盖率>95%，失败率<2%）。这将其从简单的“尝试”转变为一种“优化过程”。\n\n**关键创新点：**\n作者意识到，只有当“执行反馈”被转化为“自然语言指令”并重新输入给LLM时，真正的自我修正才发生。因此，必须建立一个共享的记忆库，让智能体能记住之前的错误，避免重蹈覆辙。\n\n### 5. 最终愿景：迈向自主演进的QA生态系统\n**验证与结论：**\n通过实验，作者验证了这种多智能体闭环模式确实能显著降低无效测试比例，提高覆盖率。这证明了“反馈驱动”优于“静态生成”。\n\n**思想升华：**\n最终，作者将这一方法定义为“Agentic Testing”。这不仅仅是一个工具，而是一个**自主的质量保证生态系统**。它标志着软件测试从“人类辅助AI”转向了“AI自主协作”，测试系统具备了类似生物的“自愈”和“适应”能力，能够随着代码库的演变而自动演进。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现问题（静态生成无效） -> 寻找类比（人类协作流程） -> 引入范式（多智能体系统） -> 构建机制（闭环反馈与收敛） -> 实现愿景（自主QA）”** 的完整逻辑链条。其核心洞察在于：**没有反馈的生成是盲目的，只有引入执行反馈和多智能体协作，AI测试才能真正落地。**"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
                    "arxiv_id": "2601.03192",
                    "authors": "Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen, Zhiyu Li, Feiyu Xiong, Yutao Qi, Bo Tang, Muning Wen",
                    "summary": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **MemRL**，这是一个旨在实现 **Self-Evolving Agents（自我演化智能体）** 的框架。它解决的是智能体如何通过经验进行自我完善和迭代的问题，而非仅仅是将LLM作为工具应用到特定领域。这直接对应了研究目标中的“自我演化”方向。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文标题和摘要中明确提到了 `Self-Evolving Agents` 和 `Reinforcement Learning`，属于核心关注点。 *   **智能体能力**：论文重点研究了 `Memory`（情景记忆 Episodic Memory）和 `Self-Improvement`（通过环境反馈持续改进）。 *   **演化机制**：论文提出了一种非参数强化学习机制，通过环境反馈在运行时迭代改进 Q-values（效用），从而实现智能体的持续演化，符合 `Iterative Improvement` 和 `Self-Refine` 的定义。 3.  **排除标准（未触发）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然论文在 HLE、BigCodeBench 等基准上进行了实验，但这些是用于验证智能体能力的通用任务，不属于“非演化型应用”的排除范畴。 4.  **特殊与模糊情况处理**： *   论文属于典型的 **自我演化** 机制研究。它提出了一种新的“在情景记忆上进行运行时强化学习”的方法，使智能体能够在不更新模型权重的情况下解决稳定性-可塑性困境并实现持续改进。这完全符合“保留：如果论文的核心是提出一种新的‘自我演化’机制”的规则。 综上所述，该论文专注于构建新的框架以实现LLM智能体的自我演化和记忆优化，是“LLM智能体及其演化”课题下的高质量相关论文。",
                    "summary2": "本文旨在解决LLM在部署后难以持续自我进化及避免灾难性遗忘的问题。针对需要Runtime Continuous Learning的场景，我们提出了一种MemRL框架，通过非参数强化学习优化情节记忆，采用基于Q值的Two-Phase Retrieval机制区分高价值策略与噪声。并在HLE、BigCodeBench、ALFWorld及Lifelong Agent Bench上通过Last Epoch Accuracy和Cumulative Success Rate验证了其有效性。",
                    "summary_translation": "人类智能的显著特征在于通过 Constructive Episodic Simulation（建设性情景模拟）掌握新技能的能力——即检索过往经验以综合解决新颖任务的方案。尽管 Large Language Models（大语言模型）拥有强大的推理能力，但它们难以模拟这种自我进化：Fine-tuning（微调）计算成本高昂且容易导致 Catastrophic Forgetting（灾难性遗忘），而现有的基于记忆的方法依赖于被动语义匹配，往往会检索到噪声。为了应对这些挑战，我们提出了 MemRL，这是一个通过在 Episodic Memory（情景记忆）上进行 Non-parametric Reinforcement Learning（非参数强化学习）来使智能体实现自我进化的框架。MemRL 明确地将冻结 LLM 的稳定推理与具有可塑性且不断进化的记忆分离开来。与传统方法不同，MemRL 采用了一种 Two-Phase Retrieval（两阶段检索）机制，首先根据语义相关性过滤候选，然后基于学习到的 Q-values（效用）进行选择。这些效用通过环境反馈以试错的方式不断优化，使智能体能够区分高价值策略与相似的噪声。在 HLE、BigCodeBench、ALFWorld 和 Lifelong Agent Bench 上进行的广泛实验表明，MemRL 显著优于 State-of-the-art（最先进的）基线。我们的分析实验证实，MemRL 有效调和了 Stability-Plasticity Dilemma（稳定性-可塑性困境），能够在不进行权重更新的情况下实现持续的运行时改进。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents",
                    "arxiv_id": "2601.02845",
                    "authors": "Kai Li, Xuanqing Yu, Ziyi Ni, Yi Zeng, Yao Xu, Zheqing Zhang, Xin Li, Jitao Sang, Xiaogang Duan, Xuelei Wang, Chengbao Liu, Jie Tan",
                    "summary": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心贡献符合第一步判断**：论文的核心是提出了一种名为 TiMem 的时间-分层记忆框架，旨在解决长视界对话智能体在管理不断增长的交互历史时面临的上下文窗口限制问题。这属于“构建、改进或演化 LLM智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等）的非演化型应用。 2.  **精准命中第二步正面指标**：我的研究焦点明确包含“单智能体”方向下的“记忆”能力。该论文专门针对智能体的“记忆”机制进行了创新，提出了时间记忆树（TMT）和语义引导的整合机制，这正是对智能体核心组件的改进。 3.  **不触犯第三步排除标准**：论文主要关注智能体的记忆架构，不涉及安全与对齐、多模态视觉技术或知识图谱等被明确排除的领域。 综上所述，该论文致力于改进LLM智能体的记忆能力，属于单智能体研究的关键子方向，符合筛选要求。",
                    "summary2": "本文旨在解决长周期对话代理中记忆碎片化及个性化不稳定的问题。针对不断增长的交互历史，我们提出了一种TiMem框架，该框架利用Temporal Memory Tree实现时间分层记忆整合与复杂度感知检索，并在LoCoMo和LongMemEval-S基准上通过准确率和记忆长度验证了其有效性。",
                    "summary_translation": "长程对话智能体必须管理不断增长的交互历史，这些历史很快就会超过大语言模型（large language models, LLMs，大语言模型）的有限上下文窗口。现有的记忆框架对跨层级的时间结构化信息支持有限，往往导致记忆碎片化和不稳定的长程个性化。我们提出了 TiMem，一个时间-层级记忆框架，它通过时间记忆树（Temporal Memory Tree, TMT，时间记忆树）组织对话，实现了从原始对话观察到逐步抽象的人设表征的系统性记忆整合。TiMem 具有三个核心特征：（1）通过 TMT 进行的时间-层级组织；（2）语义引导的整合，能够在无需微调的情况下实现跨层级的记忆整合；（3）复杂度感知的记忆召回，在不同复杂度的查询中平衡精确度和效率。在一致的评估设置下，TiMem 在两个基准测试中都达到了最先进的准确率，在 LoCoMo 上达到 75.30%，在 LongMemEval-S 上达到 76.88%。它优于所有评估的基线模型，同时在 LoCoMo 上将召回记忆长度减少了 52.20%。流形分析表明，在 LoCoMo 上存在清晰的人设分离，在 LongMemEval-S 上离散度有所降低。总的来说，TiMem 将时间连续性视为对话智能体长程记忆的首要组织原则。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
                    "arxiv_id": "2601.02744",
                    "authors": "Hanqi Jiang, Junhao Chen, Yi Pan, Ling Chen, Weihang You, Yifan Zhou, Ruidong Zhang, Yohannes Abate, Tianming Liu",
                    "summary": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断 (第一步)**: 该论文的核心贡献是提出了一种名为 Synapse 的统一记忆架构，旨在解决 LLM 智能体在长期记忆中的“不连贯”问题。这属于对 LLM 智能体核心组件（记忆）的构建与改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型推理能力的提升。因此，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: 论文直接涉及 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它引入了基于认知科学的扩散激活机制来优化智能体的记忆检索，这直接提升了智能体处理复杂时序和多跳推理任务的能力，属于单智能体方向的关键技术突破。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 论文不涉及安全、对齐或可解释性问题。 *   **多模态**: 论文专注于文本记忆架构，不涉及视觉或多模态内容。 *   **图技术**: 尽管论文中提到了“动态图”和“图遍历”，但其核心目的是构建智能体的记忆系统，而非研究图神经网络（GNN）或知识图谱算法本身。图在这里是作为智能体内部存储和检索信息的机制，服务于智能体的功能，因此不应被排除。 4.  **综合结论**: 该论文通过改进智能体的记忆机制，直接增强了 LLM 智能体的自主性和任务处理能力，完全符合“单智能体”方向中关于“记忆”的研究焦点。",
                    "summary2": "本文旨在解决LLM智能体长期记忆中的上下文隔离问题。针对长对话中的复杂推理场景，我们提出了一种基于Spreading Activation的SYNAPSE架构，构建Unified Episodic-Semantic Graph并结合Lateral Inhibition实现动态检索。我们在LoCoMo benchmark上通过F1 Score和Token消耗验证了其有效性，显著提升了多跳推理精度并降低了95%的Token使用量。",
                    "summary_translation": "尽管 Large Language Models (LLMs，大型语言模型) 在 generalized reasoning (泛化推理) 方面表现出色，但标准的 retrieval-augmented approaches (检索增强方法) 未能解决 long-term agentic memory (长期智能体记忆) 的割裂特性。为弥合这一差距，我们提出了 Synapse (Synergistic Associative Processing Semantic Encoding，协同联想处理语义编码)，这是一种超越 static vector similarity (静态向量相似度) 的 unified memory architecture (统一记忆架构)。借鉴 cognitive science (认知科学)，Synapse 将记忆建模为一个 dynamic graph (动态图)，其中相关性源于 spreading activation (扩散激活) 而非 pre-computed links (预计算链接)。通过整合 lateral inhibition (侧抑制) 和 temporal decay (时间衰减)，该系统能够动态突显相关的 sub-graphs (子图)，同时过滤干扰信息。我们实施了一种 Triple Hybrid Retrieval strategy (三重混合检索策略)，该策略将 geometric embeddings (几何嵌入) 与 activation-based graph traversal (基于激活的图遍历) 相融合。在 LoCoMo benchmark (LoCoMo 基准测试) 上的全面评估表明，Synapse 在复杂的 temporal and multi-hop reasoning tasks (时序和多跳推理任务) 中显著优于 state-of-the-art methods (最先进方法)，为 \"Contextual Tunneling\" (上下文隧道) 问题提供了稳健的解决方案。我们的代码和数据将在论文录用后公开发布。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
                    "arxiv_id": "2601.02880",
                    "authors": "Abhishek HS, Pavan C Shekar, Arpit Jain, Ashwanth Krishnan",
                    "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心判断**: *   论文的核心贡献是提出了 **ReTreVal** 这一混合框架，旨在构建和改进 LLM 智能体的多步推理能力。 *   它不是简单的应用型论文，而是提出了一种新的方法论，结合了 Tree-of-Thoughts (ToT)、自我细化和 Reflexion 记忆机制。 *   它不属于非演化型应用，也不属于基础设施或基础模型推理能力的微调，而是专注于智能体的架构设计。 2.  **符合研究焦点**: *   **单智能体**: 论文深入探讨了智能体的核心能力，包括 **Planning** (通过构建推理树进行结构化探索)、**Memory** (Reflexion memory buffer 用于持久化存储见解) 和 **Self-Reflection** (self-critique and refinement)。 *   **自我演化**: 论文明确提到了 \"persistent learning across problems\"（跨问题的持久学习）和 \"cross-problem memory\"（跨问题记忆）。智能体通过存储成功路径和失败模式的见解，利用这些经验在后续问题中进行改进，这完全符合“通过经验、反思进行自我完善”的自我演化定义。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   虽然在数学和创意写作任务上进行了评估，但这只是验证框架有效性的实验场景，论文的核心在于框架本身的机制（验证、剪枝、记忆），而非特定领域的应用。 4.  **特殊规则处理**: *   根据“推理/规划”规则，该论文属于保留范畴。它不仅仅是提出一种新的 CoT 变体，而是构建了一个包含验证、记忆和树搜索的完整 **Agentic 框架**，与 ReAct 和 Reflexion 等经典 Agentic 方法进行对比和改进。 综上所述，该论文通过引入记忆机制和结构化探索，增强了 LLM 智能体的规划和自我演化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决大语言模型在复杂多步推理任务中的局限性。针对数学和创意写作场景，我们提出了一种名为 ReTreVal 的混合框架，该框架集成了 Tree-of-Thoughts 探索、Self-Refinement、LLM-based critique scoring 和 reflexion memory。我们在 500 个数学问题和创意写作任务上，基于 Qwen 2.5 7B 模型，通过准确性和 GPT-4o mini 评分验证了其有效性。",
                    "summary_translation": "多步推理仍然是大型语言模型面临的一个关键挑战，特别是在数学和创意写作等复杂领域。尽管包括 ReAct、Reflexion 和 Self-Refine 在内的近期方法通过迭代优化和反思改进了推理能力，但它们往往缺乏对替代解题路径的结构化探索以及跨问题的持久学习能力。我们提出了 ReTreVal (Reasoning Tree with Validation，带验证的推理树)，这是一个集成了思维树探索、自我优化、基于 LLM 的批评评分以及反思记忆的混合框架，旨在实现有界且经过验证的多步推理。ReTreVal 构建了一个具有基于问题复杂度的自适应深度的结构化推理树，其中每个节点都在明确的 LLM 生成反馈指导下，经历迭代的自我批评和优化。双重验证机制评估每个节点的推理质量、连贯性和正确性，同时将来自成功推理路径的见解和失败模式持久地存储在反思记忆缓冲区中，从而实现跨问题学习。基于批评的剪枝策略在每一层仅保留得分最高的 top-k 个节点，在控制计算成本的同时保留了高质量的解题路径。我们使用 Qwen 2.5 7B 作为底层 LLM，在 500 个数学问题和创意写作任务上对 ReTreVal 与 ReAct、Reflexion 和 Self-Refine 进行了评估，结果表明 ReTreVal 通过结合结构化探索、批评驱动的优化以及跨问题记忆，始终优于现有方法，使其在需要探索性推理、严格验证和知识迁移的任务中尤为有效。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
                    "arxiv_id": "2601.02997",
                    "authors": "Waleed Khalid, Dmitry Ignatov, Radu Timofte",
                    "summary": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“Agentic AI”的核心研究范畴。 1.  **核心判断（符合自我演化机制）**： 论文的核心贡献在于构建了一个“闭环合成框架”，让LLM能够通过环境反馈（执行结果和性能评估）进行自我完善。论文详细描述了LLM经过22个监督微调周期的演化过程，通过迭代微调，模型内化了经验性的架构先验，从而提升了生成质量和性能。这完全符合“自我演化”中定义的“智能体通过经验、反思或环境反馈进行自我完善和迭代”。 2.  **符合Agentic AI特征**： 论文将LLM定位为一个“自主的、性能驱动的神经设计者”。它不仅仅是生成代码，而是通过“验证-评估-过滤-更新”的循环，展示了智能体利用工具（代码执行）和反馈机制来优化自身输出的能力，这属于Agentic AI的高级形态。 3.  **特殊情况处理（自我演化的应用）**： 虽然论文的应用场景是“神经架构设计”（NAS），属于特定领域的应用，但根据第四步的规则，只要论文的核心是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。本文的重点在于证明LLM如何通过反馈循环“从记忆走向创造”，其演化机制具有通用性和研究价值，而非单纯的应用落地。 综上所述，该论文在自我演化和智能体框架构建上具有显著贡献，符合研究课题要求。",
                    "summary2": "本文旨在探索LLM能否通过迭代微调成为自主的神经网络架构设计师。针对CIFAR-10图像分类任务，我们提出了一种基于闭环合成框架的方法，结合了MinHash-Jaccard新颖性过滤和低保真性能信号（单轮准确率）。在22个微调周期中，我们使用DeepSeek-Coder-7B-Instruct-v1.5模型，通过有效生成率、首轮准确率和结构新颖性等指标验证了其有效性。结果显示模型性能显著提升，且保持了结构多样性。",
                    "summary_translation": "大型语言模型在程序合成方面表现出色，但其在自主导航神经架构设计——即在句法可靠性、性能和结构新颖性之间取得平衡——方面的能力仍有待探索。我们通过将面向代码的LLM置于闭环合成框架中来解决这一问题，并分析了其在22个监督微调周期中的演变过程。该模型合成PyTorch卷积网络，这些网络经过验证，通过低保真性能信号（单轮准确率）进行评估，并使用MinHash-Jaccard标准进行过滤以防止结构冗余。高性能且新颖的架构被转换为提示-代码对，用于通过参数高效的LoRA适配进行迭代微调，该过程初始化自LEMUR数据集。在各个周期中，LLM内化了经验性架构先验，成为了一个鲁棒生成器。有效生成率稳定在50.6%（峰值达到74.5%），而平均首轮准确率从28.06%上升到50.99%，准确率超过40%的候选者比例从2.04%增长到96.81%。分析证实，该模型超越了复制现有模式的阶段，合成了455个原始语料库中不存在的高性能架构。通过将代码合成建立在执行反馈的基础上，这项工作提供了一个可扩展蓝图，用于将随机生成器转化为自主的、性能驱动的神经设计器，确立了LLM能够内化经验性的、非文本奖励从而超越其训练数据的结论。",
                    "inspiration_trace": "基于论文《From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures》，以下是对作者核心方法论产出过程的逻辑推演：\n\n### 第一阶段：宏观观察与问题定位\n**逻辑起点：LLM的代码能力与NAS的高昂成本之间的矛盾。**\n1.  **观察现状**：传统神经架构搜索（NAS）虽然能自动化设计网络，但依赖强化学习或进化算法，计算成本极高（通常需要训练大量模型）。\n2.  **技术契机**：大型语言模型（LLM）在代码合成方面表现出色，能够生成复杂的PyTorch代码。\n3.  **提出核心问题**：现有的LLM辅助NAS研究多将LLM视为静态的“代码生成器”，仅关注最终搜索结果的精度。作者试图转换视角——**如果将LLM视为一个可进化的“智能体”，通过自我迭代，它能否从单纯的代码生成者转变为具备设计直觉的“架构设计师”？**\n\n### 第二阶段：假设形成与关键挑战\n**逻辑推演：从“死记硬背”到“举一反三”的可能性。**\n1.  **核心假设**：如果让LLM在一个闭环中不断生成架构、接收反馈并基于自身的高质量产出进行微调，它将逐渐内化出一种“经验性的架构先验”，而不仅仅是记忆训练数据中的代码片段。\n2.  **面临的挑战**：\n    *   **可靠性**：生成的代码往往无法运行（语法错误、维度不匹配）。\n    *   **性能评估**：如果对每个生成的模型都进行完整训练，计算成本将不可接受。\n    *   **模式崩塌**：模型可能会陷入局部最优，反复生成相似的“高分”架构，失去探索新结构的能力。\n\n### 第三阶段：策略性简化与代理指标设计\n**逻辑转折：为了可行性，必须降低评估成本。**\n1.  **引入低保真代理**：为了解决计算成本问题，作者决定不追求模型的最终收敛精度，而是采用**“单轮训练后的准确率”**作为性能代理。\n    *   *思考逻辑*：一个优秀的架构通常在训练初期就能快速学习。这个指标既便宜又能反映架构的“学习潜力”。\n2.  **定义目标函数**：将优化目标从“最终精度”转化为“初始学习速度”与“语法有效性”的结合。\n\n### 第四阶段：闭环机制与多样性约束\n**逻辑构建：如何确保“进化”而非“退化”？**\n1.  **构建闭环**：设计了一个“生成-评估-筛选-微调”的迭代循环。\n    *   *生成*：LLM产出PyTorch代码。\n    *   *评估*：检查代码可执行性，并跑一轮训练获取代理分数。\n    *   *筛选*：这是关键步骤，必须同时满足三个条件：**代码可运行**、**代理分数达标**、**结构新颖**。\n2.  **引入新颖性过滤**：为了防止模型陷入重复生成已知好架构的陷阱（即过拟合），作者引入了**MinHash-Jaccard相似度**作为去重机制。\n    *   *思考逻辑*：只有那些“既好用又没见过”的架构，才有资格被加入下一轮的微调数据集。这迫使LLM不断探索设计空间的新区域，而不是在舒适区里打转。\n\n### 第五阶段：验证与结论\n**逻辑闭环：从假设到现实的映射。**\n1.  **实验验证**：通过22个周期的迭代，观察LLM的行为变化。\n2.  **结果解读**：\n    *   **可靠性提升**：有效生成率从低位稳定在50%以上，说明LLM学会了如何写“能跑的代码”。\n    *   **性能提升**：单轮准确率显著提高，说明LLM学会了什么样的结构更容易学习。\n    *   **创造力涌现**：发现了大量不在原始数据集中的高性能架构，证明了模型超越了简单的记忆，具备了设计能力。\n\n**总结：**\n作者的思考路径是从**利用LLM写代码**（工具属性），进化到**利用反馈机制训练LLM**（进化属性），最后通过**约束多样性**（探索属性），成功将一个通用的代码模型重塑为一个专用的、具备创造力的神经网络设计专家。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
                    "arxiv_id": "2601.02695",
                    "authors": "Guibin Zhang, Haiyang Yu, Kaiming Yang, Bingli Wu, Fei Huang, Yongbin Li, Shuicheng Yan",
                    "summary": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”方向。 1.  **核心贡献符合第一步判断**: 论文的核心贡献是提出了 **EvoRoute**，这是一种**自我演化**的模型路由范式。它不是将现有的智能体简单应用到某个垂直领域（非演化型应用），也不是单纯的基础设施优化，而是针对LLM智能体系统本身提出了一种新的架构和演化机制，旨在解决智能体系统中的性能、成本和延迟之间的权衡问题。 2.  **精准命中“自我演化”与“Agentic AI”焦点**: *   **自我演化**: 论文明确提到这是一个“self-evolving”范式，并且详细描述了其机制：利用不断扩大的先验经验知识库，通过环境反馈持续完善其自身的选择策略。这完全符合筛选标准中关于“智能体通过经验、反思或环境反馈进行自我完善和迭代”的定义。 *   **Agentic AI**: 论文的研究对象是“Complex agentic AI systems”，涉及LLM、工具和记忆模块的协调集成，这直接对应了研究课题中的LLM智能体构建。 3.  **符合正面指标**: 论文包含了大量核心关键词，如 `Self-Evolving`、`Agentic AI`、`LLM-based Agents`、`Iterative Improvement`（通过反馈持续完善）以及 `Tool and memory modules`。 4.  **排除标准检查**: 论文不涉及安全对齐、多模态视觉核心研究或图技术。虽然它关注了成本和延迟（通常属于基础设施范畴），但其解决手段是通过智能体的自我演化策略来实现的，因此属于智能体架构的改进，而非单纯的底层硬件或部署优化。 综上所述，该论文提出了一种新的智能体自我演化机制，属于构建和改进LLM智能体的前沿研究，应予以保留。",
                    "summary2": "本文旨在解决Agent System Trilemma，即性能、成本与效率之间的权衡问题。针对复杂的多轮LLM Agent系统，我们提出了一种名为EvoRoute的自进化模型路由范式，利用多方面检索和帕累托最优过滤动态选择LLM。我们在GAIA和BrowseComp+等基准上通过性能、成本和延迟指标验证了其有效性，在维持高性能的同时将成本降低了80%，延迟降低了70%。",
                    "summary_translation": "由大语言模型、工具和记忆模块的协调集成所驱动的复杂智能体 AI 系统，在复杂的多轮任务中展现出了卓越的能力。然而，这一成就伴随着高昂的经济成本和严重的延迟，从而揭示了一个关键但尚未被充分探索的权衡问题。我们将这一挑战形式化为 **Agent System Trilemma**（智能体系统三难困境）：即在实现最先进的性能、最小化货币成本以及确保快速任务完成这三者之间存在的内在张力。为了打破这一三难困境，我们提出了 EvoRoute，这是一种超越静态、预定义模型分配的自进化模型路由范式。EvoRoute 利用不断扩展的先验经验知识库，在每一步动态选择 Pareto-optimal（帕累托最优）的 LLM backbones（大语言模型骨干网络），在准确性、效率和资源使用之间取得平衡，同时通过环境反馈持续优化其自身的选择策略。在 GAIA 和 BrowseComp+ 等具有挑战性的智能体基准上进行的实验表明，当将 EvoRoute 集成到现成的智能体系统中时，它不仅保持甚至提升了系统性能，还将执行成本降低了高达 80%，延迟降低了超过 70%。",
                    "inspiration_trace": ""
                },
                {
                    "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
                    "arxiv_id": "2601.03204",
                    "authors": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
                    "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **InfiAgent**，这是一个用于通用自主智能体的**新框架**。 *   它解决的是 LLM 智能体在长视界任务中面临的上下文增长和误差累积问题，属于**构建和改进 LLM 智能体**的方法论研究。 *   它不是将现有智能体简单应用到特定领域（如医疗、金融），而是提出了底层的架构改进（状态外部化），因此不属于“非演化型应用”。 *   它关注的是智能体的系统架构和状态管理，而非模型的基础设施或硬件加速。 2.  **正面指标匹配（第二步）：** *   **核心范式**：明确属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文重点解决了智能体的 **`Memory`**（通过文件中心的状态抽象外部化持久状态）和 **`Planning`**（在长视界任务中保持推理稳定性）能力。这是单智能体研究中的关键子方向。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态或图技术，未触发任何排除标准。 4.  **特殊情况处理（第四步）：** *   论文关于长视界任务中的推理稳定性，属于智能体如何在复杂任务中进行规划和多步推理的范畴，符合保留条件。 **总结**：该论文通过引入显式的状态外部化机制，改进了 LLM 智能体处理长视界任务的能力，是对智能体架构和记忆机制的重要创新，直接契合“单智能体”方向中的规划与记忆研究焦点。",
                    "summary2": "本文旨在解决LLM agents在长视界任务中因上下文无界增长导致的稳定性问题。针对长视界推理场景，我们提出了一种InfiAgent框架，采用File-Centric State Abstraction将持久状态外部化，并结合分层代理架构保持上下文有界。在DeepResearch benchmark和80篇论文综述任务上，通过综合评分和覆盖率指标验证了其有效性，证明其能以20B开源模型达到与大型专有系统相当的性能。",
                    "summary_translation": "LLM agents (大语言模型智能体) 虽然具备推理和使用工具的能力，但在长视界任务中，往往因上下文的无界增长和误差累积而失效。常见的解决方案，如上下文压缩或 retrieval-augmented prompting (检索增强提示)，往往在信息保真度与推理稳定性之间引入了权衡。我们提出了 InfiAgent，这是一个通用框架，通过将持久状态外部化到 file-centric state abstraction (以文件为中心的状态抽象) 中，无论任务持续时间长短，都能将智能体的推理上下文严格限制在固定范围内。在每一步中，智能体通过结合 workspace state snapshot (工作区状态快照) 和近期操作的固定窗口来重构上下文。在 DeepResearch 数据集和一项包含 80 篇论文的文献综述任务上的实验表明，无需进行 task-specific fine-tuning (针对特定任务的微调)，配备 20B 开源模型的 InfiAgent 的性能可与更大的专有系统相媲美，并且相比 context-centric baselines (以上下文为中心的基线方法)，其长视界覆盖率显著更高。这些结果验证了 explicit state externalization (显式状态外部化) 作为构建稳定长视界智能体的实用基础的可行性。\n\nGithub 代码库：https://github.com/ChenglinPoly/infiAgent",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-05": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
                    "arxiv_id": "2601.01569",
                    "authors": "Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sihui Han, Bo An, Yike Guo, Jun Song",
                    "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断**: *   论文的核心贡献是提出了 **CaveAgent**，这是一个新的 **LLM智能体框架**。它旨在解决现有智能体系统在处理长时程任务时面临的上下文漂移和脆弱依赖问题。 *   这不是一篇将现有智能体简单应用到特定领域的应用论文，而是对智能体底层架构和运行机制的改进。 *   它不属于基础设施优化或非Agentic的基础推理研究。 2.  **正面指标匹配**: *   **Agentic AI / LLM-based Agents**: 论文明确致力于改进基于LLM的智能体系统。 *   **Memory (记忆)**: 论文的核心创新点之一是 **Stateful Runtime Management**（有状态运行时管理）。通过注入、操作和检索持久的Python对象（如DataFrames），它构建了一个高保真的 **外部记忆** 机制，以消除上下文漂移和灾难性遗忘。这直接对应了单智能体研究中的“记忆”能力。 *   **Tool Use / Tool Augmentation**: 框架利用代码生成作为工具，通过Python Runtime流来执行任务，超越了传统的JSON函数调用，属于工具使用能力的增强。 *   **Planning (规划)**: 论文提到利用代码生成高效解决相互依赖的子任务（如循环、条件判断），这涉及智能体在复杂任务中的多步规划和执行能力。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 **结论**: 该论文通过引入双流上下文架构和有状态运行时管理，显著增强了LLM智能体的记忆保持和任务执行能力，是对单智能体架构的重要改进，完全符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决传统基于JSON的LLM Agent在长周期任务中面临的上下文漂移和效率低下问题。针对复杂的工具调用和数据处理场景，我们提出了一种名为CaveAgent的框架，采用双流上下文架构和有状态运行时管理，实现对象级交互。我们在Tau 2-bench、BFCL及数据密集型任务上通过成功率和Token消耗验证了其有效性，显著提升了任务表现并降低了成本。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
                    "arxiv_id": "2601.01885",
                    "authors": "Yi Yu, Liuyi Yao, Yuexiang Xie, Qingquan Tan, Jiaqi Feng, Yaliang Li, Libing Wu",
                    "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 \"Agentic Memory (AgeMem)\" 的新框架，旨在解决LLM智能体在长程推理中的记忆管理问题。这属于对LLM智能体内部机制的构建和改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型能力的提升。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确属于 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**：论文的核心焦点是 `Memory`（记忆），这是智能体的关键能力之一。同时，它将记忆操作（存储、检索、更新等）定义为 `Tool Use / Tool Augmentation`（基于工具的动作），使智能体能自主决策。 *   **演化机制**：论文提出了一种三阶段渐进式强化学习（RL）策略来训练智能体的记忆管理策略，这涉及智能体通过环境反馈进行 `Self-Improvement`（自我完善）和策略优化。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉核心或图技术，因此不在排除之列。 综上所述，该论文通过引入统一的记忆管理框架和强化学习训练方法，显著增强了LLM智能体的自主性和适应性，是对Agentic AI核心能力的直接贡献，因此予以保留。",
                    "summary2": "本文旨在解决LLM智能体在长视界推理中因有限上下文窗口导致的记忆管理限制问题。针对LTM和STM分离且依赖启发式规则的现状，我们提出了一种名为Agentic Memory (AgeMem)的统一框架，通过基于工具的动作接口实现端到端的记忆管理，并采用三阶段渐进式RL策略进行训练。在ALFWorld、SciWorld等五个长视界基准测试上，通过Success Rate和Memory Quality等指标验证了其有效性，显著提升了任务性能和记忆质量。",
                    "summary_translation": "大语言模型智能体由于上下文窗口有限，在长程推理方面面临根本性局限，这使得有效的记忆管理变得至关重要。现有方法通常将长期记忆和短期记忆视为独立的组件进行处理，并依赖于启发式方法或辅助控制器，这限制了系统的适应性和端到端优化能力。在本文中，我们提出了 Agentic Memory (AgeMem，智能体记忆)，这是一个将长期记忆和短期记忆管理直接整合到智能体策略中的统一框架。AgeMem 将记忆操作呈现为基于工具的动作，使大语言模型智能体能够自主决定存储、检索、更新、总结或丢弃何种信息以及何时执行这些操作。为了训练这种统一行为，我们提出了一种三阶段渐进式强化学习策略，并设计了分步 GRPO，以解决由记忆操作引起的奖励稀疏和不连续问题。在五个长程基准测试上的实验表明，AgeMem 在多种大语言模型骨干网络上始终优于强大的记忆增强基线模型，实现了更优的任务性能、更高质量的长期记忆以及更高效的上下文利用率。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-02": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "An Agentic Framework for Neuro-Symbolic Programming",
                    "arxiv_id": "2601.00743",
                    "authors": "Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi",
                    "summary": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合 Agentic AI 范畴**：论文明确提出了一个名为 \"AgenticDomiKnowS (ADS)\" 的框架，其核心在于构建一个基于 LLM 的智能体工作流。这完全符合第一步中关于“构建 LLM 智能体（Agentic LLM）的方法论或新框架”的保留标准。 2.  **具备单智能体核心能力**：论文描述的智能体工作流涉及将自由形式的任务描述分解并转化为具体的程序组件，这体现了智能体的**规划**能力；同时，工作流包含“创建和测试”组件的过程，体现了**工具使用**和执行能力。这些均属于第二步正面指标中的“单智能体”核心关注点。 3.  **不属于排除项**： *   虽然论文涉及“神经符号编程”这一特定领域，但其核心贡献并非仅仅是应用 LLM 解决该领域问题，而是提出了一种通用的智能体框架来辅助编程，因此不属于“非演化型应用”。 *   尽管神经符号编程可能涉及逻辑或图结构，但论文的研究焦点是智能体框架本身，而非图神经网络或知识图谱算法的改进，因此不触犯“图”相关的排除标准。 *   论文不涉及安全、对齐或多模态视觉等排除领域。 综上所述，该论文的核心在于提出一个新的 Agentic 框架来解决复杂的编程构建任务，符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在解决神经符号编程门槛高、编写困难的问题。针对自然语言任务描述，我们提出了一种 AgenticDomiKnowS (ADS) 智能体框架，采用多智能体协作与RAG检索的分阶段工作流生成代码。在涵盖NLP、视觉和CSP的12个数据集及人类评估实验中，通过图生成正确率和开发时间验证了其有效性，显著提升了开发效率。",
                    "summary_translation": "将 symbolic constraints（符号约束）集成到 deep learning models（深度学习模型）中，能够提升模型的鲁棒性、可解释性以及数据效率。然而，这一过程依然耗时且充满挑战。现有框架（如 DomiKnowS）通过提供 high-level declarative programming interface（高级声明式编程接口）辅助这一集成过程，但它们仍预设用户已熟练掌握该库的特定语法。为消除这一依赖，我们提出了 AgenticDomiKnowS (ADS)。ADS 利用一种 agentic workflow（智能体工作流），将 free-form task descriptions（自由形式的任务描述）转化为完整的 DomiKnowS 程序，该工作流会对每个 DomiKnowS component（组件）分别进行创建和测试。该工作流支持可选的 human-in-the-loop intervention（人在回路干预），使熟悉 DomiKnowS 的用户能够优化 intermediate outputs（中间输出）。我们展示了 ADS 如何使经验丰富的 DomiKnowS 用户及非用户均能快速构建 neuro-symbolic programs（神经符号程序），将开发时间从数小时缩短至 10-15 分钟。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-31": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
                    "arxiv_id": "2512.24613",
                    "authors": "Zheyu Shi, Dong Qiu, Shanlong Yu",
                    "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心贡献符合多智能体系统 (Multi-Agent)**: 论文的核心是提出了一种“面向群体商议的多智能体对话模型”。它构建了一个包含三个不同角色的智能体架构：意见生成智能体、证据验证智能体和一致性仲裁智能体。这直接对应了我筛选标准中的“多智能体”方向，特别是智能体间的协作、通信和角色分工。 2.  **包含自我演化机制**: 论文中明确引入了“自我博弈机制”来扩展多路径推理轨迹，并设计了复合奖励函数结合改进的近端策略优化（PPO）策略进行协作训练。这种通过反馈和强化学习进行迭代优化的过程，符合“自我演化”中关于智能体通过环境反馈进行自我完善和迭代的标准。 3.  **具备智能体关键能力**: 论文模型包含检索增强模块，体现了“工具使用”能力；同时，其通过多智能体商议解决复杂推理任务的过程，涉及了复杂的“规划”和“多步推理”。 4.  **排除标准检查**: 该论文不涉及安全对齐、多模态视觉或图技术等排除项。虽然论文在HotpotQA等数据集上进行实验，但其核心贡献在于提出了新的智能体框架和训练机制，而非单纯将现有模型应用于特定领域。 综上所述，该论文在构建多智能体协作框架及引入自我博弈演化机制方面做出了实质性贡献，符合筛选要求。",
                    "summary2": "本文旨在优化单一语言模型在复杂推理任务中的局限性。针对多跳问答等场景，我们提出了一种Group Deliberation Multi-Agent Dialogue Model，构建“Generation-Verification-Integration”三级角色分工架构，引入Self-game mechanism和Retrieval Enhancement Module。在HotpotQA、2WikiMultihopQA和MeetingBank数据集上，通过Multi-hop reasoning accuracy和Consistency index验证了其有效性，显著提升了推理准确性与一致性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory",
                    "arxiv_id": "2512.24684",
                    "authors": "Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu",
                    "summary": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**：论文明确提出了 R-Debater，这是一个用于生成多轮辩论的 **\"agentic framework\"**（智能体框架）。其核心贡献在于构建了一个结合检索增强和论证记忆的智能体架构，而非仅仅将现有模型作为工具简单应用于辩论领域。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合研究焦点**：该研究完全符合 **单智能体** 方向。 *   **记忆**：论文的核心创新点是 \"Argumentative Memory\"（论证记忆），使智能体能够回忆和调整先前的论点。 *   **工具使用**：集成了辩论知识库进行检索，作为智能体获取证据的工具。 *   **规划**：摘要中提到 \"structured planning\"（结构化规划），用于在多轮对话中保持立场一致性和连贯性。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然应用场景是辩论，但其重点在于智能体如何通过记忆和规划机制来完成任务，属于智能体能力的构建，而非单纯的非演化型应用。 综上所述，该论文提出了新的智能体框架并深入探讨了记忆与规划机制，符合筛选要求。",
                    "summary2": "本文旨在解决LLM在多轮辩论生成中缺乏事实依据、立场一致性差及跨轮次连贯性弱的问题。针对ORCHID数据集上的多轮辩论场景，我们提出了一种名为R-Debater的框架，该框架集成了检索增强生成与基于角色的智能体，通过检索和重构论证记忆来生成高质量辩论。我们在ORCHID数据集上通过InspireScore和Debatrix指标验证了其有效性，实验表明其在逻辑连贯性和事实准确性上显著优于强基线模型。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-29": [
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
                    "arxiv_id": "2512.22733",
                    "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
                    "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心方法论研究。 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **FoldAct** 这一新框架，旨在解决 **Long-horizon search agents**（长视界搜索智能体）在强化学习训练中面临的上下文无限增长和非平稳观测分布问题。 *   这属于 **构建和改进 LLM 智能体** 的范畴。它关注的是智能体如何处理记忆（上下文折叠）以及如何在长任务中保持稳定性和效率，这是智能体架构和训练机制的关键部分，而非简单的应用或基础设施优化。 2.  **正面指标匹配（第二步）：** *   **核心范式**：论文明确针对 **LLM-based Agents**（特别是搜索智能体）。 *   **智能体能力**：论文重点解决了智能体的 **Memory**（通过上下文折叠压缩交互历史）和 **Planning/Search**（长视界搜索）能力。它提出的“分离损失计算”和“全上下文一致性损失”是为了优化智能体的训练过程，使其具备更好的长程任务处理能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了计算效率（5.19倍加速），但这属于算法层面的优化（选择性片段训练），而非硬件或部署基础设施的研究。 4.  **特殊情况处理（第四步）：** *   论文涉及 **Reasoning/Planning**（长视界搜索），且其核心是改进智能体在执行过程中的状态管理和训练稳定性，符合保留条件。 综上所述，FoldAct 提出了一种改进智能体记忆机制和训练稳定性的新方法，直接贡献于 LLM 智能体的构建与优化，因此被保留。",
                    "summary2": "本文旨在解决长视距强化学习中上下文折叠导致的非平稳观测分布及训练不稳定问题。针对长视距搜索代理场景，我们提出了一种FoldAct框架，通过分离损失计算、全上下文一致性损失和选择性片段训练来优化上下文折叠。在Local RAG和Web Search基准上，通过F1、EM及Pass@1等指标验证了其有效性，并实现了5.19倍的训练加速。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Memento-II: Learning by Stateful Reflective Memory",
                    "arxiv_id": "2512.22716",
                    "authors": "Jun Wang",
                    "summary": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的核心研究。 1.  **核心贡献符合第一步判断**： 论文的核心贡献是提出了一种名为“Memento-II”的理论框架，旨在解决LLM智能体的持续学习和经验学习问题。这直接对应了您筛选条件中的“构建、改进或演化 LLM智能体”以及“自我演化”的方法论。它不是将智能体作为工具应用到特定领域，而是研究智能体本身如何学习和适应的底层机制。 2.  **高度匹配正面指标**： *   **自我演化**：论文明确提出了“continual and experiential learning”（持续和经验学习），并强调智能体可以在“without back propagation or model fine tuning”（无需反向传播或模型微调）的情况下通过交互进行适应。这是典型的自我演化特征，即通过经验而非参数更新来迭代改进。 *   **智能体能力**：论文重点研究了“Memory”（情景记忆）和“Self-Reflection”（反思），将其作为智能体适应环境的关键机制。这直接命中了您关注的核心能力列表。 *   **核心范式**：该研究属于“LLM-based Agents”和“Agentic AI”范畴，特别是关于“Retrieval-based language model agents”（基于检索的语言模型智能体）。 3.  **不涉及排除标准**： 论文主要关注智能体的学习机制和理论框架，不涉及安全对齐、多模态视觉处理或图神经网络等排除领域。 综上所述，该论文通过引入“有状态反思决策过程”和结合强化学习，为LLM智能体在不更新模型参数的情况下实现自我完善和持续适应提供了理论基础，精准契合您关于“自我演化”和“单智能体”的研究目标。",
                    "summary2": "本文旨在解决LLM智能体无需微调即可实现持续学习的问题。针对开放式长视野任务，我们提出了一种Stateful Reflective Decision Process (SRDP) 框架，通过Read-Write Reflective Learning机制将 episodic memory 与 soft policy iteration 相结合。我们在软件测试、自动化数据科学和深度研究代理等场景上，通过理论收敛性证明和实证表现验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
                    "arxiv_id": "2512.22322",
                    "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
                    "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心贡献符合构建与改进 LLM 智能体的目标**： 论文提出了 **SmartSnap** 这一新范式，并定义了一种新型的 **Self-Verifying Agent（自验证智能体）**。这不仅仅是应用现有智能体去解决特定问题，而是对智能体架构和机制的实质性改进。它解决了 Agentic RL 中任务验证的瓶颈问题，将验证从“被动、事后”转变为“主动、原位”。 2.  **符合“单智能体”与“自我反思”的核心关注点**： 论文的核心在于赋予智能体双重使命：完成任务 + 证明完成。这直接对应筛选标准中的 **自我反思** 和 **自我修正** 能力。智能体通过主动寻找证据来验证自身行为，这是一种高级的元认知能力，属于 Agentic AI 的关键能力范畴。 3.  **不属于排除项**： *   虽然论文在 GUI 和移动任务上进行了实验，但其核心贡献是通用的“自验证”机制，而非针对特定领域的应用解决方案，因此不属于“非演化型应用”。 *   虽然涉及 GUI，但视觉仅作为智能体感知环境的一部分，并非研究视觉模型本身，因此不违反多模态排除规则。 *   论文关注的是智能体的任务完成验证机制，而非安全对齐或基础设施优化。 综上所述，该论文通过提出新的智能体框架来增强智能体的自主性和可靠性，是关于 LLM 智能体构建与演化的高质量研究。",
                    "summary2": "本文旨在解决Agentic RL中被动验证成本高且不可靠的瓶颈。针对复杂的GUI任务，我们提出了一种SmartSnap范式，引入了基于3C原则的Self-Verifying Agent，使其主动收集并提交关键快照证据。我们在AndroidLab基准上通过Success Rate等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-26": [
        {
            "name": "Computation and Language",
            "count": 2,
            "papers": [
                {
                    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
                    "arxiv_id": "2512.21919",
                    "authors": "KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He",
                    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合第一步（核心判断）**： *   论文的核心贡献是提出了 **SWE-RM**，一种用于软件工程智能体的奖励模型。 *   它不是简单地将智能体作为工具应用，而是致力于**改进智能体本身**的反馈机制。论文旨在解决现有基于执行的反馈（如单元测试）存在的稀疏性和依赖性问题，提出了一种免执行的反馈机制。 2.  **高度契合第二步（正面指标）与第三步（自我演化）**： *   **自我演化机制**：论文明确指出该模型旨在支持 **Reinforcement Learning (RL)** 和 **Test-time scaling (TTS)**。RL 是智能体通过环境反馈进行自我完善和迭代的核心技术，属于典型的“自我演化”范畴。TTS 则涉及智能体在推理时的搜索和规划。 *   **智能体能力**：论文讨论了如何通过奖励模型提供更细粒度的信号，帮助智能体进行 **Self-Correction**（自我修正）和 **Trajectory Selection**（轨迹选择，即规划的一部分）。 3.  **符合第四步（特殊和模糊情况）**： *   虽然论文的应用场景是软件工程（SWE），但其核心在于提出一种新的“反馈/奖励机制”，这种机制是智能体实现自我演化和改进的关键组件。根据第四步中关于“自我演化的应用”的例外规则，只要核心是提出新的演化/改进机制，即使应用在特定领域，也应保留。 综上所述，SWE-RM 提供了一种让智能体通过更高质量的反馈进行自我学习和优化的新方法，直接贡献于 LLM 智能体的演化能力，因此予以保留。",
                    "summary2": "本文旨在解决软件工程代理中基于执行的反馈稀疏且不可靠的问题。针对SWE任务中的多轮轨迹，我们提出了一种名为SWE-RM的免执行奖励模型，该模型采用混合专家架构，并强调AUC和校准能力。我们在SWE-Bench Verified数据集上通过Resolve Rate、AUC和ECE验证了其有效性。实验表明，SWE-RM在测试时扩展和强化学习中均显著提升了代理性能，达到了开源模型中的SOTA水平。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management",
                    "arxiv_id": "2512.21567",
                    "authors": "Changzhi Sun, Xiangyu Chen, Jixiang Luo, Dell Zhang, Xuelong Li",
                    "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 DAM (Decision-theoretic Agent Memory) 的框架，用于解决 LLM 智能体中的“记忆管理”问题。这直接属于“构建、改进 LLM 智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或基础模型推理能力的提升。 2.  **命中核心关注点 (第二步)**: *   **单智能体能力**: 论文明确聚焦于智能体的 **Memory (记忆)** 机制。记忆是您列出的单智能体核心能力之一（规划、记忆、工具使用、自我反思）。 *   **Agentic AI**: 论文将记忆管理视为“不确定性下的序列决策问题”，这属于 Agentic AI 的核心方法论，旨在提升智能体在长期交互中的表现。 3.  **无排除项 (第三步)**: 论文不涉及安全与对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **综合结论**: 该论文致力于改进 LLM 智能体的关键组件（记忆），提出了新的决策理论框架来替代传统的启发式方法，从而提升智能体的长期交互能力。这完全符合您的研究课题中关于“单智能体”及其能力演化的方向。",
                    "summary2": "总结生成失败",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-24": [
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "Reflection-Driven Self-Optimization 6G Agentic AI RAN via Simulation-in-the-Loop Workflows",
                    "arxiv_id": "2512.20640",
                    "authors": "Yunhao Hu, Xinchen Lyu, Chenshan Ren, Keda Chen, Qimei Cui, Xiaofeng Tao",
                    "summary": "The escalating complexity of sixth-generation (6G) networks demands unprecedented levels of autonomy beyond the capabilities of traditional optimization-based and current AI-based resource management approaches. While agentic AI has emerged as a promising paradigm for autonomous RAN, current frameworks provide sophisticated reasoning capabilities but lack mechanisms for empirical validation and self-improvement. This article identifies simulation-in-the-loop validation as a critical enabler for truly autonomous networks, where AI agents can empirically verify decisions and learn from outcomes. We present the first reflection-driven self-optimization framework that integrates agentic AI with high-fidelity network simulation in a closed-loop architecture. Our system orchestrates four specialized agents, including scenario, solver, simulation, and reflector agents, working in concert to transform agentic AI into a self-correcting system capable of escaping local optima, recognizing implicit user intent, and adapting to dynamic network conditions. Extensive experiments validate significant performance improvements over non-agentic approaches: 17.1\\% higher throughput in interference optimization, 67\\% improved user QoS satisfaction through intent recognition, and 25\\% reduced resource utilization during low-traffic periods while maintaining service quality.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“多智能体”方向**： 论文的核心贡献是提出了一种“反思驱动的自我优化框架”，这直接对应研究焦点中的“自我演化”和“自我反思”。论文构建了一个包含四个专门智能体（场景、求解器、模拟、反思智能体）的闭环架构，旨在通过经验验证和反思机制实现系统的自我修正和迭代改进，这属于构建新的Agentic框架和方法论。 2.  **符合“自我演化的应用”例外规则**： 根据筛选标准第四步第2点，虽然论文的应用领域是特定的6G网络（RAN），但其核心在于提出了一种新的“自我演化”机制（Simulation-in-the-Loop + Reflection-driven），而不仅仅是将现有智能体作为工具应用。因此，符合“保留”的例外条件。 3.  **包含关键正面指标**： 论文明确涉及了 `Multi-Agent Systems`（多智能体协作）、`Self-Reflection`（反思智能体）、`Self-Correction`（自我修正）和 `Self-Optimization`（自我优化）等核心关键词。 综上所述，该论文在特定领域（6G）中验证了具有自我演化能力的多智能体框架，符合关于“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决6G网络中现有Agentic AI缺乏经验验证与自我改进机制的问题。针对多小区RAN场景，我们提出了一种Reflection-Driven Self-Optimization Framework，通过Simulation-in-the-Loop工作流协调Scenario、Solver、Simulation和Reflector四个Agent。我们在Sionna平台上通过吞吐量、QoS满意度和资源利用率等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-23": [
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
                    "arxiv_id": "2512.20111",
                    "authors": "Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr",
                    "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”和“自我演化”方向的交叉研究。具体判断依据如下： 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种名为 ABBEL 的新框架，旨在解决 LLM 智能体在长序列决策任务中的上下文管理问题。这属于构建和改进 LLM 智能体方法论的范畴，而非简单的应用或基础设施研究。 2.  **高度契合核心关注点 (第二步)**: *   **Agentic AI & 单智能体**: 论文明确研究 LLM Agents，涉及智能体如何通过“信念状态”来处理多步交互、环境观察和行动选择，这是典型的智能体规划与记忆机制研究。 *   **自我演化**: 论文不仅提出了框架，还引入了强化学习（RL）后训练机制来改进智能体生成信念和采取行动的能力。这种通过反馈（RL奖励）进行迭代优化和自我完善的过程，符合“自我演化”的定义。 3.  **不涉及排除项 (第三步)**: 论文主要关注智能体的架构优化和性能提升，不涉及安全对齐、多模态视觉核心研究或图神经网络等排除领域。 综上所述，ABBEL 提出了一种改进智能体记忆和决策机制的方法，并结合 RL 进行自我优化，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决长序列决策任务中上下文长度超出限制的问题。针对长视距交互场景，我们提出了一种基于自然语言信念瓶颈的ABBEL框架，通过维护简洁的信念状态替代完整交互历史，并利用强化学习进行后训练优化。我们在Wordle、ColBench等六个多步环境中通过Success Rate和Peak Token Usage验证了其有效性，实现了在保持高性能的同时显著降低内存使用。",
                    "summary_translation": "随着序列决策任务长度的增加，在上下文中保留完整的交互历史在计算上变得不可行。我们介绍了一个通用框架，用于 LLM（大语言模型）智能体通过多步交互保持简洁的上下文：Acting through Belief Bottlenecks Expressed in Language (ABBEL，基于语言表达的信念瓶颈行动)，以及利用 RL post-training（强化学习后训练）进一步改进 ABBEL 智能体的方法。ABBEL 用 belief state（信念状态）替换了冗长的多步交互历史，即关于任务相关未知因素已发现内容的自然语言摘要。在 ABBEL 框架下，智能体在每一步首先利用来自环境的最新观测更新 prior belief（先验信念）以形成 posterior belief（后验信念），然后仅使用后验信念来选择动作。我们在六个不同的多步环境中系统地评估了 ABBEL 下的 frontier models（前沿模型），发现 ABBEL 支持生成可解释的信念，同时在交互步骤中保持近乎恒定的内存使用。然而，bottleneck approaches（瓶颈方法）通常容易受到 error propagation（错误传播）的影响，我们观察到由于信念更新中的错误，这导致与 full context setting（全上下文设置）相比性能较差。因此，我们通过 reinforcement learning (RL，强化学习) 训练 LLM 在 ABBEL 框架内生成信念并基于信念行动。我们尝试了 belief grading（信念评分）以奖励更高质量的信念，以及 belief length penalties（信念长度惩罚）以奖励更压缩的信念。我们的实验证明了 RL 将 ABBEL 的性能提升至超越全上下文设置的能力，同时使用的内存少于 contemporaneous approaches（同期方法）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-22": [
        {
            "name": "Artificial Intelligence",
            "count": 2,
            "papers": [
                {
                    "title": "Sophia: A Persistent Agent Framework of Artificial Life",
                    "arxiv_id": "2512.18202",
                    "authors": "Mingyang Sun, Feng Hong, Weinan Zhang",
                    "summary": "The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a \"Persistent Agent\" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心判断（第一步）：** 论文的核心贡献是构建了一个名为 Sophia 的“持久智能体框架”，并提出了“System 3”这一新的架构层级。这属于构建和改进 LLM 智能体的方法论，而非将现有智能体简单应用于特定领域（如医疗、金融），因此符合保留条件。 2.  **正面指标匹配（第二步）：** *   **Agentic AI / 单智能体：** 论文详细探讨了智能体的规划、记忆（Narrative Memory）、自我建模以及长期适应性，这些都是单智能体研究的核心能力。 *   **自我演化：** 摘要中明确提到该框架包含“continuous self-improvement loop”（连续自我改进循环）和“long-horizon adaptation”（长期适应性），这直接对应了研究焦点中的“自我演化”机制。 *   **核心范式：** 论文涉及 `Self-Reflection`（通过 System 3 验证推理）、`Memory` 和 `Iterative Improvement`。 3.  **排除标准检查（第三步）：** 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此未被排除。 4.  **特殊处理（第四步）：** 论文虽然涉及推理，但其重点在于通过智能体架构（System 3）来实现持久性和自我驱动的推理过程，而非单纯优化 LLM 的基础 Token 预测能力或数学逻辑，因此属于 Agentic 的推理范畴，应予保留。 综上所述，该论文提出了一个新的智能体框架来解决智能体的身份持久性和自我完善问题，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有AI代理缺乏长期适应性和自我改进能力的问题。针对动态网络环境，我们提出了一种名为Sophia的持久化代理框架，该框架引入了包含元认知、心智理论等模块的System 3元认知层。在36小时的浏览器沙盒实验环境中，通过复杂任务成功率提升及重复操作推理步骤减少80%等指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Propose, Solve, Verify: Self-Play Through Formal Verification",
                    "arxiv_id": "2512.18160",
                    "authors": "Alex Wilf, Pranjal Aggarwal, Bryan Parno, Daniel Fried, Louis-Philippe Morency, Paul Pu Liang, Sean Welleck",
                    "summary": "Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“多智能体”的核心研究范畴。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 \"Propose, Solve, Verify\" (PSV) 框架。这是一个基于自我博弈的训练框架，旨在通过形式验证信号来训练模型。这不仅仅是将LLM作为工具应用，而是提出了一种新的**训练和演化方法论**。 2.  **符合核心关注点 (第二步)**: *   **自我演化**: 论文明确使用了 \"Self-Play\"（自我博弈）、\"Expert Iteration\"（专家迭代）和 \"Generational Evolution\"（隐含在迭代训练中）的概念。模型通过生成问题、解决问题和验证结果的循环，实现了性能的自我提升和迭代。 *   **多智能体**: 框架中包含两个核心角色——\"Proposer\"（提议者，负责生成合成问题）和 \"Solver\"（求解器，负责解决问题）。这两个角色通过交互和对抗/协作（生成难题 vs 解决难题）共同进化，符合多智能体系统的定义。 3.  **特殊与模糊情况处理 (第四步)**: 虽然论文的应用场景是代码生成，但其核心在于提出了一种新的“自我演化”机制（Self-Play through Formal Verification）。根据筛选标准第四步中的“自我演化的应用”规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。因此，尽管涉及代码领域，由于其核心贡献在于演化框架本身，应当保留。 综上所述，该论文聚焦于通过自我博弈和专家迭代机制实现LLM的自我演化，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决代码生成中自博弈奖励信号不可靠的问题。针对形式化验证代码生成场景，我们提出了一种名为PSV的自博弈框架，利用形式化验证器构建包含难度感知提议器的自博弈循环，并在Dafny2Verus、MBPP-Verified和HumanEval-Verified数据集上通过Pass@k指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 2,
            "papers": [
                {
                    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
                    "arxiv_id": "2512.19682",
                    "authors": "Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang",
                    "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **GenEnv** 这一新框架，旨在解决训练LLM智能体时数据成本高且静态的问题。其本质是构建一种**智能体与环境模拟器之间的协同演化机制**，这直接属于“构建、改进或演化 LLM智能体”的方法论研究，而非将现有智能体简单应用到特定领域。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文明确涉及 `LLM-based Agents` 和 `Self-Evolving`（特别是 `Co-Evolution` 协同演化）。 *   **演化机制**：GenEnv 建立了一个动态的课程策略，通过 `Co-Evolutionary Game`（协同演化博弈）让环境根据智能体的能力生成任务，这完全符合“自我演化”和“迭代改进”的研究焦点。 *   **智能体能力**：虽然侧重于训练数据生成，但其目的是提升智能体在 `ALFWorld`、`API-Bank` 等需要 `Planning`（规划）和 `Tool Use`（工具使用）的基准测试上的表现。 3.  **排除标准（无冲突）**： 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此不触犯任何排除规则。 4.  **特殊与模糊情况（符合）**： 论文提出的“协同演化”机制属于典型的自我演化范畴。尽管它在多个基准（如TravelPlanner）上进行了评估，但这些仅用于验证智能体能力的提升，论文的核心在于提出了一种新的**演化训练框架**，而非单纯的应用。 综上所述，该论文通过提出智能体与环境的协同演化框架来提升智能体能力，精准契合“LLM智能体及其演化”这一研究课题。",
                    "summary2": "本文旨在解决LLM智能体训练中真实交互数据成本高且静态分布导致的瓶颈。针对智能体训练场景，我们提出了一种名为GenEnv的难度对齐协同进化框架，通过$\\alpha$-Curriculum Reward引导环境模拟器动态生成适配智能体当前能力的任务。我们在API-Bank、ALFWorld等五个基准测试上通过成功率等指标验证了其有效性，该方法显著提升了7B基线模型性能，并展现出优于静态数据增强的高数据效率。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
                    "arxiv_id": "2512.18552",
                    "authors": "Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang",
                    "summary": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文提出了“Self-play SWE-RL (SSR)”这一新框架，其核心在于通过**自我对弈**和**强化学习**来训练软件智能体。这直接对应了研究课题中的“自我演化”方向，特别是智能体通过环境反馈（测试补丁）进行自我完善和迭代改进的能力。 2.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是软件工程，但根据筛选标准第四步，只要论文的核心贡献是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。本文的核心不在于单纯解决某个具体的编程问题，而在于提出了一种让智能体**自主收集经验**、**自我注入并修复Bug**的训练范式，这属于智能体演化机制的创新。 3.  **具备明确的Agentic特征**： 论文明确研究对象是“Software Agents”，并使用了“Agentic reinforcement learning”方法。智能体在沙盒环境中进行多步交互（注入Bug、修复Bug），体现了智能体的自主性和工具使用能力。 综上所述，该论文不仅涉及LLM智能体的构建，更重点解决了智能体如何通过自我对弈实现能力演化和自我提升，高度契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在突破软件智能体依赖人类知识数据的限制，迈向超智能系统。针对仅需沙箱化代码库的最小数据假设场景，我们提出了一种 Self-play SWE-RL (SSR) 方法，通过自我博弈让同一 LLM 智能体交替扮演 Bug 注入者和求解者以迭代生成和修复复杂 Bug。我们在 SWE-bench Verified 和 SWE-Bench Pro 上通过 Resolve rate 验证了其有效性，结果表明该方法能实现持续自我改进并优于人类数据基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
                    "arxiv_id": "2512.18950",
                    "authors": "Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili",
                    "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”方向的交叉研究。 1.  **核心判断**: *   论文提出了 **MACLA** 框架，其核心贡献在于构建了一种新的 **LLM智能体** 架构。该架构通过解耦推理与学习，利用外部分层程序记忆来增强智能体的能力。 *   这不是简单的应用型论文，而是提出了关于智能体如何存储记忆、如何从经验中学习以及如何自我完善的方法论。 2.  **符合核心关注点**: *   **单智能体**: 论文专注于提升单个智能体的能力，特别是 **Memory (记忆)** 机制的构建。它提出了“分层程序记忆”，这是智能体研究中的关键组件。 *   **自我演化**: 论文明确提到了“continually improving agents”（持续改进的智能体）。其核心机制包括从轨迹中提取可复用程序、通过 **Contrastive Refinement (对比精炼)** 来修正程序，以及利用贝叶斯选择跟踪可靠性。这完全符合“自我演化”中定义的“通过经验、反思或环境反馈进行自我完善和迭代”的标准。 *   **正面指标匹配**: 涉及 `Memory`, `Self-Correction` (通过对比精炼实现), `Iterative Improvement`。 3.  **排除标准检查**: *   论文不涉及安全对齐、多模态视觉核心研究或图技术。 *   虽然在 ALFWorld 等基准上测试，但这些是通用的智能体决策基准，而非生物、医疗等特定垂直领域的非演化型应用。 综上所述，该论文致力于解决LLM智能体的记忆构建和自我迭代学习问题，属于构建和演化LLM智能体的前沿研究，应予以保留。",
                    "summary2": "本文旨在解决LLM智能体在复杂任务中缺乏高效、可解释且持续适应的学习机制问题。针对多步交互任务场景，我们提出了一种名为MACLA的框架，通过冻结LLM并结合外部分层程序化记忆，利用贝叶斯选择和对比细化机制实现推理与学习的解耦。在ALFWorld、WebShop等四个基准测试上，通过任务成功率和训练效率等指标验证了其有效性，实现了78.1%的平均性能，且构建速度比SOTA快2800倍。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
                    "arxiv_id": "2512.18622",
                    "authors": "Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen",
                    "summary": "Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合 \"Multi-Agent\" 方向**：论文的核心是提出了一种名为 MATS 的新型框架，明确使用了 \"Multi-agent mechanism\"（多智能体机制）。该机制通过为辅助智能体分配专门角色来减少工作量并促进交互，这直接对应了研究焦点中的“多智能体”及其子方向（协作、通信）。 2.  **包含 \"Self-Evolving\" 机制**：论文提出了一种基于强化学习的训练方案，利用 \"execution feedback\"（执行反馈）来对齐智能体。这符合筛选标准中的“自我演化”定义，即“智能体通过经验、反思或环境反馈进行自我完善和迭代”。这种通过反馈循环来改进智能体性能的方法是 Agentic AI 的关键特征。 3.  **属于构建新框架而非单纯应用**：虽然论文的应用场景是 Text2SQL（数据工程领域），但论文的本质并非简单地将现有智能体作为工具应用，而是为了解决小语言模型（SLM）的局限性，专门构建了一个新的多智能体框架和训练机制。根据筛选标准第四步（处理特殊和模糊情况），只要核心是提出新的机制（如自我演化或多智能体协作），即使应用在特定领域，也应保留。 综上所述，该论文在多智能体协作和基于反馈的自我改进方面做出了实质性贡献，符合 \"LLM智能体及其演化\" 的研究范围。",
                    "summary2": "本文旨在解决Text2SQL任务中外部LLM的隐私成本问题及SLM推理能力不足的挑战。针对Text2SQL任务，我们提出了一种名为MATS的多智能体框架，结合基于执行反馈的强化学习（RLEF）机制来协同多个专用智能体。我们在Spider和BIRD数据集上通过执行准确率（EX%）等指标验证了其有效性，结果表明其在单GPU环境下实现了与大型LLM相当的性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
                    "arxiv_id": "2512.18746",
                    "authors": "Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, Shuicheng Yan",
                    "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心保留的论文。具体判断依据如下： 1.  **核心贡献精准匹配 (第一步 & 第二步)**： *   论文的核心贡献是提出了 **MemEvolve**，这是一个“元演化框架”，旨在联合演化智能体的经验知识和**记忆架构**。 *   这直接对应了研究课题中的 **“自我演化”** 方向。论文不仅关注智能体如何利用记忆进行演化，更进一步提出了让记忆架构本身进行“元适应”和“演化”的机制。 *   论文涉及的核心能力是 **Memory (记忆)**，这是 Agentic AI 的关键组件之一。 2.  **符合研究焦点**： *   论文明确指出其研究对象是 **\"LLM-based agents\"**，旨在解决现有智能体记忆架构静态化的问题。 *   它不是将智能体作为工具应用到某个垂直领域（如医疗、金融），而是专注于改进智能体本身的底层架构和演化机制，因此不属于“非演化型应用”。 3.  **无排除项触发 (第三步)**： *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 *   虽然论文提到了 \"EvolveLab\" 代码库，但这是为了支持其提出的演化算法和框架，属于方法论的一部分，而非单纯的基础设施研究。 综上所述，该论文在“自我演化”和“智能体记忆机制”方面提出了创新性的框架，高度契合“LLM智能体及其演化”的研究目标。",
                    "summary2": "本文旨在解决现有智能体记忆架构静态、无法适应不同任务的问题。针对LLM智能体的自我进化场景，我们提出了一种名为MemEvolve的元进化框架，通过双层优化同时进化智能体的经验和记忆架构。我们在GAIA、WebWalkerQA、xBench-DeepSearch和TaskCraft四个基准上，通过任务成功率等指标验证了其有效性，实现了显著的性能提升和跨任务泛化。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-19": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
                    "arxiv_id": "2512.17102",
                    "authors": "Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong",
                    "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心文献。具体判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献是提出了一种名为 SAGE 的新框架，旨在通过强化学习（RL）和技能库来增强 LLM 智能体的自我改进和自我演化能力。这直接对应了我的核心目标中关于“构建、改进或演化 LLM智能体”的要求。它不是单纯的应用，也不是基础设施优化，而是提出了新的智能体演化方法论。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确涉及 `Self-Evolving`（自我演化）和 `LLM-based Agents`。 *   **演化机制**：论文详细描述了 `Self-Improvement`（自我改进）和 `Iterative Improvement`（迭代改进）机制，特别是通过“Sequential Rollout”在任务链中积累技能，体现了智能体通过经验进行迭代完善的过程。 *   **智能体能力**：引入“Skill Library”作为智能体获取和重用能力的机制，这与 `Tool Use` 和 `Memory` 的概念紧密相关，增强了智能体在新环境中的适应性。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图神经网络等排除领域，纯粹聚焦于智能体的算法架构和演化逻辑。 4.  **特殊情况处理（第四步）**：虽然论文在 AppWorld 数据集上进行了实验，但其核心在于提出一种通用的自我演化机制（SAGE），而非仅仅解决 AppWorld 领域的特定问题。根据规则，这种提出新“自我演化”机制的应用研究应当保留。 综上所述，该论文在“自我演化”这一核心研究焦点上做出了实质性贡献，符合筛选标准。",
                    "summary2": "本文旨在解决基于LLM的智能体在新环境中难以持续改进和适应的问题。针对工具使用智能体，我们提出了一种名为SAGE的RL框架，通过Sequential Rollout和Skill-integrated Reward增强技能生成与利用。在AppWorld数据集上，通过Scenario Goal Completion (SGC)和Task Goal Completion (TGC)等指标验证了其有效性，显著提升了性能与效率。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
                    "arxiv_id": "2512.17260",
                    "authors": "Jiangjie Chen, Wenxiang Chen, Jiacheng Du, Jinyi Hu, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Wenlei Shi, Zhihong Wang, Mingxuan Wang, Chenrui Wei, Shufa Wei, Huajian Xin, Fan Yang, Weihao Gao, Zheng Yuan, Tianyang Zhan, Zeyu Zheng, Tianxi Zhou, Thomas Hanwen Zhu",
                    "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文的核心贡献是提出了 **Seed-Prover 1.5**，这是一个通过 **\"agentic reinforcement learning\"（智能体强化学习）** 训练的模型。论文明确指出模型通过 **\"extensive interactions with Lean and other tools\"（与工具的广泛交互）** 来 **\"continuously accumulates experience\"（持续积累经验）**。这直接对应了筛选标准中的 **自我演化** 机制和 **Agentic AI** 的核心范式。 2.  **涉及智能体的关键能力**： 论文中提到的与 Lean（形式化证明工具）的交互属于 **Tool Use（工具使用）**；通过强化学习过程积累经验并提升能力，属于 **Self-Improvement（自我完善）** 和 **Iterative Improvement（迭代改进）**。这些都是筛选标准第二步中的核心正面指标。 3.  **符合“特殊和模糊情况”的处理规则**： 虽然论文的应用领域是数学定理证明（属于特定领域应用），但根据筛选标准第四步第2点：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。本文的核心在于提出了一种基于智能体强化学习和测试时扩展（TTS）的**新框架和方法论**，而不仅仅是应用现有模型解决数学问题。因此，它不属于“非演化型应用”的排除范畴。 4.  **排除标准检查**： 论文不涉及安全对齐、多模态视觉或图技术等排除项。 综上所述，该论文在构建具有自我演化能力的LLM智能体方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决形式化定理证明在本科及以上水平面临的计算成本高和能力不足的问题。针对Lean环境下的数学证明任务，我们提出了一种基于大规模智能体强化学习训练的Agentic Prover，并结合Rubric RL训练的Sketch模型及高效的测试时扩展工作流。我们在PutnamBench、FATE-H/X等基准数据集上，通过问题解决率验证了其有效性，显著优于现有SOTA方法。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "Dynamic Tool Dependency Retrieval for Efficient Function Calling",
                    "arxiv_id": "2512.17052",
                    "authors": "Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermovol, Bence Major",
                    "summary": "Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.",
                    "category": "cs.LG",
                    "filter_reason": "1.  **核心判断（符合）**：这篇论文的核心贡献是提出了一种名为“动态工具依赖检索（DTDR）”的方法，旨在改进LLM智能体在函数调用过程中的工具检索机制。这属于构建和改进LLM智能体的方法论，完全符合“单智能体”的研究范畴。 2.  **正面指标匹配**： *   **核心范式**：论文明确研究对象是“Function calling agents”（函数调用智能体），属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文的核心焦点在于 `Tool Use / Tool Augmentation`（工具使用/工具增强）。它解决了智能体在多步任务中如何根据上下文动态选择相关工具的问题，这直接关联到智能体的 `Planning`（规划）能力，因为它涉及处理“多步工具依赖”和“随着计划的展开”进行自适应检索。 3.  **排除标准检查**： *   论文并非将智能体作为工具应用到特定垂直领域（如生物、医疗），而是改进智能体本身的基础组件（工具检索模块）。 *   论文不涉及安全、对齐、多模态核心研究或图技术。 *   虽然摘要中提到了“evolving task context”（演化的任务上下文），但这指的是任务执行过程中的状态变化，而非智能体模型本身的“自我演化”或“自我完善”，因此归类为单智能体的工具使用改进最为准确。 4.  **结论**：该论文通过改进工具检索机制，显著提升了LLM智能体在复杂任务中的执行效率和准确性，是对Agentic AI中工具使用能力的具体增强，完全符合筛选要求。",
                    "summary2": "本文旨在解决设备端 Function Calling agents 中现有工具检索方法无法捕捉多步依赖和动态上下文的问题。针对用户查询和工具调用历史，我们提出了一种 Dynamic Tool Dependency Retrieval (DTDR) 方法，结合查询和执行历史动态检索相关工具。在 TinyAgent 和 TaskBench 等多个数据集上，通过 Function Selection Accuracy 和 Success Rate 等指标验证了其有效性，显著提升了检索精度和任务成功率。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
                    "arxiv_id": "2512.17008",
                    "authors": "Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li",
                    "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种名为 \"Turn-PPO\" 的新算法，旨在改进 Agentic LLMs（智能体大模型）在多轮交互中的强化学习（RL）训练效果。这属于构建和改进 LLM 智能体方法论的研究，而非简单的应用或基础设施优化。 2.  **符合研究焦点（单智能体）**： *   论文明确针对 \"Agentic LLMs\"，关注智能体在多轮任务中的表现。 *   它解决了智能体在长视界推理中的局限性，涉及智能体的规划和多步推理能力，这正是筛选标准中“单智能体”方向的核心关注点。 3.  **排除标准检查（通过）**： *   **非应用型**：虽然论文在 WebShop 和 Sokoban 数据集上进行了验证，但其核心在于提出一种通用的训练算法（Turn-PPO），而非解决特定领域的业务问题。 *   **非基础设施**：研究重点在于算法逻辑和策略优化，而非硬件或部署。 *   **非排除领域**：不涉及安全、对齐、多模态视觉或图技术。 4.  **特殊情况处理**： *   论文讨论的是智能体级别的交互和决策，将 MDP（马尔可夫决策过程）从 Token 级别提升到 Turn 级别，这符合“智能体如何进行规划或在复杂任务中进行多步推理”的保留规则，而非单纯的基础 Token 预测能力提升。 综上所述，该论文通过改进强化学习算法来增强 LLM 智能体的多轮交互和规划能力，是构建高质量 Agentic AI 的关键研究，应予以保留。",
                    "summary2": "本文旨在解决多轮智能体 LLM 训练中 GRPO 算法不稳定及优势估计不准确的问题。针对多轮交互场景，我们提出了一种 Turn-PPO 方法，采用 turn-level MDP 公式化，将整轮交互作为状态-动作对，并利用可学习的 Critic 进行优势估计。我们在 WebShop 和 Sokoban 数据集上通过平均奖励验证了其有效性，实验表明该方法显著提升了训练稳定性和任务性能。",
                    "summary_translation": "强化学习 (RL) 再次成为一种在真实环境中训练交互式大语言模型 (LLM) 智能体的自然途径。然而，将广泛使用的群组相对策略优化 (GRPO) 算法直接应用于多轮任务时，暴露出了显著的局限性，特别是在需要长视界推理的场景中。为应对这些挑战，我们探究了更稳定且有效的优势估计策略，特别是针对多轮场景的策略。我们首先探索了近端策略优化 (PPO) 作为替代方案，发现其比 GRPO 具有更强的鲁棒性。为了进一步增强 PPO 在多轮场景中的表现，我们引入了 turn-PPO，这是一种基于轮次级马尔可夫决策过程 (MDP) 形式化构建的变体，区别于常用的词元级 MDP。我们在 WebShop 和 Sokoban 数据集上的实验结果验证了 turn-PPO 的有效性，无论任务中是否包含长推理组件。",
                    "inspiration_trace": "基于论文《Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs》的内容，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观背景与问题识别\n**观察：** 随着LLM智能体的发展，任务从单轮推理（如数学题）转向多轮交互（如网页浏览、工具使用）。强化学习（RL）是训练此类智能体的主流方法。\n**现状：** 目前最流行的算法是 **GRPO**（Group Relative Policy Optimization，源自DeepSeek-R1），它在单轮任务上表现优异。\n**冲突：** 当直接将GRPO应用于多轮任务时，训练经常出现崩溃或不稳定，尤其是在需要长视界推理的场景下。\n\n### 第二阶段：诊断现有方法的局限性\n**思考：** 为什么在单轮任务中有效的GRPO，在多轮任务中失效了？\n**分析：**\n1.  **采样方差过大：** 多轮任务涉及环境交互，不可控因素多，导致基于采样的优势估计方差极高，训练不稳定。\n2.  **信用分配不均：** GRPO将整个轨迹的最终奖励归一化后，均等地分配给轨迹中的每一个Token。然而，在多轮对话中，不同轮次对最终成功的贡献差异巨大（有的轮次是关键决策，有的只是废话）。这种“一刀切”的优势估计引入了严重的噪声。\n\n### 第三阶段：算法选型的初步探索\n**假设：** 为了解决采样方差大和信用分配不准的问题，我们需要一个更稳定、更精确的优势估计策略。\n**尝试：** 重新引入 **PPO**（Proximal Policy Optimization）。\n**理由：** 与GRPO依赖纯采样不同，PPO引入了一个可学习的**Critic（价值函数）**。Critic可以利用广义优势估计（GAE）来平滑奖励信号，理论上比基于样本的归一化更稳定。\n**验证：** 实验表明，PPO确实比GRPO更稳定，缓解了训练崩溃的问题。\n\n### 第四阶段：深挖“状态表示”的错位\n**思考：** 虽然PPO比GRPO好，但现有的PPO实现通常沿用了单轮RLHF中的**Token级MDP**（Token-level MDP） formulation。这在多轮场景下是最优的吗？\n**洞察：** 在多轮交互中，状态转移具有“异质性”。\n*   **Token级视角：** 模型生成一个Token，状态加一个Token（平滑变化）；突然插入一大段环境输出，状态剧烈跳变。\n*   **问题：** 这种不连续的状态转移导致Critic很难学习。Critic被迫在Token级别预测价值，但任务的结构性变化发生在Turn级别。这种**“状态表示错位”**导致Critic回归到一个平均值，无法准确捕捉不同轮次的价值，从而降低了优势估计的准确性。\n\n### 第五阶段：核心创新——重新定义MDP粒度\n**顿悟：** 既然智能体的决策是以“轮”为单位的，那么RL的优化粒度也应该是“轮”，而不是“Token”。\n**方法论构建：** 提出 **Turn-PPO**。\n1.  **重新定义MDP：**\n    *   **状态：** 当前轮次的完整历史 + 当前查询。\n    *   **动作：** 当前轮次的完整回复。\n2.  **Turn-level Critic：** Critic不再预测每个Token的价值，而是预测每个Turn的价值。\n3.  **优势估计：** 在Turn级别计算GAE优势，从而实现精确的轮次级信用分配。\n\n### 第六阶段：逻辑闭环与验证\n**预期效果：**\n*   **更准确的Critic：** 状态表示与任务结构对齐，Critic更容易收敛。\n*   **更好的信用分配：** 能够准确识别哪一轮对话是好的，哪一轮是坏的，而不是模糊地分配给所有Token。\n*   **更稳定的训练：** Turn-level的裁剪机制能更好地防止策略剧烈变动。\n**结论：** 通过将MDP formulation从Token级提升到Turn级，并结合PPO的Critic机制，Turn-PPO在多轮智能体任务中实现了比GRPO和传统Token-PPO更优的性能与稳定性。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
                    "arxiv_id": "2512.16970",
                    "authors": "Kamer Ali Yuksel",
                    "summary": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心关注的“单智能体”研究方向。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 **PAACE** 框架，这是一个用于优化 LLM 智能体在复杂工作流中“演化状态”的统一框架。它不是将智能体作为工具应用到特定垂直领域（如医疗、金融），而是致力于**改进智能体本身**的上下文工程机制，解决智能体在长视界任务中的上下文膨胀和注意力稀释问题。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合核心关注点 (第二步)**: *   **Agentic AI**: 论文明确针对 LLM 智能体，涉及智能体的规划、工具使用和反思。 *   **Planning (规划)**: 论文强调 \"Plan-Aware\"（感知计划），通过分析计划结构和 next-k-task relevance modeling 来优化上下文，直接关联到智能体的规划能力。 *   **Memory (记忆)**: 论文处理的是智能体工作流中“快速扩展的上下文”，这本质上是智能体的短期记忆管理问题。 3.  **排除标准检查 (第三步)**: 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此不在排除范围内。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文关注的是智能体在多步骤工作流中的推理过程，通过压缩和优化上下文来辅助智能体更好地执行规划，属于 Agentic 的范畴，而非单纯的 LLM 基础推理能力提升（如数学题求解）。 综上所述，PAACE 提出了一种改进智能体架构（特别是上下文管理和规划感知）的新方法，能够提升智能体在复杂任务中的表现，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决LLM Agent在复杂多步骤工作流中上下文膨胀导致的注意力稀释与推理成本问题。针对长视界Agent工作流，我们提出了PAACE框架，通过next-k-task相关性建模、计划结构分析和指令协同优化实现上下文压缩。我们在AppWorld、OfficeBench和8-Objective QA上通过Accuracy、F1及Peak Context等指标验证了其有效性，结果显示PAACE在提升准确率的同时显著降低了上下文负载。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
                    "arxiv_id": "2512.17060",
                    "authors": "Monika Zamojska, Jarosław A. Chudziak",
                    "summary": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。具体判断过程如下： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了一种受交互分析理论启发的**多智能体系统（MAS）架构**。它不仅仅是将现有的LLM作为工具应用，而是设计了一种新的智能体内部结构（将智能体划分为父母、成人、儿童三种自我状态）并结合了上下文检索机制。这属于“构建、改进LLM智能体”的范畴，符合保留条件。 2.  **正面指标匹配（第二步）**： *   **多智能体**: 论文明确提出了Multi-Agent System (MAS)，旨在模拟群体动力学和社会行为。 *   **智能体能力**: 论文涉及智能体的**记忆**（通过向量存储检索上下文信息）和特定的**推理风格**（基于不同自我状态的推理）。 *   **核心范式**: 符合 `LLM-based Agents` 和 `Multi-Agent Systems` 的定义。 3.  **排除标准检查（第三步）**： *   论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此不在排除之列。 4.  **特殊情况处理（第四步）**： *   虽然论文涉及心理学领域，但其核心在于**构建智能体架构**以增强模拟的真实感，而非单纯的应用研究。它属于对智能体行为和认知结构的改进，符合研究目标。 综上所述，该论文通过引入心理学理论改进了多智能体的架构和行为模式，属于Agentic AI和多智能体系统的研究范畴。",
                    "summary2": "本文旨在解决LLM智能体缺乏心理深度和行为一致性的问题。针对模拟对话场景，我们提出了一种基于Transactional Analysis (TA) 理论的Multi-Agent System (MAS) 架构，将智能体划分为Parent、Adult和Child三种ego states，并结合上下文信息检索机制。我们在模拟的职场对话环境中，通过消融测试验证了其有效性，结果显示该架构能显著提升智能体的心理真实感和行为多样性。",
                    "summary_translation": "基于大语言模型（LLM）的智能体目前已广泛应用于从客户服务到教育等多个领域，学界和业界对其拟人化行为能力的兴趣日益浓厚。这涵盖了社会、政治及心理学等领域，这些领域的研究目标通常是对群体动力学和社会行为进行建模。然而，现有的LLM智能体往往缺乏捕捉人类思维真实模式所需的心理深度和一致性。它们通常提供直接或符合统计概率的答案，却忽略了驱动真实人类互动的深层目标、情感冲突及动机。本文提出了一种受交互分析理论启发的多智能体系统。在该系统中，每个智能体被划分为三种自我状态——父母、成人和儿童。这些自我状态被视为独立的知识结构，各自拥有独特的视角和推理风格。为了丰富其响应过程，系统引入了信息检索机制，使智能体能够从向量存储中检索相关的上下文信息。该架构在模拟对话场景中通过消融实验进行了评估，对比了具备与不具备信息检索功能的智能体表现。结果令人鼓舞，为探索基于心理学基础的结构如何丰富智能体行为开辟了新方向。本文的贡献在于提出了一种智能体架构，该架构将交互分析理论与上下文信息检索相结合，以增强基于LLM的多智能体模拟的真实感。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-18": [
        {
            "name": "Artificial Intelligence",
            "count": 17,
            "papers": [
                {
                    "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
                    "arxiv_id": "2512.16465",
                    "authors": "Jinwu Chen, Qidie Wu, Bin Li, Lin Ma, Xin Si, Yang Hu, Shouyi Yin, Jun Yang",
                    "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合你的研究范围，是一个典型的、高质量的“自我演化”与“多智能体”交叉领域的研究。 以下是详细的判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM作为工具应用到CUDA优化领域，而是**提出了一种全新的多智能体框架（cuPilot）和一种新的演化算法（策略协调的演化算法）**。其核心贡献在于方法论创新，即如何设计智能体以及如何让它们协同演化，从而更高效地完成任务。这完全符合“构建、改进或演化LLM智能体”的核心目标。它避开了“非演化型应用”的排除项，因为其核心是演化机制本身，而非应用结果。 2.  **第二步：正面指标** - 论文高度匹配你的核心关注点： - **核心范式**: 明确提到了 `Multi-Agent Systems (MAS)` 和 `Self-Evolving`（体现在标题的 \"Evolution\" 和摘要的 \"evolutionary algorithms\"）。 - **多智能体**: 标题和摘要都强调了 `Multi-agent Framework` 和 `Strategy-Coordinated`，这直接指向了智能体间的协作机制。 - **演化机制**: 论文的核心创新点之一是“策略协调的演化算法”，这属于 `Self-Improvement` 和 `Generational Evolution` 的范畴。 3.  **第三步：排除标准** - 论文不涉及任何排除标准。其焦点是算法和框架的性能，而非安全、对齐或多模态。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美范例。虽然它被应用在“CUDA内核优化”这个特定领域，但它的核心贡献是提出了一种**新的“自我演化”机制**（策略协调的演化算法、策略级种群初始化等）。根据你的规则，这种提出新机制并应用于特定领域的论文应该被保留。这篇论文的价值在于其提出的演化框架具有潜在的通用性，可以被借鉴到其他需要复杂搜索和优化的任务中。 **核心依据总结**: 该论文的核心贡献是**方法论层面的创新**，它构建了一个新颖的多智能体系统来解决一个复杂的演化问题。它研究的不是“如何用LLM写CUDA代码”，而是“如何设计一个多智能体框架，让智能体们通过协作和演化，高效地探索出最优的CUDA代码策略”。这直接命中了你研究课题中的“多智能体”和“自我演化”两个核心方向，是典型的Agentic AI研究。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决现有LLM驱动的CUDA内核进化框架因进化算法与内核代码不匹配而导致的性能不足问题。针对KernelBench中的100个CUDA内核，我们提出了一种策略协调的多智能体框架cuPilot，通过引入策略作为中间语义表示，并结合策略协调进化算法、roofline引导提示和策略级种群初始化，在KernelBench上通过平均3.09倍的加速比验证了其有效性。",
                    "summary_translation": "优化 CUDA kernels (内核) 是一项极具挑战性且劳动密集的任务，因为它不仅需要 hardware-software co-design (软硬件协同设计) 的专业知识，还受到高性能 kernel libraries (内核库) 专有性的制约。尽管近期将 large language models (LLMs, 大型语言模型) 与 evolutionary algorithms (进化算法) 相结合的方法在 automatic kernel optimization (自动内核优化) 方面展现出潜力，但现有方法由于其 suboptimal agent designs (次优的智能体设计) 和 mismatched evolution representations (不匹配的进化表示)，其性能往往不尽人意。本研究识别了这些不匹配之处，并提出了 cuPilot——一个 strategy-coordinated multi-agent framework (策略协调的多智能体框架)。该框架将 strategy (策略) 作为一种 intermediate semantic representation (中间语义表示)，用于指导 kernel evolution (内核进化) 过程。其主要贡献包括：一种 strategy-coordinated evolution algorithm (策略协调的进化算法)、一种 roofline-guided prompting (Roofline 模型引导的提示) 方法，以及一种 strategy-level population initialization (策略级种群初始化) 技术。实验结果表明，在一个包含 100 个内核的基准测试中，cuPilot 生成的内核相比 PyTorch 实现了平均 3.09 倍的加速。在 GEMM (通用矩阵乘法) 任务上，cuPilot 展示了其 sophisticated optimizations (复杂的优化) 能力，并实现了对 critical hardware units (关键硬件单元) 的高利用率。其生成的内核已在 https://github.com/champloo2878/cuPilot-Kernels.git 上开源。",
                    "inspiration_trace": "好的，以下是我对 cuPilot 论文作者思考过程的系统性推演，旨在还原其从观察到方法论的完整逻辑链。\n\n---\n\n### **作者思考过程推演：从问题洞察到框架构建**\n\n#### **第一阶段：宏观问题的确立与现有路径的审视**\n\n1.  **起点：一个公认的难题。**\n    作者首先锚定了一个宏观且重要的问题：**CUDA内核优化极其困难且耗时**。这需要深厚的软硬件协同设计知识，而高性能库的闭源性质进一步抬高了门槛。这是一个明确的、有价值的科研出发点。\n\n2.  **观察：新兴技术路径的出现与局限。**\n    作者注意到，利用大型语言模型（LLM）结合进化算法来自动化内核优化，是一个新兴且有前景的方向。然而，通过审视现有工作（如AI CUDA Engineer），他们敏锐地发现了一个核心矛盾：**尽管技术路径看起来可行，但实际生成的内核性能远未达到手动优化的水平，甚至表现不佳。**\n\n3.  **提出核心疑问：为什么“LLM+进化”这条路走不通？**\n    这个疑问是全文的驱动力。作者没有停留在“性能不好”的表面现象，而是深入探究其根本原因。他们没有否定“LLM+进化”这个大方向，而是假设问题出在**实现细节的“错配”**上。\n\n#### **第二阶段：诊断“错配”——从现象到本质的三个假设**\n\n作者通过实验（如图1所示）和逻辑分析，将宏观的“性能不佳”问题，分解为三个具体的、可论证的“错配”维度。这构成了他们方法论的三大支柱。\n\n1.  **错配一：进化表示的错配——“在错误的抽象层上做交叉”。**\n    *   **观察现象：** 现有方法直接让LLM融合两个内核的代码（代码级交叉）。实验表明，随着父代内核复杂度增加，LLM几乎无法正确识别、保留并组合其中的优化策略，导致性能退化甚至功能错误。\n    *   **形成假设：** 问题的根源在于**代码是低层次的、具体的实现，而优化策略是高层次的、抽象的思想**。直接在代码层面进行“杂交”，相当于让一个工匠在不理解设计蓝图的情况下，盲目地拼接两台复杂的机器。这必然失败。\n    *   **核心洞见：** **必须引入一个中间语义层——“策略”，将进化操作从“代码融合”提升到“策略组合”。** 这是cuPilot最核心的创新思想。\n\n2.  **错配二：适应度表示的错配——“用模糊的目标指导精细的优化”。**\n    *   **观察现象：** 现有方法仅用最终的“性能”（如延迟）作为适应度函数来指导进化。但性能是一个黑盒结果，LLM无法从这个单一数字中得知瓶颈究竟是计算吞吐量不足，还是内存带宽受限。\n    *   **形成假设：** 如果不给LLM提供更具体的、与硬件相关的诊断信息，它的优化就如同“盲人摸象”，只能在低效的试错中徘徊。\n    *   **核心洞见：** **需要将模糊的“性能目标”转化为具体的“硬件优化方向”。** 作者自然地想到了计算机体系结构中的经典分析工具——**Roofline模型**。通过定位内核在Roofline图上的位置（计算受限、内存受限或中间区域），可以为LLM提供清晰的、可操作的优化提示。\n\n3.  **错配三：种群初始化的错配——“从贫瘠的起点探索广阔的空间”。**\n    *   **观察现象：** 进化算法的潜力很大程度上取决于初始种群的多样性。现有方法通常从少数几个简单的内核开始，导致初始种群覆盖的优化策略空间非常稀疏。\n    *   **形成假设：** 一个糟糕的起点会让进化算法过早收敛到局部最优，无法探索到那些更复杂、更高效的优化组合。\n    *   **核心洞见：** **必须站在巨人的肩膀上。** 历史上的成功优化案例是宝贵的知识财富。通过**检索增强生成（RAG）**，可以从一个外部策略池中学习，为每个新任务生成一个多样化且高质量的初始策略种群。\n\n#### **第三阶段：综合与构建——从三大洞见到统一框架**\n\n在诊断出三大“错配”并提出相应的解决假设后，作者的工作进入了整合阶段，将这些零散的洞见组织成一个连贯、可执行的框架。\n\n1.  **确立核心抽象：“策略”为中心。**\n    所有设计都围绕“策略”这一新引入的中间表示展开。它成为了连接高层进化算法和底层内核代码的“通用语言”。\n\n2.  **设计多智能体协作架构。**\n    为了实现“策略”与“代码”的解耦与协作，一个多智能体框架是自然的选择：\n    *   **SCE Manager（高层）：** 负责策略层面的进化操作（初始化、交叉、选择），完全不理会代码细节。\n    *   **Strategy Translator（中间层）：** 核心枢纽。它负责将抽象的“策略”翻译成具体的“内核代码”，并在内核被优化后，反向提炼出新的“策略”。它实现了高层思想与底层实现之间的握手。\n    *   **Kernel Revisor（底层）：** 专注于代码层面的细节，如语法修正、功能验证和性能分析。\n\n3.  **将三大解决方案融入框架。**\n    *   **策略协调进化（SCE）算法：** 这是解决“错配一”的具体实现。它将传统进化算法的“交叉”操作，明确地分解为“策略级交叉”和“策略到代码的翻译”两步。\n    *   **Roofline Prophet（引导模块）：** 这是解决“错配二”的实体。它作为一个独立的智能体，在进化开始前和选择过程中，为其他智能体提供基于Roofline模型的优化方向指导。\n    *   **策略级种群初始化（RAG增强）：** 这是解决“错配三”的机制。通过构建一个外部策略池，并利用RAG技术，确保进化从一个高起点、多样化的策略空间开始。\n\n#### **总结：思想的演进脉络**\n\n作者的思考过程是一个典型的**“问题驱动 -> 深度诊断 -> 假设驱动 -> 系统构建”**的学术创新路径。\n\n*   **始于一个痛点：** 自动化CUDA优化效果不佳。\n*   **精于一次诊断：** 将模糊的“效果不佳”精准定位为三个“错配”。\n*   **成于一个核心：** 引入“策略”作为中间表示，一举解决了最核心的表示错配问题。\n*   **强于一个体系：** 将Roofline引导和RAG初始化等辅助思想，通过一个精心设计的多智能体框架有机地整合起来，形成了一个完整、自洽且高效的解决方案。\n\n最终，cuPilot的诞生并非灵光一现，而是作者在对现有方法深刻批判的基础上，层层递进、系统性地解决每一个被识别出的核心矛盾后的必然产物。"
                },
                {
                    "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
                    "arxiv_id": "2512.16424",
                    "authors": "Nguyen Xuan-Vu, Daniel Armstrong, Milena Wehrbach, Andres M Bran, Zlatko Jončev, Philippe Schwaller",
                    "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，核心判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献是构建了一个名为 **Synthelite** 的新框架，而不是简单地将现有LLM或智能体框架作为工具应用于化学领域。该框架的核心功能是“合成规划”，并允许通过自然语言进行“专家干预”和“适应其规划轨迹”。这表明其本质是关于**构建一个具有规划能力的LLM智能体**，因此符合“保留”标准，避开了“非演化型应用”的排除规则。 2.  **正面指标 (第二步):** 论文明确包含了我的核心关注点。 *   **核心范式:** 论文提出了一个 `LLM-based Agent` 框架。 *   **智能体能力:** 论文的核心是 `Planning`（合成规划）。摘要中提到的“适应其规划轨迹”和“生成端到端合成路线”都体现了智能体在复杂任务中进行多步规划和决策的能力，这与ReAct等Agentic框架的范式高度一致。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全、对齐或多模态视觉等领域，因此没有触发任何排除标准。 4.  **特殊情况处理 (第四步):** *   **推理/规划:** 这篇论文是“智能体如何进行规划或在复杂任务中进行多步推理”的典型范例。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个让LLM能够执行特定领域（化学合成）复杂规划任务的Agentic框架。因此，根据规则，应该保留。 **总结:** 尽管论文的应用领域是化学合成，但其核心贡献在于提出了一种新的、具有交互式和约束感知能力的**LLM智能体规划框架**。它研究了智能体如何利用LLM的知识进行多步规划、并根据外部反馈（人类专家的约束）调整策略，这完全属于我研究课题中“单智能体”方向下的“规划”子方向。因此，这篇论文是高度相关的前沿研究，应该被保留。",
                    "summary2": "本文旨在解决现有计算机辅助合成规划（CASP）框架缺乏与化学家互动机制的问题，实现一个能灵活适应专家约束的合成规划工具。针对策略约束和起始材料约束等场景，我们提出了一种名为Synthelite的两阶段框架，其核心是利用LLM作为中央代理，通过自然语言提示生成合成蓝图，并结合相似性引导的MCTS进行路线优化。在包含27个分子-提示对的扩展基准和Pistachio Reachable数据集上，通过Recall、Precision和Solve Rate等指标验证了其有效性，最高可达95%的成功率。",
                    "summary_translation": "Computer-aided synthesis planning (CASP) 长期以来被设想为合成化学家的补充工具。然而，现有框架往往缺乏与人类专家交互的机制，从而限制了其整合化学家洞见的能力。在本研究中，我们介绍了Synthelite，一种利用 large language models (LLMs) 直接提出 retrosynthetic transformations (逆合成转化) 的合成规划框架。Synthelite能够利用LLMs固有的化学知识和推理能力生成端到端的合成路线，同时支持通过自然语言提示进行专家干预。实验结果表明，Synthelite可以灵活调整其 planning trajectory (规划轨迹) 以适应多样化的用户指定约束，在策略约束和起始材料约束的合成任务中，成功率均高达95%。此外，Synthelite在路线设计过程中还能考虑 chemical feasibility (化学可行性)。我们期望Synthelite不仅是一个实用的工具，更是迈向一种新范式的关键一步，在该范式中，large language models (LLMs) 将成为 synthesis planning (合成规划) 的 central orchestrators (核心协调者)。",
                    "inspiration_trace": "好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出其核心方法“Synthelite”的逻辑链，还原其从观察到最终方法论的思考过程。\n\n---\n\n### **第一步：宏观问题的观察与定位**\n\n**思考起点：** 计算机辅助合成规划（CASP）领域存在一个根本性的脱节。\n\n*   **观察：** 现有的CASP工具，尽管在算法上（如MCTS结合神经网络策略）日益强大，能够找到从目标分子到起始材料的路径，但它们本质上是“黑箱”。它们擅长解决“如何合成”的问题，却无法回答“如何*按我的想法*合成”。\n*   **核心矛盾：** 在真实的化学实践中，合成化学家并非只追求“任何一条可行路径”，而是会基于经验、偏好和特定目标（如反应顺序、起始物料、成本、安全性）来设计路线。现有工具缺乏一个灵活的接口来吸纳这种“人类智慧”，导致其生成的路线往往与专家的直觉和战略意图不符，实用性大打折扣。\n*   **问题定义：** 如何构建一个不仅能“自动规划”，更能与化学家“协同思考”的CASP框架？关键在于**人机交互的缺失**和**战略对齐的困难**。\n\n### **第二步：对现有解决方案的批判性审视**\n\n**思考演进：** 既然问题明确，那么为什么现有方法没能解决？作者必然对主流技术路线进行了深入剖析。\n\n1.  **审视“神经引导的搜索范式”（如MCTS+策略网络）：**\n    *   **局限性1（战略短视）：** 这类方法将多步合成拆解为独立的单步决策（马尔可夫假设），忽略了长程依赖（如保护基的引入与脱除）。它们优化的目标是“解决率”，而非“路线质量”或“战略契合度”。\n    *   **局限性2（交互僵化）：** 虽然可以引入一些约束（如键断裂偏好），但这些约束是硬编码的、有限的。无法通过自然语言灵活地表达“请优先使用Boc保护”或“这个反应请放在后期进行”这类复杂指令。\n\n2.  **审视“将LLM作为组件”的尝试：**\n    *   **作为单步策略：** LLM虽然能提供推理，但生成SMILES/SMARTS等化学符号的可靠性极差，且在搜索循环中频繁调用LLM成本高昂，不切实际。\n    *   **作为验证器/评分器：** 这只是“事后诸葛亮”，LLM只能对已有路径进行评价或剪枝，无法从根本上引导搜索走向一个全新的、符合用户意图的方向。搜索的主导权仍在传统算法手中。\n    *   **作为少样本提示规划器：** 严重依赖已知路线的例子，缺乏真正的创造性。如果找不到与用户约束匹配的范例，方法就会失效。\n\n**结论：** 现有方法要么将LLM置于“辅助”地位，未能发挥其核心潜力；要么受限于传统搜索范式，无法实现真正灵活的人机协同。需要一个**范式转移**。\n\n### **第三步：核心洞察与大胆假设的形成**\n\n**思考跃迁：** LLM最独特的能力是什么？是**自然语言理解**、**多步推理**和**内化的隐性知识**。这恰好能弥补现有CASP的短板。\n\n*   **核心洞察：** 与其让LLM在旧框架下“打补丁”，不如让它成为合成规划的**“中央指挥官”**。LLM不应只是一个“零件”，而应是整个系统的“大脑”。\n*   **核心假设：** 一个先进的LLM，凭借其化学知识和推理能力，可以直接：\n    1.  **理解**化学家通过自然语言提出的复杂、模糊的战略约束。\n    2.  **制定**一个多步的、具有战略前瞻性的合成蓝图。\n    3.  在规划中**内隐地考虑**化学可行性（如官能团兼容性、反应选择性）。\n\n如果这个假设成立，那么LLM就能成为连接人类专家意图与自动化搜索的完美桥梁。\n\n### **第四步：直面假设的内在挑战与设计对策**\n\n**思考深化：** 这个“LLM即指挥官”的假设很美好，但直接落地会遇到两个致命障碍。作者必须先解决它们。\n\n1.  **挑战一：符号鸿沟。** LLM擅长语言，但化学是精确的符号科学（SMILES/SMARTS）。LLM直接生成这些符号的可靠性极低，这是其作为“指挥官”的致命伤。\n    *   **设计对策：** **“翻译”而非“创造”。** 既然LLM不擅长“写”化学符号，那就让它“说”化学语言。\n        *   **机制：** 预先构建一个包含大量、经过验证的反应模板库（如SMARTS）。然后，用另一个LLM将每个模板“翻译”成一句标准化的文本描述。\n        *   **工作流：** 主LLM指挥官在规划时，用自然语言描述它想要的下一步反应（如“一个Williamson醚合成反应”）。系统通过文本相似度搜索，从模板库中找到最匹配的几个模板，再交由LLM进行最终选择。这样，LLM负责**“意图描述”和“决策”**，而符号的**“合法性”**由模板库保证。\n\n2.  **挑战二：计算成本。** 如果让LLM指挥官在一个巨大的搜索空间里进行每一步决策（像传统MCTS那样），计算成本将是天文数字，无法实用。\n    *   **设计对策：** **“战略规划”与“战术搜索”的解耦。** 将LLM的昂贵推理与高效的搜索算法分离开来。\n        *   **机制：** 设计一个**两阶段框架**。\n            *   **阶段一（LLM战略规划）：** LLM指挥官进行一次“端到端”的、贪婪式的战略推演，从目标分子反向规划出一个完整的、高层次的合成“蓝图”。这个过程不进行大规模搜索，只是快速生成一个或几个候选战略。为了提升质量，可以引入多次尝试和自我反馈机制。\n            *   **阶段二（相似性引导的MCTS）：** 拿着阶段一的“蓝图”，启动一个轻量级的MCTS。但这个MCTS的搜索不再是盲目的，而是被“锚定”在蓝图附近。它的扩展策略会优先选择与LLM在阶段一中描述的步骤（文本）高度相似的反应模板。这样，MCTS的作用从“全局探索”变成了“局部精修”，既保证了效率，又维持了与LLM战略的一致性。\n\n### **第五步：最终方法论的整合与愿景**\n\n**思考闭环：** 将上述对策整合，就形成了“Synthelite”的完整方法论。\n\n*   **逻辑链总结：**\n    1.  **问题：** CASP工具是“黑箱”，无法与化学家协同。\n    2.  **批判：** 现有方法（无论是传统搜索还是LLM作为组件）都因战略短视、交互僵化或符号不可靠而无法解决此问题。\n    3.  **假设：** 让LLM成为“中央指挥官”，利用其语言和推理能力来理解意图、制定战略。\n    4.  **挑战与对策：**\n        *   为解决**符号鸿沟**，采用“文本描述-模板检索”机制，让LLM“说”而非“写”化学。\n        *   为解决**计算成本**，采用“战略规划（LLM）+战术搜索（MCTS）”的两阶段解耦框架。\n    5.  **最终方案：** Synthelite——一个以LLM为核心，通过两阶段工作流，实现了**化学家对齐**和**可行性感知**的合成规划框架。\n\n*   **升华愿景：** 这不仅仅是一个工具，更是一个新范式的证明。它展示了LLM可以作为科学推理任务的“中心协调者”，为未来更智能、更人性化的人机协作科研工具铺平了道路。这正是论文标题和结论所强调的“LLM作为合成规划的中心协调者”的愿景。"
                },
                {
                    "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
                    "arxiv_id": "2512.16295",
                    "authors": "Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Qiushi Sun, Zhaoyang Liu, Zhoumianze Liu, Yu Qiao, Xiangyu Yue, Zun Wang, Zichen Ding",
                    "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为一个工具去解决某个特定领域（如生物、金融）的问题，而是聚焦于如何**改进LLM智能体本身**。其核心贡献是构建了一个名为“OS-Oracle”的框架，用于创建和训练“评判模型”。这个评判模型本身不是一个完整的智能体，但它是一个关键的**组件或机制**，用于增强其他智能体的能力。具体来说，它通过在智能体执行动作前进行评估和纠错，直接提升了智能体的可靠性和决策质量。这完全符合“改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **智能体能力**: 论文的核心是`Self-Correction`（自我纠正）和`Self-Reflection`（自我反思）机制。评判模型在每一步行动前对智能体的决策进行评估，这是一种显式的自我纠正和反思过程。 - **核心范式**: 研究背景是`Agentic AI`和`LLM-based Agents`（即文中的CUAs），旨在解决这些智能体在长任务中的关键瓶颈。 - **演化机制**: 虽然不是跨代的演化，但评判模型提供的迭代式改进（`Iterative Improvement`）是智能体在单次任务中实现性能提升的重要方式，属于自我演化的范畴。 3.  **第三步：排除标准** - **安全与对齐**: 论文的主要目标是提升智能体的任务成功率和可靠性，而不是研究`Safety`、`Alignment`或`Hallucination`等。虽然纠错能带来更安全的结果，但这不是论文的核心贡献和研究焦点。 - **多模态与视觉**: 论文确实涉及了`VLM`（视觉语言模型），因为智能体需要理解GUI界面。然而，根据您的规则，这里的视觉是作为智能体**感知环境的工具**。论文的核心贡献不是提出一个新的VLM架构或视觉理解算法，而是**如何将一个VLM训练成一个有效的“评判模型”**，并将其集成到智能体框架中以提升其决策能力。因此，这符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外情况。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文的研究内容与智能体的规划和推理紧密相关。评判模型作用于智能体多步推理的每一步，确保其规划的合理性。这属于“关于智能体如何进行规划或在复杂任务中进行多步推理”的范畴，因此应该保留。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个用于智能体决策验证的框架。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一套完整的框架（数据、训练、评测）来构建一个能够**增强LLM智能体自我纠正和反思能力的“评判模型”**。这直接对应了您研究目标中的“改进LLM智能体”，特别是“单智能体”方向下的“自我反思”和“自我演化”方向下的“自我完善”。因此，这篇论文与您的研究课题高度相关，应该被筛选出来。",
                    "summary2": "本文旨在解决计算机使用代理因缺乏可靠步骤级决策而导致的错误累积问题。针对跨平台GUI环境，我们提出OS-Oracle框架，其核心是可合成多种错误类型负样本的数据管道，与结合SFT和一致性保持CP-GRPO的两阶段训练方法。在自建的OS-Critic Bench上，通过准确率等指标验证了其有效性，并证明了其作为pre-critic能提升基础代理在OSWorld和AndroidWorld中的任务成功率。",
                    "summary_translation": "随着 VLM (Vision Language Model) 驱动的计算机使用代理 在图形用户界面 (GUI) 导航与操作方面日益强大，可靠的步骤级决策已成为其实现实际部署的关键瓶颈。在长时程工作流中，错误会迅速累积，且不可逆的操作可能引发意外后果，这促使了评估模型的诞生，用以在执行前对每一步操作进行评估。尽管评估模型提供了一个前景广阔的解决方案，但其有效性因缺乏多样化、高质量的GUI反馈数据，以及用于计算机使用场景中步骤级评估的公共基准而受到制约。\n\n为填补这些空白，我们提出了 OS-Oracle，其核心贡献包括：(1) 一个用于合成跨平台GUI评估数据的可扩展数据管道；(2) 一个结合了监督微调 (SFT) 和一致性保持的群体相对策略优化 (CP-GRPO) 的两阶段训练范式；(3) OS-Critic Bench，一个用于评估模型在移动、Web和桌面平台上性能的综合基准。利用该框架，我们精心整理了一个包含31万条评估样本的高质量数据集。\n\n由此得到的评估模型 OS-Oracle-7B，在 OS-Critic Bench 上取得了开源VLMs中的最先进 (SOTA) 性能，并在移动领域超越了专有模型。此外，当作为预评估模型使用时，OS-Oracle-7B 能够提升原生GUI代理（如 UI-TARS-1.5-7B）在 OSWorld 和 AndroidWorld 环境中的性能。相关代码已在 https://github.com/numbmelon/OS-Oracle 上开源。",
                    "inspiration_trace": "好的，以下是我基于论文内容，对作者产出《OS-Oracle》这篇论文的思考过程的系统性推演。\n\n---\n\n### **OS-Oracle 作者思想演进逻辑链**\n\n#### **1. 宏观观察与问题定义：从“代理能做什么”到“代理如何做对”**\n\n*   **起点：** 作者团队观察到，基于视觉语言模型（VLM）的计算机使用代理在GUI自动化任务上取得了巨大进步，展现出广阔的应用前景。这是当前领域的大趋势和兴奋点。\n*   **深入实践：** 当他们将这些代理应用到更复杂、更长的真实任务中时，发现了一个关键瓶颈：**可靠性不足**。代理在单步操作上看似智能，但在多步骤的流程中，错误会迅速累积，且某些不可逆的操作会导致任务彻底失败。\n*   **问题聚焦：** 作者没有停留在“代理成功率不高”这个表面现象，而是深入剖析了失败的具体原因。他们将问题归结为四大类典型错误：操作失败（OF）、低效的错误状态恢复（IESR）、错误的任务终止（MTT）和不准确的元素定位（IEL）。这标志着他们的思考从“现象观察”深化到了“问题归因”。\n\n#### **2. 核心瓶颈的识别：评判模型的“理想”与“现实”**\n\n*   **提出解决方案方向：** 面对上述问题，作者考虑了两种主流改进路径：一是通过强化学习（RL）端到端地优化原生代理，但这成本高昂、优化困难；二是引入一个**评判模型**，在每一步动作执行前进行“把关”。他们认为后者更具成本效益和可扩展性，是一个更优雅的解耦方案。\n*   **发现关键空白：** 当他们决定沿着“评判模型”这条路探索时，立刻遇到了两个根本性的障碍，这也是该领域研究停滞不前的原因：\n    1.  **数据瓶颈：** 训练评判模型需要大量“正确动作”和“错误动作”的配对数据。但现有数据集几乎全是专家演示的“正确”轨迹，天然缺乏高质量的“负样本”。从失败轨迹中定位错误步骤极其困难，从成功轨迹中识别低效步骤也非易事。\n    2.  **评估瓶颈：** 即便有了模型，如何科学地评估它的好坏？当时缺乏一个公认的、跨平台的、高质量的评判模型基准。现有基准要么存在数据泄露风险，要么局限于单一平台。\n\n#### **3. 方法论的构建：逐个击破，形成闭环**\n\n作者的思考过程转变为一个系统工程：如何系统性地解决这两个瓶颈，并最终产出可靠的评判模型。\n\n*   **第一步：解决数据瓶颈——从“挖掘”到“创造”**\n    *   **思维转变：** 既然从真实世界中“挖掘”高质量负样本如此困难，为什么不“创造”它们呢？作者的核心假设是：**代理的错误是模式化的**。只要我们能理解这些错误模式，就可以基于已有的正确轨迹，通过规则合成出逼真的负样本。\n    *   **方法论诞生：** 这就是**可扩展数据管道**的由来。\n        1.  **定义模式：** 将之前归纳的四大类错误（OF, IESR, MTT, IEL）作为合成的目标。\n        2.  **规则合成：** 针对每种模式设计具体的转换规则。例如，对于“操作失败”，就在点击前插入一个多余的“输入”动作；对于“元素定位错误”，就用工具检测其他可点击元素并生成错误点击。\n        3.  **知识注入：** 仅有标签是不够的。为了让模型学会“为什么错”，他们利用GPT-4o为每个合成的负样本生成详细的错误理由。这相当于为数据注入了“专家知识”，将简单的标签提升为富含推理过程的样本。\n    *   **成果：** 一个包含31万样本的高质量、跨平台、带理由的评判数据集。\n\n*   **第二步：解决模型训练问题——从“学会判断”到“保持一致”**\n    *   **基础训练：** 有了数据，最直接的方法是监督微调（SFT）。这能让模型学会基本的判别和理由生成能力。\n    *   **发现新问题：** 在初步实验中，作者观察到一个微妙但关键的现象：模型有时生成的理由逻辑上支持“正确”，但最终判断却是“错误”。这种**“推理与判断不一致”**的问题损害了模型的可靠性。\n    *   **提出新假设：** SFT教会了模型“说什么”，但没有强制其“言行一致”。需要一种机制来对齐模型的内在逻辑和外在输出。\n    *   **方法论创新：** 这就是**两阶段训练范式**的由来，特别是第二阶段的**一致性保持群组相对策略优化（CP-GRPO）**。\n        1.  **SFT打基础：** 第一阶段，通过SFT让模型掌握评判任务的基本功。\n        2.  **RL对齐：** 第二阶段，引入强化学习。但作者没有直接使用标准的RL，而是在奖励函数中创新性地加入了一个**“一致性奖励”**。这个奖励会检查模型生成的理由（通过规则或另一个模型解析）是否与最终的判断相符，从而显式地鼓励模型保持内在逻辑的一致性。\n\n*   **第三步：解决评估瓶颈——从“模糊”到“精确”**\n    *   **目标：** 为了公平、准确地衡量模型性能，并推动社区发展，必须建立一个黄金标准。\n    *   **核心洞察：** GUI任务的解往往不唯一。一个动作即使与原始轨迹不同，也可能同样有效。因此，单纯与“标准答案”比对是错误的。\n    *   **方法论设计：** 这就是**OS-Critic Bench**的构建思路。\n        1.  **跨平台采样：** 从移动、Web、桌面三大主流平台的现有基准中采样，确保覆盖面。\n        2.  **生成候选动作：** 使用一个性能均衡的代理（如Qwen2.5-VL）为每个截图生成一个待评判的动作。\n        3.  **人工介入：** 最关键的一步，邀请人类专家对每个动作的“正确性”进行最终裁决。这绕除了“多解性”带来的噪声，确保了基准标签的绝对可靠性。\n    *   **成果：** 一个包含738个高质量、人工标注的跨平台评判基准。\n\n#### **4. 最终整合与升华：从“一个模型”到“一个框架”**\n\n*   **系统化思考：** 作者意识到，他们所做的不仅仅是训练了一个模型，而是构建了一个从**数据生成 -> 模型训练 -> 性能评估**的完整闭环。\n*   **框架化呈现：** 因此，他们将整个工作包装成**OS-Oracle框架**。这个框架的核心价值在于，它为任何想要构建高质量GUI评判模型的研究者提供了一套可复现、可扩展的“全栈解决方案”。\n*   **价值验证：** 最后，通过在自建基准和在线环境（如OSWorld）中的大量实验，证明了其模型（OS-Oracle-7B）的SOTA性能，以及作为“预评判器”能有效提升其他代理（如UI-TARS）的任务成功率。这完成了从理论创新到实践价值的闭环。\n\n---\n\n**总结：** 作者的思考路径是一个典型的“**问题驱动、逐层深入、系统解决**”的学术创新过程。他们从一个宏观的技术趋势出发，敏锐地捕捉到关键的可靠性瓶颈，通过精准的问题归因，识别出数据和评估两大核心挑战。随后，他们没有停留在单一的技术点，而是通过“数据合成”、“一致性对齐训练”和“人工精标基准”三大创新，系统性地构建了一个完整的解决方案，最终将其升华为一个推动领域发展的综合性框架。"
                },
                {
                    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
                    "arxiv_id": "2512.16262",
                    "authors": "Yifei She, Ping Zhang, He Liu, Yanmin Jia, Yang Jing, Zijun Liu, Peng Sun, Xiangbin Li, Xiaohe Hu",
                    "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出了一种**“Agent-side Approach”**，旨在赋予LLM智能体一种新的能力——**时间感知**，使其能够与物理世界的异步操作同步。这并非将现有智能体框架简单应用于某个领域，而是**对智能体本身能力的构建和改进**。它解决了智能体在真实世界环境中面临的一个根本性问题（Temporal Gap），因此其本质是关于构建和改进LLM智能体的方法论，完全符合“保留”标准。 2.  **第二步：正面指标——高度匹配** 论文包含了多个核心关注点： *   **核心范式**: 论文明确聚焦于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: 论文的核心是关于智能体的**规划**能力。预测等待时间 (`time.sleep(t)`) 是智能体在异步环境中执行复杂任务规划的关键一环。同时，它扩展了 `Code-as-Action` 范式，属于 `Tool Use` 的范畴。 *   **演化机制**: 摘要最后明确指出，时间感知是智能体在开放环境中进行**“autonomous evolution”（自主演化）**的一项可学习的关键能力。这直接命中了您“自我演化”的研究焦点。 3.  **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性，也未以多模态或视觉为核心研究内容。因此，它没有被任何排除标准所排除。 4.  **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 这篇论文是关于智能体如何进行规划的典型范例。它研究的不是LLM基础的数学或逻辑推理，而是智能体在执行任务时如何处理环境的时间不确定性，这是一种高级的、与环境交互的规划能力，因此应**保留**。 *   **自我演化的应用**: 论文的核心是提出一种新的“时间感知”机制，并论证了这种机制对于“自主演化”至关重要。即使它在Kubernetes这个特定领域进行实验，其贡献也是普适性的智能体能力，因此完全符合保留条件。 **最终决策**: 这篇论文的核心贡献在于**构建和改进LLM智能体**，具体来说，是赋予智能体一种全新的、与物理世界同步的时间感知能力。该能力属于智能体的**规划**和**工具使用**范畴，并且被作者明确为智能体实现**自我演化**的关键一步。论文的研究内容与您“单智能体”和“自我演化”两个方向高度契合，是一篇高质量的前沿研究，应被筛选入内。",
                    "summary2": "本文旨在解决智能体在异步物理世界中因动作延迟导致的时间鸿沟问题。针对模拟Kubernetes集群中的异步任务场景，我们提出了一种基于Agent-side Approach的时间同步方法，通过扩展Code-as-Action范式，利用语义先验和In-Context Learning (ICL)预测精确的等待时长，并在模拟Kubernetes集群环境中通过Regret Score（量化查询开销与等待延迟）验证了其有效性。",
                    "summary_translation": "现实世界中的智能体任务，与同步马尔可夫决策过程不同，通常涉及具有可变延迟的非阻塞动作，在动作启动与完成之间产生了一个根本性的\\textit{时间差}。现有的环境端解决方案，例如阻塞式包装器或频繁轮询，要么限制了可扩展性，要么用冗余观测稀释了智能体的上下文窗口。在这项工作中，我们提出了一种\\textbf{智能体端方法}，该方法使大型语言模型能够主动将其\\textit{认知时间线}与物理世界对齐。通过将“代码即动作”范式扩展到时间领域，智能体利用语义先验和上下文学习来预测精确的等待时长（\\texttt{time.sleep(t)}），从而在无需详尽检查的情况下，有效地与异步环境同步。在模拟的Kubernetes集群中的实验表明，智能体能够精确校准其内部时钟，以最小化查询开销和执行延迟，这验证了时间感知是一种可学习的能力，对于在开放环境中的自主进化至关重要。",
                    "inspiration_trace": "好的，以下是对论文《Learning to Wait: Synchronizing Agents with the Physical World》核心思路的逻辑推演，旨在还原作者从观察到提出方法论的完整思考过程。\n\n---\n\n### **第一阶段：观察与问题定义——发现被忽视的“时间鸿沟”**\n\n1.  **宏观观察：** 作者首先注意到一个根本性的脱节。当前主流的AI智能体研究（如数学推理、代码生成）大多在**同步环境**中进行。在这些任务里，智能体“思考”和“执行”的时间本身就填补了任务完成所需的时间，即 `Tact ≈ Ttrue`。因此，一个核心问题被掩盖了：**真实世界是异步的**。\n\n2.  **聚焦现实场景：** 作者将目光投向了更实际的、与现实系统交互的智能体任务，如管理Kubernetes（K8s）集群。在这些场景中，一个动作（如`kubectl`命令）的**发起**是瞬时的，但其**完成**却需要经历一个漫长且不确定的后台过程。\n\n3.  **抽象核心矛盾：** 由此，作者提炼出了核心问题——**“时间鸿沟”**。即智能体的**认知时间线**（它认为动作何时完成）与环境的**物理时间线**（动作实际何时完成）之间存在巨大且可变的差异（`Tact ≪ Ttrue`）。这个鸿沟是现有智能体框架在真实世界中失效的根源。\n\n### **第二阶段：批判与范式转移——从“改造世界”到“适应世界”**\n\n1.  **审视现有解法（环境端方案）：** 作者分析了业界如何应对这个问题，发现主要有两种思路：\n    *   **阻塞式封装：** 让环境等待任务完成后再返回结果。这本质上是把异步问题强行变成同步问题。\n    *   **频繁轮询：** 智能体不断地问“完成了吗？”。\n\n2.  **洞察根本缺陷：** 作者敏锐地指出，这两种“环境端”的工程方案存在结构性缺陷：\n    *   **阻塞方案**牺牲了并发性和效率，是一种“削足适履”的笨办法，让智能体失去了自主性。\n    *   **轮询方案**虽然非阻塞，但代价高昂。它不仅消耗计算资源，更严重的是，它会用大量无意义的“PENDING”状态污染LLM宝贵的**上下文窗口**，稀释了有效信息，最终损害智能体的推理能力。\n\n3.  **提出新范式（智能体端方案）：** 基于对现有方案的批判，作者提出了一个根本性的范式转移：**不要让环境去适应智能体，而要让智能体主动去适应环境的异步性。** 这符合AGI（通用人工智能）的愿景，即智能体应具备在开放世界中自主学习和进化的能力，而不是依赖外部脚手架。\n\n### **第三阶段：核心假设与机制化——“学会等待”**\n\n1.  **形成核心假设：** 如果智能体必须适应物理世界，那么它需要一种新的能力——**时间感知能力**。作者的核心假设是：**这种时间感知能力是可以学习的**。智能体不应被动等待，而应主动预测并管理时间。\n\n2.  **寻找实现载体：** 如何让智能体“管理时间”？作者巧妙地借用了已有的**“Code-as-Action”**范式。既然智能体已经能生成代码来执行动作，那么它同样可以生成代码来**控制时间**。`time.sleep(t)`便成了最直接、最优雅的实现工具。\n\n3.  **定义核心方法：** 由此，核心方法诞生了：**Learning to Wait**。智能体的任务不再是“做什么”，而是“做完一件事后，应该精确地等待多久，再去做下一件事”。这标志着智能体从“任务执行者”向“时间规划者”的转变。\n\n### **第四阶段：构建学习路径——从“先验”到“反馈”**\n\n1.  **解决冷启动问题：** 智能体第一次遇到一个任务时，没有任何历史数据，如何等待？作者指出，可以利用LLM自身的**语义先验知识**。例如，智能体能理解“拉取一个轻量级镜像”比“拉取一个巨大的CUDA镜像”要快得多。这为初始的、粗略的等待时间预测提供了基础。\n\n2.  **实现动态校准：** 仅靠先验是不够的，因为真实延迟是可变的。如何让智能体不断优化其预测？作者引入了**上下文学习（ICL）**机制。将上一次任务的执行结果（等待了多久、是否成功、查询了多少次）作为反馈，放入下一次任务的提示中。\n\n3.  **形成闭环：** 这就构成了一个轻量级的学习闭环：**语义先验（初始猜测） → 执行与等待 → 获取反馈 → ICL（校准预测） → 下一次更精确的等待**。智能体无需更新模型权重，仅通过与环境的交互和反思，就能逐步校准自己的“内部时钟”，使其无限逼近物理世界的真实节奏。\n\n### **第五阶段：验证与升华——证明“时间意识”的可学习性**\n\n1.  **设计实验：** 为了验证上述假设，作者构建了一个模拟的异步环境（如K8s集群），其中不同任务具有不同且随机的延迟分布。这为测试智能体区分任务、动态校准时间的能力提供了可控的试验场。\n\n2.  **定义评估标准：** 作者设计了“Regret Score”这一指标，它同时惩罚了“查询过多”和“等待过久”，完美地衡量了智能体在时间同步上的综合效率。\n\n3.  **得出结论：** 实验结果表明，不同的LLM都能通过ICL逐步降低Regret Score，证明它们确实学会了如何等待。这不仅验证了方法的可行性，更重要的是，它证实了**时间意识是LLM一种可学习的、内生的能力**。这项工作因此超越了具体的技术方案，为构建能够在真实、异步世界中持续演进的自主智能体，奠定了一个不可或缺的理论基石。"
                },
                {
                    "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving",
                    "arxiv_id": "2512.16214",
                    "authors": "Jianming Liu, Ren Zhu, Jian Xu, Kun Ding, Xu-Yao Zhang, Gaofeng Meng, Cheng-Lin Liu",
                    "summary": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是一篇应用论文。虽然它的应用领域是PDE求解，但其核心贡献是构建了一个名为“PDE-Agent”的**新型多智能体协作框架**。论文明确指出，它引入了“the first toolchain-augmented multi-agent collaboration framework”，并详细描述了其内部的“Prog-Act框架”和“Resource-Pool”等创新机制。这完全符合“核心贡献在于构建、改进LLM智能体”的要求。它不是简单地将一个已有的智能体框架拿来用，而是提出了新的方法论和架构。 2.  **第二步：正面指标** - 该论文包含了大量您关注的核心指标： - **核心范式**: `Multi-Agent Systems (MAS)` 是论文的核心。 - **智能体能力**: 论文明确提到了 `Planning` (“dynamic planning”)、`Tool Use / Tool Augmentation` (“toolchain-augmented”, “multi-tool collaboration”)、`Memory` (“graph memory”) 和 `Self-Correction` (“error correction via dual-loop mechanisms”)。 - **多智能体**: `Collaboration` 是贯穿全文的主题。 3.  **第三步：排除标准** - 论文不涉及安全与对齐、多模态与视觉等排除领域。它的焦点完全集中在智能体的架构和协作机制上。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文提出的“Prog-Act框架”正是关于智能体如何进行“动态规划”和“错误修正”的，这完全符合保留条件，因为它是在智能体框架的层面讨论规划，而非提升LLM本身的基础推理能力。 - **自我演化的应用**: 这篇论文虽然不是关于“自我演化”，但它完美地诠释了“核心是提出一种新机制，即使应用在特定领域也应保留”的原则。PDE求解在这里是验证其多智能体框架有效性的**实验平台**，而非研究的最终目的。论文的贡献在于这个框架本身，这个框架可以被迁移到其他需要复杂、多步骤、工具链协作的任务中。 **最终决策**：综合以上分析，这篇论文的核心贡献是构建了一个创新的、具备规划、记忆、工具使用和自我修正能力的多智能体框架。它直接推动了Agentic AI，特别是多智能体系统领域的发展。因此，它精准地符合您关于“LLM智能体及其演化”的研究课题，应被判定为 **True**。",
                    "summary2": "本文旨在实现从自然语言描述出发的完全自动化的偏微分方程（PDE）求解。针对复杂的、多步骤、跨步骤依赖的PDE求解任务，我们提出了一种名为PDE-Agent的工具链增强多智能体框架，其核心创新包括用于多智能体协作的Prog-Act框架和用于多工具协调的Resource-Pool机制。我们在自建的PDE-Bench基准上，通过多级评估指标（特别是逻辑协作过程评估）和任务成功率验证了其有效性。",
                    "summary_translation": "求解偏微分方程是工程与科学研究的基石。传统的偏微分方程求解方法流程繁琐，依赖于手动设置和领域专业知识。尽管物理信息神经网络引入了基于神经网络的端到端解决方案，且 DeepXDE 等框架进一步提升了自动化程度，但这些方法仍依赖于专家知识，缺乏完全的自主性。在本工作中，我们将偏微分方程求解构建为由大型语言模型驱动的智能体所执行的工具调用任务，并据此提出了PDE-Agent。PDE-Agent是首个工具链增强的多智能体协作框架，它继承了LLMs的推理能力与外部工具的可控性，能够根据自然语言描述实现偏微分方程的自动化求解。PDE-Agent通过两项关键创新，充分利用了多智能体与多工具协作的优势：(1) 一个用于多智能体协作的Prog-Act框架，该框架集成了图记忆，并通过双循环机制（局部修复与全局修订）实现有效的动态规划与错误纠正。(2) 一个用于多工具协作的Resource-Pool，其集成了工具-参数分离机制。该机制实现了运行时产物的集中化管理，并解决了现有框架中工具间的依赖性鸿沟。为验证和评估这一偏微分方程求解新范式，我们开发了PDE-Bench——一个面向基于智能体的工具协作求解的多类型偏微分方程基准，并提出了用于评估工具协调性的多级指标。评估结果表明，PDE-Agent在复杂的多步骤、跨步骤依赖任务上展现出卓越的适用性与性能。这种工具链增强的多智能体偏微分方程求解新范式，将推动自动化科学计算领域的未来发展。我们的源代码与数据集将公开发布。",
                    "inspiration_trace": "好的，以下是对《PDE-Agent》一文作者核心思路的逻辑推演，旨在还原其从观察到方法论的完整思考过程。\n\n---\n\n### **第一步：宏观问题与核心矛盾——自动化的“最后一公里”**\n\n作者的思考始于一个科学计算领域的经典痛点：**偏微分方程（PDE）求解的自动化瓶颈**。\n\n1.  **观察现状**：传统数值方法（FEM, FDM）计算成本高且严重依赖人工设置。以PINNs为代表的深度学习方法虽然实现了“端到端”，但“端到端”不等于“全自动”。研究者仍需手动编码PDE形式、调整网络结构、设计训练策略，这本质上只是将一种专业知识（数值分析）替换为另一种（深度学习），并未降低使用门槛。\n\n2.  **提炼核心矛盾**：现有方法，无论是传统还是智能，都未能实现**从自然语言描述到最终解决方案的完全自主闭环**。它们卡在了“最后一公里”——即如何将人类的高层意图（一段话描述的物理问题）自动转化为精确、可执行的求解流程。这构成了研究的根本动机。\n\n### **第二步：范式转移的引入——从“模型即解”到“智能体即协调者”**\n\n面对上述矛盾，作者没有在PINNs的框架内进行修补，而是寻求一个全新的范式。\n\n1.  **借鉴通用AI进展**：作者观察到，大型语言模型（LLMs）在通用领域展现了强大的推理和规划能力。然而，直接让LLM进行复杂的科学计算是不可靠的（幻觉、数值不精确）。\n\n2.  **关键洞见**：人类专家解决复杂问题时，并非事事亲为，而是**使用工具**（计算器、仿真软件、绘图工具）。同理，LLM的真正价值不在于成为计算器，而在于成为**能够调用和协调各种专业工具的“大脑”**。\n\n3.  **形成新范式假设**：将PDE求解问题重新定义为**“一个由LLM驱动的智能体，通过调用外部工具链来完成的多步骤任务”**。这个范式转移的核心是：**LLM负责“思考”（规划、决策），工具负责“行动”（精确计算、符号操作）**。这为解决自动化瓶颈提供了全新的路径。\n\n### **第三步：聚焦新范式的内在缺陷——从“单智能体”到“多智能体协作”**\n\n新范式虽好，但直接应用现有工具调用框架（如ReAct, Pre-Act）会暴露新问题。\n\n1.  **分析现有框架的不足**：\n    *   **ReAct**：缺乏长期规划，走一步看一步，容易在复杂任务中迷失方向。\n    *   **Pre-Act/OctoTools**：虽有全局规划，但其“逐步验证”机制在稳定的多工具协作中显得过于冗余和低效，且对跨步骤的长程依赖管理不佳。\n\n2.  **提出核心挑战**：如何设计一个既能保证全局一致性，又能高效处理局部错误的协作机制？单一智能体试图承担规划、执行、验证所有职责，认知负荷过重，系统脆弱。\n\n3.  **解决方案的演进——多智能体分工**：作者借鉴人类团队协作模式，将单一智能体的复杂职责分解为多个专门化的智能体：\n    *   **Planner**：负责高层战略规划。\n    *   **Parser/Solver**：负责具体任务的解析和配置。\n    *   **Executor**：负责工具的执行。\n    *   **Orchestrator**：作为“认知精英”，负责全局监控和纠错。\n\n    这种分工使得系统结构更清晰，为后续的精细化机制设计奠定了基础。\n\n### **第四步：解决协作中的具体痛点——两大核心创新**\n\n在多智能体框架下，作者进一步聚焦并解决了两个关键的技术痛点。\n\n1.  **痛点一：验证的效率与鲁棒性权衡**\n    *   **问题**：逐步验证太慢，端到端验证太脆。如何平衡？\n    *   **思考与洞见**：错误分为“局部小错”和“全局性错误”。前者无需打乱全局计划，后者则需要重新规划。因此，需要一个**分层、双循环的错误处理机制**。\n    *   **创新方案**：提出**Prog-Act框架**。\n        *   **内循环（局部修复）**：由Orchestrator在关键节点进行验证，或允许其他智能体主动求助，实现快速、局部的错误修正。\n        *   **外循环（全局修订）**：当内循环无法解决时，Orchestrator挑战Planner，触发全局计划的重新制定。\n        *   **辅助机制**：引入**图记忆**来追踪工具链和数据流，使错误定位和影响分析更精确。\n\n2.  **痛点二：工具间的“隐性”依赖**\n    *   **问题**：在多工具协作中，一个工具的输出（如一个训练好的神经网络模型对象、一个网格对象）无法被简单地序列化为文本传递给下一个工具。现有框架只关注显式参数传递，忽略了这些运行时产生的“隐性”资源。\n    *   **思考与洞见**：需要一个中央化的资源管理机制，作为所有工具共享的“工作台”。\n    *   **创新方案**：提出**Resource-Pool（资源池）**。所有工具的运行产物都存入池中，后续工具通过标识符从中调用。这彻底解决了工具链间的数据传递和依赖问题，实现了真正的无缝协作。\n\n### **第五步：构建验证体系——从“结果导向”到“过程与结果并重”**\n\n为了证明新范式的优越性，作者意识到必须建立与之匹配的评估体系。\n\n1.  **现有评估的局限**：Pass@K等指标只关心“最终是否成功”，无法评估“解决过程的好坏”。对于PDE-Agent这种强调过程协作的框架，这种评估是片面的。\n\n2.  **构建新评估体系的思考**：\n    *   **数据层面**：需要一个包含“自然语言描述”和“标准工具调用链”的基准。因此，作者构建了**PDE-Bench**。\n    *   **指标层面**：必须量化“协作过程”的质量。作者将工具链抽象为图，并从**全局（任务成功率）、局部（工具调用准确性）、逻辑（工具链结构合理性）**三个层次设计了全新的评估指标，特别是基于图论的逻辑评估指标。\n\n### **总结：完整的逻辑链**\n\n作者的思考路径呈现出一个清晰的“问题-范式-框架-创新-验证”的演进链条：\n\n1.  **起点**：发现PDE求解自动化的“最后一公里”难题。\n2.  **跃迁**：引入“LLM智能体+工具调用”的新范式，将问题转化为多步骤任务规划与执行。\n3.  **深化**：针对新范式的不足，从单智能体演进到分工明确的多智能体协作框架。\n4.  **攻坚**：在多智能体框架下，精准定位并解决了“验证效率”和“工具依赖”两大核心痛点，分别提出了Prog-Act和Resource-Pool两大创新。\n5.  **闭环**：为了科学地评估这一整套新方法，构建了全新的数据集（PDE-Bench）和多维评估指标，完成了从理论创新到实证验证的闭环。\n\n整个过程体现了作者从宏观问题出发，不断聚焦、层层递进，最终形成一套完整、自洽且具有创新性的方法论。"
                },
                {
                    "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs",
                    "arxiv_id": "2512.16149",
                    "authors": "Hao Chen, Zhexin Hu, Jiajun Chai, Haocheng Yang, Hang He, Xiaohan Wang, Wei Lin, Luhang Wang, Guojun Yin, Zhuofeng zhao",
                    "summary": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM或智能体作为工具去解决一个外部领域问题，而是提出了一种**构建和改进LLM智能体的新方法论**。其核心贡献是`ToolForge`，一个用于合成高质量训练数据的**框架**。这个框架的目的是为了让LLM更好地掌握**工具使用**这一核心智能体能力。因此，它直接命中了“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个您关注的核心范式和能力指标： - **智能体能力**: 论文的核心是`Tool Use / Tool Augmentation`（工具使用）。此外，摘要明确指出其生成的数据通过`multi-hop reasoning`（多跳推理）和`self-reflection mechanisms`（自我反思机制）进行了丰富。这三者都是您在“单智能体”方向下明确列出的关键子方向。 - **核心范式**: 整个`ToolForge`框架可以被视为一种`Agentic AI`的赋能技术，因为它专注于提升智能体的核心能力。 3.  **第三步：排除标准** - 论文的主要贡献不涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐问题。 - 论文也未将`Vision`或多模态作为研究核心，其焦点完全在文本和工具调用上。 - 因此，该论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文讨论的`multi-hop reasoning`（多跳推理）是智能体在复杂任务中执行一系列步骤的能力，这与`ReAct`或`ToT`等Agentic框架中的规划/推理思想一致。它不是在提升LLM本身的基础数学或逻辑能力，而是在提升智能体在**工具使用场景下的规划和推理能力**。因此，这符合“保留”的条件。 **最终决策**: 这篇论文的核心贡献是`ToolForge`，一个旨在**提升LLM智能体工具使用、多跳推理和自我反思能力**的数据合成框架。它直接贡献于您研究课题中的“单智能体”方向，提出了一个新颖的、用于改进智能体核心能力的方法论。它不是应用型、安全型或基础设施型研究，因此完全符合您的筛选要求。",
                    "summary2": "",
                    "summary_translation": "",
                    "inspiration_trace": ""
                },
                {
                    "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
                    "arxiv_id": "2512.16300",
                    "authors": "Fanrui Zhang, Qiang Zhang, Sizhuo Zhou, Jianwen Sun, Chuanhao Li, Jiaxin Ai, Yukang Feng, Yujie Zhang, Wenjie Li, Zizhen Li, Yifan Chang, Jiawei Liu, Kaipeng Zhang",
                    "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献在于构建和改进一个具有自我演化能力的LLM智能体。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质不是简单地将LLM应用于图像伪造检测领域，而是提出了一种全新的智能体框架 **ForenAgent**。该框架的核心机制是让MLLM能够“**自主生成、执行和迭代优化**”基于Python的低级工具。这直接命中了你筛选标准中的“构建、改进或演化 LLM智能体”的核心目标。它不是非演化型应用，因为其核心创新点在于智能体的工作流和自我完善机制，而非应用本身。 **第二步：正面指标——高度匹配** 论文包含了大量你的核心关注点： *   **核心范式**: 论文明确提出了一个 `Agentic AI` 框架。 *   **智能体能力**: 核心贡献是 `Tool Use / Tool Augmentation`（自主生成和执行工具）。同时，`iteratively refine` 和 `reflective reasoning` 直接对应了 `Self-Refine` 和 `Self-Reflection`。其设计的 `dynamic reasoning loop`（动态推理循环）是智能体规划和推理能力的体现。 *   **演化机制**: `iteratively refine`（迭代优化）是 `Self-Improvement` 和 `Iterative Improvement` 的典型范例，属于自我演化的范畴。 **第三步：排除标准——未触发** *   **安全与对齐**: 论文虽然提到了“interpretable”（可解释性），但这是作为其智能体框架带来的一个优点，而非论文的主要研究贡献。论文的核心是构建智能体，而不是研究可解释性本身。 *   **多模态与视觉**: 论文确实使用了MLLMs来处理图像，但这完全符合你设定的例外情况——“除非它们被用作智能体感知环境的工具，而不是研究的核心”。在这里，视觉输入是智能体需要分析和处理的环境信息，而论文的核心是智能体如何通过生成和使用工具来分析这些信息，而不是研究MLLMs的视觉能力本身。 **第四步：处理特殊和模糊情况——符合保留规则** *   **推理/规划**: 论文提出的“动态推理循环”是典型的智能体规划和推理框架，而非单纯提升LLM的基础推理能力，因此应保留。 *   **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外情况的完美范例。它的核心贡献是提出了一种新的“自我演化”机制（即迭代优化工具），并将其应用在图像伪造检测这一特定领域。根据你的规则，这种情况应该保留。 **最终决策** 综合以上分析，这篇论文的核心是构建一个名为ForenAgent的单智能体系统，它具备先进的工具使用和自我迭代优化（自我演化）能力。尽管其应用场景是图像伪造检测，但其方法论和框架创新完全聚焦于Agentic AI的构建与演化，与你的研究课题高度契合。因此，最终判断为 **True**。",
                    "summary2": "本文旨在解决现有图像伪造检测（IFD）方法难以有效融合低层级伪影与高层级语义知识的问题。针对包含真实、合成和篡改图像的复杂取证场景，我们提出了一种名为ForenAgent的多轮交互式框架，它赋能多模态大语言模型（MLLM）自主生成、执行并迭代优化基于Python的低层级取证工具，并在自建的大规模异构数据集FABench和公开的SIDA-Test数据集上通过准确率和F1分数等指标验证了其有效性。",
                    "summary_translation": "现有的图像伪造检测方法要么利用低层次的、与语义无关的伪影，要么依赖于具有高层次语义知识的多模态大语言模型。尽管这两种信息流天然互补，但它们在范式和推理上都表现出高度的异质性，这使得现有方法难以将二者统一，或有效地建模它们之间的跨层级交互。为解决这一空白，我们提出了ForenAgent，一个多轮交互式的图像伪造检测框架，该框架使多模态大语言模型能够围绕检测目标，自主地生成、执行和迭代优化基于Python的低层次工具，从而实现更灵活、可解释的伪造分析。ForenAgent采用一个两阶段的训练流程，结合了冷启动和强化微调，以逐步增强其工具交互能力和推理适应性。受人类推理的启发，我们设计了一个包含全局感知、局部聚焦、迭代探查和整体裁决的动态推理循环，并将其实例化为一种数据采样策略和一种与任务对齐的过程奖励。为了进行系统性的训练与评估，我们构建了FABench，一个异构的、高质量的智能体取证数据集，包含10万张图像和约20万个智能体交互问答对。实验表明，在低层次工具的辅助下，ForenAgent在具有挑战性的图像伪造检测任务上展现出新兴的工具使用能力和反思性推理，为实现通用的图像伪造检测指明了一条有前景的道路。代码将在审查流程完成后发布。",
                    "inspiration_trace": "好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出其核心方法“ForenAgent”的逻辑链，还原其从观察到最终方法论的思考过程。\n\n---\n\n### **作者产出《Code-in-the-Loop Forensics》的思考逻辑推演**\n\n#### **第一步：洞察核心矛盾——两种范式的割裂与互补性**\n\n作者的思考始于对图像伪造检测（IFD）领域现状的宏观观察。他们敏锐地注意到，整个领域被清晰地划分为两个阵营：\n\n1.  **低层级方法**：这类方法如同“法证化学家”，专注于像素级的、与语义无关的物理痕迹（如JPEG压缩伪影、噪声不一致性）。它们在特定场景下精确、可解释，但视野狭窄，难以应对复杂多变的伪造手法。\n2.  **多模态大模型（MLLM）方法**：这类方法如同“经验丰富的侦探”，利用高层级语义知识和常识进行推理。它们泛化能力强，但往往“眼高手低”，缺乏对细粒度物理痕迹的分析能力，其决策过程如同一个“黑箱”，缺乏透明度。\n\n**核心洞察**：作者发现，这两种方法在能力上**天然互补**。一个理想的IFD系统，应该既具备侦探的宏观洞察力，又拥有化学家的微观分析能力。然而，现实是它们在**范式和推理上高度异构**，像两条永不相交的平行线。这构成了整个研究要解决的根本矛盾。\n\n#### **第二步：定位根本症结——缺乏统一的推理框架**\n\n在识别了核心矛盾后，作者进一步追问：为什么这两种互补的信息流难以融合？\n\n他们得出的结论是：**问题不在于工具本身，而在于使用工具的“大脑”**。现有的MLLM方法本质上是“端到端”的分类器，其内部没有编码一个结构化的、类似人类专家的**取证推理流程**。MLLM不知道“何时”该用低层级工具，“如何”用，以及“如何解读”工具的输出并将其与自己的语义推理相结合。\n\n**根本症结**：缺乏一个能够**动态连接高层语义推理与低层工具控制**的统一框架。MLLM被当作一个被动的分类器，而不是一个主动的、能够自主规划调查步骤的“代理”。\n\n#### **第三步：提出核心假设——让MLLM成为“法证代理”**\n\n基于以上分析，作者的思想发生了关键性跃迁：**我们不应再试图“融合”两种方法，而应将MLLM升级为一个能够自主“调度”低层级工具的智能代理。**\n\n这个核心假设包含几个关键构想：\n\n1.  **自主性**：代理不应被动接受输入，而应能主动生成调查计划。\n2.  **工具使用**：代理的核心能力是调用外部工具。为了极致的灵活性，工具不应是固定的API，而应是**动态生成的Python代码**。这使得代理可以即时组合、定制分析流程。\n3.  **迭代式推理**：代理的决策过程应是多轮的、迭代的。它根据上一步工具的输出，调整下一步的行动，形成一个“观察-假设-验证-修正”的闭环。\n\n这个假设将研究目标从“设计一个更好的检测模型”转变为“**构建一个能够进行法证调查的智能体**”。\n\n#### **第四步：设计实现路径——从“代理”到“可训练的系统”**\n\n有了核心假设，下一步就是如何将其实现为一个可训练、可验证的系统。作者的设计思路遵循了“模仿人类专家”的原则，并分解为四个关键组件：\n\n1.  **构建“工具箱”**：既然要让代理使用工具，首先得有工具。作者系统性地梳理了经典IFD领域的低层级方法，将其抽象为12个可调用的Python工具（如FFT高频残差、JPEG鬼影图等）。这为代理提供了“法证实验室”。\n\n2.  **抽象“推理流程”**：人类法证专家是如何工作的？作者将其工作流抽象为四个阶段：\n    *   **全局感知**：先看整体，找可疑区域。\n    *   **局部聚焦**：放大可疑区域，进行细致观察。\n    *   **迭代探查**：针对该区域使用多种工具进行交叉验证。\n    *   **整体裁决**：综合所有证据，给出最终结论。\n    这个四步循环不仅成为了代理的“心智模型”，还巧妙地被设计为后续训练中的**过程奖励**，引导模型学习正确的推理习惯。\n\n3.  **创建“训练场”**：要训练这样的代理，现有数据集已过时。作者构建了**FABench**，一个包含最新生成模型（如GPT-4o, Midjourney-v7）产出的高质量、异构数据集。更重要的是，它不仅是图像+标签，而是包含了**代理交互轨迹**的数据，为训练提供了“教材”。\n\n4.  **设计“训练策略”**：如何让一个MLLM学会如此复杂的推理行为？作者采用了两阶段训练：\n    *   **冷启动**：先用强大的“老师模型”（GPT-4.1）在FABench上生成大量高质量的“推理-工具使用”范例，让学生模型（如Qwen2.5-VL）进行模仿学习，快速掌握基本技能。\n    *   **强化微调（RFT）**：模仿学习只能学会“形”，无法学会“神”。作者引入强化学习，特别是设计了与四步推理流程对齐的**过程奖励**。这激励模型不仅追求答案正确，更追求**推理过程的逻辑性和有效性**，从而涌现出自我修正等高级能力。\n\n#### **第五步：验证与升华——从“有效”到“涌现智能”**\n\n最后，通过实验验证整个设计。实验结果不仅证明了ForenAgent在检测精度上的优越性，更重要的是，通过可视化案例（如图1和图4），作者展示了模型**涌现出的类人推理行为**，如：\n\n*   **自主规划**：从全局扫描到局部聚焦的完整证据链。\n*   **自我修正**：在初步判断错误后，能根据工具反馈调整焦点，最终得出正确结论。\n\n这些“涌现行为”是作者思想逻辑的最终闭环，它证明了ForenAgent不仅仅是一个性能更好的分类器，而是一个真正具备了**初步法证推理能力的智能代理**，从而为通用的、可解释的图像伪造检测开辟了一条全新的道路。\n\n---\n\n**总结**：作者的思考路径是一个从**观察现象（范式割裂）** -> **诊断问题（缺乏统一框架）** -> **提出假设（构建法证代理）** -> **系统设计（工具箱、推理流、数据集、训练法）** -> **验证升华（证明涌现智能）** 的完整逻辑链条。其核心创新在于将研究视角从“模型设计”提升到了“智能体构建”，通过引入代码环内和过程驱动的强化学习，成功地在高层语义与底层物理证据之间架起了一座可学习的桥梁。"
                },
                {
                    "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning",
                    "arxiv_id": "2512.15943",
                    "authors": "Polaris Jhandi, Owais Kazi, Shreyas Subramanian, Neel Sendas",
                    "summary": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非将LLM智能体作为工具应用到一个新领域，而是提出了一种**改进LLM智能体核心能力**的方法论。具体来说，它专注于提升智能体的“工具调用”能力，并通过使用小语言模型（SLM）进行有针对性的微调来实现这一目标。这直接属于“构建、改进或演化LLM智能体”的范畴，因此符合保留标准。它不是非演化型应用，也不是非Agentic的推理或基础设施研究。 2.  **第二步：正面指标——高度相关** 论文命中了多个核心正面指标： *   **核心范式**: 标题和摘要明确提到了 `Agentic Tool Calling`，直接指向 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**: 论文的核心是 `Tool Use / Tool Augmentation`。工具使用是智能体自主执行任务、与环境交互的关键能力，是我研究焦点中“单智能体”方向的核心子方向之一。论文在 `ToolBench` 这个专门评估工具使用能力的基准上进行测试，进一步证实了其相关性。 3.  **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等排除领域。其焦点是智能体的效率和性能，完全在我的研究范围内。 4.  **第四步：处理特殊和模糊情况——符合保留条件** 论文涉及“推理/规划”的特殊情况。工具调用是智能体进行多步推理和规划（如ReAct框架）中的关键一环。这篇论文并非在提升LLM的基础数学或逻辑能力，而是在**优化智能体在执行任务时的工具选择和调用这一推理行为**。因此，它符合“保留关于智能体如何进行规划或在复杂任务中进行多步推理”的规则。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一种通过微调小语言模型来**高效地执行智能体工具调用任务**的方法，并证明了其在特定基准上可以超越大型模型。这直接对应了我研究目标中的“**改进LLM智能体**”，特别是“单智能体”方向下的“**工具使用**”能力。尽管论文的切入点是“效率”和“成本优化”，但其本质是**对智能体核心组件的构建与改进**，因此完全符合我的筛选要求。",
                    "summary2": "本文旨在解决大型语言模型在工具调用任务中成本高昂、效率低下的问题，实现用小型语言模型替代大型模型的目标。针对ToolBench数据集中的工具调用场景，我们提出了一种基于监督微调（SFT）的针对性优化方法，并在ToolBench基准上通过Pass Rate指标验证了其有效性。",
                    "summary_translation": "随着组织规模化采用生成式AI，模型成本优化与运营效率已成为决定其可持续性与可及性的关键因素。尽管大型语言模型在多样化任务中展现出卓越能力，但其庞大的计算需求使其在常规企业应用中成本过高。这一局限性推动了对小型语言模型的探索，这类模型可在特定应用中提供可媲美的性能，同时大幅降低基础设施开销（Irugalbandara et al., 2023）。本研究中，我们探究了用优化后的小型语言模型替代由大型语言模型驱动的工作流程的可行性。我们训练了一个领域适配的小型语言模型，用以执行传统上由大型语言模型处理的代表性任务，如文档摘要、问答和结构化数据解读。在实验部分，我们使用 Hugging Face TRL (Transformer Reinforcement Learning, 即Transformer强化学习) 库中的监督微调训练器，对 facebook/opt-350m 模型进行了微调研究（仅进行单轮训练）。OPT-350M 模型由 Meta AI 于 2022 年发布，是 OPT (Open Pretrained Transformer, 即开放预训练Transformer) 模型家族的一员。有类似研究指出，即便是 350M 参数规模的模型，也能有效贡献于指令调优流程（Mekala et al., 2024）。实验结果表明，我们微调后的小型语言模型在 ToolBench 评估中取得了 77.55% 的通过率，性能表现卓越，显著优于所有基线模型，包括 ChatGPT-CoT (26.00%)、ToolLLaMA-DFS (30.18%) 和 ToolLLaMA-CoT (16.27%)。这些发现强调了，通过对小型语言模型进行精心设计与针对性训练，可以显著降低其应用门槛，从而实现生成式AI在生产系统中经济高效的大规模集成。",
                    "inspiration_trace": "好的，以下是我基于您提供的论文内容，对作者思考过程的系统性推演。我将从一个宏观的行业痛点出发，逐步聚焦，还原其从观察到假设，再到最终方法论的完整逻辑链条。\n\n---\n\n### **作者核心思路的逻辑推演**\n\n#### **第一步：宏观观察——生成式AI的“成本鸿沟”**\n\n作者的思考始于一个普遍的行业观察：**生成式AI的巨大潜力与其高昂的运营成本之间存在尖锐矛盾。**\n\n*   **现象感知**：大型语言模型（LLM）能力强大，但在企业级大规模应用中，其计算资源消耗、API调用费用和基础设施投入构成了巨大的经济壁垒。这使得许多组织，尤其是那些需要高频、常规化AI任务的企业，难以承受。\n*   **问题提炼**：核心矛盾不再是“AI能否做到”，而是“AI能否经济、高效地做到”。这促使作者思考：是否存在一种路径，既能保留AI的核心能力，又能大幅降低其部署成本？\n\n#### **第二步：方向聚焦——从“大而全”到“小而精”**\n\n面对成本问题，最直接的思路是寻找更轻量化的解决方案。作者将目光投向了小语言模型（SLM）。\n\n*   **初步假设**：SLM虽然在通用能力上不及LLM，但它们在计算效率和部署成本上具有天然优势。那么，能否让SLM在某个**特定领域**或**特定任务**上，达到甚至超越LLM的水平？\n*   **任务选择**：为什么选择“代理工具调用”？\n    1.  **高价值性**：工具调用是AI Agent实现自动化、与外部世界交互的核心能力，具有极高的应用价值。\n    2.  **结构化**：该任务有明确的输入输出格式（如`Thought-Action-Input`），逻辑链条清晰，比开放性对话更容易被模型学习和评估。\n    3.  **成熟基准**：已有公认的评估基准（如ToolBench），可以进行严谨、可量化的对比，避免了主观评价的模糊性。\n\n至此，作者的研究方向从“如何降低AI成本”聚焦到了**“SLM能否通过针对性训练，在工具调用这一关键任务上超越LLM”**。\n\n#### **第三步：核心假设——提出“专注度优于规模”的反直觉观点**\n\n在确定了研究方向后，作者提出了一个挑战行业主流“规模定律”的核心假设：\n\n*   **传统观念**：模型性能与参数规模正相关。更大的模型拥有更强的泛化和推理能力。\n*   **作者的反直觉假设**：对于**边界清晰、模式固定**的任务（如工具调用），模型的“专注度”比“规模”更重要。一个通用LLM的庞大参数被稀释在无数种语言知识和通用推理模式上，而在工具调用这一特定场景下，这种“广度”反而可能成为“劣势”（如生成冗余文本、格式错误）。相反，一个SLM如果将所有参数都“聚焦”于学习工具调用的精确模式，可能会实现更高的效率和准确率。\n\n这个假设是全文的灵魂，它将研究从简单的“降本增效”提升到了对“智能实现路径”的探讨。\n\n#### **第四步：方法论设计——一场“以小博大”的验证实验**\n\n为了验证上述假设，作者设计了一个目标明确、对比强烈的实验方案。\n\n1.  **选择“极小”的模型**：选用`OPT-350M`（3.5亿参数）。这个选择极具策略性，因为它与基线模型（7B到175B）形成了数百倍的规模差距。如果它能成功，将极具说服力地证明“专注度”的力量。\n2.  **采用“精准”的训练策略**：选择**有监督微调（SFT）**。这是一种直接、高效的方法，能让模型通过大量范例，精确学习工具调用的格式、逻辑和API使用模式。这完全服务于“专注度”这一核心思想。\n3.  **进行“公平”的正面比较**：使用与评估基准完全一致的`ToolBench`数据集进行训练，并在标准化的`ToolEval`框架下，与强大的基线模型（如ChatGPT、ToolLLaMA）进行同场竞技。这确保了结果的公正性和可比性。\n\n整个方法论的设计逻辑是：**创造一个极端的对比环境（小模型 vs 大模型），用最直接的方式（SFT），在最公平的赛道（ToolBench）上，检验核心假设（专注度 > 规模）。**\n\n#### **第五步：结果解读与洞见提炼——从“现象”到“本质”**\n\n实验结果（77.55%的通过率）远超预期，强力验证了假设。但作者并未止步于此，而是深入剖析了背后的原因，提炼出更深层次的洞见：\n\n*   **现象**：小模型赢了。\n*   **本质原因**：\n    1.  **参数效率**：SLM的参数没有被“浪费”，100%服务于工具调用任务，实现了“参数-任务”的最优对齐。\n    2.  **行为专注**：SLM学会了抑制与任务无关的行为（如创造性解释），严格遵循高效的`Thought-Action`模式。而LLM的“通用智能”在特定任务上表现为“行为冗余”。\n    3.  **评估对齐**：SLM的训练目标与评估目标高度一致，它就是为“通过考试”而生的专家。\n\n最终，作者将这一发现升华为一种新的范式：**对于特定任务，智能的实现路径并非只有“暴力扩展”一条路，通过“精准设计”和“针对性训练”，小模型同样可以达到顶尖水平，从而实现成本与性能的双重突破。**\n\n---\n\n**总结**，作者的思考路径是一个典型的“问题驱动、假设先行、实验验证、洞见升华”的学术创新过程。它始于一个现实的商业痛点，通过层层聚焦，最终在一个具体的技术点上取得了突破，并由此提炼出对AI发展路径具有启发性的新观点。"
                },
                {
                    "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting",
                    "arxiv_id": "2512.16022",
                    "authors": "Defu Cao, Michael Gee, Jinbo Liu, Hengxuan Wang, Wei Yang, Rui Wang, Yan Liu",
                    "summary": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将LLM应用于时间序列预测，而是提出了一种**新的智能体框架**。其核心贡献在于将LLM重新定位为一个“智能法官”，这个法官能够评估、解释并策略性地协调一个由多个基础模型组成的集成系统。这本质上是在**构建一个具有特定功能的LLM智能体**，而不是将LLM作为黑盒工具直接应用。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **Agentic AI / LLM-based Agents**: 摘要中直接使用了 \"intelligent judge\" 和 \"trained agent\" 来描述LLM的角色。 - **Self-Reflection / Self-Correction**: 智能体需要 \"explain its weighting decisions\"，这是一种自我反思和解释其行为的能力。 - **Self-Improvement / Iterative Improvement**: 摘要提到智能体会 \"adaptively refine the optimization strategy\"，这是一种基于反馈和对话进行自我完善和迭代优化的机制，完全符合“自我演化”的定义。 3.  **第三步：排除标准** - **安全与对齐**: 论文虽然提到了 \"Explainable\" (可解释性)，但这并非其**主要贡献**。论文的核心是提出一个能够**产生**解释的智能体架构，而不是提出一种新的可解释性（XAI）方法本身。可解释性是该智能体框架的一个关键特性和输出，用于支持其决策过程，因此不应因此被排除。 - **多模态与视觉**: 不涉及。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”的一个绝佳范例。虽然它应用在时间序列这个特定领域，但其核心贡献是提出了一种新的“自我演化”机制——即通过迭代对话和自适应优化来完善策略。根据你的筛选规则，这种情况应该**保留**。 - **推理/规划**: 论文中的智能体并非进行简单的数学或逻辑推理，而是在一个复杂任务（协调模型集成）中进行多步决策和规划，并根据结果进行调整。这完全符合保留标准。 **最终决策**: 综合以上分析，该论文的核心贡献在于**构建了一个能够自我反思、自我优化的LLM智能体**，用于协调和决策。它直接命中了你研究范围中的“单智能体”和“自我演化”两个核心方向。尽管它以时间序列预测为应用场景，但其创新点在于智能体架构本身，而非应用本身。因此，这篇论文高度相关，应被**保留 (True)**。",
                    "summary2": "本文旨在解决单一时间序列基础模型性能不稳定且预测过程不透明的问题，实现高效且可解释的预测。针对多个时间序列基础模型构成的候选池，我们提出了一种TSOrchestra框架，将LLM重新定位为智能法官，通过SHAP引导的R1风格微调，使其能够评估、解释并协调多个基础模型的集成权重。在GIFT-Eval benchmark上通过CRPS和MASE指标验证了其有效性。",
                    "summary_translation": "时间序列基础模型的激增导致没有任何单一方法能够实现持续的优势，这使得核心挑战不再是寻找最佳模型，而是构建一个具有可解释性的最优集成。尽管大语言模型具备强大的推理能力，但将其直接应用于时间序列预测已被证明是无效的。为解决这一问题，我们将LLM的角色重塑为一个智能裁判，用以评估、解释并策略性地协调一个由基础模型构成的集成。为克服LLM在时间序列领域固有的知识匮乏，我们引入了一种R1风格的微调过程。该过程由基于SHAP的忠实度分数引导，旨在训练模型将集成权重解释为关于时间动态的有意义的因果陈述。经过训练的智能体随后通过迭代式的多轮对话，执行前瞻性评估，为其权重决策提供基于因果的解释，并自适应地调整优化策略。我们在包含23个数据集、97种设置的GIFT-Eval基准上进行了验证。结果表明，我们的方法在CRPS和MASE两项指标上均显著优于当前领先的时间序列基础模型，树立了新的最先进水平。",
                    "inspiration_trace": "好的，以下是我基于论文内容，对作者核心方法提出过程的逻辑链推演：\n\n---\n\n### **第一阶段：宏观观察与问题定义**\n\n1.  **起点：模型泛滥的困境。**\n    作者首先观察到时间序列预测领域正涌现出大量的“基础模型”（如Transformer, N-BEATS等）。然而，一个核心问题浮现：**没有任何一个单一模型能在所有场景下保持最优**。这导致研究的焦点从“寻找最好的模型”转变为“如何有效地组合这些模型”。\n\n2.  **核心挑战：从“最优”到“最优组合”。**\n    这个转变引出了两个关键需求：\n    *   **有效性**：如何组合模型才能超越任何单个模型的性能？\n    *   **可解释性**：为什么在某个特定时间序列上，这个组合是有效的？ practitioners需要理解并信任这个决策过程。\n\n### **第二阶段：探索与试错**\n\n3.  **初步假设：直接利用LLM的预测能力。**\n    鉴于大型语言模型（LLMs）在推理和决策上的强大能力，一个自然的想法是：**能否让LLM直接进行时间序列预测？**\n\n4.  **关键失败：能力错配。**\n    作者通过文献和实验发现，这条路走不通。LLMs的核心优势在于**语言理解和概念推理**（例如理解“节假日需求会激增”），但其训练目标并非**精确的数值回归**。直接强迫LLM输出数字，效果不佳且计算昂贵，无法匹敌传统预测模型。\n\n### **第三阶段：范式转换与核心洞见**\n\n5.  **重新定位：从“预测者”到“指挥家”。**\n    既然LLM不擅长亲自“演奏”（数值预测），那它能否成为一个“指挥家”？作者的核心洞见在此诞生：**将LLM从一个预测者，重新定位为一个评估、解释和协调多个专业预测模型的“智能法官”。**\n    *   **分工明确**：基础模型负责精确的数值计算，LLM负责高层次的推理和决策。\n    *   **创造价值**：LLM的价值不在于替代预测模型，而在于**驾驭一个由多个模型组成的“专家团队”**，通过权衡它们的优劣来做出更优的集体决策。\n\n### **第四阶段：方法论构建与深化**\n\n6.  **新问题：法官的“知识盲区”。**\n    新范式带来了新挑战：一个通用的LLM如何理解时间序列的特定概念？例如，它不知道“给模型A分配0.7的权重，模型B分配0.3”在时间序列语境下意味着什么（例如，可能代表“当前数据趋势性强，季节性弱”）。\n\n7.  **解决方案一：建立对话式优化框架。**\n    为了让LLM的决策过程透明且可控，作者设计了一个**迭代、多轮的对话系统**。\n    *   **基础**：先用传统的优化器（如SLSQP）生成一个初始的权重分配方案。\n    *   **对话**：LLM作为“法官”分析这个方案，提出质疑（“这个权重分配是否合理？是否考虑了未来的模式变化？”），并决定是接受该方案还是继续优化。\n    *   **目标**：将静态的、一次性的优化，转变为一个动态的、可解释的推理过程。\n\n8.  **解决方案二：为法官注入“领域知识”。**\n    为了解决LLM的知识盲区，作者需要“教会”它理解权重背后的含义。\n    *   **建立“因果”标尺**：利用SHAP（一种可解释性工具）来量化每个模型对时间序列不同成分（如趋势、季节性）的真实贡献。这为LLM的解释提供了一个客观的“事实基准”。\n    *   **专业化训练**：借鉴R1的微调思想，通过强化学习（GRPO）来训练LLM。奖励函数不仅包括决策的正确性，更关键的是**其解释与SHAP分析结果的一致性**。\n    *   **效果**：经过这种训练，LLM学会了将抽象的权重数字，翻译成有意义的因果陈述（例如，“高权重分配给模型A，是因为数据表现出强烈的自回归模式”）。\n\n### **第五阶段：最终方法形成**\n\n9.  **整合为TSOrchestra系统。**\n    将以上所有环节整合，形成了最终的方法论：\n    *   **输入**：一个时间序列和一组候选的基础预测模型。\n    *   **核心**：一个经过专业化微调的LLM“法官”。\n    *   **流程**：LLM引导传统优化器进行迭代权重调整，在每一步进行推理、评估和决策，并最终生成一个既高效又可解释的集成预测结果。\n    *   **输出**：精确的预测数值 + 基于因果关系的、人类可理解的决策解释。\n\n---\n\n**总结：** 作者的思考路径是一个典型的**“观察-假设-失败-重构-深化”**过程。他们从领域内的真实困境（模型泛滥）出发，尝试了最直接的技术方案（LLM直接预测），在失败中找到了正确的定位（LLM作为指挥家），并围绕这个新定位，系统性地解决了随之而来的新问题（知识盲区），最终构建了一个将LLM的推理能力与传统模型的数值能力完美结合的创新框架。"
                },
                {
                    "title": "WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning",
                    "arxiv_id": "2512.16108",
                    "authors": "Wendong Bi, Yirong Mao, Xianglong Liu, Kai Tian, Jian Zhang, Hanjie Wang, Wenhui Que",
                    "summary": "Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，应予以保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献并非简单地将LLM应用于音乐推荐领域，而是提出了一个名为 **WeMusic-Agent** 的**训练框架**。这个框架的核心是 **“知识内化”** 和 **“智能体边界学习”**，其目的是教会模型**智能地决定何时使用内部知识、何时调用外部工具**。这本质上是在构建和改进一个LLM智能体的决策机制，属于“构建、改进LLM智能体”的范畴，因此应**保留**。它不是“非演化型应用”，因为其创新点在于智能体的行为模式，而非应用本身。 2.  **正面指标 (第二步):** 论文明确包含了我的核心关注点。 *   **核心范式:** 论文标题和摘要中多次提及 `LLM-based Agent` 和 `Agentic`。 *   **智能体能力:** 论文的核心贡献 **“Agentic Boundary Learning”** 直接对应了 **`Tool Use / Tool Augmentation`**。智能体学习在“内部知识”和“外部工具调用”之间做决策，这是工具使用能力的关键一环，也是一种高级的规划和决策能力。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全、对齐或多模态等排除领域。它专注于提升智能体在特定任务上的效率和决策能力，因此没有触发排除标准。 4.  **特殊和模糊情况 (第四步):** *   **推理/规划:** 论文中的“智能体边界学习”可以被视为一种轻量级的规划或决策过程。智能体不是在进行复杂的数学逻辑推理，而是在规划其行动路径（内部处理 vs. 外部调用），这符合“保留”关于智能体规划和多步推理的论文的要求。 **总结:** 尽管论文的应用场景是“音乐推荐”，但其**核心贡献是提出了一种新的、用于训练LLM智能体进行高效工具使用的框架**。它解决了智能体在“何时使用工具”这一关键问题上的决策能力，这完全属于我研究焦点中的 **“单智能体”** 方向，特别是 **“工具使用”** 子方向。因此，这篇论文是高度相关的前沿研究，应该被筛选出来。",
                    "summary2": "本文旨在解决现有对话式音乐推荐方法难以平衡领域知识内化与外部工具灵活调用的问题。针对微信听书中的真实对话场景，我们提出了一种融合Knowledge Internalization与Agentic Boundary Learning的训练框架，使模型能智能决策何时使用内化知识、何时调用外部工具。并在自建的WeMusic-Bench基准上，通过相关性、个性化和多样性等指标验证了其有效性。",
                    "summary_translation": "对话场景下的个性化音乐推荐通常要求对用户偏好和细微的音乐语境有深刻理解，但现有方法往往难以平衡专业领域知识与灵活的工具集成。本文提出了 WeMusic-Agent，一个用于高效 LLM-based（基于大语言模型）的对话式音乐推荐的训练框架。通过整合 knowledge internalization（知识内化）和 agentic boundary learning（智能体边界学习），该框架旨在教会模型智能地决定何时利用内化知识，以及何时调用 specialized tools（专业工具）（例如，music retrieval APIs（音乐检索API）、music recommendation systems（音乐推荐系统））。在该框架下，我们提出了 WeMusic-Agent-M1，这是一个 agentic model（智能体模型），通过在500亿（50B）音乐相关 corpus（语料库）上进行 continued pretraining（持续预训练）来内化广泛的音乐知识，同时获得了在必要时 invoke external tools（调用外部工具）的能力。此外，考虑到对话式音乐推荐领域缺乏 open-source benchmarks（开源基准），我们还利用 WeChat Listen（微信听歌）中的 real-world data（真实世界数据）构建了一个用于个性化音乐推荐的 benchmark（基准）。该基准支持从多个维度进行全面评估，包括推荐的 relevance（相关性）、personalization（个性化）和 diversity（多样性）。在真实世界数据上的实验表明，WeMusic-Agent 相较于现有模型取得了显著的改进。",
                    "inspiration_trace": "好的，作为一名学术思维分析专家，我将为你系统性地推演《WeMusic-Agent》这篇论文背后作者的思考过程。这个逻辑链旨在还原作者从观察到最终提出核心方法的完整心路历程。\n\n---\n\n### **WeMusic-Agent 核心方法逻辑推演**\n\n#### **第一阶段：宏观问题与初始观察**\n\n**1. 出发点：对话式音乐推荐的核心挑战**\n作者首先观察到一个宏观问题：在对话场景下做个性化音乐推荐，要求模型既要深刻理解用户偏好，又要精通音乐领域的细微语境。这是一个典型的“领域知识”与“通用对话能力”相结合的复杂任务。\n\n**2. 现有方案的观察与矛盾**\n作者审视了当时主流的两种技术路径，并敏锐地发现了它们之间的内在矛盾：\n\n*   **路径一：知识内化**\n    *   **观察：** 通过在音乐语料上持续预训练，将音乐知识“灌入”模型参数。\n    *   **优点：** 推理快、对话流畅、能更好地建模个性化（因为知识在“体内”）。\n    *   **致命缺陷：** 知识是静态的，会过时；存在幻觉；最关键的是，它有一个明确的**能力边界**——对于训练数据之外的新歌、新事件无能为力。\n\n*   **路径二：智能体**\n    *   **观察：** 给大模型配备外部工具（如音乐搜索API），让模型按需调用。\n    *   **优点：** 知识是动态的、实时的，打破了知识边界，解决了幻觉问题。\n    *   **致命缺陷：** 推理慢（每次调用工具都有延迟）；难以深度个性化（工具是通用的，而非为特定用户定制）；对话体验可能不连贯。\n\n**3. 核心矛盾的提炼**\n作者将上述观察提炼为一个核心矛盾：**效率与效果的权衡**，以及**个性化与时效性的权衡**。现有方案非此即彼，无法兼得。这构成了论文要解决的根本问题。\n\n---\n\n#### **第二阶段：核心假设与思想萌芽**\n\n**1. 提出颠覆性假设**\n面对“二选一”的困境，作者没有试图优化其中一条路径，而是提出了一个更高层次的假设：\n> **“我们能否让模型学会自主决策，在‘使用内化知识’和‘调用外部工具’之间找到最佳平衡点？”**\n\n这个假设将问题从“选择哪种技术”转变为“**如何教会模型做选择**”。这是整个研究的灵魂。\n\n**2. 思想雏形：学习“能力边界”**\n基于上述假设，作者的思想进一步聚焦：模型需要学习的，本质上就是其自身内化知识的“能力边界”。\n*   **在边界内：** 优先使用高效、个性化的内部知识。\n*   **在边界外：** 果断调用外部工具，获取实时信息。\n\n这个“边界学习”的思想，成为了连接“知识内化”和“智能体”两大范式的桥梁。\n\n---\n\n#### **第三阶段：方法论构建——分步实现**\n\n为了验证这个核心假设，作者设计了一个两阶段的训练框架，逻辑上层层递进。\n\n**1. 第一步：夯实基础——构建一个强大的“知识内化”模型**\n作者认识到，要让模型学会“何时求助”，它首先得有足够强大的“内功”。如果内部知识一塌糊涂，学习边界就毫无意义。因此，他们先全力打造一个顶级的内化模型。\n\n*   **思想演进：**\n    *   **海量注入：** 通过在50B音乐语料上进行持续预训练，为模型打下坚实的知识基础。这解决了“懂不懂音乐”的问题。\n    *   **学会应用：** 通过多轮对话的监督微调，教会模型如何在对话场景中运用这些知识。这解决了“会不会聊天”的问题。\n    *   **优化质量：** 通过多目标强化学习（相关性、个性化、多样性、事实性），精细打磨推荐质量。这解决了“推荐得好不好”的问题。\n\n*   **关键洞察：** 在这一步，作者并非简单堆砌数据，而是创新性地设计了“双参考模型”来防止灾难性遗忘，以及“双向数据增强”来克服“逆转诅咒”，体现了对训练过程的深度思考。\n\n**2. 第二步：突破边界——教会模型何时“求助”**\n有了强大的基础模型后，作者开始着手实现核心假设——教会它学习能力边界。\n\n*   **思想演进：**\n    *   **冷启动一个“工具专家”：** 如何教模型使用工具？最直接的方式是先训练一个“只会用工具”的模型。这个模型虽然效率低，但提供了“如何正确调用工具”的示范轨迹。\n    *   **发现边界：** 如何定义“边界”？作者没有手动标注，而是巧妙地让两个模型“对决”。用基础模型和“工具专家”同时回答真实用户查询。基础模型答得好的，标记为“边界内（正样本）”；答得不好的，就用“工具专家”的答案替换，标记为“边界外（负样本）”。这样，就自动构建了一个用于学习边界的混合数据集。\n    *   **学习决策：** 在这个混合数据集上，通过课程学习（从单轮到多轮）和可控强化学习（引入折扣因子惩罚不必要的工具调用），训练最终的WeMusic-Agent模型。模型的目标不再是“推荐什么歌”，而是“**我应该直接回答，还是先调用工具？**”\n\n*   **关键洞察：** “智能体轨迹采样”是发现边界的关键创新，它将无形的“能力边界”问题转化为了有形的、可学习的决策数据。可控强化学习中的折扣因子，则是实现“效率优先”的精妙设计。\n\n---\n\n#### **第四阶段：综合与验证**\n\n**1. 最终产出：一个统一的智能体**\n经过上述训练，最终的WeMusic-Agent-M1诞生了。它不是一个简单的混合系统，而是一个**统一的、能够自主决策的智能体**。它在面对用户查询时，会智能地判断：\n*   如果问题在其知识范围内，直接、快速、个性化地回答。\n*   如果问题超出范围，则先调用工具获取信息，再结合上下文生成回答。\n\n**2. 验证闭环：构建专属基准**\n为了证明其优越性，作者意识到现有评测集无法充分评估其“边界决策”能力。因此，他们从真实业务数据中构建了WeMusic-Bench，并设计了多维度的评估指标（相关性、个性化、多样性），从而能够全面、公正地验证其方法的有效性。\n\n实验结果最终证实：WeMusic-Agent在各项指标上均优于纯内化模型和纯智能体模型，成功实现了效率与效果的双重最优。\n\n---\n\n### **总结：作者的思考脉络**\n\n作者的思考过程是一个典型的**“观察-矛盾-假设-验证”**的学术创新闭环：\n\n1.  **始于观察：** 发现现有技术路径的内在矛盾（效率vs效果）。\n2.  **升维思考：** 提出“让模型自主决策”的核心假设，将问题转化为“学习边界”。\n3.  **分步实现：** 先构建强大的内化模型作为“基础”，再通过巧妙的“对决”和“学习”机制教会它“决策”。\n4.  **闭环验证：** 构建专属评测基准，证明其统一框架的优越性。\n\n整个逻辑链条清晰、层层递进，从宏观问题出发，逐步聚焦到核心假设，并通过严谨的方法论设计最终实现了目标，展现了深厚的学术洞察力和工程实现能力。"
                },
                {
                    "title": "Subjective functions",
                    "arxiv_id": "2512.15948",
                    "authors": "Samuel J. Gershman",
                    "summary": "Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献并非将LLM或智能体作为工具去解决某个特定领域的问题，而是提出了一个名为“主观函数”的全新概念。这个概念旨在解决智能体如何自主生成目标这一根本性问题，即“目标函数从何而来”。这直接触及了智能体自主性的核心，属于构建和改进LLM智能体基础理论的范畴，而非简单的应用。 2.  **第二步：正面指标** - 论文的核心思想与你的多个核心关注点高度契合： - **自我演化**: 论文提出的“主观函数”是一种内生的、由智能体自身特征定义的目标函数。一个能够自主设定目标的智能体，是实现“自我完善和迭代”的先决条件。这直接关联到`Self-Evolving`、`Self-Improvement`和`Self-Reflection`等核心范式。 - **单智能体**: 论文探讨的是单个智能体如何进行高层次的认知活动——目标合成。这属于智能体内部机制的研究，与`Planning`（规划的前提是确定目标）和`Self-Reflection`（反思自身状态以确定新目标）等能力密切相关。 - **核心范式**: 论文的研究本质是`Agentic AI`，它试图赋予人工系统类似人类的高级认知能力，即自主定义追求的目标。 3.  **第三步：排除标准** - 论文的主要贡献不涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐问题。 - 论文是理论性研究，不涉及`Vision`、`MLLMs`等多模态内容。 - 因此，论文未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文不是关于提升LLM在数学或逻辑上的基础推理能力，而是关于智能体在更高层次上“推理”自己应该追求什么目标。这完全符合“保留”的条件，因为它探讨的是智能体框架内的核心认知功能。 - **自我演化的应用**: 这篇论文本身就是对“自我演化”机制的一种理论探索，它提出的“主观函数”可以被看作是自我演化机制的基石。因此，它完全符合保留标准。 **最终决策**: 这篇论文的核心贡献是提出了一种让智能体能够内生地、自主地定义自身目标（主观函数）的理论框架。这直接命中了你研究课题中“自我演化”和“单智能体”的核心方向，因为它探讨了智能体实现真正自主和自我迭代的最根本前提：如何自己决定“做什么”。因此，这篇论文不仅符合，而且是高度相关的前沿研究，应该被**保留**。",
                    "summary2": "本文旨在解决智能体如何自主生成目标函数以实现开放式学习的问题。针对智能体在马尔可夫决策过程（MDP）中可自主选择目标的场景，我们提出了一种基于预期预测误差的主观函数。该方法的核心是利用EPE同时指导策略选择与目标生成，驱动智能体追求“积极惊喜”。通过与心理学和神经科学现象（如享乐适应、信息规避等）的关联性分析，验证了该模型对人类行为的解释力。",
                    "summary_translation": "目标函数源自何处？我们如何选择要追求的目标？人类智能擅长于即时地综合新的目标函数。这一过程是如何运作的？我们能否赋予人工系统同样的能力？本文提出了一种回答这些问题的方法，其核心是引入 `subjective function (主观函数)` 的概念。这是一种 `higher-order objective function (高阶目标函数)`，它对 `agent (智能体)` 而言是内生的（即，其定义依赖于 `agent` 自身的特征，而非外部任务）。本文将 `Expected prediction error (期望预测误差)` 作为 `subjective function` 的一个具体实例进行了研究。这一提议与心理学、神经科学和机器学习等领域的诸多思想有着紧密的联系。",
                    "inspiration_trace": "好的，作为一名学术思维分析专家，我将为你系统性地推演作者在《主观函数》一文中的核心思想演进逻辑链。这个过程旨在还原作者从观察到提出完整方法论的思考路径。\n\n---\n\n### **作者核心思想的逻辑演进推演**\n\n#### **第一步：识别一个根本性的“元问题”**\n\n作者的思考始于一个超越传统机器学习范畴的宏大问题：**“客观函数从何而来？”**\n\n*   **观察与反思：** 无论是心理学还是人工智能，我们都默认智能体（人或机器）有一个给定的、需要优化的目标函数（如奖励、误差）。但人类智能最显著的特征之一，是能够**自主地、创造性地生成新目标**（例如，决定学习一门乐器、策划一场旅行、进行一场假想游戏）。现有的强化学习（RL）框架无法解释这种“目标生成”能力，它只解决了“如何实现目标”，而非“如何选择目标”。\n*   **批判现有范式：** 作者指出，追寻一个单一的“终极”目标（如生物的“繁殖适应度”）是“误导的”。这不仅无法解释人类行为的多样性，也限制了人工智能向更通用、更开放的方向发展。\n\n#### **第二步：定义核心概念，实现思想突破**\n\n为了解决“目标生成”问题，作者需要一个全新的理论工具。这促使他提出了核心概念——**“主观函数”**。\n\n*   **关键区分：** 作者首先对目标函数进行了二分法：\n    1.  **外生目标函数：** 传统RL中的奖励函数。它由外部环境定义，独立于智能体本身，可以无差别地应用于任何智能体。\n    2.  **内生目标函数：** 这是作者提出的创新点。目标函数的定义**依赖于智能体自身的内部特征**（如其知识、信念、模型）。同一个外部状态，对不同智能体可能意味着完全不同的“目标价值”。\n*   **思想飞跃：** “主观函数”的本质，就是将**目标的选择权**从外部设计者手中交还给智能体自身。它不再是一个固定的标尺，而是一个由智能体内部状态动态生成的、用于衡量“什么值得追求”的函数。这直接回应了第一步的“元问题”。\n\n#### **第三步：为抽象概念寻找一个具体的数学载体**\n\n“主观函数”是一个哲学层面的构想，要使其在科学上可操作，必须找到一个具体的数学形式。作者选择了**“预期预测误差”**。\n\n*   **为什么是EPE？** 这个选择并非偶然，而是基于深刻的洞察：\n    1.  **与现有理论的连接：** TD误差是RL和神经科学（多巴胺活动）中的核心概念，使用它作为基础，可以无缝地连接到庞大的现有知识体系。\n    2.  **直观的心理学意义：** EPE可以被解释为**“对积极惊喜的渴望”**。这完美地捕捉了人类好奇心和探索行为的驱动力——我们既不追求唾手可得的无聊，也不逃避无法理解的混乱，而是追求那些“跳一跳能够到”的挑战。\n    3.  **优雅的数学性质：** 作者推导出 `U = V - V̂`（EPE效用 = 真实价值 - 估计价值）。这个公式揭示了EPE的“主观性”本质：效用不仅取决于客观价值 `V`，更取决于智能体自身的认知 `V̂`。当 `V̂` 完美等于 `V` 时，EPE为零，驱动力消失，这解释了“享乐适应”现象。\n\n#### **第四步：构建一个可运行的双层循环架构**\n\n有了“主观函数”的概念和“EPE”这个载体，下一步是设计一个能让智能体自主运行的系统。作者构建了一个**“内-外”双层循环架构**。\n\n*   **内循环（策略优化）：** **给定一个目标，如何行动？**\n    *   **原则1：** 智能体选择能最大化EPE的策略。这意味着，对于一个固定的目标（如“到达迷宫终点”），它会选择那条能带来最多“积极惊喜”的路径。一旦完全学会，路径变得可预测，EPE降为零，智能体就会失去兴趣，不再重复。\n*   **外循环（目标生成）：** **面对众多可能性，选择哪个目标？**\n    *   **原则2：** 智能体选择那个能从当前状态出发，预期产生最大EPE的目标。这意味着它会主动选择那些它**尚未完全掌握**但**有望学会**的目标。这创造了一个永无止境的学习循环：完成一个目标 -> 该目标失去吸引力 -> 选择一个新的、更有挑战性的目标 -> 开始新的学习。\n\n这个双层架构将“目标生成”和“策略优化”统一在同一个优化目标（EPE）之下，形成了一个自洽的、开放式的学习系统。\n\n#### **第五步：广泛验证与连接，增强理论的解释力**\n\n一个新理论的生命力在于其解释力和连接力。作者通过将EPE模型与多个领域的现象联系起来，为其提供了强有力的佐证。\n\n*   **心理学与神经科学：**\n    *   **享乐适应：** 目标达成后，EPE消失，快乐感消退。\n    *   **偏好递增奖励：** EPE近似于价值的“导数”，追求EPE最大化自然导致偏好递增序列。\n    *   **信息规避/需求：** 当智能体过于乐观时，它回避可能打破乐观的信息（避免负EPE）；当它过于悲观时，它主动寻求信息（追求正EPE）。\n    *   **儿童假想游戏：** 孩子们选择一个“好玩”的（内生）目标（如走螺旋形路线），然后高效地（条件理性）去实现它，而不是走直线（外生最优解）。\n*   **机器学习：**\n    *   **内在动机：** 将EPE与“好奇心”、“学习进度”等内在奖励机制联系起来，指出EPE是这些思想的一个更精确、更具理论基础的形式。\n    *   **优势估计：** 指出当GAE（广义优势估计）中的λ=1时，它就等价于EPE。这巧妙地将一个前沿的工程技巧与一个深刻的理论概念联系在一起，赋予了前者新的理论意义。\n\n#### **第六步：总结与展望，定位理论贡献**\n\n最后，作者回归最初的问题，明确地将EPE定位为一种**“目标生成的主观函数”**的具体实现。他坦诚这仍是一个理论框架，而非一个成熟的算法，从而为未来的研究指明了方向（如如何在实际复杂系统中实现、如何与其他机制结合等）。\n\n---\n\n### **总结：作者的思考路径**\n\n**宏大问题（目标从何来？） → 批判现有范式（外生目标的局限） → 提出核心概念（内生/主观函数） → 寻找数学载体（EPE） → 构建系统架构（双层循环） → 广泛验证与连接（心理学、ML） → 定位贡献与展望未来。**\n\n这条逻辑链清晰地展示了作者如何从一个深刻的哲学观察出发，逐步将其具体化、数学化、系统化，最终形成一个具有广泛解释力和启发性的新理论框架。其核心创新在于，将智能研究的焦点从“如何优化一个给定的世界”转向了“如何创造一个值得追求的世界”。"
                },
                {
                    "title": "Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions",
                    "arxiv_id": "2512.15743",
                    "authors": "David Noever",
                    "summary": "We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel \"bag of bricks\" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a \"bag of words\" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，应予以保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的核心贡献并非简单地将LLM应用于物理装配领域，而是提出了一个**新颖的框架**。在这个框架中，LLM被用作一个智能体的核心“大脑”，通过**工具引导**来执行一个复杂的多步骤任务。具体来说，论文描述了如何让LLM使用LDraw（一个中间表示工具）和Python库，来生成符合物理约束的、分步的装配指令。这本质上是在**构建一个具备工具使用和规划能力的LLM智能体**，而不是一个非演化型的应用。因此，根据第一步的核心判断标准，应该保留。 **第二步：正面指标——高度相关** 论文包含了多个我核心关注点的正面指标： - **智能体能力**: 论文明确提到了“**Tool Use / Tool Augmentation**”（“large language models can be guided with tools”）和“**Planning**”（“produce valid step-by-step construction sequences and assembly instructions”）。这正是单智能体研究方向的核心能力。 - **核心范式**: 整个框架可以被视为一个“**LLM-based Agent**”，它接收自然语言指令，利用工具进行推理和规划，最终输出结构化的行动序列。 **第三步：排除标准——未命中** 论文的主要贡献不在于安全、对齐或多模态。虽然其最终输出可能用于物理世界，但论文的核心是生成指令的框架和方法，而不是研究视觉感知或模型安全性。因此，它没有触犯任何排除标准。 **第四步：处理特殊和模糊情况——符合保留条件** - **推理/规划**: 这篇论文是关于智能体如何进行规划的典型范例。它不是在提升LLM本身的基础数学或逻辑推理能力，而是在构建一个让LLM能够进行**复杂任务规划**（生成物理装配序列）的Agentic框架。这与ReAct等范式在精神上是一致的，即“推理+行动”。因此，它符合保留条件。 **第五步：最终决策** 综合以上分析，尽管论文的应用领域是物理装配，但其**核心贡献在于提出了一种构建LLM智能体的新方法论**。该方法论聚焦于如何让智能体通过使用工具来解决复杂的规划问题，完全符合我研究课题中“单智能体”方向下的“规划”和“工具使用”子方向。因此，这篇论文是高度相关且有价值的前沿研究，应被筛选出来。",
                    "summary2": "本文旨在解决从自然语言生成可物理实现且可扩展的组装指令的问题。针对自然语言描述，以及卫星、飞机等复杂原型组装场景，我们提出了一种“bag of bricks”方法，通过LDraw作为结构化中间表示，并利用Python工具库引导LLM，强制执行几何与连接约束。在国际空间站（3122零件）、多功能工具（47零件）和直升机等复杂原型案例上，通过D/M/I三轴评分框架（设计准确性、制造有效性、指令连贯性）和定量比较验证了其有效性。",
                    "summary_translation": "我们提出了一个用于从自然语言描述生成物理上可实现的组装指令的框架。与无约束的文本到3D方法不同，我们的方法在一个离散零件词汇表内运行，遵循几何有效性、连接约束和可构建性顺序。我们以LDraw作为一种富含文本的中间表示，证明了大型语言模型可以通过工具进行引导，为包含超过3000个组装零件的基于积木的原型生成有效的分步构建序列和组装指令。我们介绍了一个用于程序化模型生成的Python库，并在复杂的卫星、飞机和建筑领域上评估了其可构建的输出。该方法旨在实现可证明的可扩展性、模块化和保真度，以弥合语义设计意图与可制造输出之间的差距。根据自然语言规范即可进行物理原型制作。本研究提出了一种新颖的基本通用语言，以此作为先前基于像素的扩散方法或计算机辅助设计模型所缺失的关键一环，因为这些方法或模型无法支持复杂的组装指令或组件交换。在四项原创设计中，这种新颖的“积木袋”方法因此其功能相当于一个物理API：它是一个受限的词汇表，将精确朝向的积木位置与一个“词袋”相连接，通过该词袋，任意功能需求均可被编译并转化为实体现实。拥有这样一种一致且可重复的AI表示，不仅开辟了新的设计选项，同时也为制造业和工程原型设计中的自然语言实现提供了指导。",
                    "inspiration_trace": "好的，以下是基于您提供的论文内容，对作者核心方法“Prompt-to-Parts”的思考过程的系统性推演，旨在还原其从观察到最终方法论的逻辑演进。\n\n---\n\n### **作者核心方法的逻辑演进链**\n\n#### **第一步：宏观观察——从“视觉合理”到“物理可用”的鸿沟**\n\n作者的思考始于对当前生成式AI领域一个普遍现象的观察：文本到图像（Text-to-Image）和文本到3D（Text-to-3D）技术虽然发展迅猛，能够生成视觉上令人印象深刻的模型，但这些模型往往是“空中楼阁”。\n\n*   **核心痛点识别**：现有方法优化的是**视觉保真度**，而非**物理可实现性**。生成的模型可能存在结构不稳定、部件无法连接、材料不存在等问题，无法从数字世界转化为物理实体。\n*   **问题定义**：真正的挑战在于如何将**自然语言的语义意图**，直接转化为**可制造、可组装的物理原型**。这不仅仅是生成一个最终的3D形状，更是生成一个包含**组件、顺序和约束**的完整制造方案。\n\n#### **第二步：寻找“沙盒”——为何选择乐高（LEGO）？**\n\n面对“物理组装”这一宏大且复杂的难题，作者没有直接攻坚，而是选择了一个理想的“微缩世界”或“沙盒”来进行研究——乐高。\n\n*   **关键洞察**：乐高系统是现实世界**离散化、模块化建造系统**的完美缩影。无论是乐高、模块化卫星还是平板家具，其背后都共享一个核心结构：**有限的零件词汇表、有效的连接语法、以及功能约束**。\n*   **战略选择**：选择乐高并非因为其玩具属性，而是因为它提供了一个**定义明确、有边界、可验证**的实验环境。其标准化的几何尺寸、精确的公差和全球公认的连接方式，使得“物理可实现性”可以被精确地定义和检验。这为后续研究排除了无数不确定性。\n\n#### **第三步：核心假设——从“自由生成”到“约束语言”**\n\n在确定了乐高这个“沙盒”后，作者进一步思考：为什么让大型语言模型（LLM）直接生成物理组装如此困难？他们提出了一个核心假设：问题不在于LLM缺乏推理能力，而在于**缺少一种合适的中间语言**来描述物理组装。\n\n*   **类比启发**：作者借鉴了其他领域的成功经验。\n    *   **国际象棋**：Forsyth-Edwards Notation (FEN) 用一行文本就能无歧义地描述整个棋盘状态，成为人机共通的语言。\n    *   **数据库**：自然语言可以通过结构化查询语言（SQL）转化为精确的数据库操作。\n*   **核心假设形成**：如果能为物理组装找到一种类似FEN或SQL的**“元素通用语”**，LLM就能更可靠地生成物理上有效的结构。这种语言必须是**离散的、无歧义的、且内含物理约束**的。\n\n#### **第四步：方法确立——LDraw作为“编译器”目标**\n\n基于上述假设，作者没有发明一种新语言，而是选择了一个现有的、成熟的格式——**LDraw**——作为LLM的“编译目标”。\n\n*   **为什么是LDraw？** LDraw格式天然具备三大关键属性，完美契合了作者的假设：\n    1.  **离散的零件词汇**：所有零件都来自一个有限的、文档完备的库。\n    2.  **明确的空间坐标**：每个零件的位置和朝向都是精确的，消除了模糊性。\n    3.  **顺序的构建结构**：文件格式天然支持分步构建，直接对应人类可执行的指令。\n*   **方法论转变**：作者的核心方法论由此确立：**不让LLM直接“画”出模型，而是让它“写”出LDraw代码**。LLM的角色从一个“艺术家”转变为一个“程序员”，而LDraw就是它需要掌握的编程语言。\n\n#### **第五步：实现路径——“工具增强”与“一袋砖块”**\n\n如何确保LLM能写出语法正确、物理有效的LDraw代码？作者提出了“工具增强”的解决方案，并提炼出“一袋砖块”的核心思想。\n\n*   **工具增强**：作者开发了一个**自定义Python库**。这个库就像一个“语法检查器”和“API接口”，它向LLM暴露了合法的零件列表、有效的连接规则和坐标变换函数。LLM在生成每一步时，都可以调用这些工具来确保其输出符合LDraw语法和物理约束。这解决了纯靠提示工程难以维持长序列一致性的问题。\n*   **“一袋砖块”思想**：这是对整个方法论的哲学升华。正如自然语言处理中的“词袋模型”将文本视为词汇的集合，作者将物理组装视为**“砖袋模型”**。LLM的任务就是从一个有限的“砖袋”（零件库）中，根据语义需求，挑选出合适的“词”（零件），并按照“语法”（连接规则）将它们组织成有意义的“句子”（结构）。这个“砖袋”因此成为了一个**“物理API”**，连接了抽象的功能需求和具体的物质现实。\n\n#### **第六步：验证与拓展——从“生成”到“设计”**\n\n最后，为了证明该方法的有效性和前瞻性，作者设计了超越简单生成的验证体系。\n\n*   **多维度评估**：提出了D/M/I三轴评分框架（绘图准确性、模型有效性、指令连贯性），系统性地评估产出，而不仅仅是看最终图像。\n*   **极限测试**：通过生成数千个零件的国际空间站（ISS）模型，证明了方法的**可扩展性**，远超当时几十个零件的演示水平。\n*   **从生成到设计**：引入TRIZ（发明问题解决理论）作为修改任务的框架，测试LLM是否能理解工程权衡（如“在保持强度的前提下减轻重量”）。这标志着该方法从一个简单的“文本到模型”生成器，演变为一个能够进行**工程设计和迭代优化**的AI助手。\n\n---\n\n**总结**：作者的思考路径是一个典型的**“问题抽象 -> 沙盒验证 -> 假设构建 -> 方法实现 -> 价值升华”**的学术创新过程。他敏锐地抓住了当前AI生成技术与物理现实之间的根本性脱节，巧妙地利用乐高作为模型系统，通过引入LDraw这一结构化中间表示，成功地将一个复杂的物理组装问题，转化为一个LLM更擅长的、受约束的“代码生成”问题，最终构建了一个连接自然语言与物理制造的强大桥梁。"
                },
                {
                    "title": "Hypernetworks That Evolve Themselves",
                    "arxiv_id": "2512.16406",
                    "authors": "Joachim Winther Pedersen, Erwan Plantec, Eleni Nisioti, Marcello Barylli, Milton Montero, Kathrin Korte, Sebastian Risi",
                    "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** - **核心贡献**: 这篇论文的核心贡献是提出了一种名为“自引用图超网络”的新框架，其本质是一种**让神经网络（智能体）自我演化的方法论**。它不依赖于外部优化器，而是将变异、评估和适应等演化机制内嵌于网络自身。 - **符合研究目标**: 这直接命中了你研究课题中的“**自我演化**”方向。论文的目标是创造能够“自主、开放式学习的智能体”，这与构建和演化LLM智能体的目标高度一致。它不是在应用一个已有的框架，而是在创造一种全新的演化范式。 2.  **第二步：正面指标——高度匹配** - 论文摘要中充满了你的核心关注点：`Self-Evolving`（自我演化）、`Evolutionary Algorithms`（演化算法，体现在变异、遗传、种群动态等概念中）、`Self-Improvement`（自我改进）、`Iterative Improvement`（迭代改进）。这些关键词表明论文与你的研究焦点紧密相关。 3.  **第三步：排除标准——未触发** - 论文的主要贡献是关于智能体的演化机制和能力，而非安全、对齐或多模态技术。因此，它没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况——适用例外规则** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一例外规则的完美范例。论文虽然将自我演化机制应用在了强化学习的基准测试上（如CartPole, Ant-v5），但其**核心是提出一种全新的“自我演化”机制**。根据你的规则，即使应用在特定领域，只要核心是提出新的自我演化机制，就应该保留。 - **关于LLM的模糊性**: 虽然论文标题和摘要没有明确提及“LLM”，但它讨论的是“神经网络”和“超网络”的自我演化。超网络是一种可以生成其他网络权重的网络，这一概念完全可以应用于LLM。例如，一个超网络可以演化一个LLM智能体的策略网络或记忆模块。因此，这篇论文提出的方法论对于“LLM智能体的演化”具有直接的基础性和前瞻性价值。 5.  **第五步：最终决策** - 综合来看，这篇论文的核心贡献在于提出了一种创新的、内生的智能体自我演化框架。它直接解决了你研究课题中“自我演化”这一核心方向的关键问题：**智能体如何不依赖外部优化器而自主演化**。尽管其实验载体不是LLM，但其方法论对于构建和演化更高级的LLM智能体具有重要的启发和应用潜力。因此，这篇论文是高度相关且应该保留的前沿研究。",
                    "summary2": "本文旨在让神经网络无需外部优化器即可自我进化。针对非平稳强化学习环境，我们提出了一种Self-Referential Graph HyperNetworks，其内部嵌入了随机参数生成和自适应变异率机制，以实现自我变异和评估。并在CartPole-Switch、LunarLander-Switch和Ant-v5等强化学习基准上通过任务适应度分数验证了其有效性。",
                    "summary_translation": "神经网络如何在不依赖外部优化器的情况下实现自我进化？我们提出了自指图超网络，这是一种将变异与遗传机制完全嵌入网络内部的系统。通过结合超网络、随机参数生成和基于图的表示，自指图超网络能够进行自我变异与评估，同时将变异率作为一种可选性状进行适应。在我们新设计的带有环境切换的强化学习基准上，自指图超网络展现了快速、可靠的适应能力以及涌现的群体动态。在运动基准Ant-v5中，它们进化出了协调的步态，并通过自主降低种群中的变异以聚焦于有前景的解，展现了出色的微调潜力。我们的研究发现支持了这样一种观点，即可进化性本身可以从神经自指中涌现。自指图超网络代表了向更贴近生物进化的人工系统迈出的一步，为构建自主、开放式学习智能体提供了新的工具。",
                    "inspiration_trace": "好的，以下是对论文《Hypernetworks That Evolve Themselves》核心方法逻辑链的系统性推演，旨在还原作者的思考过程。\n\n---\n\n### **第一步：宏观问题的识别——从“外部优化”到“内在进化”的范式鸿沟**\n\n1.  **观察起点：主流优化方法的根本局限。**\n    作者首先观察到，当前深度学习的成功高度依赖于梯度下降等外部优化器。这些方法虽然强大，但存在固有的理论缺陷：易陷入局部最优、难以处理非可微问题、对离散结构（如网络架构）无能为力。\n\n2.  **寻找替代方案：进化算法（EAs）的复兴与启示。**\n    进化算法作为一种无导数的、基于种群的优化方法，天然地规避了梯度下降的许多缺点。这引发了作者的思考：能否用进化来优化神经网络？\n\n3.  **发现核心矛盾：进化机制的“外在性”。**\n    作者敏锐地指出了一个关键问题：在现有的神经进化方法中，进化机制（变异、选择、交叉）始终是一个**外部算法**，它像一个“上帝”一样操纵着作为“个体”的神经网络。这种“造物主”与“被造物”的分离，与生物进化中变异机制内嵌于基因本身的事实相去甚远。这引出了论文的核心哲学问题：**神经网络能否不依赖外部优化器，而是依靠自身内嵌的机制来完成进化？**\n\n### **第二步：核心假设的提出——“自指”是通往内在进化的桥梁**\n\n1.  **形成核心假设：将进化机制“内化”为网络的一部分。**\n    基于上述矛盾，作者提出了一个大胆的假设：**如果我们将产生变异和继承的“ machinery”（机制）直接嵌入到神经网络的结构中，那么网络就能实现自我进化。** 这意味着网络不仅要解决任务，还要负责生成自身的“后代”（即参数更新）。\n\n2.  **寻找理论基石：“自指”系统。**\n    如何让一个网络“生成自身”？作者从计算理论和先前的工作中找到了灵感——**自指**。一个能够指代、描述或修改自身的系统，是实现自我进化的逻辑前提。这引导他们将目光投向了两个关键技术领域：\n    *   **超网络：** 一个网络生成另一个网络的权重，这天然地提供了一种“基因型-表型”的映射关系。\n    *   **自指神经网络：** 早期工作（如Neural Network Quine, SRWM）已证明网络可以预测或修改自身的权重，为“自我操作”提供了可行性。\n\n3.  **整合假设：用“自指超网络”实现内在进化。**\n    作者的思路在此汇聚：**构建一个自指的图超网络，使其既能生成用于解决任务的“策略网络”参数（表型），又能生成用于更新自身的“超网络”参数（基因型变异）。** 这样，进化就完全内化为网络自身的行为。\n\n### **第三步：方法论的构建——从抽象概念到具体架构**\n\n为了将上述假设落地，作者需要解决三个关键的设计挑战，每一步都体现了清晰的逻辑演进：\n\n1.  **挑战一：如何实现“自我复制”而不陷入逻辑死循环？**\n    *   **问题：** 一个网络要生成自身的全部参数，其输出层必须比自身更大，这在物理上是不可能的。\n    *   **解决方案：** 作者巧妙地引入了一个**固定的随机基**作为最终输出层。网络不直接生成参数，而是生成一个用于与这个固定基结合的“系数”或“更新量”。这打破了循环依赖，使得网络能够生成一个与自身规模相当的“更新版本”，完成了自我复制的核心功能。\n\n2.  **挑战二：如何引入可控的“变异”？**\n    *   **问题：** 标准超网络是确定性的，给定输入总是产生相同输出，无法产生进化所需的多样性。\n    *   **解决方案：** 作者没有让网络直接输出参数值，而是让其输出一个**概率分布的参数**（如高斯分布的标准差）。通过从这个分布中采样，就自然地引入了随机性，构成了**变异**的基础。这个模块被称为**随机超网络**。\n\n3.  **挑战三：如何让“变异率”本身可进化？**\n    *   **问题：** 固定的变异率无法适应动态的环境。生物进化中，变异率本身就是可遗传、可选择的性状（即“可进化性”）。\n    *   **解决方案：** 作者让网络在生成变异的同时，也**预测一个节点级的“变异率”**。这个变异率会缩放最终生成的参数更新量。由于变异率本身也是由网络参数计算得出的，它也随着进化过程被选择和优化。这实现了**自适应变异**，使得“探索”与“利用”的平衡可以自发涌现。\n\n4.  **最终架构的整合：**\n    至此，完整的“自指图超网络”架构水到渠成：\n    *   一个**随机超网络**分支：接收自身的计算图，生成带有自适应变异率的参数更新，用于创造“后代”。\n    *   一个**确定性超网络**分支：接收任务策略网络的计算图，生成其参数，用于在环境中评估适应度。\n    *   两者共享底层的图神经网络（GNN）和节点嵌入，确保了“基因”与“表型”的内在关联。\n\n### **第四步：验证与洞见——通过实验设计来印证核心思想**\n\n作者设计的实验并非随意选择，而是为了精准验证其逻辑链的每一个环节：\n\n1.  **验证“快速适应能力”：**\n    *   **实验设计：** `CartPole-Switch` 和 `LunarLander-Switch`。通过在进化中途突然反转控制信号，模拟环境的剧烈变化（非平稳性）。\n    *   **预期结果：** 如果进化机制真正内化，种群应该能比传统外部进化算法（如CMA-ES, OpenES）**更快地恢复性能**。结果证实了这一点，证明了内在机制的优越性。\n\n2.  **验证“涌现的自适应变异”：**\n    *   **实验设计：** 在所有实验中，监测**种群内个体的平均距离**作为多样性指标。\n    *   **预期结果：** 在环境切换后，多样性应**激增**（增加探索）；在找到高适应度解后，多样性应**锐减**（增加利用）。结果完美印证了这一预期，表明“变异率”作为一个可选择的性状，确实引导了种群的自适应行为。\n\n3.  **验证“复杂任务的优化能力”：**\n    *   **实验设计：** `Ant-v5`，一个更复杂的连续控制任务。\n    *   **预期结果：** 即使在平稳环境中，该方法也应能有效优化策略，并观察到上述探索-利用的动态转换。结果表明，该方法不仅能进化出有效的步态，还伴随着种群多样性的自发收敛，展示了其通用性。\n\n### **总结：一条从哲学思辨到工程实现的完整链条**\n\n作者的思考路径始于对现有优化范式的深刻反思，提出了一个极具挑战性的核心问题。通过引入“自指”这一哲学概念，并将其与“超网络”这一具体技术相结合，构建了一个逻辑自洽的假设。随后，通过一系列精巧的工程设计（固定随机基、概率化输出、自适应变异率），将抽象假设转化为可执行的架构。最后，通过针对性极强的实验设计，逐一验证了该方法的核心优势，最终证明了“神经网络可以进化自身”这一开创性思想。整个过程体现了从宏观洞察到微观实现的严谨逻辑演进。"
                },
                {
                    "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
                    "arxiv_id": "2512.16063",
                    "authors": "Qidi Xu, Nuzha Amjad, Grace Giles, Alexa Cumming, De'angelo Hermesky, Alexander Wen, Min Ji Kwak, Yejin Kim",
                    "summary": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是**构建一个新的多智能体LLM框架**。摘要明确指出，“we developed a multi agent large language model framework... named Collaborative Theme Identification Agent (CoTI)”，并详细描述了其由三个具有不同角色的智能体构成。这完全符合“核心贡献在于构建、改进LLM智能体”的要求。 - 它不属于“非演化型应用”的排除范畴。虽然论文将框架应用于“定性分析”这一特定领域，但其**核心贡献是框架本身的设计和实现**，而不是简单地将一个已有的智能体框架当作工具去解决领域问题。论文的重点在于提出CoTI这个方法论，而非定性分析的医学发现。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Multi-Agent Systems (MAS)` 和 `LLM-based Agents` 在标题和摘要中都被明确提及。 - **多智能体**: 论文的核心是智能体间的 `Collaboration`（协作），通过三个智能体分工完成复杂的定性分析任务，这隐含了智能体间的 `Communication`（通信）机制。 3.  **第三步：排除标准** - 论文的主要贡献不是关于安全、对齐或多模态。它专注于智能体的协作框架，因此不触及任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文中的多智能体协作流程可以被看作是一种结构化的规划和执行过程，符合“智能体如何进行规划或在复杂任务中进行多步推理”的保留标准。 - **自我演化的应用**: 虽然这篇论文不涉及自我演化，但它的情况与“自我演化的应用”的例外规则精神一致：**核心贡献是提出一种新的机制（多智能体协作框架），即使它被应用在特定领域，也应该保留。** **最终决策**: 综合以上分析，这篇论文的核心贡献是提出一个名为CoTI的多智能体LLM框架，用于自动化定性分析。这直接命中了研究课题中的“多智能体”方向。尽管其应用场景是医疗领域的定性分析，但论文的创新点在于智能体系统的构建方法本身，而非应用结果。因此，这篇论文与“LLM智能体及其演化”的研究范围高度相关，应予以保留。",
                    "summary2": "本文旨在自动化劳动密集且主观的定性主题分析。针对心力衰竭患者的临床访谈记录，我们提出了一种名为CoTI的多智能体LLM框架，通过Instructor、Thematizer和CodebookGenerator三个专用智能体协作，自动化提取关键短语、识别主题并生成代码本。在12份心力衰竭访谈数据集上，通过Jaccard相似度和余弦相似度等指标验证，其输出比初级研究员和基线NLP模型更接近资深专家水平。",
                    "summary_translation": "理解患者体验对于推动以患者为中心的护理至关重要，尤其是在需要持续沟通的慢性病领域。然而，作为探索这些体验的主要方法，定性主题分析 仍然存在劳动密集、主观性强且难以规模化应用等问题。在本研究中，我们开发了一个名为协作主题识别智能体 的多智能体大语言模型框架，该框架通过三个智能体——即 Instructor (指导者)、Thematizer (主题化者) 和 CodebookGenerator (编码本生成器)——来实现定性主题分析的自动化。我们将 CoTI 应用于12份心力衰竭患者的访谈记录，以分析他们对用药强度的感知。与初级研究员及基线自然语言处理 模型相比，CoTI 所识别的关键短语、主题和编码本 与资深研究员的更为相似。此外，我们还将 CoTI 集成到一个面向用户的应用程序中，以在定性分析过程中实现人机交互。然而，CoTI 与初级研究员的合作仅带来了微乎其微的改进，这表明初级研究员可能过度依赖 CoTI，从而限制了其独立的批判性思维。",
                    "inspiration_trace": "好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出其核心方法（CoTI框架）的逻辑链，还原其从问题观察到方法创新的思考过程。\n\n---\n\n### **作者产出《CoTI》论文的思考过程推演**\n\n#### **第一步：识别核心矛盾——定性研究的价值与困境**\n\n*   **宏观观察：** 作者首先立足于一个公认的医疗健康领域宏观问题：以患者为中心的护理至关重要，而理解患者的真实体验（尤其是慢性病患者）是实现这一目标的关键。\n*   **锁定方法：** 他们识别出，定性主题分析是探索这些体验的“黄金标准”方法，能提供丰富的社会情境洞察。\n*   **发现痛点：** 随即，他们敏锐地指出了这一“黄金标准”的核心矛盾：**价值与效率的冲突**。该方法高度依赖专家，导致其**劳动密集、主观性强、难以规模化**。这限制了其在更大范围、更频繁的临床决策中的应用。这构成了研究的根本动机。\n\n#### **第二步：审视现有解决方案——从传统NLP到LLM的演进与不足**\n\n*   **审视旧工具（传统NLP）：** 作者没有直接跳到最新的技术，而是首先评估了现有自动化尝试。\n    *   **无监督模型（如LDA）：** 他们发现这类模型只能生成关键词簇，仍需人工解读，**自动化不彻底**。\n    *   **监督模型（如BERTopic）：** 他们指出这类模型需要预定义标签，这与定性分析**“归纳式发现”**的本质（从少量样本中发现未知主题）在概念上是**不兼容的**。\n*   **评估新工具（早期LLM应用）：** 接着，他们将目光转向大语言模型（LLM），承认其在处理长文本和生成人类可读输出方面的巨大潜力。\n*   **发现新瓶颈：** 然而，通过文献回顾和初步观察，他们发现直接使用通用LLM进行主题分析存在两个关键缺陷：\n    1.  **深度不足：** 通用指令导致输出**泛化、肤浅**，忽略了情感、心理等定性研究至关重要的细微之处。\n    2.  **流程断裂：** 即使能分析单个访谈，但**跨访谈的主题归纳与整合**仍是手动过程，未能解决规模化问题。\n\n#### **第三步：形成核心假设——从“使用工具”到“模拟专家”**\n\n*   **思维跃迁：** 作者的思考从“如何用一个工具完成任务”跃迁到“**如何模拟专家完成任务的整个流程**”。他们意识到，问题不在于LLM本身的能力，而在于缺乏一个能引导其进行深度、结构化思考的框架。\n*   **核心假设：** **如果设计一个多智能体系统，让每个智能体扮演定性分析流程中不同角色的专家（如制定策略的分析师、执行分析的编码员、整合结果的项目经理），那么就能将LLM的强大能力与人类专家的结构化工作流程相结合，从而实现既自动化又高质量的定性分析。**\n\n#### **第四步：构建方法论——将专家工作流程解构并映射为智能体**\n\n*   **解构专家流程：** 作者将一位资深研究员进行主题分析的过程拆解为三个关键步骤：\n    1.  **确立分析视角：** 在开始前，心中有一个明确的分析目标（如“关注心理社会因素”）。\n    2.  **单篇访谈分析：** 从每份访谈稿中提取关键证据，并提炼出初步主题。\n    3.  **跨访谈整合：** 将所有访谈中的相似主题进行归纳、合并，形成最终的编码本。\n*   **映射为智能体：** 基于上述解构，他们设计了三个专门的智能体：\n    *   **Instructor（指导者）：** 对应“确立分析视角”。其核心任务是**生成高质量、定制化的指令**，以解决LLM输出泛化的问题。它使用强大的推理模型（QwQ-32B）来迭代优化指令，确保分析能捕捉到特定情境下的深度。\n    *   **Thematizer（主题分析器）：** 对应“单篇访谈分析”。它接收Instructor的指令，负责从单个访谈中提取线索并识别主题，模拟了核心的分析工作。\n    *   **CodebookGenerator（编码本生成器）：** 对应“跨访谈整合”。它将Thematizer从所有访谈中产生的主题进行汇总和聚类，自动生成结构化的编码本，解决了规模化流程断裂的问题。\n*   **形成CoTI框架：** 这三个智能体协同工作，构成了**CoTI（Collaborative Theme Identification Agent）**框架。这个设计不仅实现了自动化，更重要的是，它通过**“指令工程”**和**“流程分解”**，将定性分析的“艺术性”和“专业性”注入到了自动化流程中。\n\n#### **第五步：设计验证实验——超越技术性能，探索人机协作新知**\n\n*   **验证核心假设：** 作者设计实验来验证CoTI的产出（线索、主题、编码本）是否能与“黄金标准”——资深研究员的产出高度相似。同时，将其与初级研究员和传统NLP基线对比，以证明其优越性。\n*   **探索前沿问题：** 作者没有止步于技术验证，他们进一步提出了一个更具前瞻性的问题：**在AI辅助研究中，人机协作的真实效果究竟如何？** 普遍认为“人机协作优于纯AI”，但这缺乏实证。\n*   **设计协作实验：** 他们开发了一个交互式应用，让初级研究员与CoTI的Thematizer协作。通过比较“初级研究员 alone”、“CoTI alone”和“CoTI + 初级研究员”三种模式下的产出与资深研究员的相似度，来实证检验人机协作的价值。\n*   **预设与发现：** 他们可能预设协作会提升效果，但实验结果却揭示了一个反直觉的发现：**协作带来的增益微乎其微，甚至可能因为初级研究者的“自动化偏见”而降低质量。** 这一发现本身就成了论文的重要贡献，为AI辅助研究的行为科学提供了宝贵洞见。\n\n---\n\n**总结：** 作者的思考路径是一个典型的“**问题驱动-技术审视-假设形成-方法构建-实证验证**”的学术创新闭环。他们从医疗领域的实际困境出发，批判性地评估了现有技术方案的局限性，通过将“模拟专家工作流程”这一核心思想与多智能体LLM技术相结合，创造性地提出了CoTI框架。最后，他们不仅验证了方法的有效性，还勇敢地探索并揭示了人机协作中一个被忽视的复杂现象，从而极大地提升了论文的学术价值和深度。"
                },
                {
                    "title": "Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins",
                    "arxiv_id": "2512.15930",
                    "authors": "Matthew Sinclair, Moeen Meigooni, Archit Vasan, Ozan Gokdemir, Xinran Lian, Heng Ma, Yadu Babuji, Alexander Brace, Khalid Hossain, Carlo Siebenschuh, Thomas Brettin, Kyle Chard, Christopher Henry, Venkatram Vishwanath, Rick L. Stevens, Ian T. Foster, Arvind Ramanathan",
                    "summary": "Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them \"undruggable\". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献在于构建和改进一个新颖的多智能体系统。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的核心本质是**构建一个新的LLM智能体框架**。摘要明确指出：“we designed and implemented **StructBioReasoner, a scalable multi-agent system**”（我们设计并实现了一个可扩展的多智能体系统StructBioReasoner）。 - 它提出了一个**新颖的“锦标赛式推理框架”**，让专业化的智能体相互竞争来生成和优化假设。这直接对应了您筛选标准中的“构建、改进或演化 LLM智能体的论文”。 - 尽管论文的应用领域是生物制药（一个特定领域），但论文的**核心贡献是方法论本身**，即这个多智能体系统如何工作、如何协调、如何推理，而不是它发现了什么具体的药物。这使其区别于“非演化型应用”，更符合第四步中“自我演化的应用”的例外情况精神——即核心贡献是新的智能体机制，即使它被应用在特定领域。 2.  **第二步：正面指标——高度匹配** - **核心范式**: 论文明确包含 `Multi-Agent Systems (MAS)` 和 `Agentic AI`（标题中的 \"Agentic Reasoning\"）。 - **智能体能力**: 论文详细描述了智能体的多种核心能力： - `Planning` & `Reasoning`: \"autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools\"。 - `Tool Use / Tool Augmentation`: \"Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis\"，这是典型的工具使用能力。 - **多智能体**: 论文的核心就是多智能体系统，包含了 `Collaboration` 和 `Competition`（\"specialized agents compete\"）。 3.  **第三步：排除标准——未触发** - 论文的主要贡献不是关于安全、对齐、可解释性或幻觉。 - 论文虽然提到了扩散模型，但只是作为背景技术，其核心研究内容不涉及多模态或视觉。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文提出的“锦标赛式推理框架”是典型的智能体规划和推理机制，而不是提升LLM本身的基础数学或逻辑能力，因此符合保留条件。 - **自我演化的应用**: 如第一步所述，这篇论文是“提出一种新的智能体机制并应用在特定领域”的完美范例，其价值在于机制本身，因此应该保留。 **最终决策**: 这篇论文的核心贡献是**StructBioReasoner**，一个用于解决复杂科学问题的**新颖的多智能体推理框架**。它深入探讨了智能体如何通过竞争、协作和工具使用来完成大规模、多步骤的复杂任务。这完全契合您研究课题中的“多智能体”方向，并触及了“单智能体”的规划和工具使用能力。因此，这篇论文是您研究范围内的前沿高质量论文，应予以保留。",
                    "summary2": "本文旨在解决靶向内在无序蛋白的生物制剂自主设计问题，以应对其“不可成药”的挑战。针对IDP靶点（如Der f 21和NMNAT-2）的复杂构象集合，我们提出了一种名为StructBioReasoner的可扩展多智能体系统。该系统采用基于锦标赛的推理框架，整合文献检索、结构预测与分子模拟等工具，并通过Academy中间件在HPC上协同执行。在Der f 21和NMNAT-2靶点上，通过结合自由能和识别的结合模式数量等指标验证了其有效性。",
                    "summary_translation": "Intrinsically disordered proteins (IDPs, 内在无序蛋白) 因其在疾病发生中扮演着重要角色而成为关键的治疗靶点——约80%的癌症相关蛋白含有长的无序区域——但其缺乏稳定的 secondary/tertiary structures (二级/三级结构)，使其成为“undruggable (不可成药)”靶点。尽管近期在计算方法上取得了进展，例如 diffusion models (扩散模型) 可以设计出高亲和力的 IDP binders (IDP结合物)，但要将这些成果转化为实际的药物发现过程，则需要自主系统，该系统需具备跨复杂 conformational ensembles (构象集合) 进行推理，并能够大规模地协调多种计算工具的能力。\n\n为应对这一挑战，我们设计并实现了 StructBioReasoner，这是一个可扩展的 multi-agent system (多智能体系统)，用于设计可靶向 IDP 的 biologics (生物制剂)。StructBioReasoner 采用了一种新颖的 tournament-based reasoning framework (锦标赛式推理框架)，在该框架中，专门的智能体通过竞争来生成和优化 therapeutic hypotheses (治疗性假设)，从而自然地分配 computational load (计算负载)，以高效地探索广阔的 design space (设计空间)。这些智能体将 domain knowledge (领域知识) 与对 literature synthesis (文献综合)、AI-structure prediction (AI结构预测)、molecular simulations (分子模拟) 和 stability analysis (稳定性分析) 的访问能力相结合，并通过一个可扩展的 federated agentic middleware (联邦式智能体中间件) Academy，在 HPC infrastructure (高性能计算基础设施) 上协调其执行。\n\n我们在 Der f 21 和 NMNAT-2 上对 StructBioReasoner 进行了 benchmark (基准测试)，结果表明，针对 Der f 21 设计并验证的 787 个 candidates (候选分子) 中，超过 50% 在 binding free energy (结合自由能) 方面优于文献中报道的 human-designed reference binders (人类设计的参考结合物)。对于更具挑战性的 NMNAT-2 蛋白，我们从 97,066 个 binders (结合物) 中识别出了三种 binding modes (结合模式)，其中包括已充分研究的 NMNAT2:p53 interface (NMNAT2:p53界面)。因此，StructBioReasoner 为在 Exascale platforms (E级计算平台) 上进行 IDP therapeutic discovery (治疗发现) 的 agentic reasoning systems (智能体推理系统) 奠定了基础。",
                    "inspiration_trace": "好的，以下是对该论文核心方法 `StructBioReasoner` 诞生过程的逻辑推演，旨在还原作者从观察到最终方法论的思考脉络。\n\n---\n\n### **第一步：宏观观察与问题定义**\n\n作者团队首先站在一个宏观的交叉学科视角，观察到了一个核心矛盾：\n\n1.  **生物学上的重大机遇与挑战：** 内在无序蛋白（IDPs）是极其重要的药物靶点（如80%的癌症相关蛋白都含有无序区），但其结构的高度动态性和异质性，使其成为传统药物发现的“不可成药”靶点。这是一个亟待解决的重大科学难题。\n\n2.  **计算工具的突破与局限：** 近年来，以扩散模型为代表的人工智能工具（如RFdiffusion）已经证明，它们能够设计出针对IDPs的高亲和力结合剂。这是一个里程碑式的突破，**但作者敏锐地意识到，这仅仅是“工具”层面的突破，而非“流程”层面的解决方案。**\n\n3.  **从“工具”到“实用”的鸿沟：** 作者观察到，将单个AI工具的成功转化为实际的药物发现流程，存在一个巨大的鸿沟。一个完整的药物发现流程是复杂的、多阶段的，需要：\n    *   **策略推理：** 针对特定的IDP，应该选择哪种计算策略？\n    *   **工具编排：** 如何协调文献挖掘、结构预测、分子模拟、能量计算等多样化的工具？\n    *   **规模化执行：** 如何在巨大的设计空间中进行高效探索，并利用高性能计算（HPC）资源？\n\n**核心问题由此浮现：** 当前的瓶颈不在于缺少某个强大的工具，而在于**缺少一个能够自主决策、智能编排、并规模化执行整个复杂工作流的“大脑”或“操作系统”。**\n\n---\n\n### **第二步：提出核心假设与设计哲学**\n\n基于上述观察，作者开始构思解决方案，并形成了一系列关键假设：\n\n1.  **假设一：需要“智能体”而非“自动化脚本”。**\n    *   **思考：** 简单的脚本无法应对IDP设计的复杂性。每个IDP都不同，需要动态调整策略。因此，系统必须具备类似人类科学家的**推理能力**。\n    *   **推论：** 采用**多智能体架构**是合理的。可以模拟一个科研团队：一个“首席科学家”（规划与推理智能体）负责制定战略，多个“领域专家”（文献、模拟、设计等专项智能体）负责执行具体任务。\n\n2.  **假设二：需要“竞争”而非“串行”来提升效率和探索广度。**\n    *   **思考：** IDP的设计空间极其广阔。如果让智能体串行地尝试一种方法，效率低下且容易陷入局部最优。如何才能并行、高效地探索多种可能性？\n    *   **推论：** 引入**“锦标赛式”推理框架**。让多个设计智能体（或采用不同策略的智能体）并行地生成和优化候选方案，然后相互竞争。优胜者进入下一轮，失败者被淘汰。这种机制天然地实现了计算负载的分布式处理，并鼓励了对设计空间的广泛探索。\n\n3.  **假设三：需要“知识增强”而非“凭空推理”。**\n    *   **思考：** 大语言模型（LLM）本身缺乏对IDP这一细分领域的深度知识，容易产生幻觉或偏见。如何让智能体的决策有据可依？\n    *   **推论：** 必须将**领域知识**注入到智能体中。这包括两个层面：一是通过**检索增强生成（RAG）**，让智能体能实时访问和消化最新的科学文献；二是建立一个共享的**记忆系统**，让智能体能从过去的成功和失败中学习，积累“设计直觉”。\n\n---\n\n### **第三步：整合与形成最终方法论**\n\n将上述假设整合，一个名为 `StructBioReasoner` 的系统架构便清晰地浮现出来。这个方法论的形成过程是：\n\n1.  **架构设计：** 以**多智能体**为基础骨架。核心是一个**规划与推理智能体**作为“大脑”，负责全局决策。围绕它，配置一系列**专项智能体**（文献探索、结构预测、分子模拟、结合剂设计、分析优化等），每个都像一个领域的专家。\n\n2.  **工作流引擎：** 采用**锦标赛式推理**作为核心运行机制。当接到一个设计任务时，推理智能体不是选择一条路径，而是启动多个专项智能体（例如，用不同的设计算法）并行工作。它们的结果被收集、比较、排名，表现最好的“假设”被保留并进入下一轮迭代优化。\n\n3.  **知识注入：** 集成**HiPerRAG**作为文献智能体，为整个系统提供实时、精准的领域知识。同时，建立一个**跨假设学习机制**，让系统能从所有候选方案中提炼共性规律（例如，“针对这类IDP，疏水热点比静电作用更重要”），并将这些规律转化为后续设计的软约束。\n\n4.  **规模化基石：** 为了解决计算资源问题，将整个智能体系统构建在一个名为**Academy**的联邦式智能体中间件之上。这个中间件负责将智能体的决策（如“运行一个模拟”）转化为在HPC集群（如Aurora超算）上可执行、可扩展、容错的实际计算任务，从而将高层的推理逻辑与底层的执行细节解耦。\n\n**最终，`StructBioReasoner` 不仅仅是一个工具集，而是一个具备自主推理、并行探索、知识驱动和规模化执行能力的完整科研“数字同事”。** 它的诞生逻辑，源于作者对“从工具到系统”这一鸿沟的深刻洞察，并通过引入智能体、竞争机制和知识增强等创新思想，成功地将IDP药物设计从手工作坊式的专家操作，推向了自动化、智能化的工业流程。"
                },
                {
                    "title": "CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory",
                    "arxiv_id": "2512.15813",
                    "authors": "Nishant Gaurav, Adit Akarsh, Tejas Ravishankar, Manoj Bajaj",
                    "summary": "Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献是提出了一种名为 `CodeMem` 的新架构，旨在构建和改进LLM智能体。它不是将现有智能体应用于某个特定领域，而是直接针对当前工具使用智能体存在的“概率不稳定性”这一根本性问题，提出了一种系统性的解决方案。这完全符合“构建、改进或演化LLM智能体”的核心目标。 2.  **第二步：正面指标 (高度匹配)** 论文命中了多个核心关注点： *   **核心范式**: 论文明确研究 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**: 论文的核心是解决智能体的 `Memory` 问题，具体提出了“程序性记忆”这一新概念。同时，它建立在 `Tool Use` 的研究之上（如引用CodeAct），并致力于构建可复用的“agentic workflows”，这与 `Planning` 能力密切相关。通过固化成功流程，它也实现了一种形式的 `Self-Correction`，解决了LLM的概率性不稳定性。 3.  **第三步：排除标准 (未触发)** 论文的研究焦点是智能体的架构和可靠性，不涉及安全、对齐、可解释性或视觉多模态等排除领域。 4.  **第四步：特殊和模糊情况 (符合保留规则)** *   **推理/规划**: 论文关注的是如何构建可复用、可靠的“agentic workflows”，这属于智能体层面的规划和执行框架，而非提升LLM底层的数学或逻辑推理能力，因此符合保留条件。 *   **自我演化**: 虽然论文标题未直接使用“演化”，但其提出的“程序性记忆”机制，本质上是一种让智能体通过记录和重用成功的执行路径，从而实现自我完善和迭代的方式。它让智能体从一次性的、不稳定的执行，演变为可复现、可靠的自动化流程，这与“自我演化”的精神内核高度一致。 **总结**: 该论文的核心贡献在于为LLM智能体引入了一种创新的“程序性记忆”架构，以解决其在执行重复性任务时的可靠性问题。这直接推动了单智能体在**记忆**和**工具使用**方向的发展，并蕴含了**自我完善**的演化思想。因此，它是一篇与你研究课题高度相关的前沿论文，应予以保留。",
                    "summary2": "本文旨在解决当前AI智能体的概率不稳定性问题，以构建可重现、可靠的自动化工作流。针对需要多步骤工具编排和跨会话一致性的复杂重复性任务，我们提出了一种名为CodeMem的架构，它结合了Dynamic MCP工具发现和基于代码的程序性记忆。该架构让LLM在沙箱中生成并验证Python代码，然后将成功的逻辑作为可重用技能固化，从而实现确定性执行。在包含25个多步骤任务的CodeMem Benchmarks上，通过LLM-as-a-Judge评估的正确率、平均调用次数和延迟等指标验证了其有效性。",
                    "summary_translation": "当前的工具使用 AI 智能体 存在动作空间 有限、上下文 效率低下以及概率 不稳定性 等问题，这些问题使其不适合处理重复性任务，而此类任务可由基于 n8n 和 Zapier 等平台构建的智能体工作流 可靠且高效地完成。早期的研究工作，如 CodeAct、DynaSaur 和 Code Mode，尝试通过将整个 Python 语言作为其动作空间来解决前两个问题：这使得智能体可调用的工具数量变得无限。Python 代码块能将复杂操作执行为单个步骤，并仅打印相关结果，从而有助于保持上下文的精简。然而，概率不稳定性问题依然存在。其原因在于，对于相同环境下的同一任务，由于大语言模型 (LLMs) 的概率特性，智能体可能会遵循不同的执行轨迹。因此，为确保一致性和可靠性，我们需要引入程序性记忆。本文提出了 CodeMem，这是一种通过代码实现程序性记忆的架构，可用于构建和运行具有确定性可靠性 的可复用智能体工作流。",
                    "inspiration_trace": "好的，以下是我对《CodeMem》论文作者思考过程的系统性推演，旨在还原其从宏观问题到核心方法论的逻辑演进。\n\n---\n\n### **CodeMem 作者思想演进逻辑链**\n\n#### **第一阶段：宏观观察与核心矛盾的识别**\n\n**起点：AI智能体的“能力”与“可靠性”的脱节**\n\n作者的思考始于一个根本性的观察：当前的AI智能体虽然展现出强大的通用能力，但在处理重复性、流程化的任务时，远不如传统的自动化工具（如n8n, Zapier）可靠。这形成了一个核心矛盾：\n\n*   **AI智能体**：灵活、强大，但行为是**概率性**的，不可预测，无法保证重复执行同一任务时结果一致。\n*   **自动化工具**：刻板、功能有限，但行为是**确定性**的，绝对可靠，专为重复任务设计。\n\n**核心问题**：如何构建一个既能拥有AI智能体的灵活性与强大能力，又能具备传统自动化工具的确定性与可复现性的新型智能体？\n\n---\n\n#### **第二阶段：现有方案的审视与瓶颈分析**\n\n作者没有立即提出新方案，而是首先系统性地审视了现有技术路线，并精准地定位了它们的瓶颈。\n\n**1. 解决“动作空间”与“上下文”瓶颈：从ReAct到CodeAct**\n\n*   **旧范式（ReAct）**：智能体通过JSON调用预定义工具。作者指出其两大硬伤：\n    *   **动作空间有限**：工具数量是固定的。\n    *   **上下文效率低下**：每一步工具调用和结果都消耗上下文，多步任务极易导致上下文溢出。\n*   **改进范式（CodeAct）**：以Python作为动作空间。作者肯定了其进步：\n    *   **无限动作空间**：整个Python生态都是工具。\n    *   **上下文精简**：复杂逻辑可在单个代码块内完成，只输出关键结果。\n\n**2. 发现遗留的“确定性”瓶颈：CodeAct并未解决根本问题**\n\n在肯定CodeAct进步的同时，作者敏锐地指出，它只是解决了“怎么做”更高效的问题，却没有解决“做什么”更可靠的问题。这引出了四个更深层次的结构性瓶颈：\n\n*   **概率不稳定性**：对于同一任务，LLM每次可能生成不同的代码路径，导致结果不可复现。\n*   **冗余计算**：即使是已解决过的任务，智能体仍需重新“思考、编码、调试”，浪费资源。\n*   **无法保留反馈**：用户的修正和偏好（如“按接收日期而非发送日期过滤”）是临时的，无法跨会话学习。\n*   **上下文漂移**：在长任务中，智能体容易陷入细节而忘记最终目标。\n\n**关键洞察**：现有方案，包括最先进的CodeAct，都将LLM置于一个“** perpetual improviser**”（永恒的即兴表演者）的角色。问题的根源在于，**逻辑本身是短暂、易失的，完全依赖于LLM每次的“即兴发挥”。**\n\n---\n\n#### **第三阶段：核心思想的形成——范式转移**\n\n基于以上分析，作者的思想发生了关键性的跃迁，提出了一个颠覆性的假设。\n\n**核心假设：将“过程记忆”从“指令”变为“代码”**\n\n*   **旧思路（如LangGraph）**：通过修改系统提示来更新“过程记忆”。作者认为这是“**brittle**”（脆弱的），因为它仍然依赖LLM去理解和遵循新的指令，本质上还是概率性的。\n*   **新思路（CodeMem）**：**将成功验证过的逻辑，直接固化为确定性的、可执行的代码。** 这段代码就是“过程记忆”的实体。\n\n**范式转移**：LLM的角色不再是一个重复执行任务的“**执行者**”，而是一个设计、验证并固化可重用技能的“**架构师**”。智能体的生命周期从“**即兴-执行**”转变为“**设计-验证-固化-重用**”。\n\n---\n\n#### **第四阶段：架构设计——如何实现新范式**\n\n有了核心思想，下一步就是设计一个具体的架构来支撑它。作者将架构分解为几个关键组件，每个组件都精准地对应第二阶段发现的一个瓶颈。\n\n1.  **解决上下文与工具发现**：引入 **Dynamic MCP** (`search_functions`, `load_functions`)。这不是全新发明，而是对现有优秀思想的整合，实现了工具的“按需加载”，保证了动作空间的无限扩展性。\n2.  **解决逻辑执行与计算分离**：使用 **Python Sandbox** (`execute_code`)。这继承了CodeAct的优点，将复杂的循环、数据处理等计算任务从LLM的“推理”中剥离，交由CPU确定性执行。\n3.  **解决目标漂移**：设计 **外部状态锚点** (`write_todos`)。这是一个简单但关键的机制，将任务计划从混乱的聊天历史中抽离出来，形成一个结构化的、持久化的“外部工作记忆”，确保智能体在长任务中不迷失方向。\n4.  **解决概率不稳定性与反馈保留（核心创新）**：创建 **过程记忆库** (`register_skill`)。这是整个架构的灵魂。它提供了一个机制，将在沙箱中成功运行的代码“冻结”为可重用的“技能”。从此，执行同类任务只需直接调用这个技能，完全跳过了“思考、编码、调试”的概率性过程，实现了真正的**确定性重用**。\n\n---\n\n#### **第五阶段：验证与升华**\n\n最后，作者通过案例和实验来验证其思想的正确性。\n\n*   **案例研究**：通过“Outlook-OneDrive桥接”这个复杂任务，戏剧化地展示了ReAct的“上下文崩溃”和CodeMem的“O(1)上下文复杂度”与“确定性控制流”，直观地证明了新架构的优越性。\n*   **实验分析**：实验结果并非简单地比较“谁更快”，而是揭示了更深层次的道理：更强大的模型（如Gemini 3）在CodeMem框架内更愿意进行多次迭代和工具调用，从而获得更高的成功率。这证明了CodeMem架构**鼓励和促进了可靠性**，而非盲目追求速度。\n\n**最终结论**：CodeMem的成功不在于某个单一的技术突破，而在于它通过**动态MCP、沙箱、外部状态锚点和过程记忆库**这四个组件的协同作用，系统性地解决了现有智能体在**可靠性、效率和可复现性**上的根本缺陷，成功地将AI智能体从一个“即兴表演者”升级为一个“可信赖的架构师”。"
                },
                {
                    "title": "FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction",
                    "arxiv_id": "2512.15728",
                    "authors": "Yuhan Hou, Tianji Rao, Jeremy Tan, Adler Viton, Xiyue Zhang, David Ye, Abhishek Kodi, Sanjana Dulam, Aditya Paul, Yikai Feng",
                    "summary": "The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \\emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\\% and stability of 93.33\\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合您的研究范围，核心依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非仅仅是“预测联邦基金利率”这一应用本身，而是提出了一个名为 **FedSight AI 的多智能体框架**。这个框架的设计、构建和改进是论文的精髓。它详细描述了如何使用LLM作为智能体，通过模拟FOMC成员的“分析、辩论、投票”过程来解决问题。这完全符合“构建、改进LLM智能体”的核心目标，因此它不是一个简单的“非演化型应用”，而是一个关于智能体系统架构的方法论研究。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了多个核心关注点的关键词： *   **核心范式**: 明确提出了 `Multi-Agent System Architecture`。 *   **多智能体**: 描述了智能体间的 `Collaboration`（协作）和 `Communication`（通信），具体表现为“debate options, and vote”（辩论选项和投票）。 *   **智能体能力/推理**: 提出了 `Chain-of-Draft (CoD)` 扩展，这是一种新的“多阶段推理”方法，旨在提升智能体的推理效率和准确性，这属于智能体规划和推理能力的改进。 3.  **第三步：排除标准——未触发** 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态等排除领域。虽然提到了“transparent reasoning”（透明的推理），但这是其多智能体框架设计带来的一个特性，而非研究本身的核心目标（研究目标是构建框架和提升预测性能）。 4.  **第四步：处理特殊和模糊情况——符合保留条件** 论文提出的 `Chain-of-Draft (CoD)` 方法，其目的是“enforcing concise multistage reasoning”（强制执行简洁的多阶段推理）。这完全符合“保留”条件，即它是关于**智能体如何进行规划和多步推理**的新框架，而不是单纯提升LLM底层Token预测能力的非Agentic方法。 **总结:** 该论文的核心是提出并验证了一个新颖的**多智能体系统架构**，并为其设计了一种新的**推理机制**。尽管其应用场景是金融预测，但其研究焦点和核心贡献在于**如何构建和改进这个LLM多智能体系统**，这与您的研究课题“LLM智能体及其演化”中的“多智能体”方向高度契合。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决传统利率预测模型不透明、无法模拟FOMC决策审议过程的问题。针对FOMC会议决策场景，融合了结构化经济指标与非结构化叙事（如Beige Book），我们提出了一种基于LLM的多智能体框架FedSight AI，并引入Chain-of-Draft (CoD)机制以优化推理效率。在2023-2024年FOMC会议数据上，与MiniFed和Ordinal Random Forest等基线模型对比，通过93.75%的准确率和93.33%的稳定性等指标验证了其有效性。",
                    "summary_translation": "联邦公开市场委员会 (FOMC) 负责设定联邦基金利率，从而塑造货币政策及更广泛的经济格局。本文提出了 \\emph{FedSight AI}，一个利用大型语言模型来模拟FOMC审议过程并预测政策结果的多智能体框架。在该框架中，成员智能体分析结构化指标以及《褐皮书》等非结构化输入，进行方案辩论并投票，从而复现委员会的决策推理过程。此外，我们通过一种名为思维链草稿的扩展方法，强制执行简洁的多阶段推理，从而进一步提升了框架的效率和准确性。在对2023-2024年会议的评估中，FedSight CoD的准确率达到93.75%，稳定性达到93.33%，其表现优于包括MiniFed和有序随机森林在内的基线模型。同时，该模型提供的透明推理过程与真实的FOMC官方沟通内容保持一致。",
                    "inspiration_trace": "好的，以下是我基于您提供的论文内容，对作者构思FedSight AI这一核心方法的逻辑链进行的系统性推演。\n\n---\n\n### **作者产出FedSight AI的思考过程推演**\n\n#### **第一阶段：解构问题，识别核心矛盾**\n\n1.  **宏观问题：** 如何准确且透明地预测联邦基金利率？这是一个对金融市场和宏观经济至关重要的问题。\n\n2.  **观察现有方法的缺陷：**\n    *   **传统计量经济学模型（如泰勒规则）：** 优点是逻辑清晰、可解释。但缺点是过于简化，假设了静态关系，无法捕捉复杂动态的现实。\n    *   **主流机器学习模型（如随机森林、LSTM）：** 优点是预测精度更高，能捕捉非线性关系。但缺点是“黑箱”，无法解释预测背后的原因，且难以处理非数字信息。\n\n3.  **发现关键缺口：** 作者敏锐地指出，FOMC的决策过程本身是**审议性的**和**叙事驱动的**。委员们不仅看CPI、失业率等**结构化数据**，还非常依赖《褐皮书》中的地区性轶事证据、演讲稿等**非结构化信息**。现有模型普遍忽略了这一关键维度，导致它们即使预测正确，也无法复现决策的真实逻辑。\n\n> **核心矛盾浮现：** 我们需要在**预测准确性**和**过程透明性**之间取得平衡，同时必须能融合**结构化与非结构化数据**，以反映决策的真实复杂性。\n\n#### **第二阶段：提出核心假设——从“预测结果”到“模拟过程”**\n\n1.  **思维转变：** 作者不再将利率预测视为一个从“输入数据”到“输出结果”的直接映射问题（`f(数据) -> 利率`）。\n\n2.  **核心假设：** 如果我们不直接预测结果，而是**模拟产生这个结果的过程**呢？FOMC的利率决策并非单一算法的产物，而是一个**集体智慧的涌现**。因此，最准确的预测模型，应该是一个能够复现这个集体决策过程的模型。\n\n3.  **假设的价值：** 这种“过程模拟”的方法天然地解决了第一阶段的核心矛盾。\n    *   **透明性：** 模拟过程本身就是解释。我们可以看到每个“委员”如何辩论、权衡，最终达成共识。\n    *   **数据融合：** 在模拟的审议过程中，可以自然地让“委员”们同时阅读经济报表（结构化）和《褐皮书》（非结构化）。\n    *   **准确性：** 如果模拟过程足够逼真，其“涌现”出的结果理应与现实高度一致。\n\n> **核心思想确立：** 将FOMC决策视为一个**多智能体 deliberative system（审议系统）**，预测任务转变为构建并运行这个系统。\n\n#### **第三阶段：寻找实现假设的技术载体**\n\n1.  **如何模拟“人”？** 要模拟FOMC委员，需要一个能理解复杂指令、进行推理、并表达观点的“大脑”。**大型语言模型（LLMs）** 正是理想选择。它们具备强大的自然语言理解和生成能力，可以被赋予角色和任务。\n\n2.  **如何组织“人”的互动？** 要模拟一个委员会的互动，需要一个能让多个智能体协作、通信和决策的框架。**多智能体系统（MAS）** 架构完美契合这一需求。每个LLM可以封装成一个独立的Agent，它们在预设的规则下进行交互。\n\n> **技术路线明确：** 采用 **LLM驱动的多智能体系统** 来实现FOMC决策过程的模拟。\n\n#### **第四阶段：构建方法论框架——从抽象到具体**\n\n1.  **数据输入设计：** 为了让模拟逼真，必须给Agents提供与真实委员相同的信息“食粮”。因此，作者明确地将输入分为两类：\n    *   **结构化指标：** 通胀、就业等传统经济数据。\n    *   **非结构化叙事：** 《褐皮书》、点阵图、市场预期等，这些是驱动定性判断的关键。\n\n2.  **Agent设计：** 模拟全部19位FOMC成员既不现实也无必要。作者采用了一个聪明的抽象：\n    *   通过对历史FOMC成员的特征（如鹰派/鸽派倾向、地区背景）进行**聚类**，提炼出几个有代表性的**原型Agent**（如“地区实用主义者”、“学术平衡者”）。\n    *   这既保留了观点的多样性，又控制了系统的复杂度，是一个在**真实性与效率**之间的绝佳平衡。\n\n3.  **工作流设计：** 为了让审议有序进行，作者设计了一个结构化的流程：\n    *   **分工明确：** 设立分析师、经济学家和委员等不同角色。\n    *   **流程驱动：** 分析师提供市场视角 -> 经济学家提出政策选项 -> 委员们基于所有信息进行辩论和投票。\n    *   这个流程确保了模拟不是一盘散沙，而是有组织、有逻辑的集体推理。\n\n> **方法论成型：** FedSight AI的雏形诞生——一个由分工明确的LLM Agents组成，融合多源数据，遵循结构化工作流来模拟FOMC审议的系统。\n\n#### **第五阶段：迭代优化与验证**\n\n1.  **发现问题：** 基础框架虽好，但LLM的推理可能冗长、发散，且缺乏“经验”。如何提升其效率和准确性？\n\n2.  **针对性改进：**\n    *   **提升效率与准确性：** 引入**Chain-of-Draft (CoD)**机制。通过强制Agent进行简洁、分步的“草稿-修订”式思考，减少了冗余Token（降低成本），并使推理链条更聚焦、更准确。\n    *   **赋予“经验”：** 引入**In-Context Learning (ICL)**。让Agents在历史会议中进行“模拟训练”，事后看到真实结果并进行反思。这相当于给Agent植入了“历史记忆”，使其决策更稳健。\n\n3.  **全面验证：** 作者没有只看“预测准确率”这一个指标。为了证明其方法论的优越性，他们设计了一套多维度的评估体系：\n    *   **准确性：** 总体准确率、Agent准确率。\n    *   **稳定性：** 多次模拟结果的一致性。\n    *   **透明性：** 生成的声明与真实FOMC声明的语义相似度。\n    *   **效率：** 平均Token消耗量。\n\n    通过与基线模型（MiniFed, 随机森林）在这些维度上的全面对比，有力地证明了FedSight CoD不仅在准确性上领先，更在稳定性、透明度和效率上实现了超越，完美回应了最初设定的目标。\n\n---\n\n**总结：** 作者的思考路径是一个从**问题解构**到**范式创新**，再到**技术实现**和**迭代优化**的完整闭环。他们没有在现有模型的框架内做增量改进，而是回归问题本源，洞察到FOMC决策的“过程”本质，并大胆地用LLM-MAS这一前沿技术对其进行模拟，最终通过精巧的设计和严谨的验证，产出了一篇兼具创新性与实用性的学术作品。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions",
                    "arxiv_id": "2512.15959",
                    "authors": "Armağan Amcalar, Eyup Cinar",
                    "summary": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合你的研究范围，应被保留。我的判断过程如下： 1.  **第一步：核心判断——保留** - 论文的核心贡献是提出了 **BRAID (Bounded Reasoning for Autonomous Inference and Decisions)**，这是一个全新的“有界推理框架”。 - 它的本质不是将现有智能体应用到某个领域，而是**构建和改进LLM智能体的核心推理机制**。摘要明确指出，该技术旨在“优化自主智能体系统中的推理效率”，并“为生产系统中的智能体”提高准确性和成本效率。 - 这完全符合“核心贡献在于构建、改进LLM智能体”的保留标准，不属于“非演化型应用”、“非Agentic的推理”或“基础设施”等排除类别。 2.  **第二步：正面指标——高度相关** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI`, `LLM-based Agents` (摘要中明确提到 \"agents in production systems\" 和 \"autonomous agent systems\")。 - **智能体能力**: `Planning` (BRAID框架通过“指令图”实现结构化推理，这是一种高级的规划和多步推理能力)。 - 这些正面指标强烈表明该论文与你的研究焦点高度契合。 3.  **第三步：排除标准——不适用** - 论文的主要贡献是关于智能体的推理效率和框架，而非安全、对齐或多模态。因此，不触发任何排除标准。 4.  **第四步：处理特殊和模糊情况——符合保留条件** - **推理/规划**: 这是本论文的关键。它完美地符合“保留”条件：“如果论文是关于智能体如何进行规划或在复杂任务中进行多步推理（如 ReAct、ToT 或新的Agentic框架）”。BRAID本身就是一个新的Agentic推理框架，它通过结构化的指令图来约束和引导智能体的推理过程，这与ReAct、ToT等在思想上一脉相承，但提出了新的实现方式（有界、结构化）。它不是在提升LLM的基础数学或逻辑能力，而是在为智能体提供一个更高效、更可控的“思考”框架。 5.  **第五步：最终决策** - 综合以上分析，论文《BRAID》的核心是提出一种新颖的、用于优化LLM智能体推理过程的结构化框架。它直接属于你的研究焦点中的**“单智能体”**方向，特别是**“规划”**和**“推理”**子方向。该论文为构建更高效、更强大的LLM智能体提供了新的方法论，因此完全符合你的筛选要求。",
                    "summary2": "本文旨在解决LLM推理中因无界自然语言思维链导致的成本高昂和效率低下问题。针对多GPT模型层级和复杂推理任务，我们提出了一种名为BRAID的有界推理框架，它使用基于Mermaid图的符号化结构来替代自然语言推理路径。在GSM-Hard、SCALE MultiChallenge和AdvancedIF基准数据集上，通过准确率和性能每美元指标验证了其有效性。",
                    "summary_translation": "大型语言模型在性能、成本和 token 使用量之间表现出非线性关系。本文针对使用 BRAID (Bounded Reasoning for Autonomous Inference and Decisions, 自主推理与决策的有界推理) 的结构化提示方法进行了一项定量研究。该研究在多个 GPT 模型层级上展开，并基于 AdvancedIF、GSM-Hard 和 SCALE MultiChallenge 基准数据集进行了评估。BRAID 引入了一个有界推理框架，该框架使用基于 Mermaid 的指令图，使模型能够进行结构化推理，而非通过无界的自然语言 token 扩展。研究表明，结构化的、机器可读的提示能够显著提升生产系统中智能体的推理准确性和成本效益。这些发现确立了 BRAID 作为一种优化自主智能体系统推理效率的有效且可扩展的技术。所有数据集和详细的结果日志可在 https://benchmark.openserv.ai 获取。",
                    "inspiration_trace": "好的，以下是我基于论文内容，对作者提出BRAID方法的核心思考过程的系统性推演。\n\n---\n\n### **作者产出BRAID方法的逻辑链推演**\n\n#### **第一步：宏观观察与核心矛盾**\n\n1.  **起点：LLM的“成本-性能”困境。**\n    作者首先观察到一个普遍存在的行业痛点：大型语言模型（LLMs）虽然性能强大，但其推理成本高昂，构成了大规模应用（尤其是自主代理系统）的主要瓶颈。性能、成本和token消耗之间并非简单的线性关系，这暗示着存在优化的空间。\n\n2.  **深入：现有解决方案的“副作用”。**\n    作者回顾了提升LLM推理能力的主流技术路径——从标准提示到思维链。他们肯定了CoT的突破性，但敏锐地指出了其内在缺陷：\n    *   **经济缺陷：** CoT通过自然语言“思考”，导致推理轨迹冗长，产生了大量“低语义密度”的token，直接推高了API调用成本。\n    *   **可靠性缺陷：** 自由形式的文本推理容易产生“推理漂移”，即模型可能偏离主题、重复或产生不相关的中间步骤，影响最终答案的准确性和一致性。\n\n3.  **矛盾聚焦：**\n    核心矛盾浮现：**我们希望模型进行更复杂的推理，但引导其推理的现有方法（自然语言CoT）本身却在制造成本和不可靠性。** 后续的改进方法（如Plan-and-Solve）虽然增加了结构，但仍未脱离“自然语言”这一媒介，治标不治本。\n\n#### **第二步：提出颠覆性假设**\n\n1.  **质疑根本：** 作者开始质疑：**推理的“载体”是否出了问题？** 如果自然语言本身就是一种低效、高噪的推理媒介，那么无论我们如何优化其“内容”，都无法根除其“形式”带来的问题。\n\n2.  **核心假设：** **如果我们将模型的无界自然语言推理，替换为一种有界的、符号化的、机器可读的结构，会怎样？**\n    *   **“有界”：** 约束模型的输出空间，防止推理漂移，确保路径确定性。\n    *   **“符号化”：** 用高信息密度的符号代替冗长的自然语言描述，大幅压缩token消耗。\n    *   **“结构化”：** 将逻辑流程（如A->B->C）显式表达，让模型遵循一个清晰的“路线图”，而非自由发挥。\n\n3.  **技术选型：** 基于上述假设，作者选择了Mermaid流程图作为具体实现。因为它既是文本格式（易于LLM生成和解析），又能直观地表达逻辑分支和顺序，完美契合了“符号化结构”的要求。\n\n#### **第三步：构建方法论框架**\n\n1.  **从假设到框架：** 这个假设最终固化为**BRAID（Bounded Reasoning for Autonomous Inference and Decisions）**框架。其核心机制是：**用Mermaid指令图取代自然语言CoT，作为LLM的“推理蓝图”。**\n\n2.  **架构设计：分离关注点。**\n    作者意识到，生成高质量的“推理蓝图”和执行这个蓝图是两种不同的能力。这自然引出了一个两阶段架构：\n    *   **生成阶段：** 使用一个能力强大的模型（如GPT-5 Medium）作为“架构师”，负责理解复杂问题并生成高质量的Mermaid图。这个成本是一次性的或可被摊销的。\n    *   **求解阶段：** 使用一个成本更低、速度更快的模型（如GPT-5 Nano）作为“执行者”，严格按照Mermaid图的指令进行推理并生成最终答案。这个成本是高频发生的。\n\n3.  **定义新范式：** 这种“生成-求解”分离的模式，将LLM的推理能力从单一的“模型规模”问题，解耦为**“规划质量”**和**“执行效率”**两个可独立优化的维度。\n\n#### **第四步：验证与理论升华**\n\n1.  **实验验证：** 作者设计了严谨的实验，在多个基准数据集上对比了BRAID与传统CoT。结果验证了假设：\n    *   **准确性提升：** BRAID显著提升了各层级模型的准确率，尤其是让小模型达到了以往只有大模型才能企及的水平。\n    *   **成本效率革命：** 通过“大模型生成 + 小模型求解”的组合，实现了数十倍甚至上百倍的“每美元性能”提升。\n\n2.  **理论升华：BRAID奇偶效应。**\n    基于实验结果，作者提出了一个更具普适性的论断：**推理性能 = 模型容量 × 提示结构。**\n    这意味着，**可以通过增加“提示结构”的复杂性来补偿“模型容量”的不足。** 这挑战了“模型越大越好”的传统观念，为成本敏感型应用提供了全新的优化思路。小模型在BRAID的“加持”下，可以与无结构提示的大模型“奇偶”对等，甚至超越。\n\n3.  **最终结论：** BRAID不仅仅是一个提示技巧，而是一种可扩展的、系统性的方法论。它通过重构推理的底层表示，从根本上改变了LLM推理的经济模型，使得部署可靠、高效、低成本的自主代理成为可能。\n\n---\n\n**总结：** 作者的思考路径始于一个实际的工程问题（成本），通过深入分析现有技术方案的内在缺陷，大胆地对推理的“载体”本身提出质疑，从而形成了一个“用符号化结构替代自然语言”的核心假设。基于此假设，他们构建了BRAID框架及其分离式架构，并通过实验验证了其有效性，最终将发现升华为“模型容量×提示结构”的理论，为LLM推理优化开辟了新的方向。整个过程体现了从现象观察、问题诊断、假设提出、方法构建到理论升华的完整学术创新链条。"
                },
                {
                    "title": "Adaptation of Agentic AI",
                    "arxiv_id": "2512.16301",
                    "authors": "Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han",
                    "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合你的研究范围，应予以保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将智能体作为工具去解决某个特定领域的问题，而是对“Agentic AI”的“Adaptation”（适应/演化）这一核心机制进行系统性梳理和框架构建。论文的核心贡献是提出了一个统一的框架，用于理解和设计智能体的适应策略，这直接关系到如何“改进或演化LLM智能体”，完全符合你的核心目标。 2.  **第二步：正面指标** - 论文标题和摘要中明确包含了你的核心关注点：`Agentic AI`。 - 摘要中提到了智能体的关键能力：`plan` (规划), `reason` (推理), `interact with external tools` (工具使用)。 - 论文的核心主题“Adaptation”与你的研究焦点“自我演化”高度相关，其子分类如“agent adaptations”和“tool adaptations”直接对应了智能体的改进和演化机制。这可以看作是对`Self-Improvement`和`Iterative Improvement`等概念的系统性归纳。 3.  **第三步：排除标准** - 论文的主要贡献不涉及`Safety`、`Alignment`、`Interpretability`等安全与对齐问题。 - 论文也未聚焦于`Vision`、`MLLMs`等多模态技术。因此，没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文明确将智能体的`planning`和`reasoning`作为其适应能力的基础，这属于智能体框架下的多步推理，符合保留条件。 - **自我演化的应用**: 这篇论文并非特定领域的应用，而是提出了一个通用的“适应”框架，其本身就是对“自我演化”机制的理论化和系统化，因此完全符合保留要求。 **最终决策**: 这篇论文的核心贡献在于为“LLM智能体的适应与演化”这一前沿领域提供了一个系统性的理论框架和设计路线图。它虽然不是提出一个全新的智能体模型，但它为你这样的研究者梳理了整个领域的设计空间、权衡取舍和未来方向。对于旨在“构建、改进或演化LLM智能体”的研究课题而言，这类高屋建瓴的综述和框架性论文是极具价值的导航性文献，能帮助你快速把握领域全貌，定位研究切入点。因此，它高度相关，必须保留。",
                    "summary2": "本文旨在为AI智能体的适应研究建立一个统一的系统性框架。针对现有研究碎片化的问题，我们提出了一种将适应分为智能体适应和工具适应的2x2分类法，细分为A1、A2、T1、T2四种范式。我们通过回顾代表性方法（如DeepRetrieval、s3）在多个基准上的表现，并从成本、灵活性、泛化性等维度进行比较，验证了该框架在阐明设计空间和权衡方面的有效性。",
                    "summary_translation": "前沿的 `agentic AI systems (智能体AI系统)` 构建于 `foundation models (基础模型)` 之上，这些模型可以通过适配来执行规划、推理以及与外部工具交互等操作，以完成日益复杂和专门化的任务。随着这些系统能力和范围的不断扩大，`adaptation (适配)` 已成为提升其性能、可靠性和泛化能力的核心机制。本文将这一快速发展的研究格局整合为一个系统性框架，该框架涵盖了 `agent adaptations (智能体适配)` 和 `tool adaptations (工具适配)` 两个维度。我们进一步将其细分为 `tool-execution-signaled (工具执行信号驱动的)` 和 `agent-output-signaled (智能体输出信号驱动的)` 两种 `agent adaptation (智能体适配)` 形式，以及 `agent-agnostic (与智能体无关的)` 和 `agent-supervised (智能体监督的)` 两种 `tool adaptation (工具适配)` 形式。我们证明了该框架有助于厘清 `agentic AI (智能体AI)` 中适配策略的 `design space (设计空间)`，明确了不同策略间的 `trade-offs (权衡)`，并为系统设计过程中的策略选择或切换提供了实践指导。接着，我们回顾了每个类别下的代表性方法，分析了其优势与局限性，并指出了关键的开放性挑战与未来机遇。总而言之，本文旨在为致力于构建能力更强、效率更高、可靠性更优的 `agentic AI systems (智能体AI系统)` 的研究人员与实践者，提供概念基础和实践路线图。",
                    "inspiration_trace": "### 作者核心方法逻辑链推演\n\n---\n\n#### **1. 宏观问题：智能体AI的适应性瓶颈**\n- **观察现象**：  \n  基于基础模型的智能体AI（如LLM）在复杂任务中表现优异，但存在**工具使用不可靠、长程规划能力弱、领域泛化差**等问题（§1）。  \n- **核心矛盾**：  \n  基础模型虽强大，但需针对特定任务/环境**动态调整行为**，而现有研究缺乏系统性分类框架（§1）。\n\n---\n\n#### **2. 关键假设：适应性是智能体优化的核心机制**\n- **提出假设**：  \n  适应性（Adaptation）是提升智能体性能、可靠性和泛化能力的**关键杠杆**（§1）。  \n- **验证路径**：  \n  需建立统一框架，解构适应性的**目标**（优化对象）和**信号来源**（反馈机制）。\n\n---\n\n#### **3. 框架构建：二维分类法**\n- **维度1：适应对象**（§3.2）  \n  - **智能体适应**（Agent Adaptation）：优化模型内部策略（参数/行为）。  \n  - **工具适应**（Tool Adaptation）：优化外部组件（检索器、记忆模块等）。  \n- **维度2：信号来源**（§3.2）  \n  - **工具执行信号**（Tool-Execution Signaled）：基于工具输出（如代码执行结果、检索分数）。  \n  - **智能体输出信号**（Agent-Output Signaled）：基于任务最终结果（如答案正确性）。  \n- **形成四范式**（图1）：  \n  - **A1**：工具执行信号驱动的智能体适应（§4.1）。  \n  - **A2**：智能体输出信号驱动的智能体适应（§4.2）。  \n  - **T1**：智能体无关的工具适应（§5.1）。  \n  - **T2**：智能体监督的工具适应（§5.2）。\n\n---\n\n#### **4. 框架验证：文献分析与演进脉络**\n- **A1演进**（§4.1）：  \n  - **早期**：SFT模仿成功轨迹（Toolformer）。  \n  - **突破**：RLVR（可验证奖励的强化学习）直接利用工具反馈（DeepRetrieval）。  \n  - **逻辑**：从**静态模仿**→**动态环境交互**，提升工具使用可靠性。  \n- **A2演进**（§4.2）：  \n  - **无工具**：优化推理链（DeepSeek-R1）。  \n  - **有工具**：协调工具策略（Search-R1）。  \n  - **逻辑**：从**单一推理**→**工具-推理协同**，解决长程规划问题。  \n- **T1→T2演进**（§5）：  \n  - **T1**：预训练工具即插即用（HuggingGPT）。  \n  - **T2**：工具受智能体监督动态优化（s3搜索子智能体）。  \n  - **逻辑**：从**静态工具库**→**共生适应**，降低训练成本（T2数据效率比A2高70倍）。\n\n---\n\n#### **5. 深化理解：范式权衡与协同**\n- **关键对比**（§6）：  \n  - **A1/A2**：高灵活性但成本高，易灾难性遗忘。  \n  - **T1/T2**：模块化强、数据高效，但受限于智能体能力。  \n- **协同价值**：  \n  - **混合系统**（如DeepResearch）结合A2（推理）+T2（检索），实现性能与效率平衡（§7.1）。  \n- **设计启示**：  \n  - **A1**适合工具密集型任务（代码执行）。  \n  - **T2**适合资源受限场景（轻量子智能体）。\n\n---\n\n#### **6. 未来方向：框架的局限性突破**\n- **协同适应**（§8.1）：  \n  当前范式孤立优化，需发展**智能体-工具联合优化**（如生物共生模型）。  \n- **持续适应**（§8.2）：  \n  解决非平稳环境下的**灾难性遗忘**（如动态记忆模块）。  \n- **安全适应**（§8.3）：  \n  约束RL探索行为，防止工具滥用（如安全策略投影）。\n\n---\n\n### 思想演进总结\n```mermaid\ngraph LR\nA[问题：智能体适应性不足] --> B[假设：适应性是核心优化机制]\nB --> C[框架：二维分类法]\nC --> D[验证：文献演进分析]\nD --> E[深化：范式权衡与协同]\nE --> F[突破：协同/持续/安全适应]\n```\n\n**核心逻辑链**：  \n**问题驱动** → **假设提出** → **框架构建** → **实证验证** → **系统优化** → **前沿拓展**。  \n作者通过**解构适应性本质**，将零散研究统一为可操作的设计空间，为智能体AI提供从方法论到系统工程的完整路线图。"
                },
                {
                    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
                    "arxiv_id": "2512.16917",
                    "authors": "Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille",
                    "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合您的研究范围。我的判断依据如下： 1.  **核心判断 (第一步):** - **保留**。这篇论文的本质不是将LLM作为工具应用，而是提出了一种全新的训练框架——**生成式对抗推理器**。其核心贡献在于方法论创新，即通过**对抗性强化学习共同演化**一个LLM推理器和一个LLM判别器。这直接命中了您研究目标中的“构建、改进或演化 LLM智能体”。 2.  **正面指标 (第二步):** - 论文包含了多个核心关注点： - **自我演化**: 摘要中明确使用了 **\"co-evolving\"** (共同演化) 一词，这是您研究焦点“自我演化”的直接体现。推理器和判别器在对抗过程中相互促进、共同迭代完善。 - **多智能体**: 该框架本质上是一个由**推理器**和**判别器**组成的双智能体系统。它们通过对抗性的交互和通信（判别器提供反馈）来共同完成任务，这符合“多智能体”的定义。 - **自我反思/修正**: 判别器的角色就是对推理器的推理链进行评估和纠错，这相当于为推理器提供了一个外部的、结构化的**自我反思** 和 **自我修正** 机制，是单智能体能力的关键部分。 3.  **排除标准 (第三步):** - 论文的主要贡献是提升推理能力的方法论，而非安全、对齐或多模态研究。因此，没有触发任何排除标准。 4.  **特殊和模糊情况 (第四步):** - **推理/规划**: 这篇论文虽然关注数学推理，但它并非简单地通过数据增强或微调来提升LLM的基础数学能力。相反，它构建了一个**智能体框架**，让推理过程变得可评估、可对抗、可演化。这完全符合“保留”关于智能体如何进行规划和多步推理的论文的标准。其核心是**过程**和**框架**，而非结果。 **核心依据总结**: 该论文的核心贡献是提出了一种名为“生成式对抗推理器”的**新框架**，该框架通过让两个基于LLM的智能体（推理器和判别器）**对抗性共同演化**，来提升推理能力。这精准地契合了您研究课题中的**“自我演化”**和**“多智能体”**两个核心方向，同时其判别器机制也涉及了单智能体的**“自我反思”**能力。因此，这是一篇高度相关的前沿论文，应当保留。",
                    "summary2": "本文旨在解决LLM在数学推理中的过程错误问题。针对复杂数学推理任务，我们提出了一种生成式对抗推理器框架，通过对抗性强化学习共同进化LLM推理器与判别器。其核心是将推理链划分为逻辑切片，由判别器评估以生成密集的步骤级奖励。在AIME24等多个数学基准上，通过Pass@1指标验证了其有效性，显著提升了基线模型性能。",
                    "summary_translation": "具有显式推理能力的大型语言模型在数学推理方面表现优异，但仍会犯下过程错误，如计算错误、逻辑脆弱以及步骤看似合理实则无效。本文中，我们提出了生成式对抗推理器，这是一个在策略联合训练框架，旨在通过对抗强化学习，协同进化一个LLM推理器和一个基于LLM的判别器，从而增强模型的推理能力。该框架采用一种计算高效的审查调度，将每条推理链划分为长度相近且逻辑完整的片段，判别器则通过简洁、结构化的论证来评估每个片段的合理性。学习过程耦合了两种互补信号：LLM推理器因生成逻辑一致且能得出正确答案的步骤而获得奖励，而判别器则因正确检测出错误或区分推理轨迹而获得奖励。这种机制产生了密集、校准良好的在策略步骤级奖励，用以补充稀疏的精确匹配信号，从而改善了信用分配，提高了样本效率，并提升了LLM的整体推理质量。在多个数学基准测试中，与经过标准强化学习后训练的强基线模型相比，本方法取得了稳定且一致的提升。具体而言，在AIME24基准上，我们将DeepSeek-R1-Distill-Qwen-7B的性能从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B的性能从43.7提升至53.7（+10.0）。此外，该模块化的判别器还支持灵活的奖励塑形，可应用于教师蒸馏、偏好对齐以及基于数学证明的推理等多种目标。",
                    "inspiration_trace": "### 作者思考过程的逻辑链推演\n\n基于论文内容，我系统性地还原作者提出“Generative Adversarial Reasoner (GAR)”的思考过程。逻辑链从宏观问题出发，逐步聚焦到方法论，突出思想演进而非实现细节。核心脉络是：**观察问题 → 识别现有方法不足 → 形成核心假设 → 发展方法论 → 验证与优化**。\n\n---\n\n#### **1. 观察宏观问题：LLMs推理过程的脆弱性**\n- **起点**：作者观察到大型语言模型（LLMs）在数学推理任务中表现优异（如GSM8K、AIME），但推理过程常犯“过程错误”，包括计算错误、逻辑缺陷、表面合理但无效的步骤。\n- **深层问题**：这些错误导致最终答案不准确，且现有训练方法（如监督微调）主要关注最终答案匹配，忽略了中间步骤的质量。推理过程是“黑箱”，错误难以定位和纠正。\n- **关键洞察**：推理质量是LLMs可靠性的瓶颈，需细粒度、过程级的反馈机制，而非仅依赖稀疏的最终答案奖励。\n\n---\n\n#### **2. 识别现有方法不足：监督与自监督的局限**\n- **现有方法分析**：\n  - **过程奖励模型（PRMs）**：提供步骤级监督，但依赖昂贵的人工标注，标注主观且易出错（如过奖励或欠奖励）。\n  - **基于提示的批评家（Prompt-based Critics）**：用LLMs作为裁判，成本低，但判断嘈杂、不一致，判别性弱（如无法区分细微错误）。\n- **核心不足**：这些方法要么成本高（PRMs），要么不可靠（提示方法），且无法动态适应模型进化。奖励信号稀疏（仅最终答案），导致信用分配困难，样本效率低。\n- **作者反思**：需要一个自监督、在线的框架，能生成密集、校准的步骤级奖励，减少人工依赖，并随模型共同进化。\n\n---\n\n#### **3. 形成核心假设：对抗共同进化可自生成高质量反馈**\n- **灵感来源**：GANs（生成对抗网络）的对抗机制——生成器和判别器相互竞争，共同提升。作者类比：推理器（生成推理链）和判别器（评估推理链）可类似进化。\n- **核心假设**：如果推理器和判别器通过对抗强化学习（RL）联合训练，判别器能提供可靠、细粒度的步骤级奖励，推理器则学习生成更逻辑一致的步骤。这会：\n  - 减少对人工标注的依赖。\n  - 动态校准奖励信号（避免奖励漂移）。\n  - 提供密集反馈，改善信用分配。\n- **关键推论**：推理链太长时，整体评估不可行；需分解为可管理的单元（如“片段”），使判别器任务更简单、更可靠。\n\n---\n\n#### **4. 发展方法论：从切片到对抗框架**\n- **步骤1：切片（Slicing）解决评估复杂性**\n  - **问题**：长推理链（数千token）难以整体评估，错误定位模糊。\n  - **创新**：将推理链划分为逻辑完整的“片段”（slices），长度可比（如320 token），基于分隔符合并语义单元。\n  - **理由**：短片段更易评估，提供局部化反馈（如“此步骤计算错误”），而非全局判断。这提升可靠性，并生成密集奖励（每个片段一个分数）。\n\n- **步骤2：判别器设计增强可解释性与效率**\n  - **问题**：判别器需提供可靠判断，但生成完整理由成本高。\n  - **创新**：判别器输出结构化为“分析 → 二元判断（YES/NO）→ 简洁理由”，并限制长度（如128 token）。\n  - **理由**：平衡效率与可解释性；理由用于调试，但训练中可截断以加速。添加SFT阶段，用GPT-4o-mini生成少量标注数据，初始化判别器。\n\n- **步骤3：对抗RL框架实现共同进化**\n  - **推理器奖励**：结合最终答案匹配（稀疏信号）和判别器片段奖励（密集信号），公式为 \\( R_{\\text{rea}} = \\lambda_1 R_m + \\lambda_2 R_s \\)。\n  - **判别器奖励**：双重目标——对齐奖励（与最终答案一致性）和判别奖励（区分生成与参考轨迹），公式为 \\( R_{\\text{dis}} = \\lambda_3 R_d + \\lambda_4 R_a \\)。\n  - **联合训练**：在线策略更新（如GRPO），推理器和判别器同步优化。判别器随推理器进化，检测更细微错误，避免奖励黑客（reward hacking）。\n\n- **步骤4：效率优化确保实用性**\n  - **问题**：片段评估增加计算开销。\n  - **创新**：计算高效的审查计划——判别器仅分析片段，不生成完整推理；输出截断控制成本。\n  - **理由**：实验显示，128 token限制不损性能，但显著加速训练（表3）。\n\n---\n\n#### **5. 验证与优化：实验驱动迭代**\n- **验证假设**：在数学基准（如AIME24、GSM8K）测试，GAR一致优于强基线（如DeepSeek-R1），证明对抗框架有效。\n- **关键发现**：\n  - 切片级奖励改善信用分配，样本效率提升（表4）。\n  - 联合训练防止奖励漂移，判别器随推理器进化（表4行7 vs. 行4）。\n  - 方法减少熵崩溃，保持模型探索性（图3）。\n- **优化迭代**：\n  - 调整切片长度（320-560 token最优，表9）。\n  - 添加对齐和判别奖励的互补性（表4行5-7）。\n  - 扩展应用：如部分轨迹训练（无需最终答案，表5）和推理模式蒸馏（表6）。\n\n---\n\n### 逻辑链总结\n- **思想演进脉络**：从“推理过程错误”的宏观问题，到“现有方法不可靠”的不足，再到“对抗共同进化”的假设，最终发展为“切片+对抗RL”的方法论。核心是**自生成反馈**：通过内部对抗，替代外部标注，实现密集、校准的步骤级监督。\n- **创新本质**：将GANs的对抗思想引入推理训练，结合切片分解和RL，解决稀疏奖励和成本问题。这不仅是技术改进，更是范式转变——从静态监督到动态共同进化。\n- **简洁逻辑链**：问题 → 不足 → 假设 → 方法 → 验证。作者始终围绕“如何高效提升推理过程质量”，每一步都针对前一步的痛点，形成闭环。"
                },
                {
                    "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
                    "arxiv_id": "2512.16883",
                    "authors": "Tzu-Han Lin, Wei-Lin Chen, Chen-An Li, Hung-yi Lee, Yun-Nung Chen, Yu Meng",
                    "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，应被保留。以下是我的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `AdaSearch` 的新框架，用于构建和改进LLM智能体。其核心贡献不是将现有智能体应用到某个领域，而是解决智能体内部的一个关键问题：如何自适应地决定何时使用内部知识（参数化知识），何时调用外部工具（搜索引擎）。这直接属于“构建、改进LLM智能体”的范畴。 2.  **第二步：正面指标** - 论文高度符合我的核心关注点： - **核心范式**: 论文明确提到了 `search agents`，属于 `LLM-based Agents`。 - **智能体能力**: 论文的核心是关于智能体的 `Tool Use / Tool Augmentation`（工具使用）。它研究的是智能体如何更智能地决定是否调用搜索工具，这是一种高级的规划和决策能力。此外，论文中提到的 `knowledge-boundary awareness`（知识边界感知）可以看作是 `Self-Reflection`（自我反思）的一种体现，即智能体需要了解自己知识的局限性。 3.  **第三步：排除标准** - 论文未触发任何排除标准。虽然提到了 `interpretable`（可解释性），但这只是其框架的一个优点和特性，并非论文的主要研究贡献。论文的核心目标是提升智能体的决策效率和能力，而不是研究可解释性本身。论文也未涉及安全、对齐或多模态等排除领域。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文完美地符合“保留”条件。它研究的是智能体在解决问题时的多步决策过程（先判断是否需要搜索，再进行搜索或回答），这属于智能体的规划和推理范畴，而不是提升LLM本身的基础数学或逻辑能力。 **最终决策**: 这篇论文的核心贡献是提出了一种新的强化学习框架 `AdaSearch`，用于改进LLM智能体的工具使用决策机制。它使智能体能够更智能、更高效地平衡内部知识与外部搜索，这直接对应了我研究目标中的“单智能体”方向，特别是其“工具使用”和“自我反思”子方向。因此，这篇论文是高度相关且有价值的前沿研究，应被保留。",
                    "summary2": "本文旨在构建能自适应平衡参数化知识与外部搜索的LLM代理。针对开放域问答场景，我们提出了一种名为AdaSearch的两阶段RL框架，它将问题求解与搜索决策解耦，并通过显式推理提升决策的可解释性。在多个QA基准（如NQ, HotpotQA）和不同规模的LLM上，通过EM和提出的F1_aware指标验证了其有效性。",
                    "summary_translation": "通过强化学习 (RL, 强化学习) 将搜索引擎与大语言模型 (LLMs, 大语言模型) 相结合，已成为构建搜索代理的一种有效方法。然而，过度依赖搜索会带来不必要的成本，并有暴露于噪声或恶意内容的风险；而仅依赖参数知识则可能导致幻觉。其核心挑战在于开发能够自适应地平衡参数知识与外部搜索的代理，仅在必要时调用搜索。先前的研究通过围绕工具调用次数来设计奖励函数，以缓解搜索的过度使用。然而，这些惩罚机制需要大量的奖励工程，提供模糊的信用分配，且可能被那些表面上减少调用次数的代理所利用。此外，仅通过调用次数来评估性能会将必要与不必要的搜索混为一谈，从而掩盖了对真正自适应行为的衡量。为解决这些局限性，我们首先通过一个基于F1的决策指标量化了现有搜索代理的自我知识意识，结果揭示，诸如Search-R1等方法常常忽略了唾手可得的参数知识。基于这些发现，我们提出了AdaSearch，这是一个简单的两阶段、结果驱动的强化学习 (RL) 框架，它将问题解决与是否调用搜索的决策相解耦，并使这一决策过程变得明确且可解释。这种透明性对于金融和医疗问答等高风险领域至关重要，然而先前的研究方法却在很大程度上忽视了这一点。在多个模型系列和不同规模上的实验表明，AdaSearch显著提升了知识边界感知能力，减少了不必要的搜索调用，保持了强大的任务性能，并提供了更透明、可解释的决策行为。",
                    "inspiration_trace": "### 作者思考过程的逻辑链推演\n\n#### 1. **宏观问题识别：LLMs的知识边界困境**\n   - **起点**：大型语言模型（LLMs）在处理知识密集型任务时面临根本性矛盾——仅依赖参数知识易导致幻觉（hallucination），而过度依赖外部搜索则引入成本、安全风险（如噪声或恶意内容）。核心挑战是：如何让模型自适应地判断何时使用内部知识、何时调用搜索？\n   - **关键观察**：现有方法（如RAG或RL搜索代理）无法有效平衡这一矛盾。例如，Search-R1等RL方法虽提升任务性能，但普遍存在“过度搜索”问题（即使参数知识足够也调用搜索），而奖励工程方法（如惩罚搜索次数）需复杂调优且易被模型利用（如减少必要搜索）。\n   - **问题聚焦**：作者意识到，根本症结在于决策过程的“隐式性”——模型在推理中隐式决定是否搜索，导致无法显式评估自身知识边界，进而引发低效和不可信行为。\n\n#### 2. **量化问题：揭示现有方法的缺陷**\n   - **假设形成**：作者推测，现有方法（如Search-R1）的过度搜索源于模型缺乏“自我知识意识”（self-knowledge awareness），即无法识别参数知识是否足够。\n   - **验证方法**：设计F1-aware度量（基于F1分数的决策指标），量化模型对参数知识的判断能力（正类=模型认为参数知识足够）。实验显示，Search-R1的F1-aware接近0（表1），表明它几乎从不使用参数知识；而RL w/o Search虽F1-aware较高，但任务性能差。\n   - **洞察**：问题根源是决策与问题解决的耦合——现有RL框架将“是否搜索”的决策嵌入单一推理过程，导致模型无法显式学习知识边界。同时，奖励工程（如搜索次数惩罚）引入模糊信用分配，掩盖了真实自适应行为。\n\n#### 3. **核心假设：解耦决策与问题解决**\n   - **思想演进**：作者提出，若将决策过程显式化并独立优化，模型可更清晰地学习知识边界。具体假设：\n     - **解耦必要性**：决策（“是否搜索”）和问题解决（“如何回答”）是不同子任务，应分开训练以避免干扰。\n     - **简单奖励驱动**：避免复杂奖励工程，直接用任务结果（如答案正确性）作为奖励，提供清晰学习信号。\n     - **可解释性需求**：在高风险领域（如医疗），决策过程需透明，但现有方法忽略此点。\n   - **方法论雏形**：设计两阶段框架——第一阶段训练问题解决能力（参数知识+搜索），第二阶段训练决策能力（显式评估知识边界）。\n\n#### 4. **方法提出：AdaSearch框架**\n   - **逻辑落地**：基于假设，作者构建AdaSearch：\n     - **阶段1（问题解决）**：用RL训练模型在两种模式下解决问题——仅参数知识（prompt: `s_param`）和允许搜索（prompt: `s_search`）。奖励仅基于答案正确性（二元奖励），避免搜索次数惩罚。\n     - **阶段2（决策）**：用阶段1的模型生成伪标签（通过参数知识解决率），训练决策模型（prompt: `s_decision`）显式输出“是否搜索”的判断（如`<assessment>yes/no</assessment>`）。奖励同样基于任务结果，确保决策与性能对齐。\n     - **推理解耦**：推理时先决策（显式推理），再根据结果选择问题解决模式，实现透明化。\n   - **关键创新**：通过显式决策和简单奖励，解决奖励工程的模糊性，并提升可解释性（图1左）。\n\n#### 5. **验证与优化：实验驱动迭代**\n   - **初步验证**：实验显示AdaSearch显著提升F1-aware（表2），减少不必要搜索（图5d），同时保持任务性能（EM）。例如，在Qwen2.5-3B上，F1-aware从Search-R1的0.1提升至54.0。\n   - **迭代优化**：\n     - **对比奖励工程**：AdaSearch优于Naive Shaping等方法（表2），证明解耦决策比惩罚搜索更有效。\n     - **消融分析**：验证两阶段必要性——端到端训练（AdaSearch-E2E）和SFT变体（AdaSearch-SFT）性能较差（图3b），凸显分阶段优化的优势。\n     - **泛化测试**：在不同模型（Qwen/Llama）、检索器（E5/BM25）和任务（单跳/多跳QA）上验证鲁棒性（图5c）。\n   - **洞察深化**：实验揭示，简单奖励和显式决策能自然平衡知识边界，无需人工调参（如搜索次数阈值）。\n\n#### 6. **贡献升华：从问题到范式**\n   - **思想演进终点**：作者将AdaSearch定位为“结果驱动的自适应代理”，其核心逻辑是：通过解耦决策与问题解决，用RL的简单奖励机制替代复杂工程，实现高效、可信的搜索行为。\n   - **更广泛意义**：方法不仅解决搜索平衡问题，还为高可解释性AI提供范式——显式决策过程增强用户信任（如医疗场景），推动RL代理从“黑盒”转向“透明”。\n\n### 逻辑链总结\n作者从LLMs的知识边界矛盾出发，通过量化分析揭示现有方法的决策缺陷，形成“解耦决策与问题解决”的核心假设，进而设计AdaSearch框架。整个过程以问题驱动、实验验证为主线，强调简单性和可解释性，最终实现从宏观挑战到具体范式的演进。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 3,
            "papers": [
                {
                    "title": "ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning",
                    "arxiv_id": "2512.15756",
                    "authors": "Yoonpyo Lee",
                    "summary": "Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。我的判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献并非简单地将LLM应用于核工程领域，而是提出了一种能够让LLM**超越其训练数据约束、自主演化其设计能力**的机制。论文的关键发现是“涌现的设计空间扩展”，即模型在训练后能够“自主调整Gd存量”以满足新的约束条件。这本质上是一种**自我完善和迭代**的过程，完全符合“自我演化”的定义。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** 论文的核心贡献与“自我演化”高度相关。 *   **自我演化机制:** 论文的核心是展示了一种通过Direct Preference Optimization (DPO)实现的**自我完善**机制。模型并非简单地复现训练数据中的模式，而是在对齐过程中演化出了新的、更优的设计策略（如调整Gd存量、发现非对称配置）。 *   **涌现能力:** “涌现的物理推理”和“涌现的设计空间扩展”是论文的核心亮点，这表明模型的能力在训练后发生了质变，是自我演化的典型表现。 3.  **排除标准 (第三步):** 论文虽然使用了DPO（一种对齐技术），但其目标是**对齐设计偏好**（满足功率峰值约束），而非AI安全、伦理或可解释性。因此，它不触发“安全与对齐”的排除规则。论文也不涉及多模态或视觉内容。 4.  **特殊和模糊情况 (第四步):** 这篇论文是“自我演化的应用”这一特殊情况的完美例证。 *   **规则应用:** 根据第四步的规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域（如‘用于化学实验的自我演化智能体’），也应该保留。” *   **论文分析:** ReactorFold的核心贡献正是提出并验证了一种**新的自我演化机制**（通过DPO对齐实现设计能力的涌现和扩展）。核反应堆设计仅仅是验证该机制的实验平台。研究的焦点在于模型**如何演化**，而不是演化出的反应堆设计本身。 **最终决策 (第五步):** 综合以上分析，尽管这篇论文没有涉及传统的智能体规划、工具使用或多智能体协作，但其核心贡献在于探索和实现了一种LLM的**自我演化**路径。它展示了LLM如何通过特定的训练和对齐策略，突破人类预设的框架，自主地提升其在复杂任务中的表现。这与研究课题中的“自我演化”方向高度契合，因此应被**保留**。",
                    "summary2": "本文旨在突破传统核反应堆堆芯设计方法的局限，实现新型设计拓扑的生成式发现。针对17x17压水堆（PWR）燃料组件的布局优化问题，我们提出了一种名为ReactorFold的生成式框架，通过将堆芯布局序列化，结合课程式训练（FFT、LoRA）和基于物理反馈的DPO对齐。在基于OpenMC的17x17 PWR组件仿真环境中，与遗传算法（GA）基准进行对比，通过有效增殖因子、功率峰值因子等物理指标验证了模型能涌现式地扩展设计空间并发现高性能非对称配置。",
                    "summary_translation": "核反应堆堆芯设计需要探索由复杂的中子学相互作用支配的大型离散设计空间。传统的确定性方法、元启发式方法和机器学习辅助方法在固定的、人为定义的构型空间内搜索，限制了其发现根本上全新的设计拓扑的能力。本文介绍了ReactorFold，这是一个将燃料组件设计重新表述为语言模型序列建模问题的生成式框架。该模型利用蒙特卡洛数据、参数高效微调和直接偏好优化，学习压水堆组件的潜在结构，并在单次前向传递中生成候选布局。值得注意的是，经过DPO对齐的模型表现出涌现的设计空间扩展：尽管仅在具有固定数量钆可燃毒物棒的构型上进行训练，它仍能自主调整钆的装载量以满足严格的功率峰约束。该模型还发现了挑战传统对称装载启发式方法的高性能非对称构型，进入了传统搜索方法无法触及的设计区域，这表明语言模型能够内化因果物理关系，并超越人为施加的设计约束。",
                    "inspiration_trace": "好的，以下是对论文《ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning》作者核心思路的逻辑推演，旨在还原其从问题观察到方法创新的完整思考过程。\n\n---\n\n### **第一步：宏观问题与时代背景——核能设计的紧迫性**\n\n**起点：** 全球碳中和目标与能源需求激增，使得核能，尤其是小型模块化反应堆（SMR），成为战略关键。然而，SMR的设计周期长、成本高，成为其快速部署的瓶颈。\n\n**核心矛盾：** 一方面是政策层面（如“创世纪任务”）对AI加速核能创新的迫切需求；另一方面是工程实践中，反应堆堆芯设计这一核心环节仍严重依赖传统方法，效率低下。\n\n**思考方向：** 如何从根本上颠覆现有的设计范式，以满足时代对速度和创新的要求？\n\n---\n\n### **第二步：核心观察——传统方法的“盒子”限制**\n\n**深入分析：** 作者审视了现有的主流设计方法，包括确定性优化、元启发式算法（如遗传算法GA）以及机器学习辅助优化。\n\n**关键洞察：** 这些方法无论形式如何，其本质都是在一个**预先定义好的、固定的设计空间**中进行“搜索”。工程师必须先设定好规则，比如：\n1.  **组件类型固定：** 使用哪些燃料、控制棒、可燃毒物。\n2.  **数量固定：** 例如，一个组件中必须恰好有16根含钆（Gd）毒物棒。\n3.  **结构启发式固定：** 例如，为了便于分析，通常采用对称布局。\n\n**核心瓶颈：** 这种“在盒子里搜索”的模式，根本无法发现**盒子之外**的、可能更优的设计拓扑。它限制了探索的边界，将创新的可能性禁锢在人类先验知识的框架内。GA等算法的“局部最优”陷阱，正是这种限制的体现。\n\n**思考方向：** 能否跳出“搜索”的框架，转向一种能**直接创造**全新设计的“生成”模式？\n\n---\n\n### **第三步：范式转移——从“搜索”到“生成”的类比启发**\n\n**跨界联想：** 作者将目光投向了其他科学领域，寻找解决复杂组合问题的成功范式。\n\n1.  **AlphaFold的启示（核心类比）：** 蛋白质结构预测曾是一个巨大难题。AlphaFold的革命性在于，它没有去模拟折叠过程，而是将问题**重新表述**为“从氨基酸序列（一维语言）预测三维结构”。它证明了复杂的物理结构可以被编码为一种“语言”，其内在的“语法”可以被模型学习。\n\n2.  **AlphaZero的启示：** 在围棋等游戏中，AI通过自我对弈，发现了超越人类数千年经验的策略，证明了AI可以在巨大组合空间中进行**无偏见的探索**。\n\n3.  **材料与药物发现的启示：** 生成式模型在逆向设计中表现出色，能够根据期望性能直接生成分子或材料结构，而非反复试错。\n\n**核心假设形成：** 反应堆堆芯的二维栅格布局，是否也像蛋白质序列一样，存在一种**隐藏的“物理语法”**？燃料组件的空间排列，是否构成了一种决定其中子学性能的“语言”？\n\n**思考方向：** 如果这个假设成立，那么反应堆设计问题就可以从“在固定空间内搜索最优解”，**转化为“学习物理语法并生成符合性能要求的序列”**。\n\n---\n\n### **第四步：方法论构建——将“物理语法”思想工程化**\n\n**任务分解：** 如何验证上述假设并构建一个可行的系统？\n\n1.  **问题重述：** 第一步是让语言模型能“读懂”反应堆。作者将17x17的二维栅格**序列化**，通过光栅扫描将其转换为一维的符号序列（如 'f' 代表燃料，'g' 代表Gd棒）。这成功地将一个物理设计问题，转化为了一个标准的序列建模问题。\n\n2.  **模型选择：** 选择基于Transformer架构的语言模型（如Gemma）。其**自注意力机制**天然适合捕捉反应堆中长距离的中子相互作用（一个角落的Gd棒会影响远处的功率分布），这比传统卷积网络更具物理合理性。\n\n3.  **训练策略设计（课程学习）：** 直接学习高精度物理映射数据量太大且昂贵。作者设计了一个三阶段的“课程”，让模型逐步深化理解：\n    *   **阶段一（全量微调）：** 用大量**低精度**的蒙特卡洛模拟数据，让模型广泛学习设计空间的**几何语法**——什么是有效的布局，基本的组合规则是什么。目标是“广度”。\n    *   **阶段二（参数高效微调）：** 用少量**高精度**数据，对模型进行精调，使其将序列模式与**精确的物理结果**（如有效增殖因子keff，功率峰因子Fq）关联起来。目标是“深度”。\n    *   **阶段三（直接偏好优化DPO）：** 这是实现“对齐”的关键。模型生成两个候选设计，由高精度模拟器（OpenMC）评判优劣，然后通过DPO算法直接告诉模型“这个比那个好”。这形成了一个**物理反馈闭环**，驱使模型生成更符合安全与性能目标的设计。\n\n---\n\n### **第五步：验证与惊喜——“涌现”现象的发现**\n\n**预期结果：** 与传统GA相比，这种生成式方法应该更快、更高效。\n\n**意外发现（核心贡献）：** 在DPO阶段，作者观察到了一个惊人的“涌现”现象：\n*   **设计空间的自发扩展：** 模型在训练数据中**只见过**含有16根Gd棒的布局。但在DPO的物理目标驱动下，为了满足苛刻的功率约束，模型**自主地开始生成含有29根Gd棒**的布局。\n*   **对称性启发式的打破：** 模型还发现了高性能的**非对称**布局，挑战了工程界长期信奉的“对称即最优”的经验法则。\n\n**结论升华：** 这不是简单的模式模仿。模型已经**内化**了“增加Gd棒可以抑制反应性”这一**因果物理规律**。当它发现现有“工具”（16根Gd棒）无法完成任务时，它没有放弃，而是“创造”了新工具（增加Gd棒数量）。这证明了语言模型能够超越人类预设的约束，展现出类似**物理直觉**的推理能力。\n\n**最终思考闭环：** 从“设计效率低”的宏观问题出发，通过洞察传统方法的“盒子”限制，借鉴跨领域的“生成式”范式，构建了一套将反应堆设计“语言化”的工程方法，最终在验证中发现了模型“涌现物理推理”的惊人能力，从而证明了这一新范式的巨大潜力。这不仅是一个更优的算法，更是一个全新的发现引擎。"
                },
                {
                    "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM",
                    "arxiv_id": "2512.15784",
                    "authors": "Zibin Liu, Cheng Zhang, Xi Zhao, Yunfei Feng, Bingyu Bai, Dahu Feng, Erhu Feng, Yubin Xia, Haibo Chen",
                    "summary": "Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency. To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors. Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接命中了您设定的“自我演化”方向。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将现有智能体作为工具应用，而是**提出了一种全新的、名为MOBIMEM的智能体系统架构**。其核心目标是解决现有智能体“无法在部署后自我演化”的关键问题。论文明确指出，其贡献在于“enable iterative self-evolution without model retraining”（在不重新训练模型的情况下实现迭代的自我演化）。这完全符合您筛选标准中“构建、改进或演化LLM智能体的方法论或新框架”的要求。 2.  **第二步：正面指标** - 论文包含了大量您关注的核心指标： - **核心范式**: `Self-Evolving` (标题和摘要中反复出现), `LLM-based Agents`。 - **智能体能力**: `Memory` (论文的核心，提出了三种专门的记忆原语), `Planning` (通过Experience Memory实现新任务的执行逻辑实例化，这属于规划范畴)。 - **演化机制**: `Self-Improvement` (通过记忆系统实现个性化、能力和效率的提升), `Iterative Improvement` (摘要中明确提出)。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐，也未将多模态或视觉作为研究核心。它是在移动和桌面环境中进行操作，但视觉/多模态并非其创新点，因此不触犯排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这是本案例的关键。论文虽然将MOBIMEM应用在了“AndroidWorld and top-50 apps”这一特定领域，但根据您的筛选规则：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留。” 这篇论文正是如此。它的核心价值在于MOBIMEM这个**自我演化机制本身**，而不是它在移动应用上的表现数据。应用场景仅是验证该机制有效性的试验场。 **最终决策**: 综合以上分析，这篇论文的核心贡献是构建了一个新颖的、以记忆为中心的智能体框架（MOBIMEM），旨在实现LLM智能体在部署后的自我演化和能力提升。这精准地契合了您研究课题中的“自我演化”方向。尽管它有具体的应用场景，但其根本创新在于智能体的演化机制，而非应用本身。因此，这篇论文应被**保留**。",
                    "summary2": "本文旨在解决AI智能体部署后依赖模型重训练才能演进的高成本问题。针对移动和桌面环境中的复杂工作流自动化场景，我们提出了一种以内存为中心的智能体系统MOBIMEM，它通过Profile Memory、Experience Memory和Action Memory三种专用内存原语，实现了无需模型训练的自演进。在AndroidWorld基准和真实世界的Top-50移动应用上，通过profile alignment (83.1%)、task success rate (提升50.3%)和end-to-end latency (降低9×)等指标验证了其有效性。",
                    "summary_translation": "大型语言模型 (LLM) 代理正日益广泛地部署于移动和桌面环境中，以自动化复杂的工作流。然而，当前以模型为中心的代理架构难以在部署后实现自我进化：提升个性化、能力和效率通常需要持续的模型重训练/微调，这不仅会带来高昂的计算开销，还面临着模型准确性与推理效率之间的固有权衡。为实现无需模型重训练的迭代式自我进化，我们提出了 MOBIMEM，一个以内存为中心的代理系统。MOBIMEM 首先引入了三种专用的内存原语，以将代理的进化与模型权重解耦：(1) Profile Memory (用户画像内存) 采用一种轻量级的距离图 结构来与用户偏好对齐，解决了用户画像检索中的准确率-延迟权衡问题；(2) Experience Memory (经验内存) 利用多级模板为新任务实例化执行逻辑，确保了能力的泛化；(3) Action Memory (动作内存) 记录细粒度的交互序列，减少了对昂贵模型推理的依赖。在此内存架构的基础上，MOBIMEM 进一步集成了一套受操作系统启发的服务来编排执行：一个用于协调并行子任务执行和内存操作的调度器；一个支持安全高效动作复用的代理记录与重放 机制；以及一个确保从用户中断和运行时错误中优雅恢复的上下文感知异常处理机制。在 AndroidWorld 和热门前50应用上的评估表明，MOBIMEM 实现了 83.1% 的用户画像对齐度，检索时间为 23.83 毫秒（比 GraphRAG 基线快 280 倍），将任务成功率提升了高达 50.3%，并在移动设备上将端到端延迟降低了高达 9 倍。",
                    "inspiration_trace": "### 作者思考过程的逻辑链推演\n\n#### 1. **宏观问题：AI代理部署后的进化瓶颈**\n   - **观察**：AI代理（如LLM代理）在移动和桌面环境中自动化复杂工作流（如跨应用任务调度），但部署后难以持续进化。个性化（适应用户偏好）、能力扩展（处理新任务）和效率提升（减少延迟）通常依赖模型重新训练或微调。\n   - **痛点**：训练开销巨大（云端或边缘设备均不可行），且模型规模与推理效率存在固有权衡（大模型准确但慢，小模型快但弱）。现有代理系统是“模型中心”的，进化必须修改模型权重，导致部署后停滞。\n   - **核心问题**：如何让代理在部署后自我进化，而不依赖训练？\n\n#### 2. **关键假设：内存作为进化载体**\n   - **灵感来源**：从操作系统（OS）机制（如内存管理、进程调度）中汲取灵感。OS通过内存抽象（而非修改内核）实现系统进化，代理是否可类似？\n   - **假设**：如果将进化逻辑从模型权重解耦，转移到外部内存系统，代理可通过内存更新实现个性化、能力提升和效率优化，避免训练开销。\n   - **验证假设的可行性**：现有内存技术（如RAG、GraphRAG）用于知识存储，但存在权衡（RAG快但准度低，GraphRAG准但慢）。需设计代理专用内存，兼顾效率与准确性。\n\n#### 3. **聚焦设计：内存原语的解耦与优化**\n   - **分解进化目标**：将自我进化拆解为三个维度：\n     - **个性化**：需持续学习用户偏好（如购物习惯）。\n     - **能力扩展**：需泛化到新任务（如跨应用操作）。\n     - **效率提升**：需减少冗余推理（如重复动作）。\n   - **设计内存原语**：针对每个维度，设计专用内存结构，将语义信息从模型移至内存：\n     - **Profile Memory**：解决用户偏好检索的权衡。传统RAG（向量搜索）快但忽略关系；GraphRAG（图结构）准但需LLM推理。提出DisGraph：将语义信息从边移至节点（概念-实体图），边仅表距离。检索时用向量搜索定位起点，再BFS遍历，无需LLM，实现高准低延迟。\n     - **Experience Memory**：解决能力泛化问题。现有方法存储原始轨迹，泛化差。提出多级模板：抽象任务为不变控制流（模板）和可变参数。新任务时，匹配模板并实例化参数，减少推理负担。\n     - **Action Memory**：解决效率瓶颈。代理重复执行相同任务时，仍需LLM推理。提出动作缓存：ActTree（前缀复用）和ActChain（前缀-后缀复用），直接重用动作序列，减少模型调用。\n\n#### 4. **系统整合：OS启发的服务层**\n   - **问题**：内存操作需协调，否则可能引入新开销（如更新延迟）。\n   - **假设**：OS服务机制（如调度、异常处理）可适配代理场景，确保内存高效运作。\n   - **设计服务层**：\n     - **调度器**：协调子任务并行执行（如跨应用数据传递），利用内存依赖图优化吞吐。\n     - **记录重放（AgentRR）**：安全复用动作，处理缓存失效（如UI更新）。\n     - **异常处理**：捕获用户中断，将处理逻辑存入内存，避免重复错误。\n   - **整合逻辑**：内存层提供数据，服务层管理执行，实现“内存中心”架构。\n\n#### 5. **验证与迭代：实验驱动优化**\n   - **初始验证**：在AndroidWorld和真实应用测试，DisGraph检索快280倍（vs. GraphRAG），模板提升任务成功率50.3%，动作缓存减延迟9倍。\n   - **迭代优化**：\n     - Profile Memory：动态拆分概念节点（如“旅行”分“商务/休闲”），防检索稀释。\n     - Experience Memory：自动生成模板（LLM聚类），减少人工成本。\n     - Action Memory：添加验证机制（UI元素匹配），处理环境变化。\n   - **系统反馈**：实验显示内存设计有效，但需服务层支持（如调度器利用并行性），最终形成三层架构（代理层、内存层、OS集成层）。\n\n#### 6. **最终方法论：内存中心的自进化范式**\n   - **思想演进总结**：从“模型训练进化”到“内存驱动进化”，通过解耦内存与模型，实现轻量、持续的自我优化。\n   - **核心贡献**：MOBIMEM证明内存可作为代理进化的“操作系统”，无需训练即可提升个性化、能力和效率，为边缘部署提供实用路径。\n\n此逻辑链从宏观问题出发，通过观察-假设-设计-验证的闭环，逐步聚焦到内存原语和系统服务，最终形成方法论。作者始终围绕“避免训练开销”的核心，以OS灵感为桥梁，将抽象问题转化为可操作的内存架构。"
                },
                {
                    "title": "Meta-RL Induces Exploration in Language Agents",
                    "arxiv_id": "2512.16848",
                    "authors": "Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic",
                    "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献直接聚焦于“LLM智能体及其演化”中的“自我演化”和“单智能体”方向。 **判断过程如下:** 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将现有智能体作为工具去解决某个特定领域的问题，而是提出了一种名为 `LaMer` 的**新框架**。这个框架的核心目标是**改进LLM智能体本身的能力**，特别是它们在未知环境中主动探索和适应的能力。这完全符合“构建、改进或演化LLM智能体”的核心要求。 2.  **第二步：正面指标** - 论文包含了大量核心关注点： - **核心范式**: `LLM-based Agents` (标题和摘要中明确提及)。 - **自我演化**: 这是论文最核心的贡献。`Meta-RL` (元强化学习) 本身就是一种“学习如何学习”的范式，与自我演化高度相关。摘要中提到的 `learn from the environment feedback`、`adapt their policy`、`better generalization to...unseen tasks` 都是自我演化的典型特征。 - **智能体能力**: `Self-Reflection` (摘要中明确提到 `in-context policy adaptation via reflection`)，`Planning` (主动探索和解决长视野任务必然涉及规划)。 - 这些正面指标非常强烈，表明论文与您的研究焦点高度相关。 3.  **第三步：排除标准** - 论文的主要贡献并非关于安全、对齐、可解释性或多模态。它的焦点是智能体的性能和适应能力，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化应用”规则下的一个完美**保留**案例。虽然它在Sokoban、MineSweeper等具体环境中进行实验，但其**核心贡献是提出了一种通用的“自我演化”机制（Meta-RL框架）**，而不是仅仅应用智能体去解决扫雷问题。这个机制本身是可迁移的，是研究的主体。 - **推理/规划**: 论文关注的是智能体在复杂、多轮任务中如何通过**主动探索**来学习和适应，这属于智能体层面的规划和推理，而不是改进LLM底层的数学或逻辑推理能力。因此，符合保留条件。 **最终决策:** 这篇论文的核心贡献是提出了一种基于元强化学习（Meta-RL）的新框架 `LaMer`，旨在**提升LLM智能体的主动探索能力和在测试时的自适应能力**。其关键技术点，如跨回合训练和通过反思进行上下文策略调整，直接对应了您研究目标中的**“自我演化”**和**“单智能体”**（特别是自我反思与规划）方向。它不是简单的应用，而是对智能体核心能力的根本性改进，因此是您课题下非常前沿且高度相关的一篇论文。",
                    "summary2": "本文旨在解决RL训练的语言智能体在需要主动探索的多轮任务中适应性差的问题。针对多轮、长视野的交互式任务环境，我们提出了一种名为LAMER的Meta-RL框架，其核心是通过跨episode训练和基于反思的上下文内策略适应来诱导探索。在Sokoban、MineSweeper、Webshop等多样化环境上，通过任务成功率等指标验证了其有效性，相比RL基线取得了显著性能提升。",
                    "summary_translation": "强化学习 (RL) (Reinforcement learning) 已能够用于训练大语言模型 (LLM) (large language model) 智能体，使其与环境交互并解决多回合长视野任务。然而，通过强化学习训练的智能体在需要主动探索的任务中常常表现不佳，且难以从试错经验中高效地进行适应。在本文中，我们提出了 LaMer，一个通用的元强化学习 框架，该框架使大语言模型智能体能够在测试时主动探索并从环境反馈中学习。LaMer 由两个关键组件构成： 一个跨回合训练框架，旨在鼓励探索和优化长期奖励；以及 通过反思实现的上下文内策略适应，使智能体能够根据任务反馈信号调整其策略，而无需进行梯度更新。在多种环境下的实验表明，LaMer 的性能显著优于强化学习基线模型，在 Sokoban、MineSweeper 和 Webshop 任务上分别取得了 11%、14% 和 19% 的性能提升。此外，与通过强化学习训练的智能体相比，LaMer 在更具挑战性或先前未见过的任务上也展现出更强的泛化能力。总而言之，我们的研究结果表明，元强化学习为在语言智能体中诱导探索提供了一种合理的方法，能够通过学到的探索策略，使其在面对新环境时实现更稳健的适应。",
                    "inspiration_trace": "### 作者思考过程的逻辑链还原\n\n以下基于论文内容，系统性地推演作者提出核心方法（LAMER）的思考过程。逻辑链从宏观问题出发，逐步聚焦到方法论，突出思想演进而非实现细节。语言简洁，以“问题→观察→假设→方法→验证”为主线。\n\n---\n\n#### **1. 宏观问题：LLM代理缺乏探索能力**\n- **起点**：作者观察到，大型语言模型（LLM）作为代理在多回合任务（如游戏、网页交互）中表现不佳，尤其在需要主动探索的场景（如未知环境、稀疏奖励任务）。人类能通过试错快速适应，但LLM代理常陷入次优策略，无法有效收集信息。\n- **核心矛盾**：现有方法（如标准强化学习，RL）训练的代理在单回合内优化即时奖励，忽视了探索的长期价值，导致在复杂任务中失败率高。\n\n#### **2. 关键观察：现有方法的局限性**\n- **RL训练的缺陷**：作者分析发现，标准RL（如PPO、RLOO）在LLM代理上存在两个问题：\n  - **探索不足**：RL目标函数（单回合回报）鼓励即时利用，代理倾向于重复已知行为，避免不确定动作（如MineSweeper中不敢点击未知格子）。\n  - **适应缓慢**：代理在测试时无法从反馈中快速调整策略，依赖固定策略，无法像人类一样“边做边学”。\n- **替代方案的不足**：作者回顾相关工作（如Reflexion、离线策略蒸馏），发现它们要么限于单回合推理，要么依赖静态数据，无法实现“主动探索→实时适应”的闭环。\n\n#### **3. 核心假设：Meta-RL可诱导探索**\n- **灵感来源**：作者转向元强化学习（Meta-RL），其核心是“学习如何学习”。Meta-RL在传统RL中已证明能促进探索（如通过跨任务训练学习探索策略）。\n- **关键假设**：如果将Meta-RL应用于LLM代理，代理可内化探索算法：\n  - **跨episode结构**：将任务分解为多个episode（尝试），早期episode鼓励探索（收集信息），后期episode利用信息优化决策。\n  - **上下文适应**：利用LLM的上下文学习能力，代理无需梯度更新即可从反馈中调整策略（类似人类反思）。\n- **理论支撑**：Meta-RL的长期回报目标（如跨episode折扣奖励）能平衡探索与利用，解决RL的短视问题。\n\n#### **4. 方法形成：LAMER框架的设计**\n- **设计原则**：基于假设，作者提出LAMER（LLM Agent with Meta-RL），聚焦两个创新点：\n  - **跨episode训练框架**：\n    - 将任务建模为episode序列（如多次尝试同一游戏），最大化跨episode折扣回报（公式中的γ_traj）。\n    - 早期episode奖励探索行为（如多样化动作），后期episode奖励利用（如基于反馈优化策略）。\n    - 这迫使代理学习通用探索策略，而非任务特定解法。\n  - **上下文策略适应（反思机制）**：\n    - 每个episode后，代理生成文本反思（总结失败原因、改进计划），并注入上下文指导下一episode。\n    - 反思作为“内部RL算法”，实现零梯度适应，契合LLM的上下文学习优势。\n- **简化与聚焦**：作者避免复杂设计（如梯度更新），选择反思机制因其轻量、可解释，且与LLM生成能力天然兼容。\n\n#### **5. 验证与优化：实验驱动迭代**\n- **初步验证**：作者在多个环境（Sokoban、MineSweeper等）测试LAMER：\n  - **性能提升**：LAMER显著优于RL基线（如MineSweeper上+19%），证明Meta-RL诱导探索有效。\n  - **行为分析**：轨迹多样性指标显示，LAMER保留更多探索行为（图1），而RL代理过早收敛。\n- **关键洞见**：\n  - **测试时扩展性**：LAMER在多次尝试中性能递增（如pass@1到pass@3），表明代理学会“先探索后利用”。\n  - **泛化能力**：在更难任务（如增加Sokoban箱子数）和OOD任务（如ALFWorld新类别）上，LAMER优于RL，验证了探索策略的通用性。\n- **优化调整**：通过消融实验（如γ_traj的影响、反思内容配置），作者精炼框架：\n  - 反思仅比轨迹+反思更有效，因信息更聚焦。\n  - γ_traj需任务特定调整（如MineSweeper需高γ_traj鼓励长期探索）。\n\n#### **6. 结论：思想演进总结**\n- **逻辑闭环**：从“探索不足”问题出发，通过Meta-RL假设，LAMER将探索内化为可学习策略，实现代理的自主适应。\n- **核心贡献**：Meta-RL为LLM代理提供原则性探索框架，推动更鲁棒的决策系统。\n- **未来延伸**：作者指出，该思想可结合其他RL算法或推理模型，但需解决训练效率问题（如episode依赖导致的并行化瓶颈）。\n\n此思考过程体现了“问题驱动→理论迁移→方法简化→实证验证”的演进，作者始终聚焦探索本质，而非技术细节，确保LAMER的通用性和可扩展性。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 3,
            "papers": [
                {
                    "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying",
                    "arxiv_id": "2512.15776",
                    "authors": "Shaun Baek, Sam Liu, Joseph Ukpong",
                    "summary": "Large Language Models (LLMs) act as powerful reasoning engines but struggle with \"symbol grounding\" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or \"Curse of Knowledge\"), where a knowledgeable \"Leader\" agent fails to guide a sensor-limited \"Follower\" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant \"Success Gap\": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a \"Pull-based\" protocol (active querying) is significantly more robust than standard \"Push-based\" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合你的研究范围，核心贡献属于“多智能体”方向。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心并非将LLM或智能体框架作为工具去解决一个外部领域问题（如机器人控制），而是**研究并改进多智能体系统本身**。它提出了一个新颖的“非对称辅助推理框架”和一种“Pull-based”通信协议，旨在解决多智能体协作中的一个根本性问题——“特权信息偏差”。这直接对应了筛选标准中的“构建、改进或演化 LLM智能体”，特别是多智能体系统。 2.  **第二步：正面指标——高度匹配** 论文包含了多个核心关注点： *   **核心范式**: `LLM-based Agents`, `Multi-Agent Systems (MAS)`。论文明确研究了一个由“Leader”和“Follower”组成的多智能体系统。 *   **多智能体**: `Collaboration`, `Communication`。论文的核心贡献就是提出一种新的通信机制（主动查询/Pull-based protocol）来提升智能体间的协作效率。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 论文摘要中提到了“safe human-AI and robot-robot collaboration”，但这是作为其研究成果的一个潜在应用和意义，而非论文的主要贡献。论文的核心是**通信机制**，而不是安全或对齐算法本身。因此，这不构成排除理由。 *   **多模态与视觉**: 论文提到了“embodied agents”和“sensor-limited Follower”，这暗示了视觉等感知模块的存在。然而，这些感知能力是智能体与环境交互的工具，研究的核心并非改进视觉模型，而是**智能体如何处理和沟通由感知差异带来的信息不对称**。这完全符合“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外情况。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文中的推理是发生在多智能体协作框架下的（Asymmetric Assistive Reasoning），而不是为了提升LLM本身的基础推理能力。因此，它属于应保留的Agentic框架研究。 **总结**: 该论文的核心贡献是提出了一种新的多智能体协作框架和通信协议，以解决信息不对称环境下的协作失败问题。这直接命中了你研究课题中的“多智能体”方向，特别是关于智能体间的“协作”与“通信”子方向。它不是一篇应用论文，而是一篇对Agentic AI基础机制进行深入探索和改进的前沿研究，因此应被保留。",
                    "summary2": "本文旨在解决具身智能体在信息不对称协作中的“Privileged Information Bias”问题。针对一个拥有全局视野的Leader和一个视野受限的Follower在AI2-THOR环境中进行目标导航的场景，我们提出了一种“Pull-based”主动查询协议，使Follower能主动请求澄清以解决信息不对称。在AI2-THOR模拟环境中，通过Success Rate (SR)等指标验证了该方法能显著缩小“Success Gap”，有效提升了协作成功率。",
                    "summary_translation": "大语言模型 (LLMs) 作为强大的推理引擎，但在具身环境中面临“符号接地”的挑战，尤其在信息呈非对称分布的情况下。我们研究了特权信息偏见，即“知识的诅咒”，该现象指掌握更多信息的“领导者”智能体因缺乏心智理论，而无法有效引导传感器受限的“追随者”智能体。为量化此现象，我们在AI2-THOR模拟环境中提出了一个新颖的非对称辅助推理框架。实验结果揭示了一个显著的“成功差距”：尽管“领导者”在35.0%的回合中成功感知到目标，但协作团队的整体成功率仅为17.0%。这意味着近50%原本可行的计划，其失败完全是由于沟通接地错误所致。我们的研究表明，“拉取式”协议（即主动查询）比标准的“推送式”指令具有更强的鲁棒性；在成功的协作回合中，澄清请求的出现频率是后者的两倍。本研究阐明了主动不确定性降低这一机制，并指出其是实现安全人机协作与机器人-机器人协作的必要前提。",
                    "inspiration_trace": "好的，以下是基于您提供的论文内容，对作者核心方法提出过程的逻辑链推演。\n\n---\n\n### **作者核心方法产出逻辑链推演**\n\n#### **第一步：宏观观察 —— LLM的“具身鸿沟”**\n\n*   **起点：** 作者观察到当前大型语言模型（LLMs）的一个根本性局限。尽管LLMs是强大的“推理引擎”，但它们本质上是“缸中之脑”，缺乏与物理世界的直接交互。这种“符号接地”问题意味着它们能理解“厨房”这个词，却无法真正推理在真实厨房中“谁能看到冰箱”这种具身情境。\n*   **思考演进：** 这个问题不仅是物理层面的，更是**社会层面**的。当一个“无所不知”的智能体需要与一个“感知受限”的伙伴协作时，这种“脱身”状态会导致灾难性的沟通失败。\n\n#### **第二步：问题聚焦 —— 从“具身”到“非对称协作”**\n\n*   **切入点：** 作者将目光从单个智能体的具身问题，转向了**多智能体协作**中的具体挑战。他们注意到，现有研究大多假设智能体是同质的，拥有共享的感知能力，这忽略了现实世界中普遍存在的**信息不对称**。\n*   **核心矛盾：** 真正的挑战并非简单的任务分配，而是当两个智能体的“现实”不一致时，它们如何**协商和弥合认知鸿沟**。这成为了研究的核心靶点。\n\n#### **第三步：现象识别 —— “知识诅咒”作为核心障碍**\n\n*   **命名与定义：** 作者将这种沟通失败的现象，精准地与心理学中的“知识诅咒”或“特权信息偏见”联系起来。即，知识渊博的“领导者”无法模拟知识匮乏的“追随者”的心理状态，错误地假设对方拥有和自己一样的全局视野。\n*   **形成假设：** 作者的核心假设是：**协作失败的主要原因，不是任务执行能力的不足，而是这种由“知识诅咒”导致的沟通接地错误。**\n\n#### **第四步：实验设计 —— 构建一个“成功差距”的放大器**\n\n*   **目标：** 为了验证上述假设，作者需要一个能够**隔离并量化**“知识诅咒”影响的实验环境。\n*   **设计思路：** 他们构建了一个“非对称辅助推理”框架：\n    1.  **领导者：** 拥有全局视野，负责规划。\n    2.  **追随者：** 视野严重受限（如2米内），负责执行。\n    3.  **任务：** 在AI2-THOR模拟器中进行目标导航。\n*   **关键指标：** 作者提出了一个创新的量化指标——**“成功差距”**。即，领导者单独完成任务的成功率与“领导者-追随者”团队协作成功率的差值。这个差距直接衡量了纯粹由沟通问题导致的失败比例。\n\n#### **第五步：机制假设 —— “推式”与“拉式”的二元对立**\n\n*   **分析失败根源：** 作者推断，“知识诅咒”的根源在于一种**单向的、开环的沟通模式**。领导者基于自身视角“推送”指令，而追随者则被动接收和执行。作者称之为“推式”协议。\n*   **提出解决方案：** 与之相对，作者假设一种**双向的、闭环的沟通模式**会更有效。即，当追随者遇到无法理解的指令时，应主动“拉取”信息，向领导者发出澄清请求。作者称之为“拉式”协议。\n*   **核心预测：** “拉式”协议通过引入**主动不确定性降低**机制，能有效克服“知识诅咒”，从而缩小“成功差距”。\n\n#### **第六步：方法论形成 —— “Pull-based”主动查询框架**\n\n*   **整合与验证：** 最终，作者将上述思考整合为一个完整的方法论。他们通过实验对比了“推式”与“拉式”两种协议。\n*   **实验验证：** 结果完美印证了他们的逻辑链：\n    *   **“成功差距”被证实：** 领导者成功率35%，团队仅17%，证明近50%的可行计划毁于沟通。\n    *   **“拉式”机制被证实：** 成功的案例中，追随者的主动查询频率是失败案例的**2倍**。这表明，成功的关键不在于领导者说得多，而在于追随者问得对。\n*   **最终结论：** 作者得出结论，**主动查询**是解决非对称协作中“知识诅咒”的关键机制。它迫使领导者进行“视角切换”，将全局知识转化为追随者可以验证的局部指令，从而实现了有效的协作。\n\n---\n\n**总结：** 作者的思考过程是一个从宏观到微观，从现象到机制的严谨推演。他们始于对LLM本质的深刻洞察，聚焦于非对称协作这一具体场景，精准地识别出“知识诅咒”这一核心障碍，并通过巧妙的实验设计将其量化。最终，他们提出的“拉式”主动查询方法，并非凭空想象，而是基于对失败根源（开环指令）的深刻分析，从而推导出的、具有明确因果逻辑的解决方案。"
                },
                {
                    "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
                    "arxiv_id": "2512.16250",
                    "authors": "Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli",
                    "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合你的研究范围，核心判断依据如下： 1.  **第一步：核心判断 (保留)** 论文的核心贡献并非简单地将LLM应用于一个新领域，而是提出了两个关键组件： *   **AMUSE**: 一个专门为评估“智能体推理”能力而设计的基准。它明确要求模型将复杂的视听交互分解为**规划、定位和反思**步骤，这直接命中了你对单智能体能力（规划、自我反思）的研究焦点。 *   **RAFT**: 一个“智能体对齐框架”。这个框架通过奖励优化和内在的**多模态自我评估**来提升模型在AMUSE基准上的表现。这本质上是一个**改进LLM智能体**的方法论，其核心在于让智能体通过自我评估来完善自身，这与你的“自我演化”方向高度相关。 因此，论文的本质是构建和改进LLM智能体，应予以保留。 2.  **第二步：正面指标 (高度匹配)** 论文中包含了大量你的核心关注点： *   **核心范式**: `Agentic AI` (多次提及), `LLM-based Agents` (隐含在MLLMs的agentic reasoning中)。 *   **智能体能力**: `Planning`, `Self-Reflection`, `Self-Evaluation` (自我评估是自我反思/修正的一种形式)。 *   **演化机制**: `Self-Improvement` (RAFT框架的目标), `Iterative Improvement` (通过奖励优化实现)。 3.  **第三步：排除标准 (不适用)** *   **安全与对齐**: 论文中的 \"Alignment Framework\" 指的是让模型的行为与“智能体任务的要求”对齐，即提升其在规划、反思等任务上的表现，而非通常意义上的AI安全、伦理对齐。因此，这不属于排除范围。 *   **多模态与视觉**: 论文确实涉及多模态，但完全符合你的特殊规则。这里的音频和视觉是智能体感知和交互的**环境**，而不是研究的核心。论文的核心贡献是**如何让智能体在这种多模态环境中进行推理和自我完善**，而不是提出新的视觉或音频模型。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文明确聚焦于智能体如何在复杂任务中进行多步推理和规划，这正是你希望保留的类型，而非提升LLM基础数学或逻辑能力。 *   **自我演化的应用**: 虽然论文的应用场景是多说话人理解，但其核心贡献RAFT是一种新的“自我演化/改进”机制（通过自我评估作为奖励进行优化），因此即使应用在特定领域，也应保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于提出了一套用于评估和改进LLM智能体在复杂任务中规划、反思和自我完善能力的基准与框架。它直接触及了你研究课题中的“单智能体”和“自我演化”两个核心方向，因此应被**保留**。",
                    "summary2": "本文旨在解决现有MLLM在多说话人对话场景中代理推理能力不足的问题。针对多说话人音频-视频场景，我们提出了一种RAFT对齐框架，它结合了自反思奖励优化（RRO）和选择性推理适应（SRA）。我们在提出的AMUSE benchmark上通过BLEU、Accuracy等指标验证了其有效性，实现了最高39.52%的相对准确率提升。",
                    "summary_translation": "近期的多模态大语言模型，如GPT-4o和Qwen3-Omni，展现出强大的感知能力，但在需要agentic reasoning (智能体推理) 的多说话人、以对话为中心的场景中却面临挑战。这些场景要求模型能够追踪说话人身份、维持对话角色，并随时间推移对事件进行关联 (grounding events across time)。这些场景是多模态音视频理解的核心所在，在对话视频助手和会议分析等应用中，模型必须对音频流和视觉流进行联合推理。\n\n我们提出了AMUSE，这是一个围绕具有内在agentic (智能体) 特性的任务而设计的基准测试。它要求模型将复杂的视听交互分解为规划、关联和反思三个步骤。该基准从三种模式——zero-shot (零样本)、guided (引导式) 和 agentic (智能体式)——以及六个任务类别（包括时空说话人关联和多模态对话摘要）对MLLMs进行评估。在所有评估模式下，现有模型均表现出薄弱的多说话人推理能力，并且在非智能体式和智能体式评估中均表现出不一致的行为。\n\n鉴于这些任务固有的agentic (智能体) 特性以及LLM agents (大语言模型智能体) 的最新进展，我们提出了RAFT。这是一个数据高效的agentic alignment framework (智能体对齐框架)，它将reward optimization (奖励优化) 与作为奖励的intrinsic multimodal self-evaluation (内在多模态自评估) 相结合，并采用selective parameter adaptation (选择性参数适应) 策略，以实现数据和参数的高效更新。应用RAFT框架后，我们在该基准测试上实现了高达39.52%的相对准确率提升。\n\nAMUSE和RAFT共同为检验多模态模型中的agentic reasoning (智能体推理) 并提升其能力提供了一个实用的平台。",
                    "inspiration_trace": "好的，我们来系统性地推演作者产出《AMUSE》这篇论文的思考过程。这个逻辑链可以概括为：**从一个宏观观察出发，定义一个被忽视的关键问题，通过构建评估工具来验证问题的严重性，并最终基于验证结果提出针对性的解决方案。**\n\n---\n\n### **第一步：宏观观察与问题定位**\n\n**起点：** 作者们观察到，尽管多模态大语言模型（MLLMs）如GPT-4o在通用感知能力上取得了巨大成功，但它们在处理真实世界中最常见的交互场景——**多人对话**时，表现并不理想。\n\n**思考过程：**\n1.  **现象识别：** 在会议、访谈、家庭聚会等场景中，模型无法可靠地回答“谁在什么时候说了什么？”、“接下来谁会发言？”、“A刚才的反应和B之前的发言有什么关系？”这类问题。这不仅仅是识别错误，而是缺乏对对话动态、角色和因果关系的持续理解。\n2.  **本质抽象：** 作者将这种能力缺失归结为**“智能体多说话人理解”**的不足。这里的“智能体”特指模型需要具备**规划、行动和反思**的能力，以应对对话的动态性和复杂性，而不是被动地进行单轮问答。\n3.  **问题聚焦：** 核心问题被清晰地定义为：**当前MLLMs缺乏在多说话人、时序相关的对话中进行结构化推理的能力。** 这是一种更高层次的、面向社交交互的智能，而不仅仅是感知。\n\n---\n\n### **第二步：假设提出与验证工具的构建**\n\n**假设：** 作者们提出一个核心假设：**现有模型的这一缺陷之所以未被系统性地解决，是因为缺乏一个能够精确评估和量化这种“智能体多说话人推理”能力的基准。** 如果无法衡量，就无法有效改进。\n\n**思考过程：**\n1.  **评估现有工具的不足：** 他们审视了MMBench、Video-ChatGPT等主流基准，发现它们要么关注静态感知，要么假设单一叙述者，完全忽略了对话中的**话轮转换、角色追踪和跨时序依赖**。评估工具与真实需求之间存在巨大鸿沟。\n2.  **构建新基准的构想：** 为了验证假设，必须先打造一把“尺子”。这把尺子——即AMUSE基准——必须具备以下特性：\n    *   **任务设计必须“天生智能体化”：** 任务不能是简单的识别，而必须迫使模型进行多步推理。例如，“总结对话”需要先识别说话人再整合内容；“预测下一位发言者”需要理解对话流和社交线索。\n    *   **评估模式必须分层递进：** 为了精确定位模型的弱点，他们设计了三种评估模式：\n        *   **零样本：** 测试模型的原始、内在能力。\n        *   **引导式：** 提供中间线索（如转录文本），测试模型在“拐杖”下的推理上限。\n        *   **智能体式：** 让模型自主调用工具（如语音识别、人脸检测），测试其真正的自主规划和执行能力。\n3.  **基准的预期作用：** AMUSE不仅是一个排行榜，更是一个**诊断工具**。作者预期，通过它将揭示现有模型在自主性增强时性能会急剧下滑，从而证明问题的根源在于缺乏结构化的智能体推理框架。\n\n---\n\n### **第三步：实验验证与问题根源的深化**\n\n**执行：** 使用AMUSE基准对一系列主流开源和闭源MLLMs进行系统性评估。\n\n**思考过程：**\n1.  **假设得到验证：** 实验结果完美印证了作者的猜想。所有模型在“智能体式”评估下表现都远差于“引导式”，甚至在某些情况下比“零样本”还差。这清晰地表明，模型**过度依赖提示工程**，而缺乏内在的、自主的多模态推理能力。\n2.  **根源深化：** 问题不仅仅是“数据不够”或“模型不够大”。从“引导式”到“智能体式”的性能断崖式下跌，揭示了更深层次的问题：**模型缺乏一个将感知、推理和行动整合起来的对齐机制。** 它们知道“如何做”（当被告知时），但不知道“何时做”以及“为何要这样做”。\n3.  **新方向的确立：** 既然问题根源在于“对齐”，那么解决方案就不应是简单的数据扩充，而应是一个**专门针对智能体多模态推理的对齐框架**。这个框架需要教会模型如何自我规划、如何利用工具、以及如何根据多模态反馈进行自我修正。\n\n---\n\n### **第四步：解决方案的提出与框架设计**\n\n**目标：** 设计一个数据高效、模型无关的框架，来弥补上述发现的“智能体对齐”鸿沟。\n\n**思考过程：**\n1.  **核心思想：** 将智能体行为“规划-行动-反思”显式地融入到训练过程中。RAFT框架应运而生。\n2.  **组件一：如何奖励“好的反思”？**\n    *   **问题：** 传统的奖励模型（如RLHF）通常基于文本偏好，无法评估多模态感知的正确性（如时间戳是否对齐、说话人是否匹配）。\n    *   **解决方案：** 提出**反思性奖励优化（RRO）**。让模型在生成答案后，利用自身的感知能力（或工具输出）进行自我评估，形成一个**内在的、基于感知一致性的奖励信号**。这奖励的不仅是答案对不对，更是推理过程是否站得住脚。\n3.  **组件二：如何高效地训练？**\n    *   **问题：** 全模型微调成本高昂，且可能破坏原有的通用能力。\n    *   **解决方案：** 提出**选择性推理适应（SRA）**。只更新模型中负责跨模态推理的关键部分（如适配器层），而冻结基础感知模块。这既保证了数据效率，也让训练目标更聚焦于“推理”而非“感知”。\n4.  **整合与验证：** 将RRO和SRA结合，并辅以一个时间一致性损失（防止说话人身份随时间漂移），构成了完整的RAFT框架。最后，在AMUSE上用RAFT微调模型，结果显示性能显著提升，尤其是在最具挑战性的“智能体式”模式下，从而形成了从**问题发现 -> 诊断 -> 解决 -> 验证**的完整闭环。\n\n---\n\n### **总结：作者的思考逻辑链**\n\n1.  **宏观观察：** MLLMs在多人对话场景中“失智”。\n2.  **问题定义：** 缺乏“智能体多说话人推理”能力。\n3.  **核心假设：** 缺乏有效的评估基准是阻碍进步的关键。\n4.  **构建工具（AMUSE）：** 设计天生需要多步推理的任务和分层评估模式，以诊断问题。\n5.  **验证假设：** 实验证明模型在自主模式下表现糟糕，根源是“智能体对齐”缺失。\n6.  **提出方案（RAFT）：** 设计一个包含“内在反思奖励”和“选择性参数更新”的对齐框架。\n7.  **闭环验证：** RAFT在AMUSE上取得显著成功，证明了整个思路的有效性。\n\n整个过程体现了从现象到本质，从诊断到治疗的严谨学术思维，最终产出了一套“基准+方法”的组合拳，为该领域的研究提供了清晰的评估标准和可行的改进路径。"
                },
                {
                    "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
                    "arxiv_id": "2512.16167",
                    "authors": "Shiduo Yang, Jiye Wang, Jiayu Qin, Jianbin Li, Yu Wang, Yuanhe Zhao, Kenan Guo",
                    "summary": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合你的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是将LLM智能体作为工具去解决某个特定领域（如金融、医疗）的问题。它的核心贡献是提出了一种名为 **`Ev-Trust`** 的**新机制**。这个机制基于演化博弈论，旨在**引导和塑造LLM多智能体系统中的行为演化**。这直接属于“构建、改进或演化LLM智能体”的范畴，特别是针对多智能体系统的演化机制。 2.  **第二步：正面指标** - 论文摘要中包含了大量与你研究焦点高度相关的核心范式和能力： - **核心范式**: `LLM-Based Multi-Agent Services`, `Evolutionary Games`。这直接命中了你的“多智能体”和“自我演化”两个核心方向。 - **多智能体**: 论文讨论了在去中心化环境中智能体的交互，并旨在`reinforcing high-quality collaboration`（强化高质量协作）。 - **演化机制**: 论文的核心是`Ev-Trust`机制，它通过`dynamic feedback structure`（动态反馈结构）来`guides agents' behavioral evolution toward equilibria`（引导智能体行为向均衡演化）。这完全符合“自我演化”的定义，即智能体通过环境反馈进行自我完善和迭代。摘要还明确提到了`replicator dynamics equations`（复制动态方程），这是演化博弈论的核心工具。 3.  **第三步：排除标准** - **安全与对齐**: 这是本论文最需要仔细甄别的一点。虽然论文标题和摘要中提到了`Trust`（信任）、`deception`（欺骗）、`fraud`（欺诈）和`malicious participants`（恶意参与者）等看似与“安全”相关的词汇，但其**主要贡献并非安全或对齐技术本身**。论文的核心是提出一个**博弈论框架**，通过经济和演化激励来**自然地筛选掉恶意行为者**，从而实现系统层面的稳健性。它的目标是研究智能体群体如何演化，而不是为单个智能体设计安全护栏或对齐方法。因此，它不属于“主要贡献是关于Safety, Security, Alignment”的排除范围。 - **多模态与视觉**: 论文未涉及相关内容。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文并非一个应用案例，而是提出了一种通用的“自我演化”机制。它恰好完美地体现了你所说的“例外情况”：即使它被应用在“服务网络”这个特定场景，但其核心是提出一种新的自我演化机制，因此必须保留。 **最终决策**: 综合以上分析，这篇论文的核心贡献是**提出了一种基于演化博弈论的信任机制，用以驱动LLM多智能体系统进行行为演化和策略调整**。这精准地命中了你研究课题中的“多智能体”和“自我演化”两个核心方向。它不是简单的应用，也不是关于安全对齐的研究，而是一个关于智能体系统如何演化的深刻方法论。因此，这篇论文高度相关，应该被**保留 (True)**。",
                    "summary2": "本文旨在解决LLM驱动的多智能体服务中因开放性导致的信任缺失与恶意行为问题。针对一个LLM驱动的去中心化动态服务市场场景，我们提出了一种基于演化博弈论的策略均衡信任机制Ev-Trust，它将直接信任、间接信任与预期收益相结合，构建动态反馈结构。在一个LLM驱动的开放服务市场多智能体系统实验环境中，通过信任值、集体收益、恶意策略抑制等指标验证了其有效性。",
                    "summary_translation": "在 large language models (LLMs, 大语言模型) 的驱动下，Web 正在快速向 agent-centric paradigm (以智能体为中心的范式) 演进，这使得 autonomous agents (自主智能体) 能够在复杂的 decentralized environments (去中心化环境) 中进行推理、规划和交互。然而，LLM-based multi-agent systems (基于LLM的多智能体系统) 的 openness (开放性) 和 heterogeneity (异构性) 也放大了 deception (欺骗)、fraud (欺诈) 和 misinformation (虚假信息) 的风险，对 trust establishment (信任建立) 和 system robustness (系统鲁棒性) 构成了严峻挑战。为解决此问题，我们提出了 Ev-Trust，这是一种基于 evolutionary game theory (演化博弈论) 的 strategy-equilibrium trust mechanism (策略均衡信任机制)。该机制将 direct trust (直接信任)、indirect trust (间接信任) 和 expected revenue (预期收益) 整合到一个 dynamic feedback structure (动态反馈结构) 中，从而引导 agents 的 behavioral evolution (行为演化) 趋向 equilibria (均衡)。在一个 decentralized “Request-Response-Payment-Evaluation” (请求-响应-支付-评估) 服务框架内，Ev-Trust 使 agents 能够 adaptively adjust strategies (自适应调整策略)，在自然排除 malicious participants (恶意参与者) 的同时，强化 high-quality collaboration (高质量协作)。此外，我们基于 replicator dynamics equations (复制者动态方程) 的 theoretical derivation (理论推导) 证明了 local evolutionary equilibria (局部演化均衡) 的 existence (存在性) 和 stability (稳定性)。实验结果表明，在 LLM-driven open service interaction scenarios (LLM驱动的开放服务交互场景) 中，我们的方法能有效反映 agent trustworthiness (智能体可信度)，减少 malicious strategies (恶意策略)，并提升 collective revenue (集体收益)。我们希望 Ev-Trust 能为 group evolutionary game scenarios (群体演化博弈场景) 中的 agentic service web (面向智能体的服务网络) 的 trust modeling (信任建模) 提供一个新视角。",
                    "inspiration_trace": "### 作者思考过程的逻辑链推演\n\n以下基于论文内容，系统性地还原作者从宏观问题到核心方法（Ev-Trust）的思考过程。逻辑链聚焦于思想演进，而非实现细节，突出从观察、假设到方法论的逐步聚焦。语言简洁，以步骤化方式呈现。\n\n---\n\n#### **步骤1: 识别宏观问题——Web范式转变带来的信任危机**\n- **观察起点**：Web正从人类中心的信息检索模型转向以LLM驱动的智能体为中心的行动框架（\"Agentic Web\"）。智能体具备自主推理、规划和交互能力，形成去中心化服务网络。\n- **核心问题**：开放性和异构性导致系统脆弱性——恶意智能体（如欺诈者、虚假信息传播者）利用非确定性攻击破坏协作（如图1示例），引发信任缺失和系统崩溃。\n- **关键痛点**：传统信任机制（如中心化仲裁或静态声誉系统）无法适应动态、去中心化环境，亟需一种自适应、去中心化的解决方案。\n\n#### **步骤2: 细化问题——现有方法的局限性**\n- **观察现有工作**：通过文献回顾（如EigenTrust、Beta声誉系统、ICFP），发现：\n  - 传播型方法（如EigenTrust）依赖局部评分，易受共谋攻击。\n  - 概率型方法（如BRS）缺乏长期反馈，无法抑制策略突变。\n  - 激励兼容型方法（如ICFP）需中心化协调，不适用于开放场景。\n- **假设形成**：信任机制应从\"个体优化\"转向\"群体演化\"，通过动态反馈引导系统自组织向均衡演化，而非依赖外部干预。\n\n#### **步骤3: 提出核心假设——演化博弈论作为理论基石**\n- **理论选择**：演化博弈论能捕捉群体行为动态（如策略选择、适应和淘汰），适合建模LLM智能体的非确定性和异构性。\n- **关键假设**：\n  - 信任可作为\"无形收益\"嵌入智能体决策，形成策略反馈循环。\n  - 在\"请求-响应-支付-评估\"服务链中，信任演化能自然排除恶意智能体，强化高质量协作。\n- **聚焦点**：将信任与预期收益结合，构建策略均衡机制，而非单纯优化个体奖励。\n\n#### **步骤4: 方法论设计——从理论到Ev-Trust框架**\n- **思想演进**：\n  - **信任建模**：整合直接信任（基于历史交互的贝叶斯更新）和间接信任（基于邻居推荐的传播），形成动态信任值（公式3）。这解决了单一信任源的脆弱性。\n  - **收益重构**：将信任影响函数（Φ）嵌入预期收益（公式10-12），使长期收益（如被选概率）与当前行为挂钩。这驱动智能体权衡短期欺诈与长期合作。\n  - **演化机制**：基于复制者动态方程（公式14-15），建模群体策略演化。理论证明局部演化稳定策略（ESS）的存在（定理3.1），确保系统在恶意扰动下收敛到均衡。\n- **框架落地**：在\"请求-响应-支付-评估\"服务流程中实现（图2），通过信任更新（步骤5）和群体移动（步骤6）形成闭环反馈。\n\n#### **步骤5: 验证假设——实验驱动的迭代**\n- **实验设计**：构建LLM驱动的多智能体服务市场（200个智能体），模拟恶意行为（如欺诈、虚假评价）。\n- **关键验证点**：\n  - 信任演化：Ev-Trust在突变后恢复均衡（图3），优于基准（如EigenTrust无法恢复）。\n  - 策略比例：高质量服务上升，恶意策略下降（图4），验证反馈机制有效性。\n  - 理论一致性：相位图显示均衡点收敛（图8），与复制者动态预测一致。\n- **迭代优化**：通过参数敏感性实验（如恶意比例、LLM规模），确保机制鲁棒性（附录B）。\n\n#### **步骤6: 结论升华——从问题到通用范式**\n- **思想总结**：Ev-Trust将信任从\"静态评估\"转为\"演化驱动力\"，通过博弈论实现去中心化自组织。\n- **贡献定位**：不仅解决LLM多智能体信任问题，更提供\"策略均衡\"新范式，为Agentic Web的伦理治理奠基。\n- **未来延伸**：假设可扩展至网络级信任传播（如多层交互），但需解决认知不确定性等开放问题。\n\n---\n\n### 逻辑链总结\n- **宏观问题**（Web范式转变）→ **观察**（恶意行为破坏信任）→ **假设**（演化博弈论+信任反馈）→ **方法论**（Ev-Trust框架）→ **验证**（实验证明均衡）→ **升华**（通用信任范式）。\n- **核心演进**：从\"问题驱动\"到\"理论驱动\"，最终形成\"自适应演化\"机制，突出信任作为策略均衡的杠杆。"
                },
            ]
        },
    ],
};

const availableDates = ["2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24", "2025-12-23", "2025-12-22", "2025-12-19", "2025-12-18", "2025-12-17", "2025-12-16", "2025-12-12", "2025-12-11", "2025-12-10", "2025-12-09", "2025-12-05", "2025-12-03", "2025-12-02", "2025-11-28", "2025-11-26", "2025-11-25", "2025-11-21", "2025-11-20", "2025-11-19", "2025-11-18", "2025-11-14", "2025-11-13", "2025-11-12", "2025-11-10", "2025-11-07", "2025-11-06", "2025-11-05", "2025-11-04", "2025-10-31", "2025-10-30", "2025-10-29", "2025-10-28", "2025-10-24", "2025-10-23", "2025-10-22", "2025-10-21", "2025-10-20", "2025-10-17", "2025-10-16", "2025-10-15", "2025-10-14", "2025-10-10", "2025-10-09", "2025-10-08", "2025-10-07", "2025-10-06", "2025-10-03", "2025-10-02", "2025-10-01", "2025-09-30", "2025-09-29", "2025-09-26", "2025-09-25", "2025-09-24", "2025-09-23"];
const loadedDates = new Set(["2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24", "2025-12-23", "2025-12-22", "2025-12-19", "2025-12-18"]);
const LOAD_MORE_DAYS = 7;

const dailyOverviewsRaw = {
    "2026-01-09": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-09)\n\n\u4eca\u65e5\u7684\u7814\u7a76\u805a\u7126\u4e8e\u89e3\u51b3\u5927\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u6838\u5fc3\u5728\u4e8e\u5982\u4f55\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u8d4b\u4e88\u6a21\u578b\u6301\u4e45\u7684\u201c\u8bb0\u5fc6\u201d\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a **FlashMem** \u7684\u65b0\u6846\u67b6\uff0c\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u8f85\u52a9\u7f16\u7801\u5668\u67b6\u6784\uff0c\u8f6c\u800c\u901a\u8fc7\u590d\u7528\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u72b6\u6001\u6765\u63d0\u53d6\u6f5c\u5728\u8bb0\u5fc6\u3002\u8fd9\u4e00\u53d1\u73b0\u4e0d\u4ec5\u6311\u6218\u4e86\u73b0\u6709\u7684\u5916\u6302\u5f0f\u8bb0\u5fc6\u65b9\u6848\uff0c\u66f4\u5c55\u793a\u4e86\u901a\u8fc7\u6316\u6398\u6a21\u578b\u5185\u5728\u8868\u5f81\u6765\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u5de8\u5927\u6f5c\u529b\u3002\n\n### \u6548\u7387\u4e0e\u8bb0\u5fc6\uff1a\u91cd\u65b0\u601d\u8003LLM\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\n\n*   **FlashMem** \u63d0\u51fa\u901a\u8fc7 **\u8ba1\u7b97\u590d\u7528** \u4ece\u77ac\u6001\u63a8\u7406\u72b6\u6001\u4e2d\u63d0\u53d6\u5185\u5728\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86LLM\u65e0\u72b6\u6001\u67b6\u6784\u5bfc\u81f4\u7684\u5197\u4f59\u5386\u53f2\u91cd\u7b97\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528 **Shared-KV Consolidator** \u76f4\u63a5\u4ece\u9aa8\u5e72\u7f51\u7edc\u7684\u51bb\u7ed3\u7f13\u5b58\u4e2d\u5408\u6210\u8bb0\u5fc6\uff0c\u5e76\u5f15\u5165\u65e0\u53c2\u6570\u7684 **Cognitive Monitor** \u57fa\u4e8e\u6ce8\u610f\u529b\u71b5\u81ea\u9002\u5e94\u89e6\u53d1\u6574\u5408\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5339\u914d\u91cd\u578b\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e865\u500d\u3002(2601.05505 [cs.CL])\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **\u4ece\u201c\u5916\u6302\u201d\u5230\u201c\u5185\u7701\u201d\u7684\u8303\u5f0f\u8f6c\u53d8**\uff1aFlashMem \u4ee3\u8868\u4e86\u4e00\u79cd\u8d8b\u52bf\uff0c\u5373\u4e0d\u518d\u4f9d\u8d56\u5916\u90e8\u5411\u91cf\u6570\u636e\u5e93\u6216\u8f85\u52a9\u7f16\u7801\u5668\u6765\u5b58\u50a8\u8bb0\u5fc6\uff0c\u800c\u662f\u76f4\u63a5\u6316\u6398\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684 **KV Cache**\u3002\u8fd9\u79cd\u201c\u5185\u7701\u5f0f\u201d\u8bb0\u5fc6\u63d0\u53d6\u65b9\u5f0f\uff0c\u907f\u514d\u4e86\u67b6\u6784\u5272\u88c2\uff0c\u8ba9\u8bb0\u5fc6\u4e0e\u63a8\u7406\u8fc7\u7a0b\u66f4\u52a0\u7d27\u5bc6\u3002\n*   **\u8ba1\u7b97\u590d\u7528\u7684\u6781\u81f4\u6548\u7387**\uff1a\u8be5\u7814\u7a76\u63d0\u51fa\u7684 **Shared-KV Consolidator** \u5de7\u5999\u5730\u590d\u7528\u4e86\u5df2\u6709\u7684\u8ba1\u7b97\u7ed3\u679c\uff0c\u800c\u975e\u91cd\u65b0\u7f16\u7801\u5386\u53f2\u3002\u8fd9\u4e3a\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u9ad8\u6602\u6210\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6781\u5177\u6f5c\u529b\u7684\u65b9\u5411\uff0c\u5b9e\u73b0\u4e865\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002\n*   **\u62df\u4eba\u5316\u7684\u8ba4\u77e5\u76d1\u63a7\u673a\u5236**\uff1a\u5f15\u5165 **Cognitive Monitor** \u5229\u7528\u6ce8\u610f\u529b\u71b5\u6765\u5224\u65ad\u4f55\u65f6\u9700\u8981\u8bb0\u5fc6\uff0c\u6a21\u62df\u4e86\u4eba\u7c7b\u201c\u53ea\u5728\u4e0d\u786e\u5b9a\u65f6\u624d\u5f3a\u5316\u8bb0\u5fc6\u201d\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002\u8fd9\u79cd\u81ea\u9002\u5e94\u673a\u5236\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u662f\u6784\u5efa\u9ad8\u6548\u667a\u80fd\u4f53\u7684\u5173\u952e\u8bbe\u8ba1\u3002",
    "2026-01-08": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-08)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-07": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-07)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-06": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-06)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-05": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-05)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-02": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-02)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-31": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-31)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Error code: 522",
    "2025-12-29": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-29)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-26": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-26)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-24": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-24)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-23": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-23)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-22": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-22)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-19": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-19)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-18": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-18)\n\n\u4eca\u65e5AI\u7814\u7a76\u6d6a\u6f6e\u7684\u6838\u5fc3\u65e0\u7591\u662f\u201c\u667a\u80fd\u4f53\u201d\uff0c\u4ece\u67b6\u6784\u8bbe\u8ba1\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5230\u5de5\u5177\u4f7f\u7528\uff0c\u7814\u7a76\u6b63\u671d\u7740\u66f4\u81ea\u4e3b\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u4e13\u4e1a\u7684\u65b9\u5411\u7eb5\u6df1\u53d1\u5c55\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u4e24\u5927\u663e\u8457\u8d8b\u52bf\uff1a\u4e00\u662f\u5bf9\u667a\u80fd\u4f53\u5185\u90e8\u673a\u5236\uff08\u5982\u8bb0\u5fc6\u3001\u9002\u5e94\u3001\u63a8\u7406\uff09\u7684\u7cbe\u7ec6\u5316\u63a2\u7d22\uff0c\u65e8\u5728\u89e3\u51b3\u5176\u53ef\u9760\u6027\u4e0e\u6548\u7387\u74f6\u9888\uff1b\u4e8c\u662f\u5c06\u667a\u80fd\u4f53\u8303\u5f0f\u5e94\u7528\u4e8e\u6838\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u7b49\u786c\u6838\u79d1\u5b66\u9886\u57df\uff0c\u5c55\u73b0\u51fa\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u7684\u60ca\u4eba\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u5173\u4e8e\u667a\u80fd\u4f53\u76ee\u6807\u51fd\u6570\u548c\u81ea\u6211\u8fdb\u5316\u7684\u57fa\u7840\u7406\u8bba\u63a2\u8ba8\uff0c\u4e5f\u4e3a\u8fd9\u4e00\u9886\u57df\u6ce8\u5165\u4e86\u6df1\u523b\u7684\u54f2\u5b66\u601d\u8003\u3002\n\n---\n\n### \u4e3b\u9898\u5206\u7c7b\u4e0e\u8bba\u6587\u901f\u89c8\n\n#### \u4e3b\u9898\u4e00\uff1a\u667a\u80fd\u4f53\u7684\u8fdb\u5316\uff1a\u4ece\u67b6\u6784\u5230\u81ea\u6211\u9002\u5e94\n\n\u8be5\u4e3b\u9898\u805a\u7126\u4e8e\u5982\u4f55\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684AI\u667a\u80fd\u4f53\u3002\u7814\u7a76\u4e0d\u518d\u6ee1\u8db3\u4e8e\u7b80\u5355\u7684\u5de5\u5177\u8c03\u7528\uff0c\u800c\u662f\u6df1\u5165\u63a2\u7d22\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u673a\u5236\u3001\u51b3\u7b56\u6846\u67b6\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u65e8\u5728\u5b9e\u73b0\u4ece\u201c\u88ab\u52a8\u6267\u884c\u201d\u5230\u201c\u4e3b\u52a8\u6210\u957f\u201d\u7684\u8de8\u8d8a\u3002\n\n*   **Adaptation of Agentic AI** (2512.16301 [cs.CL])\n    \u8fd9\u7bc7\u7efc\u8ff0\u6027\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u4e86\u667a\u80fd\u4f53AI\u7684\u9002\u5e94\u7b56\u7565\uff0c\u5c06\u5176\u5212\u5206\u4e3a\u667a\u80fd\u4f53\u9002\u5e94\u548c\u5de5\u5177\u9002\u5e94\u4e24\u5927\u7c7b\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u4e0d\u540c\u4fe1\u53f7\u5f62\u5f0f\u3002\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7406\u8bba\u5730\u56fe\u548c\u5b9e\u8df5\u6307\u5357\u3002\n\n*   **MOBIMEM: Beyond Training** (2512.15784 [cs.LG])\n    \u9488\u5bf9\u667a\u80fd\u4f53\u90e8\u7f72\u540e\u96be\u4ee5\u6301\u7eed\u8fdb\u5316\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5**\u5185\u5b58\u4e3a\u4e2d\u5fc3**\u7684\u7cfb\u7edfMOBIMEM\u3002\u5b83\u901a\u8fc7**Profile Memory**\u3001**Experience Memory**\u548c**Action Memory**\u4e09\u79cd\u4e13\u7528\u5185\u5b58\u539f\u8bed\uff0c\u5c06\u667a\u80fd\u4f53\u7684\u8fdb\u5316\u4e0e\u6a21\u578b\u6743\u91cd\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e2a\u6027\u5316\u3001\u80fd\u529b\u548c\u6548\u7387\u3002\n\n*   **BRAID: Bounded Reasoning** (2512.15959 [cs.CL])\n    \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd**\u6709\u754c\u63a8\u7406\u6846\u67b6BRAID**\uff0c\u4f7f\u7528\u57fa\u4e8eMermaid\u7684\u6307\u4ee4\u56fe\u6765\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u800c\u975e\u65e0\u9650\u5236\u7684\u81ea\u7136\u8bed\u8a00token\u6269\u5c55\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u79cd\u7ed3\u6784\u5316\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\n\n*   **CodeMem: Architecting Reproducible Agents** (2512.15813 [cs.AI])\n    \u4e3a\u89e3\u51b3\u5f53\u524d\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u5728\u5904\u7406\u91cd\u590d\u6027\u4efb\u52a1\u65f6\u7684\u6982\u7387\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86**CodeMem**\u67b6\u6784\u3002\u5b83\u901a\u8fc7\u4ee3\u7801\u5b9e\u73b0**\u7a0b\u5e8f\u6027\u8bb0\u5fc6**\uff0c\u65e8\u5728\u6784\u5efa\u53ef\u590d\u7528\u3001\u5177\u6709\u786e\u5b9a\u6027\u53ef\u9760\u6027\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002\n\n*   **Learning to Wait** (2512.16262 [cs.AI])\n    \u672c\u6587\u63ed\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5b58\u5728\u7684**\u201c\u65f6\u95f4\u9e3f\u6c9f\u201d**\uff08Temporal Gap\uff09\uff0c\u5373\u52a8\u4f5c\u53d1\u8d77\u4e0e\u5b8c\u6210\u4e4b\u95f4\u7684\u5ef6\u8fdf\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd**\u667a\u80fd\u4f53\u7aef\u65b9\u6cd5**\uff0c\u901a\u8fc7\u6269\u5c55\u201c\u4ee3\u7801\u5373\u52a8\u4f5c\u201d\u8303\u5f0f\uff0c\u8ba9LLM\u4e3b\u52a8\u9884\u6d4b\u7b49\u5f85\u65f6\u95f4\uff0c\u4ece\u800c\u4f7f\u5176\u8ba4\u77e5\u65f6\u95f4\u7ebf\u4e0e\u7269\u7406\u4e16\u754c\u540c\u6b65\u3002\n\n#### \u4e3b\u9898\u4e8c\uff1a\u7fa4\u4f53\u667a\u80fd\uff1a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e0e\u4fe1\u4efb\u673a\u5236\n\n\u5f53\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\u65f6\uff0c\u5982\u4f55\u5efa\u7acb\u4fe1\u4efb\u3001\u6709\u6548\u6c9f\u901a\u5e76\u5b9e\u73b0\u5171\u540c\u76ee\u6807\u6210\u4e3a\u6838\u5fc3\u6311\u6218\u3002\u672c\u4e3b\u9898\u7684\u7814\u7a76\u6db5\u76d6\u4e86\u4ece\u5177\u8eab\u667a\u80fd\u4f53\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u5230\u5f00\u653e\u670d\u52a1\u7f51\u7edc\u4e2d\u7684\u4fe1\u4efb\u535a\u5f08\uff0c\u518d\u5230\u4e13\u4e1a\u9886\u57df\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\n\n*   **Emergence: Overcoming Privileged Information Bias** (2512.15776 [cs.MA])\n    \u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u4e2d\u7684**\u201c\u7279\u6743\u4fe1\u606f\u504f\u89c1\u201d**\uff08Curse of Knowledge\uff09\uff0c\u5373\u77e5\u8bc6\u4e30\u5bcc\u7684\u201c\u9886\u5bfc\u8005\u201d\u56e0\u7f3a\u4e4f\u5fc3\u667a\u7406\u8bba\u800c\u96be\u4ee5\u6307\u5bfc\u4f20\u611f\u5668\u53d7\u9650\u7684\u201c\u8ffd\u968f\u8005\u201d\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c**\u201c\u62c9\u5f0f\u201d\u4e3b\u52a8\u67e5\u8be2\u534f\u8bae**\u6bd4\u4f20\u7edf\u7684\u201c\u63a8\u5f0f\u201d\u6307\u4ee4\u66f4\u4e3a\u6709\u6548\uff0c\u6210\u529f\u5730\u5c06\u6c9f\u901a\u9519\u8bef\u7387\u964d\u4f4e\u4e86\u8fd150%\u3002\n\n*   **Ev-Trust: A Strategy Equilibrium Trust Mechanism** (2512.16167 [cs.MA])\n    \u9762\u5bf9LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u670d\u52a1\u7f51\u7edc\u4e2d\u7684\u4fe1\u4efb\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6f14\u5316\u535a\u5f08\u8bba\u7684**Ev-Trust\u673a\u5236**\u3002\u5b83\u901a\u8fc7\u6574\u5408\u76f4\u63a5\u4fe1\u4efb\u3001\u95f4\u63a5\u4fe1\u4efb\u548c\u9884\u671f\u6536\u76ca\uff0c\u5f15\u5bfc\u667a\u80fd\u4f53\u884c\u4e3a\u5411\u7b56\u7565\u5747\u8861\u6f14\u5316\uff0c\u4ece\u800c\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u81ea\u7136\u6392\u9664\u6076\u610f\u53c2\u4e0e\u8005\u3002\n\n*   **AMUSE: Audio-Visual Benchmark and Alignment Framework** (2512.16250 [cs.MA])\n    \u9488\u5bf9\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u63a8\u7406\u77ed\u677f\uff0c\u672c\u6587\u63a8\u51fa\u4e86**AMUSE\u57fa\u51c6**\u548c**RAFT\u5bf9\u9f50\u6846\u67b6**\u3002RAFT\u901a\u8fc7\u5956\u52b1\u4f18\u5316\u548c\u5185\u5728\u591a\u6a21\u6001\u81ea\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65f6\u7a7a\u8bf4\u8bdd\u4eba\u5b9a\u4f4d\u548c\u591a\u6a21\u6001\u5bf9\u8bdd\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe39.52%\u3002\n\n*   **FedSight AI: Multi-Agent System Architecture** (2512.15728 [cs.AI])\n    \u672c\u6587\u6784\u5efa\u4e86**FedSight AI**\uff0c\u4e00\u4e2a\u6a21\u62dfFOMC\uff08\u8054\u90a6\u516c\u5f00\u5e02\u573a\u59d4\u5458\u4f1a\uff09\u51b3\u7b56\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u6210\u5458\u667a\u80fd\u4f53\u5206\u6790\u7ecf\u6d4e\u6570\u636e\u5e76\u8fdb\u884c\u8fa9\u8bba\uff0c\u5176**Chain-of-Draft (CoD)** \u6269\u5c55\u673a\u5236\u5728\u9884\u6d4b\u8054\u90a6\u57fa\u91d1\u5229\u7387\u65f6\u8fbe\u5230\u4e8693.75%\u7684\u51c6\u786e\u7387\u3002\n\n*   **A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis** (2512.16063 [cs.AI])\n    \u4e3a\u89e3\u51b3\u5b9a\u6027\u4e3b\u9898\u5206\u6790\u7684\u52b3\u52a8\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u672c\u6587\u5f00\u53d1\u4e86**CoTI**\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u5bfc\u8005\u3001\u4e3b\u9898\u5316\u548c\u4ee3\u7801\u672c\u751f\u6210\u5668\u4e09\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u81ea\u52a8\u5316\u5206\u6790\u8fc7\u7a0b\u3002\u5728\u5fc3\u529b\u8870\u7aed\u60a3\u8005\u8bbf\u8c08\u5206\u6790\u4e2d\uff0c\u5176\u7ed3\u679c\u4e0e\u8d44\u6df1\u7814\u7a76\u5458\u7684\u5206\u6790\u9ad8\u5ea6\u76f8\u4f3c\u3002\n\n#### \u4e3b\u9898\u4e09\uff1a\u6b66\u88c5\u5230\u7259\u9f7f\uff1a\u667a\u80fd\u4f53\u7684\u5de5\u5177\u4f7f\u7528\u4e0e\u4e13\u4e1a\u5316\u63a8\u7406\n\n\u5de5\u5177\u662f\u667a\u80fd\u4f53\u611f\u77e5\u548c\u6539\u9020\u4e16\u754c\u7684\u5ef6\u4f38\u3002\u672c\u4e3b\u9898\u7684\u7814\u7a76\u805a\u7126\u4e8e\u5982\u4f55\u8ba9\u667a\u80fd\u4f53\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u5730\u4f7f\u7528\u5de5\u5177\uff0c\u5305\u62ec\u5408\u6210\u9ad8\u8d28\u91cf\u5de5\u5177\u8c03\u7528\u6570\u636e\u3001\u6784\u5efa\u8de8\u5e73\u53f0GUI\u8bc4\u5224\u6a21\u578b\uff0c\u4ee5\u53ca\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982PDE\u6c42\u89e3\u3001\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\uff09\u5b9e\u73b0\u4e13\u4e1a\u5316\u63a8\u7406\u3002\n\n*   **ToolForge: A Data Synthesis Pipeline** (2512.16149 [cs.AI])\n    \u9488\u5bf9\u5de5\u5177\u8c03\u7528\u6570\u636e\u4f9d\u8d56\u6602\u8d35\u771f\u5b9eAPI\u7684\u95ee\u9898\uff0c**ToolForge**\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5408\u6210\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u865a\u62df\u5de5\u5177\u5373\u53ef\u751f\u6210\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u8df3\u641c\u7d22\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u75288B\u53c2\u6570\u7684\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u5176\u6027\u80fd\u53ef\u8d85\u8d8aGPT-4o\u3002\n\n*   **OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models** (2512.16295 [cs.AI])\n    \u4e3a\u89e3\u51b3GUI\u667a\u80fd\u4f53\u5728\u957f\u6d41\u7a0b\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86**OS-Oracle\u6846\u67b6**\uff0c\u5305\u542b\u4e00\u4e2a\u53ef\u6269\u5c55\u7684GUI\u8bc4\u5224\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08SFT + **CP-GRPO**\uff09\u3002\u5176\u8bad\u7ec3\u51fa\u7684**OS-Oracle-7B**\u6a21\u578b\u5728\u8de8\u5e73\u53f0\u8bc4\u5224\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86SOTA\u3002\n\n*   **PDE-Agent: A toolchain-augmented multi-agent framework** (2512.16214 [cs.AI])\n    \u672c\u6587\u5c06\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6c42\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3aLLM\u9a71\u52a8\u7684\u5de5\u5177\u8c03\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u5de5\u5177\u94fe\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6**PDE-Agent**\u3002\u5176**Prog-Act\u6846\u67b6**\u548c**\u8d44\u6e90\u6c60**\u673a\u5236\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u548c\u591a\u5de5\u5177\u7684\u6709\u6548\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742PDE\u6c42\u89e3\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002\n\n*   **Code-in-the-Loop Forensics: Agentic Tool Use** (2512.16300 [cs.AI])\n    \u672c\u6587\u63d0\u51fa\u4e86**ForenAgent**\uff0c\u4e00\u4e2a\u591a\u8f6e\u4ea4\u4e92\u5f0f\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u3002\u5b83\u4f7fMLLM\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u3001\u6267\u884c\u548c\u8fed\u4ee3\u4f18\u5316\u57fa\u4e8ePython\u7684\u4f4e\u5c42\u5de5\u5177\uff0c\u4ece\u800c\u5c06\u9ad8\u5c42\u8bed\u4e49\u77e5\u8bc6\u4e0e\u4f4e\u5c42\u8bed\u4e49\u65e0\u5173\u7684\u75d5\u8ff9\u6709\u6548\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u89e3\u91ca\u7684\u4f2a\u9020\u5206\u6790\u3002\n\n*   **Small Language Models for Efficient Agentic Tool Calling** (2512.15943 [cs.AI])\n    \u672c\u7814\u7a76\u6311\u6218\u4e86\u201c\u8d8a\u5927\u8d8a\u597d\u201d\u7684\u89c2\u5ff5\uff0c\u8bc1\u660e\u7ecf\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u7684**\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09**\u5728\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u4e0a\u53ef\u4ee5\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u3002\u5176\u5fae\u8c03\u540e\u7684OPT-350M\u6a21\u578b\u5728ToolBench\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e8677.55%\u7684\u901a\u8fc7\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5305\u62ecChatGPT\u5728\u5185\u7684\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002\n\n#### \u4e3b\u9898\u56db\uff1a\u8de8\u754c\u7834\u58c1\uff1aAI\u667a\u80fd\u4f53\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u4e0e\u4e13\u4e1a\u5e94\u7528\n\nAI\u667a\u80fd\u4f53\u6b63\u4ece\u901a\u7528\u4efb\u52a1\u8d70\u5411\u4e13\u4e1a\u9886\u57df\uff0c\u6210\u4e3a\u79d1\u5b66\u5bb6\u7684\u5f3a\u5927\u52a9\u624b\u548c\u521b\u610f\u5de5\u5177\u3002\u672c\u4e3b\u9898\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5728\u6838\u53cd\u5e94\u5806\u8bbe\u8ba1\u3001\u5316\u5b66\u5408\u6210\u3001\u751f\u7269\u5236\u836f\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u97f3\u4e50\u63a8\u8350\u7b49\u9886\u57df\u7684\u7a81\u7834\u6027\u5e94\u7528\u3002\n\n*   **ReactorFold: Generative discovery of nuclear reactor cores** (2512.15756 [cs.LG])\n    **ReactorFold**\u5c06\u6838\u53cd\u5e94\u5806\u5806\u82af\u8bbe\u8ba1\u521b\u65b0\u5730\u8f6c\u5316\u4e3a\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\u3002\u7ecf\u8fc7DPO\u5bf9\u9f50\u7684\u6a21\u578b\u4e0d\u4ec5\u5b66\u4f1a\u4e86\u8bbe\u8ba1\uff0c\u66f4\u5c55\u73b0\u51fa**\u6d8c\u73b0\u5f0f\u7684\u8bbe\u8ba1\u7a7a\u95f4\u6269\u5c55\u80fd\u529b**\uff1a\u5b83\u80fd\u81ea\u4e3b\u8c03\u6574\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u9486\u68d2\u6570\u91cf\uff0c\u5e76\u53d1\u73b0\u4e86\u6311\u6218\u4f20\u7edf\u5bf9\u79f0\u52a0\u8f7d\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u9ad8\u6027\u80fd\u975e\u5bf9\u79f0\u914d\u7f6e\u3002\n\n*   **Synthelite: Chemist-aligned and feasibility-aware synthesis planning** (2512.16424 [cs.AI])\n    **Synthelite**\u662f\u4e00\u4e2a\u5229\u7528LLM\u8fdb\u884c\u5316\u5b66\u5408\u6210\u89c4\u5212\u7684\u6846\u67b6\uff0c\u5b83\u5141\u8bb8\u5316\u5b66\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u5e72\u9884\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u7075\u6d3b\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u7ea6\u675f\uff0c\u5728\u7b56\u7565\u7ea6\u675f\u548c\u8d77\u59cb\u6750\u6599\u7ea6\u675f\u7684\u5408\u6210\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u9ad8\u8fbe95%\u3002\n\n*   **Scalable Agentic Reasoning for Designing Biologics** (2512.15930 [cs.AI])\n    \u9488\u5bf9\u9776\u5411\u201c\u4e0d\u53ef\u6210\u836f\u201d\u7684\u5185\u6e90\u6027\u65e0\u5e8f\u86cb\u767d\uff08IDP\uff09\u7684\u6311\u6218\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86**StructBioReasoner**\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5b83\u91c7\u7528**\u9526\u6807\u8d5b\u5f0f\u63a8\u7406\u6846\u67b6**\uff0c\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u8c03\u7528\u6587\u732e\u3001AI\u7ed3\u6784\u9884\u6d4b\u548c\u5206\u5b50\u6a21\u62df\u7b49\u5de5\u5177\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8bbe\u8ba1\u7684\u5019\u9009\u836f\u7269\u7ed3\u5408\u80fd\u8d85\u8d8a\u4e86\u4eba\u7c7b\u8bbe\u8ba1\u7684\u53c2\u8003\u836f\u7269\u3002\n\n*   **Conversational Time Series Foundation Models** (2512.16022 [cs.AI])\n    \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5c06LLM\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u667a\u80fd\u201c\u6cd5\u5b98\u201d\uff0c\u8d1f\u8d23\u8bc4\u4f30\u3001\u89e3\u91ca\u548c\u534f\u8c03\u4e00\u4e2a\u7531\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7ec4\u6210\u7684\u96c6\u6210\u4f53\u3002\u901a\u8fc7**R1\u98ce\u683c\u7684\u5fae\u8c03**\uff0c\u8be5\u6a21\u578b\u5728GIFT-Eval\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u56e0\u679c grounded \u7684\u89e3\u91ca\u3002\n\n*   **WeMusic-Agent: Efficient Conversational Music Recommendation** (2512.16108 [cs.AI])\n    **WeMusic-Agent**\u662f\u4e00\u4e2a\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7**\u77e5\u8bc6\u5185\u5316**\u548c**\u667a\u80fd\u4f53\u8fb9\u754c\u5b66\u4e60**\uff0c\u6559\u4f1a\u6a21\u578b\u4f55\u65f6\u4f7f\u7528\u5185\u90e8\u77e5\u8bc6\uff0c\u4f55\u65f6\u8c03\u7528\u5916\u90e8\u97f3\u4e50\u5de5\u5177\u3002\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u8861\u4e86\u4e13\u4e1a\u77e5\u8bc6\u548c\u7075\u6d3b\u6027\u3002\n\n#### \u4e3b\u9898\u4e94\uff1a\u5176\u4ed6\u524d\u6cbf\u7814\u7a76\n\n*   **Prompt-to-Parts: Generative AI for Physical Assembly** (2512.15743 [cs.AI])\n    \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u7269\u7406\u4e0a\u53ef\u5b9e\u73b0\u7684\u88c5\u914d\u6307\u4ee4\u7684\u6846\u67b6\u3002\u901a\u8fc7\u4f7f\u7528LDraw\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0cLLM\u80fd\u591f\u4e3a\u8d85\u8fc73000\u4e2a\u96f6\u4ef6\u7684\u590d\u6742\u539f\u578b\uff08\u5982\u536b\u661f\u3001\u98de\u673a\uff09\u751f\u6210\u6709\u6548\u7684\u5206\u6b65\u6784\u5efa\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u4ece\u8bed\u4e49\u8bbe\u8ba1\u5230\u53ef\u5236\u9020\u8f93\u51fa\u7684\u8de8\u8d8a\u3002\n\n*   **Hypernetworks That Evolve Themselves** (2512.16406 [cs.AI])\n    \u672c\u6587\u63a2\u7d22\u4e86\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u4e0d\u4f9d\u8d56\u5916\u90e8\u4f18\u5316\u5668\u800c\u81ea\u6211\u8fdb\u5316\u3002\u63d0\u51fa\u7684**\u81ea\u5f15\u7528\u56fe\u8d85\u7f51\u7edc**\u5c06\u53d8\u5f02\u548c\u7ee7\u627f\u673a\u5236\u5d4c\u5165\u7f51\u7edc\u5185\u90e8\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5feb\u901f\u9002\u5e94\u548c\u6d8c\u73b0\u7684\u79cd\u7fa4\u52a8\u6001\uff0c\u5411\u6a21\u62df\u751f\u7269\u8fdb\u5316\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002\n\n*   **Subjective functions** (2512.15948 [cs.AI])\n    \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df1\u523b\u7684\u95ee\u9898\uff1a\u76ee\u6807\u51fd\u6570\u4ece\u4f55\u800c\u6765\uff1f\u5b83\u5f15\u5165\u4e86**\u4e3b\u89c2\u51fd\u6570**\u7684\u6982\u5ff5\uff0c\u5373\u4e00\u4e2a\u5185\u751f\u4e8e\u667a\u80fd\u4f53\u7684\u9ad8\u9636\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u4ee5\u9884\u671f\u9884\u6d4b\u8bef\u5dee\u4e3a\u4f8b\u8fdb\u884c\u63a2\u8ba8\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u81ea\u4e3b\u5408\u6210\u76ee\u6807\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **\u667a\u80fd\u4f53\u8303\u5f0f\u5168\u9762\u7206\u53d1**\uff1a\u4eca\u65e5\u8bba\u6587\u51e0\u4e4e\u88ab\u201c\u667a\u80fd\u4f53\u201d\u6982\u5ff5\u5168\u9762\u8986\u76d6\uff0c\u4ece\u67b6\u6784\u3001\u534f\u4f5c\u3001\u5de5\u5177\u4f7f\u7528\u5230\u4e13\u4e1a\u5e94\u7528\uff0c\u5f62\u6210\u4e86\u4e00\u80a1\u6e05\u6670\u7684\u7814\u7a76\u6d6a\u6f6e\u3002\u8fd9\u6807\u5fd7\u7740AI\u7814\u7a76\u6b63\u4ece\u201c\u6a21\u578b\u201d\u4e3a\u4e2d\u5fc3\uff0c\u52a0\u901f\u8f6c\u5411\u4ee5\u201c\u667a\u80fd\u4f53\u201d\u4e3a\u4e2d\u5fc3\u7684\u65b0\u8303\u5f0f\u3002\n\n*   **\u8d85\u8d8a\u4eba\u7c7b\u542f\u53d1\u5f0f\uff1aAI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u81ea\u4e3b\u521b\u9020\u529b**\uff1a**ReactorFold**\u7684\u7814\u7a76\u5c24\u4e3a\u60ca\u8273\uff0c\u5b83\u4e0d\u4ec5\u4f18\u5316\u8bbe\u8ba1\uff0c\u66f4\u80fd**\u7a81\u7834\u4eba\u7c7b\u9884\u8bbe\u7684\u7ea6\u675f**\uff0c\u81ea\u4e3b\u8c03\u6574\u8bbe\u8ba1\u53c2\u6570\u5e76\u53d1\u73b0\u5168\u65b0\u7684\u9ad8\u6027\u80fd\u914d\u7f6e\u3002\u8fd9\u9884\u793a\u7740AI\u6b63\u4ece\u201c\u4f18\u5316\u5de5\u5177\u201d\u8f6c\u53d8\u4e3a\u80fd\u591f\u8fdb\u884c\u201c\u521b\u9020\u6027\u63a2\u7d22\u201d\u7684\u79d1\u7814\u4f19\u4f34\u3002\n\n*   **\u6548\u7387\u4e3a\u738b\uff1a\u5c0f\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u53cd\u8d85**\uff1a**Small Language Models for Efficient Agentic Tool Calling**\u7684\u7814\u7a76\u6709\u529b\u5730\u8bc1\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u9488\u5bf9\u6027\u5fae\u8c03\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u5de5\u5177\u8c03\u7528\uff09\u7684\u6027\u80fd\u53ef\u4ee5\u5168\u9762\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\u3002\u8fd9\u5bf9\u964d\u4f4e\u4f01\u4e1aAI\u5e94\u7528\u6210\u672c\u3001\u63a8\u52a8\u6280\u672f\u666e\u60e0\u5177\u6709\u91cd\u5927\u5b9e\u8df5\u610f\u4e49\u3002\n\n*   **\u8ffd\u95ee\u672c\u6e90\uff1a\u4ece\u201c\u76ee\u6807\u51fd\u6570\u201d\u5230\u201c\u4e3b\u89c2\u51fd\u6570\u201d\u7684\u54f2\u5b66\u601d\u8fa8**\uff1a\u5728\u4f17\u591a\u5e94\u7528\u9a71\u52a8\u7684\u7814\u7a76\u4e2d\uff0c**Subjective functions**\u5982\u540c\u4e00\u80a1\u6e05\u6d41\uff0c\u5b83\u56de\u5f52AI\u7684\u6839\u672c\u95ee\u9898\u2014\u2014\u201c\u76ee\u6807\u4ece\u4f55\u800c\u6765\uff1f\u201d\u3002\u5c06\u76ee\u6807\u51fd\u6570\u5185\u751f\u5316\uff0c\u63a2\u8ba8\u667a\u80fd\u4f53\u7684\u4e3b\u89c2\u80fd\u52a8\u6027\uff0c\u4e3a\u6784\u5efa\u771f\u6b63\u81ea\u4e3b\u3001\u751a\u81f3\u5177\u6709\u81ea\u6211\u610f\u8bc6\u7684AI\u7cfb\u7edf\u57cb\u4e0b\u4e86\u4f0f\u7b14\u3002",
};
const dailyOverviews = {};
for (const date in dailyOverviewsRaw) {
    dailyOverviews[date] = dailyOverviewsRaw[date];
}


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文
        let isLoadingMore = false; // 是否正在加载更多

        // 获取未加载的日期
        function getUnloadedDates() {
            return availableDates.filter(date => !loadedDates.has(date));
        }

        // 加载更多日期的数据
        async function loadMoreDates() {
            if (isLoadingMore) return;

            const unloadedDates = getUnloadedDates();
            if (unloadedDates.length === 0) {
                showSimpleToast('已加载全部数据');
                return;
            }

            isLoadingMore = true;
            const loadBtn = document.getElementById('load-more-btn');
            if (loadBtn) {
                loadBtn.disabled = true;
                loadBtn.innerHTML = '<span class="animate-spin inline-block mr-2">⏳</span>加载中...';
            }

            const datesToLoad = unloadedDates.slice(0, LOAD_MORE_DAYS);
            let loadedCount = 0;

            for (const date of datesToLoad) {
                try {
                    const response = await fetch(`data/${date}.json`);
                    if (!response.ok) continue;

                    const dateData = await response.json();

                    // 将数据添加到 allPapers
                    allPapers[date] = dateData.categories;

                    // 添加每日速览
                    if (dateData.overview) {
                        dailyOverviews[date] = dateData.overview;
                    }

                    loadedDates.add(date);
                    loadedCount++;
                } catch (e) {
                    console.error(`加载 ${date} 数据失败:`, e);
                }
            }

            isLoadingMore = false;

            if (loadedCount > 0) {
                renderPapers();
                showSimpleToast(`已加载 ${loadedCount} 天的数据`);
            }

            updateLoadMoreButton();
        }

        // 更新"加载更多"按钮状态
        function updateLoadMoreButton() {
            const loadBtn = document.getElementById('load-more-btn');
            const unloadedCount = getUnloadedDates().length;

            if (loadBtn) {
                if (unloadedCount === 0) {
                    loadBtn.style.display = 'none';
                } else {
                    loadBtn.style.display = 'inline-flex';
                    loadBtn.disabled = false;
                    loadBtn.innerHTML = `📥 加载更多 (还有 ${unloadedCount} 天)`;
                }
            }
        }

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            const listItem = paperEl.closest('li');
            const categoryContent = paperEl.closest('.category-content');
            const sectionEl = paperEl.closest('section[data-date-section]');
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                if (listItem) {
                    listItem.remove();
                } else {
                    paperEl.remove();
                }

                updateCategoryView(categoryContent);
                updateDateSection(sectionEl);
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        function updateCategoryView(categoryContent) {
            if (!categoryContent) return;
            const listEl = categoryContent.querySelector('ul');
            if (!listEl) return;

            const paperItems = listEl.querySelectorAll('.paper-item').length;
            let placeholder = listEl.querySelector('.empty-category-placeholder');

            if (paperItems === 0) {
                if (!placeholder) {
                    placeholder = document.createElement('li');
                    placeholder.className = 'empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400';
                    placeholder.textContent = '此分类下暂无论文。';
                    listEl.appendChild(placeholder);
                }
            } else if (placeholder) {
                placeholder.remove();
            }

            const toggle = document.querySelector(`.category-toggle[data-target="${categoryContent.id}"]`);
            if (toggle) {
                const countBadge = toggle.querySelector('.category-count');
                if (countBadge) {
                    countBadge.textContent = paperItems;
                }
            }
        }

        function updateDateSection(sectionEl) {
            if (!sectionEl) return;
            const totalPapers = sectionEl.querySelectorAll('.paper-item').length;
            const header = sectionEl.querySelector('[data-date-heading]');

            if (header) {
                const dateLabel = header.dataset.dateHeading || header.textContent.split(' ')[0];
                header.textContent = `${dateLabel} (${totalPapers} 篇论文)`;
            }

            if (totalPapers === 0) {
                sectionEl.remove();
            }
        }

        // 可折叠功能
        function toggleCollapsible(header) {
            const content = header.nextElementSibling;
            const isOpen = header.classList.contains('open');
            
            if (isOpen) {
                header.classList.remove('open');
                content.classList.remove('open');
            } else {
                header.classList.add('open');
                content.classList.add('open');
            }
        }

        // 渲染所有 Markdown 内容
        function renderAllMarkdown() {
            // 配置 marked 选项
            if (typeof marked !== 'undefined') {
                marked.setOptions({
                    breaks: true,
                    gfm: true,
                    headerIds: false,
                    mangle: false
                });
                
                // 遍历所有灵感溯源的容器并渲染 Markdown
                for (const date in allPapers) {
                    const categories = allPapers[date];
                    categories.forEach(category => {
                        if (category.papers) {
                            category.papers.forEach(paper => {
                                if (paper.inspiration_trace) {
                                    const elementId = `inspiration-${paper.arxiv_id}`;
                                    const element = document.getElementById(elementId);
                                    if (element) {
                                        try {
                                            element.innerHTML = marked.parse(paper.inspiration_trace);
                                        } catch (e) {
                                            console.error('Markdown 渲染失败:', e);
                                            // 如果渲染失败，使用纯文本显示
                                            element.textContent = paper.inspiration_trace;
                                        }
                                    }
                                }
                            });
                        }
                    });
                }
            }
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-3 sm:mb-4">
                        <div class="flex items-start space-x-2 sm:space-x-3 flex-1 min-w-0">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5 sm:h-6 sm:w-6" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-base sm:text-lg font-semibold text-black dark:text-white leading-tight break-words">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-2 sm:ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-3 sm:mb-4">
                        <div class="flex flex-wrap items-center gap-2 sm:gap-4 text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                            <span class="break-all"><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300 whitespace-nowrap">
                                ${paper.category}
                            </span>
                            <span class="whitespace-nowrap">${date}</span>
                        </div>
                        <div class="text-xs sm:text-sm text-black dark:text-white break-words">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-3 sm:mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50 w-4 h-4">
                            <span class="ml-2 text-xs sm:text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">筛选原因</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.filter_reason}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">AI总结</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.summary2}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">原始摘要</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-3 sm:p-4 rounded-r-lg">
                                    ${paper.summary_translation ? `
                                    <div class="chinese-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: block;">
                                        ${paper.summary_translation}
                                    </div>
                                    ` : ''}
                                    ${paper.summary ? `
                                    <div class="english-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: none;">
                                        ${paper.summary}
                                    </div>
                                    ` : ''}
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.inspiration_trace ? `
                    <!-- 灵感溯源 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">灵感溯源</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-red-50/70 dark:bg-red-950/20 border-l-3 border-red-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed markdown-content break-words" id="inspiration-${paper.arxiv_id}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors whitespace-nowrap">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors whitespace-nowrap">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors whitespace-nowrap">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-2 sm:p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-2 sm:space-x-3 min-w-0 flex-1">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform flex-shrink-0" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400 text-sm sm:text-base truncate">${category.name}</span>
                        </div>
                        <span class="category-count text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5 ml-2 flex-shrink-0">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-2 sm:ml-4">
                        <ul class="space-y-3 sm:space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-6 sm:mb-8" data-date-section="${date}">
                        <h2 class="text-base sm:text-lg font-medium text-slate-500 dark:text-slate-400 mb-3 sm:mb-4" data-date-heading="${date}">${date} (${dateVisibleTotal} 篇论文)</h2>
                `;
                
                // 添加该日期的AI论文速览（如果存在）
                if (dailyOverviews[date]) {
                    html += `
                        <div class="mb-3 sm:mb-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md p-3 sm:p-5">
                            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                                <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2 text-blue-600 dark:text-blue-400 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path>
                                </svg>
                                <span class="font-semibold text-slate-900 dark:text-white text-sm sm:text-base">今日AI论文速览</span>
                            </div>
                            <div class="collapsible-content">
                                <div class="inner">
                                    <div class="markdown-content text-slate-700 dark:text-slate-200 text-xs sm:text-sm" id="overview-${date}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }
                
                html += `
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-3 sm:p-4 lg:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }

            // 添加"加载更多"按钮
            const unloadedCount = getUnloadedDates().length;
            if (unloadedCount > 0) {
                html += `
                    <div class="text-center py-6">
                        <button id="load-more-btn" onclick="loadMoreDates()"
                            class="inline-flex items-center px-6 py-3 text-base font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg shadow-md transition-all duration-200 hover:shadow-lg">
                            📥 加载更多 (还有 ${unloadedCount} 天)
                        </button>
                    </div>
                `;
            }

            mainContent.innerHTML = html;
            updateStats();
            
            // 渲染所有日期的 Markdown 速览内容
            for (const date in dailyOverviews) {
                const overview = dailyOverviews[date];
                const elementId = `overview-${date}`;
                const element = document.getElementById(elementId);
                if (element && overview) {
                    try {
                        if (typeof marked !== 'undefined') {
                            element.innerHTML = marked.parse(overview);
                        } else {
                            element.textContent = overview;
                        }
                    } catch (e) {
                        console.error('Markdown 渲染失败:', e);
                        element.textContent = overview;
                    }
                }
            }
            
            // 渲染所有论文的 Markdown 内容
            renderAllMarkdown();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>