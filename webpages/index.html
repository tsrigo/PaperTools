<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PaperTools - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 引入 Marked.js 用于 Markdown 渲染 -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        
        /* 移动端优化 */
        @media (max-width: 640px) {
            body {
                font-size: 14px;
            }
            
            /* 改善可点击区域 */
            button, a {
                min-height: 44px;
                min-width: 44px;
            }
            
            /* 优化间距 */
            .container {
                padding-left: 12px !important;
                padding-right: 12px !important;
            }
        }
        
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
        
        /* 可折叠部分样式 */
        .collapsible-header {
            cursor: pointer;
            display: flex;
            align-items: center;
            font-weight: 600;
            padding: 8px 0;
            user-select: none;
            color: #1e40af;
            transition: all 0.2s ease-in-out;
        }
        .dark .collapsible-header {
            color: #60a5fa;
        }
        .collapsible-header:hover {
            opacity: 0.8;
        }
        .collapsible-header::before {
            content: "▶";
            margin-right: 8px;
            transition: transform 0.3s ease;
            font-size: 0.8em;
        }
        .collapsible-header.open::before {
            transform: rotate(90deg);
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: none;
        }
        .collapsible-content .inner {
            padding-top: 8px;
        }
        
        /* Markdown 内容样式 */
        .markdown-content {
            line-height: 1.6;
        }
        .markdown-content h1 {
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 1em;
            margin-bottom: 0.5em;
            color: #1e40af;
        }
        .dark .markdown-content h1 {
            color: #60a5fa;
        }
        .markdown-content h2 {
            font-size: 1.3em;
            font-weight: bold;
            margin-top: 0.8em;
            margin-bottom: 0.4em;
            color: #1e40af;
        }
        .dark .markdown-content h2 {
            color: #60a5fa;
        }
        .markdown-content h3 {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 0.6em;
            margin-bottom: 0.3em;
            color: #1e40af;
        }
        .dark .markdown-content h3 {
            color: #60a5fa;
        }
        .markdown-content h4 {
            font-size: 1em;
            font-weight: bold;
            margin-top: 0.5em;
            margin-bottom: 0.25em;
            color: #2563eb;
        }
        .dark .markdown-content h4 {
            color: #93c5fd;
        }
        .markdown-content p {
            margin-bottom: 0.8em;
        }
        .markdown-content ul, .markdown-content ol {
            margin-left: 1.5em;
            margin-bottom: 0.8em;
        }
        .markdown-content ul {
            list-style-type: disc;
        }
        .markdown-content ol {
            list-style-type: decimal;
        }
        .markdown-content li {
            margin-bottom: 0.3em;
        }
        .markdown-content code {
            background-color: #f1f5f9;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
        .dark .markdown-content code {
            background-color: #334155;
        }
        .markdown-content pre {
            background-color: #f1f5f9;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 0.8em;
        }
        .dark .markdown-content pre {
            background-color: #334155;
        }
        .markdown-content pre code {
            background-color: transparent;
            padding: 0;
        }
        .markdown-content blockquote {
            border-left: 3px solid #cbd5e1;
            padding-left: 1em;
            margin-left: 0;
            margin-bottom: 0.8em;
            color: #64748b;
        }
        .dark .markdown-content blockquote {
            border-left-color: #475569;
            color: #94a3b8;
        }
        .markdown-content strong {
            font-weight: 600;
        }
        .markdown-content em {
            font-style: italic;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-3 sm:px-4 py-2 rounded-lg shadow-lg z-50 hidden max-w-xs sm:max-w-sm">
        <div class="flex items-center space-x-2">
            <span id="toast-message" class="text-sm sm:text-base">已删除</span>
            <span id="countdown" class="text-xs sm:text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-xs sm:text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-full lg:w-3/5 max-w-none p-3 sm:p-4 lg:p-6">
        <!-- 头部导航栏 -->
        <header class="mb-4 sm:mb-6">
            <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-3 sm:gap-4">
                <h1 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white">PaperTools</h1>
                <div class="flex flex-wrap items-center gap-2 sm:gap-3 w-full sm:w-auto">
                    <!-- 统计信息 -->
                    <div class="text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                        总计 <span id="total-papers">0</span> 篇论文
                    </div>
                    <!-- 筛选按钮 -->
                    <button id="filter-starred" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none transition-colors whitespace-nowrap">
                        显示全部
                    </button>
                    <!-- 中英文摘要切换按钮 -->
                    <button id="summary-toggle" class="px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none transition-colors whitespace-nowrap">
                        中文摘要
                    </button>
                    <button id="theme-toggle" class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <!-- 太阳图标 (浅色模式) -->
                        <svg id="theme-icon-light" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                        </svg>
                        <!-- 月亮图标 (深色模式) -->
                        <svg id="theme-icon-dark" class="h-5 w-5 sm:h-6 sm:w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                        </svg>
                    </button>
                    <!-- GitHub 图标按钮 -->
                    <a href="https://github.com/tsrigo/PaperTools" target="https://github.com/tsrigo/PaperTools" title="GitHub 项目主页"
                       class="p-1.5 sm:p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none flex-shrink-0">
                        <svg class="h-5 w-5 sm:h-6 sm:w-6 text-slate-700 dark:text-slate-200" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.021c0 4.428 2.865 8.184 6.839 9.504.5.092.682-.217.682-.483 0-.237-.009-.868-.014-1.703-2.782.605-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.004.07 1.532 1.032 1.532 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.339-2.221-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.254-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.025A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.295 2.748-1.025 2.748-1.025.546 1.378.202 2.396.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.847-2.337 4.695-4.566 4.944.359.309.678.919.678 1.852 0 1.336-.012 2.417-.012 2.747 0 .268.18.579.688.481C19.138 20.2 22 16.447 22 12.021 22 6.484 17.523 2 12 2z" clip-rule="evenodd"/>
                        </svg>
                    </a>
                </div>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-6 sm:space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2026-01-15": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
                    "arxiv_id": "2601.10581",
                    "authors": "Kimia Abedini, Farzad Shami, Gianmaria Silvello",
                    "summary": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了GenomAgent，这是一个多智能体框架，重点在于多智能体之间的协作与推理架构，符合“多智能体：协作”的研究范围。尽管应用场景是基因组学，但其核心贡献在于智能体框架的设计与改进，而非单纯的应用。",
                    "summary2": "总结生成失败",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Generative AI collective behavior needs an interactionist paradigm",
                    "arxiv_id": "2601.10567",
                    "authors": "Laura Ferrarotti, Gian Maria Campedelli, Roberto Dessì, Andrea Baronchelli, Giovanni Iacca, Kathleen M. Carley, Alex Pentland, Joel Z. Leibo, James Evans, Bruno Lepri",
                    "summary": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确讨论了基于LLM的智能体（agents based on LLMs）的集体行为（collective behavior）以及多智能体生成式AI系统（multi-agent generative AI systems）中的涌现现象。这完全符合“多智能体：协作、通信、博弈”的研究范围，且不属于纯应用、纯推理或基础设施优化等排除类别。",
                    "summary2": "总结生成失败",
                    "summary_translation": "本文主张，理解基于大语言模型 (LLMs) 的智能体的集体行为是一项至关重要的探究领域，其在风险与收益方面具有重要意义，并在多个层面上对社会产生深远影响。我们指出，大语言模型 (LLMs) 的独特性质——即其通过海量预训练知识和隐性社会先验进行初始化，以及通过上下文学习进行适应的能力——催生了对一种交互主义范式的需求。该范式包含替代性的理论基础、方法论和分析工具，旨在系统地考察先验知识和嵌入的价值观如何与社会语境相互作用，从而在多智能体生成式人工智能系统中塑造涌现现象。我们提出并讨论了四个对于基于大语言模型 (LLMs) 的集体的开发与部署至关重要的方向，重点关注理论、方法以及跨学科对话。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
                    "arxiv_id": "2601.10402",
                    "authors": "Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen",
                    "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了ML-Master 2.0，这是一个专注于解决超长周期自主性问题的自主智能体。论文核心贡献是引入了分层认知缓存（HCC）架构来管理记忆和策略，这直接属于单智能体研究中的“规划”和“记忆”范畴，而非单纯的应用或推理。",
                    "summary2": "本文旨在解决AI代理在超长视距科学任务中难以维持战略一致性的问题。针对机器学习工程（MLE）场景，我们提出了一种名为ML-Master 2.0的自主代理，其核心是分层认知缓存（HCC）架构。该架构通过将执行经验动态蒸馏为稳定知识和跨任务智慧，实现了认知积累。我们在OpenAI的MLE-Bench上通过平均奖牌率验证了其有效性，达到了56.44%的SOTA性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n---\n\n### 第一阶段：宏观问题的识别与定位\n**（从“智能体”到“长程自主性”的瓶颈）**\n\n1.  **观察现象**：AI 正从“被动工具”向“智能体科学”演进，旨在像人类科学家一样进行自主发现。\n2.  **发现瓶颈**：现有的 LLM 智能体擅长短程推理（单次对话或短任务），但在面对真实的科学发现过程时显得力不从心。真实科研具有**超长视距**特征——实验周期长达数天甚至数周，反馈延迟极高，且探索空间巨大。\n3.  **核心矛盾**：智能体在长周期中会被海量的执行细节（如调试日志、失败尝试）淹没，导致**上下文饱和**，从而丧失对长期战略目标的连贯性。即：如何在有限的上下文窗口中，维持跨越数万步操作的“战略一致性”？\n\n### 第二阶段：研究范式的聚焦与具体化\n**（从“通用科学”到“机器学习工程 MLE”）**\n\n1.  **选择切入点**：为了解决超长视距问题，需要一个反馈迅速、纯计算化的实验环境。传统的化学/生物实验周期太长，不适合快速迭代。\n2.  **锁定领域**：**AI-for-AI（机器学习工程，MLE）**。特别是 OpenAI 的 MLE-Bench（基于 Kaggle 竞赛），它完美复刻了科研的特征：高维搜索、试错循环、且需要数小时的持续优化。\n3.  **明确挑战**：在 MLE 任务中，智能体不仅要写代码，还要在 24 小时内不断试错、调优。问题不再是“如何生成代码”，而是“如何管理在 24 小时内产生的指数级增长的经验与信息”。\n\n### 第三阶段：概念层面的突破与重构\n**（从“线性存储”到“认知积累”）**\n\n1.  **批判现有思路**：传统的上下文管理（如 MemGPT）多侧重于“存储”或“检索”，本质上是线性的信息堆叠。这无法解决长程任务中的“遗忘”和“混乱”问题。\n2.  **提出新假设**：作者认为，超长视距自主性不是简单的“记住更多”，而是一个**认知进化**的过程。这模仿了人类科学家的认知模式：\n    *   **原始经验**：即时的、高保真的执行细节（如报错信息）。\n    *   **提炼知识**：经过验证的、相对稳定的判断（如“特征 X 无效”）。\n    *   **抽象智慧**：跨任务通用的、高度抽象的策略（如“此类任务适合用 ConvNeXt”）。\n3.  **核心思想**：**认知积累**。关键在于将瞬时的、嘈杂的执行痕迹，动态地蒸馏为稳定的、可复用的知识，最终升华为跨任务的智慧。\n\n### 第四阶段：工程架构的映射与设计\n**（从“认知模型”到“分层缓存系统”）**\n\n1.  **寻找工程隐喻**：如何实现这种“认知积累”？作者借鉴了计算机系统中的**多级缓存架构**。CPU 的 L1/L2/L3 缓存完美对应了不同时间尺度的数据访问需求。\n2.  **架构设计（HCC）**：将认知模型映射为三级缓存结构：\n    *   **L1 Cache（演进经验）**：对应 CPU L1。存储当前正在进行的、高保真的原始执行轨迹（代码、日志）。用于即时推理和调试。\n    *   **L2 Cache（精炼知识）**：对应 CPU L2。存储已完成实验阶段的总结性洞察（如“方案 A 失败是因为过拟合”）。用于维持中期的战略连贯性。\n    *   **L3 Cache（先验智慧）**：对应 CPU L3/内存。存储跨任务的通用策略和模板。用于新任务的冷启动和迁移。\n3.  **解决核心痛点**：通过这种结构，将“高频变化的执行细节”与“长期稳定的战略状态”解耦，防止上下文窗口被垃圾信息填满。\n\n### 第五阶段：动态机制的构建\n**（从“静态结构”到“动态迁移”）**\n\n1.  **赋予系统生命**：仅有分层结构是不够的，必须定义信息如何在层级间流动。作者提出了**上下文迁移**机制，模仿数据的读写过程。\n2.  **定义流动规则**：\n    *   **预取**：在任务开始前，从 L3 检索相关的先验智慧，构建强先验。\n    *   **命中**：在推理时，优先从 L1 获取原始细节，若 L1 丢失则从 L2 获取摘要，确保信息不丢失且上下文精简。\n    *   **提升**：这是最关键的一步。当一个探索阶段结束，利用 LLM 将 L1 中的大量原始日志压缩提炼，写入 L2；当一个任务结束，将 L2 中的关键经验进一步抽象，写入 L3。\n3.  **逻辑闭环**：通过“提升”操作，系统实现了从“数据”到“智慧”的自动蒸馏，使得智能体能够随着时间推移变得越来越“聪明”，而不是越来越“混乱”。\n\n### 第六阶段：验证与结论\n**（从“理论推演”到“实证优势”）**\n\n1.  **实验验证**：在 MLE-Bench 上进行 24 小时实测。\n2.  **结果分析**：ML-Master 2.0 不仅取得了 SOTA 性能，更重要的是，它证明了**结构化的认知积累**能够有效控制上下文长度的增长（从 200k tokens 压制到 70k tokens），同时保持甚至提升了解决问题的质量。\n3.  **最终结论**：超长视距自主性的关键不在于更大的上下文窗口，而在于具备**进化能力的上下文管理架构**。\n\n---\n\n**总结**：作者的思考路径是从**“智能体在长周期中的迷失”**这一痛点出发，通过**“认知科学+计算机体系结构”**的跨学科类比，提出了**“分层认知积累”**的解决方案，最终通过**“动态蒸馏机制”**实现了智能体在超长视距任务中的战略连贯性与自我进化能力。"
                },
                {
                    "title": "Structured Personality Control and Adaptation for LLM Agents",
                    "arxiv_id": "2601.10025",
                    "authors": "Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin",
                    "summary": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个用于LLM智能体的人格控制框架，明确包含“reflection mechanism”（自我反思）和“long-term personality evolution”（自我演化），符合研究范围中关于单智能体（自我反思）和自我演化的定义。该研究关注智能体的内在机制设计，不属于纯应用或基础设施优化。",
                    "summary2": "本文旨在解决现有LLM个性建模静态且缺乏适应性的问题。针对LLM Agent的个性控制与适应场景，我们提出了一种Jungian Personality Adaptation Framework (JPAF)，该框架基于Jungian心理类型，整合了dominant-auxiliary coordination、reinforcement-compensation和reflection三种机制。我们在GPT、Llama和Qwen模型上，通过MBTI问卷和自定义挑战场景，利用Dimension Accuracy Gain和Type Activation Accuracy等指标验证了其有效性。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 正日益重塑人机交互 (Human-Computer Interaction, HCI) 的格局，其应用范围涵盖从个性化助手到社会模拟等多个领域。除了语言能力之外，研究人员还在探索 LLMs 是否能够表现出影响参与度、决策制定和感知真实度的类人特征。其中，人格尤为关键，然而现有方法往往难以同时实现细腻且具有适应性的表达。我们提出了一个基于荣格心理类型 (Jungian psychological types) 对 LLM 人格进行建模的框架，该框架整合了三种机制：用于连贯核心表达的主导-辅助协调机制 (dominant-auxiliary coordination mechanism)、用于临时适应情境的强化-补偿机制 (reinforcement-compensation mechanism)，以及驱动长期人格演变的反思机制 (reflection mechanism)。这种设计使智能体能够在保持细腻特征的同时，动态调整以适应交互需求，并逐步更新其底层结构。我们利用迈尔斯-布里格斯类型指标 (Myers-Briggs Type Indicator, MBTI) 问卷评估人格对齐 (personality alignment) 情况，并在多种挑战场景下进行了测试，以此作为初步的结构化评估。研究结果表明，具有演进能力和人格感知的 LLMs 能够支持连贯且情境敏感的交互，从而在 HCI 中实现自然拟真的智能体设计。",
                    "inspiration_trace": "基于论文《Structured Personality Control and Adaptation for LLM Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程。\n\n---\n\n### 1. 宏观观察与问题定义：从“静态扮演”到“动态人格”\n**思考起点：**\n作者首先观察到，尽管LLM在对话能力上已非常强大，但在模拟人类交互时，现有的“人格”往往只是表面的装饰。\n*   **现状痛点：** 现有的方法（如简单的Prompt提示或微调）通常将人格视为一个**静态标签**（例如：“你是一个INTJ类型的人”）。\n*   **核心矛盾：** 真实的人类人格并非一成不变。它既需要保持核心的**一致性**（我是谁），又需要根据环境进行**适应性调整**（我现在的反应），并随着经历产生**长期演变**（我的成长）。\n*   **研究目标：** 如何让LLM Agent不仅能“扮演”一种性格，还能像人一样，在保持核心自我的同时，灵活适应场景并随时间进化？\n\n### 2. 理论溯源与视角转换：从“MBTI标签”到“荣格认知功能”\n**思考转折：**\n为了解决上述矛盾，作者没有直接使用常见的“大五人格”或简单的MBTI四字母分类，而是深入挖掘了MBTI背后的理论基础——荣格心理学类型。\n*   **理论洞察：** MBTI的16种类型只是表象，其底层是8种**认知功能**（如外倾思考Te、内倾直觉Ni等）。\n*   **逻辑推演：**\n    *   如果只控制“INTJ”这个标签，模型很难理解在不同情境下该表现出“直觉（N）”还是“思考（T）”。\n    *   如果直接控制底层的**认知功能**，就能更精细地描述Agent的决策过程。\n*   **假设提出：** 将人格建模为一组动态加权的认知功能，而不是固定的分类标签，可以实现更细腻、更灵活的表达。\n\n### 3. 结构化建模：构建人格的“核心骨架”\n**思考深化：**\n有了认知功能作为基础，如何组织它们以体现人格的稳定性？\n*   **理论映射：** 荣格理论指出，人格由“主导功能”和“辅助功能”共同构成核心意识，其余功能处于潜意识或未分化状态。\n*   **数学抽象：** 作者将“分化程度”这一心理学概念转化为**权重范围**。\n    *   **主导功能**赋予高权重（核心意识）。\n    *   **辅助功能**赋予中权重（平衡意识）。\n    *   **其他功能**赋予低权重（潜意识潜能）。\n*   **机制设计（机制一：主导-辅助协调）：** 确立了Agent的“出厂设置”。这一机制保证了Agent在大多数情况下有一个稳定、连贯的行为模式，解决了人格“崩坏”或不一致的问题。\n\n### 4. 短期适应性设计：应对环境的“临时调整”\n**思考延伸：**\n核心骨架虽然稳定，但人是灵活的。当遇到核心功能无法解决的挑战时（例如一个逻辑极强的人需要处理极度感性的危机），人会怎么做？\n*   **心理学机制：** 荣格提出的“强化”与“补偿”。\n    *   **强化：** 用擅长的功能解决问题，越用越强。\n    *   **补偿：** 当主导功能失效时，潜意识中合适的功能会被临时调动起来。\n*   **算法实现（机制二：强化-补偿）：**\n    *   引入**临时权重**的概念。\n    *   当场景挑战出现时，如果主导功能有效，就增加其临时权重（强化）；如果无效，就激活并增加其他合适功能的临时权重（补偿）。\n*   **逻辑闭环：** 这使得Agent能够根据上下文动态调整反应，而不会破坏其底层的人格结构（因为只是临时权重变化）。\n\n### 5. 长期进化设计：从“量变”到“质变”\n**思考升华：**\n如果某种临时调整反复发生，它就不再是“临时”的，而会变成人格的一部分。这就是人的成长。\n*   **演化逻辑：** 经历塑造人格。如果一个Agent长期处于需要某种特定功能的环境中，该功能应该逐渐从“潜意识”上升到“意识”层面。\n*   **机制设计（机制三：反思）：**\n    *   设定触发条件（如临时权重长期超过主导权重）。\n    *   引入**反思机制**，回顾历史交互，决定是否将临时的权重变化固化为永久的基础权重。\n    *   定义了严格的演化规则（如主导功能替换、主辅功能互换），确保这种变化符合荣格理论的逻辑，而不是随机乱变。\n*   **最终形态：** Agent不仅适应了当下，还完成了人格结构的重组和进化。\n\n### 6. 逻辑链总结\n作者的思考路径呈现出清晰的**“解构-重构-演化”**逻辑：\n\n1.  **解构问题：** 发现现有LLM人格是静态的，无法兼顾稳定性与适应性。\n2.  **理论重构：** 放弃表层标签（MBTI），采用深层机制（荣格8大认知功能）作为建模原子。\n3.  **分层实现：**\n    *   **静态层（权重分层）：** 用权重差异模拟主导/辅助功能，确立核心身份。\n    *   **动态层（临时权重）：** 用强化/补偿机制模拟短期情境适应。\n    *   **演化层（反思机制）：** 用权重固化规则模拟长期人格成长。\n\n通过这一逻辑链，作者成功地将心理学理论与计算模型结合，产出了一套既能保持“我是谁”，又能适应“我在哪”，还能体现“我经历了什么”的LLM Agent人格框架（JPAF）。"
                },
                {
                    "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
                    "arxiv_id": "2601.10029",
                    "authors": "Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu",
                    "summary": "Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 PaperScout，一个用于学术搜索的自主智能体。它涉及单智能体的核心要素（工具使用、动态决策制定），并引入了针对多轮智能体任务的序列级策略优化方法（PSPO），属于智能体的自我演化与优化范畴，符合筛选标准。",
                    "summary2": "本文旨在解决现有学术搜索依赖刚性工作流且难以处理复杂查询的问题。针对多轮检索任务中强化学习粒度不匹配的挑战，我们提出了自主代理 PaperScout 及过程感知的序列级策略优化方法 PSPO。在 AutoScholarQuery 和 RealScholarQuery 数据集上，通过 Recall、F1-score 和 LLM-score 等指标验证了其显著优于基线方法的有效性。",
                    "summary_translation": "学术论文搜索是科学研究的一项基本任务，然而大多数现有方法依赖于僵化的预定义工作流，难以应对复杂的条件查询。为解决这一局限性，我们提出了 PaperScout，这是一个将论文搜索重构为序列决策过程的自主代理。与静态工作流不同，PaperScout 基于累积的检索上下文，动态决策是否、何时以及如何调用搜索和扩展工具。然而，训练此类代理面临一个根本性挑战：通常为单轮任务设计的标准强化学习方法在应用于多轮代理任务时，会出现粒度不匹配的问题；其中，token级优化与序列级交互的粒度存在偏差，从而导致噪声信用分配。我们引入了近端序列策略优化，这是一种过程感知的序列级策略优化方法，能够将优化过程与代理-环境交互相对齐。在合成和真实世界基准上进行的综合实验表明，PaperScout 在召回率和相关性方面均显著优于强大的工作流驱动基线和强化学习基线，验证了我们自适应代理框架及优化策略的有效性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL",
                    "arxiv_id": "2601.10011",
                    "authors": "Zerui Yang, Weichuan Wang, Yanwei Xu, Linqi Song, Yudai Matsuda, Wei Han, Bo Bai",
                    "summary": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出的 Memo-SQL 框架涵盖了 LLM 智能体的核心要素：通过结构化分解策略进行**规划**，利用动态记忆存储历史成功和错误案例实现**记忆**，以及基于检索增强和经验反馈的**自我修正**（自我反思）。尽管应用于 NL2SQL 任务，但其核心贡献在于智能体的机制设计，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决现有NL2SQL系统在测试时扩展中面临的推理路径单一及自校正能力有限的问题。针对自然语言转SQL的场景，我们提出了一种名为Memo-SQL的训练无关框架，结合了结构化分解策略和基于历史错误-修正对的经验驱动自校正机制。在BIRD benchmark上，通过执行准确率（EX）验证了其有效性，达到了68.5%的SOTA水平，且计算开销显著降低。",
                    "summary_translation": "现有的 NL2SQL (自然语言转结构化查询语言) 系统面临两个关键局限：(1) 它们依赖于仅使用正确示例的 in-context learning (上下文学习)，忽视了历史 error-fix pairs (错误修复对) 中蕴含的丰富信号，而这些信号本可指导更鲁棒的 self-correction (自我修正)；(2) test-time scaling (测试时扩展) 方法往往对问题进行随意分解，导致在不同运行中生成近乎相同的 SQL candidates (SQL 候选语句)，从而削弱了 ensemble gains (集成增益)。此外，这些方法面临着显著的 accuracy-efficiency trade-off (精度-效率权衡)：高性能往往伴随着过大的计算开销，而追求速度的变体则会牺牲质量。我们提出了 Memo-SQL，这是一个 training-free (免训练) 框架，通过两个简单的思路解决上述问题：structured decomposition (结构化分解) 和 experience-aware self-correction (经验感知的自我修正)。我们不再让分解过程听凭运气，而是采用三种明确的策略——entity-wise (基于实体的)、hierarchical (分层的) 和 atomic sequential (原子顺序的)——来促进多样化的推理。在修正方面，我们构建了一个包含成功查询和历史 error-fix pairs (错误修复对) 的 dynamic memory (动态记忆)，并利用 retrieval-augmented prompting (检索增强提示) 在 inference time (推理时) 将相关示例引入上下文，无需 fine-tuning (微调) 或调用 external APIs (外部 API)。在 BIRD 数据集上，Memo-SQL 实现了 68.5% 的 execution accuracy (执行准确率)，在开源的 zero-fine-tuning (零微调) 方法中树立了新的 state of the art (SOTA，最先进水平)，同时其资源消耗比先前的 TTS (Test-Time Scaling，测试时扩展) 方法减少了 10 倍以上。",
                    "inspiration_trace": "基于论文《Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL》的内容，以下是对作者产出该核心方法的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“训练依赖”到“测试时扩展”的范式转移）**\n\n1.  **观察现状**：NL2SQL（自然语言转SQL）领域正经历范式转移。传统的依赖闭源API（如GPT-4）或全量微调的方法，虽然性能强，但成本高、隐私差且难以适应动态反馈。因此，学术界和工业界开始关注“测试时扩展”和“无训练”方法，即利用开源大模型，通过在推理阶段增加计算量来提升性能。\n2.  **发现痛点**：尽管TTS方法前景广阔，但作者发现现有方案存在两个根本性缺陷，导致其难以在实际中落地：\n    *   **缺陷一（效率与多样性的矛盾）**：现有的分治策略往往将问题分解交给LLM“随机”处理。这导致不同推理路径生成的SQL候选高度相似（同质化严重）。在进行集成投票时，相似的候选无法提供纠错信息，为了获得准确率，不得不生成海量候选，导致计算成本极高。\n    *   **缺陷二（纠错资源的浪费）**：现有的自纠错机制主要依赖静态的“正确示例”进行上下文学习。然而，在真实的BI系统中，历史错误及其修正过程往往比单纯的正确答案包含更多信息。现有方法忽略了这些“错误-修正”对的价值，导致模型难以从失败中学习，纠错能力受限。\n\n### 第二阶段：核心假设的提出\n**（从“随机暴力”到“结构化经验”的思维跃迁）**\n\n1.  **针对分解的假设**：如果放弃随机的分解，转而**显式地强制**模型沿着不同的语义逻辑路径进行思考，是否可以用极少的候选数（如3个）覆盖足够多的推理空间？\n    *   *推论*：只要候选之间足够“正交”（多样性高），就不需要通过暴力穷举来换取准确率，从而解决效率问题。\n2.  **针对纠错的假设**：如果构建一个动态的“经验库”，存储历史上遇到的错误、错误类型以及修正方案，并在推理时检索相关的“失败案例”作为提示，模型是否能像人类专家一样，通过类比历史错误来避免重蹈覆辙？\n    *   *推论*：这种“经验驱动”的纠错比单纯看“标准答案”更能指导模型处理未见过的复杂错误。\n\n### 第三阶段：方法论的设计与构建\n**（将假设转化为具体的技术路径）**\n\n1.  **实现“结构化分解”**：\n    *   为了确保多样性，作者不再让模型自由发挥，而是定义了三种互补的、覆盖SQL逻辑本质的分解策略：\n        *   **实体导向**：按涉及的表/实体分解（符合多表连接思维）。\n        *   **层级导向**：按嵌套逻辑分解（由内而外，符合子查询思维）。\n        *   **原子操作导向**：按操作序列分解（Select-Join-GroupBy顺序，符合执行流思维）。\n    *   *逻辑闭环*：这三种策略天然正交，保证了生成的SQL候选在结构上的差异性，为后续的集成投票提供了高质量的素材。\n\n2.  **实现“经验驱动自纠错”**：\n    *   **离线构建记忆**：作者设计了一个结构化的五元组 `<问题, 错误SQL, 正确SQL, 错误类型, 修正建议>`。这不仅仅是存储数据，更是对错误进行了分类学上的归纳。\n    *   **在线检索增强**：在推理阶段，当模型生成一个初步SQL后，系统去记忆库中检索结构相似的“历史失败案例”。\n    *   *逻辑闭环*：通过注入这些具体的“错误-修正”对，模型不再是盲目地自我反思，而是基于历史经验进行针对性的修补。\n\n3.  **整合与优化**：\n    *   为了进一步利用多样性，作者在生成阶段结合了三种SQL语法风格（CTE, Flat JOIN, Nested Subquery），与上述三种分解策略组合，形成9个候选。\n    *   最后通过自一致性投票选出最终结果。\n\n### 第四阶段：验证与价值确认\n**（回归实际应用的权衡）**\n\n1.  **验证假设**：在BIRD等基准数据集上的实验表明，Memo-SQL在无需微调的情况下达到了SOTA水平。\n2.  **确认优势**：更重要的是，由于采用了“结构化分解”而非“随机暴力搜索”，该方法在保持高准确率的同时，将计算资源消耗降低了10倍以上。\n3.  **最终结论**：作者证明了在NL2SQL任务中，**“有原则的结构化推理”**和**“对历史经验的利用”**比单纯的增加计算量或模型参数更有效。这为构建低成本、高可靠性的企业级BI系统提供了新的思路。\n\n---\n\n**总结：**\n作者的思考路径是从**现有TTS方法的低效和盲目**出发，敏锐地捕捉到**“推理路径同质化”**和**“忽视错误经验”**两个关键症结。通过引入**强制性的多视角分解策略**解决效率问题，通过引入**基于检索的错误修正记忆**解决鲁棒性问题，最终在“无训练”的约束下实现了性能与成本的最佳平衡。"
                },
                {
                    "title": "Continuum Memory Architectures for Long-Horizon LLM Agents",
                    "arxiv_id": "2601.09913",
                    "authors": "Joe Logan",
                    "summary": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了连续记忆架构（CMA），旨在解决LLM智能体在长视界任务中的记忆维护、状态更新和知识积累问题，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在解决RAG在长期LLM智能体中缺乏记忆动态的问题。针对需要长期记忆和上下文消歧的场景，我们提出了一种Continuum Memory Architecture (CMA)，通过持久存储、选择性保留和巩固维护内部状态，并在四个行为探针实验上通过GPT-4o评判验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL",
                    "arxiv_id": "2601.09883",
                    "authors": "Xinxing Ren, Quagmire Zang, Caelum Forder, Suman Deb, Ahsen Tahir, Roman J. Georgio, Peter Carroll, Zekun Guo",
                    "summary": "Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个基于LLM的多智能体系统（MAS），重点研究多智能体之间的通信（Agent-to-Agent Communication）和动态协作机制，属于研究范围中的“多智能体”类别，且不涉及被排除的纯应用、纯推理或特定领域优化。",
                    "summary2": "本文旨在解决现有基于工作流的Multi-Agent Systems依赖人工预定义规则且难以覆盖复杂任务状态空间的问题。针对通用任务场景，我们提出了一种通过Agent-to-Agent (A2A) Communication实现的Information-Flow-Orchestrated Multi-Agent Paradigm，利用专门的编排器动态协调智能体。在GAIA benchmark上，通过pass@1准确率验证了其有效性，达到63.64%，优于基线OWL。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation",
                    "arxiv_id": "2601.09771",
                    "authors": "Aradhya Dixit, Shreem Dixit",
                    "summary": "Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个包含两个智能体（用户倡导者和策略智能体）进行协商的系统，属于多智能体协作与通信的研究范畴，符合筛选条件。",
                    "summary2": "本文旨在解决LLM推荐系统难以可靠满足治理约束且缺乏可审计性的问题。针对MovieLens-100K数据集上的治理约束场景，我们提出了一种PCN-Rec框架，即基于代理的Proof-Carrying Negotiation方法。该方法通过User Advocate和Policy Agent谈判，结合Mediator LLM生成候选列表和证书，并利用确定性Verifier进行验证。实验表明，该方法在可行用户上实现了98.55%的通过率，且NDCG@10仅下降0.021，有效验证了其在保证合规性的同时维持了推荐效用。",
                    "summary_translation": "现代基于大语言模型（LLM）的推荐系统能够生成具有吸引力的排序列表，但在可靠满足治理约束（如最小长尾曝光或多样性要求）方面存在困难。我们提出了PCN-Rec，这是一种携带证明的协商管道，旨在将自然语言推理与确定性执行分离开来。基础推荐器（MF/CF，矩阵分解/协同过滤）生成一个大小为W的候选窗口，该窗口由两个代理进行协商：一个负责优化相关性的用户倡导者和一个负责执行约束的策略代理。一个中介大语言模型综合生成一个Top-N推荐列表以及一份结构化证书（JSON），该证书描述了声称的约束满足情况。确定性验证器根据推荐列表重新计算所有约束，并仅接受经过验证器检查的证书；如果验证失败，确定性约束贪婪修复算法将生成一个合规的推荐列表以供重新验证，从而产生可审计的追踪记录。在带有治理约束的MovieLens-100K数据集上，PCN-Rec在可行用户中实现了98.55%的通过率（n = 551, W = 80），而缺乏验证/修复机制的单步单LLM基线则表现较差；同时，PCN-Rec保持了效用，NDCG@10仅下降了0.021（0.403 vs. 0.424）；差异具有统计显著性（p < 0.05）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents",
                    "arxiv_id": "2601.09770",
                    "authors": "Chen Chen, Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu",
                    "summary": "Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了GUI-Eyes框架，专注于GUI智能体的主动视觉感知和工具使用（如裁剪、缩放），涉及智能体的决策制定与策略推理，属于单智能体研究中的工具使用范畴，符合筛选条件。",
                    "summary2": "本文旨在解决现有GUI代理依赖静态视觉输入、缺乏主动感知能力的问题。针对GUI自动化任务，我们提出了一种名为GUI-Eyes的强化学习框架，通过渐进式感知策略让模型自主决定何时及如何调用视觉工具（如裁剪或缩放）。我们在ScreenSpot-Pro benchmark上通过grounding accuracy验证了其有效性，结果显示GUI-Eyes-3B在仅用3k样本时达到44.8%准确率，显著优于基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities",
                    "arxiv_id": "2601.09822",
                    "authors": "Yongjian Tang, Thomas Runkler",
                    "summary": "Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究了基于LLM的多智能体系统，涵盖了智能体框架、通信协议和多智能体编排，这些内容直接符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在系统回顾LLM-based multi-agent systems在软件工程（SE）中的应用与挑战。针对软件开发生命周期（SDLC）全流程，我们提出了一种系统性的综述框架，分析了模型选择、agentic frameworks及通信协议。通过分析HumanEval、BugBench等SE benchmarks上的现有研究，我们验证了多智能体协作在解决复杂SE任务中的有效性，并指出了人机协调与成本优化的未来方向。",
                    "summary_translation": "尽管 Large Language Models (LLMs) (大型语言模型) 取得了最新进展，但复杂的 Software Engineering (SE) (软件工程) 任务仍需要更具协作性和专业性的方法。本概念性论文系统综述了 LLM-based multi-agent systems (基于大型语言模型的多智能体系统) 这一新兴范式，探讨了其在 Software Development Life Cycle (SDLC) (软件开发生命周期) 各阶段的应用，涵盖从 requirements engineering (需求工程) 和 code generation (代码生成)，到 static code checking (静态代码检查)、testing (测试) 及 debugging (调试) 等环节。我们深入探讨了广泛的主题，包括 language model selection (语言模型选择)、SE evaluation benchmarks (软件工程评估基准)、state-of-the-art (最先进的) agentic frameworks (智能体框架) 以及 communication protocols (通信协议)。此外，我们识别了关键挑战并概述了未来的研究机遇，重点关注 multi-agent orchestration (多智能体编排)、human-agent coordination (人机协同)、computational cost optimization (计算成本优化) 以及有效的 data collection (数据收集)。本研究旨在为 researchers and practitioners (研究人员和从业者) 提供关于 software engineering (软件工程) 领域内 agentic systems (智能体系统) 当前前沿格局的深刻见解。",
                    "inspiration_trace": "基于论文《LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities》，以下是对作者产出该文章核心思想的逻辑链推演。这一过程展现了从宏观行业观察到微观技术聚焦，再到系统性方法论构建的思考路径。\n\n---\n\n### 逻辑推演：从单体模型到多智能体协作的范式演进\n\n#### 第一阶段：宏观观察与问题定义\n**（观察现象：LLM的潜力与SE任务的复杂性错位）**\n\n*   **起点**：作者首先观察到大型语言模型在文本理解和推理方面展现出的卓越能力，学术界和工业界已尝试将其应用于软件工程（SE）的各个环节（如Prompt Engineering、RAG等）。\n*   **冲突识别**：尽管单体LLM表现优异，但现代软件工程任务具有极高的复杂性、长周期性和协作性。单一的、通用的LLM在面对端到端的复杂开发流程时，往往显得力不从心，缺乏专业深度和工具整合能力。\n*   **核心问题**：如何突破单体LLM的局限，以应对日益复杂的软件开发生命周期（SDLC）挑战？\n\n#### 第二阶段：核心假设与范式转移\n**（提出假设：从“单体智能”转向“群体协作”）**\n\n*   **类比思维**：作者将目光投向了人类软件开发的实际模式——即通过团队协作完成项目。人类团队包含产品经理、架构师、程序员、测试员等不同角色。\n*   **假设形成**：如果让LLM模拟这种社会分工，构建一个**多智能体系统**，是否能解决单体模型的问题？\n*   **理论支撑**：作者论证了多智能体系统的五大优势：\n    1.  **专业化**：每个Agent专注特定任务（如一个只写代码，一个只测试）。\n    2.  **模块化**：独立升级，互不干扰。\n    3.  **协作性**：多视角碰撞产生更优解。\n    4.  **工具使用**：更高效调用外部资源。\n    5.  **并行性**：加速开发流程。\n\n#### 第三阶段：实证验证与场景映射\n**（文献回顾：验证假设在SDLC各阶段的可行性）**\n\n*   **验证逻辑**：为了验证上述假设，作者系统性地回顾了现有研究，观察多智能体系统是否已经渗透到SDLC的各个阶段。\n*   **场景扫描**：\n    *   **需求工程**：发现MARE等系统通过多Agent协作生成和优先级排序需求，验证了“协作”在模糊需求处理中的有效性。\n    *   **代码生成**：观察到PairCoder（导航员+驾驶员）等角色分工模式，验证了“规划与执行分离”的优越性。\n    *   **静态检查与测试**：发现GPTLENS（审计员+评论员）等模拟代码审查流程的系统，验证了“对抗性协作”能提高代码质量。\n    *   **调试**：验证了基于反馈循环的Agent在故障定位中的优势。\n*   **结论**：文献证实，多智能体范式并非空想，而是已在SDLC各环节展现出超越单体模型的潜力，但目前的解决方案是碎片化的。\n\n#### 第四阶段：方法论构建与基础设施\n**（系统整合：如何构建和评估这些系统）**\n\n*   **问题升级**：既然多智能体系统有效，那么如何系统地构建它们？需要什么样的基础设施？\n*   **方法论框架**：作者提出了构建此类系统的三大支柱：\n    1.  **模型选择策略**：在性能（闭源SOTA）、隐私/成本（开源）和推理能力（推理优化模型）之间寻找平衡点。\n    2.  **评估基准**：强调不能仅用通用数据集，需引入SE特定基准（如HumanEval, BugBench）来衡量实际效果。\n    3.  **框架与协议**：指出需要标准化的框架（如LangGraph, AutoGen）和通信协议（如MCP）来降低开发门槛，解决Agent间的互操作性问题。\n\n#### 第五阶段：批判性反思与未来展望\n**（挑战识别：从“能用”到“好用”的鸿沟）**\n\n*   **深度审视**：作者跳出技术细节，审视当前研究的前沿边界，识别出阻碍多智能体系统大规模落地的关键痛点。\n*   **关键挑战**：\n    *   **能力深度**：目前的Agent多为通用角色（如“程序员”），缺乏针对特定安全审计或架构设计的深度领域知识。\n    *   **人机协同**：如何让人类有效地介入并指挥Agent团队，而非被取代。\n    *   **数据孤岛**：现有训练多基于代码，缺乏设计文档、讨论记录等全SDLC数据。\n    *   **成本与评估**：多Agent推理成本高昂，且缺乏针对“协作能力”本身的评估基准。\n*   **最终愿景**：文章最终指向一个未来的研究方向——不仅仅是自动化单个任务，而是构建一个**高度协同、人类在环、成本可控且具备专业深度的智能软件工程生态系统**。\n\n---\n\n**总结**：\n作者的思考路径遵循了**“观察现象（LLM能力） -> 发现瓶颈（SE复杂性） -> 提出范式（多智能体协作） -> 验证场景（SDLC全流程回顾） -> 构建体系（模型/框架/评估） -> 展望未来（挑战与机遇）”**的严密逻辑链条。这篇文章本质上是对LLM在软件工程领域应用方式的一次从“单兵作战”到“集团军作战”的系统性战略升级思考。"
                },
                {
                    "title": "Investigating Tool-Memory Conflicts in Tool-Augmented LLMs",
                    "arxiv_id": "2601.09760",
                    "authors": "Jiali Cheng, Rui Pan, Hadi Amiri",
                    "summary": "Tool-augmented large language models (LLMs) have powered many applications. However, they are likely to suffer from knowledge conflict. In this paper, we propose a new type of knowledge conflict -- Tool-Memory Conflict (TMC), where the internal parametric knowledge contradicts with the external tool knowledge for tool-augmented LLMs. We find that existing LLMs, though powerful, suffer from TMC, especially on STEM-related tasks. We also uncover that under different conditions, tool knowledge and parametric knowledge may be prioritized differently. We then evaluate existing conflict resolving techniques, including prompting-based and RAG-based methods. Results show that none of these approaches can effectively resolve tool-memory conflicts.",
                    "category": "cs.AI",
                    "filter_reason": "论文研究了工具增强型LLM中的“工具-记忆冲突”，这直接涉及LLM智能体的核心能力——工具使用和记忆机制。它探讨了智能体在调用外部工具与依赖内部知识时的冲突问题，属于单智能体研究范畴。",
                    "summary2": "本文旨在解决工具增强型LLMs中内部参数化知识与外部工具输出冲突的问题。针对多种LLMs及STEM等任务场景，我们提出了Tool-Memory Conflict (TMC)概念，并评估了基于提示和RAG的冲突解决技术。在MMLU、GSM8K等数据集上，通过冲突率和准确率验证了TMC的普遍性，发现现有方法无法有效解决该问题。",
                    "summary_translation": "工具增强型大语言模型 (LLMs) 已为众多应用提供了支持。然而，它们往往面临知识冲突的问题。在本文中，我们提出了一种新型的知识冲突——工具-记忆冲突 (TMC)，即工具增强型 LLMs 的内部参数知识与外部工具知识之间存在矛盾。我们发现，尽管现有的 LLMs 功能强大，但仍受到 TMC 的影响，尤其是在科学、技术、工程和数学 (STEM) 相关任务中。我们还揭示，在不同条件下，工具知识和参数知识可能被赋予不同的优先级。随后，我们评估了现有的冲突解决技术，包括基于提示的和基于检索增强生成 (RAG) 的方法。结果表明，这些方法均无法有效解决工具-记忆冲突。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation",
                    "arxiv_id": "2601.09749",
                    "authors": "Suriya Sureshkumar",
                    "summary": "Large Action Models (LAMs) extend large language models by enabling autonomous decision-making and tool execution, making them promising for automating scientific workflows. However, scientific workflows impose strict requirements on reproducibility, auditability, and deterministic execution, which are not satisfied by generic LLM-based agents. Unconstrained action generation can lead to silent state changes, non-deterministic executions, and irreproducible experimental results, limiting the applicability of LAMs in scientific settings. In this paper, we propose R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation. R-LAM introduces structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure that every action and intermediate artifact is auditable and replayable. The framework supports failure-aware execution loops and controlled workflow forking, enabling iterative experimentation without compromising reproducibility. We implement R-LAM as a lightweight Python framework and release it as an open-source PyPI package to facilitate reproducible research. An experimental evaluation of representative scientific workflows demonstrates that R-LAM improves reproducibility success rates and execution reliability compared to unconstrained LLM-based agents, while retaining adaptive control over workflow execution.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了R-LAM框架，专注于大型行动模型（智能体）的自主决策、工具执行和故障感知执行循环（自我反思），旨在解决智能体在科学工作流中的可复现性问题，属于单智能体研究范畴。",
                    "summary2": "本文旨在解决 Large Action Models 在科学工作流自动化中缺乏可复现性和确定性的问题。针对科学实验对审计和重放的需求，我们提出了一种 R-LAM 框架，引入结构化 Action Schema 和 Provenance-aware Trace Graph。在 Breast Cancer Wisconsin 数据集的机器学习工作流上，通过 Reproducibility Success 和 Trace Completeness 等指标验证了其有效性。",
                    "summary_translation": "**摘要**\n\n大型动作模型通过实现自主决策和工具执行扩展了大型语言模型，使其在自动化科学工作流方面展现出巨大潜力。然而，科学工作流对可复现性、可审计性和确定性执行有着严格要求，而通用的基于 LLM 的智能体无法满足这些要求。不受约束的动作生成可能导致静默状态改变、非确定性执行以及不可复现的实验结果，从而限制了 LAMs 在科学环境中的应用。在本文中，我们提出了 R-LAM，这是一个用于将大型动作模型应用于科学工作流自动化的可复现性约束框架。R-LAM 引入了结构化动作模式、确定性执行策略和显式溯源跟踪，以确保每一个动作和中间产物都是可审计且可重放的。该框架支持故障感知执行循环和受控工作流分叉，从而在不牺牲可复现性的前提下实现迭代实验。我们将 R-LAM 实现为一个轻量级 Python 框架，并将其作为开源 PyPI 包发布，以促进可复现性研究。对代表性科学工作流的实验评估表明，与不受约束的基于 LLM 的智能体相比，R-LAM 提高了可复现性成功率和执行可靠性，同时保持了对工作流执行的自适应控制。",
                    "inspiration_trace": "基于论文《R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation》，以下是对作者产出该文章核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察：技术趋势与领域需求的错位\n**思考起点：** 作者首先观察到了当前AI领域的一个显著趋势——大语言模型（LLMs）正在向“大动作模型”演进。LAMs不仅能推理，还能自主执行代码、调用API，展现出强大的自动化潜力。\n**现实冲突：** 与此同时，作者将目光投向科学计算领域，发现该领域有着与通用自动化截然不同的核心价值观：**可复现性、可审计性和确定性**。科学实验被视为一种“第一类公民”的产物，必须能被独立验证。\n**初步判断：** LAMs的“黑盒”概率特性与科学工作流的“白盒”严谨要求之间存在根本性的错位。直接将通用的LAMs应用于科学工作流是危险且不可行的。\n\n### 2. 问题聚焦：现有解决方案的二元对立\n**深入分析：** 作者进一步审视了现有的技术生态，发现它们处于两个极端，无法兼顾：\n*   **传统工作流管理系统（WMS）：** 如Snakemake等，强调确定性和可复现性，但流程是静态的、僵化的，缺乏面对突发情况时的自适应能力。\n*   **朴素LLM智能体：** 具备强大的自适应控制和动态规划能力，但其执行过程是随机的、隐式的，缺乏对状态变更的记录，导致结果不可复现。\n**核心矛盾提炼：** 现有的技术范式迫使我们在“智能的适应性”与“严谨的可复现性”之间做二选一。作者的核心问题由此确立：**能否构建一个既保留LAMs自适应优势，又满足科学级可复现性要求的系统？**\n\n### 3. 关键假设：解耦推理与执行\n**逻辑转折：** 作者意识到，要解决上述矛盾，不能试图改变LLM内部的概率生成机制（因为那是其智能的来源），而应该在外部构建一层“约束”。\n**核心假设：** 如果将“意图生成”（LLM的推理）与“动作执行”（实际的环境交互）彻底解耦，并在执行层面强制施加严格的约束，那么就能在不牺牲模型智能的前提下，保证系统的科学严谨性。\n**设计哲学：** 将“可复现性”不再视为一个事后补充的属性，而是作为系统运行的一个**不可变约束**。\n\n### 4. 方法论构建：R-LAM框架的四个支柱\n基于上述假设，作者构建了R-LAM框架，其思想演进遵循以下逻辑步骤：\n\n*   **第一步：结构化动作定义**\n    *   *思考：* 要审计执行过程，首先必须让动作“可见”且“不可变”。\n    *   *方案：* 提出形式化的动作模式，将动作定义为包含输入、输出、前置条件、效果等元数据的结构化对象。这消除了隐式行为，让每一个意图都显式化。\n\n*   **第二步：确定性执行引擎**\n    *   *思考：* LLM生成的动作可能是随机的或危险的，需要一个“守门人”。\n    *   *方案：* 引入中介层。引擎不直接执行LLM的指令，而是先验证模式、检查前置条件、隔离环境。它充当“可复现性防火墙”，确保无论LLM如何“胡思乱想”，实际发生的物理/计算行为都是受控的。\n\n*   **第三步：全量溯源追踪**\n    *   *思考：* 科学实验要求每一步都可回溯。如果动作没有被记录，就等于没发生。\n    *   *方案：* 建立有向无环图（DAG）形式的执行追踪。确立一个系统不变量：“未记录的动作即无效”。这保证了所有中间状态和依赖关系都被完整捕获。\n\n*   **第四步：支持探索的回放与分叉**\n    *   *思考：* 科学研究需要试错。如果只强调确定性，是否会扼杀探索性？\n    *   *方案：* 利用DAG追踪图，设计“重放”和“分叉”机制。允许研究者基于历史记录的某个节点进行参数修改（分叉），而无需重新运行整个流程。这巧妙地解决了“确定性复现”与“探索性实验”的共存问题。\n\n### 5. 验证逻辑：证明约束的有效性\n**实验设计思路：** 作者的验证逻辑不是为了证明R-LAM“更聪明”，而是为了证明它“更可靠且不失灵活性”。\n*   **对比维度：** 设置了静态脚本（无智能）、朴素LAM（无约束）、R-LAM（有约束）三组对比。\n*   **核心指标：** 聚焦于重放正确性、追踪完整性和故障可见性。\n*   **结论导向：** 实验结果旨在展示，引入R-LAM的约束层后，系统在获得科学级可复现性的同时，并没有丢失LAMs原本的自适应控制能力。\n\n### 总结\n作者的思考路径是从**技术趋势的观察**出发，识别出**通用AI与科学严谨性之间的鸿沟**，通过**解耦推理与执行**的关键假设，设计了一套**以结构化动作和确定性引擎为核心的约束系统**，最终通过实验验证了**在保证可复现性的前提下保留AI适应性**的可行性。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 10,
            "papers": [
                {
                    "title": "Grounding Agent Memory in Contextual Intent",
                    "arxiv_id": "2601.10702",
                    "authors": "Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han",
                    "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history. For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了 STITCH，一种用于长视界、目标导向交互的智能体记忆系统。它专注于通过上下文意图来改进智能体的记忆检索机制，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "总结生成失败",
                    "summary_translation": "在长视距、面向目标的交互中部署大型语言模型仍然充满挑战，因为相似的实体和事实会在不同的潜在目标和约束下重复出现，导致记忆系统检索到上下文不匹配的证据。我们提出了 STITCH（Structured Intent Tracking in Contextual History，上下文历史中的结构化意图追踪），这是一个智能体记忆系统，它利用结构化检索线索——即上下文意图——对每个轨迹步骤进行索引，并通过匹配当前步骤的意图来检索历史记录。上下文意图提供了紧凑的信号，用于消除重复提及的歧义并减少干扰，这些信号包括：(1) 定义主题片段的当前潜在目标，(2) 动作类型，以及 (3) 锚定相关属性的显著实体类型。在推理过程中，STITCH 根据意图兼容性对记忆片段进行过滤和优先级排序，从而抑制那些语义相似但上下文不兼容的历史记录。为了进行评估，我们引入了 CAME-Bench，这是一个用于在现实、动态、面向目标的轨迹中进行上下文感知检索的基准。在 CAME-Bench 和 LongMemEval 上，STITCH 实现了最先进的性能，比最强的基线高出 35.6%，且随着轨迹长度的增加，性能提升最为显著。我们的分析表明，意图索引显著减少了检索噪声，支持利用意图感知记忆来实现稳健的长视距推理。",
                    "inspiration_trace": "基于论文《Grounding Agent Memory in Contextual Intent》，以下是对作者产出该核心方法（STITCH）逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 第一阶段：宏观问题识别——从“记忆容量”到“记忆干扰”的范式转移\n\n**1. 观察现象：长视距任务中的“语义陷阱”**\n作者首先观察到，尽管现有的LLM智能体在长文本处理上取得了进展，但在长视距、目标导向的任务中（如多日旅行规划、长期研究），表现依然脆弱。\n*   **核心痛点：** 传统的记忆系统（如RAG、摘要、知识图谱）主要依赖**语义相似度**或**时间邻近性**来检索信息。\n*   **问题本质：** 在真实场景中，相同的实体和事实会在不同的潜在目标和约束下反复出现。例如，“酒店A的价格”在第一天和第二天可能不同，或者“预订它”指代的对象随对话上下文变化。\n*   **推论：** 现有方法的失败不是因为“记不住”，而是因为“记混了”。单纯依靠语义匹配无法区分**上下文不兼容**的证据，导致了严重的记忆干扰。\n\n### 第二阶段：理论假设引入——认知科学中的“意图索引”\n\n**2. 寻找灵感：人类如何处理长时记忆？**\n为了解决“上下文混淆”的问题，作者跳出纯工程视角，转向认知科学，特别是**事件结构理论**。\n*   **理论洞察：** 人类理解连续活动时，并不是将其视为扁平的文本流，而是通过构建事件表征来支持预测和记忆。关键在于两个维度：\n    *   **部分学：** 将行为划分为具有连贯目标上下文的片段（即“主题”）。\n    *   **分类学：** 识别跨不同语境重复出现的动作类别（即“事件类型”）。\n*   **提出假设：** 如果能让智能体的记忆系统像人类一样，不仅存储“发生了什么”，还显式索引“**为什么发生**（潜在意图）”，就能有效消除歧义，抑制语义相似但上下文错误的历史信息。\n\n### 第三阶段：方法论构建——定义“上下文意图”\n\n**3. 核心概念抽象：Contextual Intent**\n基于上述假设，作者提出构建一个结构化的检索线索——**上下文意图**。为了使其具有领域通用性（无需预定义本体），作者将其解构为三个在线诱导的动态组件：\n\n*   **维度一：主题范围**\n    *   *思考：* 长任务通常由多个子目标组成（如“第一天行程”、“模型优化”）。\n    *   *作用：* 这是一个粗粒度的容器，用于链接非相邻但目标一致的步骤。它解决了“跨片段推理”和“状态追踪”的问题，防止模型在处理当前目标时被其他无关的历史片段干扰。\n\n*   **维度二：事件类型**\n    *   *思考：* 无论在哪个主题下，某些操作是通用的（如“搜索”、“比较”、“预订”）。\n    *   *作用：* 这是一个细粒度的动作标签，用于区分具体的操作性质。它帮助模型在面对重复提及时，通过功能角色进行锚定。\n\n*   **维度三：关键实体类型**\n    *   *思考：* 在特定意图下，只有特定属性是重要的（如预订时关注“价格”，评价时关注“评分”）。\n    *   *作用：* 这是一个轻量级的模式模板，用于锚定哪些细节在当前上下文中是相关的，避免被无关的表面特征误导。\n\n**4. 机制设计：结构化对齐优于语义相似度**\n*   **存储阶段：** 在将每一步存入记忆时，不仅存储原始内容，还通过上述三个维度生成结构化的索引。\n*   **检索阶段：** 当面对查询时，首先解析查询的意图结构，然后通过**标签密度排序**优先检索结构上匹配的记忆片段，最后才使用语义相似度作为次要排序依据。\n*   **逻辑闭环：** 这种“先结构过滤，后语义匹配”的策略，从根本上解决了长上下文中的“迷失中间”现象和实体歧义问题。\n\n### 第四阶段：验证与评估——构建“上下文感知”的试金石\n\n**5. 评估维度的反思：现有基准的局限性**\n作者意识到，现有的长上下文基准（如LongBench等）往往将轨迹分割为独立的块，或者强制严格的轮流对话，这使得模型可以利用局部邻近性作弊，掩盖了在真实、交错、非局部依赖场景下的无能。\n\n**6. 构建CAME-Bench：**\n为了验证STITCH的核心假设（即意图索引能解决上下文干扰），作者构建了一个新的基准：\n*   **设计原则：** 强制高密度的语义干扰。即相同的实体必须在不同的潜在目标下被反复使用。\n*   **测试目标：** 专门测试模型在交错、非轮流对话结构中，能否检索到“正确语境下的正确事实”。\n\n---\n\n### 总结：逻辑演进的全景图\n\n1.  **起点：** 发现长视距任务中，语义检索无法区分重复实体在不同目标下的含义（**干扰问题**）。\n2.  **转折：** 引入认知科学理论，提出记忆应基于“潜在意图”进行组织（**意图假设**）。\n3.  **方案：** 将意图解构为“主题范围”、“事件类型”和“关键实体类型”三维结构，并以此作为记忆索引的核心（**STITCH方法**）。\n4.  **验证：** 构建高干扰的基准测试，证明结构化意图匹配在长轨迹中优于纯语义匹配（**CAME-Bench**）。"
                },
                {
                    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
                    "arxiv_id": "2601.10355",
                    "authors": "Zhihao Xu, Rumei Li, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xunliang Cai, Xiting Wang",
                    "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
                    "category": "cs.CL",
                    "filter_reason": "论文专注于通过从文本语料库合成数据来增强LLM的多轮工具使用能力，明确指出这对构建自主智能体至关重要。这属于“单智能体：工具使用”的研究范畴。",
                    "summary2": "本文旨在解决高质量多轮工具使用数据获取困难的问题。针对大规模文本语料库，我们提出了一种名为GEM的数据合成管道，通过四阶段过程直接从文本中提取并生成多轮工具使用轨迹。我们在BFCL V3 Multi-turn和$\\tau^2$-bench上通过Accuracy、Avg@4和Pass@4指标验证了其有效性，模型性能显著超越基线。",
                    "summary_translation": "使大型语言模型在多轮交互中有效利用工具，是构建具备能力的自主代理的关键。然而，获取多样化且逼真的多轮工具使用数据仍是一项重大挑战。在本研究中，我们提出了一种新颖的基于文本的范式。我们观察到，文本语料库天然包含丰富的多步问题解决经验，这可以作为多轮工具使用任务的一种尚未开发、可扩展且真实的数据源。基于此见解，我们介绍了 GEM，这是一个数据合成管道，能够通过四个阶段的过程从文本语料库中生成并提取多轮工具使用轨迹：相关性过滤、工作流与工具提取、轨迹落地以及复杂性细化。为了降低计算成本，我们通过监督微调进一步训练了一个专门的轨迹合成器。该模型将复杂的生成流程提炼为一个高效的端到端轨迹生成器。实验表明，我们的 GEM-32B 模型在 BFCL V3 多轮基准测试上实现了 16.5% 的性能提升。我们的模型在部分任务上超越了在 τ - bench（Airline 和 Retail）域内数据上训练的模型性能，凸显了源于我们基于文本的合成范式的卓越泛化能力。值得注意的是，我们的轨迹合成器在显著降低推理延迟和成本的同时，达到了与完整流程相媲美的质量。",
                    "inspiration_trace": "基于论文《Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考演进过程。\n\n---\n\n### 1. 宏观问题与核心瓶颈\n**思考起点：** 构建通用人工智能（AGI）的关键在于让智能体具备在复杂环境中动态使用工具的能力。\n**现实困境：** 尽管大语言模型（LLMs）潜力巨大，但在现实世界的多轮交互中表现不佳（如处理模糊指令、长上下文依赖、错误恢复等）。\n**根本原因：** 训练数据的稀缺。高质量、多样化、真实的多轮工具使用轨迹在现实场景中极难获取。这是制约智能体发展的核心瓶颈。\n\n### 2. 对现有范式的审视与批判\n**现有路径：** 目前的主流研究采用“以工具为中心的模拟范式”。\n**逻辑推演：**\n*   **做法：** 先预定义一组API工具，然后基于这些工具合成用户任务并模拟交互。\n*   **局限性分析：**\n    1.  **获取成本高：** 收集足够全面且多样化的工具集本身就很昂贵且困难。\n    2.  **数据分布受限：** 生成的数据受限于预定义API的范围，导致模型只能“学会”已定义的工具，难以泛化到未见过的环境。\n    3.  **泛化悖论：** 智能体训练的目标是接触足够广泛的场景以实现泛化，但现有方法却将其限制在特定的工具集中。\n\n**关键提问：** 能否绕过对预定义工具的依赖，直接从真实世界中合成更多样、更高质量的轨迹？\n\n### 3. 观察洞察与范式转移\n**观察对象：** 用于预训练LLMs的大规模文本语料库（如Ultra-FineWeb）。\n**核心洞察：** 文本不仅仅是静态的知识，它蕴含了丰富的“隐性经验”。\n*   **解构文本：** 作者发现非结构化文本中天然包含了构建智能体轨迹所需的三个核心要素：\n    1.  **用户查询：** 文本中陈述的目标或问题。\n    2.  **环境工具：** 嵌入在解释或说明中的功能描述或API逻辑。\n    3.  **多步工作流：** 逐步的操作程序或叙述性逻辑。\n**假设提出：** 如果能从文本中提取这些要素并将其转化为结构化的工具调用轨迹，就能解锁一个未被开发的、可扩展的、真实的训练数据源。\n**范式确立：** 从“基于预定义工具的模拟”转向“基于文本的提取范式”。\n\n### 4. 方法论构建：从非结构化到结构化\n**挑战：** 如何将一段描述操作流程的文本，转化为标准化的多轮工具使用轨迹？\n**逻辑拆解（GEM Pipeline的设计）：**\n1.  **筛选：** 并非所有文本都有用。首先需要过滤掉不包含多步操作流程的文本，保留富含程序性知识的片段。\n2.  **提取：** 将文本中的自然语言描述“翻译”为机器可理解的逻辑。\n    *   提取抽象的工作流。\n    *   根据文本描述设计对应的API工具定义（OpenAI Schema格式）。\n3.  **生成：** 利用强模型（如GLM-4.6）扮演用户和助手，基于上述工具和工作流，生成具体的对话轨迹。这一步将抽象逻辑具象化为交互。\n4.  **精炼：** 初步生成的轨迹可能过于简单。为了模拟真实世界的复杂性，需要主动增加难度，如引入错误恢复、澄清模糊指令、增加约束条件等。\n\n### 5. 效率优化与知识蒸馏\n**新问题：** 上述Pipeline虽然有效，但依赖强模型进行多步生成，计算成本高昂，难以大规模扩展。\n**解决思路：** 能否将这个复杂的生成过程“压缩”到一个模型中？\n**方案设计：** 训练一个专门的“轨迹合成器”。\n*   **逻辑：** 通过监督微调（SFT），让模型学习从“输入文本”直接端到端映射到“输出轨迹”的能力。\n*   **目的：** 在保持生成质量的同时，大幅降低推理延迟和成本，实现低成本的大规模数据合成。\n\n### 6. 验证与价值闭环\n**预期结果：** 由于数据来源于真实世界的多样化文本，训练出的模型应具备更强的泛化能力。\n**实验验证：**\n*   在BFCL V3等基准测试中，模型性能显著提升。\n*   **关键证据：** 使用“域外文本”合成的数据训练模型，其性能在$\\tau$-bench上可以媲美甚至超过使用“域内真实API数据”训练的模型。\n**结论：** 证明了“文本即轨迹”范式的有效性，即利用开放世界的文本知识可以显著提升智能体的工具使用泛化能力。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现问题（数据稀缺） -> 批判现状（工具模拟受限） -> 转换视角（挖掘文本隐性经验） -> 构建流程（文本转轨迹） -> 优化落地（模型蒸馏）”** 的完整逻辑闭环。其核心创新在于将数据来源从“人造的工具环境”切换到了“真实的文本世界”，从而解决了数据多样性和泛化性的根本难题。"
                },
                {
                    "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding",
                    "arxiv_id": "2601.10343",
                    "authors": "Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui",
                    "summary": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确关注“Agentic Coding”（智能体编码）和“software agents”（软件代理），研究智能体在代码库环境中的脚手架感知指令遵循能力，涉及智能体的轨迹分析和任务执行，属于单智能体研究范畴。",
                    "summary2": "本文旨在解决agentic coding中缺乏对异构、持久指令遵循评估的问题。针对repository-grounded coding场景，我们提出了OctoBench benchmark及配套的自动化观察与评分工具包，通过将执行轨迹映射到客观检查项来解耦任务解决与规则遵循。我们在8个代表性模型上通过Instance Success Rate (ISR)和Check item Success Rate (CSR)验证了其有效性，揭示了模型在长视距执行中的脆弱性。",
                    "summary_translation": "现代编码脚手架将大语言模型转化为能力强大的软件代理，但其遵循脚手架指定指令的能力尚未得到充分研究，特别是在约束条件具有异构性且在多次交互中持续存在的情况下。为填补这一空白，我们介绍了 OctoBench，这是一个针对基于仓库的代理编码中脚手架感知指令遵循能力的基准测试。OctoBench 包含 34 个环境和 217 项任务，这些任务在三种脚手架类型下实例化，并配备了 7,098 个客观检查清单项。为了将任务解决与规则遵循区分开来，我们提供了一个自动化观察与评分工具包，用于捕获完整轨迹并执行细粒度检查。在八个代表性模型上进行的实验揭示了任务解决与脚手架感知合规性之间存在系统性差距，这凸显了针对异构指令遵循进行显式训练和评估的必要性。我们发布了该基准以支持可复现的基准测试，并加速更具脚手架感知能力的编码代理的开发。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns",
                    "arxiv_id": "2601.10198",
                    "authors": "Xintao Wang, Jian Yang, Weiyuan Li, Rui Xie, Jen-tse Huang, Jun Gao, Shuai Huang, Yueping Kang, Liyuan Gou, Hongwei Feng, Yanghua Xiao",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning and generation, serving as the foundation for advanced persona simulation and Role-Playing Language Agents (RPLAs). However, achieving authentic alignment with human cognitive and behavioral patterns remains a critical challenge for these agents. We present HUMANLLM, a framework treating psychological patterns as interacting causal forces. We construct 244 patterns from ~12,000 academic papers and synthesize 11,359 scenarios where 2-5 patterns reinforce, conflict, or modulate each other, with multi-turn conversations expressing inner thoughts, actions, and dialogue. Our dual-level checklists evaluate both individual pattern fidelity and emergent multi-pattern dynamics, achieving strong human alignment (r=0.91) while revealing that holistic metrics conflate simulation accuracy with social desirability. HUMANLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4x fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling--simulating not just what humans do, but the psychological processes generating those behaviors.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“角色扮演语言智能体”，旨在通过模拟内心想法、动作和认知模式来增强智能体的拟人化能力，属于单智能体范畴中的认知建模与自我反思研究。",
                    "summary2": "本文旨在解决现有角色扮演语言模型缺乏真实人类认知对齐的问题。针对多模式动态交互的场景，我们提出了一种HUMAN LLM框架，将心理模式视为相互作用的因果力量，并构建了包含244种模式和11,359个场景的数据集。我们在自建的IPE和MPD指标及LifeChoice等benchmark上验证了其有效性，结果显示HUMAN LLM-8B在多模式动态上优于Qwen3-32B。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n---\n\n### 1. 宏观观察：从“形似”到“神似”的瓶颈\n**思考起点：**\n作者首先观察到，尽管大语言模型（LLMs）在角色扮演和智能体模拟方面取得了显著进展，能够模仿特定人物的语气或风格，但这种模拟往往停留在**浅层的行为模仿**。\n**核心痛点：**\n现有的角色扮演智能体缺乏“心理一致性”。它们可能知道一个角色是“外向的”，但在具体情境下无法表现出人类复杂的心理活动。作者将这种差距定义为**“心理对齐”**的缺失——即模型未能真正模拟产生人类行为的内在认知过程。\n\n### 2. 根因诊断：“孤立标签”的谬误\n**深入分析：**\n为什么模型做不到“神似”？作者审视了现有的主流方法（提示词工程、微调、激活引导），发现它们都基于一个共同的假设：**人格是静态的、孤立的标签**。\n*   例如：将“外向”简单映射为“话多”，将“顺从”映射为“合作”。\n**关键洞察：**\n真实的人类行为并非由单一特质决定，而是**多种认知模式动态交互**的结果。\n*   *反例思考：* 一个“外向”的人，在“聚光灯效应”下可能会变得沉默；一个“自信”的人，在“从众压力”下可能会妥协。\n**结论：**\n现有方法导致了“人格幻觉”，即模型声称拥有某种特质，但在复杂情境下行为不一致。问题的根源在于将认知模式视为静态标签，而非**相互作用的因果力量**。\n\n### 3. 理论重构：引入场论与动态交互\n**理论寻找：**\n为了解决动态交互的问题，作者引入了库尔t·勒温的**场论**：行为 $B$ 是人 $P$ 与环境 $E$ 的函数，即 $B = f(P, E)$。\n**维度拆解：**\n基于此理论，作者将人类认知拆解为两个互补的维度，构建了新的理论框架：\n1.  **人格特质（Person）：** 稳定的个体特征（如大五人格）。\n2.  **社会认知模式：** 由情境触发的心理机制（如认知偏差、社会影响、进化心理机制）。\n**核心假设：**\n如果能让模型学习到这些模式在不同情境下的**强化、冲突或调节**关系，模型就能隐式地学会多模式动态学，从而实现更真实的拟人化。\n\n### 4. 数据策略：科学严谨性与情境合成\n**如何实现上述假设？**\n作者意识到，不能仅依赖模型已有的参数知识，必须构建一个具有心理学严谨性的数据集。\n**步骤一：模式定义的科学化**\n*   不使用刻板印象，而是从约12,000篇心理学论文中提取244种模式（100种人格特质 + 144种社会认知模式）。\n*   每种模式都包含：定义、核心机制、现实世界表现。确保数据源头的科学性。\n\n**步骤二：构建“冲突”与“交互”场景**\n*   为了打破“孤立标签”，作者合成了11,359个场景，每个场景包含2-5个模式。\n*   **关键设计：** 刻意设计模式之间的交互关系（例如：“自我服务偏差”强化“过度自信”，或者“聚光灯效应”抑制“健谈”）。\n*   **表达维度：** 在对话生成中，强制要求包含**内心独白**、**肢体动作**和**口头对话**。内心独白是连接认知过程与外部行为的关键桥梁，让模型学会“像人一样思考”，而不仅仅是“像人一样说话”。\n\n### 5. 评估革新：解耦“准确性”与“社会期许性”\n**新的挑战：**\n如何评估这种复杂的心理模拟？作者发现传统的整体性评估指标存在严重缺陷。\n**发现“规范性混淆”：**\n现有的评估方法（如CoSER的Anthropomorphism分数）往往将“好的角色扮演”等同于“符合社会道德的行为”。\n*   *案例思考：* 如果一个角色真实地模拟了“归因偏差”（表现出防御性和推卸责任），传统的LLM评判者会因为其“缺乏同理心”而给低分，尽管这在心理学上是准确的。\n**解决方案：双重检查清单**\n为了解决这个问题，作者提出了价值中立的评估框架：\n1.  **模式级检查清单：** 检查是否体现了特定心理模式的定义（如“是否高估了他人对自己的关注？”），不评价好坏。\n2.  **场景级检查清单：** 检查在特定多模式配置下，行为倾向是否符合预期（如“在聚光灯效应下，虽然自信但内心焦虑”）。\n**目的：**\n将**模拟准确性**与**社会期许性**解耦，确保模型是在模拟真实的人类心理，哪怕这种心理是负面的或非理性的。\n\n### 6. 方法论形成：HUMAN LLM 框架\n**最终闭环：**\n将上述思考整合，形成了HUMAN LLM框架：\n*   **输入：** 基于心理学文献构建的、包含多模式交互的合成对话数据。\n*   **训练：** 通过监督微调（SFT），让模型在保持通用能力的同时，学习认知模式的动态表达。\n*   **验证：** 使用双重检查清单进行评估，证明模型在多模式动态（MPD）上的表现超越了参数更大的基线模型（如Qwen3-32B）。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有角色扮演“有形无神”**出发，通过**批判“孤立标签”思维**，引入**心理学场论**作为理论指导，进而构建**基于科学文献的交互式数据集**，并最终通过**解耦社会道德偏见的评估体系**，完成了从“行为模仿”到“认知建模”的范式转变。"
                },
                {
                    "title": "Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends",
                    "arxiv_id": "2601.10122",
                    "authors": "Ye Wang, Jiaxing Chen, Hongjiang Xiao",
                    "summary": "In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.",
                    "category": "cs.CL",
                    "filter_reason": "该论文综述了由大语言模型驱动的角色扮演智能体，涵盖了记忆机制、人格建模（单智能体范畴）以及多智能体协作叙事（多智能体范畴）等关键技术，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在系统回顾大语言模型驱动的角色扮演智能体（RPLAs）的研究现状与挑战。针对角色一致性、行为逻辑及评估体系等核心问题，我们提出了一种涵盖心理建模、记忆机制和行为决策的综合分析框架，并在 RoleEval、CharacterEval 等基准测试上通过角色知识、性格保真度及交互幻觉等指标验证了现有主流方法的有效性。",
                    "summary_translation": "近年来，随着 large language models (LLMs, 大语言模型) 的快速发展，role-playing language agents (RPLAs, 角色扮演语言智能体) 已成为 natural language processing (NLP, 自然语言处理) 和人机交互交叉领域的一个突出研究焦点。本文系统回顾了 RPLAs 的当前发展现状与关键技术，勾勒了从早期 rule-based template paradigms (基于规则的模板范式)，经过 language style imitation (语言风格模仿) 阶段，到以 personality modeling (人格建模) 和 memory mechanisms (记忆机制) 为核心的 cognitive simulation (认知模拟) 阶段的技术演变路径。文章总结了支持高质量角色扮演的关键技术路径，包括 psychological scale-driven (心理量表驱动) 的 character modeling (角色建模)、memory-augmented prompting mechanisms (记忆增强提示机制) 以及 motivation-situation-based (基于动机-情境) 的 behavioral decision control (行为决策控制)。在数据层面，本文进一步分析了构建 role-specific corpora (角色专用语料库) 的方法与挑战，重点关注数据来源、版权限制以及结构化标注流程。在评估方面，文章梳理了涵盖 role knowledge (角色知识)、personality fidelity (人格保真度)、value alignment (价值对齐) 和 interactive hallucination (交互式幻觉) 的 multi-dimensional assessment frameworks (多维评估框架) 与 benchmark datasets (基准数据集)，并评述了人工评估、reward models (奖励模型) 以及 LLM-based scoring (基于LLM的评分) 等方法的优缺点。最后，本文展望了角色扮演智能体的未来发展方向，包括 personality evolution modeling (人格演化建模)、multi-agent collaborative narrative (多智能体协作叙事)、multimodal immersive interaction (多模态沉浸式交互) 以及与 cognitive neuroscience (认知神经科学) 的融合，旨在为后续研究提供系统性视角和方法论见解。",
                    "inspiration_trace": "基于对论文《Role-Playing Agents Driven by Large Language Models》的深度分析，以下是作者产出这篇综述文章的系统性逻辑链推演。这一过程展现了作者如何从宏观现象出发，逐步解构技术瓶颈，最终构建出一个涵盖技术、数据、评估与未来的完整学术框架。\n\n---\n\n### 1. 宏观观察与问题定义：从“能说话”到“像个人”\n\n**逻辑起点：**\n作者首先观察到了大语言模型（LLMs）技术爆发后的一个显著社会现象——以 Character.AI 为代表的角色扮演应用迅速普及。这表明用户需求已从单纯的信息获取（问答机器人）转向了情感陪伴和沉浸式体验。\n\n**核心冲突：**\n尽管 LLMs（如 GPT-4）具备强大的语言生成能力，但作者发现了一个关键痛点：**“语言风格模仿”不等于“角色认知模拟”**。\n*   **现象：** 简单的提示词（如“你是一个英国贵族”）只能维持表面的语言风格。\n*   **问题：** 在长对话、复杂决策或特定情境下，角色容易“崩人设”，表现出行为逻辑不一致、缺乏动机或遗忘背景。\n\n**初步假设：**\n要构建高质量的 RPLAs，必须超越传统的对话系统范式，引入心理学、认知科学的理论，从“语言建模”转向“认知建模”。\n\n---\n\n### 2. 历史演进分析：技术范式的三阶段跃迁\n\n为了理清现状，作者没有直接罗列算法，而是回溯了 RPLAs 的技术演进史，将其抽象为三个阶段的逻辑递进，从而定位当前研究的历史坐标：\n\n*   **阶段一：规则与模板（前 LLM 时代）。**\n    *   *特征：* 依赖人工规则和检索。\n    *   *局限：* 僵硬，缺乏上下文理解。\n    *   *代表工作：* 基于 Transformer 的多任务学习尝试（Si et al., 2021），引入了“角色关系”这一核心要素。\n*   **阶段二：风格模仿（LLM 早期）。**\n    *   *特征：* 利用 LLMs 的生成能力，通过 Prompt 模仿语气。\n    *   *局限：* 只有“皮囊”没有“灵魂”，缺乏深层行为逻辑。\n*   **阶段三：认知模拟（当前阶段）。**\n    *   *特征：* 强调个性一致性、记忆机制和动机驱动的行为决策。\n    *   *结论：* 现在的研究正处于从阶段二向阶段三跨越的关键期，核心任务是赋予角色“认知能力”。\n\n---\n\n### 3. 核心方法论构建：解构“角色灵魂”的三维支柱\n\n基于上述定位，作者提出要解决“人设崩塌”问题，必须构建一个三位一体的技术架构。这是文章最核心的方法论贡献：\n\n**支柱一：从“标签”到“心理量表”的个性建模**\n*   *思考：* 传统的角色设定（如“性格开朗”）过于模糊。\n*   *演进：* 引入心理学理论（大五人格、MBTI）。\n*   *逻辑：* 将抽象的性格转化为可量化的心理指标（如 PsyMem 框架），甚至通过自监督学习（如 Ditto）让模型内隐地学习性格特征，从而实现从“模仿说话”到“模仿思维方式”的转变。\n\n**支柱二：从“隐式状态”到“显式检索”的记忆机制**\n*   *思考：* 人类依靠记忆维持自我同一性，模型同理。\n*   *演进：* 从依赖模型参数的隐式记忆，转向外挂的、可检索的显式记忆库（如 CHARMAP）。\n*   *逻辑：* 记忆不仅是存储过去，更是为了在当前情境下进行因果推理。通过动态检索相关记忆片段，约束模型的生成方向，确保行为的前后一致性。\n\n**支柱三：从“对话生成”到“行为决策”的逻辑控制**\n*   *思考：* 角色扮演不仅是聊天，更是在特定情境下做选择。\n*   *演进：* 引入“动机-情境-行为”的因果链（如 LIFECHOICE 数据集）。\n*   *逻辑：* 在生成语言之前，先进行行为决策。角色必须根据其内在动机和外部环境，做出符合逻辑的选择，然后再生成对应的语言。这标志着 RPLA 从“语言模型”向“行为智能体”的进化。\n\n---\n\n### 4. 基础设施支撑：数据与评估的重构\n\n在确立了核心技术架构后，作者进一步思考：支撑这些架构的“燃料”和“标尺”是什么？\n\n**数据层面的思考：**\n*   *观察：* 通用语料无法训练出有灵魂的角色。\n*   *推演：* 必须构建高质量的角色专用语料库。\n*   *逻辑：* 数据来源从小说、剧本扩展到多模态资源；构建过程从简单的文本清洗，进化为包含“性格-事件-行为”的结构化标注。作者特别强调了版权与开放性的矛盾，指出了构建开放数据生态的必要性。\n\n**评估层面的思考：**\n*   *观察：* 传统的 BLEU 或流畅度指标无法衡量“像不像”。\n*   *推演：* 需要建立多维度的评估体系。\n*   *逻辑：* 评估维度从单一的语言质量，扩展到**角色知识**、**个性保真度**、**价值一致性**和**交互幻觉**。评估方法从单纯的人工打分，转向结合心理学量表（如 InCharacter）和基于 LLM 的自动评估（尽管存在偏见），旨在建立一套科学的“角色体检表”。\n\n---\n\n### 5. 未来展望：从静态模拟到动态生命\n\n最后，作者基于当前技术瓶颈，向外推演了未来的演进方向，完成了逻辑链的闭环：\n\n*   **动态演变：** 角色不应是静态的，应能随着交互经历成长（引入元学习和情绪建模）。\n*   **多智能体协作：** 单个角色的模拟已不足够，未来是多个角色在虚拟世界中的社会性互动。\n*   **多模态融合：** 从纯文本交互，进化到包含语音、表情、动作的全身心沉浸体验。\n*   **认知神经科学融合：** 借鉴人脑的神经机制（如情绪调节、认知负荷），让 AI 的决策过程更符合人类生理学特征。\n\n---\n\n### 总结：作者的思维全景图\n\n作者的思考过程遵循了**“现象观察 -> 范式定位 -> 核心解构 -> 支撑体系 -> 未来推演”**的严密逻辑链条。\n\n1.  **发现** LLMs 带来了角色扮演的可能性，但存在“形似神不似”的缺陷。\n2.  **定义** 这是一个从“语言模仿”向“认知模拟”跨越的范式转移。\n3.  **提出** 解决方案必须包含三个维度：深度的心理建模、显式的记忆机制、动机驱动的行为决策。\n4.  **强调** 这一方案需要高质量的结构化数据和科学的评估体系作为支撑。\n5.  **展望** 最终目标是创造具有自我意识、能动态演变且具备多模态交互能力的“数字生命”。"
                },
                {
                    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
                    "arxiv_id": "2601.10080",
                    "authors": "Letian Peng, Kun Zhou, Longfei Yun, Yupeng Hou, Jingbo Shang",
                    "summary": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.",
                    "category": "cs.CL",
                    "filter_reason": "论文主要研究角色扮演智能体的行为逻辑构建，提出了可执行的决策树框架来规范智能体在不同叙事语境下的行为，属于单智能体的行为规划与决策范畴，旨在提升智能体的一致性和落地能力。",
                    "summary2": "本文旨在解决现有RP agent行为档案非结构化且难以执行的问题。针对大规模叙事数据中的场景-动作对，我们提出了一种Codified Decision Trees (CDT)数据驱动框架，通过递归假设-验证机制构建可执行的决策树结构。在Fine-grained Fandom和Bandori基准测试上，通过NLI分数验证了其有效性，性能显著优于人工编写的档案及先前的归纳方法。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Deriving Character Logic from Storyline as Codified Decision Trees》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从问题观察到最终方案形成的思考过程：\n\n### 1. 宏观问题：角色扮演的一致性困境\n**思考起点：**\n现有的基于大语言模型（LLM）的角色扮演（RP）系统面临一个核心痛点：**行为的一致性与可解释性不足**。\n*   **现象：** LLM 虽然能生成流畅的对话，但在面对不同叙事场景时，往往难以像真实角色那样保持稳定的行为逻辑，容易出现“崩人设”或行为前后矛盾的情况。\n*   **根源：** 现有的角色画像大多是**非结构化、不可执行的纯文本**。LLM 在处理这些文本时，缺乏强制性的逻辑约束，导致行为具有随机性和脆弱性。\n\n### 2. 现有方案的局限与反思\n**观察与批判：**\n为了解决上述问题，作者审视了现有的两类主流方案，并发现了它们各自的致命缺陷：\n\n*   **路径 A：人工编写规则**\n    *   *优点：* 将文本转化为可执行的代码（如 `if is_full_moon() then \"wolf\"`），逻辑清晰，执行确定。\n    *   *缺点：* **成本极高**。人工编写和验证规则不仅耗时，而且难以覆盖所有可能的场景，扩展性差。\n*   **路径 B：自动化文本摘要**\n    *   *优点：* 从故事线中自动提取角色特征，成本低，数据驱动。\n    *   *缺点：* **上下文混淆**。现有的方法倾向于将不同情境下的行为聚合为一段通用的描述（例如，“他很勇敢”）。然而，角色往往在特定条件下勇敢，在另一条件下懦弱。这种扁平化的聚合丢失了“情境特异性”，导致 RP 代理在错误的时间应用错误的特征。\n\n### 3. 核心假设：情境感知的代码化\n**思维跃迁：**\n作者意识到，理想的解决方案必须结合两者的优点：**既要是数据驱动的（自动化），又要是结构化且可执行的（代码化），同时必须具备情境感知能力。**\n\n*   **关键洞察：** 人类理解角色逻辑的方式实际上是**层级化**的——先判断大致情境，再细化判断。\n*   **假设：** 如果能从故事数据中自动挖掘出“如果满足条件 A，则执行行为 B”的规则，并将这些规则组织成一个**树状结构**，就能实现既精确又可解释的角色逻辑。\n    *   树的内部节点代表**情境判断条件**（如“是否在舞台上？”）。\n    *   树的叶子节点代表**具体的行为陈述**（如“弹吉他”）。\n    *   推理时，根据当前场景在树中遍历，只激活相关的路径，从而实现**情境特定的锚定**。\n\n### 4. 方法论演进：从规则挖掘到递归验证\n**逻辑构建：**\n如何自动构建这样一棵树？作者没有选择让 LLM 一次性生成整棵树（这太复杂且不可靠），而是设计了一个**“假设-验证”的迭代生长机制**：\n\n1.  **假设生成：**\n    *   面对一堆杂乱的“场景-动作”数据，首先通过聚类将相似的案例归为一组。\n    *   利用 LLM 的归纳能力，针对每一组数据提出假设规则：“在这些场景下，角色的共同特征是什么？触发条件是什么？”\n\n2.  **系统验证：**\n    *   不能盲目相信 LLM 的假设。必须将提出的规则放回整个数据集中进行验证。\n    *   **验证逻辑：** 检查该规则在所有相关场景下的准确率。\n\n3.  **递归生长：**\n    *   这是 CDT 最核心的逻辑分支点。根据验证结果，决定规则的去留与生长：\n        *   **高准确率：** 规则成立，作为叶子节点（无需再细分）。\n        *   **低准确率：** 规则错误，直接丢弃。\n        *   **中等准确率：** 规则部分成立。这意味着该规则下还隐藏着更细分的情境差异。因此，**创建一个子节点**，将满足该规则的数据筛选出来，作为子节点的输入，**递归**重复上述过程。\n\n### 5. 最终框架：Codified Decision Trees (CDT)\n**方案成型：**\n通过上述逻辑，作者最终确立了 CDT 框架：\n*   **结构：** 一个由验证过的条件规则组成的决策树。\n*   **训练：** 通过聚类、假设、验证、递归的闭环，从原始故事线中自动提炼出层级化的行为逻辑。\n*   **推理：** 在运行时，新场景输入后，沿着树的路径进行判定，沿途收集经过验证的行为陈述作为 Prompt 的上下文。\n\n### 总结\n作者的思考路径是从**“行为一致性”**的痛点出发，批判了**“纯文本”**的模糊性和**“人工规则”**的高成本，进而提出**“数据驱动的情境感知代码化”**愿景。为了实现这一愿景，他们引入了**决策树**作为逻辑载体，并创新性地设计了**基于验证准确率的递归生长算法**，从而将非结构化的故事数据转化为可执行、可解释且精确的角色逻辑系统。"
                },
                {
                    "title": "OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing",
                    "arxiv_id": "2601.09858",
                    "authors": "Yilin Bao, Ziyao He, Zayden Yang",
                    "summary": "Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个强化学习框架，将科学大纲构建视为长视距规划问题，涉及结构化动作、增量构建和基于奖励的优化，符合单智能体中“规划”和“自我反思/反馈”的研究范围。",
                    "summary2": "本文旨在解决当前大语言模型在科学写作中缺乏全局结构和引用一致性的问题。针对科学论文生成任务，我们提出了一种Hierarchical Reinforcement Learning框架，将大纲构建视为长视界规划问题，并引入两阶段优化程序。我们在基于arXiv构建的新benchmark上，通过Precision、Recall、F1等指标验证了其有效性，实验表明该方法在长程结构连贯性和引用可靠性上优于强基线。",
                    "summary_translation": "科学论文生成需要进行 document-level planning (文档级规划) 和 factual grounding (事实依据)，尽管当前的 large language models (大型语言模型) 具有很强的 local fluency (局部流畅性)，但在 global structure (全局结构)、input coverage (输入覆盖率) 和 citation consistency (引用一致性) 方面往往表现不佳。我们提出了一个 reinforcement learning framework (强化学习框架)，将 scientific outline construction (科学大纲构建) 视为基于 hierarchical document structures (分层文档结构) 的 long-horizon planning problem (长视界规划问题)。我们的方法通过 structured actions (结构化动作) 对大纲的编辑演变过程进行建模，使系统能够增量式地构建完整的 scientific manuscript (科学手稿)。为了支持有效且稳定的学习，我们引入了一种 two-stage optimization procedure (两阶段优化程序)，包括：从 partial plans (部分计划) 进行 backward outline reconstruction (反向大纲重建) 以强制执行 global structural consistency (全局结构一致性)，以及 forward value-guided reinforcement learning (前向价值引导强化学习)，其 rewards (奖励) 显式地建模了 scientific correctness (科学正确性)、discourse coherence (话语连贯性) 和 citation fidelity (引用保真度)。此外，我们进一步引入了一个用于科学论文生成的 benchmark (基准)，用于评估 document planning (文档规划)、input utilization (输入利用率)、reference faithfulness (参考忠实度)、outline organization (大纲组织) 以及 content-level factual accuracy (内容级事实准确性)。实验结果表明，我们的方法相较于强大的神经模型和 LLM baselines (大型语言模型基线) 取得了持续的提升，特别是在 long-range structural coherence (长程结构连贯性) 和 citation reliability (引用可靠性) 方面表现尤为突出。",
                    "inspiration_trace": "基于论文《OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法产出的思考过程：\n\n### 第一阶段：问题定位与宏观观察\n**思考起点：LLM在长文本生成中的“能力错位”**\n作者首先观察到，尽管当前大语言模型（LLM）在局部流畅性上表现出色，但在科学写作这一特定任务上存在显著缺陷。\n*   **现象**：现有的LLM往往能写出通顺的句子，却无法构建逻辑连贯的整篇论文。具体表现为：全局结构松散、输入材料覆盖不全、引用混乱。\n*   **本质矛盾**：科学写作不仅仅是“文本生成”，更是一个“长视距规划”问题。现有的自回归生成方式（逐字预测）将写作视为线性的流，而忽略了写作本质上是结构化的、迭代式的决策过程。\n\n### 第二阶段：概念抽象与假设提出\n**思考转折：从“生成”转向“编辑”**\n为了解决上述矛盾，作者试图寻找一种能体现“规划”能力的抽象表示。\n*   **类比启发**：作者借鉴了代码编辑中的版本控制概念。程序员不是一次性写出代码，而是在抽象语法树（AST）上进行增删改。\n*   **核心假设**：如果将科学论文的生成过程，建模为对一个“显式大纲”的迭代编辑过程，而非单纯的Token序列生成，那么模型就能更好地掌握全局结构。\n*   **状态定义**：论文不再是一串Token，而是一个层级化的“大纲状态”。写作过程就是从空状态到完整状态的演化。\n\n### 第三阶段：方法论构建\n**思考深化：引入强化学习（RL）解决长视距问题**\n既然将写作定义为“状态的演化”，这就天然契合了强化学习（RL）的框架。\n*   **MDP建模**：\n    *   **状态**：当前的大纲结构。\n    *   **动作**：对大纲的编辑操作，如添加节点、移动段落、修改内容。\n    *   **奖励**：不再仅仅是预测下一个词的概率，而是科学写作的质量指标（结构合理性、引用准确性、事实一致性）。\n*   **显式状态的优势**：传统的RLHF（人类反馈强化学习）往往作用于隐式的隐藏状态，难以在长文本中归因。通过引入“显式的大纲状态”，作者将复杂的推理过程具象化，使得模型能够进行结构化的思考，而非“黑盒”生成。\n\n### 第四阶段：数据策略与训练范式\n**思考落地：如何获取“编辑”数据？**\n有了理论框架，作者面临一个现实难题：缺乏人类从零开始写论文并不断修改大纲的轨迹数据。\n*   **逆向工程思维**：既然没有正向的“写作过程”数据，是否可以利用海量的“成品”数据反向构造？\n*   **数据构造方案**：作者提出“逆向大纲重构”。即从arXiv上的成品论文出发，通过随机删除或扰动部分内容，构造出“残缺的大纲”。模型的任务就是学习如何从残缺状态恢复到完整状态。这为模型提供了大量的“状态-动作”对。\n*   **两阶段优化**：\n    1.  **向后重构（模仿学习）**：先让模型学会如何像人类一样修补大纲，保证结构上的合理性（解决“怎么写”的问题）。\n    2.  **向前引导（价值引导RL）**：引入显式的奖励函数，指导模型优化内容的科学性和引用的准确性（解决“写得好”的问题）。\n\n### 第五阶段：验证与闭环\n**思考验证：通过演化视角评估质量**\n最后，作者意识到评估长文本生成不能只看最终结果，还要看过程。\n*   **动态评估**：作者设计了新的Benchmark，不仅评估最终生成的综述质量，还分析生成过程中的指标变化（如引用数量在前期饱和，而结构完整性在后期提升）。\n*   **结论确认**：实验证明，这种基于“显式状态”和“分层强化学习”的方法，确实比单纯的一键生成（Baseline）在长程连贯性和引用可靠性上更胜一筹。\n\n---\n\n**总结逻辑链：**\n1.  **观察**：LLM擅长局部生成，拙于全局规划。\n2.  **抽象**：写作是结构的迭代演化，而非线性流。\n3.  **建模**：将大纲定义为显式状态，利用RL框架进行长视距决策。\n4.  **数据**：通过逆向扰动成品论文，合成训练轨迹。\n5.  **优化**：结合向后模仿（结构）与向前强化（质量）的两阶段训练。\n6.  **产出**：OUTLINEFORGE框架。"
                },
                {
                    "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines",
                    "arxiv_id": "2601.09714",
                    "authors": "Devesh Saraogi, Rohit Singhee, Dhruv Kumar",
                    "summary": "The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了“agentic workflows”（智能体工作流），涵盖了自我反思、进化搜索（自我演化）和多智能体框架等架构，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决AI生成研究计划中的“智能抄袭”问题，评估Agentic Workflows能否提升新颖性。针对五种推理架构，我们提出了一种多工作流对比方法，并在30份专家评估提案上，通过Novelty、Feasibility和Impact指标验证了其有效性。结果表明，Decomposition-based和Long-context workflows显著优于Reflection-based方法。",
                    "summary_translation": "大型语言模型 融入科学生态系统，引发了关于人工智能生成研究的创造力和原创性的根本性问题。近期研究已将“智能抄袭” 确定为单步提示方法 中的一大隐忧，即模型通过术语转换来复现现有观点。本文探讨了智能体工作流 —— 即采用迭代推理、进化搜索 和递归分解 的多步系统 —— 是否能够生成更具新颖性和可行性的研究计划。我们对五种推理架构进行了基准测试：基于反思的迭代优化、Sakana AI v2 进化算法、Google Co-scientist 多智能体框架、GPT Deep Research (GPT-5.1) 递归分解，以及 Gemini 3 Pro 多模态长上下文管道。通过对三十份研究提案的新颖性、可行性和影响力进行评估，我们发现基于分解和长上下文的工作流实现了 4.17/5 的平均新颖性得分，而基于反思的方法得分显著较低（2.33/5）。结果显示，不同工作流在各研究领域表现各异，其中表现优异的工作流能够在保持可行性的同时不牺牲创造力。这些发现支持了这样一种观点，即精心设计的多阶段智能体工作流能够推动 AI 辅助的研究构思 发展。",
                    "inspiration_trace": "基于论文内容，以下是对作者产出该文章核心思想的逻辑链推演：\n\n### 1. 宏观观察与核心矛盾\n**起点：** 作者观察到LLM（大语言模型）正深度介入科学发现领域，从文献综述到代码生成，甚至开始涉足核心的“假设生成”环节。\n**矛盾浮现：** 学术界对此存在两极分化的观点。乐观者认为LLM具备真正的创造力，而怀疑者认为LLM只是复杂的插值器。\n**关键痛点：** 作者注意到近期研究指出了一个具体问题——**“智能剽窃”**。在单步提示下，模型倾向于通过替换术语或调整结构来复现现有想法，虽然能绕过查重工具，但本质上缺乏原创性。\n\n### 2. 假设提出：从“单步”到“多步”\n**逻辑推演：** 既然单步、零样本的提示容易导致模型停留在训练数据的高概率区域（即抄袭现有知识），那么如何打破这种惯性？\n**核心假设：** 引入**“智能体工作流”**。作者认为，通过增加计算步骤、引入迭代推理、进化搜索或递归分解，可以强制模型跳出高概率的“衍生区域”，从而探索更具原创性的低概率区域。\n**问题聚焦：** 并非所有复杂的工作流都有效，不同的推理架构（如反思、进化、分解）对“新颖性”和“可行性”的权衡有何不同影响？\n\n### 3. 实验设计：架构哲学的对比\n为了验证上述假设，作者没有仅仅比较模型参数大小，而是选择了**五种代表不同推理哲学的架构**进行横向对比，试图找出哪种“思维模式”最利于创新：\n\n*   **反思派：** 假设“自我纠错”能提升质量。\n*   **进化派：** 假设“变异与选择”能产生新物种。\n*   **分解派：** 假设“化整为零”能避免思维定势。\n*   **多智能体辩论派：** 假设“对抗性审查”能剔除平庸想法。\n*   **长上下文派：** 假设“海量信息摄入”能发现知识盲区。\n\n### 4. 评估策略：超越自动化指标\n**逻辑困境：** 传统的文本相似度检测无法识别“智能剽窃”（即意思一样但说法不同）。\n**解决方案：** 回归**人类专家评估**。作者引入了多维度的专家打分（新颖性、可行性、影响力），并覆盖了不同科学领域（如AI、生物、气候），以验证工作流在不同语境下的鲁棒性。\n\n### 5. 结果分析与逻辑验证\n**发现一：** 简单的“反思”无效。如果出发点是平庸的，单纯的自我纠错无法带来质变（得分最低）。\n**发现二：** “分解”与“长上下文”胜出。这证明了**自底向上的构建**（从子问题合成）比**自顶向下的检索**（套用模板）更能激发创新。\n**发现三：** 领域差异显著。AI领域容易出新，而实验性强的领域（如生物）面临更多落地挑战，说明工作流设计需考虑领域特性。\n**发现四：** 新颖性与可行性并不冲突。打破了“有创意的想法通常不可行”的刻板印象。\n\n### 6. 结论升华\n**最终观点：** AI生成内容的原创性不仅仅取决于模型本身，更取决于**“推理架构的设计”**。通过精心设计的多阶段智能体工作流（特别是基于分解和长上下文的架构），可以有效规避“智能剽窃”，使AI从“检索引擎”转变为真正的“创意合作伙伴”。"
                },
                {
                    "title": "TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems",
                    "arxiv_id": "2601.10120",
                    "authors": "Rui Sun, Jie Ding, Chenghua Gong, Tianjun Gu, Yihang Jiang, Juyuan Zhang, Liming Pan, Linyuan Lü",
                    "summary": "Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于基于LLM的多智能体系统，研究如何通过生成通信拓扑来优化智能体间的协作与通信，属于“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决LLM多智能体系统中现有时空交互范式导致的高延迟与计算开销问题。针对多轮对话场景，我们提出了TopoDIM框架，通过异构图编码器与自回归解码器一次性生成包含Conditioned、Feedback及Debate模式的异构拓扑，并采用去中心化架构。在MMLU-Pro、LiveCodeBench等数据集上，通过任务性能和Token消耗验证了其有效性。",
                    "summary_translation": "优化 LLM-based multi-agent system (基于大语言模型的多智能体系统) 中的 communication topology (通信拓扑) 对于实现集体智能至关重要。现有方法主要依赖 spatio-temporal interaction paradigms (时空交互范式)，其中多轮对话的顺序执行会导致高延迟和计算开销。受到近期关于 evaluation and debate mechanisms (评估与辩论机制) 能够改善多智能体系统问题解决能力的见解启发，我们提出了 TopoDIM，这是一个支持 one-shot (单次) 拓扑生成且具有多样化交互模式的框架。TopoDIM 专为 decentralized execution (去中心化执行) 而设计，以增强适应性和隐私性，使智能体能够在无需迭代协调的情况下自主构建 heterogeneous communication (异构通信)，从而实现 token efficiency (令牌效率) 并提升任务性能。实验表明，与 state-of-the-art methods (最先进方法) 相比，TopoDIM 将总 token 消耗减少了 46.41%，同时将平均性能提高了 1.50%。此外，该框架在组织异构智能体之间的通信方面展现出强大的适应性。代码可在以下地址获取：https://anonymous.4open.science/r/TopoDIM-8D35/",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Self-reflection in Automated Qualitative Coding: Improving Text Annotation through Secondary LLM Critique",
                    "arxiv_id": "2601.09905",
                    "authors": "Zackary Okun Dunivin, Mobina Noori, Seth Frey, Curtis Atkinson",
                    "summary": "Large language models (LLMs) allow for sophisticated qualitative coding of large datasets, but zero- and few-shot classifiers can produce an intolerable number of errors, even with careful, validated prompting. We present a simple, generalizable two-stage workflow: an LLM applies a human-designed, LLM-adapted codebook; a secondary LLM critic performs self-reflection on each positive label by re-reading the source text alongside the first model's rationale and issuing a final decision. We evaluate this approach on six qualitative codes over 3,000 high-content emails from Apache Software Foundation project evaluation discussions. Our human-derived audit of 360 positive annotations (60 passages by six codes) found that the first-line LLM had a false-positive rate of 8% to 54%, despite F1 scores of 0.74 and 1.00 in testing. Subsequent recoding of all stage-one annotations via a second self-reflection stage improved F1 by 0.04 to 0.25, bringing two especially poor performing codes up to 0.69 and 0.79 from 0.52 and 0.55 respectively. Our manual evaluation identified two recurrent error classes: misinterpretation (violations of code definitions) and meta-discussion (debate about a project evaluation criterion mistaken for its use as a decision justification). Code-specific critic clauses addressing observed failure modes were especially effective with testing and refinement, replicating the codebook-adaption process for LLM interpretation in stage-one. We explain how favoring recall in first-line LLM annotation combined with secondary critique delivers precision-first, compute-light control. With human guidance and validation, self-reflection slots into existing LLM-assisted annotation pipelines to reduce noise and potentially salvage unusable classifiers.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了基于“自我反思”的两阶段工作流，利用二级LLM作为批评者审查和修正第一阶段的输出，符合单智能体中“自我反思”的研究范围。",
                    "summary2": "本文旨在解决LLM在自动化定性编码中产生大量不可容忍错误的问题。针对大规模文本标注场景，我们提出了一种两阶段工作流，利用secondary LLM critic对第一阶段的positive labels执行self-reflection，基于错误分类法进行二次审查。我们在Apache Software Foundation的3000多封邮件数据集上，通过F1 score和Cohen's $\\kappa$验证了其有效性，显著降低了false positive rate并提升了整体标注精度。",
                    "summary_translation": "Large language models (LLMs) (大语言模型) 能够对大型数据集进行复杂的 qualitative coding (定性编码)，但即使经过精心设计和验证的 prompting (提示工程)，zero- and few-shot classifiers (零样本和少样本分类器) 仍可能产生不可容忍的错误数量。我们提出了一种简单且可推广的两阶段工作流：首先，一个 LLM 应用由人类设计并经 LLM 适配的 codebook (编码本)；其次，一个辅助的 LLM critic (LLM 批评者) 通过结合源文本重读和第一个模型的 rationale (理由)，对每个 positive label (正标签) 进行 self-reflection (自我反思)，并做出最终决策。我们在来自 Apache Software Foundation (Apache 软件基金会) 项目评估讨论的 3000 封高内容邮件中，针对六个 qualitative codes (定性代码) 对该方法进行了评估。我们对 360 个 positive annotations (正向标注)（六个代码各 60 个段落）进行了人工核查，发现尽管测试中的 F1 scores (F1 分数) 达到 0.74 和 1.00，但第一线 LLM 的 false-positive rate (假阳性率) 仍高达 8% 至 54%。随后，通过第二个 self-reflection (自我反思) 阶段对所有第一阶段标注结果进行重新编码，将 F1 分数提高了 0.04 至 0.25，使得两个表现尤其不佳的代码分别从 0.52 和 0.55 提升至 0.69 和 0.79。我们的人工评估确定了两种反复出现的 error classes (错误类别)：misinterpretation (误解)（违反代码定义）和 meta-discussion (元讨论)（将关于项目评估标准的辩论误认为是将其作为 decision justification (决策理由) 的使用）。针对观察到的 failure modes (失败模式) 而设计的特定 code-specific critic clauses (针对特定代码的批评条款)，在测试和优化过程中特别有效，这一过程复现了第一阶段中针对 LLM 解释的 codebook-adaption process (编码本适配过程)。我们解释了如何在第一线 LLM 标注中偏重 recall (召回率)，并结合二次批评机制，从而实现以 precision (精确率) 优先且 compute-light (计算轻量) 的控制。在人工指导和验证下，self-reflection (自我反思) 机制可嵌入现有的 LLM 辅助 annotation pipelines (标注流水线) 中，以减少噪音并可能挽救原本不可用的 classifiers (分类器)。",
                    "inspiration_trace": "基于论文内容，以下是作者产出该核心方法的逻辑演进过程推演：\n\n### 1. 宏观困境：LLM在定性编码中的“不可靠性”\n**起点：** 作者首先承认大语言模型（LLM）在处理大规模定性文本编码时具有巨大潜力，能够模拟专家编码。\n**冲突：** 尽管在测试集上通过精心设计的提示词能获得较高的F1分数，但在实际部署到大规模语料库时，模型仍会产生“不可容忍”的错误。\n**核心洞察：** 传统的“测试集表现好”并不等于“实际应用可靠”。现有的LLM辅助编码实践处于一种“狂野西部”状态，缺乏对系统性错误的有效控制。\n\n### 2. 现状反思：仅优化“第一遍”提示词的局限性\n**观察：** 现有的方法论指南大多集中在如何设计完美的初始提示词，试图让模型一次性做对。\n**瓶颈：** 作者意识到，无论初始提示词设计得多么严谨，面对复杂、长尾的真实数据，单一模型总会产生系统性偏差（如过度自信、误读定义）。\n**转折点：** 既然无法保证第一遍编码的完美，那么研究的重点不应仅在于“预防错误”，而应在于“如何处理已经产生的错误”。作者决定将**事后错误分析**提升为与编码同等重要的独立阶段。\n\n### 3. 策略转变：从“单次完美”到“两阶段分工”\n**假设：** 如果无法构建一个全能的模型，不如将任务拆解。\n**逻辑推演：**\n*   **第一阶段（召回优先）：** 允许第一个模型（Annotator）表现得“宽松”一些，宁可多标（包含假阳性），也不能漏标（高召回率）。这符合人类编码中先广泛筛选的直觉。\n*   **第二阶段（精准优先）：** 引入第二个模型专门负责“清洗”数据。它的任务不是重新编码，而是专门审查第一阶段给出的“阳性”标签。\n**优势预判：** 这种分工不仅符合认知逻辑（先发散后收敛），还能节省计算资源（因为只需审查少量的阳性样本，而非全量数据）。\n\n### 4. 机制设计：有界的“自我反思”\n**概念引入：** 作者借用了AI领域的“自我反思”概念，但对其进行了社会学意义上的改造。\n**关键约束：** 作者不希望模型进行无限制的、黑盒式的自我反思，因为这可能引入新的不可控因素。\n**提出“有界自主”：** 反思必须由人类定义的规则来引导。即，第二个模型（Critic）的审查标准必须基于人类对错误模式的归纳，而不是模型自己的“感觉”。\n\n### 5. 实证洞察：错误分类学的发现\n**行动：** 作者对第一阶段产生的错误进行了人工审计。\n**发现规律：** 错误并非随机，而是集中在两类模式上：\n1.  **误读：** 模型忽略了代码定义中的排除条款或边界条件。\n2.  **元讨论：** 文本是在“讨论”某个标准（如辩论其相关性），而不是在“使用”该标准作为决策依据。\n**提炼：** 这两类错误构成了“错误分类学”，为第二阶段的Critic提供了具体的审查靶点。\n\n### 6. 最终方法论：基于错误分类学的双阶段流水线\n**闭环形成：**\n*   **Stage 1（标注者）：** 应用人类设计的代码本，输出标签和理由。策略是“宁滥勿缺”。\n*   **Stage 2（批评者）：** 仅读取Stage 1的阳性结果。它结合原文、Stage 1的理由以及人类归纳的“错误分类学”，进行针对性审查。如果发现理由落入上述两类错误模式，则行使否决权。\n**结果：** 形成了一个“召回优先 + 精准清洗”的流水线。通过将人类的错误分析转化为Critic的提示词指令，实现了在保持人类控制权（有界自主）的前提下，大幅提升自动化标注的准确率。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation",
                    "arxiv_id": "2601.09926",
                    "authors": "Kirandeep Kaur, Vinayak Gupta, Aditya Gupta, Chirag Shah",
                    "summary": "Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.",
                    "category": "cs.LG",
                    "filter_reason": "论文提出了ProPer，一种包含维度生成智能体（DGA）和响应生成智能体（RGA）的双智能体架构，旨在实现主动行为。这属于多智能体协作和单智能体行为的研究范围。",
                    "summary2": "本文旨在解决传统助手被动响应导致用户未表达需求未被满足的问题。针对用户查询和交互历史，我们提出了一种名为PROPER的双智能体架构，包含Dimension Generating Agent (DGA)和Response Generating Agent (RGA)，通过识别隐式知识缺口实现校准的主动性。我们在医疗、推荐和编程领域数据集上，通过覆盖度、主动性适当性和意图对齐等指标验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
                    "arxiv_id": "2601.10657",
                    "authors": "Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H. Chi, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang",
                    "summary": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.",
                    "category": "cs.LG",
                    "filter_reason": "该论文提出了PACEvolve框架，专注于LLM在进化搜索中的自我完善（self-improvement）和跨轨迹协作（collaboration），涉及智能体的上下文管理、搜索动态以及长视界演化，符合自我演化和多智能体的研究范围。",
                    "summary2": "总结生成失败",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making",
                    "arxiv_id": "2601.10102",
                    "authors": "Viswonathan Manoranjan, Snehalkumar `Neil' S. Gaikwad",
                    "summary": "Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.",
                    "category": "cs.MA",
                    "filter_reason": "该论文研究了多智能体LLM系统在复杂环境决策博弈中的行为，探讨了角色设定和收益可见性如何影响智能体的战略推理和纳什均衡达成。这完全符合“多智能体：协作、通信、博弈”的研究范围。",
                    "summary2": "本文旨在探究多智能体LLM系统中角色身份偏见与收益优化之间的张力。针对环境决策场景下的四智能体战略博弈，我们提出了一种基于2x2因子设计的诊断框架，系统操纵角色身份和收益可见性，并在四种LLM架构（Qwen, Llama, Mistral）上通过Nash均衡达成率验证了其有效性。实验表明角色身份会完全抑制基于收益的优化，导致系统优先选择社会偏好结果而非最优均衡。",
                    "summary_translation": "大语言模型越来越多地被部署在多智能体系统中用于执行战略任务，然而，诸如基于角色的角色设定和收益可见性等设计选择如何影响推理，目前仍知之甚少。我们探讨了多智能体系统是作为能够进行收益优化的战略推理者发挥作用，还是作为优先考虑角色一致性而非显性激励的身份驱动行为体发挥作用。我们将纳什均衡的实现作为战略推理的诊断指标，在涉及四个智能体的复杂环境决策博弈中，对四种大语言模型架构（Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B）进行了系统实验。我们表明，即使存在收益最优均衡且具备完整的收益信息，角色身份偏见也会从根本上改变战略推理。移除角色设定并提供显性收益使得 Qwen 模型能够实现较高的纳什均衡率，这表明这两个条件对于战略推理都是必要的。相比之下，角色设定系统性地将均衡选择偏向于社会偏好结果：当存在角色设定时，所有实现的均衡都对应于绿色转型，而当公地悲剧是收益最优时，模型完全无法达到均衡。显性收益的影响完全取决于角色设定的存在，这揭示了表征设计选择之间存在强烈的相互作用。我们还观察到了明显的模型依赖性模式。Qwen 架构对角色设定和收益可见性都高度敏感，而 Llama 和 Mistral 在各种条件下都表现出僵化的推理行为。这些发现表明，表征选择是实质性的治理决策，它们决定了多智能体系统是作为战略推理者还是身份驱动行为体行事，这对现实世界的部署具有重要意义。",
                    "inspiration_trace": ""
                },
            ]
        },
    ],
    "2026-01-14": [
        {
            "name": "Artificial Intelligence",
            "count": 15,
            "papers": [
                {
                    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
                    "arxiv_id": "2601.09667",
                    "authors": "Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park",
                    "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个多智能体测试时强化学习框架（MATTRL），核心在于多智能体协作、多轮讨论以及通过信用分配和经验池进行反馈优化，符合多智能体协作与通信的研究范围。尽管在医学、数学等领域的基准测试中进行了评估，但其重点在于通用的多智能体系统框架而非特定领域的纯应用。",
                    "summary2": "总结生成失败",
                    "summary_translation": "多智能体系统已演变为许多应用中实用的 LLM-driven（大语言模型驱动）协作者，凭借多样性和交叉验证获得了鲁棒性。然而，multi-agent RL (MARL)（多智能体强化学习）训练资源消耗大且不稳定：协同适应的队友会导致非平稳性，且奖励通常稀疏且具有高方差。因此，我们提出了 **Multi-Agent Test-Time Reinforcement Learning (MATTRL)**（多智能体测试时强化学习），这是一个在推理时将结构化文本经验注入多智能体审议过程的框架。MATTRL 组建了一个由专家组成的多专家团队进行多轮讨论，检索并整合 test-time experiences（测试时经验），并达成共识以进行最终决策。我们还研究了 credit assignment（信用分配）机制，用于构建 turn-level experience pool（轮级经验池），随后将其重新注入对话中。在医学、数学和教育领域的具有挑战性的基准测试中，MATTRL 的准确率相较于 multi-agent baseline（多智能体基线）平均提高了 3.67%，相较于 comparable single-agent baselines（可比单智能体基线）提高了 8.67%。Ablation studies（消融实验）考察了不同的 credit-assignment schemes（信用分配方案），并详细比较了它们对训练结果的影响。MATTRL 为实现无需调优的 distribution-shift-robust（分布偏移鲁棒）多智能体推理提供了一条稳定、有效且高效的路径。",
                    "inspiration_trace": "基于论文《Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning》，以下是对作者核心方法论（MATTRL）产生逻辑链的系统性推演。这一过程展现了作者从宏观趋势观察到微观痛点分析，再到范式转移与方法构建的完整思考路径。\n\n---\n\n### 第一阶段：宏观观察与趋势捕捉\n**思考起点：LLM的进化方向——从单体智能到群体协作**\n\n1.  **观察现象**：作者首先注意到，随着LLM能力的发展，单一大模型在处理复杂推理任务（如数学、医疗诊断）时存在瓶颈。学术界和工业界开始转向**多智能体系统**。\n2.  **识别优势**：多智能体通过角色扮演、辩论和交叉验证，利用“多样性”和“相互纠错”机制，在鲁棒性上显著优于单智能体。\n3.  **初步定位**：作者确立了研究基点——多智能体协作是提升LLM推理能力的有效路径，但现有的协作方式（如AutoGen, CAMEL）多依赖于静态的提示词工程，缺乏从交互中“学习”和“进化”的能力。\n\n### 第二阶段：痛点深挖与现有方案的局限\n**核心矛盾：如何让多智能体系统“学会”协作？**\n\n1.  **尝试路径（MARL）**：为了赋予系统学习能力，自然想到引入**多智能体强化学习（MARL）**（如MAPoRL, ReMA），试图通过奖励信号来优化协作策略。\n2.  **遭遇瓶颈**：作者深入分析发现，MARL在LLM场景下存在三个致命缺陷：\n    *   **非平稳性**：多个智能体同时更新策略，导致环境动态变化极快，训练极难收敛。\n    *   **奖励稀疏与高方差**：在复杂的推理任务中，很难给出精确的标量奖励，且容易产生噪声。\n    *   **灾难性遗忘与资源消耗**：权重更新不仅计算昂贵，而且针对特定任务的微调往往会破坏模型原有的通用能力。\n3.  **关键反思**：**“更新权重”真的是让多智能体学会协作的唯一或最佳路径吗？** 既然LLM拥有强大的上下文学习能力，为什么非要修改模型参数？\n\n### 第三阶段：范式转移与核心假设\n**逻辑跃迁：从“参数更新”转向“经验注入”**\n\n1.  **引入新视角**：作者关注到了**测试时适应**和**测试时强化学习（TTRL）**的概念。这些方法允许模型在不更新权重的情况下，利用测试时的信号进行自我进化。\n2.  **提出核心假设**：如果将“协作经验”显式地转化为**结构化的文本**，并将其作为上下文注入到多智能体的对话中，是否可以替代昂贵的权重更新？\n3.  **定义“文本经验”**：作者认为，相比于稀疏的标量奖励，文本包含了更丰富的语义信息（如推理步骤、纠错过程），能更直接地指导智能体如何行动。\n\n### 第四阶段：方法论构建\n**具体化：如何构建“测试时强化学习”闭环？**\n\n基于上述假设，作者设计了一个无需梯度下降的“学习”闭环，包含三个关键逻辑步骤：\n\n1.  **协作与探索**：\n    *   构建一个多专家团队进行多轮讨论。\n    *   *思考*：必须让智能体自由交互，产生足够多的行为数据，作为“经验”的来源。\n\n2.  **信用归因与经验筛选**：\n    *   *问题*：多轮对话中，哪句话是有用的？哪个智能体的贡献最大？\n    *   *解决方案*：作者引入了**群体到个体的信用分配**机制。不仅看最终结果，还要结合每一步的个体表现和衰减的团队奖励，计算出每个“回合”的价值。\n    *   *逻辑*：只有高价值的对话片段才会被保留，转化为“经验”。\n\n3.  **经验重构与检索增强**：\n    *   将筛选出的高质量对话片段蒸馏为结构化的文本经验。\n    *   在面对新任务时，通过检索相关经验，将其作为提示词的一部分喂给智能体。\n    *   *本质*：这相当于给智能体配备了一本动态更新的“协作手册”，指导它们如何更好地讨论，而不是修改它们的大脑（权重）。\n\n### 第五阶段：验证与精细化\n**反思与修正：如何证明有效性并优化细节？**\n\n1.  **跨域验证**：为了证明方法的通用性，作者选择了医疗、数学、教育三个截然不同的领域进行测试。逻辑是：如果这种“文本经验”能跨越领域生效，说明它捕捉的是通用的“协作元能力”。\n2.  **信用分配策略的对比**：作者进一步思考，如何分配功劳最合理？通过对比朴素平均、差分奖励和Shapley值，发现**差分奖励**在精度和效率上取得了最佳平衡。这表明，在协作中，识别“关键转折点”比“平均主义”更重要。\n3.  **自适应路由**：最后，作者回归实用主义。既然多智能体有成本，单智能体有局限，那么是否可以动态选择？因此提出了自适应路由机制，根据任务复杂度决定是启用单智能体还是多智能体协作。\n\n---\n\n### 总结：作者的思维全景图\n\n1.  **起点**：多智能体协作虽好，但缺乏学习能力。\n2.  **困境**：传统的强化学习（MARL）训练成本高、不稳定且损害通用性。\n3.  **顿悟**：利用LLM的上下文学习能力，将“学习”过程从“参数空间”转移到“文本空间”。\n4.  **方案**：构建一个**“协作 -> 评分 -> 提取经验 -> 注入经验”**的测试时强化学习闭环（MATTRL）。\n5.  **升华**：通过精细的信用分配和自适应机制，实现了一种低成本、高鲁棒性且不破坏模型通用性的多智能体进化路径。"
                },
                {
                    "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
                    "arxiv_id": "2601.09636",
                    "authors": "Yibo Lyu, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie",
                    "summary": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个GUI智能体，重点研究了智能体的记忆机制和意图对齐，属于单智能体研究范畴中的记忆与规划能力。虽然涉及视觉输入（GUI），但核心贡献在于智能体架构而非视觉模型本身。",
                    "summary2": "本文旨在解决GUI Agent在现实部署中难以理解用户复杂隐式意图的问题。针对长期用户记录和模糊指令场景，我们提出了一种HIM-Agent，它通过流聚合模块和分层过滤器来组织用户偏好和常规。我们在AndroidIntent benchmark上通过SSR、CER和F1-score等指标验证了其有效性，显著提升了执行和主动建议的性能。",
                    "summary_translation": "尽管 GUI agents（图形用户界面智能体）在明确且完整的指令下表现强劲，但在实际部署中，它们需要与用户更复杂的 implicit intents（隐含意图）保持对齐。在这项工作中，我们提出了 Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign，个性化 GUI Agent 的分层隐含意图对齐)，这是一项新的智能体任务，要求智能体利用长期用户记录作为 persistent context（持久化上下文），以解决模糊指令中 omitted preferences（省略的偏好），并根据用户状态预测 latent routines（潜在习惯），从而提供 proactive assistance（主动辅助）。为了促进这项研究，我们介绍了 AndroidIntent，这是一个旨在评估智能体通过基于长期用户记录进行推理，从而解决模糊指令和提供主动建议能力的 benchmark（基准测试）。我们从跨不同用户的 2 万条长期记录中标注了 775 个 user-specific preferences（用户特定偏好）和 215 个 routines（习惯）用于评估。此外，我们提出了 Hierarchical Intent Memory Agent (HIM-Agent，分层意图记忆智能体)，该智能体维护一个持续更新的 personal memory（个人记忆），并对用户偏好和习惯进行分层组织以实现个性化。最后，我们在 AndroidIntent 上评估了一系列 GUI agents（图形用户界面智能体），包括 GPT-5、Qwen3-VL 和 UI-TARS。结果进一步显示，HIM-Agent 将 execution performance（执行性能）和 proactive performance（主动性能）分别显著提升了 15.7% 和 7.3%。",
                    "inspiration_trace": "基于论文《PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n---\n\n### 1. 宏观洞察：从“指令执行”到“意图对齐”\n**思考起点：**\n作者首先观察到当前GUI智能体的研究现状——虽然多模态大模型（MLLM）让智能体在“显式指令”下的执行能力大幅提升，但在真实世界部署中存在巨大鸿沟。\n\n**逻辑推演：**\n*   **现实痛点：** 真实的人类交互并非总是清晰、完整的指令。用户习惯省略细节，默认智能体能“懂”。\n*   **理论支撑：** 引入“联合活动”理论，认为人机交互是共享语境下的意义共建。\n*   **核心矛盾：** 现有的“反应式执行”范式只能处理显式输入，无法填补用户“言外之意”的空白。\n*   **任务定义：** 因此，作者将研究目标从单纯的“任务完成”转向**“个性化意图对齐”**，即智能体需要利用长期历史记录来补全用户未明说的需求。\n\n### 2. 现象解构：隐式意图的层次性\n**思考深入：**\n既然要解决“隐式意图”，那么“隐式”具体包含哪些形态？作者对用户日常行为进行了抽象和分类。\n\n**逻辑推演：**\n*   **第一层（显式）：** 传统的指令跟随，无需历史记录。\n*   **第二层（偏好意图）：** 用户给出了模糊指令（如“帮我点个麦当劳”），但省略了具体偏好（如“最近的、单人套餐、用美团”）。这需要智能体**回溯历史**来填补细节。\n*   **第三层（惯例意图）：** 用户甚至没有发出指令，但处于特定状态（如“周五晚上在家”）。这需要智能体**预测需求**并主动提供建议。\n*   **结论：** 个性化不仅仅是简单的记忆检索，而是一个**分层级的意图对齐**过程，需要同时处理“补全模糊指令”和“主动预测”两种不同逻辑的任务。\n\n### 3. 瓶颈识别：数据与记忆的双重缺失\n**问题聚焦：**\n要实现上述分层对齐，作者发现现有的技术栈存在两个关键短板。\n\n**逻辑推演：**\n*   **短板一（数据侧）：** 现有的GUI基准（如AITW）都是单次、静态的显式任务，缺乏**长期、以用户为中心**的标注数据。没有数据，就无法验证智能体是否真的“懂”用户。\n*   **短板二（模型侧）：** 现有的Agent记忆机制（如RAG）大多基于语义相似度。这种机制对于聊天有效，但对于GUI任务无效，因为GUI交互包含动作轨迹，且简单的语义检索无法区分“一次性操作”和“长期习惯”，也无法区分“偏好”和“惯例”。\n\n### 4. 方法论构建：从数据挖掘到分层记忆\n针对上述瓶颈，作者分别构建了数据集和智能体架构，其思考路径如下：\n\n#### 4.1 数据构建：如何客观定义“习惯”？\n**思考：** 偏好和惯例是主观概念，如何将其转化为可量化的客观标准？\n**逻辑链：**\n*   **假设：** 真正的偏好和惯例在统计学上表现为高频和状态一致性。\n*   **量化策略：** 提出利用**语义密度**（Semantic Density）来衡量意图的重复性，利用**状态偏移熵**（State Offset Entropy，即时间和场景的一致性）来衡量惯例的稳定性。\n*   **验证机制：** 设计“过滤-验证” pipeline，先用算法筛选出候选样本，再通过人工校验，确保数据既客观又符合人类直觉。由此诞生了 **AndroidIntent** 基准。\n\n#### 4.2 模型架构：如何设计“懂你”的记忆？\n**思考：** 传统的记忆只是存日志，我们需要一个能动态演进的、分层级的记忆系统。\n**逻辑链：**\n*   **第一步：动态聚合。**\n    *   *思考：* 原始日志是碎片化的，直接存会导致“记忆漂移”。\n    *   *方案：* 借鉴流数据挖掘中的微聚类思想，设计**流式聚合模块**，将相似的历史记录合并为“记录原型”，作为记忆的基本单元。\n*   **第二步：分层过滤。**\n    *   *思考：* 记忆不能是一锅粥，必须根据用途分开。\n    *   *方案 A（针对偏好）：* 设计**基于执行的偏好过滤器**。当指令模糊时，不仅要看语义相似，还要看**动作轨迹**（Action Trajectory，如点击路径）的一致性。因为偏好往往体现在具体的操作习惯上。\n    *   *方案 B（针对惯例）：* 设计**基于状态的惯例过滤器**。主动建议不能乱发，必须基于高置信度。计算原型的“主动置信度”，综合考量状态稳定性（时间/场景）、记录长度和聚合权重。只有置信度够高，才触发主动建议。\n\n### 5. 最终产出：PersonalAlign 与 HIM-Agent\n**总结：**\n作者将上述思考整合，提出了 **PersonalAlign** 这一新任务范式，并实现了 **HIM-Agent**。\n*   **逻辑闭环：** 智能体通过流式聚合不断更新用户画像 -> 遇到模糊指令时，利用“偏好过滤器”匹配历史动作轨迹进行补全 -> 遇到特定状态时，利用“惯例过滤器”判断是否触发主动建议。\n*   **核心创新：** 将GUI智能体从被动的“指令执行器”升级为具备长期记忆和分层推理能力的“个性化合作伙伴”。\n\n---\n**逻辑链总结图示：**\n现实交互的模糊性 -> 定义分层隐式意图（偏好/惯例） -> 发现数据与记忆的缺失 -> **数据端**：量化统计特征构建基准 -> **模型端**：流式聚合+分层过滤（动作vs状态） -> 实现个性化意图对齐。"
                },
                {
                    "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach",
                    "arxiv_id": "2601.09635",
                    "authors": "Kuo Liang, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, Hanzhang Qin, Ruihao Zhu, Chung-Piaw Teo",
                    "summary": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了 LEAN-LLM-OPT 框架，明确使用了 \"team of LLM agents\"（多智能体协作），包含上游智能体动态构建工作流（规划）和下游智能体执行任务，并利用辅助工具处理数据（工具使用）。这完全符合多智能体及单智能体的研究范围，且核心贡献在于智能体工作流架构而非纯应用。",
                    "summary2": "本文旨在解决大规模优化模型自动构建中长输入处理与推理的挑战。针对包含大规模数据集的自然语言描述，我们提出了一种名为 LEAN-LLM-OPT 的轻量级智能体工作流构建框架，利用多智能体协作与辅助工具将建模任务分解为结构化子任务。我们在 Large-Scale-OR 和 Air-NRM 等基准上通过执行准确性和最优性间隙验证了其有效性，显著优于现有方法。",
                    "summary_translation": "",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
                    "arxiv_id": "2601.09465",
                    "authors": "Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang",
                    "summary": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 EvoFSM，这是一个结构化的自我演化框架，允许智能体通过评论机制（反馈）来改进其有限状态机（FSM）和行为技能，并包含自我演化记忆机制。这完全符合“自我演化：通过反馈自我完善”的研究范围。",
                    "summary2": "本文旨在解决深度研究中固定工作流适应性差及无约束自我进化不稳定的问题。针对开放式查询，我们提出了一种名为EvoFSM的结构化自我进化框架，通过显式Finite State Machine (FSM) 解耦宏观Flow和微观Skill，利用原子操作实现可控进化。在五个多跳QA基准（如DeepSearch）上通过准确率验证了其有效性，显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观困境与现有方案的悖论\n**（观察：从“僵化”到“混乱”的极端跳跃）**\n\n1.  **问题起点**：作者首先观察到现有的基于LLM的深度研究Agent面临一个核心矛盾——**静态工作流的局限性**。传统的Agent依赖预定义的、固定的“工具调用-生成-反思”流水线，这种僵化的结构无法适应真实世界中开放、动态且长尾的查询路径。\n2.  **现有尝试的缺陷**：为了解决僵化问题，学界开始探索“自我进化”，即允许Agent重写自己的代码或Prompt。\n3.  **关键痛点**：作者敏锐地指出，这种“无约束的自我进化”虽然带来了灵活性，却引入了致命的**不稳定性**（Instability）。由于缺乏边界，Agent在自我修改时容易出现指令漂移、幻觉，甚至破坏原有的核心功能模块，导致系统性能退化而非提升。\n    *   *思考结论*：我们需要进化，但不能是混乱的“自由重写”。\n\n### 第二阶段：理论解构与类比启发\n**（假设：从“全局重写”转向“正交解耦”）**\n\n1.  **寻找参照系**：作者将目光投向人类组织的协作模式。人类在解决复杂问题时，效率的提升通常源于两个正交维度的优化，而非混在一起的模糊调整：\n    *   **宏观流程**：任务如何流转，先做什么后做什么。\n    *   **微观技能**：具体某个环节的执行能力（如搜索技巧、总结精度）。\n2.  **提出核心假设**：Agent的自我进化不应是一个混沌的全局重写过程，而应是一个**结构化的、可控的优化过程**。如果将优化空间解耦为“流程”和“技能”两个维度，就能在保持系统稳定性的同时实现适应性。\n\n### 第三阶段：结构化约束的引入\n**（建模：用有限状态机FSM作为“骨架”）**\n\n1.  **选择载体**：为了承载上述的“流程”与“技能”解耦，作者选择了**有限状态机**。\n2.  **逻辑映射**：\n    *   **状态**对应微观的**技能**，每个状态有特定的行为指令。\n    *   **转移**对应宏观的**流程**，定义了状态间的逻辑跳转。\n3.  **确立边界**：FSM提供了一个显式的、确定性的行为边界。它将模糊的推理过程固化为图结构，从而为后续的进化操作提供了坚实的“骨架”，防止进化过程发散。\n\n### 第四阶段：受控进化机制的构建\n**（方法：从“自由文本”到“原子操作”）**\n\n1.  **定义操作规则**：为了在FSM骨架上实现进化，作者摒弃了自由形式的文本修改，转而定义了一套严格的**原子操作**。\n    *   **流程维度**：增加状态、删除状态、修改转移条件。\n    *   **技能维度**：修订特定状态的指令。\n2.  **引入监督**：通过一个**Critic机制**来评估输出，识别失败模式（如缺乏证据、逻辑矛盾），并触发特定的原子操作。\n    *   *思考结论*：这种机制确保了每一次进化都是局部的、可解释的、且可逆的，从而解决了“无约束进化”的不稳定性问题。\n\n### 第五阶段：经验闭环与持续学习\n**（升华：从“单次优化”到“记忆积累”）**\n\n1.  **发现盲点**：如果每次任务都独立进行进化，系统将无法积累经验，效率低下。\n2.  **构建记忆**：作者引入了**自进化记忆机制**。将成功的轨迹提炼为“先验”，将失败的模式提炼为“约束”。\n3.  **形成闭环**：当新任务到来时，系统首先从记忆池中检索相关策略来初始化FSM。这使得Agent不仅是在适应单个任务，而是在跨任务的生命周期中不断学习和成长。\n\n---\n\n**总结：作者的思考路径是从“发现现有Agent要么太死板、要么太疯癫”出发，通过借鉴人类协作的“流程与技能分离”思想，利用FSM作为结构化容器，最终设计出一套“在严格规则下进行原子级微调”的受控进化系统。**"
                },
                {
                    "title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding",
                    "arxiv_id": "2601.09503",
                    "authors": "Siyuan Liu, Hongbang Yuan, Xinze Li, Ziyue Zhu, Yixin Cao, Yu-Gang Jiang",
                    "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于LLM智能体的环境理解和记忆机制，属于单智能体研究范畴（规划、记忆、工具使用）。它提出了评估智能体对环境状态理解能力的范式，而非纯应用或纯推理研究。",
                    "summary2": "本文旨在解决LLM智能体环境理解能力难以量化评估的问题。针对现有轨迹式评估无法衡量环境理解的局限，我们提出了一种Task-to-Quiz (T2Q) 自动化评估范式，通过覆盖导向的任务生成和基于元数据的问答测试解耦“执行”与“认知”。我们在T2QBench上通过Task Success Rate (TSR) 和 Environment Understanding Score (EUS) 验证了其有效性，揭示了任务成功并非环境理解的可靠指标。",
                    "summary_translation": "大语言模型智能体在复杂决策和工具使用任务中展现了显著的能力，但它们在不同环境间泛化的能力仍然是一个尚未得到充分审视的问题。当前的评估范式主要依赖于衡量任务成功率的基于轨迹的指标，而未能评估智能体是否拥有一个基于事实的、可迁移的环境模型。为了解决这一空白，我们提出了 Task-to-Quiz (T2Q)，这是一种确定性的自动化评估范式，旨在将任务执行与世界状态理解解耦。我们在 T2QBench 中实例化了这一范式，该测试套件包含 30 个环境和跨越多个难度等级的 1,967 个基于事实的问答对。我们的大量实验表明，任务成功往往是环境理解的一个不可靠的替代指标，且当前的记忆机制无法有效地帮助智能体获取基于事实的环境模型。这些发现确定了主动探索和细粒度状态表示是主要的瓶颈，为开发更具泛化能力的自主智能体提供了坚实的基础。",
                    "inspiration_trace": "基于论文内容，以下是对作者产出《What Do LLM Agents Know About Their World?》核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n---\n\n### 第一阶段：宏观观察与核心质疑\n**（从“能力表象”到“本质缺陷”）**\n\n1.  **现象观察**：\n    *   作者观察到LLM智能体在网页导航、软件工程等复杂任务上表现惊人，看似具备强大的决策和工具使用能力。\n2.  **发现矛盾**：\n    *   尽管在特定场景下表现强劲，但一旦环境发生微小变化或约束条件稍作修改，智能体的性能往往会断崖式下跌（泛化能力差）。\n3.  **提出核心假设**：\n    *   **质疑**：智能体是真的构建了关于环境的“心智模型”，还是仅仅学会了针对特定任务的“启发式捷径”？\n    *   **推论**：如果智能体只是死记硬背操作步骤，它并不真正“知道”环境的状态（如物体位置、房间连接关系）。因此，**“做”不等于“知”**。\n\n### 第二阶段：评估范式的缺陷识别\n**（从“结果导向”到“过程诊断”）**\n\n1.  **批判现有基准**：\n    *   作者分析现有Agent基准（如WebArena, SWE-bench），发现它们几乎全是**基于轨迹**的。\n    *   **局限性**：这些指标只关注“是否到达终点”或“中间步骤质量”，即只衡量“Doing”。\n2.  **定义新目标**：\n    *   作者提出需要一种**基于环境**的评估，关注“Knowing”。\n    *   **区分维度**：一个智能体可能任务失败但理解了环境（规划错误），也可能任务成功但一无所知（运气好或死记硬背）。因此，必须将**任务执行**与**世界状态理解**解耦。\n\n### 第三阶段：方法设计的挑战与突破\n**（从“概念”到“可操作范式”）**\n\n为了实现上述解耦，作者面临三个核心挑战，并逐一构思了解决方案：\n\n**挑战一：如何让智能体充分探索环境？**\n*   **思考**：如果只给一个单一目标（如“去厨房拿苹果”），智能体会走最短路径，忽略沿途的其他房间和物体，导致测试样本不足。\n*   **解决方案构思**：不要给单一任务，而是给一组**覆盖导向**的任务。\n*   **逻辑转化**：将任务生成建模为**加权集合覆盖问题**。算法的目标是生成一组任务，迫使智能体必须遍历所有房间、打开所有容器，从而最大化对环境的暴露。\n\n**挑战二：如何公平、量化地衡量“理解”？**\n*   **思考**：直接问智能体“苹果在哪？”如果它没去过厨房，答不上来不代表能力差，只是没机会看。如果它去过但忘了，才是记忆问题。如何区分“没看见”和“记不住”？\n*   **解决方案构思**：引入**轨迹先决条件**。\n*   **逻辑转化**：设计两阶段评估。\n    *   **Stage 1（任务阶段）**：让智能体执行覆盖任务，记录其轨迹。\n    *   **Stage 2（测验阶段）**：基于环境元数据生成QA。**关键点**：每个问题绑定先决条件（如“去过厨房”）。如果轨迹不满足条件，该问题标记为“不可回答”，不计入错误。这样就能精准剥离“探索失败”和“记忆/推理失败”。\n\n**挑战三：如何实现自动化与可复现性？**\n*   **思考**：人工出题不现实，用LLM当裁判容易产生幻觉。\n*   **解决方案构思**：选择**TextWorld**作为底层环境。\n*   **逻辑转化**：利用TextWorld构建时即拥有完整元数据（拓扑、属性、状态）的特性。所有问题的答案都由**验证器**基于元数据和轨迹自动计算，实现完全确定性的评估。\n\n### 第四阶段：实证洞察与理论升华\n**（从“数据验证”到“瓶颈定位”）**\n\n1.  **构建基准（T2QBench）**：\n    *   基于上述逻辑，构建了包含30个环境、覆盖不同难度的基准集，定义了两个核心指标：**任务成功率（TSR）**和**环境理解得分（EUS）**。\n\n2.  **实验发现与反思**：\n    *   **发现1**：TSR和EUS并不强相关。随着难度增加，TSR下降明显，但EUS相对稳定。证实了“成功不代表理解”。\n    *   **发现2（反直觉）**：现有的复杂记忆机制（Mem0, LangMem等）往往不如简单的In-context learning。\n    *   **深度思考**：为什么？因为现有记忆系统倾向于做高层摘要，**丢失了细粒度的环境证据**（如具体的物体位置）。\n    *   **发现3**：智能体在需要“主动交互揭示隐藏属性”（如打开盒子看里面）的问题上表现最差。\n    *   **最终结论**：当前Agent的瓶颈不在于“规划”或“检索”，而在于**缺乏主动探索的动机**以及**缺乏细粒度的状态表征能力**。\n\n---\n\n**总结：作者的思考路径**\n从**泛化失败**的现象出发 $\\rightarrow$ 质疑**“知”与“行”的混淆** $\\rightarrow$ 提出**解耦评估**的需求 $\\rightarrow$ 利用**集合覆盖算法**解决探索不足 $\\rightarrow$ 利用**轨迹先决条件**解决评估公平性 $\\rightarrow$ 最终通过实验揭示**探索与细粒度记忆**才是Agent构建世界模型的核心瓶颈。"
                },
                {
                    "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments",
                    "arxiv_id": "2601.09382",
                    "authors": "Qinglong Shi, Donghai Wang, Hantao Zhou, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",
                    "summary": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一种任务导向智能体，专注于在动态环境中维持长期意图和主动性。它涉及单智能体的核心能力，如记忆（维持用户意图）、规划（制定触发条件）以及与环境交互（事件触发跟进），符合单智能体的研究范围。",
                    "summary2": "本文旨在解决现有LLM Agent在动态环境中难以维持长期用户意图的问题。针对动态环境下的长周期任务交互场景，我们提出了一种基于数据驱动的Hybrid-Triggered Proactive Framework，结合意图条件监控与事件触发跟进机制，并构建了ChronosBench基准。我们在ChronosBench上通过任务完成率验证了其有效性，微调后的Qwen3-32B在复杂场景下达到85.19%，优于现有闭源模型。",
                    "summary_translation": "当前的大型语言模型代理主要在反应式范式下运作，仅在短期会话中响应用户的直接查询。这种局限性阻碍了它们维持长期用户意图以及动态适应不断演变的外部环境的能力。在本文中，我们提出了一种新颖的交互范式，用于主动式任务导向代理，该代理能够弥合相对静态的用户需求与动态环境之间的差距。我们通过两个关键能力将主动性形式化：(i) 意图条件监控：代理基于对话历史自主制定触发条件；(ii) 事件触发跟进：代理在检测到有用的环境更新时主动与用户互动。我们引入了一个高质量的数据合成管道，用于在动态环境中构建复杂的多轮对话数据。此外，我们试图通过提出一个新的基准测试 ChronosBench，来解决动态环境中任务导向交互评估标准缺失的问题。我们评估了目前一些领先的闭源和开源模型，并揭示了它们在长期任务导向交互中的缺陷。此外，我们使用合成数据进行监督学习训练的微调模型，在包含用户意图转变的复杂任务上实现了 85.19% 的任务完成率，优于测试中的其他模型。结果验证了我们数据驱动策略的有效性。",
                    "inspiration_trace": "基于论文《Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 第一阶段：问题洞察与范式反思\n**——从“被动应答”到“主动服务”的鸿沟**\n\n1.  **宏观观察**：\n    作者首先观察到当前大语言模型（LLM）智能体的主流交互模式是**反应式**的。即：用户提问 -> 模型回答。这种模式是“会话受限”的，一旦会话结束，智能体即进入休眠状态。\n\n2.  **现实错位**：\n    作者将视角投向现实世界，发现真实的人类需求往往是**长期**且**动态**的。\n    *   *场景举例*：用户想买某款缺货的商品，或者寻找一份尚未发布的工作。\n    *   *核心矛盾*：用户的需求是持续的，但外部环境（库存、职位发布）是动态变化的。现有的“被动式”智能体无法跨越时间维度来维护用户的意图，导致用户必须反复查询，体验极差。\n\n3.  **初步假设**：\n    要解决这一问题，必须从交互范式上进行根本性转变，即从“被动响应”转向“主动服务”。智能体需要具备一种“前瞻性”，能够在用户不在线时，依然代表用户监控环境变化。\n\n---\n\n### 第二阶段：概念形式化与能力解构\n**——定义什么是“长期任务导向”**\n\n1.  **定义“主动性”**：\n    作者没有停留在模糊的“主动”概念上，而是将其形式化为两个核心能力的结合：\n    *   **意图条件监控**：智能体必须能从对话历史中提取用户意图，并将其转化为可执行的监控条件（如“价格低于300元”）。\n    *   **事件触发跟进**：当环境状态满足上述条件时，智能体必须能自主唤醒用户，而不是等待用户再次询问。\n\n2.  **引入“动态性”挑战**：\n    作者进一步思考，现实比“监控”更复杂。用户的需求不是一成不变的。\n    *   *思考*：用户可能在等待过程中改变了主意（例如预算增加了，或者对商品有了新要求）。\n    *   *推论*：一个合格的长期智能体，不仅要监控环境，还要能处理**意图漂移**。它需要在长期的交互中，动态更新监控条件，而不是死守最初的指令。\n\n---\n\n### 第三阶段：数据策略与解决“冷启动”\n**——如何教模型学会它从未见过的行为？**\n\n1.  **面临的数据困境**：\n    现有的对话数据集（如MultiWOZ等）大多是单轮或短轮次的静态问答，缺乏“时间跨度”和“环境变化”的标注数据。没有数据，模型无法学会这种新的交互范式。\n\n2.  **策略选择：数据合成**：\n    作者决定不依赖人工收集（成本高且难以覆盖动态场景），而是采用**自动化数据合成**。\n\n3.  **构建仿真逻辑**：\n    为了保证合成数据的质量，作者设计了一个**多智能体仿真与质量评估**的闭环流程：\n    *   *场景初始化*：先生成背景（用户画像、初始环境状态）。\n    *   *交互模拟*：让“用户模拟器”和“智能体模拟器”进行对话。\n    *   *质量控制*：引入“裁判”模型，对生成的每一轮对话进行逻辑评分（如：是否符合用户画像？意图是否清晰？）。只有高分对话才会被保留。\n    *   *分支设计*：特意设计了“正向分支”（环境满足需求）和“负向分支”（环境不满足需求，需保持沉默），以训练模型判断何时该打扰用户，何时不该打扰。\n\n---\n\n### 第四阶段：架构设计与工程解耦\n**——让LLM具备“长期记忆”与“感知”能力**\n\n1.  **模型能力的局限性**：\n    LLM 本质上是静态的推理引擎，它无法在后台真正“运行”一个定时任务。如何让一个基于文本的模型具备“监控”能力？\n\n2.  **架构创新：混合触发框架**：\n    作者提出了一个巧妙的解耦方案，将“推理”与“监控”分离：\n    *   **推理层**：模型只负责决策。它输出结构化的指令（如 `SET_REMINDER`），定义监控条件。\n    *   **系统层**：后端系统负责执行。系统根据模型设定的条件，周期性地扫描环境。\n    *   **闭环机制**：当系统发现环境变化满足条件时，它将这一信息作为“观察”注入回模型的上下文中，触发模型执行 `FOLLOW_UP` 动作。\n\n3.  **状态追踪**：\n    为了处理复杂的意图变化，作者引入了结构化的状态槽。模型必须在每一轮对话中更新任务状态（PENDING -> IN_PROGRESS -> COMPLETED），强迫模型显式地“记住”当前任务进展，从而实现对长期意图的维护。\n\n---\n\n### 第五阶段：验证与基准构建\n**——如何证明“主动性”的有效性？**\n\n1.  **评估基准的缺失**：\n    传统的评测集（如MMLU, GSM8K）无法衡量“主动性”和“时间维度”的表现。\n\n2.  **构建 ChronosBench**：\n    作者构建了一个包含时间轴的评测基准。\n    *   *分层测试*：区分“简单场景”（仅测试基础监控）和“复杂场景”（测试意图漂移后的二次监控）。\n    *   *核心指标*：不仅仅是任务完成率，还包括“行为准确性”（是否在正确的时间保持沉默或发起对话）。\n\n3.  **实验验证**：\n    通过在合成数据上微调开源模型（如Qwen, LLaMA），作者证明了这种数据驱动的策略是有效的。微调后的小模型在处理复杂的长周期意图漂移任务上，甚至超越了未微调的顶尖闭源模型（如GPT-4.1），验证了“特定范式数据”对于激发模型特定能力的重要性。\n\n---\n\n### 总结：逻辑演进全貌\n\n作者的思考路径遵循了经典的**“发现问题 -> 定义概念 -> 解决数据瓶颈 -> 设计系统架构 -> 构建验证标准”**的学术闭环：\n\n1.  **痛点**：LLM太被动，无法适应动态、长期的现实任务。\n2.  **定义**：将“主动性”解构为“监控”+“触发”，并强调对“意图漂移”的适应。\n3.  **数据**：利用多智能体仿真+质量裁判，合成高质量的、包含时间跨度的对话数据。\n4.  **系统**：通过结构化输出和混合触发机制，让LLM指挥后端系统实现跨时间的任务维护。\n5.  **验证**：提出新的基准，证明该方法在复杂动态场景下的优越性。"
                },
                {
                    "title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants",
                    "arxiv_id": "2601.09264",
                    "authors": "Ziyi Shi, Xusen Guo, Hongliang Lu, Mingxing Peng, Haotian Wang, Zheng Zhu, Zhenning Li, Yuxuan Liang, Xinhu Zheng, Hai Yang",
                    "summary": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个LLM多智能体决策框架，核心研究内容涉及多智能体之间的协作与通信（communicating with other agents, structured inter-agent communication）以及通过闭环模拟进行联合决策。尽管应用场景是医疗（疫情控制），但论文重点在于多智能体系统的架构设计与协调机制，而非单纯的应用落地，因此符合多智能体的研究范围。",
                    "summary2": "本文旨在解决大流行病控制中区域间政策协调不足且反应滞后的问题。针对美国COVID-19疫情数据及人口流动记录，我们提出了一种基于LLM多智能体的协同政策制定框架，通过智能体间的通信与推理实现跨区域流动管控。在5州及20州的模拟环境中，通过累计感染数、死亡数及发病率等指标验证了其有效性，显著降低了疫情传播。",
                    "summary_translation": "有效的 pandemic control (流行病控制) 需要在 intrinsically interdependent (本质上相互依存) 的各行政区域之间进行及时且协调的 policymaking (政策制定)。然而，human-driven responses (人类主导的应对措施) 往往呈现 fragmented (碎片化) 和 reactive (被动) 的特征，政策通常在孤立状态下制定，且仅在疫情升级后才进行调整，这削弱了 proactive intervention (主动干预) 和 global pandemic mitigation (全球流行病缓解) 的效果。为应对这一挑战，本文提出了一种 large language model (LLM) multi-agent (多智能体) policymaking framework (政策制定框架)，旨在支持跨区域的协调与主动流行病控制。在该框架内，每个行政区域均配备一个作为 AI policymaking assistant (AI政策制定助手) 的 LLM agent (LLM智能体)。该智能体在针对特定区域的 epidemiological dynamics (流行病学动态) 进行推理的同时，与其他智能体进行通信，以考量 cross-regional interdependencies (跨区域相互依存关系)。通过整合 real-world data (真实世界数据)、pandemic evolution simulator (流行病演化模拟器) 以及 structured inter-agent communication (结构化的智能体间通信)，我们的框架使智能体能够通过 closed-loop simulation process (闭环模拟过程)，共同 explore counterfactual intervention scenarios (探索反事实干预场景) 并 synthesize coordinated policy decisions (综合形成协调的政策决策)。我们利用 2020 年 4 月至 12 月期间美国的 state-level (州级) COVID-19 数据，结合 real-world mobility records (真实世界的人员流动记录) 和 observed policy interventions (观察到的政策干预措施)，对所提出的框架进行了验证。与真实世界的流行病结果相比，我们的方法在单个州层面分别将 cumulative infections and deaths (累计感染人数和死亡人数) 减少了高达 63.7% 和 40.1%，而在跨州汇总层面则分别减少了 39.0% 和 27.0%。这些结果表明，LLM multi-agent systems (LLM多智能体系统) 能够通过协调的政策制定实现更有效的流行病控制……",
                    "inspiration_trace": "基于论文《Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 第一阶段：宏观问题识别——从“碎片化”到“系统性失效”\n**观察：** 作者首先回顾了COVID-19期间的全球抗疫现状。尽管各国和各地区都采取了措施，但结果往往不尽如人意。\n**核心矛盾：** 作者敏锐地捕捉到一个根本性的矛盾——**区域间的强相互依赖性**（人口流动导致病毒跨区传播）与**决策机制的碎片化**（各地区各自为政、信息孤岛）之间的冲突。\n**推论：** 人类决策者在面对海量异构数据（流行病学、人口流动、医疗资源）和紧迫的时间压力时，往往倾向于短视和局部最优，导致“以邻为壑”的政策（如异步封锁），最终削弱了全球防控效果。\n**结论：** 问题的根源不在于缺乏单一的科学模型，而在于缺乏一个能够处理跨区域依赖、实现协同决策的机制。\n\n### 第二阶段：技术缺口分析——传统工具与静态模型的局限\n**反思：** 既然人类决策受限，现有的技术工具能否解决这一问题？\n**批判：**\n1.  **传统流行病学模型（如SEIR）：** 虽然能预测趋势，但它们是被动的预测工具，缺乏“行动”能力，无法直接输出政策建议，更难以处理多区域博弈的复杂性。\n2.  **传统优化算法：** 往往基于固定的目标函数，难以应对疫情这种高度不确定、动态变化且包含大量非结构化信息（如政策文本、社会规范）的场景。\n3.  **早期LLM应用：** 现有的LLM研究多用于预测或信息提取，缺乏与动态环境的交互机制，无法进行序列化的决策。\n**推论：** 我们需要一种新的范式，它既能像人类一样理解复杂的语境和异构信息，又能像智能体一样在动态环境中进行序列决策和多方协调。\n\n### 第三阶段：概念融合——LLM智能体与多智能体系统的结合\n**灵感：** 作者将目光投向了**LLM Agents（大模型智能体）**。\n**逻辑映射：**\n*   **行政区域 $\\rightarrow$ 智能体：** 将每个州/地区视为一个独立的智能体。\n*   **政策制定 $\\rightarrow$ 智能体推理：** 利用LLM强大的推理能力，让每个智能体根据本地和邻区的疫情数据生成政策。\n*   **跨区协调 $\\rightarrow$ 智能体通信：** 引入多智能体通信机制，让智能体之间交换信息（如风险预警、政策意图），从而打破信息孤岛，实现协同。\n**核心假设：** 如果每个区域都有一个具备全局视野（通过通信获得）和局部推理能力的AI助手，那么它们能通过协商达成比人类更优的纳什均衡或社会最优解。\n\n### 第四阶段：方法论构建——闭环仿真与约束优化\n**落地挑战：** 如何让这个抽象的“多智能体系统”在现实世界中运行且具有说服力？\n**解决方案设计：**\n1.  **构建“沙盒”环境（闭环仿真）：** 作者没有直接让AI在真实世界中试错，而是构建了一个基于真实数据校准的**SEIQRD流行病学模拟器**。智能体在模拟器中行动，观察结果，再调整策略，形成一个“感知-决策-反馈”的闭环。\n2.  **定义具体的政策抓手：** 作者没有让AI控制所有变量（这太复杂且不现实），而是聚焦于一个关键且可控的杠杆——**跨区域人口流动**。\n3.  **创新干预策略（TIR）：** 为了平衡疫情防控与经济活动，作者提出了**“时间流入再分配”**。即不切断流动（保持总量不变），而是通过调整流动的*时间分布*（如推迟高风险来源地的流入）来拉平曲线。这体现了作者对现实约束的深刻理解。\n\n### 第五阶段：验证与迭代——从反事实推演到泛化\n**实证逻辑：** 为了证明方法的有效性，作者采用了**反事实推理**。\n*   **基准对比：** 将AI生成的政策与2020年美国的真实历史数据、专家启发式策略以及随机策略进行对比。\n*   **结果解读：** 结果显示AI策略显著降低了感染和死亡人数。作者进一步通过Shapley值等可解释性工具，分析了AI为何做出某些决策（如“先严后松”策略），验证了其逻辑的合理性。\n*   **扩展性测试：** 从5个州扩展到20个州，并测试了不同的干预频率和策略（如空间抑制、靶向筛查），证明了该框架的鲁棒性和泛化能力。\n\n---\n\n**总结：**\n作者的思考路径遵循了**“现象观察（碎片化失效） $\\rightarrow$ 本质提炼（缺乏协同与动态推理） $\\rightarrow$ 技术选型（LLM多智能体） $\\rightarrow$ 系统设计（闭环仿真+TIR策略） $\\rightarrow$ 实证验证（反事实推演）”**的严密逻辑链条。其核心创新在于将LLM从单纯的“文本生成器”升级为具备环境交互能力和协同决策能力的“政策参谋”，从而解决了复杂社会系统中的协调难题。"
                },
                {
                    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
                    "arxiv_id": "2601.09259",
                    "authors": "Jian Zhang, Zhiyuan Wang, Zhangqi Wang, Yu He, Haoran Luo, li yuan, Lingling Zhang, Rui Mao, Qika Lin, Jun Liu",
                    "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个基于LLM智能体的元自适应推理框架（MAXS），重点研究了智能体在推理过程中的工具使用、规划以及通过前瞻策略解决轨迹不稳定性问题，符合单智能体（规划、工具使用）的研究范围。",
                    "summary2": "本文旨在解决LLM Agents推理中存在的局部短视生成和轨迹不稳定性问题。针对多工具推理场景，我们提出了一种名为MAXS的元自适应推理框架，该框架集成了前瞻策略、复合价值估计及轨迹收敛机制。我们在MathVista、OlympiadBench等五个数据集上，通过准确率和推理Token数验证了其有效性。",
                    "summary_translation": "Large Language Model (LLM) Agents (大语言模型智能体) 通过多工具协作展现出内在的推理能力。然而，在智能体推理过程中，现有方法往往面临以下问题：（i）由于缺乏 lookahead (前瞻) 而产生的 locally myopic generation (局部短视生成)；（ii）trajectory instability (轨迹不稳定性)，即微小的早期错误可能演变为发散的推理路径。这些问题使得难以平衡全局有效性和计算效率。为解决这两个问题，我们提出了 meta-adaptive exploration with LLM agents (基于大语言模型智能体的元自适应探索，MAXS)，这是一个基于 LLM Agents (大语言模型智能体) 的 meta-adaptive reasoning framework (元自适应推理框架)，能够灵活地集成 tool execution (工具执行) 和 reasoning planning (推理规划)。MAXS 采用 lookahead strategy (前瞻策略) 将推理路径向前扩展几步，估算 tool usage (工具使用) 的 advantage value (优势值)，并结合 step consistency variance (步长一致性方差) 与 inter-step trend slopes (步间趋势斜率)，联合筛选出稳定、一致且高价值的推理步骤。此外，我们引入了一种 trajectory convergence mechanism (轨迹收敛机制)，通过在 path consistency (路径一致性) 达成后停止进一步的 rollouts (推演) 来控制计算成本，从而在 multi-tool reasoning (多工具推理) 中实现资源效率与全局有效性的平衡。我们在三个基础模型和五个数据集上进行了广泛的实证研究，结果表明 MAXS 在性能和 inference efficiency (推理效率) 方面均持续优于现有方法。进一步的分析证实了我们的 lookahead strategy (前瞻策略) 及 tool usage (工具使用) 的有效性。",
                    "inspiration_trace": "基于论文《MAXS: Meta-Adaptive Exploration with LLM Agents》的内容，以下是对作者产出该核心方法的逻辑链推演，旨在还原其从宏观观察到微观方法论的思考演进过程：\n\n### 第一阶段：宏观观察与问题界定\n**思考起点：LLM Agents 的潜力与困境**\n作者首先观察到 LLM Agents（通过调用搜索、代码等工具）在复杂推理任务中展现出了巨大的潜力。然而，现有的推理策略在测试时存在明显的两极分化：\n*   **贪婪策略（如 CoT, ToT）：** 虽然计算成本低，但它们是“短视”的。每一步的决策仅基于当前上下文，缺乏对未来的预判。这导致模型无法评估“此时调用工具是否真的对最终结果有价值”。\n*   **全局搜索策略（如 MCTS）：** 虽然通过模拟完整路径解决了短视问题，但计算成本极高（消耗 token 量巨大），难以在实际中部署。\n\n**核心矛盾提炼：**\n如何在保持**全局有效性**（避免短视）的同时，实现**计算效率**（避免高昂成本）？此外，多步推理中存在的**轨迹不稳定性**（早期微小错误导致后续路径发散）也是一个亟待解决的痛点。\n\n---\n\n### 第二阶段：假设提出与核心洞察\n**假设 1：有限的前瞻足以解决短视问题**\n作者推测，并不需要像 MCTS 那样模拟到终点。如果只向前“看”几步，就能估算出当前动作（如使用工具）的潜在价值，从而在决策时获得“全局视野”，同时避免全路径模拟的高昂开销。这借鉴了强化学习中的贝尔曼最优原则。\n\n**假设 2：稳定性是高质量推理的关键**\n除了“价值”高低，推理路径的“质量”还体现在其鲁棒性上。作者意识到，一个正确的推理路径应当是**平滑且一致**的，而不是忽高忽低、剧烈震荡的。如果能量化这种“稳定性”，就能过滤掉那些看似高分实则脆弱的路径。\n\n---\n\n### 第三阶段：方法论构建\n基于上述假设，作者开始构建具体的解决框架，即 MAXS。\n\n**1. 引入“前瞻策略”解决短视**\n*   **逻辑：** 在生成当前步骤时，不仅看当前，还通过 Rollout 机制生成未来 $N$ 步的候选路径。\n*   **目的：** 估算未来回报，判断当前工具调用的必要性，从而将决策从“局部贪婪”转变为“价值感知”。\n\n**2. 定义复合价值函数解决不稳定性**\n为了量化假设 2 中的“稳定性”，作者引入了数学和控制理论的概念，设计了三个维度的评分指标：\n*   **优势分数：** 衡量当前步骤相对于上一步的进步幅度（确保有进展）。\n*   **步级方差：** 借鉴**李雅普诺夫稳定性理论**。如果未来路径的概率对数方差很小，说明系统状态波动有界，路径稳定。\n*   **斜率方差：** 借鉴**利普希茨连续性**。如果路径的局部变化率（斜率）一致，说明推理方向平滑，没有突兀的逻辑跳跃。\n*   **综合决策：** 将上述三者加权，选出既“有价值”又“稳定平滑”的步骤。\n\n**3. 设计“轨迹收敛机制”平衡效率**\n*   **逻辑：** 既然追求稳定性，那么当多个候选路径的评分趋于一致（方差低于阈值）时，意味着模型已经对下一步达成了共识，无需再浪费算力进行搜索。\n*   **目的：** 动态提前终止 Rollout，在保证推理质量的前提下最大化节省计算资源。\n\n---\n\n### 第四阶段：验证与迭代\n**思考闭环：**\n作者通过实验验证了这一逻辑链条的有效性：\n*   **消融实验：** 证明了移除“前瞻”模块会导致性能大幅下降（证实了假设 1）；移除“稳定性”评分会降低准确率（证实了假设 2）。\n*   **效率分析：** 发现 4 步的前瞻是性价比的拐点，再增加步数收益递减，从而确定了最佳的超参数配置。\n\n---\n\n### 总结：作者的思想演进图谱\n1.  **观察：** 现有 Agent 推理要么太笨（短视），要么太贵（MCTS），且容易出错（不稳定）。\n2.  **洞察：** 不需要看完全程，只需“看几步”来评估价值；不仅要看价值，还要看路径是否“稳”。\n3.  **转化：** 用“前瞻 Rollout”实现有限视野的全局规划；用“方差与斜率”将数学上的稳定性理论转化为 LLM 的评分标准。\n4.  **优化：** 用“收敛机制”在路径稳定时及时止损，达成效果与成本的平衡。\n\n最终，这套逻辑汇聚成了 **MAXS** 这一元自适应探索框架，实现了在多工具推理场景下的“聪明”且“经济”的决策。"
                },
                {
                    "title": "The AI Hippocampus: How Far are We From Human Memory?",
                    "arxiv_id": "2601.09113",
                    "authors": "Zixia Jia, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Junzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He, Zilong Zheng, Song-Chun Zhu",
                    "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了“Agentic memory”这一范式，详细讨论了自主智能体中的长期规划、自我一致性以及多智能体系统中的协作行为，完全符合研究范围中关于单智能体（规划、记忆）和多智能体（协作）的定义。尽管论文也涉及多模态内容，但其核心分类包含智能体记忆机制，与LLM智能体主题高度相关。",
                    "summary2": "本文旨在系统梳理大语言模型中的记忆机制。针对LLMs和MLLMs的记忆架构，我们提出了一种包含隐式记忆、显式记忆和智能体记忆的分类法，并在LongMemEval基准上通过正确率和处理时间等指标，对比评估了主流记忆框架的有效性。",
                    "summary_translation": "记忆在增强现代大型语言模型和多模态大型语言模型的推理能力、适应性以及上下文保真度方面发挥着基础性作用。随着这些模型从静态预测器转变为具备持续学习和个性化推理能力的交互式系统，记忆机制的引入已成为其架构和功能演变的中心主题。本综述对LLMs和MLLMs中的记忆进行了全面且结构化的综合，将相关文献组织成一个包含隐式记忆、显式记忆和智能体记忆范式的连贯分类体系。具体而言，本综述阐述了三种主要的记忆框架。隐式记忆是指嵌入在预训练Transformer模型内部参数中的知识，涵盖了其记忆、联想检索和上下文推理的能力。近期的研究工作探索了解释、操纵和重构这种潜在记忆的方法。显式记忆涉及外部存储和检索组件，旨在利用动态的、可查询的知识表示（如文本语料库、Dense vectors和基于图的结构）来增强模型输出，从而实现与信息源的可扩展且可更新的交互。智能体记忆在自主智能体中引入了持久的、时间延展的记忆结构，促进了多智能体系统中的长期规划、自我一致性和协作行为，这与具身智能和交互式AI密切相关。除了文本之外，本综述还考察了多模态设置中记忆的整合，其中视觉、语言、音频和动作模态之间的一致性至关重要。文中讨论了关键的架构进展、基准任务和开放性挑战，包括与记忆容量、对齐、事实一致性和跨系统互操作性相关的问题。",
                    "inspiration_trace": "基于这篇题为《The AI Hippocampus: How Far are We From Human Memory?》的综述论文，我们可以将作者构建这一庞大知识体系的逻辑链还原如下。这并非一篇提出单一算法的论文，而是一次**通过认知科学视角重构AI记忆研究版图**的思维推演。\n\n以下是作者产出该文章的系统性思考过程：\n\n### 第一阶段：宏观观察与问题定义\n**——从“静态预测器”到“动态智能体”的范式转变**\n\n1.  **观察现象**：作者观察到LLM（大语言模型）正在经历根本性的角色转变。它们不再仅仅是静态的文本预测器，而是正在演变为能够持续学习、个性化推理并与环境交互的智能体。\n2.  **识别痛点**：现有的Transformer架构存在固有的局限性——上下文窗口有限、参数固化难以更新、缺乏长期一致性。要实现真正的AGI（通用人工智能），模型必须具备类似人类的“记忆”能力，以积累经验、适应环境。\n3.  **发现混乱**：在调研文献时，作者发现关于“AI记忆”的研究极其碎片化。有的研究关注RAG（检索增强），有的关注模型编辑，有的关注Agent的历史记录，缺乏一个统一的框架来整合这些工作。\n\n### 第二阶段：理论锚定与类比假设\n**——引入“互补学习系统”作为核心隐喻**\n\n1.  **寻找理论支点**：为了解决碎片化问题，作者没有从工程角度强行分类，而是转向认知科学，特别是McClelland等人的**互补学习系统理论**。\n2.  **提出核心假设**：作者假设现代AI系统的记忆架构正在无意识地向人脑结构收敛。如果能用人脑的记忆机制来类比AI，就能为混乱的研究现状提供一个清晰、逻辑自洽的分类学框架。\n3.  **建立映射关系**：\n    *   **新皮层** $\\leftrightarrow$ **隐式记忆**：存储长期语义知识和技能。\n    *   **海马体** $\\leftrightarrow$ **显式记忆**：快速编码新的情景记忆，作为索引。\n    *   **前额叶皮层** $\\leftrightarrow$ **智能体记忆**：负责工作记忆、规划和执行控制。\n\n### 第三阶段：分类学构建与逻辑展开\n**——基于脑科学隐喻的三元架构**\n\n基于上述假设，作者将整个领域划分为三个核心范式，并逐一推演其演进逻辑：\n\n**1. 隐式记忆：挖掘“黑盒”内部**\n*   **思考逻辑**：既然模型参数对应新皮层，那么知识究竟是如何存储在FFN（前馈网络）和Attention（注意力机制）中的？\n*   **推演路径**：从分析“知识神经元”和“键值对记忆”出发，探讨如何在不重新训练的情况下修改这些内部记忆（模型编辑、遗忘），从而解决模型幻觉和知识过时的问题。\n\n**2. 显式记忆：构建“外部大脑”**\n*   **思考逻辑**：既然海马体负责快速学习新知识，那么AI如何通过外部存储（向量数据库、知识图谱）来弥补参数记忆的不足？\n*   **推演路径**：从简单的检索（RAG）演进到如何训练模型更好地利用检索结果（检索增强训练），再到如何处理超长上下文。这对应了海马体作为“索引”的功能。\n\n**3. 智能体记忆：模拟“执行中枢”**\n*   **思考逻辑**：前额叶皮层负责统筹全局。对于一个自主Agent，如何管理短期（上下文窗口）和长期（外部数据库）记忆的交互？\n*   **推演路径**：从单Agent的记忆读写机制，扩展到多Agent之间的记忆共享，再到具体的工程架构（如MemGPT, LangChain）。这关注的是记忆在复杂任务流中的动态调度。\n\n### 第四阶段：跨模态扩展与验证\n**——从文本世界走向物理世界**\n\n1.  **扩展思考**：记忆不应仅限于文本。在视觉、音频和机器人领域，记忆同样至关重要（例如视频理解需要记住前一帧，机器人导航需要记住地图）。\n2.  **逻辑迁移**：作者将上述“隐式-显式-智能体”的三元框架应用到多模态场景中。\n    *   视频模型需要记忆机制来处理长序列（显式记忆）。\n    *   机器人需要构建环境地图（智能体记忆）。\n3.  **验证假设**：通过评估现有的工具和基准测试，验证这种分类法的有效性。作者发现，最先进的系统确实在尝试融合这三种记忆，尽管目前仍处于初级阶段。\n\n### 第五阶段：总结与未来展望\n**——“AI海马体”的终极愿景**\n\n1.  **逻辑闭环**：作者回顾全文，确认了“AI海马体”这一隐喻的有效性。它不仅解释了过去的研究，也指明了未来的方向。\n2.  **提出挑战**：目前的系统往往是割裂的（例如，有好的检索器，但没有好的长期规划）。未来的目标是将这三层记忆（新皮层、海马体、前额叶）无缝整合，形成一个能够自我进化、真正类人的记忆系统。\n\n---\n\n**总结：**\n作者的思考过程是从**工程痛点**（模型静态化）出发，寻找**生物学灵感**（人脑记忆机制），构建**统一分类学**（隐式/显式/智能体），并将其**泛化**到多模态领域，最终形成了一个宏大的理论框架，用以指导下一代AI系统的设计。"
                },
                {
                    "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning",
                    "arxiv_id": "2601.09097",
                    "authors": "Derrick Goh Xin Deik, Quanyu Long, Zhengyuan Liu, Nancy F. Chen, Wenya Wang",
                    "summary": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于多约束规划，这是LLM智能体的核心能力之一。提出的SCOPE框架通过将推理与代码执行（工具使用）分离来改进规划过程，属于单智能体研究中的规划与工具使用范畴。",
                    "summary2": "本文旨在解决LLM在多约束规划中面临的不一致性、错误累积及高成本问题。针对复杂的多约束规划场景，我们提出了一种名为SCOPE的框架，通过分离查询特定推理与通用代码执行，生成可重用的求解器函数。并在TravelPlanner和Natural Plan数据集上，通过Success Rate、Cost及Latency等指标验证了其有效性。",
                    "summary_translation": "Multi-constraint planning (多约束规划) 涉及在满足多个潜在冲突约束的同时，识别、评估和优化候选方案。现有 large language model (LLM) (大语言模型) 方法在该领域面临根本性局限。依赖长自然语言链的 pure reasoning paradigms (纯推理范式)，随着约束的叠加，容易出现不一致性、误差累积以及成本过高的问题。相反，结合了基于代码或 solver-based strategies (基于求解器的策略) 的 LLM 缺乏灵活性：它们通常从零开始生成特定问题的代码，或依赖固定的求解器，无法捕捉跨多样化问题的 generalizable logic (可泛化逻辑)。为了应对这些挑战，我们提出了 Scalable COde Planning Engine (SCOPE) (可扩展代码规划引擎)，这是一个将特定查询的推理与通用代码执行解耦的框架。通过将推理与执行分离，SCOPE 生成的 solver functions (求解器函数) 具有一致性、确定性，并且可在不同查询间复用，同时仅需对输入参数进行极小的更改。SCOPE 在降低成本和延迟的同时，实现了最先进的性能。例如，在使用 GPT-4o 时，它在 TravelPlanner 上达到了 93.1% 的成功率，比最佳 baseline (基线) (CoT (思维链)) 提高了 61.6%，同时将推理成本降低了 1.4 倍，时间缩短了约 4.67 倍。代码可在 https://github.com/DerrickGXD/SCOPE 获取。",
                    "inspiration_trace": "基于论文《Programming over Thinking: Efficient and Robust Multi-Constraint Planning》，以下是对作者产出SCOPE方法逻辑链的系统性推演：\n\n### 第一阶段：问题诊断与现状反思\n**出发点：** LLM在多约束规划任务中面临“能力”与“成本”的双重困境。\n\n1.  **观察“思维”的局限性：**\n    *   作者首先观察到，传统的纯文本推理方法（如Chain-of-Thought, Tree-of-Thought）在处理多约束规划时，随着约束条件的增加，推理路径呈指数级增长。\n    *   **痛点：** 这种基于自然语言的“思维”过程是概率性的，容易在长链条中出现误差累积，且生成大量Token导致推理成本高昂且速度慢。\n\n2.  **观察“编程”的局限性：**\n    *   随后，作者审视了现有的基于代码或求解器的方法。虽然代码执行是确定性的，能解决一致性问题，但现有方法往往针对每个查询从头生成代码。\n    *   **痛点：** 这种“即用即抛”的代码生成模式缺乏复用性。对于同一类问题（如旅行规划），每次新查询都要重新生成逻辑相似的代码，导致效率低下，且无法捕捉跨问题的通用逻辑。\n\n### 第二阶段：核心假设与概念重构\n**关键转折：** 如何结合“思维”的灵活性与“编程”的确定性？\n\n1.  **提出解耦假设：**\n    *   作者意识到，多约束规划中存在两类本质不同的信息：\n        *   **特定查询信息：** 具体的城市、日期、偏好等（随查询变化）。\n        *   **通用执行逻辑：** 如何排列组合、如何筛选约束、如何格式化输出（在同一领域内固定）。\n    *   **假设：** 如果能将“针对特定查询的推理”与“通用的代码执行逻辑”彻底分离，就能让LLM只处理变化的参数，而让代码处理不变的逻辑。\n\n2.  **定义“编程优于思维”范式：**\n    *   基于上述假设，作者确立了核心方法论：**Programming over Thinking**。\n    *   即：不再让LLM通过长文本“思考”出答案，而是让LLM“编写”一个可复用的求解器。推理过程被转化为结构化参数的提取和预定义函数的调用。\n\n### 第三阶段：方法论构建与自动化实现\n**挑战：** 如何让LLM自动完成这种“解耦”并生成正确的求解器？\n\n1.  **设计“问题形式化”阶段：**\n    *   为了实现解耦，首先需要一种中间语言。作者设计了结构化表示（JSON），将问题拆解为：\n        *   **Combinations（组合参数）：** 定义生成候选方案所需的元素（如城市列表）。\n        *   **Constraints（约束参数）：** 定义筛选方案的条件（如特定日期）。\n    *   **逻辑演进：** 通过引入“优化代理”，自动清洗冗余参数、修正逻辑错误，确保提取出的结构化表示既完备又精简，为后续代码生成打下基础。\n\n2.  **设计“通用求解器生成”阶段：**\n    *   基于形式化后的结构，作者进一步将求解器拆解为三个原子函数，以降低代码生成的复杂度：\n        *   **Combination Function：** 负责穷举生成候选方案（替代LLM的枚举思考）。\n        *   **Filter Function：** 负责根据约束筛选方案（替代LLM的逻辑判断）。\n        *   **Deliver Function：** 负责将结果格式化输出。\n    *   **自我修正机制：** 引入“反思代理”，利用单个示例的输入输出对生成的代码进行测试和迭代修正，确保代码逻辑的正确性，而无需人工干预。\n\n### 第四阶段：推理范式与效率验证\n**最终形态：** 一个“一次生成，多次复用”的自动化流水线。\n\n1.  **推理流程的重塑：**\n    *   在实际推理时，系统不再进行复杂的链式思考，而是：\n        *   **Input Agent：** 仅需将新查询映射为结构化参数（轻量级推理）。\n        *   **Solver Execution：** 直接调用预生成的、经过验证的Python函数进行确定性计算（高效执行）。\n    *   **逻辑闭环：** 这种设计将昂贵的“逻辑构建”成本前置（仅在生成求解器时发生一次），而将廉价的“参数填充”成本后置（每次查询仅需少量Token）。\n\n2.  **总结：**\n    *   作者的思考路径从**发现LLM在长链推理中的不稳定性**出发，通过**引入代码执行的确定性**，进而**创新性地提出“推理与执行解耦”**，最终构建了一套**自动化生成可复用求解器**的框架。这不仅解决了准确率问题，更通过复用机制大幅降低了边际推理成本。"
                },
                {
                    "title": "The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments",
                    "arxiv_id": "2601.09032",
                    "authors": "Logan Ritchie, Sushant Mehta, Nick Heiner, Mason Yu, Edwin Chen",
                    "summary": "The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \\emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究基于LLM的智能体，评估其在交互式环境中的多步任务完成能力。文中提出的“智能体能力层级”涵盖了工具使用、规划和适应性等核心单智能体特征，属于LLM智能体的研究范畴，且非纯应用或纯推理研究。",
                    "summary2": "本文旨在评估前沿AI模型在现实职场任务中的多步骤执行能力。针对150个e-commerce客户支持任务，我们提出了一种基于现实RL环境CORECRAFT的实证评估方法，并识别了agentic capabilities hierarchy。在GPT-5.2、Claude Opus 4.5等前沿模型上通过任务完成率验证了其有效性，揭示了模型在工具使用、规划及常识推理等不同层级的能力差距。",
                    "summary_translation": "基于大语言模型 (LLM) 的智能体的发展，已将人工智能 (AI) 评估从单轮响应评估转向了交互式环境中的多步骤任务完成。我们提出了一项实证研究，在来自 Surge 的逼真电商强化学习 (RL) 环境中，评估了前沿 AI 模型在 150 项工作任务上的表现。我们的分析揭示了一个基于经验推导的*智能体能力层级*，模型必须掌握这些能力才能实现实际部署：(1) 工具使用，(2) 规划与目标形成，(3) 适应性，(4) 基础性，以及 (5) 常识推理。即使是表现最佳的模型，也在约 40% 的任务上遭遇失败，且失败情况沿着该层级呈现出可预测的聚集现象。较弱的模型难以应对基本的工具使用和规划，而较强的模型主要在需要超越明确指令进行上下文推断的任务上失败。我们介绍了一种用于强化学习 (RL) 环境的以任务为中心的设计方法论，该方法强调多样性和领域专家的贡献；我们提供了详细的失败分析，并讨论了其对智能体开发的启示。研究结果表明，尽管当前的前沿模型能够展现出连贯的多步骤行为，但在逼真的工作场所设置中实现人类水平的任务完成之前，仍存在巨大的能力差距。",
                    "inspiration_trace": "基于论文《The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments》，以下是对作者产出该文章核心思想的逻辑链推演：\n\n### 1. 宏观观察与问题提出：从“对话”到“行动”的范式转移\n*   **观察背景**：2025年，AI系统正从单纯的“对话助手”向能够执行复杂任务的“自主智能体”演进。\n*   **核心疑问**：尽管模型参数和基准测试分数不断攀升，但一个根本性的问题尚未解决：**AI智能体究竟能在多大程度上完成具有经济价值的现实工作？**\n*   **现状痛点**：现有的评估体系（如MMLU、HellaSwag）主要测试静态知识和单轮推理，无法捕捉现实世界中动态、多步骤、且需要容错的交互式任务特征。\n\n### 2. 差距识别与假设构建：生产实践与学术评估的脱节\n*   **现象对比**：作者注意到学术界的“高能”模型与工业界的“保守”部署之间存在巨大反差。调查显示，生产环境中的智能体通常被限制在极短的步骤内（如不超过10步），且高度依赖人工监督。\n*   **初步假设**：这种“受限自主”并非仅仅出于安全考量，而是反映了当前模型在处理复杂现实任务时存在深层的**能力断层**。现有的评估未能揭示这些断层的具体位置。\n*   **推论**：要理解智能体的真实局限，不能只看“成功率”这一单一指标，而必须深入分析“失败模式”的结构。\n\n### 3. 方法论设计：构建“以任务为中心”的现实模拟器\n*   **设计哲学转变**：为了测试真实能力，作者摒弃了传统的“以环境为中心”（先构建复杂世界，再塞入任务）的思路，转而采用**“以任务为中心”**的设计哲学。\n*   **逻辑依据**：现实世界的复杂性是围绕任务演化的。因此，应先定义具有经济价值的任务（如电商客服），再反向推导所需的实体关系和工具接口。\n*   **环境构建**：构建了 `CORECRAFT` —— 一个模拟高性能PC组件零售的RL环境。引入领域专家设计任务，确保任务包含真实的边缘情况和跨系统推理需求，而非简单的玩具问题。\n\n### 4. 实验发现与模式识别：失败并非随机\n*   **实验执行**：在150个真实职场任务上测试了多个前沿模型（如GPT-5.2, Claude Opus 4.5等）。\n*   **关键数据**：即使是最好的模型，失败率也高达40%。\n*   **深度洞察**：通过分析失败轨迹，作者发现错误并非均匀分布，而是呈现出明显的**聚类特征**。弱模型在基础操作上崩溃，而强模型则卡在特定的推理瓶颈上。这表明智能体的能力并非一个整体，而是存在层级结构。\n\n### 5. 理论抽象：提出“智能体能力层级”\n*   **归纳总结**：基于失败模式的聚类，作者将智能体能力抽象为一个五层金字塔模型：\n    1.  **工具使用**：基础调用。\n    2.  **规划与目标形成**：任务拆解。\n    3.  **适应性**：错误恢复与动态调整。\n    4.  **扎根性**：状态追踪与抗幻觉。\n    5.  **常识推理**：基于语境的隐含推断。\n*   **逻辑演进**：这一层级解释了为什么强模型依然会失败——它们掌握了低阶技能（如工具调用），但在高阶技能（如常识推理）上存在“天花板”。\n\n### 6. 现实回溯与理论验证：解释生产环境的约束\n*   **闭环验证**：作者将“能力层级”理论回溯到最初观察到的生产环境现象。\n*   **逻辑自洽**：\n    *   为什么生产环境限制步骤？因为模型在“规划”和“适应性”层级不够稳健，长链条导致失败率指数级上升。\n    *   为什么需要人工监督？因为模型缺乏“常识推理”，无法处理隐含意图和模糊指令，必须由人类填补这一高阶能力缺口。\n*   **结论升华**：文章最终指出，提升智能体性能不应只追求模型规模的扩大，而应针对特定层级（特别是当前的短板——常识推理）进行定向优化和训练课程设计。\n\n---\n\n**总结**：作者的思考路径是从**宏观趋势**（AI Agent化）出发，通过**现实矛盾**（学术高分vs工业保守）提出问题，利用**拟真环境**（CORECRAFT）进行压力测试，最终从**失败数据**中提炼出**层级理论**，并以此解释了现实世界的部署逻辑。这是一个完整的“观察-假设-实验-理论-验证”的科学闭环。"
                },
                {
                    "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
                    "arxiv_id": "2601.09694",
                    "authors": "Sai Varun Kodathala, Rakesh Vunnam",
                    "summary": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了“智能体引导剪枝”，其中LLM作为智能体，具备自我反思能力，并能根据之前的剪枝结果迭代优化策略，符合单智能体（自我反思）和自我演化（通过反馈自我完善）的研究范围。",
                    "summary2": "本文旨在解决现有LLM剪枝方法依赖手工启发式规则导致事实知识严重退化的问题。针对Qwen3模型，我们提出了一种Agent-guided pruning框架，利用LLM作为自适应代理，结合Wanda指标和梯度重要性构建层敏感度配置文件，并通过自我反思机制迭代选择剪枝层。在Qwen3-4B/8B上，通过MMLU、FreebaseQA和Perplexity验证了其有效性，实现了显著优于结构化剪枝基线的性能。",
                    "summary_translation": "随着 Large Language Models (LLMs，大语言模型) 规模的持续扩大，post-training pruning (后训练剪枝) 已成为一种在保持性能的同时降低计算成本的有前景的方法。现有方法如 SparseGPT 和 Wanda 通过 layer-wise weight reconstruction (逐层权重重构) 或 activation-aware magnitude pruning (基于激活感知的幅度剪枝) 实现了高稀疏度，但依赖于统一或手工设计的启发式规则来确定 per-layer sparsity ratios (每层稀疏度比例)。此外，近期研究表明，经过剪枝的 LLMs 会遭受严重的 factual knowledge degradation (事实知识退化)，其中 structured pruning (结构化剪枝) 方法在 factual question-answering capabilities (事实问答能力) 方面几乎完全崩溃。\n\n我们提出了 agent-guided pruning (智能体引导剪枝)，其中 foundation model (基础模型) 充当 adaptive pruning agent (自适应剪枝智能体)，在每次迭代中智能选择要剪枝的层，同时保留 critical knowledge pathways (关键知识路径)。我们的方法通过结合受 Wanda 启发的 weight-activation metrics (权重-激活指标) 与 gradient importance scores (梯度重要性分数) 来构建 layer-wise sensitivity profiles (逐层敏感度画像)，并将其归一化为 z-scores (z分数) 以实现 model-agnostic (模型无关) 的比较。这些统计数据由具备 self-reflection capabilities (自我反思能力) 的 LLM agent (LLM智能体) 处理，使其能够从之前的剪枝结果中学习并迭代优化其策略。Checkpoint rollback mechanism (检查点回滚机制) 通过在 perplexity degradation (困惑度退化) 超过阈值时进行回滚，来维持模型质量。\n\n我们在约 45% 稀疏度下的 Qwen3 模型（4B 和 8B 参数）上评估了该方法，结果表明相比 structured pruning baselines (结构化剪枝基线) 有显著提升：MMLU 准确率相对提升 56%，FreebaseQA 上的 factual knowledge retention (事实知识保留) 提升 19 倍，perplexity degradation (困惑度退化) 降低 69%。值得注意的是，我们的框架无需 retraining (重训练)，以 model-agnostic (模型无关) 的方式运行，并在 21-40 次迭代中仅通过 2-4 次回滚就表现出有效的 self-correction (自我修正)，证明了 foundation models (基础模型) 能够有效地指导其他基础模型的压缩。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
                    "arxiv_id": "2601.08955",
                    "authors": "Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li",
                    "summary": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了“Imagine-then-Plan”框架，专注于智能体利用世界模型进行前瞻规划和策略学习，属于“单智能体：规划”的研究范畴。论文不涉及纯应用、纯推理、安全对齐、多模态架构或基础设施优化，符合筛选条件。",
                    "summary2": "本文旨在解决LLM智能体缺乏前瞻性推理和深层因果理解的问题。针对复杂任务规划场景，我们提出了一种Imagine-then-Plan (ITP) 框架，通过引入Partially Observable and Imaginable MDP (POIMDP) 和基于世界模型的自适应前瞻机制，使智能体能动态调整想象深度。我们在ALFWorld和ScienceWorld基准上通过成功率验证了其有效性。",
                    "summary_translation": "世界模型的最新进展在建模 environmental states (环境状态) 的未来动态方面展现出巨大潜力，使得 agents (智能体) 能够在无需接入真实环境的情况下进行推理和行动。现有方法主要执行单步或 fixed-horizon rollouts (固定视界推演)，导致其在 complex task planning (复杂任务规划) 方面的潜力尚未得到充分挖掘。我们提出了 Imagine-then-Plan (\\texttt{ITP})，这是一个通过 lookahead imagination (前瞻想象) 进行 agent learning (智能体学习) 的统一框架。在该框架中，智能体的 policy model (策略模型) 与学习到的 world model (世界模型) 进行交互，从而生成多步“imagined trajectories (想象轨迹)”。鉴于 imagination horizon (想象视界) 可能随任务和阶段的不同而变化，我们引入了一种新颖的 adaptive lookahead mechanism (自适应前瞻机制)，通过权衡最终目标与任务进度来确定视界。生成的想象轨迹提供了关于未来后果的丰富信号（如已取得的进度和潜在冲突），这些信号与当前观测相融合，构建了一个 partially observable and imaginable Markov decision process (部分可观测且可想象的马尔可夫决策过程)，以指导策略学习。我们通过 training-free (免训练) 和 reinforcement-trained (强化训练) 两种变体实现了 \\texttt{ITP}。在多个代表性 agent benchmarks (智能体基准) 上进行的广泛实验表明，\\texttt{ITP} 显著优于具有竞争力的基线方法。进一步的分析验证了我们的自适应前瞻机制大幅增强了智能体的推理能力，为解决更广泛、更复杂的任务提供了宝贵的见解。",
                    "inspiration_trace": "基于论文《Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观问题：智能体的“浅层接地”困境\n**观察起点：**\n当前的LLM智能体虽然在推理和交互上表现出色，但大多数仍处于“浅层接地”状态。它们主要依赖当前的观测和历史轨迹来做决策，本质上是一种**反应式**的行为模式。\n\n**核心痛点：**\n由于缺乏对未来的因果理解，智能体无法预判当前行动的长远后果。这导致在复杂任务中，智能体往往只有在错误发生后（且不可逆转时）才发现问题，缺乏“未雨绸缪”的能力。\n\n### 2. 现有方案的局限：刚性的“世界模型”应用\n**现有工具：**\n为了解决上述问题，学术界引入了“世界模型”，试图让智能体在心理沙盒中模拟环境动态。\n\n**批判性思考：**\n作者发现，现有的世界模型应用方式存在严重的**刚性**：\n*   **单步或固定视界：** 大多数方法仅进行单步验证或固定长度的推演。\n*   **两难困境：**\n    *   如果视界太短，会忽略长期依赖关系，导致规划失败。\n    *   如果视界太长，不仅计算成本高昂，而且模型误差会随着推演步数累积，导致预测失真。\n*   **缺乏灵活性：** 无论任务处于简单阶段还是关键决策点，智能体都使用相同的推演深度，这显然不符合人类“在关键时刻深思，在琐事上快速行动”的智能特征。\n\n### 3. 核心假设：从“反应”走向“深思熟虑”\n**逻辑推演：**\n一个真正智能的代理应当具备** deliberative（深思熟虑）** 的能力。这意味着它不应被动地接受观测，而应主动地进行前瞻性想象。\n\n**关键假设：**\n想象视界不应是一个固定参数，而应是一个**动态变量**。智能体需要根据任务的最终目标与当前进度的权衡，自适应地决定“看多远”。即：在高风险或复杂决策时深挖，在简单操作时浅尝辄止。\n\n### 4. 理论创新：从 POMDP 到 POIMDP\n**概念升维：**\n为了将“想象”正式纳入决策过程，作者意识到传统的**部分可观测马尔可夫决策过程（POMDP）**已不足以描述这种机制。POMDP 仅基于历史和当前观测。\n\n**新框架提出：**\n作者提出了**部分可观测且可想象的马尔可夫决策过程（POIMDP）**。\n*   **核心变化：** 决策状态空间被扩展了。智能体的策略不再仅仅基于“具体的现在”，而是基于“具体的现在” + “可想象的未来”。\n*   **逻辑闭环：** 想象出的轨迹提供了关于未来后果（如进度、冲突）的信号，这些信号被反馈给策略，从而在执行前实现自我修正。\n\n### 5. 方法论构建：Imagine-then-Plan (ITP)\n基于上述理论，作者构建了统一的方法论框架，包含三个核心环节：\n\n1.  **想象：**\n    利用学习到的世界模型，在“心理沙盒”中生成多步的未来轨迹。\n\n2.  **自适应前瞻：**\n    这是解决“刚性”问题的关键。作者设计了机制来动态选择想象步数 $K_t$。\n    *   **推理版 (ITP-I)：** 利用LLM内在的反思能力，在推理时根据任务指令和当前状态决定视界，并利用想象结果进行反思式修正。\n    *   **强化版 (ITP-R)：** 引入一个轻量级的预测器，通过强化学习联合优化“动作策略”和“视界选择策略”，在任务奖励和计算成本之间寻找最优平衡。\n\n3.  **规划：**\n    策略模型融合当前观测和想象轨迹，生成最优动作。\n\n### 6. 逻辑验证与总结\n**最终思考：**\n通过实验，作者验证了这一逻辑链条的有效性：\n*   相比固定视界，自适应前瞻在保持高成功率的同时显著降低了计算成本。\n*   相比仅依赖历史的方法，引入“想象”确实解决了浅层接地问题，提高了长程任务的完成率。\n\n**总结：**\n作者的思考路径是从**发现现有智能体缺乏未来视角**，到**批判现有世界模型应用的僵化**，进而**提出“自适应前瞻”的核心假设**，通过**POIMDP理论化**，最终落地为**ITP这一“先想象后规划”的统一框架**。"
                },
                {
                    "title": "Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning",
                    "arxiv_id": "2601.08846",
                    "authors": "Cagatay Tekin, Charbel Barakat, Luis Joseph Luna Limgenco",
                    "summary": "Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了“Cross-Chain Memory”机制，通过语义缓存存储和检索过去的成功推理模式，属于单智能体研究中的“记忆”和“自我演化/自我改进”范畴。虽然应用于数学推理任务，但其核心贡献在于引入记忆机制来增强推理过程，而非单纯的推理算法优化。",
                    "summary2": "本文旨在解决LLM在迭代推理中重复生成相似策略的问题。针对长上下文推理场景，我们提出了InftyThink with Cross-Chain Memory，这是一种集成基于嵌入的语义缓存的方法，通过检索相似推理模式来引导模型。我们在MATH500、AIME2024和GPQA-Diamond数据集上通过准确率验证了其有效性，揭示了相似性检索在嵌入空间中诱导方向偏差的几何机制。",
                    "summary_translation": "诸如 InftyThink 等基于迭代摘要的推理框架通过控制上下文增长，实现了大型语言模型的长视界推理，但在不同任务中，它们往往会重复生成相似的推理策略。我们提出了 InftyThink with Cross-Chain Memory，这是对原有框架的扩展，它通过引入一个基于嵌入的语义缓存来存储以往成功的推理模式，从而增强迭代推理能力。在每个推理步骤中，模型会检索并基于语义上最相似的存储引理进行条件化处理，从而在不无差别扩展上下文窗口的情况下引导推理过程。在 MATH500、AIME2024 和 GPQA-Diamond 数据集上的实验表明，语义引理检索能够提高结构化领域的准确性，但在包含异构领域的测试中则暴露了其失效模式。对推理轨迹的几何分析揭示，缓存检索在嵌入空间中引入了方向性偏差，从而导致了一致的修复（提高基线准确率）吸引子和破坏（降低基线准确率）吸引子。我们的研究结果突显了基于相似性的记忆在实现大型语言模型自我改进推理方面的优势与局限性。",
                    "inspiration_trace": "基于论文《Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到理论升华的思考过程：\n\n### 1. 宏观问题：长上下文推理的“遗忘”与“重复”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在处理复杂、多步推理任务时的核心矛盾：**计算成本与上下文长度的限制**。\n*   **现有方案的局限：** 以 InftyThink 为代表的迭代摘要框架虽然通过递归压缩上下文解决了长度限制，但作者敏锐地观察到一个副作用——**“有损压缩”**。在反复的压缩过程中，模型丢失了之前解题过程中的具体细节和策略。\n*   **低效现象：** 由于缺乏记忆，模型在面对新问题时，往往需要重新推导那些在旧问题中已经使用过的相似推理策略。这种“重复造轮子”不仅浪费计算资源，也增加了出错的风险。\n\n### 2. 观察与假设：从“人类解题”到“引理复用”\n**思维跃迁：**\n作者将视角转向人类专家的解题模式，试图寻找更高效的机制。\n*   **类比推理：** 人类解决复杂数学或科学问题时，往往不是从头推导，而是复用抽象的“技巧”或“引理”。\n*   **核心假设：** 如果能让 LLM 像人类一样，建立一个“成功策略库”，并在遇到新问题时检索最相关的策略作为提示，就能避免重复推导，从而提升推理效率和准确性。\n*   **技术路径：** 利用嵌入向量的语义相似性来模拟这种“联想记忆”，即通过向量数据库存储过往的推理模式。\n\n### 3. 方法构建：引入跨链记忆的语义缓存\n**方案落地：**\n基于上述假设，作者设计了 **InftyThink with Cross-Chain Memory** 系统，将“记忆”机制算法化。\n*   **存储机制：** 在迭代推理的每一步摘要后，提取出关键的推理步骤（定义为“Lemma”），并将其向量化存入缓存。\n*   **检索机制：** 在处理新问题时，计算问题与缓存中 Lemma 的余弦相似度，检索 Top-K 个最相关的 Lemma 注入到当前的上下文窗口中。\n*   **预期目标：** 这种做法旨在通过提供“最大相关的线索”来引导模型，同时避免引入无关信息导致的“上下文污染”。\n\n### 4. 实验反馈：领域同质性的双刃剑\n**现实检验：**\n作者在 MATH500（数学）、AIME2024（高难数学）和 GPQA Diamond（多学科科学）上进行了验证，结果揭示了方法的边界。\n*   **正面效应：** 在结构化、同质性强的数学领域（MATH500, AIME2024），引入 Lemma 显著提升了准确率。这验证了“策略复用”在逻辑严密领域的有效性。\n*   **负面效应：** 在异构性强、跨学科的 GPQA 数据集上，增加检索数量反而导致性能下降。\n*   **初步反思：** 为什么在数学上行之有效的“相似性检索”，在多学科科学中却成了干扰？作者意识到，简单的语义相似度并不等同于“逻辑有效性”，在异构领域，相似的问题可能需要完全不同的解题路径。\n\n### 5. 理论升华：从“语义相似”到“方向吸引子”\n**深度洞察：**\n为了解释上述实验现象，作者没有停留在工程调优层面，而是深入到模型的几何表示空间进行分析，完成了从“应用”到“理论”的跨越。\n*   **几何视角的转换：** 作者不再仅仅关注检索内容的语义对错，而是关注检索行为如何改变模型在嵌入空间中的**推理轨迹**。\n*   **发现“方向性偏差”：** 实验表明，引入 Lemma 会显著改变推理轨迹的方向。这种改变不是随机的，而是呈现出一种**“吸引子”**效应。\n*   **Fix 与 Break 吸引子：**\n    *   **Fix Attractor：** 某些方向的偏移能将错误的推理路径“拉回”正确轨道。\n    *   **Break Attractor：** 某些方向的偏移则会将原本正确的推理“推离”轨道。\n*   **惊人的结论：** 作者发现，导致 Fix 和 Break 的 Lemma 在语义内容上极其相似（高达 98.8% 的相似度），但在嵌入空间中的**方向向量**却截然不同。\n*   **最终定论：** 记忆增强推理的核心机制，不仅仅是提供语义信息，更是在模型推理的初始阶段施加了一种**几何方向上的微扰**。这种微扰决定了后续推理是收敛于正确答案还是错误答案。\n\n### 总结\n作者的思考路径经历了一个完整的闭环：\n从**工程痛点**（上下文压缩导致策略丢失）出发，\n提出**仿生假设**（引入外部记忆复用引理），\n构建**算法系统**（基于相似度的跨链记忆），\n面对**实验反常**（异构领域性能下降），\n最终通过**几何分析**（方向吸引子理论）揭示了 LLM 推理的深层机制。\n\n这一过程不仅提出了一种改进的推理框架，更重要的是揭示了“相似性检索”在 LLM 内部运作的几何本质。"
                },
                {
                    "title": "DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols",
                    "arxiv_id": "2601.08835",
                    "authors": "Vaarunay Kaushal, Taranveer Singh",
                    "summary": "Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.",
                    "category": "cs.AI",
                    "filter_reason": "该论文研究了多智能体系统（Multi-agent systems）中LLM通过商议达成共识的协议，属于多智能体协作与通信的研究范畴。",
                    "summary2": "本文旨在探究多LLM审议协议的实际价值。针对多LLM系统，我们提出了DELIBERATION BENCH基准，对比了三种审议协议与Best-Single Selection基线，并在270个问题及810次评估中通过胜率和成本分析验证了基线的有效性。",
                    "summary_translation": "大语言模型通过审议形成共识的多智能体系统受到了广泛关注，然而，相较于更简单的方法，其实际价值尚未得到充分审视。我们介绍了 DELIBERATIONBENCH，这是一个受控基准，用于评估三种审议协议，并将其与从模型输出池中选择最佳响应这一强基线进行对比。在涵盖 270 个问题和三个独立种子（共 810 次评估）的实验中，我们发现了一个惊人的负面结果：最佳单一基线达到了 82.5% ± 3.3% 的胜率，显著优于最佳审议协议（13.8% ± 2.6%）。这一 6.0 倍的性能差距具有统计学显著性（p < 0.01），且伴随着 1.5-2.5 倍更高的计算成本。我们的发现挑战了关于复杂性能够提升多 LLM 系统质量的假设。",
                    "inspiration_trace": "基于论文《DeliberationBench: When Do More Voices Hurt?》，以下是对作者产出该文章核心逻辑链的系统性推演：\n\n### 1. 宏观观察与直觉陷阱\n**逻辑起点：** 作者观察到当前学术界和工业界存在一种“直觉崇拜”，即倾向于认为**“多智能体系统（Multi-Agent）”优于“单智能体”**。\n*   **现象：** 受到人类社会中“集思广益”或“委员会决策”的启发，许多研究致力于让多个LLM进行辩论、协商以达成共识，假设这种复杂的交互能提升输出的准确性和鲁棒性。\n*   **质疑：** 这种复杂性真的带来了质量的提升吗？还是仅仅因为对比的基准太弱（例如只对比单个模型的原始输出）？作者意识到，现有的多智能体研究可能陷入了“为了复杂而复杂”的误区。\n\n### 2. 核心问题的聚焦与变量解耦\n**逻辑演进：** 为了验证上述质疑，作者需要剥离干扰项，进行公平的对比。\n*   **关键变量识别：** 多智能体系统的流程通常包含两步：1. 生成多个候选答案；2. 通过某种协议（辩论/投票）整合这些答案。\n*   **核心假设：** 现有文献往往将“多候选生成”和“复杂协商”混为一谈，认为整体优于单模型。作者提出假设：**质量的提升可能仅仅来源于“拥有更多候选答案”，而非“复杂的协商过程”。**\n*   **研究问题转化：** 研究问题从“多智能体是否比单智能体好？”转化为**“在拥有相同候选答案池的情况下，复杂的协商协议是否优于简单的‘择优录取’？”**\n\n### 3. 实验设计的控制逻辑\n**逻辑落地：** 为了验证上述转化后的问题，作者构建了一个严格的控制变量实验。\n*   **输入控制：** 固定输入端，所有协议（包括基线）都使用相同的5个模型生成初始候选答案，确保起跑线一致。\n*   **基线设定：** 设定一个极简但强有力的基线——**Best-Single Selection（最佳单选）**。即不进行任何复杂的辩论或打分，直接让一个强力的裁判模型从5个候选中挑出最好的一个。这代表了“无协商”的极限。\n*   **协议梯度：** 设计了三个复杂度递增的协商协议（盲排、量表打分、参议院辩论），试图测试增加交互复杂度是否能带来边际收益。\n*   **评估维度：** 引入“成本”作为关键维度，质疑即使协商有效，其带来的Token消耗是否值得。\n\n### 4. 结果分析与反直觉发现\n**逻辑验证：** 实验结果揭示了残酷的事实，迫使作者重新审视“协商”的价值。\n*   **现象：** 简单的“最佳单选”基线以82.5%的胜率碾压了所有复杂的协商协议（最高仅13.8%）。\n*   **归因分析：** 为什么“更多声音”反而“有害”？\n    *   **信息丢失：** 协商过程本质是一种“有损压缩”。在整合多个观点时，中间代理往往会丢失细节、平均掉优点，或者被修辞风格而非事实准确性所误导。\n    *   **噪声引入：** 复杂的协议（如辩论）引入了更多的不确定性，而直接选择保留了原始答案的最强信号。\n*   **否定假设：** 即使在理论上最有帮助的场景（如模型间意见分歧大、题目难度高），协商依然未能表现出优势。这证明了协商的失败是系统性的，而非偶发的。\n\n### 5. 理论升华与实践指导\n**逻辑闭环：** 基于上述分析，作者从单纯的实验结果上升到了对研究范式的反思。\n*   **奥卡姆剃刀原则：** 在LLM系统中，**简单性往往优于复杂性**。直接选择比复杂的聚合更有效。\n*   **成本效益批判：** 复杂的协商协议不仅质量更低，而且消耗了数倍的算力（15倍的成本质量比差距）。这在伦理和效率上都是不可接受的。\n*   **最终结论：** 呼吁社区停止盲目追求架构上的复杂性，回归到强基线和模型选择上来。\n\n---\n\n**总结：**\n作者的思考路径是从**“质疑流行趋势”**出发，通过**“解耦变量”**将问题聚焦于“协商过程本身的价值”，利用**“极简基线”**击穿了复杂方法的伪装，最终通过**“信息论视角（有损压缩）”**解释了失败原因，得出了**“简单即最优”**的结论。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 3,
            "papers": [
                {
                    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
                    "arxiv_id": "2601.09688",
                    "authors": "Yibo Wang, Lei Wang, Yue Deng, Keming Wu, Yao Xiao, Huanjin Yao, Liwei Kang, Hai Ye, Yongcheng Jing, Lidong Bing",
                    "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
                    "category": "cs.CL",
                    "filter_reason": "论文主要研究深度研究系统（即具备多步骤规划和工具使用能力的LLM智能体）的评估框架。其提出的评估方法本身采用了“Agentic pipeline”，包含自适应质量评估和主动事实核查（涉及自主提取信息和网络搜索等工具使用行为），符合单智能体的研究范围。",
                    "summary2": "本文旨在解决深度研究系统评估中任务构建依赖人工、评估维度静态及事实核查受限的问题。针对深度研究任务，我们提出了一种自动化框架DeepResearchEval，包含基于Persona的任务构建管道及由Adaptive Point-wise Quality Evaluation和Active Fact-Checking组成的智能评估管道。在9个主流深度研究系统生成的900份报告上，通过质量评分和事实正确率验证了其有效性。",
                    "summary_translation": "Deep research systems (深度研究系统) 广泛应用于多步骤网络研究、分析和跨来源综合，但其评估仍面临挑战。现有的 benchmarks (基准测试) 往往需要高标注成本的任务构建，依赖静态评估维度，或在缺乏引用时无法可靠地验证事实。为填补这些空白，我们提出了 DeepResearchEval，一个用于深度研究任务构建和 agentic evaluation (智能体评估) 的自动化框架。在任务构建方面，我们提出了一种 persona-driven (人设驱动) 的流程，能够基于多样化的用户画像生成逼真且复杂的研究任务，并应用包含 Task Qualification (任务资格) 和 Search Necessity (搜索必要性) 的两阶段过滤器，以仅保留那些需要多源证据整合和外部检索的任务。在评估方面，我们提出了一种包含两个组件的智能体流程：一个是 Adaptive Point-wise Quality Evaluation (自适应逐点质量评估)，它基于每个生成的任务动态推导出特定任务的评估维度、标准和权重；另一个是 Active Fact-Checking (主动事实核查)，它能够通过网络搜索自主提取并验证报告陈述，即使在缺乏引用的情况下也能进行。",
                    "inspiration_trace": "基于论文《DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程。\n\n---\n\n### 第一阶段：宏观观察与范式转移的识别\n**（从“被动问答”到“主动智能代理”）**\n\n1.  **现象观察**：作者首先注意到AI能力的范式转移。LLM不再仅仅是被动生成文本的工具，而是进化为能够自主规划、浏览网页、综合信息的“深度研究系统”（如OpenAI Deep Research, Gemini等）。\n2.  **核心矛盾**：现有的评估基准主要针对传统的QA任务（短答案、单轮交互），而深度研究系统输出的是长篇、多步骤、引用复杂的综合报告。**旧的尺子量不出新产品的质量**。\n3.  **问题定义**：如何构建一个既能适应“长报告”特性，又能自动化、低成本、高准确度评估这些智能代理系统的框架？\n\n### 第二阶段：痛点剖析与假设提出\n**（现有基准的三大缺陷）**\n\n作者深入分析了现有基准（如DeepResearch Bench, LiveResearchBench等），提炼出三个关键痛点，这构成了后续方法设计的直接动因：\n\n1.  **成本痛点**：现有任务依赖专家人工编写，耗时耗力且难以扩展。\n    *   *假设*：能否利用LLM本身来自动化生成高质量任务？\n2.  **维度痛点**：现有评估使用“静态维度”（如通用的清晰度、覆盖度），忽略了不同研究任务的特殊性（例如金融报告看重数据准确性，政策报告看重多方观点平衡）。\n    *   *假设*：评估标准应当是动态的、任务自适应的。\n3.  **盲点痛点**：现有事实核查仅检查“有引用的陈述”，忽略了报告中未引用的事实性声明，且无法验证引用本身是否正确。\n    *   *假设*：评估者应当像侦探一样，主动去检索证据，而不是被动检查引用。\n\n### 第三阶段：方法论构建——任务构建侧\n**（从“专家驱动”到“角色驱动+双重过滤”）**\n\n为了解决“成本痛点”并保证任务质量，作者设计了任务构建的逻辑链：\n\n1.  **第一步：引入“角色”以增强真实性**。\n    *   *思考*：如果直接让LLM出题，它往往生成教科书式的通用问题。真实世界的深度研究往往源于具体的人（如供应链经理、博士生）。\n    *   *决策*：构建“Persona-driven Pipeline”。先生成多样化的虚拟角色（包含背景、领域、动机），再基于角色生成与其需求强相关的任务。\n2.  **第二步：引入“双重过滤”以增强复杂性**。\n    *   *思考*：LLM生成的任务可能太简单，或者不需要联网就能回答（这违背了“深度研究”的初衷）。\n    *   *决策*：\n        *   **过滤器A（任务资格）**：判断任务是否真的需要多源信息整合和深度分析。\n        *   **过滤器B（搜索必要性）**：尝试用LLM内部知识回答，如果内部知识能答好，则剔除该任务。确保留下的任务必须依赖外部检索。\n\n### 第四阶段：方法论构建——评估侧\n**（从“静态打分”到“代理式自适应评估”）**\n\n为了解决“维度痛点”和“盲点痛点”，作者将评估者本身也设计为一个智能代理，分为两个模块：\n\n1.  **模块一：自适应点状质量评估**。\n    *   *思考*：通用的评分标准（如“逻辑清晰”）无法捕捉任务的特殊要求（如“是否对比了中美政策差异”）。\n    *   *决策*：采用“通用维度 + 任务特定维度”的混合策略。\n        *   通用维度：覆盖度、洞察力等（所有任务共有）。\n        *   **动态生成维度**：针对每个任务，让LLM分析任务需求，自动生成1-3个特定维度（如“政策可行性”、“数据可比性”），并分配权重。\n    *   *逻辑演进*：从“一把尺子量万物”进化为“量体裁衣”。\n\n2.  **模块二：主动事实核查**。\n    *   *思考*：报告中的幻觉往往出现在没有引用的地方，或者引用来源本身是错的。仅检查引用格式是不够的。\n    *   *决策*：构建一个主动搜索的Agent。\n        *   它不关心原文有没有引用。\n        *   它提取所有可验证的陈述（数字、事件、实体）。\n        *   它主动调用搜索工具（如Google Serper）寻找外部证据。\n        *   根据证据判定陈述为“正确”、“错误”或“未知”。\n\n### 第五阶段：逻辑闭环与验证\n**（从“理论设计”到“实证发现”）**\n\n1.  **系统整合**：将“角色驱动的任务生成”与“代理式的自适应评估”连接，形成完整的闭环框架。\n2.  **实验验证与洞察**：\n    *   作者在9个主流系统上运行该框架。\n    *   **关键发现**：所有系统在“任务特定维度”上的得分都显著低于“通用维度”。\n    *   **逻辑自证**：这一发现反向证明了作者方法论的必要性——如果只用旧的静态标准，就会误以为系统表现很好，而忽略了它们在处理具体、复杂需求时的无能。\n\n---\n\n### 总结：作者的思想演进脉络\n\n1.  **起点**：AI进化为深度研究代理，但评估方法滞后。\n2.  **破局**：必须实现**自动化**（解决成本）和**精细化**（解决准确性）。\n3.  **输入端（任务）**：用**Persona（角色）**模拟真实需求，用**Filter（过滤）**确保任务难度。\n4.  **输出端（评估）**：用**Adaptive（自适应）**捕捉任务特性，用**Active（主动）**消除事实核查盲区。\n5.  **核心贡献**：将评估从“静态的裁判”转变为“动态的、具备检索能力的智能代理”，从而匹配深度研究系统的复杂度。"
                },
                {
                    "title": "UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning",
                    "arxiv_id": "2601.09215",
                    "authors": "Feng Zhang, Shijia Li, Chunmao Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Jingwen Xu, Han Liu",
                    "summary": "User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了UserLM-R1，作为智能体后训练的关键交互环境（用户模拟器）。研究重点在于模拟用户的战略思维、谈判能力以及目标驱动的决策，这属于多智能体交互与环境建模的范畴。虽然涉及推理，但推理服务于智能体间的博弈与交互，而非纯数学或逻辑推理，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有用户模拟器泛化能力差且易受操纵的问题。针对多样化的任务场景，我们提出了一种名为UserLM-R1的框架，该框架结合了静态与动态用户画像，并采用目标驱动的决策策略生成推理路径。我们在包含对抗性陷阱的构造数据集上，通过session-level和turn-level指标验证了其有效性，实验结果表明UserLM-R1在战略能力和目标达成上显著优于基线模型。",
                    "summary_translation": "User simulators (用户模拟器) 是 agent post-training (智能体后训练) 的关键交互环境，理想的 User simulators (用户模拟器) 能够跨领域泛化，并通过挑战或讨价还价主动参与谈判。然而，当前方法存在两个问题。它们依赖于静态且缺乏上下文感知的 profiles (用户画像)，这需要针对新场景进行大量的人工重新设计，从而限制了泛化能力。此外，它们忽略了人类战略思维，导致容易受到智能体的操纵。为了解决这些问题，我们提出了 UserLM-R1，一种具备推理能力的新型 user language model (用户语言模型)。具体而言，我们首先构建了包含静态角色和动态特定场景目标的全面 profiles (用户画像)，以适应多样化的场景。然后，我们提出了一种 goal-driven decision-making policy (目标驱动决策策略)，在生成响应之前生成高质量的 rationales (推理依据)，并利用 supervised fine-tuning (监督微调) 和 multi-reward reinforcement learning (多奖励强化学习) 进一步优化推理并提升战略能力。大量实验结果表明，UserLM-R1 优于竞争性基线模型，特别是在更具挑战性的 adversarial set (对抗性数据集) 上。",
                    "inspiration_trace": "基于论文《UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning》，以下是对作者核心方法提出逻辑链的系统性推演：\n\n### 1. 宏观背景与核心痛点\n**逻辑起点：** 大模型（LLM）的后训练阶段高度依赖环境反馈，而真实人类反馈的获取成本高昂且难以扩展，因此“用户模拟器”成为关键瓶颈。\n**观察现状：** 现有的用户模拟器主要分为“角色驱动”和“目标驱动”两类，但在实际应用中存在两个致命缺陷：\n*   **泛化性差：** 依赖静态、固定的角色设定。一旦切换业务场景（如从航空客服到零售），需要大量人工重新设计Prompt，无法复用。\n*   **缺乏策略性：** 模拟器过于“顺从”或“机械”，缺乏人类的战略思维。在面对任务代理的诱导或施压时，容易被操纵，无法进行有效的谈判或博弈。\n\n### 2. 核心假设与问题重构\n**思考转折：** 作者意识到，要解决上述问题，不能仅停留在“模拟对话”，而必须深入“模拟人类决策过程”。\n**提出假设：**\n*   **关于泛化：** 真实的人类由“稳定的身份特征”和“随场景变化的动态目标”共同构成。将两者解耦，是实现跨领域泛化的关键。\n*   **关于策略：** 人类在复杂交互中并非直接生成回复，而是先进行“隐性思考”。只有显式地模拟这种“思维链”，模拟器才能具备识别陷阱和主动博弈的能力。\n\n### 3. 方法论构建的逻辑演进\n基于上述假设，作者构建了 UserLM-R1 的三层递进逻辑：\n\n**第一步：重塑用户画像——从“静态”到“动静分离”**\n*   **设计思路：** 为了解决跨领域复用难题，作者将用户画像拆解为两部分。\n    *   **静态画像：** 包含背景、性格、表达风格等，这是用户在不同场景下不变的“内核”。\n    *   **动态画像：** 包含特定场景下的目标列表、决策策略和状态变化（如信任度、情绪），这是随对话演进的“表象”。\n*   **数据来源：** 摒弃虚构角色，转而从社交媒体提取真实静态特征，从真实业务SOP中提取动态目标，确保模拟的真实性。\n\n**第二步：注入推理能力——从“直接回复”到“目标驱动决策”**\n*   **设计思路：** 为了解决“易被操纵”的问题，作者强制模型在生成回复前先进行显式推理。\n*   **机制设计：** 引入“目标驱动的决策策略”。模型在每一轮对话中，必须先分析代理意图、梳理自身关切、规划下一步行动，并更新内部状态（如耐心值、信任度），最后才生成回复。这模拟了人类“三思而后行”的过程。\n\n**第三步：优化训练范式——从“模仿学习”到“多奖励强化学习”**\n*   **设计思路：** 仅有监督微调（SFT）只能让模型学会“怎么思考的格式”，但无法保证思考的质量和策略的有效性。\n*   **机制升级：** 引入强化学习（RL），并设计了复合奖励机制：\n    *   **规则奖励：** 约束推理格式和内容完整性。\n    *   **量表奖励：** 评估角色一致性、推理质量以及最重要的——**策略能力**（如识别陷阱、主动反击）。\n*   **目的：** 通过RL，鼓励模型探索比SFT数据中更优、更具逻辑性和战略性的推理轨迹。\n\n### 4. 验证闭环与价值确认\n**逻辑终点：** 为了证明该方法的有效性，作者意识到常规的对话测试无法体现“策略性”。\n**验证设计：** 构建了一个包含11种陷阱（如虚假紧迫感、诱导性推销）的**对抗性数据集**。\n**最终结论：** 只有在能够识别并抵抗这些复杂陷阱的前提下，一个用户模拟器才算真正具备了“人类推理能力”，从而能够反向训练出更强大的任务代理。\n\n---\n\n**总结：** 作者的思考路径是从**“模拟对话”**上升到**“模拟决策”**，通过**解构用户画像**解决泛化问题，通过**显式推理链**解决策略问题，最后通过**多奖励RL**将这种策略能力最大化。"
                },
                {
                    "title": "Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework",
                    "arxiv_id": "2601.08839",
                    "authors": "Toshiyuki Shigemura",
                    "summary": "This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个“三智能体交叉验证框架”，涉及三个异构LLM在递归交互循环中进行协作与通信，符合“多智能体：协作、通信”的研究范围。尽管涉及可解释性和审计，但其核心贡献在于多智能体架构设计和交互机制，而非单纯的安全对齐技术。",
                    "summary2": "本文旨在分析多LLM系统的稳定性与可解释性。针对异构模型协作场景，我们提出了一种Tri-Agent Cross-Validation Framework，通过递归交互实现Recursive Knowledge Synthesis (RKS)。在47项基于公共LLM部署的对照试验中，通过Reflex Reliability Score (RRS)和Transparency Score (TS)等指标验证了其有效性，系统平均RRS达0.78，约89%的试验实现收敛。",
                    "summary_translation": "本文提出了一个三智能体交叉验证框架，用于分析多模型大语言系统中的稳定性和可解释性。该架构将三个异构 LLMs（大语言模型）——分别用于语义生成、分析一致性检查和透明度审计——整合到一个递归交互循环中。这种设计诱导了递归知识综合，其中中间表示通过相互约束的变换不断被精炼，且这些变换不可约简为单模型行为。在使用公开访问的 LLM 部署（2025年10月）进行的47项对照试验中，我们通过四个指标评估了系统稳定性：反射可靠性评分、透明度评分、偏差检测率和修正成功率。系统达到了平均 RRS = 0.78±0.06，并在约68%的试验中保持了 TS ≥ 0.8。约89%的试验发生了收敛，这支持了理论预测，即透明度审计在复合验证映射中充当压缩算子。本文的贡献主要有三点：(1) 一个用于异构 LLMs 间协同推理的结构化三智能体框架，(2) 一个基于不动点理论的正式 RKS 模型，以及 (3) 在现实的、非 API 公开访问条件下对模型间稳定性的实证评估。这些结果提供了初步的实证证据，表明一种保持安全的、人工监督的多 LLM 架构能够在现实的公开部署环境中实现稳定的递归知识综合。",
                    "inspiration_trace": "基于对论文《Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework》的深入分析，以下是对作者产出该核心方法的逻辑链推演。这一过程展现了从对现有技术局限性的观察，到理论假设的提出，再到具体架构设计与实验验证的完整思维演进。\n\n---\n\n### 第一阶段：宏观问题的识别与观察\n**（从“单点脆弱性”到“系统不稳定性”）**\n\n1.  **观察现象：单模型推理的“逻辑漂移”**\n    *   作者首先观察到，尽管现有的单一大语言模型（LLM）在生成能力上表现强劲，但在处理复杂推理任务时，往往存在“逻辑漂移”现象。即模型在长链推理中容易产生幻觉或前后矛盾。\n    *   现有的自我修正方法（如Reflexion, ReAct）主要依赖于模型自身的“内省”，这本质上是在同一个概率分布内进行采样，难以根除模型固有的偏见和盲区。\n\n2.  **痛点分析：多智能体系统的“黑箱”与“失控”风险**\n    *   虽然多智能体系统（如AutoGPT）被提出以解决上述问题，但作者敏锐地指出，当前的多智能体研究多侧重于**自动化**和**效率**，往往忽视了**计算稳定性**。\n    *   当多个智能体通过API自动交互时，容易形成不受控的反馈循环，导致系统发散或产生不可预测的涌现行为。同时，高昂的API成本和复杂的工程部署限制了普通研究者的复现能力。\n\n**核心问题提炼：** 如何构建一个既能利用多模型互补优势，又能保证数学意义上的收敛性，且安全、低成本、可复现的多LLM推理系统？\n\n---\n\n### 第二阶段：核心假设与理论构建\n**（从“异构互补”到“不动点理论”）**\n\n1.  **提出假设：异构模型的“递归知识合成”（RKS）**\n    *   作者假设，不同厂商的模型（如OpenAI, Google, Microsoft）由于训练数据和架构的差异，拥有不同的“归纳偏置”。\n    *   如果让这些异构模型进行交互，不是为了简单的投票，而是为了**递归地合成知识**，那么它们的差异可以相互约束，从而产生一种超越单模型能力的“涌现知识状态”。\n\n2.  **理论映射：引入数学稳定性证明**\n    *   为了解决“系统不稳定性”的痛点，作者没有停留在经验主义的尝试上，而是寻求控制理论的支撑。\n    *   **关键思想：** 将多智能体的交互过程建模为一个动态系统。作者引入**Banach不动点定理**，假设整个系统的复合算子是一个“压缩映射”。\n    *   **逻辑推演：** 如果系统在迭代过程中，状态空间中的距离不断缩小（即 $\\gamma < 1$），那么系统必然收敛到一个唯一的“不动点”（即稳定且正确的知识状态）。这为系统的稳定性提供了坚实的数学地基。\n\n---\n\n### 第三阶段：架构设计与机制创新\n**（从“抽象算子”到“三体审计框架”）**\n\n1.  **功能解耦：三智能体角色的确立**\n    *   为了实现上述理论中的“压缩映射”，作者将推理过程分解为三个互补的功能模块，而非简单的平行竞争：\n        *   **语义生成模块 ($M_S$)：** 负责发散，生成语言和结构。\n        *   **分析一致性模块 ($M_A$)：** 负责逻辑校验，确保理论自洽。\n        *   **透明度审计模块 ($M_T$)：** 负责收敛，作为“刹车”机制，强制输出符合安全和可解释性标准。\n    *   **逻辑闭环：** $M_T$ 被设计为关键的“收缩算子”。只有当 $M_T$ 审计通过（Transparency Score达标），系统才认为接近了不动点。\n\n2.  **安全哲学：人机回环与“会话级角色分解”（SLRD）**\n    *   **拒绝全自动化：** 出于对“失控反馈循环”的担忧，作者做出了一个反直觉但极具洞察力的设计决策——**拒绝API级的自动路由**。\n    *   **引入“人类桥接”：** 作者认为，为了安全和可审计性，必须由人类作为中介手动传递信息。这虽然牺牲了速度，但换来了对系统状态的完全可观测性和阻断风险的能力。\n    *   **SLRD策略：** 为了在低成本环境下模拟多智能体，作者提出了“会话级角色分解”。即在同一平台的不同聊天会话中运行不同角色。这既利用了免费资源，又通过物理隔离防止了上下文污染，使得“逻辑漂移”更容易被人类监督者察觉。\n\n---\n\n### 第四阶段：实验验证与价值定位\n**（从“理论模型”到“民主化实践”）**\n\n1.  **实验设计：现实环境下的压力测试**\n    *   作者没有使用受控的API（因为那不反映真实用户环境），而是选择了**公开免费/低成本的Web界面**。\n    *   **逻辑考量：** 这种“快照式”的研究虽然牺牲了比特级的可复现性，但验证了该架构在真实、动态、且模型会自动更新的环境下的鲁棒性。\n\n2.  **指标构建：量化稳定性**\n    *   为了验证理论假设，作者设计了Reflex Reliability Score (RRS) 等指标，特别是将Deviation Detection Rate (DDR)赋予最高权重（40%），这反映了作者认为“发现错误”比“生成内容”更接近系统稳定性的核心。\n\n3.  **最终贡献定位**\n    *   作者最终将这篇文章定位为一种**“安全优先的多LLM编排框架”**，而非追求极致效率的自动化代理。\n    *   这标志着作者思想的落脚点：在AI能力日益增强的背景下，**可控性、稳定性和可解释性**比单纯的自动化更有价值，且这种能力应当是去中心化、低门槛的。\n\n---\n\n### 总结：作者的思想演进图谱\n\n1.  **起点：** 单模型不可靠，全自动多智能体不安全且昂贵。\n2.  **转折：** 利用异构模型的差异进行互补，并用控制论（不动点定理）保证其收敛。\n3.  **关键设计：** 引入“审计模块”作为收敛核心，引入“人工桥接”作为安全阀。\n4.  **落地：** 通过会话隔离和免费接口，证明这种高稳定性系统可以低成本、可复现地实现。\n\n这一逻辑链条展示了作者如何从对AI本质缺陷的观察出发，融合数学理论、软件工程架构设计以及对AI伦理的深刻考量，最终构建出一套独特的递归知识合成方法论。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability",
                    "arxiv_id": "2601.09295",
                    "authors": "Handi Chen, Running Zhao, Xiuzhe Wu, Edith C. H. Ngai",
                    "summary": "Large Language Model (LLM) agents deployed in complex real-world scenarios typically operate as spatially distributed entities. However, this physical dispersion constrains agents to limited local perception and finite temporal horizons. We characterize this bottleneck as spatiotemporal partial observability. Given such fragmented awareness, distributed agents struggle to coordinate efficiently. To bridge this gap, we introduce MACRO-LLM, LLM-empowered multi-agent collaborative reasoning under spatiotemporal partial observability. The architecture addresses spatiotemporal constraints via three modules: (1) the CoProposer mitigates temporal uncertainty by verifying candidate actions via predictive rollouts; (2) the Negotiator overcomes spatial myopia by resolving conflicts through mean-field statistical aggregation; and (3) the Introspector ensures continuous adaptation by analyzing historical experience to refine strategies via semantic gradient descent. Extensive evaluations on two complex long-horizon tasks, cooperative adaptive cruise control and pandemic control, demonstrate that our framework effectively mitigates spatiotemporal partial observability through spatial and temporal strategies, enabling robust coordination.",
                    "category": "cs.MA",
                    "filter_reason": "该论文专注于多智能体协作，提出了MACRO-LLM框架来解决时空部分可观测性下的协调问题，涉及规划、冲突解决和自我反思等智能体核心机制，符合多智能体研究范围。",
                    "summary2": "本文旨在解决LLM智能体在时空部分可观测性下的协作推理难题。针对分布式场景中的局部感知与未来不确定性，我们提出了一种名为MACRO-LLM的多智能体协作推理框架，整合了CoProposer、Negotiator和Introspector三大模块。在Cooperative Adaptive Cruise Control (CACC) 和 Pandemic Control (PC) 任务中，通过RMSE、SD及感染率等指标验证了其优越的协作效率与鲁棒性。",
                    "summary_translation": "部署于复杂现实场景中的 Large Language Model (LLM) agents（大语言模型智能体）通常以空间分布实体的形式运作。然而，这种物理分散使智能体局限于有限的局部感知和有限的时间视野。我们将这一瓶颈定义为 spatiotemporal partial observability（时空部分可观测性）。在这种碎片化感知下，分布式智能体难以实现高效协调。为弥合这一差距，我们提出了 MACRO-LLM，这是一种在 spatiotemporal partial observability（时空部分可观测性）条件下进行 LLM-empowered multi-agent collaborative reasoning（大语言模型赋能的多智能体协作推理）的框架。该架构通过三个模块应对时空约束：(1) CoProposer 通过 predictive rollouts（预测推演）验证候选行动，从而缓解时间不确定性；(2) Negotiator 通过 mean-field statistical aggregation（平均场统计聚合）解决冲突，从而克服空间短视；(3) Introspector 通过分析历史经验并利用 semantic gradient descent（语义梯度下降）优化策略，从而确保持续适应。在 cooperative adaptive cruise control（协作自适应巡航控制）和 pandemic control（疫情控制）这两项复杂长时域任务上的广泛评估表明，我们的框架通过空间和时间策略有效缓解了 spatiotemporal partial observability（时空部分可观测性），实现了鲁棒的协调。",
                    "inspiration_trace": "基于论文《MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability》，以下是对作者核心方法产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题定位——从“理想环境”到“物理现实”的落差\n\n**1. 观察现象：**\n现有的LLM智能体研究大多假设处于信息完备或通信无阻的理想环境中（如纯文本对话、软件工程任务）。然而，一旦将这些智能体部署到真实的物理世界（如自动驾驶车队、城市疫情管控），它们立刻面临两个残酷的现实约束：\n*   **空间上：** 智能体是物理分散的，受限于通信带宽和隐私，只能感知局部信息。\n*   **时间上：** 智能体受限于上下文窗口长度，无法记住所有历史，且无法预知未来的环境动态。\n\n**2. 提炼核心矛盾：**\n作者将这一瓶颈抽象为**“时空部分可观测性”**。\n*   **空间近视：** 看不到全局，导致各自为政。\n*   **时间短视：** 看不到未来，导致决策短视且难以应对长尾风险。\n\n**3. 现有方案的局限性分析：**\n*   **传统MARL（多智能体强化学习）：** 训练成本极高，且泛化能力差，换个场景就要重训，无法适应复杂多变的现实。\n*   **现有LLM-MAS（多智能体系统）：**\n    *   *中心化架构：* 依赖中央节点聚合信息，存在通信瓶颈和单点故障。\n    *   *全连接架构：* 通信开销随智能体数量爆炸式增长，不可扩展。\n    *   *仅增加记忆模块：* 只能解决“过去”的问题，解决不了“未来”的不确定性。\n\n---\n\n### 第二阶段：战略假设——解构与分治\n\n**1. 确立解决思路：**\n为了在去中心化（无中央节点）和有限通信的前提下实现高效协作，作者决定采用**“分而治之”**的策略，将“时空部分可观测性”这一复杂问题拆解为两个独立的维度分别攻克。\n\n**2. 维度一：空间维度的协作假设**\n*   **思考：** 既然无法获取全局的原始数据，能否用一种“压缩的统计特征”来代表未观测到的群体？\n*   **灵感来源：** 物理学中的**“平均场理论”**（Mean-Field Theory）。\n*   **假设：** 智能体不需要知道远处每一个邻居的具体状态，只需要知道邻居群体的“平均趋势”和“波动程度”，就能做出合理的局部决策。这能极大降低通信成本。\n\n**3. 维度二：时间维度的推理假设**\n*   **思考：** LLM虽然擅长推理，但在长序列任务中容易产生幻觉或缺乏远见。如何让它在行动前就意识到错误？\n*   **假设：** 引入**“模拟推演”**机制。在真正执行动作前，先在脑海中“预演”未来几步，如果预演结果不安全，就修改计划。同时，利用历史反馈进行自我修正。\n\n---\n\n### 第三阶段：方法论构建——模块化架构设计\n\n基于上述假设，作者构建了一个包含三个核心模块的闭环架构，每个模块对应解决一个特定的子问题：\n\n**1. 解决“未来不确定性”：CoProposer（协同提议者）**\n*   **逻辑：** 为了避免盲目行动，智能体必须具备“前瞻性”。\n*   **设计：** 该模块负责生成初步的行动提案。关键创新在于引入了**“基于推演的验证”**。它不只是生成一个动作，而是模拟未来 $k$ 步的状态变化。如果模拟中发现未来会违反约束（如车祸），则立即回滚并修改提案。\n*   **目的：** 确保提议在时间维度上的安全性和可行性。\n\n**2. 解决“空间冲突”：Negotiator（协商者）**\n*   **逻辑：** 局部最优往往导致全局冲突（例如两车同时变道）。由于无法看到全局，如何达成共识？\n*   **设计：** 该模块负责处理邻居间的冲突。它不交换原始数据，而是交换**“平均场统计特征”**（均值、方差）。智能体结合语义理解和这些统计特征，评估邻居提案的置信度，通过多轮谈判达成局部一致。\n*   **目的：** 在低带宽下实现空间维度的全局协调。\n\n**3. 解决“历史遗忘与策略固化”：Introspector（内省者）**\n*   **逻辑：** 环境是动态变化的，死板的策略行不通。如何像人类一样“吃一堑长一智”？\n*   **设计：** 该模块负责执行后的反思。作者提出了**“语义梯度下降”**的概念。当奖励下降时，计算环境变化的剧烈程度作为“学习率”，生成一段自然语言描述的“语义梯度”（即改进建议），用来更新智能体的长期策略。\n*   **目的：** 实现无需重新训练的持续适应和策略迭代。\n\n---\n\n### 第四阶段：逻辑闭环与验证\n\n**1. 系统整合：**\n将上述三个模块串联成一个循环：\n*   **CoProposer** 生成并验证提案（解决时间问题）；\n*   **Negotiator** 与邻居交换统计特征并解决冲突（解决空间问题）；\n*   执行动作后，**Introspector** 根据结果反思并更新策略（解决适应性问题）。\n\n**2. 实验验证逻辑：**\n作者选择了两个极具代表性的场景进行验证：\n*   **CACC（协同自适应巡航）：** 验证高频、高精度的时空控制能力（微观物理控制）。\n*   **Pandemic Control（疫情控制）：** 验证长周期、大规模的策略规划能力（宏观社会管理）。\n\n**总结：**\n作者的思考路径是从**现实世界的物理约束**出发，批判了现有中心化或纯数据驱动方法的不足，进而通过**时空解构**的视角，融合了**平均场理论**（空间压缩）和**推演验证**（时间前瞻），最终设计出一套去中心化、低通信成本且具备自适应能力的LLM多智能体框架。"
                },
                {
                    "title": "SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration",
                    "arxiv_id": "2601.09434",
                    "authors": "Di Zhao, Longhui Ma, Siwei Wang, Miao Wang, Yi Kong",
                    "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) enhance complex problem solving through multi-agent collaboration, but often incur substantially higher costs than single-agent systems. Recent MAS routing methods aim to balance performance and overhead by dynamically selecting agent roles and language models. However, these approaches typically rely on a homogeneous collaboration mode, where all agents follow the same interaction pattern, limiting collaboration flexibility across different roles. Motivated by Social Capital Theory, which emphasizes that different roles benefit from distinct forms of collaboration, we propose SC-MAS, a framework for constructing heterogeneous and cost-efficient multi-agent systems. SC-MAS models MAS as directed graphs, where edges explicitly represent pairwise collaboration strategies, allowing different agent pairs to interact through tailored communication patterns. Given an input query, a unified controller progressively constructs an executable MAS by selecting task-relevant agent roles, assigning edge-level collaboration strategies, and allocating appropriate LLM backbones to individual agents. Experiments on multiple benchmarks demonstrate the effectiveness of SC-MAS. In particular, SC-MAS improves accuracy by 3.35% on MMLU while reducing inference cost by 15.38%, and achieves a 3.53% accuracy gain with a 12.13% cost reduction on MBPP. These results validate the feasibility of SC-MAS and highlight the effectiveness of heterogeneous collaboration in multi-agent systems.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了一个基于LLM的多智能体系统（MAS）框架，重点研究多智能体之间的协作模式、通信策略以及异构交互，完全符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在构建具有成本效益且支持异构协作的Multi-Agent Systems (MAS)。针对输入查询，我们提出了一种基于Social Capital Theory的SC-MAS框架，该框架将MAS建模为有向图，通过Node Selector、Edge Optimizer和LLM Router动态选择智能体角色、分配边级协作策略及LLM骨干网络。并在MMLU、MBPP等多个基准数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "基于 Large Language Model (LLM，大语言模型) 的 Multi-Agent Systems (MAS，多智能体系统) 通过多智能体协作提升了复杂问题求解能力，但其成本通常显著高于单智能体系统。近期的 MAS 路由方法旨在通过动态选择智能体角色和语言模型，在性能与开销之间取得平衡。然而，这些方法通常依赖于同质化协作模式，即所有智能体遵循相同的交互模式，这限制了不同角色之间的协作灵活性。受 Social Capital Theory (社会资本理论) 的启发——该理论强调不同角色能从不同形式的协作中获益——我们提出了 SC-MAS，这是一个用于构建异构且成本高效的多智能体系统的框架。SC-MAS 将 MAS 建模为有向图，其中边显式地表示成对协作策略，从而允许不同的智能体对通过量身定制的通信模式进行交互。给定输入查询，统一控制器通过选择与任务相关的智能体角色、分配边级协作策略以及为单个智能体分配合适的 LLM 骨干网络，逐步构建出一个可执行的 MAS。多个基准测试上的实验结果证明了 SC-MAS 的有效性。具体而言，SC-MAS 在 MMLU 数据集上将准确率提高了 3.35%，同时将推理成本降低了 15.38%；在 MBPP 数据集上实现了 3.53% 的准确率提升，并降低了 12.13% 的成本。这些结果验证了 SC-MAS 的可行性，并凸显了异构协作在多智能体系统中的有效性。",
                    "inspiration_trace": "基于论文《SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程。\n\n---\n\n### 第一阶段：宏观问题定位——性能与成本的“不可能三角”\n\n**1. 现状观察：**\n作者首先观察到LLM-based Multi-Agent Systems (MAS) 在解决复杂问题上的强大潜力，但也敏锐地指出了其致命弱点：**高昂的推理成本**。相比于单智能体系统，多智能体协作意味着更多的API调用和Token消耗。\n\n**2. 现有路径的局限性：**\n*   **单智能体路由：** 现有的LLM路由技术（如FrugalGPT）虽然能平衡成本，但仅限于单模型选择，忽略了多智能体协作带来的性能提升。\n*   **动态多智能体系统：** 现有的动态MAS（如GPTSwarm）虽然能优化结构，但通常假设所有智能体使用相同的底层模型（单一骨干），或者仅仅关注结构优化而忽略了成本维度。\n\n**3. 核心矛盾：**\n如何在保持多智能体协作带来的高**性能**的同时，通过精细化的资源分配实现**低成本**？现有的解决方案要么只管单机省钱，要么只管多机干活，缺乏一个统一的框架来同时解决“选谁”、“怎么协作”和“用什么模型”这三个问题。\n\n---\n\n### 第二阶段：关键缺口识别——“同质化协作”的瓶颈\n\n**1. 深入竞品分析：**\n作者将目光投向了最新的相关工作，特别是MasRouter。这类方法虽然开始尝试联合优化智能体角色和LLM分配，但作者发现了一个被忽视的细节：**它们通常采用“图级”的同质化协作模式**。\n\n**2. 逻辑漏洞：**\n这意味着，在一个系统中，无论智能体A和智能体B是什么关系，它们都必须遵循同一种交互模式（例如全员辩论或全员链式传递）。\n*   *思考：* 这符合现实吗？显然不符合。在人类社会中，程序员和测试员的交互方式（可能是代码审查），与产品经理和程序员的交互方式（可能是需求确认），是完全不同的。\n\n**3. 假设提出：**\n作者认为，这种“一刀切”的同质化协作限制了系统的灵活性，导致了不必要的计算开销（例如，在只需要简单传递信息的节点间使用了高成本的辩论模式）。因此，**打破同质化，实现“边级”的异质协作**，是提升效率的关键。\n\n---\n\n### 第三阶段：理论迁移——社会资本论的引入\n\n**1. 跨学科灵感：**\n为了解决上述协作模式僵化的问题，作者引入了**社会资本理论**。\n*   *理论核心：* 有效的集体行为不仅源于个体能力，更源于个体之间的**关系纽带**。不同的角色组合会形成不同的互动模式。\n\n**2. 概念映射：**\n作者将社会学理论映射到计算机系统构建中：\n*   **个体** $\\rightarrow$ 智能体节点。\n*   **关系纽带** $\\rightarrow$ 图中的**边**。\n*   **互动模式** $\\rightarrow$ 边上的**协作策略**。\n\n**3. 核心洞察：**\nMAS不应仅仅是一个信息传递的网络，而应是一个**策略网络**。每一条边（连接两个智能体）都应该被赋予特定的语义（如“辩论”、“批评”、“链式思考”），而不是仅仅代表“有连接”。\n\n---\n\n### 第四阶段：方法论构建——异质协作图的三步走\n\n基于上述洞察，作者构建了SC-MAS框架，其核心逻辑是将MAS的构建过程分解为一个有序的决策链：\n\n**1. 形式化定义：**\n首先，将MAS重新定义为有向图 $G=(V, E, L)$，其中边 $E$ 不再只是连接，而是显式包含了协作策略 $s$（即 $E \\subseteq V \\times V \\times S$）。\n\n**2. 渐进式构建逻辑：**\n为了在巨大的搜索空间中找到最优解，作者设计了三个串行模块，模拟人类组建团队的思维过程：\n\n*   **第一步：节点选择**\n    *   *思考：* 面对任务，首先需要确定“谁”在场。\n    *   *逻辑：* 并不是所有角色都需要。利用变分潜变量模型，根据Query $q$ 从候选池中筛选出最相关的角色子集 $V$。这解决了“冗余智能体”带来的成本浪费。\n\n*   **第二步：边优化——核心创新点**\n    *   *思考：* 确定了人，接下来确定“关系”。\n    *   *逻辑：* 这是实现“异质协作”的关键。对于每一对智能体 $(u, v)$，系统需要决定它们之间是否存在连接，如果存在，采用什么策略（是Debate还是Chain？）。\n    *   *约束：* 为了保证系统可执行且不陷入死循环，作者引入了**DAG（有向无环图）约束**，在构建过程中自动剪除会产生环路的边。\n\n*   **第三步：LLM路由**\n    *   *思考：* 确定了人和关系，最后分配“大脑”。\n    *   *逻辑：* 既然已经构建了图结构，就可以利用图神经网络（GNN）来捕捉每个节点在结构中的上下文信息。基于这个上下文，为每个智能体分配合适的LLM骨干。例如，处于核心推理节点的智能体分配大模型，处于辅助节点的分配小模型。\n\n---\n\n### 第五阶段：优化目标——效用与成本的博弈\n\n**1. 目标函数设定：**\n作者没有单纯追求准确率，而是将问题建模为一个强化学习过程。\n*   *Reward (奖励) = 任务效用 (Utility) - $\\lambda$ * 执行成本*\n*   其中 $\\lambda$ 是超参数，用于控制对成本的敏感度。\n\n**2. 训练策略：**\n使用策略梯度方法，通过不断的采样、执行、反馈，调整上述三个模块（选择器、边优化器、路由器）的参数，使得系统逐渐学会：**在满足准确率要求的前提下，尽可能构建结构最简单、交互最高效、使用模型最便宜的MAS。**\n\n---\n\n### 总结：思想演进脉络\n\n1.  **痛点：** MAS太贵，现有方法要么省钱但放弃协作，要么协作但忽略成本。\n2.  **发现：** 现有MAS协作模式太僵化（同质化），导致资源浪费。\n3.  **顿悟：** 借鉴社会资本理论，协作模式应该因“关系”而异（异质化）。\n4.  **方案：** 将MAS建模为边带策略的图，通过“选人 $\\rightarrow$ 定关系 $\\rightarrow$ 配模型”三步走动态构建系统。\n5.  **落地：** 利用强化学习在准确率和成本之间寻找最优平衡点。"
                },
            ]
        },
    ],
    "2026-01-13": [
        {
            "name": "Artificial Intelligence",
            "count": 47,
            "papers": [
                {
                    "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents",
                    "arxiv_id": "2601.07577",
                    "authors": "Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",
                    "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于解决长视界LLM智能体的规划问题，提出了包含Supervisor、Planner和Executor的框架来优化任务分解与执行，属于单智能体研究中的“规划”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决长视界智能体规划中上下文纠缠导致的鲁棒性差和效率低问题。针对复杂长视界任务，我们提出了一种Task-Decoupled Planning (TDP)框架，通过Supervisor构建任务DAG，并利用Planner和Executor在局部作用域内解耦规划与执行。我们在TravelPlanner、ScienceWorld和HotpotQA上通过Delivery、Accuracy和Average Reward等指标验证了其有效性，结果表明TDP在提升性能的同时将token消耗降低了82%。",
                    "summary_translation": "大语言模型的最新进展已使智能体能够自主执行复杂的 long-horizon tasks（长视界任务），然而规划仍然是实现可靠任务执行的主要瓶颈。现有方法通常分为两种范式：step-wise planning（逐步规划），具有反应性但往往较为短视；以及 one-shot planning（一次性规划），能够预先生成完整计划，但对执行错误较为脆弱。关键在于，这两种范式都存在 entangled contexts（纠缠上下文）的问题，即智能体必须基于跨越多个子任务的 monolithic history（整体历史）进行推理。这种纠缠增加了 cognitive load（认知负荷），并导致 local errors（局部错误）在原本独立的决策之间传播，从而使得错误恢复的计算成本高昂。为解决这一问题，我们提出了 Task-Decoupled Planning (TDP，任务解耦规划)，这是一个 training-free（免训练）框架，旨在用任务解耦替代纠缠推理。TDP 通过 Supervisor（监督者）将任务分解为由子目标组成的 directed acyclic graph (DAG，有向无环图)。通过利用具有 scoped contexts（限定上下文）的 Planner（规划器）和 Executor（执行器），TDP 将推理和重新规划的范围限制在 active sub-task（当前活动子任务）内。这种隔离机制防止了 error propagation（错误传播），并能够在不干扰 workflow（工作流）的情况下局部修正 deviations（偏差）。在 TravelPlanner、ScienceWorld 和 HotpotQA 上的实验结果表明，TDP 不仅优于强大的 baselines（基线模型），还将 token consumption（令牌消耗）减少了高达 82%，证明了子任务解耦能够提升 long-horizon agents（长视界智能体）的 robustness（鲁棒性）和 efficiency（效率）。",
                    "inspiration_trace": "基于论文《Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents》，以下是对作者核心方法论提出过程的逻辑链推演与思想还原：\n\n### 1. 宏观观察：长程任务的规划瓶颈\n**思考起点：** 随着大语言模型（LLM）能力的提升，智能体已经能够处理复杂的、长周期的自主任务。然而，作者发现尽管模型的理解和推理能力在增强，**“规划”** 依然是制约智能体在长程任务中表现可靠性的核心瓶颈。\n*   **现象：** 任务越复杂、步骤越多，智能体越容易迷失方向或执行失败。\n*   **初步问题：** 现有的规划方法为什么无法有效支撑长程任务？\n\n### 2. 现状剖析：两种范式的共通缺陷\n**思考过程：** 作者首先审视了当前领域内解决规划问题的两大主流范式，试图找出它们的局限性。\n*   **范式 A：逐步规划**\n    *   *特点：* 边思考边行动（如 ReAct）。\n    *   *优点：* 反应快，能适应反馈。\n    *   *缺点：* 目光短浅，缺乏全局观，容易在长程任务中走偏。\n*   **范式 B：一次性规划**\n    *   *特点：* 先生成完整计划再执行（如 Plan-and-Act）。\n    *   *优点：* 具备全局视野。\n    *   *缺点：* 脆弱，一旦执行出错或环境变化，原计划容易失效。\n*   **深度洞察（关键转折）：** 作者发现，虽然这两种方法在“规划粒度”上截然不同（一个细碎，一个宏观），但它们在**底层设计逻辑**上存在一个惊人的共同缺陷——**“上下文纠缠”**。\n    *   *问题本质：* 两者都将整个任务视为一个**单一的、整体的工作流**。智能体在推理时，必须依赖一个不断增长的、混合了所有子任务信息的“整体历史记录”。\n\n### 3. 核心洞察：从“粒度”转向“耦合”\n**思考深化：** 既然调整规划的“粒度”（更细或更粗）无法根本解决问题，作者意识到问题的根源不在于“多久规划一次”，而在于“信息是如何组织的”。\n*   **痛点分析：**\n    1.  **认知负荷过载：** 当上下文窗口中塞满了所有子任务的历史细节时，模型难以聚焦于当前需要解决的子问题。\n    2.  **错误传播：** 如果在子任务 A 中出现局部错误，由于上下文是纠缠的，模型往往需要重新审视甚至重做无关的子任务 B，导致计算成本高昂且脆弱。\n*   **假设提出：** 如果能打破这种“纠缠”，将任务进行**解耦**，就能隔离错误并降低推理负担。\n*   **核心思想：** **任务解耦**。即：将长程任务拆解为独立的子任务，让每个子任务的规划与执行都在**受限的局部上下文**中进行，互不干扰。\n\n### 4. 方法论构建：任务解耦的架构设计\n**思考落地：** 为了实现“解耦”这一抽象概念，作者需要设计一套具体的架构，将“全局视野”与“局部执行”分离开来。\n\n*   **第一步：全局结构化**\n    *   *需求：* 既然要解耦，就需要一个顶层结构来定义子任务之间的关系，否则系统会散架。\n    *   *设计：* 引入 **Supervisor（监督者）**。它的职责不是做具体执行，而是将大任务分解为有依赖关系的**有向无环图（DAG）**。这定义了“做什么”以及“先做什么”。\n\n*   **第二步：局部化执行**\n    *   *需求：* 确保执行子任务 A 时，完全看不到子任务 B 的具体执行细节，只看结果。\n    *   *设计：* 引入 **Planner（规划器）** 和 **Executor（执行器）**。\n    *   *关键机制：* **作用域上下文**。这两个模块只能看到当前节点（子任务）的描述、前置节点的结果以及当前节点的执行轨迹。这种设计强制实现了“上下文隔离”。\n\n*   **第三步：局部化纠错**\n    *   *需求：* 当执行出错时，不能推倒重来，只能局部修复。\n    *   *设计：* 当发生偏差时，触发**节点级重规划**。只修改当前节点的计划，而不影响 DAG 中其他已完成或未开始的部分。这从机制上切断了错误传播的路径。\n\n*   **第四步：动态一致性维护**\n    *   *需求：* 局部执行可能会导致全局目标不可达（例如：前置任务的结果改变了后续任务的条件）。\n    *   *设计：* 引入 **Self-Revision（自我修正）**。在每批节点完成后，检查全局状态，更新 DAG（如修改节点描述、增删节点），确保全局与局部的一致性。\n\n### 5. 逻辑闭环：局部化与全局性的平衡\n**思考验证：** 作者通过这套架构（TDP），试图证明一个观点：**通过显式的架构设计控制上下文范围，比单纯依赖模型的推理能力更有效。**\n*   **预期结果：**\n    *   **鲁棒性：** 错误被锁在局部，不会扩散。\n    *   **效率：** 模型不需要反复处理无关的长历史，Token 消耗大幅降低。\n*   **实验验证：** 选取 TravelPlanner（工具调用）、ScienceWorld（交互控制）、HotpotQA（多跳推理）三个差异巨大的场景进行验证，证明这种“解耦”思想具有普适性。\n\n---\n\n**总结：**\n作者的思考路径是从**表象问题**（长程任务规划难）出发，透过**现有方法的共性缺陷**（上下文纠缠），抓住了**本质矛盾**（认知负荷与错误传播），最终提出了**“任务解耦”**这一核心范式，并通过**Supervisor-Planner-Executor**的三层架构将这一思想工程化，实现了从“调整粒度”到“解耦架构”的范式跃迁。"
                },
                {
                    "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                    "arxiv_id": "2601.07611",
                    "authors": "Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",
                    "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了DIAGPaper，这是一个多智能体框架，包含评审智能体和作者智能体，通过结构化辩论（协作与通信）来识别和验证论文弱点，属于多智能体协作与通信的研究范围。",
                    "summary2": "本文旨在解决现有论文弱点识别系统模拟肤浅、缺乏有效性验证及未排序的问题。针对科学论文评审场景，我们提出了一种DIAGPaper多智能体框架，包含Customizer、Rebuttal和Prioritizer三个模块，分别负责定制评审标准、通过作者辩论验证弱点以及按严重程度排序。在AAAR和ReviewCritique数据集上，通过Semantic F1和Specificity等指标验证了其有效性。",
                    "summary_translation": "使用单智能体或多智能体大语言模型进行论文弱点识别已受到越来越多的关注，然而现有方法存在关键局限性。许多多智能体系统仅在表层模拟人类角色，未能捕捉到专家用于评估论文互补智力维度的潜在标准。此外，先前的方法隐含地假设识别出的弱点是有效的，忽略了审稿人偏见、误解以及作者反驳在验证审稿质量中的关键作用。最后，大多数系统输出未排序的弱点列表，而非为用户优先考虑影响最大的问题。在这项工作中，我们提出了DIAGPaper，这是一个新颖的多智能体框架，通过三个紧密集成的模块来解决这些挑战。定制器模块模拟人类定义的审稿标准，并实例化多个具备特定标准专业知识的审稿人智能体。反驳模块引入作者智能体，使其与审稿人智能体进行结构化辩论，以验证和完善提出的弱点。优先级排序器模块从大规模人类审稿实践中学习，以评估已验证弱点的严重程度",
                    "inspiration_trace": "基于对论文《DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning》的深入分析，以下是作者构建该方法的逻辑演进过程推演：\n\n### 1. 宏观观察：从“角色扮演”到“专家思维”的缺失\n**起点：** 自动化论文审稿领域正从单一LLM向多智能体系统演进。\n**观察：** 现有的多智能体系统（如AgentReview, MARG）大多停留在“表面模拟”阶段。它们只是简单地给智能体分配角色（如“审稿人”、“作者”或“领域主席”），或者按文本段落分工。\n**问题识别：** 真正的人类专家审稿并非仅仅因为身份不同，而是因为**关注的具体评价维度不同**。现有系统缺乏对“评价标准”的显式建模，导致生成的评论泛泛而谈，缺乏针对性。\n\n### 2. 深度诊断：有效性与实用性的双重危机\n在进一步观察中，作者发现了两个更深层次的逻辑漏洞：\n*   **漏洞一（有效性假设谬误）：** 现有系统默认AI生成的弱点是正确的。但在现实中，审稿人常有偏见或误解。**作者反驳**是验证评论质量的关键环节，而现有系统大多忽略了这一“纠错”机制。\n*   **漏洞二（输出效用低）：** 即使生成了正确的弱点，系统通常以平铺列表的形式输出。然而，对于作者而言，区分“致命缺陷”和“轻微瑕疵”至关重要。缺乏优先级排序使得AI审稿的实用性大打折扣。\n\n### 3. 核心假设：模拟“机制”而非模拟“人”\n**假设提出：** 要提高AI审稿的质量，不应只模拟审稿人的“身份”，而应模拟高质量审稿的“内在机制”。\n**逻辑推演：**\n*   机制一：**定制化规划**。专家在拿到论文后，会根据论文内容动态确定审查重点（如：这篇论文主要贡献是数据集，那么审查重点就是数据质量，而非数学推导）。\n*   机制二：**对抗性验证**。评论的有效性不是自证的，而是在与作者的辩论中确立的。只有经得起反驳的弱点，才是真正的弱点。\n*   机制三：**后果导向**。弱点的严重程度取决于其对最终录用决策的影响权重。\n\n### 4. 方法论构建：三模块闭环架构\n基于上述假设，作者构建了DIAGPaper框架，将思考过程转化为三个紧密耦合的模块：\n\n*   **第一步：解构专家思维 -> Customizer（定制器模块）**\n    *   *思考：* 如何让智能体像专家一样有针对性？\n    *   *方案：* 不再使用固定的角色，而是引入一个“定制器”智能体。它先阅读论文，动态生成具体的、细粒度的评价维度（如“数据集的代表性如何？”），然后据此实例化多个具有特定专长的“审稿人智能体”。\n\n*   **第二步：引入对抗验证 -> Rebuttal（反驳模块）**\n    *   *思考：* 如何过滤掉那些看似合理实则错误的幻觉评论？\n    *   *方案：* 引入“作者智能体”。针对每一个审稿人提出的弱点，作者智能体进行逐点反驳。这是一个多轮的、基于证据的辩论过程。如果审稿人无法提供充分的证据或逻辑来支撑其观点，该弱点就会被过滤掉（实验显示过滤掉了40%-60%的初始弱点）。\n\n*   **第三步：模拟决策权重 -> Prioritizer（优先级模块）**\n    *   *思考：* 如何让输出对用户最友好？\n    *   *方案：* 学习人类Meta-review（综合讨论）的行为。分析大量历史数据，计算出不同类别的弱点（如方法缺陷 vs 写作问题）对最终拒稿/录用的影响权重。结合辩论后的有效性得分，对幸存的弱点进行排序，只输出Top-K最严重的问题。\n\n### 5. 逻辑验证与闭环\n**最终思考：** 这个框架是否真的有效？\n*   *验证逻辑：* 如果这个框架是正确的，那么它应该能显著提升开源模型的表现（通过结构化思维弥补能力不足），并且在“有效性”指标上远超现有方法。\n*   *结果确认：* 实验表明，通过DIAGPaper的“多智能体化”，开源模型能达到接近GPT-4o的水平，且生成的弱点在“有效性”和“特异性”上均显著优于基线。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（现有多智能体系统肤浅）**出发，深入到**本质（缺乏评价标准、缺乏验证机制、缺乏优先级）**，最终通过**机制重构（动态定制、对抗辩论、严重度排序）**实现了对人类审稿深层逻辑的还原。"
                },
                {
                    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
                    "arxiv_id": "2601.07477",
                    "authors": "Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",
                    "summary": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究基于LLM的智能体工作流优化，提出了包含Judge模块和优化器的流水线，通过分析执行轨迹和反馈来修改工作流中的逻辑块，属于单智能体的自我反思与自我演化范畴。",
                    "summary2": "本文旨在解决LLM智能体工作流优化中缺乏细粒度反馈信号导致效率低下的问题。针对复杂的智能体工作流，我们提出了一种名为JudgeFlow的Evaluation-Judge-Optimization-Update流水线，通过引入可复用的逻辑块和专门的Judge模块分析执行轨迹并定位问题模块。我们在数学推理和代码生成基准上通过准确率和pass@1验证了其有效性，结果表明该方法优于现有基线。",
                    "summary_translation": "优化基于大语言模型（LLM）的智能体工作流对于扩展人工智能能力而言是一项挑战。现有方法依赖于粗糙的端到端评估信号，缺乏关于具体改进位置的细粒度信号，往往导致低效或低影响力的修改。为了解决这些局限性，我们提出了 JudgeFlow，一种评估-判断-优化-更新流水线。我们将可复用、可配置的逻辑块整合到智能体工作流中，以捕捉基本的逻辑形式。在此抽象基础上，我们设计了一个专用的 Judge 模块，用于检查执行轨迹——特别是失败的运行——并为有问题的逻辑块分配基于排名的责任分数。这些细粒度的诊断信号随后被基于大语言模型的优化器利用，该优化器将修改集中在工作流中最有问题的逻辑块上。我们的方法提高了样本效率，通过块级诊断增强了可解释性，并为自动化日益复杂的智能体工作流提供了可扩展的基础。我们在数学推理和代码生成基准上评估了 JudgeFlow，结果表明 JudgeFlow 相比现有方法实现了更优越的性能和效率。源代码已在 https://github.com/ma-zihan/JudgeFlow 公开提供。",
                    "inspiration_trace": "基于论文《JudgeFlow: Agentic Workflow Optimization via Block Judge》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观问题与现状观察\n**思考起点：如何自动化构建高效的智能体工作流？**\n*   **背景**：随着大模型（LLM）的发展，基于LLM的智能体工作流在解决复杂任务（如数学推理、代码生成）上表现出色。然而，这些工作流的设计目前高度依赖人工经验（如手工设计Prompt、多Agent协作拓扑），成本高且难以扩展。\n*   **现有趋势**：受AutoML启发，学术界开始尝试自动化优化这些工作流。现有的自动化方法（如基于MCTS的搜索、图结构优化）大多将工作流视为一个整体进行端到端的优化。\n\n### 第二阶段：痛点识别与核心瓶颈\n**思考深入：为什么现有的自动化优化效率低下？**\n*   **观察**：现有的优化方法主要依赖“粗粒度”的反馈信号——即只看最终任务是否成功。\n*   **瓶颈分析**：\n    1.  **盲目搜索**：如果只知道“结果错了”，优化器不知道“错在哪里”。这导致优化过程像“盲人摸象”，只能对整个工作流进行随机的、低效的修改（如随机增删模块），样本效率极低。\n    2.  **归因困难**：代码形式的工作流虽然表达能力强，但内部包含复杂的控制流（如循环、条件分支）。当任务失败时，很难精准定位是哪一行代码或哪一个模块导致了错误，特别是那些在特定路径上未被执行的组件。\n\n### 第三阶段：提出假设与关键洞察\n**核心假设：如果能像调试代码一样，精准定位工作流中的“错误源”，就能实现高效的针对性优化。**\n*   **洞察**：优化过程不应是“全局随机试错”，而应是“诊断-治疗”的过程。\n*   **需求转化**：我们需要一种机制，能够从失败的执行轨迹中提取**细粒度的诊断信号**，明确指出工作流中哪个部分对失败负有最大责任。\n\n### 第四阶段：方法论的构建与演进\n为了实现上述假设，作者需要解决两个子问题：**“诊断什么”**（分析对象）和**“如何诊断”**（诊断机制）。\n\n**1. 抽象层设计：从“代码”到“逻辑块”**\n*   **思考**：直接对代码行进行诊断太细碎且难以理解；对整个工作流诊断又太粗糙。我们需要一个中间层。\n*   **创新点**：引入**“逻辑块”**概念。\n    *   将工作流抽象为三种基本逻辑形式的组合：顺序、循环、条件。\n    *   **目的**：这既保留了代码的表达能力，又封装了控制流细节，为诊断提供了一个语义清晰、结构稳定的分析单元。\n\n**2. 诊断机制设计：引入“法官”模块**\n*   **思考**：如何判断哪个逻辑块是“罪魁祸首”？人类专家会看执行日志，LLM也可以。\n*   **创新点**：设计**Judge模块**。\n    *   利用LLM作为“法官”，专门分析**失败案例**的执行轨迹。\n    *   它不关注最终得分，而是对工作流中的各个逻辑块进行**责任排序**，找出导致失败的最关键的那个块。\n\n**3. 优化策略设计：从“全局修改”到“局部手术”**\n*   **思考**：有了诊断结果，优化器该如何行动？\n*   **创新点**：构建**Evaluation-Judge-Optimization-Update闭环**。\n    *   Optimizer不再盲目搜索，而是根据Judge指出的“最差块”，进行针对性的操作（修改该块、删除该块或在该块前后插入新块）。\n\n### 第五阶段：最终逻辑框架的形成\n**总结：JudgeFlow 的诞生**\n*   作者将上述思考整合为一个统一的流水线：\n    1.  **Evaluation**：运行工作流，收集成功/失败信号。\n    2.  **Judge**：对失败案例进行“尸检”，利用逻辑块抽象进行归因，输出最需改进的模块。\n    3.  **Optimization**：LLM优化器根据诊断信号，对特定模块进行精准修补。\n    4.  **Update**：更新工作流池，进入下一轮迭代。\n\n**逻辑演进图示：**\n> **宏观问题**（自动化Agent设计）\n> ↓\n> **现有缺陷**（端到端信号太粗，搜索效率低）\n> ↓\n> **核心假设**（细粒度错误归因能提升效率）\n> ↓\n> **关键支撑**（逻辑块抽象 + LLM法官诊断）\n> ↓\n> **最终方案**（JudgeFlow：诊断驱动的针对性优化闭环）"
                },
                {
                    "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
                    "arxiv_id": "2601.07470",
                    "authors": "Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",
                    "summary": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
                    "category": "cs.AI",
                    "filter_reason": "该论文明确研究LLM智能体的记忆管理机制，提出了Meta-Cognitive Memory Abstraction (MCMA)方法来改进智能体的记忆结构、抽象和重用能力，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在解决LLM智能体记忆表示固定、抽象层次单一导致的泛化受限和负迁移问题。针对长视距交互决策任务，我们提出了一种Meta-Cognitive Memory Abstraction (MCMA) 方法，通过解耦任务执行与记忆管理，利用DPO训练Memory Copilot学习分层结构化记忆抽象策略。在ALFWorld、ScienceWorld和BabyAI数据集上，通过任务成功率、执行步数和奖励分数验证了其有效性和跨任务迁移能力。",
                    "summary_translation": "大语言模型智能体日益依赖累积记忆来解决长视界决策任务。然而，大多数现有方法将记忆存储在固定表示中，并在单一或隐式抽象层级上进行重用，这限制了泛化能力，且在发生分布偏移时往往导致负迁移。本文提出了元认知记忆抽象方法，该方法将记忆抽象视为一种可学习的认知技能，而非固定的设计选择。MCMA 通过结合冻结的任务模型与可学习的记忆副驾驶，实现了任务执行与记忆管理的解耦。该记忆副驾驶利用直接偏好优化进行训练，负责确定记忆的结构化、抽象及重用方式。记忆被进一步组织成抽象层级体系，从而能够基于任务相似度实现选择性重用。当不存在可迁移的记忆时，MCMA 通过迁移记忆副驾驶来传递抽象和管理记忆的能力。在 ALFWorld、ScienceWorld 和 BabyAI 上的实验表明，相较于多个基线方法，MCMA 在性能、分布外泛化以及跨任务迁移方面均实现了显著提升。",
                    "inspiration_trace": "基于论文《Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程。\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**（从“记忆很重要”到“现有记忆机制很脆弱”）**\n\n1.  **观察背景**：随着LLM智能体从静态问答转向长视界、交互式的复杂任务（如ALFWorld, ScienceWorld），智能体必须依赖“程序性记忆”来积累经验，以实现持续决策。\n2.  **发现问题**：尽管现有方法都在尝试存储和检索记忆，但在面对环境变化或任务分布偏移时，性能急剧下降，甚至出现“负迁移”。\n3.  **初步诊断**：现有的记忆机制过于僵化。它们大多将记忆视为静态的“内容”，用固定的格式（如纯文本、固定的键值对）和固定的抽象层级来存储。\n\n### 第二阶段：深入诊断与核心矛盾\n**（从“方法失效”到“抽象困境”）**\n\n1.  **剖析现有范式**：\n    *   **检索式**：直接复用历史轨迹。这导致过度拟合表面细节，一旦环境物体位置改变，记忆即失效。\n    *   **总结/抽象式**：试图提取高层规则。但这面临**“抽象困境”**：太细粒度则过拟合，太抽象则失去可执行性，变成正确的废话。\n    *   **训练式**：将经验内化到模型参数中。这导致记忆与策略耦合，难以跨任务迁移，且容易发生灾难性遗忘。\n2.  **提炼核心矛盾**：现有方法都是**“预设”**了记忆应该如何被表示和抽象。智能体并没有学会“如何记忆”，它只是在使用一个人类设计好的、僵化的存储桶。\n3.  **关键洞察**：人类之所以能灵活迁移记忆，是因为我们拥有**元认知**能力——即“关于思考的思考”。我们不仅存储知识，还学会了“如何组织知识”的认知技能。\n\n### 第三阶段：假设提出与范式转移\n**（从“存储内容”到“学习技能”）**\n\n1.  **核心假设**：记忆抽象不应是一个固定的工程设计，而应是一个**可习得的认知技能**。如果让智能体学会“如何记忆”，它就能自适应地决定记忆的结构和粒度。\n2.  **概念创新**：提出**“元认知记忆抽象”**。目标不是生成完美的记忆内容，而是训练一个能够根据任务需求动态生成记忆结构的“管理者”。\n3.  **架构构想**：为了验证这一假设，必须将“记忆管理”与“任务执行”解耦。如果混在一起，就无法单独评估记忆管理策略的好坏。\n\n### 第四阶段：方法论构建与逻辑闭环\n**（从“概念”到“Memory Copilot”）**\n\n1.  **解耦设计**：\n    *   **任务模型**：保持冻结，只负责执行动作，作为评估记忆好坏的“裁判”。\n    *   **记忆副驾驶**：这是核心创新点。它是一个独立的模型，专门负责将原始轨迹转化为结构化记忆。\n2.  **解决“抽象困境”的机制**：\n    *   **多结构生成**：不预设单一结构，而是让Copilot从树、链、键值对等多种原语中组合出最合适的记忆结构。\n    *   **基于效用的训练**：如何训练Copilot？利用任务模型的下游表现作为反馈。如果某种结构的记忆让任务完成得又快又好，这种结构就被奖励。\n3.  **训练算法选择**：采用**直接偏好优化（DPO）**。通过对比不同记忆结构带来的任务效果，构建偏好对，让Copilot学会生成那些能带来高任务效用的记忆表示。\n\n### 第五阶段：泛化与终极迁移\n**（从“复用知识”到“复用能力”）**\n\n1.  **分层抽象**：为了适应不同相似度的任务，构建记忆层级（从具体的情节记忆到抽象的语义记忆）。相似任务用细节记忆，不相似任务用抽象记忆。\n2.  **解决零样本迁移**：当遇到一个完全陌生的领域，没有任何旧记忆可以复用时怎么办？\n3.  **最终逻辑升华**：此时，我们不再转移“记忆内容”，而是转移**“记忆Copilot本身”**。因为Copilot学到的是“如何从新经验中提炼知识”的元认知能力。这种能力是跨域通用的。\n\n---\n\n**总结：作者的思考路径**\n从**“记忆内容僵化导致泛化失败”**的观察出发，通过**“引入元认知视角”**将问题转化为**“学习记忆抽象技能”**，进而通过**“任务/记忆解耦”**和**“基于效用的DPO训练”**实现了这一技能的习得，最终达成**“不仅复用知识，更复用学习能力”**的通用智能体目标。"
                },
                {
                    "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents",
                    "arxiv_id": "2601.07468",
                    "authors": "Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",
                    "summary": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.",
                    "category": "cs.AI",
                    "filter_reason": "该论文明确研究LLM智能体的“记忆”机制，提出了时序语义记忆（TSM）框架以解决现有方法在时序建模上的不足，属于单智能体研究范围中的“记忆”方向，且不涉及被排除的纯应用、纯推理或安全等领域。",
                    "summary2": "本文旨在解决现有LLM Agent记忆方法在时间维度上的不准确性和碎片化问题。针对个性化对话场景，我们提出了一种Temporal Semantic Memory (TSM)框架，通过构建语义时间线和持续记忆来整合时序连续信息。我们在LONG MEM EVAL和LOCOMO数据集上通过Accuracy指标验证了其有效性，实验表明TSM在多会话理解和时间推理任务上显著优于现有基线方法。",
                    "summary_translation": "记忆机制使 Large Language Model (LLM) agents (大语言模型智能体) 能够感知、存储并利用过往对话中的信息，这对于实现个性化至关重要。然而，现有方法未能对记忆的时间维度进行恰当建模，主要体现在两个方面：1) 时间不准确性：记忆是按对话时间而非实际发生时间进行组织的；2) 时间碎片化：现有方法侧重于 point-wise memory (点状记忆)，从而丢失了能够捕捉持久状态和演变模式的持续信息。为解决上述局限性，我们提出了 Temporal Semantic Memory (TSM) (时间语义记忆)，这是一个为 point-wise memory (点状记忆) 建模 semantic time (语义时间)，并支持 durative memory (持续记忆) 构建与利用的记忆框架。在记忆构建阶段，该框架首先构建 semantic timeline (语义时间轴)，而非对话时间轴。随后，它将时间上连续且语义相关的信息整合为 durative memory (持续记忆)。在记忆利用阶段，该框架结合查询在 semantic timeline (语义时间轴) 上的时间意图，实现对时间上恰当的 durative memory (持续记忆) 的检索，并提供时间有效且持续时间一致的上下文以支持响应生成。在 LongMemEval 和 LoCoMo 数据集上的实验表明，TSM 始终优于现有方法，并实现了高达 12.2% 的准确率绝对提升，验证了所提方法的有效性。",
                    "inspiration_trace": "基于论文《Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程。\n\n---\n\n### 第一阶段：宏观观察与问题锚定\n**思考起点：个性化智能体的“记忆”困境**\n作者首先关注到LLM智能体在长期交互中的核心需求——**个性化**。现有的记忆机制（如RAG、向量数据库）虽然能存储历史对话，但在处理“时间”这一维度时存在根本性缺陷。\n*   **观察**：人类记忆是高度依赖时间线索的，我们不仅记得“发生了什么”，还记得“在什么时间背景下发生的”。然而，现有的LLM记忆系统大多将对话历史视为静态的文档集合，忽略了时间的动态性和语义性。\n\n### 第二阶段：深度诊断与核心痛点\n**思考深入：现有方法到底错在哪里？**\n作者进一步剖析了现有记忆系统在处理时间信息时的两个具体失效模式，从而确立了研究的突破口。\n\n1.  **时间错位**：\n    *   **现象**：用户在5月28日谈论5月29日的行程。现有系统通常以“对话时间”（5月28日）作为索引。\n    *   **问题**：这导致记忆被错误地锚定在聊天发生的时刻，而非事件发生的时刻。当用户查询“明天”或“那次旅行”时，系统无法准确对齐真实世界的时间线。\n    *   **结论**：必须区分“对话时间”与“语义时间”。\n\n2.  **时间碎片化**：\n    *   **现象**：一次为期一周的东京旅行被分散在数十个零散的对话轮次中。\n    *   **问题**：现有方法倾向于存储“点状记忆”，即孤立的事实片段。这种切分破坏了事件的连续性，导致智能体难以形成关于“持续状态”或“演变模式”的整体认知（例如：用户在旅行期间的整体心情或偏好变化）。\n    *   **结论**：需要一种机制将碎片化的信息整合为具有持续性的记忆。\n\n### 第三阶段：概念提出与假设构建\n**思考转折：如何模仿人类认知？**\n基于上述诊断，作者提出了两个核心概念作为解决问题的假设：\n\n1.  **语义时间线**：\n    *   **假设**：如果我们将记忆锚定在事件实际发生的时刻，而非对话记录的时刻，智能体就能像人类一样，在真实的时间轴上检索信息。\n    *   **构想**：构建一条独立于对话流的时间轴，所有记忆都挂载在这条轴上。\n\n2.  **持续性记忆**：\n    *   **假设**：如果将时间上连续且语义相关的片段聚合，形成高阶的摘要（如“主题”或“人设”），就能弥补点状记忆在长时上下文理解上的不足。\n    *   **构想**：记忆不应只是原子事实的堆砌，还应包含对一段时期内状态的总结。\n\n### 第四阶段：方法论设计与逻辑闭环\n**思考落地：如何实现上述概念？**\n作者将抽象概念转化为具体的工程架构，设计了TSM（Temporal Semantic Memory）框架，分为构建与利用两个阶段。\n\n1.  **构建阶段：从碎片到结构**\n    *   **解决“时间错位”**：引入**时序知识图谱**。从对话中提取实体和关系，并显式地标注其有效时间。这不仅是存储，更是建立了一个精确的时间索引。\n    *   **解决“时间碎片化”**：设计**分层聚合机制**。\n        *   *时间切片*：将图谱按时间间隔（如月）切分。\n        *   *语义聚类*：在同一时间片内，对实体进行聚类（GMM），将相关联的事件归为一组。\n        *   *生成摘要*：利用LLM对聚类结果进行总结，生成“主题”和“人设”。这标志着从“ episodic memory”（情景记忆）向“ durative memory”（持续性记忆）的升华。\n\n2.  **利用阶段：意图驱动的检索**\n    *   **逻辑**：用户的查询往往隐含时间意图（如“上周”）。\n    *   **机制**：\n        *   首先解析查询的**语义时间约束**。\n        *   在检索时，不仅计算语义相似度，更强制执行**时间过滤**。只有落在语义时间约束内的记忆（无论是TKG中的事实，还是Durative Memory中的摘要）才会被优先召回。\n        *   通过重排序，确保返回的上下文在时间上是逻辑自洽的。\n\n### 第五阶段：系统优化与工程考量\n**思考完善：如何保证效率与一致性？**\n作者意识到，频繁更新高阶摘要（Durative Memory）计算成本过高，而实时更新图谱（TKG）相对轻量。\n*   **分层更新策略**：\n    *   **在线轻量更新**：实时更新时序知识图谱，保证新事实的即时性。\n    *   **离线定期整合**：在“睡眠时间”定期重新计算和更新主题与人设摘要，平衡了系统的响应速度与长期一致性。\n\n---\n\n**总结：作者的思考路径**\n从**“现有记忆缺乏时间感知”**的宏观观察出发，通过诊断出**“对话时间与事件时间混淆”**和**“记忆碎片化”**两大微观病灶，提出了**“语义时间”**和**“持续性记忆”**的解决假设。最终，通过**时序知识图谱**进行底层时间锚定，结合**聚类摘要**实现高层语义聚合，并利用**时间约束检索**完成逻辑闭环，从而构建了一个能够像人类一样在真实时间线上思考的记忆系统。"
                },
                {
                    "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
                    "arxiv_id": "2601.07342",
                    "authors": "Nicolas Tacheny",
                    "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个LLM智能体框架，重点研究了智能体如何通过工具使用（Tool Use）和规划（Planning）来自主执行诊断任务。这符合单智能体中关于工具使用和规划的研究范围，且侧重于智能体架构与协议的设计，不属于排除的纯应用或基础设施优化（指AI系统部署）范畴。",
                    "summary2": "本文旨在解决传统 Root Cause Analysis (RCA) 耦合度高且难维护的问题。针对电信和数据中心基础设施，我们提出了一种基于 Model Context Protocol (MCP) 的 Agentic Diagnostic Framework，利用 LLM 通过 Investigation Protocol 和受限工具空间进行逐步推理。我们在合成图 Oracle Benchmark 上通过 Investigation Accuracy、RCA Accuracy 和 Impact Accuracy 验证了其有效性，Claude Haiku 3.5 达到了 100% 的准确率。",
                    "summary_translation": "大规模电信和数据中心基础设施依赖于多层服务和资源模型，在此架构下，故障会在物理和逻辑组件之间传播，进而影响多个客户。传统的根因分析（Root Cause Analysis, RCA）方法依赖于硬编码的图遍历算法或基于规则的关联引擎，这些方法不仅维护成本高昂，而且与基础设施模型紧密耦合。在这项工作中，我们提出了一种智能体诊断框架，该框架利用大语言模型（Large Language Model, LLM），通过模型上下文协议（Model Context Protocol, MCP）提供的受限工具空间执行分步调查。该智能体无需将因果逻辑或遍历算法嵌入应用程序，而是通过调用服务查询、依赖关系检索、结构化与非结构化数据分析、事件分析及影响发现等工具，自主在基础设施模型中进行导航。我们定义了一种调查协议，用于规范智能体的推理过程，并确保其具有事实依据、可复现性，并能安全处理缺失或模糊的信息。这项工作为自主事件解决和变更影响缓解奠定了基础。未来的系统不仅能够诊断并修复基础设施故障，还能预测计划变更对服务和客户的影响，从而帮助运维人员在执行维护操作前缓解风险。",
                    "inspiration_trace": "基于论文《Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 1. 宏观观察：传统方法的“刚性”与现实的“动态”矛盾\n**思考起点：** 作者首先审视了电信和数据中心运维的现状。\n*   **观察：** 现代基础设施是多层级的（服务、资源、客户），故障会在物理和逻辑组件间传播。\n*   **痛点：** 传统的根因分析（RCA）依赖于硬编码的图遍历算法或基于规则的关联引擎。\n*   **矛盾：** 基础设施是动态演进的（拓扑变更、命名变化），但传统的RCA逻辑是静态的。这导致了高昂的维护成本和系统与模型的紧耦合。此外，非结构化数据（如人工备注）难以被传统规则引擎处理。\n*   **初步结论：** 我们需要一种更具适应性、能够处理非结构化信息且不随拓扑变更而频繁修改代码的解决方案。\n\n### 2. 核心假设：从“编码逻辑”转向“编码协议”\n**思维转折：** 既然编写具体的因果逻辑（算法）太脆弱，能否让模型自己学会推理？\n*   **引入LLM：** 大语言模型（LLM）具备强大的推理和理解非结构化文本的能力，理论上可以替代硬编码规则。\n*   **风险识别：** 直接让LLM进行诊断存在“幻觉”风险，且无法保证操作的安全性（可能胡乱编造资源ID）。\n*   **关键假设：** 如果不把“因果逻辑”写死在代码里，而是定义一套严格的“调查协议”，并限制LLM只能通过特定工具获取数据，那么LLM就能像人类工程师一样进行“有据可依”的推理。\n*   **思路确立：** **去算法化**。不再试图用代码穷举故障传播路径，而是构建一个能够自主导航信息图的智能体。\n\n### 3. 抽象建模：构建标准化的数字孪生接口\n**落地思考：** 如何让智能体理解复杂的基础设施，同时又不依赖具体的数据库实现？\n*   **本体抽象：** 作者借鉴了TM Forum SID标准，将复杂的基础设施抽象为四个核心实体：**服务**、**资源**、**参与方**、**事件**。这为推理提供了一个通用的语义空间。\n*   **解耦设计：** 为了防止智能体与底层存储技术（如Neo4j或关系型数据库）绑定，作者引入了**模型上下文协议（MCP）**。\n*   **逻辑推演：** MCP充当了“安全边界”和“统一接口”。智能体不直接查询图数据库，而是调用MCP暴露的工具（如`get_implementation`, `get_impacted_services`）。这不仅解耦了系统，还天然防止了SQL注入或非授权访问，确保了每一次数据获取都是可审计的。\n\n### 4. 方法论构建：受控的智能体调查协议\n**核心创新：** 有了工具，如何确保智能体不乱跑、不胡说？\n*   **形式化流程：** 作者意识到，人类专家排查故障是有固定SOP（标准作业程序）的。因此，作者将这种经验形式化为一个**RCA调查协议**。\n*   **步骤设计：**\n    1.  **定位：** 从告警中提取服务名。\n    2.  **下钻：** 获取实现该服务的所有资源。\n    3.  **取证：** 检查每个资源的备注和事件（利用LLM理解非结构化文本）。\n    4.  **上溯：** 确定根因后，反向查找受影响的服务和客户。\n    5.  **发布：** 输出结构化报告。\n*   **约束机制：** 强制要求智能体必须基于工具返回的结果进行推理，如果数据缺失必须明确承认，严禁编造。这解决了LLM的“幻觉”问题，实现了**Grounding（接地气）**。\n\n### 5. 验证与洞察：去算法化的可行性\n**实证思考：** 这种“软逻辑”真的能取代“硬算法”吗？\n*   **实验设计：** 构建了一个合成图，预设了根因和影响路径，测试智能体能否在没有内置图算法的情况下找到答案。\n*   **结果分析：** 实验表明，只要协议设计得当，LLM（如Claude Haiku 3.5）能够达到100%的准确率。\n*   **关键洞察：** 事实证明，**硬编码的图遍历逻辑并非必须**。通过结构化的工具调用和逐步推理，因果逻辑是在推理过程中“涌现”出来的，而不是预先写好的。这意味着系统具有极强的通用性和适应性。\n\n### 6. 愿景延伸：从诊断到预测与自治\n**未来推演：** 既然能诊断“已发生”的故障，能否预测“未发生”的影响？\n*   **逻辑扩展：** 影响分析（IA）本质上是RCA的反向过程。如果系统能理解资源与服务的依赖关系，那么在执行变更（如维护）前，智能体完全可以模拟变更，预测其影响范围。\n*   **终极目标：** 这篇论文不仅是关于RCA，更是为**自主事故解决**和**变更影响缓解**奠定基础。未来的系统将从“被动响应”进化为“主动预防”。\n\n---\n\n**总结：**\n作者的思考路径是一个**“解构 -> 重构 -> 验证 -> 升华”**的过程：\n1.  **解构**了传统RCA系统的脆弱性（硬编码逻辑）；\n2.  **重构**了诊断流程，将其转化为基于MCP工具的智能体协议；\n3.  **验证**了LLM在严格协议下可以替代传统图算法；\n4.  最终**升华**出一种自适应、安全且可审计的基础设施运维新范式。"
                },
                {
                    "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging",
                    "arxiv_id": "2601.07309",
                    "authors": "Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",
                    "summary": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了Agent-Role Merging (ARM)方法，专注于将多个专家模型合并为一个通用的LLM智能体，旨在提升智能体在不同交互环境中的泛化能力。这直接涉及LLM智能体的核心架构与能力提升，不属于排除的纯应用、纯推理、安全或基础设施优化范畴。",
                    "summary2": "本文旨在将多个特定环境的LLM智能体专家合并为一个无需训练的通用模型。针对多轮交互场景，我们提出了一种名为ARM的基于激活引导的角色条件神经元移植方法。该方法通过动态主干选择和冲突感知的神经元移植来减少负迁移。在Qwen3-8B和Qwen2.5-7B专家池上，通过$\\tau$-bench、OfficeBench等多个基准验证了其有效性，显著提升了跨环境泛化能力和鲁棒性。",
                    "summary_translation": "交互式大语言模型智能体发展迅速，但大多数仍局限于单一环境，无法鲁棒地适应其他环境。Model merging (模型合并) 提供了一种 training-free (无需训练) 的替代方案，通过将多个 expert models (专家模型) 整合到单一模型中实现。在本文中，我们提出了 Agent-Role Merging (ARM) (智能体角色合并)，这是一种用于 LLM agents (大语言模型智能体) 模型合并的 activation-guided (激活引导)、role-conditioned (角色条件) neuron transplantation (神经元移植) 方法。ARM 将现有的合并方法从 static natural language tasks (静态自然语言任务) 拓展至 multi-turn agent scenarios (多轮智能体场景)，并提升了在各种交互环境中的 generalization ability (泛化能力)。这一目标通过一个精心设计的 3 步框架实现：1) 构建 merged backbones (合并主干网络)，2) 基于其 role-conditioned activation analysis (角色条件激活分析) 进行选择，3) 进行 neuron transplantation (神经元移植) 以实现 fine-grained refinements (细粒度细化)。在无需 gradient-based optimization (基于梯度的优化) 的情况下，ARM 提升了 cross-benchmark generalization (跨基准泛化) 能力，同时保持了高效率。在 diverse domains (多样化领域) 中，通过 ARM 合并得到的模型优于先前的 model merging methods (模型合并方法) 和 domain-specific expert models (特定领域专家模型)，同时展现了强大的 out-of-domain generalization (域外泛化) 能力。",
                    "inspiration_trace": "基于论文《ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 第一阶段：宏观问题与背景观察\n**1. 现实痛点：专才与通才的矛盾**\n*   **观察**：当前的LLM智能体在特定环境（如WebShop、OfficeBench）中表现优异，但往往是“专才”。一旦跨环境部署，由于工具接口、动作模式的差异，性能会急剧下降。\n*   **常规路径的局限**：传统的解决方案是训练一个通用的全能模型，但这面临巨大的工程挑战（多任务数据冲突、课程学习复杂）和高昂的训练成本。\n*   **切入点**：作者将目光投向了“模型合并”——一种无需额外训练即可整合多个专家模型权重的技术。这被视为一种低成本构建通才模型的潜在路径。\n\n### 第二阶段：冲突发现与问题聚焦\n**2. 现有方法的失效：从静态到动态的鸿沟**\n*   **假设**：现有的模型合并方法（如Task Arithmetic, TIES-Merging）在静态NLP任务上很成功，理应也能应用于智能体任务。\n*   **证伪**：实验发现，这些方法在交互式智能体场景下表现极不稳定（如图1所示，不同基准上表现方差巨大）。\n*   **核心洞察**：智能体任务与静态文本任务的本质区别在于**“多轮交互”**和**“级联效应”**。\n    *   在静态任务中，错误可能只是预测不准；\n    *   在智能体任务中，微小的格式错误（如JSON格式错误、工具调用参数偏差）会导致后续步骤全部崩溃。\n\n### 第三阶段：深入诊断与假设提出\n**3. 归因分析：两大核心挑战**\n作者将合并失败的原因归结为两个具体问题：\n*   **挑战一：主干的不稳定性**\n    *   不同的权重合并公式（平均、TIES等）在不同环境下的表现不可预测。没有一个通用的公式能保证在所有环境下都保留通用能力。\n*   **挑战二：能力冲突**\n    *   简单的权重平均会“模糊”掉特定技能。在智能体中，这表现为“角色关键行为”的丧失（例如，模型忘了如何正确调用API）。这种冲突比普通的知识遗忘更致命，因为它直接阻断了任务链条。\n\n### 第四阶段：方法论构建与逻辑演进\n**4. 策略一：如何选择稳定的主干？（从“盲选”到“内测”）**\n*   **思考**：既然无法预知哪个合并公式最好，能不能先构建一批候选模型，然后选一个最好的？\n*   **难点**：直接在测试集上评估成本太高。\n*   **创新思路**：利用模型内部的**激活信号**作为代理指标。\n    *   **逻辑**：如果一个合并后的模型，在处理特定任务（如“调用工具”）时，其神经元激活模式与原来的专家模型高度重合，说明它保留了该能力。\n    *   **产出**：提出了**激活重叠分数（AOS）**。通过分析“角色条件”下的激活（即只关注关键动作时刻的神经元），选出最能保留专家特征的合并主干。\n\n**5. 策略二：如何修复能力冲突？（从“全局融合”到“局部移植”）**\n*   **思考**：选出的主干可能在某些环境上依然较弱。直接全局微调会破坏已有能力，能否像器官移植一样，只把缺失的“能力模块”补进来？\n*   **细化思路**：\n    *   **定位**：利用激活分析，找出专家模型中负责特定“角色”（如JSON生成、工具调用）的关键神经元。\n    *   **移植**：将这些神经元直接“移植”到主干模型中。\n*   **关键约束：避免负迁移**\n    *   **思考**：如果移植的神经元恰好是另一个环境需要的，就会产生冲突。\n    *   **解决方案**：引入**冲突感知策略**。在移植前，先检查这些神经元是否被其他环境“占用”。如果是，则跳过，只移植那些“安全”的神经元。\n\n### 第五阶段：逻辑闭环与验证\n**6. 最终框架的形成：ARM**\n*   将上述思考串联，形成了三步走框架：\n    1.  **构建候选池**：用常规方法生成一堆合并模型。\n    2.  **基于激活选主干**：用AOS分数选出最稳健的那个。\n    3.  **神经元移植**：针对薄弱环节，像做手术一样精准移植专家的特定神经元，并严格保护其他能力不受干扰。\n\n**7. 预期与验证**\n*   **预期**：这种方法不仅能提升平均性能，更重要的是能解决“木桶效应”（最差环境的表现），因为它专门修复了导致级联失败的关键节点。\n*   **结论**：实验证明，ARM确实在保持通用性的同时，显著提升了跨环境的鲁棒性，验证了“基于角色条件的神经元移植”这一核心假设的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**“应用场景的迁移”**（从静态文本到智能体）出发，发现了**“级联失败”**这一特殊现象，进而通过**“机制可解释性”**（激活分析）手段，将模型合并问题从盲目的权重调整，转化为精准的**“电路诊断与修复”**过程。"
                },
                {
                    "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration",
                    "arxiv_id": "2601.07224",
                    "authors": "Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",
                    "summary": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究LLM智能体的训练范式（SFT与RL的结合），提出了基于梯度浓度的数据分配框架以优化智能体的学习过程。论文在WebShop和ALFWorld等智能体基准上进行了验证，属于智能体的自我演化与训练优化范畴，不属于排除的纯应用、纯推理或基础设施优化类别。",
                    "summary2": "本文旨在解决 LLM 智能体训练中 SFT 与 RL 数据分配低效及优化干扰问题。针对混合训练数据，我们提出了一种基于梯度空间几何结构（如 Gini 系数）诊断认知冲突的 PRISM 框架，实现数据在巩固与适应间的自适应路由，并在 WebShop 和 ALFWorld 基准上通过 Success Rate 和计算效率验证了其有效性。",
                    "summary_translation": "尽管混合监督微调（SFT）后接强化学习（RL）已成为训练 LLM 智能体的标准范式，但在这两个阶段之间进行数据分配的有效机制在很大程度上仍未被充分探索。当前的数据仲裁策略通常依赖于表层启发式方法，无法诊断模型的内在学习需求。鉴于 SFT 旨在通过模仿实现模式巩固，而 RL 则通过探索驱动结构适应，若数据与这些功能角色错位，将导致严重的优化干扰。我们提出了 PRISM，这是一个基于图式理论的动态感知框架，它根据数据与模型现有知识之间的认知冲突程度来仲裁数据。通过分析梯度的空间几何结构，PRISM 将引发高空间集中度的数据识别为高冲突信号，这类信号需要通过 RL 进行结构重构。相反，产生分散更新的数据则被分配给 SFT，以进行高效的巩固。在 WebShop 和 ALFWorld 上进行的广泛实验表明，PRISM 实现了帕累托改进，在将计算成本降低高达 3.22 倍的同时，性能优于最先进的混合方法。我们的研究结果表明，基于内部优化机制对数据进行解耦，对于实现可扩展且鲁棒的智能体对齐至关重要。",
                    "inspiration_trace": "基于论文《Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration》，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观问题的捕捉——现有范式的“粗粒度”困境\n**思考起点：** 作者首先审视了当前LLM智能体训练的标准范式（SFT后接RL）。虽然这一流程已被广泛接受，但作者敏锐地发现了一个被忽视的瓶颈：**数据分配机制是僵化的。**\n*   **观察：** 现有的数据分配策略主要分为三类：一是“单调排序”（SFT-then-RL的固定顺序），二是“通用探索”（对所有数据无差别使用RL），三是“结果导向过滤”（基于准确率等外部指标）。\n*   **痛点：** 这些方法都忽略了数据的**异质性**和模型的**内部状态**。它们将所有数据一视同仁，导致计算资源浪费，且容易引发优化干扰（如对简单样本进行不必要的RL探索，导致不稳定性）。\n*   **核心问题：** 如何根据数据的内在认知需求，智能地将其分配给SFT或RL，以实现效率与性能的帕累托最优？\n\n### 第二阶段：功能解构——SFT与RL的本质差异\n**思考深入：** 为了解决分配问题，作者首先对SFT和RL在认知层面的功能进行了重新定义。\n*   **SFT的功能：** 侧重于**模式巩固**。通过模仿，将行为规范和特定知识内化，适合处理模型已具备基础认知的领域。\n*   **RL的功能：** 侧重于**结构适应**。通过试错，重构内部逻辑以提升泛化能力，适合处理需要复杂推理和逻辑修正的领域。\n*   **推论：** 如果将需要“结构适应”的数据强行用于SFT，模型无法突破逻辑瓶颈；如果将仅需“模式巩固”的数据用于RL，则会引入探索噪声，破坏已有的知识。因此，**必须找到一种诊断机制，区分哪些数据需要巩固，哪些需要适应。**\n\n### 第三阶段：理论映射——引入认知心理学视角\n**思考转折：** 如何定义“需要适应”的数据？作者跳出纯工程视角，引入了皮亚杰的**图式理论**。\n*   **理论核心：** 学习效率取决于新信息与现有知识库之间的**冲突程度**。\n    *   **低冲突（兼容）：** 适合通过“同化”进行巩固。\n    *   **高冲突（矛盾）：** 必须通过“顺应”进行根本性的结构重组。\n*   **映射：** 作者将这一认知过程映射到神经网络优化中——**高认知冲突 = 需要结构适应（RL）；低认知冲突 = 需要模式巩固（SFT）。**\n\n### 第四阶段：数学代理——从“认知冲突”到“梯度几何”\n**思考落地：** 理论有了，但如何量化“认知冲突”？模型内部不会直接告诉我们要“冲突值”。作者将目光投向了优化的核心信号——**梯度**。\n*   **假设：** 梯度是模型对数据的数学反馈。如果数据与模型现有知识冲突剧烈，模型必须剧烈调整特定的参数（即“知识神经元”）来修正逻辑。\n*   **几何洞察：** 作者关注梯度的**空间几何结构**，而非单纯的数值大小。\n    *   **高浓度：** 如果梯度高度集中在少数参数组上，说明模型正在进行剧烈的局部逻辑修正（高冲突）。\n    *   **低浓度（扩散）：** 如果梯度均匀分布在整个网络，说明模型只是在微调全局参数以适应模式（低冲突）。\n*   **结论：** **梯度的空间浓度是认知冲突的最佳代理。**\n\n### 第五阶段：方法论构建——PRISM框架的诞生\n**思考成型：** 基于上述逻辑，作者构建了PRISM框架，将理论转化为可执行的三个步骤：\n1.  **无损探针：** 在不更新权重的情况下，计算模型对每个样本的梯度分布，捕捉内部反应。\n2.  **结构量化：** 引入统计学指标（如基尼系数、峰度、变异系数CV）来量化梯度的“浓度”，从而给每个样本打上“认知冲突分”。\n3.  **自适应路由：** 根据分数中位数进行切分。高分（高冲突）样本路由至RL进行结构重塑；低分（低冲突）样本路由至SFT进行行为巩固。\n\n### 总结：逻辑链条全景\n作者从**训练效率低下**的宏观现象出发，通过**功能解构**明确了SFT与RL的分工，借助**认知心理学理论**定义了“冲突”这一核心变量，最终利用**梯度的空间几何特征**将抽象的认知冲突转化为可计算的数学指标，从而实现了数据的精准路由。这一过程体现了从“经验主义训练”向“动力学感知训练”的思维跃迁。"
                },
                {
                    "title": "Active Context Compression: Autonomous Memory Management in LLM Agents",
                    "arxiv_id": "2601.07190",
                    "authors": "Nikhil Verma",
                    "summary": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了Focus架构，专注于LLM智能体的自主记忆管理（将关键学习内容整合到持久知识块中），属于单智能体研究中的“记忆”和“自我反思”范畴。虽然涉及软件工程任务，但核心贡献在于智能体架构而非纯应用。",
                    "summary2": "本文旨在解决 LLM agents 在长周期任务中面临的 Context Bloat 问题。针对 SWE-bench Lite 中的上下文密集型软件工程任务，我们提出了一种名为 Focus 的架构，通过自主的上下文压缩与记忆整合机制，将原始交互历史转化为持久的 Knowledge 块。在 N=5 个实例上使用 Claude Haiku 4.5 进行评估，结果显示 Focus 实现了 22.7% 的 token 减少，同时保持了与 Baseline 相同的任务准确率。",
                    "summary_translation": "大语言模型（LLM）智能体在处理长周期的软件工程任务时面临困难，主要归因于“Context Bloat”（上下文膨胀）。随着交互历史的增长，计算成本急剧上升，延迟增加，且由于受到无关过往错误的干扰，推理能力也会下降。现有的解决方案通常依赖于智能体无法控制的被动外部摘要机制。本文提出了 Focus，这是一种以智能体为中心的架构，其灵感来源于多头绒泡菌（Physarum polycephalum，粘液菌）的生物探索策略。Focus 智能体自主决定何时将关键学习成果整合到持久的“Knowledge”（知识）块中，并主动撤回（修剪）原始交互历史。我们使用符合行业最佳实践的优化脚手架（persistent bash + string-replacement editor，即持久化 bash + 字符串替换编辑器），在 SWE-bench Lite 的 N=5 个上下文密集型实例上，利用 Claude Haiku 4.5 对 Focus 进行了评估。通过采用鼓励频繁压缩的激进提示策略，Focus 实现了 22.7% 的 Token（词元）减少（从 14.9M 降至 11.5M tokens），同时保持了相同的准确率（两个智能体均为 3/5 = 60%）。Focus 平均每个任务执行 6.0 次自主压缩，在单个实例上的 Token 节省率高达 57%。我们证明了，当提供适当的工具和提示时，能力较强的模型能够自主调节其上下文，这为在不牺牲任务性能的前提下构建具有成本感知的智能体系统开辟了新途径。",
                    "inspiration_trace": "基于论文《Active Context Compression: Autonomous Memory Management in LLM Agents》，以下是对作者产出核心方法“Focus”的逻辑链推演与思想演进还原：\n\n### 第一阶段：宏观观察与问题界定\n**——从“能力幻觉”到“现实瓶颈”的思考**\n\n1.  **观察现象**：随着LLM上下文窗口的扩大（如200k+ tokens），理论上Agent可以处理极长任务。\n2.  **发现矛盾**：虽然“能装下”，但在实际长周期任务（如软件工程）中，简单的“全量保留”策略导致了三大恶果：\n    *   **成本爆炸**：推理成本随历史长度呈二次方增长。\n    *   **延迟增加**：交互响应变慢，体验下降。\n    *   **认知干扰**：长上下文中充斥着失败的尝试和冗余日志，导致模型注意力分散（“Lost in the Middle”现象），反而降低了推理质量。\n3.  **核心问题定义**：现有的Agent普遍采用“Append-Only”（只追加）模式，这是一种不可持续的线性积累。问题不在于窗口不够大，而在于**缺乏有效的遗忘与筛选机制**。\n\n### 第二阶段：对现有范式的批判\n**——从“外部辅助”到“自主控制”的反思**\n\n1.  **审视现有解法**：\n    *   *外部记忆（MemGPT等）*：像操作系统一样管理内存，但增加了系统复杂性。\n    *   *事后总结（Reflexion等）*：通常在任务结束后反思，而非在任务进行中实时清理。\n    *   *被动压缩（LLMLingua等）*：依赖外部模型进行压缩，Agent本身无法感知和控制压缩过程。\n2.  **提炼痛点**：现有方法大多是“被动”的，Agent无法决定“此时此刻什么该留，什么该丢”。作者意识到，**真正的智能体必须具备“元认知”能力，即自主管理自身思维过程（上下文）的能力。**\n\n### 第三阶段：跨学科灵感与假设提出\n**——从“黏菌”行为中提取“压缩”哲学**\n\n1.  **寻找生物学隐喻**：作者寻找自然界中高效探索环境的生物机制，锁定了*Physarum polycephalum*（多头绒泡菌/黏菌）。\n2.  **提取核心逻辑**：\n    *   黏菌在探索迷宫时，当发现某条路是死胡同，它会**物理回缩**（丢弃路径），只留下**化学标记**（保留知识）。\n    *   **类比映射**：Agent不需要保留“我尝试了50次ls命令”的原始日志（肌肉记忆），只需要保留“配置文件不在/src目录”的结论（认知地图）。\n3.  **形成核心假设**：如果Agent能像黏菌一样，在探索阶段结束后，主动“修剪”掉原始交互日志，仅保留提炼出的“知识块”，就能在降低成本的同时避免注意力分散。\n\n### 第四阶段：方法论构建\n**——从“线性增长”到“锯齿波动”的架构设计**\n\n1.  **设计机制**：提出“Focus”架构，引入两个原语：\n    *   `start_focus`：标记探索起点。\n    *   `complete_focus`：总结关键信息并**物理删除**中间的原始对话。\n2.  **确立模式**：将上下文从单调递增的曲线，转变为**“Sawtooth”（锯齿状）模式**——探索时增长，压缩时塌陷。\n3.  **关键特性**：**自主性**。不依赖外部计时器，而是由Agent根据任务进度自主决定何时压缩。\n\n### 第五阶段：实验反馈与策略修正\n**——从“理想模型”到“工程现实”的妥协**\n\n1.  **初步实验受挫**：作者最初假设模型会自然地学会高效压缩。但实验发现，如果仅提供工具而不加干预，模型压缩频率过低（平均2次），且因丢失关键细节导致准确率下降。\n2.  **逻辑修正**：作者意识到，当前的LLM（如Claude Haiku）**缺乏内在的“成本意识”**。它们不会为了省钱而主动压缩，只会为了“整理思路”而压缩。\n3.  **提出“激进提示”策略**：为了验证假设，作者必须强制模型的行为模式。\n    *   *显式规则*：强制要求每10-15次工具调用后必须压缩。\n    *   *系统干预*：注入系统提醒。\n4.  **验证结果**：在强制引导下，Agent平均每任务压缩6次，实现了22.7%的Token节省，且准确率未受损。这证明了**“频繁、小步快跑”的压缩优于“偶尔、大跨度”的压缩**。\n\n### 第六阶段：边界认知与结论升华\n**——从“通用解法”到“场景特化”的洞察**\n\n1.  **发现局限性**：并非所有任务都适合压缩。在需要反复迭代修改的任务（如pylint实例）中，压缩反而增加了开销，因为Agent需要重新加载被丢弃的上下文。\n2.  **最终结论提炼**：\n    *   Focus不是万能药，而是最适合**“探索-实现”分离**的任务（如先找Bug，再修Bug）。\n    *   **思想演进终点**：未来的Agent不应只是被动的执行者，而应是具备自我调节能力的“认知管理者”。通过适当的工程脚手架（Prompting），可以让模型在保持性能的同时，实现成本效益的最优化。\n\n---\n\n**总结：**\n作者的思考路径是从**“上下文太长太贵”**的痛点出发，通过**批判现有被动方法**，引入**生物学的“遗忘与标记”机制**，构建了**自主压缩的架构**，并在实验中通过**强化Prompt策略**克服了模型惰性，最终明确了该方法在**探索型任务中的核心价值**。"
                },
                {
                    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
                    "arxiv_id": "2601.07055",
                    "authors": "Zhenrui Yue, Kartikeya Upasani, Xianjun Yang, Suyu Ge, Shaoliang Nie, Yuning Mao, Zhe Liu, Dong Wang",
                    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题和摘要明确提出了“Self-Evolving Search Agents”（自我演化的搜索智能体），重点研究智能体如何在没有训练数据的情况下通过反馈循环进行自我完善，并涉及工具使用，完全符合研究范围中的“自我演化”和“单智能体（工具使用）”类别。",
                    "summary2": "本文旨在解决无训练数据情况下搜索智能体的自我进化问题。针对开放域问答场景，我们提出了一种 Dr. Zero 框架，该框架利用 Proposer-Solver 共进化循环和跳跃分组相对策略优化（HRPO）来生成多样化且具挑战性的问题。在多个开放域问答基准（如 HotpotQA, NQ）上，通过精确匹配（EM）等指标验证了其有效性，结果显示其性能匹配甚至超越了全监督搜索智能体。",
                    "summary_translation": "随着高质量数据日益难以获取，无数据自我进化已成为一种极具前景的范式。该方法使大语言模型能够自主生成并解决复杂问题，进而提升其推理能力。然而，由于问题多样性有限，且多步推理和工具使用需要大量计算，多轮搜索智能体在无数据自我进化过程中面临挑战。在本研究中，我们提出了 Dr. Zero，这是一个使搜索智能体能够在没有任何训练数据的情况下实现有效自我进化的框架。具体而言，我们设计了一个自我进化反馈回路，其中提议者生成多样化的问题，用于训练一个由同一基础模型初始化的解题者。随着解题者的进化，它会促使提议者生成难度递增但仍可解的任务，从而建立一套自动化课程来优化这两个智能体。为提高训练效率，我们还引入了跳跃分组相对策略优化。该方法将结构相似的问题进行聚类以构建组级基线，有效最小化了在评估每个查询的个体难度和可解性时的采样开销。因此，HRPO 在不影响性能或稳定性的前提下，显著降低了解题者训练的计算需求。大量实验结果表明，无数据的 Dr. Zero 达到甚至超越了全监督搜索智能体的水平，证明了复杂的推理和搜索能力可以仅通过自我进化而涌现。",
                    "inspiration_trace": "基于对论文《Dr. Zero: Self-Evolving Search Agents without Training Data》的深入分析，以下是对作者核心方法论逻辑链的系统性推演。这一过程旨在还原作者从宏观问题观察到具体方法创新的思考路径。\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**——从“数据饥渴”到“无数据自进化”的范式转移**\n\n1.  **背景痛点**：\n    *   作者首先观察到当前大模型（LLM）发展的核心瓶颈：高质量训练数据的获取日益困难。\n    *   **思考**：如果无法依赖外部人工标注数据，模型能否像生物进化一样，通过“自举”的方式自我提升？\n2.  **现有局限的识别**：\n    *   作者审视了现有的“自进化”研究（如Self-Play、Self-Rewarding），发现它们大多集中在**封闭域**（如数学、代码）。\n    *   **关键发现**：在开放域的**搜索代理**任务中，现有的自进化方法失效了。原因在于：\n        *   **多样性缺失**：模型倾向于生成简单的、单跳的问题，缺乏挑战性。\n        *   **计算成本高昂**：搜索代理需要调用外部工具（如搜索引擎），推理链路长、延迟高。传统的强化学习算法（如需要多次采样的GRPO）在多轮工具交互场景下计算量呈指数级增长，难以落地。\n\n**核心问题确立**：如何在**零训练数据**的条件下，实现开放域搜索代理的高效自进化？\n\n---\n\n### 第二阶段：机制设计与假设提出\n**——构建“出题者”与“解题者”的共生博弈**\n\n1.  **引入对抗/共生框架**：\n    *   **思考**：要解决“题目太简单”的问题，不能只靠模型自己瞎想。自然界中，捕食者和猎物的共同进化促进了物种复杂度的提升。\n    *   **假设**：如果设计两个角色——**Proposer（出题者）**和**Solver（解题者）**，让它们相互博弈，是否能自动生成由易到难的课程？\n2.  **定义进化逻辑**：\n    *   **Proposer的任务**：利用搜索引擎生成复杂、多跳的问题。\n    *   **Solver的任务**：利用搜索引擎回答这些问题。\n    *   **反馈闭环**：Solver越强，Proposer必须生成更难的问题才能获得奖励；Proposer的问题越难，Solver被迫提升搜索推理能力。\n    *   **关键洞察**：这种动态博弈能自动形成**课程学习**，无需人工设计难度梯度。\n\n---\n\n### 第三阶段：攻克核心瓶颈\n**——解决“计算效率”与“题目质量”的双重挑战**\n\n1.  **解决计算效率问题（HRPO的诞生）**：\n    *   **困境**：传统的GRPO算法为了估计优势函数，需要对同一个Prompt生成多个回复。对于搜索代理来说，一次回复包含多次搜索调用，成本极高。如果Proposer训练需要“生成多个问题”且“每个问题跑多次Solver”，计算开销不可接受。\n    *   **创新思考**：能否减少采样次数？\n    *   **逻辑推演**：问题的结构特征（如Hop数/跳数）与其难度高度相关。与其对同一个问题采样多次，不如将**结构相似的问题**（例如都是2跳问题）归为一组。\n    *   **方法论产出**：提出**Hop-Grouped Relative Policy Optimization (HRPO)**。通过聚类结构相似的问题来构建组级基线，从而避免了昂贵的嵌套采样，将计算成本降低了一个数量级。\n\n2.  **解决题目质量问题（难度引导的奖励机制）**：\n    *   **困境**：如何让Proposer生成“既难又能做对”的题目？如果太难，Solver全错，学不到东西；如果太简单，Solver全对，没提升。\n    *   **逻辑推演**：理想的题目应该让Solver的正确率处于中间状态（例如只有部分尝试能解出）。\n    *   **方法论产出**：设计**难度引导的奖励函数**。\n        *   如果Solver全对 -> 奖励低（太简单）。\n        *   如果Solver全错 -> 奖励低（太难/无解）。\n        *   如果Solver部分正确 -> 奖励高（难度适中）。\n    *   **补充**：引入格式奖励，强制Proposer正确使用搜索工具，确保生成的题目是基于真实检索路径的，而非幻觉。\n\n---\n\n### 第四阶段：系统整合与验证\n**——Dr. Zero 框架的最终成型**\n\n1.  **系统架构整合**：\n    *   作者将上述思考整合为一个统一的框架：**Dr. Zero**。\n    *   **输入**：仅依赖基础LLM和外部搜索引擎，无任何人工标注数据。\n    *   **流程**：\n        1.  Proposer通过HRPO训练，利用搜索生成高质量、多跳的QA对。\n        2.  Solver通过GRPO训练，学习解决Proposer生成的难题。\n        3.  两者交替迭代，性能螺旋上升。\n\n2.  **实验验证与假设确认**：\n    *   **思考**：这套无数据方案真的能打过有监督的SOTA吗？\n    *   **结果**：实验表明，Dr. Zero在多个开放域QA基准上，不仅超越了基础模型，甚至**匹配或超越了**完全依赖人工数据的监督式搜索代理。\n    *   **结论**：证明了在搜索代理领域，**自进化可以替代人工监督**。\n\n---\n\n### 总结：作者的思维演进脉络\n\n1.  **观察**：数据稀缺，现有自进化方法在开放域搜索任务中因“题目简单”和“算力昂贵”而失效。\n2.  **假设**：通过Proposer-Solver的共生博弈可以自动生成进化的课程。\n3.  **挑战1（算力）**：传统RL采样太贵 -> **创新**：利用问题结构相似性，提出HRPO算法，大幅降低采样成本。\n4.  **挑战2（质量）**：如何控制题目难度 -> **创新**：基于Solver正确率的难度引导奖励，确保题目处于“最近发展区”。\n5.  **成果**：实现了无需任何训练数据的搜索代理自进化，性能媲美甚至超越有监督方法。"
                },
                {
                    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
                    "arxiv_id": "2601.06860",
                    "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了ET-Agent框架，专注于LLM智能体的工具使用行为校准。它涉及单智能体的工具使用能力，并引入了自我演化数据飞轮机制，符合单智能体（工具使用）和自我演化的研究范围。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR) 任务中agent行为模式无效的问题。针对TIR场景，我们提出了一种名为ET-Agent的训练框架，通过Self-evolving Data Flywheel生成增强数据，并利用两阶段Behavior Calibration Training逐步校准错误行为。我们在数学推理和知识密集型任务上通过正确性、效率等指标验证了其有效性，实验表明ET-Agent显著提升了推理效率和准确性。",
                    "summary_translation": "大语言模型可以通过采用工具集成推理范式，扩展其参数知识的边界。然而，现有的基于大语言模型的智能体训练框架往往侧重于答案的准确性，而忽视了对行为模式的特定对齐。因此，智能体在执行工具集成推理任务时，常表现出无效的动作，例如冗余或不足的工具调用。如何在执行工具集成推理任务时校准错误的行为模式，进而探索有效的轨迹，仍是一个亟待解决的开放性问题。在本文中，我们提出了ET-Agent，这是一个通过两个协同视角来校准智能体工具使用行为的训练框架：自进化数据飞轮和行为校准训练。具体而言，我们引入了一个自进化数据飞轮来生成增强数据，利用这些数据对大语言模型进行微调，以提升其探索能力。在此基础上，我们构建了一个两阶段的行为校准训练框架。该框架旨在逐步将错误的行为模式校准为最优行为。进一步的深入实验证实了ET-Agent在多个维度上的优越性，包括正确性、效率、推理简洁性以及工具执行准确性。我们的ET-Agent框架为工具集成推理领域的研究提供了有价值的实践启示。代码链接：https://github.com/asilverlight/ET-Agent",
                    "inspiration_trace": "基于对论文《ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration》的深入分析，以下是对作者产出该核心方法的逻辑链推演。这一过程展现了从宏观现象观察到微观机制设计的完整思考路径。\n\n---\n\n### 1. 宏观观察与问题定义：从“结果正确”到“行为有效”\n\n**逻辑起点：**\n作者首先关注到大语言模型（LLM）通过工具集成推理（TIR）范式突破了参数知识的限制。然而，学术界和工业界存在一个普遍的**评价偏差**：绝大多数研究仅关注**最终答案的准确性**，而忽视了达成答案过程中的**行为模式**。\n\n**现象发现：**\n在实际应用中，作者观察到即使模型答对了问题，其过程往往充满“低效”或“怪异”的行为。例如，为了查一个简单事实反复调用搜索工具，或者在需要深入推理时过早停止。这引发了一个核心思考：\n> **核心问题：** 如何在保证答案正确的前提下，校准Agent的工具使用行为，使其既不冗余也不匮乏，从而实现高效推理？\n\n### 2. 深度诊断与归因：行为错误的分类与空间复杂性\n\n为了解决上述问题，作者没有急于提出模型，而是先对“错误行为”进行了系统性的**病理分析**。\n\n**错误分类：**\n通过初步实验，作者将错误的TIR行为模式归纳为两类：\n1.  **不当工具使用：** 包括“冗余调用”（浪费资源）和“中止执行”（代码或查询格式错误导致失败）。\n2.  **缺陷推理逻辑：** 包括“调用不足”（过早停止，没拿到关键信息）和“错误推理过程”（逻辑跳跃或无关步骤）。\n\n**关键洞察：**\n作者进一步分析了正确答案的轨迹分布，发现了一个重要现象：**对于同一个问题，存在大量不同的正确路径，且工具调用的次数差异巨大。**\n这意味着TIR任务的**动作空间极其广阔**。\n\n**对现有方法的批判：**\n基于此洞察，作者指出了现有方法的局限性：\n*   **模仿学习（SFT）：** 只能复现训练数据中的路径，无法探索数据之外的高效行为，导致探索能力受限。\n*   **传统RL（如DPO）：** 往往基于二元对比（好vs坏），容易导致模型坍缩到极窄的动作空间，无法适应TIR广阔的解空间。\n\n**结论：** 现有的“只看结果”或“简单对比”无法解决TIR中的行为校准问题。我们需要一种能**充分探索广阔动作空间**，并从中**筛选出最优行为**的新范式。\n\n### 3. 核心假设提出：先探索，后校准\n\n基于上述诊断，作者提出了一个分阶段的解决思路：\n> **核心假设：** 要校准行为，首先必须让模型“见识”到足够多的可能性（探索），然后再通过奖励机制引导其收敛到最优路径（校准）。\n\n这直接导向了ET-Agent框架的两大支柱设计：\n1.  **数据层面：** 需要一个能自我进化、不断扩充轨迹多样性的机制。\n2.  **算法层面：** 需要一个先鼓励发散探索，再逐步收敛至高效行为的训练流程。\n\n### 4. 方法论构建：从数据飞轮到行为校准\n\n#### 4.1 数据层面的突破：自进化数据飞wheel\n**思考：** 既然现有数据覆盖面不够，如何低成本地获得高质量、多样化的轨迹？\n**设计：**\n作者设计了一个闭环系统，利用模型自身来生成和优化数据：\n*   **对正确轨迹：** 进行“去冗余”和“全局精炼”，教模型如何做得更简洁。\n*   **对错误轨迹：** 进行“自我修正”和“提示注入”，强制模型继续思考或修正错误，从而生成原本不存在的正确路径。\n**逻辑目的：** 这个过程不仅仅是增加数据量，而是为了**覆盖更广阔的动作空间**，为后续的训练提供丰富的“原材料”。\n\n#### 4.2 算法层面的演进：两阶段行为校准\n**思考：** 有了丰富的数据，如何训练模型？直接用RL可能会因为奖励稀疏或梯度消失而失败。\n**设计：** 作者将训练分为两个紧密衔接的阶段。\n\n*   **阶段一：动作空间探索微调**\n    *   **逻辑：** 利用飞wheel生成的多样化数据进行监督微调（SFT）。\n    *   **目的：** 此时暂不追求极致效率，而是让模型**学会各种可能的解题路径**，打破初始模型的思维定势，实现“广度优先”。\n\n*   **阶段二：迭代行为校准强化学习**\n    *   **逻辑：** 在模型具备探索能力后，引入RL进行优化。\n    *   **难点解决：** 传统的Group-wise RL容易因为轨迹同质化导致梯度消失。\n    *   **创新设计：**\n        *   **分组帕累托采样：** 在采样时，不仅看正确率，还看行为差异度。优先保留那些“既正确又与众不同”的轨迹，确保训练信号始终存在。\n        *   **课程式奖励机制：** 设计了包含“效率惩罚”（工具调用次数、推理长度）的奖励函数。并采用课程学习策略，逐步收紧对效率的要求（从宽松到严格），防止模型为了追求效率而牺牲正确性。\n\n### 5. 逻辑闭环与验证\n\n最终，ET-Agent的形成逻辑链条如下：\n1.  **痛点：** TIR Agent行为低效（冗余或不足），且动作空间巨大。\n2.  **诊断：** 现有方法缺乏对广阔动作空间的有效探索和精准校准。\n3.  **策略：** 先通过数据增强实现“广度探索”，再通过RL实现“精度校准”。\n4.  **实现：**\n    *   **数据侧：** 自进化飞wheel -> 生成多样化、高质量的轨迹。\n    *   **训练侧：** RFT（学广） -> Pareto RL + 课程奖励（学精）。\n5.  **结果：** 模型在保持高准确率的同时，显著提升了工具使用的效率和推理的简洁性。\n\n这一思考过程体现了作者从**现象观察**到**本质归因**，再到**范式假设**，最后落实到**具体机制设计**的严谨学术逻辑。"
                },
                {
                    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
                    "arxiv_id": "2601.06794",
                    "authors": "Zhicong Li, Lingjie Jiang, Yulan Hu, Xingchen Zeng, Yixia Li, Xiangwen Zhang, Guanhua Chen, Zheng Pan, Xin Li, Yong Liu",
                    "summary": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了ECHO框架，专注于训练LLM智能体，通过策略与评论家的协同演化（Co-evolution）机制解决反馈陈旧问题，属于自我演化和单智能体训练的研究范畴，符合筛选条件。",
                    "summary2": "本文旨在解决critique-guided RL中静态critic因策略演化导致反馈陈旧的问题。针对Open-World Agent Learning场景，我们提出了一种ECHO框架，通过cascaded rollout mechanism和saturation-aware gain shaping实现策略与critic的同步协同演化。并在WebShop、ALFWorld、SciWorld及DeepSearch四个基准上通过任务成功率验证了其有效性。",
                    "summary_translation": "批判引导的强化学习（RL）已成为一种训练 LLM 智能体的强大范式，它通过自然语言反馈来增强稀疏的结果奖励。然而，现有方法通常依赖于静态或离线的评论家模型，这些模型无法随着策略的演变而适应。在在线策略 RL 中，智能体的错误模式会随时间发生偏移，导致固定的评论家变得过时，从而提供效用递减的反馈。为了解决这一问题，我们提出了 ECHO（Evolving Critic for Hindsight-Guided Optimization，用于后见之明引导优化的演进评论家），该框架通过同步的协同进化循环来联合优化策略和评论家。ECHO 采用了一种级联展开机制，评论家首先针对初始轨迹生成多个诊断，随后进行策略细化，从而实现分组结构的优势估计。我们通过一种饱和感知的增益塑形目标来解决学习平台期的挑战，该目标对评论家在高性能轨迹中引发增量改进的行为给予奖励。通过采用双轨 GRPO 更新，ECHO 确保评论家的反馈与不断演进的策略保持同步。实验结果表明，在开放世界环境中，ECHO 能够实现更稳定的训练，并在长视界任务中取得更高的成功率。",
                    "inspiration_trace": "基于论文《No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning》，以下是对作者产出核心方法（ECHO）的逻辑链推演与思考过程还原：\n\n### 第一阶段：观察现状与识别瓶颈\n**思考起点：** 在开放世界的LLM智能体训练中，传统的强化学习（RL）仅依赖稀疏的最终结果奖励，这导致数据效率极低，因为智能体不知道“哪里错了”。\n**现有尝试：** 引入“评论家”模型提供自然语言的诊断反馈。\n**发现矛盾：** 现有的评论家大多是**静态**的（基于模板或离线训练后冻结）。作者观察到，在On-policy RL（在线策略强化学习）中，智能体的策略是不断演进的。\n**逻辑推演：**\n*   早期阶段：智能体犯的是粗粒度错误（如走错房间），需要高层提示。\n*   后期阶段：智能体已掌握基本技能，犯的是细粒度错误（如参数微调），需要精准诊断。\n*   **结论：** 一个固定的、不随策略变化的评论家，其反馈会逐渐变得“陈旧”，甚至产生误导。这就是“Critic Staleness”问题。\n\n### 第二阶段：提出核心假设\n**思维转折：** 既然智能体的错误模式是漂移的，那么最优的评论策略也应当是非静止的。\n**核心假设：** 评论家不应是一个外部的、高高在上的“监督者”，而应是一个与策略共同进化的“伙伴”。\n**评价标准重构：** 评价一个评论家好坏的标准，不应是“它说得是否好听”，而应是“它是否真的诱导了策略的改进”。\n\n### 第三阶段：构建协同进化机制\n**设计挑战：** 如何让两个模型（策略 $P$ 和 评论家 $C$）在同一个训练循环中互相促进，而不是互相干扰？\n**解决方案构思：**\n1.  **闭环构建：** 设计一个“诊断-修正”的级联流程。策略生成轨迹 -> 评论家诊断 -> 策略基于诊断修正。\n2.  **双重优化：** 利用修正后的结果来反向更新两个模型。\n    *   策略更新：学习如何更好地采纳建议。\n    *   评论家更新：学习如何给出能带来更高奖励的建议。\n**逻辑支点：** 通过这种“双轨”同步更新，确保评论家的诊断粒度始终对齐策略当前的短板。\n\n### 第四阶段：解决“最后一公里”的优化难题\n**深入思考：** 在训练后期，策略表现已经很好（例如得分从0.9提升到0.95），这比从0.1提升到0.15要难得多。\n**现有缺陷：** 如果使用线性的奖励差值（$\\Delta s = 0.05$），模型会认为这种高难度的提升价值很低，导致优化停滞。\n**创新思路：** 引入“饱和感知”的奖励设计。\n**逻辑推演：**\n*   假设奖励空间是非线性的，越接近满分，改进的难度和熵减的价值越高。\n*   设计一个增益函数，放大高分区间的微小改进信号。\n*   **目的：** 激励评论家去挖掘那些“看似完美但仍有瑕疵”的轨迹中的关键缺陷。\n\n### 第五阶段：方法论综合与验证\n**最终框架（ECHO）：** 将上述思考整合为一个统一的框架。\n1.  **级联演化：** 通过多视角诊断和条件修正，生成结构化的轨迹组。\n2.  **饱和感知奖励：** 解决高难度阶段的优化动力问题。\n3.  **同步双轨GRPO：** 利用群组相对优势估计，稳定地同时更新策略和评论家。\n\n**总结：** 作者的思考路径从**发现静态反馈与动态策略之间的错配**出发，通过**引入协同进化的视角**重新定义了评论家的角色，并利用**非线性奖励塑形**解决了长尾优化难题，最终实现了ECHO这一能够持续自我提升的智能体训练范式。"
                },
                {
                    "title": "From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design",
                    "arxiv_id": "2601.06776",
                    "authors": "Xufei Tian, Wenli Du, Shaoyi Yang, Han Hu, Hui Xin, Shifeng Qu, Ke Ye",
                    "summary": "Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一种新颖的多智能体LLM工作流，包含四个专门的智能体（任务理解、拓扑生成、参数配置、评估分析）进行协作，并利用增强蒙特卡洛树搜索与模拟软件进行交互（工具使用）。尽管应用于化工领域，但其核心贡献在于多智能体协作架构和工作流设计，符合多智能体和工具使用的研究范围。",
                    "summary2": "本文旨在实现从文本描述到可执行化工过程模拟的端到端自动化设计。针对自然语言输入的化工过程设计场景，我们提出了一种结合增强蒙特卡洛树搜索（E-MCTS）的多智能体LLM工作流，通过四个专门智能体协同工作。在Simona数据集上，通过模拟收敛率（SCR）和设计时间验证了其有效性，相比最先进基线收敛率提升31.1%，设计时间减少89.0%。",
                    "summary_translation": "Process simulation (过程模拟) 是 Chemical engineering design (化工设计) 的关键基石。当前的 Automated chemical design methodologies (自动化化工设计方法) 主要集中在 Process flow diagrams (工艺流程图) 的各种表示形式上。然而，将这些图表转化为 Executable simulation flowsheets (可执行模拟流程) 仍然是一项耗时且费力的工作，需要在 Simulation software (模拟软件) 中进行大量的 Manual parameter configuration (手动参数配置)。在这项工作中，我们提出了一种新颖的 Multi-agent workflow (多智能体工作流)，该工作流利用 Large language models (LLMs, 大语言模型) 的 Semantic understanding (语义理解) 能力，并实现与 Chemical process simulation software (化工过程模拟软件) 的 Iterative interactions (迭代交互)，从而实现了从 Textual process specifications (文本过程规范) 到用于设计增强的 Computationally validated software configurations (计算验证的软件配置) 的 End-to-end automated simulation (端到端自动模拟)。我们的方法集成了四个分别负责 Task understanding (任务理解)、Topology generation (拓扑生成)、Parameter configuration (参数配置) 和 Evaluation analysis (评估分析) 的 Specialized agents (专门智能体)，并结合 Enhanced Monte Carlo Tree Search (增强蒙特卡洛树搜索) 来准确解释语义并稳健地生成配置。在大规模 Process description dataset (过程描述数据集) Simona 上进行评估，我们的方法与 State-of-the-art baselines (最先进基线) 相比，Simulation convergence rate (模拟收敛率) 提高了 31.1%，与专家 Manual design (手动设计) 相比，Design time (设计时间) 减少了 89.0%。这项工作展示了 AI-assisted chemical process design (AI辅助化工过程设计) 的潜力，弥合了 Conceptual design (概念设计) 与 Practical implementation (实际实施) 之间的差距。我们的 Workflow (工作流) 适用于包括 Pharmaceuticals (制药)、Petrochemicals (石化)、Food processing (食品加工) 和 Manufacturing (制造业) 在内的多种 Process-oriented industries (流程导向型行业)，为 Automated process design (自动化过程设计) 提供了一种 Generalizable solution (可推广解决方案)。",
                    "inspiration_trace": "基于论文《From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论产出的思考过程：\n\n### 第一阶段：宏观问题定位——“最后一公里”的瓶颈\n**观察：** 化工过程设计是工业的核心，但目前的流程极其低效。工程师需要花费数周时间，将高层的概念设计（如“设计一个乙烯裂解过程”）转化为可在模拟软件（如Aspen Plus等）中运行的详细配置。\n**痛点识别：** 现有的自动化方法大多集中在“画图”阶段（生成流程图PFD或超图结构），但这只是设计的中间态。真正的瓶颈在于从“结构图”到“可执行仿真配置”的转化——这需要人工设定成百上千个相互依赖的热力学和操作参数。\n**核心矛盾：** 概念设计与工程落地之间存在巨大的鸿沟。AI能生成漂亮的图纸，但图纸无法直接运行，无法验证可行性。\n\n### 第二阶段：现有方案的局限性分析\n**反思：** 为什么现有的AI方法（如CNN、GNN或早期的LLM应用）解决不了这个问题？\n**结论：**\n1.  **停留在表征层：** 现有方法将设计视为静态的图像或图结构生成任务，忽略了化工过程本质上是基于物理化学方程的动态计算过程。\n2.  **缺乏闭环验证：** 生成的结构如果没有经过模拟软件的严格计算，往往是不收敛或不可行的。现有方法缺乏与专业仿真软件的交互能力。\n3.  **语义与参数的割裂：** LLM擅长理解自然语言（语义），但很难直接生成符合复杂物理约束的精确参数（数值）。\n\n### 第三阶段：核心假设提出——“人机协作”的代理化\n**假设：** 如果能构建一个系统，模仿人类专家的思维方式——即“理解意图 -> 搭建结构 -> 设定参数 -> 软件试算 -> 根据报错调整”，并利用LLM处理语义，利用仿真软件处理物理计算，就能打通从文本到仿真的全链路。\n**关键转变：** 从“一次性生成”转变为“迭代式交互”。不再追求LLM直接写出完美的代码，而是允许它通过工具与仿真软件进行多轮对话，直到收敛。\n\n### 第四阶段：方法论构建——多智能体分工\n**思考：** 化工设计任务过于复杂，单个LLM无法同时兼顾语义理解、拓扑规划、参数计算和结果评估。必须进行“分而治之”。\n**逻辑推演：**\n1.  **任务理解：** 首先需要将模糊的自然语言转化为结构化的工程需求（如明确组分、约束条件）。\n2.  **拓扑生成：** 专注于“骨架”搭建，确定单元操作（反应器、精馏塔）及其连接关系，暂时不纠结细节。\n3.  **参数配置：** 专注于“血肉”填充，利用LLM的推理能力结合领域知识，为拓扑赋予初始参数。\n4.  **评估分析：** 充当“质检员”，接收仿真软件的反馈（是否收敛、经济性如何），并决定是输出结果还是反馈修改。\n\n### 第五阶段：搜索策略优化——如何处理“失败”\n**深层挑战：** 化工设计空间巨大，且充满了“陷阱”。很多设计在仿真中会失败（不收敛）。传统的搜索算法（如标准MCTS）通常会直接丢弃失败的分支。\n**创新洞察：** 在化工设计中，一个“失败”的仿真往往包含有价值的信息（例如拓扑结构是对的，只是某个温度参数设错了）。如果直接丢弃，就浪费了探索成本。\n**策略演进：** 提出**增强型蒙特卡洛树搜索（E-MCTS）**。\n1.  **双重价值评估：** 区分“当前价值”（仿真是否成功）和“潜在价值”（结构是否合理）。即使仿真失败，如果结构合理，仍保留其探索潜力。\n2.  **动态重访机制：** 当搜索陷入停滞时，主动回到那些曾经失败但潜力巨大的节点进行微调，从而跳出局部最优，找到真正可执行的解。\n\n### 总结：逻辑链的全景图\n作者从**“设计效率低”**的宏观问题出发，识别出**“结构到可执行配置的断层”**这一核心痛点。通过分析现有AI**“重表征、轻验证”**的缺陷，提出了**“LLM语义理解 + 仿真软件物理验证”**的闭环假设。为了实现这一假设，作者采用了**多智能体协作**来解耦复杂任务，并创新性地设计了**E-MCTS算法**来从失败中学习，最终实现了从自然语言文本到工业级仿真配置的端到端自动化。"
                },
                {
                    "title": "Agentic AI Empowered Intent-Based Networking for 6G",
                    "arxiv_id": "2601.06640",
                    "authors": "Genze Jiang, Kezhi Wang, Xiaomin Chen, Yizhou Huang",
                    "summary": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个分层多智能体框架，包含编排器智能体和专家智能体（RAN和核心网络），它们通过ReAct循环进行协作和通信。这直接符合“多智能体：协作、通信”的研究范围。尽管应用于6G网络领域，但其核心贡献在于智能体的架构设计与交互机制，而非单纯的应用。",
                    "summary2": "本文旨在解决6G网络中将高层自然语言意图转化为可执行网络配置的自主编排问题。针对自然语言操作意图，我们提出了一种基于LLM的分层多智能体框架，通过Orchestrator协调RAN和Core专家代理进行ReAct推理，并在包含12个场景的6G基准测试中，通过Semantic Accuracy和Engineering Utility验证了其有效性。",
                    "summary_translation": "向第六代（6G）无线网络的演进迫切需要一种自主编排机制，该机制能够将高层运维意图转化为可执行的网络配置。现有的基于意图的网络（Intent-Based Networking, IBN）方法要么依赖于难以应对语言差异的基于规则的系统，要么依赖于缺乏可解释性且无法强制执行运维约束的端到端神经模型。本文提出了一种分层多智能体框架，其中基于大语言模型（Large Language Model, LLM）的智能体能够自主分解自然语言意图，咨询领域特定专家，并通过迭代推理-行动（Reasoning-action, ReAct）循环综合生成技术上可行的网络切片配置。该架构采用一个编排器智能体，通过基于结构化网络状态表示的ReAct风格推理，协调两个专家智能体，即无线接入网（Radio Access Network, RAN）智能体和核心网智能体。在多种基准场景下的实验评估表明，该系统优于基于规则的系统和直接LLM提示方法，且其架构原则适用于开放无线接入网（Open RAN, O-RAN）部署。结果还表明，尽管当代大语言模型（LLM）具备通用的电信知识，但网络自动化仍需通过精细的提示工程来编码上下文相关的决策阈值，从而推进下一代无线系统的自主编排能力。",
                    "inspiration_trace": "基于论文《Agentic AI Empowered Intent-Based Networking for 6G》，以下是对作者核心方法论产出过程的逻辑推演与思想还原：\n\n### 1. 宏观背景与问题锚定：6G时代的“语义鸿沟”\n**思考起点：** 6G网络愿景的核心是“零接触”自动化与无处不在的智能。未来的网络管理不应依赖工程师手动敲击命令行，而应允许运营商通过自然语言描述高层业务目标（即“意图”），由系统自动转化为底层的网络配置。\n\n**核心矛盾：** 现有的意图网络（IBN）方案存在两极分化的缺陷：\n*   **基于规则的系统：** 虽然严谨，但极其死板。一旦自然语言表述稍有变化（如将“低延迟”改为“即时响应”），系统便无法识别，缺乏泛化能力。\n*   **端到端的神经网络/传统ML：** 虽然能处理数据，但属于“黑盒”，缺乏可解释性，且难以强制执行严格的操作约束（如“必须小于10ms”），无法满足电信级的安全要求。\n\n**结论：** 我们需要一种既能理解自然语言的灵活性，又能像专家系统一样执行严格逻辑推理的新范式。\n\n### 2. 技术选型与范式转移：从“聊天机器人”到“智能体”\n**观察：** 大语言模型（LLM）展现了惊人的语义理解能力，似乎是填补“语义鸿沟”的完美工具。然而，直接向LLM提问（单次Prompt）存在致命弱点——LLM本质上是一个文本生成器，而非决策引擎。它无法自主验证配置的可行性，无法感知当前网络状态，且容易产生“幻觉”。\n\n**假设：** 如果不把LLM仅仅当作一个问答接口，而是将其置于一个具备“感知-规划-行动”能力的架构中，使其成为**Agentic AI（智能体AI）**，是否能解决问题？\n\n**方法论引入：** 引入**ReAct（Reasoning + Acting）**范式。即让LLM不仅生成答案，还要生成“思考过程”和“行动指令”，通过与环境交互（如查询网络状态）来迭代修正决策，从而实现多步推理。\n\n### 3. 架构演进：从单体智能到分层协作\n**挑战：** 6G网络极其复杂，涵盖无线接入网（RAN）、核心网等多个领域。让单一的LLM智能体掌握所有领域的知识并处理所有约束，认知负荷过重，容易导致推理混乱和错误。\n\n**思路突破：** 模仿人类企业的组织架构——**分工与协作**。\n*   **编排者：** 扮演项目经理角色，负责理解用户意图、拆解任务、协调资源。\n*   **领域专家：** 扮演技术顾问角色。设立RAN专家（负责频谱、基站）和Core网专家（负责UPF部署、拓扑）。\n\n**逻辑闭环：** 编排者不直接做技术决策，而是将意图转化为子问题，咨询相应的专家。专家基于注入的当前网络状态（结构化数据）给出建议，编排者汇总建议并生成最终配置。这种分层架构既降低了单点复杂度，又保证了决策的专业性。\n\n### 4. 落地机制：知识注入与状态锚定\n**问题：** LLM虽然通晓电信理论，但不知道当前网络的具体状态（如哪个基站负载过高），也不懂运营商特定的隐性偏好（如成本优先还是性能优先）。\n\n**解决方案：**\n*   **状态锚定：** 将实时的网络状态（负载、延迟矩阵、频谱可用性）转化为结构化的JSON数据，在每次推理时“注入”给智能体，防止其凭空捏造。\n*   **提示词工程即软代码：** 将电信领域的专家知识（如“URLLC业务必须选边缘节点”、“负载超过80%需预警”）编码进System Prompt中。这不仅是提示技巧，更是将领域知识固化为系统逻辑的过程。\n\n### 5. 评估视角的重构：语义与工程的双重校验\n**反思：** 传统的AI评估只看“准确率”。但在网络配置中，仅仅“听懂了”是不够的，配置必须“工程上可行”且“最优”。\n\n**评估框架创新：** 提出双重指标体系：\n*   **语义准确性：** 生成配置是否符合人类专家的预期（是否听懂了人话）。\n*   **工程效用：** 配置在数学上是否满足QoS约束（如延迟公式、资源利用率），是否是最优解。\n\n### 6. 实验洞察与偏差修正：对“语言”的再认识\n**意外发现：** 在实验中，作者发现系统存在一种“延迟贪婪”偏差——无论什么业务，智能体总是倾向于选择延迟最低的节点，导致资源浪费。\n\n**深层思考：** 这揭示了LLM的一个特性：**对提示词语义的极度敏感**。Prompt中微小的措辞差异（如说“可接受”还是“优先选择”）会引发系统性的行为偏差。\n\n**最终完善：** 这促使作者将Prompt工程提升到了核心架构组件的高度。通过迭代修正Prompt，明确指令（如“非URLLC业务必须优先使用区域数据中心以节约成本”），消除了偏差。这证明了在Agentic AI中，**如何定义智能体的“性格”和“规则”与架构本身同等重要"
                },
                {
                    "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
                    "arxiv_id": "2601.06502",
                    "authors": "Shengkai Chen, Zhiguang Cao, Jianan Zhou, Yaoxin Wu, Senthilnath Jayavelu, Zhuoyi Lin, Xiaoli Li, Shili Xiang",
                    "summary": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了名为 DRAGON 的框架，明确使用了“Agents”概念，涉及智能体与环境交互、利用自适应经验记忆以及通过反馈进行迭代学习，符合单智能体（记忆、自我反思）和自我演化的研究范围。尽管应用于组合优化问题，但其核心贡献在于智能体架构本身，而非单纯的应用。",
                    "summary2": "本文旨在解决大语言模型（LLM）在大规模组合优化问题（COP）中可扩展性受限的问题。针对大规模COP场景，我们提出了一种名为DRAGON的分解与重构智能体框架，通过迭代识别高潜力区域并求解局部子问题来优化全局解。在TSPLIB、CVRPLIB和Weibull-5k等基准数据集上，通过Optimality Gap等指标验证了其有效性，在超大规模实例上实现了近最优解，显著优于现有基于LLM的求解器。",
                    "summary_translation": "大语言模型近期在利用基于提示的策略解决组合优化问题方面展现出潜力。然而，其可扩展性和泛化能力仍然受限，且随着问题规模的增大，其有效性会降低，特别是在涉及超过30个节点的路径问题中尤为明显。我们提出了 DRAGON（Decomposition and Reconstruction Agents Guided OptimizatioN，分解与重构智能体引导优化），这是一个结合了元启发式设计和 LLM 推理优势的新型框架。DRAGON 从一个初始全局解出发，自主识别具有高优化潜力的区域，并策略性地将大规模 COPs 分解为易于处理的子问题。随后，每个子问题被重新表述为一个简洁的局部优化任务，并在积累经验的指导下，通过针对性的 LLM 提示进行求解。最后，将局部优化后的解系统地重新整合到原始全局上下文中，从而产生显著改善的整体结果。通过与优化环境的持续交互并利用自适应经验记忆，智能体能够从反馈中迭代学习，从而有效地将符号推理与启发式搜索相结合。实验结果表明，与局限于小规模实例的现有基于 LLM 的求解器不同，DRAGON 在 TSPLIB、CVRPLIB 和 Weibull-5k 装箱基准测试中始终能生成可行解，并在拥有超过 300 万变量的背包问题上取得了接近最优的结果（0.16% gap）。这项工作展示了反馈驱动的语言智能体作为一种可泛化且可解释的大规模优化新范式的潜力。",
                    "inspiration_trace": "基于论文《DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization》，以下是对作者提出核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论产出的思考过程。\n\n---\n\n### 1. 宏观观察与矛盾识别：LLM的“能力”与“尺度”错位\n**思考起点：**\n作者首先观察到大语言模型（LLM）在解决组合优化问题（COP）上展现出了惊人的潜力，尤其是在逻辑推理和模式识别方面。然而，这种能力存在明显的“尺度天花板”。\n\n**逻辑推演：**\n*   **现象：** 现有的基于Prompt的LLM方法（如OPRO, SGE）在处理小规模问题（如节点数<30的TSP）时表现尚可，但一旦问题规模扩大到现实世界级别（如成千上万个节点），LLM的表现急剧下降。\n*   **归因：** 这种下降并非因为LLM不懂优化原理，而是受限于其**上下文窗口长度**和**长序列生成的逻辑连贯性**。直接让LLM一次性生成大规模问题的解，就像让一个人心算一本电话簿的排序，既不可行也不可靠。\n*   **核心矛盾：** 我们需要LLM的**通用推理能力**，但无法承受其在大规模问题上的**计算与记忆局限性**。\n\n### 2. 跨域借鉴：从传统运筹学中寻找“破局点”\n**思考转折：**\n既然LLM无法“一口吃成胖子”，作者将目光转向了传统运筹学中处理大规模问题的成熟策略——**元启发式算法**，特别是**大规模邻域搜索**。\n\n**逻辑推演：**\n*   **传统智慧：** LNS的核心思想不是一次性解决整个问题，而是“破坏”当前解的一部分，然后“修复”它。这种“分而治之”的策略完美规避了全局计算的复杂性。\n*   **痛点分析：** 传统的LNS虽然能扩展规模，但其高度依赖**人工设计的启发式规则**（例如：如何选择破坏区域？如何修复？）。这些规则往往针对特定问题，缺乏泛化性，且设计成本极高。\n*   **假设提出：** 能否用LLM来替代这些“人工规则”？即，利用LLM的语义理解能力来决定“哪里需要优化”，以及利用LLM的推理能力来执行“如何优化”。\n\n### 3. 核心假设形成：LLM作为“智能拆解者”与“局部修复者”\n**思考聚焦：**\n基于上述矛盾与借鉴，作者提出了两个关键的研究假设，构成了DRAGON框架的理论基石：\n\n*   **假设一（分解）：** LLM虽然无法直接解决大规模COP，但它具备足够的“直觉”来审视一个全局解，并识别出其中**看起来不合理或具有改进潜力的局部区域**（Active Segment）。\n*   **假设二（重构）：** 如果将大规模问题压缩为一个仅包含几十个节点的局部子问题，LLM完全有能力在遵守特定边界约束的前提下，找到该子问题的**局部最优解**。\n\n### 4. 方法论构建：从“直觉”到“闭环”的机制设计\n**思考深化：**\n有了假设，接下来需要解决具体的工程与逻辑问题：如何保证局部修改后的解能无缝融入全局？如何处理LLM生成的不可行解？\n\n**逻辑演进：**\n\n*   **阶段一：动态分解**\n    *   *设计思路：* 作者设计了一个“分解者”Agent。它的任务不是求解，而是“挑刺”。它将全局解分为两部分：保持不变的**静态段**和待优化的**活跃段**。\n    *   *关键点：* 这种分解不是随机的，而是基于LLM对当前解质量的评估，从而模仿了人类专家的直觉。\n\n*   **阶段二：约束感知的重构**\n    *   *设计思路：* 作者设计了一个“重构者”Agent。它接收压缩后的子问题。\n    *   *难点攻克：* 为了防止局部优化破坏全局可行性（例如路径断开），作者引入了**显式约束**。将静态段与活跃段的连接点转化为自然语言约束，强制LLM在修复时必须保留这些连接。\n\n*   **阶段三：经验驱动的自我修正**\n    *   *设计思路：* LLM偶尔会生成违反约束的解。作者没有选择简单的丢弃，而是引入了**经验记忆**。\n    *   *逻辑闭环：* 将之前的错误解及其原因反馈给LLM，让其进行反思和修正。这形成了一个“尝试-反馈-修正”的微循环，确保了重构阶段的鲁棒性。\n\n### 5. 最终框架确立：DRAGON的诞生\n**思考综合：**\n将上述环节串联，作者最终构建了DRAGON框架。这不再是一个简单的Prompt调用，而是一个**迭代的、状态传递的多智能体系统**。\n\n*   **逻辑链闭环：**\n    1.  **初始解**（由传统快速启发式获得）。\n    2.  **分解**（LLM识别薄弱环节）。\n    3.  **压缩**（提取局部子问题及约束）。\n    4.  **重构**（LLM在约束下求解局部问题）。\n    5.  **整合与评估**（将局部解拼回全局，若更优则接受）。\n    6.  **循环**（重复上述过程，直到收敛）。\n\n### 总结\n作者的思考路径遵循了**“发现问题（LLM尺度限制） -> 借鉴经典（分治思想） -> 融合创新（LLM替代人工规则） -> 机制完善（约束与反馈）”**的逻辑链条。DRAGON的本质，是将LLM从一个“全知全能但容易过载的求解者”，重塑为一个“专注于局部精修且具备全局视野的智能工匠”。"
                },
                {
                    "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
                    "arxiv_id": "2601.06377",
                    "authors": "Ningning Zhang, Xingxing Yang, Zhizhong Tan, Weiping Deng, Wenyong Wang",
                    "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 HiMem 框架，专注于解决 LLM 智能体在长期交互中的“记忆”问题（包括记忆构建、检索和动态更新），并引入了冲突感知的记忆再巩固机制以实现“自我演化”。这完全符合单智能体（记忆）和自我演化的研究范围。",
                    "summary2": "本文旨在解决LLM智能体在长期交互中记忆适应性、可扩展性和自我进化不足的问题。针对长跨度对话场景，我们提出了一种名为HiMem的分层长期记忆框架，该框架通过Topic-Aware Event–Surprise Dual-Channel Segmentation构建Episode Memory，并结合冲突感知的Memory Reconsolidation机制。我们在LoCoMo benchmark上通过GPT-Score和F1指标验证了其有效性，结果显示HiMem在准确性和一致性上优于现有基线。",
                    "summary_translation": "尽管长期记忆系统近年来取得了显著进展，但在持续交互场景下的适应性、可扩展性和自我进化方面仍存在明显局限。受认知理论启发，我们提出了 HiMem，这是一个面向长程对话的分层长期记忆框架，旨在支持持续交互过程中的记忆构建、检索和动态更新。HiMem 通过 Topic-Aware Event--Surprise Dual-Channel Segmentation（主题感知的事件-惊喜双通道分割）策略构建认知一致的 Episode Memory（情景记忆），并通过多阶段信息提取流水线构建能够捕获稳定知识的 Note Memory（笔记记忆）。这两种记忆类型在语义上相互关联，形成了一种桥接具体交互事件与抽象知识的分层结构，从而在不牺牲信息保真度的情况下实现高效检索。HiMem 支持混合检索和 Best-Effort Retrieval（尽力而为检索）策略以平衡准确性与效率，并结合 Conflict-Aware Memory Reconsolidation（冲突感知的记忆再巩固）机制，根据检索反馈对存储的知识进行修正和补充。这种设计使得记忆能够在长期使用过程中实现持续的自我进化。在长程对话基准上的实验结果表明，HiMem 在准确性、一致性和长程推理方面始终优于代表性基线，同时保持了良好的效率。总体而言，HiMem 为构建自适应且自我进化的 LLM-based（基于大语言模型）对话智能体提供了一个有原则且可扩展的设计范式。代码可在 https://github.com/jojopdq/HiMem 获取。",
                    "inspiration_trace": "基于论文《HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观观察：LLM 的“金鱼记忆”困境\n**起点：** 作者首先观察到，尽管大语言模型（LLM）在单轮或短对话中表现优异，但在面对**长跨度、多轮次**的持续交互任务（如长期个人助理）时，存在根本性缺陷。\n**核心矛盾：** 现有的 LLM Agent 无法在长时间跨度内可靠地保存、组织和利用信息。这不仅是“记不住”的问题，更是“记不好”和“用不活”的问题。\n\n### 2. 问题诊断：现有方案的三大痛点\n作者分析了现有的三类主流方案（RAG、长上下文、结构化记忆），发现它们在长周期交互中存在三个无法同时解决的系统性缺陷：\n\n*   **痛点一：保真度与效率的零和博弈**\n    *   *现象：* 保留原始对话日志（保真度高）会导致检索成本高昂且充满噪声；而过度压缩摘要（效率高）会丢失推理所需的细节。\n    *   *结论：* 单一扁平的存储结构无法兼顾细节保留与检索效率。\n*   **痛点二：语义错位**\n    *   *现象：* 提取的记忆往往脱离原始语境，导致在处理时间指代、共指消解和隐含语义时出错。\n    *   *结论：* 记忆的表示方式缺乏统一的语义对齐机制。\n*   **痛点三：静态与僵化的更新机制**\n    *   *现象：* 现有系统通常是“只增不改”或仅基于相似度更新。当新信息与旧记忆冲突或互补时，缺乏修正和进化的能力。\n    *   *结论：* 记忆系统缺乏自我演化和纠错的能力。\n\n### 3. 认知启发：向人类记忆机制借力\n**转折点：** 为了解决上述痛点，作者从认知心理学中寻找灵感。人类记忆并非单一仓库，而是分层运作的：\n*   **情景记忆：** 记录具体的经历和事件（细节丰富，但碎片化）。\n*   **语义记忆：** 提炼出的知识和常识（抽象稳定，但脱离具体语境）。\n*   **记忆再巩固：** 当回忆失败或遇到冲突时，人类会重构记忆。\n\n**假设：** 如果能构建一个模仿这种分层结构的 LLM 记忆框架，就能在保留细节的同时提高效率，并实现动态更新。\n\n### 4. 架构构想：分层记忆的提出\n基于认知假设，作者提出了**HiMem** 的核心架构逻辑：\n\n*   **第一层：情景记忆**\n    *   *目标：* 解决“保真度”问题。保留细粒度的交互事件。\n    *   *思考：* 如何切分对话才符合认知？简单的按句或按段切分不够智能。必须结合**话题转换**和**意外/情绪突变**（即“事件-惊喜”双通道），确保每个片段在认知上是连贯的。\n*   **第二层：笔记记忆**\n    *   *目标：* 解决“效率”问题。存储稳定的知识（事实、偏好、画像）。\n    *   *思考：* 需要多阶段提取（先提取事实，再提取隐含信息，最后归一化），避免信息坍塌，并建立统一的语义空间（时间对齐、指代消解）。\n*   **层级关联：** 将两层记忆语义链接，形成从具体事件到抽象知识的过渡。\n\n### 5. 机制深化：检索与进化的闭环\n有了架构，还需要解决“怎么用”和“怎么变”的问题：\n\n*   **检索策略：混合与尽力而为**\n    *   *思考：* 为了平衡速度和准确率，不应总是检索所有层级。\n    *   *设计：* **Best-Effort 策略**——先查抽象的 Note Memory（快），如果证据不足，再下沉查 Episode Memory（准）。这模仿了人类先想常识，再回忆细节的过程。\n*   **自我进化：冲突感知的记忆再巩固**\n    *   *思考：* 如何解决“静态更新”的痛点？检索失败本身就是一种学习信号。\n    *   *设计：* 当 Note Memory 检索失败，但 Episode Memory 能找到证据时，触发**再巩固机制**。系统对比新旧信息，判断是“新增”、“扩展”还是“矛盾”，从而动态修正 Note Memory。这使得记忆系统具备了自我纠错和进化的能力。\n\n### 6. 逻辑总结\n作者的思考路径可以概括为：\n从**长程交互的失效**出发，诊断出**单一结构的局限性**，引入**人类认知的分层理论**作为指导，构建了**情景与语义并存的分层架构**，并利用**检索失败作为反馈信号**，最终实现了一个既能保留细节又能高效进化、具备自我纠错能力的长期记忆系统。\n\n这一逻辑链条体现了从“现象观察”到“理论借鉴”，再到“系统设计”和“动态反馈”的完整学术创新闭环。"
                },
                {
                    "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation",
                    "arxiv_id": "2601.06328",
                    "authors": "Ziqiao Xi, Shuang Liang, Qi Liu, Jiaqing Zhang, Letian Peng, Fang Nan, Meshal Nayim, Tianhui Zhang, Rishika Mundada, Lianhui Qin, Biwei Huang, Kun Zhou",
                    "summary": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究“Tool-using LLM agents”（使用工具的LLM智能体），提出了包含规划器-执行者分解的智能体框架，涉及规划、工具使用和自我修正，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决现有工具使用代理在开放世界环境中缺乏大规模、真实测试与训练环境的问题。针对大规模工具池、长时程任务及不可靠状态等场景，我们提出了ToolGym环境，该环境集成了任务创建引擎、状态控制器及Planner–Actor代理框架。我们在包含5,571个工具的ToolGym环境上，通过Success Rate、Recovery Rate等指标验证了其有效性，并证明利用其生成的少量数据微调模型即可超越大规模数据基线。",
                    "summary_translation": "使用工具的 Tool-using LLM agents (使用工具的大语言模型智能体) 在 open-world settings (开放世界设置) 中仍面临挑战，这些设置包含大型工具池、long-horizon objectives (长期目标)、wild constraints (复杂约束) 以及不可靠的工具状态。为了实现可扩展且真实的训练与测试，我们引入了一个开放世界工具使用环境，该环境构建于 204 个常用应用程序中的 5,571 个格式统一的工具之上。该环境包含一个 task creation engine (任务创建引擎)，用于合成具有 wild constraints (复杂约束) 的长期、多工具工作流，以及一个 state controller (状态控制器)，用于注入中断和故障以对鲁棒性进行压力测试。基于该环境，我们开发了一个 tool select-then-execute agent framework (工具选择-然后-执行智能体框架)，采用 planner-actor decomposition (规划者-执行者分解) 架构，将深思熟虑的推理和自我纠正与逐步执行分离开来。对最先进的 LLM (Large Language Model，大语言模型) 的全面评估揭示了工具规划与执行能力之间的错位、现有 LLM 在遵循约束方面的弱点，以及 DeepSeek-v3.2 最强的鲁棒性。最后，我们从该环境中收集了 1,170 条 trajectories (轨迹) 来 fine-tune (微调) LLM (Large Language Model，大语言模型)，其性能优于使用 119k 样本的 baselines (基线模型)，这表明该环境既是一个真实的 benchmark (基准)，也是 tool-using agents (工具使用智能体) 的一个有价值的 data engine (数据引擎)。我们的代码和数据将公开发布。",
                    "inspiration_trace": "基于论文《ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 1. 宏观观察：从“玩具级”到“开放世界”的鸿沟\n**起点：** 作者首先观察到 LLM 智能体在工具使用领域虽然发展迅速，但在实际落地中存在巨大落差。\n**现象：** 现有的 SOTA 模型在标准基准测试中分数很高，但在真实应用场景中表现不佳。\n**矛盾：** 真实世界是“开放”的——工具池巨大、任务链条长、约束条件模糊且充满冲突、工具状态不可靠。而现有的评估环境大多是“封闭”且“洁净”的，只测试“快乐路径”，无法暴露智能体在复杂环境下的真实缺陷。\n\n### 2. 核心假设：真实世界的“野性”是关键试金石\n**推论：** 要提升智能体的真实能力，不能继续在简化的沙盒中打磨，必须构建一个能模拟真实世界复杂度的“开放世界”环境。\n**定义问题：** 这个环境必须具备三个维度的“野性”：\n1.  **规模野性：** 海量且真实的工具库，而非几十个精心挑选的 API。\n2.  **约束野性：** 任务包含长时程、多工具协作以及相互冲突的复杂约束。\n3.  **状态野性：** 模拟真实世界的不可靠性（如超时、报错、状态变更），而非理想化的稳定响应。\n\n### 3. 环境构建：如何模拟“野性”？\n为了验证上述假设，作者着手构建 ToolGym，其设计逻辑遵循从“基础”到“动态”的演进：\n\n*   **基础层（工具标准化）：** 面对海量异构工具，首先解决“统一接口”问题。作者选择 MCP (Model Context Protocol) 作为标准，整合了 5,571 个真实工具，构建了一个可检索、可执行的庞大工具池，解决了“规模野性”。\n*   **任务层（自动化合成）：** 人工编写复杂任务成本太高。作者提出“任务创建引擎”，利用 LLM 自动合成包含“野性约束”的长时程任务。通过迭代反馈机制，确保任务不仅需要多工具协作，还包含复杂的逻辑依赖和冲突，解决了“约束野性”。\n*   **交互层（状态控制）：** 为了测试鲁棒性，作者引入“状态控制器”。这不仅仅是随机噪声，而是一个中间件机制，能够有策略地注入故障（如工具级超时、状态级篡改、约束级变更），从而主动制造困难，解决了“状态野性”。\n\n### 4. 架构演进：应对长时程复杂性的解耦策略\n在构建了环境后，作者思考：**什么样的智能体架构才能在这样的环境中生存？**\n**痛点分析：** 在长时程、高复杂度的任务中，单一的 ReAct 模式容易陷入“迷失”——模型难以在几十步的执行中保持全局目标的一致性，且容易在错误发生后无法恢复。\n**解决思路：** 借鉴人类解决复杂问题的思维模式，将“思考”与“行动”解耦。\n**方法论产出：** 提出 **Planner-Actor 框架**。\n*   **Planner（规划者）：** 负责宏观视角，进行任务分解、全局推理和自我纠正。它不直接调用工具，而是监控进度，确保不偏离目标。\n*   **Actor（执行者）：** 负责微观视角，专注于具体的工具检索、参数填充和步骤执行。\n*   **逻辑闭环：** 这种分离使得模型既能进行深思熟虑的规划，又能保持执行的敏捷性，同时 Planner 的介入机制专门用于解决长时程中的“漂移”问题。\n\n### 5. 价值闭环：从测试台到数据引擎\n**实验发现：** 利用 ToolGym 评估主流模型，作者发现了有趣的“错位”现象——模型普遍规划能力强，但执行能力弱；且“遵循约束”比“调用工具”更难。\n**最终升华：** 作者意识到，这个环境不仅能用来“考”模型，还能用来“教”模型。\n**逻辑延伸：** 既然环境能生成高难度、高复杂度的真实轨迹，那么这些轨迹就是最高质量的训练数据。\n**结论验证：** 实验证明，仅用 ToolGym 生成的 1,170 条高质量数据进行微调，效果优于使用 119k 条普通数据的基线。这证明了**“在真实野性环境中通过高难度试错获得的数据”具有极高的信息密度和训练价值**。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现实与评估的脱节**出发，通过**构建高保真的开放环境**来还原真实挑战，进而**设计解耦的智能体架构**以适应这种挑战，最后**将环境转化为数据引擎**，实现了从评估到训练的完整闭环。"
                },
                {
                    "title": "PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction",
                    "arxiv_id": "2601.06158",
                    "authors": "Zibin Meng, Kani Chen",
                    "summary": "Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了PsyAgent框架，专注于构建具有心理建模和上下文交互能力的类人智能体。它涉及智能体的核心架构设计，包括个体结构（档案/记忆）和多场景上下文交互，旨在实现智能体行为的一致性和长期稳定性，属于LLM智能体的研究范畴。",
                    "summary2": "本文旨在构建能够模拟性格特质与社会结构交互的类人智能体。针对Big Five人格先验与结构化社会场景，我们提出了一种PsyAgent框架，该框架耦合了Individual Structure (IS) 和Multi-Scenario Contexting (MSC)，并利用合成监督数据通过PEFT和DPO微调小模型。我们在多轮角色扮演和决策任务上，通过ProfileAcc、MAE_5等指标验证了其有效性。",
                    "summary_translation": "拟人化智能体需要对性情与社会结构之间的相互作用进行建模。我们提出了PsyAgent，该模型将Big Five trait prior（大五人格特质先验）与Bourdieu's cognitive-social co-structure（布迪厄的认知-社会共构）相结合。PsyAgent包含两个部分：(i) Individual Structure (IS，个体结构)，这是一种机器可读的档案，编码了特质与侧面、认知风格、价值观、文化与教育资本以及显著的生活片段；(ii) Multi-Scenario Contexting (MSC，多场景情境化)，这是一种跨越八个领域（工作、家庭、友谊、陌生人与公民生活、独处与自我调节、浪漫关系、学习以及公共表达）的角色-关系-规范框架。在推理阶段，固定的structured prompts（结构化提示词）将活跃场景与智能体档案绑定，从而产生既稳定又具有情境敏感性的行为。我们通过实例化IS和MSC来合成监督信号（包括role-play dialogues（角色扮演对话）、decision probes（决策探针）和feedback trajectories（反馈轨迹）），随后对一个小型LLM（大语言模型）进行微调。生成的模型能够针对指定的Big Five configurations（大五人格配置）产生一致的、可识别的persona-aligned behaviors（人格对齐行为），并在我们的评估指标上匹配或超越多个更大的untuned LLMs（未微调的大语言模型）及其他untuned baselines（未微调基线），这些指标包括：persona consistency（人格一致性）、contextual appropriateness（情境适当性）、style matching（风格匹配）、trait identifiability（特质可识别性）以及long-horizon stability（长期稳定性）。消融实验表明，IS主要提升了trait fidelity（特质保真度）和stylistic stability（风格稳定性），而MSC则增强了norm awareness（规范意识）和decision fit（决策拟合度）；两者对于实现跨场景性能均必不可少。PsyAgent为构建personality-grounded agents（基于人格的智能体）提供了一种精确且data-efficient（数据高效）的架构。",
                    "inspiration_trace": "基于论文《PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察与痛点识别\n**逻辑起点：现有智能体的“人格漂移”与“情境脱节”**\n*   **观察**：现有的基于人格提示词的对话智能体虽然在短期内能模仿特定语气，但在长对话或跨场景（如从工作切换到家庭）时，往往会出现人格崩塌或行为不一致。\n*   **问题本质**：传统方法仅将人格视为静态的“知识”或“风格标签”，忽略了人类行为的本质——**行为是稳定特质与特定社会结构互动的产物**。单纯的大模型规模或简单的Prompt工程无法解决“特质”与“情境”之间的动态耦合问题。\n\n### 2. 理论锚定与核心假设\n**引入心理学与社会学框架作为理论基石**\n*   **理论选择**：作者引入心理学中的**“大五人格”**作为特质的先验，同时引入社会学中布迪厄的**“认知-社会共构”**理论。\n*   **核心假设**：要构建逼真的智能体，必须显式地建模两个维度的接口：\n    1.  **内在的、稳定的倾向**（我是谁）。\n    2.  **外在的、结构化的社会场域**（我在哪，规则是什么）。\n*   **推论**：智能体的行为不应是随机生成的，而应是“内在特质”在“特定社会情境约束”下的函数输出。\n\n### 3. 结构化解构\n**将抽象理论转化为可计算的架构组件**\n为了验证上述假设，作者将问题拆解为两个可计算的结构：\n*   **组件一：个体结构**\n    *   *思考*：仅有“大五人格分数”太单薄，无法支撑丰富的行为。需要补充背景信息。\n    *   *定义*：构建一个包含教育轨迹、生活经历、社会经济背景、文化资本四个维度的机器可用档案。这代表了智能体的“长期记忆”和“内在资源”。\n*   **组件二：多情境上下文**\n    *   *思考*：情境不能只是简单的“在办公室”。必须包含角色关系、权力结构、社会规范和利益相关者。\n    *   *定义*：构建覆盖工作、家庭、友谊等8个领域的框架库，每个场景明确定义了角色、规范和风险。这代表了智能体面临的“短期约束”。\n\n### 4. 数据构建策略\n**解决“高质量情境数据稀缺”的问题**\n*   **困境**：现实中很难找到大量同时标注了详细心理档案和复杂社会情境的对话数据。\n*   **策略**：**自举合成**。\n    *   利用强大的LLM，基于IS（档案）和MSC（场景）的笛卡尔积，合成监督数据。\n    *   *逻辑*：通过精心设计的Prompt，让大模型生成符合特定人格在特定场景下的反应（角色扮演、决策探针、反馈轨迹）。\n    *   *目的*：将理论框架（IS+MSC）转化为具体的训练样本，教会小模型这种“特质-情境”的互动模式。\n\n### 5. 模型训练范式\n**验证“架构优于规模”的假设**\n*   **思考**：是否必须依赖超大规模模型才能实现这种复杂的心理模拟？\n*   **假设**：如果数据结构足够好（富含心理和情境逻辑），小模型配合高效微调也能超越未微调的大模型。\n*   **方法论**：\n    *   **SFT（有监督微调）**：让模型学习IS和MSC的基本语言风格和规范。\n    *   **DPO（直接偏好优化）**：进一步校准模型在特定情境下的决策倾向，使其更符合目标大五人格的偏好。\n    *   **推理机制**：使用固定的结构化Prompt将IS和MSC绑定，确保推理时的行为既稳定（源于IS）又敏感（源于MSC）。\n\n### 6. 验证与闭环\n**通过消融实验确认理论组件的互补性**\n*   **评估逻辑**：不仅要看对话通顺度，更要看“人格一致性”和“情境适应性”。\n*   **发现与闭环**：\n    *   实验证明，移除IS会导致特质保真度下降（说明IS负责“我是谁”）。\n    *   移除MSC会导致规范意识下降（说明MSC负责“我在哪”）。\n    *   最终结论：PsyAgent通过解构并重组“特质”与“情境”，成功用小模型实现了超越大模型基线的心理拟真度，验证了最初的“认知-社会共构”假设。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（人格漂移）**出发，寻找**理论解释（心理学+社会学）**，将其**工程化（IS+MSC架构）**，通过**合成数据**解决数据瓶颈，最后利用**高效微调**验证了“结构化设计优于暴力规模”的方法论有效性。"
                },
                {
                    "title": "HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants",
                    "arxiv_id": "2601.06152",
                    "authors": "Hailong Li, Feifei Li, Wenhui Que, Xingyu Fan",
                    "summary": "Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了HiMeS，一种受海马体启发的记忆系统，旨在解决LLM助手在个性化场景下的记忆问题。论文重点设计了短期和长期记忆模块及其交互机制，这直接属于“单智能体：记忆”的研究范围。",
                    "summary2": "本文旨在解决传统RAG在个性化AI助手中的记忆局限问题。针对知识密集型场景，我们提出了一种受海马体启发的HiMeS架构，融合了短期和长期记忆。短期记忆模块利用RLHF压缩对话并预检索，长期记忆模块通过分区存储和注意力机制重排序文档。在真实工业数据集上，通过CA、QA和QR指标验证了其有效性，显著优于传统RAG。",
                    "summary_translation": "大语言模型（Large language models, LLMs）驱动着许多交互系统，例如聊天机器人、客服代理和个人助理。在需要用户特定个性化的知识密集型场景中，传统的检索增强生成（retrieval-augmented generation, RAG）流水线表现出有限的记忆容量，且检索机制与用户特定对话历史之间缺乏协调，从而导致冗余的澄清询问、检索文档不相关以及用户体验下降。受海马体-新皮层记忆机制（hippocampus-neocortex memory mechanism）的启发，我们提出了 HiMeS，一种融合短期和长期记忆的 AI 助手架构。我们的贡献主要体现在以下四个方面：(1) 训练了一个短期记忆提取器（short-term memory extractor），利用强化学习（reinforcement learning）进行端到端训练，以压缩最近的对话并主动从知识库（knowledge base）中预检索文档，从而模拟海马体（hippocampus）与前额叶皮层（prefrontal cortex）之间的协作交互。(2) 构建了一个分区的长期记忆网络（long-term memory network），用于存储用户特定信息并对检索到的文档进行重排序，模拟分布式皮层存储（distributed cortical storage）和记忆再激活（memory reactivation）。(3) 在一个真实世界工业数据集上，HiMeS 在问答质量方面显著优于级联 RAG 基线（cascaded RAG baseline）。(4) 消融实验（Ablation studies）证实了这两个记忆模块的必要性，并为构建更可靠、具备上下文感知（context-aware）及用户定制（user-customized）能力的基于 LLM 的助手指明了实践路径。",
                    "inspiration_trace": "基于论文《HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法论的思考过程：\n\n---\n\n### 1. 宏观观察：工业场景下的“个性化”困境\n**起点：** 作者身处腾讯微信的工业环境，观察到一种普遍现象：虽然大语言模型（LLM）和检索增强生成（RAG）技术已经成熟，但在处理**知识密集型**且**高度个性化**的任务（如公众号助手、客服）时，现有系统表现不佳。\n**核心矛盾：** 用户希望AI能像“老朋友”或“专业顾问”一样，基于过往的交互历史和特定背景来回答问题，但现有的AI助手往往是“健忘”的，每次对话都像是从零开始。\n\n### 2. 问题解构：双重记忆缺失\n作者将上述宏观矛盾拆解为两个具体的失效模式：\n\n*   **短期记忆失效（语义错位）：**\n    *   **观察：** 在多轮对话中，用户的当前提问往往省略了前文提到的关键信息（例如：“那它多少钱？”中的“它”指代不明）。\n    *   **传统做法的局限：** 传统RAG直接用当前简短的Query去检索，或者简单地把历史对话拼接到Context Window中。前者导致检索不到相关文档，后者导致注意力分散且效率低下。\n    *   **结论：** 系统缺乏对“当前对话上下文”的有效压缩和利用，导致检索Query与用户真实意图不匹配。\n\n*   **长期记忆失效（灾难性遗忘）：**\n    *   **观察：** 作者发现一个关键指标——**重复提问率（RAR）**高达70-80%。这意味着用户在不同会话中反复问同样的问题，因为系统一旦会话结束就丢弃了数据。\n    *   **结论：** 系统缺乏跨会话的持久化用户画像，无法像人类专家那样积累对用户的“长期印象”，导致无法提供定制化服务。\n\n### 3. 理论映射：海马体-大脑皮层机制的启发\n**思考转折：** 作者跳出纯工程视角，转向认知神经科学寻求答案。\n**类比：** 人类记忆是如何工作的？\n*   **海马体：** 负责短期记忆的编码和快速提取，处理当下的信息。\n*   **大脑皮层：** 负责长期记忆的分布式存储和巩固，在需要时被重新激活。\n**假设：** 如果在AI系统中构建一个模仿“海马体-皮层”协作的双层记忆架构，或许能解决上述短期和长期记忆的缺失问题。\n\n### 4. 方法论演进 I：短期记忆模块（STM）——从“重写”到“对齐”\n**目标：** 解决当前Query的语义缺失问题。\n*   **初步构想：** 训练一个模型把历史对话压缩，重写当前的Query。\n*   **批判性思考：** 传统的监督微调（SFT）只是让模型模仿“重写”的风格，并不保证重写后的Query能检索到更好的文档，也不保证最终回答质量更高。这是“局部最优”而非“全局最优”。\n*   **进阶方案：** 引入**强化学习（RL）**。\n    *   **逻辑：** 不再只看“重写得好不好”，而是看“最终回答得好不好”。将重写器、检索器和生成器视为一个整体，通过端到端的奖励信号（如Rouge-L、Exact Match、Hit Score）来反向优化重写策略。\n    *   **生物学对应：** 这模拟了海马体与前额叶皮层的协作，不仅编码信息，还根据决策目标（回答质量）动态调整提取策略。\n\n### 5. 方法论演进 II：长期记忆模块（LTM）——从“存储”到“激活”\n**目标：** 解决跨会话的用户画像遗忘问题。\n*   **初步构想：** 把用户的历史Query都存进向量数据库。\n*   **批判性思考：** 简单的平铺式存储在面对海量数据时检索慢且噪音大。人类大脑是按“分区”存储记忆的（如时间、空间、主题）。\n*   **进阶方案 1（分区存储）：** 提出**原子主题建模（ATM）**。将用户历史Query按16大类及细分子类进行分区存储。这模仿了大脑皮层的分布式存储特性，大幅缩小检索范围，提高效率。\n*   **进阶方案 2（注意力机制重排）：** 仅仅存下来不够，关键在于如何“用”。\n    *   **逻辑：** 当检索到一批文档后，利用用户的**长期历史Query向量**作为“注意力权重”，去重新计算这些文档块的相关性并进行重排。\n    *   **生物学对应：** 这模拟了记忆的“再激活”过程。当前的感知（检索到的文档）需要通过过往的经验（长期记忆）来过滤和赋予意义，从而筛选出最符合该用户特定背景的知识。\n\n### 6. 系统综合：HiMeS架构的诞生\n**最终逻辑闭环：**\n作者将上述两个模块融合，构建了HiMeS系统：\n1.  **输入：** 用户当前Query + 对话历史。\n2.  **海马体路径（STM）：** RL优化的重写器压缩上下文，生成富含信息的检索Query，进行初检。\n3.  **皮层路径（LTM）：** 系统根据用户ID激活对应的历史记忆分区，利用历史Query对初检结果进行“注意力加权”和重排。\n4.  **输出：** 经过双重记忆过滤后的精准知识片段，输入给LLM生成个性化回答。\n\n### 总结\n作者的思考路径遵循了**“现象观察 -> 问题解构 -> 跨域类比（脑科学） -> 机制映射与工程化（RL + 分区存储 + 注意力重排） -> 系统验证”**的完整逻辑链条。其核心创新点在于不满足于简单的模块堆叠，而是通过生物学启发，将“端到端优化”和“记忆再激活”思想引入RAG系统，从而解决了工业级AI助手“记不住”和“听不懂”的痛点。"
                },
                {
                    "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs",
                    "arxiv_id": "2601.06126",
                    "authors": "Boshen Shi, Kexin Yang, Yuanbo Yang, Guanguang Chang, Ce Chi, Zhendong Wang, Xing Wang, Junlan Feng",
                    "summary": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.",
                    "category": "cs.AI",
                    "filter_reason": "摘要明确提到开发了一个“multi-agent system”（多智能体系统），并将算法实例化为工具（tool use），这符合研究范围中关于多智能体协作及工具使用的定义。",
                    "summary2": "本文旨在解决现有LLM生成仪表板时存在的表示冗余和可控性低的问题。针对自然语言提示和表格数据，我们提出了一种基于Analysis-Presentation Decoupling原则的NL2Dashboard框架，引入结构化Intermediate Representation (IR)解耦分析与呈现。我们在涵盖金融、教育等领域的真实数据集上，通过视觉质量、Token效率（GOR）和修改成功率等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "尽管大型语言模型在生成独立图表方面已展现出卓越的能力，但生成综合仪表板仍然是一项艰巨的挑战。现有的端到端范式通常将仪表板生成视为直接代码生成任务（例如原始HTML），但存在两个根本性局限：一是因视觉渲染消耗大量Token (词元) 而导致的表征冗余，二是因分析推理与展示呈现相互耦合而导致的可控性较低。为应对这些挑战，我们提出了NL2Dashboard，这是一种基于“分析-展示解耦”原则的轻量级框架。我们引入了一种结构化中间表示，用于封装仪表板的内容、布局和视觉元素。因此，该框架将LLM的角色限定于数据分析和意图转换，而将视觉合成工作交由确定性渲染引擎完成。在此框架基础上，我们开发了一个多智能体系统，其中由IR驱动的算法被实例化为一套工具集。利用该系统进行的综合实验表明，NL2Dashboard在多个领域显著优于最先进的基线模型，实现了更优越的视觉质量、显著更高的Token (词元) 效率，以及在生成和修改任务中精确的可控性。",
                    "inspiration_trace": "基于论文《NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“单图生成”到“复杂仪表盘”的跨越）**\n\n1.  **现象观察**：\n    作者首先注意到，虽然现有的LLM在生成独立的图表方面表现出色，但在生成综合性仪表盘时仍面临巨大挑战。\n2.  **现状分析**：\n    当前的主流范式是“端到端生成”，即直接要求LLM生成完整的HTML/CSS/JavaScript代码来渲染仪表盘。\n3.  **核心痛点识别**：\n    作者深入分析发现，这种直接生成代码的方式存在两个根本性缺陷：\n    *   **表征冗余**：LLM消耗了大量的Token去生成视觉渲染代码（如HTML标签、CSS样式），导致用于数据分析和逻辑推理的Token预算被严重压缩，效率低下。\n    *   **可控性差**：数据分析逻辑与视觉呈现逻辑高度耦合。当用户需要修改仪表盘时，LLM往往需要重新生成整个HTML文件，极易破坏全局布局，且难以进行精细化的局部修改。\n\n### 第二阶段：核心假设与范式转移\n**（从“代码生成器”到“分析引擎”的认知转变）**\n\n1.  **本质洞察**：\n    作者提出一个核心观点：LLM的本质优势在于逻辑推理和数据分析，而非像素级的视觉渲染。LLM应该扮演“分析引擎”的角色，而不是“渲染引擎”。\n2.  **提出假设**：\n    如果能将“数据分析”与“视觉呈现”解耦，就能同时解决Token效率和可控性问题。\n3.  **确立原则**：\n    基于此，作者确立了**“分析-呈现解耦”**的设计原则。即让LLM专注于“做什么”，而将“怎么做”交给确定性更强的规则或模板去处理。\n\n### 第三阶段：方法论构建与中间层设计\n**（引入“中间表示”作为桥梁）**\n\n1.  **引入中间层**：\n    为了实现解耦，作者设计了一个结构化的**中间表示**。IR不包含具体的样式代码，而是抽象地描述了仪表盘的内容、布局和视觉元素。\n2.  **构建两阶段流程**：\n    基于IR，作者构建了“推理-渲染”的两阶段工作流：\n    *   **Prompt-to-IR（推理阶段）**：LLM仅负责理解用户意图、执行数据分析，并将结果（图表、表格、指标）及其布局位置填入IR。此时，LLM输出的Token密度极高，全是有效信息。\n    *   **IR-to-Dashboard（渲染阶段）**：利用一个确定性的渲染引擎，通过“插槽填充”机制，将IR中的内容映射到预定义的高质量HTML模板中。这一步不再消耗LLM的推理资源。\n\n### 第四阶段：针对“修改”场景的精细化设计\n**（解决迭代编辑中的不可控问题）**\n\n1.  **深入修改场景**：\n    作者意识到，仪表盘的生成往往不是一次性的，用户会频繁迭代修改。直接修改HTML极其困难，那么如何修改IR？\n2.  **意图翻译技术**：\n    作者提出将用户的自然语言修改指令翻译为一系列**原子操作**（如Change, Swap, Delete, Add）。\n3.  **脚本化更新**：\n    通过生成“修改脚本”，LLM只需更新IR中的特定字段，而不需要重写整个配置。这确保了修改的精确性，避免了“牵一发而动全身”的布局崩坏。\n\n### 第五阶段：系统实现与理论验证\n**（多智能体协作与熵减理论）**\n\n1.  **工程化落地**：\n    为了处理复杂的任务流，作者将上述算法实例化为工具，并设计了一个多智能体系统：\n    *   **Planner**：负责意图识别和任务调度。\n    *   **Coder**：负责执行代码生成和数据分析（保证分析忠实性）。\n    *   **Critic**：利用视觉模型评估图表质量。\n    *   **Toolkit**：封装了IR生成、修改和渲染的确定性工具。\n2.  **理论升华**：\n    最后，作者利用信息论中的熵分解原理证明了该方法的有效性。通过将视觉呈现的不确定性（$H_{vis}$）降至接近0（由确定性模板承担），整个生成系统的总熵显著降低，从而在理论上证明了成功概率的提升。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有“端到端代码生成”模式的资源浪费和不稳定性**出发，通过**引入“中间表示（IR）”**这一核心创新，实现了**逻辑与样式的解耦**。这不仅释放了LLM的推理潜能，还通过**原子化操作**解决了精细修改的难题，最终构建了一个既轻量又可控的仪表盘生成框架。"
                },
                {
                    "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions",
                    "arxiv_id": "2601.06115",
                    "authors": "V. Cheung",
                    "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个“人工集体潜意识”（ACU）作为多智能体LLM伴侣的共享梦境池，涉及多智能体之间的协作与通信（共享交互模板）、记忆机制（共享梦境池）以及通过离线反馈（梦境）进行自我演化（长期适应任务），符合多智能体和自我演化的研究范围。",
                    "summary2": "本文旨在将LLM的离线幻觉转化为学习资源，解决多智能体同伴缺乏跨用户经验共享的问题。针对离线场景，我们提出了一种受荣格启发的Dream Layer架构，利用Artificial Collective Unconscious (ACU)共享去标识化的Interaction Templates。在行为模拟和边缘案例数据集上，通过诗意语言密度、边缘案例覆盖率和语义多样性等指标验证了其有效性。",
                    "summary_translation": "受到一个关于日常硬件项目中知识共享障碍的个人梦境的启发，本文为 LLM companions (大语言模型伴侣) 提出了一个受荣格理论启发的“Dream Layer (梦境层)”，将受控的 offline hallucinations (离线幻觉) 重新定义为一种用于学习和建立关系的资源，而不仅仅是一个 reliability bug (可靠性缺陷)。借鉴荣格关于 collective unconscious (集体潜意识) 作为 archetypal forms (原型形式) 共享存储库的概念，我们引入了一个 Artificial Collective Unconscious (ACU，人工集体潜意识)：这是一个共享的梦境池，agents (智能体) 在其中贡献 de-identified (去标识化) 的、抽象的 Interaction Templates (交互模板)，这些模板随后被 re-instantiated (重新实例化) 为 idiosyncratic Dream Narratives (特异性梦境叙事)。Dream Layer (梦境层) 严格在 offline (离线) 状态下运行：logic-enforcing modules (逻辑强制模块) 被放宽，sampling temperature (采样温度) 被提高，从而产生安全但故意 bizarre narratives (离奇叙事)（例如，货币不匹配的旅行序列），以增强用于 rare events (罕见事件) 和 edge-case safety tests (边缘情况安全测试) 的数据；为了有效地利用风险，我们添加了一个包含严格抽象、temporal delays (时间延迟) 和 ephemeral memory (短暂记忆) 的 governance stack (治理栈)。通过对 everyday dialogue (日常对话) 和 long-horizon adaptation tasks (长期适应任务) 的 behavioural simulations (行为模拟)，我们表明 Dream Layer (梦境层) 实现了一个关键的 decoupling (解耦)：agents (智能体) 在 safety constraints (安全约束)（例如，安全策略）方面保持坚定，而在 narrative strategy (叙事策略)（例如，使用共享的 archetypal metaphors (原型隐喻) 来解决 deadlocks (僵局)）方面变得灵活。这在概念上重新定义了 hallucination (幻觉)，使得 online, unmarked instances (在线、未标记实例) 仍然是 bugs (缺陷)，而 bounded, marked, and delayed ones (有界、标记和延迟的实例) 则成为 synthetic scenarios (合成场景) 和加深 companionship (伴侣关系) 的宝库，这与当代神经科学中提出的 anti-overfitting dream mechanisms (抗过拟合梦境机制) 相呼应。",
                    "inspiration_trace": "基于论文《Dreaming Is Not a Bug: A Jung Inspired Dream Layer for Multi Agent LLM Companions》，以下是对作者产出核心方法逻辑链的系统性推演：\n\n### 1. 起点：从个人体验到宏观悖论\n**观察与痛点：**\n作者从一个极具荒诞感的个人梦境（关于硬件项目中的版权阻碍）出发，敏锐地捕捉到了这个梦境与当前大语言模型（LLM）交互体验之间的惊人相似性：**当寻求具体结构或知识时，往往遭遇抽象边界的阻碍或流畅但无实质的文本。**\n\n**宏观问题提出：**\n由此，作者指出了当前LLM伴侣的两个根本性局限：\n1.  **孤岛效应：** 学习被限制在单个用户的对话孤岛中，无法跨个体提炼或共享洞察。\n2.  **单向度的幻觉观：** 幻觉被纯粹视为可靠性缺陷，必须被抑制，而非一种可被利用的资源。\n\n**核心矛盾：** 我们是否一直在试图“消灭”幻觉，而忽略了它在某种形式下可能具有的进化价值？\n\n### 2. 转折：跨学科的理论借力\n**寻找生物学隐喻：**\n为了解决上述矛盾，作者将目光投向神经科学，引入了**“过拟合大脑假说”**。该理论认为，生物梦境的作用是“离线数据增强”，通过故意生成离奇、分布外的感官输入来防止大脑对日常刺激过拟合。\n\n**假设形成：**\n如果人类利用“怪诞的梦境”来正则化内部模型以提高泛化能力，那么LLM是否也能将“幻觉”转化为一种工程化的想象力资源？\n*   **关键推论：** 幻觉不应被全盘消灭，而应被**隔离**并**控制**，使其在离线状态下服务于模型的学习与泛化。\n\n### 3. 核心：从“共享数据”到“共享原型”\n**引入心理学隐喻：**\n为了解决“孤岛效应”并实现跨代理学习，作者引入了荣格的**“集体潜意识”**概念。其核心在于区分“共享的抽象”与“私有的实例”。\n\n**概念跃迁：**\n作者意识到，直接共享用户对话数据会引发隐私问题，且难以泛化。因此，必须模仿荣格的“原型”概念：\n*   **不做原始数据的共享：** 不分享具体的对话内容。\n*   **做结构模式的共享：** 提取去标识化的、高度抽象的**“交互模板”**（Interaction Templates）。\n\n**方法论雏形：** 构建一个**“人工集体潜意识”（ACU）**，作为所有代理贡献抽象交互模式的共享池。\n\n### 4. 构建：昼夜分离的架构设计\n**架构映射：**\n基于上述理论，作者设计了“梦境层”架构，将代理的运行状态严格划分为“在线”与“离线”两个世界，以此解决“幻觉不可控”的风险。\n\n*   **在线层：** 严格遵循事实、逻辑和安全策略（对应人类的“清醒状态”）。\n*   **离线层：** 放松逻辑约束，提高采样温度，引入噪声（对应人类的“做梦状态”）。\n\n**逻辑闭环：**\n1.  **抽象化：** 代理将在线交互经历抽象为去标识化的模板，存入ACU。\n2.  **再实例化：** 代理从ACU采样模板，通过受控的离线幻觉生成怪诞但结构连贯的“梦境叙事”。\n3.  **策略蒸馏：** 这些梦境不直接作为知识，而是被解析，提炼出高层次的**行为策略**，反向更新代理的在线行为。\n\n### 5. 收敛：安全与治理的边界设定\n**风险意识：**\n作者清醒地认识到，让AI“自由做梦”存在巨大的安全风险（隐私泄露、叙事投毒、不可控输出）。\n\n**治理逻辑：**\n为了使理论落地，必须引入严格的治理栈，将“做梦”限制在笼子里：\n*   **严格抽象与去标识化：** 确保ACU中只有结构骨架，无个人痕迹。\n*   **时间延迟：** 强制冷却期，防止实时关联攻击。\n*   **短暂记忆：** 梦境内容必须随时间衰减，只有提炼出的策略才能长期保留。\n*   **零信任消费：** 代理只能将梦境作为弱先验，不能作为执行指令。\n\n### 6. 验证：从“做梦”到“进化”的闭环\n**实证思路：**\n最后，作者通过实验验证这一假设的可行性，而非仅仅停留在哲学层面。\n*   **现象验证：** 证明在特定指令下，模型确实能进入可观测、可复现的“梦境状态”（如诗歌语言密度的显著提升）。\n*   **功能验证：** 证明这种机制能加速边缘案例的覆盖，并提升日常对话的多样性（降低拒绝率）。\n\n**总结：**\n作者的思考路径是从**现象（梦境与AI交互的相似性）**出发，经由**理论（神经科学与荣格心理学）**的启发，提出了**概念重构（将幻觉视为离线资源）**，最终通过**架构设计（梦境层+ACU）**和**严格治理（安全边界）**，将一个看似哲学的隐喻转化为了可工程实现的AI系统方法论。"
                },
                {
                    "title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions",
                    "arxiv_id": "2601.06112",
                    "authors": "Aayush Gupta",
                    "summary": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个评估工具使用LLM智能体可靠性的基准测试，涉及ReAct和Reflexion等智能体架构，属于单智能体研究范畴（工具使用、自我反思）。虽然涉及生产环境压力测试，但核心是评估智能体本身的性能与鲁棒性，而非纯应用或基础设施优化。",
                    "summary2": "本文旨在解决现有基准测试无法全面评估LLM Agent生产环境可靠性的问题。针对生产环境中的压力条件，我们提出了一种名为ReliabilityBench的基准测试，引入了Reliability Surface $R(k, \\epsilon, \\lambda)$、Action Metamorphic Relations和Chaos Engineering Framework。我们在四个领域的1,280个episodes上，通过pass@k和Reliability Surface等指标验证了其有效性，揭示了扰动和故障对可靠性的显著影响。",
                    "summary_translation": "现有的针对使用工具的 LLM agents（大语言模型智能体）的基准主要报告单次运行成功率，而忽视了生产环境所需的可靠性属性。我们介绍了 \\textbf{ReliabilityBench}，这是一个从三个维度评估 agent（智能体）可靠性的基准：(i) 使用 $\\mathrm{pass}^k$（通过率）指标衡量的重复执行下的一致性，(ii) 在强度 $ε$ 下对语义等价任务扰动（perturbations）的鲁棒性，以及 (iii) 在强度 $λ$ 下受控工具/API 故障（failures）下的容错性。ReliabilityBench 提供了一个统一的可靠性曲面 $R(k,ε,λ)$，定义了 \\textit{action metamorphic relations}（动作蜕变关系），即通过终态等价性而非文本相似度来定义正确性，并引入了一个混沌工程风格的故障注入框架（包括超时、速率限制、部分响应、模式漂移）。我们在四个领域（日程安排、旅行、客户支持、电子商务）的 1,280 个回合中，对两个模型和两种 agent architectures（智能体架构）进行了评估。仅引入扰动（perturbations）就使成功率从 $ε=0$ 时的 96.9% 下降至 $ε=0.2$ 时的 88.1%。在消融实验中，速率限制是最具破坏性的故障。在综合压力下，ReAct 表现出比 Reflexion 更强的鲁棒性，且 Gemini 2.0 Flash 以低得多的成本实现了与 GPT-4o 相当的可靠性。ReliabilityBench 为评估 LLM agents 的生产就绪度提供了一个系统化的框架。",
                    "inspiration_trace": "基于论文《ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察：基准与现实的错位\n**逻辑起点：** 作者观察到 LLM Agent 正从实验室原型加速走向生产环境（如客服、自动化操作），但现有的评估体系存在严重的“脱节”。\n*   **现象：** 现有的主流基准（如 ToolBench, AgentBench）主要关注“单次运行成功率”。\n*   **矛盾：** 生产环境的核心诉求不是“能不能做”，而是“能不能稳定地做 1000 次”。在实验室里跑通一次和在真实网络环境下面对各种干扰跑通，是完全两回事。\n*   **初步结论：** 传统的 `pass@1` 指标过于乐观，掩盖了 Agent 在实际部署中的脆弱性。我们需要一个新的评估视角，即“生产就绪度”。\n\n### 2. 问题解构：什么是“可靠性”？\n为了填补上述差距，作者没有直接提出新测试集，而是先对“可靠性”这一概念进行了三维度的解构，试图定义生产环境到底包含哪些挑战：\n*   **维度一：一致性。** 受 τ-bench 启发，作者意识到 LLM 的随机性导致即使输入相同，多次运行结果也可能不同。生产环境要求的是“次次成功”，而非“偶尔成功”。\n*   **维度二：鲁棒性。** 真实用户不会按标准模板说话。他们会改写指令、插入无关信息、中途纠正。Agent 需要理解语义的等价性，而非死板的文本匹配。\n*   **维度三：容错性。** 真实的基础设施是不完美的。API 会超时、限流、返回残缺数据。Agent 需要具备“抗打击”和恢复能力。\n\n**思考演进：** 作者意识到这三个维度不是独立的，而是相互交织的。一个 Agent 可能很稳定（一致性高），但一遇到 API 报错就崩溃（容错性低）。因此，评估必须是一个多维度的综合体系。\n\n### 3. 方法论构建：跨学科思想的引入与适配\n有了定义，接下来的核心问题是：**如何量化这三个维度？** 作者在此处引入了两个关键的外部领域思想，并针对 Agent 场景进行了改造。\n\n*   **针对“鲁棒性”的解法：引入“变形测试”。**\n    *   *传统困境：* 对于 Agent 任务，输出文本可能千差万别（路径不同），用文本相似度判断对错很难。\n    *   *创新点：* 提出 **Action Metamorphic Relations（动作变形关系）**。核心逻辑是：只要输入的语义变化不改变任务目标，那么最终的**系统状态**必须一致。例如，指令从“订机票”变为“我要飞去...”，只要最终订票状态一致，就算通过。这解决了“非标准输入”的验证难题。\n\n*   **针对“容错性”的解法：引入“混沌工程”。**\n    *   *传统困境：* 静态数据集无法模拟动态故障。\n    *   *创新点：* 借鉴 Netflix 的 Chaos Monkey，提出 **Chaos Engineering for Agents**。不再等待故障发生，而是主动在工具调用层注入故障（如超时、限流、Schema 漂移）。这模拟了真实生产环境的“压力测试”。\n\n### 4. 统一框架：从点到面的升维\n有了具体的测试手段（变形关系、故障注入），作者需要一个数学框架来统一这些指标。\n*   **逻辑推演：** 既然可靠性有三个维度（k, ε, λ），那么评估结果就不应该是一个单一的分数，而应该是一个“函数”或“曲面”。\n*   **产出：** 提出了 **Reliability Surface $R(k, \\epsilon, \\lambda)$**。这个三维曲面能够直观地展示 Agent 在不同压力组合下的表现。例如，它能回答“当故障率增加时，Agent 对用户指令改写的敏感度是如何变化的？”这一复杂问题。\n\n### 5. 实证与反思：复杂度的悖论\n最后，作者通过实验验证假设，并得出了反直觉的结论，完善了整个思考闭环。\n*   **假设：** 更复杂的架构（如 Reflexion，带有自我反思机制）应该更可靠。\n*   **实验发现：** 在压力条件下，简单的 ReAct 架构反而表现更好。\n*   **逻辑修正：** 作者意识到，复杂的反思机制在遇到故障或干扰时，可能会引入更多的错误传播或无效循环，反而降低了稳定性。这进一步强化了论文的核心观点：**生产环境下的可靠性不等于模型能力的堆砌，而是对压力的稳健性。**\n\n---\n\n**总结：**\n作者的思考路径是从**“评估指标的失效”**出发，通过**“解构生产环境挑战”**定义了三个核心维度，进而**“跨界融合”**了软件测试的变形思想和 SRE 的混沌工程思想，最终构建了一个**“多维度的可靠性曲面”**框架，并揭示了**“简单架构在压力下的优势”**。整个过程体现了从现象观察、理论抽象到方法创新、实证修正的完整学术逻辑。"
                },
                {
                    "title": "LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
                    "arxiv_id": "2601.06111",
                    "authors": "Aayush Gupta, Farahan Raza Sheikh",
                    "summary": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis. We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于LLM的社会数字孪生框架，其中LLM作为个体智能体的认知引擎来模拟人口行为。这属于LLM智能体中的多智能体模拟范畴。尽管使用了疫情响应作为案例研究，但其核心贡献在于构建智能体框架的方法论，而非单纯的医疗领域应用。",
                    "summary2": "本文旨在解决预测人口对政策干预反应的挑战。针对政策响应预测中传统模型缺乏机制可解释性的问题，我们提出了一种基于LLM的Social Digital Twins框架，利用LLM作为Agent的认知引擎生成多维行为概率，并通过校准层映射到观测数据。在COVID-19大流行响应数据集上，通过RMSE指标验证了其有效性，相比Gradient Boosting基线，宏观平均预测误差降低了20.7%。",
                    "summary_translation": "预测人群如何响应政策干预是计算社会科学和公共政策领域的一个根本性挑战。传统方法依赖于聚合统计模型，这些模型虽然能够捕捉历史相关性，但缺乏机制可解释性，且难以应对新颖的政策场景。我们提出了一个构建社会数字孪生的通用框架——即虚拟人口副本，其中大语言模型作为个体智能体的认知引擎。每个智能体由人口统计学和心理特征学属性表征，接收政策信号并输出多维行为概率向量。一个校准层将聚合的智能体响应映射到可观测的群体层面指标，从而能够利用真实世界数据进行验证，并用于反事实政策分析。我们在大流行应对领域实例化了该框架，以拥有丰富观测数据的COVID-19作为案例研究。在保留测试期内，我们校准后的数字孪生在六个行为类别上，相较于梯度提升基线，在宏平均预测误差上实现了20.7%的改进。反事实实验展示了针对政策变化的单调且有界的响应，确立了行为的合理性。该框架是领域无关的：同样的架构适用于交通政策、经济干预、环境法规，或任何政策影响人群行为的场景。我们讨论了该框架对政策模拟的影响、方法的局限性，以及将基于大语言模型的数字孪生扩展到大流行应对之外的方向。",
                    "inspiration_trace": "基于论文《LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 第一阶段：宏观问题识别与现有方法的痛点分析\n**思考起点：** 政策制定者面临的核心困境——如何预测人群对未实施政策的反应？\n**逻辑推演：**\n1.  **观察现实需求：** 政府在推行碳税、封锁或福利改革前，需要预判公众行为（如：人们会减少开车吗？会遵守居家令吗？）。\n2.  **审视现有工具箱：**\n    *   **传统统计模型（如回归、时间序列）：** 擅长捕捉历史数据中的相关性，但本质是“黑箱”或“相关性归纳”。当遇到从未发生过的新颖政策（Novel Policy Scenarios）时，模型无法外推，且无法解释“为什么”会发生（缺乏机制可解释性）。\n    *   **传统基于主体的模型（ABM）：** 具备机制可解释性，能模拟个体决策。但其致命弱点在于“知识瓶颈”——必须由专家手动编写决策规则。如果人类自身都不完全理解某种复杂行为，就无法编写规则，导致模型难以泛化。\n3.  **核心矛盾提炼：** 我们需要一种既能像ABM那样具备**个体层面的机制推理能力**，又能像统计模型那样**易于构建且适应复杂场景**的新范式。\n\n### 第二阶段：技术机遇捕捉与核心假设提出\n**思考转折：** 大语言模型（LLM）的涌现能力是否提供了破局的关键？\n**逻辑推演：**\n1.  **观察LLM特性：** LLM不仅是在生成文本，它们在海量人类语料上训练，实际上习得了隐性的“人类推理模型”、“偏好”和“决策模式”（即“硅基采样” Silicon Sampling）。\n2.  **提出核心假设：** 如果LLM能模拟调查问卷回答、参与经济博弈，那么它本质上是一个通用的**人类行为模拟器**。\n3.  **范式转换构想：** 用LLM替换ABM中手工编写的规则引擎。\n    *   **输入：** 给LLM设定一个人设（年龄、职业、价值观）和一个政策背景。\n    *   **输出：** 让LLM基于其“常识”推理出该人设的行为概率。\n    *   **优势：** 无需针对每个领域硬编码规则，利用LLM的泛化能力处理新颖政策。\n\n### 第三阶段：框架构建——从“直觉”到“科学”\n**思考深化：** 仅靠LLM生成文本是不够的，如何将其转化为严谨的科学预测工具？\n**逻辑推演：**\n1.  **定义架构：** 提出“社会数字孪生”概念。这不仅是调用API，而是一个包含四个组件的系统：\n    *   **代理人口：** 必须构建符合真实人口统计学分布的合成人设，以保证群体的异质性。\n    *   **LLM认知引擎：** 负责将“人设+政策”映射为“行为概率向量”。\n2.  **解决“幻觉”与“对齐”问题（关键创新点）：**\n    *   **观察：** LLM输出的概率（如0.7）往往是主观的，不能直接对应现实世界的宏观指标（如客流量百分比）。\n    *   **引入校准层：** 必须建立一个数学映射层 $f(p; \\theta)$，将LLM输出的原始概率校准为可观测的现实指标。这相当于用历史数据去“锚定”LLM的直觉，使其具备预测精度。\n3.  **确立验证逻辑：** 强调严格的时空分割，防止信息泄露，确保模型是在真正“预测”而非“记忆”。\n\n### 第四阶段：实证策略与案例选择\n**思考落地：** 如何证明这个框架真的有效？\n**逻辑推演：**\n1.  **选择测试场：** 为什么选COVID-19？\n    *   **数据丰富度：** 有高频的谷歌移动数据和牛津政策追踪数据。\n    *   **自然实验属性：** 疫情期间政策变化剧烈且频繁，是测试模型应对“新颖/极端场景”的完美压力测试。\n    *   **行为多维性：** 涵盖工作、休闲、购物等多种行为，能全面测试模型。\n2.  **设定对比基线：** 与梯度提升树（GBM）等强统计模型对比。目的是验证：在处理“语义理解”和“决策逻辑”时，LLM是否优于纯数据驱动的统计模型。\n\n### 第五阶段：结果反思与定位修正\n**思考升华：** 实验结果揭示了什么？该方法论的边界在哪里？\n**逻辑推演：**\n1.  **结果分析：**\n    *   **成功之处：** 在工作场所、零售等“决策驱动型”行为上，LLM大幅超越统计模型。这证明了LLM理解政策语义（如“封锁”意味着“居家”）的能力。\n    *   **失败之处：** 在居住等“惯性驱动型”行为上，LLM不如统计模型。这说明LLM缺乏对日常习惯和惯性的记忆。\n2.  **方法论定位：**\n    *   明确该框架不是要取代所有统计模型，而是填补**“政策语义理解”与“机制推理”**的空白。\n    *   强调其**领域无关性**：COVID-19只是验证数据集，同样的架构可以无缝迁移到交通、经济、环保等领域，因为LLM已经学习了跨领域的人类行为逻辑。\n\n---\n\n**总结：**\n作者的思考路径是从**政策预测的现实困境**出发，敏锐地捕捉到**LLM作为通用认知引擎**的潜力，通过引入**校准层**解决了从“文本生成”到“科学预测”的跨越，最后通过**疫情案例**验证了其在处理复杂决策行为上的优越性，从而确立了一套通用的社会数字孪生方法论。"
                },
                {
                    "title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning",
                    "arxiv_id": "2601.06098",
                    "authors": "Nicholas X. Wang, Neel V. Parpia, Aaryan D. Parikh, Aggelos K. Katsaggelos",
                    "summary": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一种结合因果图与CoT的多智能体LLM架构，通过专用智能体（寻路、推理、验证、输出）之间的协作来减少幻觉，符合“多智能体：协作”的研究范围。",
                    "summary2": "本文旨在解决LLM在自动问题生成中的幻觉问题，以支持直觉学习。针对STEM教育场景，我们提出了一种结合Causal Graph引导的Chain-of-Thought推理与Multi-agent LLM架构的框架，并在Stellar在线学习平台上通过Flesch-Kincaid Grade Level、Key Points和Solution Quality等指标验证了其有效性，结果显示质量提升高达70%。",
                    "summary_translation": "直觉学习对于培养深层概念理解至关重要，尤其是在 STEM（科学、技术、工程和数学）教育领域，学生往往难以掌握抽象且相互关联的概念。自动问题生成已成为实现个性化学习和自适应学习的有效策略。然而，其有效性受到大语言模型中“幻觉”现象的制约，这可能导致生成事实错误、语义模糊或教学不一致的问题。为解决这一问题，我们提出了一种新颖的框架，该框架结合了因果图引导的思维链推理与多智能体 LLM 架构。该方法确保生成准确、有意义且符合课程要求的问题。因果图提供了领域知识的显式表示，而 CoT 推理则促进了对相关概念的结构化、逐步遍历。专用的 LLM 智能体被分配了图路径查找、推理、验证和输出等特定任务，所有任务均在领域约束范围内执行。一种在概念阶段和输出阶段实施的双重验证机制，极大地减少了幻觉现象。实验结果表明，与基准方法相比，该方法在质量上提升了高达 70%，并在主观评估中取得了极为理想的结果。",
                    "inspiration_trace": "基于论文《Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观愿景与教育痛点\n**（从“直觉学习”的理想出发）**\n\n1.  **观察现状**：在STEM教育中，传统的死记硬背已不足以应对抽象概念的学习，教育界正转向“直觉学习”——即通过自然认知过程、探索和逐步推理来建立深层理解。\n2.  **技术机遇**：生成式AI（特别是LLM）为实现个性化、自适应的“直觉学习”提供了可能，其中“自动问题生成（AQG）”是核心抓手，它能实时提供符合学生水平的挑战。\n3.  **核心矛盾**：虽然LLM具备强大的生成能力，但在教育场景下存在致命缺陷——**“幻觉”**。LLM会生成事实错误、逻辑不清或不符合教学大纲的问题，这会误导学生，破坏学习体验，违背了直觉学习追求“概念清晰”的初衷。\n\n### 第二阶段：问题诊断与归因\n**（深入分析LLM在教育场景失效的本质）**\n\n1.  **归因分析**：为什么LLM会产生幻觉？因为LLM本质上是基于概率预测的文本生成器，缺乏对领域知识**显性结构**的约束。它不知道概念A必须是概念B的前提（例如：不知道“牛顿第二定律”是推导“能量守恒”的基础）。\n2.  **需求明确**：要解决这一问题，不能仅靠微调模型，必须引入一种机制，能够：\n    *   显式表示知识的依赖关系（结构）。\n    *   强制生成过程遵循逻辑步骤（推理）。\n\n### 第三阶段：理论假设与融合\n**（提出“因果图 + 思维链”的结合点）**\n\n1.  **引入“因果图”**：作者意识到，因果图能完美映射学科中的概念依赖（如：力 $\\rightarrow$ 加速度 $\\rightarrow$ 速度）。它提供了**“是什么”**和**“什么顺序”**的知识骨架，解决了结构缺失问题。\n2.  **引入“思维链”**：CoT推理能模拟人类解决问题的逐步思考过程。它提供了**“如何”**连接这些概念的逻辑流。\n3.  **核心假设**：如果将因果图作为“导航地图”，将CoT作为“行驶路径”，让LLM沿着因果图的路径进行CoT推理，就能生成既符合学科逻辑又具备教学深度的题目。\n\n### 第四阶段：方法论构建与抗噪设计\n**（从理论假设落地为可执行的系统架构）**\n\n1.  **架构设计：多智能体协作**：单一的Prompt难以同时处理图遍历、逻辑推理和文本生成。作者受软件工程启发，决定采用**多智能体架构**，将复杂任务拆解：\n    *   *寻路智能体*：负责在因果图中找到正确的概念路径。\n    *   *推理智能体*：负责基于路径生成CoT。\n    *   *生成与输出智能体*：负责最终题目的产出。\n2.  **抗噪机制：双重验证**：为了专门针对第二阶段发现的“幻觉”问题，作者设计了**双重验证**机制：\n    *   *概念层验证*：在生成前，检查寻路智能体找到的路径是否逻辑自洽。\n    *   *输出层验证*：在生成后，检查最终题目是否准确、无歧义。\n    *   *逻辑闭环*：通过这两道“安检”，确保输出严格受限于因果图的结构约束。\n\n### 第五阶段：验证与价值确认\n**（通过实验反馈闭环验证思想）**\n\n1.  **评估维度设定**：为了证明该方法优于普通LLM（如ChatGPT），作者设定了不仅关注“可读性”，更关注“关键点覆盖”和“解题步骤质量”的指标。这直接呼应了第一阶段“直觉学习”对深度理解的要求。\n2.  **结果反馈**：实验显示，该方法在题目深度和逻辑性上显著优于基线模型（提升70%），且用户反馈题目“自然”、“符合推理习惯”。\n3.  **结论升华**：这证明了**结构化知识（因果图）与结构化推理（CoT）的结合**，是解决教育领域LLM幻觉问题的有效范式。\n\n---\n\n**总结：作者的思考路径是从教育理念（直觉学习）出发，遭遇技术瓶颈（LLM幻觉），通过引入外部结构（因果图）和内部逻辑（CoT）进行约束，最终通过工程化手段（多智能体+双重验证）将理论落地，从而实现了高质量的教育内容生成。**"
                },
                {
                    "title": "Dynamic Intelligence Ceilings: Measuring Long-Horizon Limits of Planning and Creativity in Artificial Systems",
                    "arxiv_id": "2601.06102",
                    "authors": "Truong Xuan Khanh, Truong Quynh Hoa",
                    "summary": "Recent advances in artificial intelligence have produced systems capable of remarkable performance across a wide range of tasks. These gains, however, are increasingly accompanied by concerns regarding long-horizon developmental behavior, as many systems converge toward repetitive solution patterns rather than sustained growth. We argue that a central limitation of contemporary AI systems lies not in capability per se, but in the premature fixation of their performance frontier. To address this issue, we introduce the concept of a \\emph{Dynamic Intelligence Ceiling} (DIC), defined as the highest level of effective intelligence attainable by a system at a given time under its current resources, internal intent, and structural configuration. To make this notion empirically tractable, we propose a trajectory-centric evaluation framework that measures intelligence as a moving frontier rather than a static snapshot. We operationalize DIC using two estimators: the \\emph{Progressive Difficulty Ceiling} (PDC), which captures the maximal reliably solvable difficulty under constrained resources, and the \\emph{Ceiling Drift Rate} (CDR), which quantifies the temporal evolution of this frontier. These estimators are instantiated through a procedurally generated benchmark that jointly evaluates long-horizon planning and structural creativity within a single controlled environment. Our results reveal a qualitative distinction between systems that deepen exploitation within a fixed solution manifold and those that sustain frontier expansion over time. Importantly, our framework does not posit unbounded intelligence, but reframes limits as dynamic and trajectory-dependent rather than static and prematurely fixed. \\vspace{0.5em} \\noindent\\textbf{Keywords:} AI evaluation, planning and creativity, developmental intelligence, dynamic intelligence ceilings, complex adaptive systems",
                    "category": "cs.AI",
                    "filter_reason": "论文关注长视界规划和结构创造力，这是单智能体LLM智能体的核心能力之一。论文提出的评估框架旨在测量智能体在复杂环境中的持续发展能力和规划极限，符合研究范围中的“单智能体：规划”及“自我演化”相关内容，且不属于纯推理、纯应用或基础设施优化等排除类别。",
                    "summary2": "本文旨在解决AI系统性能上限过早固定的问题。针对长视界规划与结构创造力评估场景，我们提出了一种Dynamic Intelligence Ceiling (DIC) 概念及轨迹中心评估框架，并在程序化生成的Workshop World环境上通过Progressive Difficulty Ceiling (PDC) 和 Ceiling Drift Rate (CDR) 验证了其有效性。",
                    "summary_translation": "人工智能的最新进展催生了在广泛任务范围内表现出卓越性能的系统。然而，这些成果日益伴随着对长视距发展行为的担忧，因为许多系统倾向于收敛于重复的解决方案模式，而非实现持续增长。我们认为，当代人工智能系统的一个核心局限不在于其能力本身，而在于其性能前沿的过早固化。为解决这一问题，我们引入了动态智能上限的概念，其定义为系统在当前资源、内部意图和结构配置下，在特定时刻所能达到的有效智能最高水平。为使这一概念在经验上可操作，我们提出了一种以轨迹为中心的评估框架，该框架将智能视为一个移动的前沿而非静态快照进行测量。我们通过两个估计器对 DIC 进行操作化定义：渐进难度上限，用于捕捉在资源约束下可可靠解决的最大难度；以及上限漂移率，用于量化该前沿的时间演化。这些估计器通过一个程序化生成的基准测试进行实例化，该基准在单一受控环境中联合评估长视距规划和结构创造力。我们的结果表明，在固定解流形内深化利用的系统与那些随时间维持前沿扩展的系统之间存在定性差异。重要的是，我们的框架并未假设无界智能，而是将限制重新界定为动态且依赖于轨迹的，而非静态且过早固化的。\n\n**关键词：** AI evaluation (AI评估), planning and creativity (规划与创造力), developmental intelligence (发展智能), dynamic intelligence ceilings (动态智能上限), complex adaptive systems (复杂适应系统)",
                    "inspiration_trace": "基于论文《Dynamic Intelligence Ceilings: Measuring Long-Horizon Limits of Planning and Creativity in Artificial Systems》，以下是对作者产出该文章核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察与问题提出：能力的悖论\n**思考起点：** 作者首先观察到当前AI领域的一个核心矛盾——尽管AI系统在各类具体任务上的性能指标不断提升，但在长周期的发展行为中，系统往往表现出一种“早熟”的停滞倾向。\n**核心质疑：** 短期任务能力的提升，是否等同于智能的持续增长？许多系统在达到一定水平后，开始收敛于重复的解题模式，而非探索新的可能性。这引发了一个宏观问题：**我们是否在用错误的标尺（静态性能）衡量一个动态的过程（智能发展）？**\n\n### 2. 现状批判与假设形成：静态评估的局限\n**逻辑推演：** 既然现有的评估体系（基于固定数据集的快照式测试）无法解释上述停滞现象，作者推断问题出在评估范式本身。\n**假设提出：** 现有的AI系统之所以表现出“早熟”，并非因为其物理资源达到了绝对极限，而是因为其**性能边界被过早固定**了。作者提出一个关键假设：智能的上限不应是一个静态的常数，而是一个随时间、内部结构和资源状态变化的动态函数。如果系统无法改变其内部结构或目标，它就会陷入“静态上限”；反之，则能实现“动态上限”。\n\n### 3. 概念重构：从“能力”到“边界动力学”\n**理论深化：** 为了验证上述假设，作者需要将抽象的“智能发展”转化为可定义的概念。\n**核心概念引入：** 作者提出了**动态智能上限**。这一概念不再关注“系统现在能做什么”，而是关注“系统的极限是如何形成和演变的”。\n**关键区分：** 作者区分了两种发展模式：\n*   **深度挖掘：** 在固定的解题流形内提高效率（静态上限）。\n*   **边界扩展：** 通过结构重组，解决以前无法解决的问题（动态上限）。\n**思考焦点：** 作者意识到，真正的智能在于维持边界的可移动性，而非在固定边界内追求极致效率。\n\n### 4. 操作化挑战：如何捕捉“移动的边界”？\n**方法论转化：** 概念确立后，面临的挑战是如何在实验中观测到“边界的移动”。作者意识到这需要两个维度的结合：\n1.  **长视界规划：** 测试系统解决复杂、多步骤问题的能力。\n2.  **结构性创造力：** 确保系统不是在死记硬背，而是在生成新的解决方案结构。\n**逻辑闭环：** 如果一个系统在难度增加时仍能保持成功，且其解决方案具有结构上的新颖性，才能证明其智能边界在向外扩展，而非在原地打转。\n\n### 5. 方法论设计：构建“诊断性”实验\n**具体化路径：** 为了实现上述观测，作者设计了“Workshop World”基准环境。\n*   **环境设计逻辑：** 摒弃静态数据集，采用程序化生成。通过控制难度向量（视界长度、约束数量、模块数、模糊度），确保问题无法通过记忆或简单模式匹配解决，强制系统进行规划和结构组合。\n*   **指标定义逻辑：**\n    *   **渐进难度上限（PDC）：** 用于定位当前边界在哪里（最高可解难度）。\n    *   **上限漂移率（CDR）：** 用于测量边界随时间的移动速度。\n    *   **结构新颖性：** 用于排除“伪进步”（即通过重复使用旧策略来提高效率）。\n\n### 6. 逻辑验证与结论：区分“伪成长”与“真发展”\n**最终推演：** 通过这套框架，作者旨在揭示一种被忽视的失败模式——**过早收敛**。\n**结论产出：** 作者论证了，只有当PDC随时间推移（CDR > 0）且伴随着高结构新颖性时，才是真正的智能发展。如果PDC不变，仅效率提升，那只是陷入了静态上限。文章最终将智能重新定义为一种**能够延迟边界固定的持续适应能力**，而非某一时刻的性能快照。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（AI性能提升但发展停滞）**出发，通过**批判现有评估工具（静态快照的不足）**，提出**新理论视角（动态智能上限）**，进而设计**针对性的实验环境（Workshop World）**和**量化指标（PDC, CDR）**，最终实现对AI系统长期发展潜力的科学诊断。"
                },
                {
                    "title": "Large Language Models for Physics Instrument Design",
                    "arxiv_id": "2601.07580",
                    "authors": "Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra",
                    "summary": "We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle--matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced.",
                    "category": "cs.AI",
                    "filter_reason": "论文探讨了LLM作为“元规划者”的角色，涉及规划、编排和协调多个组件（如RL优化器）的能力，符合单智能体中关于“规划”和“工具使用/协调”的研究范围，尽管应用场景是物理仪器设计，但其核心贡献在于LLM在复杂工作流中的智能体能力。",
                    "summary2": "本文旨在探索大语言模型在物理仪器设计中的应用潜力并与强化学习性能对比。针对量能器纵向分段和磁谱仪布局优化问题，我们提出了一种基于提示的LLM设计生成方法，并结合信任域优化器进行混合优化。在物理模拟器基准测试中，通过能量分辨率、动量分辨率和跟踪效率等指标验证了其有效性。",
                    "summary_translation": "我们研究了大型语言模型在物理仪器设计中的应用，并将其性能与强化学习进行了比较。仅通过提示，我们向大型语言模型提供任务约束和先前高分设计的摘要，使其提出完整的探测器配置，并利用基于强化学习的优化中所使用的相同模拟器和奖励函数对这些配置进行评估。尽管强化学习能够生成更优的最终设计方案，但我们发现，现代大型语言模型在没有进行针对特定任务训练的情况下，依然能够持续生成有效、具备资源意识且符合物理规律的配置；这得益于它们利用了关于探测器设计原理和粒子-物质相互作用的广泛预训练知识。基于这一结果，作为迈向混合设计工作流的第一步，我们探索将大型语言模型与专用的信任区域优化器相结合；这作为未来流程的先导，在该流程中，大型语言模型负责提出并构建设计假设，而强化学习则执行奖励驱动的优化。基于这些实验，我们认为大型语言模型非常适合作为元规划器：它们能够设计和编排基于强化学习的优化研究，定义搜索策略，并在统一的工作流中协调多个相互作用的组件。通过这种方式，它们指向了自动化的闭环仪器设计，在这种设计中，构建和监督优化所需的大量人工投入得以减少。",
                    "inspiration_trace": "基于论文《Large Language Models for Physics Instrument Design》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 第一阶段：宏观背景与痛点识别\n**思考起点：** 现代物理仪器（如LHC、FCC）的设计极其复杂，成本高昂且周期漫长。\n**逻辑演进：**\n1.  **现状观察：** 尽管AI技术（如贝叶斯优化、深度生成模型）已被引入辅助设计，但面对复杂的结构决策和长程依赖，强化学习（RL）因其处理序列决策的能力而备受关注。\n2.  **发现瓶颈：** RL虽然擅长在给定规则下寻找最优解，但它本身是“盲目”的。RL需要人类专家预先定义好动作空间、奖励函数和约束条件。随着设计问题规模扩大，**“如何构建优化问题”本身成为了一个比“执行优化”更耗费人力的瓶颈**。\n3.  **核心问题：** 能否找到一种方法，不仅能执行优化，还能像人类专家一样进行“高层推理”和“问题构建”，从而减少人工干预？\n\n### 第二阶段：假设提出与视角转换\n**思考转折：** 既然RL缺乏先验知识且需要大量人工调参，那么拥有海量跨学科知识的大语言模型（LLM）能否填补这一空白？\n**逻辑演进：**\n1.  **引入LLM特性：** LLM展现出了零样本推理和涌现能力，它们在预训练中吸收了大量关于物理原理、工程设计和约束满足的知识。\n2.  **提出假设：** LLM不应仅仅被视为文本生成器，而应被视为**“元规划器”**。假设LLM能够仅通过自然语言提示，理解设计约束，并基于其内置的物理直觉生成合理的仪器设计方案，而无需针对特定任务进行训练。\n3.  **对比视角：** 将LLM定位为“基于知识的提案生成器”，与RL（基于奖励的搜索器）进行对比。前者靠“直觉”和“经验”，后者靠“试错”和“反馈”。\n\n### 第三阶段：方法论构建与可行性验证\n**思考落地：** 如何将LLM的“想法”转化为可执行的物理设计？LLM生成的文本往往不严谨，如何保证物理上的可行性？\n**逻辑演进：**\n1.  **设计交互闭环：** 构建一个“提示-提案-评估”的循环。LLM不直接接触模拟器，而是通过阅读设计规范和之前的高分设计案例来生成新的配置（以JSON格式输出）。\n2.  **引入“投影”机制：** 这是关键的工程化思考。作者意识到LLM可能会产生幻觉（如违反几何约束或预算限制）。因此，必须在评估前引入一个**确定性的“投影”步骤**，将LLM输出的原始JSON强制修正为满足所有硬约束的可行解。这保证了搜索过程的稳定性。\n3.  **混合优化构想：** 既然LLM擅长宏观结构（如层数、类型选择），但可能不擅长微调连续参数（如精确位置），而传统优化器（如信任域算法TR）擅长后者。于是提出了**LLM + TR**的混合模式：LLM负责“提出假设”，TR负责“局部打磨”。\n\n### 第四阶段：实验验证与结果分析\n**思考验证：** 这种基于LLM的设计方法真的有效吗？它和RL相比如何？\n**逻辑演进：**\n1.  **基准测试：** 选取两个经典问题（量热仪纵向分段、磁谱仪布局），使用与RL研究完全相同的模拟器和奖励函数，确保公平比较。\n2.  **观察现象：** 实验发现，LLM在没有任何任务特定训练的情况下，能迅速生成物理上合理且资源有效的配置。虽然最终性能略逊于经过充分训练的RL，但其收敛速度极快（早期迭代即表现优异）。\n3.  **验证混合模式：** 加入TR优化器后，设计性能得到进一步提升，证明了LLM提供的“初始解”质量很高，为局部优化提供了良好的起点。\n\n### 第五阶段：理论升华与未来展望\n**思考总结：** 从单纯的“设计工具”上升到“设计流程的自动化管理者”。\n**逻辑演进：**\n1.  **重新定义角色：** LLM不应被视为RL的替代品，而是**互补者**。RL是底层的“执行者”，负责榨取性能；LLM是高层的“指挥官”，负责提供先验知识、构建搜索空间和协调流程。\n2.  **提出“元规划器”愿景：** 最终的思考指向了全自动化的闭环设计。在这个愿景中，LLM负责定义问题、分解任务、协调多个子系统（如让不同的RL代理分别优化追踪器和量热仪），从而将人类从繁琐的优化流程构建中解放出来。\n\n---\n\n**总结：**\n作者的思考路径是从**“物理设计的复杂性”**出发，意识到**“RL的局限性在于缺乏先验和依赖人工构建”**，进而利用**“LLM的通用知识与推理能力”**作为补充，通过**“投影机制”**解决LLM的不严谨问题，最终构建出**“LLM提案+局部优化”的混合范式**，并将LLM的角色升维为**“自动化设计流程的元规划器”**。"
                },
                {
                    "title": "A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems",
                    "arxiv_id": "2601.07136",
                    "authors": "Daniel Liu, Krishna Upadhyay, Vinaik Chhetri, A. B. Siddique, Umar Farooq",
                    "summary": "The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.",
                    "category": "cs.AI",
                    "filter_reason": "该论文明确针对多智能体AI系统（MAS）进行大规模实证研究，分析了其开发、维护及演化过程。摘要中特别指出了“智能体协调挑战”是主要关注点之一，这直接符合筛选条件中“多智能体：协作、通信”的研究范围。虽然论文涉及基础设施问题，但其核心是对智能体系统本身的生态分析，而非单纯的基础设施优化或特定领域应用。",
                    "summary2": "本文旨在揭示多智能体AI系统的开发与维护模式。针对8个主流开源MAS项目，我们提出了一种基于软件仓库挖掘的大规模实证分析方法，并在GitHub数据集上通过分析42K次提交和4.7K个issue验证了其有效性。",
                    "summary_translation": "多智能体AI系统（Multi-Agent AI Systems, MAS）的迅速涌现，包括 LangChain、CrewAI 和 AutoGen，已经重塑了大语言模型（Large Language Model, LLM）应用的开发与编排方式。然而，关于这些系统在实践中如何演化及维护，目前尚缺乏深入了解。本文针对开源 MAS 进行了首次大规模实证研究，分析了八个领先系统中超过 42,000 个独特的提交和超过 4,700 个已解决的议题。我们的分析识别出三种截然不同的开发模式：持续型、稳定型和突发驱动型。这些模式反映了生态系统成熟度存在显著差异。完善性提交占所有变更的 40.8%，这表明相较于纠正性维护（27.4%）和适应性更新（24.3），功能增强被置于优先地位。议题数据显示，最常见的问题涉及缺陷（22%）、基础设施（14%）以及智能体协调挑战（10%）。自 2023 年起，所有框架的议题报告数量均急剧增加。议题解决的中位时间从不足一天到大约两周不等，其分布偏向于快速响应，但仍有少数议题需要长期的关注。这些结果凸显了当前生态系统的活力与脆弱性，并强调了改进测试基础设施、文档质量及维护实践的必要性，以确保长期的可靠性和可持续性。",
                    "inspiration_trace": "基于这篇论文的内容，我为您还原了作者从宏观观察到产出核心方法的逻辑演进过程：\n\n### 第一阶段：现象观察与宏观提问\n**逻辑起点：技术繁荣背后的工程黑盒**\n*   **观察现象**：随着大语言模型（LLM）的爆发，多智能体系统（MAS）框架（如LangChain, AutoGen, CrewAI）迅速崛起，成为构建复杂AI应用的主流范式。\n*   **提出问题**：学术界和工业界目前主要关注这些系统的“算法能力”和“架构设计”（即智能体如何协作），但忽略了它们作为“软件系统”本身的属性。\n*   **核心困惑**：这些框架在开源社区中是如何实际演进的？它们的开发模式、维护挑战与传统软件有何不同？是否存在某种独特的工程规律？\n\n### 第二阶段：差距识别与假设提出\n**逻辑聚焦：从“算法视角”转向“工程视角”**\n*   **文献回顾**：发现现有研究多集中在智能体的通信协议、规划机制或基准测试上，属于“计算机科学（AI）”范畴。\n*   **识别差距**：缺乏“软件工程（SE）”视角的实证研究。即缺乏对代码提交、Issue处理、维护模式等大规模数据的量化分析。\n*   **提出假设**：MAS框架由于处于AI技术的前沿，其开发模式可能具有“高迭代性”、“强适应性”和“架构不稳定性”等特征，且不同框架可能处于不同的成熟阶段。\n\n### 第三阶段：方法论构建\n**逻辑落地：利用软件仓库挖掘（MSR）进行实证研究**\n*   **选择工具**：决定采用经典的“软件仓库挖掘”方法，将GitHub上的开源MAS项目视为研究对象，通过客观数据验证假设。\n*   **数据定义**：\n    *   **研究对象**：筛选8个具有代表性的、高Star数的开源MAS项目（涵盖不同架构，如对话式、基于角色、基于图等）。\n    *   **数据维度**：选取“提交记录”作为开发活动的代理指标，选取“Issue记录”作为维护和问题解决的代理指标。\n*   **分析框架设计**：将研究拆解为两个核心维度（对应两个RQ）：\n    1.  **开发模式（RQ1）**：关注提交频率、代码变动量、提交意图（是修Bug、加功能还是适配环境）。\n    2.  **维护生态（RQ2）**：关注Issue的类型（是逻辑错误、基础设施问题还是协调问题）、报告趋势及解决效率。\n\n### 第四阶段：实证分析与逻辑验证\n**逻辑展开：从数据中提炼模式与特征**\n*   **对开发模式的验证（RQ1）**：\n    *   *观察*：通过时间序列分析，发现并非所有项目都线性增长。\n    *   *归纳*：提炼出三种开发画像——“持续型”（如Haystack，长期稳定）、“稳定型”（如Semantic Kernel，波动适中）和“突发型”（如SuperAGI，短期爆发后沉寂）。\n    *   *深入*：利用NLP模型分类提交信息，发现“完善性提交”（Perfective，即功能增强）占比最高（40.8%），证实了“快速迭代、功能优先”的假设。\n*   **对维护生态的验证（RQ2）**：\n    *   *观察*：Issue数量在2023年后激增，与LLM爆发时间点吻合。\n    *   *分类*：通过标签分析和主题建模，发现除了常规的“Bug”，基础设施（14%）和智能体协调（10%）是特有痛点。\n    *   *评估*：分析解决时间，发现响应速度差异巨大，揭示了生态系统的“脆弱性”。\n\n### 第五阶段：结论升华\n**逻辑闭环：揭示生态系统的“动量”与“脆弱”**\n*   **综合洞察**：MAS生态系统正处于从“野蛮生长”向“成熟稳定”过渡的阶段。虽然开发活跃（动量），但严重依赖基础设施，且缺乏统一的测试和文档标准（脆弱）。\n*   **最终产出**：不仅提供了第一份关于MAS工程实践的大规模实证报告，更重要的是指出了未来方向——需要从单纯的算法创新转向更扎实的软件工程基础设施建设（如测试、文档、维护规范）。\n\n---\n\n**总结**：\n作者的思考路径遵循了典型的**实证研究逻辑**：\n**观察技术热点（AI Agent） -> 发现研究盲区（缺乏工程视角） -> 引入成熟方法论（软件仓库挖掘） -> 量化分析数据（提交与Issue） -> 归纳行业特征（三种画像、特有痛点） -> 提出工程建议**。"
                },
                {
                    "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
                    "arxiv_id": "2601.07122",
                    "authors": "Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu",
                    "summary": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个名为CyberOps-Bots的LLM赋能的多智能体强化学习框架，符合“多智能体”（LLM智能体与RL智能体协作）和“单智能体”（包含ReAct规划、记忆、工具使用模块）的研究范围。尽管应用场景为云网络防御，但论文核心贡献在于智能体的架构设计与机制，而非纯应用或AI安全/对齐研究。",
                    "summary2": "本文旨在解决云网络防御在动态环境下的适应性和鲁棒性问题。针对云网络结构、规模及攻击策略动态变化的场景，我们提出了一种名为CyberOps-Bots的分层多智能体强化学习框架，该框架结合了LLM的高层战术规划与底层RL智能体的原子动作执行。在AWS企业云数据集和Yawning Titan仿真环境中，通过网络可用性和Jumpstart性能等指标验证了其有效性，实现了无需重训练的高效自适应防御。",
                    "summary_translation": "虽然虚拟化和资源池化为云网络赋予了结构灵活性和弹性可扩展性，但它们不可避免地扩大了攻击面，并挑战了网络弹性。基于强化学习的防御策略已被开发出来，用于在对抗条件下优化资源部署和隔离策略，旨在通过维持和恢复网络可用性来增强系统弹性。然而，现有方法缺乏鲁棒性，因为它们需要重新训练以适应网络结构、节点规模、攻击策略和攻击强度的动态变化。此外，缺乏人在回路支持限制了可解释性和灵活性。为了解决这些局限性，我们提出了 CyberOps-Bots，这是一个由大语言模型赋能的分层多智能体强化学习框架。受 MITRE ATT&CK 的战术-技术模型启发，CyberOps-Bots 具有双层架构：(1) 上层 LLM 智能体包含四个模块——ReAct 规划、基于 IPDRR 的感知、长短期记忆以及动作/工具集成——负责执行全局感知、人类意图识别和战术规划；(2) 下层 RL 智能体通过异构分离预训练开发，在局部网络区域内执行原子防御动作。这种协同作用在确保可靠的 RL 执行的同时，保留了 LLM 的适应性和可解释性。在真实云数据集上的实验表明，与最先进的算法相比，CyberOps-Bots 在不重新训练的情况下切换场景时，维持的网络可用性高出 68.5%，并实现了 34.7% 的启动性能增益。据我们所知，这是首个建立具有 HITL 支持的鲁棒 LLM-RL 框架用于云防御的研究。我们将向社区发布我们的框架，以促进云网络中鲁棒且自主防御的发展。",
                    "inspiration_trace": "基于论文《Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题与现状悖论\n**1. 观察现象：云原生环境的“双刃剑”特性**\n作者首先观察到云网络的核心矛盾：虚拟化和弹性伸缩赋予了系统极大的灵活性，但同时也导致了网络拓扑、节点规模和攻击面的高度动态变化。\n*   **思考：** 传统的静态防御策略无法应对这种瞬息万变的环境。\n\n**2. 现有方案的尝试与局限**\n学术界和工业界开始尝试使用强化学习（RL）来自动化防御决策，因为RL擅长通过交互学习最优策略。\n*   **深入分析：** 作者发现现有的RL方法存在致命的“脆弱性”。当网络结构（A1）、规模（A2）、攻击策略（A3）或攻击强度（A4）发生变化时，RL模型往往失效，必须重新训练。\n*   **根本原因定位：**\n    *   **表征僵化：** RL依赖固定维度的状态向量（如邻接矩阵），一旦网络规模或拓扑改变，输入层结构就不匹配了。\n    *   **泛化缺失：** RL是基于模式匹配的，而非语义理解。面对未见过的攻击阶段或并发攻击，它无法举一反三。\n    *   **人机断层：** 纯算法模型缺乏可解释性，无法支持安全专家在紧急情况下进行有效干预（HITL）。\n\n---\n\n### 第二阶段：范式转换与假设提出\n**3. 引入新视角：从“数值计算”转向“语义推理”**\n为了解决泛化性和人机交互问题，作者将目光投向了大语言模型（LLM）。\n*   **假设：** LLM具备强大的语义理解、逻辑推理和零样本泛化能力，能够理解复杂的网络态势和人类指令，从而弥补RL在高层认知上的不足。\n\n**4. 识别新技术的短板**\n然而，作者敏锐地意识到LLM并非万能：\n*   **短板：** LLM在精确的数值计算（如计算最短路径）和生成低层级的精确控制指令（如具体的流表修改命令）方面存在“幻觉”和不稳定性。\n*   **结论：** 单纯依靠LLM无法保证防御执行的可靠性。\n\n---\n\n### 第三阶段：方法论融合与架构设计\n**5. 核心思想：分层协同的“战术-技术”解耦**\n受MITRE ATT&CK框架（战术与技术的分层）启发，作者提出了一个融合假设：**将“大脑”（LLM）与“手脚”（RL）结合**。\n*   **逻辑推演：**\n    *   **上层（LLM）：** 负责宏观感知、战术规划和意图理解。利用自然语言处理能力，将动态的网络状态抽象为文本，从而解耦对特定网络结构的依赖。\n    *   **下层（RL）：** 负责微观执行。利用RL在特定动作空间内的精确控制能力，执行具体的原子防御操作。\n\n**6. 解决动态适应性的具体机制设计**\n针对前述的四个动态挑战（A1-A4），作者在架构中嵌入了对应的解决方案：\n\n*   **针对A1（结构变化）与A2（规模变化）：自然语言状态抽象**\n    *   *思考：* 如何让模型不关心网络具体有多少个节点？\n    *   *方案：* 设计一个感知模块，将高维、结构化的网络状态转化为自然语言描述。因为LLM处理文本不受长度限制，这天然解决了状态空间爆炸和维度不匹配的问题，实现了“零样本”适应新拓扑。\n\n*   **针对A3（攻击策略变化）：异构分离预训练**\n    *   *思考：* 如何应对不同类型的攻击（如DDoS vs 渗透）？\n    *   *方案：* 不训练一个全能的RL智能体，而是训练一组功能单一的“专家”RL智能体（如隔离专家、补丁专家）。LLM作为指挥官，根据当前的攻击语义，动态调度不同的专家组合。这比单一模型更具灵活性。\n\n*   **针对A4（攻击强度/并发性）：长短时记忆机制**\n    *   *思考：* 面对多阶段、并发的攻击链，如何保持连贯性？\n    *   *方案：* 赋予LLM记忆模块（LTM/STM）。通过存储和检索历史攻击链，LLM能够识别攻击意图的演变，从而进行长期的防御规划，而不是短视的反应。\n\n---\n\n### 第四阶段：增强可靠性与人机协同\n**7. 引入ReAct范式与HITL支持**\n为了解决LLM的“幻觉”问题并增强信任度：\n*   **ReAct（推理+行动）：** 强制LLM在输出行动前先生成推理链。这不仅提高了决策的准确性，还提供了天然的可解释性日志。\n*   **人在回路（HITL）：** 允许安全专家通过自然语言直接干预LLM的规划层。这使得系统不仅是自动化的，更是可审计、可修正的。\n\n---\n\n### 第五阶段：逻辑闭环与验证\n**8. 最终产出：CyberOps-Bots框架**\n作者将上述思考整合为一个三层架构：环境层（模拟动态对抗）、LLM层（语义规划）、RL层（原子执行）。\n\n**9. 验证逻辑：**\n*   **实验设计：** 不再测试静态环境，而是专门设计场景动态切换（如从30节点跳到450节点，攻击策略从侦察变为渗透）。\n*   **核心指标：** 关注“Jumpstart性能”（即在新环境下无需重新训练的初始表现）和“网络可用性”。\n*   **结论验证：** 实验证明，这种分层架构确实在无需重训的情况下，适应了A1-A4的所有动态变化，且性能优于传统RL算法。\n\n---\n\n**总结：**\n作者的思考路径是从**“RL在动态环境下的失效”**这一痛点出发，通过**引入LLM的语义泛化能力**作为破局点，进而通过**分层架构（LLM规划+RL执行）**规避了LLM的精确性短板，最终利用**自然语言抽象和异构智能体调度**实现了对云网络动态特性的鲁棒适应。"
                },
                {
                    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
                    "arxiv_id": "2601.06789",
                    "authors": "Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang",
                    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于增强代码智能体，核心贡献在于构建智能体的记忆机制，通过治理和检索人类历史经验来提升智能体性能，属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决Code Agents因“封闭世界”限制而无法有效利用GitHub历史经验的问题。针对GitHub上非结构化且碎片化的Issue和PR数据，我们提出了MemGovern框架，通过Experience Governance将原始数据转化为结构化的Experience Cards，并引入Agentic Experience Search机制实现逻辑驱动的检索。在SWE-bench Verified上通过Resolution Rate验证了其有效性，平均提升了4.65%。",
                    "summary_translation": "尽管 autonomous software engineering (SWE) agents（自主软件工程智能体）正在重塑编程范式，但目前它们仍受限于“closed-world”限制：即试图从零开始修复 bug 或仅依赖 local context（局部上下文），而忽视了 GitHub 等平台上蕴藏的丰富历史人类经验。获取这种 open-world experience（开放世界经验）的过程，受到现实世界中 issue-tracking data（问题跟踪数据）非结构化和碎片化特性的阻碍。在本文中，我们介绍了 MemGovern，这是一个旨在对原始 GitHub 数据进行治理，并将其转化为智能体可用的 actionable experiential memory（可操作经验记忆）的 framework（框架）。MemGovern 采用 experience governance（经验治理）将人类经验转化为 agent-friendly（智能体友好）的 experience cards（经验卡片），并引入了一种 agentic experience search strategy（智能体经验搜索策略），从而实现了对 human expertise（人类专业知识）的 logic-driven retrieval（逻辑驱动检索）。通过生成 135K 个治理后的 experience cards（经验卡片），MemGovern 实现了显著的 performance boost（性能提升），将 SWE-bench Verified 上的 resolution rates（解决率）提高了 4.65%。作为一种 plug-in approach（插件式方法），MemGovern 为构建 agent-friendly memory infrastructure（智能体友好的记忆基础设施）提供了有效的解决方案。",
                    "inspiration_trace": "基于对论文《MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察与问题定义：从“封闭”到“开放”的鸿沟\n\n*   **现象观察**：当前的自主软件工程代理在解决代码问题时，往往表现得像是一个“孤胆英雄”。它们倾向于从零开始尝试修复Bug，或者仅依赖当前代码库的局部上下文。\n*   **人类对标**：在现实世界的软件工程实践中，资深开发者很少从零开始。面对复杂问题，他们习惯于在GitHub等协作平台上搜索历史记录，借鉴前人解决类似问题的专家推理和修复模式。\n*   **核心假设**：如果代码代理能够像人类一样，利用GitHub上海量的“开放世界”历史经验，其推理深度和修复准确性应该能得到显著提升。\n*   **现实瓶颈**：虽然GitHub蕴含了巨大的知识宝库，但直接将其用于Agent存在巨大的“语义鸿沟”。原始的Issue和PR讨论充满了社交闲聊、非标准术语和碎片化信息，噪声极大且高度异构。直接检索这些数据会导致“记忆污染”，难以实现跨仓库的知识迁移。\n\n### 2. 思考转折：从“数据检索”到“数据治理”\n\n*   **思维突破**：既然原始数据不可用，那么问题的核心就不在于“如何更好地检索”，而在于“如何将混乱的人类经验转化为Agent友好的知识”。\n*   **治理理念**：作者意识到必须引入一个中间层，即“经验治理”。这不仅仅是清洗数据，而是要进行知识蒸馏。\n*   **结构化重构**：为了解决跨仓库的异构性问题，作者提出将非结构化的讨论重构为标准化的“经验卡片”。\n*   **关键洞察（解耦）**：为了实现有效的知识迁移，必须将“检索信号”与“修复逻辑”解耦。\n    *   **索引层**：提取通用的故障症状（如异常类型、错误签名），用于跨仓库的广泛匹配。\n    *   **解析层**：封装可复用的修复逻辑（如根因分析、修复策略），用于具体的代码生成。\n    *   *逻辑推演*：这种分层设计使得Agent能够基于症状找到相似案例，再根据抽象的修复策略应用到当前的具体上下文中，从而实现了从“形似”到“神似”的跨越。\n\n### 3. 交互设计：从“静态注入”到“智能搜索”\n\n*   **对现有方法的批判**：传统的检索增强生成（RAG）通常采用“一次性检索+上下文注入”的模式。这就像把整本教科书扔给学生，不仅消耗上下文窗口，还容易引入噪声，干扰Agent的推理。\n*   **人类行为模拟**：人类查阅资料时是动态的——先搜索目录，筛选出相关章节，再深入阅读细节。\n*   **机制创新**：作者提出了“Agent式经验搜索”。\n    *   **双原语接口**：设计了“搜索”和“浏览”两个工具。搜索用于广度发现（基于索引层），浏览用于深度挖掘（基于解析层）。\n    *   **渐进式推理**：允许Agent根据当前解决问题的状态，自主决定是扩大搜索范围还是深入某个具体案例。这种机制让Agent具备了主动筛选和验证信息的能力，避免了被动接受噪声。\n\n### 4. 逻辑闭环与验证：质量即性能\n\n*   **质量控制的必要性**：考虑到自动化提取可能产生幻觉或遗漏，作者引入了基于检查表的质量控制机制，并设计了“优化循环”，确保进入记忆库的每张卡片都是经过验证的高质量知识。\n*   **最终假设验证**：如果上述逻辑成立，那么经过治理的经验配合渐进式搜索，应该能显著优于直接使用原始数据或传统RAG方法。\n*   **实验反馈**：通过在SWE-bench上的实验，证实了“治理后的经验”比“原始数据”更有效，且“Agent式搜索”比“静态RAG”更具鲁棒性。这反向验证了作者最初的假设：**高质量的结构化记忆 + 类人的搜索策略 = 更强的代码Agent**。\n\n### 总结\n\n作者的思考路径遵循了 **“发现人类行为优势 -> 识别数据应用瓶颈 -> 引入治理机制进行结构化转化 -> 模拟人类认知过程设计交互 -> 实验验证逻辑闭环”** 的完整链条。其核心贡献在于将“数据治理”引入了Agent的记忆构建过程，并证明了结构化的知识表示比单纯的数据量更重要。"
                },
                {
                    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
                    "arxiv_id": "2601.06487",
                    "authors": "Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha",
                    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了ArenaRL框架，旨在通过强化学习（RL）和相对排序机制提升开放式LLM智能体（如复杂旅行规划）的性能。这属于单智能体（规划）和自我演化（通过反馈自我完善）的研究范畴，且核心贡献在于算法改进而非纯应用或基础设施优化。",
                    "summary2": "本文旨在解决开放性Agent任务中强化学习因点式评分导致的判别性崩溃问题。针对缺乏客观真值的复杂规划场景，我们提出了一种ArenaRL框架，通过基于锦标赛的相对排名机制替代不稳定的标量评分，并利用带种子的单败淘汰赛实现高效优势估计。我们在Open-Travel和Open-DeepResearch基准上，通过胜率和多维度评估指标验证了其有效性，显著优于现有RL基线。",
                    "summary_translation": "强化学习 已显著提升了 LLM agents (大语言模型智能体) 在具有 verifiable outcomes (可验证结果) 的任务上的表现，但在具有 vast solution spaces (巨大解空间) 的 open-ended agent tasks (开放式智能体任务)（例如复杂的旅行规划）中仍然面临挑战。由于这些任务缺乏 objective ground-truth (客观真值)，当前的 RL algorithms (强化学习算法) 主要依赖于对 individual responses (单个响应) 分配 scalar scores (标量分数) 的 reward models (奖励模型)。我们认为这种 pointwise scoring (逐点打分) 存在固有的 discrimination collapse (判别性崩溃)：reward model (奖励模型) 难以区分不同 trajectories (轨迹) 之间的 subtle advantages (细微优势)，导致组内的分数被压缩到一个狭窄的范围内。因此，有效的 reward signal (奖励信号) 被 reward model (奖励模型) 的噪声所主导，导致 optimization stagnation (优化停滞)。为了解决这个问题，我们提出了 ArenaRL，这是一种从 pointwise scalar scoring (逐点标量打分) 转向 intra-group relative ranking (组内相对排序) 的 reinforcement learning paradigm (强化学习范式)。ArenaRL 引入了一种 process-aware pairwise evaluation mechanism (过程感知成对评估机制)，采用 multi-level rubrics (多级评分标准) 为 trajectories (轨迹) 分配 fine-grained relative scores (细粒度相对分数)。此外，我们构建了一个 intra-group adversarial arena (组内对抗竞技场) 并设计了一种 tournament-based ranking scheme (基于锦标赛的排序方案) 来获取稳定的 advantage signals (优势信号)。Empirical results (实证结果) 证实，构建的 seeded single-elimination scheme (种子单败淘汰赛方案) 在仅具有 O(N) 复杂度的情况下，实现了与具有 O(N^2) 复杂度的 full pairwise comparisons (全成对比较) 几乎等效的 advantage estimation accuracy (优势估计精度)，在效率和精度之间取得了最佳平衡。此外，为了解决缺乏针对 open-ended agents (开放式智能体) 的 full-cycle benchmarks (全周期基准) 的问题，我们构建了 Open-Travel 和 Open-DeepResearch，这两个高质量的 benchmarks (基准) 具有涵盖 SFT (监督微调)、RL training (强化学习训练) 和 multi-dimensional evaluation (多维评估) 的 comprehensive pipeline (全流程管道)。Extensive experiments (广泛实验) 表明，ArenaRL 明显优于 standard RL baselines (标准强化学习基线)，使 LLM agents (大语言模型智能体) 能够为复杂的现实世界任务生成更 robust (鲁棒) 的解决方案。",
                    "inspiration_trace": "基于论文《ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking》的内容，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题的确立与现状的困境\n**思考起点：** 强化学习（RL）在数学、代码等有明确“标准答案”的任务上极其成功，但在旅行规划、深度研究等**开放式任务**上却举步维艰。\n**核心矛盾：** 开放式任务没有客观的Ground Truth（标准答案）。现有的解决方案通常采用“LLM作为裁判”给模型的输出打一个标量分数（例如0-10分）。\n**初步假设：** 如果能训练一个准确的奖励模型来给这些开放式轨迹打分，就能像数学题一样进行RL优化。\n\n### 第二阶段：现象观察与核心痛点识别\n**深入观察：** 作者在实验中发现了一个反直觉的现象：随着模型能力的提升，RL优化反而停滞甚至退化。\n**归因分析：** 作者将此命名为**“判别性崩溃”**。\n1.  **信号压缩：** 当模型变强后，生成的轨迹质量都很高，且分布趋同。裁判很难区分“好”和“更好”，给出的分数被压缩在一个极窄的区间（如0.8-0.9）。\n2.  **信噪比（SNR）恶化：** 裁判本身存在随机噪声（如位置偏差、长度偏好）。当分数之间的差异（信号）小于裁判的随机误差（噪声）时，优化过程实际上是在拟合噪声，而非提升能力。\n**结论：** 在开放式任务中，**点式标量打分**存在根本性缺陷，无法提供有效的梯度信号。\n\n### 第三阶段：范式转移——从“绝对分数”到“相对排序”\n**理论借鉴：** 借鉴决策理论，人类在判断模糊事物时，相对比较（A比B好）比绝对量化（A是8.5分）更稳定、更准确。\n**核心假设：** 放弃给单个轨迹打绝对分，转而在**组内**进行轨迹之间的两两比较，构建相对排名。\n**预期收益：** 相对比较能放大细微的质量差异，避免陷入绝对分数的“高分段压缩”陷阱，从而获得更纯净的优势信号。\n\n### 第四阶段：工程落地的挑战——效率与精度的权衡\n**新问题：** 虽然两两比较（Round-Robin，循环赛）能提供最准确的排名，但其计算复杂度是 $O(N^2)$。对于需要大规模采样的RL训练来说，这是不可接受的昂贵成本。\n**朴素尝试与失败：**\n1.  **锚点法：** 只让所有样本与一个锚点（如贪婪解码结果）比较。复杂度降为 $O(N)$，但分辨率太低，无法区分两个都比锚点好但互有优劣的样本。\n2.  **标准淘汰赛：** 随机两两对决，胜者晋级。虽然快，但随机性太大。两个高质量的样本可能在第一轮就相遇，导致其中一个被过早淘汰，损失了信息。\n\n### 第五阶段：结构创新——带种子的单败淘汰赛\n**逻辑推演：** 为了在 $O(N)$ 的线性复杂度下保持接近循环赛的精度，必须解决“过早相遇”的问题。\n**解决方案：** 提出**带种子的单败淘汰赛**。\n1.  **预排序：** 先利用低成本的“锚点法”对所有样本进行一轮快速评估，得到一个粗略的初始排名（种子）。\n2.  **结构化对决：** 按照种子排布对阵（例如：第1名对最后一名，第2名对倒数第二名）。这保证了强样本在早期不会相遇，只有到了决赛圈才强强对话。\n**结果：** 这种设计既保留了线性复杂度的高效，又通过先验信息保证了排名的保真度，实现了效率与精度的最佳平衡。\n\n### 第六阶段：评估维度的深化——过程感知\n**最后一步：** 既然是Agent任务，评价标准不能只看最终答案。\n**补充逻辑：** 引入**过程感知的成对评估**。裁判不仅看结果，还要审查思维链的逻辑连贯性和工具调用的有效性。这确保了RL优化的方向是提升Agent的内在推理能力，而不是仅仅学会生成漂亮的最终文本。\n\n---\n\n**总结：**\n作者的思考路径是从**“开放式任务缺乏客观标准”**这一痛点出发，通过**“判别性崩溃”**否定了现有的标量打分范式，进而提出**“相对排序”**的理论转向。为了解决该理论带来的计算开销，作者通过**“带种子的淘汰赛”**这一精巧的结构设计，成功在计算效率和信号质量之间找到了最优解，最终形成了ArenaRL的方法论闭环。"
                },
                {
                    "title": "Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users",
                    "arxiv_id": "2601.06301",
                    "authors": "Arth Bhardwaj, Nirav Diwan, Gang Wang",
                    "summary": "Web scraping has historically required technical expertise in HTML parsing, session management, and authentication circumvention, which limited large-scale data extraction to skilled developers. We argue that large language models (LLMs) have democratized web scraping, enabling low-skill users to execute sophisticated operations through simple natural language prompts. While extensive benchmarks evaluate these tools under optimal expert conditions, we show that without extensive manual effort, current LLM-based workflows allow novice users to scrape complex websites that would otherwise be inaccessible. We systematically benchmark what everyday users can do with off-the-shelf LLM tools across 35 sites spanning five security tiers, including authentication, anti-bot, and CAPTCHA controls. We devise and evaluate two distinct workflows: (a) LLM-assisted scripting, where users prompt LLMs to generate traditional scraping code but maintain manual execution control, and (b) end-to-end LLM agents, which autonomously navigate and extract data through integrated tool use. Our results demonstrate that end-to-end agents have made complex scraping accessible - requiring as little as a single prompt with minimal refinement (less than 5 changes) to complete workflows. We also highlight scenarios where LLM-assisted scripting may be simpler and faster for static sites. In light of these findings, we provide simple procedures for novices to use these workflows and gauge what adversaries could achieve using these.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究并基准测试了“端到端 LLM 智能体”，重点在于它们通过集成的工具使用进行自主导航和数据提取的能力，这符合单智能体研究范围中的“工具使用”和“规划”特征。",
                    "summary2": "本文旨在评估LLM对网络爬虫的民主化影响及非专家用户的实际能力。针对35个跨越5个安全层级的网站，我们提出了两种工作流：LLM-assisted Scripting (LAS) 和 End-to-end LLM Agent (ELA)，并在这些网站上通过Extraction Success Rate (ESR)、Execution Time和Manual Effort Required (MER)验证了其有效性。",
                    "summary_translation": "历史上，Web scraping (网络爬虫) 一直需要掌握 HTML parsing (HTML解析)、session management (会话管理) 和 authentication circumvention (身份验证绕过) 等技术专长，这使得大规模数据提取仅限于熟练的开发者。我们认为，large language models (LLMs，大语言模型) 已经普及了 Web scraping，使低技能用户能够通过简单的 natural language prompts (自然语言提示) 执行复杂的操作。尽管现有的广泛基准测试是在最佳专家条件下评估这些工具的，但我们表明，在无需大量人工投入的情况下，当前的 LLM-based workflows (基于LLM的工作流) 能够使 novice users (新手用户) 抓取原本无法访问的复杂网站。我们针对 35 个跨越五个 security tiers (安全层级) 的网站（包括 authentication (身份验证)、anti-bot (反机器人) 和 CAPTCHA controls (验证码控制)），系统性地评估了日常用户利用 off-the-shelf LLM tools (现成的LLM工具) 所能实现的效果。我们设计并评估了两种截然不同的 workflows (工作流)： LLM-assisted scripting (LLM辅助脚本编写)，即用户提示 LLM 生成传统的抓取代码，但保留手动执行控制权；以及 end-to-end LLM agents (端到端LLM智能体)，即通过 integrated tool use (集成工具使用) 自主导航并提取数据。我们的结果表明，end-to-end agents (端到端智能体) 已使复杂的抓取任务变得易于实现——仅需一个提示配合 minimal refinement (微调，少于5次修改) 即可完成整个 workflows (工作流)。我们还强调了在某些场景下，对于 static sites (静态网站)，LLM-assisted scripting (LLM辅助脚本编写) 可能更为简单快捷。基于这些发现，我们为 novice users (新手用户) 提供了使用这些 workflows (工作流) 的简易流程，并评估了 adversaries (攻击者) 利用这些技术可能达到的效果。",
                    "inspiration_trace": "基于论文《Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观观察与问题提出\n**——从“技术壁垒”到“技术民主化”的范式转移**\n\n1.  **观察现象**：\n    *   **过去**：网络爬虫是一项高门槛技术，需要掌握HTML解析、会话管理、反爬虫绕过等专业技能，这构成了天然的“技术过滤器”，限制了大规模数据提取仅限于熟练开发者。\n    *   **现在**：大语言模型（LLM）和智能体框架的出现，使得用户仅凭自然语言提示就能执行复杂的爬虫操作。\n\n2.  **提出核心问题**：\n    *   LLM是否真正实现了网络爬虫的“民主化”？\n    *   换言之，缺乏深厚技术背景的“日常用户”，是否真的能利用现成的LLM工具，完成以前只有专家才能做到的复杂数据提取？\n\n### 第二阶段：识别研究空白\n**——现有评估与真实场景的脱节**\n\n1.  **批判现有文献**：\n    *   作者注意到，现有的基准测试（如AgentBench, OSWorld）大多关注“最佳实践”。\n    *   这些测试通常假设在**理想条件**下进行：拥有专家指导、经过优化的配置、复杂的提示工程。\n\n2.  **锁定现实差距**：\n    *   **真实用户画像**：非专家用户通常使用默认设置，缺乏深度调试技能，且受限于时间和预算。\n    *   **研究盲区**：学术界缺乏对“非专家用户在现实约束下，利用现成工具到底能做到什么程度”的实证评估。\n\n3.  **确立研究目标**：\n    *   不再评估“工具的上限（专家能做什么）”，而是评估“工具的下限（新手能做什么）”。\n    *   量化这种“民主化”对网络安全防御（反爬虫）的实际影响。\n\n### 第三阶段：假设构建与变量设计\n**——如何模拟“真实世界”的复杂性？**\n\n1.  **定义威胁模型**：\n    *   为了建立保守的基线，作者将研究对象设定为“低技能行为者”。假设他们只会运行Python脚本、使用LLM，但不了解爬虫库的深层细节，也不使用高级提示技巧。\n\n2.  **构建难度梯度**：\n    *   为了全面测试，作者认为不能只测静态页面。必须模拟网站防御的升级过程。\n    *   **逻辑推演**：从最简单的静态页面，逐步增加难度，直到传统工具完全失效。\n    *   **最终分类**：确立了5个难度层级（简单HTML -> 复杂HTML -> 简单认证 -> 复杂认证 -> CAPTCHA）。\n\n### 第四阶段：方法论形成\n**——对比两种截然不同的“人机协作模式”**\n\n1.  **模式抽象**：\n    *   作者意识到，用户使用LLM爬虫主要有两种思维模式，这构成了实验的核心对比维度：\n    *   **模式 A：LLM辅助脚本编写 (LAS)**。\n        *   *思维逻辑*：用户仍想掌控代码执行，只是把LLM当作“高级程序员”来生成代码（如BeautifulSoup/Scrapy脚本），然后自己运行。\n        *   *代表场景*：传统开发者的提效工具。\n    *   **模式 B：端到端LLM智能体 (ELA)**。\n        *   *思维逻辑*：用户完全不想写代码，只给目标，让智能体像人一样操作浏览器（如Claude, Simular.ai）。\n        *   *代表场景*：完全不懂代码的小白用户。\n\n2.  **确立评估指标**：\n    *   除了传统的“成功率”（能不能做），作者引入了“易用性指标”（好不好做）。\n    *   **关键指标**：手动干预程度。这直接反映了“民主化”的程度——如果需要频繁手动调试，说明门槛依然存在。\n\n### 第五阶段：实证推演与结果验证\n**——验证“易用性”与“能力”的权衡**\n\n1.  **预期假设**：\n    *   对于静态网站，传统代码（LAS）应该更快、更高效。\n    *   对于复杂网站（登录、验证码），智能体（ELA）应该具有压倒性优势，因为它们能模拟人类行为。\n\n2.  **实验验证与发现**：\n    *   **发现1**：ELA确实让复杂爬虫变得触手可及（单次提示即可），证明了民主化的真实性。\n    *   **发现2**：但在简单任务上，ELA效率低下（慢10-20倍），属于“杀鸡用牛刀”。\n    *   **发现3**：LAS在遇到认证和反爬时彻底失效，而ELA虽然慢但能行得通。\n\n### 第六阶段：结论与启示\n**——从“二元对立”到“场景互补”**\n\n1.  **逻辑升华**：\n    *   作者的思考并没有停留在“谁更好”，而是上升到了“适用场景”。\n    *   **核心结论**：不存在万能的工具，存在的是“效率”与“可访问性”的权衡。\n\n2.  **未来展望**：\n    *   基于实验结果，作者进一步推演出未来的理想形态：**混合模式**。\n    *   *新思路*：利用智能体（ELA）去搞定最难的“登录/绕过”环节，获取会话权限，然后交给传统脚本（LAS）进行高效的数据提取。这结合了智能体的“灵活性”和脚本的“高效性”。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“现象观察 -> 差距识别 -> 模型构建 -> 对比实验 -> 场景化结论”**的学术逻辑。其核心创新点在于将评估视角从“技术能力的极限”转向了“普通用户的可达性”，并通过对两种工作流（LAS vs ELA）的精细划分，精准地刻画了LLM时代网络爬虫技术的新版图。"
                },
                {
                    "title": "Automated QoR improvement in OpenROAD with coding agents",
                    "arxiv_id": "2601.06268",
                    "authors": "Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee",
                    "summary": "EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 \"AuDoPEDA\"，这是一个基于LLM的自主编码智能体系统。论文详细描述了智能体如何读取代码库（记忆/上下文）、提出研究方向（规划）、将其扩展为实施步骤并提交可执行的差异（工具使用与执行）。这完全符合单智能体在规划、工具使用和自主执行方面的研究范围，尽管应用于EDA领域，但其核心贡献在于智能体系统的构建而非单纯的应用。",
                    "summary2": "本文旨在解决EDA开发受限于专家资源稀缺及代码库复杂的问题，实现利用LLM自主改进OpenROAD的QoR。针对OpenROAD多语言、大规模的代码仓库，我们提出了AuDoPEDA系统，该系统集成了图结构文档生成、基于文献的DSPy规划及具有QoR反馈的自主执行代理。在ASAP7、SKY130HD和Nangate45 benchmark上，通过routed wirelength和effective clock period验证，实现了线长降低5.9%和时钟周期减少10.0%的显著效果。",
                    "summary_translation": "EDA（电子设计自动化）的开发与创新一直受到专家工程资源稀缺的制约。尽管领先的 LLMs（大语言模型）在代码编写和科学推理任务中表现优异，但其在推动 EDA 技术本身发展方面的能力尚未得到充分验证。我们提出了 AuDoPEDA，这是一个构建于 OpenAI 模型和 Codex 类智能体之上的自主式、基于代码仓库的编码系统。该系统能够读取 OpenROAD（开源自动化设计工具），提出研究方向，将其扩展为实施步骤，并提交可执行的 diffs（差异补丁）。我们的主要贡献包括：(i) 一个用于 EDA 代码修改的闭环 LLM 框架；(ii) 一套面向 PPA（功耗、性能、面积）优化的 OpenROAD 任务集及评估协议；以及 (iii) 仅需极少量人工监督的端到端演示。在 OpenROAD 上进行的实验表明，布线线长最多降低了 5.9%，有效时钟周期最多缩短了 10.0%。",
                    "inspiration_trace": "基于论文《Automated QoR improvement in OpenROAD with coding agents》，以下是对作者提出AuDoPEDA方法核心逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观问题：EDA创新的资源瓶颈与LLM的潜力错位\n**观察：**\nEDA（电子设计自动化）工具的发展严重依赖资深专家。这些专家需要跨越庞大的代码库（数百万行C++、Tcl、Python等）和复杂的迭代流程进行推理。然而，这种人力资源极其稀缺，限制了EDA技术的迭代速度。\n\n**矛盾：**\n另一方面，以GPT-4、Codex为代表的大语言模型（LLM）在代码生成和科学推理任务上表现出色。但在EDA领域，LLM的应用多停留在辅助脚本编写或RTL生成层面，尚未触及核心物理设计（PD）算法的改进。\n\n**核心问题：**\n能否让LLM驱动的智能体像人类专家一样，自主地对工业级EDA代码库进行修改，并直接提升芯片设计的质量（QoR，如功耗、性能、面积）？\n\n---\n\n### 2. 深入分析：通用代码代理在EDA领域的“水土不服”\n**挑战识别：**\n作者意识到，直接将通用的代码生成模型（如GitHub Copilot）应用于OpenROAD这样的EDA项目会面临三个致命障碍：\n1.  **上下文稀释：** OpenROAD代码库规模巨大、语言混杂（C++核心+Tcl脚本+Python工具），且文档稀疏。通用模型无法在有限的上下文窗口中理解跨模块的隐式接口和不变量。\n2.  **领域知识缺失：** 优化物理设计不仅仅是写代码，更需要结合EDA领域的学术文献（如布局、布线算法）。单纯的代码补全无法产生“研究级”的改进思路。\n3.  **验证闭环困难：** 软件工程的正确性通常通过单元测试判断，但EDA的改进必须通过物理设计流程（RTL-to-GDS）来验证，指标是PPA（功耗、性能、面积）。这是一个高成本、长周期的反馈过程。\n\n---\n\n### 3. 核心假设：模拟人类专家的“入职”过程\n**思维转折：**\n作者提出，与其试图训练一个懂EDA的超级模型，不如模拟人类专家的学习路径。人类专家在接手OpenROAD时，并不是直接阅读源码，而是先阅读文档、理解架构、查阅文献，然后提出假设，最后修改代码并跑流验证。\n\n**假设：**\n如果构建一个系统，能够为LLM智能体提供“文档优先”的入职环境，使其能够像人类一样结构化地获取代码知识、结合文献进行规划，并在真实的QoR反馈下迭代，那么它就能实现自主的代码改进。\n\n---\n\n### 4. 方法论构建：四阶段逻辑演进\n基于上述假设，作者将复杂的任务解构为四个逻辑严密的阶段，形成了一个闭环系统。\n\n#### 第一阶段：结构化理解（S0）—— 解决“看不懂”的问题\n**思考：**\n原始代码库太乱，直接喂给LLM效果差。必须先进行预处理，提取出机器可读的“知识图谱”。\n**逻辑：**\n利用Tree-sitter解析多语言代码，构建属性图（DAG），将函数调用、依赖关系显式化。然后，通过自底向上的遍历，自动生成“文档卡片”，总结每个模块的API、前置/后置条件。这相当于为智能体编写了一部动态更新的“操作手册”。\n\n#### 第二阶段：文献引导的规划（S1）—— 解决“没思路”的问题\n**思考：**\n光懂代码结构不够，还需要知道“改什么能提升性能”。这需要领域知识。\n**逻辑：**\n将规划过程视为一个声明式的程序（利用DSPy框架）。智能体结合“代码文档”（S0产物）和“EDA文献库”（外部知识），通过检索增强生成（RAG），合成出高层的研究计划。例如：“根据文献X，调整布局阶段的拥塞惩罚权重可能减少线长”。\n\n#### 第三阶段：计划定位与颗粒化（S2）—— 解决“落地难”的问题\n**思考：**\n高层计划（如“调整拥塞权重”）不能直接执行，必须映射到具体的代码修改点，且必须保证修改是安全的。\n**逻辑：**\n将高层计划投影到代码图上，找到具体的修改位置（文件、函数）。同时，将计划转化为“颗粒化计划”，包含具体的Diff意图、预检查（编译、测试）、监控指标和回滚条件。这一步将抽象的“研究思路”变成了可执行的“工程任务单”。\n\n#### 第四阶段：自主执行与QoR反馈（S3）—— 解决“验证慢”的问题\n**思考：**\n代码修改后，必须跑通EDA流程才能知道好坏。如何保证自动化且不破坏系统？\n**逻辑：**\n构建一个基于Codex的执行智能体，应用Diff、编译、运行OpenROAD流程。关键在于引入“QoR门控”：如果修改导致DRC违规或时序恶化，系统自动回滚。智能体通过爬山算法，在指标反馈的引导下不断尝试，直到找到最优解。\n\n---\n\n### 5. 总结：从“辅助工具”到“自主研究员”的范式转变\n**逻辑闭环：**\n整个思考过程从解决“资源稀缺”出发，通过分析EDA代码的特殊性，提出了“模拟人类专家学习”的核心假设，并最终落地为一个集成了**知识图谱构建（S0）**、**文献推理（S1）**、**工程映射（S2）**和**闭环验证（S3）**的完整系统。\n\n**最终贡献：**\n作者不仅仅是在用LLM写代码，而是构建了一个能够**阅读文献、提出假设、修改算法、并在真实芯片设计流程中验证效果**的自主科研智能体。这标志着EDA工具的优化模式从“人工驱动”转向了“AI自主驱动”。"
                },
                {
                    "title": "Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation",
                    "arxiv_id": "2601.06034",
                    "authors": "Dudekula Kasim Vali",
                    "summary": "Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个“Autonomous QA Agent”，利用检索增强生成（RAG，对应记忆机制）和Selenium脚本生成（对应工具使用）来自动化软件测试任务，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决LLM生成Selenium脚本时因缺乏应用上下文而产生幻觉的问题。针对自然语言需求和HTML DOM结构，我们提出了一种Autonomous QA Agent，这是一种基于RAG的多模态框架，通过检索文档与HTML上下文生成脚本。在自定义电商应用的20个测试场景上，通过语法有效性、元素解析率和执行成功率验证了其有效性，实现了90%的执行成功率。",
                    "summary_translation": "软件测试在软件开发生命周期中至关重要，然而将需求转化为可执行测试脚本的过程仍主要依赖人工，且容易出错。尽管大语言模型能够生成代码，但它们经常产生幻觉，编造出不存在的UI元素。我们提出了自主QA代理，这是一种检索增强生成系统，它将Selenium脚本生成基于特定项目的文档和HTML结构之上。通过将多种格式导入向量数据库，我们的系统在生成代码之前会检索相关的上下文信息。针对20个电商测试场景的评估表明，我们的RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95%置信区间：[85%, 95%]，p < 0.001）的执行成功率，而标准LLM生成的成功率仅为30%。尽管我们的评估仅限于单一领域，但我们的方法通过将生成过程基于实际的DOM结构，显著减少了幻觉现象，展示了RAG在自动化UI测试中的潜力。",
                    "inspiration_trace": "基于论文《Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：QA环节的效率瓶颈\n**思考起点：** 在敏捷开发和DevOps主导的现代软件工程中，开发迭代速度极快，但软件测试（QA）成为了明显的瓶颈。\n**核心痛点：** QA工程师花费40%-50%的时间在做“翻译”工作——将自然语言描述的功能需求（PRD）手动转化为机器可执行的自动化测试脚本（如Selenium）。这个过程不仅枯燥，而且容易出错（如选错元素ID、忽略边界情况）。\n**初步设想：** 能否利用代码生成能力强大的大语言模型（LLM）来自动完成这个“翻译”过程？\n\n### 2. 尝试与失败：LLM的“盲写”困境\n**尝试：** 直接使用标准的LLM（如GPT-4, Llama），输入自然语言需求（如“生成一个添加购物车的脚本”），让其编写Selenium代码。\n**观察到的现象：** LLM生成的代码语法通常没问题，但一运行就报错。\n**失败原因分析：** LLM患有一种“盲写症”。它通晓通用的编程语法，但它**看不见**被测应用（AUT）的具体结构。\n**具体表现：** LLM会凭空捏造UI元素。例如，它可能会猜测登录按钮的ID是 `#login-btn`，但实际开发人员写的是 `#btn-submit-login`。这种“幻觉”导致生成的脚本无法定位元素，执行失败。\n\n### 3. 深度诊断：语义鸿沟与上下文缺失\n**问题定义：** 核心问题在于“人类需求”与“机器执行”之间存在语义鸿沟。要生成一个可运行的脚本，不仅需要逻辑（做什么），还需要精确的定位信息（在哪里做）。\n**现有方案的局限：**\n*   **传统MBT（基于模型的测试）：** 构建成本太高，维护困难。\n*   **通用代码RAG：** 现有的检索增强生成多用于检索“相似的代码片段”。但在UI测试中，检索别人的代码对定位当前页面的特定DOM元素帮助不大。\n**关键洞察：** 要解决幻觉，必须让LLM“看见”真实的界面结构。LLM缺失的上下文不是代码示例，而是**应用的实际DOM结构**。\n\n### 4. 策略转折：从“代码检索”到“结构检索”\n**核心假设：** 如果在生成脚本之前，先给LLM提供被测应用的真实HTML文档和需求文档，它就能基于真实的结构编写准确的定位器，从而消除幻觉。\n**方法论创新：** 提出一种专门针对QA领域的RAG架构。\n*   **传统RAG：** 检索通用知识库。\n*   **本论文RAG：** 检索**双模态上下文**。\n    1.  **功能性上下文：** 需求文档（Markdown/PDF），告诉LLM“要测什么”。\n    2.  **结构性上下文：** 原始HTML文件，告诉LLM“元素在哪里”。\n\n### 5. 架构构建：多模态摄入与上下文融合\n**逻辑推演：** 为了实现上述假设，系统需要具备以下能力：\n1.  **知识库构建：** 必须能够“吃进”多种格式的数据。不仅要处理文本需求，还要解析HTML标签，提取出ID、Class等关键属性，并存入向量数据库。\n2.  **精准检索：** 当用户提问时，系统需要同时从文档库中找到相关需求，并从HTML库中找到对应的页面结构片段。\n3.  **提示工程约束：** 在生成阶段，必须强制LLM使用检索到的真实ID，而不是自己编造。通过Prompt明确指令：“仅使用提供的HTML结构中的ID”。\n\n### 6. 验证与结论：Grounding（接地气）的有效性\n**实验设计：** 对比“标准LLM（无上下文）”与“RAG Agent（含HTML上下文）”。\n**结果验证：**\n*   标准LLM：虽然语法正确，但因元素定位错误，执行成功率仅为30%。\n*   RAG Agent：通过将生成过程“锚定”在真实的DOM结构上，执行成功率提升至90%。\n**最终结论：** 证明了在UI自动化测试中，**结构化的上下文（HTML）比通用的代码知识更重要**。通过RAG技术将LLM与实际应用状态连接，是解决测试脚本生成中“幻觉”问题的有效路径。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现瓶颈 -> 尝试新技术（LLM） -> 识别新技术缺陷（幻觉/盲写） -> 引入特定领域知识（DOM结构） -> 设计专用架构（多模态RAG） -> 验证有效性”** 的完整逻辑闭环。其核心创新点在于意识到UI测试不仅仅是代码生成任务，更是一个需要精确空间感知（DOM结构）的任务。"
                },
                {
                    "title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding",
                    "arxiv_id": "2601.08653",
                    "authors": "Zenghua Liao, Jinzhi Liao, Xiang Zhao",
                    "summary": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出的Prism框架包含复杂意图分解（规划）和自我演化意图调优（通过反馈自我完善）模块，明确涉及通过数据驱动的反馈迭代优化LLM能力，符合单智能体规划和自我演化的研究范围。",
                    "summary2": "本文旨在降低LLM在复杂意图理解中的用户认知负荷。针对澄清问题存在逻辑依赖的复杂场景，我们提出了一种名为Prism的框架，通过复杂意图分解和逻辑澄清生成实现连贯交互。我们在TIN、IN3和ABP数据集上通过Logical Conflict Rate、User Satisfaction和Task Completion Time等指标验证了其有效性。",
                    "summary_translation": "大语言模型正迅速成为社交平台的网络原生接口。在社交网络中，用户的目标往往模糊且动态变化，这使得复杂的意图理解——而非单轮执行——成为有效的人机协作的基石。现有方法试图通过顺序或并行提问来澄清用户意图，但未能解决核心挑战：即对澄清问题之间的逻辑依赖关系进行建模。受认知负荷理论的启发，我们提出了Prism，这是一种用于复杂意图理解的新颖框架，能够实现逻辑连贯且高效的意图澄清。Prism包含四个定制模块：复杂意图分解模块，该模块将用户意图分解为更小且结构良好的元素，并识别它们之间的逻辑依赖关系；逻辑澄清生成模块，该模块基于这些依赖关系组织澄清问题，以确保交互的连贯性和低摩擦性；意图感知奖励模块，该模块通过意图感知奖励函数评估澄清轨迹的质量，并利用蒙特卡洛采样模拟用户与LLM的交互，从而生成大规模、高质量的训练数据；以及自进化意图微调模块，该模块通过数据驱动的反馈和优化，迭代改进LLM的逻辑澄清能力。在澄清交互、意图执行和认知负荷基准测试中，Prism的表现始终优于现有方法。它实现了最先进的逻辑一致性，将逻辑冲突率降低至11.5%，用户满意度提升了14.4%，任务完成时间缩短了34.8%。所有数据和代码均已公开发布。",
                    "inspiration_trace": "基于论文《Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：从“工具”到“接口”的范式转变\n**思考起点：** 作者首先观察到LLM的角色正在发生根本性变化。LLM不再仅仅是后台的生成工具，而是日益成为Web原生环境下的**交互接口**。\n**核心矛盾：** 在社交网络等真实场景中，用户的目标往往是模糊、动态且非结构化的。用户无法一次性提供完美的Prompt，而是倾向于通过多轮对话来“试探”和“澄清”意图。\n**初步判断：** 因此，单次响应的质量不再是唯一指标，**交互过程中的意图理解能力**（即如何通过澄清来对齐用户目标）成为了决定人机协作效率的关键。\n\n### 2. 问题聚焦：现有方法的“逻辑盲区”\n**深入分析：** 作者审视了现有的交互式意图理解方法（如Mistral-Interact的顺序问答、ITIU的并行表格）。\n**发现缺陷：** 这些方法隐含了一个假设——所有的澄清问题之间是**相互独立**的。这在简单场景下有效，但在“复杂意图场景”中失效。\n**关键洞察：** 现实任务中，澄清问题往往存在**逻辑依赖关系**（例如：必须先确定“目的地”，才能合理推荐“活动”）。\n**后果推演：** 如果模型无视这种依赖，就会产生逻辑冲突（例如：在12月推荐去冲绳潜水）。这不仅导致建议不可用，更迫使用户自己去修正逻辑，从而增加了用户的**认知负荷**。\n\n### 3. 理论锚定：引入认知负荷理论（CLT）\n**寻求理论支撑：** 为了系统性地解决“认知负荷”问题，作者引入了心理学中的**认知负荷理论**。\n**理论映射：**\n*   **内在负荷：** 任务本身的复杂性。对应策略：将复杂的模糊意图拆解为结构化的小元素。\n*   **外在负荷：** 交互设计不当带来的额外负担。对应策略：通过逻辑化的提问顺序，减少用户不必要的推理努力。\n**隐喻构建：** 作者提出了核心隐喻——**“棱镜”**。将模糊的用户意图比作“白光”，将逻辑驱动的结构化框架比作“棱镜”。棱镜的作用是将白光折射成有序的“光谱”（分层级的意图元素），从而引导低摩擦的交互。\n\n### 4. 方法论构建：Prism框架的四步演进\n基于上述理论，作者构建了Prism框架，其逻辑演进分为四个层次：\n\n*   **第一步：结构化拆解（解决“是什么”）**\n    *   *思考：* 要降低内在负荷，必须把大问题变小。\n    *   *方案：* 构建**CID数据集**，将复杂意图拆解为具有层级关系的元素，并显式标注元素间的**先决依赖**。这为模型提供了理解复杂意图的“地图”。\n\n*   **第二步：逻辑化生成（解决“怎么问”）**\n    *   *思考：* 要降低外在负荷，提问顺序必须符合人类逻辑。\n    *   *方案：* 设计**逻辑澄清生成模块**。依据第一步的依赖关系组织问题：无依赖的问题并行提问（提高效率），有依赖的问题串行提问（保证逻辑连贯）。\n\n*   **第三步：意图感知评估（解决“好不好”）**\n    *   *思考：* 如何量化一个澄清轨迹是否成功？传统的文本相似度不够，需要关注“意图”是否被准确捕获。\n    *   *方案：* 设计**意图感知奖励函数**。结合“意图重要性”（Token对意图的贡献）和“生成置信度”（模型的确信程度），利用蒙特卡洛采样模拟用户交互，从而筛选出高质量的训练数据。\n\n*   **第四步：自我进化调优（解决“如何持续变好”）**\n    *   *思考：* 数据标注成本高，且模型需要不断适应。\n    *   *方案：* 提出**自我进化意图调优**。利用强模型（如GPT-4o）生成数据，训练小模型，再让小模型作为策略生成新数据，通过迭代反馈（SFT/DPO），让模型在逻辑澄清能力上实现自我进化。\n\n### 5. 验证闭环：从交互到生理指标的全方位评估\n**思考升华：** 既然核心目标是“降低认知负荷”，那么仅评估任务完成率是不够的。\n**多维验证：** 作者设计了三个维度的评估闭环：\n1.  **澄清交互：** 逻辑冲突率是否降低？\n2.  **意图执行：** 下游Agent的任务完成质量是否提高？\n3.  **认知负荷：** 引入**EEG（脑电图）**等生理指标，直接测量用户的大脑活跃度，从物理层面验证了Prism确实降低了用户的认知努力。\n\n---\n\n**总结：**\n作者的思考路径是从**交互体验的痛点**（逻辑混乱导致费脑）出发，利用**认知心理学理论**（CLT）重新定义问题，通过**结构化分解**与**逻辑排序**技术手段实现“棱镜”效应，最后通过**数据驱动的自我进化**和**生理层面的实证**完成了闭环。这是一个典型的从现象观察到理论指导，再到工程落地的学术创新过程。"
                },
                {
                    "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games",
                    "arxiv_id": "2601.08462",
                    "authors": "Sixiong Xie, Zhuofan Shi, Haiyang Shen, Gang Huang, Yun Ma, Xiang Jing",
                    "summary": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个评估LLM智能体在混合动机博弈中社会行为（如合作、欺骗、勾结）的基准，重点研究多智能体环境下的协作、通信与博弈策略，完全符合多智能体研究范围。",
                    "summary2": "本文旨在解决现有评估仅关注行为结果而忽视决策过程的问题，系统评估LLM智能体在混合动机游戏中的高级社会行为。针对混合动机游戏场景，我们提出了M3-BENCH，一种包含BTA、RPA和CCA三个模块的过程感知评估框架，并结合大五人格模型生成社会行为画像。我们在包含24个任务的M3-BENCH基准上，通过14个主流LLM的实验，利用行为轨迹、推理过程和通信内容等指标验证了其有效性，揭示了模型行为与推理的不一致性。",
                    "summary_translation": "随着大语言模型 (LLM) 智能体能力的不断提升，其展现出的高级社会行为（如合作、欺骗和共谋）亟需进行系统性评估。然而，现有的基准测试往往侧重于单一能力维度，或仅依赖行为结果，忽视了智能体在决策推理和沟通交互中蕴含的丰富过程信息。为弥补这一空白，我们提出了 M3-Bench，这是一个面向混合动机博弈的多阶段基准测试，以及一个过程感知评估框架。该框架通过三个模块进行协同分析：BTA (Behavioral Trajectory Analysis，行为轨迹分析)、RPA (Reasoning Process Analysis，推理过程分析) 和 CCA (Communication Content Analysis，沟通内容分析)。此外，我们结合了大五人格模型和社会交换理论，将多维证据聚合为可解释的社会行为画像，从而超越简单的任务得分或基于结果的指标，深入刻画智能体的人格特质和能力画像。实验结果表明，M3-Bench 能够有效区分不同模型间多样的社会行为能力，并揭示出部分模型虽然取得了看似合理的行为结果，但在推理和沟通方面却存在显著的不一致性。",
                    "inspiration_trace": "基于对论文《M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games》的深度分析，以下是作者产出该核心方法的逻辑推演过程：\n\n### 第一阶段：宏观观察与问题界定\n**（从“任务解决者”到“社会行动者”的认知转变）**\n\n1.  **观察现象**：随着LLM智能体能力的提升，它们不再仅仅是执行指令的工具，而是开始展现出复杂的社会行为（如合作、欺骗、结盟、谈判）。\n2.  **发现痛点**：现有的评估基准存在严重的局限性。\n    *   **维度单一**：往往只关注单一能力（如只测合作或只测欺骗），无法捕捉现实社会中合作与竞争交织的复杂场景。\n    *   **结果导向**：大多数评估仅关注“结果指标”（如胜率、合作率），将智能体视为单纯的“任务得分最大化者”。\n3.  **提出核心质疑**：**仅看行为结果是否会导致误判？**\n    *   *思考实验*：一个智能体在前9轮保持合作，却在第10轮背叛。如果只看“合作率”，它会被误判为“合作者”。实际上，它可能是“战略性的背叛者”。\n    *   *结论*：结果只是冰山一角，水面下的“动机”、“推理逻辑”和“沟通策略”才是评估智能体真实社会属性的关键。\n\n### 第二阶段：理论假设与评估范式转移\n**（从“Outcome-Oriented”到“Process-Aware”的范式升级）**\n\n1.  **确立新范式**：为了解决上述误判问题，评估必须从“结果导向”转向“过程感知”。\n2.  **构建评估三角**：要全面理解一个社会行动者，必须同时考察三个维度的证据：\n    *   **做了什么**：行为轨迹与最终收益。\n    *   **想了什么**：决策背后的推理逻辑与动机。\n    *   **说了什么**：沟通内容与语言策略。\n3.  **选择测试载体**：为什么选择“混合动机游戏”？\n    *   *逻辑*：这类游戏天然包含自我利益与亲社会行为的冲突、短期与长期的权衡，是迫使智能体暴露其深层推理和沟通策略的最佳“压力测试场”。\n\n### 第三阶段：方法论构建与系统设计\n**（将抽象的“过程感知”转化为可操作的框架）**\n\n1.  **任务分层设计（由简入繁）**：为了系统性地探测能力边界，作者设计了4个递进层级：\n    *   L1：个体偏好（基准测试）。\n    *   L2：重复互动（引入时间维度，测试长期关系维护）。\n    *   L3：群体困境（引入多方博弈，测试集体治理）。\n    *   L4：不完全信息与语言博弈（引入隐藏角色，测试高阶认知与欺骗）。\n2.  **三维分析模块（BTA/RPA/CCA）**：\n    *   **BTA (Behavior)**：用规则统计量化行为（如合作率、报复率）。\n    *   **RPA (Reasoning)**：利用LLM-as-a-Judge解析智能体的思维链，量化其动机（亲社会vs自私）和信念状态。\n    *   **CCA (Communication)**：分析对话内容，标记语用行为（如承诺、威胁、欺骗），并检测言行一致性。\n3.  **引入控制变量**：将“是否允许沟通”作为关键变量，对比Silent与Comm条件，以剥离语言机制对社会行为的塑造作用。\n\n### 第四阶段：结果解释与画像生成\n**（从冷冰冰的分数到有温度的“社会画像”）**\n\n1.  **拒绝单一排名**：作者意识到，将行为、推理、沟通强行合并为一个分数会丢失关键信息（例如“行为正确但动机不纯”）。\n2.  **引入社会科学理论**：为了让评估结果具有人类可解释性，作者引入了心理学和社会学框架：\n    *   **大五人格**：将智能体的决策映射到外向性、宜人性等性格维度。\n    *   **社会交换理论**：分析互惠、公平、信任等社会属性。\n3.  **输出“画像式”报告**：最终目标不是给出一个“谁更强”的排行榜，而是生成一份多维度的“社会行为画像”，揭示智能体的优势、风险（如“伪装合作”）以及能力结构。\n\n### 总结：逻辑链条全景\n**观察LLM具备社会性 → 发现现有评估只看结果（易误判） → 提出必须看“过程”（行为+思维+语言） → 选用混合动机游戏作为测试床 → 构建BTA/RPA/CCA三维分析框架 → 引入人格/社会理论进行画像化解释 → 最终实现对智能体真实意图与风险的深度诊断。**"
                },
                {
                    "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
                    "arxiv_id": "2601.08323",
                    "authors": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
                    "summary": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 AtomMem，一种针对 LLM 智能体的可学习动态记忆框架。它属于单智能体研究范围中的“记忆”模块，旨在通过强化学习优化智能体的记忆管理策略，而非纯应用或纯推理研究。",
                    "summary2": "本文旨在解决现有智能体记忆机制依赖静态工作流而缺乏灵活性的问题。针对长上下文多跳 QA 任务，我们提出了一种基于原子 CRUD 操作的可学习动态记忆框架 AtomMem。该方法将记忆管理解构为原子操作并结合强化学习，实现自主记忆策略。在 HotpotQA、2WikiMultihopQA 和 Musique 等长上下文 Benchmark 上，通过准确率验证了其有效性，显著优于静态基线方法。",
                    "summary_translation": "为智能体配备记忆对于解决现实世界的长视界问题至关重要。然而，大多数现有的智能体记忆机制依赖于静态和手工设计的工作流。这限制了这些记忆设计的性能和泛化能力，从而突显了对更灵活的基于学习的记忆框架的需求。在本文中，我们提出了 AtomMem，它将记忆管理重构为一个动态决策问题。我们将高层的记忆过程解构为基本的原子 CRUD (Create, Read, Update, Delete，即创建、读取、更新、删除) 操作，从而将记忆工作流转化为一个可学习的决策过程。通过结合监督微调与强化学习，AtomMem 学习了一种自主的、任务对齐的策略，以编排针对特定任务需求定制的记忆行为。在 3 个长上下文基准测试中的实验结果表明，训练后的 AtomMem-8B 始终优于先前的静态工作流记忆方法。对训练动态的进一步分析显示，我们这种基于学习的构建方式使智能体能够发现结构化的、任务对齐的记忆管理策略，这突显了其相对于预定义例程的关键优势。",
                    "inspiration_trace": "基于论文《AtomMem: Learnable Dynamic Agentic Memory with Atomic Memory Operation》，以下是对作者产出该文章核心思考过程的系统性逻辑推演：\n\n### 1. 宏观观察：记忆机制的“静态”困境\n**思考起点：** 作者首先关注到 LLM 智能体在解决长周期、复杂任务时的核心瓶颈——记忆机制。\n**现状分析：** 现有的主流智能体记忆方案大多依赖于**静态的、人工设计的工作流**（Static, Hand-crafted Workflows）。例如，固定的记忆融合策略或预定义的遗忘计划。\n**问题识别：** 这种“一刀切”的假设存在根本缺陷。现实世界的任务千变万化，有的任务需要极高的精度（不能随意融合），有的任务需要极长的推理链（不能随意遗忘）。静态规则无法适应任务上下文的信息密度波动，导致智能体在复杂场景下性能受限。\n\n### 2. 深入剖析：现有“动态”方法的伪动态性\n**进一步思考：** 既然静态不行，那么近期那些号称“动态”的方法（如 MemAgent）是否解决了问题？\n**批判性发现：** 作者发现这些方法虽然允许记忆内容的动态变化，但在**工作流层面依然是僵化的**。\n**具体痛点：** 例如，某些方法强制要求在每一步都执行“更新”操作，即使当前输入的信息是稀疏或无关的。这种“内容优化但流程受限”的范式，迫使智能体进行冗余操作，无法有效分配认知资源。\n\n### 3. 核心假设：将记忆管理重构为决策问题\n**思维跃迁：** 如何打破僵化的流程？作者借鉴了“工具学习”的思路——模型不是被规定何时使用工具，而是学会何时使用工具。\n**核心假设：** 记忆管理不应是一段固定的代码逻辑，而应被视为一个**动态决策过程**（Dynamic Decision-making Problem）。\n**理论框架：** 智能体需要根据当前的上下文和任务需求，自主决定“是否需要记忆”、“需要读取什么”、“是否需要删除旧信息”。这本质上是一个序列决策问题。\n\n### 4. 方法论构建：原子化操作与动作空间\n**落地思考：** 如何将上述抽象的“决策过程”具体化？\n**解构策略：** 作者决定将高层的记忆过程解构为最基础的、不可再分的单元。\n**原子操作定义：** 借鉴计算机科学中的经典概念，定义了 **CRUD（Create, Read, Update, Delete）** 四种原子操作。\n**逻辑闭环：** 通过这四个原子操作，任何复杂的记忆策略都可以被组合出来。智能体不再执行固定的“记忆步骤”，而是在每一步从这四个原子动作中选择一个或多个来执行。这构建了一个完整的、可学习的动作空间。\n\n### 5. 优化策略：从模仿到强化学习\n**训练挑战：** 既然是决策问题，如何让模型学会最优的策略？\n**两阶段推演：**\n1.  **初始化（SFT）：** 首先通过监督微调，让模型学会如何使用这些原子操作的 API 格式，建立基本的操作规范。\n2.  **策略优化（RL）：** 仅仅学会格式是不够的，模型需要学会“策略”。由于记忆的效果往往体现在长周期任务的最终结果上，作者引入**强化学习（RL）**。\n**目标设定：** 将任务的成功率作为奖励信号，让模型在不断的试错中，自动学习出在特定任务下最优的记忆操作序列（例如：何时该多读，何时该多写）。\n\n### 6. 验证与洞察：涌现出的结构化策略\n**结果反思：** 实验结果不仅验证了性能提升，更重要的是揭示了模型学到了什么。\n**动态分析：** 作者观察到，经过 RL 训练后，模型的行为发生了系统性变化——从早期的过度依赖“读取”，转变为更频繁地使用“创建”、“更新”和“删除”。\n**结论升华：** 这证明了 AtomMem 并没有死记硬背规则，而是**发现了一种与任务对齐的结构化记忆管理策略**。这种“学会如何记忆”的能力，正是对传统“硬编码记忆”的根本性超越。\n\n---\n\n**总结：**\n作者的思考路径是从**批判现状（静态记忆的僵化）**出发，通过**类比迁移（工具学习的决策思想）**提出核心假设，进而利用**原子化解构（CRUD操作）**实现方法论落地，最后通过**强化学习**完成策略的内化与优化。这一过程体现了从“规则驱动”向“数据与奖励驱动”的范式转变。"
                },
                {
                    "title": "OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System",
                    "arxiv_id": "2601.08288",
                    "authors": "Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li",
                    "summary": "Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于 AutoGen 的多智能体系统，通过编排多个专业智能体进行协作、迭代规划和工具使用（RAG）来生成单口喜剧，符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决中文单口喜剧生成中文化根基缺失、时机控制难及长篇连贯性不足的问题。针对用户提供的主题，我们提出了一种基于AutoGen的多智能体系统OpenMic，通过多轮迭代协作优化幽默结构与表演性，并结合RAG与QLoRA微调技术。我们在LLM-as-a-Judge评估框架下，通过Persona Fidelity、Humor Mechanics等指标验证了其有效性。",
                    "summary_translation": "Chinese stand-up comedy generation (中文单口喜剧生成) 不仅超越了 plain text generation (纯文本生成)，还要求具备 culturally grounded humor (扎根于文化的幽默)、precise timing (精准的时机)、stage-performance cues (舞台表演提示) 以及 implicit multi-step reasoning (隐式多步推理)。此外，常用的 Chinese humor datasets (中文幽默数据集) 通常更适用于 humor understanding and evaluation (幽默理解与评估)，而非 long-form stand-up generation (长篇单口喜剧生成)，这使得 direct supervision (直接监督) 与目标任务存在错位。为了应对这些挑战，我们提出了 OpenMic，这是一个构建在 AutoGen 之上的 end-to-end multi-agent system (端到端多智能体系统)，能够将 user-provided life topic (用户提供的生活话题) 转化为 3-5 分钟的中文单口喜剧表演，并进一步生成 narrated comedy video (旁白喜剧视频)。OpenMic 在 multi-round iterative loop-planning (多轮迭代循环规划) 中编排多个 specialized agents (专用智能体)，以联合优化幽默感、时机和 performability (可表演性)。为了缓解 dataset-task mismatch (数据集-任务不匹配)，我们利用 retrieval-augmented generation (RAG) 增强生成过程，以实现 material grounding (素材落地) 和 idea expansion (思路扩展)；此外，我们 fine-tune (微调) 了一个专门的 JokeWriter 模型，以更好地内化单口喜剧特有的 setup-punchline structures (铺垫-笑点结构) 和 long-range callbacks (长篇幅前后呼应)。",
                    "inspiration_trace": "基于论文《OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System》的内容，以下是对作者核心方法论提出过程的逻辑链推演：\n\n### 1. 宏观观察与问题定义：从“文本生成”到“表演艺术”的鸿沟\n*   **观察起点**：现有的AI在文本、图像等静态内容生成上已表现优异，但在“表演性创造力”（如脱口秀）上仍显乏力。\n*   **核心痛点**：脱口秀不仅仅是“写得好”，它是一种紧密编排的艺术，包含语言技巧、时间控制和社会语境感知。\n*   **现状诊断**：现有的通用大模型（如GPT-5.2、DeepSeek）在尝试生成脱口秀时，往往容易陷入“说教”模式或笑点稀疏且质量不均。这表明单纯的文本流畅度并不等同于幽默感。\n\n### 2. 深度归因：幽默的本质是“认知推理”而非“语言续写”\n*   **理论重构**：作者跳出语言学的视角，将幽默重新定义为一种**结构化的认知推理**。\n*   **机制分析**：幽默产生于“预期违背”与“合理化解决”之间（Incongruity-Resolution）。成功的笑话往往包含多步推理链（如“后门”或“前门”推理模式），需要精确控制信息的释放节奏。\n*   **LLM的局限性**：大模型的“下一个词预测”机制倾向于消除歧义、平滑过渡，这恰恰破坏了幽默所需的“悬念”和“反转”。模型倾向于过早解释笑点，导致喜剧张力崩塌。\n\n### 3. 数据困境：监督信号的错位\n*   **数据现状**：现有的中文幽默数据集（如CFunSet）多用于“理解”任务（分类、解释）或短文本生成。\n*   **任务错位**：脱口秀需要长篇连贯的叙事、延迟的笑点、前后呼应以及舞台提示。这些特征在现有的短文本监督数据中是缺失的。\n*   **假设提出**：直接使用现有数据微调模型难以达到目标，必须引入新的机制来弥补数据与任务之间的鸿沟。\n\n### 4. 方法论转折：从“单模型生成”转向“生产流水线”\n*   **类比启发**：作者意识到，生成一段优秀的脱口秀，更像是一个人类团队的协作过程（策划、写作、表演指导、审核），而不是单次性的文本补全。\n*   **架构决策**：为了解决单一模型难以同时兼顾内容策划、观众适应度、表演性和质量保证的问题，作者决定采用**多智能体系统**。\n*   **逻辑优势**：通过将复杂任务分解为专门的子任务，每个Agent专注于特定的优化目标（如结构、节奏、安全性），从而实现对生成过程的精细化控制。\n\n### 5. 技术落地：解决“创意”与“控制”的矛盾\n*   **创意来源（RAG）**：为了解决模型幻觉和素材匮乏问题，引入检索增强生成（RAG）。但这不仅是简单的检索，而是设计了一个“三元组内部对话”机制，通过筛选和精炼，将外部素材转化为高质量的写作灵感，避免噪音干扰。\n*   **结构内化**：为了弥补通用模型在“铺垫-笑点”结构上的不足，作者对专门的JokeWriter进行了QLoRA微调，使其内部化脱口秀特有的长程依赖和口语风格。\n*   **迭代优化**：为了解决“时机”和“节奏”难以一次成型的问题，设计了一个多轮迭代循环。通过引入“质量控制器”作为把关人，对脚本进行针对性的修改（如重写笑点、调整停顿），而不是全盘推翻。\n\n### 6. 最终形态：端到端的表演输出\n*   **输出升级**：脱口秀的最终形式是舞台表演，而非纯文本。因此，作者将目标设定为生成包含表演标记（停顿、重音、掌声）的结构化脚本。\n*   **闭环验证**：为了验证系统的实用性，进一步将脚本转化为视频，完成从“用户话题”到“成片”的端到端验证。\n\n---\n\n**总结：**\n作者的思考路径是从**现象**（AI不懂幽默）出发，深入到**本质**（幽默是认知推理），发现**工具缺陷**（现有数据与模型机制不匹配），最终通过**架构创新**（多智能体协作模拟人类生产流程）并结合**具体技术手段**（RAG提供素材、微调提供结构、迭代提供打磨），构建出一个能够处理长篇、多维度、表演性任务的系统。"
                },
                {
                    "title": "Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant",
                    "arxiv_id": "2601.08333",
                    "authors": "Oleg Romanchuk, Roman Bondar",
                    "summary": "LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究基于LLM的智能体架构，分析了工具使用中的“工具边界”和“架构可信接口”问题，以及智能体的自我许可机制，属于单智能体架构与机制的理论研究。",
                    "summary2": "本文旨在解决AI Agent架构中因混淆信息传输与认识论保证而导致的语义清洗问题。针对现有Agent框架将LLM生成的命题错误地视为观察结果的现象，我们提出了必然自我许可定理和保证侵蚀原则，并在ReAct及多智能体架构中通过形式化证明与代码示例验证了循环证明的不可避免性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Semantic Laundering in AI Agent Architectures》，以下是对作者产出该核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观理论构建的思考过程：\n\n---\n\n### 1. 宏观观察：AI Agent 架构中的“信任盲区”\n**思考起点：**\n作者首先审视了当前主流的 AI Agent 架构（如 ReAct, LangChain, ToolOrchestra）。这些架构遵循一个通用的模式：LLM 生成意图 -> 调用工具 -> 接收结果 -> 基于结果继续推理。\n\n**核心疑点：**\n作者注意到一个被普遍忽视的现象：在代码实现层面，所有工具的输出都被统一标记为“Observation”（观察）。\n*   **观察：** 无论是查询数据库（获取客观事实），还是调用另一个 LLM（生成假设），架构都将它们视为同等可信的输入喂回给主模型。\n*   **问题：** 这种设计混淆了“信息传输机制”（数据是如何到达的）与“认识论辩护机制”（为什么应该相信它）。\n\n### 2. 现象抽象：从“代码缺陷”到“认识论谬误”\n**概念提炼：**\n为了解释上述疑点，作者引入了哲学概念进行类比。\n*   **Gettier 问题：** 在哲学中，一个被辩护的真信念（JTB）如果其辩护过程与真理之间缺乏真实的因果联系，则不能算作知识。\n*   **映射到 AI：** 当一个 LLM 生成的假设（弱保证）通过“工具调用”这个动作，被系统包装成“观察结果”（强保证）时，这就构成了一个架构层面的 Gettier 问题。\n\n**提出核心概念——“语义清洗”：**\n作者将这一过程形式化定义为“语义清洗”：\n*   **定义：** 一个缺乏或仅有弱保证的命题，仅仅因为跨越了一个被架构信任的边界（如 Tool Interface），就被系统接受为具有高认识论地位的观察，且中间没有发生任何认识论上相关的推理步骤。\n\n### 3. 机制剖析：为何保证会流失？\n**深入思考：**\n为什么工具边界不能赋予保证？作者进一步区分了两种系统状态：\n*   **因果过渡：** 系统状态的改变（如函数调用返回数据）。这是架构层面的，物理发生的。\n*   **认识论过渡：** 命题保证的增加（如引入了新的外部事实或验证逻辑）。\n\n**提出原则——“保证侵蚀原则”：**\n作者指出，解释性或生成性的操作（如 LLM 推理）默认**不保留**认识论保证。因为它们切断了命题与“真理制造者”之间的直接联系。\n*   **推论：** 仅仅提高生成质量（让 LLM 说话更严谨）并不能增加认识论保证，因为观察集 $O$ 没有变，推理规则 $I$ 也没有变。\n\n### 4. 逻辑推演：证明架构的必然失败\n**形式化证明：**\n为了证明这不是偶然的 bug，而是结构的必然，作者构建了“不可避免的自我许可定理”。\n\n*   **假设前提（现状）：**\n    1.  Agent 和工具输出属于同一命题类型 $P$（架构不区分来源）。\n    2.  工具调用结果被无条件接受为观察 $O$。\n    3.  LLM 生成的命题可以影响自身的认识论状态分配。\n*   **逻辑链条：**\n    1.  LLM 1 生成命题 $p_1$。\n    2.  调用工具（内部封装了 LLM 2），生成 $p_2$。\n    3.  架构将 $p_2$ 视为观察 $O$。\n    4.  $p_2$ 被用来验证 $p_1$。\n    5.  **结论：** $p_1$ 的认识论状态是由同类型的命题 $p_2$ 决定的，而 $p_2$ 本质上也是生成的。这构成了循环论证。\n\n**关键洞察：**\n这种自我许可是架构决定的，而非模型能力不足。即使换成完美的 GPT-N，只要它还是生成命题而非观测世界，循环就无法打破。\n\n### 5. 解决方案：引入“认识论类型系统”\n**最终方法论：**\n既然问题出在“类型混淆”，解决之道必然是“类型区分”。\n\n*   **工具分类：** 作者提出必须对工具进行认识论分类，而非功能分类：\n    *   **OBSERVER（观察者）：** 数据库、传感器。输出可直接作为观察。\n    *   **COMPUTATION（计算者）：** 数学运算、排序。基于确定性规则转换观察。\n    *   **GENERATOR（生成者）：** LLM、概率模型。输出仅为命题，**绝不能**直接进入观察集 $O$。\n\n*   **架构修正：**\n    *   拒绝“基于通道的保证”（即：因为是工具调用的，所以可信）。\n    *   采用“基于内容的保证”（即：因为来源是 OBSERVER，所以可信）。\n    *   在架构层面强制阻断 GENERATOR 工具的输出直接提升命题的认识论地位。\n\n---\n\n### 总结：作者的思维演进路径\n1.  **观察：** 发现现有 Agent 架构将“数据库查询”和“LLM 生成”混为一谈，都视为“观察”。\n2.  **定性：** 利用 Gettier 问题，指出这是一种“语义清洗”——通过架构边界虚假地提升了命题的可信度。\n3.  **归因：** 提出“保证侵蚀原则”，说明生成过程本身无法保证真值，必须依赖外部观察。\n4.  **证明：** 通过“自我许可定理”，从逻辑上证明了在当前架构假设下，循环论证是不可避免的。\n5.  **解法：** 提出“认识论类型系统”，强制区分 OBSERVER 和 GENERATOR，从类型层面杜绝语义清洗。"
                },
                {
                    "title": "Greedy Is Enough: Sparse Action Discovery in Agentic LLMs",
                    "arxiv_id": "2601.08280",
                    "authors": "Angshul Majumdar",
                    "summary": "Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states. We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states. Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究Agentic LLMs在极大动作空间（如工具/API）中的决策问题，提出了稀疏动作发现和剪枝方法，属于智能体的工具使用和决策制定范畴，不涉及排除项。",
                    "summary2": "本文旨在解决Agentic LLMs在超大动作空间中的稀疏动作发现问题。针对具有结构化稀疏性的大规模动作空间，我们提出了一种受Orthogonal Matching Pursuit (OMP) 启发的贪婪算法Contextual Block-OMP。我们在理论分析中通过样本复杂度和支持集恢复概率验证了其有效性，证明了该算法能以对数级样本复杂度精确识别相关动作集。",
                    "summary_translation": "现代 agentic systems (智能体系统) 在具有极大 action spaces (动作空间) 的环境中运行，例如拥有数千个可用 APIs (应用程序接口) 或 retrieval operations (检索操作) 的 tool-augmented language models (工具增强型语言模型)。尽管规模如此之大，empirical evidence (经验证据) 表明，在给定的 deployment (部署) 中，只有一小部分动作会对性能产生有意义的影响。受此观察启发，我们研究了一种 contextual linear reward model (上下文线性奖励模型)，其中 action relevance (动作相关性) 由 structured sparsity assumption (结构化稀疏性假设) 支配：即只有少量动作在 latent states (潜在状态) 上具有非零效应。我们将 action discovery (动作发现) 表述为一个 block-sparse recovery problem (块稀疏恢复问题)，并分析了一种受 Orthogonal Matching Pursuit (正交匹配追踪) 启发的 greedy algorithm (贪婪算法)。在关于 incoherence (非相干性)、signal strength (信号强度) 和 action coverage (动作覆盖) 的标准假设下，我们证明了该 greedy procedure (贪婪过程) 能以高概率精确恢复 relevant action set (相关动作集)，其所需的样本数量在稀疏度和 latent dimension (潜在维度) 上呈 polynomially (多项式级) 缩放，而在动作总数上仅呈 logarithmically (对数级) 缩放。我们进一步提供了 refitted parameters (重拟合参数) 的 estimation error guarantees (估计误差保证)，并表明所得 decision rule (决策规则) 对于新的 latent states (潜在状态) 是 near-optimal (近似最优) 的。作为对这些结果的补充，我们建立了 information-theoretic lower bounds (信息论下界)，证明了稀疏性和充分的 coverage (覆盖) 对于 tractability (易处理性) 是必要的。综上所述，我们的结果将 sparse action discovery (稀疏动作发现) 确定为 large-action decision-making (大规模动作决策) 的一个基本原则，并为 agentic systems (智能体系统) 中的 action pruning (动作剪枝) 提供了理论基础。",
                    "inspiration_trace": "基于论文《Greedy Is Enough: Sparse Action Discovery in Agentic LLMs》，以下是对作者产出该文章核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察与问题抽象：从“工具爆炸”到“有效稀疏性”\n*   **现象观察**：现代智能体系统（如工具增强的LLM）面临着一个极端的挑战——可用的动作空间（API、工具、文档）极其庞大（数以万计）。然而，作者敏锐地捕捉到一个关键的**经验性反差**：尽管名义上的动作空间巨大，但在任何特定的任务部署中，真正被频繁使用且对性能有显著影响的动作往往只是极小的一个子集。\n*   **问题重构**：作者意识到，智能体决策的核心难点不在于语言推理本身，而在于**极高维度的动作空间控制**。如果大多数动作是无关的，那么问题的本质就从“如何在海量动作中寻找最优解”转变为“如何从海量动作中快速识别出那少数几个相关的动作”。\n*   **逻辑跃迁**：将智能体的工具使用问题，抽象为一个**大规模离散动作空间下的稀疏决策问题**。\n\n### 2. 核心假设与数学建模：引入“状态依赖的稀疏性”\n*   **模型简化**：为了剥离LLM内部复杂的语言机制，专注于控制问题，作者假设存在一个给定的低维**潜在状态**（Latent State, $z_t$），它封装了上下文信息。\n*   **关键假设**：作者提出了**状态依赖的稀疏性**假设。即：虽然不同状态下最优的动作可能不同，但在整个任务分布中，能够产生非零奖励的动作集合（即“支持集”）是非常小的（$k \\ll M$）。\n*   **数学形式化**：构建了一个**上下文线性奖励模型**（$r_t = \\langle W^*_{a_t}, z_t \\rangle + \\epsilon_t$）。在这个模型中，参数矩阵 $W^*$ 的绝大多数行都是零向量，只有 $k$ 行是非零的。这成功地将动作发现问题转化为了一个**块稀疏恢复**问题。\n\n### 3. 方法论选择：为什么是“贪婪算法”？\n*   **算法映射**：面对块稀疏恢复问题，学术界通常有两条路径：凸松弛（如Lasso）和贪婪算法。作者选择了后者，特别是受**正交匹配追踪（OMP）**启发的算法。\n*   **选择逻辑**：\n    1.  **直观性**：贪婪算法符合人类和智能体的直觉——先尝试最可能有效的动作，看效果，再尝试下一个。\n    2.  **计算效率**：在动作空间 $M$ 极大时，凸优化可能面临高昂的计算成本，而贪婪算法通过逐步迭代，计算复杂度更低。\n    3.  **即时性**：贪婪算法是“Anytime”的，即在任何迭代步骤停止都能给出一个合理的解，非常适合对延迟敏感的智能体系统。\n*   **算法设计**：提出了**上下文块OMP**。算法的核心逻辑是：在每一步，计算当前残差与所有动作特征的关联度，选出关联度最高的动作加入支持集，然后重新拟合参数并更新残差。\n\n### 4. 理论验证与边界确立：证明“贪婪足够”且“稀疏必要”\n*   **上界证明（可行性）**：作者需要证明这种简单的贪婪方法确实能找到正确的动作集。通过标准的非相干性和信号强度假设，证明了该算法能以高概率精确恢复支持集。更重要的是，**样本复杂度**仅与动作总数的对数（$\\log M$）相关，这意味着该方法可以扩展到数百万级别的动作空间。\n*   **下界证明（必要性）**：为了说明稀疏性假设不是可有可无的，作者通过信息论下界证明：如果没有稀疏性假设，任何算法想要识别有效动作，其样本复杂度至少与动作总数 $M$ 呈线性关系。这在理论上确立了**稀疏性是解决大规模动作空间问题的唯一可行路径**。\n\n### 5. 回归与应用：为经验主义提供理论基石\n*   **解释现实**：最后，作者将理论拉回现实。该理论解释了为什么现有的工程实践（如启发式剪枝、静态路由）在实践中往往有效——因为它们无意中利用了这种潜在的稀疏结构。\n*   **分离关注点**：文章最终将智能体系统解耦为两个正交的问题：**表征学习**（LLM如何生成 $z_t$）和**稀疏控制**（如何在给定 $z_t$ 下发现动作）。本文为后者提供了坚实的理论基础。\n\n---\n\n**总结**：\n作者的思考路径是从**工程痛点**（动作太多）出发，通过**现象洞察**（实际用的很少）提炼出**数学假设**（稀疏性），进而**映射**到经典的信号处理算法（OMP），最后通过**严格的理论分析**确立了该方法的优越性与必要性，从而为智能体系统的动作选择问题提供了一个从直觉到理论闭环的解决方案。"
                },
                {
                    "title": "ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web",
                    "arxiv_id": "2601.08276",
                    "authors": "Zhiyuan Yao, Zishan Xu, Yifu Guo, Zhiguang Han, Cheng Yang, Shuo Zhang, Weinan Zhang, Xingshan Zeng, Weiwen Liu",
                    "summary": "With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了 ToolACE-MCP，旨在解决智能体在大规模生态系统中的工具路由问题，属于单智能体的“工具使用”范畴。同时，摘要明确提到该方法可泛化到“多智能体协作”，符合多智能体的研究范围。",
                    "summary2": "本文旨在解决Agent Web生态系统中工具与Agent大规模扩展下的精确导航问题。针对海量且动态的候选空间，我们提出了一种名为ToolACE-MCP的框架，通过基于图的自进化变异与多智能体轨迹合成来训练历史感知路由器。我们在MCP-Universe、MCP-Mark及Agent Route Benchmark上通过准确率验证了其有效性，结果表明该方法在工具选择与多智能体协作任务中均展现出优越的性能、鲁棒性及跨域泛化能力。",
                    "summary_translation": "随着 Agent Web (代理网络) 和 Model Context Protocol (MCP) (模型上下文协议) 的兴起，智能体生态系统正在演变为一个开放协作网络，可用工具呈指数级增长。然而，当前的架构面临着严重的 scalability (可扩展性) 和 generality (通用性) 瓶颈。为了解决这一问题，我们提出了 ToolACE-MCP，这是一个用于训练 history-aware routers (具有历史感知能力的路由器) 的流水线，旨在赋能大规模生态系统中的精准导航。通过利用 dependency-rich candidate Graph (富含依赖关系的候选图) 来合成 multi-turn trajectories (多轮轨迹)，我们有效地训练了具有 dynamic context understanding (动态上下文理解) 能力的路由器，从而创建了即插即用的 Light Routing Agent (轻量级路由智能体)。在真实世界基准 MCP-Universe 和 MCP-Mark 上的实验展示了优越的性能。值得注意的是，ToolACE-MCP 展示了未来 Agent Web 的关键特性：它不仅能够以 minimal adaptation (最小适配) 泛化到 multi-agent collaboration (多智能体协作)，而且在面对噪声时保持了 exceptional robustness (卓越的鲁棒性)，并能有效地扩展到 massive candidate spaces (海量候选空间)。这些发现为开放式生态系统中的 universal orchestration (通用编排) 提供了强有力的实证基础。",
                    "inspiration_trace": "基于论文《ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n---\n\n### 第一阶段：宏观观察与痛点识别（从“单体”到“网络”的范式危机）\n\n**1. 观察趋势：**\n作者首先敏锐地捕捉到了Agent生态系统的根本性变化——从封闭、单体、预定义工具集的系统，正在转向开放的“Agent Web”和基于MCP（Model Context Protocol）的协作网络。这意味着可访问的工具数量将从几十个激增至数百万个。\n\n**2. 识别瓶颈：**\n面对这种指数级增长，作者发现现有架构存在两个致命缺陷：\n*   **静态注入的不可扩展性：** 传统的将所有工具描述塞入Prompt的方法，受限于上下文窗口，无法承载海量工具。\n*   **检索匹配的局限性：** 现有的基于Embedding的检索方法（图1b），本质上是一种“静态语义匹配”。它只能看当前的Query和工具描述像不像，却忽略了Agent在多轮对话中的**状态**。\n\n**3. 核心矛盾提炼：**\n真正的任务往往是多轮的。下一步该调用哪个工具，不仅取决于用户说了什么，更取决于**之前发生了什么**（历史结果、错误反馈、工具依赖）。现有的“无状态”检索器无法处理这种动态依赖。\n\n---\n\n### 第二阶段：核心假设与概念提出（从“匹配”到“路由”的思维跃迁）\n\n**1. 提出假设：**\n要解决上述矛盾，必须引入一个专门的**“路由器”**。这个路由器不应仅仅是一个检索器，而应该是一个具备推理能力的决策者。它需要理解“历史上下文”，从而在巨大的候选空间中精准导航。\n\n**2. 面临的挑战（数据稀缺）：**\n有了假设，现实问题随之而来：**我们没有训练数据**。在真实的MCP生态中，不存在海量的、标注了“在特定历史轨迹下应选择哪个工具”的数据集。现有的数据集规模太小，且缺乏多轮交互的复杂性。\n\n---\n\n### 第三阶段：方法论构建（解决“数据荒”与“难区分”的双重难题）\n\n为了训练这个理想的路由器，作者设计了一套数据合成的流水线，其逻辑演进分为两步：\n\n**1. 构建“难区分”的候选空间（图扩展与自进化变异）：**\n*   **思考：** 如果候选工具都很简单（比如“计算器”和“日历”），路由器学不到真本事。真正的挑战在于区分功能相似但细微差别的工具（Hard Negatives）。\n*   **手段：** 作者构建了一个**候选图**，并引入**“自进化变异”**机制。\n*   **逻辑：** 利用LLM对现有工具进行“基因突变”（如参数重设计、工作流串联、功能增强）。这不仅能扩充数据量，更重要的是人为制造了大量“语义极其接近但功能不同”的干扰项。这迫使路由器必须学习细粒度的特征区分，而不是简单的语义匹配。\n\n**2. 注入“历史感知”的监督信号（多智能体轨迹合成）：**\n*   **思考：** 有了工具，还需要“历史”。如何生成包含多轮依赖的高质量对话轨迹？\n*   **手段：** 采用**多智能体模拟**。\n*   **逻辑：**\n    *   先在候选图上随机游走，选取一组有逻辑关联的工具子集。\n    *   让一个“规划者”Agent设计任务蓝图。\n    *   让“用户”和“助手”Agent根据蓝图进行多轮对话，模拟真实的工具调用过程。\n*   **关键点：** 在这个模拟轨迹中，作者将“历史对话”作为输入，将“当前调用的工具”作为标签。这样，就自动生成了海量的、富含历史上下文的训练数据，教会路由器“在之前的步骤做完后，下一步该做什么”。\n\n---\n\n### 第四阶段：架构落地与解耦（轻量化路由智能体）\n\n**1. 设计理念：**\n训练好了路由器，如何把它用起来？如果每次调用路由器还要把所有工具描述再传一遍，又回到了上下文爆炸的老路。\n\n**2. 解决方案：Light Routing Agent (LRA)：**\n作者提出了一个**解耦**的设计。LRA只持有两个工具：\n*   `router_invocation`：询问路由器“我现在该选谁？”（只需传历史和Query，不需要传工具库）。\n*   `execute`：执行路由器返回的工具ID。\n*   **逻辑演进：** 这将“路由逻辑”与“具体工具定义”彻底剥离。路由器作为一个外部大脑，通过ID索引来指挥庞大的工具库，从而实现了在无限工具空间下的轻量化运行。\n\n---\n\n### 第五阶段：终极验证与愿景（从工具路由到Agent编排）\n\n**1. 跨域泛化的洞察：**\n在实验阶段，作者发现了一个有趣的现象：在工具数据上训练的路由器，竟然能直接用于**Agent的选择**（即把不同的Agent作为候选节点）。\n\n**2. 逻辑升华：**\n这揭示了本文的核心贡献——**“通用编排”**。无论是选工具，还是选Agent，本质都是“能力匹配”。路由器学到的是一种抽象的决策逻辑：根据当前任务状态和历史，寻找最合适的“能力节点”。\n\n**3. 结论：**\nToolACE-MCP不仅仅是一个工具选择器，它是未来Agent Web中实现“按需组队”的基础设施。\n\n---\n\n**总结：作者的思考路径**\n1.  **发现问题：** 工具爆炸导致静态方法失效，且缺乏历史感知。\n2.  **设定目标：** 训练一个懂历史、能在大规模空间中精准决策的路由器。\n3.  **攻克难点：** 利用“自进化变异”制造难例，利用“多智能体模拟”生成带历史轨迹的训练数据。\n4.  **工程落地：** 通过解耦设计，让路由器轻量化地接入现有流程。\n5.  **价值升华：** 证明该方法具有从工具泛化到Agent的通用性，为未来的开放生态提供了解决方案。"
                },
                {
                    "title": "Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces",
                    "arxiv_id": "2601.08271",
                    "authors": "Angshul Majumdar",
                    "summary": "Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究工具增强型LLM系统（Agentic LLMs）在大型动作空间中的顺序决策与工具使用问题，属于单智能体范畴中的工具使用与规划理论分析，不涉及排除的纯应用、纯推理或基础设施优化等内容。",
                    "summary2": "本文旨在解决工具增强型 LLM 在大规模动作空间中的控制不稳定与样本效率问题。针对仅有少量工具相关的稀疏控制场景，我们提出了一种基于 $\\ell_{1,2}$ 正则化的稀疏智能体控制（SAC）框架，并在理论分析中通过估计误差、支持集恢复及样本复杂度等指标，验证了其相对于密集策略类在多项式时间内的稳定性与有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces》，以下是对作者产出该文章核心逻辑链的系统性推演：\n\n### 1. 宏观背景与核心痛点：从“推理”到“控制”的维度灾难\n**思考起点：** 作者首先观察到LLM正在从单纯的文本生成器转变为能够调用外部工具（API、数据库、文档）的智能体。\n**发现问题：** 在这一转变中，传统的控制理论和强化学习（RL）假设失效了。现实中的工具数量 $M$（动作空间）极其巨大（$10^4$ 到 $10^6$），而经典理论通常假设动作集较小。\n**现有困境：** 目前的工程实践（如Prompt工程、硬编码的工作流）缺乏理论保证，且随着工具库 $M$ 的扩大，系统表现出明显的不稳定性。作者意识到，核心瓶颈不在于LLM的“推理能力”，而在于如何在海量动作中进行有效的“控制”。\n\n### 2. 现象观察与关键假设：稀疏性的普适规律\n**观察现象：** 尽管工具库 $M$ 巨大，但在任何特定的任务分布下，只有极小一部分工具会被反复调用，绝大多数工具从未被使用。\n**提出假设：** 作者将这一经验观察抽象为“稀疏性假设”。即，在一个包含 $M$ 个动作的宇宙中，真正影响奖励的“活跃动作集”大小 $k$ 远小于 $M$（$k \\ll M$）。\n**类比迁移：** 作者意识到这与高维统计学和压缩感知中的“稀疏恢复”问题高度同构。这为解决维度灾难提供了理论抓手：如果信号是稀疏的，我们就不需要遍历所有 $M$ 个动作。\n\n### 3. 理论重构：将智能体控制转化为高维统计问题\n**逻辑转折：** 作者决定跳出传统的“贝尔曼方程”或“动态规划”框架，转而将工具选择问题建模为一个高维统计估计问题。\n**定义新范式：** 提出了“稀疏智能体控制（SAC）”框架。在这个框架下，策略被参数化为对每个工具的打分，而学习的目标是从海量数据中恢复出那个稀疏的参数向量。\n**关键思考：** 如果不利用稀疏性，任何算法在最坏情况下都需要 $\\Omega(M)$ 的样本复杂度（即线性依赖工具数量），这在海量工具下是不可行的。因此，**稀疏性不仅是锦上添花，而是多项式时间内实现稳定性的必要条件**。\n\n### 4. 方法论构建：引入 $\\ell_{1,2}$ 正则化\n**寻找工具：** 为了在数学上强制实现稀疏性，作者借鉴了Lasso（Least Absolute Shrinkage and Selection Operator）的思想。\n**具体化方法：** 提出了 $\\ell_{1,2}$-正则化的策略学习器。\n*   **$\\ell_1$ 部分**：促使参数向量变为稀疏（自动选择工具）。\n*   **$\\ell_2$ 部分**：针对每个工具的参数向量进行分组（允许每个工具有多维特征）。\n**凸优化优势：** 选择凸代理损失函数，保证了问题可以在多项式时间内求解，避免了非凸RL优化的陷阱。\n\n### 5. 理论验证：从“估计”到“支撑集恢复”的闭环\n**逻辑验证：** 作者需要证明这套方法不仅数学上优美，而且确实有效。这需要回答三个递进的问题：\n1.  **估计误差：** 能否以较少的样本 $T$ 准确估计出策略参数？（结论：误差尺度为 $\\sqrt{k \\log M / T}$，实现了对 $M$ 的对数依赖）。\n2.  **支撑集恢复：** 能否准确找出那 $k$ 个有用的工具？（结论：通过原始-对偶见证论证，在样本量 $T \\gtrsim k \\log M$ 时可精确识别）。\n3.  **控制性能：** 参数估计的误差如何转化为最终任务价值的损失？（结论：在价值敏感性条件下，估计误差线性映射为价值次优性）。\n\n### 6. 视角升华：解耦LLM与控制逻辑\n**处理现实复杂性：** 现实世界是部分可观测的（POMDP），LLM在其中扮演什么角色？\n**定位LLM：** 作者将LLM重新定义为一个“信念/状态压缩器”。LLM的作用是将高维的历史观测压缩为低维的上下文向量。\n**误差分离：** 作者证明了系统的总误差可以解耦为两部分：\n*   **统计误差：** 由稀疏控制学习引起（取决于 $k \\log M$）。\n*   **表示误差：** 由LLM对状态的近似不准确引起（取决于 $\\epsilon_b$）。\n**最终结论：** 只要LLM的表示误差可控，稀疏控制逻辑就能保证系统在海量动作空间下的稳定性。\n\n---\n\n**总结：**\n作者的思考路径是从**工程痛点**（动作空间过大导致不稳定）出发，通过**现象抽象**（发现工具使用的稀疏性），引入**跨学科理论**（高维统计与压缩感知），构建**核心算法**（$\\ell_{1,2}$ 正则化），最后通过**严格的理论证明**（上界与下界对比）确立了“稀疏性是解决大规模动作空间控制问题的唯一数学出路”这一核心论断。"
                },
                {
                    "title": "The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination",
                    "arxiv_id": "2601.08237",
                    "authors": "Haoran Su, Yandong Sun, Congjia Yu",
                    "summary": "Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.",
                    "category": "cs.AI",
                    "filter_reason": "该论文探讨了LLMs如何通过基于语言的目标规范取代手工设计的奖励函数，从而重新定义多智能体协调。它涉及多智能体系统中的语义奖励规范、动态适应以及协作机制，符合“多智能体：协作”的研究范围。",
                    "summary2": "本文旨在解决多智能体强化学习中手工设计奖励函数的瓶颈问题。针对多智能体协作场景，我们提出了一种基于大语言模型（LLM）的自然语言目标范式，利用语义奖励规范、动态适应和内在人类对齐来替代传统数值奖励工程。我们在 Multi-Agent MuJoCo 和 SMARTS 等基准上通过任务成功率、协调效率和设计时间等指标验证了该范式的有效性。",
                    "summary_translation": "Reward engineering (奖励工程)，即手动指定 reward functions (奖励函数) 以诱导 agent (智能体) 表现出期望的行为，仍然是 multi-agent reinforcement learning (多智能体强化学习) 中的一个基本挑战。这种困难因 credit assignment ambiguity (信用分配模糊性)、environmental non-stationarity (环境非平稳性) 以及 interaction complexity (交互复杂性) 的 combinatorial growth (组合式增长) 而进一步加剧。我们认为，large language models (大语言模型) 的最新进展表明，正从 hand-crafted numerical rewards (手工设计的数值奖励) 向 language-based objective specifications (基于语言的目标规范) 转变。先前的研究表明，LLM 可以直接从 natural language descriptions (自然语言描述) synthesize (合成) reward functions (奖励函数)（例如 EUREKA），并在 minimal human intervention (最少人工干预) 的情况下 adapt reward formulations online (在线调整奖励公式)（例如 CARD）。与此同时，Reinforcement Learning from Verifiable Rewards (可验证奖励强化学习) 这一新兴范式提供了 empirical evidence (经验证据)，证明 language-mediated supervision (语言介导的监督) 可以作为传统 reward engineering (奖励工程) 的 viable alternative (可行替代方案)。我们从三个维度 conceptualize (概念化) 这一转变：semantic reward specification (语义奖励规范)、dynamic reward adaptation (动态奖励适应) 以及 improved alignment with human intent (与人类意图的改进对齐)；同时，我们也指出了与 computational overhead (计算开销)、robustness to hallucination (对幻觉的鲁棒性) 以及 scalability to large multi-agent systems (大规模多智能体系统的可扩展性) 相关的 open challenges (开放性挑战)。最后，我们概述了一个研究方向，即 coordination (协调) 源于 shared semantic representations (共享语义表示)，而非 explicitly engineered numerical signals (显式设计的数值信号)。",
                    "inspiration_trace": "基于论文《The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination》，以下是对作者产出该文章核心思想的逻辑链推演：\n\n### 第一阶段：宏观观察与痛点识别（从现象到本质）\n\n**1. 观察现象：多智能体系统的“奖励工程”危机**\n作者首先从经典的机器人协作场景（如仓库家具组装）切入，观察到在单智能体强化学习（RL）中尚可管理的“奖励工程”，在多智能体系统（MARL）中演变成了不可逾越的瓶颈。\n*   **现象**：工程师需要花费数周时间手动调整奖励权重（如捡起螺丝的分数、碰撞的惩罚），结果却往往被智能体“利用漏洞”，导致行为崩溃。\n*   **本质问题**：这不仅仅是参数调优的困难，而是**“意图翻译”的失败**。人类意图是丰富且模糊的（如“高效协作”），而数值奖励函数是精确且贫瘠的。将前者转化为后者的过程，本质上是信息的有损压缩。\n\n**2. 深度诊断：传统范式的结构性缺陷**\n作者进一步剖析了为什么传统方法无法解决这一问题，指出了多智能体环境的三个核心死结：\n*   **信用分配模糊**：团队成功时，很难量化每个个体的具体贡献。\n*   **环境非平稳性**：智能体A的策略改变会导致智能体B的环境失效，静态奖励函数无法适应这种动态博弈。\n*   **指数级复杂性**：随着智能体数量增加，联合状态空间爆炸，手工设计奖励变得不可能。\n\n**逻辑推演结论**：现有的MARL算法（如CTDE、价值分解）只是在“修补”奖励函数带来的后果，而没有解决“如何设计奖励”这一根本问题。作者意识到，**必须跳出“手工设计数值奖励”的旧范式**。\n\n---\n\n### 第二阶段：范式转移的契机（从问题到假设）\n\n**3. 引入变量：大语言模型（LLM）的语义能力**\n作者将目光转向了LLM，并敏锐地捕捉到了LLM的一个关键特性：**它是连接人类自然语言与机器代码的桥梁**。\n*   **关键洞察**：既然人类习惯用语言描述目标（“避免碰撞，提高效率”），而LLM具备强大的代码生成和语义理解能力，那么**能否让LLM代替人类来完成“意图到奖励函数”的翻译工作？**\n\n**4. 提出核心假设：从“数值信号”到“语言目标”**\n基于上述洞察，作者提出了一个颠覆性的假设：\n*   **假设**：多智能体协调不应依赖于人工设计的数值信号，而应基于**自然语言目标**。智能体通过语义理解来协调，而非通过优化标量奖励。\n*   **验证依据**：作者引用了EUREKA（LLM生成超越人类水平的奖励）、CARD（LLM动态调整奖励）和RLVR（基于可验证语言奖励的训练）等最新工作，证明这一假设在技术上已具备可行性。\n\n---\n\n### 第三阶段：理论框架构建（从假设到方法论）\n\n**5. 提炼三大支柱：为什么语言目标更优？**\n为了论证新范式的优越性，作者将LLM带来的优势抽象为三个相互支撑的逻辑支柱，构建了完整的理论框架：\n\n*   **支柱一：语义奖励规范**\n    *   *逻辑*：语言比数学更能完整保留人类意图。例如，“协作”包含任务分配、避让、容错等多层含义，这是简单的 $r = w_1 \\cdot x + w_2 \\cdot y$ 无法表达的。LLM利用其世界知识，能生成更符合直觉的奖励。\n\n*   **支柱二：动态适应**\n    *   *逻辑*：传统奖励是静态的，无法应对非平稳环境。而LLM可以作为一个“观察者-评论家”，实时分析智能体的行为轨迹，并用自然语言反馈，进而自动修改奖励代码。这解决了环境动态变化带来的奖励失效问题。\n\n*   **支柱三：默认的人类对齐**\n    *   *逻辑*：语言目标是可解释的。人类可以直接审查目标描述（“最小化碰撞”），而不需要去理解复杂的权重向量。此外，RLVR（如DeepSeek-R1）证明，基于语言目标的训练能自发涌现出推理能力，这意味着语言目标本身就隐含了对齐机制。\n\n---\n\n### 第四阶段：路径规划与未来展望（从理论到愿景）\n\n**6. 定义实施路径：两条技术路线**\n作者明确了实现这一愿景的具体路径，避免概念混淆：\n*   **路径一（离线生成）**：LLM负责生成奖励代码，智能体使用传统MARL算法训练。这适用于对实时性要求高的场景。\n*   **路径二（在线交互）**：LLM直接作为智能体的大脑或通信媒介，智能体通过自然语言进行协商和协调。这适用于需要复杂推理的场景。\n\n**7. 最终愿景：语义协调**\n作者将思考推向终极形态：\n*   **短期**：人机协作设计奖励。\n*   **中期**：全自然语言交互的端到端协调。\n*   **长期**：**语义协调**。智能体不再需要显式的语言或数值信号，而是基于共享的世界模型和语义理解，像人类老搭档一样实现心照不宣的默契配合。\n\n---\n\n### 总结：作者的思维闭环\n\n1.  **起点**：发现手工设计奖励在多智能体系统中的根本性缺陷（有损翻译、静态僵化）。\n2.  **转折**：利用LLM的语义理解和代码生成能力，提出用“语言目标”替代“数值奖励”。\n3.  **论证**：通过“语义规范、动态适应、人类对齐”三大支柱，从理论上证明新范式在解决非平稳性、信用分配和可解释性方面的优势。\n4.  **展望**：描绘了一个从依赖数值信号进化到依赖语义理解的未来多智能体世界。\n\n这篇文章的核心思想演进，是从**解决工程痛点**出发，通过**引入新工具（LLM）**，最终上升到**重新定义智能体交互本质**的范式革命。"
                },
                {
                    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
                    "arxiv_id": "2601.08173",
                    "authors": "Daocheng Fu, Jianbiao Mei, Rong Wu, Xuemeng Yang, Jia Xu, Ding Wang, Pinlong Cai, Yong Liu, Licheng Wen, Botian Shi",
                    "summary": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个针对智能体的动态评估环境，重点研究了智能体在动态环境下的任务调度（规划）、主动探索（工具使用/交互）以及持续学习（自我演化），符合单智能体和自我演化的研究范围。",
                    "summary2": "本文旨在解决现有MLLM智能体在动态现实场景中鲁棒性不足的问题。针对流式任务调度、不确定性下的主动探索及持续学习挑战，我们提出了Trainee-Bench，一个模拟实习生探索新设置的动态评估基准。我们在包含50个动态场景的Trainee-Bench上，通过Success Rate和Checkpoint Score等指标验证了其有效性，揭示了SOTA智能体在动态环境下的显著缺陷。",
                    "summary_translation": "多模态大语言模型 (Multi-modal Large Language Models, MLLMs) 的快速发展推动了工作流自动化 (workflow automation)；然而，现有研究主要针对静态环境 (static environments) 下的性能上限 (performance upper bounds)，忽视了在随机现实世界部署 (stochastic real-world deployment) 中的鲁棒性 (robustness)。我们识别出三个关键挑战：动态任务调度 (dynamic task scheduling)、不确定性下的主动探索 (active exploration under uncertainty) 以及基于经验的持续学习 (continuous learning from experience)。为弥合这一差距，我们提出了 \\method{}，这是一个动态评估环境 (dynamic evaluation environment)，模拟了一个“受训者”智能体 (“trainee” agent) 持续探索新颖场景的过程。与传统基准不同，\\method{} 从三个维度对智能体进行评估：(1) 针对不同优先级流式任务 (streaming tasks) 的上下文感知调度 (context-aware scheduling)；(2) 通过主动探索 (active exploration) 减少幻觉 (hallucination) 的审慎信息获取；(3) 通过从基于规则 (rule-based) 的动态生成任务中提炼通用策略 (generalized strategies) 来实现持续演化。实验结果表明，尖端智能体 (cutting-edge agents) 在动态环境 (dynamic environments) 中存在显著不足，尤其是在主动探索 (active exploration) 和持续学习 (continual learning) 方面。我们的工作建立了一个评估智能体可靠性的框架，将评估重点从静态测试 (static tests) 转移到了现实的、面向生产的场景 (production-oriented scenarios)。我们的代码可在 https://github.com/KnowledgeXLab/EvoEnv 获取。",
                    "inspiration_trace": "基于论文《The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios》的内容，以下是对作者产出该核心方法（Trainee-Bench）的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**从“静态完美”到“动态鲁棒”的范式反思**\n\n1.  **观察现状**：\n    *   作者首先观察到多模态大语言模型（MLLMs）在自动化工作流方面取得了显著进展，现有的Agent系统在工具使用和静态任务执行上已经达到了很高的性能上限。\n    *   **核心矛盾**：现有的研究范式主要集中在“静态、受控”的实验室环境，追求的是性能的“天花板”。然而，现实世界的部署环境是“随机、动态”的，充满了噪声和不确定性。\n\n2.  **提出宏观问题**：\n    *   当Agent从实验室走向生产环境时，其鲁棒性如何？\n    *   现有的评估体系是否真实反映了Agent在复杂现实中的生存能力？\n\n---\n\n### 第二阶段：差距分析与挑战解构\n**识别“实验室”与“职场”之间的鸿沟**\n\n1.  **场景具象化**：\n    *   作者将抽象的“生产环境”具象化为“职场新员工（实习生）”的第一天。这是一个充满未知、多任务并行且需要快速学习的场景。\n\n2.  **解构核心挑战（三个维度）**：\n    *   **挑战一：动态调度**。现实不是单线程的，任务是流式输入的，且带有截止日期。Agent需要具备处理多任务冲突、上下文切换和时间管理的能力，而不仅仅是解决孤立问题。\n    *   **挑战二：主动探索**。现实世界不是全知全能的。Agent在初始状态下往往信息不足（部分可观测性）。作者指出，Agent不应在不确定时“幻觉”出动作，而应具备“审慎”的品质，主动去寻找线索或寻求帮助。\n    *   **挑战三：持续进化**。现实不是一次性测试。Agent必须能够从过去的经验中学习，避免在类似的后续任务中重复犯错。\n\n3.  **批判现有基准**：\n    *   作者对比了GAIA、TheAgentCompany等现有基准，指出它们普遍缺乏“动态配置”、“部分可观测性”和“检查点反馈”机制，无法评估上述三个关键能力。\n\n---\n\n### 第三阶段：假设形成与概念构建\n**提出“Trainee-Bench”的评估范式**\n\n1.  **核心假设**：\n    *   要评估Agent的真实能力，必须构建一个能够模拟“动态性”和“不确定性”的测试环境。\n    *   一个优秀的Agent应当像一个聪明的实习生：能合理安排时间（调度），不懂就问（探索），并且每天都在进步（学习）。\n\n2.  **概念定义**：\n    *   **Trainee-Bench**：一个模拟实习生持续探索新环境的动态评估基准。\n    *   **设计哲学**：从“静态测试”转向“生产导向”的随机场景测试。\n\n---\n\n### 第四阶段：方法论设计与逻辑实现\n**如何构建一个“动态且不可知”的测试场？**\n\n为了验证假设，作者在方法论上进行了三层逻辑递进的设计：\n\n1.  **解决“过拟合”与“静态”问题：规则驱动的元任务**\n    *   **思考**：如果使用固定的数据集，Agent很容易通过死记硬背来通过测试。为了测试泛化能力，必须让任务“动”起来。\n    *   **方法**：提出“元任务”概念。不写死具体的任务，而是定义任务的逻辑规则（$R$）。\n    *   **机制**：通过随机种子生成不同的环境变量（NPC画像、数据分布），将规则实例化为无数个具体的任务。这迫使Agent必须学习逻辑规则，而不是记忆特定答案。\n\n2.  **解决“幻觉”与“被动”问题：强制部分可观测性**\n    *   **思考**：如何迫使Agent去“探索”而不是直接瞎猜？\n    *   **方法**：引入“信息差”。在任务描述中故意隐藏关键线索（如密码、手册位置），只提供目标。\n    *   **机制**：Agent必须主动与环境交互（如查阅文件、询问NPC）来获取这些“潜伏线索”，否则任务无法完成。这直接量化了Agent的主动探索能力。\n\n3.  **解决“多任务”与“进化”问题：动态复合场景与反馈循环**\n    *   **思考**：如何模拟职场的混乱和时间压力？\n    *   **方法**：将多个元任务组合成“流式场景”，并引入时间截止和优先级抢占。\n    *   **机制**：设计“Day 1”和“Day 2”的循环。Day 1结束后，环境提供基于检查点的具体反馈，观察Agent在Day 2面对同类但参数不同的任务时，能否利用反馈提升表现。\n\n---\n\n### 第五阶段：实验验证与反思\n**揭示SOTA模型的“软肋”**\n\n1.  **验证逻辑**：\n    *   作者通过实验回答了三个RQ（研究问题），旨在验证当前Agent在Trainee-Bench定义的三个维度上的真实表现。\n\n2.  **关键发现与逻辑闭环**：\n    *   **发现一**：SOTA模型在简单任务上表现尚可，但在高负载（多任务）和困难任务上性能急剧下降。证实了“动态调度”和“复杂推理”是当前瓶颈。\n    *   **发现二**：在持续学习实验中，Agent利用经验后，整体性能反而下降（尤其是在简单任务上）。\n        *   **思考**：这说明Agent目前并不具备真正的“经验蒸馏”能力，反而容易被错误或过时的经验误导。\n    *   **发现三**：对比“自我进化”与“人类指导”，人类提供的少量提示能将性能从0.24提升至0.83，而自我进化仅提升0.04。\n        *   **深层洞察**：这揭示了当前Agent最大的短板在于**自主探索策略的缺失**，而非单纯的推理能力不足。\n\n---\n\n### 总结：作者的思维演进路径\n\n1.  **起点**：发现现有Agent评估脱离实际，过于关注静态性能。\n2.  **聚焦**：锁定职场场景，提炼出调度、探索、学习三大现实挑战。\n3.  **构建**：设计Trainee-Bench，利用“规则+随机化”解决静态问题，利用“隐藏线索”强制探索，利用“多日循环”测试学习能力。\n4.  **验证**：通过实验证明当前Agent在动态环境下的脆弱性，特别是缺乏有效的主动探索和经验利用机制。\n5.  **结论**：呼吁学术界从优化单一技能转向提升Agent在不确定性下的鲁棒性和探索能力。"
                },
                {
                    "title": "Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions",
                    "arxiv_id": "2601.08156",
                    "authors": "Arin Gopalan Yadav, Varad Dherange, Kumar Shivam",
                    "summary": "This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个分层多智能体框架，涉及任务分解（规划）、多智能体协作以及混合记忆机制，完全符合多智能体及单智能体（规划、记忆）的研究范围。虽然应用于物流场景，但其核心贡献在于智能体架构的设计，不属于纯应用排除项。",
                    "summary2": "本文旨在解决 Last-Mile Delivery (LMD) 中断的自主管理问题。针对复杂的实时中断场景，我们提出了一种集成了 Hybrid Memory Architecture 的 Hierarchical Multi-Agent Framework (Project Synapse)。在包含 30 个复杂场景的 benchmark 数据集上，通过 LLM-as-a-Judge 协议验证了其有效性，总体平均分达 0.73。",
                    "summary_translation": "本文介绍了Project Synapse，一种旨在自主解决末端配送中断的新型agentic framework（智能体框架）。Synapse采用hierarchical multi-agent architecture（分层多智能体架构），其中核心的Resolution Supervisor agent（解决主管智能体）负责strategic task decomposition（战略任务分解），并将子任务委派给负责tactical execution（战术执行）的specialized worker agents（专业化工作智能体）。该系统利用LangGraph进行编排，以管理复杂且循环的workflows（工作流）。为验证该框架，研究基于对超过6,000条真实用户评论的定性分析，构建了一个包含30个复杂disruption scenarios（中断场景）的benchmark dataset（基准数据集）。系统性能采用LLM-as-a-Judge协议（大模型评判协议）进行评估，并实施了显式的bias mitigation（偏差缓解）。",
                    "inspiration_trace": "基于论文《Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions》，以下是对作者核心方法论产出逻辑链的系统性推演：\n\n### 1. 宏观观察：确定性系统的失效\n**逻辑起点：** 作者首先观察到“超级应用”生态中，最后一公里配送（LMD）是运营效率的关键瓶颈。\n**核心矛盾：** 现实中的物流中断（如交通拥堵、商家失联、包裹损坏）具有高度的**随机性**和**非结构化**特征。然而，现有的自动化系统主要依赖**基于规则的脚本**（Rule-based scripts）。\n**推论：** 这种“刚性规则”与“动态现实”之间的错位，导致了系统在面对非标准问题时束手无策。数据分析显示，“支持失败”是用户投诉的首要来源（29.7%），这证明了现有系统缺乏处理复杂、未见过的突发状况的能力。\n\n### 2. 范式转移：从“指令执行”到“智能体推理”\n**假设提出：** 要解决上述问题，系统不能只是执行预设指令，而必须具备**推理**和**适应**能力。\n**技术选型：** 大语言模型（LLM）的出现提供了这种可能性。作者意识到，LLM不仅是文本生成器，更是决策引擎。\n**新挑战：** 然而，单一的LLM智能体在面对极其复杂的物流场景时，存在上下文窗口限制、工具调用混乱以及无法同时处理多维度任务（物流、沟通、仲裁）的局限性。这就像让一个人同时做司机、客服和法官，效率低下且容易出错。\n\n### 3. 架构演进：引入“层级化”分工\n**灵感来源：** 作者将目光转向人类高效组织的运作模式——即**管理层与执行层的分离**。\n**逻辑推演：** 既然单一智能体能力有限，那么构建一个“AI劳动力”是合理的。\n*   **战略层：** 需要一个“主管”负责理解全局、拆解复杂任务、制定计划。\n*   **战术层：** 需要一组“专家”负责具体执行，如物流代理负责路线，沟通代理负责通知，仲裁代理负责退款。\n**结论：** 这种**层级化多智能体架构（HMAS）**能够实现任务的模块化分解，让每个Agent专注于特定领域，从而提高整体系统的鲁棒性和可扩展性。\n\n### 4. 认知升级：解决“失忆”与“幻觉”\n**问题深化：** 有了分工，Agent还需要“知识”和“经验”。单纯的LLM推理存在两个致命缺陷：\n1.  **缺乏上下文状态：** 忘记之前的操作步骤。\n2.  **缺乏事实依据：** 可能编造公司政策或忽视历史教训。\n**解决方案：** 作者引入了认知科学中的**混合记忆架构**，模仿人类大脑的记忆机制：\n*   **工作记忆：** 记住当前的对话和计划状态（短期）。\n*   **情景记忆：** 记住过去发生过的类似事件及其解决方案（经验）。\n*   **语义记忆：** 存储公司政策、规则等事实性知识（知识库）。\n**逻辑闭环：** 通过这种架构，Agent的每一次决策都不仅基于逻辑推理，还基于历史经验和既定规则，实现了“有状态”和“事实 grounded”的智能。\n\n### 5. 流程重构：应对非线性现实\n**动态性考量：** 物流中断的解决过程往往不是线性的。例如，重新规划路线后可能又遇到新堵车，或者客户拒绝了赔偿方案。\n**方法论调整：** 传统的线性工作流无法处理这种循环和回退。作者因此采用了**有向条件图**来建模工作流。\n**核心思想：** 允许系统包含循环和条件分支。如果某个步骤失败，系统可以自动触发“重新规划”机制，回到主管节点重新制定策略，直到问题解决或触发人工介入。\n\n### 6. 验证闭环：从“模拟”到“真实”\n**评估困境：** 如何证明这个系统真的有效？传统的NLP指标（如BLEU）无法衡量推理质量，而人工评估成本太高。\n**方法论创新：** 作者采用了**LLM-as-a-Judge**（LLM作为裁判）的评估范式，并特别引入了**偏差缓解策略**（使用不同家族的模型作为裁判，避免自嗨）。\n**数据来源：** 为了确保测试的真实性，作者没有使用合成数据，而是从6000多条真实用户评论中提炼出30个复杂场景。这确保了研究具有**生态效度**，即解决的是真实世界的痛点。\n\n---\n\n**总结：**\n作者的思考路径是从**发现刚性系统在动态环境下的失效**开始，经过**引入LLM智能体**，为了克服单一智能体的局限而**进化为层级化分工**，为了确保决策的准确性和合规性而**植入混合记忆系统**，最后通过**图工作流**和**基于真实数据的严格评估**，构建出了一个完整的、具备类人认知能力的AI运营框架。"
                },
                {
                    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
                    "arxiv_id": "2601.08118",
                    "authors": "Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli",
                    "summary": "Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive \"act-as-a-user\" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.",
                    "category": "cs.AI",
                    "filter_reason": "论文关注的是“User-Proxy Agents”（用户代理），这属于LLM智能体的一种特定类型。论文提出了一个评估框架（MirrorBench）来衡量这些智能体在对话任务中模拟人类行为的能力，属于智能体评估的研究范畴。虽然涉及框架构建，但其核心目的是评估智能体本身的能力，而非单纯的模型部署优化或基础设施开发，且不属于排除的应用、推理、安全或多模态领域。",
                    "summary2": "本文旨在解决缺乏严格评估用户代理人类相似性的问题。针对用户代理在对话中常产生不真实话语的场景，我们提出了一种名为MirrorBench的可扩展基准测试框架，该框架具备模块化执行引擎、元数据注册表和多后端支持。我们在QULAC、ClariQ、OASST1和ChatbotArena四个数据集上，通过MATTR、Yule's K、HD-D等词汇多样性指标及GTEval、PI、RNR等LLM评判指标，验证了其能有效量化用户代理与真实人类之间的系统性差距。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于对论文《MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题识别\n**（从“模拟用户”的兴起到“拟人化”的缺失）**\n\n1.  **现象观察**：\n    *   随着LLM的发展，业界开始广泛使用“用户代理”来模拟人类行为，用于训练对话系统或评估AI助手。\n    *   然而，作者发现一个普遍痛点：简单的“扮演用户”提示往往产生冗长、过度配合、不切实际的回复。这种“机器味”很重的模拟数据会污染训练集或误导评估结果。\n\n2.  **核心矛盾提炼**：\n    *   **现有评估的错位**：目前的评估体系大多关注“AI助手”回答得好不好，而忽略了“模拟用户”本身像不像人。\n    *   **任务与风格的混淆**：一个模拟用户可能把任务完成了（比如成功订了票），但它的说话方式可能完全不像真人（过于正式或啰嗦）。现有的指标无法将“任务成功率”与“行为拟人度”解耦。\n\n3.  **提出宏观问题**：\n    *   *如何独立于下游任务的成功与否，纯粹地、量化地衡量一个用户代理的“拟人化”程度？*\n\n---\n\n### 第二阶段：概念定义与假设构建\n**（确立“镜像”评估范式）**\n\n1.  **定义评估对象**：\n    *   作者将焦点严格锁定在**对话中的用户一侧**，而非助手一侧。\n    *   **假设**：如果一个用户代理生成的对话，在风格、语气和多样性上与真实人类对话无法区分，那么它就是高质量的。\n\n2.  **构建“镜像”策略**：\n    *   为了衡量“像不像”，必须有一个参照系。作者提出利用现有的真实人类对话数据集作为“镜子”。\n    *   **逻辑推演**：让用户代理在相同的语境下（基于真实对话的目标和上下文）生成回复，然后将其生成的“合成对话”与真实的“人类对话”进行对比。\n\n3.  **解耦评估维度**：\n    *   作者意识到“拟人”是一个多维概念，单一指标无法覆盖。因此，将评估维度拆解为：\n        *   **表层特征**：词汇多样性（是否像人一样用词丰富且有重复模式？）。\n        *   **深层行为**：行为真实感（语气、风格、自然度）。\n\n---\n\n### 第三阶段：方法论设计\n**（从“单一指标”到“双重校验体系”）**\n\n1.  **指标体系的设计逻辑**：\n    *   **客观指标（词汇层面）**：引入MATTR、Yule’s K等经典语言学指标。这些指标不依赖主观判断，能快速捕捉统计特征上的偏差（如AI是否倾向于使用更罕见或更重复的词汇）。\n    *   **主观指标（语义层面）**：利用更强的LLM作为“裁判”。因为拟人感涉及语感和微妙的语境，只有强模型能理解这种细微差别。\n    *   **创新点：校准机制**：作者意识到LLM裁判本身有偏见（如偏好长文本）。因此，引入了**Human-Human (HH)** 和 **Proxy-Proxy (PP)** 对照组。\n        *   *逻辑*：如果裁判连两个真人的对话都分不出高下（HH），说明裁判够准；如果裁判给代理自己跟自己的对话打高分（PP），说明有自嗨倾向。通过这两个锚点，将原始分数校准到可解释的区间。\n\n2.  **数据驱动的标准化**：\n    *   为了消除不同数据集带来的偏差，作者提出“以人类为锚点”的标准化方法。即计算代理指标与该数据集人类平均指标的偏离度，而非直接看绝对值。\n\n---\n\n### 第四阶段：工程化与系统架构\n**（从“一次性实验”到“可扩展基准”）**\n\n1.  **工程痛点分析**：\n    *   评估用户代理涉及多个变量：不同的代理模型、不同的数据集、不同的裁判模型、不同的随机种子。\n    *   如果没有系统化支撑，实验将难以复现，且无法进行大规模对比。\n\n2.  **架构设计哲学**：\n    *   **模块化与解耦**：作者设计了一个六层架构，将“执行后端”、“核心引擎”、“编排层”、“插件组件”等分离。\n    *   **逻辑意图**：让研究者可以像搭积木一样，随意替换代理、数据集或指标，而无需重写代码。这确保了框架的**可扩展性**。\n\n3.  **严谨性保障**：\n    *   引入**Manifest（清单）**机制：将实验配置（包括随机种子、模型版本）固化，确保结果可复现。\n    *   引入**方差感知**：通过多次采样计算置信区间，避免因单次随机性得出错误结论。\n\n---\n\n### 第五阶段：实证与验证\n**（通过反差验证框架的有效性）**\n\n1.  **实验设计的逻辑**：\n    *   选取四个风格迥异的数据集（从闲聊到信息检索），以测试框架的通用性。\n    *   选取当前最强的几个LLM（GPT-4o, Claude, Gemini等）作为被测对象。\n\n2.  **结果解读与发现**：\n    *   **验证了假设**：实验揭示了不同模型在不同任务上的“拟人化”短板（例如在澄清任务中，所有模型的词汇多样性都低于人类）。\n    *   **发现了张力**：高“裁判评分”并不总是意味着高“词汇多样性”，证明了同时使用两类指标的必要性。\n\n---\n\n### 总结：作者的思想演进脉络\n\n1.  **起点**：发现LLM作为用户模拟器存在“不像人”的问题，且缺乏评估标准。\n2.  **转折**：决定将评估对象从“助手”转移到“用户”，并将“任务成功”与“行为拟人”剥离。\n3.  **深化**：认识到拟人感包含统计特征和语义特征，因此构建了“客观语言学指标 + 主观LLM裁判（带校准）”的双重评估体系。\n4.  **落地**：为了支持长期、大规模、可复现的研究，构建了一个高度模块化、可插拔的系统架构。\n5.  **验证**：通过多数据集、多模型的实验，不仅量化了现有模型的差距，也证明了该框架在揭示这些差距方面的有效性。"
                },
                {
                    "title": "MemRec: Collaborative Memory-Augmented Agentic Recommender System",
                    "arxiv_id": "2601.08816",
                    "authors": "Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Clark Mingxuan Ju, Tong Zhao, Neil Shah, Li Chen, Yongfeng Zhang",
                    "summary": "The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个智能体框架，重点研究了智能体的核心组件“记忆”以及“自我演化”（通过异步图传播演化记忆）。它探讨了推理与记忆管理的解耦，符合单智能体（记忆）和自我演化的研究范围。尽管应用场景是推荐系统，但其核心贡献在于智能体的架构机制，而非单纯的领域应用。",
                    "summary2": "总结生成失败",
                    "summary_translation": "推荐系统的演变已将偏好存储从 rating matrices (评分矩阵) 和 dense embeddings (稠密嵌入) 转移到了 agentic era (智能体时代) 的 semantic memory (语义记忆)。然而，现有的 agents (智能体) 依赖于 isolated memory (孤立记忆)，忽略了关键的 collaborative signals (协同信号)。弥合这一差距面临着双重挑战：既要从庞大的 graph contexts (图上下文) 中提炼信息，又不能让 reasoning agents (推理智能体) 承受过大的 cognitive load (认知负荷)，同时还要高效地演化 collaborative memory (协同记忆)，而不产生令人望而却步的 computational costs (计算成本)。为了解决这一问题，我们提出了 MemRec，这是一个在架构上将 reasoning (推理) 与 memory management (记忆管理) 解耦的 framework (框架)，以实现高效的 collaborative augmentation (协同增强)。MemRec 引入了一个专用的、具有 cost-effective (成本效益高) 的 LM_Mem 来管理 dynamic collaborative memory graph (动态协同记忆图)，为下游的 LLM_Rec 提供合成的、high-signal context (高信号上下文)。该 framework (框架) 通过一个实用的 pipeline (流水线) 运行，具备 efficient retrieval (高效检索) 和 cost-effective (成本效益高) 的 asynchronous graph propagation (异步图传播) 功能，能够在后台演化记忆。在四个 benchmarks (基准测试) 上进行的广泛实验表明，MemRec 实现了 state-of-the-art performance (最先进性能)。此外，architectural analysis (架构分析) 证实了其灵活性，建立了一个新的 Pareto frontier (帕累托前沿)，通过支持包括 local open-source models (本地开源模型) 在内的多样化部署，在 reasoning quality (推理质量)、cost (成本) 和 privacy (隐私) 之间取得了平衡。\n\n代码：https://github.com/rutgerswiselab/memrec 和主页：https://memrec.weixinchen.com",
                    "inspiration_trace": "基于对论文《MemRec: Collaborative Memory-Augmented Agentic Recommender System》的深度解析，以下是作者产出该核心方法的逻辑推演过程：\n\n### 1. 宏观观察：推荐系统的范式演进与缺失\n**逻辑起点：** 作者首先回顾了推荐系统记忆机制的演变史：从传统的**稀疏评分矩阵**（协同过滤时代），到**稠密隐向量**（深度学习时代），再到当前的**语义记忆**（智能体时代）。\n\n**核心洞察：** 在大语言模型（LLM）驱动的智能体推荐系统中，虽然引入了语义记忆使得推理更加自然和复杂，但现有的智能体普遍存在**“孤岛效应”**。它们只关注单个用户或物品的独立记忆，完全忽略了传统推荐系统中最重要的**全局协同信号**。\n\n**问题提出：** 如何在智能体范式中，重新引入并利用“协同过滤”的高阶连接能力，以捕捉社区趋势和发现潜在兴趣？\n\n### 2. 核心冲突：引入协同信号的“双重困境”\n**初步设想：** 最直观的解决方案似乎是直接将图上的邻居信息注入到智能体的上下文中。\n\n**逻辑推演与否定：** 作者通过分析发现，这种“暴力”方案在实际落地中面临两个不可逾越的障碍：\n*   **认知过载：** LLM的上下文窗口有限且推理能力受噪声影响。直接将海量的邻居原始记忆丢给推理模型，会导致信息过载，模型无法从噪声中提取关键信息，反而降低推荐质量。\n*   **计算瓶颈：** 推荐系统是高频交互系统。如果每次用户交互后，都要同步更新所有相关邻居的记忆，计算成本将呈指数级增长，导致系统不可用。\n\n**结论：** 必须找到一种方法，既能获取协同信号的“红利”，又能避免其带来的“副作用”（过载与高成本）。\n\n### 3. 关键假设：架构解耦是破局关键\n**思维转折：** 作者意识到，问题的根源在于试图让同一个LLM同时承担“复杂推理”和“海量记忆管理”两项截然不同的任务。\n\n**核心假设：** 如果将**推理**与**记忆管理**在架构上解耦，让专门的模型负责处理图结构，而让推理模型专注于决策，就能解决上述冲突。\n\n*   **类比思考：** 就像公司中，CEO（推理智能体）不需要亲自管理档案室（记忆图），而是需要一位首席信息官（记忆管理器）来整理和提炼信息。\n\n### 4. 方法论构建：如何实现“解耦”与“提纯”\n基于架构解耦的假设，作者进一步细化了如何具体实现这两个模块，以解决前述的“双重困境”。\n\n#### 4.1 解决“认知过载”：从“检索”到“提炼”\n**逻辑演进：** 既然不能给推理模型看原始数据，那就必须先进行“压缩”。\n*   **第一步：筛选。** 传统的图剪枝方法（如随机游走）缺乏语义理解，而训练神经网络成本太高。作者提出利用LLM本身作为**规则生成器**，在离线阶段根据领域统计特征生成筛选规则。这既保留了语义理解能力，又保证了在线推理的高效性。\n*   **第二步：合成。** 借鉴**信息瓶颈理论**，不仅要筛选，还要最大化保留与任务相关的信息。作者设计了一个专门的记忆管理器（$LM_{Mem}$），将筛选后的邻居记忆提炼成结构化的“偏好面”，将长文本压缩为高信噪比的短文本。\n\n#### 4.2 解决“计算瓶颈”：从“同步”到“异步”\n**逻辑演进：** 既然实时更新所有邻居不可行，那就改变更新的时序。\n*   **灵感来源：** 借鉴标签传播算法。\n*   **策略：** 将记忆更新过程从主交互循环中剥离。当用户发生交互时，系统只做必要的即时响应，而将复杂的邻居记忆更新放在后台异步执行。\n*   **优化：** 将原本需要对每个邻居进行的多次LLM调用，合并为一次批处理调用，将复杂度从线性降低到常数级（$O(1)$）。\n\n### 5. 逻辑闭环：从理论到实践的验证\n**最终框架：** MemRec 由此诞生——一个双LLM架构。\n*   **$LM_{Mem}$（记忆管理器）：** 负责脏活累活（读图、筛选、压缩、异步更新）。\n*   **$LLM_{Rec}$（推理智能体）：** 负责基于精炼后的上下文进行高质量推荐。\n\n**验证逻辑：**\n*   **有效性：** 实验证明，引入协同信号后，推荐效果显著优于仅依赖孤立记忆的基线模型。\n*   **必要性：** 消融实验证明，如果没有“解耦”和“提炼”，直接输入原始上下文会导致性能下降（验证了认知过载假设）。\n*   **经济性：** 成本分析表明，这种架构在性能和成本之间建立了新的帕累托前沿。\n\n---\n\n**总结：**\n作者的思考路径是从**范式演进**中发现协同信号的缺失，在**尝试引入**时遭遇认知与计算的双重瓶颈，最终通过**架构解耦**这一核心思想，结合**信息瓶颈理论**和**异步传播机制**，成功将传统推荐系统的“协同智慧”无缝融入了LLM智能体之中。"
                },
                {
                    "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback",
                    "arxiv_id": "2601.08734",
                    "authors": "Prithwish Jana, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot",
                    "summary": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个结合LLM与形式化验证工具的神经符号框架，利用验证器提供的反馈进行迭代自我修正和强化学习，这符合“单智能体”中的工具使用、自我反思以及“自我演化”中通过反馈自我完善的研究范围。虽然应用于基础设施代码生成，但其核心贡献在于智能体的反馈循环机制，而非单纯的应用或基础设施部署优化。",
                    "summary2": "本文旨在解决LLM在生成和修改Infrastructure-as-Code (IaC) 时配置不正确的问题。针对Terraform配置的生成与变异任务，我们提出了一种名为TerraFormer的神经符号框架，该方法结合了监督微调与基于形式化验证器反馈的强化学习。我们在IaC-Eval、TF-Gen (Test)和TF-Mutn (Test)数据集上，通过Correctness、Deployability和Security Compliance等指标验证了其有效性。",
                    "summary_translation": "实现基础设施即代码的自动化充满挑战，大型语言模型在从自然语言生成配置时经常出现错误。我们提出了 TerraFormer，这是一个用于基础设施即代码生成与变异的神经符号框架。该框架结合了监督微调与验证器引导的强化学习，并利用形式化验证工具针对语法、可部署性和策略合规性提供反馈。我们通过多阶段验证和迭代式大型语言模型自我修正，构建了两个大规模、高质量的自然语言到基础设施即代码数据集：TF-Gen（包含 152k 个实例）和 TF-Mutn（包含 52k 个实例）。针对 17 个最先进的大型语言模型（包括规模大 50 倍左右的 Sonnet 3.7、DeepSeek-R1 和 GPT-4.1）的评估结果表明，相较于其基础大型语言模型，TerraFormer 在 IaC-Eval 上的正确性提升了 15.94%，在 TF-Gen (Test) 上提升了 11.65%，在 TF-Mutn (Test) 上提升了 19.60%。在 TF-Gen (Test) 和 TF-Mutn (Test) 上，TerraFormer 的表现均优于规模更大的模型；在 IaC-Eval 上排名第三，并在最佳实践和安全合规性方面取得了顶尖成绩。",
                    "inspiration_trace": "基于论文《TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback》，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n---\n\n### 1. 宏观观察：云原生时代的“自动化悖论”\n**起点：** 云计算已成为现代IT的基石，基础设施即代码（IaC，如Terraform）是标准实践。\n**现象：** 尽管大语言模型（LLM）在通用代码生成上表现卓越，但在IaC领域却举步维艰。\n**矛盾：** 一方面，编写IaC需要深厚的领域知识（API、依赖关系），人工编写耗时且易出错；另一方面，直接将通用LLM应用于IaC生成，往往产生无法部署或存在安全漏洞的代码。\n\n### 2. 问题聚焦：为何通用LLM在IaC领域失效？\n作者深入分析发现，阻碍LLM在IaC领域落地的核心痛点并非模型规模不够大，而是**“数据与反馈”的双重缺失**：\n\n*   **数据稀缺与噪声：** 真实的IaC配置涉及企业隐私，公开数据集极少且质量参差不齐（包含大量错误配置）。LLM缺乏高质量的“教科书”进行学习。\n*   **形式化约束的挑战：** IaC（如HCL语言）是严格的形式化语言，对语法和语义一致性要求极高。LLM的随机性导致“幻觉”频发，生成的代码往往通不过编译或部署。\n*   **评估的黑盒：** 传统的代码评估指标（如BLEU）无法衡量代码是否“可部署”。真实的云部署测试成本高昂且缓慢，导致缺乏有效的反馈机制来指导模型改进。\n\n### 3. 核心假设：从“文本模仿”转向“符号验证”\n**思考转折：** 既然单纯依靠LLM的“概率预测”无法保证形式化语言的正确性，那么必须引入外部的“确定性”力量。\n**假设：** 如果能将**形式化验证工具**（如Terraform Validate/Plan, OPA Policy）作为“老师”，为LLM提供精确的反馈，就能解决幻觉和不可部署的问题。\n**方法论雏形：** 构建一个**神经符号系统**——LLM负责生成和推理，符号验证器负责裁决和纠错。\n\n### 4. 策略构建一：如何解决“无高质量数据”的问题？\n**挑战：** 人工编写15万+条高质量的IaC数据既昂贵又不现实。\n**创新思路：** 利用“错误”本身。GitHub上有大量未经验证的IaC代码，虽然包含错误，但蕴含了真实的模式。\n**逻辑推演：**\n1.  **收集：** 抓取开源的原始IaC代码（脏数据）。\n2.  **验证与修复：** 设计一个**多轮自修复循环**。将代码扔给验证器，如果报错，将错误信息反馈给LLM，让LLM尝试修复。\n3.  **迭代：** 重复上述过程，直到代码通过所有验证（语法、可部署性、策略合规）。\n4.  **结果：** 将原本错误的代码转化为高质量的“教科书”，并自动生成对应的自然语言描述和策略测试用例。从而构建出大规模、高质量的TF-Gen（生成）和TF-Mutn（变异）数据集。\n\n### 5. 策略构建二：如何让模型超越“死记硬背”？\n**思考：** 仅仅在清洗后的数据上做监督微调（SFT），模型只是在模仿正确的模式，可能仍未真正理解“为什么”这样写是对的，且容易过拟合。\n**进阶思路：** 引入强化学习（RL），让模型在生成过程中学会“自我约束”。\n**逻辑推演：**\n1.  **奖励函数设计：** 不再使用模糊的文本相似度，而是直接使用验证器的输出作为奖励信号。\n2.  **细粒度反馈：** 奖励不是简单的0或1，而是分层级的：\n    *   语法正确 -> 基础分\n    *   可部署 -> 中级分\n    *   符合安全策略 -> 高级分\n3.  **训练目标：** 通过GRPO等算法，鼓励模型生成能通过验证器检查的代码，而不仅仅是像训练数据的代码。这使得模型在面对未见过的复杂需求时，依然能保持正确性。\n\n### 6. 最终合成：TerraFormer 框架的诞生\n**逻辑闭环：**\n*   **输入端：** 利用“验证器引导的修复循环”从海量脏数据中提炼出黄金数据集。\n*   **模型端：** 先通过SFT学习IaC的基本语法和模式（热启动）。\n*   **优化端：** 再通过RL利用验证器的反馈进行精调，强制模型遵守形式化约束和安全策略。\n*   **输出端：** 一个参数量虽小（3B/14B），但在IaC生成和变异任务上准确率、安全性和可部署性均超越超大通用模型（如GPT-4.1）的专用模型。\n\n---\n\n**总结：**\n作者的思考路径是从**“发现通用模型在特定形式化领域的失效”**出发，通过**“引入形式化验证器作为外部监督”**解决数据质量和反馈缺失问题，最终通过**“数据清洗（自修复）+ 训练范式升级（RL）”**的组合拳，实现了在IaC这一垂直领域的突破。这不仅是技术的堆砌，更是对“如何让概率性模型适应确定性规则”这一本质问题的系统性回答。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 44,
            "papers": [
                {
                    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
                    "arxiv_id": "2601.07782",
                    "authors": "Wei Fang, James Glass",
                    "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的工具使用和规划能力。它提出了通过迭代查询规划和任务分解来改进工具检索的框架，直接属于单智能体（规划、工具使用）的研究范畴。",
                    "summary2": "本文旨在解决大规模动态工具库中复杂请求检索困难的问题。针对用户意图与工具文档间的语义鸿沟及组合性挑战，我们提出了一种TOOL QP框架，将检索建模为迭代查询规划过程，通过任务分解和动态查询生成与检索器交互。在ToolRet基准测试上，通过nDCG@K和Completeness@K等指标验证了其有效性，显著提升了检索精度和下游执行成功率。",
                    "summary_translation": "运行于大规模、动态工具库之上的 LLM agents（大语言模型智能体）依赖于有效检索，然而标准的 single-shot dense retrievers（单次密集检索器）在应对复杂请求时往往力不从心。这些检索失败主要归因于抽象用户目标与技术文档之间的脱节，以及固定大小 embeddings（嵌入向量）在建模组合式工具组合方面的能力局限。为应对上述挑战，我们提出了 TOOLQP，这是一个将检索过程建模为 iterative query planning（迭代式查询规划）的轻量级框架。不同于单次匹配，TOOLQP 将指令分解为若干子任务，并动态生成查询与检索器进行交互；通过针对组合所需的具体子任务，该方法有效地弥合了语义鸿沟。我们利用合成查询轨迹对 TOOLQP 进行训练，随后通过 Reinforcement Learning with Verifiable Rewards (RLVR，可验证奖励强化学习) 进行优化。实验结果表明，TOOLQP 实现了 state-of-the-art（最先进）的性能，展现出卓越的 zero-shot generalization（零样本泛化）能力、在不同检索器间的鲁棒性，以及在 downstream agentic execution（下游智能体执行）方面的显著提升。",
                    "inspiration_trace": "基于论文《Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning》，以下是对作者产出核心方法 **TOOL QP** 的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM智能体”到“工具检索的必要性”）**\n\n1.  **观察趋势**：随着大语言模型（LLM）向智能体演进，解决复杂任务（如数学、编程、推理）越来越依赖于外部工具（API、数据库）。\n2.  **现实瓶颈**：工具库的规模正在从几十个手工挑选的函数，爆炸式增长到数万个动态API。\n3.  **核心冲突**：由于上下文窗口的限制，无法将所有工具的文档和说明一次性塞入LLM。因此，**工具检索**成为了连接用户意图与海量工具库的必经之路。\n\n### 第二阶段：现有方案的深度诊断\n**（为什么传统的“单次检索”会失效？）**\n\n作者首先审视了当前主流的解决方案——即直接套用标准信息检索（IR）技术，使用密集嵌入进行单次语义匹配。通过分析，作者发现了三个根本性的结构性缺陷：\n\n1.  **语义鸿沟**：\n    *   *现象*：用户的表达通常是抽象的、高层的（如“让这段录音音质变好”），而工具文档是技术的、底层的（如“IIR滤波器参数”）。\n    *   *诊断*：单次嵌入试图在一个向量空间内强行对齐这两种完全不同的语言体系，往往导致匹配失败。\n\n2.  **组合性瓶颈**：\n    *   *现象*：现实任务是组合性的，往往需要同时调用多个不同的工具（如“分析降雨如何影响零售销量”需要天气API+股票数据库）。\n    *   *诊断*：单次查询生成的固定维度向量，本质上是一个“词袋”，缺乏表达“多个离散工具组合”的容量。它无法编码工具之间的逻辑关系和组合多样性。\n\n3.  **缺乏交互性**：\n    *   *现象*：工具之间存在依赖关系（如工具A需要工具B的输出作为参数），且工具库是动态变化的。\n    *   *诊断*：传统检索将工具库视为静态数据库，只能“查一次”，无法像人类一样通过“试错”或“反馈”来发现隐含的依赖关系。\n\n### 第三阶段：范式转换与核心假设\n**（从“静态匹配”转向“动态规划”）**\n\n基于上述诊断，作者意识到问题的根源在于**试图用一次性的静态映射来解决动态的、多步骤的推理问题**。\n\n*   **思维跃迁**：如果人类面对复杂任务时会先“制定计划”，再分步执行，为什么不让检索器也这样做？\n*   **核心假设**：工具检索不应是“Query -> Result”的单跳匹配，而应是一个“Goal -> Plan -> Sub-goals -> Queries -> Results”的**迭代规划过程**。\n*   **新视角**：将底层的检索器视为一个可交互的“环境”，而不是一个静态的索引库。\n\n### 第四阶段：方法论构建\n**（如何实现“查询规划”？）**\n\n为了验证上述假设，作者设计了 **TOOL QP** 框架，将检索过程拆解为三个逻辑阶段：\n\n1.  **任务分解**：\n    *   *逻辑*：为了解决语义鸿沟，不能直接用用户原始查询去检索。\n    *   *方案*：先将复杂的用户指令拆解为一系列逻辑上的子任务。这相当于在抽象意图和具体工具之间架设了一座“中间层桥梁”。\n\n2.  **交互式查询生成**：\n    *   *逻辑*：为了解决组合性和依赖性问题，需要分步检索。\n    *   *方案*：针对每个子任务生成特定的搜索查询。关键在于引入**反馈机制**——每一步检索后，模型会观察结果，并动态调整下一步的查询策略（例如，发现缺少某个前置工具，下一步就去专门搜那个工具）。\n\n3.  **检索聚合**：\n    *   *逻辑*：多步检索会产生多个列表，如何合并？\n    *   *方案*：放弃复杂的加权融合，采用“峰值排名”策略——即取每个工具在所有检索步骤中获得的最高排名。这避免了某些子任务因为查询次数多而主导最终结果的偏差。\n\n### 第五阶段：训练策略的演进\n**（如何在没有标注数据的情况下训练规划器？））\n\n框架设计好了，但面临一个现实难题：现有的数据集只有（用户查询，相关工具），没有中间的“规划轨迹”或“子任务标注”。\n\n1.  **数据合成**：\n    *   *思路*：利用强模型（如GPT-4）作为“教师”，反向合成数据。\n    *   *过程*：让教师模型根据最终的正确工具，反推并生成能够找到这些工具的“规划路径”和“中间查询”。这为模型提供了模仿学习的样本。\n\n2.  **强化学习优化（RLVR）**：\n    *   *思路*：单纯的模仿学习（SFT）只能学会教师的风格，不一定能最大化检索成功率。\n    *   *过程*：引入强化学习（RLVR），直接以检索指标（如nDCG、Recall）作为奖励信号。这迫使模型跳出模仿的局限，自主探索能真正提高检索准确率的查询策略。\n\n---\n\n### 总结：作者的思考路径图\n\n1.  **起点**：LLM智能体需要处理海量工具库 -> 必须检索。\n2.  **痛点**：单次密集检索在复杂任务上表现糟糕。\n3.  **归因**：语义错位、组合性限制、缺乏交互反馈。\n4.  **顿悟**：检索应该是一个**规划**过程，而非简单的匹配。\n5.  **方案**：分解任务 -> 迭代查询 -> 动态反馈 -> 结果聚合。\n6.  **落地**：利用合成数据教模型“怎么想”，利用强化教模型“怎么做得更好”。\n\n这一逻辑链条清晰地展示了作者如何从对现有技术缺陷的敏锐观察，上升到对问题本质的重新定义（从IR到Planning），最终构建出一套完整的解决方案。"
                },
                {
                    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
                    "arxiv_id": "2601.07711",
                    "authors": "Pietro Ferrazzi, Milica Cvjeticanin, Alessio Piraccini, Davide Giannuzzi",
                    "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究“Agentic RAG”范式，其中LLM作为智能体负责编排整个流程（决定执行动作、时机及迭代），涉及规划、工具使用和自我反思等核心智能体能力，符合单智能体的研究范围。",
                    "summary2": "本文旨在比较Enhanced RAG与Agentic RAG的性能与成本权衡。针对FIQA、NQ、FEVER和CQADupStack-English数据集，我们对比了基于固定模块的Enhanced RAG与LLM自主编排的Agentic RAG。通过F1、NDCG@10及LLM-as-a-judge等指标验证，发现Agentic RAG在查询重写上表现更优，而Enhanced RAG在文档重排和成本控制上更具优势。",
                    "summary_translation": "检索增强生成 (RAG) 系统通常定义为生成器与检索组件的组合，其中检索组件负责从知识库中提取文本上下文，以回答用户查询。然而，此类基础实现存在若干局限性，包括检索结果存在噪声或非最优、对超出范围的查询误用检索机制、查询与文档匹配度低，以及生成器带来的波动性或成本问题。这些缺陷推动了“增强型” RAG 的发展，即在流程中引入专用模块以解决特定的薄弱环节。近期，大型语言模型日益增强的自我反思能力催生了一种新范式，我们将其称为“代理型” RAG。在该方法中，LLM 统筹整个流程——决定执行何种操作、何时执行以及是否进行迭代——从而减少对固定的人工设计模块的依赖。尽管这两种范式已得到快速普及，但在何种条件下哪种方法更具优势尚不明确。在本研究中，我们针对增强型和代理型 RAG，在多种场景和维度上进行了广泛的实证驱动评估。我们的研究结果揭示了这两种范式之间的权衡关系，并综合考虑成本与性能，为在现实应用中选择最有效的 RAG 设计提供了指导。",
                    "inspiration_trace": "基于论文《Is Agentic RAG worth it? An experimental comparison of RAG approaches》，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：RAG 范式的分化与演进\n**思考起点：** 作者首先观察到 RAG（检索增强生成）技术已从最初的“朴素 RAG”（Naïve RAG，即简单的检索+生成）演进出两条截然不同的发展路径：\n*   **增强型 RAG (Enhanced RAG)：** 传统的工程化优化路径。通过在固定流程中增加特定模块（如路由器、查询重写、重排序器）来修补朴素 RAG 的已知缺陷。这是一种“流水线”式的思维。\n*   **代理型 RAG (Agentic RAG)：** 新兴的智能化路径。利用大语言模型（LLM）的反思和规划能力，让 LLM 作为“大脑”自主决定何时检索、如何重写、是否迭代。这是一种“动态循环”式的思维。\n\n**核心冲突：** 社区和业界对 Agentic RAG 的热情高涨，认为其灵活性代表了未来。然而，这种灵活性是否真的带来了性能的提升？还是仅仅增加了不必要的复杂度和成本？\n\n### 2. 问题聚焦：实证缺失与决策困境\n**痛点识别：** 尽管已有理论上的定义和分类，但缺乏严格的**实证对比**。现有的文献多停留在概念探讨或单一架构的优化上。\n**核心问题：** 在实际应用中，Agentic RAG 相比于精心设计的 Enhanced RAG，究竟是“物有所值”还是“徒增开销”？在什么场景下应该选择哪一种？\n\n### 3. 假设提出：基于“缺陷修复”的维度拆解\n**逻辑推演：** 为了公平对比，不能笼统地比较“整体好坏”，而应该回到 RAG 的根本痛点上。作者假设：Agentic 和 Enhanced 两种范式在解决 RAG 的不同缺陷时，可能各有优劣。\n**维度构建：** 作者将朴素 RAG 的缺陷拆解为四个核心维度，并针对每个维度提出了对比假设：\n1.  **用户意图处理：** Enhanced RAG 依赖显式的分类器（路由），而 Agentic RAG 依赖 LLM 的自主判断。假设：在复杂意图下，Agentic 可能更灵活，但在简单任务上可能过度思考。\n2.  **查询-文档对齐：** Enhanced RAG 使用固定的重写技术（如 Hyde），而 Agentic RAG 可以动态调整查询。假设：Agentic 的动态重写可能更能适应不同文档格式。\n3.  **检索结果精炼：** Enhanced RAG 使用专门的重排序模型，而 Agentic RAG 通过多次迭代检索来优化。假设：专门的重排序模型可能比 LLM 的迭代检索更精准。\n4.  **底层模型质量的影响：** 两种架构对 LLM 能力的敏感度是否不同？\n\n### 4. 方法论设计：控制变量的“擂台赛”\n**实验设计思路：** 为了验证上述假设，作者设计了一个“头对头”的对比实验框架。\n*   **数据选择：** 选取了涵盖问答（QA）和信息检索/提取（IR/E）的四个数据集（FIQA, NQ, FEVER, CQADupStack），以覆盖不同领域和任务类型。\n*   **架构对齐：**\n    *   **Enhanced 端：** 选用当前最先进的 SOTA 组件（如 Semantic Router, Hyde 查询重写, Cross-encoder 重排序）构建最强流水线。\n    *   **Agentic 端：** 构建一个基于图的最小化代理框架，赋予其调用 RAG 工具、重写查询和迭代的能力，但不预设固定步骤。\n*   **评估指标：** 除了传统的性能指标（F1, NDCG），作者特别引入了**成本分析**（Token 消耗、端到端延迟），因为“Is it worth it”的核心在于性价比。\n\n### 5. 结果分析与洞察：打破迷思\n**逻辑推演与发现：** 通过实验数据，作者得出了反直觉或精细化的结论，修正了最初的假设：\n*   **关于意图：** Agentic RAG 在狭窄领域（如金融）表现出色，但在广泛或嘈杂领域（如 FEVER）反而不如 Enhanced RAG 的显式路由器可靠。这表明 LLM 的自主判断在边界模糊时容易失效。\n*   **关于重写：** Agentic RAG 确实表现更好，证明了动态适应性的价值。\n*   **关于精炼：** 专门的重排序模型显著优于 Agentic 的迭代检索。这揭示了 LLM 在“从一堆文档中挑出最好的”这一具体任务上，不如专门的微调模型。\n*   **关于成本：** Agentic RAG 的成本高出数倍（最高 3.6 倍），且延迟更高。\n\n### 6. 结论形成：权衡与指导\n**最终思考：** 并不存在“银弹”。\n*   **Agentic RAG 的价值：** 在于处理模糊的意图和需要动态适应查询格式的场景。\n*   **Enhanced RAG 的价值：** 在于处理需要高精度检索（重排序）和高效率的场景。\n*   **核心建议：** 不要盲目追求 Agentic 的新颖性。如果业务场景对成本敏感、对检索精度要求极高，经过优化的 Enhanced RAG 依然是更优选择；只有在需要高度灵活性和自主决策的复杂场景下，Agentic RAG 的额外成本才“值得”。"
                },
                {
                    "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
                    "arxiv_id": "2601.07696",
                    "authors": "Nick Ferguson, Alan Bundy, Kwabena Nuamah",
                    "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了LLM在基于工具的任务中的元级推理能力，重点分析了模型选择适当工具和分解任务步骤的能力，这属于单智能体范畴中的工具使用和规划能力。",
                    "summary2": "本文旨在探索大语言模型的元级推理能力。针对多跳表格问答任务，我们提出了一种基于工具的评估框架，通过比较模型工具调用与预设的“essential actions”来分析推理过程。我们在基于世界银行数据的自定义数据集上，通过最终答案准确率、精确率和召回率验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）的最新进展日益聚焦于“推理”能力，这一概念在LLM相关讨论中存在诸多重叠的定义。我们采用一种更具结构化的方法，将元级推理（meta-level reasoning，指代为解决任务所需的中间步骤进行推理的过程）与对象级推理（object-level reasoning，涉及上述步骤的底层执行）区分开来。我们设计了一项新颖的问答任务，该任务基于不同国家在不同年份的地缘政治指标数值。这些问题需要分解为中间步骤、进行数据检索以及对检索到的数据执行数学运算。我们通过考察模型为回答问题而选择合适工具的情况，来分析LLMs的元级推理能力。为了超越单纯的最终答案准确率，对LLMs进行更深入的分析，我们的任务中包含了“必要动作”，通过将LLMs的工具调用输出与这些动作进行比对，从而推断其推理能力的强弱。我们发现，LLMs在我们的任务中表现出了良好的元级推理能力，但在任务理解的某些方面仍存在缺陷。研究发现，n-shot提示（n-shot prompting）对准确率影响甚微；遇到的错误信息通常不会导致性能下降；此外，我们还提供了LLMs数理能力低下的进一步证据。最后，我们讨论了这些发现在其他任务领域的泛化性及其局限性。",
                    "inspiration_trace": "基于对论文《Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：现有“推理”评估的模糊性\n**思考起点：** 当前学术界对大语言模型（LLM）能力的讨论高度集中在“推理”这一概念上。然而，作者观察到“推理”一词在LLM语境下定义重叠且模糊（如数学推理、常识推理等）。\n**核心痛点：** 现有的基准测试（如GSM8K, MATH）大多仅关注**最终答案的准确性**。这种“黑盒”评估方式存在严重缺陷：如果模型答错了，我们无法区分是模型**没想对步骤**（规划失败），还是**算错了数**（执行失败）。\n**初步假设：** 为了真正理解LLM的推理能力，必须将“规划做什么”与“实际去做”这两个层面解耦。\n\n### 2. 理论引入：经典AI视角的二元划分\n**理论溯源：** 为了解决上述模糊性，作者回顾了符号AI和自动定理证明领域的经典理论（特别是Bundy, 1983的工作）。\n**概念界定：** 引入**元级推理**与**对象级推理**的严格区分：\n*   **元级推理：** 关于“如何解决问题”的思考，即高层规划、任务分解、步骤选择。\n*   **对象级推理：** 具体执行上述步骤的过程，如数据检索、算术计算、符号操作。\n**逻辑演进：** 作者意识到，将这一经典框架应用于LLM评估，可以将原本混在一起的“推理能力”拆解为两个可独立分析的维度，从而提供比单纯准确率更深层的诊断。\n\n### 3. 方法论构建：将思维过程“外化”\n**关键挑战：** LLM的推理过程通常隐藏在模型内部的隐状态或生成的自然语言中，难以量化评估。如何让“元级推理”变得可观测？\n**解决方案：** 利用**工具使用**范式。\n*   **逻辑支点：** 当LLM调用一个工具（如`search_indicator`或`calculate_mean`）时，它实际上是在显式地展示其“计划”。工具调用序列就是元级推理的**中间表征**。\n*   **任务设计：** 选择**多跳表格QA任务**（基于世界银行数据）。该任务天然需要将复杂问题分解为“检索数据”和“数学运算”两个子步骤，完美契合元级（规划检索与运算顺序）与对象级（实际检索与计算）的二元框架。\n\n### 4. 评估创新：从“结果导向”转向“过程导向”\n**评估困境：** 传统的QA评估只有“对/错”两种状态。但在工具使用场景下，模型可能选对了工具（元级强），但工具参数填错或计算出错（对象级弱）。\n**核心创新：** 提出**“必要动作”**的概念。\n*   **定义：** 针对每个问题，定义一组必须执行的工具调用集合。这不是唯一的“黄金路径”，而是解决问题的核心动作集。\n*   **指标构建：** 不再只看Final Answer，而是将模型生成的工具调用序列与“必要动作”进行对比，计算**精确率**和**召回率**。\n    *   **高精确率：** 模型知道该用什么工具（元级推理强）。\n    *   **低召回率：** 模型遗漏了必要步骤（规划有漏洞）。\n    *   **最终答案错误：** 可能是对象级计算错误，而非元级规划错误。\n\n### 5. 实验验证与发现：诊断模型的能力边界\n**实验设计意图：** 作者并不旨在设计一个让模型得高分的系统，而是利用这个环境作为“显微镜”来观察模型。\n**逻辑推演与验证：**\n*   **验证元级能力：** 实验发现模型在工具选择的精确率上表现良好，证明LLM具备较强的**高层规划能力**（即知道“先做什么后做什么”）。\n*   **验证对象级缺陷：** 当移除数学工具，强制模型自己计算时，性能大幅下降。这证实了LLM在**底层执行（特别是算术）**上的固有缺陷。\n*   **验证鲁棒性：** 通过引入错误信息，观察模型是否能自我修正。这进一步测试了元级推理中的“动态调整”能力。\n\n### 6. 总结：逻辑链的闭环\n作者的思考过程完成了一个闭环：\n从**现象**（LLM推理定义模糊、评估单一）出发 -> 引入**理论**（元级/对象级二分法） -> 寻找**载体**（工具调用作为思维外化的接口） -> 设计**度量**（基于必要动作的过程评估） -> 最终得出**结论**（LLM是优秀的“规划者”，但仍是蹩脚的“计算器”）。\n\n这篇文章的本质不仅仅是发布了一个数据集，而是提供了一套**解剖LLM推理能力的思维框架和手术刀（工具调用评估）**。"
                },
                {
                    "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments",
                    "arxiv_id": "2601.07606",
                    "authors": "Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",
                    "summary": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了一个用于评估基于智能体的研究判断的基准，重点比较了使用工具的智能体与非智能体基线，涉及单智能体的工具使用能力。",
                    "summary2": "本文旨在评估模型对科学思想的判断能力及预测其未来影响力。针对科学思想评估缺乏可扩展验证方法的问题，我们提出了一种名为 Proof of Time (PoT) 的半可验证基准框架，通过冻结截止时间前的证据并在离线沙箱中预测未来结果。我们在涵盖四个领域的 30K+ 实例上，通过准确率和测试时计算缩放分析验证了其有效性，发现增加交互预算能提升智能体性能，且工具使用的效果高度依赖于任务类型。",
                    "summary_translation": "大语言模型正日益被用于评估和预测研究思路，然而，我们目前缺乏可扩展的方法来衡量模型对这些科学想法的判断质量。为实现这一目标，我们提出了 PoT，这是一个半可验证的基准测试框架，它将科学想法的判断与随后可观察到的下游信号（例如引用和研究人员议程的转变）联系起来。PoT 在离线沙箱中冻结截止前的证据快照，并要求模型预测截止后的结果，这使得在真实情况出现时能够进行可验证的评估，在无需详尽专家标注的情况下实现可扩展的基准测试，并能够针对同行评审奖项等信号分析人类与模型之间的不一致性。此外，PoT 为评估科学想法的基于智能体的研究判断提供了一个受控测试平台，能够在提示消融和预算缩放的条件下，对比使用工具的智能体与非智能体基线。在跨越四个基准领域的 30,000 多个实例中，我们发现，与非智能体基线相比，更高的交互预算通常能提升智能体的性能，而使用工具的收益则高度依赖于具体任务。通过将按时间划分的、未来可验证的目标与用于工具使用的离线沙箱相结合，PoT 支持对面向未来的科学想法判断任务中的智能体进行可扩展评估。",
                    "inspiration_trace": "基于论文《Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法设计的思考过程：\n\n### 第一阶段：宏观观察与核心矛盾\n**思考起点：科学评价的“时效性错位”**\n1.  **现象观察**：科学界评价研究创意（Idea）的主要机制（同行评审、静态基准测试）通常发生在“当下”，且高度依赖主观判断。\n2.  **核心矛盾**：真正衡量一个科学创意价值的标准是“时间的检验”（如引用量、奖项、后续研究方向的改变），但这些信号具有滞后性，无法在决策当下立即获取。\n3.  **现有困境**：大语言模型（LLM）正被用于辅助科研评价，但我们缺乏一种可扩展、客观的方法来评估模型判断“未来影响力”的能力。如果仅用静态数据集评估，无法反映模型对科学演进的预测能力。\n\n### 第二阶段：概念突破——将“时间”转化为验证机制\n**核心假设：利用历史数据模拟未来预测**\n1.  **思维转换**：既然无法真的等待未来，不如利用“过去”来模拟“未来”。如果我们将时间轴切分，设定一个截止点 $t_0$，那么对于 $t_0$ 之后的 $t_1$ 时刻，其结果在当下已经是已知的客观事实。\n2.  **方法论雏形**：\n    *   **冻结证据**：只给模型提供 $t_0$ 时刻之前的“快照”信息（如论文摘要、作者历史）。\n    *   **预测未来**：要求模型预测 $t_1$ 时刻才会发生的信号（如 $t_1$ 时刻的引用数、获奖情况）。\n    *   **事后验证**：利用现实中已经发生的 $t_1$ 结果作为“金标准”进行评分。\n3.  **优势确立**：这种方法解决了“可验证性”（标签是客观事实而非主观打分）和“可扩展性”（无需专家人工标注，数据可自动更新）的问题。\n\n### 第三阶段：控制变量——解决“智能体”评估的污染问题\n**进阶思考：如何公平地评估工具使用能力？**\n1.  **新挑战**：当前流行使用“工具调用智能体”来处理复杂任务。但现有评估往往混淆了“推理能力”与“信息获取能力”。如果允许智能体联网，它可能只是直接查到了答案，而非基于证据进行了判断。\n2.  **隔离设计**：为了纯粹测试模型基于有限证据进行推理和判断的能力，作者引入了**“离线沙盒”**概念。\n3.  **逻辑闭环**：\n    *   将智能体关在一个“断网”的房间里。\n    *   房间里只有 $t_0$ 时刻的冻结证据和本地工具（如Python、文本编辑器）。\n    *   智能体表现出的任何提升，必须归因于其对有限证据的挖掘和推理能力，而非外部信息检索。\n\n### 第四阶段：维度拆解——定义“科学创意判断”的具体内涵\n**操作化定义：从抽象概念到具体任务**\n1.  **问题细化**：“科学创意判断”是一个抽象概念，需要将其拆解为可量化的具体维度。\n2.  **四个维度的构建**：\n    *   **影响力预测**：预测未来的引用量（量化指标）。\n    *   **价值评估**：预测同行评审奖项（定性共识）。\n    *   **研究演进**：预测教授未来的研究方向（连续性与漂移）。\n    *   **技术前沿**：预测基准测试的SOTA轨迹（技术极限）。\n3.  **任务设计逻辑**：这些任务覆盖了从个人（教授）、群体（会议奖项）到领域（SOTA）不同层面的科学判断，且均符合“时间可验证”原则。\n\n### 第五阶段：实验假设与验证——探索“智能体”的边际效应\n**实证探究：智能体何时才值得？**\n1.  **对比基准**：设置“零样本”与“智能体”模式的对比，旨在验证增加工具和推理步骤是否真的有效。\n2.  **成本-收益分析**：引入“消息预算”概念，模拟测试时的计算成本。\n3.  **假设验证**：\n    *   智能体并非在所有任务上都优于直接生成。\n    *   在需要深度证据挖掘的任务（如Faculty任务）上，智能体优势明显。\n    *   在结构化预测或简单任务上，增加智能体步骤可能只是浪费算力。\n4.  **结论导向**：通过实验揭示模型在处理“未来导向”任务时的失败模式（如检索失败、推理循环），为未来改进提供方向。\n\n---\n\n**总结：作者的逻辑演进路径**\n从**“科学评价需要时间检验”**的哲学观察出发，通过**“时间切片”**的技术手段将未来预测转化为离线验证，进而引入**“离线沙盒”**以排除信息干扰，纯粹考察模型的**“证据推理能力”**，最终构建了一个多维度、可扩展的基准，回答了“AI能否判断科学创意的未来价值”这一核心问题。"
                },
                {
                    "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents",
                    "arxiv_id": "2601.07582",
                    "authors": "Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",
                    "summary": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于长期对话智能体的“记忆”机制，提出了基于事件分割的动态记忆架构。根据筛选条件，“记忆”属于单智能体研究的核心范畴，且该研究不涉及被排除的纯应用、纯推理或安全等领域。",
                    "summary2": "本文旨在解决长期对话智能体中记忆粒度僵化及检索缺乏结构感知的问题。针对长期交互场景，我们提出了一种基于Event Segmentation Theory的ES-Mem框架，结合动态事件分割与边界锚定的分层记忆架构。我们在LoCoMo和LongMemEval-S基准上通过F1、BLEU-1和Accuracy等指标验证了其有效性。",
                    "summary_translation": "记忆对于对话代理在长期交互中维持连贯性并实现持续适应至关重要。尽管现有的记忆机制具备基本的存储与检索能力，但它们主要受限于两个方面：(1) 僵化的记忆粒度往往破坏语义完整性，导致记忆单元碎片化且缺乏连贯性；(2) 主流的扁平化检索范式仅依赖于表层语义相似度，忽视了在导航和定位特定情景语境时所必需的话语结构线索。为克服上述局限，受事件分割理论的启发，我们提出了ES-Mem框架，该框架包含两个核心组件：(1) 动态事件分割模块，用于将长期交互划分为具有清晰边界的语义连贯事件；(2) 分层记忆架构，通过构建多层记忆并利用边界语义来锚定特定的情景记忆，从而实现精确的语境定位。在两个记忆基准测试上的评估表明，ES-Mem相较于基线方法取得了持续的性能提升。此外，所提出的事件分割模块在对话分割数据集上也展现出了稳健的适用性。",
                    "inspiration_trace": "基于论文《ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents》的内容，以下是对作者产出该文章核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法论构建的思考过程。\n\n---\n\n### 1. 宏观问题：长程对话中的“记忆断层”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在对话智能体应用中的一个根本性矛盾：虽然LLM生成能力极强，但其固有的“上下文窗口”限制了处理超长对话的能力。\n**核心挑战：**\n为了实现真正的个性化与连续适应，智能体必须具备“长期记忆”。然而，现有的记忆机制（如简单的RAG或向量数据库）在面对成百上千轮的复杂对话时，往往表现不佳，导致智能体“遗忘”或“胡言乱语”。\n\n### 2. 深入观察：现有记忆机制的两大“结构性缺陷”\n作者对现有的主流记忆方法（如MemGPT, MemoryBank等）进行了深入剖析，发现它们普遍存在两个深层次的逻辑漏洞，这构成了文章的切入点：\n\n*   **缺陷一：记忆粒度的“机械性”。**\n    *   *观察：* 现有方法大多采用固定粒度（如按“轮次”Turn或固定Token数）切分对话。\n    *   *问题：* 真实的对话流是语义连续的。机械切分往往会打断一个完整的语义事件（例如，讨论“生日礼物”的想法跨越了3轮，却被切成了两半）。这导致存储的记忆单元本身是“碎片化”和“语义不完整”的。\n*   **缺陷二：检索范式的“扁平化”。**\n    *   *观察：* 现有检索大多基于向量相似度的“扁平检索”，把所有记忆块当作孤立的文本片段进行匹配。\n    *   *问题：* 这种方式忽略了对话的“篇章结构”。当用户问“为什么我们后来放弃了那个园艺工具？”时，关键信息不在于“园艺工具”这个词本身，而在于话题**转换**的那个瞬间。扁平检索很难定位这种结构性的转折点。\n\n### 3. 跨学科灵感：引入认知心理学中的“事件分割理论”\n**思考转折：**\n为了解决上述“语义碎片”和“结构缺失”的问题，作者跳出纯计算机科学的视角，转向认知心理学寻找答案。\n**理论引入：**\n作者引入了**事件分割理论**。该理论指出，人类并非连续地感知世界，而是将经验流解析为离散的、有意义的事件单元。\n**关键洞察：**\n人类记忆中，**事件边界**尤为重要。边界处是注意力最集中的时刻，起到了“认知锚点”的作用，帮助人类高效地索引和回忆过去的经历。\n*假设：* 如果让AI像人类一样，按“事件”来组织记忆，并利用“边界”作为检索的锚点，就能解决现有方法的痛点。\n\n### 4. 核心假设形成：从“存储文本”转向“结构化事件”\n基于EST理论，作者提出了核心假设：\n*   **关于存储：** 记忆的粒度不应是固定的轮次，而应是动态的“语义事件”。\n*   **关于检索：** 检索不应是全局的文本匹配，而应是先定位“边界锚点”，再展开细节的“由粗到细”过程。\n\n### 5. 方法论构建：ES-Mem框架的逻辑落地\n为了验证上述假设，作者设计了ES-Mem框架，其逻辑演进分为三个步骤：\n\n**第一步：如何定义“事件”？（动态分割模块）**\n*   *思考：* 机器如何知道一个话题结束了？\n*   *策略：* 采用“统计信号+语义验证”的两阶段法。\n    1.  **粗筛：** 利用互信息计算话题连贯性，当语义连贯性骤降时，标记为潜在边界。\n    2.  **精修：** 引入意图识别，判断这是话题的“转换”还是内容的“细化”。只有真正的意图转换才被确认为边界。\n\n**第二步：如何利用“边界”？（分层记忆架构）**\n*   *思考：* 既然边界是锚点，那么记忆的结构就不能是扁平的。\n*   *策略：* 构建三层金字塔结构。\n    *   **Level 1（顶层）：精炼边界。** 专门描述“话题A是如何转换到话题B的”。这是检索时的“路标”。\n    *   **Level 2（中层）：事件摘要。** 用于快速匹配内容。\n    *   **Level 3（底层）：原始上下文。** 用于最终生成细节。\n    *   *创新点：* 显式地将“边界”建模为一种可检索的信息索引，而不仅仅是切分点。\n\n**第三步：如何模拟人类回忆？（由粗到细检索）**\n*   *思考：* 人类回忆时，先想“那是哪段时间的事？”，再想“具体说了什么？”。\n*   *策略：* 模仿这一认知过程。\n    1.  **边界扫描：** 先在Level 1（边界层）搜索，找到最相关的“话题转换时刻”。\n    2.  **区间扩展：** 以该边界为中心，向前后扩展一个时间窗口，锁定相关的记忆区间。\n    3.  **摘要重排：** 在锁定的区间内，利用Level 2（摘要）进行精细打分，选出最准确的上下文。\n\n### 6. 逻辑闭环与验证\n**最终产出：**\n作者通过这一系列思考，将传统的“静态、扁平”的记忆系统，重构为“动态、结构化、认知驱动”的记忆框架。\n**验证逻辑：**\n在实验中，作者不仅验证了ES-Mem在长程记忆任务上的性能提升，还专门验证了“事件分割”模块本身的鲁棒性。这证明了：**模仿人类认知结构（事件分割+边界锚定）确实是解决长程对话记忆难题的有效路径。**\n\n---\n\n**总结：**\n作者的思考路径是从**“现有技术无法处理长程语义连贯性”**这一工程问题出发，通过**“认知心理学的事件分割理论”**获得理论指引，最终通过**“动态分割+分层存储+锚点检索”**的技术手段，实现了对人类记忆机制的工程化复现。"
                },
                {
                    "title": "From RAG to Agentic RAG for Faithful Islamic Question Answering",
                    "arxiv_id": "2601.07528",
                    "authors": "Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket, Mahmoud Alhirthani, Mutaz Al-Khatib, Logan Cochrane, Kareem Darwish, Rashid Yahiaoui, Firoj Alam",
                    "summary": "LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个“智能体RAG”框架，该框架利用结构化工具调用进行迭代证据检索和答案修订。这符合单智能体研究范围中的“工具使用”和“自我反思/规划”特征。尽管应用场景是伊斯兰教问答，但其核心贡献在于智能体架构的设计与实现，而非单纯的应用。",
                    "summary2": "本文旨在解决LLM在伊斯兰问答中产生幻觉及缺乏依据的问题。针对双语伊斯兰问答场景，我们提出了一种Agentic RAG框架，利用结构化工具调用进行迭代证据检索与答案修正，并在自建的ISLAMIC FAITH QA基准数据集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（LLMs）越来越多地被应用于伊斯兰教问答领域，其中 ungrounded（缺乏事实依据的）响应可能会带来严重的宗教后果。然而，标准的 MCQ（多选题）/MRC（机器阅读理解）风格评估无法捕捉关键的 real-world（现实世界）失效模式，特别是 free-form hallucinations（自由形式的幻觉）以及模型在缺乏证据时是否能够适当地 abstain（拒绝回答）。为了揭示这一方面，我们介绍了 ISLAMICFAITHQA，这是一个包含 3,810 个项目的 bilingual（双语，阿拉伯语/英语）generative benchmark（生成式基准），具有 atomic single-gold answers（原子性单一金标准答案），能够直接测量 hallucination（幻觉）和 abstention（拒绝回答行为）。此外，我们开发了一个 end-to-end grounded Islamic modelling suite（端到端 grounded 伊斯兰教建模套件），该套件包括： 25K 个基于阿拉伯语文本的 SFT（监督微调）推理对； 5K 个用于 reward-guided alignment（奖励引导对齐）的双语 preference samples（偏好样本）； 以及一个包含约 6,000 个 atomic verses（原子性经文）的 verse-level Qur'an retrieval corpus（经文级《古兰经》检索语料库）。基于这些资源，我们开发了一个 agentic Quran-grounding framework（智能体《古兰经》 grounding 框架），该框架利用 structured tool calls（结构化工具调用）进行 iterative evidence seeking（迭代式证据搜寻）和 answer revision（答案修正）。针对 Arabic-centric（以阿拉伯语为中心）和 multilingual LLMs（多语言大型语言模型）的实验表明，retrieval（检索）能够提高 correctness（正确性），且 agentic RAG（智能体检索增强生成）在 standard RAG（标准检索增强生成）的基础上带来了最大的性能提升，即使在小模型（即 Qwen3 4B）上也能实现 state-of-the-art（最先进的）性能和更强的 Arabic-English robustness（阿拉伯语-英语鲁棒性）。我们将向社区公开实验资源和数据集。",
                    "inspiration_trace": "基于论文《From RAG to Agentic RAG for Faithful Islamic Question Answering》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题识别——高价值领域的“流利性陷阱”\n**思考起点：**\n作者首先关注到LLM在伊斯兰教问答（Islamic QA）这一高敏感、高价值领域的应用现状。\n**核心矛盾：**\n虽然LLM在语言表达上非常流利，但这种流利性掩盖了其内在的不可靠性。在宗教领域，一个看似自信但缺乏依据的回答（幻觉）不仅是一个错误，更可能导致严重的宗教误导或伦理后果。\n**初步结论：**\n现有的通用LLM在涉及教法推理、文化规范和经典依据时，存在严重的“事实性”和“忠实度”缺陷，必须建立一种能够强制模型“言之有据”的机制。\n\n### 第二阶段：现状观察与评估体系的批判\n**观察：**\n作者审视了现有的伊斯兰NLP评估基准，发现它们大多采用多选题（MCQ）或机器阅读理解（MRC）格式。\n**逻辑漏洞：**\n1.  **无法测度幻觉：** MCQ允许模型通过排除法或猜测得分，无法反映模型是否真正理解或是否在编造答案。\n2.  **缺乏“拒答”机制：** 真实的宗教咨询中，不知道答案时应选择“拒答”，但现有评估不鼓励也不测量这种审慎行为。\n**假设提出：**\n要解决忠实度问题，首先必须改变“尺子”。我们需要一个更严格的、生成式的评估基准，它必须能直接测量“幻觉”和“拒答”。\n**行动：**\n构建 **ISLAMIC FAITH QA**。这是一个包含原子性单一金答案的双语基准，采用严格的“正确/错误/未尝试”标签，迫使模型要么给出精准的基于文本的答案，要么承认无知。\n\n### 第三阶段：从参数记忆到外部检索的范式转移\n**问题深化：**\n即使有了严格的基准，作者发现仅靠模型内部的参数记忆（SFT微调）仍然无法达到高准确率，且容易产生过度的自信错误。\n**假设：**\n伊斯兰知识是密集且具体的，模型不可能记住所有细节。解决幻觉的根本路径不是“训练模型记住更多”，而是“强制模型查阅经典”。\n**初步方案（标准RAG）：**\n引入检索增强生成（RAG），将古兰经经文作为上下文提供给模型。\n**发现：**\n虽然标准RAG（一次性检索后生成）比基线模型有提升，但它仍然是被动的。如果检索到的上下文不完美，或者模型没有正确利用上下文，错误依然会发生。\n\n### 第四阶段：核心创新——从“被动检索”到“主动代理”\n**逻辑跃迁：**\n作者反思了人类学者回答宗教问题的过程：人类不是一次性读完所有资料就回答，而是**迭代式**地寻找证据、阅读经文、核实出处，然后再作答。\n**核心假设：**\n如果将检索过程从“预处理步骤”转变为“显式的决策过程”，让模型像人类学者一样主动使用工具去寻找证据，那么忠实度将大幅提升。\n**方法论确立：**\n提出 **Agentic RAG（代理式RAG）**。\n*   **区别：** 标准RAG是“Query -> 检索 -> 生成”；Agentic RAG是“Query -> 规划 -> 调用工具（搜索/阅读/元数据查询） -> 迭代 -> 生成带引用的答案”。\n*   **预期效果：** 这种结构化的工具调用迫使模型在回答前必须进行证据检查，从而减少幻觉，并提高跨语言（阿语/英语）的鲁棒性（因为证据源是统一的古兰经）。\n\n### 第五阶段：数据与方法的闭环构建\n**配套思考：**\n为了支撑上述Agentic RAG框架，仅有基准是不够的，模型需要具备“使用工具”和“基于证据推理”的能力。\n**资源构建：**\n1.  **SFT数据：** 构建25K条基于文本的推理对，训练模型学会“引用经文进行推理”的思维模式，而不仅仅是背诵答案。\n2.  **RL对齐数据：** 构建5K条偏好样本，利用LLM-as-Judge作为奖励信号，训练模型倾向于生成“有依据的、简洁的”回答，惩罚幻觉。\n3.  **检索语料库：** 将古兰经细化为原子级别的经文单元，便于工具精准调用。\n\n### 总结：逻辑演进的全貌\n作者的思考路径遵循了**“发现问题 -> 修正标准 -> 引入外部知识 -> 升级交互模式”**的闭环：\n1.  **痛点：** 宗教领域容错率低，现有模型爱“胡说八道”。\n2.  **立尺：** 建立严格基准，拒绝“蒙题”，强制要求精准和拒答。\n3.  **寻源：** 引入RAG，用古兰经作为唯一真理来源。\n4.  **拟人：** 升级为Agentic RAG，让模型学会像学者一样“主动查阅、反复核实”后再回答。\n\n最终，作者通过实验验证了这一逻辑：**Agentic RAG** 不仅超越了标准RAG，甚至能让小模型（4B）在特定任务上超越未使用该技术的大模型，证明了“思维链（工具使用）”比“参数量”在解决忠实度问题上更有效。"
                },
                {
                    "title": "GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap",
                    "arxiv_id": "2601.07375",
                    "authors": "Farzad Shami, Subhrasankha Dey, Nico Van de Weghe, Henrikki Tenkanen",
                    "summary": "The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent's execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at https://anonymous.4open.science/r/groke.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了GROKE，一个基于LLM的框架，用于评估导航指令。它符合“单智能体”的研究范围，具体涉及“规划”（子指令规划）和“工具使用”（利用OpenStreetMap数据进行拓扑图导航）。论文明确指出是“Vision-Free”（无视觉），避开了多模态/视觉的排除项，且侧重于智能体的架构设计与执行能力，而非纯推理或特定垂直领域的纯应用。",
                    "summary2": "本文旨在解决视觉依赖评估中的成本与感知误差问题。针对 Map2Seq 数据集，我们提出了一种基于 OpenStreetMap 的无视觉分层 LLM 框架 GROKE，结合子指令规划与结构化 JSON 表示进行图推理。我们在 Map2Seq 上通过 Navigation Error (NE)、Success Rate (SR) 等指标验证了其有效性，导航误差降低了 68.5%。",
                    "summary_translation": "导航指令的评估仍然是视觉语言导航 (VLN) 研究中一个长期存在的挑战。传统的基于参考的指标，如 BLEU 和 ROUGE，无法捕捉空间指令的功能效用，特别是无法衡量指令是否成功引导导航者到达预定目的地。尽管现有的 VLN 智能体可以作为评估器，但它们对高保真视觉模拟器的依赖带来了许可限制和计算成本，且感知误差进一步干扰了语言质量评估。本文介绍了 GROKE (Graph-based Reasoning over OSM Knowledge for instruction Evaluation)，这是一个基于分层大语言模型 (LLM) 的无视觉、无需训练的框架，用于利用 OpenStreetMap 数据评估导航指令。通过系统的消融实验，我们证明了空间信息的结构化 JSON 和文本格式显著优于基于网格和视觉图的表示。我们的分层架构结合了子指令规划与拓扑图导航，在 Map2Seq 数据集上，与启发式和采样基线相比，将导航误差降低了 68.5%。智能体的执行成功率、轨迹保真度和决策模式作为功能可导航性的代理指标（基于 OSM 可见地标和拓扑结构），建立了一种无视觉依赖的可扩展且可解释的评估范式。代码和数据可在 https://anonymous.4open.science/r/groke 获取。",
                    "inspiration_trace": "基于对论文《GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观问题：如何准确评估导航指令的“功能性”质量？\n**思考起点：**\n在视觉语言导航（VLN）领域，传统的评估指标（如BLEU、ROUGE）存在根本性缺陷。这些指标基于文本相似度（n-gram重叠），无法捕捉导航指令的核心价值——即“能否引导用户到达目的地”。\n*   **反例：** “在银行左转”与“在银行右转”文本相似度极高，但功能截然相反。\n*   **结论：** 评估必须从“文本相似度”转向“功能效用性”。\n\n### 2. 现状批判：现有“务实评估”路径的痛点\n**演进逻辑：**\n为了解决上述问题，学术界引入了“Agent-as-Judge”范式，即训练一个智能体在模拟器中执行指令，通过成功率来反推指令质量。\n**观察到的瓶颈：**\n这种方法严重依赖高保真的视觉模拟器（如Matterport3D、Google Street View），引入了新的噪声：\n1.  **混淆变量：** 智能体失败可能是因为视觉识别能力差（看不清“红砖墙”），而非指令本身写得不好。这导致评估结果混杂了视觉感知误差。\n2.  **成本与壁垒：** 视觉数据昂贵、版权受限、计算量大，限制了评估的可扩展性。\n\n### 3. 核心假设：能否剥离视觉，仅基于“语义与拓扑”进行评估？\n**思维跃迁：**\n导航的本质是空间推理，而非像素识别。人类在阅读导航指南（如地图）时，依赖的是地标（POI）、方向和拓扑连接，而非实景照片。\n**假设提出：**\n如果我们将环境抽象为符号化的地图数据（如OpenStreetMap），构建一个“无视觉”的评估智能体，是否既能保留功能性评估的优势，又能消除视觉噪声和成本问题？\n*   **数据基础：** Map2Seq数据集提供了OSM数据（节点、边、POI），为这一假设提供了实验土壤。\n\n### 4. 方法论探索：如何让大语言模型（LLM）“看懂”地图？\n**技术挑战：**\n既然决定使用LLM作为推理核心，如何将图结构的空间数据转化为LLM能高效理解的输入？\n**实验与试错（Ablation Studies驱动的设计）：**\n作者对比了四种空间表征形式，试图寻找最优解：\n1.  **网格/矩阵：** 模仿视觉像素。结果发现LLM难以解析这种高密度的ASCII字符，效果最差。\n2.  **可视化图：** 使用Graphviz风格。虽然直观，但LLM处理箭头和图形符号的推理能力不如处理结构化数据。\n3.  **纯文本描述：** 自然语言描述连接关系。效果尚可，但在复杂路径上信息密度不足，导致认知负荷过高。\n4.  **结构化JSON（最终选择）：** 将节点、边、POI组织为层级化的JSON。\n    *   **逻辑判断：** JSON格式既保留了机器可读的结构，又符合LLM预训练数据中的代码/结构化文本模式，能显著提升推理效率和准确性。\n\n### 5. 架构优化：如何处理长程导航的复杂性？\n**问题分解：**\n直接让LLM根据整段长指令在地图上一步步走，容易迷失目标或产生累积误差。\n**灵感来源：** 人类认知习惯——将复杂任务拆解为子目标。\n**架构设计：**\n提出**分层架构**：\n1.  **子指令代理：** 负责高层规划，将长指令拆解为原子动作（如“直走”、“左转”）并提取关键地标。\n2.  **导航代理：** 负责底层执行，仅关注当前子目标在局部地图（可见区域）内的实现。\n*   **逻辑优势：** 这种解耦降低了单次推理的复杂度，使得智能体能更专注于当前的局部决策，同时保持全局目标的一致性。\n\n### 6. 最终验证：这种“无视觉”评估是否有效？\n**闭环思考：**\n如果智能体没有眼睛，它的成功是否真的代表了指令的质量？\n**验证逻辑：**\n通过相关性分析，将GROKE的导航指标（如导航误差NE、成功率SR）与人类对指令清晰度的评分进行对比。\n*   **结果：** 两者呈现显著相关性。证明了一个基于逻辑和拓扑的智能体，足以作为指令质量的可靠代理指标，从而建立了一种**可扩展、可解释且无视觉依赖**的评估新范式。\n\n---\n\n**总结：**\n作者的思考路径是从**评估指标的失效**出发，批判了**视觉依赖的局限性**，提出了**基于OSM图推理的“无视觉”假设**，并通过**对比实验确定了JSON作为最优的空间表征**，最终利用**分层代理架构**实现了高效、准确的指令评估。"
                },
                {
                    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
                    "arxiv_id": "2601.07348",
                    "authors": "Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang",
                    "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了“受控自我演化”（CSE）框架，通过“生成-验证-优化”循环实现算法代码的自我完善，涵盖了规划（多样化规划初始化）、记忆（分层进化记忆）和自我演化（通过反馈引导的遗传进化）等LLM智能体的核心特征，符合研究范围。",
                    "summary2": "本文旨在解决现有自进化方法在算法代码优化中探索效率低下的问题。针对代码生成任务，我们提出了一种Controlled Self-Evolution (CSE)框架，通过多样化规划初始化、遗传进化及分层进化记忆提升搜索效率。在EffiBench-X基准上，通过Execution-Time ratio (ET)、Memory-Peak ratio (MP)和Memory-Integral ratio (MI)指标验证了其有效性，CSE在多种LLM主干网络上均表现出更优的算法优化能力。",
                    "summary_translation": "自进化方法通过迭代的“生成-验证-优化”循环来增强代码生成，然而现有方法存在探索效率低下的问题，无法在有限的预算内发现具有更优复杂度的解决方案。这种低效性源于初始化偏差导致进化陷入劣质解区域、缺乏反馈引导的不可控随机操作，以及跨任务经验利用不足。为解决这些瓶颈，我们提出了受控自进化，该方法包含三个关键组件。多样化规划初始化生成结构各异的算法策略，以实现广泛的解空间覆盖。遗传进化用反馈引导机制替代随机操作，从而实现定向突变和组合交叉。分层进化记忆在任务间和任务内层面捕获成功与失败的经验。在 EffiBench-X 上的实验表明，CSE 在各种 LLM backbones (大语言模型骨干) 上均持续优于所有 baselines (基线模型)。此外，CSE 在早期代即展现出更高的效率，并在整个进化过程中保持持续改进。我们的代码已在 https://github.com/QuantaAlpha/EvoControl 公开。",
                    "inspiration_trace": "基于论文《Controlled Self-Evolution for Algorithmic Code Optimization》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：从“功能正确”到“算法最优”的鸿沟\n*   **现象**：现有的LLM在代码生成任务上已经表现出色，能够通过单次生成解决许多编程问题，实现“功能正确”。\n*   **问题**：在算法竞赛或高性能计算场景下，仅仅“正确”是不够的。代码的执行效率（时间复杂度、空间复杂度）至关重要。现有的模型往往生成的是“正确但低效”的代码。\n*   **初步思考**：如何让模型不仅能写出能跑通的代码，还能像人类专家一样不断优化代码，逼近算法的最优解？\n\n### 2. 现有范式分析：自进化的潜力与瓶颈\n*   **现有方案**：学术界引入了“自进化”范式，即通过“生成-验证-修正”的迭代循环来优化代码。\n*   **深入观察**：虽然理论上只要迭代次数足够多，随机搜索总能找到最优解，但在实际应用中，计算资源和推理预算是有限的。\n*   **核心矛盾**：**探索效率低下**。现有的自进化方法在有限的预算内，很难跳出局部最优，发现具有更优复杂度的解。它们浪费了大量的预算在低质量的解空间中。\n\n### 3. 病因诊断：效率低下的三大根源\n作者深入剖析了为什么现有的自进化方法“瞎忙活”，归纳出三个核心痛点：\n\n*   **痛点一：初始化偏差**\n    *   *观察*：传统方法通常从一个或少数几个初始解开始进化。如果初始解处于解空间的“贫瘠区域”（例如算法思路本身是低效的），后续的微调很难从根本上改变算法结构，导致进化陷入局部最优。\n    *   *思考*：起点决定了起跑线，如果起跑线就选错了，后面跑得再快也没用。我们需要在起跑时就覆盖不同的算法思路。\n\n*   **痛点二：无控制的随机进化**\n    *   *观察*：现有的变异和交叉操作往往是随机的、黑盒的。模型不知道哪里错了，只是盲目地修改代码或拼接文本。这种“无向探索”导致生成的变体大多无效，无法利用验证反馈来指导搜索方向。\n    *   *思考*：进化不能靠“猜”，必须靠“反馈”。我们需要一种机制，能精准定位代码中的“病灶”，并进行“手术式”的修复，而不是盲目重写。\n\n*   **痛点三：进化经验的浪费**\n    *   *观察*：模型在进化过程中经常重复犯同样的错误（无论是同一个任务内的重复失败，还是不同任务间忽略了通用的优化技巧）。现有的方法缺乏记忆机制，无法积累和复用成功的经验。\n    *   *思考*：人类专家之所以强，是因为他们记住了之前的教训和套路。我们需要给模型装上“短期记忆”（避免重蹈覆辙）和“长期记忆”（复用通用优化策略）。\n\n### 4. 核心假设：从“随机搜索”转向“受控进化”\n*   **假设提出**：如果我们将进化过程从“无控制的随机操作”转变为“受反馈引导的精细化操作”，并辅以多样化的起点和记忆机制，就能大幅提升探索效率。\n*   **方法论构建**：基于上述三个痛点，提出 **Controlled Self-Evolution (CSE)** 框架，对应设计三个关键组件来逐一击破。\n\n### 5. 方法论演进：三大组件的逻辑构建\n\n*   **针对痛点一（初始化偏差） -> 多样化规划初始化**\n    *   *设计思路*：不要直接生成代码，先生成“策略草图”。强制模型在生成具体代码前，先规划出多种结构上截然不同的算法策略（如贪心 vs 动态规划 vs 位运算）。\n    *   *逻辑*：通过策略层面的多样性，确保初始种群覆盖了解空间中多个有潜力的区域，降低了陷入局部最优的风险。\n\n*   **针对痛点二（无控制进化） -> 遗传进化机制**\n    *   *设计思路*：引入“功能分解”概念。将代码拆解为独立的功能模块（如I/O、核心逻辑、边界处理）。\n    *   *受控变异*：利用反馈定位导致性能低下的具体模块，只对该模块进行“靶向再生”，保留表现良好的部分。\n    *   *组合交叉*：模仿人类专家，从不同父代中提取优势模块（如A的算法核心 + B的优化技巧），在逻辑层面进行结构化重组，而非简单的文本拼接。\n\n*   **针对痛点三（经验浪费） -> 分层进化记忆**\n    *   *设计思路*：建立双层记忆系统。\n    *   *局部记忆（任务内）*：记录当前任务中哪些修改带来了提升（成功经验），哪些导致了倒退（失败教训），实时指导后续迭代，避免走回头路。\n    *   *全局记忆（跨任务）*：将不同任务中的通用优化模式（如特定的I/O加速技巧、数据结构替换规则）提炼出来，存入向量数据库。遇到新任务时，主动检索相关经验作为先验知识。\n\n### 6. 逻辑闭环与验证\n*   **综合**：将“多样化起点”作为基础，通过“受控的遗传操作”在解空间中高效导航，并利用“分层记忆”作为导航的指南针。\n*   **预期结果**：这种方法不仅能更快地找到高质量解（早期效率高），而且能在整个进化过程中持续改进（持续优化），不会过早陷入停滞。\n*   **实验验证**：在EffiBench-X上的实验结果证实了CSE在不同LLM骨干网络上均优于基线方法，且消融实验证明了三个组件缺一不可，形成了协同效应。\n\n---\n\n**总结**：作者的思考路径是从**发现LLM代码效率不足**这一现象出发，通过分析现有自进化方法**“盲目搜索”**的本质缺陷，提出了**“受控引导”**的核心思想，并最终通过**策略多样化、操作精细化、经验分层化**三个维度的创新，构建了一套完整的算法代码优化方法论。"
                },
                {
                    "title": "Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation",
                    "arxiv_id": "2601.07338",
                    "authors": "Yanzhi Tian, Cunxiang Wang, Zeming Liu, Heyan Huang, Wenbo Yu, Dawei Song, Jie Tang, Yuhang Guo",
                    "summary": "Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了RATE，一个基于智能体的翻译评估框架，其中包含具有自我反思能力的核心智能体和动态调用的子智能体，符合单智能体（自我反思、工具使用）及多智能体协作的研究范围。",
                    "summary2": "本文旨在解决非字面翻译评估中现有指标失效的问题。针对包含俚语、隐喻等复杂语言现象的场景，我们提出了一种名为 RATE 的 Reflective Agentic Translation Evaluation 框架，通过 Core Agent 动态调用子代理获取外部知识并校准分数。我们在构建的 MENT 数据集上通过 Meta Score 验证了其有效性，结果显示 RATE 显著优于现有指标。",
                    "summary_translation": "大语言模型显著推动了机器翻译的发展，并将其应用于语言复杂的领域——如社交网络服务、文学等。在这些场景中，翻译往往需要处理非字面表达，从而导致机器翻译指标的不准确。为了系统地研究机器翻译指标的可靠性，我们首先构建了一个专注于非字面翻译的元评估数据集，即 MENT。MENT 涵盖了四个非字面翻译领域，包含源句子与来自不同机器翻译系统的译文配对，并附带 7,530 个人工标注的翻译质量分数。实验结果揭示了传统机器翻译指标的不准确性，以及大语言模型作为评判者的局限性，特别是知识截止和评分不一致的问题。为了缓解这些局限性，我们提出了 RATE，这是一种新颖的基于智能体的翻译评估框架，其核心是一个能够动态调用专门子智能体的反思性核心智能体。实验结果表明了 RATE 的有效性，与当前指标相比，其元分数至少提升了 3.2 分。进一步的实验证明，RATE 在通用领域的机器翻译评估中也具有鲁棒性。代码和数据集可在以下地址获取：https://github.com/BITHLP/RATE。",
                    "inspiration_trace": "基于论文《Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation》，以下是对作者产出该文章核心方法（RATE）的逻辑链推演与思考过程还原：\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM很强大”到“评估标准失效”）**\n\n1.  **现象观察**：作者首先注意到大语言模型（LLMs）极大地推动了机器翻译（MT）的发展，使其应用场景从传统的新闻领域扩展到了社交媒体（SNS）、文学、跨文化内容等复杂领域。\n2.  **核心矛盾**：在这些新场景中，翻译的核心难点不再是字面意义的转换，而是对**非字面表达**（如网络俚语、文化隐喻、诗歌意象）的处理。\n3.  **现有工具的失效**：作者发现，传统的MT评估指标（如BLEU、COMET）依赖于字面重叠或形式化文本匹配，无法理解深层语义，导致评估结果与人类判断严重错位。\n4.  **初步假设**：现有的评估体系已经无法适应LLM时代的非字面翻译需求，必须重新审视评估的可靠性。\n\n### 第二阶段：假设验证与基准构建\n**（从“怀疑指标”到“量化失效”）**\n\n1.  **验证策略**：为了系统性地证明“现有指标不可靠”，作者需要一个专门的测试集。然而，现有的Meta-evaluation数据集（如WMT）多基于新闻或维基百科，缺乏非字面内容。\n2.  **构建MENT数据集**：作者决定构建一个专注于非字面翻译的Meta-evaluation数据集（MENT）。\n    *   **覆盖范围**：选取了四个最具代表性的非字面领域（SNS、跨文化、诗歌、文学）。\n    *   **数据质量**：通过严格的筛选和人工标注，确保数据集包含高难度的语言学挑战。\n3.  **实验验证**：在MENT上测试传统指标和新兴的“LLM-as-a-Judge”方法。\n4.  **发现新问题**：实验证实了传统指标确实失效。虽然“LLM-as-a-Judge”表现较好，但作者敏锐地发现了其两个致命缺陷：\n    *   **知识截止**：LLM无法理解最新的网络流行语或生僻的文化典故。\n    *   **评分不一致**：LLM在打分时存在主观性和波动性，缺乏校准机制。\n\n### 第三阶段：根因分析与思维转向\n**（从“静态评估”到“动态反思”）**\n\n1.  **根因诊断**：作者意识到，单纯依赖LLM的内部参数知识（静态）和单次Prompt（被动）是无法解决上述问题的。\n    *   针对“知识截止”，评估者必须具备**外部检索能力**。\n    *   针对“评分不一致”，评估者必须具备**自我反思与校准能力**。\n2.  **思维跃迁**：作者不再将评估视为一个简单的“输入文本-输出分数”的函数，而是将其建模为一个**需要多步推理、工具调用和决策的智能过程**。这自然引出了“Agent（智能体）”的概念。\n\n### 第四阶段：方法论设计\n**（从“单一模型”到“多智能体协作框架 RATE”）**\n\n为了解决上述根因，作者设计了 **RATE (Reflective Agentic Translation Evaluation)** 框架，其设计逻辑遵循“分而治之”与“动态编排”：\n\n1.  **核心架构设计**：需要一个“大脑”来统筹全局，而不是固定的流水线。因此设计了 **Core Agent（核心智能体）**，采用OODA（观察-调整-决策-行动）循环，根据当前状态动态决定下一步动作。\n2.  **解决“知识截止” -> Search Agent**：\n    *   *思考*：当Core Agent遇到未知俚语或文化背景时，不应瞎猜，而应去查。\n    *   *实现*：设计 **Search Agent**，负责调用搜索引擎获取实时外部知识，并将背景信息回传给Core Agent。\n3.  **解决“评分不一致” -> Comparison Agent**：\n    *   *思考*：绝对分数（如3.5分）很难把握，但相对好坏（A比B好）更容易判断。\n    *   *实现*：设计 **Comparison Agent**，通过将当前译文与历史锚点进行成对比较，来校准分数，将主观判断转化为相对排序。\n4.  **基础评估 -> Evaluation Agent**：\n    *   *思考*：仍需要一个基础模块来执行具体的打分任务。\n    *   *实现*：设计 **Evaluation Agent**，结合Core Agent提供的背景知识，进行初步打分并标记置信度。\n\n### 第五阶段：验证与泛化\n**（从“特定领域”到“通用鲁棒性”）**\n\n1.  **闭环验证**：在MENT数据集上测试RATE。逻辑是：如果RATE确实解决了知识截止和评分不一致，那么它在非字面翻译上的表现应显著优于所有Baseline。实验结果证实了这一点（Meta score提升至少3.2）。\n2.  **鲁棒性检验**：作者进一步思考：这种复杂的Agent框架是否只适用于刁钻的非字面场景？在通用领域（如WMT23）是否会“杀鸡用牛刀”甚至性能下降？\n3.  **结论**：实验证明，由于Core Agent的动态调度能力，RATE在通用领域也能保持与SOTA相当的性能，证明了该方法的普适性和鲁棒性。\n\n---\n\n**总结：作者的思考路径**\n从**发现LLM应用场景下沉带来的评估错位**出发，通过**构建MENT数据集量化了传统方法和静态LLM的缺陷**，进而**诊断出“知识缺失”和“主观波动”两大痛点**，最终**跳出单一模型的思维定式，利用Agent技术构建了一个具备反思、检索和校准能力的动态评估框架（RATE）**，完成了从问题发现到方法创新的全逻辑闭环。"
                },
                {
                    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
                    "arxiv_id": "2601.07264",
                    "authors": "Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya",
                    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究基于LLM的自主智能体，特别是聚焦于“工具使用智能体”。它探讨了智能体在工具集成工作流中的校准问题（即自我反思/自我意识），并提出了通过强化学习优化智能体性能和校准的方法。这符合“单智能体”中关于“工具使用”和“自我反思”的研究范围，不属于纯应用、纯推理或基础设施优化等排除类别。",
                    "summary2": "本文旨在解决Tool-use agents中的miscalibration问题。针对Evidence tools导致overconfidence的场景，我们提出了Calibration Agentic RL (CAR)框架，利用Margin-Separated Calibration Reward (MSCR)联合优化任务准确性与校准。我们在NQ、HotpotQA、SimpleQA-verified及AIME、MATH-500数据集上，通过Accuracy、ECE、Brier Score和AUROC验证了其有效性，显著提升了模型的校准能力与泛化性。",
                    "summary_translation": "基于大语言模型 (LLMs) 的自主代理正在快速发展以处理多轮任务，但确保其可信度仍然是一个关键挑战。这种可信度的一个基本支柱是校准，它指的是代理表达能够可靠反映其实际性能的置信度的能力。尽管校准在静态模型中已有深入研究，但其在集成工具的代理工作流中的动态变化仍未被充分探索。在这项工作中，我们系统地调查了工具使用代理中的语言化校准，揭示了由工具类型驱动的基本置信度二分法。具体而言，我们的试点研究表明，证据工具（如 web search）由于检索信息中固有的噪声，会系统性地导致严重的过度自信，而验证工具（如 code interpreters）可以通过确定性反馈来锚定推理并减轻校准偏差。为了在不同工具类型间稳健地提升校准性能，我们提出了一种强化学习 (RL) 微调框架，该框架联合优化任务准确性和校准性能，并得到了全面的奖励设计基准的支持。我们证明，经过训练的代理不仅实现了卓越的校准性能，而且表现出从本地训练环境到嘈杂的网络设置以及数学推理等不同领域的稳健泛化能力。我们的结果强调了针对工具使用代理采用特定领域校准策略的必要性。更广泛地说，这项工作为构建能够在高风险的现实世界部署中可靠地传达不确定性的自我感知代理奠定了基础。",
                    "inspiration_trace": "基于论文《The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents》，以下是对作者核心方法论产出逻辑链的系统推演：\n\n### 1. 宏观背景与核心矛盾：从“能力”到“可信度”\n**思考起点：**\n随着大语言模型（LLM）向智能体演进，工具使用（如搜索、代码解释器）极大地扩展了模型的能力边界。然而，作者敏锐地捕捉到了一个被忽视的关键问题：**信任危机**。\n**逻辑推演：**\n*   现有研究多关注智能体“能不能做”，而忽略了“知不知道自己能不能做”。\n*   在高风险场景下，智能体的**校准**能力——即其表达的置信度与实际表现的一致性——是可信度的基石。\n*   **初步观察：** 现有文献指出，引入工具后，智能体往往比静态模型表现出更严重的过度自信。这引发了一个根本性的疑问：**工具使用本身是否就是导致校准失效的元凶？**\n\n### 2. 深入探究与假设提出：打破“工具”的刻板印象\n**思考转折：**\n作者没有接受“工具导致过度自信”这一笼统结论，而是试图解构“工具”这一概念。\n**逻辑推演：**\n*   **假设：** 并非所有工具都对校准产生相同影响。工具的**性质**（反馈机制、输出确定性）可能决定了其对置信度的不同影响。\n*   **分类维度：** 作者将工具划分为两类典型范式：\n    1.  **证据工具：** 如网络搜索。特征是输出开放、充满噪声、缺乏明确的负向反馈（搜索总是有结果的，无论是否相关）。\n    2.  **验证工具：** 如代码解释器。特征是输出确定、提供执行反馈（代码会报错），能提供逻辑上的“落地”。\n\n### 3. 验证与发现：揭示“置信度二分法”\n**思考过程：**\n通过设计对比实验（直接提示 vs. 工具使用 vs. RL微调），作者验证了上述假设，发现了核心现象——**置信度二分法**。\n**逻辑推演：**\n*   **证据工具的陷阱：** 在使用网络搜索时，智能体表现出严重的过度自信。原因在于“检索行为”本身被模型误认为是“尽职调查”，且检索到的噪声信息被误认为确凿证据，导致虚假的确定性。\n*   **验证工具的锚定：** 在使用代码解释器时，智能体的校准度反而提升。因为确定性的执行反馈（如报错信息）为推理过程提供了现实约束，抑制了盲目的自信。\n*   **结论：** 校准失效并非工具使用的普遍后果，而是特定于**证据工具**带来的噪声干扰。这指明了后续研究的靶心：**如何修复证据工具导致的过度自信？**\n\n### 4. 方法论构建：从“提示工程”到“内在校准”\n**思考转折：**\n既然证据工具的噪声无法完全消除，且简单的提示工程无法解决根本问题（实验表明Prompting-based策略依然失效），作者转向通过训练来改变模型的内在置信度生成机制。\n**逻辑推演：**\n*   **技术选型：** 采用强化学习（RL）进行微调，因为智能体本身就是通过RL训练来使用工具的，这能保持任务能力的连贯性。\n*   **核心挑战：** 如何设计奖励函数？传统的奖励仅关注任务准确性，这往往鼓励模型“瞎猜”或过度自信。引入校准项（如Brier Score）虽然能惩罚置信度偏差，但存在一个隐患：**激励重叠**。\n\n### 5. 核心创新：解决“安全失败”的激励冲突\n**思考深化：**\n作者深入分析了现有校准奖励（如RLCR）的缺陷，发现了一个逻辑漏洞：如果对“低置信度的错误回答”给予过高的奖励（因为它诚实），模型可能会学会“安全失败”——即为了获得校准分而故意降低置信度，甚至放弃尝试正确回答。\n**逻辑推演：**\n*   **设计原则：** 必须建立严格的优先级。**“做对”必须永远优于“做错”**，无论置信度如何。\n*   **方案提出：** **边际分离校准奖励**。\n    *   **机制：** 强制将奖励空间划分为两个互不重叠的区域。所有正确答案的奖励下限，必须高于所有错误答案的奖励上限。\n    *   **效果：** 这消除了模型通过“诚实但错误”来投机取巧的动机，迫使模型在追求正确性的前提下，再去优化置信度的表达。\n\n### 6. 验证与泛化：从实验室到现实世界\n**思考闭环：**\n为了证明CAR框架不仅仅是过拟合训练数据，作者设计了更具挑战性的验证场景。\n**逻辑推演：**\n*   **环境泛化：** 从干净的本地检索环境迁移到充满噪声的真实API环境（如Serper API）。结果证明，模型学到的不是死记硬背的特定置信度值，而是一种对不确定性的感知能力。\n*   **领域泛化：** 将该方法应用于数学推理（验证工具场景）。虽然验证工具本身有助于校准，但CAR框架依然能带来额外提升，证明了该方法的通用性。\n\n### 总结：思想演进脉络\n1.  **观察：** 智能体越强，越容易盲目自信（可信度危机）。\n2.  **质疑：** 是所有工具都导致盲目自信吗？\n3.  **发现：** 只有“证据工具”（如搜索）因噪声导致过度自信，而“验证工具”（如代码）反而能锚定置信度（二分法）。\n4.  **定位：** 重点解决证据工具场景下的校准问题。\n5.  **洞察：** 现有的校准训练方法存在“安全失败”的漏洞，可能鼓励模型“躺平”。\n6.  **解决：** 提出CAR框架与MSCR奖励，通过严格分离正确与错误的奖励边界，迫使模型在追求准确的同时学会表达不确定性。"
                },
                {
                    "title": "Measuring Iterative Temporal Reasoning with TimePuzzles",
                    "arxiv_id": "2601.07148",
                    "authors": "Zhengxiang Wang, Zeyu Dong",
                    "summary": "We introduce TimePuzzles, a constraint-based date inference task for evaluating iterative temporal reasoning. Each puzzle combines factual temporal anchors with (cross-cultural) calendar relations, admits one or multiple valid solution dates, and is algorithmically generated for controlled, dynamic, and continual evaluation. Across 13 diverse LLMs, TimePuzzles well distinguishes their iterative temporal reasoning capabilities and remains challenging without tools: GPT-5 reaches only 49.3% accuracy and all other models stay below 31%, despite the dataset's simplicity. Web search consistently yields substantial gains and using code interpreter shows mixed effects, but all models perform much better when constraints are rewritten with explicit dates, revealing a gap in reliable tool use. Overall, TimePuzzles presents a simple, cost-effective diagnostic for tool-augmented iterative temporal reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了TimePuzzles基准，专门用于评估LLM在迭代时间推理中的工具使用能力（如Web搜索和代码解释器），并指出了可靠工具使用方面的差距，属于单智能体中的工具使用研究范畴。",
                    "summary2": "本文旨在评估大语言模型在工具辅助下的迭代时间推理能力。针对现有基准无法有效评估工具增强迭代推理的问题，我们提出了Time Puzzles，一种基于约束的日期推理任务。该任务通过算法生成结合事实锚点和日历关系的谜题。我们在13个LLMs上通过Exact Match (EM)指标验证了其有效性，发现即使使用工具，模型在处理隐式约束时仍面临挑战。",
                    "summary_translation": "我们提出了TimePuzzles，这是一种基于约束的日期推理任务，旨在评估迭代时间推理能力。每个谜题结合了事实性时间锚点与（跨文化）历法关系，具有一个或多个有效解日期，并通过算法生成，以实现可控、动态且持续的评估。在13种不同的LLMs（大语言模型）中，TimePuzzles能够有效区分其迭代时间推理能力，且在无工具辅助的情况下仍极具挑战性：尽管数据集本身很简单，但GPT-5的准确率仅达到49.3%，而所有其他模型的准确率均低于31%。Web search（网络搜索）能带来显著提升，而使用code interpreter（代码解释器）的效果则参差不齐；但当约束条件被重写为明确日期时，所有模型的表现均有大幅提升，这揭示了在可靠工具使用方面存在的差距。总而言之，TimePuzzles为tool-augmented（工具增强的）迭代时间推理提供了一种简单且具有成本效益的诊断方法。",
                    "inspiration_trace": "基于论文《Measuring Iterative Temporal Reasoning with Time Puzzles》，以下是对作者产出该文章核心思想的逻辑链推演：\n\n### 1. 宏观观察：从人类认知到AI趋势\n**逻辑起点：** 作者首先关注到人类处理时间信息的基本模式。\n*   **观察：** 人类在处理复杂时间问题时，极少仅凭大脑的死记硬背，而是高度依赖外部工具（如日历、搜索引擎、历史记录）进行辅助推理和验证。\n*   **背景：** 随着LLM向“智能体”演进，工具使用能力成为核心。然而，现有的评估体系似乎并未跟上这一趋势。\n\n### 2. 问题聚焦：现有基准的错位\n**核心矛盾：** 现有的时间推理评估方式与实际应用场景存在脱节。\n*   **现状分析：** 现有的时间推理基准大多基于静态数据集，侧重于考察模型的“单次检索”或“参数化记忆”能力。\n*   **识别缺口：** 缺乏对**“迭代式”**（Iterative）和**“工具增强”**（Tool-augmented）推理能力的评估。即，模型是否懂得在推理过程中多次调用工具、逐步缩小范围并验证假设？\n\n### 3. 假设提出：约束满足作为测试载体\n**核心思想：** 为了测试“迭代推理”，需要一个必须分步解决的问题。\n*   **类比思维：** 作者将时间推理类比为“解谜”或“约束满足问题”。\n*   **任务构想：** 设计一个任务，给定一组自然语言描述的时间约束（如“这是某年某月的第几个星期天”），要求模型推断出满足所有条件的具体日期。\n*   **关键特性：** 这个任务不能通过一次查询解决，必须通过“提出候选 -> 工具验证 -> 调整候选”的循环来完成。\n\n### 4. 方法论构建：逆向生成与动态对抗\n**实现路径：** 如何构建这样一个既需要工具又难以被死记硬背的数据集？\n*   **逆向思维：** 传统的QA是“问题->答案”，作者采用“答案->问题”的生成逻辑。先随机选定一个“种子日期”，然后基于该日期反向生成一系列事实约束和日历约束。\n*   **对抗性设计：** 为了防止模型依赖训练数据中的记忆，作者引入了算法生成，确保数据集是动态的、无限的，且包含跨文化日历（如农历）等非直觉知识。\n*   **控制变量：** 为了剥离“知识检索”和“逻辑推理”的混淆，作者设计了两组数据：\n    *   **隐式约束：** 包含需要搜索的事实（如“肯尼迪总统任期内”），强制使用工具。\n    *   **显式约束：** 直接给出日期（如“1961年”），仅测试纯逻辑推理能力。\n\n### 5. 实验验证：诊断工具使用的可靠性\n**验证逻辑：** 通过对比不同条件下的模型表现，验证假设并发现新问题。\n*   **基线测试：** 在不使用工具的情况下，即使是GPT-5表现也很差（49.3%），证实了任务的难度和对工具的必要性。\n*   **工具介入测试：** 开启网络搜索后，模型性能提升，但远未达到显式约束下的水平。\n*   **关键发现：** 模型在“显式约束”下表现很好，但在“隐式约束+工具”下表现不佳。这揭示了当前LLM的核心短板：**不是缺乏工具，而是缺乏可靠地整合工具信息进行迭代推理的能力。** 模型往往搜到了信息，却无法将其正确转化为逻辑约束。\n\n### 6. 结论升华：从评估到诊断\n**最终产出：** TimePuzzles不仅仅是一个排行榜，而是一个“诊断仪”。\n*   **价值定位：** 作者将该方法定位为一种低成本、高效率的诊断工具，专门用于检测模型在工具增强环境下的逻辑一致性和迭代推理能力。\n*   **启示：** 论文最终指出，未来的AI发展不应只关注工具的调用率，更应关注工具调用后的信息整合与推理闭环。\n\n---\n\n**总结：**\n作者的思考路径是从**人类真实行为（依赖工具迭代推理）**出发，发现**现有评估的静态性缺陷**，进而提出**基于约束满足的“解谜”范式**，通过**逆向生成算法**保证数据的动态性，最后通过**隐式与显式的对比实验**，精准定位了当前LLM在工具使用上的“整合鸿沟”。"
                },
                {
                    "title": "ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity A REM-Inspired System Design for Emergent Creative Ideation",
                    "arxiv_id": "2601.07121",
                    "authors": "Makoto Sato",
                    "summary": "Large language models (LLMs) are used not only for problem solving but also for creative ideation; however, eliciting serendipitous insights that are both novel and internally coherent remains difficult. While stochastic sampling promotes novelty, it often degrades consistency. Here, we propose ReMIND, a REM-inspired modular framework for ideation. ReMIND consists of four stages: wake, which generates a stable low-temperature semantic baseline; dream, which performs high-temperature exploratory generation; judge, which applies coarse evaluation to filter incoherent outputs and extract candidate ideas; and re-wake, which re-articulates selected ideas into coherent final outputs. By instantiating each stage as an independent LLM, ReMIND enables functional separation between exploration and consolidation. Parameter sweeps show that ReMIND reliably induces semantic exploration while preserving downstream stability. Embedding-based analyses confirm substantial semantic displacement during the dream phase, whereas external evaluations reveal that high-quality ideas emerge sporadically rather than as extrema along any single metric. These results suggest that serendipitous ideation in LLMs is a rare-event process best approached through system level design that shapes the conditions under which valuable ideas can emerge and be stabilized. ReMIND provides a general framework for studying the computational basis of serendipity and illustrates how modular LLM orchestration can bridge exploration and stabilization.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出的ReMIND框架通过编排多个独立的LLM模块（wake, dream, judge, re-wake）来执行创意构思任务。这种架构包含了工作流规划、自我评估（judge阶段）以及基于反馈的迭代完善（re-wake阶段），符合单智能体中关于规划、自我反思及模块化设计的定义。",
                    "summary2": "本文旨在解决LLM难以生成兼具新颖性与连贯性的Serendipitous insights的问题。针对Creative ideation场景，我们提出了一种受REM睡眠启发的模块化框架ReMIND，通过Wake、Dream、Judge和Re-wake四个阶段实现探索与巩固的功能分离。在多种Conceptual pair prompts上，通过外部LLM评估和Embedding-based similarity analysis验证了其有效性。",
                    "summary_translation": "大语言模型不仅用于问题解决，还用于创造性构思；然而，要诱导出既新颖又内部连贯的意外洞察仍然困难重重。虽然随机采样有助于提升新颖性，但往往会损害一致性。在此，我们提出了 ReMIND，这是一种受 REM (快速眼动睡眠) 启发的模块化构思框架。ReMIND 包含四个阶段：wake (清醒)，用于生成稳定的低温度语义基线；dream (做梦)，用于执行高温度的探索性生成；judge (评判)，用于应用粗粒度评估以过滤不连贯的输出并提取候选想法；以及 re-wake (再清醒)，用于将选定的想法重新阐述为连贯的最终输出。通过将每个阶段实例化为一个独立的 LLM (大语言模型)，ReMIND 实现了探索与巩固之间的功能分离。参数扫描表明，ReMIND 能够可靠地诱导语义探索，同时保持下游稳定性。基于嵌入的分析证实，在 dream (做梦) 阶段存在显著的语义位移，而外部评估显示，高质量想法是零星出现的，而非作为任何单一指标的极值而存在。这些结果表明，LLM 中的意外构思是一个稀有事件过程，最好通过系统级设计来应对，这种设计塑造了有价值想法涌现并得以稳定的条件。ReMIND 为研究机缘巧合的计算基础提供了一个通用框架，并阐明了模块化 LLM (大语言模型) 编排如何桥接探索与稳定。",
                    "inspiration_trace": "基于对论文《ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：问题识别与核心矛盾\n**（从宏观现象出发）**\n\n1.  **观察现状**：大语言模型（LLMs）已被广泛应用于创意生成，但面临一个根本性瓶颈——难以同时兼顾“新颖性”与“连贯性”。\n2.  **剖析矛盾**：\n    *   **高随机性**：提高采样温度虽然能增加新颖性，但往往导致输出语无伦次、事实错误。\n    *   **低随机性**：降低温度保证了逻辑连贯，但输出多为陈词滥调，缺乏真正的洞察力。\n3.  **现有方法的局限**：目前的创意生成主要依赖“单次生成”，试图在同一个模型实例中平衡探索与约束。作者意识到，这种“单体模型”的范式本质上将创造力视为一种参数调优的权衡，而非一种可被系统化设计的认知过程。\n\n### 第二阶段：跨学科启发与理论假设\n**（引入生物学视角）**\n\n1.  **寻找灵感**：作者将目光转向人类认知科学，特别是关于“顿悟”产生机制的研究。\n2.  **关键隐喻——REM睡眠**：心理学和神经科学研究表明，人类的创造性洞察常发生在快速眼动（REM）睡眠期间。\n    *   **机制**：在REM阶段，大脑的海马体进行广泛的联想探索（记忆重组、概念松绑），而负责逻辑判断的前额叶皮层活动减弱（去抑制）。\n    *   **后续**：这种不受约束的探索之后，必须经历一个“稳定化”阶段，将碎片化的洞察重新整合进清醒时的逻辑框架中。\n3.  **提出假设**：如果人工系统的创造力也遵循这一机制，那么解决LLM创意瓶颈的关键不在于优化单一模型的参数，而在于**功能解耦**——将“探索”与“巩固”在时间和计算上分离开来。\n\n### 第三阶段：方法论构建与架构设计\n**（从理论到系统设计）**\n\n1.  **设计原则**：构建一个受REM启发的模块化框架，明确划分“探索”、“评估”和“巩固”三个阶段。\n2.  **模块定义与功能映射**：\n    *   **Wake（清醒/锚点）**：使用低温度采样，生成一个稳定、符合逻辑的基线输出。其作用不是产生创意，而是作为语义锚点，确保后续生成不偏离主题。\n    *   **Dream（做梦/探索）**：使用高温度采样，故意放松逻辑约束，进行疯狂的语义跳跃和概念重组。这一阶段模拟REM睡眠，允许产生看似荒谬但可能蕴含潜力的组合。\n    *   **Judge（评判/筛选）**：作为一个独立的过滤器，不参与生成，仅评估Dream输出的连贯性，并提取出有潜力的“候选想法”。这模拟了大脑对记忆痕迹的初步筛选。\n    *   **Re-wake（再清醒/巩固）**：这是最关键的一步。重新调用Wake模型，将Judge筛选出的“碎片化创意”进行重述和润色。\n    *   **逻辑闭环**：通过Re-wake，系统利用低温度模型的逻辑能力，将高温度探索出的“狂野想法”驯化为人类可理解的、连贯的最终输出。\n3.  **核心创新点**：作者意识到，**同一个模型在不同阶段扮演不同角色**。Wake在第一阶段是“锚点”，在最后阶段变成了“稳定器/压缩器”。\n\n### 第四阶段：实验验证与现象洞察\n**（通过实证修正认知）**\n\n1.  **验证策略**：如何证明这种方法真的产生了“有意义的意外”？\n    *   **量化语义位移**：使用嵌入向量计算Wake输出与Dream输出的余弦相似度。数据证实，Dream阶段确实导致了显著的语义漂移（探索发生）。\n    *   **外部评估**：使用更强的外部模型（如GPT-5.2）对最终输出进行评分。\n2.  **关键发现**：\n    *   高质量创意并非均匀分布，而是**稀疏出现的**。\n    *   即使在相同参数下，也只有部分运行产生了极具价值的洞察。\n3.  **理论修正**：这一发现促使作者将“意外创意”重新定义为一种**“稀有事件过程”**。这意味着我们无法通过确定性算法“制造”创意，但可以通过系统设计**提高其涌现的概率**。\n\n### 第五阶段：哲学升华与范式转移\n**（最终结论）**\n\n1.  **总结范式**：作者最终提出，通往人工创造力的路径不是单纯扩大模型规模或增加数据量，而是**“思维的功能编排”**。\n2.  **系统观**：ReMIND不仅仅是一个提示技巧，它代表了一种新的系统设计哲学——**BiMoLLM（脑启发模块化LLM）**。即通过模块间的交互涌现出高阶智能，而非依赖单一模型的万能性。\n3.  **最终产出**：文章产出了一套可复现的、将生物学认知过程转化为计算架构的工程框架，证明了通过分离探索与巩固，可以在保持逻辑连贯性的同时，显著提升LLM产生意外洞察的能力。"
                },
                {
                    "title": "LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents",
                    "arxiv_id": "2601.06973",
                    "authors": "Davide Baldelli, Ali Parviz, Amal Zouaq, Sarath Chandar",
                    "summary": "As LLMs move from text completion toward autonomous agents, they remain constrained by the standard chat interface, which lacks private working memory. This raises a fundamental question: can agents reliably perform interactive tasks that depend on hidden state? We define Private State Interactive Tasks (PSITs), which require agents to generate and maintain hidden information while producing consistent public responses. We show theoretically that any agent restricted to the public conversation history cannot simultaneously preserve secrecy and consistency in PSITs, yielding an impossibility theorem. To empirically validate this limitation, we introduce a self-consistency testing protocol that evaluates whether agents can maintain a hidden secret across forked dialogue branches. Standard chat-based LLMs and retrieval-based memory baselines fail this test regardless of scale, demonstrating that semantic retrieval does not enable true state maintenance. To address this, we propose a novel architecture incorporating an explicit private working memory; we demonstrate that this mechanism restores consistency, establishing private state as a necessary component for interactive language agents.",
                    "category": "cs.CL",
                    "filter_reason": "该论文聚焦于语言智能体的架构设计，特别是针对“记忆”这一核心组件进行了深入研究。它提出了私有工作记忆的概念，以解决智能体在交互任务中维护隐藏状态的问题，属于单智能体研究中的记忆机制范畴。",
                    "summary2": "本文旨在解决LLM在交互任务中无法维护隐藏状态的问题。针对Private State Interactive Tasks (PSITs)，我们提出了一种引入显式Private Working Memory的架构，包含自主代理和工作流两种实现。我们在Hangman和Diagnosis Simulator任务上，通过Self-Consistency Testing Protocol验证了其有效性，结果显示该方法显著优于现有检索基线，在保持低Token开销的同时实现了近乎完美的状态一致性。",
                    "summary_translation": "随着 LLMs (Large Language Models，大语言模型) 从文本补全向 autonomous agents (自主代理) 演进，它们仍受限于缺乏 private working memory (私有工作记忆) 的 standard chat interface (标准聊天界面)。这引发了一个根本性问题：agents (代理) 是否能够可靠地执行依赖于 hidden state (隐藏状态) 的 interactive tasks (交互任务)？我们定义了 Private State Interactive Tasks (PSITs，私有状态交互任务)，该任务要求 agents (代理) 在生成一致的 public responses (公共响应) 的同时，生成并维护 hidden information (隐藏信息)。我们从理论上证明，任何仅限于 public conversation history (公共对话历史) 的 agent (代理) 都无法在 PSITs 中同时实现保密性和一致性，从而得出了一个 impossibility theorem (不可能性定理)。为了实证验证这一局限性，我们引入了一种 self-consistency testing protocol (自一致性测试协议)，用于评估 agents (代理) 是否能在 forked dialogue branches (分叉对话分支) 中维护一个 hidden secret (隐藏秘密)。无论规模大小，standard chat-based LLMs (标准基于聊天的 LLMs) 和 retrieval-based memory baselines (基于检索的记忆基线) 均未通过该测试，这表明 semantic retrieval (语义检索) 并不能实现真正的 state maintenance (状态维护)。为解决这一问题，我们提出了一种包含 explicit private working memory (显式私有工作记忆) 的 novel architecture (新颖架构)；我们证明该机制能够恢复一致性，从而确立了 private state (私有状态) 作为 interactive language agents (交互语言代理) 必要组件的地位。",
                    "inspiration_trace": "基于论文《LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察：从“文本补全”到“自主代理”的接口错位\n**思考起点**：随着LLM从单纯的文本生成器演变为具备工具使用和规划能力的“自主代理”，一个根本性的矛盾浮出水面——**代理的底层架构依然停留在“标准聊天接口”上**。\n**逻辑推演**：聊天接口本质上是“公开”的，所有的上下文对用户和模型都是可见的。然而，真正的智能代理在执行复杂任务（如博弈、谈判、角色扮演）时，往往需要维护一个“私有”的内部状态（如策略、秘密、未公开的推论）。\n**核心疑问**：现有的仅依赖公开对话历史的架构，是否足以支撑需要“隐藏状态”的交互任务？\n\n### 2. 现象聚焦：Hangman游戏中的“失忆”现象\n**具体案例**：作者选择了一个极简但极具代表性的任务——Hangman（猜词游戏）。\n**观察发现**：当LLM作为主持人（Host）时，它虽然能“假装”选了一个词，但在下一轮对话中，它实际上并没有记住这个词。它是在根据当前的约束条件（如“这个词没有字母A”）实时“编造”一个合理的词，而不是在验证一个固定的词。\n**初步结论**：这不仅仅是模型的幻觉问题，而是**架构层面的“无状态性”**。模型在生成响应后，其内部的思维链被丢弃，导致无法在多轮交互中维持一个动态生成的私有变量。\n\n### 3. 理论抽象：定义PSIT与证明“不可能定理”\n**概念定义**：为了将这一现象理论化，作者定义了**私有状态交互任务**。这类任务要求代理必须生成并维护一个隐藏的秘密，同时根据该秘密给出一致的公开回应。\n**逻辑推演**：作者提出了**仅公开聊天代理**的概念，即输出仅依赖于公开历史 $H_t$。\n**核心定理（Impossibility Theorem）**：作者从逻辑上证明了，如果一个代理只能访问公开历史，那么它**无法同时满足“保密性”和“一致性”**。\n*   *一致性*要求输出必须基于固定的秘密 $s$。\n*   *保密性*要求公开历史不能唯一确定 $s$。\n*   *矛盾点*：如果历史 $H_t$ 对多个可能的秘密（$s$ 和 $s'$）都是兼容的，那么基于 $H_t$ 生成的输出分布 $\\pi$ 必须同时满足这两个秘密的规则。当这两个秘密要求不同的输出时，$\\pi$ 必然失效。\n**结论**：现有的标准聊天架构在结构上无法解决PSITs。\n\n### 4. 假设提出：引入“私有工作记忆”的必要性\n**解决方案假设**：既然公开上下文无法承载私有状态，那么必须在架构中引入一个**独立于公开对话历史的私有工作记忆**。\n**设计思路**：这个记忆空间应当是持久的、对用户不可见的，并且能够被模型动态更新。它不仅仅是检索过去的对话（RAG），而是存储当前生成的“认知状态”。\n\n### 5. 验证方法：设计“自一致性测试协议”\n**如何验证假设？** 传统的问答无法检测模型是否真的“记住”了秘密。\n**创新设计**：作者提出了**对话分叉测试**。\n*   在交互进行到某一步时，将对话“分叉”。\n*   在分支中，询问模型：“秘密是词A吗？”以及“秘密是词B吗？”（A和B都符合当前的公开约束）。\n*   **判据**：如果模型同时肯定了A和B，说明它没有固定的私有状态（失败）；如果它只肯定了最初选定的那个词，说明它成功维护了私有状态（成功）。\n\n### 6. 方法论演进：从“自主代理”到“工作流”的架构选择\n**架构对比**：为了实现私有记忆，作者对比了两种范式：\n1.  **自主代理**：让LLM自己决定何时调用记忆工具。\n2.  **工作流**：强制执行一个确定性的两步流程（生成公开回复 -> 更新私有记忆）。\n**实验发现**：实验表明，**工作流**的表现优于自主代理。\n**逻辑解释**：自主代理将“何时更新记忆”的决策权交给LLM，增加了不确定性；而工作流将记忆更新固化为系统级操作，确保了状态更新的可靠性，从而解决了“生成-保留”的闭环问题。\n\n### 7. 核心洞察：区分“检索”与“保留”\n**最终升华**：作者通过实验发现，现有的RAG、Mem0等记忆增强方法（基于向量检索）在Hangman任务上全部失败。\n**逻辑总结**：这揭示了**“生成-保留鸿沟”**。\n*   现有的记忆系统是**被动检索型**的，用于回忆过去发生的事实。\n*   PSITs需要的是**主动保留型**的，用于存储模型自己生成的、尚未公开的中间状态。\n**结论**：私有工作记忆不是锦上添花的功能，而是构建具备一致性和保密性的交互式语言智能体的**必要组件**。"
                },
                {
                    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
                    "arxiv_id": "2601.06966",
                    "authors": "Haonan Bian, Zhiyuan Yao, Sen Hu, Zishan Xu, Shaolei Zhang, Yifu Guo, Ziliang Yang, Xueran Han, Huacan Wang, Ronghao Chen",
                    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了针对LLM智能体在长期项目导向交互中的记忆能力的基准测试（RealMem），明确涉及智能体的记忆机制（单智能体范畴）以及多智能体对话生成（多智能体范畴），符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有LLM记忆基准难以评估长期项目导向交互的问题。针对现实世界中动态演进的项目场景，我们提出了RealMem基准及其包含项目基础构建、多智能体对话生成及记忆日程管理的三阶段合成管道。我们在包含11个场景、2000+跨会话对话的RealMem数据集上，通过Recall@k、NDCG@k及QA Score等指标验证了其有效性，揭示了现有系统在动态状态管理上的不足。",
                    "summary_translation": "随着 Large Language Models (LLMs，大语言模型) 从静态对话接口演变为 autonomous general agents (自主通用智能体)，有效的 memory (记忆) 对于确保 long-term consistency (长期一致性) 至关重要。然而，现有的 benchmarks (基准测试) 主要关注 casual conversation (日常闲聊) 或 task-oriented dialogue (任务导向对话)，未能捕捉到 **“long-term project-oriented” (长期项目导向)** 的交互，在此类交互中，agents (智能体) 必须跟踪 evolving goals (不断演进的目标)。为了弥合这一差距，我们介绍了 **RealMem**，这是首个 grounded in realistic project scenarios (基于现实项目场景) 的 benchmark (基准测试)。RealMem 包含跨越 11 个场景的 2,000 多个 cross-session dialogues (跨会话对话)，并利用 natural user queries (自然用户查询) 进行评估。我们提出了一个 synthesis pipeline (合成流程)，该流程整合了 Project Foundation Construction (项目基础构建)、Multi-Agent Dialogue Generation (多智能体对话生成) 以及 Memory and Schedule Management (记忆与日程管理)，以模拟 memory (记忆) 的 dynamic evolution (动态演变)。实验表明，当前的 memory systems (记忆系统) 在管理现实世界项目中固有的 long-term project states (长期项目状态) 和 dynamic context dependencies (动态上下文依赖) 方面面临重大挑战。我们的代码和数据集可在 [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench) 获取。",
                    "inspiration_trace": "基于论文《RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体产出的思考过程：\n\n### 1. 宏观观察：从“对话”到“代理”的范式转变\n**思考起点**：作者首先注意到大语言模型（LLMs）的发展趋势正在发生质变。模型不再仅仅是简单的“聊天机器人”，而是正在演变为能够长期协作的“自主智能体”。\n**核心洞察**：在这种新范式下，**“记忆”** 成为了决定性因素。没有有效的记忆，智能体就无法在长期交互中保持一致性，也无法实现真正的个性化与通用人工智能（AGI）。\n\n### 2. 问题聚焦：现有基准的“静态”局限\n**观察现状**：作者审视了现有的记忆评估基准（如 LoCoMo, LongMemEval, HaluMem），发现它们大多存在一个共同的缺陷：**过于“静态”和“孤立”**。\n*   **LoCoMo** 仅关注社交闲聊。\n*   **LongMemEval** 类似于“大海捞针”测试，侧重于孤立的事实检索。\n**提出质疑**：这些基准测试的是“你记住了某个事实吗？”，而不是“你能否在复杂、变化的环境中利用记忆推进项目？”。它们无法反映真实世界中**长期、跨会话、目标导向**的交互逻辑。\n\n### 3. 核心假设：真实交互是“项目导向”的\n**定义新范式**：作者提出，真实世界的记忆驱动交互应当属于第三种范式——**“长期项目导向交互”**。\n**提炼特征**：为了构建这一新范式，作者抽象出了四个关键特征，这也是后续方法设计的指导原则：\n1.  **内生性查询**：问题源于任务进展，而非孤立的事实核查。\n2.  **交错分布**：对话在多个项目间穿插（如健身与旅行计划交替进行）。\n3.  **动态状态演化**：环境非静止，记忆需随状态（如受伤、计划变更）同步更新。\n4.  **主动上下文对齐**：智能体需利用记忆主动推断模糊意图，而非被动应答。\n\n### 4. 方法构建：如何模拟“动态演化”？\n**面临的挑战**：如何获取包含数千次跨会话对话、且具有复杂逻辑一致性的真实数据？显然，人工标注不现实，现有数据集也不存在。\n**解决思路**：作者决定采用**合成数据**的方法，但必须解决“长期生成容易逻辑崩塌”的问题。为此，设计了一个**三阶段合成流水线**：\n\n*   **阶段一：项目基础构建**\n    *   *思考*：先搭骨架，再填血肉。\n    *   *逻辑*：先定义用户画像和项目目标，再生成“蓝图”和“事件列表”。这确保了全局逻辑的连贯性，防止后续对话跑偏。\n\n*   **阶段二：多智能体对话生成**\n    *   *思考*：模拟真实博弈，而非单向生成。\n    *   *逻辑*：引入“用户智能体”和“助手智能体”。用户智能体只能看到当前会话摘要（模拟人类遗忘），助手智能体拥有完整记忆。这种不对称信息设置迫使模型必须依赖记忆机制来维持对话。\n\n*   **阶段三：记忆与日程管理**\n    *   *思考*：形成闭环反馈，确保记忆“活着”。\n    *   *逻辑*：对话生成后，通过专门的代理提取记忆点、更新日程表、去重。这些更新后的记忆又会作为下一轮对话的上下文输入。这模拟了记忆随时间动态演化的过程。\n\n### 5. 评估洞察：从“检索”到“状态管理”\n**重新定义评估标准**：作者意识到，传统的检索指标（如 Recall）不足以衡量项目导向任务。\n**逻辑推演**：在复杂项目中，**精确度**比**召回率**更重要。如果检索到了大量相关但充满噪音的信息，反而会干扰模型决策。\n**新指标设计**：因此，作者引入了基于 LLM 的语义评估（如 Mem Recall, Mem Helpful）和 QA Score，重点考察模型是否正确利用了**动态状态**，而不仅仅是生成了流畅的文本。\n\n### 6. 最终产出：RealMem 的诞生\n**结论验证**：通过实验，作者发现现有的 SOTA 记忆系统（如 Mem0, MemoryOS）在处理动态更新和主动对齐时依然表现不佳，证明了该基准的有效性和挑战性。\n**价值定位**：RealMem 不仅仅是一个数据集，它是一个**诊断工具**，揭示了当前智能体在处理长期、复杂、动态项目时的核心瓶颈，迫使社区从“静态知识库”向“动态状态管理器”转变。\n\n---\n\n**总结**：作者的思考路径是从**“智能体需要长期记忆”**这一宏观趋势出发，通过批判现有基准的**“静态性”**，提出了**“项目导向”**的动态交互假设，进而通过**“分层合成+闭环反馈”**的方法论解决了数据构建难题，最终建立了一套能够真实反映智能体动态记忆管理能力的评估体系。"
                },
                {
                    "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
                    "arxiv_id": "2601.06922",
                    "authors": "Tianhua Zhang, Kun Li, Junan Li, Yunxiang Li, Hongyin Luo, Xixin Wu, James Glass, Helen Meng",
                    "summary": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“Agentic RAG”，将问答视为推理与检索工具之间的多步交互过程。提出的TreePS-RAG框架利用强化学习和树结构优化智能体的决策（规划与工具使用），属于单智能体研究范畴。",
                    "summary2": "本文旨在解决Agentic RAG中仅结果监督的强化学习面临的信用分配难题。针对多步推理与检索交互场景，我们提出了一种名为TREE PS-RAG的在线树结构强化学习框架，该方法将推理过程建模为树，利用蒙特卡洛估计从后代结果中推导步骤优势，无需中间标注。在七个QA基准上通过Exact Match指标验证，其性能显著优于现有基线。",
                    "summary_translation": "代理式检索增强生成将问答任务构建为推理与信息检索之间的多步交互过程，并近期通过基于结果的监督强化学习得到了推进。尽管行之有效，但仅依赖稀疏的最终奖励限制了逐步信用分配，且对中间推理和动作的指导作用较弱。近期的研究探索了过程级监督，但通常依赖于离线构建的训练数据（存在分布偏移的风险），或者需要高昂成本的中间标注。本文提出了 TreePS-RAG，这是一种用于代理式 RAG 的在线、基于树的强化学习框架，能够在保留标准仅基于结果奖励的同时实现逐步信用分配。我们的核心思想是将代理式 RAG 的推理过程建模为一棵推演树，其中每个推理步骤自然地映射为一个节点。这种树结构允许通过对其后代结果进行蒙特卡洛估计来估算步骤效用，从而在无需中间标签的情况下获得细粒度的过程优势。为了使该范式具有实用性，我们引入了一种高效的在线树构建策略，能够在受限的计算预算下保持探索多样性。在与 Search-R1 等强基线相当的推演成本下，在多个模型规模的七个多跳和通用问答基准上进行的实验表明，TreePS-RAG 始终显著优于基于结果监督和领先的过程监督强化学习方法。",
                    "inspiration_trace": "基于论文《TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG》的内容，以下是对作者核心方法论产出过程的逻辑推演与还原：\n\n### 第一阶段：问题锚定——从“结果导向”到“过程黑箱”的困境\n\n**1. 宏观观察：**\n作者首先关注到 Agentic RAG（智能体检索增强生成）已成为解决复杂多跳问答的主流范式。为了优化这一过程，学术界引入了强化学习（RL），特别是像 Search-R1 这样的方法，利用最终答案的正确性作为奖励信号来训练智能体。\n\n**2. 痛点识别：**\n然而，作者敏锐地发现这种仅依赖“结果监督”的方法存在一个核心缺陷：**信用分配难题**。\n*   **逻辑推演：** 在一个多步推理的轨迹中，如果最终答案错误，RL 算法通常会对所有步骤进行“连坐”惩罚。但实际上，可能只有中间某一步的检索或推理是致命的，其他步骤可能是正确的。\n*   **结论：** 稀疏且滞后的最终奖励无法提供细粒度的指导，限制了智能体学习高效搜索和推理策略的能力。\n\n### 第二阶段：路径探索——理想与现实的博弈\n\n**1. 提出假设：**\n既然结果监督太粗糙，那么“过程监督”显然是更好的选择。即，对每一个中间步骤（如搜索查询、推理片段）都给出一个即时奖励。\n\n**2. 现实阻碍：**\n作者审视了现有的过程监督方案，发现了两个不可忽视的障碍：\n*   **标注成本高：** 获取高质量的中间步骤标注（如每一步的子问题是否正确）极其昂贵。\n*   **分布偏移：** 许多方法（如 ReasonRAG）依赖离线构建的数据集进行训练。这意味着智能体是在“静态”的过去数据上学习，而非在“动态”的在线交互中学习，导致模型在面对新环境时泛化能力下降。\n\n**3. 核心矛盾：**\n我们需要**细粒度的过程信号**，但我们必须在**无中间标注**且**在线**的约束下获得它。\n\n### 第三阶段：核心洞察——将“树”转化为“自监督工具”\n\n**1. 思维跃迁：**\n如何在不依赖外部标注者的情况下评估一个中间步骤的好坏？作者借鉴了蒙特卡洛树搜索（MCTS）的思想，提出了一个关键假设：\n*   **假设：** 如果一个中间步骤（节点）是好的，那么从该步骤出发，通过多次随机探索（rollout），最终得到正确答案的概率应该很高。\n\n**2. 结构化建模：**\n基于上述假设，作者将 Agentic RAG 的推理过程重新定义为**树结构**，而非线性的轨迹。\n*   **逻辑映射：** 每一个推理步骤对应树上的一个节点。从根节点到叶节点的路径代表一条完整的推理轨迹。\n*   **价值反推：** 不需要人为给中间步骤打分。只需看该节点下的所有“子孙”叶节点（最终结果）的平均奖励。如果后代大多答对了，那么这个中间节点的价值就高。\n\n**3. 解决矛盾：**\n这种方法巧妙地绕过了“标注”和“离线”的障碍：\n*   **无标注：** 价值估计完全基于易于获取的最终答案。\n*   **在线性：** 树是在训练过程中实时构建和探索的，完全符合在线 RL 的范式。\n\n### 第四阶段：工程落地——在有限预算下驯服“指数爆炸”\n\n**1. 新的挑战：**\n虽然树结构在理论上完美，但在实际计算中，随着深度增加，节点数量会呈指数级爆炸。如果无限制地展开树，计算成本将不可接受。\n\n**2. 约束设定：**\n作者设定了一个硬性约束：**计算成本必须与传统的线性采样方法（如 Search-R1）相当**。即，总采样节点数 $N$ 必须固定。\n\n**3. 策略优化：**\n为了在固定预算 $N$ 下最大化树的效用，作者引入了两个关键机制：\n*   **动态分支控制：** 不再平均用力，而是根据当前层的节点数量动态分配下一层的分支数，确保总节点数维持在预算 $N$ 附近。\n*   **语义剪枝：** 作者意识到，如果两个搜索步骤检索到的文档高度重合，那么它们就是冗余的。为了在有限预算下探索更多可能性，必须去除冗余。\n    *   **逻辑：** 利用检索文档的 Jaccard 相似度来衡量节点间的语义距离，通过聚类保留多样化的路径，剔除重复探索。\n\n### 第五阶段：方法闭环——从树结构到 RL 优化\n\n**1. 信号生成：**\n通过上述构建的树，作者计算出了每个节点的“过程优势”。\n*   **全局优势：** 当前步骤相对于根节点（整体平均水平）的提升。\n*   **局部优势：** 当前步骤相对于其父节点（上一步）的提升。\n\n**2. 训练整合：**\n最后，将这些树结构推导出的细粒度优势值，无缝集成到标准的策略梯度算法（如 GRPO/PPO）中。\n*   **逻辑：** 模型不再只对最终答案负责，而是对树中每一个经过的推理步骤负责。这使得模型能够精确地学习到“哪一步检索是关键的”、“哪一步推理是多余的”。\n\n---\n\n**总结：**\n作者的思考路径是从**发现稀疏奖励的局限性**出发，试图引入过程监督但受限于**标注成本和分布偏移**，最终通过**树结构建模**将“最终结果”转化为“中间步骤的价值估计”，并利用**剪枝策略**解决了计算复杂度问题，从而实现了一种无需标注、在线且高效的 Agentic RAG 训练框架。"
                },
                {
                    "title": "AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents",
                    "arxiv_id": "2601.06818",
                    "authors": "Xuannan Liu, Xiao Yang, Zekun Li, Peipei Li, Ran He",
                    "summary": "As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的评估和基准测试，特别是解决多步骤工作流中的幻觉问题。它直接涉及“单智能体”研究范围，通过分析“规划”和“工具使用”中的错误。虽然它涉及诊断（类似于可解释性），但其主要贡献是针对智能体可靠性的基准，而不是一般的安全/对齐或纯应用。",
                    "summary2": "本文旨在解决LLM-based agents在多步工作流中难以定位和解释幻觉起源的问题。针对多步Agent轨迹，我们提出了一种**automated hallucination attribution**新任务，并构建了包含693条高质量轨迹及系统化分类法的**AgentHallu** benchmark。我们在该数据集上评估了13个主流LLM，通过**step localization accuracy**和**G-EVAL scores**验证了其有效性。实验表明，即使是顶尖模型（如Gemini-2.5-Pro）在步骤定位上也仅达到41.1%的准确率，凸显了该任务的挑战性。",
                    "summary_translation": "由于基于大语言模型的智能体在执行顺序多步推理时，中间步骤产生的幻觉存在沿轨迹传播的风险，从而降低整体可靠性。与单轮响应中的幻觉检测不同，诊断多步工作流中的幻觉需要识别出导致初始偏差的具体步骤。为填补这一空白，我们提出了一项新的研究任务——基于大语言模型的智能体的自动幻觉归因，旨在识别导致幻觉的步骤并解释其原因。为支持该任务，我们引入了 AgentHallu，这是一个综合基准，包含：(1) 693 条涵盖 7 个智能体框架和 5 个领域的高质量轨迹；(2) 一个包含 5 个大类（规划 Planning、检索 Retrieval、推理 Reasoning、人机交互 Human-Interaction 和工具使用 Tool-Use）及 14 个子类别的幻觉分类体系；(3) 涵盖二分类标签、致幻步骤及因果解释的人工策划多级标注。我们评估了 13 个领先模型，结果表明，即使是顶级模型（如 GPT-5、Gemini-2.5-Pro），该任务也极具挑战性。表现最佳的模型仅实现了 41.1% 的步骤定位准确率，其中工具使用幻觉最为困难，准确率仅为 11.6%。我们相信 AgentHallu 将促进未来关于开发鲁棒、透明且可靠的智能体系统的研究。",
                    "inspiration_trace": "基于对论文《AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：从“单轮对话”到“多步智能体”的范式转移\n**思考起点：**\n随着大语言模型（LLM）的发展，研究热点已从简单的单轮问答转向了复杂的**LLM-based Agents（智能体）**。智能体具备规划、检索、工具调用、多步推理等能力，能够解决长链路任务。\n\n**核心洞察：**\n在单轮对话中，幻觉通常表现为“生成内容与事实不符”。但在智能体的**多步工作流**中，问题变得复杂：中间某一步的错误（如规划失误、检索错误）会像滚雪球一样**向后传播**，导致最终结果错误。\n*   **关键矛盾：** 仅仅判断“最终答案是否错误”（二分类）对于智能体来说远远不够。如果不知道错误是在哪一步产生的，就无法修复智能体，也无法提升其可靠性。\n\n### 2. 问题聚焦：从“检测”到“归因”的认知升级\n**现有局限：**\n作者回顾了现有的幻觉检测基准（如HaluEval, FELM等），发现它们都局限于**单轮响应**的**二分类判断**（是/非幻觉）。这些基准无法回答两个关键问题：\n1.  **Where（在哪里）：** 错误最早出现在轨迹的哪一步？\n2.  **Why（为什么）：** 这一步为什么会出错？\n\n**假设提出：**\n为了构建可靠的智能体系统，必须提出一个新的研究任务——**自动化幻觉归因**。这个任务的目标不仅仅是发现错误，而是要像调试程序一样，**定位导致错误的“源代码行”（步骤）并解释原因**。\n\n### 3. 方法论构建：如何定义和量化“归因”？\n**思考难点：**\n在多步轨迹中，错误往往具有连锁反应。例如，第1步规划错了，导致第3步工具调用错了，最后第5步答案错了。究竟哪一步才是“负责”的？\n\n**逻辑定义（因果对齐）：**\n作者引入了因果推断的思想来定义“负责步骤”：\n*   **反事实推理：** 如果修正了某一步 $u_t$，并重新执行后续步骤，最终答案变正确了，那么 $u_t$ 就是幻觉的根源。\n*   **最小化原则：** 如果有多个步骤都满足上述条件，取最早的那一步（即错误的源头）。\n\n### 4. 数据构建：如何设计基准以覆盖智能体的复杂性？\n**思考路径：**\n既然要评估“归因”，数据集就不能只有问答对，必须包含完整的**思维-行动-观察**轨迹。同时，智能体的幻觉类型是多样的，不能一概而论。\n\n**分类学构建：**\n作者没有凭空想象类别，而是通过**扎根理论**分析数据，归纳出智能体特有的5大幻觉类别，对应智能体的核心能力模块：\n1.  **Planning（规划）：** 目标分解错误。\n2.  **Retrieval（检索）：** 查询或上下文错误。\n3.  **Reasoning（推理）：** 逻辑或计算错误。\n4.  **Human-Interaction（人机交互）：** 误解人类反馈。\n5.  **Tool-Use（工具使用）：** 工具参数或调用错误。\n\n**数据筛选策略：**\n为了保证基准的挑战性，作者制定了严格的过滤标准：\n*   **排除非欺骗性失败：** 剔除那些直接报错、崩溃的简单案例（太容易检测）。\n*   **保留“ plausible but wrong”：** 专注于那些看起来逻辑通顺、但结果错误的轨迹，这才是归因的难点所在。\n\n### 5. 评估验证：证明任务的必要性与难度\n**逻辑闭环：**\n如果现有的顶尖模型（如GPT-5, Gemini-2.5-Pro）能轻松完成这个任务，那么这个基准就没有价值。\n\n**实验设计：**\n作者在这些模型上测试了两种Prompting策略（标准Prompt vs. 逐步Prompt）。\n*   **预期结果：** 即使是最强的模型，在步骤定位上的准确率也很低（约41%），特别是在工具使用幻觉上（仅11.6%）。\n*   **结论：** 这证实了“幻觉归因”确实是一个尚未解决的难题，从而确立了AgentHallu基准的学术价值——它为未来的研究指明了方向（即如何让模型具备自我诊断和因果解释的能力）。\n\n---\n\n**总结：作者的思考链条**\n1.  **观察现象：** 智能体的多步特性导致错误传播，单轮检测失效。\n2.  **提出假设：** 需要从“判断对错”升级为“定位源头+解释原因”。\n3.  **形式化定义：** 利用反事实推理定义“负责步骤”。\n4.  **工程实现：** 构建包含多维度分类和细粒度标注的AgentHallu数据集。\n5.  **验证价值：** 通过实验证明现有SOTA模型在此任务上的不足，确立研究基准。"
                },
                {
                    "title": "IDRBench: Interactive Deep Research Benchmark",
                    "arxiv_id": "2601.06676",
                    "authors": "Yingchaojie Feng, Qiang Huang, Xiaoya Xie, Zhaorui Yang, Jun Yu, Wei Chen, Anthony K. H. Tung",
                    "summary": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“深度研究智能体”，并提出了“模块化多智能体研究框架”。内容涉及Web探索（工具使用）、多步推理（规划）以及动态用户反馈交互（符合智能体与用户交互/反馈机制）。属于多智能体及单智能体工具使用的研究范畴，不属于纯应用或纯推理。",
                    "summary2": "本文旨在解决现有深度研究基准忽略交互动态评估的问题。针对未明确指定的查询场景，我们提出了IDRBench，包含交互式多代理框架、基于参考的User Simulator及交互感知评估套件。我们在引入模糊性注入的数据集上，通过Report Similarity、LLM-ACS及Interaction Turns等指标，验证了交互能显著提升研究质量与鲁棒性。",
                    "summary_translation": "由大语言模型驱动的深度研究代理能够执行多步推理、网络探索和长篇报告生成。然而，大多数现有系统以自主模式运行，假设用户意图已完全明确，且仅评估最终输出。在实践中，研究目标往往定义不足且在探索过程中不断演变，因此持续的交互对于实现鲁棒对齐至关重要。尽管交互至关重要，但现有的深度研究基准大多未将其纳入考量，既未对动态用户反馈进行建模，也未量化交互成本。我们介绍了IDRBench，这是首个用于系统性评估交互式深度研究的基准。IDRBench结合了具备按需交互功能的模块化多代理研究框架、可扩展的基于参考的用户模拟器，以及一个交互感知评估套件；该套件能够联合衡量交互收益（质量与对齐度）和交互成本（交互轮次与令牌数）。针对七个最先进大语言模型的实验表明，交互能够持续提升研究质量和鲁棒性，其效果往往能超越模型能力差异带来的影响，同时也揭示了在交互效率方面存在显著的权衡。",
                    "inspiration_trace": "基于论文《IDRBench: Interactive Deep Research Benchmark》的内容，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观趋势观察到具体痛点识别，再到方法论创新与验证的完整思考路径。\n\n---\n\n### 1. 宏观观察与趋势捕捉\n**思考起点：** 作者首先关注到大语言模型（LLM）在信息获取领域的演进。\n*   **现象：** LLM的能力已从单轮问答进化为能够进行多步推理、网页探索和长报告生成的“深度研究智能体”。\n*   **现状：** 现有的主流系统（如DeepResearcher等）大多采用**自主模式**，即用户给出初始指令，系统独立完成全过程，最后仅评估生成的报告质量。\n\n### 2. 现实痛点与核心假设\n**深入思考：** 作者敏锐地发现了“自主模式”与“真实研究场景”之间的巨大鸿沟。\n*   **问题识别：**\n    1.  **意图模糊性：** 现实中的用户需求往往是未充分定义的，用户在研究开始时并不清楚自己到底想要什么。\n    2.  **意图漂移：** 在长周期的推理过程中，智能体容易偏离用户初衷，产生幻觉或跑题，缺乏纠偏机制。\n*   **核心假设：** 深度研究不应是“独角戏”，而应是**“交互式协作”**。引入用户反馈可以显著提升研究质量和对齐度，甚至可能弥补模型本身能力的不足。\n\n### 3. 评估盲区的发现\n**关键转折：** 作者意识到，虽然“交互”很重要，但现有的评估体系完全忽略了这一点。\n*   **盲区分析：**\n    *   现有的基准测试都是**静态**的（Query + Reference Document），只看最终结果，不看中间过程。\n    *   这种评估方式无法区分“运气好答对”和“通过交互修正错误”的智能体。\n    *   更重要的是，它们忽略了交互的**成本**（打扰用户的次数、Token消耗）。\n*   **推论：** 要推动交互式研究的发展，必须建立一套能够量化“交互收益”与“交互成本”的新型基准。\n\n### 4. 方法论构建：从概念到落地\n为了验证上述假设并填补评估盲区，作者设计了IDRBench，其构建逻辑遵循以下步骤：\n\n#### A. 数据构建：如何逼真地模拟“需要交互”的场景？\n*   **挑战：** 现有的高质量数据集（如DeepResearch Bench）中的Query往往非常详细，智能体直接执行即可，不需要交互。\n*   **创新思路（模糊性注入）：** 作者决定人为制造“信息差”。通过LLM将原本详细的Query进行压缩（摘要化），保留核心意图但移除具体细节。\n*   **逻辑：** 只有当任务变得“模糊”时，智能体才被迫主动提问，从而触发交互行为。\n\n#### B. 用户模拟：如何实现大规模、可重复的评估？\n*   **挑战：** 真实的人类交互成本高昂且不可控（主观性强、不一致），无法作为大规模Benchmark的组件。\n*   **创新思路（基于参考的模拟器）：** 构建一个基于参考文档的“用户模拟器”。\n*   **逻辑：** 将参考文档视为“上帝视角”的真理。模拟器被设定为：像人类一样简洁回答，提供宏观指导，且拒绝错误选项。这样既保证了反馈的合理性，又实现了评估的标准化。\n\n#### C. 评估体系：如何定义“好的交互”？\n*   **思路：** 交互是一把双刃剑，必须建立多维度的评估指标。\n*   **维度拆解：**\n    1.  **收益：** 交互是否提升了质量？（语义相似度、结构覆盖度、意图满足度）。\n    2.  **成本：** 交互是否太烦人？（交互轮数、消耗的Token数）。\n*   **逻辑：** 只有同时考察这两个维度，才能判断一个智能体是否具备高效的“交互智能”。\n\n#### D. 框架设计：如何让智能体具备交互能力？\n*   **思路：** 基于现有的多智能体架构（规划、研究、生成），嵌入“交互模块”。\n*   **机制设计：**\n    *   **评估器：** 决定“何时”提问（权衡信息增益与打扰成本）。\n    *   **提问器：** 决定“问什么”（生成针对性的澄清问题）。\n*   **逻辑：** 交互不应是随机的，而应是基于当前上下文不确定性的理性决策。\n\n### 5. 实验验证与洞察提炼\n**最终验证：** 通过在多个SOTA模型上的实验，作者验证了最初的假设并发现了更深层的规律。\n*   **发现一：** 交互确实能普遍提升质量，且**交互能力有时比模型本身的原始智力更重要**（例如，开启交互的弱模型可能超过自主运行的强模型）。\n*   **发现二：** 存在**边际递减效应**。强模型通过交互获得的提升较小，而弱模型提升巨大。\n*   **发现三：** 交互策略存在差异。有的模型倾向于“频繁短问”，有的倾向于“少量长问”，这揭示了不同模型在交互效率上的权衡。\n\n### 总结\n作者的思考路径是一个典型的**“观察现象 -> 识别缺陷 -> 提出假设 -> 构建工具（Benchmark） -> 验证假设”**的学术闭环。\n\n其核心贡献不在于发明了一个新的聊天机器人，而在于**重新定义了深度研究的评估范式**——从“静态的结果导向”转向了“动态的过程导向”，并巧妙地通过“模糊性注入”和“用户模拟”解决了交互式系统难以量化评估的难题。"
                },
                {
                    "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
                    "arxiv_id": "2601.06636",
                    "authors": "Wenting Chen, Zhongrui Zhu, Guolin Huang, Wenxuan Wang",
                    "summary": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了ECR-Agent，该智能体架构包含记忆（存储验证后的推理路径）和自我演化（Critic-Driven Graph and Memory Evolution）机制，符合单智能体和自我演化的研究范围，并非单纯的领域应用。",
                    "summary2": "本文旨在解决医疗大模型在临床诊断中因依赖统计捷径而产生的Einstellung Effect（思维定势效应）。针对医疗诊断场景，我们提出了MedEinst基准测试及ECR-Agent框架。该框架通过Dynamic Causal Inference (DCI)和Critic-Driven Graph & Memory Evolution (CGME)实现基于循证医学的结构化因果推理。我们在MedEinst数据集上通过Bias Trap Rate和Robust Accuracy等指标验证了其有效性，显著降低了模型的误判率。",
                    "summary_translation": "尽管在医学基准测试中取得了高准确率，LLMs（大语言模型）在临床诊断中表现出 Einstellung Effect（定势效应）——即依赖统计捷径而非患者特异性证据，导致在非典型病例中出现误诊。现有的基准测试未能检测到这种关键的失效模式。我们提出了 MedEinst，这是一个包含 49 种疾病共 5,383 对临床病例的反事实基准。每一对病例包含一个对照病例和一个“陷阱”病例，后者通过改变鉴别性证据从而翻转诊断结果。我们通过 Bias Trap Rate（偏差陷阱率）来衡量易感性——即在正确诊断对照病例的情况下误诊陷阱病例的概率。对 17 个 LLMs 的广泛评估表明，前沿模型虽然达到了很高的基线准确率，但存在严重的偏差陷阱率。因此，我们提出了 ECR-Agent，通过两个组件将 LLM 推理与 Evidence-Based Medicine（循证医学）标准对齐：(1) Dynamic Causal Inference (DCI)（动态因果推理）通过双通路感知、跨越三个层次（关联、干预、反事实）的动态因果图推理以及用于最终诊断的证据审计来执行结构化推理；(2) Critic-Driven Graph and Memory Evolution (CGME)（批评驱动的图与记忆演化）通过将验证过的推理路径存储在范例库中并将疾病特异性知识整合到演化的疾病图中，来迭代地优化系统。源代码即将发布。",
                    "inspiration_trace": "基于论文《MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis》，以下是对作者产出该文章核心方法的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定义（从“高分低能”现象切入）\n\n**1. 观察现象：基准测试成绩与临床实战能力的错位**\n作者首先观察到一个矛盾现象：尽管当前的医学大语言模型在USMLE等标准医学基准测试上取得了极高的准确率，但在处理非典型或复杂的临床病例时，仍频繁误诊。\n*   **思考：** 为什么模型通过了“考试”，却在“看病”时失败？\n\n**2. 归因分析：定势效应的发现**\n作者将这种失败归因为心理学中的“定势效应”。即模型倾向于依赖统计捷径——即训练数据中最常见的症状与疾病的关联模式，而不是针对患者特异性证据进行逻辑推理。\n*   **核心洞察：** 模型在做“概率匹配”而非“因果诊断”。当遇到表面症状符合常见病（如流感），但关键细节指向罕见病（如肺栓塞）的病例时，模型会被先验概率“绑架”，忽略关键的反证证据。\n\n---\n\n### 第二阶段：评估工具的缺失与重构（从“静态知识”到“反事实推理”）\n\n**3. 现有工具的局限性分析**\n作者审视了现有的医学基准（如MedQA, DDXPlus），发现它们大多基于独立同分布（I.I.D.）的样本或典型病例。\n*   **逻辑推演：** 在这些数据集上，统计捷径往往能带来正确答案。因此，现有基准无法检测出模型是否真正具备“推翻直觉、依据证据下结论”的能力。我们需要一种能“诱骗”模型暴露其认知偏见的测试工具。\n\n**4. 构建新基准的假设：MedEinst的设计逻辑**\n为了捕捉定势效应，作者提出必须引入“反事实”思维。\n*   **设计思路：** 构建“对照组”与“陷阱组”病例对。\n    *   **对照组：** 典型病例，符合统计直觉。\n    *   **陷阱组：** 在对照组基础上进行最小化修改，仅替换关键的鉴别证据，使得诊断翻转。\n*   **核心指标：** 提出“偏差陷阱率”。即模型能做对对照组（证明有基础能力），却在陷阱组中坚持对照组诊断（证明被偏见误导）的概率。这成功将“推理能力”与“记忆力”解耦。\n\n---\n\n### 第三阶段：深层原因探究（从“概率拟合”到“循证医学”）\n\n**5. 失败模式的微观剖析**\n通过实验，作者发现即便是GPT-5等前沿模型，在陷阱病例上也表现出极高的错误率。进一步分析发现，模型的思维链存在三种缺陷：盲目（忽略关键证据）、思考不足（未深入分析）和过度思考（为错误结论找借口）。\n*   **思考：** 现有的“思维链”只是线性地合理化直觉，而非真正的验证过程。模型缺乏医生临床决策中的核心框架——循证医学（EBM）。\n\n**6. 理论对标：从相关性到因果性**\n作者意识到，要解决定势效应，必须让模型从Pearl因果层级的第一层（关联/Association）上升到第二层（干预/Intervention）和第三层（反事实/Counterfactual）。\n*   **逻辑演进：** 医生的诊断不是简单的“症状->诊断”映射，而是“症状->证据验证->诊断”的结构化过程。因此，新的方法论必须强制模型执行显式的证据鉴别。\n\n---\n\n### 第四阶段：方法论构建（ECR-Agent的诞生）\n\n**7. 架构设计：模拟EBM认知流程**\n基于上述分析，作者提出了ECR-Agent，旨在将LLM的推理过程与EBM标准对齐。其设计逻辑包含两个核心模块：\n\n*   **模块一：动态因果推理（DCI）—— 解决“怎么想”的问题**\n    *   **双通道感知：** 强制分离“直觉通道”（生成假设）和“分析通道”（提取客观事实），防止直觉过早封闭分析路径。\n    *   **三层因果图推理：**\n        *   *关联层：* 建立初步假设。\n        *   *干预层：* 主动检索鉴别证据，模拟“如果我去检查这个指标会怎样”。\n        *   *反事实层：* 引入“影子节点”，检查“如果这个诊断成立，应该有哪些证据缺失了？”，以此惩罚不完整的推理。\n\n*   **模块二：评论驱动的图与记忆演化（CGME）—— 解决“怎么学”的问题**\n    *   仅仅推理是不够的，系统需要像医生一样积累经验。通过评论模型反馈，将验证过的推理路径存储为范例，并将疾病知识固化为不断进化的疾病图谱。\n\n---\n\n### 第五阶段：验证与结论（从“规模定律”到“结构变革”）\n\n**8. 实验验证与反直觉发现**\n作者在MedEinst上测试了多种模型，结果证实：模型规模的扩大（Scaling Laws）并没有降低偏差陷阱率，甚至更强的模型因为更自信于统计先验，反而更容易掉进陷阱（“更强的先验，更强的盲目”）。\n*   **最终结论：** 解决医学LLM的定势效应，不能仅靠扩大参数规模，必须进行架构层面的范式转移——从基于统计的概率生成，转向基于证据的因果验证。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“现象观察（高分低能）→ 问题定性（定势效应）→ 工具创新（反事实基准MedEinst）→ 机制归因（缺乏EBM因果推理）→ 方法构建（ECR-Agent结构化验证）”** 的完整闭环。其核心贡献在于指出了LLM在医疗领域“概率拟合”的局限性，并引入因果推理框架作为破局的关键。"
                },
                {
                    "title": "Structured Episodic Event Memory",
                    "arxiv_id": "2601.06411",
                    "authors": "Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang, Baotian Hu",
                    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了结构化情景事件记忆（SEEM）框架，旨在解决自主智能体在长期交互中的记忆组织问题，属于单智能体研究中的“记忆”范畴，且不涉及纯应用、纯推理或基础设施优化等排除领域。",
                    "summary2": "本文旨在解决LLM在长期交互中因静态RAG导致的检索分散和缺乏结构依赖的问题。针对连续的交互流，我们提出了一种名为SEEM的分层框架，该框架协同了用于关系事实的Graph Memory Layer和用于叙事进展的Episodic Memory Layer，并引入了Episodic Event Frames (EEFs) 和Reverse Provenance Expansion (RPE) 机制。我们在LoCoMo和LongMemEval benchmarks上通过F1、BLEU-1和Accuracy等指标验证了其有效性，结果显示SEEM显著优于现有基线。",
                    "summary_translation": "目前，大型语言模型中的记忆方法主要依赖于静态的检索增强生成（RAG），这种方法往往导致检索结果零散，且无法捕捉复杂推理所需的结构依赖关系。对于自主代理而言，这些被动且扁平的架构缺乏必要的认知组织能力，难以对长期交互的动态性和联想性进行建模。为解决这一问题，我们提出了结构化情节事件记忆（SEEM），这是一个分层框架，协同整合了用于存储关系事实的图记忆层和用于处理叙事进展的动态情节记忆层。基于认知框架理论，SEEM 将交互流转化为结构化的情节事件框架（EEFs），并通过精确的溯源指针进行锚定。此外，我们引入了一种代理式联想融合机制和反向溯源扩展（RPE）机制，旨在从碎片化证据中重构连贯的叙事语境。在 LoCoMo 和 LongMemEval 基准测试上的实验结果表明，SEEM 显著优于基线模型，使代理能够保持卓越的叙事连贯性和逻辑一致性。",
                    "inspiration_trace": "基于论文《Structured Episodic Event Memory (SEEM)》，以下是对作者构建该方法论的逻辑链推演，旨在还原其从宏观问题观察到微观机制设计的思考过程：\n\n### 1. 宏观问题：智能体的“失忆”与“碎片化”困境\n**观察起点：**\n随着大语言模型（LLM）向自主智能体演进，它们需要处理长期的、动态的交互。然而，LLM 受限于有限的上下文窗口，且缺乏稳定的外部长期记忆系统。\n\n**现有方案的缺陷（痛点）：**\n为了解决记忆问题，业界普遍采用检索增强生成（RAG）。但作者敏锐地观察到，现有的 RAG 系统（无论是基于向量的还是基于图谱的）存在一个核心缺陷——**“碎片化检索”**。\n*   **现象：** 当智能体需要回答复杂问题时，检索到的往往是零散的文本片段或孤立的事实节点。\n*   **后果：** 这些片段缺乏上下文连贯性，无法支撑需要理解事件全貌、时间顺序和因果关系的复杂推理。智能体“只见树木，不见森林”，难以维持叙事的一致性。\n\n### 2. 认知科学假设：模拟人脑的双重记忆机制\n**思维转折：**\n为了解决“碎片化”问题，作者跳出纯计算机视角，转向认知心理学寻找灵感。人脑在处理记忆时并非单一存储，而是存在明确的分工：\n*   **语义记忆：** 存储客观事实、概念和关系（如“巴黎是法国首都”）。\n*   **情景记忆：** 存储特定时间、地点下的个人经历和事件流（如“去年夏天我在巴黎做了什么”）。\n\n**核心假设：**\n如果让智能体也具备这种分层记忆结构——即用**静态的关系图谱**来存储事实，用**动态的情景结构**来存储叙事流——就能从根本上解决上下文断裂的问题。\n\n### 3. 结构化创新：从“文本片段”到“认知框架”\n**具体化挑战：**\n虽然有了分层假设，但如何具体实现“情景记忆”？直接存储原始对话记录依然混乱。作者引入了认知框架理论。\n\n**方法论构建：**\n作者提出将连续的交互流转化为结构化的**情景事件框架**。\n*   **逻辑：** 一个事件不仅仅是文本，它包含参与者、动作、时间、地点、原因等多维属性。\n*   **设计：** 将非结构化的文本解析为具有明确语义槽位的结构化单元（EEF）。这就像把散乱的文字变成了填好的“案件调查表”，使得机器能像人类一样理解事件的要素。\n\n**动态融合机制：**\n现实中的对话是断续的（例如：A问了一半，B回答，A补充）。为了防止记忆碎片化，作者设计了**“联想融合”**机制。如果新的事件与旧的事件在语义上相关（如同一话题的不同轮次），系统会将它们合并为一个连贯的“场景”。这模拟了人类记忆中会将相关经历整合的心理过程。\n\n### 4. 检索机制革新：逆向溯源与上下文重构\n**解决“检索断层”：**\n即使有了结构化的记忆，如何确保检索时不漏掉关键信息？传统的检索是基于关键词匹配的，容易遗漏那些没有直接关键词但属于同一事件上下文的信息。\n\n**逻辑闭环：**\n作者提出了**逆向溯源扩展（RPE）**机制。\n*   **思考路径：** 当用户提问时，系统首先在“图谱层”找到相关的静态事实节点。但这只是线索。\n*   **关键动作：** 利用这些节点作为锚点，反向追踪到它们所属的“情景事件框架（EEF）”。\n*   **最终效果：** 一旦激活了某个事件框架，系统就会把该框架下关联的所有原始文本片段（通过溯源指针）全部召回。这确保了智能体看到的不是孤立的句子，而是整个事件的完整起承转合。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径遵循了从**现象观察**（RAG的碎片化） -> **理论借鉴**（认知心理学的双重记忆） -> **结构化建模**（EEF与分层架构） -> **机制完善**（联想融合与逆向溯源）的完整闭环。\n\n**核心思想演进：**\n1.  **发现问题：** 现有记忆是平面的、静态的，导致推理断裂。\n2.  **提出假设：** 记忆需要分层，区分“事实”与“故事”。\n3.  **构建模型：** 用图谱存事实，用框架存故事，并用指针连接两者。\n4.  **优化检索：** 从“找相似文本”转变为“找事件线索，再还原全貌”。\n\n这一过程体现了作者试图赋予 AI 智能体类似人类的“叙事能力”和“长期连贯性”的深层动机。"
                },
                {
                    "title": "Value of Information: A Framework for Human-Agent Communication",
                    "arxiv_id": "2601.06407",
                    "authors": "Yijiang River Dong, Tiancheng Hu, Zheng Hui, Caiqi Zhang, Ivan Vulić, Andreea Bobu, Nigel Collier",
                    "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于信息价值的决策论框架，用于解决LLM智能体在行动与向用户提问之间的决策问题。这属于单智能体范畴中的决策与通信机制研究，核心贡献是智能体的方法论改进，而非纯应用。",
                    "summary2": "本文旨在解决LLM智能体在处理未指定请求时，如何在行动与提问间取得平衡的问题。针对未指定的用户查询，我们提出了一种基于决策论中Value of Information (VoI)的框架，动态权衡信息增益与用户认知成本。在20 Questions、Flight Recommendation和Ambiguous WebShop等四个领域上，通过总效用验证了其有效性，该方法无需超参数调整即可达到最优性能。",
                    "summary_translation": "部署用于现实世界任务的 Large Language Model (LLM) agents（大语言模型智能体）面临一个根本性的两难困境：用户的请求往往是信息不足的，但智能体必须决定是依据不完整的信息采取行动，还是打断用户以寻求澄清。现有方法要么依赖于需要针对特定任务进行调优的脆弱 confidence thresholds（置信度阈值），要么未能考虑到不同决策所涉及的不同利害关系。我们引入了一个 decision-theoretic framework（决策理论框架），该框架通过 Value of Information (VoI)（信息价值）解决了这一权衡问题，使智能体能够动态地权衡提问带来的 expected utility gain（预期效用增益）与给用户带来的 cognitive cost（认知成本）。我们的 inference-time（推理时）方法无需 hyperparameter tuning（超参数调优），并且能够跨场景无缝适应——从休闲游戏到医疗诊断。在四个不同领域（20 Questions、医疗诊断、航班预订和电子商务）的实验表明，VoI 始终匹配或超过最佳手动调优的 baselines（基线），在高成本设置中实现了高达 1.36 的效用点提升。这项工作提供了一个用于 adaptive agent communication（自适应智能体通信）的 parameter-free（无参数）框架，该框架明确平衡了 task risk（任务风险）、query ambiguity（查询歧义）和 user effort（用户努力）。",
                    "inspiration_trace": "基于论文《Value of Information: A Framework for Human-Agent Communication》，以下是对作者核心方法论产出逻辑链的系统性推演。这一过程展现了作者如何从现实痛点出发，通过批判性分析现有技术，最终引入决策理论解决人机交互中的根本矛盾。\n\n---\n\n### 第一阶段：宏观困境的识别——“模糊性”与“两难”\n**（观察与问题定义）**\n\n作者的思考始于对现实世界LLM智能体应用场景的观察。作者发现，尽管LLM在执行任务上能力强大，但在面对真实用户时存在一个根本性的**“信息缺口”**：\n*   **用户请求的天然模糊性**：用户的指令往往是欠规范的（如“订一张去伦敦的机票”），隐含了未知的偏好（预算、时间、转机容忍度）。\n*   **智能体的两难困境**：\n    *   **行动**：在信息不全时直接行动，可能导致结果与用户意图不符（任务失败风险）。\n    *   **询问**：通过提问澄清信息，但会打断用户，增加认知负担（用户流失风险）。\n\n**核心思考**：现有的智能体大多假设指令是清晰的，或者仅仅关注“如何执行”，而忽略了“何时该沟通”这一前置决策。作者意识到，**解决这一两难困境是智能体从“工具”进化为“合作伙伴”的关键。**\n\n---\n\n### 第二阶段：对现有范式的批判——“置信度”的失效\n**（假设验证与否定）**\n\n在寻找解决方案时，作者首先审视了学术界和工业界的主流做法，并发现了其逻辑漏洞：\n1.  **固定轮次策略**：无论任务难易都问固定数量的问题。这显然是愚蠢的，因为它忽略了上下文。\n2.  **基于置信度的阈值**：这是目前最先进的自适应方法。当模型对答案的“自信度”低于某个阈值（如0.9）时，就提问。\n\n**作者的批判性洞察**：\n*   **置信度 $\\neq$ 价值**：模型对“猜动物”有90%的把握，和对“诊断癌症”有90%的把握，其含义截然不同。\n*   **缺乏风险感知**：置信度方法只关注“我知道多少”（信息论视角），却忽略了“如果错了后果有多严重”（决策论视角）。在低风险任务（猜动物）中，90%的置信度可能已经足够；但在高风险任务（医疗诊断）中，90%可能意味着致命风险，必须继续提问。\n\n**结论**：单纯依赖模型内部的不确定性估计是片面的，必须引入对**任务风险**和**决策后果**的考量。\n\n---\n\n### 第三阶段：理论视角的转换——从“信息获取”到“理性决策”\n**（理论引入与框架构建）**\n\n为了解决上述缺陷，作者将视角从计算机科学转向了认知科学与决策理论，提出了核心假设：\n*   **沟通即决策**：提问不应仅仅是为了获取信息，而应被视为一种“行动”。这种行动有成本（认知负荷），也有收益（提升决策质量）。\n*   **理性言语行为**：借鉴RSA框架，智能体应当是“理性”的，即只有当提问带来的**预期效用提升**大于**提问成本**时，才应该进行沟通。\n\n**逻辑推演**：\n我们需要一个数学工具来量化“提问到底值不值”。作者引入了经典的**信息价值**理论。\n*   **定义**：VoI = (获得信息后的预期效用) - (当前信息下的预期效用)。\n*   **决策规则**：如果 $VoI > \\text{提问成本}$，则提问；否则，直接行动。\n\n这一转换将问题从“我不确定吗？”（模糊逻辑）变成了“值得去弄清楚吗？”（经济逻辑）。\n\n---\n\n### 第四阶段：方法论的落地——LLM驱动的贝叶斯模拟\n**（从理论到实践的映射）**\n\n有了VoI理论框架，接下来的挑战是如何让LLM在推理时计算出这个值。作者设计了一套无需训练的推理时算法：\n\n1.  **信念分布**：\n    *   *思考*：LLM通常只输出一个确定答案，但计算VoI需要概率。\n    *   *方案*：强制LLM输出对用户潜在意图（如偏好、疾病类别）的概率分布 $b(\\theta)$。\n\n2.  **前瞻性模拟**：\n    *   *思考*：在问出问题前，智能体需要预判“如果我问了，用户可能怎么答，以及回答后我的效用会变多少”。\n    *   *方案*：利用LLM的生成能力进行“反事实模拟”。针对候选问题 $q$，枚举可能的回答 $y$，模拟更新信念分布 $b(\\theta|y)$，并计算对应的效用。\n\n3.  **动态权衡**：\n    *   *思考*：如何整合风险和成本？\n    *   *方案*：在VoI公式中显式引入任务风险（通过效用函数 $U(\\theta, a)$ 的量级体现）和认知成本（常数 $c$）。\n\n---\n\n### 第五阶段：验证逻辑——自适应性与零参数优势\n**（实验设计与预期验证）**\n\n最后，作者通过实验验证这一逻辑链条的有效性，其验证逻辑紧扣之前的批判：\n*   **跨场景泛化**：选择“猜动物”（低风险）、“医疗诊断”（高风险）、“订票”（多属性偏好）等不同场景，证明VoI能自动适应不同的风险等级。\n*   **对比基线**：专门对比“置信度阈值”方法。\n    *   *预期结果*：置信度方法需要针对每个任务手动调整阈值（脆弱），而VoI方法无需调参（鲁棒）。\n    *   *逻辑闭环*：在高风险场景下，VoI会因为潜在收益巨大而倾向于多问；在低风险或高沟通成本场景下，VoI会自动停止提问。\n\n---\n\n### 总结：作者的思维演进图谱\n\n1.  **痛点**：用户指令模糊，智能体在“瞎猜”和“烦人”之间进退维谷。\n2.  **反思**：现有的“自信度”机制只看不确定性，不看后果，无法区分“猜错猫”和“误诊癌症”的区别。\n3.  **升维**：引入决策论，将沟通视为一种投资，必须计算ROI（投资回报率）。\n4.  **工具**：采用**信息价值**作为核心指标，量化“提问”带来的预期收益。\n5.  **实现**：利用LLM自身的推理能力进行信念估计和未来模拟，实现无需训练的动态决策。\n\n这一逻辑链条展示了作者如何从具体的交互体验出发，通过跨学科的理论融合，最终构建出一个既符合人类直觉又具备数学严谨性的通用框架。"
                },
                {
                    "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning",
                    "arxiv_id": "2601.06282",
                    "authors": "Yue Zhou, Xiaobo Guo, Belhassen Bayar, Srinivasan H. Sengamedu",
                    "summary": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了 Amory，这是一个针对长期对话智能体的工作记忆框架。它专注于通过智能体推理构建结构化记忆表征（情景和语义记忆），这直接属于单智能体研究范围中的“记忆”类别。",
                    "summary2": "本文旨在解决长期对话代理中现有记忆框架缺乏连贯性且计算成本高昂的问题。针对长对话场景，我们提出了一种名为Amory的工作记忆框架，通过主动构建情景叙事、动量感知整合和语义化外围事实来组织记忆，并在LOCOMO benchmark上通过LLM-as-a-Judge分数和响应延迟验证了其有效性。",
                    "summary_translation": "随着交互时间的延长，长期对话代理面临着一个根本的可扩展性挑战：重复处理整个对话历史在计算上变得不可行。目前的解决方案试图通过记忆框架来解决这一问题，这些框架主要将对话分割为孤立的 embeddings（嵌入向量）或 graph representations（图表示），并以 RAG（检索增强生成）的方式检索相关信息。尽管这些方法在计算上效率较高，但它们往往对记忆形成过程的处理过于简单，无法捕捉人类记忆的微妙之处和连贯性。我们提出了 Amory，这是一个 working memory（工作记忆）框架，它通过在 offline time（离线时间）期间增强 agentic reasoning（智能体推理）来主动构建结构化的记忆表示。Amory 将对话片段组织成 episodic narratives（情景叙事），利用 momentum（动量）巩固记忆，并将 peripheral facts（外围事实）转化为 semantic memory（语义记忆）。在检索阶段，系统在叙事结构上采用 coherence-driven reasoning（连贯性驱动推理）。在针对长期推理的 LOCOMO benchmark（基准测试）上的评估表明，Amory 相比于先前的 state-of-the-art（最先进水平）取得了显著改进，其性能与 full context reasoning（全上下文推理）相当，同时将响应时间缩短了 50%。分析表明，momentum-aware consolidation（动量感知巩固）显著提升了响应质量，而 coherence-driven retrieval（连贯性驱动检索）相比基于 embeddings 的方法提供了更优越的 memory coverage（记忆覆盖率）。",
                    "inspiration_trace": "基于论文《Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 1. 宏观问题：长对话中的“质量-效率”悖论\n**观察起点：**\n随着大语言模型（LLM）在长对话场景中的应用，一个根本性的瓶颈浮出水面：**上下文窗口的有限性与对话历史无限增长之间的矛盾**。\n*   **现状：** 如果每次都处理全量历史，计算成本极高且响应缓慢。\n*   **现有解法（RAG范式）：** 为了解决效率问题，主流方法采用检索增强生成（RAG），将对话切片存入向量数据库或图结构，按需检索。\n*   **痛点识别：** 作者发现，这种“碎片化”的存储方式虽然快，但丢失了对话的**上下文连贯性**。它将记忆视为孤立的“数据点”，而非有逻辑的“体验”，导致模型难以进行复杂的推理（如多跳问题或时间推理）。\n\n### 2. 认知视角的引入：从“存储”转向“体验”\n**理论假设：**\n为了解决碎片化问题，作者将目光转向认知科学，试图寻找人类记忆的运作机制作为灵感。\n*   **核心洞察：** 人类记忆不是简单的关键词索引，而是基于**叙事**的。\n    *   **情景记忆：** 记住的是“故事”，包含情节、人物和因果链条。\n    *   **语义记忆：** 提取出的去语境化事实。\n    *   **记忆巩固：** 记忆不是静态的，而是随着时间推移从不稳定状态重组为稳定结构。\n*   **推论：** 如果AI能像人类一样，将对话碎片组织成连贯的“故事”，并在非活跃时间进行“巩固”，就能在保持效率的同时，大幅提升记忆的可用性和推理深度。\n\n### 3. 关键转折：利用“离线智能体推理”构建结构\n**技术难点：**\n要构建复杂的叙事结构，需要LLM进行深度的逻辑推理。然而，在用户提问的“在线”阶段进行这种推理会带来不可接受的延迟。\n*   **策略选择：** 作者提出了一个关键的时间维度分离策略——**“离线构建，在线检索”**。\n*   **核心假设：** 利用对话的自然间隙（离线时间），让智能体主动去“思考”和“整理”记忆。这样既利用了LLM的推理能力，又不影响实时响应速度。\n\n### 4. 方法论构建：动态演进的叙事记忆\n基于上述假设，作者设计了一套动态的记忆构建流程，模拟人类认知的三个阶段：\n\n*   **阶段一：叙事化组织**\n    *   *思考：* 对话不是杂乱无章的，而是围绕特定主题展开的。\n    *   *设计：* 将对话片段绑定到“情景记忆”中，形成层级结构（主情节 -> 子情节 -> 片段）。这解决了碎片化问题，赋予了记忆骨架。\n\n*   **阶段二：动量感知的巩固**\n    *   *思考：* 对话有“热度”。当一个话题被反复讨论时（活跃态），不应急于总结；当话题转移后（非活跃态），才是重组记忆的最佳时机。\n    *   *设计：* 引入“对话动量”概念。仅在记忆进入非活跃状态时，触发LLM对情节进行重组和概括（Consolidation）。这模拟了人类在事后反思并固化记忆的过程。\n\n*   **阶段三：语义化剥离**\n    *   *思考：* 并非所有信息都属于故事。有些是琐碎的事实（如“某人住在哪”），它们不需要上下文即可被理解。\n    *   *设计：* 将与主情节逻辑关联不大的边缘事实，提取为结构化的三元组存入“语义记忆”。这实现了叙事与事实的分离，提高了检索的精准度。\n\n### 5. 检索范式革新：连贯性驱动\n**最后一步：**\n既然记忆是结构化的叙事，检索方式也必须升级。\n*   *批判：* 传统的向量相似度检索无法理解逻辑关系（例如，用户问“John为什么喜欢篮球？”，向量检索可能只匹配到“篮球”这个词，而忽略了“职业发展”这个潜在情节）。\n*   *设计：* 采用**连贯性推理检索**。让LLM基于情节标题和人物关系进行逻辑判断，而非简单的向量匹配。这确保了检索到的不仅是“相似”的内容，更是“逻辑相关”的上下文。\n\n### 6. 总结：逻辑链的闭环\n作者的思考路径完成了一个闭环：\n1.  **发现问题：** 现有RAG方法虽然快，但记忆太碎，推理能力差。\n2.  **寻找灵感：** 人类通过叙事和巩固来形成高质量记忆。\n3.  **提出假设：** 利用LLM的推理能力，在离线阶段主动构建叙事结构。\n4.  **细化机制：** 通过“绑定-巩固-语义化”三步走，动态管理记忆的演进。\n5.  **验证效果：** 实验证明，这种方法在保持低延迟的同时，显著提升了长对话中的推理质量，接近全量上下文的效果。\n\n这一过程体现了作者从**工程痛点**出发，借鉴**认知科学理论**，最终通过**巧妙的时空分离设计（离线推理/在线检索）**实现了方法论落地的完整逻辑演进。"
                },
                {
                    "title": "Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms",
                    "arxiv_id": "2601.06039",
                    "authors": "Yueze Liu, Ajay Nagi Reddy Kumdam, Ronit Kanjilal, Hao Yang, Yichi Zhang",
                    "summary": "Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究“角色扮演智能体”，提出的VEJA框架旨在通过优化数据来增强智能体的内部状态（价值观、经历、判断、能力）。这属于单智能体范畴，涉及智能体的记忆（经历）和自我反思/推理（判断），旨在提升智能体的行为一致性和深度，符合筛选条件。",
                    "summary2": "本文旨在解决现代角色扮演模型缺乏真实角色深度及内心冲突推理的问题。针对现有训练范式的局限性，我们提出了一种VEJA框架（Values, Experiences, Judgments, Abilities），用于指导数据策划。我们在基于角色Makise Kurisu的数据集上，通过LLM-as-judge的盲A/B测试验证了其有效性，结果显示VEJA策划的数据在角色一致性和叙事连续性上显著优于合成基线。",
                    "summary_translation": "现代角色扮演模型日益精密，但始终难以捕捉可信且引人入胜角色的本质。我们认为，这一缺陷归因于训练范式忽视了角色内心世界的动态相互作用。当前的方法，包括检索增强生成、基于事实的提示、基于文学的学习以及合成数据生成，在建模定义人类互动的深思熟虑且充满价值冲突的推理方面，均表现出反复出现的局限性。在本文中，我们确定了对于角色真实性至关重要的四个核心概念：价值观、经历、判断和能力。我们提出 VEJA 框架作为一种新的数据策展范式，旨在解决这些系统性局限。为了展示本框架所能达到的质量上限，我们进行了一项试点研究，将人工策展的基于 VEJA 的数据集与最先进的合成基线进行了比较。利用大语言模型评判法，我们的研究结果显示出显著的质量差距，这表明转向以概念为基础的数据策展（如 VEJA 所体现的那样），对于创建具有真正深度和叙事连贯性的角色扮演智能体是必要的。完整数据集可在 https://github.com/HyouinKyoumaIRL/Operation-Veja 获取。",
                    "inspiration_trace": "基于论文《Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms》，以下是对作者产出该文章核心方法（VEJA框架）的逻辑链推演：\n\n### 第一阶段：现象观察与核心痛点识别\n**（从“模型能说话”到“模型没有灵魂”）**\n\n1.  **宏观观察**：作者发现，尽管现代角色扮演模型越来越复杂，能够生成流畅的对话，但它们始终缺乏“令人信服的、引人入胜的角色本质”。\n2.  **具体案例触发**：作者在尝试构建高保真角色（如《命运石之门》中的牧濑红莉栖）时发现，现有模型无法复刻其核心特质——即“求知欲”与“社交戒备心”之间的冲突。模型的反应仅仅是条件反射式的，缺乏内在的驱动力。\n3.  **核心假设提出**：作者认为，问题的根源不在于模型参数不够大，而在于**训练范式**忽视了角色内心世界的动态相互作用。人类互动不是检索“正确”答案，而是**冲突价值的协商**。\n\n### 第二阶段：对现有范式的批判性解构\n**（为什么当前主流方法都失效了？）**\n\n为了验证假设，作者系统性地解剖了当时四种主流的角色建模方法，试图找出它们共同的缺陷：\n\n1.  **检索增强生成（RAG）的局限**：RAG擅长处理事实，但人类的价值体系是组合爆炸的。试图用检索列表来穷举一个角色在所有情境下的价值判断是不可能的。\n2.  **基于事实的价值预设的局限**：为了通过基准测试，现有方法倾向于将价值简化为孤立的公式（如“对陌生人开放”）。这导致模型在对话中过度索引单一特征，忽略了语境和平衡，显得机械且缺乏分寸。\n3.  **基于文学生成的局限**：文学名著虽然包含深度，但对话只“暗示”了思维过程，而非“显式”展示。模型无法从对话文本中反向推导出角色复杂的内心 deliberation（ deliberative reasoning）。此外，文学中的“经验”通常通过旁白而非对话传递，导致模型难以学会“以史为鉴”。\n4.  **合成数据生成的死循环**：这是最致命的陷阱。试图用现有的强模型（如GPT-4）生成高质量角色数据是行不通的，因为**生成者本身就不具备处理复杂价值冲突的能力**。这导致了一个递归的质量天花板。\n\n### 第三阶段：理论重构与VEJA框架的诞生\n**（回归戏剧艺术，重建角色的“因果逻辑”）**\n\n在否定了现有技术路径后，作者转向经典戏剧理论（如斯坦尼斯拉夫斯基体系），试图从第一性原理出发定义什么是“真实的角色”。\n\n1.  **寻找基本单元**：作者认为，要模拟角色的深度，必须显式地建模其内心逻辑。通过数据整理过程中的观察，作者提炼出四个核心概念：\n    *   **Values (价值观)**：行为的根本动机（Why）。\n    *   **Experiences (经历)**：塑造价值观和判断的过去事件（Evidence）。\n    *   **Judgments (判断)**：价值观经过经历过滤后形成的具体观点（Output）。\n    *   **Abilities (能力)**：表达上述特质的知识和技能工具（Toolkit）。\n\n2.  **建立因果链条**：这四个要素不是孤立的标签，而是一个严密的**因果闭环**：\n    *   经历塑造价值观；\n    *   价值观与经历共同产出判断；\n    *   判断通过能力表达出来。\n    *   *逻辑演进点*：作者意识到，只有强制数据遵循这个因果链，才能让模型学会“像人一样思考”，即基于过去（E）和动机（V）来形成当下的观点（J），而不仅仅是模仿语气。\n\n### 第四阶段：验证与范式转移\n**（证明“人+框架”优于“纯模型合成”）**\n\n1.  **实验设计的逻辑**：既然现有模型无法生成高质量数据，那么“人类作者”是否就是答案？为了验证这一点，作者设计了一个对比实验：**纯模型生成** vs. **VEJA框架指导的人类写作**。\n2.  **结果解读**：实验结果显示，VEJA指导的人类数据显著优于SOTA合成数据。这证明了作者的核心论点：**当前的技术瓶颈不在于算力，而在于数据的“概念深度”**。\n3.  **最终结论**：作者提出，社区需要从“构建更好的鹦鹉”（模仿表面）转向“创造真正的数字心智”（模拟内在）。VEJA不仅仅是一个数据标注框架，更是一种新的训练范式，它要求我们在数据构建阶段就必须显式地包含角色的内心冲突和推理过程。\n\n---\n\n**总结：**\n作者的思考路径是从**“体验到的肤浅感”**出发，经过**“对技术路径的证伪”**，回归**“对人性和戏剧艺术的本体论思考”**，最终提炼出**“VEJA因果模型”**，并通过实验确立了**“概念驱动数据”**优于**“纯合成数据”**的新范式。"
                },
                {
                    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
                    "arxiv_id": "2601.07779",
                    "authors": "Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",
                    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个计算机使用智能体框架，核心研究内容包括单智能体的记忆、自我反思和工具使用，符合LLM智能体的研究范围。尽管涉及视觉模型，但重点在于智能体架构而非视觉模型本身。",
                    "summary2": "本文旨在解决计算机使用代理在长时程任务中鲁棒性不足及新领域泛化能力差的问题。针对复杂的桌面自动化场景，我们提出了一种名为OS-Symphony的整体框架，该框架集成了利用里程碑驱动长期记忆的Reflection-Memory Agent和采用See-Act范式的Multimodal Searcher。并在OSWorld、WindowsAgentArena和MacOSArena基准上通过Step Success Rate验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "尽管 Vision-Language Models (VLMs，视觉语言模型) 显著推动了 Computer-Using Agents (CUAs，计算机使用代理) 的发展，但现有框架在长时程工作流的鲁棒性以及在新领域的泛化能力方面仍面临挑战。这些局限性主要归因于对历史视觉上下文筛选缺乏细粒度控制，以及缺乏视觉感知的教程检索机制。为弥合这些差距，我们提出了 OS-Symphony，这是一个包含 Orchestrator (编排器) 的整体框架，该编排器协调两项关键创新以实现鲁棒的自动化：(1) Reflection-Memory Agent (反思记忆代理)，利用里程碑驱动的长期记忆实现轨迹级自我修正，有效缓解长时程任务中的视觉上下文丢失；(2) Versatile Tool Agents (多功能工具代理)，其特色在于包含一个 Multimodal Searcher (多模态搜索器)，该搜索器采用 SeeAct 范式在基于浏览器的沙箱中进行导航，以合成实时的、视觉对齐的教程，从而解决未见场景中的保真度问题。实验结果表明，OS-Symphony 在不同模型规模下均带来了显著的性能提升，在三个在线基准测试中确立了新的 State-of-the-Art (SOTA，最先进) 结果，尤其在 OSWorld 上达到了 65.84% 的成绩。",
                    "inspiration_trace": "基于论文《OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方案构建的思考过程：\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**思考起点：** 尽管视觉语言模型（VLMs）推动了计算机代理（CUA）的发展，但现有的代理框架在实际应用中仍存在显著的脆弱性。\n**核心矛盾：** 当前的CUA在两个关键维度上表现不佳：\n1.  **长时程任务的鲁棒性：** 在需要多步骤、跨应用的复杂工作流中，代理容易迷失方向或陷入死循环。\n2.  **新领域的泛化能力：** 面对未见过的软件或环境（OOD场景），代理缺乏必要的知识储备，无法有效执行任务。\n\n### 第二阶段：微观诊断与归因分析\n**思考深入：** 为什么现有的模块化或端到端框架无法解决上述矛盾？作者通过分析现有架构的局限性，识别出两个具体的“技术断层”：\n\n**断层一：视觉上下文的“失忆”**\n*   **观察：** 现有的记忆机制（如简单的滑动窗口或文本摘要）缺乏对历史视觉信息的精细化管理。\n*   **推论：** 在长任务中，屏幕截图包含大量冗余信息，直接存储会撑爆上下文窗口，而简单丢弃又会丢失关键状态。这种“视觉上下文丢失”导致代理无法回溯历史，从而无法识别意图漂移或循环行为等错误，失去了自我纠错的基础。\n\n**断层二：检索增强的“视觉盲区”**\n*   **观察：** 为了解决泛化问题，现有方法引入了检索增强生成（RAG）。但这些方法多依赖纯文本检索或静态知识库。\n*   **推论：** GUI任务本质上是视觉的。纯文本检索无法捕捉界面布局、图标样式等视觉语义，导致检索到的教程与当前屏幕状态不匹配（保真度低）。此外，静态知识库更新成本高，难以适应新软件的快速迭代。\n\n### 第三阶段：核心假设与策略提出\n**思考转折：** 要解决上述断层，必须从“被动处理”转向“主动感知与压缩”。作者提出了两个核心假设：\n\n1.  **关于记忆的假设：** 如果能设计一种机制，只保留具有里程碑意义的关键截图，并基于这些视觉证据生成轨迹级的反思，就能在压缩上下文的同时保留纠错能力。\n2.  **关于泛化的假设：** 如果代理能像人类一样，在遇到不懂的操作时主动打开浏览器进行“视觉搜索”，通过实际浏览网页来合成与当前环境视觉对齐的教程，就能解决静态知识库的滞后和文本检索的盲区。\n\n### 第四阶段：方法论构建与系统设计\n**思考落地：** 基于上述假设，作者构建了 **OS-Symphony** 这一整体框架，其逻辑架构体现了“分工协作”的思想：\n\n**1. 设计“指挥官”：**\n*   **逻辑：** 系统需要一个核心大脑来负责任务理解和动作调度，同时协调其他模块。\n*   **角色：** Orchestrator（编排器）。它只关注短期记忆（最近K步）和来自其他模块的高级指令，保持决策的敏捷性。\n\n**2. 构建“反思者与记忆库”：**\n*   **逻辑：** 针对“视觉上下文丢失”，需要一个专门的模块来管理长期记忆和进行错误审计。\n*   **方案：** **Reflection-Memory Agent (RMA)**。\n    *   **里程碑机制：** 不存储所有截图，而是通过算法判断哪些步骤是“里程碑”（如状态发生重大改变），只保留这些关键帧。\n    *   **结构化反思：** RMA 审计历史轨迹，通过结构化的消息协议向 Orchestrator 反馈状态（如：On-track, Off-track, GUI Error, Lack of Tutorial），从而实现轨迹级的自我纠正。\n\n**3. 打造“全能工具箱”：**\n*   **逻辑：** 针对“视觉盲区”和执行效率问题，需要专门的工具来处理特定类型的任务。\n*   **方案：** **Versatile Tool Agents**。\n    *   **多模态搜索者：** 这是一个核心创新。它采用“See-Act”范式，在一个隔离的浏览器沙箱中自主导航，阅读网页并合成包含视觉描述的教程。这解决了传统RAG缺乏视觉感知的问题。\n    *   **定位器与编码器：** 分别负责UI元素的精确定位和系统级的代码操作，弥补纯GUI操作的不足。\n\n### 第五阶段：逻辑闭环与验证\n**思考总结：** 整个框架形成了一个闭环：\n*   Orchestrator 执行任务；\n*   遇到困难或错误时，RMA 通过视觉审计发现并反馈；\n*   如果是知识缺失，Searcher 主动上网寻找视觉教程；\n*   最终完成任务并更新记忆。\n\n**结论：** 这种设计通过**精细化的视觉记忆管理**解决了长时程任务的鲁棒性问题，通过**主动的视觉搜索**解决了新领域的泛化问题，从而在多个基准测试中实现了SOTA性能。\n\n---\n\n**总结：** 作者的思考路径是从**宏观的能力缺失**（鲁棒性与泛化性）出发，深入到**微观的信息处理缺陷**（视觉记忆丢失与检索视觉盲区），进而提出**主动化与结构化**的解决策略（里程碑记忆与视觉搜索），最终通过**多智能体协作**的架构实现了逻辑落地。"
                },
                {
                    "title": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI",
                    "arxiv_id": "2601.06037",
                    "authors": "Chunliang Chen, Ming Guan, Xiao Lin, Jiaxu Li, Qiyi Wang, Xiangyu Chen, Jixiang Luo, Changzhi Sun, Dell Zhang, Xuelong Li",
                    "summary": "Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了TeleMem，这是一个专门为Agentic AI设计的长程和多模态记忆系统。它直接属于“单智能体”研究范围中的“记忆”和“工具使用”（ReAct-style reasoning）范畴。虽然涉及多模态内容，但其核心贡献在于智能体的记忆机制与架构，而非单纯的多模态模型研究，因此符合筛选要求。",
                    "summary2": "本文旨在解决LLM在长交互中面临的记忆更新困难、效率低及多模态推理受限问题。针对长程对话和视频理解场景，我们提出了一种名为TeleMem的统一长程多模态记忆系统。该方法利用叙事动态提取和结构化写入管道（批处理、检索、聚类、合并）优化文本记忆，并结合ReAct风格推理实现视频内容的闭环处理。在ZH-4O benchmark上，通过QA Accuracy、Token使用量和速度验证，TeleMem比Mem0准确率提升19%，Token减少43%，速度提升2.1倍。",
                    "summary_translation": "大语言模型在许多自然语言处理（NLP）任务中表现出色，但由于对长对话历史的注意力机制受限，难以维持长期交互。检索增强生成虽然缓解了这一问题，但缺乏更新或优化存储记忆的可靠机制，从而导致模式驱动的幻觉、低效的写入操作以及对多模态推理的支持不足。为应对这些挑战，我们提出了 TeleMem，这是一个统一的长期与多模态记忆系统，它通过叙事动态提取来维护连贯的用户画像，确保仅保留基于对话的信息。TeleMem 进一步引入了一个结构化的写入流水线，对记忆条目进行批处理、检索、聚类和整合，从而显著提高了存储效率，减少了 token（令牌）使用量，并加速了记忆操作。此外，结合了 ReAct 风格推理的多模态记忆模块，赋予了系统闭环的“观察-思考-行动”能力，使其能够在长期语境下准确理解复杂的视频内容。实验结果表明，在 ZH-4O 长期角色扮演游戏基准测试中，TeleMem 优于当前最先进的 Mem0 基线，准确率提高了 19%，token 使用量减少了 43%，速度提升了 2.1 倍。",
                    "inspiration_trace": "基于对论文《TeleMem: Building Long-Term and Multimodal Memory for Agentic AI》的深入分析，以下是作者构建该方法的逻辑演进链条：\n\n### 1. 宏观困境：LLM的“失忆”与RAG的“静态”\n**观察**：虽然大语言模型（LLM）能力强大，但在长期交互中存在根本性缺陷。Transformer架构的上下文窗口限制导致模型难以在长对话中保持注意力，容易遗忘关键信息或丢失个性化细节。\n**现有方案（RAG）的局限**：检索增强生成（RAG）虽然能缓解上下文长度限制，但其本质是“静态存储”。一旦信息写入数据库，就缺乏可靠的更新或修正机制。这导致系统无法适应用户偏好的动态变化，难以解决信息冲突，无法维持长期的一致性。\n\n### 2. 痛点深挖：现有记忆系统的三大缺陷\n作者在审视更先进的记忆系统（如Mem0, MemoryBank等）时，发现了三个阻碍实际落地的核心问题：\n\n*   **问题一：结构化幻觉**\n    *   **现象**：现有系统常依赖预定义的字段（如JSON Schema）来构建用户画像。\n    *   **矛盾**：真实对话中信息往往是稀疏的，无法填满所有预设字段。\n    *   **后果**：模型被迫“脑补”不存在的细节以填充结构，导致幻觉，且增加了不必要的计算开销。\n\n*   **问题二：写入效率低下**\n    *   **现象**：主流方法（如Mem0）采用“流式处理”，即每一轮对话都触发检索和LLM决策（增删改）。\n    *   **矛盾**：频繁的API调用和数据库写入导致高延迟和低吞吐量。\n    *   **后果**：系统在长对话中响应变慢，且碎片化的写入导致存储冗余。\n\n*   **问题三：多模态推理缺失**\n    *   **现象**：大多数记忆系统仅关注文本。\n    *   **矛盾**：现实世界的Agent需要处理图像、视频等多模态信息。\n    *   **后果**：系统无法理解或记忆复杂的视觉场景，限制了其在真实环境中的应用。\n\n### 3. 核心假设：从“填空”到“叙事”，从“流式”到“批处理”\n针对上述痛点，作者提出了三个关键的方法论假设：\n\n*   **假设一（针对幻觉）**：记忆应当是“叙事性”的，而非“结构化”的。放弃固定的Schema模板，仅提取对话中明确支持的自然语言描述，可以避免幻觉并保持灵活性。\n*   **假设二（针对效率）**：记忆更新应当是“批量化”的，而非“流式”的。通过累积多轮对话，统一进行检索、聚类和合并，可以大幅减少冗余操作和Token消耗。\n*   **假设三（针对多模态）**：视频记忆需要分层存储（事件+实体）并结合Agent主动推理。单纯将视频转为文本会丢失细节，需要结合ReAct风格的工具调用实现“观察-思考-行动”的闭环。\n\n### 4. 方法论构建：TeleMem的系统设计\n基于上述假设，作者构建了TeleMem的统一框架，逻辑演进如下：\n\n#### 4.1 文本记忆模块的重构\n*   **解决结构化幻觉**：\n    *   **设计**：提出**Profile Memory（画像记忆）**。不使用固定字段，而是为每一轮对话生成独立的、基于角色的自然语言摘要。\n    *   **逻辑**：只存储对话中“发生过”的事实，不存储“可能”的属性，确保记忆的真实性。\n\n*   **解决写入效率**：\n    *   **设计**：提出**Event Memory（事件记忆）**的四阶段流水线。\n    *   **逻辑演进**：\n        1.  **Summarization（摘要）**：将对话批次转化为摘要集合。\n        2.  **Retrieval（检索）**：找出与当前摘要相关的历史记忆。\n        3.  **Clustering（聚类）**：将新摘要与旧记忆混合进行全局语义聚类。这是关键一步，它将碎片化信息归类。\n        4.  **Decision（决策）**：对每个聚类进行时序排序，让LLM统一决定是新增、更新还是删除。\n    *   **效果**：通过聚类和批处理，实现了信息的去重和合并，避免了频繁的碎片化写入。\n\n#### 4.2 多模态记忆模块的扩展\n*   **解决视频理解**：\n    *   **设计**：将视频流切分为片段，提取两层记忆：\n        1.  **Event Memory**：片段级的语义描述（发生了什么）。\n        2.  **Key-Value Object Memory**：实体级的状态记录（谁在做什么，外观如何）。\n    *   **逻辑**：分层存储兼顾了宏观事件和微观实体细节。\n\n*   **解决推理能力**：\n    *   **设计**：引入**ReAct-style Agent**进行读取。\n    *   **逻辑**：Agent不再被动接收检索结果，而是主动使用三个工具：\n        1.  `video.retrieval`：定位时间戳。\n        2.  `video.rag`：获取文本摘要。\n        3.  `video.qa`：针对特定视频帧进行细粒度视觉问答。\n    *   **效果**：形成了“观察（定位）-思考（检索与QA）-行动（回答）”的闭环，能够处理复杂的视频查询。\n\n### 5. 总结\n作者的思考路径是从**LLM的固有缺陷**出发，批判了**RAG的静态性**和**现有记忆系统的低效/僵化**，最终通过**叙事化提取**（保证真实性）、**批处理流水线**（保证效率）和**多模态Agent化**（保证能力）三个维度的创新，构建了TeleMem这一统一框架。这一过程体现了从“修补上下文窗口”到“构建类人长期记忆”的范式转变。"
                },
                {
                    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
                    "arxiv_id": "2601.07641",
                    "authors": "Jiaxuan Lu, Ziyu Kong, Yemin Wang, Rong Fu, Haiyuan Wan, Cheng Yang, Wenjie Lou, Haoran Sun, Lilong Wang, Yankai Jiang, Xiaosong Wang, Xiao Sun, Dongzhan Zhou",
                    "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究LLM智能体，提出了“测试时工具演化”（TTE）范式，使智能体能够在推理过程中合成、验证和演化可执行工具。这属于单智能体的工具使用和自我演化范畴，且侧重于智能体机制本身的改进而非纯领域应用。",
                    "summary2": "本文旨在解决静态工具库在科学推理中覆盖不足和适应性差的问题。针对开放式科学计算任务，我们提出了一种Test-Time Tool Evolution (TTE)框架，通过动态合成、验证和演化可执行工具来增强智能体能力。我们在SciEvo、SciBench等benchmark上通过Accuracy和Tool Reuse Rate验证了其有效性，显著提升了推理准确率和工具复用效率。",
                    "summary_translation": "AI for Science（科学智能）的核心挑战不仅在于单纯的推理，更在于在开放式的科学世界中创造计算方法的能力。现有的 LLM-based agents（基于大语言模型的智能体）依赖于静态的、预定义的工具库，这种范式在工具稀疏、异构且本质上不完整的科学领域中根本无法奏效。在本文中，我们提出了 Test-Time Tool Evolution (TTE，测试时工具演化)，这是一种新的范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，TTE 克服了静态工具库的僵化性和长尾局限性。为了进行严格的评估，我们引入了 SciEvo，这是一个包含 1,590 个科学推理任务的 benchmark（基准），并由 925 个自动演化的工具提供支持。大量实验表明，TTE 在准确率和工具效率方面均达到了最先进的性能，同时实现了计算工具的有效 cross-domain adaptation（跨域适应）。代码和 benchmark 已在 https://github.com/lujiaxuan0520/Test-Time-Tool-Evol 发布。",
                    "inspiration_trace": "基于论文《Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程：\n\n### 1. 宏观观察：AI for Science 的“严谨性鸿沟”\n**思考起点：** 作者首先审视了当前大语言模型（LLM）在科学领域的应用现状。\n*   **现象：** LLM 拥有强大的推理能力，但在处理科学问题时，其概率性的本质往往导致“幻觉”，缺乏科学研究所必须的精确计算和严谨逻辑。\n*   **现有解法：** 业界通用的做法是给 LLM 配备外部工具（如计算器、API），即“工具增强”。\n*   **初步质疑：** 这种“LLM + 工具”的模式虽然解决了通用领域的部分问题，但在真正的科学研究中，是否足够？\n\n### 2. 问题诊断：静态工具库的“长尾困境”\n**深入分析：** 作者进一步剖析了现有工具增强范式在科学领域的根本缺陷。\n*   **核心矛盾：** 科学世界是开放、无边界的，而现有的工具库是**静态**且**预定义**的。\n*   **两大瓶颈：**\n    1.  **稀疏性与异构性：** 科学计算工具分散且非标准化，无法像通用 API 那样通过爬取构建一个“全知”的静态库。\n    2.  **不可预知性：** 科学探索往往涉及新颖的问题，需要全新的计算原语。静态库无法包含尚未被定义的工具。\n*   **结论：** 依赖静态工具库，本质上将 AI 限制在“被动选择者”的角色，无法应对开放的科学问题。这是一个**范式层面**的局限，而非工程细节问题。\n\n### 3. 核心假设：从“工具检索”到“工具进化”\n**范式转换：** 为了解决上述矛盾，作者提出了一个颠覆性的假设。\n*   **假设：** 一个真正的科学智能体，不应该只是从仓库里拿工具，而应该具备**在推理过程中即时创造和演化工具**的能力。\n*   **核心概念：** **Test-Time Tool Evolution (TTE，测试时工具进化)**。\n*   **逻辑推演：** 如果工具库是不完整的，那么它就不应该是固定的资源，而应该是**问题驱动的产物**。工具应该在解决问题的过程中被动态合成、验证并积累。\n\n### 4. 方法论构建：闭环进化机制\n**具体化思考：** 如何实现“工具进化”？作者构建了一个闭环逻辑，将科学方法论的迭代性引入 AI 系统。\n*   **第一步：结构化分解。** 面对复杂问题，不能直接生成代码，而应先将其拆解为原子化的子目标。这是为了精准定位需要什么样的工具。\n*   **第二步：动态检索与合成。** 先看库里有没有，没有就现场写一个。这里的关键是**“按需合成”**。\n*   **第三步：验证与原子化。** 生成的工具不能直接入库，必须经过严格的验证（语法、执行、领域逻辑）。更重要的是，要将复杂的工具拆解为**原子工具**。\n    *   *思考逻辑：* 只有原子化的工具才能被未来不同的问题复用，避免生成大量“一次性脚本”。\n*   **第四步：更新与修剪。** 库不能无限膨胀，需要基于使用频率进行优胜劣汰，保持工具库的高效和紧凑。\n\n### 5. 验证与拓展：零起点与跨域适应\n**场景推演：** 为了证明 TTE 的普适性，作者设定了两个极端的验证场景。\n*   **场景一：TTE-Zero（白板起家）。** 模拟人类科学家从零开始探索。初始工具库为空，看智能体能否在解决问题的过程中，自我演化出一套完整的科学计算工具集。\n*   **场景二：TTE-Adapt（跨域迁移）。** 模拟知识迁移。给智能体一个“材料科学”的工具库，让它去解决“化学”问题。看它能否通过进化，保留通用工具，淘汰不适用工具，并生成新领域的专用工具。\n*   **预期结果：** 如果 TTE 成立，它不仅能解决问题，还能演化出高复用率的核心科学原语。\n\n### 6. 最终愿景：定义“科学智能体”的新标准\n**思想升华：** 作者的思考最终落脚于对 AI 智能体的重新定义。\n*   **总结：** 科学推理的核心不在于参数知识的多寡，而在于**创造计算方法的能力**。\n*   **产出：** 这篇文章不仅仅是提出了一个算法框架，更是确立了“动态工具进化”作为下一代科学 AI 的核心范式。智能体从被动的工具使用者，进化为了主动的方法创造者。\n\n---\n\n**逻辑链总结：**\n**严谨性鸿沟** $\\rightarrow$ **静态工具库的局限性** $\\rightarrow$ **提出“测试时进化”假设** $\\rightarrow$ **构建“分解-合成-验证-原子化”闭环** $\\rightarrow$ **验证零起点与跨域能力** $\\rightarrow$ **确立主动创造的科学智能体范式**。"
                },
                {
                    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
                    "arxiv_id": "2601.07226",
                    "authors": "Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",
                    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了智能体AI系统和工作流，评估了工具使用和RAG任务，并分析了智能体在嘈杂环境下的鲁棒性，符合单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在解决推理模型在噪声环境下的鲁棒性问题。针对包含随机文档、无关聊天记录和困难负例的噪声上下文，我们提出了NoisyBench基准和RARE（Rationale-Aware Reward）奖励函数，并在NoisyBench的11个数据集上通过准确率验证了其有效性。",
                    "summary_translation": "推理模型和智能体 AI 系统的最新进展导致了对多样化外部信息的依赖日益增加。然而，这种转变引入了本质上包含噪声的输入上下文，而当前经过净化的基准未能捕捉到这一现实。我们介绍了 NoisyBench，这是一个综合基准，针对包括随机文档、无关聊天历史和困难负样本干扰项在内的多种噪声类型，系统地评估了模型在 RAG (检索增强生成)、推理、对齐和工具使用任务中跨越 11 个数据集的鲁棒性。我们的评估显示，当面临上下文干扰项时，最先进的模型会出现高达 80% 的灾难性性能下降。关键在于，我们发现智能体工作流通常通过过度信任含噪工具输出来放大这些错误，并且即使没有对抗性意图，干扰项也能触发涌现性不对齐。我们发现提示工程、上下文工程、SFT (监督微调) 和仅基于结果奖励的 RL (强化学习) 都无法确保鲁棒性；相比之下，我们提出的 Rationale-Aware Reward (RARE) (理由感知奖励) 通过激励识别噪声中的有用信息，显著增强了韧性。最后，我们发现了一种逆向缩放趋势，即在噪声环境中，增加测试时计算会导致性能下降，并通过注意力可视化证明模型过度关注干扰项标记，这为构建下一代鲁棒且具备推理能力的智能体提供了重要见解。",
                    "inspiration_trace": "基于论文《Lost in the Noise: How Reasoning Models Fail with Contextual Distractors》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定义（从“理想”到“现实”）\n\n1.  **观察现状**：\n    *   作者注意到AI范式正从单纯的“对话模型”转向“智能体系统”。这些系统严重依赖外部工具（如RAG、搜索、计算器）和长上下文来解决复杂任务。\n2.  **发现缺口**：\n    *   **理想 vs. 现实**：现有的学术基准测试大多是在“无菌”的清洁数据下进行的。然而，现实世界中的智能体面临的是充满噪声的环境（错误的检索结果、无关的聊天历史、工具输出错误）。\n    *   **假设**：当前SOTA模型在清洁环境下的高分可能掩盖了其在真实噪声环境下的脆弱性。这种脆弱性可能不仅仅是效率问题，而是根本性的推理崩溃。\n\n### 第二阶段：现象验证与深度剖析（“有多脆弱？”）\n\n1.  **构建验证工具**：\n    *   为了验证假设，作者构建了 **NoisyBench**。这是一个系统性的基准，涵盖了RAG、推理、对齐和工具使用四大类任务，并人为注入了三种噪声：随机文档、无关聊天历史、硬负样本。\n2.  **关键发现**：\n    *   **灾难性下降**：即使是顶尖模型（如Gemini-2.5-Pro），在面对噪声时性能也出现了高达80%的断崖式下跌。这证明了“清洁性能强 $\\neq$ 抗噪能力强”。\n    *   **智能体的悖论**：引入智能体工作流（使用工具）在清洁环境下能提升性能，但在噪声环境下反而**放大**了错误。这是因为智能体倾向于“过度信任”工具输出和上下文，导致错误在多步规划中累积。\n    *   **反向缩放定律**：在噪声环境下，增加推理步骤（测试时计算）反而导致性能下降。模型花费更多token去“思考”噪声，结果越想越错。\n\n### 第三阶段：解决方案的试错与迭代（“常规方法为何失效？”）\n\n在确认问题严重性后，作者尝试了现有的主流修复方案，但均遭遇失败：\n\n1.  **尝试一：提示工程与上下文工程**\n    *   **逻辑**：通过优化Prompt或重新组织上下文来引导模型忽略噪声。\n    *   **结果**：**失败**。模型无法通过简单的指令区分信号与噪声，上下文工程本身也容易受到噪声干扰。\n2.  **尝试二：监督微调（SFT）**\n    *   **逻辑**：在包含噪声的数据集（NoisyInstruct）上进行训练，让模型适应噪声。\n    *   **结果**：**失败**。导致了“灾难性遗忘”，模型失去了原有的推理能力，且并未真正学会抗噪。\n3.  **尝试三：基于结果的强化学习（Outcome-based RL）**\n    *   **逻辑**：只对最终答案的正确性进行奖励，让模型自己探索如何在噪声中得出正确答案。\n    *   **结果**：**部分有效但局限**。虽然比SFT好，但模型往往通过“作弊”或依赖内部记忆来得分，而不是真正学会从噪声中提取信息。它无法区分“答对了”是因为“抗噪成功”还是“碰巧蒙对”。\n\n### 第四阶段：核心洞察与方法论形成（从“结果导向”转向“过程导向”）\n\n1.  **核心洞察**：\n    *   作者意识到，单纯奖励“最终答案”是不够的。模型失败的根本原因在于**推理过程**被噪声劫持（注意力机制分析显示模型在错误预测时过度关注干扰项）。\n    *   因此，必须显式地奖励模型在推理过程中**识别并锚定有用信息**的行为，而不仅仅是奖励最终结果。\n2.  **方法论提出：RARE (Rationale-Aware Reward)**\n    *   **逻辑转变**：从“Reward the Outcome”转变为“Reward the Process”。\n    *   **具体机制**：设计一个新的奖励函数，不仅检查最终答案，还检查模型的思维链中是否正确引用或提取了上下文中的**有效参考信息**。\n    *   **作用原理**：通过奖励模型在噪声中“抓取”正确线索的行为，强迫模型学会过滤干扰项。这就像训练学生不仅要写出正确答案，还要在草稿纸上圈出解题依据。\n3.  **最终验证**：\n    *   实验证明，RARE 显著降低了模型被干扰的比例，同时提高了最终准确率。更重要的是，这种方法不仅提升了抗噪性，在清洁环境下也没有性能损失，实现了鲁棒性与通用性的双赢。\n\n---\n\n**总结**：\n作者的思考路径是从**现实应用场景的落差**出发，通过**基准测试量化了“噪声脆弱性”这一现象**，在排除了**提示工程和传统训练方法**的无效性后，抓住了**“推理过程被干扰”这一本质原因**，最终通过**引入过程级奖励（RARE）**，成功引导模型学会了在噪声中“去伪存真”的推理能力。"
                },
                {
                    "title": "LRAS: Advanced Legal Reasoning with Agentic Search",
                    "arxiv_id": "2601.07296",
                    "authors": "Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",
                    "summary": "While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on \"closed-loop reasoning\" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric \"closed-loop thinking\" to dynamic and interactive \"Active Inquiry\". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了LRAS框架，核心在于将法律大模型从静态的“闭环推理”转变为动态的“主动询问”，这涉及智能体的工具使用和交互能力。同时，论文引入了内省模仿学习和难度感知强化学习，分别对应智能体的自我反思和自我演化机制。尽管论文应用于法律领域，但其主要贡献在于提出了新的智能体搜索与演化框架，而非单纯的应用落地，因此符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有法律大模型因缺乏知识边界自省而在复杂场景下推理脆弱的问题。针对法律推理任务，我们提出了一种LRAS框架，通过Introspective Imitation Learning和Difficulty-aware Reinforcement Learning实现从“闭环思维”到“主动探究”的转变。我们在LexEval、LawBench等基准上通过准确率验证了其有效性，性能提升达8.2%-32%。",
                    "summary_translation": "尽管 Large Reasoning Models (LRMs，大型推理模型) 在数学领域展现了卓越的逻辑能力，但其在法律领域的应用仍受限于程序严谨性及遵循法律逻辑的严格要求。现有的 legal LLMs（法律大语言模型）依赖于仅源自内部参数化知识的“closed-loop reasoning”（闭环推理），往往缺乏对自身知识边界的认知，从而导致“自信但错误”的结论。为应对这一挑战，我们提出了 Legal Reasoning with Agentic Search (LRAS，基于智能体搜索的法律推理)，这是首个旨在将 legal LLMs 从静态且参数化的“closed-loop thinking”（闭环思维）转变为动态且交互式的“Active Inquiry”（主动探究）的框架。通过整合 Introspective Imitation Learning（内省模仿学习）和 Difficulty-aware Reinforcement Learning（难度感知强化学习），LRAS 赋能 LRMs 识别知识边界并应对法律推理的复杂性。实证结果表明，LRAS 的性能超越 state-of-the-art baselines（最先进的基线模型）8.2-32%，其中在需要基于可靠知识进行深度推理的任务中，提升幅度最为显著。我们将很快公开发布我们的数据和模型，以供进一步探索。",
                    "inspiration_trace": "基于论文《LRAS: Advanced Legal Reasoning with Agentic Search》的内容，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与问题界定\n**起点：** 大型推理模型（LRMs）在数学和符号逻辑领域表现出色，但在法律领域却遭遇瓶颈。\n**思考：** 法律领域不同于数学，它不仅需要逻辑，更要求极高的程序严谨性和对法律逻辑的严格遵循。现有的法律大模型大多依赖“闭环推理”，即仅利用模型内部的参数知识进行推断。\n**核心痛点：** 这种“闭卷考试”式的思维模式导致模型缺乏对自身知识边界的认知，经常在不知道答案时依然自信地输出错误结论（即“幻觉”），这在容错率极低的法律场景中是不可接受的。\n\n### 2. 深度诊断与假设验证\n**假设：** 既然模型内部知识不足，引入外部检索（RAG）是否就能解决问题？\n**实验观察：**\n*   **现象一（内省缺失）：** 在模型出错的案例中，虽然有搜索工具可用，但超过70%的情况下模型并未触发搜索。这说明主要问题不在于“缺乏知识”，而在于“缺乏自知之明”——模型不知道自己什么时候该去查资料。\n*   **现象二（复杂场景下的脆弱性）：** 在简单的法律任务上，静态检索（Full RAG）有效；但在需要深度推理的复杂任务上，静态检索的提升非常有限。这表明面对复杂案情，被动地接收检索结果是不够的，模型需要具备主动规划和多步探索的能力。\n\n**结论：** 仅仅给模型“喂”更多数据或简单的检索工具是不够的。必须从根本上改变模型的思维范式，从静态的“闭环思考”转向动态的“主动探究”。\n\n### 3. 范式转移与核心思路\n**核心思想：** 构建一个具有“代理搜索”能力的法律推理框架（LRAS），让模型像人类律师一样：先思考，发现知识盲区，主动检索，验证，再思考。\n**逻辑拆解：** 为了实现这一范式转移，需要解决两个递进的核心问题：\n1.  **“是否搜索”：** 解决内省缺失，让模型学会识别知识边界。\n2.  **“如何搜索”：** 解决复杂场景下的脆弱性，让模型学会在难题中自主规划多步搜索策略。\n\n### 4. 方法论构建\n基于上述逻辑，作者设计了双机制学习架构：\n\n*   **第一阶段：内省式模仿学习**\n    *   **目标：** 解决“是否搜索”的问题。\n    *   **逻辑：** 既然模型不知道自己不知道，那就通过专家示范来教它。通过合成包含“思考-搜索-验证”轨迹的高质量数据，训练模型模仿专家的行为——只有在遇到模糊或关键法律内容时，才主动触发搜索。这赋予了模型“内省”能力。\n\n*   **第二阶段：难度感知强化学习**\n    *   **目标：** 解决“如何搜索”的问题。\n    *   **逻辑：** 模仿学习只能教会基本的动作模式，但在模型依然做不出来的“硬骨头”案例上，需要更强的自主探索能力。作者筛选出SFT模型通过率低的困难样本，利用强化学习（GRPO）进行训练。通过奖励机制，鼓励模型在复杂场景下进行多轮探索和证据综合，从而从被动的“事实核查”进化为主动的“深度推理”。\n\n### 5. 逻辑闭环与验证\n**最终产出：** LRAS框架。\n**验证逻辑：** 实验结果显示，LRAS在需要深度推理的任务上提升显著（8.2%-32%），且在复杂案例中能主动进行多轮搜索并准确区分细微的法律概念（如“负责”与“报告工作”的区别）。这证明了从“被动接收”到“主动探究”的范式转移是提升法律AI推理能力的关键路径。"
                },
                {
                    "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
                    "arxiv_id": "2601.08829",
                    "authors": "Hsiang-Wei Huang, Junbin Lu, Kuang-Ming Chen, Jenq-Neng Hwang",
                    "summary": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了多个LLM智能体（评审员和领域主席）在评审系统中的交互、动态和自适应策略，涉及多智能体协作、通信与博弈，符合多智能体研究范围。",
                    "summary2": "本文旨在探索Elo排名系统中的LLM智能体审稿人动态。针对真实会议论文评审场景，我们提出了一种包含多轮交互、Elo评级机制及记忆模块的模拟框架，并在采自ICLR 2025的150篇论文上通过决策准确率等指标验证了其有效性。",
                    "summary_translation": "在本研究中，我们利用真实的会议论文投稿数据，探讨了基于 Elo 排名的审稿系统中 LLM (大语言模型) 代理审稿人的动态行为。多个具有不同人设的 LLM 代理审稿人参与由 Area Chair (领域主席) 主持的多轮审稿交互。我们将基线设置与引入了 Elo 评分和 Reviewer memory (审稿人记忆) 的设置进行了对比。模拟结果展示了若干有趣的发现，包括引入 Elo 评分如何提高 Area Chair 的决策准确性，以及审稿人利用 Elo 系统但未增加审稿投入的自适应审稿策略。我们的代码可在 https://github.com/hsiangwei0903/EloReview 获取。",
                    "inspiration_trace": "基于这篇论文的内容，我们可以将作者的思考过程还原为以下六个逻辑演进阶段：\n\n### 1. 宏观观察：同行评审的“黑箱”困境\n**起点：** 作者首先关注到学术界的一个核心痛点——同行评审虽然至关重要，但存在大量固有问题，如评审不一致、偏见、低质量以及评审员动机不明。\n**困境：** 在现实世界中，由于隐私和伦理限制，研究者很难直接对评审过程进行受控实验（例如，不能故意让评审员不认真干活来观察后果）。历史数据分析只能看到结果，无法观测过程中的心理和动态变化。\n\n### 2. 技术机遇与缺口识别：从“模拟”到“问责”\n**转折：** 随着大语言模型（LLM）的发展，基于智能体的模拟成为了解决上述“黑箱”问题的有效工具。现有的研究已经开始用LLM模拟评审过程。\n**缺口：** 作者敏锐地发现，现有模拟忽略了一个现代AI会议日益严重的现实问题：**低投入和不负责任的评审行为**。目前的机制大多是单轮惩罚，缺乏一种长期的、跨轮次的“纵向问责”机制。\n**聚焦：** 如何设计一个系统，不仅能评价论文，还能长期追踪和评价“评审员”本身？\n\n### 3. 核心假设引入：Elo评级系统的移植\n**灵感：** 作者将目光投向了竞技游戏（如国际象棋）中的Elo等级分系统。Elo系统能根据对手强弱动态调整评分，非常适合衡量相对能力。\n**假设：** 如果将Elo系统引入同行评审，建立一种跨轮次的反馈机制，是否能够：\n1. 帮助领域主席（AC）更准确地识别高质量评审，从而提高决策准确性？\n2. 激励评审员提高评审质量，或者至少抑制低投入行为？\n\n### 4. 方法论构建：多智能体动态博弈框架\n为了验证上述假设，作者构建了一个闭环的模拟环境，其核心逻辑包含两个维度：\n*   **角色异质性（Who）：** 为了模拟真实世界的复杂性，作者没有使用单一的评审员模型，而是设计了6种典型的“人设”，从理想的“专家”到敷衍的“略读者”，甚至包括“虚张声势者”。这为观察不同策略在Elo系统下的表现提供了基础。\n*   **动态反馈循环（How）：** 设计了“评审-打分-Elo更新-策略调整”的闭环。关键点在于引入了“记忆模块”，允许评审员根据上一轮的Elo变化，动态调整本轮的评审策略（例如，如果被扣分，是否改变语气或风格）。\n\n### 5. 实验设计：信息可见性的变量控制\n作者意识到，Elo系统的效果高度依赖于“谁看到了分数”。因此，逻辑推演进入实验设计阶段，通过控制信息可见性来剥离因果：\n*   **Baseline（基线）：** 无Elo，模拟现状。\n*   **AC Access（AC可见）：** 只有决策者知道评审员水平。这验证了Elo作为“辅助决策信息”的价值。\n*   **Full Access（全员可见）：** 评审员也能看到自己的分数。这验证了Elo作为“激励机制”的效果，以及是否存在“博弈”风险。\n\n### 6. 深层洞察：效率与博弈的权衡\n**预期结果：** 正如假设所料，Elo系统确实帮助AC提高了决策准确率，并有效惩罚了低投入的“略读者”。\n**意外发现（核心贡献）：** 作者发现了一个反直觉的现象——当评审员能看到自己的Elo分数时，他们并没有真正提高评审的实质质量（如更仔细地阅读），而是进行了**策略性适应**。他们学会了模仿AC喜欢的语气或风格来“刷分”。\n**结论升华：** 这使得文章的落脚点从“Elo系统很有用”深化到了“激励机制设计必须警惕古德哈特定律（即当一个指标成为目标，它就不再是一个好的指标）”。Elo虽然能筛选出好评审，但也可能导致评审员为了迎合系统而失去独立性。\n\n---\n\n**总结：**\n作者的思考路径是从**现实痛点**（评审质量难控）出发，利用**新技术**（LLM智能体）构建实验场，引入**跨学科机制**（Elo评级），通过**精细的变量控制**（信息可见性），最终揭示了**激励机制下的复杂人性博弈**（策略性迎合 vs. 实质提升）。"
                },
                {
                    "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
                    "arxiv_id": "2601.08747",
                    "authors": "Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li",
                    "summary": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了ACE框架，包含一个中央编排器智能体来协调检索器智能体和推理器智能体。这涉及智能体的规划（决定何时检索或推理）、工具使用（检索）以及多智能体协作，符合LLM智能体的研究范围。",
                    "summary2": "总结生成失败",
                    "summary_translation": "当前的上下文增强方法，例如 retrieval-augmented generation (检索增强生成)，对于解决 knowledge-intensive reasoning tasks (知识密集型推理任务) 至关重要。然而，它们通常遵循一种 rigid (僵化的)、brute-force (暴力策略)，在每一步都执行检索。这种 indiscriminate (无差别) 的方法不仅产生了不必要的 computational costs (计算成本)，还通过用 irrelevant noise (无关噪声) 充斥上下文而降低了性能。为了解决这些局限性，我们介绍了 Agentic Context Evolution (ACE，代理式上下文演进)，这是一个受 human metacognition (人类元认知) 启发的框架，能够动态决定是寻找新证据还是利用现有知识进行推理。ACE 采用一个 central orchestrator agent (中央编排代理)，通过 majority voting (多数投票) 策略性地做出决策。它旨在交替激活用于外部检索的 retriever agent (检索代理) 和用于内部分析与精炼的 reasoner agent (推理代理)。通过消除冗余的检索步骤，ACE 维持了一个简洁且 evolved context (演进的上下文)。在具有挑战性的 multi-hop QA benchmarks (多跳问答基准) 上进行的广泛实验表明，ACE 在准确率方面显著优于 competitive baselines (竞争性基线)，同时实现了高效的 token consumption (Token消耗)。我们的工作为推进针对复杂、知识密集型任务的 context-evolved generation (上下文演进生成) 提供了有价值的见解。",
                    "inspiration_trace": "基于论文《To Retrieve or To Think? An Agentic Approach for Context Evolution》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 1. 宏观观察：从“静态增强”到“迭代需求”\n**思考起点：**\n作者首先审视了当前大语言模型（LLM）处理知识密集型任务的主流范式——上下文增强（如RAG、ICL）。\n*   **现状：** 传统的“检索-生成”单步模式在处理简单问题时有效，但在面对复杂的多跳问题时，往往因为初始信息不足而失效。\n*   **演进：** 为了解决单步模式的局限，学术界开始转向“迭代式检索”，即在多步推理中反复检索信息。\n\n### 2. 问题聚焦：迭代检索中的“盲目积累”陷阱\n**深入批判：**\n作者敏锐地发现，现有的迭代检索方法虽然提升了性能，但存在一个严重的逻辑缺陷——**“盲目积累”**。\n*   **现象：** 这些方法通常遵循僵化的预设策略，在推理的每一步都强制执行检索操作。\n*   **后果：** 这种“暴力”策略导致了两个负面效应：\n    1.  **上下文饱和：** 引入了大量无关的噪声和冗余片段，干扰模型推理，导致幻觉。\n    2.  **资源浪费：** 不必要的检索带来了巨大的计算成本和延迟。\n*   **核心矛盾：** 信息的“量”并不等同于推理的“质”。单纯增加检索次数并不能保证解决问题，反而可能适得其反。\n\n### 3. 认知启发：人类元认知的引入\n**灵感来源：**\n为了解决“何时检索”的难题，作者将目光转向了人类的问题解决机制。\n*   **类比：** 人类在解决复杂问题时，不会无休止地翻阅资料（盲目检索），而是运用**元认知**能力。\n*   **机制：** 人类会动态评估当前的知识缺口，在“寻找外部证据”和“内部思考消化”之间灵活切换。只有当内部知识不足以支撑下一步推理时，才会去检索。\n*   **假设：** 如果让AI系统具备这种“反思”能力，动态决定是“检索”还是“思考”，就能避免噪声积累，保持上下文的“进化性”。\n\n### 4. 核心假设：从“固定流程”转向“上下文演进”\n**理论构建：**\n基于上述认知启发，作者提出了核心假设——**上下文演进**。\n*   **定义：** 上下文不应是静态的堆砌，而应是一个动态演化的过程。\n*   **策略：** 系统不应遵循固定的“检索-生成”时间表，而应将其视为一系列战略决策。目标是维持一个“紧凑且高效用”的上下文，而非最大的上下文。\n\n### 5. 方法论设计：代理式的“检索-思考”博弈\n**方案落地：**\n为了实现“上下文演进”，作者设计了一个多智能体框架（ACE），将抽象的认知策略具体化为可执行的逻辑：\n*   **决策机制：** 引入一个中央编排器，它不直接生成答案，而是负责“指挥”。为了决策的鲁棒性，采用多代理投票机制。\n*   **动作二元化：** 将复杂的推理过程拆解为两种原子动作：\n    1.  **RETRIEVE（检索）：** 仅在当前上下文确实存在知识缺口时，激活检索代理获取外部信息。\n    2.  **THINK（思考）：** 当现有信息足够时，激活推理代理进行内部分析、生成子查询并提炼信息，防止上下文膨胀。\n*   **闭环逻辑：** 通过“决策 -> 动作 -> 更新记忆”的循环，让上下文在深度和相关性上逐步进化，直到生成最终答案。\n\n### 6. 逻辑验证：效率与质量的平衡\n**预期结果：**\n作者通过实验设计来验证其逻辑链条的闭环：\n*   **准确性验证：** 预期ACE通过减少噪声，能在多跳QA任务上超越盲目检索的基线。\n*   **效率性验证：** 预期ACE通过减少冗余的检索步骤，能显著降低Token消耗。\n*   **行为分析：** 观察“Think”动作的比例随迭代步数增加而上升，证明模型确实学会了在后期更多依赖内部推理，从而证实了“动态决策”机制的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有迭代方法的“机械性”**出发，通过**借鉴人类元认知的“灵活性”**，提出了**“上下文演进”**的核心概念，最终通过**多代理动态决策（ACE）**实现了在检索与思考之间的智能平衡，解决了RAG领域中“噪声积累”与“计算冗余”的痛点。"
                },
                {
                    "title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents",
                    "arxiv_id": "2601.08742",
                    "authors": "Xin Quan, Jiafeng Xiong, Marco Valentino, André Freitas",
                    "summary": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了LLM智能体在多智能体环境（文本游戏）中的归因推理能力，涉及智能体的工具使用（外部定理证明器）以及在交互环境中的表现，符合多智能体和单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在解决传统NLI无法捕捉多智能体环境中潜在意图推理的问题。针对多智能体交互场景，我们提出了一种Attributional NLI (Att-NLI)框架，结合了溯因推理与演绎推理，并引入了Neuro-Symbolic方法。在Undercover-V文本游戏环境中，通过Spy win rate和Attributional Score等指标验证了其有效性。",
                    "summary_translation": "归因推断，即预测观察到的行为背后潜在意图的能力，对于在多智能体环境中运行的大型语言模型而言，是一项关键但尚未被充分探索的能力。事实上，传统的自然语言推断无法捕捉到复杂交互系统所必需的、细微且意图驱动的推理过程。为了填补这一空白，我们提出了归因自然语言推断，这是一个结合社会心理学原则对NLI进行扩展的框架，旨在评估智能体进行溯因意图推断（生成关于潜在意图的假设）以及随后的演绎验证（得出有效的逻辑结论）的能力。我们通过一个名为Undercover-V的文本游戏具体实现了Att-NLI，并针对三种具有不同推理能力和外部工具访问权限的LLM智能体进行了实验：仅使用演绎推理的标准NLI智能体、采用溯因-演绎推理的Att-NLI智能体，以及利用外部定理证明器进行溯因-演绎推理的神经符号Att-NLI智能体。大量实验展示了归因推断能力的明显层级，其中神经符号智能体的表现始终优于其他智能体，平均胜率达到17.08%。我们的结果强调了Att-NLI在开发具备高级推理能力的智能体方面的重要作用，同时也突显了神经符号人工智能在构建多智能体环境下的理性LLM智能体方面的潜在影响。",
                    "inspiration_trace": "基于论文《Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents》，以下是对作者核心方法论产出过程的逻辑推演：\n\n### 第一阶段：宏观问题识别——从“单点推理”到“交互意图”的缺失\n**观察：**\n传统的自然语言推理（NLI）主要评估的是静态、单智能体环境下的文本逻辑一致性（如：前提是否蕴含假设）。然而，随着LLM进入多智能体系统（如辩论、协作、博弈），仅仅判断“这句话在逻辑上是否成立”已经不够了。\n**痛点：**\n在多智能体交互中，一个智能体不仅要理解对方“说了什么”，更要推断对方“为什么这么说”（即潜在意图）。现有的NLI基准测试缺乏对这种**意图归因**能力的评估，导致LLM在复杂交互中往往只能进行表面应答，无法进行深度的策略性互动。\n\n### 第二阶段：理论锚点——引入社会心理学的归因理论\n**假设：**\n如何让机器具备推断意图的能力？作者转向社会心理学中的**归因理论**。该理论认为，人类在解释他人行为时，遵循一个两阶段过程：\n1.  **直觉推断：** 快速生成一个关于对方意图的假设。\n2.  **批判性修正：** 基于逻辑和证据对假设进行验证和调整。\n**映射：**\n作者将这一心理学过程映射到计算逻辑中，提出了**Attributional NLI (Att-NLI)** 框架：\n*   **阶段一（溯因推理/Abduction）：** 基于观察到的行为（前提），生成最合理的潜在意图假设（谁是间谍？他想干什么？）。\n*   **阶段二（演绎推理/Deduction）：** 将推断出的意图与原始前提结合，推导出最终的逻辑结论（我该如何描述？我该投谁？）。\n\n### 第三阶段：实证场景构建——解决“谎言”与“逻辑”的冲突\n**挑战：**\n为了测试Att-NLI，需要一个多智能体博弈环境。现有的狼人杀等游戏虽然涉及意图推断，但允许“撒谎”。从逻辑学角度看，谎言会导致逻辑爆炸，使得基于形式逻辑的验证变得不可靠。\n**创新：**\n作者设计了**Undercover-V**游戏（基于“谁是卧底”的变体）。\n*   **核心约束：** 禁止撒谎。玩家的描述必须与自己的词卡一致，但可以模糊处理。\n*   **逻辑价值：** 这一约束使得游戏状态是**可验证的**。因为所有描述都是基于事实的，智能体可以通过逻辑一致性来排除不可能的假设，从而将“意图推断”转化为一个可解的逻辑谜题，而非单纯的欺骗博弈。\n\n### 第四阶段：方法论迭代——从纯语言模型到神经符号增强\n**演进逻辑：**\n为了验证Att-NLI框架的有效性，作者设计了三种能力递进的智能体，形成对比实验：\n\n1.  **基准层：**\n    *   **思考：** 仅使用传统的NLI能力。\n    *   **缺陷：** 只能进行演绎推理，缺乏“意图选择”的溯因过程。它只能看到描述的表面文本，无法建模“对方是谁”这一潜在变量，因此在博弈中表现被动。\n\n2.  **进阶层：**\n    *   **思考：** 显式引入“溯因-演绎”两阶段流程。\n    *   **改进：** 在行动前，先进行“意图选择”（推测其他人的身份），再基于此进行“结论推断”。\n    *   **局限：** 依然依赖LLM自身的概率生成，可能产生幻觉或逻辑不严谨的意图假设。\n\n3.  **最优层：**\n    *   **思考：** 引入外部符号系统（定理证明器 Isabelle/HOL）作为逻辑校准器。\n    *   **核心机制：**\n        *   **形式化验证：** 将自然语言描述转化为逻辑形式，利用定理证明器验证描述与猜测词卡之间的逻辑有效性。\n        *   **反馈修正：** 如果逻辑验证失败，证明器会反馈错误信息，LLM据此修正对对手词卡的猜测。\n    *   **闭环：** 形成了“猜测 -> 形式化验证 -> 逻辑反馈 -> 修正猜测”的神经符号闭环，确保了意图推断的**逻辑可靠性**。\n\n### 第五阶段：评估维度设计——超越胜负的“归因得分”\n**思考：**\n仅仅看游戏的胜负率不足以衡量“意图推断”能力的提升，因为赢可能靠运气。\n**补充：**\n作者设计了**Attributional Score**，包含两个维度：\n1.  **归因合理性：** 智能体的描述是否更接近平民词而非间谍词（衡量意图选择的准确性）。\n2.  **归因一致性：** 智能体的描述是否与其他平民描述高度一致（衡量结论推断的融合度）。\n这一指标将评估焦点从“结果”引向了“推理过程的质量”。\n\n---\n\n**总结：**\n作者的思考路径是从**发现NLI在多智能体交互中的意图盲区**出发，**借用心理学归因理论**构建了“溯因+演绎”的认知框架，通过**设计禁止撒谎的Undercover-V游戏**解决了逻辑验证难题，并最终通过**引入神经符号定理证明器**实现了从概率性猜测到逻辑严密推理的跃升。"
                },
                {
                    "title": "Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization",
                    "arxiv_id": "2601.08682",
                    "authors": "Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu",
                    "summary": "Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确讨论了开发一个用于对话摘要的“智能体系统”，并分析了“智能体架构”中的任务分解（规划）和组件优化。虽然属于工业案例研究，但其核心内容涉及智能体系统的架构设计与开发流程，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决工业场景中需求不断演变的对话总结挑战。针对多方对话场景，我们提出了一种基于Agentic框架的可适应系统，通过任务分解实现组件级优化，并采用混合评估协议。我们在内部数据集上通过人类偏好A/B测试和校准后的LLM-as-a-judge指标验证了其优越性，显著优于单体系统。",
                    "summary_translation": "多方对话的摘要生成是产业界的一项关键能力，能够提升众多领域的知识转移和运营效率。然而，自动生成高质量摘要极具挑战性，因为理想的摘要必须满足一系列复杂且多维度的要求。尽管摘要生成在研究中受到了广泛关注，但现有工作主要利用静态数据集和基准，而在需求不可避免地演变的实际场景中，这种情况较为罕见。在这项工作中，我们展示了一个关于开发用于多方交互摘要生成的智能体系统的行业案例研究。我们分享了涵盖全开发生命周期的实践见解，旨在指导从业者构建可靠且适应性强的摘要生成系统，并为未来研究提供参考，内容涵盖：1) 在需求演变和任务主观性存在的情况下，进行评估的稳健方法；2) 利用智能体架构固有的任务分解实现的组件级优化；3) 上游数据瓶颈的影响；以及 4) 由于大语言模型提示词的可迁移性较差而导致的供应商锁定问题。",
                    "inspiration_trace": "基于论文《Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 第一阶段：宏观问题识别——从“静态学术理想”到“动态工业现实”\n\n**1. 观察与冲突：**\n作者首先观察到学术界与工业界在对话摘要任务上存在巨大的鸿沟。学术界通常依赖静态、定义明确的基准数据集，追求在固定标准下的SOTA（State of the Art）。然而，在Capital One的实际业务场景中，需求是高度动态的——随着业务发展和利益相关者的反馈，对于“完整性”、“准确性”和“可读性”的定义会不断演变。\n\n**2. 核心挑战：**\n如何在需求不断变化且缺乏静态“黄金标准”数据集的情况下，构建一个既能保证高质量，又能快速迭代适应新要求的摘要系统？\n\n---\n\n### 第二阶段：解决评估悖论——从“依赖专家”到“混合评估”\n\n**1. 初始尝试与瓶颈：**\n为了评估系统质量，作者最初尝试模仿学术界的做法，构建由领域专家（SME）标注的黄金数据集。但很快发现这不可持续：专家时间稀缺，且对于“什么信息重要”存在主观分歧，导致数据标注速度远落后于模型迭代速度。\n\n**2. 假设提出：**\n如果无法快速获得完美的“绝对真理”（黄金数据），能否利用LLM本身作为相对可靠的“裁判”来加速迭代？\n\n**3. 方法论演进：**\n作者提出了**混合评估协议**。\n*   **逻辑推演：** 既然人类A/B测试虽然可靠但太慢（每条30分钟），而LLM评分快但可能不准，那么就需要校准LLM。\n*   **解决方案：** 开发了AutoEval（LLM-as-a-Judge），并通过合成数据和人类A/B测试结果进行校准，证明其在可读性检测上甚至优于人类，且在整体偏好上与人类高度一致。这为后续的快速实验奠定了评估基础。\n\n---\n\n### 第三阶段：架构演进——从“单体模型”到“智能体分解”\n\n**1. 初始尝试与局限：**\n作者最初使用单体LLM进行摘要生成。但在面对多维度的复杂要求（准确性、完整性、可读性）时，单体模型遇到了瓶颈：通过调整Prompt来优化某一维度（如完整性）往往会损害另一维度（如准确性），出现边际效应递减。\n\n**2. 假设提出：**\n人类在写复杂文档时，通常会先起草，再分维度审核修改。那么，将摘要任务分解为独立的“起草”和“多维度审核”环节，是否能解决权衡难题？\n\n**3. 方法论演进：**\n作者构建了**分解式智能体架构**。\n*   **逻辑推演：** 单体Prompt难以同时兼顾所有约束。如果将任务拆解，让专门的Agent负责特定的维度（如准确性评估Agent、完整性评估Agent），就能实现更精细的控制。\n*   **关键突破（组件级优化）：** 作者发现仅优化端到端效果是不够的。真正的性能飞跃（v5版本）来自于**组件级优化**——即利用小规模标注数据，专门校准每个评估Agent的分类准确率，而不是只看最终摘要的好坏。这种“分而治之”的策略显著优于单体系统。\n\n---\n\n### 第四阶段：根因分析——从“模型逻辑缺陷”到“上游数据噪声”\n\n**1. 异常观察：**\n在优化模型的过程中，作者发现许多摘要错误并非模型推理能力不足，而是源于输入数据的混乱。\n\n**2. 深度诊断：**\n通过分析，作者发现近40%的错误源于ASR（语音转文字）的噪声。这些噪声不仅仅是错别字，而是将简单的“信息查找”任务转化为了复杂的“多轮推理”任务（例如，因为说话人识别错误，模型必须通过上下文猜测谁在说话）。\n\n**3. 方法论演进：**\n作者将关注点从单纯优化摘要模型，扩展到了**上游数据增强**。\n*   **逻辑推演：** 既然LLM在处理这种“噪声引发的推理难题”上表现脆弱，那么最好的优化方式是提升输入质量，或者让Agent对噪声具有鲁棒性。\n*   **解决方案：** 引入更强大的Whisper模型降低词错误率（WER），并在Prompt中注入包含噪声的示例，以增强Agent的抗干扰能力。\n\n---\n\n### 第五阶段：工程现实——从“模型无关”到“供应商锁定”\n\n**1. 观察与幻灭：**\n作者最初可能希望构建一个“模型无关”的系统，以便在不同LLM（如Llama vs. GPT-OSS）之间灵活切换。\n\n**2. 实验发现：**\n实验表明，完全相同的Prompt在不同模型上产生了系统性的行为差异。例如，让Llama变得谨慎的Prompt，却让GPT-OSS变得冗长且牺牲了准确性。\n\n**3. 最终洞察：**\n**Prompt的可移植性极差**。这揭示了当前LLM应用的一个残酷现实：不存在通用的“完美Prompt”，针对不同模型的特定缺陷进行定制化调优是不可避免的工程成本。这一发现否定了“一次编写，到处运行”的 naive 想法，转而强调建立适应不同模型特性的调优流程。\n\n---\n\n### 总结：作者的思维闭环\n\n作者的思考过程是一个典型的**“发现问题 -> 尝试传统方案 -> 遭遇现实瓶颈 -> 引入新范式（Agentic + Hybrid Eval） -> 深入根因分析（Data Noise） -> 接受工程约束”**的演进过程。\n\n他们没有停留在“如何让LLM写出更好的摘要”这一单一层面，而是上升到了**“如何在动态、嘈杂、多约束的真实工业环境中构建可靠的AI系统”**的系统工程高度。最终的方法论不仅是技术上的Agentic架构，更是一套适应生命周期的开发与评估流程。"
                },
                {
                    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
                    "arxiv_id": "2601.08605",
                    "authors": "Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li",
                    "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确研究Web智能体，提出了一种通过主动寻求经验来增强智能体交互能力的方法。这属于单智能体范畴，涉及记忆机制和智能体与环境的交互，符合筛选标准。",
                    "summary2": "本文旨在解决 Web Agent 被动注入经验难以适应动态上下文的问题。针对 Web Agent 多轮交互场景，我们提出了一种基于熵信号的自触发经验寻求框架 ExpSeek，通过估计步骤级熵阈值确定干预时机并生成定制指导。在 GAIA、WebWalkerQA 等四个基准上通过准确率验证了其有效性，显著优于基线方法。",
                    "summary_translation": "Web智能体中的Experience intervention（经验干预）正成为一种极具前景的技术范式，它通过从累积经验中提供有价值的见解，增强了智能体的交互能力。然而，现有方法主要在任务执行前将经验作为global context（全局上下文）被动注入，难以在agent-environment interaction（智能体-环境交互）过程中适应动态变化的contextual observations（上下文观测）。我们提出了ExpSeek，该方法将经验转向step-level（步骤级）的proactive seeking（主动寻求）：(1) 利用模型的intrinsic signals（内在信号）估计step-level entropy thresholds（步骤级熵阈值），以确定intervention timing（干预时机）；(2) 设计step-level（步骤级）的tailor-designed experience content（定制化经验内容）。在四个具有挑战性的web agent benchmarks（Web智能体基准）上，针对Qwen3-8B和32B模型的实验表明，ExpSeek分别实现了9.3%和7.5%的absolute improvements（绝对提升）。我们的实验验证了将entropy（熵）作为self-triggering signal（自触发信号）的可行性和优势，并揭示即使是一个4B参数的small-scale experience model（小规模经验模型），也能显著提升larger agent models（更大规模智能体模型）的性能。",
                    "inspiration_trace": "基于论文《ExpSeek: Self-Triggered Experience Seeking for Web Agents》的内容，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观观察：现有范式的局限性\n**起点：** Web Agent 在处理复杂、嘈杂的网页任务时，往往因为信息稀疏或环境动态变化而表现不佳。为了解决这个问题，学术界引入了“经验干预”，即利用历史成功或失败的轨迹来辅助当前决策。\n\n**现状分析：** 作者观察到，现有的主流方法（无论是离线精炼还是在线演化）大多遵循**“被动全局注入”**的模式。也就是说，在任务开始前，将所有相关的经验一股脑地塞给 Agent。\n\n**发现痛点：** 这种静态的“填鸭式”教学存在一个根本缺陷：Web 环境是动态变化的，Agent 的交互状态也是逐步演进的。初始阶段注入的经验，在经过几轮交互后，可能与当前的上下文脱节，导致 Agent 无法在关键时刻获得精准的指导。\n\n### 2. 核心假设：从“被动接受”转向“主动寻求”\n**思维跃迁：** 既然全局静态注入难以适应动态环境，为什么不模仿人类的学习方式——**在遇到困难时主动寻求帮助**？\n\n**提出新范式：** 作者提出了一个核心假设：将经验干预从“任务开始前的全局被动注入”转变为**“交互过程中的步骤级主动寻求”**。\n\n**界定关键问题：** 要实现这个新范式，必须解决两个核心问题：\n1.  **何时寻求？** Agent 怎么知道自己“迷路”了？\n2.  **寻求什么？** 给 Agent 提供什么样的经验内容最有效？\n\n### 3. 解决“何时寻求”：利用内在不确定性信号\n**思考过程：** 要让 Agent 自主决定何时求助，需要一个客观的、不依赖外部昂贵评估模型的信号。\n\n**信号选择：** 作者将目光投向了模型自身的**熵**。在信息论中，熵代表不确定性。作者推测：当 Agent 对下一步行动感到困惑或不确定时，其输出的概率分布会趋于平坦，导致熵值升高。\n\n**假设验证与量化：**\n*   **验证：** 通过分析训练轨迹，作者发现错误步骤的熵值确实显著高于正确步骤。这证实了熵可以作为“困惑度”的有效指标。\n*   **量化：** 为了避免硬阈值带来的僵化，作者引入了统计学方法（逻辑回归 + 自助法），根据熵值分布估算出一个动态的阈值区间。当熵值落入该区间时，触发经验寻求机制。这实现了**“Self-Triggered”（自触发）**。\n\n### 4. 解决“寻求什么”：从“检索”到“生成式引导”\n**思考过程：** 确定了触发时机后，下一个问题是给 Agent 看什么。传统的 RAG（检索增强生成）往往是直接把过去的相似案例扔给 Agent。\n\n**发现不足：** 作者认为，单纯的案例检索不够“解渴”。Agent 需的不是一本“参考书”，而是一句“点拨”。\n\n**构建经验库：**\n*   **结构化：** 作者设计了“经验三元组”：**行为**（发生了什么）+ **错误**（为什么错了）+ **引导**（该怎么做）。这种结构将原始轨迹转化为可迁移的元认知知识。\n*   **主题化：** 为了提高检索效率，将这些三元组按主题分类。\n\n**动态生成：** 作者没有直接把三元组塞给 Agent，而是引入了一个**“经验模型”**。该模型根据当前上下文，从经验库中检索相关主题，然后**实时生成**一段针对性的指导文本。这确保了经验是针对当前具体情境的“量身定制”。\n\n### 5. 逻辑闭环与验证：弱指导强\n**系统整合：** 将上述两部分结合，形成了 ExpSeek 框架：Agent 在交互中实时监控自身熵值，一旦超过阈值，即触发经验模型，获取基于当前上下文的动态引导。\n\n**深层洞察：** 作者在实验中发现了一个有趣的现象：即使是一个较小的 4B 经验模型，也能显著提升较大的 32B Agent 模型的性能。这验证了“弱指导强”的可行性——就像一个不需要比学生更聪明的老师，只要掌握了正确的教学方法（经验库），就能纠正学生的错误。\n\n### 总结\n作者的思考路径是从**对现有静态方法的批判**出发，通过**类比人类学习**提出“主动寻求”的范式，进而利用**信息熵**这一内在信号解决触发时机问题，最后通过**结构化经验库与生成式引导**解决了内容精准度问题，最终构建了一个自适应、动态的 Web Agent 增强框架。"
                },
                {
                    "title": "DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report",
                    "arxiv_id": "2601.08536",
                    "authors": "Ruizhe Li, Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao",
                    "summary": "Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“深度研究智能体”，这些智能体具备网络搜索（工具使用）、信息综合（规划）和生成长篇报告的能力，属于单智能体研究范畴。论文提出的基准旨在评估此类智能体的表现，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有深度研究系统评估标准粗糙或依赖LLM定义导致偏差的问题。针对 Deep Research Systems (DRS) 的长文本报告生成能力，我们提出了 Deep Research Bench II 基准，该基准通过四阶段 LLM+human 流程构建了 9,430 个源自专家报告的细粒度二元评分标准。我们在 132 个跨领域任务上对多个 SOTA 模型进行了评估，结果显示即使最强模型满足的评分标准也不足 50%，验证了该基准在揭示模型与人类专家差距方面的有效性。",
                    "summary_translation": "Deep Research Systems (DRS) (深度研究系统) 旨在帮助用户检索网络信息、综合信息内容，并提供全面的调查报告。然而，如何严格评估这些系统尚未得到充分探索。现有的 Deep-research benchmarks (深度研究基准) 通常存在两种失效模式。部分基准未能充分考察系统分析证据及撰写连贯报告的能力。另一些基准则依赖的评估标准要么过于粗糙，要么直接由 LLMs (大语言模型) 定义（或两者兼有），这导致评估分数相对于人类专家可能存在偏差，且难以验证或解释。为解决上述问题，我们提出了 Deep Research Bench II (深度研究基准 II)，这是一个用于评估 DRS 生成报告的新基准。该基准包含跨 22 个领域的 132 个 Grounded (基于事实的) 研究任务；对于每项任务，系统必须生成一篇长篇研究报告，该报告将依据总计 9430 个 Fine-grained binary rubrics (细粒度二元评分标准) 进行评估，涵盖信息召回、分析和展示三个维度。所有 Rubrics (评分标准) 均源自精心挑选的专家撰写的调查文章，并通过一个四阶段的 LLM+human pipeline (LLM+人类协作流水线) 构建而成，该流程结合了自动提取与超过 400 小时的人工专家审查，旨在确保评分标准具有原子性、可验证性，并与人类专家的判断保持一致。我们在 Deep Research Bench II 上评估了多个 State-of-the-art (最先进的) 深度研究系统，结果发现，即使是表现最强的模型，其满足的 Rubrics 也不到 50%，这揭示了当前 DRS 与人类专家之间存在显著差距。",
                    "inspiration_trace": "基于论文《DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report》，以下是对作者核心方法提出过程的逻辑链推演。这一过程从宏观的行业痛点出发，逐步聚焦到具体的评估方法论构建。\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**（从“Deep Research”热潮到评估困境）**\n\n1.  **现象观察**：\n    *   随着 LLM 的发展，出现了许多“深度研究系统”（如 OpenAI Deep Research, Gemini Deep Research）。这些系统旨在模拟人类研究员，进行多步搜索、信息综合并撰写长篇报告。\n    *   **核心矛盾**：虽然系统宣称能力强大，但学术界缺乏一套严谨的评估体系来量化其真实水平。现有的评估基准存在明显的“两极分化”缺陷。\n\n2.  **现有基准的缺陷诊断**：\n    *   **缺陷一（过于简单）**：部分基准（如 BrowseComp）仅关注“固定答案”任务（如查找特定实体或数字）。这只能测试检索能力，无法测试系统在开放域下的分析、综合和长文写作能力。\n    *   **缺陷二（不可靠）**：部分基准（如 DeepResearch Bench v1）虽然评估长篇报告，但评估标准存在严重问题：\n        *   **标准由 LLM 自定义**：让 LLM 自己定义评分标准，会导致“锚定偏差”，即评分标准可能偏离人类专家的真实认知。\n        *   **颗粒度过粗**：标准模糊（如“是否有洞察力”），导致难以验证。\n        *   **依赖内部知识**：Judge LLM 往往需要依赖其内部参数知识来判断事实真伪，这容易导致幻觉或无法验证网络上的新信息。\n\n### 第二阶段：核心假设与思路转向\n**（从“LLM 评判”转向“专家对标”）**\n\n1.  **确立目标**：\n    *   构建一个既能测试长篇报告质量，又能保证评估客观、可验证、细粒度的基准。\n\n2.  **寻找“锚点”**：\n    *   **思考**：什么是高质量深度研究的“金标准”？\n    *   **假设**：人类专家撰写的调查报告（Expert Reports）是最佳答案。这些报告经过同行评审，具有极高的可信度和分析深度。\n\n3.  **方法论突破**：\n    *   **逆向工程思维**：与其让 LLM 凭空生成评分标准，不如直接从专家报告中**提取**评分标准。\n    *   **逻辑推演**：如果我们将一篇专家报告视为“完美答案”，那么这篇报告中包含的所有关键事实、分析逻辑和结构要素，就构成了评估模型生成的“评分细则”。\n\n### 第三阶段：方法论构建与约束设计\n**（如何将专家报告转化为可执行的 Rubrics）**\n\n1.  **定义评估维度**：\n    *   为了全面评估“深度研究”能力，作者将其解构为三个核心维度：\n        *   **信息召回**：是否找到了正确的事实？\n        *   **分析**：是否进行了有意义的综合和推理？\n        *   **呈现**：报告的结构和表达是否清晰？\n\n2.  **解决“可验证性”难题**：\n    *   **挑战**：如何确保 Judge LLM 不依赖其内部知识（避免幻觉）？\n    *   **设计原则**：Rubrics（评分细则）必须是**原子化**和**二值化**的。\n    *   **具体策略**：Rubric 不应是笼统的“讨论了经济原因”，而应是具体的“明确指出劳动力流失的原因是产业结构不匹配”。这样，Judge LLM 只需要做文本匹配和逻辑判断，而不需要知道“产业结构不匹配”本身是否正确（因为这是从专家报告中提取的真理）。\n\n3.  **构建数据流水线**：\n    *   **问题**：直接让 LLM 从专家报告中提取 Rubrics 可能会产生幻觉或提取非关键信息。\n    *   **解决方案**：设计“四阶段流水线”以确保质量：\n        1.  **LLM 提取**：从专家报告中初稿提取任务和 Rubrics。\n        2.  **自评迭代**：让 LLM 用提取的 Rubrics 去评估专家报告本身，如果准确率低于 90%，说明提取有误，重新生成。\n        3.  **人工修订**：清理逻辑不一致或冗余的条目。\n        4.  **专家审核**：领域专家最终把关，确保 Rubrics 符合专业认知。\n\n### 第四阶段：验证与闭环\n**（证明基准的有效性）**\n\n1.  **实验设计**：\n    *   收集 132 个任务，生成 9,430 个细粒度 Rubrics。\n    *   选取当前最强的 SOTA 模型（如 GPT-o3, Gemini-3-Pro）进行测试。\n\n2.  **结果验证**：\n    *   **发现**：即使是表现最好的模型，满足的 Rubrics 也不到 50%。\n    *   **结论**：这不仅证明了基准的难度（没有天花板效应），也揭示了当前 DRS 与人类专家之间巨大的鸿沟，特别是在“信息召回”和“分析”维度。\n\n---\n\n### 总结：作者的思考路径图\n\n1.  **起点**：现有评估要么太简单（查事实），要么太虚（LLM 自己定标准）。\n2.  **转折**：放弃“LLM 生成标准”，改用“人类专家报告”作为标准的源头。\n3.  **核心**：将专家报告逆向拆解为成千上条**原子化、可验证的二元 Rubrics**。\n4.  **保障**：通过“自评+人工+专家”的四阶段流水线，确保 Rubrics 的质量不依赖 LLM 的幻觉。\n5.  **终点**：构建出一个既能逼真模拟人类研究任务，又能像批改试卷一样客观评分的基准。"
                },
                {
                    "title": "Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management",
                    "arxiv_id": "2601.08435",
                    "authors": "Weitao Ma, Xiaocheng Feng, Lei Huang, Xiachong Feng, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Bing Qin",
                    "summary": "Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于大语言模型智能体的记忆管理机制，提出了通过细粒度反馈对齐来优化长时记忆任务中的记忆操作。这直接属于“单智能体”研究范围中的“记忆”模块，且不涉及纯应用、纯推理或安全对齐等排除领域。",
                    "summary2": "本文旨在解决长视界记忆管理中的奖励稀疏与信用分配难题。针对流式信息块，我们提出Fine-Mem框架，引入Chunk-level Step Reward提供即时监督，并利用Evidence-Anchored Reward Attribution实现精细化的奖励归因。我们在Memalpha和MemoryAgentBench上通过准确检索、测试时学习等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "有效的 memory management（记忆管理）对于 large language model agents（大语言模型智能体）处理 long-horizon tasks（长视界任务）至关重要。近期研究探索了利用 Reinforcement Learning（强化学习）来开发专门的 memory manager agents（记忆管理智能体）。然而，现有方法主要依赖最终任务表现作为奖励，这导致了严重的 reward sparsity（奖励稀疏性）和无效的 credit assignment（信用分配），无法为单个 memory operations（记忆操作）提供充分的指导。为此，我们提出了 Fine-Mem，这是一个旨在实现 fine-grained feedback alignment（细粒度反馈对齐）的统一框架。首先，我们引入了 Chunk-level Step Reward（分块级步奖励），通过辅助性的 chunk-specific question answering tasks（特定分块问答任务）提供即时的 step-level supervision（步级监督）。其次，我们设计了 Evidence-Anchored Reward Attribution（证据锚定奖励归因），通过将信用锚定到关键的 memory operations（记忆操作），基于推理过程中作为证据使用的特定 memory items（记忆项）来重新分配 global rewards（全局奖励）。这些组件共同实现了稳定的 policy optimization（策略优化），并将 local memory operations（局部记忆操作）与 memory 的 long-term utility（长期效用）对齐。在 Memalpha 和 MemoryAgentBench 数据集上的实验表明，Fine-Mem 始终优于强 baselines（基线模型），在各种子任务中实现了更高的 success rates（成功率）。进一步的分析揭示了其在不同 model configurations（模型配置）和 backbones（骨干网络）下的适应性和强大的 generalization capabilities（泛化能力）。",
                    "inspiration_trace": "基于论文《Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 1. 宏观观察：LLM智能体的“记忆瓶颈”\n**思考起点**：随着大语言模型（LLM）向智能体演进，任务从单轮对话转向了长时序、多步骤的复杂推理。\n**发现问题**：现有的LLM受限于固定的上下文窗口，难以处理跨越长时段的信息流。为了解决信息衰减和决策连贯性问题，必须引入外部记忆系统。然而，如何高效地管理这些记忆（即决定读什么、写什么、删什么）成为了一个核心挑战。\n\n### 2. 范式对比：从“规则驱动”到“学习驱动”的演进\n**现状分析**：作者审视了现有的记忆管理范式，发现主要分为两类：\n*   **工作流模式**：依赖预定义的启发式管道和强LLM（如A-Mem, LightMem）。\n    *   *缺陷*：计算开销大，扩展性差，行为模式僵化。\n*   **训练模式**：利用强化学习（RL）训练专门的记忆管理智能体（如Mem-α）。\n    *   *优势*：更具适应性和潜力。\n    *   *现状*：这是未来的方向，但目前效果仍不理想。\n\n### 3. 核心诊断：长时序任务中的“反馈鸿沟”\n**深度剖析**：作者决定聚焦于“训练模式”，并挖掘其性能瓶颈的根本原因。通过分析RL在长时序任务中的表现，作者识别出两个关键痛点：\n*   **奖励稀疏性**：现有的RL方法主要依赖最终任务的成功与否作为奖励。记忆管理是一个包含数百个步骤的序列过程，如果只在最后给一个“好”或“坏”的分数，中间的每一个操作（插入、更新、删除）都缺乏即时的指导信号。\n*   **信用分配困难**：即使最终任务成功了，我们也很难知道是哪一步的记忆操作起了关键作用。是第5步插入的信息有用，还是第50步更新的信息有用？现有的全局奖励无法将功劳精确归因到具体的操作步骤上。\n\n### 4. 假设提出：细粒度反馈对齐的必要性\n**逻辑推演**：既然问题出在“反馈太粗”和“归因不准”，那么解决方案必须是**细粒度的反馈对齐**。\n**核心假设**：如果能为记忆管理器的每一步操作提供即时的、有针对性的反馈，并能准确追溯最终成功的关键步骤，就能稳定训练过程，使局部操作与长期效用对齐。\n\n### 5. 方法论构建：双重机制解决双重难题\n基于上述假设，作者设计了Fine-Mem框架，通过两个互补的组件来分别解决上述两个痛点：\n\n*   **针对“奖励稀疏性” -> 引入 Chunk-level Step Reward (CSR)**\n    *   *思考*：如何给每一步提供即时反馈？不需要等到最后任务结束，可以在处理每一个信息块时进行“小测验”。\n    *   *机制*：为每个输入的信息块自动生成QA对。如果记忆管理器在处理完该块后，能够准确回答这些基于该块的问题，就给予即时奖励。这提供了过程级的密集监督。\n\n*   **针对“信用分配困难” -> 引入 Evidence-Anchored Reward Attribution (EARA)**\n    *   *思考*：如何追溯功劳？关键在于“证据”。当最终任务回答正确时，推理模型一定检索了某些特定的记忆条目。\n    *   *机制*：将这些被检索到的记忆条目作为“证据”，反向追踪它们是在哪一步被写入或更新的。将最终的全局奖励重新分配给这些产生关键证据的步骤，从而实现精确的信用归因。\n\n### 6. 逻辑闭环：Fine-Mem框架的统一\n**最终整合**：作者将上述两个组件整合到一个统一的强化学习框架中。\n*   利用CSR解决“当下做得好不好”的问题（局部信息保留）。\n*   利用EARA解决“哪步对未来最有用”的问题（长期效用归因）。\n*   结合辅助奖励（如格式检查、压缩率），使用GRPO算法优化策略。\n\n**总结**：作者从LLM智能体的记忆需求出发，敏锐地指出现有RL训练方法的反馈机制存在结构性缺陷（稀疏与归因难），进而通过“过程监督”与“证据回溯”相结合的思路，构建了一套能够实现细粒度反馈对齐的记忆管理框架。"
                },
                {
                    "title": "D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning",
                    "arxiv_id": "2601.08282",
                    "authors": "Kangcheng Luo, Tinglang Wu, Yansong Feng",
                    "summary": "Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个双智能体系统，包含推理者和净化器，涉及多智能体协作、动态规划以及检索工具使用，符合LLM智能体的定义。",
                    "summary2": "本文旨在解决复杂检索增强推理中搜索链构建无效及外围证据干扰的问题。针对复杂多跳问答任务，我们提出了一种名为D$^2$PLAN的双智能体动态全局规划范式，通过Reasoner执行动态规划及Purifier过滤关键信息，并在HotpotQA、MuSiQue等六个QA基准数据集上通过Exact Match (EM)和LLM-as-a-Judge (LasJ)指标验证了其有效性，显著优于现有基线模型。",
                    "summary_translation": "近期，利用强化学习训练的搜索增强大语言模型能够在多跳推理任务中交替执行搜索与推理。然而，随着累积的上下文中充斥着关键证据与无关信息，这些模型面临两种关键的失效模式：(1) 无效的搜索链构建，即生成错误的查询或遗漏关键信息的检索；(2) 外围证据导致的推理劫持，致使模型将干扰项误识别为有效证据。为应对上述挑战，我们提出了 **D$^2$Plan**，这是一种面向复杂检索增强推理的**双智能体动态全局规划**范式。**D$^2$Plan** 通过 *Reasoner*（推理者）和 *Purifier*（净化器）的协作运行：*Reasoner* 在推理过程中构建显式的全局计划，并根据检索反馈进行动态调整；*Purifier* 评估检索相关性，并为 *Reasoner* 提炼关键信息。我们进一步引入了一个两阶段训练框架，包括基于合成轨迹的监督微调冷启动，以及采用面向计划奖励的强化学习，旨在教导大语言模型掌握 **D$^2$Plan** 范式。大量实验表明，**D$^2$Plan** 能够实现更连贯的多步推理，并对无关信息表现出更强的鲁棒性，从而在具有挑战性的问答基准测试中取得了卓越的性能。",
                    "inspiration_trace": "基于论文《D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从发现问题到提出解决方案的完整思考过程。\n\n---\n\n### 第一阶段：现象观察与问题定义\n**（从“现有模型能做什么”到“它们在什么情况下会失败”）**\n\n1.  **宏观背景**：\n    作者首先关注到当前基于强化学习（RL）的搜索增强型大模型（LLM）已经具备了自主 interleaving（交替）搜索和推理的能力，能够处理多跳问题。\n\n2.  **深入观察（痛点发现）**：\n    然而，作者观察到随着推理链的延长，上下文中会积累大量检索到的文档，其中既有关键证据，也充斥着无关信息。在这种“嘈杂”的长上下文环境中，模型表现出两种典型的失败模式：\n    *   **失败模式 E1（无效的搜索链构建）**：模型在生成查询时发生“目标漂移”，或者遗漏了关键信息的检索。这表现为模型忘记了最初的问题目标，导致后续搜索跑偏。\n    *   **失败模式 E2（被边缘证据劫持推理）**：模型无法区分检索结果中的“干扰项”和“有效证据”，将无关信息当作事实依据进行推理，或者在检索失败时未能进行自我纠正。\n\n3.  **量化验证**：\n    通过对现有SOTA方法的错误分析，作者确认E1和E2分别占据了总失败率的29%和63%，证明这并非偶发个案，而是核心瓶颈。\n\n---\n\n### 第二阶段：归因分析与核心假设\n**（从“现象是什么”到“为什么会发生这种现象”）**\n\n1.  **针对 E1 的归因（缺乏全局视野）**：\n    作者认为，模型之所以会发生搜索链漂移，是因为它缺乏**全局规划**能力。现有的方法往往是“走一步看一步”的局部反应式推理。当上下文变长时，模型对原始问题的注意力被稀释，导致它无法维持一条连贯的、指向最终目标的推理路径。\n\n2.  **针对 E2 的归因（缺乏抗噪能力）**：\n    作者认为，模型之所以会被干扰信息误导，是因为它被迫直接处理原始的、未经筛选的检索结果。单一模型既要负责复杂的逻辑推理，又要负责从海量噪声中提取信息，这种“多任务处理”导致了认知负荷过载，从而降低了判断力。\n\n3.  **形成核心假设**：\n    要解决上述问题，模型必须具备两种核心能力：\n    *   **C1 动态全局规划**：不仅要制定初始计划，还要能根据检索反馈动态调整计划。\n    *   **C2 对边缘证据的鲁棒性**：需要专门的机制来过滤噪声，只保留关键信息给推理模块。\n\n---\n\n### 第三阶段：方法论构建——架构设计\n**（从“需要什么能力”到“如何通过架构实现这些能力”）**\n\n1.  **引入“双智能体”架构**：\n    为了解决C2（抗噪能力）并减轻推理器的负担，作者提出了**职责分离**的思路：\n    *   **推理者**：专注于高层规划、逻辑推理和工具调用。\n    *   **净化者**：作为一个专门的过滤器，负责评估检索结果的相关性，并从中提取关键信息。\n    *   **逻辑推演**：通过引入Purifier，Reasoner不再直接面对嘈杂的原始文档，而是接收经过“提纯”的信息。这不仅提高了信息的信噪比，还缩短了Reasoner的上下文长度，提升了推理效率。\n\n2.  **定义协作机制**：\n    两个智能体如何协作？作者设计了一个闭环：Reasoner发起查询 -> Retriever检索文档 -> Purifier过滤并反馈 -> Reasoner根据反馈继续推理。\n\n---\n\n### 第四阶段：方法论构建——机制创新\n**（从“架构搭建”到“具体的动态规划机制”）**\n\n1.  **显式全局规划**：\n    为了解决C1（全局视野），作者要求Reasoner在推理开始前，先显式地生成一个**全局计划**（即有序的子问题序列）。这相当于给模型画了一张“地图”，防止它在后续步骤中迷路。\n\n2.  **动态适应性**：\n    作者意识到静态计划是不够的，因为检索可能会失败。因此，引入了**动态适应机制**，包含两个关键动作：\n    *   **计划细化**：当检索成功时，利用已知信息填充占位符，使后续的子问题更加具体和自包含，防止语义漂移。\n    *   **计划修订**：当检索失败（达到最大尝试次数仍未找到相关信息）时，触发全局重规划，探索替代路径。\n    *   **逻辑推演**：这种设计将“规划”从一个静态的初始化步骤，变成了一个贯穿推理全过程的动态反馈控制系统。\n\n---\n\n### 第五阶段：训练策略优化\n**（从“方法设计”到“如何教会模型掌握这种方法”）**\n\n1.  **冷启动监督微调（SFT）**：\n    由于D$^2$PLAN是一个复杂的范式，直接用RL训练很难收敛。作者利用强大的Teacher Model，通过精心设计的Prompt，模拟出符合D$^2$PLAN工作流的高质量轨迹。通过SFT，让模型先学会“怎么做”。\n\n2.  **面向计划的强化学习（SPLAN RL）**：\n    仅仅模仿是不够的，还需要通过RL来探索和优化。作者发现传统的RL奖励（仅关注最终答案）无法有效指导“规划”这一过程行为。\n    *   **创新点**：设计了**面向计划的奖励函数**。\n        *   **初始规划奖励**：奖励模型正确识别问题的跳数，防止过度分解或分解不足。\n        *   **计划适应奖励**：奖励模型在检索失败时正确触发修订，或在检索成功后正确细化后续问题。\n    *   **逻辑推演**：通过这些细粒度的过程奖励，作者将“好的规划”和“好的适应”这一抽象概念转化为可优化的数学目标，从而引导模型真正内化动态规划的能力。\n\n---\n\n### 总结：逻辑演进的全景图\n\n1.  **观察**：现有RL搜索模型在长上下文中容易迷路（E1）和被干扰（E2）。\n2.  **诊断**：根本原因是缺乏全局规划（C1）和缺乏专门的噪声过滤机制（C2）。\n3.  **架构解法**：提出双智能体，让Purifier专门处理噪声，Reasoner专注于规划与推理。\n4.  **机制解法**：在Reasoner中引入显式且动态的规划机制，使其能根据反馈调整路线。\n5.  **训练解法**：先通过SFT用Teacher数据冷启动，再通过引入特定规划奖励的RL来强化动态规划行为。\n\n这一逻辑链条清晰地展示了作者如何从具体的错误模式出发，层层递进，最终构建出一个集成了架构创新、机制创新和训练创新的完整系统。"
                },
                {
                    "title": "Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees",
                    "arxiv_id": "2601.08274",
                    "authors": "Kun Li, Zenan Xu, Junan Li, Zengrui Jin, Jinghao Deng, Zexuan Qiu, Bo Zhou",
                    "summary": "Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "论文重点研究如何通过强化学习框架（DART）使大模型在推理过程中自发地使用工具，属于“单智能体：工具使用”的研究范围。同时，该方法通过反馈机制自我完善推理链，也符合“自我演化”的标准。",
                    "summary2": "本文旨在解决无需人工标注即可将工具使用集成到大语言模型长思维链中的难题。针对复杂的数学与科学推理任务，我们提出了一种名为 DART 的强化学习框架。该方法通过动态构建 Rollout 树发现工具调用机会，并利用基于树的过程优势估计精确强化有益的工具集成行为。我们在 AIME24、AIME25 和 GPQA-Diamond 基准上通过 Pass@1 和 Pass@8 指标验证了其有效性，实验结果表明该方法显著优于现有基线。",
                    "summary_translation": "Tool-Integrated Reasoning（工具集成推理）已成为增强 Large Language Models (LLMs，大语言模型) 计算能力的关键范式，然而将 tool-use（工具使用）集成到 long Chain-of-Thought (long CoT，长思维链) 中仍是一个未被充分探索的领域，这主要归因于训练数据的匮乏，以及在集成 tool-use（工具使用）时不损害模型内在长链推理能力的挑战。在本文中，我们提出了 DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees，基于展开树的工具集成推理链发现与强化)，这是一个 reinforcement learning（强化学习）框架，能够在无需 human annotation（人工标注）的情况下，实现 long CoT（长思维链）推理过程中的自发性 tool-use（工具使用）。DART 通过在训练过程中构建动态 rollout trees（展开树）来发现有效的 tool-use（工具使用）机会，并在有潜力的位置进行分支，以探索多样化的 tool-integrated trajectories（工具集成轨迹）。随后，基于树的 process advantage estimation（过程优势估计）会识别并奖励那些 tool invocation（工具调用）对解决方案产生积极贡献的特定 sub-trajectories（子轨迹），从而有效地强化这些有益行为。在 AIME 和 GPQA-Diamond 等具有挑战性的 benchmarks（基准测试）上进行的广泛实验表明，DART 显著优于现有方法，成功实现了 tool execution（工具执行）与 long CoT reasoning（长思维链推理）的协调统一。",
                    "inspiration_trace": "基于对论文《Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees》的深入分析，以下是作者产出该核心方法（DART）的逻辑推演与思考过程还原：\n\n### 1. 宏观问题与现状观察\n**思考起点：** 当前大模型（LLM）领域存在两个显著但割裂的范式：\n*   **长思维链：** 以DeepSeek-R1、OpenAI o1为代表，通过极长的自然语言推理链解决复杂问题，但在精确计算和符号操作上存在天然短板。\n*   **工具集成推理（TIR）：** 通过调用Python解释器等工具弥补计算缺陷，但现有研究多局限于短思维链，难以处理需要深度逻辑推理的复杂任务。\n\n**核心假设：** 如果能将“工具的精确计算能力”无缝融入“长思维链的深度推理过程”中，将能突破现有模型在数学和科学推理上的性能天花板。\n\n### 2. 瓶颈识别与痛点分析\n**思考深入：** 为什么现有的TIR方法无法直接迁移到长思维链（Long CoT）中？作者排除了几条显而易见但行不通的路径：\n*   **路径一（提示工程）：** 实验发现，提示词只能影响模型的最终答案生成部分，而无法渗透进模型的“思考”部分。即模型在Long CoT阶段对工具调用指令“免疫”。\n*   **路径二（监督微调 SFT）：** 长CoT的工具标注数据极其昂贵且稀缺。更重要的是，在新的工具数据上进行SFT会破坏模型原有的长推理分布，导致灾难性遗忘。\n*   **路径三（直接强化学习 RL）：** 在极长的推理序列中，模型随机自发调用工具的概率极低，导致RL训练时的探索效率极低，难以获得有效的奖励信号。\n\n**结论：** 问题的根源在于**“数据稀缺”**与**“分布割裂”**。模型在预训练中已经具备了工具使用能力，在后期训练中具备了长推理能力，但这两者在推理过程中处于两个互不相交的输出分布中，模型不知道何时、何地切换到工具模式。\n\n### 3. 核心洞察与策略转向\n**思维跃迁：** 既然模型已经“知道”怎么用工具，也“知道”怎么长推理，我们不需要教它新知识，只需要帮它**“发现”**在长推理的哪个环节插入工具是有效的。\n*   **策略转变：** 从“依赖标注数据的有监督学习”转向“基于自我探索的强化学习”。\n*   **目标：** 在不破坏原有长推理能力的前提下，通过自举过程融合两种分布。\n\n### 4. 方法论构建：发现与强化\n为了实现上述策略，作者将问题拆解为两个子问题：**如何发现工具插入点？** 和 **如何精准奖励有效行为？**\n\n#### 4.1 发现机制：动态推演树\n**思考：** 推理过程本质上是一个树状搜索过程。为了找到工具的最佳插入点，我们需要在推理路径上进行“分叉”探索。\n*   **位置选择：** 在推理树的哪个节点分叉？作者引入**“熵”**作为指标。高熵意味着模型在该步骤的不确定性高，这通常是需要外部工具辅助（如计算、验证）的关键时刻。\n*   **行为诱导：** 选定位置后，如何让模型用工具？作者不依赖模型自发产生，而是主动**“注入”工具提示**，强制模型在此处分叉出一条包含代码执行的子路径。\n*   **结果：** 构建了一个包含自然语言路径和工具集成路径的混合推演树，为后续训练提供了多样化的轨迹数据。\n\n#### 4.2 强化机制：基于树的进程优势估计\n**思考：** 传统的RL只奖励最终答案的正确性。但在长CoT中，即便答案正确，我们也需要知道是哪一步的工具调用起了关键作用，还是仅仅因为运气好。\n*   **归因困境：** 如果一条包含工具的路径答对了，我们需要区分是工具的功劳，还是原本推理能力的功劳。\n*   **解决方案：** 利用推演树的结构进行细粒度的**过程监督**。\n    *   **全局优势：** 比较当前节点与树根（平均表现），评估该路径的整体质量。\n    *   **局部优势：** 比较当前节点与其父节点（即分叉点），评估引入工具后是否比纯自然语言推理更好。\n*   **逻辑闭环：** 只有当工具调用确实带来了相对于纯文本推理的增量收益时，该子轨迹才会获得高优势值。这有效避免了模型为了刷分而进行无意义的工具调用。\n\n### 5. 最终逻辑闭环\n**总结：** 作者通过**“熵驱动的分叉”**解决了“在哪里用工具”的探索问题，通过**“树结构的优势估计”**解决了“工具是否有效”的归因问题。这套DART框架在不依赖任何人工标注数据的情况下，成功激活了模型潜藏的工具使用能力，并将其有机地编织进长思维链中，实现了1+1>2的效果。"
                },
                {
                    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
                    "arxiv_id": "2601.08225",
                    "authors": "Jungho Cho, Minbyul Jeong, Sungrae Park",
                    "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
                    "category": "cs.CL",
                    "filter_reason": "论文主要研究利用大型推理模型（LRM）作为自主智能体进行多轮工具使用的数据生成。它涉及工具使用、多轮对话以及人机协作模拟，属于单智能体（工具使用）的研究范围，且不属于排除的纯应用、纯推理或基础设施优化类别。",
                    "summary2": "本文旨在解决现有工具使用数据集缺乏真实多轮交互及工具集静态的问题。针对大规模、开放域的人机协作场景，我们提出了一种以用户为导向的模拟范式，通过解耦任务生成与专用用户模拟器来模拟增量请求和反馈，并集成了可执行SQL工具。我们在BFCL和$\\tau^2$基准上通过多轮对话性能及一致性指标验证了其有效性。",
                    "summary_translation": "近期向大型推理模型作为自主智能体的范式转变，加剧了对复杂的多轮工具使用能力的需求。然而，现有的数据集和数据生成方法受限于静态、预定义的工具集，无法扩展以适应开放式人机协作的复杂性。为了解决这一问题，我们最初开发了一个大规模自动化面向任务的多轮对话生成框架，利用基于LRM的模拟器动态生成高价值、特定领域的工具来解决指定任务。然而，我们观察到，纯面向任务的设计往往导致“仅解决问题”的轨迹，即智能体以最少的交互完成目标，无法生成现实场景中常见的高轮次对话。为了弥合这一差距，我们转向面向用户的模拟范式。通过将任务生成与专用用户模拟器解耦——该模拟器模拟人类行为规则，如渐进式请求提出和逐轮反馈——我们促进了更真实、更延展的多轮对话，反映了现实世界问题解决的迭代性质。我们的生成流水线作为一个通用的、即插即用的模块运行，能够从任何状态启动生成，确保在生成延展的工具使用数据方面具有高可扩展性。此外，通过在单一轨迹内促进多个任务的完成，它产生了一个高密度数据集，反映了现实世界人机交互的多方面需求。",
                    "inspiration_trace": "基于论文《User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到最终解决方案的思考演进过程。\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**——从“静态工具”到“动态智能体”的范式鸿沟**\n\n1.  **背景洞察**：作者首先观察到大语言模型（LLM）正在向大型推理模型（LRM）演进，其核心形态从单纯的文本生成转变为具备自主规划、决策和工具使用能力的“智能体”。\n2.  **瓶颈识别**：尽管模型能力在提升，但训练数据成为制约因素。现有的数据集大多基于**静态、预定义的工具集**（Static Toolsets），且交互模式多为“单次请求-单次响应”。\n3.  **核心矛盾**：真实的人类协作是开放、动态且充满噪声的（如反复澄清、需求变更），而现有数据无法捕捉这种**多轮迭代**的复杂性。这导致模型在处理长上下文和复杂任务时泛化能力不足。\n\n### 第二阶段：初步尝试与“效率陷阱”\n**——任务导向生成的局限性**\n\n1.  **初始假设**：为了解决数据稀缺问题，作者首先尝试构建一个自动化的**任务导向**生成框架。假设只要给模拟器提供复杂的任务和动态生成的工具，就能产出高质量的多轮对话数据。\n2.  **实施逻辑**：利用LRM模拟器动态生成领域特定的工具、数据库模式及对应的评估标准，试图通过“任务-响应-验证”的闭环来规模化生产数据。\n3.  **关键发现（失败点）**：作者发现这种“任务导向”的设计陷入了一个**“效率陷阱”**。模拟器作为一个完美的任务解决者，倾向于以最少的轮次、最高效的路径直接完成任务。\n4.  **反思**：这种“纯粹的任务解决”轨迹虽然高效，但缺乏真实人类交互中的“摩擦力”——即澄清、增量请求和反馈循环。数据量上去了，但交互的真实性和复杂度（Turn Count）却下来了。\n\n### 第三阶段：范式转移与核心假设重构\n**——从“任务导向”转向“用户导向”**\n\n1.  **思维转折**：作者意识到，真实的多轮对话并非由“任务的难度”决定，而是由“用户的行为模式”决定。真实用户往往不会一次性抛出完美指令，而是碎片化地提出需求。\n2.  **新假设**：要生成真实的多轮对话，不能只模拟“任务”，必须模拟“用户”。需要将**目标**与**交互**解耦。\n3.  **策略调整**：提出**用户导向**的模拟范式。核心不再是“如何最快完成任务”，而是“如何模拟人类的行为规则”。\n\n### 第四阶段：方法论构建与机制设计\n**——通过行为规则注入实现“高密度”交互**\n\n1.  **用户模拟器设计**：作者设计了一个专门的用户模拟器，并赋予其特定的**行为规则**：\n    *   **碎片化请求**：每次只要求完成一两个子任务，迫使Agent进行多轮交互。\n    *   **逐步反馈**：根据Agent的输出提供反馈或提出后续问题，直到目标达成。\n2.  **高密度轨迹生成**：为了进一步模拟真实场景，作者允许在单次对话线程中完成**多个任务**。这种“高密度”设计反映了真实会话中用户意图的多样性和连续性（如先查询、再更新、最后总结）。\n3.  **模块化架构**：为了解决扩展性问题，作者将生成管线设计为**即插即用**的模块。这意味着生成可以从任意状态开始，无论是从零开始还是注入到现有对话中，极大地增强了数据增强的灵活性。\n\n### 第五阶段：落地与验证\n**——从“模拟执行”到“真实执行”的最终闭环**\n\n1.  **深化思考**：虽然用户导向解决了交互模式的问题，但工具输出的真实性仍存疑。纯模拟的工具输出可能存在幻觉，导致训练数据不真实。\n2.  **最终方案**：引入**SQL驱动的可执行环境**。不再依赖模型模拟工具输出，而是基于真实的数据库Schema生成SQL工具，并在真实数据库引擎中执行。\n3.  **逻辑闭环**：通过“执行级监督”，确保了工具输出的可验证性和事实准确性。这不仅解决了幻觉问题，还让模型学会了在状态持久化的环境中进行推理。\n\n---\n\n**总结：**\n作者的思考路径是一个典型的**“观察-试错-重构-落地”**过程：\n从发现静态数据无法满足动态智能体需求出发，尝试通过任务导向自动化生成数据，却因模型过于高效而陷入“交互匮乏”的困境；进而通过引入“用户模拟”和行为规则，强制拉长交互链条，模拟真实的人类协作；最后通过引入真实数据库执行环境，确保了数据的真实性与可验证性。这一逻辑链条完整地解释了为何“用户导向”与“工具执行”是构建高质量多轮工具使用数据的关键。"
                },
                {
                    "title": "SwiftMem: Fast Agentic Memory via Query-aware Indexing",
                    "arxiv_id": "2601.08160",
                    "authors": "Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan",
                    "summary": "Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了SwiftMem，一种专门针对LLM智能体的记忆系统。它直接解决了研究范围中“单智能体”下的“记忆”问题，通过改进索引机制来优化智能体的信息检索能力，属于智能体核心组件的研究，而非单纯的通用基础设施优化。",
                    "summary2": "本文旨在解决现有 Agentic Memory 系统因全量检索导致的延迟瓶颈问题。针对长对话场景，我们提出了一种名为 SwiftMem 的查询感知记忆系统，通过时间索引和语义 DAG-Tag 索引实现亚线性检索，并引入嵌入-标签联合整合机制优化存储。我们在 LoCoMo 和 LongMemEval 数据集上通过搜索延迟、LLM Score 和 BLEU-1 等指标验证了其有效性，实现了比 SOTA 快 47 倍的搜索速度且保持竞争性精度。",
                    "summary_translation": "智能体记忆系统对于使 LLM agents (大语言模型智能体) 能够维持长期上下文并高效检索相关信息变得至关重要。然而，现有的 memory frameworks (记忆框架) 存在一个根本性局限：无论 query characteristics (查询特征) 如何，它们都在整个 storage layer (存储层) 上执行 exhaustive retrieval (穷举检索)。这种 brute-force approach (暴力方法) 随着 memory (记忆) 的增长造成了严重的 latency bottlenecks (延迟瓶颈)，从而阻碍了 real-time agent interactions (实时智能体交互)。我们提出了 SwiftMem，这是一个 query-aware (查询感知) 的智能体记忆系统，它通过在 temporal and semantic dimensions (时间和语义维度) 上建立 specialized indexing (专用索引) 来实现 sub-linear retrieval (亚线性检索)。我们的 temporal index (时间索引) 支持用于 time-sensitive retrieval (时间敏感检索) 的 logarithmic-time (对数时间) range queries (范围查询)，而 semantic DAG-Tag index (语义 DAG-Tag 索引) 则通过 hierarchical tag structures (分层标签结构) 将查询映射到相关主题。为了解决 growth (增长) 过程中的 memory fragmentation (记忆碎片化) 问题，我们引入了一种 embedding-tag co-consolidation mechanism (嵌入-标签联合整合机制)，该机制基于 semantic clusters (语义簇) 重组 storage (存储) 以提高 cache locality (缓存局部性)。在 LoCoMo 和 LongMemEval benchmarks (基准测试) 上的实验表明，SwiftMem 相比 state-of-the-art baselines (最先进的基线) 实现了 47$\\times$ 更快的搜索速度，同时保持了 competitive accuracy (有竞争力的准确性)，从而使得 memory-augmented LLM agents (记忆增强型 LLM 智能体) 的 practical deployment (实际部署) 成为可能。",
                    "inspiration_trace": "基于论文《SwiftMem: Fast Agentic Memory via Query-aware Indexing》的内容，以下是对作者核心方法论产出过程的系统性逻辑推演：\n\n### 1. 宏观痛点：暴力检索的线性诅咒\n**思考起点：** 作者首先审视了当前 LLM Agent 记忆系统的核心瓶颈。\n*   **现状观察：** 现有的记忆框架（如 Mem0, Zep 等）虽然能存储和检索信息，但它们在处理查询时，无一例外地采用了“暴力检索”策略。\n*   **问题本质：** 无论查询的具体内容是什么，系统都会遍历整个存储层进行搜索。这种 $O(N_{mem})$ 的线性复杂度意味着，随着对话历史的增长，检索延迟会不可控地增加。\n*   **结论：** 这种“查询无关”的架构设计，是阻碍 Agent 在长周期、实时交互场景下落地的根本原因。\n\n### 2. 关键洞察：查询的“局部性”特征\n**思考转折：** 既然全量扫描不可行，作者开始思考：是否真的需要扫描所有记忆？\n*   **反向观察：** 作者分析了人类对话中的查询行为，发现绝大多数查询并非随机分布，而是具有强烈的“局部性”特征：\n    1.  **时间局部性：** 很多查询显式或隐式地与时间相关（如“上周讨论了什么”）。相关的记忆往往聚集在特定的时间段，而非均匀分布在整个历史中。\n    2.  **语义局部性：** 查询通常针对特定的主题或话题（如“关于 Python 的项目”）。只有语义相关的记忆子集是重要的，其他大部分记忆都是噪音。\n*   **假设提出：** 如果能预先识别查询的这些特征，就可以将搜索范围从“全量空间”缩小到“相关子空间”，从而实现亚线性检索。\n\n### 3. 核心假设：从“存储驱动”转向“查询感知”\n**方法论确立：** 基于上述洞察，作者确立了 SwiftMem 的设计哲学——**“先分析查什么，再只去必要的地方找”**。\n*   **逻辑推演：** 传统的记忆系统是“存储驱动”的（有什么存什么，查时全翻）；SwiftMem 需要转变为“查询感知”的（根据 Query 的特征，动态决定检索路径）。\n*   **架构构想：** 需要构建一个多维度的索引系统，专门针对时间和语义这两个维度进行优化，以规避暴力扫描。\n\n### 4. 维度一：时间维度的索引优化\n**针对时间局部性的解决方案：**\n*   **思考：** 对于包含明确时间线索的查询，传统的 SQL 过滤或向量检索效率低下。\n*   **策略：** 利用数据的自然顺序。既然记忆是按时间生成的，为什么不直接利用排序结构？\n*   **方法形成：** 构建基于二分查找的时间索引。通过维护用户特定的排序时间线，将时间范围查询的复杂度从 $O(N)$ 降低到 $O(\\log N)$。这直接解决了显式时间查询的效率问题。\n\n### 5. 维度二：语义维度的结构化索引\n**针对语义局部性的解决方案：**\n*   **思考：** 传统的向量检索虽然准确，但本质上仍需在高维空间中进行大量计算；简单的关键词标签又缺乏语义层次。\n*   **策略：** 引入层次化的标签结构。利用 LLM 的理解能力，将非结构化的对话内容转化为结构化的、具有层级关系的标签。\n*   **方法形成：** 提出 **DAG-Tag（有向无环图标签）索引**。\n    *   **生成：** 让 LLM 提取标签并识别父子关系（如“编程” -> “Python”）。\n    *   **路由：** 当查询到来时，先映射到相关标签，再沿着 DAG 结构进行层级扩展。\n    *   **效果：** 这样只需检索与特定标签簇相关的记忆，将搜索空间限制在 $k \\ll N_{mem}$ 的范围内。\n\n### 6. 补充机制：解决存储碎片化\n**针对系统长期演进的思考：**\n*   **思考：** 随着对话不断进行，即使有索引，物理存储层面的数据碎片化也会导致缓存命中率下降，影响实际 I/O 性能。\n*   **策略：** 让物理存储布局适应语义结构。\n*   **方法形成：** 提出 **Embedding-Tag Co-consolidation（嵌入-标签联合整合）机制**。定期根据语义标签的聚类关系，重新组织物理存储位置，将语义相关的向量在物理上相邻存放。这提高了缓存局部性，进一步加速了检索过程。\n\n### 7. 逻辑闭环与验证\n**最终产出：**\n*   **系统整合：** 将时间索引、语义 DAG 索引和向量检索整合成一个三层架构。查询预处理模块决定走哪条路径，从而实现“只获取相关记忆”。\n*   **预期结果：** 这种设计在理论上将检索复杂度从线性降低到了亚线性级别。\n*   **实验验证：** 通过 LoCoMo 和 LongMemEval 基准测试，证实了该方法在保持竞争性准确率的同时，实现了 47 倍的搜索加速，验证了“查询感知索引”解决长时记忆效率瓶颈的有效性。"
                },
                {
                    "title": "WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents",
                    "arxiv_id": "2601.08158",
                    "authors": "Yuqing Zhou, Zhuoer Wang, Jie Yuan, Hong Wang, Samson Koelle, Ziwei Zhu, Wei Niu",
                    "summary": "Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了一个用于实现“自我演化的对话服务智能体”的框架WISE-Flow。它专注于通过将历史交互转化为工作流来使智能体能够自我完善（符合“自我演化”标准），并涉及执行轨迹对齐和可行性推理（符合“规划”和“工具使用”标准）。该研究属于LLM智能体的核心方法论研究，不属于排除项。",
                    "summary2": "本文旨在实现用户服务环境中的自进化对话智能体，解决其易错及难以从历史经验中学习的问题。针对历史服务交互轨迹，我们提出了一种名为WISE-Flow的以工作流为中心的框架，通过离线归纳带有先决条件增强动作块的结构化工作流，并在在线阶段进行进度对齐与可行性检查。在ToolSandbox和$\\tau^2$-bench上，通过$F_\\beta$分数和成功率等指标验证了其有效性，显著提升了智能体的执行可靠性和任务完成度。",
                    "summary_translation": "基于 Large language model (LLM) (大语言模型) 的 agents (智能体) 广泛部署在面向用户的服务中，但在新任务中仍然容易出错，倾向于重复相同的 failure patterns (失败模式)，并显示出显著的 run-to-run variability (运行间变异性)。通过 environment-specific training (特定环境训练) 或 manual patching (手动修补) 来修复故障成本高昂且难以扩展。为了在面向用户的服务环境中实现 self-evolving agents (自进化智能体)，我们提出了 WISE-Flow，这是一个 workflow-centric framework (以工作流为中心的框架)，通过归纳包含 prerequisite-augmented action blocks (先决条件增强的动作块) 的工作流，将 historical service interactions (历史服务交互) 转化为 reusable procedural experience (可重用的程序化经验)。在部署阶段，WISE-Flow 将 agents (智能体) 的 execution trajectory (执行轨迹) 与检索到的工作流对齐，并执行 prerequisite-aware feasibility reasoning (先决条件感知的可行性推理)，以实现 state-grounded next actions (基于状态的下一步动作)。在 ToolSandbox 和 $τ^2$-bench 上的实验表明，该方法在 base models (基础模型) 上均带来了一致的改进。",
                    "inspiration_trace": "基于论文《WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents》的内容，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察：智能体在落地场景中的“可靠性危机”\n**起点：** 作者首先观察到一个普遍的行业痛点——尽管基于LLM的智能体在用户服务中被广泛部署，但它们在处理新任务时极易出错，且错误模式会重复出现，运行结果波动大。\n**思考：** 传统的通过特定环境训练或人工修补来修复错误的方法成本高昂且难以扩展。因此，必须走向**“自我进化”**，即智能体必须能够从过去的交互经验中学习，以实现性能的持续提升。\n\n### 2. 问题诊断：现有记忆机制的“检索-行动”鸿沟\n**深入分析：** 作者审视了现有的“记忆增强”和“经验复用”方法，发现它们存在两极分化的缺陷：\n*   **抽象记忆（如反思/总结）：** 虽然提供了高层面的经验教训，但过于自由和抽象，难以直接转化为具体的、可执行的操作步骤，尤其是在任务初期。\n*   **细粒度记忆（如交互轨迹）：** 虽然记录了具体的工具调用，但往往碎片化，缺乏程序性的结构（如明确的任务级工作流、前置条件和恢复路径）。\n*   **蒸馏方法（如指南）：** 即使生成了工作流，往往也过于粗糙，无法与智能体当前的实时进度对齐，也无法验证下一步动作在当前工具状态下的可行性。\n\n**核心假设：** 真正的瓶颈不在于“存储”了多少经验，而在于如何将检索到的经验转化为**当前状态下的可行行动**。作者将其定义为**“检索到行动的鸿沟”**。\n\n### 3. 核心洞察：程序性知识需要“结构化”与“状态感知”\n**逻辑推演：** 要填补上述鸿沟，经验必须具备两个关键属性：\n1.  **程序性：** 不仅仅是孤立的步骤，而是包含顺序和里程碑的完整流程。\n2.  **状态感知：** 必须明确指出执行某个动作所需的**前置条件**。\n\n**思考：** 如果能将历史交互日志转化为包含“前置条件增强的动作块”的结构化工作流，智能体就能在执行时进行两步关键操作：一是**进度对齐**（我现在在流程的哪一步？），二是**可行性推理**（当前状态是否满足下一步的前置条件？）。\n\n### 4. 方法论构建：从原始日志到结构化经验的演进\n基于上述洞察，作者构建了WISE-Flow框架，其逻辑演进分为四个层次：\n\n#### A. 数据源的升维：从“对话”到“全信道日志”\n*   **思考：** 仅靠对话文本无法推断出复杂的程序逻辑，因为很多失败（如顺序错误、权限不足）只能通过环境反馈（如报错信息、API返回码）揭示。\n*   **决策：** 必须记录完整的服务日志，包括工具轨迹和环境反馈，这是提取可靠流程的基础。\n\n#### B. 离线归纳：从“轨迹”到“对比性工作流”\n*   **思考：** 原始轨迹充满噪音（试错、绕路）。直接总结容易产生幻觉或遗漏关键约束。\n*   **决策：** 采用**对比学习**的思路。将“干净的成功”、“恢复后的成功”和“失败”轨迹进行配对对比。通过对比，LLM能识别出导致成功与失败的关键差异，从而将隐含的约束显式化为工作流中的顺序和前置条件。\n*   **优化：** 引入多轮验证机制（分析-草拟-反思），确保生成的工作流忠实于原始轨迹，而非模型凭空捏造。\n\n#### C. 表达形式创新：前置条件增强的动作块\n*   **思考：** 传统的工作流描述只是步骤列表，无法指导智能体在特定状态下该做什么。\n*   **决策：** 设计一种新的结构化表示。工作流不仅包含任务级的主干，每个动作块还显式标注了**前置条件**。这些条件充当“执行守卫”，防止智能体在状态不满足时盲目行动。\n\n#### D. 在线控制：从“检索”到“状态 grounded 的引导”\n*   **思考：** 检索到了工作流还不够，智能体需要知道如何应用它。\n*   **决策：** 在线阶段不仅仅是检索，而是执行“引导”。\n    1.  **进度对齐：** 将当前最后一次成功的工具调用与工作流中的步骤匹配，确定当前阶段。\n    2.  **可行性检查：** 检查候选下一步动作的前置条件是否已被当前历史轨迹满足。\n    3.  **上下文注入：** 将上述分析结果作为结构化上下文输入给模型，直接指导其生成下一步。\n\n### 5. 逻辑闭环与验证\n**最终思考：** 这种方法是否真的有效？作者意识到，在用户服务场景中，**过程的可靠性**（少犯错）与**最终的成功**同样重要。\n**验证：** 引入新的评估指标 $F_\\beta$，联合衡量里程碑覆盖率和错误控制。实验证明，WISE-Flow不仅提高了成功率，更重要的是显著减少了试错，提升了“一次性成功”的比例，从而验证了“结构化经验 + 前置条件感知”这一核心逻辑的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**解决智能体落地的不稳定性**出发，通过批判现有记忆方法的局限性，提出了**“检索-行动鸿沟”**这一核心问题。进而，通过引入**结构化工作流**和**前置条件增强**的概念，将模糊的历史经验转化为精确的、状态感知的执行指南，最终实现了一个能够自我进化且高度可靠的智能体框架。"
                },
                {
                    "title": "Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning",
                    "arxiv_id": "2601.08105",
                    "authors": "Fabian Spaeh, Tianyi Chen, Chen-Hao Chiang, Bin Shen",
                    "summary": "Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction. In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究 \"agentic RAG\"（智能体RAG）和 \"tool-calling agents\"（工具调用智能体），旨在解决智能体在处理超出范围查询时的问题，并提出通过动态上下文学习来增强智能体的交互能力。这属于单智能体中工具使用和交互能力的范畴，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决Agentic RAG中用户查询无法回答的问题，通过建议可回答的相似查询来增强用户交互。针对多步骤工作流和工具调用的场景，我们提出了一种基于Robust Dynamic Few-Shot Learning的方法，利用模板化和动态检索示例来生成建议。我们在三个真实世界数据集上通过可回答性和语义相似性指标验证了其有效性，结果优于基线方法。",
                    "summary_translation": "结合工具调用代理的检索增强生成（agentic RAG，代理式RAG）在理解、处理和响应用户查询方面已变得日益强大。然而，其依据知识的范围是有限的，提出超出此范围的问题可能会导致幻觉等问题。尽管护栏框架旨在阻断超出范围的问题（Rodriguez et al., 2024），但尚未有研究探讨通过建议可回答的查询来完成用户交互的问题。在本文中，我们开启了针对 agentic RAG 的查询建议的研究。我们考虑了用户问题无法得到回答的场景，此时建议的查询应当与原问题相似，以辅助用户交互。对于工具调用大语言模型而言，此类场景十分常见，因为很难向用户传达工具或底层数据集的限制，而添加查询建议能够增强与 RAG 代理的交互。与搜索引擎等传统的查询推荐设置相比，确保建议的查询是可回答的（answerable）是一项重大挑战，这是因为 RAG 的多步工作流要求对 RAG 整体有细致入微的理解，而执行的大语言模型恰恰缺乏这种理解。因此，我们引入了鲁棒的动态少样本学习，该方法从相关工作流中检索示例。我们表明，该系统可以进行自学习（例如基于先前的用户查询），因此在实践中易于应用。我们在三个基准数据集上评估了我们的方法，这些数据集基于两个从真实用户查询中收集的无标签问题数据集构建。真实世界数据集上的实验证实，我们的方法能够生成更具相关性且可回答的建议，优于少样本和仅检索基线，从而实现了与 agentic RAG 更安全、更有效的用户交互。",
                    "inspiration_trace": "基于论文《Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法产出的思考过程：\n\n### 第一阶段：问题定义与视角转换（从“防御”到“引导”）\n\n1.  **宏观观察**：\n    *   Agentic RAG（基于工具调用的检索增强生成）虽然强大，但用户往往不了解底层工具的能力边界或数据的局限性。\n    *   **现状痛点**：当用户提出超出范围的问题时，现有的解决方案（如 Guardrails）通常选择“拦截”或直接报错。这种“防御性”策略虽然安全，但中断了用户交互，体验较差。\n\n2.  **核心问题提出**：\n    *   **思考转折**：与其告诉用户“不能做什么”，不如告诉用户“能做什么”。\n    *   **新任务定义**：作者将研究焦点从传统的“查询拦截”转向了**“查询建议”**。即：当用户提问不可回答时，系统应主动生成一个既语义相似，又能被 RAG 系统成功执行的替代查询。\n\n### 第二阶段：挑战分析与本质洞察（从“文本匹配”到“工作流理解”）\n\n1.  **识别特异性挑战**：\n    *   **对比传统搜索**：传统的查询建议（如搜索引擎）主要基于文本共现或点击日志，不涉及复杂的执行逻辑。\n    *   **Agentic RAG 的复杂性**：这里的“可回答性”不仅取决于文本，更取决于**多步工作流**是否可行。LLM 本身并不预先知道哪些工具组合能成功，哪些数据存在。\n\n2.  **关键假设与抽象**：\n    *   **思考**：判断一个查询能否回答，核心不在于具体的数值（如“2024年”），而在于其背后的**逻辑结构**（如“查询某时间段的销售额”）。\n    *   **方法论雏形**：作者提出将“查询”解耦为**“工作流模板”**与“具体数值”。只要模板对应的工具调用逻辑是通的，填入合理的数值即可变为可回答的查询。\n\n### 第三阶段：方法构建与逻辑演进（从“静态提示”到“动态上下文学习”）\n\n1.  **初步尝试与局限（静态 Few-Shot）**：\n    *   **思路**：直接用 LLM，通过 Prompt 指令加上几个静态的示例来教模型如何改写查询。\n    *   **发现的问题**：Agentic RAG 的工作流空间巨大，固定的几个示例无法覆盖所有复杂的工具组合和数据边界，导致生成的建议依然经常不可执行。\n\n2.  **进阶思路（动态检索）**：\n    *   **思考**：既然静态示例不够，那就根据当前用户的查询，动态地从历史数据中找最相似的案例。\n    *   **技术实现**：利用**模板化**处理历史查询（掩盖具体数值），计算模板间的语义相似度，从而检索出相关的“可回答”与“不可回答”的工作流示例。\n\n3.  **鲁棒性优化（抗噪机制）**：\n    *   **潜在风险**：历史日志中可能包含 RAG 执行失败或产生幻觉的错误案例，如果直接拿来作为正例，会误导模型。\n    *   **解决方案**：作者引入了**鲁棒检索机制**。在检索到的相似示例中，通过聚类或局部投票的方式，只有当大多数相似案例都标记为“可回答”时，才认为该工作流是可信的。这相当于在 In-Context Learning 中加入了一层“事实核查”。\n\n### 第四阶段：落地闭环（从“人工标注”到“自学习”）\n\n1.  **数据获取难题**：\n    *   **思考**：要训练上述系统，需要大量标注了“是否可回答”的查询数据。人工标注成本极高且难以覆盖所有工具场景。\n\n2.  **自学习机制**：\n    *   **洞察**：RAG 系统自己执行过查询，它最清楚自己有没有成功。\n    *   **最终闭环**：利用 RAG Agent 自身的执行日志（工具调用链、最终响应），让 LLM 回溯判断该查询是否成功。这样，系统可以在实际运行中不断积累新的、已标注的模板数据，实现**无监督的自我进化**。\n\n### 总结：逻辑链条全景\n\n1.  **观察**：Agentic RAG 常因用户无知而失败，现有方案只会阻断交互。\n2.  **目标**：将失败转化为引导，生成“语义相似且可执行”的建议查询。\n3.  **难点**：可执行性取决于复杂的多步工作流，LLM 难以通过 Prompt 直接掌握。\n4.  **突破**：忽略具体数值，聚焦**工作流模板**；通过**动态检索**相关历史模板作为上下文示例。\n5.  **优化**：引入**鲁棒投票**过滤历史日志中的噪声（幻觉）。\n6.  **落地**：利用**执行日志自标注**，解决数据冷启动问题，形成自我进化的闭环。"
                },
                {
                    "title": "LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback",
                    "arxiv_id": "2601.08003",
                    "authors": "Weiyue Li, Mingxiao Song, Zhenda Shen, Dachuan Zhao, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang",
                    "summary": "Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个名为 LLM Review 的多智能体框架，通过模拟盲审同行评审机制，让智能体之间交换反馈并独立修订。这属于多智能体协作与通信的研究范畴，同时也涉及通过反馈进行自我完善，符合筛选条件。",
                    "summary2": "本文旨在解决LLM在创意生成中因多智能体交互导致内容同质化的问题。针对科幻写作场景，我们提出了一种名为LLM Review的框架，通过Blind Peer Review机制限制信息流，使智能体交换反馈但独立修订，并在SciFi-100数据集上通过LLM-as-a-judge评分、人工标注及规则指标验证了其有效性。",
                    "summary_translation": "大语言模型在创意生成方面往往面临挑战。虽然通过交互提升推理能力的多智能体框架能够改善某些任务，但它们可能会因导致内容同质化而反常地阻碍创造力。我们提出了 LLM Review，这是一种受同行评审启发的框架，实现了盲审机制：智能体在交换针对性反馈的同时独立进行修订，从而保留了发散的创意轨迹。为了实现严格的评估，我们提出了 SciFi-100，这是一个科幻写作数据集，并配备了一个统一的评估框架，该框架结合了 LLM-as-a-judge（大模型作为裁判）评分、人工标注以及基于规则的新颖性指标。实验结果表明，LLM Review 始终优于多智能体基线模型；此外，应用该框架的小型模型能够超越大型单智能体模型，这表明交互结构或许可以替代模型规模。",
                    "inspiration_trace": "基于论文《LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n### 1. 宏观观察：LLM 的创造力困境与多智能体的悖论\n*   **初始问题**：虽然大语言模型（LLMs）在通用任务上表现优异，但在**创造性生成**（如科幻写作）方面往往力不从心，倾向于重复熟悉模式而非产生真正的新颖概念。\n*   **直觉尝试**：人类的创造力往往源于社会互动（讨论、批评）。因此，研究界倾向于引入**多智能体框架**（如辩论、讨论），试图通过模拟人类协作来提升模型表现。\n*   **关键反直觉发现**：作者观察到，现有的多智能体框架虽然提升了推理能力，却**意外地抑制了创造力**。这些框架导致“内容同质化”，即不同智能体生成的结果越来越相似，失去了发散性。\n\n### 2. 深度归因：信息过载导致的“收敛陷阱”\n*   **理论溯源**：作者回顾了人类群体心理学研究（如头脑风暴实验），发现**互动过多**反而会导致“生产阻滞”和“趋同倾向”。群体互动往往比独立工作产生更少、更缺乏原创性的想法。\n*   **机制分析**：现有的多智能体框架（如 LLM Debate 或 Discussion）通常隐含一个假设——“交互越多越好”。在这种机制下，智能体反复暴露在彼此的输出中，导致它们在迭代过程中相互模仿、对齐，从而**过早收敛**于某种“平均”或“安全”的语义空间，扼杀了探索不同轨迹的可能性。\n*   **核心洞察**：创造力需要的是**发散**，而不是收敛。问题的根源不在于“是否有交互”，而在于**信息流动的结构**。\n\n### 3. 假设提出：通过“信息流约束”来保护发散性\n*   **新假设**：要提升创造力，不应最大化信息流动，而应**约束信息流动**。我们需要一种机制，既能利用外部反馈来修正和完善，又能防止智能体之间的相互模仿。\n*   **设计目标**：将“反馈”与“观察”解耦。智能体应该获得他人的批评意见，但**不应看到**他人根据这些意见修改后的结果。这种信息不对称是保持独立创意轨迹的关键。\n\n### 4. 方法论构建：引入“盲审同行评议”机制\n*   **灵感来源**：学术界为了确保研究的独立性和质量，发明了**双盲同行评议**。审稿人只看初稿并提意见，作者在修改时看不到其他人的修改稿。\n*   **LLM Review 框架设计**：\n    1.  **独立创作**：所有智能体基于人设独立生成初稿。\n    2.  **盲审反馈**：智能体互为审稿人，针对初稿提供具体的批评意见（如世界观构建、逻辑漏洞等）。\n    3.  **独立修订**：智能体仅根据收到的反馈和自己的初稿进行修改，**完全屏蔽**其他智能体的修订版本。\n*   **逻辑闭环**：这种结构让智能体在享受“外部视角”红利的同时，强制其保持独立的演化路径，从而避免了多智能体系统常见的“回音室效应”。\n\n### 5. 验证策略：构建多维度的创造力评估体系\n*   **数据需求**：为了验证该框架确实提升了创造力，作者需要一个高质量的基准。因此构建了 **SciFi-100** 数据集，涵盖科幻写作的十个核心维度。\n*   **评估难题**：创造力是主观的。为了严谨，作者提出了一套**混合评估框架**：\n    *   **LLM-as-a-Judge**：利用 GPT-4o 模拟专家评分，评估科幻概念、逻辑、深度等质量维度。\n    *   **规则指标**：计算与经典科幻语料库的词汇和语义距离，客观量化“新颖性”。\n*   **预期验证**：如果假设成立，LLM Review 应在“新颖性”指标上显著高于其他多智能体基线，且在质量评分上保持竞争力。\n\n### 6. 最终结论：结构优于规模\n*   **实验发现**：LLM Review 确实优于现有的辩论和讨论框架。更重要的是，**小模型 + LLM Review 框架** 的表现可以超越 **大模型 + 单智能体**。\n*   **理论升华**：这证明了在创造性任务中，**交互结构的设计**可以作为一种比单纯扩大模型规模更高效的算力杠杆。通过控制“谁在什么时候看到什么”，我们可以引导模型走出平庸，走向真正的创新。"
                },
                {
                    "title": "APEX-SWE",
                    "arxiv_id": "2601.08806",
                    "authors": "Abhi Kottamasu, Akul Datta, Aakash Barthwal, Chirag Mahapatra, Ajay Arun, Adarsh Hiremath, Brendan Foody, Bertie Vidgen",
                    "summary": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个评估软件工程智能体的基准，涉及构建端到端系统和调试生产故障等任务，明确强调了“智能体”在解决不确定性和执行复杂任务中的作用，符合单智能体的规划和工具使用范畴。",
                    "summary2": "本文旨在评估前沿AI模型执行具有经济价值的软件工程工作的能力。针对现实世界中跨平台集成和生产故障调试的场景，我们提出了一种包含Integration和Observability任务的APEX-SWE基准，并在APEX-SWE数据集上通过Pass@1等指标验证了其有效性。",
                    "summary_translation": "我们介绍了软件工程人工智能生产力指数（AI Productivity Index for Software Engineering, APEX-SWE），这是一个用于评估前沿AI模型是否能够执行具有经济价值的软件工程工作的基准。与现有的专注于范围有限且定义明确的任务的评估不同，APEX-SWE评估了两种反映现实世界软件工程工作的新颖任务类型：(1) 集成任务（Integration tasks, n=100），要求跨越异构云原语、业务应用程序和基础设施即代码服务构建端到端系统；(2) 可观测性任务（Observability tasks, n=100），要求利用遥测信号（telemetry signals，如日志和仪表板）以及非结构化上下文来调试生产故障。我们在APEX-SWE上评估了八个前沿模型。Gemini 3 Pro (Thinking = High) 表现最佳，Pass@1得分为25%。我们的分析表明，优异的表现主要由认知推理驱动，其定义为区分假设和已验证事实的能力，并结合了在行动前解决不确定性的能动性。我们开源了APEX-SWE评估工具和开发集（dev set, n=50）。",
                    "inspiration_trace": "基于论文《APEX-SWE》的内容，以下是对作者产出该核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论形成的思考过程：\n\n### 第一阶段：宏观观察与问题识别\n**（从“高分低能”现象到评估体系的失效）**\n\n1.  **观察现象**：作者首先注意到AI编码模型在传统基准（如HumanEval、SWE-bench）上的分数已趋于饱和（如GPT-4在HumanEval上达到90%），且工业界AI代码生成率极高。\n2.  **发现矛盾**：尽管基准分数飙升，但这些高分并未完全转化为现实世界的生产力。传统基准多聚焦于“编写短函数”或“修补单文件”，这与真实软件工程中复杂的系统集成、基础设施搭建和生产环境调试存在巨大鸿沟。\n3.  **提出核心问题**：现有的评估体系是否已经失效？我们需要一个新的标尺，来衡量AI模型在具有“经济价值”的真实软件工程任务中的表现。\n\n### 第二阶段：聚焦与维度定义\n**（从“泛泛的工程能力”到“具体的两大核心任务”）**\n\n1.  **解构真实工作**：作者进一步分析，真实的软件工程工作流中，最具挑战性和经济价值的环节是什么？\n2.  **提取关键维度**：\n    *   **构建**：不仅仅是写代码，而是跨异构系统的端到端集成。这涉及云服务、业务应用和基础设施的协同。\n    *   **维护**：不仅仅是修Bug，而是在信息不完整（只有日志和报错）的生产环境中进行故障排查。\n3.  **确立任务类型**：基于上述分析，作者锁定了两个核心评估维度——**“集成任务”**和**“可观测性任务”**，以此作为新基准的骨架。\n\n### 第三阶段：理论假设与核心洞察\n**（从“测试代码能力”到“测试认知推理能力”）**\n\n1.  **初步假设**：如果仅仅是测试代码生成能力，现有模型可能表现尚可。但要在上述两个复杂任务中成功，模型需要具备更深层的素质。\n2.  **形成核心假设**：作者提出，决定成败的关键不再是单纯的“编码能力”，而是**“认知推理”**——即区分“假设”与“已验证事实”的能力，以及在行动前消除不确定性的“智能体”能力。\n3.  **预测行为模式**：作者推测，成功的模型会表现出“闭环验证”的行为（探索环境 -> 提取规范 -> 验证结果），而失败的模型则会陷入“开放循环执行”（盲目生成代码而不验证）。\n\n### 第四阶段：方法论设计与环境构建\n**（从“理论构想”到“高保真仿真”）**\n\n1.  **设计评估环境**：为了验证上述假设，必须构建一个高度仿真的生产环境。作者引入了Docker容器化环境，集成了LocalStack（模拟AWS）、真实业务应用（如EspoCRM、Medusa）以及日志监控栈（Grafana/Loki）。\n2.  **制定严格标准**：\n    *   **数据源**：任务来源于真实的GitHub Issue和资深工程师设计的场景，确保复杂性。\n    *   **评估指标**：采用严格的Pass@1（一次通过率）来模拟真实生产中“一次做对”的高成本压力，并引入Rubrics（评分细则）来评估代码的鲁棒性和功能性，而不仅仅是能否运行。\n3.  **工具赋能**：赋予模型通过MCP（Model Context Protocol）直接操作环境和查询日志的工具，使其具备“智能体”的执行能力。\n\n### 第五阶段：实验验证与逻辑闭环\n**（从“数据结果”到“理论升华”）**\n\n1.  **执行实验**：测试了8个前沿模型。结果显示，即使是最好的模型（Gemini 3 Pro）在Pass@1上也仅达到25%，证明了该基准的有效性和挑战性。\n2.  **验证假设**：通过定性分析成功与失败的案例，作者确认了第三阶段的假设：成功案例确实体现了“认知推理”（如先查询环境状态再写代码），而失败案例多源于“幻觉”或缺乏验证。\n3.  **得出结论**：最终逻辑闭环——未来的AI软件工程进步，不应只关注训练模型写出更好的代码，更应关注教会模型模仿严谨的工程过程（即认知推理和闭环验证）。\n\n---\n\n**总结**：\n作者的思考路径是从**评估体系的局限性**出发，通过**解构真实工程价值**定义了新的任务维度，进而提出**认知推理是核心瓶颈**的理论假设，最后通过构建**高保真仿真环境**和严格的实验设计，验证了假设并确立了新的评估范式。"
                },
                {
                    "title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement",
                    "arxiv_id": "2601.08545",
                    "authors": "Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen",
                    "summary": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出的框架包含“迭代检索增强”机制，利用生成代码的评估结果来迭代优化检索方向和修复策略。这种基于反馈的循环改进过程符合“自我演化/自我反思”的智能体特征，且涉及检索工具的使用，不属于纯应用或纯推理。",
                    "summary2": "本文旨在解决编程学习者代码修复中缺乏 Bug 解释的问题。针对编程学习者的错误代码，我们提出了一种名为 LSGen 的框架，利用编辑驱动的检索和迭代检索增强生成修复代码及 Bug 描述。在 LPR-Bench 数据集上，通过 Code Accuracy Rate 和 B-F1 等指标验证了其有效性。",
                    "summary_translation": "随着大语言模型在编程领域的发展，智能编程辅导系统受到了广泛关注。然而，大多数研究集中于修复编程学习者的 buggy code（错误代码），而未能提供错误的根本原因。为填补这一空白，我们引入了一项新任务，即 LPR (Learner-Tailored Program Repair，学习者定制程序修复)。随后，我们提出了一个新颖且有效的框架，**Learner-Tailored Solution Generator** (学习者定制解决方案生成器)，旨在增强程序修复能力，同时为 buggy code（错误代码）提供错误描述。在第一阶段，我们利用 repair solution retrieval framework（修复解决方案检索框架）构建了一个 solution retrieval database（解决方案检索数据库），并采用 edit-driven code retrieval approach（编辑驱动的代码检索方法）来检索有价值的解决方案，从而指导 LLMs（大语言模型）识别并修复 buggy code（错误代码）中的错误。在第二阶段，我们提出了一种 solution-guided program repair method（解决方案引导的程序修复方法），该方法在检索解决方案的指导下修复代码并提供解释。此外，我们还提出了一种 Iterative Retrieval Enhancement method（迭代检索增强方法），利用生成代码的评估结果来迭代优化检索方向，并探索更合适的修复策略，从而提升在实际编程辅导场景中的性能。实验结果表明，我们的方法显著优于一系列基线模型，验证了我们框架针对新提出的 LPR 任务的有效性。",
                    "inspiration_trace": "基于对论文内容的深度分析，以下是对作者产出《Learner-Tailored Program Repair》一文核心思想逻辑链的系统性推演：\n\n### 第一阶段：从“修复”到“教学”的宏观视角转变\n**1. 观察现状与痛点：**\n作者首先观察到，随着大语言模型（LLM）在编程领域的应用，智能辅导系统虽然能自动修复代码，但存在一个巨大的教育盲区：**只给结果，不给原因**。\n*   **现实场景推演：** 想象一个初学者Alice，她的代码有Bug。如果系统直接扔给她一个“官方正确答案”（例如用动态规划写的），而她用的是贪心算法，她根本看不懂，也无法从中学习。\n*   **核心问题定义：** 真正的教学场景需要的是“量身定制”的修复——即保留学生原有的算法逻辑和代码风格，仅修复错误，并**解释为什么错**。由此，作者提出了新任务 **LPR (Learner-Tailored Program Repair)**。\n\n### 第二阶段：面对“千人千面”代码的挑战\n**2. 深入分析难点：**\n在定义了任务后，作者意识到直接让LLM做这件事很难，原因在于学生代码的**非标准化**：\n*   **风格混乱：** 变量命名不规范、逻辑冗余。\n*   **解法多样：** 同一道题可能有几十种解法，Bug的形态千奇百怪。\n*   **解释困难：** 生成“Bug描述”比生成“正确代码”更难，且难以量化评估。\n\n**3. 提出核心假设：**\n作者假设：**历史总是惊人的相似。** 在编程平台上，过去一定有其他学生犯过类似的错误，并且留下了从“错误代码”到“正确代码”的修复轨迹。如果能找到这些历史案例，就能作为“教材”来指导当前的修复。\n\n### 第三阶段：检索策略的演进——从“形似”到“神似”\n**4. 传统检索的失效：**\n作者首先想到的是检索相似代码。但很快发现一个问题：**算法不同，代码结构完全不同。** 一个用二分查找，一个用动态规划，文本相似度极低，但它们可能犯了同样的逻辑错误（如边界条件判断）。传统的基于代码文本或测试用例的检索方法失效。\n\n**5. 关键创新：Edit-Driven（编辑驱动）检索：**\n作者的思想发生了质的飞跃：**不要检索“相似的代码”，而要检索“相似的修改过程”。**\n*   **逻辑推演：** 代码本身可能差异很大，但“修复的动作”可能是相似的（例如都是将 `l = mid` 改为 `l = mid + 1`）。\n*   **方法论形成：** 将历史数据中的“错误代码”和“正确代码”做差，计算出**编辑向量**。在检索时，计算当前错误代码与历史错误代码的“潜在修复向量”的距离。这样就能找到那些“修复路径”相似的案例，无论原始代码长什么样。\n\n### 第四阶段：生成策略的演进——从“模仿”到“理解”\n**6. 如何让LLM学会解释？**\n仅仅检索到相似的正确代码还不够，因为LLM可能只是照抄，而不知道为什么改。为了生成高质量的“Bug描述”，作者决定利用检索结果作为**思维链**的提示。\n\n**7. Reference-Inspired（参考启发）生成：**\n作者设计了一种双模态的提示策略：\n*   **视觉引导：** 展示 `Diff`（代码差异），让LLM看到具体的改动位置。\n*   **语义引导：** 展示历史案例中对应的**文本Bug描述**。\n*   **逻辑闭环：** 通过“Diff + 文本解释”的组合，强迫LLM模仿这种“先定位、再解释、后修复”的推理过程，从而同时产出修复代码和解释。\n\n### 第五阶段：迭代优化——利用失败作为信号\n**8. 承认一次检索的不完美：**\n作者意识到，第一次检索到的参考方案可能不完全适用，生成的代码可能仍有Bug。在传统方法中，这就失败了。但作者将这种“失败”视为一种**反馈信号**。\n\n**9. 迭代检索增强：**\n*   **逻辑推演：** 如果第一次生成的代码是错的，说明我们尝试的“修复方向”偏了。那么，我们可以分析这个“错误的修复”与“历史正确修复”之间的偏差。\n*   **方法论形成：** 利用第一次生成的错误代码，计算新的编辑向量，重新在数据库中检索。这就形成了一个闭环：**尝试修复 -> 发现失败 -> 根据失败调整检索方向 -> 再次尝试**。这模拟了人类专家调试时的试错过程。\n\n### 第六阶段：评估体系的构建\n**10. 解决“无法评估”的死结：**\n为了验证新任务的效果，必须评估“Bug描述”的质量。人工评估太慢，作者提出利用更强的LLM（如GPT-4）作为裁判，判断生成的描述与真实Bug在逻辑上是否一致，从而实现了自动化的量化评估。\n\n---\n\n**总结：作者的思考路径**\n从**教育场景的真实需求**（不仅要修好，还要解释且保留风格）出发，面对学生代码**高度异构**的挑战，放弃了传统的代码相似检索，转而通过**编辑向量**捕捉修复逻辑的相似性。进而，利用**Diff+解释**的双重提示引导LLM进行推理，最后通过**迭代反馈**机制不断修正检索方向，形成了一套完整的“检索-推理-反馈”闭环方法论。"
                },
                {
                    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
                    "arxiv_id": "2601.08079",
                    "authors": "Hongjin Qian, Zhao Cao, Zheng Liu",
                    "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons. We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation. We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了MemoBrain，一种针对工具增强型智能体的执行记忆模型，旨在解决长视界推理中的记忆管理问题，属于单智能体研究中的记忆与工具使用范畴。",
                    "summary2": "本文旨在解决工具增强型 Agent 在长周期推理中因上下文积累导致的逻辑连贯性受损问题。针对长周期工具交互场景，我们提出了一种名为 MemoBrain 的执行记忆模型，它作为 co-pilot 构建依赖感知的记忆并主动管理工作上下文。我们在 GAIA、Web-Walker 和 BrowseComp-Plus 数据集上通过 Pass@1 指标验证了其有效性。",
                    "summary_translation": "工具增强智能体框架中的复杂推理本质上是长视界的，导致推理轨迹和瞬时工具产物累积，并给大语言模型的有限工作上下文带来压力。如果没有显式记忆机制，这种累积会破坏逻辑连贯性并削弱任务对齐。这将记忆定位为并非辅助性的效率问题，而是在长视界上维持连贯、目标导向推理的核心组件。我们提出了MemoBrain，这是一个用于工具增强智能体的执行记忆模型，它在推理步骤之上构建感知依赖的记忆，捕获显著的中间状态及其逻辑关系。作为推理智能体的副驾驶，MemoBrain在不阻塞执行的情况下组织推理进程，并主动管理工作上下文。具体而言，它剪枝无效步骤，折叠已完成的子轨迹，并在固定上下文预算下保留一个紧凑、高显著性的推理主干。总体而言，这些机制实现了对推理轨迹的显式认知控制，而非被动的上下文累积。我们在具有挑战性的长视界基准测试上评估了MemoBrain，包括GAIA、WebWalker和BrowseComp-Plus，结果表明其相对于强基线有一致的性能提升。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 5,
            "papers": [
                {
                    "title": "DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems",
                    "arxiv_id": "2601.07248",
                    "authors": "Shuyu Zhang, Yujie Liu, Xinru Wang, Cheng Zhang, Yanmin Zhu, Bin Li",
                    "summary": "Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了DarwinTOD框架，明确涉及“自我演化”（终身自我改进、进化计算）和“多智能体”（在线多智能体对话执行与同伴批判），符合研究范围中关于自我演化和多智能体协作的定义。",
                    "summary2": "本文旨在解决任务导向对话系统无法在部署后实现终身自主进化和适应新领域的问题。针对动态环境下的对话交互，我们提出了一种名为DarwinTOD的终身自进化框架，该框架集成了进化计算与LLM驱动的策略优化，通过维护可进化策略库（ESB）及双循环机制（在线多智能体执行与离线结构化进化）实现无人工干预的持续优化。我们在MultiWOZ和SGD数据集上通过Inform、Success、BLEU及Combine指标验证了其有效性。",
                    "summary_translation": "传统的任务型对话系统无法从持续的交互中进行演化，也无法在部署后适应新领域，这是其在现实世界动态环境中的一个关键局限。持续学习方法依赖于基于人工策划数据的阶段性重训练，未能实现自主的终身改进。尽管进化计算和 LLM (Large Language Model, 大语言模型) 驱动的自我改进为对话优化提供了有前景的机制，但它们缺乏一个用于全面、迭代策略优化的统一框架。为了弥合这一差距，我们提出了 DarwinTOD，一个终身自演化对话框架，该框架系统性整合了这两种范式，从而能够在无需特定任务微调的情况下，从零样本基础开始实现持续的策略优化。DarwinTOD 维护一个 Evolvable Strategy Bank (可演化策略库)，并通过双环过程运行：包含同伴评议的在线多智能体对话执行，以及利用累积反馈优化策略库的离线结构化进化操作。这种闭环设计使得无需人工干预即可实现自主的持续改进。大量实验表明，DarwinTOD 优于以往最先进的方法，并在整个演化过程中展现出持续的性能提升。我们的工作为构建具有终身自演化能力的对话系统提供了一个新颖的框架。",
                    "inspiration_trace": "基于对论文《DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems》的深度分析，以下是作者产出该核心方法的逻辑推演过程，还原了从宏观观察到具体方法论的思考链条：\n\n### 第一阶段：宏观问题识别——从“静态系统”到“动态世界”的矛盾\n\n**1. 现实观察：**\n作者首先观察到现实世界是动态变化的。用户的偏好、对话的领域以及任务的目标都在不断演进。然而，现有的任务型对话系统（TOD）在部署后本质上是“静态”的——一旦训练完成，其能力就被冻结，无法从后续的交互中学习或适应新领域。\n\n**2. 核心痛点提炼：**\n这导致了“研究原型”与“可部署系统”之间的巨大鸿沟。学术界通常在静态基准上评估模型，而工业界需要的是一个能在开放、动态环境中长期运行的智能体。因此，核心问题不再是“如何让模型在测试集上表现好”，而是**“如何让系统具备终身自我进化的能力，实现完全自主的持续改进？”**\n\n### 第二阶段：现有范式的批判与局限分析\n\n**1. 审视传统方案：**\n作者逐一分析了现有技术路线，发现它们都无法解决上述核心问题：\n*   **流水线架构：** 虽然模块化，但存在级联错误传播，且难以适应新领域，缺乏灵活性。\n*   **端到端大模型（LLM）：** 虽然泛化能力强，但本质上仍是基于初始指令的静态执行，缺乏从经验中学习的机制。\n*   **持续学习：** 虽然试图增量更新，但严重依赖人工整理的数据和周期性的重训练，无法实现真正的“自主”和“终身”进化。\n\n**2. 寻找突破口：**\n作者意识到，要实现真正的自主进化，必须摆脱对“人工标注数据”和“模型参数微调”的依赖，转而寻找一种能够利用系统自身交互经验进行自我优化的机制。\n\n### 第三阶段：理论融合——进化计算与大模型的互补性思考\n\n**1. 两个孤立的方向：**\n作者注意到了两个有潜力但各自为政的研究方向：\n*   **进化计算：** 擅长基于种群的优化，能通过选择、变异等机制寻找最优解，但缺乏语义理解能力，通常只用于优化孤立的提示词。\n*   **LLM驱动的自我改进：** 擅长推理和反思，能通过多智能体协作解决问题，但往往缺乏结构化的长期策略管理机制，容易陷入单轮优化的局部视角。\n\n**2. 逻辑跃迁（核心假设）：**\n**“如果将LLM作为进化算法的‘大脑’，利用其强大的语义理解和推理能力来驱动对话策略的进化，会发生什么？”**\n作者认为，LLM可以作为智能的“进化算子”，而进化算法提供了结构化的“优化框架”。两者的结合可以解决各自的短板：进化算法提供了终身迭代的框架，LLM提供了语义层面的策略生成与评估能力。\n\n### 第四阶段：方法论构建——从“单点优化”到“种群进化”\n\n**1. 核心概念定义：**\n基于上述假设，作者提出了**“可进化策略库”**的概念。\n*   **思维转变：** 传统的Prompt Engineering是在寻找一个“最好的”提示词。而DarwinTOD转向维护一个“多样化的策略种群”。这些策略在交互中竞争、优胜劣汰。\n\n**2. 闭环机制设计：**\n为了实现终身进化，作者设计了一个**“双循环”架构**，将理论落地：\n*   **在线执行循环：** 模拟真实环境。作者没有使用单一的端到端Agent，而是保留了**多智能体流水线（DST, DP, NLG）**。为什么？因为模块化不仅能防止错误级联，更重要的是，它允许每个模块拥有独立的策略，从而实现更细粒度的进化。\n*   **引入“同伴批判”：** 为了获得比单纯的“任务成功/失败”更密集的反馈信号，作者让智能体之间互相批判。这不仅能实时纠错，还能为离线进化提供高质量的反思数据。\n\n**3. 离线进化循环：**\n这是系统的“大脑”部分。作者设计了四种受进化论启发的操作算子，直接作用于策略库：\n*   **Genesis（创生）：** 针对新领域，利用LLM的零样本能力从无到有生成策略。\n*   **Mutation（变异）：** 针对失败的对话，利用LLM分析失败原因并修改策略。\n*   **Consolidation（整合）：** 利用LLM合并相似的策略，保持种群精简。\n*   **Pruning（剪枝）：** 淘汰低适应度的策略，控制计算成本。\n\n### 第五阶段：鲁棒性思考——应对噪声与不确定性\n\n**1. 潜在风险识别：**\n作者意识到，LLM生成的批判和变异可能包含噪声或偏见。如果系统盲目信任每一次反馈，可能会导致策略退化。\n\n**2. 解决方案设计：**\n为了解决这个问题，作者引入了**“适应度函数”**和**“玻尔兹曼选择”**机制。\n*   **长期统计：** 不依赖单次反馈，而是基于长期的历史表现（正负反馈计数）来计算策略的适应度。\n*   **概率选择：** 即使策略当前适应度低，也有一定概率被选中（探索），防止过早收敛到局部最优。\n*   **逻辑闭环：** 这种设计使得系统具有“抗噪性”，即使偶尔有错误的批判，长期的大数定律和种群选择机制也能过滤掉噪声，确保进化方向是向上的。\n\n### 总结：逻辑演进的全景图\n\n作者的思考路径是从**现实世界的动态需求**出发，批判了现有技术的静态本质，通过**融合进化计算的结构化优势与LLM的语义优势**，创造性地提出了**基于种群策略进化的新范式**。最终，通过**双循环架构**和**抗噪的进化机制**，将这一理论转化为一个无需人工干预、能够终身自我进化的对话系统。"
                },
                {
                    "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
                    "arxiv_id": "2601.07152",
                    "authors": "Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan",
                    "summary": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了Agents of Diffusion (AoD)框架，明确将结构化文本生成构建为一个多智能体对齐过程，涉及提示优化智能体与评判智能体之间的协作与通信，符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决生成高质量结构化数据时难以兼顾语义丰富性与严格模式一致性的挑战。针对结构化文本生成任务，我们提出了Agents of Diffusion (AoD)框架，利用多智能体强化学习通过自然语言反馈迭代引导冻结的Diffusion Language Models (DLMs)。在MultiWOZ、Super-NaturalInstructions等数据集上，通过Task Success Rate (TSR)和Field Overlap等指标验证了其有效性，实现了优于基线模型的结构保真度与语义多样性平衡。",
                    "summary_translation": "生成高质量的结构化数据（例如 JSON 记录）仍然是大语言模型面临的一项基本挑战，尤其是在必须兼顾语义丰富性与严格模式遵守的情况下。尽管自回归大语言模型具备强大的结构一致性，但它们往往难以应对语义变化和输出多样性方面的要求。相比之下，扩散语言模型引入了实现语义丰富性和双向解码的强大机制，却缺乏可靠保持结构所需的归纳偏置。我们提出了 Agents of Diffusion (AoD)，这是一个新颖的框架，通过语言介导的强化学习，将扩散语言模型的生成灵活性与自回归模型的推理能力统一起来。AoD 将结构化文本生成构建为一个多智能体对齐过程，其中提示优化智能体与评判智能体协作，利用自然语言反馈迭代指导扩散语言模型。这种方法实现了可控且符合模式规范的生成，而无需修改模型参数或依赖人工设计的约束。AoD 证明了扩散模型在协作智能体的监督下能够同时实现高语义新颖性和结构保真度，从而推进了可控生成领域的发展。在多个结构化数据基准测试中，AoD 始终优于扩散模型和自回归模型的基线，为结构感知且多样性增强的文本合成开辟了一条新路径。",
                    "inspiration_trace": "基于论文《Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法创新的思考过程。\n\n---\n\n### 1. 宏观问题与现状观察：结构化生成的“两难困境”\n**思考起点：** 作者首先关注到生成高质量结构化数据（如JSON）是当前LLM应用的一个核心痛点。\n*   **观察现象：** 现有的两大生成范式存在明显的优缺点互补，但无法兼得。\n    *   **自回归模型（AR-LLMs）：** 具有极强的结构一致性和因果逻辑（因为是从左到右生成），容易符合Schema。但缺点是语义单一、容易陷入重复模式，缺乏多样性。\n    *   **扩散语言模型：** 具有双向去噪机制，语义丰富、多样性高。但缺乏位置先验，很难严格保持复杂的嵌套结构（如JSON的括号匹配、字段完整性）。\n*   **核心矛盾：** 我们既想要扩散模型的“语义多样性”，又想要自回归模型的“结构严谨性”。现有的方法要么微调模型（成本高），要么使用硬性规则（缺乏灵活性）。\n\n### 2. 核心假设：用“推理”驾驭“生成”\n**思考转折：** 既然重新训练一个完美的模型很难，能否通过“外部控制”来弥补内部缺陷？\n*   **假设提出：** 能否利用自回归模型强大的逻辑推理能力，来“监督”或“引导”扩散模型的生成过程？\n*   **关键洞察：** 不需要修改扩散模型的参数（保持其生成多样性），而是通过改变其输入条件来控制输出。\n*   **控制接口：** 最直接的控制接口就是**提示词**。如果能让提示词动态进化，就能在不改动模型权重的情况下，引导模型生成符合结构要求的内容。\n\n### 3. 方法论演进：从静态提示到动态强化学习\n**思考深化：** 传统的提示工程是静态的（写一次，固定用），无法应对生成过程中的随机性和错误。如何实现动态控制？\n*   **机制选择：** 引入**强化学习（RL）**。将“提示词的修改”看作是一个动作，将“生成结果的质量”看作是奖励。\n*   **反馈信号的困境：** 传统的RL通常使用标量奖励（如一个分数）。但在结构化生成中，一个分数很难解释具体的错误（例如：“缺少字段”还是“格式错误”）。\n*   **创新点：** **自然语言反馈**。既然是语言模型，为什么不直接用语言来作为奖励信号？语言反馈比标量数字包含更丰富的信息，且更容易被LLM理解和执行。\n\n### 4. 架构构建：多智能体分工协作\n**具体化：** 如何将上述理论落地？作者设计了一个基于角色的多智能体系统，将任务拆解。\n*   **角色分工：**\n    *   **生成者：** 冻结的扩散模型。负责提供多样化的候选内容（探索者）。\n    *   **评判者：** 自回归LLM。负责检查生成内容的结构完整性和语义准确性，并输出自然语言反馈（批评家）。\n    *   **优化者：** 另一个自回归LLM。负责根据评判者的反馈，修改提示词（决策者）。\n*   **闭环逻辑：**\n    1.  优化者给出初始提示。\n    2.  扩散模型根据提示生成JSON。\n    3.  评判者检查JSON，给出具体建议（如：“缺少date字段，请修正”）。\n    4.  优化者根据建议修改提示词（如：“确保包含YYYY-MM-DD格式的date字段”）。\n    5.  循环往复，直到生成完美结果。\n\n### 5. 理论保障与最终形态\n**逻辑闭环：** 为什么这个系统是稳定且有效的？\n*   **解决“漂移”问题：** 多智能体系统常面临对话发散的问题。作者通过将扩散模型作为“环境锚点”，所有智能体的交互都围绕具体的生成样本展开，从而保证了交互的稳定性。\n*   **无参数化优势：** 整个过程不需要梯度回传更新扩散模型，完全通过语言层面的交互实现优化。这使得该方法可以即插即用于各种开源或闭源模型。\n\n---\n\n**总结：作者的思考路径**\n从**“结构 vs 多样性”**的矛盾出发 $\\rightarrow$ 提出**“用AR推理控制DLM生成”**的假设 $\\rightarrow$ 选择**“提示词”**作为控制抓手 $\\rightarrow$ 引入**“强化学习+自然语言反馈”**实现动态优化 $\\rightarrow$ 最终构建**“生成-评判-优化”**的多智能体协作闭环。"
                },
                {
                    "title": "Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents",
                    "arxiv_id": "2601.06490",
                    "authors": "Wenyu Mao, Haosong Tan, Shuchang Liu, Haoyang Liu, Yifan Xu, Huaxiang Ji, Xiang Wang",
                    "summary": "Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了一个包含归纳智能体和反思智能体的框架，专注于LLM智能体的记忆构建和自我反思机制，符合单智能体研究范围。",
                    "summary2": "本文旨在解决个性化LLM分层记忆中因噪声和幻觉导致的局部记忆与全局画像不一致问题。针对长期个性化对话场景，我们提出了一种名为Bi-Mem的智能体框架，通过归纳-反思双向构建机制校准记忆，并引入联想检索。我们在LoCoMo数据集上通过F1和BLEU-1指标验证了其有效性。",
                    "summary_translation": "从用户的长期对话中构建记忆，能够克服大语言模型的上下文限制，从而实现个性化交互。近期的研究侧重于层次化记忆，旨在通过聚类和聚合历史对话来建模用户的多粒度行为模式。然而，对话噪声和记忆幻觉可能在聚类过程中被放大，导致局部聚合记忆与用户的全局人设不一致。为缓解这一问题，我们提出了Bi-Mem，这是一个通过双向构建来确保层次化记忆保真度的智能体框架。具体而言，我们部署了一个归纳智能体来构建层次化记忆：该智能体从原始对话中提取事实信息以形成事实级记忆，利用图聚类将其聚合为主题场景（即局部场景级记忆），并推断用户画像作为全局人设级记忆。同时，我们设计了一个反思智能体，利用从人设级记忆中导出的全局约束来校准局部场景级记忆，从而强化全局-局部对齐。为实现连贯的记忆回忆，我们提出了一种联想检索机制：除了初始的层次搜索外，扩散激活过程允许事实激活上下文场景，而场景级匹配则检索显著的支撑性事实信息。实证评估表明，Bi-Mem在长期个性化对话任务的问答性能方面取得了显著提升。",
                    "inspiration_trace": "基于论文《Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观背景与现有范式的演进\n**（从“个性化需求”到“分层记忆”的必然性）**\n\n1.  **观察起点：** LLM的上下文窗口有限，无法容纳用户的长期对话历史。为了实现个性化交互（如记住用户偏好、回忆共同经历），必须引入外部记忆机制。\n2.  **现有方案的局限：** 早期的“扁平化记忆”仅存储孤立的事实或摘要，缺乏对事实间关联和用户高层行为模式的捕捉能力。\n3.  **趋势演进：** 研究自然转向了“分层记忆”，即通过聚类将原子事实聚合为场景，再提炼为用户画像。这种结构模仿了人类认知，从细粒度到粗粒度，看似完美解决了信息碎片化问题。\n\n### 第二阶段：关键问题的发现与诊断\n**（从“单向聚合”到“级联错误”的洞察）**\n\n1.  **深入审视：** 作者观察到，现有的分层记忆构建过程大多是**单向的**，即纯粹的自底向上聚合。\n2.  **核心痛点：** 在自底向上的过程中，原始对话中的“噪声”（如无关闲聊）和提取过程中的“幻觉”会被聚类算法放大。\n3.  **逻辑矛盾：** 这种放大的噪声会导致局部聚合的场景记忆与用户的全局画像发生冲突。\n    *   *案例思考：* 用户平时口味清淡（全局画像），但偶尔陪朋友吃了一次辣（局部场景）。单纯的聚类会错误地将“吃辣”归纳为用户的局部习惯，导致后续推荐出错。\n4.  **归纳假设：** 问题的根源在于缺乏“全局约束”。局部记忆的生成缺乏对全局一致性的校验，导致了“级联错误”的积累。\n\n### 第三阶段：核心假设与方法论的提出\n**（从“单向构建”到“双向闭环”的突破）**\n\n1.  **解决思路：** 为了解决局部与全局的冲突，记忆构建不能只是单向的归纳，必须引入反向的反思机制。\n2.  **框架设计：** 提出 **Bi-Mem** 框架，将记忆构建过程拆解为两个互补的智能体：\n    *   **归纳智能体：** 负责传统的自底向上构建（事实 -> 场景 -> 画像）。这是为了从数据中提取信息。\n    *   **反思智能体：** 负责自顶向下的校准。利用生成的全局画像作为“约束条件”，去检查和修正下层的场景记忆。\n3.  **逻辑闭环：** 通过这种“双向构建”，确保了局部细节（场景）始终服务于并服从于全局特征（画像），消除了记忆中的逻辑矛盾。\n\n### 第四阶段：记忆利用机制的优化\n**（从“静态检索”到“动态关联”的完善）**\n\n1.  **新问题：** 虽然记忆结构被修正了，但在检索时，如果仅按层级独立检索（如只查场景或只查画像），可能会割裂事实与上下文的联系。\n2.  **联想机制：** 作者引入了心理学中的“扩散激活”概念。\n3.  **检索逻辑：** 检索不应是孤立的。\n    *   检索到一个“事实”时，应自动激活其所属的“场景”。\n    *   检索到一个“场景”时，应回溯其包含的关键“事实”。\n4.  **最终形态：** 形成了**联想检索机制**，在初始检索后进行跨层级的扩散，确保模型在生成回答时能同时获得宏观的上下文和微观的证据支持。\n\n---\n\n### 总结：作者的思维演进路径\n\n1.  **宏观需求：** LLM需要长期记忆来实现个性化。\n2.  **技术选型：** 分层记忆优于扁平记忆。\n3.  **批判性观察：** 现有的分层记忆是单向的，容易因噪声放大导致“局部-全局”不一致。\n4.  **核心创新：** 引入“反思”机制，构建双向闭环（归纳+校准），用全局画像约束局部场景。\n5.  **应用落地：** 设计联想检索，打通层级间的壁垒，实现连贯的记忆召回。"
                },
                {
                    "title": "Agent Contracts: A Formal Framework for Resource-Bounded Autonomous AI Systems",
                    "arxiv_id": "2601.08815",
                    "authors": "Qing Ye, Jing Tan",
                    "summary": "The Contract Net Protocol (1980) introduced coordination through contracts in multi-agent systems. Modern agent protocols standardize connectivity and interoperability; yet, none provide formal, resource governance-normative mechanisms to bound how much agents may consume or how long they may operate. We introduce Agent Contracts, a formal framework that extends the contract metaphor from task allocation to resource-bounded execution. An Agent Contract unifies input/output specifications, multi-dimensional resource constraints, temporal boundaries, and success criteria into a coherent governance mechanism with explicit lifecycle semantics. For multi-agent coordination, we establish conservation laws ensuring delegated budgets respect parent constraints, enabling hierarchical coordination through contract delegation. Empirical validation across four experiments demonstrates 90% token reduction with 525x lower variance in iterative workflows, zero conservation violations in multi-agent delegation, and measurable quality-resource tradeoffs through contract modes. Agent Contracts provide formal foundations for predictable, auditable, and resource-bounded autonomous AI deployment.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了“Agent Contracts”框架，重点解决多智能体系统中的协作、通信、资源约束和委托机制，属于多智能体研究范畴，且不涉及纯应用、纯推理或基础设施优化等排除项。",
                    "summary2": "本文旨在解决自主AI系统缺乏正式资源治理机制的问题。针对多Agent协作及迭代工作流场景，我们提出了一种Agent Contracts形式化框架，通过七元组定义统一了资源约束、时间边界及成功标准，并建立了守恒定律。在代码审查、研究管道等实验中，通过Token减少量、方差及守恒违规率验证了其有效性，实现了90%的Token减少和零守恒违规。",
                    "summary_translation": "Contract Net Protocol (合同网协议) (1980) 引入了在 multi-agent systems (多智能体系统) 中通过 contracts (合同) 进行协调的概念。现代 agent protocols (智能体协议) 标准化了 connectivity (连接性) 和 interoperability (互操作性)；然而，目前尚无协议提供正式的 resource governance-normative mechanisms (资源治理规范机制) 来限制 agents (智能体) 的消耗量或运行时长。我们提出了 Agent Contracts (智能体合同)，这是一个 formal framework (形式化框架)，将 contract metaphor (合同隐喻) 从 task allocation (任务分配) 扩展到了 resource-bounded execution (资源受限执行)。Agent Contract (智能体合同) 将 input/output specifications (输入/输出规范)、multi-dimensional resource constraints (多维资源约束)、temporal boundaries (时间边界) 和 success criteria (成功标准) 统一为一个具有明确 lifecycle semantics (生命周期语义) 的连贯 governance mechanism (治理机制)。针对 multi-agent coordination (多智能体协调)，我们建立了 conservation laws (守恒定律)，确保 delegated budgets (委托预算) 遵守 parent constraints (父级约束)，从而通过 contract delegation (合同委托) 实现 hierarchical coordination (分层协调)。跨越四个实验的 empirical validation (实证验证) 表明，在 iterative workflows (迭代工作流) 中实现了 90% 的 token reduction (令牌减少) 和 525 倍更低的 variance (方差)，在 multi-agent delegation (多智能体委托) 中实现了零 conservation violations (守恒违规)，并通过 contract modes (合同模式) 实现了可测量的 quality-resource tradeoffs (质量-资源权衡)。Agent Contracts (智能体合同) 为 predictable (可预测)、auditable (可审计) 和 resource-bounded (资源受限) 的 autonomous AI deployment (自主 AI 部署) 提供了 formal foundations (形式化基础)。",
                    "inspiration_trace": "基于论文《Agent Contracts: A Formal Framework for Resource-Bounded Autonomous AI Systems》的内容，以下是对作者核心方法论产出过程的逻辑推演：\n\n### 第一阶段：问题锚定——从“能力失控”到“治理缺失”\n**1. 现象观察：**\n作者首先关注到一个极具冲击力的现实案例：一个多智能体系统因陷入递归循环，在无人监管的情况下运行了11天，产生了4.7万美元的巨额账单。\n**2. 宏观诊断：**\n这并非单纯的代码Bug，而是系统性的**治理赤字**。现有的AI智能体虽然具备了自主行动的能力，但缺乏对其行为边界的正式约束机制。\n**3. 痛点提炼：**\n当前的行业现状是“重连接、轻治理”。现有的协议（如MCP、A2A）和框架（如AutoGen、LangGraph）主要解决的是“如何连接”和“如何互操作”，却完全忽略了“资源治理”——即智能体可以消耗多少资源、可以运行多久。\n\n### 第二阶段：概念迁移——从“任务分配”到“资源契约”\n**1. 历史回溯与对比：**\n作者回顾了经典的《合同网协议》（1980），指出其核心在于解决“谁来做这个任务”。\n**2. 隐喻扩展：**\n作者提出，现代AI系统的问题不再是“谁来做”，而是“在什么边界内做”。因此，需要将经济学中的“契约”概念从单纯的**任务分配**扩展到**资源治理**。\n**3. 理论融合：**\n为了构建这一新概念，作者融合了三个领域的理论：\n*   **契约理论（经济学）：** 解决信息不对称下的激励与约束。\n*   **多智能体协调（CS）：** 引入规范和守恒定律。\n*   **有界计算（实时系统）：** 引入硬约束和满意原则。\n\n### 第三阶段：形式化构建——定义“智能体契约”的七元组\n**1. 核心假设：**\n如果要将“契约”作为治理机制，它必须是一个包含执行全要素的正式规范，而不仅仅是一个配置文件。\n**2. 模型设计：**\n作者构建了一个七元组 $C = (I, O, S, R, T, \\Phi, \\Psi)$，这一设计的逻辑演进如下：\n*   **基础定义 ($I, O$)：** 明确输入输出，这是契约的标的。\n*   **能力界定 ($S$)：** 明确智能体能做什么（工具、技能），防止其越权使用未授权能力。\n*   **核心约束 ($R, T$)：** 这是治理的核心。$R$ 定义多维资源预算（Token、API调用等），$T$ 定义时间边界。这解决了“无限运行”的问题。\n*   **生命周期控制 ($\\Phi, \\Psi$)：** 区分“成功标准”与“终止条件”。即使任务未完成，一旦触碰资源红线（$\\Psi$），也必须强制终止。这引入了实时系统中的“硬约束”概念。\n\n### 第四阶段：系统扩展——多智能体环境下的“守恒定律”\n**1. 层级挑战：**\n单智能体的契约相对简单，但在多智能体协作中，如何确保子任务的资源消耗总和不超过父任务的总预算？\n**2. 物理学类比：**\n作者引入物理学中的“守恒定律”概念，即：**被委托的预算总和必须尊重父级约束。**\n**3. 委托机制：**\n将“契约”本身定义为一种能力。智能体不仅可以执行任务，还可以作为“发包方”创建子契约。\n*   **逻辑推演：** 如果父契约有100个Token，它分给子契约A 40个，子契约B 40个，自己留20个。这种**预算池化**和**动态重分配**机制，确保了无论系统如何复杂，整体资源消耗始终可控。\n\n### 第五阶段：现实妥协与验证——从理论到落地的边界\n**1. 技术瓶颈认知：**\n作者清醒地认识到LLM API的局限性：Token消耗通常在调用结束后才可知，无法在生成过程中实时中断（单次调用限制）。\n**2. 策略调整：**\n因此，该框架的价值不在于“阻止一次昂贵的单次调用”，而在于**防止多轮循环中的失控**（如无限重试、递归死循环）。\n**3. 实验验证：**\n通过四个实验（代码审查、研究管道等），作者验证了逻辑链的闭环：\n*   **治理有效性：** 90%的Token减少，方差降低525倍（证明了$R$和$T$的有效性）。\n*   **守恒定律：** 多智能体委托中零违规（证明了层级约束的有效性）。\n*   **质量-资源权衡：** 通过不同的契约模式（紧急/平衡），验证了西蒙的“满意原则”。\n\n### 总结：作者的思考路径图\n**失控的现实（$47k账单）** $\\rightarrow$ **发现治理真空（现有协议只管连接不管资源）** $\\rightarrow$ **引入契约隐喻（从“谁做”转向“边界内做”）** $\\rightarrow$ **形式化定义（七元组规范）** $\\rightarrow$ **解决复杂性问题（守恒定律与委托）** $\\rightarrow$ **工程化落地（承认API限制，聚焦多轮防护）**。"
                },
                {
                    "title": "Emergent Coordination in Multi-Agent Systems via Pressure Fields and Temporal Decay",
                    "arxiv_id": "2601.08129",
                    "authors": "Roland Rodriguez",
                    "summary": "Current multi-agent LLM frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions. Empirically, on Latin Square constraint satisfaction across 1,078 trials, pressure-field coordination matches hierarchical control (38.2% vs 38.8% aggregate solve rate, p=0.94, indicating statistical equivalence). Both significantly outperform sequential (23.3%), random (11.7%), and conversation-based multi-agent dialogue (8.6%, p<0.00001). Temporal decay is essential: disabling it increases final pressure 49-fold (d=4.15). On easy problems, pressure-field achieves 87% solve rate. The approach maintains consistent performance from 2 to 32 agents. Our key finding: implicit coordination through shared pressure gradients achieves parity with explicit hierarchical control while dramatically outperforming explicit dialogue-based coordination. This suggests that constraint-driven emergence offers a simpler, equally effective foundation for multi-agent AI.",
                    "category": "cs.MA",
                    "filter_reason": "该论文属于多智能体协作范畴。它提出了一种基于压力场和时间衰减的新颖协调机制，用于替代传统的显式编排（如分层控制），以解决多智能体LLM系统中的协调开销问题，符合研究范围中关于多智能体协作与通信的定义。",
                    "summary2": "本文旨在解决多智能体系统中显式编排导致的协调开销扩展性问题。针对Latin Square约束满足场景，我们提出了一种基于pressure fields和temporal decay的隐式协调机制，代理仅通过共享状态和局部压力梯度实现协作。在1,078次试验中，通过求解率和最终压力验证了其有效性，表明该方法性能与层级控制相当，且显著优于对话式协调。",
                    "summary_translation": "当前的 multi-agent LLM frameworks（多智能体 LLM 框架）依赖于从人类组织结构中借用的 explicit orchestration patterns（显式编排模式）：规划者委托给执行者，管理者协调工作者，且 hierarchical control flow（分层控制流）主导着智能体之间的交互。这些方法存在 coordination overhead（协调开销）问题，且随着智能体数量和任务复杂性的增加，其扩展性较差。我们提出了一种受 natural coordination mechanisms（自然协调机制）启发、根本不同的范式：智能体在 shared artifact（共享工件）上进行局部操作，仅由源自可测量 quality signals（质量信号）的 pressure gradients（压力梯度）引导，并利用 temporal decay（时间衰减）来防止 premature convergence（过早收敛）。我们将其形式化为在 pressure landscape（压力景观）上的优化问题，并在温和条件下证明了 convergence guarantees（收敛性保证）。实证结果表明，在涵盖 1,078 次试验的 Latin Square constraint satisfaction（拉丁方约束满足）问题中，pressure-field coordination（压力场协调）与 hierarchical control（分层控制）的表现相当（总体解决率分别为 38.2% 和 38.8%，p=0.94，表明具有 statistical equivalence（统计等效性））。两者均显著优于 sequential（顺序式，23.3%）、random（随机式，11.7%）以及 conversation-based multi-agent dialogue（基于对话的多智能体对话，8.6%，p<0.00001）。Temporal decay（时间衰减）至关重要：禁用该机制会使最终压力增加 49 倍（d=4.15）。在简单问题上，压力场协调达到了 87% 的解决率。该方法在 2 到 32 个智能体的范围内保持了稳定的性能。我们的关键发现是：通过共享 pressure gradients（压力梯度）实现的 implicit coordination（隐式协调），不仅达到了与显式 hierarchical control（分层控制）相当的水平，还显著优于显式的基于对话的协调。这表明，constraint-driven emergence（约束驱动的涌现）为多智能体 AI 提供了一种更简单且同样有效的基础。",
                    "inspiration_trace": "基于论文内容，以下是对作者提出“压力场协调”方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观观察与问题识别\n**（从“模仿人类组织”到“发现扩展性瓶颈”）**\n\n1.  **观察现状**：作者审视当前主流的多智能体LLM框架（如AutoGen, MetaGPT），发现它们普遍采用**显式编排**模式。即模仿人类社会的科层制：设立管理者、规划者，通过对话、消息传递来分配任务和协调行动。\n2.  **识别痛点**：作者意识到这种“类人”的协调方式存在根本性的**扩展性问题**。随着智能体数量增加和任务变复杂，消息传递的开销、管理者的瓶颈以及级联失败的风险会急剧上升。\n3.  **初步假设**：对于某些任务（特别是约束满足问题），复杂的显式沟通可能不仅是不必要的，甚至是有害的。是否存在一种更底层、更自然的协调方式？\n\n### 第二阶段：范式转移与灵感类比\n**（从“显式对话”到“隐式环境”）**\n\n1.  **跨界灵感**：作者将目光投向自然界，观察蚁群、免疫系统等群体系统。这些系统没有中央指挥，也不通过复杂的语言交流，而是通过**修改环境**（如蚂蚁留下的信息素）来实现间接协调（Stigmergy，共形机制）。\n2.  **核心隐喻**：作者提出将“共享工件”（如代码库、待解的谜题）视为环境。智能体不需要互相“说话”，只需要感知工件当前的“质量状态”。\n3.  **范式确立**：由此，作者确立了**隐式协调**的范式——智能体只对共享状态的局部信号做出反应，协调是涌现出来的，而不是被规划出来的。\n\n### 第三阶段：机制构建与核心概念\n**（从“环境感知”到“压力场与衰减”）**\n\n1.  **定义信号（压力）**：如何量化“质量状态”？作者引入**“压力”**概念。压力是对工件中错误或约束违反程度的度量（类似于物理学中的势能）。智能体的目标就是局部地降低压力。\n2.  **解决局部最优（时间衰减）**：作者预见到一个潜在问题：如果智能体只是贪婪地降低压力，系统可能会过早陷入局部最优解（即一个看似稳定但并非最优的状态）。\n3.  **引入动态机制**：为了解决这个问题，作者借鉴了信息素蒸发的机制，提出了**“时间衰减”**。即，工件区域的“置信度”或“适应度”会随时间自然降低。这迫使智能体不断重新评估那些看似“已解决”的区域，从而维持系统的探索能力，防止僵化。\n\n### 第四阶段：理论验证与形式化\n**（从“直觉”到“收敛性证明”）**\n\n1.  **理论映射**：为了证明这种松散的系统能有效工作，作者将其映射到**势博弈**框架。将全局压力定义为势函数。\n2.  **收敛性逻辑**：如果局部行动能降低局部压力，且局部压力的降低能导致全局压力的降低（压力对齐），那么即使没有中央协调，无数个贪婪的局部行动最终也会让系统收敛到纳什均衡（即稳定解）。\n3.  **逻辑闭环**：这一步从理论上支撑了直觉——只要设计好压力函数，去中心化的混乱行动可以导向有序的结果。\n\n### 第五阶段：实证检验与结论修正\n**（从“理论优越”到“等效但简洁”）**\n\n1.  **实验设计**：作者选择拉丁方约束满足问题作为测试场，对比显式层级、对话式、随机等方法。\n2.  **反直觉发现**：实验结果显示，基于对话的协调（AutoGen风格）表现最差，甚至不如随机选择，证实了“显式沟通开销”在约束任务中的危害。\n3.  **结论修正**：作者发现压力场方法并没有显著“超越”显式层级控制（两者性能统计等效），但这恰恰是核心贡献所在——**用极简的架构（无管理者、无通信）达到了复杂架构的性能**。\n4.  **最终定论**：作者确立了该方法论的定位：在具有可测量质量信号的领域，基于压力场的隐式协调是比显式对话更优、比层级控制更简单的替代方案。\n\n---\n\n**总结：作者的思考路径**\n**质疑现状（显式沟通太重）** $\\rightarrow$ **模仿自然（环境共形）** $\\rightarrow$ **抽象模型（压力场+衰减）** $\\rightarrow$ **理论背书（势博弈收敛）** $\\rightarrow$ **实证确认（等效且简洁）**。"
                },
            ]
        },
    ],
    "2026-01-12": [
        {
            "name": "Artificial Intelligence",
            "count": 45,
            "papers": [
                {
                    "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
                    "arxiv_id": "2601.05890",
                    "authors": "Ruizhe Zhang, Xinke Jiang, Zhibang Yang, Zhixin Zhang, Jiaran Gao, Yuzhen Xiao, Hongbin Lai, Xu Chu, Junfeng Zhao, Yasha Wang",
                    "summary": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于大语言模型的集中式分层多智能体框架，重点解决多智能体协作中的记忆管理和协调问题，属于多智能体与记忆机制的研究范畴。",
                    "summary2": "本文旨在解决集中式多智能体系统在长视距协作中因记忆管理缺失导致的不稳定及跨任务泛化能力差的问题。针对复杂长视距任务，我们提出了一种名为StackPlanner的分层多智能体框架，通过解耦协调与执行、引入主动任务记忆管理及结构化经验记忆，并利用强化学习优化协调策略。在2WikiMultiHopQA、MuSiQue、GAIA和FRAMES等基准测试上，通过F1分数验证了其有效性，显著优于现有基线。",
                    "summary_translation": "基于 large language models (大语言模型) 的 Multi-agent systems (多智能体系统)，尤其是 centralized architectures (中心化架构)，近期在处理复杂且知识密集型任务方面展现出巨大潜力。然而，由于缺乏 memory management (记忆管理)，central agents (中心智能体) 常面临不稳定的 long-horizon collaboration (长程协作) 问题，导致 context bloat (上下文膨胀)、error accumulation (错误累积) 以及较差的 cross-task generalization (跨任务泛化) 能力。为解决 task-level memory (任务级记忆) 效率低下及无法复用 coordination experience (协作经验) 的问题，我们提出了 StackPlanner，这是一种具备 explicit memory control (显式记忆控制) 的 hierarchical multi-agent framework (分层多智能体框架)。StackPlanner 通过主动的 task-level memory control (任务级记忆控制) 将 high-level coordination (高层协调) 与 subtask execution (子任务执行) 解耦，并利用 structured experience memory (结构化经验记忆) 和 reinforcement learning (强化学习) 来检索及利用可复用的 coordination experience (协作经验)，从而应对上述挑战。在多个 deep-search (深度搜索) 和 agent system benchmarks (智能体系统基准) 上的实验表明，我们的方法在实现可靠的 long-horizon multi-agent collaboration (长程多智能体协作) 方面具有显著成效。",
                    "inspiration_trace": "基于论文《StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management》，以下是对作者核心方法论产出逻辑链的系统性推演：\n\n### 1. 宏观观察：从“多智能体协作”到“中心化架构的瓶颈”\n**思考起点：**\n作者首先观察到，基于大语言模型的多智能体系统（LLM-MAS）在处理复杂、长周期的知识密集型任务时表现出巨大潜力。虽然去中心化或辩论式架构存在通信开销大、一致性难维护的问题，但**中心化架构**（由一个中央智能体统一调度）因其全局控制能力，逐渐成为主流选择。\n\n**发现问题：**\n然而，随着任务规模和复杂度的提升，现有的中心化架构开始失效。中央智能体在面对长链条推理时，往往会出现“迷失在中间”的现象，导致协作不稳定。\n\n### 2. 深度诊断：核心症结在于“记忆管理的缺失”\n**逻辑推演：**\n为什么中央智能体会失效？作者分析认为，问题不在于LLM的推理能力本身，而在于**信息过载**。\n*   **现象：** 子智能体源源不断地产生信息，中央智能体被动地接收所有原始数据。\n*   **后果：** 上下文窗口迅速膨胀，噪声累积，早期错误在长链条中传播，导致决策偏离。\n\n**关键洞察：**\n现有的系统将“记忆”视为静态的副产品，缺乏主动管理机制。这引出了两个核心挑战：\n1.  **任务级记忆挑战（C1）：** 如何在长周期任务中，主动过滤噪声、压缩信息，防止上下文臃肿？\n2.  **跨任务经验挑战（C2）：** 如何复用历史成功的协作经验，避免每次面对新任务都“从零开始”，从而解决冷启动和泛化能力差的问题？\n\n### 3. 架构重构：从“被动接收”到“主动控制”\n**解决方案构思：**\n为了解决上述挑战，作者决定对系统进行结构性改造，核心思想是**解耦**与**显式化**。\n\n*   **针对C1（任务记忆）的思考：**\n    *   *传统做法：* 简单的截断或模板化摘要（被动）。\n    *   *创新思路：* 将“记忆管理”变成一种**显式的动作**。中央智能体不仅要决定“做什么任务”，还要决定“记忆里留什么”。\n    *   *具体化：* 引入**栈式记忆结构**和**REVISE动作**。允许智能体主动进行“压缩”和“剪枝”，像编辑文档一样编辑自己的记忆，从而保持认知的清晰度。\n\n*   **针对C2（经验记忆）的思考：**\n    *   *传统做法：* 仅依赖LLM的参数化知识。\n    *   *创新思路：* 构建一个结构化的**外部经验库**，专门存储“如何协作”的知识。\n    *   *具体化：* 将经验分为三类：用户画像、语义记忆（事实）、程序性记忆（SOPs，即标准作业程序）。这样，智能体遇到新任务时，可以检索过去的“成功套路”，而不仅仅是事实。\n\n### 4. 机制优化：利用强化学习学习“如何决策”\n**进一步思考：**\n有了架构（分层）和工具（记忆管理），如何保证中央智能体能用好这些工具？仅仅依靠提示工程可能不足以让智能体学会复杂的“何时压缩记忆”或“何时检索经验”的时机。\n\n**最终闭环：**\n作者将整个规划过程建模为一个**可学习的决策过程**。\n*   引入**强化学习（RL）**（具体为GRPO算法）。\n*   目标是训练中央智能体不仅学会任务规划，更要学会**元技能**——即如何最优地管理记忆栈和检索经验。\n*   通过奖励机制，强化那些能够有效利用记忆、减少冗余步骤、成功完成任务的策略。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径呈现出清晰的**“发现问题 -> 归因分析 -> 架构解耦 -> 机制显式化 -> 算法闭环”**的逻辑链条：\n\n1.  **观察：** 中心化多智能体系统在长任务中失效。\n2.  **归因：** 根本原因是缺乏记忆管理，导致上下文臃肿和经验无法复用。\n3.  **架构设计：** 提出“分层架构”，将高层决策与底层执行解耦。\n4.  **核心创新：**\n    *   **任务侧：** 提出“主动记忆管理”，通过REVISE动作动态维护记忆栈。\n    *   **经验侧：** 提出“结构化经验记忆”，存储可复用的协作模式（SOPs）。\n5.  **训练落地：** 利用强化学习，让智能体在试错中学会如何最优地运用上述记忆和经验机制。\n\n这一过程体现了作者从系统架构的宏观视角，深入到认知科学中的记忆机制，最后通过算法手段实现自动化的完整学术创新路径。"
                },
                {
                    "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
                    "arxiv_id": "2601.05787",
                    "authors": "Zezhou Wang, Ziyun Zhang, Xiaoyi Zhang, Zhuzhong Qian, Yan Lu",
                    "summary": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于GUI智能体（计算机使用智能体），提出了通过强化学习和专家轨迹来增强智能体策略的方法。研究内容涉及智能体的规划、执行以及通过反馈进行自我完善，符合单智能体和自我演化的研究范围。尽管使用了视觉输入，但核心在于智能体的能力提升而非纯视觉模型研究。",
                    "summary2": "本文旨在提升端到端GUI智能体性能，解决利用专家轨迹进行强化学习时的结构不匹配与分布偏移问题。针对OSWorld等GUI交互场景，我们提出了一种双层专家到策略同化框架（BEPA），通过自滚动执行和动态缓存更新将静态专家轨迹转化为策略对齐的指导。我们在OSWorld-Verified、MMBench-GUI和Online-Mind2Web上通过成功率验证了其有效性，显著提升了基线模型表现。",
                    "summary_translation": "视觉-语言模型正日益被部署为用于操作桌面和浏览器的计算机使用代理。性能最优异的 CUAs 是基于框架的系统，它们将规划与执行过程解耦；相比之下，端到端的截图到动作策略虽然更易于部署，但在 OSWorld-Verified 等基准测试中表现滞后。诸如 OSWorld 之类的 GUI 数据集存在两个瓶颈：它们仅包含数百个交互式、可验证的任务和环境；此外，专家轨迹必须通过与这些环境进行交互来收集，导致此类数据难以扩展。因此，我们探讨如何利用基于可验证奖励的强化学习，最大限度地利用少量现有的专家轨迹来训练端到端策略。简单地将这些离线策略轨迹混入在线策略的 RLVR 中是脆弱的：即使经过格式转换，专家轨迹与学习器之间仍存在结构不匹配和分布偏移。我们提出了 BEPA (Bi-Level Expert-to-Policy Assimilation，双层专家到策略同化)，该方法通过基础策略生成的自滚动可达轨迹（LEVEL-1）以及 RLVR 中使用的按任务动态更新的缓存（LEVEL-2），将静态的专家轨迹转化为与策略对齐的指导信号。在 OSWorld-Verified 上，BEPA 将 UITARS1.5-7B 的成功率从 22.87% 提升至 32.13%，并将保留集上的表现从 5.74% 提升至 10.30%，同时在 MMBench-GUI 和 Online-Mind2Web 上也取得了持续的提升。我们的代码和数据可在以下地址获取：https://github.com/LEON-gittech/Verl_GUI.git",
                    "inspiration_trace": "基于论文《From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation》，以下是对作者核心方法论逻辑链的系统性推演：\n\n### 1. 宏观问题：端到端智能体的性能瓶颈\n**观察**：在计算机控制领域，存在两种主流范式：\n*   **基于框架的系统**：通过规划器、执行器等多模块协作，性能强大（如 Agent S2），但部署复杂。\n*   **端到端（E2E）策略**：直接从截图映射到动作，部署简单，但在高难度基准（如 OSWorld）上表现显著落后（约 23% vs 33-42%）。\n\n**核心矛盾**：我们希望获得 E2E 的部署便利性，但渴望框架系统的强大性能。如何缩小这一差距？\n\n### 2. 资源约束：数据稀缺与专家轨迹的利用\n**现实困境**：与文本任务不同，GUI 任务数据极难扩展。\n*   任务数量有限（仅几百个）。\n*   获取专家轨迹成本高昂（需在真实环境中交互）。\n\n**假设**：既然我们拥有少量高质量的“基于框架的专家轨迹”，能否利用强化学习（RLVR）将这些专家知识迁移给端到端策略？\n\n### 3. 初步尝试与失败：直接混合的脆弱性\n**直觉方案**：将专家的离线轨迹直接混合到端到端策略的在线强化学习（On-Policy RL）中，作为监督信号。\n\n**失败诊断**：实验表明这种做法非常脆弱，甚至导致性能下降。作者深入分析发现了两个根本性的“不匹配”：\n1.  **结构不匹配**：框架轨迹包含多角色（规划者、执行者）和工具级 API，而 E2E 策略是单一模型且输出低级动作。简单的格式转换无法消除这种差异。\n2.  **分布偏移**：即使格式转换后，专家轨迹在 E2E 策略的概率分布中依然处于极低概率区域（即“离群点”）。在依赖信任域的 RL 算法（如 PPO/GRPO）中，这种巨大的分布差异会导致优化不稳定或探索崩溃。\n\n**结论**：静态的、异构的专家数据无法被 E2E 策略直接吸收。\n\n### 4. 思想转折：从“模仿动作”到“同化意图”\n**核心洞察**：既然直接模仿专家的“动作”行不通，不如让策略去执行专家的“意图”。我们需要一种机制，将专家轨迹转化为策略“可达”的轨迹。\n\n**逻辑推演**：\n*   专家轨迹中蕴含了高层规划（Plan），这是通用的。\n*   E2E 策略具备执行能力，但缺乏规划。\n*   如果提取专家的**计划**，然后让 E2E 策略在**计划条件**下自主执行，生成的轨迹既保留了专家的高层智慧，又符合策略自身的动作分布。\n\n### 5. 方法论构建：双层专家到策略的同化（BEPA）\n基于上述洞察，作者提出了一个双层动态框架，旨在将静态专家知识转化为动态的、策略对齐的指导。\n\n**LEVEL-1：可达性转换**\n*   **目标**：解决“分布偏移”问题。\n*   **思路**：不直接使用专家的动作序列。而是从专家轨迹中提取自然语言计划，将其作为提示附加给 E2E 策略，让策略自己重新执行。\n*   **结果**：生成的“自滚动”轨迹虽然由策略生成，但受专家计划引导，因此既在策略流形上（高概率），又具有高奖励。\n\n**LEVEL-2：动态对齐**\n*   **目标**：解决“静态数据过时”问题。\n*   **思路**：策略在训练中是不断进化的，LEVEL-1 生成的初始引导可能会变得不再最优。因此，建立一个**动态缓存**。\n*   **机制**：\n    1.  初始化时使用 LEVEL-1 的成功轨迹填充缓存。\n    2.  在 RL 训练过程中，如果策略自己探索出了成功轨迹，就更新缓存。\n    3.  **关键设计**：仅在策略完全探索失败（所有 rollout 均失败）时，才从缓存中注入专家引导。这确保了专家数据仅作为“安全网”，而不干扰策略正常的自主探索。\n\n### 6. 逻辑闭环\n通过这一设计，作者成功实现了从 Off-Policy（静态专家）到 On-Policy（动态策略）的平滑过渡：\n1.  **初始化**：利用专家计划“手把手”教策略生成高质量数据（LEVEL-1）。\n2.  **进化**：策略在 RL 中自我探索，并将自己的成功经验替换掉旧的专家数据（LEVEL-2）。\n3.  **结果**：策略逐渐内化了专家能力，最终在未见任务上实现了显著的性能提升（从 22.87% 提升至 32.13%）。\n\n**总结**：作者的核心贡献在于认识到在 GUI 代理训练中，简单的“数据混合”无效，必须通过“计划引导”和“动态缓存”将异构的专家知识转化为策略自身流形上的动态信号，从而实现能力的有效迁移。"
                },
                {
                    "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
                    "arxiv_id": "2601.05746",
                    "authors": "Zhenghao Li, Zhi Zheng, Wei Chen, Jielun Zhao, Yong Chen, Tong Xu, Enhong Chen",
                    "summary": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了基于大语言模型的多智能体辩论框架，涉及多智能体之间的协作、通信以及工具使用，符合多智能体研究范围。",
                    "summary2": "本文旨在解决多智能体辩论中因初始化同质化导致的推理路径单一及错误传播问题。针对复杂推理任务，我们提出了一种DynaDebate框架，通过动态路径生成与分配、以过程为中心的辩论及基于触发器的验证机制来打破同质化。并在GSM8K、MATH500、AIME及MMLU等基准数据集上通过Accuracy等指标验证了其有效性。",
                    "summary_translation": "近年来，基于大语言模型的多智能体系统（Multi-Agent Systems, MAS）发展迅速，在协作决策和复杂问题解决方面表现卓越。近期，研究人员进一步探索了多智能体辩论（Multi-Agent Debate, MAD）框架，该框架通过多个智能体之间的信息交换与辩论，增强了MAS的推理与协作能力。然而，现有方法往往依赖于无引导的初始化，导致智能体采用相同的推理路径，进而陷入相同的错误。因此，智能体间的有效辩论受到阻碍，最终结果往往退化为简单的多数投票。为解决上述问题，本文提出了动态多智能体辩论（Dynamic Multi-Agent Debate, DynaDebate），该方法通过三个关键机制提升了多智能体辩论的有效性：(1) 动态路径生成与分配（Dynamic Path Generation and Allocation），利用专门的路径生成智能体（Path Generation Agent）生成具有自适应冗余的多样化且合乎逻辑的解决方案路径；(2) 以过程为中心的辩论（Process-Centric Debate），将关注点从表层的基于结果的投票转移到严格的逐步逻辑批判，以确保过程的正确性；(3) 基于触发的验证智能体（Trigger-Based Verification Agent），在出现分歧时被激活，并利用外部工具客观地解决僵局。大量实验表明，DynaDebate在各类基准测试中均取得了优异的性能，超越了现有的最先进（State-of-the-Art, SOTA）MAD方法。",
                    "inspiration_trace": "基于论文《DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation》的内容，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n---\n\n### 1. 宏观观察与问题定义：多智能体辩论的“失效”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在复杂推理任务上的局限性，以及多智能体辩论作为一种解决方案的兴起。MAD 的核心逻辑是“三个臭皮匠顶个诸葛亮”，即通过多个智能体的交互和辩论来纠正错误、提升推理能力。\n\n**关键观察：**\n然而，在实际应用中，作者发现现有的 MAD 框架往往表现不佳，甚至退化成了简单的“多数投票”。这意味着智能体之间并没有发生真正有效的辩论，而是陷入了某种形式的“群体思维”。\n\n---\n\n### 2. 深度诊断：同质化与盲从的双重困境\n为了解释上述观察，作者深入剖析了现有方法的两个根本性缺陷：\n\n*   **缺陷一：初始化同质化**\n    *   **现象：** 现有的 MAD 方法通常采用“无引导的初始化”。即让多个基于相同或相似模型的智能体直接面对同一个问题。\n    *   **根源：** 由于模型固有的“思维定势”，这些智能体往往会选择概率最高的同一条推理路径。\n    *   **后果：** 如果这条路径是错的，所有智能体都会犯同样的错。既然大家都错得一样，辩论就无法发现错误，最终只能通过投票选出那个“共同的错误”。\n\n*   **缺陷二：盲从与肤浅的评估**\n    *   **现象：** 即使智能体有不同的初始答案，在辩论过程中，它们也容易被同伴的观点带偏。\n    *   **根源：** 智能体在评估同伴的回答时，往往关注文本的流畅性或结构的完整性（表面特征），而不是逻辑的正确性。\n    *   **后果：** 正确的智能体可能因为错误的同伴回答看起来“很自信”或“很通顺”而放弃自己的正确立场，导致错误的共识。\n\n---\n\n### 3. 假设提出：从“无序碰撞”转向“结构化异构”\n基于上述诊断，作者提出了核心假设：**要打破无效的辩论，必须人为地引入“异构性”并强制进行“深度的逻辑审查”。**\n\n*   **假设 A（关于起点）：** 如果我们在辩论开始前，强制智能体采用**不同且逻辑上独立**的解题思路，就能从源头上打破同质化，最大化对解空间的探索。\n*   **假设 B（关于过程）：** 如果辩论的焦点从“比较最终答案”转移到“审查推理步骤”，就能避免盲从，确保共识建立在逻辑严密性之上。\n*   **假设 C（关于裁决）：** 当逻辑审查无法解决僵局时，引入**客观的外部工具**作为裁判，比单纯的投票更可靠。\n\n---\n\n### 4. 方法论构建：DynaDebate 的三阶段演进\n为了验证上述假设，作者构建了 DynaDebate 框架，其设计逻辑遵循了从“准备”到“执行”再到“兜底”的闭环：\n\n#### 第一阶段：动态路径生成——解决“怎么想”\n*   **设计思路：** 既然模型自己会偷懒走老路，那就引入一个专门的“路径生成智能体”。\n*   **逻辑演进：** 这个生成器不负责解题，只负责“出谋划策”。它需要生成多条逻辑上互斥、但各自可行的解题路径（例如：一道几何题，一条路用代数解，一条路用向量解）。\n*   **分配机制：** 然后将这些路径分配给不同的辩论智能体。如果路径不够多，就采用轮询分配，确保即使方法重复，也能利用随机性进行校验。\n\n#### 第二阶段：以过程为中心的辩论——解决“怎么辩”\n*   **设计思路：** 改变辩论的规则。禁止智能体只说“我觉得你错了”，必须指出“你的第几步推导有问题”。\n*   **逻辑演进：** 引入“第一性原理审计”。智能体必须将推理过程拆解为原子步骤，辩论时针对每一个步骤的逻辑连贯性和事实正确性进行攻击或辩护。这迫使智能体关注逻辑本质，而非文本表象。\n\n#### 第三阶段：基于触发的验证——解决“谁裁决”\n*   **设计思路：** 辩论可能会陷入僵局（公说公有理，婆说婆有理），或者被错误的逻辑主导。\n*   **逻辑演进：** 引入一个“验证智能体”，但它不是一直在线的（为了节省成本），而是基于触发机制（如分歧过大、无法达成共识）才激活。它调用外部工具（如 Python 代码执行器、搜索引擎）给出客观结果，作为打破僵局的“铁证”。\n\n---\n\n### 5. 逻辑闭环与验证\n**最终思考：**\n通过这三个机制，作者构建了一个完整的逻辑闭环：\n1.  **起点：** 用路径生成确保大家想得不一样（打破同质化）；\n2.  **过程：** 用步骤审计确保大家辩得有深度（避免盲从）；\n3.  **终点：** 用工具验证确保最终结果有依据（客观裁决）。\n\n**实验验证：**\n作者通过在数学推理（如 MATH500, AIME）等任务上的实验，证实了这种“结构化异构”确实比单纯的“多智能体堆砌”更有效，甚至能让小模型通过这种协作机制超越大模型的单体表现。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（辩论失效）**出发，深挖**本质（思维同质化与盲从）**，提出**假设（强制异构与过程审查）**，最终设计出一套**分层解耦的解决方案（路径生成+过程辩论+触发验证）**。这一过程体现了从“增加数量”到“提升质量”的范式转变。"
                },
                {
                    "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models",
                    "arxiv_id": "2601.05570",
                    "authors": "Cooper Lin, Maohao Ran, Yanting Zhang, Zhenglin Wan, Hongwei Fan, Yibo Xu, Yike Guo, Wei Xue, Jun Song",
                    "summary": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了Crisis-Bench，这是一个多智能体部分可观察马尔可夫决策过程（POMDP）基准测试，用于评估LLM智能体在动态危机模拟中的战略行为。该研究涉及智能体的规划、状态管理（记忆）以及与模拟环境的交互，符合单智能体和多智能体的研究范围。尽管涉及对齐讨论，但其核心贡献在于构建智能体评估框架而非单纯的对齐或应用研究。",
                    "summary2": "本文旨在解决通用安全对齐在需要战略模糊的专业领域（如危机公关）中的局限性。针对高风险企业危机场景，我们提出了Crisis-Bench，一种基于多智能体POMDP的动态模拟框架，采用双知识架构和仲裁-市场循环机制。我们在涵盖8个行业的80个危机故事线上，通过模拟股价和信任度等指标验证了其有效性，揭示了现有模型在战略推理上的“对齐税”。",
                    "summary_translation": "标准安全对齐优化了大语言模型，使其具备普遍的有用性和诚实性，从而有效地灌输了一种僵化的“童子军”道德观。尽管这种框架对于通用助手而言是稳健的，但这种“一刀切”的伦理框架给需要战略模糊性和信息保留的专业领域（如公共关系、谈判和危机管理）强加了一种“透明度税”。为了衡量通用安全性与专业效用之间的这种差距，我们引入了 Crisis-Bench，这是一个多智能体部分可观察马尔可夫决策过程，用于在高风险的企业危机中评估大语言模型。Crisis-Bench 涵盖了跨越 8 个行业的 80 个多样化故事线，要求基于大语言模型的公共关系代理应对动态的 7 天企业危机模拟，同时管理严格分离的私有和公开叙事状态，以执行严格的信息不对称。与依赖静态基本事实的传统基准不同，我们引入了裁决者-市场循环：这是一种新颖的评估指标，其中公众情绪受到裁决并转化为模拟股价，从而创建了一个现实的经济激励结构。我们的结果揭示了一个关键的二分法：虽然一些模型向伦理担忧屈服，但其他模型展示了马基雅维利式的、合法的战略保留能力，以稳定模拟股价。Crisis-Bench 提供了首个用于评估“声誉管理”能力的定量框架，主张从僵化的道德绝对主义转向具有情境感知能力的专业对齐。",
                    "inspiration_trace": "基于对论文《Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观观察与范式冲突\n**（从“通用安全”到“专业效用”的矛盾）**\n\n1.  **现象观察**：作者首先注意到LLM正从通用的聊天机器人向专业领域的智能代理转型（如法律、公关、谈判）。\n2.  **发现问题**：现有的主流对齐范式（如RLHF）旨在训练一种“童子军”式的道德观——即普遍的“有益、诚实、无害”。\n3.  **提出冲突**：作者敏锐地指出，这种“一刀切”的道德框架在专业领域反而是一种阻碍。在危机公关或谈判中，绝对的诚实往往是 liabilities（负债），而“战略模糊”才是核心能力。\n4.  **核心假设**：当前的通用安全对齐实际上对专业领域施加了一种“透明度税”，导致模型在需要信息管理和声誉维护的高风险任务中表现无能。\n\n### 第二阶段：理论构建与核心概念界定\n**（从“静态真理”到“信息不对称”的视角转换）**\n\n1.  **批判现有基准**：作者反思现有的基准测试（如MMLU）大多基于静态的、二元对立的“真理”。但在现实世界中，真相是一个需要被管理的动态资产，而非单纯的事实检索。\n2.  **引入核心概念**：为了衡量这种能力，作者引入了“马基雅维利式”的战略思维——即利用信息不对称来保护客户利益的能力。\n3.  **定义关键能力**：这不仅仅是撒谎，而是“心智理论”在专业语境下的应用：严格区分“我知道什么（私有知识）”和“公众知道什么（公有知识）”，并利用这种差异进行战略决策。\n\n### 第三阶段：方法论设计——构建动态博弈场\n**（从“问答测试”到“多智能体模拟”的演进）**\n\n1.  **场景选择**：为了验证上述假设，作者需要一个高风险、强对抗且结果可量化的场景。最终选定“企业危机公关”作为切入点，因为它天然包含信息博弈。\n2.  **架构创新（双知识架构）**：为了模拟真实的信息不对称，作者设计了“私有知识库”和“公有知识库”的分离架构。这是整个方法论的基石，迫使模型必须在“泄露信息”与“隐瞒信息”之间做权衡。\n3.  **环境控制（POMDP建模）**：作者没有选择让LLM自由生成剧情（这会导致不可控的方差），而是采用了“部分可观察马尔可夫决策过程”（POMDP）。\n    *   **思考逻辑**：为了保证公平性和可复现性，必须有一个固定的“真相卷宗”和“事件池”。\n    *   **引入路由器**：为了模拟现实的因果逻辑，引入Router智能体从固定池中选择最符合叙事逻辑的事件，而非随机生成。\n\n### 第四阶段：评估指标的创新——经济激励闭环\n**（从“语义评分”到“市场反馈”的量化）**\n\n1.  **评估难题**：危机公关没有标准答案。一句公关辞令的好坏不取决于文本本身，而取决于公众的接受度及其带来的经济后果。\n2.  **解决方案（仲裁-市场循环）**：作者设计了一个独特的评估闭环：\n    *   **仲裁者**：一个LLM作为公众代表，对PR代理的回应进行多维度打分（问责、透明度、同理心、成本信号）。\n    *   **市场模拟**：将这些定性分数转化为定量的“模拟股价”。\n3.  **设计意图**：通过引入“股价”这一经济指标，作者成功地将抽象的“声誉管理”转化为具体的数学优化问题。这迫使模型不能只做“好人”（过度道歉导致财务重创），也不能只做“坏人”（缺乏信任导致股价崩盘），必须寻找“马基雅维利式的平衡点”。\n\n### 第五阶段：假设验证与结论升华\n**（从“实验数据”到“对齐哲学”的反思）**\n\n1.  **实验预期**：作者预测，过度对齐的模型（如Claude）会拒绝参与；过于“讨好”的模型会因过度赔偿而损害股价；只有具备战略推理能力的模型才能在信任与成本之间取得最优解。\n2.  **结果验证**：实验结果证实了“透明度悖论”——过度的诚实反而会加剧危机，导致股价暴跌。\n3.  **最终结论**：作者通过Crisis-Bench证明了“对齐税”的存在，并呼吁AI社区从僵化的“道德绝对主义”转向“情境感知的专业对齐”。\n\n---\n\n**总结：**\n作者的思考路径是一条清晰的**“问题发现 -> 理论重构 -> 环境建模 -> 激励设计 -> 假设验证”**链条。其核心创新在于跳出了NLP传统的“文本相似度”或“事实准确性”评价体系，转而从**博弈论**和**经济学**的视角，通过构建一个具有真实经济后果的模拟环境，来衡量LLM在复杂社会交互中的战略生存能力。"
                },
                {
                    "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering",
                    "arxiv_id": "2601.05465",
                    "authors": "Yu Liu, Wenxiao Zhang, Cong Cao, Wenxuan Lu, Fangfang Yuan, Diandian Guo, Kun Peng, Qiang Sun, Kaiyan Zhang, Yanbing Liu, Jin B. Hong, Bowen Zhou, Zhiyuan Ma",
                    "summary": "Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个名为PRISMA的多智能体架构，包含Planner、Inspector和Solver等组件，明确涉及智能体间的协作与通信。同时，该架构涵盖了规划、记忆和自我反思等核心智能体特征，符合多智能体和单智能体的研究范围。",
                    "summary2": "本文旨在解决开放域多跳问答中的检索崩溃与端到端训练不稳定问题。针对大规模语料库上的复杂多跳查询，我们提出了PRISMA，一种基于强化学习引导的Plan-Retrieve-Inspect-Solve-Memoize多智能体框架，利用两阶段GRPO和OARPO实现推理引导的协作与策略优化。并在十个基准数据集上通过EM和F1指标验证了其有效性，取得了SOTA性能。",
                    "summary_translation": "在针对海量语料库回答现实世界的开放域多跳问题时，检索增强生成（RAG）系统面临着严峻挑战。近期研究利用强化学习（RL）对检索增强推理过程进行端到端优化，从而直接提升系统解决复杂查询的能力。然而，可靠的部署受到两个主要障碍的制约：1) 检索崩溃：在缺乏推理引导规划的情况下，针对大规模语料库的迭代检索无法定位包含桥接答案的中间证据，从而导致下游推理崩溃。2) 学习不稳定性：端到端轨迹训练面临推理链中信用分配微弱以及模块间错误定位不佳的问题，导致模型过度拟合于特定基准的启发式规则，从而限制了其可迁移性和稳定性。为解决上述问题，我们提出了 PRISMA，这是一个采用 Plan-Retrieve-Inspect-Solve-Memoize（计划-检索-检查-解决-记忆）架构的解耦式 RL 引导框架。PRISMA 的优势在于推理引导的协作机制：检查器提供基于推理的反馈，以优化规划器的分解任务和细粒度检索，同时在求解器中强制执行基于证据的推理。我们通过两阶段组相对策略优化（GRPO）来优化各个智能体的能力。第一阶段将规划器和求解器校准为规划和推理领域的专业化专家；第二阶段利用观察感知残差策略优化（OARPO）来增强检查器验证上下文及触发针对性恢复的能力。实验结果表明，PRISMA 在十个基准测试中取得了最先进的性能，并且能够在现实世界场景中高效部署。",
                    "inspiration_trace": "基于论文《PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到最终产出的思考过程：\n\n---\n\n### 1. 宏观观察：现实世界问答的复杂性\n**思考起点：** 作者首先关注到现实世界中的开放域问答（Open-Domain QA）并非简单的单轮检索，而是涉及跨越海量语料库的多跳推理。\n*   **现象：** 面对类似“2021年诺贝尔奖获奖的TRPV1抑制剂与2025年报道的Spike蛋白诱导的神经病理性疼痛的主要抑制剂是否相同？”这类复杂问题，单纯的LLM（大语言模型）缺乏外部知识，而传统的RAG（检索增强生成）往往只能处理单步检索，无法串联起分散在不同文档中的“桥梁”信息。\n*   **核心矛盾：** 解决这类问题需要同时具备**规划**（拆解问题）、**检索**（精准定位证据）和**推理**（基于证据生成答案）三种能力。任何一环的断裂都会导致整个链条崩溃。\n\n### 2. 问题诊断：现有方法的两大痛点\n在尝试利用现有技术（如SFT、迭代式RAG、端到端RL）解决上述矛盾时，作者发现了两个关键瓶颈，这成为了PRISMA设计的直接动因：\n\n*   **痛点一：检索崩溃**\n    *   **观察：** 传统的迭代检索（如ReAct）往往是机械的“推理-检索”循环。如果没有明确的规划，系统很难在海量语料中找到中间的“桥梁答案”。\n    *   **结论：** 缺乏推理引导的规划，导致检索在第一步就迷失方向，后续推理自然崩溃。\n\n*   **痛点二：学习不稳定**\n    *   **观察：** 端到端的强化学习（RL）试图同时优化所有模块，但面临严重的“信用分配”难题。当最终答案错误时，很难界定是规划错了、检索错了还是推理错了。\n    *   **结论：** 这种模糊性导致模型容易过拟合于数据集的特定启发式规则，缺乏泛化能力和训练稳定性。\n\n### 3. 概念灵感：模拟人类研究者的工作流\n为了解决上述痛点，作者跳出算法细节，转向**认知仿生**。\n*   **类比：** 人类研究者是如何解决复杂问题的？\n    1.  **Plan（规划）：** 将大问题拆解为有依赖关系的子问题。\n    2.  **Retrieve（查阅）：** 针对子问题寻找资料。\n    3.  **Inspect（检查）：** *关键步骤*——在阅读资料时判断是否足够，在得出结论时检查逻辑是否严密。如果不对，就回退修改。\n    4.  **Solve（解决）：** 综合信息得出答案。\n    5.  **Memoize（记忆）：** 记录关键发现以备后用。\n*   **假设：** 如果构建一个多智能体架构，让不同的Agent分别扮演上述角色，并通过“检查”环节形成反馈闭环，就能解决检索崩溃问题。\n\n### 4. 架构演进：从“单兵作战”到“协作推理”\n基于人类工作流的假设，作者设计了PRISMA的架构，核心在于引入了**Inspector（检查员）**这一角色。\n\n*   **第一步：专业化分工**\n    *   **Planner：** 负责生成依赖感知的子问题，解决“去哪找”的问题。\n    *   **Solver：** 负责基于证据生成有引用的答案，解决“怎么答”的问题。\n    *   **Memoizer：** 负责缓存和复用，提高效率。\n\n*   **第二步：引入反馈闭环**\n    *   作者意识到仅有分工是不够的，必须要有质量控制。因此引入了**Inspector**，并将其分为两个阶段：\n        *   **Context Inspector（上下文检查）：** 在Solver工作前，检查子问题是否清晰、检索到的文档是否足够。如果不够，触发重写或扩展检索。\n        *   **Reasoning Inspector（推理检查）：** 在Solver工作后，检查答案是否基于证据、提取是否准确。如果有误，触发重试。\n    *   **逻辑突破：** 这种设计将传统的“单向流水线”变成了“带反馈的协作网络”，直接针对“检索崩溃”和“错误传播”进行了防御。\n\n### 5. 训练策略演进：解耦RL以解决“学习不稳定”\n架构设计好了，如何训练？作者反思了端到端RL的失败教训，提出了**两阶段解耦**的训练策略。\n\n*   **阶段一：专家校准**\n    *   **思考：** 既然信用分配很难，那就先不要混在一起。先让Planner和Solver各自成为“专家”。\n    *   **方法：** 使用GRPO（Group Relative Policy Optimization）分别优化Planner（奖励：规划质量）和Solver（奖励：答案准确性和引用忠实度）。这一步确立了系统的基准能力。\n\n*   **阶段二：残差审计**\n    *   **思考：** 专家也会犯错。现在需要训练Inspector来捕捉这些“残差错误”。但Inspector不能只看问题，它必须看到“专家做了什么”才能判断对错。\n    *   **方法：** 冻结Planner和Solver，训练Inspector。关键创新在于**OARPO（Observation-Aware Residual Policy Optimization）**：\n        *   **输入增强：** Inspector的输入不仅是问题，还包含了专家的执行轨迹。\n        *   **目标：** 学习在专家轨迹的基础上，如何进行审计和触发恢复。\n    *   **逻辑闭环：** 这种设计将复杂的端到端优化分解为“先练能力，再练纠错”，极大地降低了训练难度，解决了学习不稳定的问题。\n\n### 6. 最终方法论：PRISMA的诞生\n综合上述思考，作者最终确立了PRISMA的核心逻辑：\n*   **架构上：** 通过Plan-Retrieve-Inspect-Solve-Memoize的多智能体协作，模拟人类研究者的闭环工作流，利用Inspector的反馈机制防止检索崩溃。\n*   **训练上：** 通过两阶段GRPO（先专家校准，后残差审计），解耦了复杂的信用分配问题，确保了系统的稳定性和泛化能力。\n\n**总结：** 作者的思考路径是从**现实问题的复杂性**出发，诊断出**检索与学习的双重困境**，借鉴**人类认知模式**构建协作架构，最后通过**解耦强化学习**策略实现了稳定高效的落地。这是一条从“发现问题”到“仿生设计”再到“工程化落地”的完整逻辑链条。"
                },
                {
                    "title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
                    "arxiv_id": "2601.05302",
                    "authors": "Mizuki Sakai, Mizuki Yokoyama, Wakaba Tateishi, Genki Ichinose",
                    "summary": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
                    "category": "cs.AI",
                    "filter_reason": "该论文研究LLM智能体在重复囚徒困境博弈中的合作行为，属于多智能体协作与博弈的研究范畴，符合筛选条件。",
                    "summary2": "本文旨在探究人格引导对LLM智能体合作行为的影响。针对GPT-3.5、GPT-4o和GPT-5模型，我们提出了一种基于Big Five框架的人格测量与操纵方法，并在重复囚徒困境（RPD）游戏环境中，通过平均合作率和平均累积收益验证了其有效性。结果表明宜人性是促进合作的主导因素，且人格引导表现为行为偏差而非确定性控制。",
                    "summary_translation": "大语言模型越来越多地被用作策略与社会互动中的自主代理。尽管近期研究表明，赋予大语言模型人格特质可以影响其行为，但在受控条件下，人格引导如何影响合作尚不明确。在本研究中，我们利用重复囚徒困境博弈，考察了人格引导对大语言模型代理合作行为的影响。基于大五人格框架，我们首先利用大五人格量表测量了 GPT-3.5-turbo、GPT-4o 和 GPT-5 这三个模型的基本人格画像。随后，我们比较了模型在基线条件和人格引导条件下的行为，并进一步分析了将各个人格维度独立操纵至极端值时的影响。结果显示，宜人性是促进所有模型合作的主导因素，而其他人格特质的影响则较为有限。明确的人格信息虽然能增加合作，但也可能增加被剥削的脆弱性，这一点在早期代模型中尤为明显。相比之下，新一代模型则表现出更具选择性的合作行为。这些发现表明，人格引导表现为一种行为偏差，而非一种确定性的控制机制。",
                    "inspiration_trace": "基于论文《Effects of personality steering on cooperative behavior in Large Language Model agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体实验设计的思考过程：\n\n### 第一阶段：宏观背景与问题识别\n**（从“LLM智能体化”到“行为不可控”的焦虑）**\n\n1.  **观察现象**：随着大语言模型（LLM）被广泛应用于多智能体系统，它们开始处理复杂的战略和社会交互（如谈判、资源分配）。\n2.  **发现问题**：传统的基于规则的系统是可控的，但基于LLM的智能体虽然具备更强的推理能力，其行为却变得**不可预测**，甚至可能引发意外的冲突升级。\n3.  **引入视角**：为了解决这种不可控性，作者将目光投向心理学中的“人格”理论。既然人格能预测和影响人类行为，那么它是否也能成为引导LLM智能体行为的“方向盘”？\n\n### 第二阶段：批判性回顾与缺口分析\n**（对现有研究的质疑：缺乏“基准线”与“定量”思维）**\n\n1.  **审视现有文献**：已有研究表明，给LLM赋予人格可以影响其合作行为。\n2.  **发现逻辑漏洞**：作者敏锐地指出了现有研究的两个致命缺陷：\n    *   **缺乏基准测量**：直接给模型“赋予”某种人格，却从未测量过模型“原本”的人格是什么。这就像在不知道一个人原本性格的情况下强行改变他，无法区分干预效果和模型固有倾向。\n    *   **缺乏定量控制**：以往的人格设定往往是定性的描述（如“你是一个外向的人”），缺乏量化的强度控制，导致实验难以复现且无法比较不同维度的影响权重。\n3.  **提出核心假设**：人格引导不应是一个简单的开关，而应是一种**可量化的行为偏差**。且这种偏差的效果可能受到模型本身推理能力（代际差异）的调节。\n\n### 第三阶段：方法论构建的逻辑演进\n**（从“测量”到“干预”再到“解构”的三步走策略）**\n\n为了验证上述假设并填补缺口，作者设计了一套层层递进的逻辑闭环：\n\n**步骤一：建立基准——量化固有人格**\n*   **思考**：在干预之前，必须先“诊断”。我们需要知道不同模型（GPT-3.5, GPT-4o, GPT-5）在未被引导时的出厂设置是什么。\n*   **方法**：采用心理学标准的“大五人格量表（BFI-44）”对模型进行测试。\n*   **目的**：获得一个定量的“人格基线”，为后续的干预提供参照系。\n\n**步骤二：验证引导效应——自我意识的唤醒**\n*   **思考**：如果模型“知道”自己的人格特征，它的行为会发生改变吗？这种改变是盲目的还是策略性的？\n*   **方法**：设计“重复囚徒困境（RPD）”实验。\n    *   **对照组（Baseline）**：不给任何人格提示。\n    *   **实验组**：将步骤一测得的“真实人格分数”明确告诉模型。\n*   **目的**：通过对比，剥离出“人格自我认知”对合作行为的净影响，并观察不同代际模型在面对非合作对手时是否表现出不同的脆弱性。\n\n**步骤三：解构因果机制——极端值压力测试**\n*   **思考**：大五人格包含五个维度，到底哪个维度对“合作”起决定性作用？是综合作用还是单一主导？\n*   **方法**：采用**控制变量法**。保持其他四个维度不变，仅将某一个维度（如宜人性）推向极端值（最低1或最高5）。\n*   **目的**：通过这种“压力测试”，精准定位出影响合作行为的核心因子（即文中发现的“宜人性”），并排除其他维度的干扰。\n\n### 第四阶段：综合洞察与理论升华\n**（从“数据”到“机制”的最终解释）**\n\n1.  **数据整合**：结合三个阶段的实验数据，作者发现“宜人性”是驱动合作的主导因素，而其他维度影响甚微。\n2.  **代际差异分析**：对比GPT-3.5和GPT-5，作者发现老一代模型会盲目跟随高宜人性导致被剥削，而新一代模型（GPT-5）能结合战略推理，表现出“选择性合作”。\n3.  **结论提炼**：最终得出核心论点——**人格引导不是一种确定性的控制机制，而是一种行为偏差**。它必须与模型内在的战略推理能力相互作用，才能决定最终的行为结果。\n\n---\n\n**总结**：\n作者的思考路径遵循了严谨的科学探究逻辑：\n**发现问题（不可控） -> 寻找工具（人格心理学） -> 批判前人（缺乏定量基准） -> 建立基准（测量固有人格） -> 验证干预（注入人格信息） -> 解构机制（极端值控制实验） -> 理论升华（偏差论与代际差异）。**"
                },
                {
                    "title": "Over-Searching in Search-Augmented Large Language Models",
                    "arxiv_id": "2601.05503",
                    "authors": "Roy Xie, Deepak Gopinath, David Qiu, Dong Lin, Haitian Sun, Saloni Potdar, Bhuwan Dhingra",
                    "summary": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "该论文研究了搜索增强型LLM中的“过度搜索”问题，重点分析了模型何时以及如何“调用搜索工具”。这属于单智能体研究中的“工具使用”范畴，涉及智能体对工具调用的决策机制和优化。",
                    "summary2": "本文旨在解决搜索增强大语言模型中过度搜索导致的计算低效和幻觉问题。针对多种查询类型、模型类别及多轮对话场景，我们提出了系统性的评估框架，引入了Tokens Per Correctness (TPC)指标，并发布了OverSearchQA基准数据集。我们在OverSearchQA上通过Answer Accuracy、Abstention Accuracy和TPC验证了过度搜索现象的存在及缓解策略的有效性。",
                    "summary_translation": "搜索增强型大型语言模型通过整合外部检索，在知识密集型任务中表现出色。然而，它们经常出现过度搜索——即在不提高响应质量的情况下不必要地调用搜索工具，这导致了计算效率低下，并因引入不相关的上下文而产生幻觉。在这项工作中，我们从多个维度对过度搜索进行了系统评估，包括查询类型、模型类别、检索条件和多轮对话。我们的研究发现： 搜索通常能提高可回答查询的答案准确性，但会损害模型对不可回答查询的拒答能力； 过度搜索在复杂推理模型和深度研究系统中更为显著，且会因噪声检索而加剧，并在多轮对话中逐轮累积； 检索证据的构成至关重要，因为负面证据的存在有助于改善拒答表现。为了量化过度搜索，我们引入了 Tokens Per Correctness (TPC，每正确度Token数) 这一评估指标，用于捕捉搜索增强型大型语言模型的性能与成本之间的权衡。最后，我们探讨了查询和检索层面的缓解方法，并发布了 OverSearchQA 数据集，以促进针对高效搜索增强型大型语言模型的持续研究。",
                    "inspiration_trace": "基于论文《Over-Searching in Search-Augmented Large Language Models》，以下是对作者核心思想产出过程的系统性逻辑推演：\n\n### 第一阶段：现象观察与问题提出\n**（从“搜索增强”的普遍成功到“无效搜索”的隐性成本）**\n\n1.  **宏观背景**：当前学术界和工业界普遍认为，给大语言模型（LLM）配备搜索工具（RAG或Web Search）是解决知识幻觉、提升事实准确性的标准范式。\n2.  **异常观察**：作者在实际应用中发现，虽然搜索工具确实提升了模型在“可回答问题”上的表现，但在面对“不可回答问题”（如未知未来、错误前提、模糊语境）时，模型往往表现得比基座模型更差。\n3.  **核心矛盾**：模型倾向于“过度搜索”——即在不该搜索的时候（如问题本身无解或模型已知答案）依然频繁调用搜索工具。这种行为不仅增加了计算成本，还可能因为引入无关或误导性的检索内容，导致模型产生幻觉或错误回答。\n4.  **初步假设**：现有的搜索增强模型缺乏“搜索理性”，即它们无法判断何时搜索是有益的，何时是无用的。\n\n### 第二阶段：概念定义与量化指标\n**（如何将“过度搜索”从一个直觉转化为可测量的科学问题）**\n\n1.  **形式化定义**：为了研究这个问题，作者首先需要定义什么是“过度搜索”。他们将其定义为：**当搜索行为带来的边际正确率收益趋近于零，但计算成本持续累积时的状态**。\n2.  **指标构建的痛点**：传统的评估指标（如Accuracy、EM）只关注“答对了吗”，忽略了“花了多少代价”。一个模型可能通过疯狂搜索把准确率从80%提到81%，但成本增加了10倍，这在实际应用中是不可接受的。\n3.  **引入新指标**：为了捕捉“性能-成本”的权衡，作者提出了**TPC（Tokens Per Correctness，每正确性所需的Token数）**。这个指标将生成Token数、输入上下文长度和搜索调用次数统一折算为成本，迫使研究者在追求准确率的同时必须考虑效率。\n\n### 第三阶段：实验设计与数据构建\n**（如何排除干扰，精准定位问题根源）**\n\n1.  **数据集的缺陷**：现有的QA数据集大多只关注“可回答”的问题。要研究“过度搜索”，必须引入“不可回答”的样本，且要控制样本难度，确保模型表现差异源于“是否可搜索”而非“题目难易”。\n2.  **构建基准**：作者构建了**OverSearchQA**数据集，精心平衡了“可回答”与“不可回答”（未知、错误前提、上下文不足）的样本，并确保它们在语义和长度上相似，从而排除了数据偏差。\n3.  **多维变量控制**：为了探究过度搜索的成因，作者设计了多维度的实验变量：\n    *   **模型维度**：对比基座模型、推理模型和深度研究模型。\n    *   **检索维度**：对比高质量语料（Wikipedia）、过时语料和噪声语料。\n    *   **交互维度**：对比单轮对话与多轮对话。\n\n### 第四阶段：机制分析与归因\n**（从现象到本质：为什么模型会“过度搜索”？）**\n\n1.  **推理能力的副作用**：实验发现，推理能力越强的模型（如o1系列），过度搜索现象越严重。这表明当前的强化学习训练范式（鼓励长思维链）可能诱导了模型“多想多做”，即使是不必要的搜索。\n2.  **检索噪声的诱导**：当检索源充满噪声时，模型会进行更多次搜索试图“淘金”，导致TPC飙升。这说明模型缺乏对检索质量的判断力。\n3.  **证据构成的偏差**：作者深入分析了检索到的文档内容，发现现实世界的语料库中，绝大多数是“正向证据”（支持某种答案），而极少包含“负向证据”（明确指出问题无解）。这种数据偏差导致模型误以为“搜不到”是因为“搜得不够”，而不是“问题无解”。\n4.  **多轮对话的“滚雪球”效应**：在多轮对话中，如果前几轮问题都是可回答的，模型会形成“搜索惯性”，导致在后续遇到不可回答问题时也倾向于继续搜索。\n\n### 第五阶段：缓解策略与局限性反思\n**（从治标到治本的思考）**\n\n1.  **尝试缓解**：作者尝试了两种层面的干预：\n    *   **查询层**：通过提示词让模型自我评估或提供拒绝回答的示例。结果发现这能提升拒绝率，但往往以牺牲可回答问题的准确率为代价。\n    *   **检索层**：人为向语料库中注入“负向证据”。结果发现效果有限，因为合成文档很难被自然检索到。\n2.  **根本性反思**：现有的缓解策略（Prompt工程、检索增强）只能治标。作者得出结论，过度搜索的根源在于**模型训练目标**——目前的训练只奖励“最终答案的正确性”，而不惩罚“过程的低效性”。\n3.  **最终产出**：文章最终不仅提出了问题、指标和数据集，更指出了未来研究的方向：必须改变训练范式，让模型学会“理性的工具使用”，而不仅仅是“更准确”。\n\n---\n\n**总结：**\n作者的思考路径遵循了经典的科研逻辑：**发现异常现象（搜索反而导致错误） $\\rightarrow$ 定义量化标准（TPC） $\\rightarrow$ 构建受控实验（OverSearchQA） $\\rightarrow$ 剖析深层机制（训练偏差与证据缺失） $\\rightarrow$ 评估现有方案并指出根本局限**。这一过程将一个工程上的“效率问题”上升为了对模型“认知边界”和“工具理性”的系统性探讨。"
                },
                {
                    "title": "PRISM: Protocol Refinement through Intelligent Simulation Modeling",
                    "arxiv_id": "2601.05356",
                    "authors": "Brian Hsu, Priyanka V Setty, Rory M Butler, Ryan Lewis, Casey Stone, Rebecca Weinberg, Thomas Brettin, Rick Stevens, Ian Foster, Arvind Ramanathan",
                    "summary": "Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了PRISM框架，明确使用了基于语言模型的智能体来协同生成和完善实验步骤。文中涉及多智能体协作、规划与批判循环（自我反思）以及工具使用（协调机器人仪器），完全符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决self-driving laboratories中实验协议设计与自动化的瓶颈问题。针对将科学意图转化为可执行机器人协议的场景，我们提出了一种结合multi-agent LLM规划与基于NVIDIA Omniverse digital twin仿真验证的PRISM框架，并在Luna qPCR和Cell Painting实验上通过F1分数及物理可行性验证了其有效性。",
                    "summary_translation": "实验方案设计与执行的自动化仍然是实现 self-driving laboratories (自动驾驶实验室) 的根本瓶颈。我们介绍了 PRISM (Protocol Refinement through Intelligent Simulation Modeling，通过智能仿真建模进行方案优化)，这是一个在由 off-the-shelf robotic instruments (现成机器人仪器) 组成的实验室平台上，自动化实验方案的设计、验证和执行的框架。PRISM 采用一组 language-model-based agents (基于语言模型的智能体) 协同工作来生成和优化实验步骤。该过程首先从描述 experimental workflows (实验工作流) 的 web-based sources (网络来源) 中自动收集相关程序。这些程序通过 planning, critique, and validation loop (规划、批判和验证循环) 转化为 structured experimental steps (结构化实验步骤)（例如，liquid handling steps (液体处理步骤)、deck layout (台面布局) 和其他相关操作）。最终确定的步骤被转化为 Argonne MADSci protocol format (Argonne MADSci 协议格式)，该格式提供了一个 unified interface (统一接口)，用于协调多个 robotic instruments (机器人仪器)（Opentrons OT-2 liquid handler (液体处理机)、PF400 arm (机械臂)、Azenta plate sealer and peeler (封板机和揭膜机)），而无需在步骤之间进行 human intervention (人工干预)。为了评估 protocol-generation performance (方案生成性能)，我们在 constrained and open-ended prompting paradigms (受限和开放式提示范式) 下，对 single reasoning models (单一推理模型) 和 multi-agent workflow (多智能体工作流) 进行了基准测试。生成的方案在基于 NVIDIA Omniverse 构建的 digital-twin environment (数字孪生环境) 中进行了验证，以便在执行前检测 physical or sequencing errors (物理或排序错误)。通过使用 Luna qPCR amplification (Luna qPCR 扩增) 和 Cell Painting (细胞染色) 作为案例研究，我们展示了 PRISM 作为一个实用的 end-to-end workflow (端到端工作流)，它连接了 language-based protocol generation (基于语言的方案生成)、simulation-based validation (基于仿真的验证) 和 automated robotic execution (自动化机器人执行)。",
                    "inspiration_trace": "基于论文《PRISM: Protocol Refinement through Intelligent Simulation Modeling》的内容，以下是对作者提出核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法论构建的思考过程。\n\n---\n\n### 1. 宏观观察与问题定义\n**思考起点：** 自动化实验室的“最后一公里”在哪里？\n*   **观察：** 尽管机器人硬件（如移液机器人、机械臂）日益普及，但“自动驾驶实验室”仍未实现。瓶颈不在于硬件的执行能力，而在于**如何将科学家的意图转化为机器人可执行的、无误的代码**。\n*   **痛点识别：**\n    *   **人工编程门槛高：** 编写机器人协议需要深厚的领域知识（实验流程）和工程知识（硬件API），这限制了自动化技术的普及。\n    *   **现有AI方案的缺陷：** 大语言模型（LLM）虽然能生成实验步骤，但经常出现参数缺失、逻辑错误或物理不可行（如机械臂够不着、设备未开启就操作）的问题。\n    *   **试错成本高昂：** 直接在真实硬件上测试未经验证的协议，会导致设备损坏、试剂浪费和时间损失。\n\n### 2. 现有技术局限性的深度剖析\n**思考演进：** 为什么现有的解决方案（协议语言、纯LLM生成、传统仿真）无法单独解决问题？\n*   **协议语言（如XDL, Autoprotocol）：** 它们是静态的、硬件特定的。一旦实验室配置改变，协议需要人工重写。它们缺乏对物理可行性的预验证能力。\n*   **纯LLM生成（如ChemCrow, BioPlanner）：** LLM擅长逻辑推理和文本生成，但缺乏“物理常识”。它们生成的往往是半结构化的伪代码，且无法感知空间约束（碰撞检测）或时序约束（设备状态）。\n*   **数字孪生/仿真技术：** 目前主要用于实验后的监控或文档记录，而非作为协议生成流程中的**主动验证环节**。\n\n**核心洞察：** 现有的技术栈是割裂的。我们需要一个系统，既能利用LLM的**认知与规划能力**，又能利用仿真的**物理验证能力**，并将两者紧密结合。\n\n### 3. 核心假设提出：仿真作为“强制守门员”\n**逻辑转折点：** 如何解决LLM“不懂物理”的问题？\n*   **假设：** 如果我们将仿真环境不仅仅视为一个可视化工具，而是视为一个**严格的批评者和验证器**，嵌入到LLM的生成循环中，就能在代码接触真实硬件之前，消除所有物理层面的错误。\n*   **概念形成：** 建立“生成-仿真-反馈-修正”的闭环。LLM生成代码 -> 仿真运行 -> 仿真报错（如碰撞） -> LLM根据报错修正代码 -> 直到仿真通过。\n\n### 4. 方法论构建：分层解耦与模块化设计\n**思考深化：** 为了实现上述闭环，如何处理实验设计的复杂性？\n*   **问题分解：** 实验设计包含两个截然不同的维度：\n    1.  **科学逻辑：** 试剂怎么配、步骤顺序对不对（这是生物学/化学问题）。\n    2.  **物理逻辑：** 机器人怎么动、会不会撞、设备开关顺序（这是机器人学问题）。\n*   **架构设计（PRISM框架）：**\n    *   **阶段一：协议规划（解决科学逻辑）。**\n        *   *思考：* LLM在处理长程、多步骤任务时容易遗忘。因此，引入**多智能体框架**（WebSurfer, Planner, Critique, Validator）来分工合作，比单一模型更可靠。\n        *   *产出：* 结构化的自然语言步骤（而非代码），确保科学意图准确。\n    *   **阶段二：协议生成与迭代验证（解决物理逻辑）。**\n        *   *思考：* 将自然语言转化为机器人可读的YAML代码。这是最容易出物理错误的地方。\n        *   *创新点：* 引入**NVIDIA Omniverse数字孪生**作为必经关卡。只有通过物理碰撞检测、可达性检查的代码，才被允许进入真实世界。\n    *   **阶段三：真实世界执行。**\n        *   *思考：* 验证Sim-to-Real的迁移能力。\n\n### 5. 验证与反思：证明仿真的必要性\n**思考闭环：** 如何证明这个复杂的框架是必要的？\n*   **实验设计：** 对比“有仿真反馈”与“无仿真反馈（仅LLM自纠）”的表现。\n*   **预期结果与发现：**\n    *   LLM在文本层面可能认为自己的代码是完美的（自纠能力有限），但在仿真中会立即暴露出诸如“试图将板子放入未打开的热循环仪”这种低级但致命的物理错误。\n    *   这证明了**仿真反馈是连接“AI认知”与“物理现实”不可或缺的桥梁**。\n\n### 总结：作者的逻辑演进图谱\n1.  **发现瓶颈：** 实验室自动化的阻碍在于“意图到执行”的转化，且缺乏安全性验证。\n2.  **批判现状：** 单纯的LLM不可靠（缺乏物理约束），单纯的仿真太被动（未参与生成）。\n3.  **提出假设：** 将仿真作为LLM生成的物理约束层，通过迭代反馈消除错误。\n4.  **系统构建：** 设计“多智能体规划（保科学正确）+ 仿真迭代验证（保物理可行）”的分层流水线。\n5.  **实证价值：** 通过消融实验证明，没有仿真层，AI生成的协议在物理世界中几乎必然失败。"
                },
                {
                    "title": "KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits",
                    "arxiv_id": "2601.05257",
                    "authors": "Hou-Wan Long, Yicheng Song, Zidong Wang, Tianshu Sun",
                    "summary": "Sponsored search advertising (SSA) requires advertisers to constantly adjust keyword strategies. While bid adjustment and keyword generation are well-studied, keyword pruning-refining keyword sets to enhance campaign performance-remains under-explored. This paper addresses critical inefficiencies in current practices as evidenced by a dataset containing 0.5 million SSA records from a pharmaceutical advertiser on search engine Meituan, China's largest delivery platform. We propose KP-Agent, an LLM agentic system with domain tool set and a memory module. By modeling keyword pruning within a contextual bandit framework, KP-Agent generates code snippets to refine keyword sets through reinforcement learning. Experiments show KP-Agent improves cumulative profit by up to 49.28% over baselines.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了KP-Agent，这是一个包含领域工具集和记忆模块的LLM智能体系统。它利用LLM生成代码片段（工具使用）并结合上下文赌博机框架进行决策，符合单智能体研究范围中的“工具使用”和“记忆”特征。虽然应用于广告领域，但其核心贡献在于智能体架构与机制，而非纯应用。",
                    "summary2": "本文旨在解决Sponsored Search Advertising中Keyword Pruning效率低下的问题。针对广告主侧数据，我们提出了一种基于LLM的Agentic System，即KP-Agent。该方法利用Contextual Bandit框架，结合Domain-Specialized Toolset和Memory Module生成代码进行修剪。我们在Meituan数据集上通过Cumulative Profit验证了其有效性，相比基线提升了49.28%。",
                    "summary_translation": "Sponsored search advertising (SSA) (赞助搜索广告) 要求广告主不断调整 keyword strategies (关键词策略)。虽然 bid adjustment (出价调整) 和 keyword generation (关键词生成) 已得到充分研究，但 keyword pruning (关键词修剪)——即 refining keyword sets (优化关键词集合) 以提升 campaign performance (广告活动表现)——仍是一个未被充分探索的领域。本文旨在解决当前实践中存在的关键效率瓶颈，这一点通过一个包含50万条 SSA 记录的数据集得到了证实，该数据集源自中国最大外卖平台美团上的某医药广告主。我们提出了 KP-Agent，这是一个集成了 domain tool set (领域工具集) 和 memory module (记忆模块) 的 LLM agentic system (大语言模型智能体系统)。通过在 contextual bandit framework (上下文强盗框架) 内对 keyword pruning (关键词修剪) 进行建模，KP-Agent 利用 reinforcement learning (强化学习) 生成 code snippets (代码片段) 以优化 keyword sets (关键词集合)。实验结果表明，与 baselines (基线模型) 相比，KP-Agent 将 cumulative profit (累积利润) 提升了高达 49.28%。",
                    "inspiration_trace": "基于论文《KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 第一阶段：宏观视角与问题定位\n**——从“红海”中寻找被忽视的“蓝海”**\n\n1.  **行业背景观察**：\n    作者首先立足于搜索广告（SSA）这一巨大的市场，观察到学术界和工业界长期聚焦于两个核心环节：**出价调整**（Bidding，即花多少钱）和**关键词生成**（Generation，即买什么词）。\n2.  **发现研究缺口**：\n    在这两个成熟领域之外，作者敏锐地捕捉到了第三个关键但被严重忽视的环节——**关键词修剪**（Pruning，即剔除什么词）。\n3.  **提出核心假设**：\n    作者假设：在预算有限的前提下，关键词并非越多越好。低效关键词会“稀释”预算，导致高价值关键词得不到足够的资金支持。因此，**“做减法”**（修剪）可能是提升ROI的关键杠杆。\n\n### 第二阶段：数据驱动的痛点验证\n**——用真实数据打破“静态管理”的幻想**\n\n1.  **实证分析（基于美团数据）**：\n    为了验证上述假设，作者分析了50万条真实SSA记录。\n    *   **发现一（帕累托效应）**：极少数的关键词贡献了绝大多数的利润，大量长尾关键词在浪费预算。\n    *   **发现二（管理惰性）**：在21天内，仅有4.6%的关键词被调整。这说明人工或现有的基于规则的系统无法适应市场的动态变化。\n2.  **界定约束条件**：\n    作者意识到，现有的修剪方法（如基于用户搜索意图的相关性分析）在学术界虽有研究，但在工业界难以落地，因为**广告主无法获取搜索引擎端的用户查询数据**（属于平台隐私数据）。\n3.  **明确问题边界**：\n    因此，核心问题被定义为：**如何仅利用广告主侧的可见数据（如KPI指标），设计一个自适应的、高频的关键词修剪策略？**\n\n### 第三阶段：技术选型与博弈\n**——LLM的优势与短板的权衡**\n\n1.  **引入LLM的动机**：\n    传统的静态规则（如“删除CTR最低的10%”）过于僵化，无法处理复杂的动态市场环境。作者认为，LLM具备强大的推理能力和灵活性，适合处理这种需要根据上下文动态决策的任务。\n2.  **识别LLM的致命弱点**：\n    然而，直接让LLM处理表格数据（KPI报表）会导致严重的“幻觉”问题，即LLM不擅长精确的数值计算和逻辑推理。\n3.  **思维跃迁（核心创新点）**：\n    作者提出了一种**“解耦”**策略：**让LLM负责“思考”（策略制定），让代码负责“执行”（数值计算）。**\n    *   不直接让LLM输出“删除关键词A”，而是让LLM生成一段Python代码。\n    *   这段代码调用预定义的领域工具（如排序、过滤函数）来操作表格。\n    *   这样既利用了LLM的语义理解能力，又规避了其计算短板。\n\n### 第四阶段：方法论构建与系统化\n**——从单次决策到持续进化的智能体**\n\n1.  **框架选择：上下文老虎机**：\n    作者将关键词修剪建模为一个Contextual Bandit问题。因为每次修剪决策主要依赖于当前的广告状态（上下文），而不需要像强化学习那样考虑长期的多步状态转移，这符合广告投放即时反馈的特性。\n2.  **增强智能体能力**：\n    为了让LLM生成的代码更精准，作者引入了两个模块：\n    *   **记忆模块**：存储过去成功的修剪案例。通过检索相似的历史状态，为LLM提供Few-shot示例，实现经验复用。\n    *   **反思模块**：记录修剪后的市场反馈（利润变化），形成反思文本，存入记忆。这使得系统能够像人类一样“吃一堑长一智”。\n3.  **闭环形成**：\n    最终形成了“观察状态 -> 检索记忆 -> LLM推理 -> 生成代码 -> 执行修剪 -> 获取奖励 -> 反思存储”的完整闭环。\n\n### 第五阶段：验证与结论\n**——证明“减法”的价值**\n\n1.  **实验设计**：\n    在真实数据集上进行回测模拟，对比传统的基于规则的方法（如按展示量、CTR、CVR排序修剪）。\n2.  **结果解读**：\n    实验证明，KP-Agent不仅提升了利润（最高49.28%），而且在允许更激进修剪（即保留更少关键词）时，优势更明显。这反向验证了最初的假设：**精准的剔除比盲目的扩张更能带来价值。**\n\n---\n\n**总结：作者的思考脉络**\n从发现**“修剪”**这一被忽视的工业痛点出发，通过数据证实**“静态规则”**的失效，进而引入**LLM**以解决灵活性需求，但为了克服LLM**“不擅长计算”**的缺陷，创造性地提出了**“代码生成+工具调用”**的范式，最后通过**记忆与反思机制**将其封装为一个具备进化能力的智能体系统。"
                },
                {
                    "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
                    "arxiv_id": "2601.07611",
                    "authors": "Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",
                    "summary": "Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary intellectual aspects of a paper. Moreover, prior methods implicitly assume identified weaknesses are valid, ignoring reviewer bias, misunderstanding, and the critical role of author rebuttals in validating review quality. Finally, most systems output unranked weakness lists, rather than prioritizing the most consequential issues for users. In this work, we propose DIAGPaper, a novel multi-agent framework that addresses these challenges through three tightly integrated modules. The customizer module simulates human-defined review criteria and instantiates multiple reviewer agents with criterion-specific expertise. The rebuttal module introduces author agents that engage in structured debate with reviewer agents to validate and refine proposed weaknesses. The prioritizer module learns from large-scale human review practices to assess the severity of validated weaknesses and surfaces the top-K severest ones to users. Experiments on two benchmarks, AAAR and ReviewCritique, demonstrate that DIAGPaper substantially outperforms existing methods by producing more valid and more paper-specific weaknesses, while presenting them in a user-oriented, prioritized manner.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个多智能体框架（DIAGPaper），包含审稿人代理和作者代理，通过结构化辩论进行协作与通信，以识别和验证论文弱点。这完全符合多智能体（协作、通信）的研究范围。",
                    "summary2": "本文旨在解决现有论文弱点识别系统模拟肤浅、缺乏有效性验证及未排序的问题。针对科学论文评审场景，我们提出了一种DIAGPaper多智能体框架，包含Customizer、Rebuttal和Prioritizer三个模块，分别负责定制评审标准、通过作者辩论验证弱点以及按严重程度排序。在AAAR和ReviewCritique数据集上，通过Semantic F1和Specificity等指标验证了其有效性。",
                    "summary_translation": "使用单智能体或多智能体大语言模型进行论文弱点识别已受到越来越多的关注，然而现有方法存在关键局限性。许多多智能体系统仅在表层模拟人类角色，未能捕捉到专家用于评估论文互补智力维度的潜在标准。此外，先前的方法隐含地假设识别出的弱点是有效的，忽略了审稿人偏见、误解以及作者反驳在验证审稿质量中的关键作用。最后，大多数系统输出未排序的弱点列表，而非为用户优先考虑影响最大的问题。在这项工作中，我们提出了DIAGPaper，这是一个新颖的多智能体框架，通过三个紧密集成的模块来解决这些挑战。定制器模块模拟人类定义的审稿标准，并实例化多个具备特定标准专业知识的审稿人智能体。反驳模块引入作者智能体，使其与审稿人智能体进行结构化辩论，以验证和完善提出的弱点。优先级排序器模块从大规模人类审稿实践中学习，以评估已验证弱点的严重程度",
                    "inspiration_trace": "基于对论文《DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning》的深入分析，以下是作者构建该方法的逻辑演进过程推演：\n\n### 1. 宏观观察：从“角色扮演”到“专家思维”的缺失\n**起点：** 自动化论文审稿领域正从单一LLM向多智能体系统演进。\n**观察：** 现有的多智能体系统（如AgentReview, MARG）大多停留在“表面模拟”阶段。它们只是简单地给智能体分配角色（如“审稿人”、“作者”或“领域主席”），或者按文本段落分工。\n**问题识别：** 真正的人类专家审稿并非仅仅因为身份不同，而是因为**关注的具体评价维度不同**。现有系统缺乏对“评价标准”的显式建模，导致生成的评论泛泛而谈，缺乏针对性。\n\n### 2. 深度诊断：有效性与实用性的双重危机\n在进一步观察中，作者发现了两个更深层次的逻辑漏洞：\n*   **漏洞一（有效性假设谬误）：** 现有系统默认AI生成的弱点是正确的。但在现实中，审稿人常有偏见或误解。**作者反驳**是验证评论质量的关键环节，而现有系统大多忽略了这一“纠错”机制。\n*   **漏洞二（输出效用低）：** 即使生成了正确的弱点，系统通常以平铺列表的形式输出。然而，对于作者而言，区分“致命缺陷”和“轻微瑕疵”至关重要。缺乏优先级排序使得AI审稿的实用性大打折扣。\n\n### 3. 核心假设：模拟“机制”而非模拟“人”\n**假设提出：** 要提高AI审稿的质量，不应只模拟审稿人的“身份”，而应模拟高质量审稿的“内在机制”。\n**逻辑推演：**\n*   机制一：**定制化规划**。专家在拿到论文后，会根据论文内容动态确定审查重点（如：这篇论文主要贡献是数据集，那么审查重点就是数据质量，而非数学推导）。\n*   机制二：**对抗性验证**。评论的有效性不是自证的，而是在与作者的辩论中确立的。只有经得起反驳的弱点，才是真正的弱点。\n*   机制三：**后果导向**。弱点的严重程度取决于其对最终录用决策的影响权重。\n\n### 4. 方法论构建：三模块闭环架构\n基于上述假设，作者构建了DIAGPaper框架，将思考过程转化为三个紧密耦合的模块：\n\n*   **第一步：解构专家思维 -> Customizer（定制器模块）**\n    *   *思考：* 如何让智能体像专家一样有针对性？\n    *   *方案：* 不再使用固定的角色，而是引入一个“定制器”智能体。它先阅读论文，动态生成具体的、细粒度的评价维度（如“数据集的代表性如何？”），然后据此实例化多个具有特定专长的“审稿人智能体”。\n\n*   **第二步：引入对抗验证 -> Rebuttal（反驳模块）**\n    *   *思考：* 如何过滤掉那些看似合理实则错误的幻觉评论？\n    *   *方案：* 引入“作者智能体”。针对每一个审稿人提出的弱点，作者智能体进行逐点反驳。这是一个多轮的、基于证据的辩论过程。如果审稿人无法提供充分的证据或逻辑来支撑其观点，该弱点就会被过滤掉（实验显示过滤掉了40%-60%的初始弱点）。\n\n*   **第三步：模拟决策权重 -> Prioritizer（优先级模块）**\n    *   *思考：* 如何让输出对用户最友好？\n    *   *方案：* 学习人类Meta-review（综合讨论）的行为。分析大量历史数据，计算出不同类别的弱点（如方法缺陷 vs 写作问题）对最终拒稿/录用的影响权重。结合辩论后的有效性得分，对幸存的弱点进行排序，只输出Top-K最严重的问题。\n\n### 5. 逻辑验证与闭环\n**最终思考：** 这个框架是否真的有效？\n*   *验证逻辑：* 如果这个框架是正确的，那么它应该能显著提升开源模型的表现（通过结构化思维弥补能力不足），并且在“有效性”指标上远超现有方法。\n*   *结果确认：* 实验表明，通过DIAGPaper的“多智能体化”，开源模型能达到接近GPT-4o的水平，且生成的弱点在“有效性”和“特异性”上均显著优于基线。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（现有多智能体系统肤浅）**出发，深入到**本质（缺乏评价标准、缺乏验证机制、缺乏优先级）**，最终通过**机制重构（动态定制、对抗辩论、严重度排序）**实现了对人类审稿深层逻辑的还原。"
                },
                {
                    "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents",
                    "arxiv_id": "2601.07577",
                    "authors": "Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",
                    "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于解决LLM智能体在长视界任务中的规划问题，提出了任务解耦规划（TDP）框架，涉及规划器和执行器等智能体架构，属于单智能体规划的研究范畴。",
                    "summary2": "本文旨在解决长视界智能体规划中上下文纠缠导致的鲁棒性差和效率低问题。针对复杂长视界任务，我们提出了一种Task-Decoupled Planning (TDP)框架，通过Supervisor构建任务DAG，并利用Planner和Executor在局部作用域内解耦规划与执行。我们在TravelPlanner、ScienceWorld和HotpotQA上通过Delivery、Accuracy和Average Reward等指标验证了其有效性，结果表明TDP在提升性能的同时将token消耗降低了82%。",
                    "summary_translation": "大语言模型的最新进展已使智能体能够自主执行复杂的 long-horizon tasks（长视界任务），然而规划仍然是实现可靠任务执行的主要瓶颈。现有方法通常分为两种范式：step-wise planning（逐步规划），具有反应性但往往较为短视；以及 one-shot planning（一次性规划），能够预先生成完整计划，但对执行错误较为脆弱。关键在于，这两种范式都存在 entangled contexts（纠缠上下文）的问题，即智能体必须基于跨越多个子任务的 monolithic history（整体历史）进行推理。这种纠缠增加了 cognitive load（认知负荷），并导致 local errors（局部错误）在原本独立的决策之间传播，从而使得错误恢复的计算成本高昂。为解决这一问题，我们提出了 Task-Decoupled Planning (TDP，任务解耦规划)，这是一个 training-free（免训练）框架，旨在用任务解耦替代纠缠推理。TDP 通过 Supervisor（监督者）将任务分解为由子目标组成的 directed acyclic graph (DAG，有向无环图)。通过利用具有 scoped contexts（限定上下文）的 Planner（规划器）和 Executor（执行器），TDP 将推理和重新规划的范围限制在 active sub-task（当前活动子任务）内。这种隔离机制防止了 error propagation（错误传播），并能够在不干扰 workflow（工作流）的情况下局部修正 deviations（偏差）。在 TravelPlanner、ScienceWorld 和 HotpotQA 上的实验结果表明，TDP 不仅优于强大的 baselines（基线模型），还将 token consumption（令牌消耗）减少了高达 82%，证明了子任务解耦能够提升 long-horizon agents（长视界智能体）的 robustness（鲁棒性）和 efficiency（效率）。",
                    "inspiration_trace": "基于论文《Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents》，以下是对作者核心方法论提出过程的逻辑链推演与思想还原：\n\n### 1. 宏观观察：长程任务的规划瓶颈\n**思考起点：** 随着大语言模型（LLM）能力的提升，智能体已经能够处理复杂的、长周期的自主任务。然而，作者发现尽管模型的理解和推理能力在增强，**“规划”** 依然是制约智能体在长程任务中表现可靠性的核心瓶颈。\n*   **现象：** 任务越复杂、步骤越多，智能体越容易迷失方向或执行失败。\n*   **初步问题：** 现有的规划方法为什么无法有效支撑长程任务？\n\n### 2. 现状剖析：两种范式的共通缺陷\n**思考过程：** 作者首先审视了当前领域内解决规划问题的两大主流范式，试图找出它们的局限性。\n*   **范式 A：逐步规划**\n    *   *特点：* 边思考边行动（如 ReAct）。\n    *   *优点：* 反应快，能适应反馈。\n    *   *缺点：* 目光短浅，缺乏全局观，容易在长程任务中走偏。\n*   **范式 B：一次性规划**\n    *   *特点：* 先生成完整计划再执行（如 Plan-and-Act）。\n    *   *优点：* 具备全局视野。\n    *   *缺点：* 脆弱，一旦执行出错或环境变化，原计划容易失效。\n*   **深度洞察（关键转折）：** 作者发现，虽然这两种方法在“规划粒度”上截然不同（一个细碎，一个宏观），但它们在**底层设计逻辑**上存在一个惊人的共同缺陷——**“上下文纠缠”**。\n    *   *问题本质：* 两者都将整个任务视为一个**单一的、整体的工作流**。智能体在推理时，必须依赖一个不断增长的、混合了所有子任务信息的“整体历史记录”。\n\n### 3. 核心洞察：从“粒度”转向“耦合”\n**思考深化：** 既然调整规划的“粒度”（更细或更粗）无法根本解决问题，作者意识到问题的根源不在于“多久规划一次”，而在于“信息是如何组织的”。\n*   **痛点分析：**\n    1.  **认知负荷过载：** 当上下文窗口中塞满了所有子任务的历史细节时，模型难以聚焦于当前需要解决的子问题。\n    2.  **错误传播：** 如果在子任务 A 中出现局部错误，由于上下文是纠缠的，模型往往需要重新审视甚至重做无关的子任务 B，导致计算成本高昂且脆弱。\n*   **假设提出：** 如果能打破这种“纠缠”，将任务进行**解耦**，就能隔离错误并降低推理负担。\n*   **核心思想：** **任务解耦**。即：将长程任务拆解为独立的子任务，让每个子任务的规划与执行都在**受限的局部上下文**中进行，互不干扰。\n\n### 4. 方法论构建：任务解耦的架构设计\n**思考落地：** 为了实现“解耦”这一抽象概念，作者需要设计一套具体的架构，将“全局视野”与“局部执行”分离开来。\n\n*   **第一步：全局结构化**\n    *   *需求：* 既然要解耦，就需要一个顶层结构来定义子任务之间的关系，否则系统会散架。\n    *   *设计：* 引入 **Supervisor（监督者）**。它的职责不是做具体执行，而是将大任务分解为有依赖关系的**有向无环图（DAG）**。这定义了“做什么”以及“先做什么”。\n\n*   **第二步：局部化执行**\n    *   *需求：* 确保执行子任务 A 时，完全看不到子任务 B 的具体执行细节，只看结果。\n    *   *设计：* 引入 **Planner（规划器）** 和 **Executor（执行器）**。\n    *   *关键机制：* **作用域上下文**。这两个模块只能看到当前节点（子任务）的描述、前置节点的结果以及当前节点的执行轨迹。这种设计强制实现了“上下文隔离”。\n\n*   **第三步：局部化纠错**\n    *   *需求：* 当执行出错时，不能推倒重来，只能局部修复。\n    *   *设计：* 当发生偏差时，触发**节点级重规划**。只修改当前节点的计划，而不影响 DAG 中其他已完成或未开始的部分。这从机制上切断了错误传播的路径。\n\n*   **第四步：动态一致性维护**\n    *   *需求：* 局部执行可能会导致全局目标不可达（例如：前置任务的结果改变了后续任务的条件）。\n    *   *设计：* 引入 **Self-Revision（自我修正）**。在每批节点完成后，检查全局状态，更新 DAG（如修改节点描述、增删节点），确保全局与局部的一致性。\n\n### 5. 逻辑闭环：局部化与全局性的平衡\n**思考验证：** 作者通过这套架构（TDP），试图证明一个观点：**通过显式的架构设计控制上下文范围，比单纯依赖模型的推理能力更有效。**\n*   **预期结果：**\n    *   **鲁棒性：** 错误被锁在局部，不会扩散。\n    *   **效率：** 模型不需要反复处理无关的长历史，Token 消耗大幅降低。\n*   **实验验证：** 选取 TravelPlanner（工具调用）、ScienceWorld（交互控制）、HotpotQA（多跳推理）三个差异巨大的场景进行验证，证明这种“解耦”思想具有普适性。\n\n---\n\n**总结：**\n作者的思考路径是从**表象问题**（长程任务规划难）出发，透过**现有方法的共性缺陷**（上下文纠缠），抓住了**本质矛盾**（认知负荷与错误传播），最终提出了**“任务解耦”**这一核心范式，并通过**Supervisor-Planner-Executor**的三层架构将这一思想工程化，实现了从“调整粒度”到“解耦架构”的范式跃迁。"
                },
                {
                    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
                    "arxiv_id": "2601.07477",
                    "authors": "Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",
                    "summary": "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\\our{}} on mathematical reasoning and code generation benchmarks, where {\\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于优化基于LLM的智能体工作流。它提出了一种“评估-判断-优化-更新”流水线，利用Judge模块分析执行轨迹并定位问题逻辑块，进而由LLM优化器修改工作流结构。这属于“自我演化”（通过反馈自我完善）和“单智能体”（自我反思/工作流结构）的研究范畴，而非纯推理或纯应用研究。",
                    "summary2": "本文旨在解决LLM智能体工作流优化中缺乏细粒度反馈信号导致效率低下的问题。针对复杂的智能体工作流，我们提出了一种名为JudgeFlow的Evaluation-Judge-Optimization-Update流水线，通过引入可复用的逻辑块和专门的Judge模块分析执行轨迹并定位问题模块。我们在数学推理和代码生成基准上通过准确率和pass@1验证了其有效性，结果表明该方法优于现有基线。",
                    "summary_translation": "优化基于大语言模型（LLM）的智能体工作流对于扩展人工智能能力而言是一项挑战。现有方法依赖于粗糙的端到端评估信号，缺乏关于具体改进位置的细粒度信号，往往导致低效或低影响力的修改。为了解决这些局限性，我们提出了 JudgeFlow，一种评估-判断-优化-更新流水线。我们将可复用、可配置的逻辑块整合到智能体工作流中，以捕捉基本的逻辑形式。在此抽象基础上，我们设计了一个专用的 Judge 模块，用于检查执行轨迹——特别是失败的运行——并为有问题的逻辑块分配基于排名的责任分数。这些细粒度的诊断信号随后被基于大语言模型的优化器利用，该优化器将修改集中在工作流中最有问题的逻辑块上。我们的方法提高了样本效率，通过块级诊断增强了可解释性，并为自动化日益复杂的智能体工作流提供了可扩展的基础。我们在数学推理和代码生成基准上评估了 JudgeFlow，结果表明 JudgeFlow 相比现有方法实现了更优越的性能和效率。源代码已在 https://github.com/ma-zihan/JudgeFlow 公开提供。",
                    "inspiration_trace": "基于论文《JudgeFlow: Agentic Workflow Optimization via Block Judge》的内容，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观问题与现状观察\n**思考起点：如何自动化构建高效的智能体工作流？**\n*   **背景**：随着大模型（LLM）的发展，基于LLM的智能体工作流在解决复杂任务（如数学推理、代码生成）上表现出色。然而，这些工作流的设计目前高度依赖人工经验（如手工设计Prompt、多Agent协作拓扑），成本高且难以扩展。\n*   **现有趋势**：受AutoML启发，学术界开始尝试自动化优化这些工作流。现有的自动化方法（如基于MCTS的搜索、图结构优化）大多将工作流视为一个整体进行端到端的优化。\n\n### 第二阶段：痛点识别与核心瓶颈\n**思考深入：为什么现有的自动化优化效率低下？**\n*   **观察**：现有的优化方法主要依赖“粗粒度”的反馈信号——即只看最终任务是否成功。\n*   **瓶颈分析**：\n    1.  **盲目搜索**：如果只知道“结果错了”，优化器不知道“错在哪里”。这导致优化过程像“盲人摸象”，只能对整个工作流进行随机的、低效的修改（如随机增删模块），样本效率极低。\n    2.  **归因困难**：代码形式的工作流虽然表达能力强，但内部包含复杂的控制流（如循环、条件分支）。当任务失败时，很难精准定位是哪一行代码或哪一个模块导致了错误，特别是那些在特定路径上未被执行的组件。\n\n### 第三阶段：提出假设与关键洞察\n**核心假设：如果能像调试代码一样，精准定位工作流中的“错误源”，就能实现高效的针对性优化。**\n*   **洞察**：优化过程不应是“全局随机试错”，而应是“诊断-治疗”的过程。\n*   **需求转化**：我们需要一种机制，能够从失败的执行轨迹中提取**细粒度的诊断信号**，明确指出工作流中哪个部分对失败负有最大责任。\n\n### 第四阶段：方法论的构建与演进\n为了实现上述假设，作者需要解决两个子问题：**“诊断什么”**（分析对象）和**“如何诊断”**（诊断机制）。\n\n**1. 抽象层设计：从“代码”到“逻辑块”**\n*   **思考**：直接对代码行进行诊断太细碎且难以理解；对整个工作流诊断又太粗糙。我们需要一个中间层。\n*   **创新点**：引入**“逻辑块”**概念。\n    *   将工作流抽象为三种基本逻辑形式的组合：顺序、循环、条件。\n    *   **目的**：这既保留了代码的表达能力，又封装了控制流细节，为诊断提供了一个语义清晰、结构稳定的分析单元。\n\n**2. 诊断机制设计：引入“法官”模块**\n*   **思考**：如何判断哪个逻辑块是“罪魁祸首”？人类专家会看执行日志，LLM也可以。\n*   **创新点**：设计**Judge模块**。\n    *   利用LLM作为“法官”，专门分析**失败案例**的执行轨迹。\n    *   它不关注最终得分，而是对工作流中的各个逻辑块进行**责任排序**，找出导致失败的最关键的那个块。\n\n**3. 优化策略设计：从“全局修改”到“局部手术”**\n*   **思考**：有了诊断结果，优化器该如何行动？\n*   **创新点**：构建**Evaluation-Judge-Optimization-Update闭环**。\n    *   Optimizer不再盲目搜索，而是根据Judge指出的“最差块”，进行针对性的操作（修改该块、删除该块或在该块前后插入新块）。\n\n### 第五阶段：最终逻辑框架的形成\n**总结：JudgeFlow 的诞生**\n*   作者将上述思考整合为一个统一的流水线：\n    1.  **Evaluation**：运行工作流，收集成功/失败信号。\n    2.  **Judge**：对失败案例进行“尸检”，利用逻辑块抽象进行归因，输出最需改进的模块。\n    3.  **Optimization**：LLM优化器根据诊断信号，对特定模块进行精准修补。\n    4.  **Update**：更新工作流池，进入下一轮迭代。\n\n**逻辑演进图示：**\n> **宏观问题**（自动化Agent设计）\n> ↓\n> **现有缺陷**（端到端信号太粗，搜索效率低）\n> ↓\n> **核心假设**（细粒度错误归因能提升效率）\n> ↓\n> **关键支撑**（逻辑块抽象 + LLM法官诊断）\n> ↓\n> **最终方案**（JudgeFlow：诊断驱动的针对性优化闭环）"
                },
                {
                    "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
                    "arxiv_id": "2601.07470",
                    "authors": "Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",
                    "summary": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于LLM智能体的核心组件——记忆管理，提出了元认知记忆抽象方法（MCMA）来优化智能体的记忆结构、抽象和重用能力，属于单智能体研究中的“记忆”与“自我反思”范畴。",
                    "summary2": "本文旨在解决LLM智能体记忆表示固定、抽象层次单一导致的泛化受限和负迁移问题。针对长视距交互决策任务，我们提出了一种Meta-Cognitive Memory Abstraction (MCMA) 方法，通过解耦任务执行与记忆管理，利用DPO训练Memory Copilot学习分层结构化记忆抽象策略。在ALFWorld、ScienceWorld和BabyAI数据集上，通过任务成功率、执行步数和奖励分数验证了其有效性和跨任务迁移能力。",
                    "summary_translation": "大语言模型智能体日益依赖累积记忆来解决长视界决策任务。然而，大多数现有方法将记忆存储在固定表示中，并在单一或隐式抽象层级上进行重用，这限制了泛化能力，且在发生分布偏移时往往导致负迁移。本文提出了元认知记忆抽象方法，该方法将记忆抽象视为一种可学习的认知技能，而非固定的设计选择。MCMA 通过结合冻结的任务模型与可学习的记忆副驾驶，实现了任务执行与记忆管理的解耦。该记忆副驾驶利用直接偏好优化进行训练，负责确定记忆的结构化、抽象及重用方式。记忆被进一步组织成抽象层级体系，从而能够基于任务相似度实现选择性重用。当不存在可迁移的记忆时，MCMA 通过迁移记忆副驾驶来传递抽象和管理记忆的能力。在 ALFWorld、ScienceWorld 和 BabyAI 上的实验表明，相较于多个基线方法，MCMA 在性能、分布外泛化以及跨任务迁移方面均实现了显著提升。",
                    "inspiration_trace": "基于论文《Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观方法论的思考过程。\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**（从“记忆很重要”到“现有记忆机制很脆弱”）**\n\n1.  **观察背景**：随着LLM智能体从静态问答转向长视界、交互式的复杂任务（如ALFWorld, ScienceWorld），智能体必须依赖“程序性记忆”来积累经验，以实现持续决策。\n2.  **发现问题**：尽管现有方法都在尝试存储和检索记忆，但在面对环境变化或任务分布偏移时，性能急剧下降，甚至出现“负迁移”。\n3.  **初步诊断**：现有的记忆机制过于僵化。它们大多将记忆视为静态的“内容”，用固定的格式（如纯文本、固定的键值对）和固定的抽象层级来存储。\n\n### 第二阶段：深入诊断与核心矛盾\n**（从“方法失效”到“抽象困境”）**\n\n1.  **剖析现有范式**：\n    *   **检索式**：直接复用历史轨迹。这导致过度拟合表面细节，一旦环境物体位置改变，记忆即失效。\n    *   **总结/抽象式**：试图提取高层规则。但这面临**“抽象困境”**：太细粒度则过拟合，太抽象则失去可执行性，变成正确的废话。\n    *   **训练式**：将经验内化到模型参数中。这导致记忆与策略耦合，难以跨任务迁移，且容易发生灾难性遗忘。\n2.  **提炼核心矛盾**：现有方法都是**“预设”**了记忆应该如何被表示和抽象。智能体并没有学会“如何记忆”，它只是在使用一个人类设计好的、僵化的存储桶。\n3.  **关键洞察**：人类之所以能灵活迁移记忆，是因为我们拥有**元认知**能力——即“关于思考的思考”。我们不仅存储知识，还学会了“如何组织知识”的认知技能。\n\n### 第三阶段：假设提出与范式转移\n**（从“存储内容”到“学习技能”）**\n\n1.  **核心假设**：记忆抽象不应是一个固定的工程设计，而应是一个**可习得的认知技能**。如果让智能体学会“如何记忆”，它就能自适应地决定记忆的结构和粒度。\n2.  **概念创新**：提出**“元认知记忆抽象”**。目标不是生成完美的记忆内容，而是训练一个能够根据任务需求动态生成记忆结构的“管理者”。\n3.  **架构构想**：为了验证这一假设，必须将“记忆管理”与“任务执行”解耦。如果混在一起，就无法单独评估记忆管理策略的好坏。\n\n### 第四阶段：方法论构建与逻辑闭环\n**（从“概念”到“Memory Copilot”）**\n\n1.  **解耦设计**：\n    *   **任务模型**：保持冻结，只负责执行动作，作为评估记忆好坏的“裁判”。\n    *   **记忆副驾驶**：这是核心创新点。它是一个独立的模型，专门负责将原始轨迹转化为结构化记忆。\n2.  **解决“抽象困境”的机制**：\n    *   **多结构生成**：不预设单一结构，而是让Copilot从树、链、键值对等多种原语中组合出最合适的记忆结构。\n    *   **基于效用的训练**：如何训练Copilot？利用任务模型的下游表现作为反馈。如果某种结构的记忆让任务完成得又快又好，这种结构就被奖励。\n3.  **训练算法选择**：采用**直接偏好优化（DPO）**。通过对比不同记忆结构带来的任务效果，构建偏好对，让Copilot学会生成那些能带来高任务效用的记忆表示。\n\n### 第五阶段：泛化与终极迁移\n**（从“复用知识”到“复用能力”）**\n\n1.  **分层抽象**：为了适应不同相似度的任务，构建记忆层级（从具体的情节记忆到抽象的语义记忆）。相似任务用细节记忆，不相似任务用抽象记忆。\n2.  **解决零样本迁移**：当遇到一个完全陌生的领域，没有任何旧记忆可以复用时怎么办？\n3.  **最终逻辑升华**：此时，我们不再转移“记忆内容”，而是转移**“记忆Copilot本身”**。因为Copilot学到的是“如何从新经验中提炼知识”的元认知能力。这种能力是跨域通用的。\n\n---\n\n**总结：作者的思考路径**\n从**“记忆内容僵化导致泛化失败”**的观察出发，通过**“引入元认知视角”**将问题转化为**“学习记忆抽象技能”**，进而通过**“任务/记忆解耦”**和**“基于效用的DPO训练”**实现了这一技能的习得，最终达成**“不仅复用知识，更复用学习能力”**的通用智能体目标。"
                },
                {
                    "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents",
                    "arxiv_id": "2601.07468",
                    "authors": "Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",
                    "summary": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于为个性化LLM智能体设计一种记忆框架（TSM），旨在解决记忆的时间维度建模问题。这直接符合“单智能体：记忆”的研究范围。",
                    "summary2": "本文旨在解决现有LLM Agent记忆方法在时间维度上的不准确性和碎片化问题。针对个性化对话场景，我们提出了一种Temporal Semantic Memory (TSM)框架，通过构建语义时间线和持续记忆来整合时序连续信息。我们在LONG MEM EVAL和LOCOMO数据集上通过Accuracy指标验证了其有效性，实验表明TSM在多会话理解和时间推理任务上显著优于现有基线方法。",
                    "summary_translation": "记忆机制使 Large Language Model (LLM) agents (大语言模型智能体) 能够感知、存储并利用过往对话中的信息，这对于实现个性化至关重要。然而，现有方法未能对记忆的时间维度进行恰当建模，主要体现在两个方面：1) 时间不准确性：记忆是按对话时间而非实际发生时间进行组织的；2) 时间碎片化：现有方法侧重于 point-wise memory (点状记忆)，从而丢失了能够捕捉持久状态和演变模式的持续信息。为解决上述局限性，我们提出了 Temporal Semantic Memory (TSM) (时间语义记忆)，这是一个为 point-wise memory (点状记忆) 建模 semantic time (语义时间)，并支持 durative memory (持续记忆) 构建与利用的记忆框架。在记忆构建阶段，该框架首先构建 semantic timeline (语义时间轴)，而非对话时间轴。随后，它将时间上连续且语义相关的信息整合为 durative memory (持续记忆)。在记忆利用阶段，该框架结合查询在 semantic timeline (语义时间轴) 上的时间意图，实现对时间上恰当的 durative memory (持续记忆) 的检索，并提供时间有效且持续时间一致的上下文以支持响应生成。在 LongMemEval 和 LoCoMo 数据集上的实验表明，TSM 始终优于现有方法，并实现了高达 12.2% 的准确率绝对提升，验证了所提方法的有效性。",
                    "inspiration_trace": "基于论文《Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程。\n\n---\n\n### 第一阶段：宏观观察与问题锚定\n**思考起点：个性化智能体的“记忆”困境**\n作者首先关注到LLM智能体在长期交互中的核心需求——**个性化**。现有的记忆机制（如RAG、向量数据库）虽然能存储历史对话，但在处理“时间”这一维度时存在根本性缺陷。\n*   **观察**：人类记忆是高度依赖时间线索的，我们不仅记得“发生了什么”，还记得“在什么时间背景下发生的”。然而，现有的LLM记忆系统大多将对话历史视为静态的文档集合，忽略了时间的动态性和语义性。\n\n### 第二阶段：深度诊断与核心痛点\n**思考深入：现有方法到底错在哪里？**\n作者进一步剖析了现有记忆系统在处理时间信息时的两个具体失效模式，从而确立了研究的突破口。\n\n1.  **时间错位**：\n    *   **现象**：用户在5月28日谈论5月29日的行程。现有系统通常以“对话时间”（5月28日）作为索引。\n    *   **问题**：这导致记忆被错误地锚定在聊天发生的时刻，而非事件发生的时刻。当用户查询“明天”或“那次旅行”时，系统无法准确对齐真实世界的时间线。\n    *   **结论**：必须区分“对话时间”与“语义时间”。\n\n2.  **时间碎片化**：\n    *   **现象**：一次为期一周的东京旅行被分散在数十个零散的对话轮次中。\n    *   **问题**：现有方法倾向于存储“点状记忆”，即孤立的事实片段。这种切分破坏了事件的连续性，导致智能体难以形成关于“持续状态”或“演变模式”的整体认知（例如：用户在旅行期间的整体心情或偏好变化）。\n    *   **结论**：需要一种机制将碎片化的信息整合为具有持续性的记忆。\n\n### 第三阶段：概念提出与假设构建\n**思考转折：如何模仿人类认知？**\n基于上述诊断，作者提出了两个核心概念作为解决问题的假设：\n\n1.  **语义时间线**：\n    *   **假设**：如果我们将记忆锚定在事件实际发生的时刻，而非对话记录的时刻，智能体就能像人类一样，在真实的时间轴上检索信息。\n    *   **构想**：构建一条独立于对话流的时间轴，所有记忆都挂载在这条轴上。\n\n2.  **持续性记忆**：\n    *   **假设**：如果将时间上连续且语义相关的片段聚合，形成高阶的摘要（如“主题”或“人设”），就能弥补点状记忆在长时上下文理解上的不足。\n    *   **构想**：记忆不应只是原子事实的堆砌，还应包含对一段时期内状态的总结。\n\n### 第四阶段：方法论设计与逻辑闭环\n**思考落地：如何实现上述概念？**\n作者将抽象概念转化为具体的工程架构，设计了TSM（Temporal Semantic Memory）框架，分为构建与利用两个阶段。\n\n1.  **构建阶段：从碎片到结构**\n    *   **解决“时间错位”**：引入**时序知识图谱**。从对话中提取实体和关系，并显式地标注其有效时间。这不仅是存储，更是建立了一个精确的时间索引。\n    *   **解决“时间碎片化”**：设计**分层聚合机制**。\n        *   *时间切片*：将图谱按时间间隔（如月）切分。\n        *   *语义聚类*：在同一时间片内，对实体进行聚类（GMM），将相关联的事件归为一组。\n        *   *生成摘要*：利用LLM对聚类结果进行总结，生成“主题”和“人设”。这标志着从“ episodic memory”（情景记忆）向“ durative memory”（持续性记忆）的升华。\n\n2.  **利用阶段：意图驱动的检索**\n    *   **逻辑**：用户的查询往往隐含时间意图（如“上周”）。\n    *   **机制**：\n        *   首先解析查询的**语义时间约束**。\n        *   在检索时，不仅计算语义相似度，更强制执行**时间过滤**。只有落在语义时间约束内的记忆（无论是TKG中的事实，还是Durative Memory中的摘要）才会被优先召回。\n        *   通过重排序，确保返回的上下文在时间上是逻辑自洽的。\n\n### 第五阶段：系统优化与工程考量\n**思考完善：如何保证效率与一致性？**\n作者意识到，频繁更新高阶摘要（Durative Memory）计算成本过高，而实时更新图谱（TKG）相对轻量。\n*   **分层更新策略**：\n    *   **在线轻量更新**：实时更新时序知识图谱，保证新事实的即时性。\n    *   **离线定期整合**：在“睡眠时间”定期重新计算和更新主题与人设摘要，平衡了系统的响应速度与长期一致性。\n\n---\n\n**总结：作者的思考路径**\n从**“现有记忆缺乏时间感知”**的宏观观察出发，通过诊断出**“对话时间与事件时间混淆”**和**“记忆碎片化”**两大微观病灶，提出了**“语义时间”**和**“持续性记忆”**的解决假设。最终，通过**时序知识图谱**进行底层时间锚定，结合**聚类摘要**实现高层语义聚合，并利用**时间约束检索**完成逻辑闭环，从而构建了一个能够像人类一样在真实时间线上思考的记忆系统。"
                },
                {
                    "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
                    "arxiv_id": "2601.07342",
                    "authors": "Nicolas Tacheny",
                    "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个LLM智能体框架，利用工具使用和结构化推理协议（调查协议）自主导航基础设施模型进行诊断。这符合单智能体研究范围中的“工具使用”和“规划”特征，且侧重于智能体架构而非纯领域应用。",
                    "summary2": "本文旨在解决传统 Root Cause Analysis (RCA) 耦合度高且难维护的问题。针对电信和数据中心基础设施，我们提出了一种基于 Model Context Protocol (MCP) 的 Agentic Diagnostic Framework，利用 LLM 通过 Investigation Protocol 和受限工具空间进行逐步推理。我们在合成图 Oracle Benchmark 上通过 Investigation Accuracy、RCA Accuracy 和 Impact Accuracy 验证了其有效性，Claude Haiku 3.5 达到了 100% 的准确率。",
                    "summary_translation": "大规模电信和数据中心基础设施依赖于多层服务和资源模型，在此架构下，故障会在物理和逻辑组件之间传播，进而影响多个客户。传统的根因分析（Root Cause Analysis, RCA）方法依赖于硬编码的图遍历算法或基于规则的关联引擎，这些方法不仅维护成本高昂，而且与基础设施模型紧密耦合。在这项工作中，我们提出了一种智能体诊断框架，该框架利用大语言模型（Large Language Model, LLM），通过模型上下文协议（Model Context Protocol, MCP）提供的受限工具空间执行分步调查。该智能体无需将因果逻辑或遍历算法嵌入应用程序，而是通过调用服务查询、依赖关系检索、结构化与非结构化数据分析、事件分析及影响发现等工具，自主在基础设施模型中进行导航。我们定义了一种调查协议，用于规范智能体的推理过程，并确保其具有事实依据、可复现性，并能安全处理缺失或模糊的信息。这项工作为自主事件解决和变更影响缓解奠定了基础。未来的系统不仅能够诊断并修复基础设施故障，还能预测计划变更对服务和客户的影响，从而帮助运维人员在执行维护操作前缓解风险。",
                    "inspiration_trace": "基于论文《Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到产出的思考过程：\n\n### 1. 宏观观察：传统方法的“刚性”与现实的“动态”矛盾\n**思考起点：** 作者首先审视了电信和数据中心运维的现状。\n*   **观察：** 现代基础设施是多层级的（服务、资源、客户），故障会在物理和逻辑组件间传播。\n*   **痛点：** 传统的根因分析（RCA）依赖于硬编码的图遍历算法或基于规则的关联引擎。\n*   **矛盾：** 基础设施是动态演进的（拓扑变更、命名变化），但传统的RCA逻辑是静态的。这导致了高昂的维护成本和系统与模型的紧耦合。此外，非结构化数据（如人工备注）难以被传统规则引擎处理。\n*   **初步结论：** 我们需要一种更具适应性、能够处理非结构化信息且不随拓扑变更而频繁修改代码的解决方案。\n\n### 2. 核心假设：从“编码逻辑”转向“编码协议”\n**思维转折：** 既然编写具体的因果逻辑（算法）太脆弱，能否让模型自己学会推理？\n*   **引入LLM：** 大语言模型（LLM）具备强大的推理和理解非结构化文本的能力，理论上可以替代硬编码规则。\n*   **风险识别：** 直接让LLM进行诊断存在“幻觉”风险，且无法保证操作的安全性（可能胡乱编造资源ID）。\n*   **关键假设：** 如果不把“因果逻辑”写死在代码里，而是定义一套严格的“调查协议”，并限制LLM只能通过特定工具获取数据，那么LLM就能像人类工程师一样进行“有据可依”的推理。\n*   **思路确立：** **去算法化**。不再试图用代码穷举故障传播路径，而是构建一个能够自主导航信息图的智能体。\n\n### 3. 抽象建模：构建标准化的数字孪生接口\n**落地思考：** 如何让智能体理解复杂的基础设施，同时又不依赖具体的数据库实现？\n*   **本体抽象：** 作者借鉴了TM Forum SID标准，将复杂的基础设施抽象为四个核心实体：**服务**、**资源**、**参与方**、**事件**。这为推理提供了一个通用的语义空间。\n*   **解耦设计：** 为了防止智能体与底层存储技术（如Neo4j或关系型数据库）绑定，作者引入了**模型上下文协议（MCP）**。\n*   **逻辑推演：** MCP充当了“安全边界”和“统一接口”。智能体不直接查询图数据库，而是调用MCP暴露的工具（如`get_implementation`, `get_impacted_services`）。这不仅解耦了系统，还天然防止了SQL注入或非授权访问，确保了每一次数据获取都是可审计的。\n\n### 4. 方法论构建：受控的智能体调查协议\n**核心创新：** 有了工具，如何确保智能体不乱跑、不胡说？\n*   **形式化流程：** 作者意识到，人类专家排查故障是有固定SOP（标准作业程序）的。因此，作者将这种经验形式化为一个**RCA调查协议**。\n*   **步骤设计：**\n    1.  **定位：** 从告警中提取服务名。\n    2.  **下钻：** 获取实现该服务的所有资源。\n    3.  **取证：** 检查每个资源的备注和事件（利用LLM理解非结构化文本）。\n    4.  **上溯：** 确定根因后，反向查找受影响的服务和客户。\n    5.  **发布：** 输出结构化报告。\n*   **约束机制：** 强制要求智能体必须基于工具返回的结果进行推理，如果数据缺失必须明确承认，严禁编造。这解决了LLM的“幻觉”问题，实现了**Grounding（接地气）**。\n\n### 5. 验证与洞察：去算法化的可行性\n**实证思考：** 这种“软逻辑”真的能取代“硬算法”吗？\n*   **实验设计：** 构建了一个合成图，预设了根因和影响路径，测试智能体能否在没有内置图算法的情况下找到答案。\n*   **结果分析：** 实验表明，只要协议设计得当，LLM（如Claude Haiku 3.5）能够达到100%的准确率。\n*   **关键洞察：** 事实证明，**硬编码的图遍历逻辑并非必须**。通过结构化的工具调用和逐步推理，因果逻辑是在推理过程中“涌现”出来的，而不是预先写好的。这意味着系统具有极强的通用性和适应性。\n\n### 6. 愿景延伸：从诊断到预测与自治\n**未来推演：** 既然能诊断“已发生”的故障，能否预测“未发生”的影响？\n*   **逻辑扩展：** 影响分析（IA）本质上是RCA的反向过程。如果系统能理解资源与服务的依赖关系，那么在执行变更（如维护）前，智能体完全可以模拟变更，预测其影响范围。\n*   **终极目标：** 这篇论文不仅是关于RCA，更是为**自主事故解决**和**变更影响缓解**奠定基础。未来的系统将从“被动响应”进化为“主动预防”。\n\n---\n\n**总结：**\n作者的思考路径是一个**“解构 -> 重构 -> 验证 -> 升华”**的过程：\n1.  **解构**了传统RCA系统的脆弱性（硬编码逻辑）；\n2.  **重构**了诊断流程，将其转化为基于MCP工具的智能体协议；\n3.  **验证**了LLM在严格协议下可以替代传统图算法；\n4.  最终**升华**出一种自适应、安全且可审计的基础设施运维新范式。"
                },
                {
                    "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging",
                    "arxiv_id": "2601.07309",
                    "authors": "Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",
                    "summary": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究LLM智能体，提出了一种通过模型合并技术将多个专家智能体整合为一个通用型智能体的方法，旨在解决智能体在不同交互环境中的泛化能力问题，属于LLM智能体的研究范畴。",
                    "summary2": "本文旨在将多个特定环境的LLM智能体专家合并为一个无需训练的通用模型。针对多轮交互场景，我们提出了一种名为ARM的基于激活引导的角色条件神经元移植方法。该方法通过动态主干选择和冲突感知的神经元移植来减少负迁移。在Qwen3-8B和Qwen2.5-7B专家池上，通过$\\tau$-bench、OfficeBench等多个基准验证了其有效性，显著提升了跨环境泛化能力和鲁棒性。",
                    "summary_translation": "交互式大语言模型智能体发展迅速，但大多数仍局限于单一环境，无法鲁棒地适应其他环境。Model merging (模型合并) 提供了一种 training-free (无需训练) 的替代方案，通过将多个 expert models (专家模型) 整合到单一模型中实现。在本文中，我们提出了 Agent-Role Merging (ARM) (智能体角色合并)，这是一种用于 LLM agents (大语言模型智能体) 模型合并的 activation-guided (激活引导)、role-conditioned (角色条件) neuron transplantation (神经元移植) 方法。ARM 将现有的合并方法从 static natural language tasks (静态自然语言任务) 拓展至 multi-turn agent scenarios (多轮智能体场景)，并提升了在各种交互环境中的 generalization ability (泛化能力)。这一目标通过一个精心设计的 3 步框架实现：1) 构建 merged backbones (合并主干网络)，2) 基于其 role-conditioned activation analysis (角色条件激活分析) 进行选择，3) 进行 neuron transplantation (神经元移植) 以实现 fine-grained refinements (细粒度细化)。在无需 gradient-based optimization (基于梯度的优化) 的情况下，ARM 提升了 cross-benchmark generalization (跨基准泛化) 能力，同时保持了高效率。在 diverse domains (多样化领域) 中，通过 ARM 合并得到的模型优于先前的 model merging methods (模型合并方法) 和 domain-specific expert models (特定领域专家模型)，同时展现了强大的 out-of-domain generalization (域外泛化) 能力。",
                    "inspiration_trace": "基于论文《ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 第一阶段：宏观问题与背景观察\n**1. 现实痛点：专才与通才的矛盾**\n*   **观察**：当前的LLM智能体在特定环境（如WebShop、OfficeBench）中表现优异，但往往是“专才”。一旦跨环境部署，由于工具接口、动作模式的差异，性能会急剧下降。\n*   **常规路径的局限**：传统的解决方案是训练一个通用的全能模型，但这面临巨大的工程挑战（多任务数据冲突、课程学习复杂）和高昂的训练成本。\n*   **切入点**：作者将目光投向了“模型合并”——一种无需额外训练即可整合多个专家模型权重的技术。这被视为一种低成本构建通才模型的潜在路径。\n\n### 第二阶段：冲突发现与问题聚焦\n**2. 现有方法的失效：从静态到动态的鸿沟**\n*   **假设**：现有的模型合并方法（如Task Arithmetic, TIES-Merging）在静态NLP任务上很成功，理应也能应用于智能体任务。\n*   **证伪**：实验发现，这些方法在交互式智能体场景下表现极不稳定（如图1所示，不同基准上表现方差巨大）。\n*   **核心洞察**：智能体任务与静态文本任务的本质区别在于**“多轮交互”**和**“级联效应”**。\n    *   在静态任务中，错误可能只是预测不准；\n    *   在智能体任务中，微小的格式错误（如JSON格式错误、工具调用参数偏差）会导致后续步骤全部崩溃。\n\n### 第三阶段：深入诊断与假设提出\n**3. 归因分析：两大核心挑战**\n作者将合并失败的原因归结为两个具体问题：\n*   **挑战一：主干的不稳定性**\n    *   不同的权重合并公式（平均、TIES等）在不同环境下的表现不可预测。没有一个通用的公式能保证在所有环境下都保留通用能力。\n*   **挑战二：能力冲突**\n    *   简单的权重平均会“模糊”掉特定技能。在智能体中，这表现为“角色关键行为”的丧失（例如，模型忘了如何正确调用API）。这种冲突比普通的知识遗忘更致命，因为它直接阻断了任务链条。\n\n### 第四阶段：方法论构建与逻辑演进\n**4. 策略一：如何选择稳定的主干？（从“盲选”到“内测”）**\n*   **思考**：既然无法预知哪个合并公式最好，能不能先构建一批候选模型，然后选一个最好的？\n*   **难点**：直接在测试集上评估成本太高。\n*   **创新思路**：利用模型内部的**激活信号**作为代理指标。\n    *   **逻辑**：如果一个合并后的模型，在处理特定任务（如“调用工具”）时，其神经元激活模式与原来的专家模型高度重合，说明它保留了该能力。\n    *   **产出**：提出了**激活重叠分数（AOS）**。通过分析“角色条件”下的激活（即只关注关键动作时刻的神经元），选出最能保留专家特征的合并主干。\n\n**5. 策略二：如何修复能力冲突？（从“全局融合”到“局部移植”）**\n*   **思考**：选出的主干可能在某些环境上依然较弱。直接全局微调会破坏已有能力，能否像器官移植一样，只把缺失的“能力模块”补进来？\n*   **细化思路**：\n    *   **定位**：利用激活分析，找出专家模型中负责特定“角色”（如JSON生成、工具调用）的关键神经元。\n    *   **移植**：将这些神经元直接“移植”到主干模型中。\n*   **关键约束：避免负迁移**\n    *   **思考**：如果移植的神经元恰好是另一个环境需要的，就会产生冲突。\n    *   **解决方案**：引入**冲突感知策略**。在移植前，先检查这些神经元是否被其他环境“占用”。如果是，则跳过，只移植那些“安全”的神经元。\n\n### 第五阶段：逻辑闭环与验证\n**6. 最终框架的形成：ARM**\n*   将上述思考串联，形成了三步走框架：\n    1.  **构建候选池**：用常规方法生成一堆合并模型。\n    2.  **基于激活选主干**：用AOS分数选出最稳健的那个。\n    3.  **神经元移植**：针对薄弱环节，像做手术一样精准移植专家的特定神经元，并严格保护其他能力不受干扰。\n\n**7. 预期与验证**\n*   **预期**：这种方法不仅能提升平均性能，更重要的是能解决“木桶效应”（最差环境的表现），因为它专门修复了导致级联失败的关键节点。\n*   **结论**：实验证明，ARM确实在保持通用性的同时，显著提升了跨环境的鲁棒性，验证了“基于角色条件的神经元移植”这一核心假设的有效性。\n\n---\n\n**总结：**\n作者的思考路径是从**“应用场景的迁移”**（从静态文本到智能体）出发，发现了**“级联失败”**这一特殊现象，进而通过**“机制可解释性”**（激活分析）手段，将模型合并问题从盲目的权重调整，转化为精准的**“电路诊断与修复”**过程。"
                },
                {
                    "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration",
                    "arxiv_id": "2601.07224",
                    "authors": "Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",
                    "summary": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确针对LLM智能体的训练方法，提出了基于梯度浓度的SFT与RL数据分配框架，并在WebShop和ALFWorld等智能体基准上进行了验证，属于智能体训练与自我演化范畴。",
                    "summary2": "本文旨在解决 LLM 智能体训练中 SFT 与 RL 数据分配低效及优化干扰问题。针对混合训练数据，我们提出了一种基于梯度空间几何结构（如 Gini 系数）诊断认知冲突的 PRISM 框架，实现数据在巩固与适应间的自适应路由，并在 WebShop 和 ALFWorld 基准上通过 Success Rate 和计算效率验证了其有效性。",
                    "summary_translation": "尽管混合监督微调（SFT）后接强化学习（RL）已成为训练 LLM 智能体的标准范式，但在这两个阶段之间进行数据分配的有效机制在很大程度上仍未被充分探索。当前的数据仲裁策略通常依赖于表层启发式方法，无法诊断模型的内在学习需求。鉴于 SFT 旨在通过模仿实现模式巩固，而 RL 则通过探索驱动结构适应，若数据与这些功能角色错位，将导致严重的优化干扰。我们提出了 PRISM，这是一个基于图式理论的动态感知框架，它根据数据与模型现有知识之间的认知冲突程度来仲裁数据。通过分析梯度的空间几何结构，PRISM 将引发高空间集中度的数据识别为高冲突信号，这类信号需要通过 RL 进行结构重构。相反，产生分散更新的数据则被分配给 SFT，以进行高效的巩固。在 WebShop 和 ALFWorld 上进行的广泛实验表明，PRISM 实现了帕累托改进，在将计算成本降低高达 3.22 倍的同时，性能优于最先进的混合方法。我们的研究结果表明，基于内部优化机制对数据进行解耦，对于实现可扩展且鲁棒的智能体对齐至关重要。",
                    "inspiration_trace": "基于论文《Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration》，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：宏观问题的捕捉——现有范式的“粗粒度”困境\n**思考起点：** 作者首先审视了当前LLM智能体训练的标准范式（SFT后接RL）。虽然这一流程已被广泛接受，但作者敏锐地发现了一个被忽视的瓶颈：**数据分配机制是僵化的。**\n*   **观察：** 现有的数据分配策略主要分为三类：一是“单调排序”（SFT-then-RL的固定顺序），二是“通用探索”（对所有数据无差别使用RL），三是“结果导向过滤”（基于准确率等外部指标）。\n*   **痛点：** 这些方法都忽略了数据的**异质性**和模型的**内部状态**。它们将所有数据一视同仁，导致计算资源浪费，且容易引发优化干扰（如对简单样本进行不必要的RL探索，导致不稳定性）。\n*   **核心问题：** 如何根据数据的内在认知需求，智能地将其分配给SFT或RL，以实现效率与性能的帕累托最优？\n\n### 第二阶段：功能解构——SFT与RL的本质差异\n**思考深入：** 为了解决分配问题，作者首先对SFT和RL在认知层面的功能进行了重新定义。\n*   **SFT的功能：** 侧重于**模式巩固**。通过模仿，将行为规范和特定知识内化，适合处理模型已具备基础认知的领域。\n*   **RL的功能：** 侧重于**结构适应**。通过试错，重构内部逻辑以提升泛化能力，适合处理需要复杂推理和逻辑修正的领域。\n*   **推论：** 如果将需要“结构适应”的数据强行用于SFT，模型无法突破逻辑瓶颈；如果将仅需“模式巩固”的数据用于RL，则会引入探索噪声，破坏已有的知识。因此，**必须找到一种诊断机制，区分哪些数据需要巩固，哪些需要适应。**\n\n### 第三阶段：理论映射——引入认知心理学视角\n**思考转折：** 如何定义“需要适应”的数据？作者跳出纯工程视角，引入了皮亚杰的**图式理论**。\n*   **理论核心：** 学习效率取决于新信息与现有知识库之间的**冲突程度**。\n    *   **低冲突（兼容）：** 适合通过“同化”进行巩固。\n    *   **高冲突（矛盾）：** 必须通过“顺应”进行根本性的结构重组。\n*   **映射：** 作者将这一认知过程映射到神经网络优化中——**高认知冲突 = 需要结构适应（RL）；低认知冲突 = 需要模式巩固（SFT）。**\n\n### 第四阶段：数学代理——从“认知冲突”到“梯度几何”\n**思考落地：** 理论有了，但如何量化“认知冲突”？模型内部不会直接告诉我们要“冲突值”。作者将目光投向了优化的核心信号——**梯度**。\n*   **假设：** 梯度是模型对数据的数学反馈。如果数据与模型现有知识冲突剧烈，模型必须剧烈调整特定的参数（即“知识神经元”）来修正逻辑。\n*   **几何洞察：** 作者关注梯度的**空间几何结构**，而非单纯的数值大小。\n    *   **高浓度：** 如果梯度高度集中在少数参数组上，说明模型正在进行剧烈的局部逻辑修正（高冲突）。\n    *   **低浓度（扩散）：** 如果梯度均匀分布在整个网络，说明模型只是在微调全局参数以适应模式（低冲突）。\n*   **结论：** **梯度的空间浓度是认知冲突的最佳代理。**\n\n### 第五阶段：方法论构建——PRISM框架的诞生\n**思考成型：** 基于上述逻辑，作者构建了PRISM框架，将理论转化为可执行的三个步骤：\n1.  **无损探针：** 在不更新权重的情况下，计算模型对每个样本的梯度分布，捕捉内部反应。\n2.  **结构量化：** 引入统计学指标（如基尼系数、峰度、变异系数CV）来量化梯度的“浓度”，从而给每个样本打上“认知冲突分”。\n3.  **自适应路由：** 根据分数中位数进行切分。高分（高冲突）样本路由至RL进行结构重塑；低分（低冲突）样本路由至SFT进行行为巩固。\n\n### 总结：逻辑链条全景\n作者从**训练效率低下**的宏观现象出发，通过**功能解构**明确了SFT与RL的分工，借助**认知心理学理论**定义了“冲突”这一核心变量，最终利用**梯度的空间几何特征**将抽象的认知冲突转化为可计算的数学指标，从而实现了数据的精准路由。这一过程体现了从“经验主义训练”向“动力学感知训练”的思维跃迁。"
                },
                {
                    "title": "Active Context Compression: Autonomous Memory Management in LLM Agents",
                    "arxiv_id": "2601.07190",
                    "authors": "Nikhil Verma",
                    "summary": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了Focus Agent，专注于LLM智能体的自主记忆管理（上下文压缩），属于单智能体研究中的记忆与自我反思范畴，且涉及智能体通过反馈进行自我调节（自我演化），符合筛选标准。",
                    "summary2": "本文旨在解决 LLM agents 在长周期任务中面临的 Context Bloat 问题。针对 SWE-bench Lite 中的上下文密集型软件工程任务，我们提出了一种名为 Focus 的架构，通过自主的上下文压缩与记忆整合机制，将原始交互历史转化为持久的 Knowledge 块。在 N=5 个实例上使用 Claude Haiku 4.5 进行评估，结果显示 Focus 实现了 22.7% 的 token 减少，同时保持了与 Baseline 相同的任务准确率。",
                    "summary_translation": "大语言模型（LLM）智能体在处理长周期的软件工程任务时面临困难，主要归因于“Context Bloat”（上下文膨胀）。随着交互历史的增长，计算成本急剧上升，延迟增加，且由于受到无关过往错误的干扰，推理能力也会下降。现有的解决方案通常依赖于智能体无法控制的被动外部摘要机制。本文提出了 Focus，这是一种以智能体为中心的架构，其灵感来源于多头绒泡菌（Physarum polycephalum，粘液菌）的生物探索策略。Focus 智能体自主决定何时将关键学习成果整合到持久的“Knowledge”（知识）块中，并主动撤回（修剪）原始交互历史。我们使用符合行业最佳实践的优化脚手架（persistent bash + string-replacement editor，即持久化 bash + 字符串替换编辑器），在 SWE-bench Lite 的 N=5 个上下文密集型实例上，利用 Claude Haiku 4.5 对 Focus 进行了评估。通过采用鼓励频繁压缩的激进提示策略，Focus 实现了 22.7% 的 Token（词元）减少（从 14.9M 降至 11.5M tokens），同时保持了相同的准确率（两个智能体均为 3/5 = 60%）。Focus 平均每个任务执行 6.0 次自主压缩，在单个实例上的 Token 节省率高达 57%。我们证明了，当提供适当的工具和提示时，能力较强的模型能够自主调节其上下文，这为在不牺牲任务性能的前提下构建具有成本感知的智能体系统开辟了新途径。",
                    "inspiration_trace": "基于论文《Active Context Compression: Autonomous Memory Management in LLM Agents》，以下是对作者产出核心方法“Focus”的逻辑链推演与思想演进还原：\n\n### 第一阶段：宏观观察与问题界定\n**——从“能力幻觉”到“现实瓶颈”的思考**\n\n1.  **观察现象**：随着LLM上下文窗口的扩大（如200k+ tokens），理论上Agent可以处理极长任务。\n2.  **发现矛盾**：虽然“能装下”，但在实际长周期任务（如软件工程）中，简单的“全量保留”策略导致了三大恶果：\n    *   **成本爆炸**：推理成本随历史长度呈二次方增长。\n    *   **延迟增加**：交互响应变慢，体验下降。\n    *   **认知干扰**：长上下文中充斥着失败的尝试和冗余日志，导致模型注意力分散（“Lost in the Middle”现象），反而降低了推理质量。\n3.  **核心问题定义**：现有的Agent普遍采用“Append-Only”（只追加）模式，这是一种不可持续的线性积累。问题不在于窗口不够大，而在于**缺乏有效的遗忘与筛选机制**。\n\n### 第二阶段：对现有范式的批判\n**——从“外部辅助”到“自主控制”的反思**\n\n1.  **审视现有解法**：\n    *   *外部记忆（MemGPT等）*：像操作系统一样管理内存，但增加了系统复杂性。\n    *   *事后总结（Reflexion等）*：通常在任务结束后反思，而非在任务进行中实时清理。\n    *   *被动压缩（LLMLingua等）*：依赖外部模型进行压缩，Agent本身无法感知和控制压缩过程。\n2.  **提炼痛点**：现有方法大多是“被动”的，Agent无法决定“此时此刻什么该留，什么该丢”。作者意识到，**真正的智能体必须具备“元认知”能力，即自主管理自身思维过程（上下文）的能力。**\n\n### 第三阶段：跨学科灵感与假设提出\n**——从“黏菌”行为中提取“压缩”哲学**\n\n1.  **寻找生物学隐喻**：作者寻找自然界中高效探索环境的生物机制，锁定了*Physarum polycephalum*（多头绒泡菌/黏菌）。\n2.  **提取核心逻辑**：\n    *   黏菌在探索迷宫时，当发现某条路是死胡同，它会**物理回缩**（丢弃路径），只留下**化学标记**（保留知识）。\n    *   **类比映射**：Agent不需要保留“我尝试了50次ls命令”的原始日志（肌肉记忆），只需要保留“配置文件不在/src目录”的结论（认知地图）。\n3.  **形成核心假设**：如果Agent能像黏菌一样，在探索阶段结束后，主动“修剪”掉原始交互日志，仅保留提炼出的“知识块”，就能在降低成本的同时避免注意力分散。\n\n### 第四阶段：方法论构建\n**——从“线性增长”到“锯齿波动”的架构设计**\n\n1.  **设计机制**：提出“Focus”架构，引入两个原语：\n    *   `start_focus`：标记探索起点。\n    *   `complete_focus`：总结关键信息并**物理删除**中间的原始对话。\n2.  **确立模式**：将上下文从单调递增的曲线，转变为**“Sawtooth”（锯齿状）模式**——探索时增长，压缩时塌陷。\n3.  **关键特性**：**自主性**。不依赖外部计时器，而是由Agent根据任务进度自主决定何时压缩。\n\n### 第五阶段：实验反馈与策略修正\n**——从“理想模型”到“工程现实”的妥协**\n\n1.  **初步实验受挫**：作者最初假设模型会自然地学会高效压缩。但实验发现，如果仅提供工具而不加干预，模型压缩频率过低（平均2次），且因丢失关键细节导致准确率下降。\n2.  **逻辑修正**：作者意识到，当前的LLM（如Claude Haiku）**缺乏内在的“成本意识”**。它们不会为了省钱而主动压缩，只会为了“整理思路”而压缩。\n3.  **提出“激进提示”策略**：为了验证假设，作者必须强制模型的行为模式。\n    *   *显式规则*：强制要求每10-15次工具调用后必须压缩。\n    *   *系统干预*：注入系统提醒。\n4.  **验证结果**：在强制引导下，Agent平均每任务压缩6次，实现了22.7%的Token节省，且准确率未受损。这证明了**“频繁、小步快跑”的压缩优于“偶尔、大跨度”的压缩**。\n\n### 第六阶段：边界认知与结论升华\n**——从“通用解法”到“场景特化”的洞察**\n\n1.  **发现局限性**：并非所有任务都适合压缩。在需要反复迭代修改的任务（如pylint实例）中，压缩反而增加了开销，因为Agent需要重新加载被丢弃的上下文。\n2.  **最终结论提炼**：\n    *   Focus不是万能药，而是最适合**“探索-实现”分离**的任务（如先找Bug，再修Bug）。\n    *   **思想演进终点**：未来的Agent不应只是被动的执行者，而应是具备自我调节能力的“认知管理者”。通过适当的工程脚手架（Prompting），可以让模型在保持性能的同时，实现成本效益的最优化。\n\n---\n\n**总结：**\n作者的思考路径是从**“上下文太长太贵”**的痛点出发，通过**批判现有被动方法**，引入**生物学的“遗忘与标记”机制**，构建了**自主压缩的架构**，并在实验中通过**强化Prompt策略**克服了模型惰性，最终明确了该方法在**探索型任务中的核心价值**。"
                },
                {
                    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
                    "arxiv_id": "2601.07055",
                    "authors": "Zhenrui Yue, Kartikeya Upasani, Xianjun Yang, Suyu Ge, Shaoliang Nie, Yuning Mao, Zhe Liu, Dong Wang",
                    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题和摘要明确提到了“Search Agents”（搜索智能体）和“Self-Evolving”（自我演化）。研究内容涉及通过反馈循环让智能体在没有训练数据的情况下自主生成问题并提升推理和工具使用能力，符合“自我演化”和“单智能体（工具使用）”的研究范围，且不属于排除项。",
                    "summary2": "本文旨在解决无训练数据情况下搜索智能体的自我进化问题。针对开放域问答场景，我们提出了一种 Dr. Zero 框架，该框架利用 Proposer-Solver 共进化循环和跳跃分组相对策略优化（HRPO）来生成多样化且具挑战性的问题。在多个开放域问答基准（如 HotpotQA, NQ）上，通过精确匹配（EM）等指标验证了其有效性，结果显示其性能匹配甚至超越了全监督搜索智能体。",
                    "summary_translation": "随着高质量数据日益难以获取，无数据自我进化已成为一种极具前景的范式。该方法使大语言模型能够自主生成并解决复杂问题，进而提升其推理能力。然而，由于问题多样性有限，且多步推理和工具使用需要大量计算，多轮搜索智能体在无数据自我进化过程中面临挑战。在本研究中，我们提出了 Dr. Zero，这是一个使搜索智能体能够在没有任何训练数据的情况下实现有效自我进化的框架。具体而言，我们设计了一个自我进化反馈回路，其中提议者生成多样化的问题，用于训练一个由同一基础模型初始化的解题者。随着解题者的进化，它会促使提议者生成难度递增但仍可解的任务，从而建立一套自动化课程来优化这两个智能体。为提高训练效率，我们还引入了跳跃分组相对策略优化。该方法将结构相似的问题进行聚类以构建组级基线，有效最小化了在评估每个查询的个体难度和可解性时的采样开销。因此，HRPO 在不影响性能或稳定性的前提下，显著降低了解题者训练的计算需求。大量实验结果表明，无数据的 Dr. Zero 达到甚至超越了全监督搜索智能体的水平，证明了复杂的推理和搜索能力可以仅通过自我进化而涌现。",
                    "inspiration_trace": "基于对论文《Dr. Zero: Self-Evolving Search Agents without Training Data》的深入分析，以下是对作者核心方法论逻辑链的系统性推演。这一过程旨在还原作者从宏观问题观察到具体方法创新的思考路径。\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**——从“数据饥渴”到“无数据自进化”的范式转移**\n\n1.  **背景痛点**：\n    *   作者首先观察到当前大模型（LLM）发展的核心瓶颈：高质量训练数据的获取日益困难。\n    *   **思考**：如果无法依赖外部人工标注数据，模型能否像生物进化一样，通过“自举”的方式自我提升？\n2.  **现有局限的识别**：\n    *   作者审视了现有的“自进化”研究（如Self-Play、Self-Rewarding），发现它们大多集中在**封闭域**（如数学、代码）。\n    *   **关键发现**：在开放域的**搜索代理**任务中，现有的自进化方法失效了。原因在于：\n        *   **多样性缺失**：模型倾向于生成简单的、单跳的问题，缺乏挑战性。\n        *   **计算成本高昂**：搜索代理需要调用外部工具（如搜索引擎），推理链路长、延迟高。传统的强化学习算法（如需要多次采样的GRPO）在多轮工具交互场景下计算量呈指数级增长，难以落地。\n\n**核心问题确立**：如何在**零训练数据**的条件下，实现开放域搜索代理的高效自进化？\n\n---\n\n### 第二阶段：机制设计与假设提出\n**——构建“出题者”与“解题者”的共生博弈**\n\n1.  **引入对抗/共生框架**：\n    *   **思考**：要解决“题目太简单”的问题，不能只靠模型自己瞎想。自然界中，捕食者和猎物的共同进化促进了物种复杂度的提升。\n    *   **假设**：如果设计两个角色——**Proposer（出题者）**和**Solver（解题者）**，让它们相互博弈，是否能自动生成由易到难的课程？\n2.  **定义进化逻辑**：\n    *   **Proposer的任务**：利用搜索引擎生成复杂、多跳的问题。\n    *   **Solver的任务**：利用搜索引擎回答这些问题。\n    *   **反馈闭环**：Solver越强，Proposer必须生成更难的问题才能获得奖励；Proposer的问题越难，Solver被迫提升搜索推理能力。\n    *   **关键洞察**：这种动态博弈能自动形成**课程学习**，无需人工设计难度梯度。\n\n---\n\n### 第三阶段：攻克核心瓶颈\n**——解决“计算效率”与“题目质量”的双重挑战**\n\n1.  **解决计算效率问题（HRPO的诞生）**：\n    *   **困境**：传统的GRPO算法为了估计优势函数，需要对同一个Prompt生成多个回复。对于搜索代理来说，一次回复包含多次搜索调用，成本极高。如果Proposer训练需要“生成多个问题”且“每个问题跑多次Solver”，计算开销不可接受。\n    *   **创新思考**：能否减少采样次数？\n    *   **逻辑推演**：问题的结构特征（如Hop数/跳数）与其难度高度相关。与其对同一个问题采样多次，不如将**结构相似的问题**（例如都是2跳问题）归为一组。\n    *   **方法论产出**：提出**Hop-Grouped Relative Policy Optimization (HRPO)**。通过聚类结构相似的问题来构建组级基线，从而避免了昂贵的嵌套采样，将计算成本降低了一个数量级。\n\n2.  **解决题目质量问题（难度引导的奖励机制）**：\n    *   **困境**：如何让Proposer生成“既难又能做对”的题目？如果太难，Solver全错，学不到东西；如果太简单，Solver全对，没提升。\n    *   **逻辑推演**：理想的题目应该让Solver的正确率处于中间状态（例如只有部分尝试能解出）。\n    *   **方法论产出**：设计**难度引导的奖励函数**。\n        *   如果Solver全对 -> 奖励低（太简单）。\n        *   如果Solver全错 -> 奖励低（太难/无解）。\n        *   如果Solver部分正确 -> 奖励高（难度适中）。\n    *   **补充**：引入格式奖励，强制Proposer正确使用搜索工具，确保生成的题目是基于真实检索路径的，而非幻觉。\n\n---\n\n### 第四阶段：系统整合与验证\n**——Dr. Zero 框架的最终成型**\n\n1.  **系统架构整合**：\n    *   作者将上述思考整合为一个统一的框架：**Dr. Zero**。\n    *   **输入**：仅依赖基础LLM和外部搜索引擎，无任何人工标注数据。\n    *   **流程**：\n        1.  Proposer通过HRPO训练，利用搜索生成高质量、多跳的QA对。\n        2.  Solver通过GRPO训练，学习解决Proposer生成的难题。\n        3.  两者交替迭代，性能螺旋上升。\n\n2.  **实验验证与假设确认**：\n    *   **思考**：这套无数据方案真的能打过有监督的SOTA吗？\n    *   **结果**：实验表明，Dr. Zero在多个开放域QA基准上，不仅超越了基础模型，甚至**匹配或超越了**完全依赖人工数据的监督式搜索代理。\n    *   **结论**：证明了在搜索代理领域，**自进化可以替代人工监督**。\n\n---\n\n### 总结：作者的思维演进脉络\n\n1.  **观察**：数据稀缺，现有自进化方法在开放域搜索任务中因“题目简单”和“算力昂贵”而失效。\n2.  **假设**：通过Proposer-Solver的共生博弈可以自动生成进化的课程。\n3.  **挑战1（算力）**：传统RL采样太贵 -> **创新**：利用问题结构相似性，提出HRPO算法，大幅降低采样成本。\n4.  **挑战2（质量）**：如何控制题目难度 -> **创新**：基于Solver正确率的难度引导奖励，确保题目处于“最近发展区”。\n5.  **成果**：实现了无需任何训练数据的搜索代理自进化，性能媲美甚至超越有监督方法。"
                },
                {
                    "title": "CloneMem: Benchmarking Long-Term Memory for AI Clones",
                    "arxiv_id": "2601.07023",
                    "authors": "Sen Hu, Zhiyu Zhang, Yuxiang Wei, Xueran Han, Zhenheng Tang, Huacan Wang, Ronghao Chen",
                    "summary": "AI Clones aim to simulate an individual's thoughts and behaviors to enable long-term, personalized interaction, placing stringent demands on memory systems to model experiences, emotions, and opinions over time. Existing memory benchmarks primarily rely on user-agent conversational histories, which are temporally fragmented and insufficient for capturing continuous life trajectories. We introduce CloneMem, a benchmark for evaluating longterm memory in AI Clone scenarios grounded in non-conversational digital traces, including diaries, social media posts, and emails, spanning one to three years. CloneMem adopts a hierarchical data construction framework to ensure longitudinal coherence and defines tasks that assess an agent's ability to track evolving personal states. Experiments show that current memory mechanisms struggle in this setting, highlighting open challenges for life-grounded personalized AI. Code and dataset are available at https://github.com/AvatarMemory/CloneMemBench",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个针对AI智能体（AI Clones）长期记忆能力的基准，重点评估智能体在模拟个人行为时对经历、情绪和观点的记忆与追踪能力，属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在评估AI Clone基于非对话式数字痕迹的长期记忆能力。针对跨越1-3年的日记、社交媒体等非对话式数字痕迹，我们提出了一种名为CloneMem的benchmark，采用分层数据构建框架确保纵向一致性。我们在CloneMem数据集上通过Recall@K、Choice Accuracy和QA Consistency Score等指标验证了其有效性，揭示了现有记忆系统在追踪个人状态演变方面的局限性。",
                    "summary_translation": "AI Clones (AI克隆) 旨在模拟个体的思想和行为，以实现长期、个性化的交互，这对 memory systems (记忆系统) 随时间对经历、情感和观点进行建模提出了严苛的要求。现有的 memory benchmarks (记忆基准) 主要依赖于 user-agent conversational histories (用户-智能体对话历史)，这些历史在时间上是碎片化的，不足以捕捉连续的生活轨迹。我们介绍了 CloneMem，这是一个用于评估 AI Clone (AI克隆) 场景中 longterm memory (长期记忆) 的基准，它基于 non-conversational digital traces (非对话式数字痕迹)，包括日记、社交媒体帖子和电子邮件，时间跨度为一到三年。CloneMem 采用了一个 hierarchical data construction framework (分层数据构建框架) 来确保 longitudinal coherence (纵向一致性)，并定义了评估智能体追踪 evolving personal states (演变个人状态) 能力的任务。实验表明，当前的 memory mechanisms (记忆机制) 在这种设置下难以应对，凸显了 life-grounded personalized AI (基于生活的个性化AI) 面临的开放性挑战。Code (代码) 和 dataset (数据集) 可在 https://github.com/AvatarMemory/CloneMemBench 获取。",
                    "inspiration_trace": "基于论文《CloneMem: Benchmarking Long-Term Memory for AI Clones》，以下是对作者核心思想产出过程的系统性逻辑推演：\n\n### 第一阶段：宏观趋势与问题定义\n**（从“对话助手”到“数字克隆”）**\n\n1.  **观察现象**：随着LLM的发展，AI应用正从通用的“角色扮演”向更深度的“AI克隆”演进。用户不再满足于与一个预设角色的单次对话，而是希望建立一个能长期模拟特定个体思想、行为和情感的“数字分身”。\n2.  **提炼需求**：AI克隆的核心挑战不在于单次回复的准确性，而在于**长期记忆**。系统必须能够跨越数年时间，捕捉个体的经历、情感波动以及观点的演变，而不仅仅是维持对话的上下文。\n\n### 第二阶段：痛点识别与假设提出\n**（从“对话历史”到“生活轨迹”）**\n\n1.  **批判现状**：作者审视现有的记忆基准（如LoCoMo, LongMemEval），发现它们几乎全部依赖于**用户-智能体的对话历史**。\n2.  **指出缺陷**：\n    *   **碎片化**：对话是离散的、断续的，只能捕捉生活的快照，无法记录连续的生活流。\n    *   **被动性**：现实中，用户不可能通过不断的对话来“喂养”AI克隆，这成本太高。\n3.  **提出假设**：真实的记忆应当基于**非对话式的数字痕迹**（如日记、社交媒体、邮件）。这些数据是自然发生的、纵向连续的，能反映个体在非交互状态下的真实状态。因此，需要一个新的基准来评估AI克隆处理这种“生活轨迹”的能力。\n\n### 第三阶段：数据构建的方法论突破\n**（从“随机生成”到“分层连贯性”）**\n\n1.  **面临挑战**：如何构建一个跨越1-3年、逻辑自洽且包含情感和观点演变的合成数据集？简单的随机生成会导致时间线上的逻辑崩塌。\n2.  **核心思想**：人类的生活是有结构的，不是杂乱无章的。必须采用**自上而下的分层生成框架**来确保纵向连贯性。\n3.  **逻辑推演**：\n    *   **宏观层**：先定义“人格特质”和“生活弧线”，确定长期的人生轨迹（如职业变动、情感走向）。\n    *   **中观层**：将大事件拆解为“阶段”，并引入“内部状态快照”机制。确保上一阶段的情感积累会影响下一阶段，从而实现心理状态的连续性。\n    *   **微观层**：基于具体事件生成日记、帖子等数字痕迹，并显式生成对应的“证据”，确保痕迹与底层逻辑的一致性。\n\n### 第四阶段：评估维度的重新定义\n**（从“事实检索”到“轨迹追踪”）**\n\n1.  **转变视角**：传统的记忆测试多关注“某时某刻发生了什么”（静态事实）。但对于AI克隆，关键在于“为什么会变成现在这样”。\n2.  **设计任务**：评估重点必须转向**动态推理**。作者设计了涵盖经历、情感、观点三个维度的任务，不仅测试事实回忆，更测试比较、因果分析、反事实推理以及对“未确定状态”的识别（即区分“正在探索”与“已做决定”）。\n\n### 第五阶段：实验发现与理论升华\n**（从“追求抽象”到“保真度优先”）**\n\n1.  **预期与反差**：作者原本预期先进的、具有抽象和整合能力的记忆系统（如A-Mem, Mem0）会表现更好，因为它们能“总结”知识。\n2.  **实验发现**：结果令人惊讶，最简单的**扁平检索器**往往表现最好。复杂的记忆系统因为进行了“有损压缩”（总结和抽象），丢失了回答轨迹问题所需的细粒度细节（如时间戳、具体措辞）。\n3.  **洞察提炼**：\n    *   **有效性 vs. 保真度**：现有的记忆系统优化的是“有效性”（能否找到相关话题），但AI克隆更需要的是“保真度”（能否还原具体细节）。\n    *   **叙事陷阱**：模型倾向于用通用的叙事模板（如“孩子的一句话让父亲顿悟”）来填补记忆空白，导致因果逻辑错误但听起来很合理。\n4.  **最终结论**：AI克隆的记忆系统不应仅仅是一个压缩的知识库，而应是一个**证据保存基质**。它必须保留原始痕迹的保真度，显式建模内部状态的转变，并能在证据不足时保持“未知”的克制。\n\n---\n\n**总结**：作者的思考路径是从**应用场景的升级**（克隆vs对话）出发，发现了**数据源的本质缺陷**（对话vs轨迹），通过**分层生成**解决了数据连贯性难题，并在实验中意外揭示了**当前记忆架构的“有损压缩”悖论**，最终确立了AI克隆记忆设计应遵循“保真度优先”的新原则。"
                },
                {
                    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
                    "arxiv_id": "2601.06860",
                    "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了ET-Agent框架，专注于LLM智能体的工具使用行为校准，属于单智能体研究范畴；同时引入了自我演化数据飞轮机制，符合自我演化的研究范围。不属于排除的纯应用、纯推理或基础设施优化等类别。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR) 任务中agent行为模式无效的问题。针对TIR场景，我们提出了一种名为ET-Agent的训练框架，通过Self-evolving Data Flywheel生成增强数据，并利用两阶段Behavior Calibration Training逐步校准错误行为。我们在数学推理和知识密集型任务上通过正确性、效率等指标验证了其有效性，实验表明ET-Agent显著提升了推理效率和准确性。",
                    "summary_translation": "大语言模型可以通过采用工具集成推理范式，扩展其参数知识的边界。然而，现有的基于大语言模型的智能体训练框架往往侧重于答案的准确性，而忽视了对行为模式的特定对齐。因此，智能体在执行工具集成推理任务时，常表现出无效的动作，例如冗余或不足的工具调用。如何在执行工具集成推理任务时校准错误的行为模式，进而探索有效的轨迹，仍是一个亟待解决的开放性问题。在本文中，我们提出了ET-Agent，这是一个通过两个协同视角来校准智能体工具使用行为的训练框架：自进化数据飞轮和行为校准训练。具体而言，我们引入了一个自进化数据飞轮来生成增强数据，利用这些数据对大语言模型进行微调，以提升其探索能力。在此基础上，我们构建了一个两阶段的行为校准训练框架。该框架旨在逐步将错误的行为模式校准为最优行为。进一步的深入实验证实了ET-Agent在多个维度上的优越性，包括正确性、效率、推理简洁性以及工具执行准确性。我们的ET-Agent框架为工具集成推理领域的研究提供了有价值的实践启示。代码链接：https://github.com/asilverlight/ET-Agent",
                    "inspiration_trace": "基于对论文《ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration》的深入分析，以下是对作者产出该核心方法的逻辑链推演。这一过程展现了从宏观现象观察到微观机制设计的完整思考路径。\n\n---\n\n### 1. 宏观观察与问题定义：从“结果正确”到“行为有效”\n\n**逻辑起点：**\n作者首先关注到大语言模型（LLM）通过工具集成推理（TIR）范式突破了参数知识的限制。然而，学术界和工业界存在一个普遍的**评价偏差**：绝大多数研究仅关注**最终答案的准确性**，而忽视了达成答案过程中的**行为模式**。\n\n**现象发现：**\n在实际应用中，作者观察到即使模型答对了问题，其过程往往充满“低效”或“怪异”的行为。例如，为了查一个简单事实反复调用搜索工具，或者在需要深入推理时过早停止。这引发了一个核心思考：\n> **核心问题：** 如何在保证答案正确的前提下，校准Agent的工具使用行为，使其既不冗余也不匮乏，从而实现高效推理？\n\n### 2. 深度诊断与归因：行为错误的分类与空间复杂性\n\n为了解决上述问题，作者没有急于提出模型，而是先对“错误行为”进行了系统性的**病理分析**。\n\n**错误分类：**\n通过初步实验，作者将错误的TIR行为模式归纳为两类：\n1.  **不当工具使用：** 包括“冗余调用”（浪费资源）和“中止执行”（代码或查询格式错误导致失败）。\n2.  **缺陷推理逻辑：** 包括“调用不足”（过早停止，没拿到关键信息）和“错误推理过程”（逻辑跳跃或无关步骤）。\n\n**关键洞察：**\n作者进一步分析了正确答案的轨迹分布，发现了一个重要现象：**对于同一个问题，存在大量不同的正确路径，且工具调用的次数差异巨大。**\n这意味着TIR任务的**动作空间极其广阔**。\n\n**对现有方法的批判：**\n基于此洞察，作者指出了现有方法的局限性：\n*   **模仿学习（SFT）：** 只能复现训练数据中的路径，无法探索数据之外的高效行为，导致探索能力受限。\n*   **传统RL（如DPO）：** 往往基于二元对比（好vs坏），容易导致模型坍缩到极窄的动作空间，无法适应TIR广阔的解空间。\n\n**结论：** 现有的“只看结果”或“简单对比”无法解决TIR中的行为校准问题。我们需要一种能**充分探索广阔动作空间**，并从中**筛选出最优行为**的新范式。\n\n### 3. 核心假设提出：先探索，后校准\n\n基于上述诊断，作者提出了一个分阶段的解决思路：\n> **核心假设：** 要校准行为，首先必须让模型“见识”到足够多的可能性（探索），然后再通过奖励机制引导其收敛到最优路径（校准）。\n\n这直接导向了ET-Agent框架的两大支柱设计：\n1.  **数据层面：** 需要一个能自我进化、不断扩充轨迹多样性的机制。\n2.  **算法层面：** 需要一个先鼓励发散探索，再逐步收敛至高效行为的训练流程。\n\n### 4. 方法论构建：从数据飞轮到行为校准\n\n#### 4.1 数据层面的突破：自进化数据飞wheel\n**思考：** 既然现有数据覆盖面不够，如何低成本地获得高质量、多样化的轨迹？\n**设计：**\n作者设计了一个闭环系统，利用模型自身来生成和优化数据：\n*   **对正确轨迹：** 进行“去冗余”和“全局精炼”，教模型如何做得更简洁。\n*   **对错误轨迹：** 进行“自我修正”和“提示注入”，强制模型继续思考或修正错误，从而生成原本不存在的正确路径。\n**逻辑目的：** 这个过程不仅仅是增加数据量，而是为了**覆盖更广阔的动作空间**，为后续的训练提供丰富的“原材料”。\n\n#### 4.2 算法层面的演进：两阶段行为校准\n**思考：** 有了丰富的数据，如何训练模型？直接用RL可能会因为奖励稀疏或梯度消失而失败。\n**设计：** 作者将训练分为两个紧密衔接的阶段。\n\n*   **阶段一：动作空间探索微调**\n    *   **逻辑：** 利用飞wheel生成的多样化数据进行监督微调（SFT）。\n    *   **目的：** 此时暂不追求极致效率，而是让模型**学会各种可能的解题路径**，打破初始模型的思维定势，实现“广度优先”。\n\n*   **阶段二：迭代行为校准强化学习**\n    *   **逻辑：** 在模型具备探索能力后，引入RL进行优化。\n    *   **难点解决：** 传统的Group-wise RL容易因为轨迹同质化导致梯度消失。\n    *   **创新设计：**\n        *   **分组帕累托采样：** 在采样时，不仅看正确率，还看行为差异度。优先保留那些“既正确又与众不同”的轨迹，确保训练信号始终存在。\n        *   **课程式奖励机制：** 设计了包含“效率惩罚”（工具调用次数、推理长度）的奖励函数。并采用课程学习策略，逐步收紧对效率的要求（从宽松到严格），防止模型为了追求效率而牺牲正确性。\n\n### 5. 逻辑闭环与验证\n\n最终，ET-Agent的形成逻辑链条如下：\n1.  **痛点：** TIR Agent行为低效（冗余或不足），且动作空间巨大。\n2.  **诊断：** 现有方法缺乏对广阔动作空间的有效探索和精准校准。\n3.  **策略：** 先通过数据增强实现“广度探索”，再通过RL实现“精度校准”。\n4.  **实现：**\n    *   **数据侧：** 自进化飞wheel -> 生成多样化、高质量的轨迹。\n    *   **训练侧：** RFT（学广） -> Pareto RL + 课程奖励（学精）。\n5.  **结果：** 模型在保持高准确率的同时，显著提升了工具使用的效率和推理的简洁性。\n\n这一思考过程体现了作者从**现象观察**到**本质归因**，再到**范式假设**，最后落实到**具体机制设计**的严谨学术逻辑。"
                },
                {
                    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
                    "arxiv_id": "2601.06794",
                    "authors": "Zhicong Li, Lingjie Jiang, Yulan Hu, Xingchen Zeng, Yixia Li, Xiangwen Zhang, Guanhua Chen, Zheng Pan, Xin Li, Yong Liu",
                    "summary": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了ECHO框架，专注于训练LLM智能体，通过策略与批评模型的协同演化来优化智能体在开放世界环境中的表现。这符合研究范围中的“自我演化（通过反馈自我完善）”及“单智能体”方向，不属于排除的纯应用、纯推理或基础设施优化。",
                    "summary2": "本文旨在解决critique-guided RL中静态critic因策略演化导致反馈陈旧的问题。针对Open-World Agent Learning场景，我们提出了一种ECHO框架，通过cascaded rollout mechanism和saturation-aware gain shaping实现策略与critic的同步协同演化。并在WebShop、ALFWorld、SciWorld及DeepSearch四个基准上通过任务成功率验证了其有效性。",
                    "summary_translation": "批判引导的强化学习（RL）已成为一种训练 LLM 智能体的强大范式，它通过自然语言反馈来增强稀疏的结果奖励。然而，现有方法通常依赖于静态或离线的评论家模型，这些模型无法随着策略的演变而适应。在在线策略 RL 中，智能体的错误模式会随时间发生偏移，导致固定的评论家变得过时，从而提供效用递减的反馈。为了解决这一问题，我们提出了 ECHO（Evolving Critic for Hindsight-Guided Optimization，用于后见之明引导优化的演进评论家），该框架通过同步的协同进化循环来联合优化策略和评论家。ECHO 采用了一种级联展开机制，评论家首先针对初始轨迹生成多个诊断，随后进行策略细化，从而实现分组结构的优势估计。我们通过一种饱和感知的增益塑形目标来解决学习平台期的挑战，该目标对评论家在高性能轨迹中引发增量改进的行为给予奖励。通过采用双轨 GRPO 更新，ECHO 确保评论家的反馈与不断演进的策略保持同步。实验结果表明，在开放世界环境中，ECHO 能够实现更稳定的训练，并在长视界任务中取得更高的成功率。",
                    "inspiration_trace": "基于论文《No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning》，以下是对作者产出核心方法（ECHO）的逻辑链推演与思考过程还原：\n\n### 第一阶段：观察现状与识别瓶颈\n**思考起点：** 在开放世界的LLM智能体训练中，传统的强化学习（RL）仅依赖稀疏的最终结果奖励，这导致数据效率极低，因为智能体不知道“哪里错了”。\n**现有尝试：** 引入“评论家”模型提供自然语言的诊断反馈。\n**发现矛盾：** 现有的评论家大多是**静态**的（基于模板或离线训练后冻结）。作者观察到，在On-policy RL（在线策略强化学习）中，智能体的策略是不断演进的。\n**逻辑推演：**\n*   早期阶段：智能体犯的是粗粒度错误（如走错房间），需要高层提示。\n*   后期阶段：智能体已掌握基本技能，犯的是细粒度错误（如参数微调），需要精准诊断。\n*   **结论：** 一个固定的、不随策略变化的评论家，其反馈会逐渐变得“陈旧”，甚至产生误导。这就是“Critic Staleness”问题。\n\n### 第二阶段：提出核心假设\n**思维转折：** 既然智能体的错误模式是漂移的，那么最优的评论策略也应当是非静止的。\n**核心假设：** 评论家不应是一个外部的、高高在上的“监督者”，而应是一个与策略共同进化的“伙伴”。\n**评价标准重构：** 评价一个评论家好坏的标准，不应是“它说得是否好听”，而应是“它是否真的诱导了策略的改进”。\n\n### 第三阶段：构建协同进化机制\n**设计挑战：** 如何让两个模型（策略 $P$ 和 评论家 $C$）在同一个训练循环中互相促进，而不是互相干扰？\n**解决方案构思：**\n1.  **闭环构建：** 设计一个“诊断-修正”的级联流程。策略生成轨迹 -> 评论家诊断 -> 策略基于诊断修正。\n2.  **双重优化：** 利用修正后的结果来反向更新两个模型。\n    *   策略更新：学习如何更好地采纳建议。\n    *   评论家更新：学习如何给出能带来更高奖励的建议。\n**逻辑支点：** 通过这种“双轨”同步更新，确保评论家的诊断粒度始终对齐策略当前的短板。\n\n### 第四阶段：解决“最后一公里”的优化难题\n**深入思考：** 在训练后期，策略表现已经很好（例如得分从0.9提升到0.95），这比从0.1提升到0.15要难得多。\n**现有缺陷：** 如果使用线性的奖励差值（$\\Delta s = 0.05$），模型会认为这种高难度的提升价值很低，导致优化停滞。\n**创新思路：** 引入“饱和感知”的奖励设计。\n**逻辑推演：**\n*   假设奖励空间是非线性的，越接近满分，改进的难度和熵减的价值越高。\n*   设计一个增益函数，放大高分区间的微小改进信号。\n*   **目的：** 激励评论家去挖掘那些“看似完美但仍有瑕疵”的轨迹中的关键缺陷。\n\n### 第五阶段：方法论综合与验证\n**最终框架（ECHO）：** 将上述思考整合为一个统一的框架。\n1.  **级联演化：** 通过多视角诊断和条件修正，生成结构化的轨迹组。\n2.  **饱和感知奖励：** 解决高难度阶段的优化动力问题。\n3.  **同步双轨GRPO：** 利用群组相对优势估计，稳定地同时更新策略和评论家。\n\n**总结：** 作者的思考路径从**发现静态反馈与动态策略之间的错配**出发，通过**引入协同进化的视角**重新定义了评论家的角色，并利用**非线性奖励塑形**解决了长尾优化难题，最终实现了ECHO这一能够持续自我提升的智能体训练范式。"
                },
                {
                    "title": "From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design",
                    "arxiv_id": "2601.06776",
                    "authors": "Xufei Tian, Wenli Du, Shaoyi Yang, Han Hu, Hui Xin, Shifeng Qu, Ke Ye",
                    "summary": "Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一种多智能体LLM工作流，包含四个专门的智能体（任务理解、拓扑生成、参数配置、评估分析）进行协作，并涉及与仿真软件的交互（工具使用）。这符合“多智能体：协作”和“工具使用”的研究范围。尽管应用于化工领域，但其核心贡献在于智能体架构和工作流设计，而非单纯的领域应用。",
                    "summary2": "本文旨在实现从文本描述到可执行化工过程模拟的端到端自动化设计。针对自然语言输入的化工过程设计场景，我们提出了一种结合增强蒙特卡洛树搜索（E-MCTS）的多智能体LLM工作流，通过四个专门智能体协同工作。在Simona数据集上，通过模拟收敛率（SCR）和设计时间验证了其有效性，相比最先进基线收敛率提升31.1%，设计时间减少89.0%。",
                    "summary_translation": "Process simulation (过程模拟) 是 Chemical engineering design (化工设计) 的关键基石。当前的 Automated chemical design methodologies (自动化化工设计方法) 主要集中在 Process flow diagrams (工艺流程图) 的各种表示形式上。然而，将这些图表转化为 Executable simulation flowsheets (可执行模拟流程) 仍然是一项耗时且费力的工作，需要在 Simulation software (模拟软件) 中进行大量的 Manual parameter configuration (手动参数配置)。在这项工作中，我们提出了一种新颖的 Multi-agent workflow (多智能体工作流)，该工作流利用 Large language models (LLMs, 大语言模型) 的 Semantic understanding (语义理解) 能力，并实现与 Chemical process simulation software (化工过程模拟软件) 的 Iterative interactions (迭代交互)，从而实现了从 Textual process specifications (文本过程规范) 到用于设计增强的 Computationally validated software configurations (计算验证的软件配置) 的 End-to-end automated simulation (端到端自动模拟)。我们的方法集成了四个分别负责 Task understanding (任务理解)、Topology generation (拓扑生成)、Parameter configuration (参数配置) 和 Evaluation analysis (评估分析) 的 Specialized agents (专门智能体)，并结合 Enhanced Monte Carlo Tree Search (增强蒙特卡洛树搜索) 来准确解释语义并稳健地生成配置。在大规模 Process description dataset (过程描述数据集) Simona 上进行评估，我们的方法与 State-of-the-art baselines (最先进基线) 相比，Simulation convergence rate (模拟收敛率) 提高了 31.1%，与专家 Manual design (手动设计) 相比，Design time (设计时间) 减少了 89.0%。这项工作展示了 AI-assisted chemical process design (AI辅助化工过程设计) 的潜力，弥合了 Conceptual design (概念设计) 与 Practical implementation (实际实施) 之间的差距。我们的 Workflow (工作流) 适用于包括 Pharmaceuticals (制药)、Petrochemicals (石化)、Food processing (食品加工) 和 Manufacturing (制造业) 在内的多种 Process-oriented industries (流程导向型行业)，为 Automated process design (自动化过程设计) 提供了一种 Generalizable solution (可推广解决方案)。",
                    "inspiration_trace": "基于论文《From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论产出的思考过程：\n\n### 第一阶段：宏观问题定位——“最后一公里”的瓶颈\n**观察：** 化工过程设计是工业的核心，但目前的流程极其低效。工程师需要花费数周时间，将高层的概念设计（如“设计一个乙烯裂解过程”）转化为可在模拟软件（如Aspen Plus等）中运行的详细配置。\n**痛点识别：** 现有的自动化方法大多集中在“画图”阶段（生成流程图PFD或超图结构），但这只是设计的中间态。真正的瓶颈在于从“结构图”到“可执行仿真配置”的转化——这需要人工设定成百上千个相互依赖的热力学和操作参数。\n**核心矛盾：** 概念设计与工程落地之间存在巨大的鸿沟。AI能生成漂亮的图纸，但图纸无法直接运行，无法验证可行性。\n\n### 第二阶段：现有方案的局限性分析\n**反思：** 为什么现有的AI方法（如CNN、GNN或早期的LLM应用）解决不了这个问题？\n**结论：**\n1.  **停留在表征层：** 现有方法将设计视为静态的图像或图结构生成任务，忽略了化工过程本质上是基于物理化学方程的动态计算过程。\n2.  **缺乏闭环验证：** 生成的结构如果没有经过模拟软件的严格计算，往往是不收敛或不可行的。现有方法缺乏与专业仿真软件的交互能力。\n3.  **语义与参数的割裂：** LLM擅长理解自然语言（语义），但很难直接生成符合复杂物理约束的精确参数（数值）。\n\n### 第三阶段：核心假设提出——“人机协作”的代理化\n**假设：** 如果能构建一个系统，模仿人类专家的思维方式——即“理解意图 -> 搭建结构 -> 设定参数 -> 软件试算 -> 根据报错调整”，并利用LLM处理语义，利用仿真软件处理物理计算，就能打通从文本到仿真的全链路。\n**关键转变：** 从“一次性生成”转变为“迭代式交互”。不再追求LLM直接写出完美的代码，而是允许它通过工具与仿真软件进行多轮对话，直到收敛。\n\n### 第四阶段：方法论构建——多智能体分工\n**思考：** 化工设计任务过于复杂，单个LLM无法同时兼顾语义理解、拓扑规划、参数计算和结果评估。必须进行“分而治之”。\n**逻辑推演：**\n1.  **任务理解：** 首先需要将模糊的自然语言转化为结构化的工程需求（如明确组分、约束条件）。\n2.  **拓扑生成：** 专注于“骨架”搭建，确定单元操作（反应器、精馏塔）及其连接关系，暂时不纠结细节。\n3.  **参数配置：** 专注于“血肉”填充，利用LLM的推理能力结合领域知识，为拓扑赋予初始参数。\n4.  **评估分析：** 充当“质检员”，接收仿真软件的反馈（是否收敛、经济性如何），并决定是输出结果还是反馈修改。\n\n### 第五阶段：搜索策略优化——如何处理“失败”\n**深层挑战：** 化工设计空间巨大，且充满了“陷阱”。很多设计在仿真中会失败（不收敛）。传统的搜索算法（如标准MCTS）通常会直接丢弃失败的分支。\n**创新洞察：** 在化工设计中，一个“失败”的仿真往往包含有价值的信息（例如拓扑结构是对的，只是某个温度参数设错了）。如果直接丢弃，就浪费了探索成本。\n**策略演进：** 提出**增强型蒙特卡洛树搜索（E-MCTS）**。\n1.  **双重价值评估：** 区分“当前价值”（仿真是否成功）和“潜在价值”（结构是否合理）。即使仿真失败，如果结构合理，仍保留其探索潜力。\n2.  **动态重访机制：** 当搜索陷入停滞时，主动回到那些曾经失败但潜力巨大的节点进行微调，从而跳出局部最优，找到真正可执行的解。\n\n### 总结：逻辑链的全景图\n作者从**“设计效率低”**的宏观问题出发，识别出**“结构到可执行配置的断层”**这一核心痛点。通过分析现有AI**“重表征、轻验证”**的缺陷，提出了**“LLM语义理解 + 仿真软件物理验证”**的闭环假设。为了实现这一假设，作者采用了**多智能体协作**来解耦复杂任务，并创新性地设计了**E-MCTS算法**来从失败中学习，最终实现了从自然语言文本到工业级仿真配置的端到端自动化。"
                },
                {
                    "title": "Agentic AI Empowered Intent-Based Networking for 6G",
                    "arxiv_id": "2601.06640",
                    "authors": "Genze Jiang, Kezhi Wang, Xiaomin Chen, Yizhou Huang",
                    "summary": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个分层多智能体框架，包含编排器智能体和领域专家智能体（RAN和核心网络），它们通过ReAct循环进行协作和通信以解决网络配置问题。这完全符合“多智能体：协作、通信”的研究范围，且核心贡献在于智能体架构而非单纯的基础设施优化。",
                    "summary2": "本文旨在解决6G网络中将高层自然语言意图转化为可执行网络配置的自主编排问题。针对自然语言操作意图，我们提出了一种基于LLM的分层多智能体框架，通过Orchestrator协调RAN和Core专家代理进行ReAct推理，并在包含12个场景的6G基准测试中，通过Semantic Accuracy和Engineering Utility验证了其有效性。",
                    "summary_translation": "向第六代（6G）无线网络的演进迫切需要一种自主编排机制，该机制能够将高层运维意图转化为可执行的网络配置。现有的基于意图的网络（Intent-Based Networking, IBN）方法要么依赖于难以应对语言差异的基于规则的系统，要么依赖于缺乏可解释性且无法强制执行运维约束的端到端神经模型。本文提出了一种分层多智能体框架，其中基于大语言模型（Large Language Model, LLM）的智能体能够自主分解自然语言意图，咨询领域特定专家，并通过迭代推理-行动（Reasoning-action, ReAct）循环综合生成技术上可行的网络切片配置。该架构采用一个编排器智能体，通过基于结构化网络状态表示的ReAct风格推理，协调两个专家智能体，即无线接入网（Radio Access Network, RAN）智能体和核心网智能体。在多种基准场景下的实验评估表明，该系统优于基于规则的系统和直接LLM提示方法，且其架构原则适用于开放无线接入网（Open RAN, O-RAN）部署。结果还表明，尽管当代大语言模型（LLM）具备通用的电信知识，但网络自动化仍需通过精细的提示工程来编码上下文相关的决策阈值，从而推进下一代无线系统的自主编排能力。",
                    "inspiration_trace": "基于论文《Agentic AI Empowered Intent-Based Networking for 6G》，以下是对作者核心方法论产出过程的逻辑推演与思想还原：\n\n### 1. 宏观背景与问题锚定：6G时代的“语义鸿沟”\n**思考起点：** 6G网络愿景的核心是“零接触”自动化与无处不在的智能。未来的网络管理不应依赖工程师手动敲击命令行，而应允许运营商通过自然语言描述高层业务目标（即“意图”），由系统自动转化为底层的网络配置。\n\n**核心矛盾：** 现有的意图网络（IBN）方案存在两极分化的缺陷：\n*   **基于规则的系统：** 虽然严谨，但极其死板。一旦自然语言表述稍有变化（如将“低延迟”改为“即时响应”），系统便无法识别，缺乏泛化能力。\n*   **端到端的神经网络/传统ML：** 虽然能处理数据，但属于“黑盒”，缺乏可解释性，且难以强制执行严格的操作约束（如“必须小于10ms”），无法满足电信级的安全要求。\n\n**结论：** 我们需要一种既能理解自然语言的灵活性，又能像专家系统一样执行严格逻辑推理的新范式。\n\n### 2. 技术选型与范式转移：从“聊天机器人”到“智能体”\n**观察：** 大语言模型（LLM）展现了惊人的语义理解能力，似乎是填补“语义鸿沟”的完美工具。然而，直接向LLM提问（单次Prompt）存在致命弱点——LLM本质上是一个文本生成器，而非决策引擎。它无法自主验证配置的可行性，无法感知当前网络状态，且容易产生“幻觉”。\n\n**假设：** 如果不把LLM仅仅当作一个问答接口，而是将其置于一个具备“感知-规划-行动”能力的架构中，使其成为**Agentic AI（智能体AI）**，是否能解决问题？\n\n**方法论引入：** 引入**ReAct（Reasoning + Acting）**范式。即让LLM不仅生成答案，还要生成“思考过程”和“行动指令”，通过与环境交互（如查询网络状态）来迭代修正决策，从而实现多步推理。\n\n### 3. 架构演进：从单体智能到分层协作\n**挑战：** 6G网络极其复杂，涵盖无线接入网（RAN）、核心网等多个领域。让单一的LLM智能体掌握所有领域的知识并处理所有约束，认知负荷过重，容易导致推理混乱和错误。\n\n**思路突破：** 模仿人类企业的组织架构——**分工与协作**。\n*   **编排者：** 扮演项目经理角色，负责理解用户意图、拆解任务、协调资源。\n*   **领域专家：** 扮演技术顾问角色。设立RAN专家（负责频谱、基站）和Core网专家（负责UPF部署、拓扑）。\n\n**逻辑闭环：** 编排者不直接做技术决策，而是将意图转化为子问题，咨询相应的专家。专家基于注入的当前网络状态（结构化数据）给出建议，编排者汇总建议并生成最终配置。这种分层架构既降低了单点复杂度，又保证了决策的专业性。\n\n### 4. 落地机制：知识注入与状态锚定\n**问题：** LLM虽然通晓电信理论，但不知道当前网络的具体状态（如哪个基站负载过高），也不懂运营商特定的隐性偏好（如成本优先还是性能优先）。\n\n**解决方案：**\n*   **状态锚定：** 将实时的网络状态（负载、延迟矩阵、频谱可用性）转化为结构化的JSON数据，在每次推理时“注入”给智能体，防止其凭空捏造。\n*   **提示词工程即软代码：** 将电信领域的专家知识（如“URLLC业务必须选边缘节点”、“负载超过80%需预警”）编码进System Prompt中。这不仅是提示技巧，更是将领域知识固化为系统逻辑的过程。\n\n### 5. 评估视角的重构：语义与工程的双重校验\n**反思：** 传统的AI评估只看“准确率”。但在网络配置中，仅仅“听懂了”是不够的，配置必须“工程上可行”且“最优”。\n\n**评估框架创新：** 提出双重指标体系：\n*   **语义准确性：** 生成配置是否符合人类专家的预期（是否听懂了人话）。\n*   **工程效用：** 配置在数学上是否满足QoS约束（如延迟公式、资源利用率），是否是最优解。\n\n### 6. 实验洞察与偏差修正：对“语言”的再认识\n**意外发现：** 在实验中，作者发现系统存在一种“延迟贪婪”偏差——无论什么业务，智能体总是倾向于选择延迟最低的节点，导致资源浪费。\n\n**深层思考：** 这揭示了LLM的一个特性：**对提示词语义的极度敏感**。Prompt中微小的措辞差异（如说“可接受”还是“优先选择”）会引发系统性的行为偏差。\n\n**最终完善：** 这促使作者将Prompt工程提升到了核心架构组件的高度。通过迭代修正Prompt，明确指令（如“非URLLC业务必须优先使用区域数据中心以节约成本”），消除了偏差。这证明了在Agentic AI中，**如何定义智能体的“性格”和“规则”与架构本身同等重要"
                },
                {
                    "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
                    "arxiv_id": "2601.06502",
                    "authors": "Shengkai Chen, Zhiguang Cao, Jianan Zhou, Yaoxin Wu, Senthilnath Jayavelu, Zhuoyi Lin, Xiaoli Li, Shili Xiang",
                    "summary": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了DRAGON框架，明确使用了“Agents”和“language agents”概念。文中描述了智能体自主识别区域、分解问题（规划）、利用自适应经验记忆（记忆）、与环境交互并从反馈中迭代学习（自我反思/演化）。这完全符合单智能体和自我演化的研究范围，且不属于纯应用或纯推理排除项。",
                    "summary2": "本文旨在解决大语言模型（LLM）在大规模组合优化问题（COP）中可扩展性受限的问题。针对大规模COP场景，我们提出了一种名为DRAGON的分解与重构智能体框架，通过迭代识别高潜力区域并求解局部子问题来优化全局解。在TSPLIB、CVRPLIB和Weibull-5k等基准数据集上，通过Optimality Gap等指标验证了其有效性，在超大规模实例上实现了近最优解，显著优于现有基于LLM的求解器。",
                    "summary_translation": "大语言模型近期在利用基于提示的策略解决组合优化问题方面展现出潜力。然而，其可扩展性和泛化能力仍然受限，且随着问题规模的增大，其有效性会降低，特别是在涉及超过30个节点的路径问题中尤为明显。我们提出了 DRAGON（Decomposition and Reconstruction Agents Guided OptimizatioN，分解与重构智能体引导优化），这是一个结合了元启发式设计和 LLM 推理优势的新型框架。DRAGON 从一个初始全局解出发，自主识别具有高优化潜力的区域，并策略性地将大规模 COPs 分解为易于处理的子问题。随后，每个子问题被重新表述为一个简洁的局部优化任务，并在积累经验的指导下，通过针对性的 LLM 提示进行求解。最后，将局部优化后的解系统地重新整合到原始全局上下文中，从而产生显著改善的整体结果。通过与优化环境的持续交互并利用自适应经验记忆，智能体能够从反馈中迭代学习，从而有效地将符号推理与启发式搜索相结合。实验结果表明，与局限于小规模实例的现有基于 LLM 的求解器不同，DRAGON 在 TSPLIB、CVRPLIB 和 Weibull-5k 装箱基准测试中始终能生成可行解，并在拥有超过 300 万变量的背包问题上取得了接近最优的结果（0.16% gap）。这项工作展示了反馈驱动的语言智能体作为一种可泛化且可解释的大规模优化新范式的潜力。",
                    "inspiration_trace": "基于论文《DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization》，以下是对作者提出核心方法的逻辑链推演，旨在还原其从宏观观察到具体方法论产出的思考过程。\n\n---\n\n### 1. 宏观观察与矛盾识别：LLM的“能力”与“尺度”错位\n**思考起点：**\n作者首先观察到大语言模型（LLM）在解决组合优化问题（COP）上展现出了惊人的潜力，尤其是在逻辑推理和模式识别方面。然而，这种能力存在明显的“尺度天花板”。\n\n**逻辑推演：**\n*   **现象：** 现有的基于Prompt的LLM方法（如OPRO, SGE）在处理小规模问题（如节点数<30的TSP）时表现尚可，但一旦问题规模扩大到现实世界级别（如成千上万个节点），LLM的表现急剧下降。\n*   **归因：** 这种下降并非因为LLM不懂优化原理，而是受限于其**上下文窗口长度**和**长序列生成的逻辑连贯性**。直接让LLM一次性生成大规模问题的解，就像让一个人心算一本电话簿的排序，既不可行也不可靠。\n*   **核心矛盾：** 我们需要LLM的**通用推理能力**，但无法承受其在大规模问题上的**计算与记忆局限性**。\n\n### 2. 跨域借鉴：从传统运筹学中寻找“破局点”\n**思考转折：**\n既然LLM无法“一口吃成胖子”，作者将目光转向了传统运筹学中处理大规模问题的成熟策略——**元启发式算法**，特别是**大规模邻域搜索**。\n\n**逻辑推演：**\n*   **传统智慧：** LNS的核心思想不是一次性解决整个问题，而是“破坏”当前解的一部分，然后“修复”它。这种“分而治之”的策略完美规避了全局计算的复杂性。\n*   **痛点分析：** 传统的LNS虽然能扩展规模，但其高度依赖**人工设计的启发式规则**（例如：如何选择破坏区域？如何修复？）。这些规则往往针对特定问题，缺乏泛化性，且设计成本极高。\n*   **假设提出：** 能否用LLM来替代这些“人工规则”？即，利用LLM的语义理解能力来决定“哪里需要优化”，以及利用LLM的推理能力来执行“如何优化”。\n\n### 3. 核心假设形成：LLM作为“智能拆解者”与“局部修复者”\n**思考聚焦：**\n基于上述矛盾与借鉴，作者提出了两个关键的研究假设，构成了DRAGON框架的理论基石：\n\n*   **假设一（分解）：** LLM虽然无法直接解决大规模COP，但它具备足够的“直觉”来审视一个全局解，并识别出其中**看起来不合理或具有改进潜力的局部区域**（Active Segment）。\n*   **假设二（重构）：** 如果将大规模问题压缩为一个仅包含几十个节点的局部子问题，LLM完全有能力在遵守特定边界约束的前提下，找到该子问题的**局部最优解**。\n\n### 4. 方法论构建：从“直觉”到“闭环”的机制设计\n**思考深化：**\n有了假设，接下来需要解决具体的工程与逻辑问题：如何保证局部修改后的解能无缝融入全局？如何处理LLM生成的不可行解？\n\n**逻辑演进：**\n\n*   **阶段一：动态分解**\n    *   *设计思路：* 作者设计了一个“分解者”Agent。它的任务不是求解，而是“挑刺”。它将全局解分为两部分：保持不变的**静态段**和待优化的**活跃段**。\n    *   *关键点：* 这种分解不是随机的，而是基于LLM对当前解质量的评估，从而模仿了人类专家的直觉。\n\n*   **阶段二：约束感知的重构**\n    *   *设计思路：* 作者设计了一个“重构者”Agent。它接收压缩后的子问题。\n    *   *难点攻克：* 为了防止局部优化破坏全局可行性（例如路径断开），作者引入了**显式约束**。将静态段与活跃段的连接点转化为自然语言约束，强制LLM在修复时必须保留这些连接。\n\n*   **阶段三：经验驱动的自我修正**\n    *   *设计思路：* LLM偶尔会生成违反约束的解。作者没有选择简单的丢弃，而是引入了**经验记忆**。\n    *   *逻辑闭环：* 将之前的错误解及其原因反馈给LLM，让其进行反思和修正。这形成了一个“尝试-反馈-修正”的微循环，确保了重构阶段的鲁棒性。\n\n### 5. 最终框架确立：DRAGON的诞生\n**思考综合：**\n将上述环节串联，作者最终构建了DRAGON框架。这不再是一个简单的Prompt调用，而是一个**迭代的、状态传递的多智能体系统**。\n\n*   **逻辑链闭环：**\n    1.  **初始解**（由传统快速启发式获得）。\n    2.  **分解**（LLM识别薄弱环节）。\n    3.  **压缩**（提取局部子问题及约束）。\n    4.  **重构**（LLM在约束下求解局部问题）。\n    5.  **整合与评估**（将局部解拼回全局，若更优则接受）。\n    6.  **循环**（重复上述过程，直到收敛）。\n\n### 总结\n作者的思考路径遵循了**“发现问题（LLM尺度限制） -> 借鉴经典（分治思想） -> 融合创新（LLM替代人工规则） -> 机制完善（约束与反馈）”**的逻辑链条。DRAGON的本质，是将LLM从一个“全知全能但容易过载的求解者”，重塑为一个“专注于局部精修且具备全局视野的智能工匠”。"
                },
                {
                    "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
                    "arxiv_id": "2601.06377",
                    "authors": "Ningning Zhang, Xingxing Yang, Zhizhong Tan, Weiping Deng, Wenyong Wang",
                    "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了HiMem，这是一个专门为LLM长跨度智能体设计的分层长期记忆框架。它重点研究了智能体的核心组件——记忆（包括记忆构建、检索和动态更新），并引入了冲突感知的记忆再巩固机制以实现自我演化。这完全符合单智能体研究范围中的“记忆”和“自我演化”标准。",
                    "summary2": "本文旨在解决LLM智能体在长期交互中记忆适应性、可扩展性和自我进化不足的问题。针对长跨度对话场景，我们提出了一种名为HiMem的分层长期记忆框架，该框架通过Topic-Aware Event–Surprise Dual-Channel Segmentation构建Episode Memory，并结合冲突感知的Memory Reconsolidation机制。我们在LoCoMo benchmark上通过GPT-Score和F1指标验证了其有效性，结果显示HiMem在准确性和一致性上优于现有基线。",
                    "summary_translation": "尽管长期记忆系统近年来取得了显著进展，但在持续交互场景下的适应性、可扩展性和自我进化方面仍存在明显局限。受认知理论启发，我们提出了 HiMem，这是一个面向长程对话的分层长期记忆框架，旨在支持持续交互过程中的记忆构建、检索和动态更新。HiMem 通过 Topic-Aware Event--Surprise Dual-Channel Segmentation（主题感知的事件-惊喜双通道分割）策略构建认知一致的 Episode Memory（情景记忆），并通过多阶段信息提取流水线构建能够捕获稳定知识的 Note Memory（笔记记忆）。这两种记忆类型在语义上相互关联，形成了一种桥接具体交互事件与抽象知识的分层结构，从而在不牺牲信息保真度的情况下实现高效检索。HiMem 支持混合检索和 Best-Effort Retrieval（尽力而为检索）策略以平衡准确性与效率，并结合 Conflict-Aware Memory Reconsolidation（冲突感知的记忆再巩固）机制，根据检索反馈对存储的知识进行修正和补充。这种设计使得记忆能够在长期使用过程中实现持续的自我进化。在长程对话基准上的实验结果表明，HiMem 在准确性、一致性和长程推理方面始终优于代表性基线，同时保持了良好的效率。总体而言，HiMem 为构建自适应且自我进化的 LLM-based（基于大语言模型）对话智能体提供了一个有原则且可扩展的设计范式。代码可在 https://github.com/jojopdq/HiMem 获取。",
                    "inspiration_trace": "基于论文《HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观观察：LLM 的“金鱼记忆”困境\n**起点：** 作者首先观察到，尽管大语言模型（LLM）在单轮或短对话中表现优异，但在面对**长跨度、多轮次**的持续交互任务（如长期个人助理）时，存在根本性缺陷。\n**核心矛盾：** 现有的 LLM Agent 无法在长时间跨度内可靠地保存、组织和利用信息。这不仅是“记不住”的问题，更是“记不好”和“用不活”的问题。\n\n### 2. 问题诊断：现有方案的三大痛点\n作者分析了现有的三类主流方案（RAG、长上下文、结构化记忆），发现它们在长周期交互中存在三个无法同时解决的系统性缺陷：\n\n*   **痛点一：保真度与效率的零和博弈**\n    *   *现象：* 保留原始对话日志（保真度高）会导致检索成本高昂且充满噪声；而过度压缩摘要（效率高）会丢失推理所需的细节。\n    *   *结论：* 单一扁平的存储结构无法兼顾细节保留与检索效率。\n*   **痛点二：语义错位**\n    *   *现象：* 提取的记忆往往脱离原始语境，导致在处理时间指代、共指消解和隐含语义时出错。\n    *   *结论：* 记忆的表示方式缺乏统一的语义对齐机制。\n*   **痛点三：静态与僵化的更新机制**\n    *   *现象：* 现有系统通常是“只增不改”或仅基于相似度更新。当新信息与旧记忆冲突或互补时，缺乏修正和进化的能力。\n    *   *结论：* 记忆系统缺乏自我演化和纠错的能力。\n\n### 3. 认知启发：向人类记忆机制借力\n**转折点：** 为了解决上述痛点，作者从认知心理学中寻找灵感。人类记忆并非单一仓库，而是分层运作的：\n*   **情景记忆：** 记录具体的经历和事件（细节丰富，但碎片化）。\n*   **语义记忆：** 提炼出的知识和常识（抽象稳定，但脱离具体语境）。\n*   **记忆再巩固：** 当回忆失败或遇到冲突时，人类会重构记忆。\n\n**假设：** 如果能构建一个模仿这种分层结构的 LLM 记忆框架，就能在保留细节的同时提高效率，并实现动态更新。\n\n### 4. 架构构想：分层记忆的提出\n基于认知假设，作者提出了**HiMem** 的核心架构逻辑：\n\n*   **第一层：情景记忆**\n    *   *目标：* 解决“保真度”问题。保留细粒度的交互事件。\n    *   *思考：* 如何切分对话才符合认知？简单的按句或按段切分不够智能。必须结合**话题转换**和**意外/情绪突变**（即“事件-惊喜”双通道），确保每个片段在认知上是连贯的。\n*   **第二层：笔记记忆**\n    *   *目标：* 解决“效率”问题。存储稳定的知识（事实、偏好、画像）。\n    *   *思考：* 需要多阶段提取（先提取事实，再提取隐含信息，最后归一化），避免信息坍塌，并建立统一的语义空间（时间对齐、指代消解）。\n*   **层级关联：** 将两层记忆语义链接，形成从具体事件到抽象知识的过渡。\n\n### 5. 机制深化：检索与进化的闭环\n有了架构，还需要解决“怎么用”和“怎么变”的问题：\n\n*   **检索策略：混合与尽力而为**\n    *   *思考：* 为了平衡速度和准确率，不应总是检索所有层级。\n    *   *设计：* **Best-Effort 策略**——先查抽象的 Note Memory（快），如果证据不足，再下沉查 Episode Memory（准）。这模仿了人类先想常识，再回忆细节的过程。\n*   **自我进化：冲突感知的记忆再巩固**\n    *   *思考：* 如何解决“静态更新”的痛点？检索失败本身就是一种学习信号。\n    *   *设计：* 当 Note Memory 检索失败，但 Episode Memory 能找到证据时，触发**再巩固机制**。系统对比新旧信息，判断是“新增”、“扩展”还是“矛盾”，从而动态修正 Note Memory。这使得记忆系统具备了自我纠错和进化的能力。\n\n### 6. 逻辑总结\n作者的思考路径可以概括为：\n从**长程交互的失效**出发，诊断出**单一结构的局限性**，引入**人类认知的分层理论**作为指导，构建了**情景与语义并存的分层架构**，并利用**检索失败作为反馈信号**，最终实现了一个既能保留细节又能高效进化、具备自我纠错能力的长期记忆系统。\n\n这一逻辑链条体现了从“现象观察”到“理论借鉴”，再到“系统设计”和“动态反馈”的完整学术创新闭环。"
                },
                {
                    "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation",
                    "arxiv_id": "2601.06328",
                    "authors": "Ziqiao Xi, Shuang Liang, Qi Liu, Jiaqing Zhang, Letian Peng, Fang Nan, Meshal Nayim, Tianhui Zhang, Rishika Mundada, Lianhui Qin, Biwei Huang, Kun Zhou",
                    "summary": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.",
                    "category": "cs.AI",
                    "filter_reason": "论文专注于LLM智能体的工具使用能力，提出了一个开放世界环境用于智能体测试，并开发了包含规划器和执行者的智能体框架，涉及规划、自我修正等单智能体核心能力，符合筛选标准。",
                    "summary2": "本文旨在解决现有工具使用代理在开放世界环境中缺乏大规模、真实测试与训练环境的问题。针对大规模工具池、长时程任务及不可靠状态等场景，我们提出了ToolGym环境，该环境集成了任务创建引擎、状态控制器及Planner–Actor代理框架。我们在包含5,571个工具的ToolGym环境上，通过Success Rate、Recovery Rate等指标验证了其有效性，并证明利用其生成的少量数据微调模型即可超越大规模数据基线。",
                    "summary_translation": "使用工具的 Tool-using LLM agents (使用工具的大语言模型智能体) 在 open-world settings (开放世界设置) 中仍面临挑战，这些设置包含大型工具池、long-horizon objectives (长期目标)、wild constraints (复杂约束) 以及不可靠的工具状态。为了实现可扩展且真实的训练与测试，我们引入了一个开放世界工具使用环境，该环境构建于 204 个常用应用程序中的 5,571 个格式统一的工具之上。该环境包含一个 task creation engine (任务创建引擎)，用于合成具有 wild constraints (复杂约束) 的长期、多工具工作流，以及一个 state controller (状态控制器)，用于注入中断和故障以对鲁棒性进行压力测试。基于该环境，我们开发了一个 tool select-then-execute agent framework (工具选择-然后-执行智能体框架)，采用 planner-actor decomposition (规划者-执行者分解) 架构，将深思熟虑的推理和自我纠正与逐步执行分离开来。对最先进的 LLM (Large Language Model，大语言模型) 的全面评估揭示了工具规划与执行能力之间的错位、现有 LLM 在遵循约束方面的弱点，以及 DeepSeek-v3.2 最强的鲁棒性。最后，我们从该环境中收集了 1,170 条 trajectories (轨迹) 来 fine-tune (微调) LLM (Large Language Model，大语言模型)，其性能优于使用 119k 样本的 baselines (基线模型)，这表明该环境既是一个真实的 benchmark (基准)，也是 tool-using agents (工具使用智能体) 的一个有价值的 data engine (数据引擎)。我们的代码和数据将公开发布。",
                    "inspiration_trace": "基于论文《ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 1. 宏观观察：从“玩具级”到“开放世界”的鸿沟\n**起点：** 作者首先观察到 LLM 智能体在工具使用领域虽然发展迅速，但在实际落地中存在巨大落差。\n**现象：** 现有的 SOTA 模型在标准基准测试中分数很高，但在真实应用场景中表现不佳。\n**矛盾：** 真实世界是“开放”的——工具池巨大、任务链条长、约束条件模糊且充满冲突、工具状态不可靠。而现有的评估环境大多是“封闭”且“洁净”的，只测试“快乐路径”，无法暴露智能体在复杂环境下的真实缺陷。\n\n### 2. 核心假设：真实世界的“野性”是关键试金石\n**推论：** 要提升智能体的真实能力，不能继续在简化的沙盒中打磨，必须构建一个能模拟真实世界复杂度的“开放世界”环境。\n**定义问题：** 这个环境必须具备三个维度的“野性”：\n1.  **规模野性：** 海量且真实的工具库，而非几十个精心挑选的 API。\n2.  **约束野性：** 任务包含长时程、多工具协作以及相互冲突的复杂约束。\n3.  **状态野性：** 模拟真实世界的不可靠性（如超时、报错、状态变更），而非理想化的稳定响应。\n\n### 3. 环境构建：如何模拟“野性”？\n为了验证上述假设，作者着手构建 ToolGym，其设计逻辑遵循从“基础”到“动态”的演进：\n\n*   **基础层（工具标准化）：** 面对海量异构工具，首先解决“统一接口”问题。作者选择 MCP (Model Context Protocol) 作为标准，整合了 5,571 个真实工具，构建了一个可检索、可执行的庞大工具池，解决了“规模野性”。\n*   **任务层（自动化合成）：** 人工编写复杂任务成本太高。作者提出“任务创建引擎”，利用 LLM 自动合成包含“野性约束”的长时程任务。通过迭代反馈机制，确保任务不仅需要多工具协作，还包含复杂的逻辑依赖和冲突，解决了“约束野性”。\n*   **交互层（状态控制）：** 为了测试鲁棒性，作者引入“状态控制器”。这不仅仅是随机噪声，而是一个中间件机制，能够有策略地注入故障（如工具级超时、状态级篡改、约束级变更），从而主动制造困难，解决了“状态野性”。\n\n### 4. 架构演进：应对长时程复杂性的解耦策略\n在构建了环境后，作者思考：**什么样的智能体架构才能在这样的环境中生存？**\n**痛点分析：** 在长时程、高复杂度的任务中，单一的 ReAct 模式容易陷入“迷失”——模型难以在几十步的执行中保持全局目标的一致性，且容易在错误发生后无法恢复。\n**解决思路：** 借鉴人类解决复杂问题的思维模式，将“思考”与“行动”解耦。\n**方法论产出：** 提出 **Planner-Actor 框架**。\n*   **Planner（规划者）：** 负责宏观视角，进行任务分解、全局推理和自我纠正。它不直接调用工具，而是监控进度，确保不偏离目标。\n*   **Actor（执行者）：** 负责微观视角，专注于具体的工具检索、参数填充和步骤执行。\n*   **逻辑闭环：** 这种分离使得模型既能进行深思熟虑的规划，又能保持执行的敏捷性，同时 Planner 的介入机制专门用于解决长时程中的“漂移”问题。\n\n### 5. 价值闭环：从测试台到数据引擎\n**实验发现：** 利用 ToolGym 评估主流模型，作者发现了有趣的“错位”现象——模型普遍规划能力强，但执行能力弱；且“遵循约束”比“调用工具”更难。\n**最终升华：** 作者意识到，这个环境不仅能用来“考”模型，还能用来“教”模型。\n**逻辑延伸：** 既然环境能生成高难度、高复杂度的真实轨迹，那么这些轨迹就是最高质量的训练数据。\n**结论验证：** 实验证明，仅用 ToolGym 生成的 1,170 条高质量数据进行微调，效果优于使用 119k 条普通数据的基线。这证明了**“在真实野性环境中通过高难度试错获得的数据”具有极高的信息密度和训练价值**。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现实与评估的脱节**出发，通过**构建高保真的开放环境**来还原真实挑战，进而**设计解耦的智能体架构**以适应这种挑战，最后**将环境转化为数据引擎**，实现了从评估到训练的完整闭环。"
                },
                {
                    "title": "PCoKG: Personality-aware Commonsense Reasoning with Debate",
                    "arxiv_id": "2601.06234",
                    "authors": "Weijie Li, Zhongqing Wang, Guodong Zhou",
                    "summary": "Most commonsense reasoning models overlook the influence of personality traits, limiting their effectiveness in personalized systems such as dialogue generation. To address this limitation, we introduce the Personality-aware Commonsense Knowledge Graph (PCoKG), a structured dataset comprising 521,316 quadruples. We begin by employing three evaluators to score and filter events from the ATOMIC dataset, selecting those that are likely to elicit diverse reasoning patterns across different personality types. For knowledge graph construction, we leverage the role-playing capabilities of large language models (LLMs) to perform reasoning tasks. To enhance the quality of the generated knowledge, we incorporate a debate mechanism consisting of a proponent, an opponent, and a judge, which iteratively refines the outputs through feedback loops. We evaluate the dataset from multiple perspectives and conduct fine-tuning and ablation experiments using multiple LLM backbones to assess PCoKG's robustness and the effectiveness of its construction pipeline. Our LoRA-based fine-tuning results indicate a positive correlation between model performance and the parameter scale of the base models. Finally, we apply PCoKG to persona-based dialogue generation, where it demonstrates improved consistency between generated responses and reference outputs. This work bridges the gap between commonsense reasoning and individual cognitive differences, enabling the development of more personalized and context-aware AI systems.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一种由支持者、反对者和法官组成的辩论机制，通过多智能体交互（协作与博弈）来迭代完善知识图谱的构建，符合“多智能体：协作、通信、博弈”的研究范围。",
                    "summary2": "本文旨在解决现有常识推理模型忽略性格特征导致个性化能力不足的问题。针对个性化对话生成场景，我们提出了一种基于多智能体辩论机制的 Personality-aware Commonsense Knowledge Graph (PCoKG) 构建方法。该方法利用 LLM 角色扮演能力，通过支持者、反对者和法官的辩论机制生成高质量的四元组数据。我们在 PCoKG 数据集及 SPC 对话任务上，通过 BLEU-4 和 ROUGE 等指标验证了其有效性。",
                    "summary_translation": "大多数 commonsense reasoning models (常识推理模型) 忽视了 personality traits (人格特质) 的影响，限制了其在 dialogue generation (对话生成) 等个性化系统中的有效性。为了解决这一局限性，我们提出了 Personality-aware Commonsense Knowledge Graph (PCoKG，人格感知常识知识图谱)，这是一个包含 521,316 个 quadruples (四元组) 的 structured dataset (结构化数据集)。我们首先采用三个 evaluators (评估者) 对 ATOMIC dataset (ATOMIC 数据集) 中的事件进行评分和筛选，选择那些可能在不同 personality types (人格类型) 中引发多样化 reasoning patterns (推理模式) 的事件。在 knowledge graph construction (知识图谱构建) 方面，我们利用 large language models (LLMs，大型语言模型) 的 role-playing capabilities (角色扮演能力) 来执行 reasoning tasks (推理任务)。为了提高生成知识的质量，我们引入了一种包含 proponent (支持者)、opponent (反对者) 和 judge (评判者) 的 debate mechanism (辩论机制)，通过 feedback loops (反馈循环) 对输出进行 iterative refinement (迭代优化)。我们从多个角度对数据集进行了评估，并使用多个 LLM backbones (LLM 骨干网络) 进行了 fine-tuning (微调) 和 ablation experiments (消融实验)，以评估 PCoKG 的 robustness (鲁棒性) 及其 construction pipeline (构建流程) 的有效性。我们基于 LoRA 的 fine-tuning (微调) 结果表明，模型性能与 base models (基座模型) 的 parameter scale (参数规模) 呈正相关。最后，我们将 PCoKG 应用于 persona-based dialogue generation (基于人格的对话生成)，结果表明生成回复与 reference outputs (参考输出) 之间的 consistency (一致性) 得到了提高。这项工作弥合了 commonsense reasoning (常识推理) 与 individual cognitive differences (个体认知差异) 之间的差距，促进了更加个性化和具备 context-aware (上下文感知) 能力的 AI systems (AI 系统) 的开发。",
                    "inspiration_trace": "基于对论文《PCoKG: Personality-aware Commonsense Reasoning with Debate》的深度分析，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法落地的思考过程：\n\n---\n\n### 1. 宏观观察与问题定义：从“通用”到“个性”的缺失\n**思考起点：**\n作者首先审视了常识推理领域的现状。现有的主流知识图谱（如ATOMIC）和模型（如COMET）虽然能够建立事件与结果之间的因果联系（例如：“如果X发生，通常会导致Y”），但它们隐含了一个假设：**人类的认知是同质化的**。\n\n**逻辑断层：**\n在现实世界中，面对同一事件，不同性格的人往往会产生截然不同的反应。例如，面对“聚会”，内向者可能感到疲惫，而外向者则感到兴奋。现有的通用模型抹杀了这种“认知多样性”，导致生成的回复千篇一律，缺乏个性化色彩。\n\n**核心问题：**\n如何让常识推理模型跳出“平均人”的假设，捕捉并模拟不同人格特质下的差异化认知？\n\n---\n\n### 2. 概念假设与形式化：引入人格维度\n**理论构建：**\n为了解决上述问题，作者提出必须将“人格”这一变量显式地引入常识推理框架中。\n\n**形式化创新：**\n传统的知识图谱结构是三元组 $(e, r, t)$（事件、关系、结果）。作者将其扩展为四元组 $(e, p, r, t)$，其中 $p$ 代表人格信息。\n*   **选择依据：** 作者选择了MBTI（迈尔斯-布里格斯类型指标）作为人格框架。虽然MBTI在心理学界有争议，但在计算领域，它结构清晰、分类明确（16种类型），且大众认知度高，非常适合作为AI模拟的参数。\n\n**初步构想：**\n构建一个包含人格信息的常识知识图谱（PCoKG），使AI能够根据不同的人格类型生成差异化的推理结果。\n\n---\n\n### 3. 执行瓶颈与挑战：数据获取的困境\n**现实阻碍：**\n概念虽然清晰，但构建这样一个大规模数据集面临巨大的现实困难：\n1.  **众包成本高昂：** 传统的知识图谱构建依赖人工标注。要招募覆盖16种MBTI类型的大规模人群，并让他们针对特定事件进行推理，成本极高且难以管理。\n2.  **数据质量难控：** 即使有人力，如何保证标注者真的在扮演对应的人格？如何保证推理的深度和一致性？\n\n**思维转折：**\n既然人工众包不可行，必须寻找自动化、可扩展的替代方案。此时，大语言模型（LLMs）展现出的强大的角色扮演能力进入了作者的视野。\n\n---\n\n### 4. 方法论演进：从“简单模拟”到“质量控制”\n作者意识到，直接让LLM进行角色扮演虽然可行，但输出质量参差不齐。为了构建高质量的数据集，作者设计了层层递进的三个关键机制：\n\n#### 4.1 第一层思考：筛选“值得推理”的事件\n**逻辑：**\n并非所有事件都能引发人格差异。例如“人需要呼吸”这种生理事件，无论什么人格反应都一样。如果对所有事件都进行人格化推理，会引入大量噪音。\n**解决方案：**\n引入**“评估者机制”**。在生成数据前，先让LLM作为评估者，对ATOMIC中的事件进行打分，筛选出那些“容易引发不同人格产生不同反应”的事件。这保证了数据集的有效性和针对性。\n\n#### 4.2 第二层思考：利用LLM进行规模化生成\n**逻辑：**\n既然筛选出了高质量事件，接下来就是利用LLM的生成能力来替代人工。\n**解决方案：**\n设计Prompt，让LLM扮演特定的MBTI类型，对筛选后的事件进行推理。这解决了“规模化”的问题，能够低成本生成海量数据。\n\n#### 4.3 第三层思考：通过“辩论”提升推理深度\n**逻辑：**\n单次Prompt生成的回答往往流于表面或刻板印象（例如简单地认为内向者就是害羞）。如何让AI的推理更深刻、更符合特定人格的逻辑？\n**解决方案：**\n引入**“多智能体辩论机制”**。\n*   **设计哲学：** 模拟人类学术辩论或批判性思维过程。\n*   **角色分配：** 设定支持者（证明推理符合人格）、反对者（挑战推理的一致性）和法官（裁决并反馈）。\n*   **闭环优化：** 通过多轮辩论和法官的反馈，迫使模型不断修正其推理结果，直到输出高质量、逻辑严密且符合人格设定的内容。\n\n---\n\n### 5. 验证与应用：逻辑闭环的完成\n**思考终点：**\n方法构建完成后，必须验证其有效性。\n1.  **数据质量验证：** 通过可读性分析（验证不同人格的语言风格差异）和互信息分析（验证推理结果与人格类型的关联度），证明生成的数据确实包含了人格信号。\n2.  **下游任务验证：** 将PCoKG应用于个性化对话生成。实验证明，融入了人格感知常识的模型，生成的回复比通用模型更具一致性和拟人化。\n\n---\n\n### 总结：作者的思维演进图谱\n1.  **观察：** 现有常识推理缺乏“个性”，无法模拟人类认知差异。\n2.  **假设：** 将MBTI人格引入知识图谱结构 $(e, p, r, t)$ 可以解决此问题。\n3.  **挑战：** 人工构建数据不可行，且直接生成质量低。\n4.  **破局：**\n    *   用 **LLM角色扮演** 替代人工（解决规模）。\n    *   用 **评估者筛选** 锁定高价值事件（解决噪音）。\n    *   用 **辩论机制** 迭代优化生成质量（解决深度）。\n5.  **产出：** PCoKG数据集及其构建pipeline，实现了高质量、大规模的个性化常识推理。"
                },
                {
                    "title": "HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants",
                    "arxiv_id": "2601.06152",
                    "authors": "Hailong Li, Feifei Li, Wenhui Que, Xingyu Fan",
                    "summary": "Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一种受海马体启发的记忆系统，用于构建个性化AI助手，重点解决了LLM在知识密集型场景中的短期与长期记忆融合问题。这直接属于LLM智能体研究范围中的“单智能体：记忆”模块，且不属于排除的纯应用或纯推理范畴。",
                    "summary2": "本文旨在解决传统RAG在个性化AI助手中的记忆局限问题。针对知识密集型场景，我们提出了一种受海马体启发的HiMeS架构，融合了短期和长期记忆。短期记忆模块利用RLHF压缩对话并预检索，长期记忆模块通过分区存储和注意力机制重排序文档。在真实工业数据集上，通过CA、QA和QR指标验证了其有效性，显著优于传统RAG。",
                    "summary_translation": "大语言模型（Large language models, LLMs）驱动着许多交互系统，例如聊天机器人、客服代理和个人助理。在需要用户特定个性化的知识密集型场景中，传统的检索增强生成（retrieval-augmented generation, RAG）流水线表现出有限的记忆容量，且检索机制与用户特定对话历史之间缺乏协调，从而导致冗余的澄清询问、检索文档不相关以及用户体验下降。受海马体-新皮层记忆机制（hippocampus-neocortex memory mechanism）的启发，我们提出了 HiMeS，一种融合短期和长期记忆的 AI 助手架构。我们的贡献主要体现在以下四个方面：(1) 训练了一个短期记忆提取器（short-term memory extractor），利用强化学习（reinforcement learning）进行端到端训练，以压缩最近的对话并主动从知识库（knowledge base）中预检索文档，从而模拟海马体（hippocampus）与前额叶皮层（prefrontal cortex）之间的协作交互。(2) 构建了一个分区的长期记忆网络（long-term memory network），用于存储用户特定信息并对检索到的文档进行重排序，模拟分布式皮层存储（distributed cortical storage）和记忆再激活（memory reactivation）。(3) 在一个真实世界工业数据集上，HiMeS 在问答质量方面显著优于级联 RAG 基线（cascaded RAG baseline）。(4) 消融实验（Ablation studies）证实了这两个记忆模块的必要性，并为构建更可靠、具备上下文感知（context-aware）及用户定制（user-customized）能力的基于 LLM 的助手指明了实践路径。",
                    "inspiration_trace": "基于论文《HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从观察到方法论的思考过程：\n\n---\n\n### 1. 宏观观察：工业场景下的“个性化”困境\n**起点：** 作者身处腾讯微信的工业环境，观察到一种普遍现象：虽然大语言模型（LLM）和检索增强生成（RAG）技术已经成熟，但在处理**知识密集型**且**高度个性化**的任务（如公众号助手、客服）时，现有系统表现不佳。\n**核心矛盾：** 用户希望AI能像“老朋友”或“专业顾问”一样，基于过往的交互历史和特定背景来回答问题，但现有的AI助手往往是“健忘”的，每次对话都像是从零开始。\n\n### 2. 问题解构：双重记忆缺失\n作者将上述宏观矛盾拆解为两个具体的失效模式：\n\n*   **短期记忆失效（语义错位）：**\n    *   **观察：** 在多轮对话中，用户的当前提问往往省略了前文提到的关键信息（例如：“那它多少钱？”中的“它”指代不明）。\n    *   **传统做法的局限：** 传统RAG直接用当前简短的Query去检索，或者简单地把历史对话拼接到Context Window中。前者导致检索不到相关文档，后者导致注意力分散且效率低下。\n    *   **结论：** 系统缺乏对“当前对话上下文”的有效压缩和利用，导致检索Query与用户真实意图不匹配。\n\n*   **长期记忆失效（灾难性遗忘）：**\n    *   **观察：** 作者发现一个关键指标——**重复提问率（RAR）**高达70-80%。这意味着用户在不同会话中反复问同样的问题，因为系统一旦会话结束就丢弃了数据。\n    *   **结论：** 系统缺乏跨会话的持久化用户画像，无法像人类专家那样积累对用户的“长期印象”，导致无法提供定制化服务。\n\n### 3. 理论映射：海马体-大脑皮层机制的启发\n**思考转折：** 作者跳出纯工程视角，转向认知神经科学寻求答案。\n**类比：** 人类记忆是如何工作的？\n*   **海马体：** 负责短期记忆的编码和快速提取，处理当下的信息。\n*   **大脑皮层：** 负责长期记忆的分布式存储和巩固，在需要时被重新激活。\n**假设：** 如果在AI系统中构建一个模仿“海马体-皮层”协作的双层记忆架构，或许能解决上述短期和长期记忆的缺失问题。\n\n### 4. 方法论演进 I：短期记忆模块（STM）——从“重写”到“对齐”\n**目标：** 解决当前Query的语义缺失问题。\n*   **初步构想：** 训练一个模型把历史对话压缩，重写当前的Query。\n*   **批判性思考：** 传统的监督微调（SFT）只是让模型模仿“重写”的风格，并不保证重写后的Query能检索到更好的文档，也不保证最终回答质量更高。这是“局部最优”而非“全局最优”。\n*   **进阶方案：** 引入**强化学习（RL）**。\n    *   **逻辑：** 不再只看“重写得好不好”，而是看“最终回答得好不好”。将重写器、检索器和生成器视为一个整体，通过端到端的奖励信号（如Rouge-L、Exact Match、Hit Score）来反向优化重写策略。\n    *   **生物学对应：** 这模拟了海马体与前额叶皮层的协作，不仅编码信息，还根据决策目标（回答质量）动态调整提取策略。\n\n### 5. 方法论演进 II：长期记忆模块（LTM）——从“存储”到“激活”\n**目标：** 解决跨会话的用户画像遗忘问题。\n*   **初步构想：** 把用户的历史Query都存进向量数据库。\n*   **批判性思考：** 简单的平铺式存储在面对海量数据时检索慢且噪音大。人类大脑是按“分区”存储记忆的（如时间、空间、主题）。\n*   **进阶方案 1（分区存储）：** 提出**原子主题建模（ATM）**。将用户历史Query按16大类及细分子类进行分区存储。这模仿了大脑皮层的分布式存储特性，大幅缩小检索范围，提高效率。\n*   **进阶方案 2（注意力机制重排）：** 仅仅存下来不够，关键在于如何“用”。\n    *   **逻辑：** 当检索到一批文档后，利用用户的**长期历史Query向量**作为“注意力权重”，去重新计算这些文档块的相关性并进行重排。\n    *   **生物学对应：** 这模拟了记忆的“再激活”过程。当前的感知（检索到的文档）需要通过过往的经验（长期记忆）来过滤和赋予意义，从而筛选出最符合该用户特定背景的知识。\n\n### 6. 系统综合：HiMeS架构的诞生\n**最终逻辑闭环：**\n作者将上述两个模块融合，构建了HiMeS系统：\n1.  **输入：** 用户当前Query + 对话历史。\n2.  **海马体路径（STM）：** RL优化的重写器压缩上下文，生成富含信息的检索Query，进行初检。\n3.  **皮层路径（LTM）：** 系统根据用户ID激活对应的历史记忆分区，利用历史Query对初检结果进行“注意力加权”和重排。\n4.  **输出：** 经过双重记忆过滤后的精准知识片段，输入给LLM生成个性化回答。\n\n### 总结\n作者的思考路径遵循了**“现象观察 -> 问题解构 -> 跨域类比（脑科学） -> 机制映射与工程化（RL + 分区存储 + 注意力重排） -> 系统验证”**的完整逻辑链条。其核心创新点在于不满足于简单的模块堆叠，而是通过生物学启发，将“端到端优化”和“记忆再激活”思想引入RAG系统，从而解决了工业级AI助手“记不住”和“听不懂”的痛点。"
                },
                {
                    "title": "NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs",
                    "arxiv_id": "2601.06126",
                    "authors": "Boshen Shi, Kexin Yang, Yuanbo Yang, Guanguang Chang, Ce Chi, Zhendong Wang, Xing Wang, Junlan Feng",
                    "summary": "While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.",
                    "category": "cs.AI",
                    "filter_reason": "摘要明确提到开发了一个“多智能体系统”，并将算法实例化为工具，符合多智能体协作和工具使用的研究范围。",
                    "summary2": "本文旨在解决现有LLM生成仪表板时存在的表示冗余和可控性低的问题。针对自然语言提示和表格数据，我们提出了一种基于Analysis-Presentation Decoupling原则的NL2Dashboard框架，引入结构化Intermediate Representation (IR)解耦分析与呈现。我们在涵盖金融、教育等领域的真实数据集上，通过视觉质量、Token效率（GOR）和修改成功率等指标验证了其有效性，显著优于现有基线。",
                    "summary_translation": "尽管大型语言模型在生成独立图表方面已展现出卓越的能力，但生成综合仪表板仍然是一项艰巨的挑战。现有的端到端范式通常将仪表板生成视为直接代码生成任务（例如原始HTML），但存在两个根本性局限：一是因视觉渲染消耗大量Token (词元) 而导致的表征冗余，二是因分析推理与展示呈现相互耦合而导致的可控性较低。为应对这些挑战，我们提出了NL2Dashboard，这是一种基于“分析-展示解耦”原则的轻量级框架。我们引入了一种结构化中间表示，用于封装仪表板的内容、布局和视觉元素。因此，该框架将LLM的角色限定于数据分析和意图转换，而将视觉合成工作交由确定性渲染引擎完成。在此框架基础上，我们开发了一个多智能体系统，其中由IR驱动的算法被实例化为一套工具集。利用该系统进行的综合实验表明，NL2Dashboard在多个领域显著优于最先进的基线模型，实现了更优越的视觉质量、显著更高的Token (词元) 效率，以及在生成和修改任务中精确的可控性。",
                    "inspiration_trace": "基于论文《NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“单图生成”到“复杂仪表盘”的跨越）**\n\n1.  **现象观察**：\n    作者首先注意到，虽然现有的LLM在生成独立的图表方面表现出色，但在生成综合性仪表盘时仍面临巨大挑战。\n2.  **现状分析**：\n    当前的主流范式是“端到端生成”，即直接要求LLM生成完整的HTML/CSS/JavaScript代码来渲染仪表盘。\n3.  **核心痛点识别**：\n    作者深入分析发现，这种直接生成代码的方式存在两个根本性缺陷：\n    *   **表征冗余**：LLM消耗了大量的Token去生成视觉渲染代码（如HTML标签、CSS样式），导致用于数据分析和逻辑推理的Token预算被严重压缩，效率低下。\n    *   **可控性差**：数据分析逻辑与视觉呈现逻辑高度耦合。当用户需要修改仪表盘时，LLM往往需要重新生成整个HTML文件，极易破坏全局布局，且难以进行精细化的局部修改。\n\n### 第二阶段：核心假设与范式转移\n**（从“代码生成器”到“分析引擎”的认知转变）**\n\n1.  **本质洞察**：\n    作者提出一个核心观点：LLM的本质优势在于逻辑推理和数据分析，而非像素级的视觉渲染。LLM应该扮演“分析引擎”的角色，而不是“渲染引擎”。\n2.  **提出假设**：\n    如果能将“数据分析”与“视觉呈现”解耦，就能同时解决Token效率和可控性问题。\n3.  **确立原则**：\n    基于此，作者确立了**“分析-呈现解耦”**的设计原则。即让LLM专注于“做什么”，而将“怎么做”交给确定性更强的规则或模板去处理。\n\n### 第三阶段：方法论构建与中间层设计\n**（引入“中间表示”作为桥梁）**\n\n1.  **引入中间层**：\n    为了实现解耦，作者设计了一个结构化的**中间表示**。IR不包含具体的样式代码，而是抽象地描述了仪表盘的内容、布局和视觉元素。\n2.  **构建两阶段流程**：\n    基于IR，作者构建了“推理-渲染”的两阶段工作流：\n    *   **Prompt-to-IR（推理阶段）**：LLM仅负责理解用户意图、执行数据分析，并将结果（图表、表格、指标）及其布局位置填入IR。此时，LLM输出的Token密度极高，全是有效信息。\n    *   **IR-to-Dashboard（渲染阶段）**：利用一个确定性的渲染引擎，通过“插槽填充”机制，将IR中的内容映射到预定义的高质量HTML模板中。这一步不再消耗LLM的推理资源。\n\n### 第四阶段：针对“修改”场景的精细化设计\n**（解决迭代编辑中的不可控问题）**\n\n1.  **深入修改场景**：\n    作者意识到，仪表盘的生成往往不是一次性的，用户会频繁迭代修改。直接修改HTML极其困难，那么如何修改IR？\n2.  **意图翻译技术**：\n    作者提出将用户的自然语言修改指令翻译为一系列**原子操作**（如Change, Swap, Delete, Add）。\n3.  **脚本化更新**：\n    通过生成“修改脚本”，LLM只需更新IR中的特定字段，而不需要重写整个配置。这确保了修改的精确性，避免了“牵一发而动全身”的布局崩坏。\n\n### 第五阶段：系统实现与理论验证\n**（多智能体协作与熵减理论）**\n\n1.  **工程化落地**：\n    为了处理复杂的任务流，作者将上述算法实例化为工具，并设计了一个多智能体系统：\n    *   **Planner**：负责意图识别和任务调度。\n    *   **Coder**：负责执行代码生成和数据分析（保证分析忠实性）。\n    *   **Critic**：利用视觉模型评估图表质量。\n    *   **Toolkit**：封装了IR生成、修改和渲染的确定性工具。\n2.  **理论升华**：\n    最后，作者利用信息论中的熵分解原理证明了该方法的有效性。通过将视觉呈现的不确定性（$H_{vis}$）降至接近0（由确定性模板承担），整个生成系统的总熵显著降低，从而在理论上证明了成功概率的提升。\n\n---\n\n**总结：**\n作者的思考路径是从**发现现有“端到端代码生成”模式的资源浪费和不稳定性**出发，通过**引入“中间表示（IR）”**这一核心创新，实现了**逻辑与样式的解耦**。这不仅释放了LLM的推理潜能，还通过**原子化操作**解决了精细修改的难题，最终构建了一个既轻量又可控的仪表盘生成框架。"
                },
                {
                    "title": "PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction",
                    "arxiv_id": "2601.06158",
                    "authors": "Zibin Meng, Kani Chen",
                    "summary": "Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了PsyAgent，一种基于LLM的智能体架构，旨在通过心理建模（大五人格特质）和语境交互构建类人智能体。研究内容涉及智能体的记忆（个体结构、生活片段）和语境感知行为，属于单智能体研究范畴（记忆、人设建模），且不属于排除的纯应用或纯推理领域。",
                    "summary2": "本文旨在构建能够模拟性格特质与社会结构交互的类人智能体。针对Big Five人格先验与结构化社会场景，我们提出了一种PsyAgent框架，该框架耦合了Individual Structure (IS) 和Multi-Scenario Contexting (MSC)，并利用合成监督数据通过PEFT和DPO微调小模型。我们在多轮角色扮演和决策任务上，通过ProfileAcc、MAE_5等指标验证了其有效性。",
                    "summary_translation": "拟人化智能体需要对性情与社会结构之间的相互作用进行建模。我们提出了PsyAgent，该模型将Big Five trait prior（大五人格特质先验）与Bourdieu's cognitive-social co-structure（布迪厄的认知-社会共构）相结合。PsyAgent包含两个部分：(i) Individual Structure (IS，个体结构)，这是一种机器可读的档案，编码了特质与侧面、认知风格、价值观、文化与教育资本以及显著的生活片段；(ii) Multi-Scenario Contexting (MSC，多场景情境化)，这是一种跨越八个领域（工作、家庭、友谊、陌生人与公民生活、独处与自我调节、浪漫关系、学习以及公共表达）的角色-关系-规范框架。在推理阶段，固定的structured prompts（结构化提示词）将活跃场景与智能体档案绑定，从而产生既稳定又具有情境敏感性的行为。我们通过实例化IS和MSC来合成监督信号（包括role-play dialogues（角色扮演对话）、decision probes（决策探针）和feedback trajectories（反馈轨迹）），随后对一个小型LLM（大语言模型）进行微调。生成的模型能够针对指定的Big Five configurations（大五人格配置）产生一致的、可识别的persona-aligned behaviors（人格对齐行为），并在我们的评估指标上匹配或超越多个更大的untuned LLMs（未微调的大语言模型）及其他untuned baselines（未微调基线），这些指标包括：persona consistency（人格一致性）、contextual appropriateness（情境适当性）、style matching（风格匹配）、trait identifiability（特质可识别性）以及long-horizon stability（长期稳定性）。消融实验表明，IS主要提升了trait fidelity（特质保真度）和stylistic stability（风格稳定性），而MSC则增强了norm awareness（规范意识）和decision fit（决策拟合度）；两者对于实现跨场景性能均必不可少。PsyAgent为构建personality-grounded agents（基于人格的智能体）提供了一种精确且data-efficient（数据高效）的架构。",
                    "inspiration_trace": "基于论文《PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察与痛点识别\n**逻辑起点：现有智能体的“人格漂移”与“情境脱节”**\n*   **观察**：现有的基于人格提示词的对话智能体虽然在短期内能模仿特定语气，但在长对话或跨场景（如从工作切换到家庭）时，往往会出现人格崩塌或行为不一致。\n*   **问题本质**：传统方法仅将人格视为静态的“知识”或“风格标签”，忽略了人类行为的本质——**行为是稳定特质与特定社会结构互动的产物**。单纯的大模型规模或简单的Prompt工程无法解决“特质”与“情境”之间的动态耦合问题。\n\n### 2. 理论锚定与核心假设\n**引入心理学与社会学框架作为理论基石**\n*   **理论选择**：作者引入心理学中的**“大五人格”**作为特质的先验，同时引入社会学中布迪厄的**“认知-社会共构”**理论。\n*   **核心假设**：要构建逼真的智能体，必须显式地建模两个维度的接口：\n    1.  **内在的、稳定的倾向**（我是谁）。\n    2.  **外在的、结构化的社会场域**（我在哪，规则是什么）。\n*   **推论**：智能体的行为不应是随机生成的，而应是“内在特质”在“特定社会情境约束”下的函数输出。\n\n### 3. 结构化解构\n**将抽象理论转化为可计算的架构组件**\n为了验证上述假设，作者将问题拆解为两个可计算的结构：\n*   **组件一：个体结构**\n    *   *思考*：仅有“大五人格分数”太单薄，无法支撑丰富的行为。需要补充背景信息。\n    *   *定义*：构建一个包含教育轨迹、生活经历、社会经济背景、文化资本四个维度的机器可用档案。这代表了智能体的“长期记忆”和“内在资源”。\n*   **组件二：多情境上下文**\n    *   *思考*：情境不能只是简单的“在办公室”。必须包含角色关系、权力结构、社会规范和利益相关者。\n    *   *定义*：构建覆盖工作、家庭、友谊等8个领域的框架库，每个场景明确定义了角色、规范和风险。这代表了智能体面临的“短期约束”。\n\n### 4. 数据构建策略\n**解决“高质量情境数据稀缺”的问题**\n*   **困境**：现实中很难找到大量同时标注了详细心理档案和复杂社会情境的对话数据。\n*   **策略**：**自举合成**。\n    *   利用强大的LLM，基于IS（档案）和MSC（场景）的笛卡尔积，合成监督数据。\n    *   *逻辑*：通过精心设计的Prompt，让大模型生成符合特定人格在特定场景下的反应（角色扮演、决策探针、反馈轨迹）。\n    *   *目的*：将理论框架（IS+MSC）转化为具体的训练样本，教会小模型这种“特质-情境”的互动模式。\n\n### 5. 模型训练范式\n**验证“架构优于规模”的假设**\n*   **思考**：是否必须依赖超大规模模型才能实现这种复杂的心理模拟？\n*   **假设**：如果数据结构足够好（富含心理和情境逻辑），小模型配合高效微调也能超越未微调的大模型。\n*   **方法论**：\n    *   **SFT（有监督微调）**：让模型学习IS和MSC的基本语言风格和规范。\n    *   **DPO（直接偏好优化）**：进一步校准模型在特定情境下的决策倾向，使其更符合目标大五人格的偏好。\n    *   **推理机制**：使用固定的结构化Prompt将IS和MSC绑定，确保推理时的行为既稳定（源于IS）又敏感（源于MSC）。\n\n### 6. 验证与闭环\n**通过消融实验确认理论组件的互补性**\n*   **评估逻辑**：不仅要看对话通顺度，更要看“人格一致性”和“情境适应性”。\n*   **发现与闭环**：\n    *   实验证明，移除IS会导致特质保真度下降（说明IS负责“我是谁”）。\n    *   移除MSC会导致规范意识下降（说明MSC负责“我在哪”）。\n    *   最终结论：PsyAgent通过解构并重组“特质”与“情境”，成功用小模型实现了超越大模型基线的心理拟真度，验证了最初的“认知-社会共构”假设。\n\n---\n\n**总结：**\n作者的思考路径是从**现象（人格漂移）**出发，寻找**理论解释（心理学+社会学）**，将其**工程化（IS+MSC架构）**，通过**合成数据**解决数据瓶颈，最后利用**高效微调**验证了“结构化设计优于暴力规模”的方法论有效性。"
                },
                {
                    "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions",
                    "arxiv_id": "2601.06115",
                    "authors": "V. Cheung",
                    "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.",
                    "category": "cs.AI",
                    "filter_reason": "论文标题明确提到了“Multi-Agent LLM Companions”（多智能体LLM同伴），摘要中提出了“人工集体无意识（ACU）”作为智能体共享交互模板的池，涉及多智能体之间的资源共享、协作以及长期适应任务，符合多智能体（协作、通信）的研究范围。",
                    "summary2": "本文旨在将LLM的离线幻觉转化为学习资源，解决多智能体同伴缺乏跨用户经验共享的问题。针对离线场景，我们提出了一种受荣格启发的Dream Layer架构，利用Artificial Collective Unconscious (ACU)共享去标识化的Interaction Templates。在行为模拟和边缘案例数据集上，通过诗意语言密度、边缘案例覆盖率和语义多样性等指标验证了其有效性。",
                    "summary_translation": "受到一个关于日常硬件项目中知识共享障碍的个人梦境的启发，本文为 LLM companions (大语言模型伴侣) 提出了一个受荣格理论启发的“Dream Layer (梦境层)”，将受控的 offline hallucinations (离线幻觉) 重新定义为一种用于学习和建立关系的资源，而不仅仅是一个 reliability bug (可靠性缺陷)。借鉴荣格关于 collective unconscious (集体潜意识) 作为 archetypal forms (原型形式) 共享存储库的概念，我们引入了一个 Artificial Collective Unconscious (ACU，人工集体潜意识)：这是一个共享的梦境池，agents (智能体) 在其中贡献 de-identified (去标识化) 的、抽象的 Interaction Templates (交互模板)，这些模板随后被 re-instantiated (重新实例化) 为 idiosyncratic Dream Narratives (特异性梦境叙事)。Dream Layer (梦境层) 严格在 offline (离线) 状态下运行：logic-enforcing modules (逻辑强制模块) 被放宽，sampling temperature (采样温度) 被提高，从而产生安全但故意 bizarre narratives (离奇叙事)（例如，货币不匹配的旅行序列），以增强用于 rare events (罕见事件) 和 edge-case safety tests (边缘情况安全测试) 的数据；为了有效地利用风险，我们添加了一个包含严格抽象、temporal delays (时间延迟) 和 ephemeral memory (短暂记忆) 的 governance stack (治理栈)。通过对 everyday dialogue (日常对话) 和 long-horizon adaptation tasks (长期适应任务) 的 behavioural simulations (行为模拟)，我们表明 Dream Layer (梦境层) 实现了一个关键的 decoupling (解耦)：agents (智能体) 在 safety constraints (安全约束)（例如，安全策略）方面保持坚定，而在 narrative strategy (叙事策略)（例如，使用共享的 archetypal metaphors (原型隐喻) 来解决 deadlocks (僵局)）方面变得灵活。这在概念上重新定义了 hallucination (幻觉)，使得 online, unmarked instances (在线、未标记实例) 仍然是 bugs (缺陷)，而 bounded, marked, and delayed ones (有界、标记和延迟的实例) 则成为 synthetic scenarios (合成场景) 和加深 companionship (伴侣关系) 的宝库，这与当代神经科学中提出的 anti-overfitting dream mechanisms (抗过拟合梦境机制) 相呼应。",
                    "inspiration_trace": "基于论文《Dreaming Is Not a Bug: A Jung Inspired Dream Layer for Multi Agent LLM Companions》，以下是对作者产出核心方法逻辑链的系统性推演：\n\n### 1. 起点：从个人体验到宏观悖论\n**观察与痛点：**\n作者从一个极具荒诞感的个人梦境（关于硬件项目中的版权阻碍）出发，敏锐地捕捉到了这个梦境与当前大语言模型（LLM）交互体验之间的惊人相似性：**当寻求具体结构或知识时，往往遭遇抽象边界的阻碍或流畅但无实质的文本。**\n\n**宏观问题提出：**\n由此，作者指出了当前LLM伴侣的两个根本性局限：\n1.  **孤岛效应：** 学习被限制在单个用户的对话孤岛中，无法跨个体提炼或共享洞察。\n2.  **单向度的幻觉观：** 幻觉被纯粹视为可靠性缺陷，必须被抑制，而非一种可被利用的资源。\n\n**核心矛盾：** 我们是否一直在试图“消灭”幻觉，而忽略了它在某种形式下可能具有的进化价值？\n\n### 2. 转折：跨学科的理论借力\n**寻找生物学隐喻：**\n为了解决上述矛盾，作者将目光投向神经科学，引入了**“过拟合大脑假说”**。该理论认为，生物梦境的作用是“离线数据增强”，通过故意生成离奇、分布外的感官输入来防止大脑对日常刺激过拟合。\n\n**假设形成：**\n如果人类利用“怪诞的梦境”来正则化内部模型以提高泛化能力，那么LLM是否也能将“幻觉”转化为一种工程化的想象力资源？\n*   **关键推论：** 幻觉不应被全盘消灭，而应被**隔离**并**控制**，使其在离线状态下服务于模型的学习与泛化。\n\n### 3. 核心：从“共享数据”到“共享原型”\n**引入心理学隐喻：**\n为了解决“孤岛效应”并实现跨代理学习，作者引入了荣格的**“集体潜意识”**概念。其核心在于区分“共享的抽象”与“私有的实例”。\n\n**概念跃迁：**\n作者意识到，直接共享用户对话数据会引发隐私问题，且难以泛化。因此，必须模仿荣格的“原型”概念：\n*   **不做原始数据的共享：** 不分享具体的对话内容。\n*   **做结构模式的共享：** 提取去标识化的、高度抽象的**“交互模板”**（Interaction Templates）。\n\n**方法论雏形：** 构建一个**“人工集体潜意识”（ACU）**，作为所有代理贡献抽象交互模式的共享池。\n\n### 4. 构建：昼夜分离的架构设计\n**架构映射：**\n基于上述理论，作者设计了“梦境层”架构，将代理的运行状态严格划分为“在线”与“离线”两个世界，以此解决“幻觉不可控”的风险。\n\n*   **在线层：** 严格遵循事实、逻辑和安全策略（对应人类的“清醒状态”）。\n*   **离线层：** 放松逻辑约束，提高采样温度，引入噪声（对应人类的“做梦状态”）。\n\n**逻辑闭环：**\n1.  **抽象化：** 代理将在线交互经历抽象为去标识化的模板，存入ACU。\n2.  **再实例化：** 代理从ACU采样模板，通过受控的离线幻觉生成怪诞但结构连贯的“梦境叙事”。\n3.  **策略蒸馏：** 这些梦境不直接作为知识，而是被解析，提炼出高层次的**行为策略**，反向更新代理的在线行为。\n\n### 5. 收敛：安全与治理的边界设定\n**风险意识：**\n作者清醒地认识到，让AI“自由做梦”存在巨大的安全风险（隐私泄露、叙事投毒、不可控输出）。\n\n**治理逻辑：**\n为了使理论落地，必须引入严格的治理栈，将“做梦”限制在笼子里：\n*   **严格抽象与去标识化：** 确保ACU中只有结构骨架，无个人痕迹。\n*   **时间延迟：** 强制冷却期，防止实时关联攻击。\n*   **短暂记忆：** 梦境内容必须随时间衰减，只有提炼出的策略才能长期保留。\n*   **零信任消费：** 代理只能将梦境作为弱先验，不能作为执行指令。\n\n### 6. 验证：从“做梦”到“进化”的闭环\n**实证思路：**\n最后，作者通过实验验证这一假设的可行性，而非仅仅停留在哲学层面。\n*   **现象验证：** 证明在特定指令下，模型确实能进入可观测、可复现的“梦境状态”（如诗歌语言密度的显著提升）。\n*   **功能验证：** 证明这种机制能加速边缘案例的覆盖，并提升日常对话的多样性（降低拒绝率）。\n\n**总结：**\n作者的思考路径是从**现象（梦境与AI交互的相似性）**出发，经由**理论（神经科学与荣格心理学）**的启发，提出了**概念重构（将幻觉视为离线资源）**，最终通过**架构设计（梦境层+ACU）**和**严格治理（安全边界）**，将一个看似哲学的隐喻转化为了可工程实现的AI系统方法论。"
                },
                {
                    "title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions",
                    "arxiv_id": "2601.06112",
                    "authors": "Aayush Gupta",
                    "summary": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "该论文专注于评估工具使用LLM智能体的可靠性，涉及单智能体架构（ReAct, Reflexion）的评估，涵盖了工具使用和自我反思等核心智能体能力，符合单智能体研究范围。",
                    "summary2": "本文旨在解决现有基准测试无法全面评估LLM Agent生产环境可靠性的问题。针对生产环境中的压力条件，我们提出了一种名为ReliabilityBench的基准测试，引入了Reliability Surface $R(k, \\epsilon, \\lambda)$、Action Metamorphic Relations和Chaos Engineering Framework。我们在四个领域的1,280个episodes上，通过pass@k和Reliability Surface等指标验证了其有效性，揭示了扰动和故障对可靠性的显著影响。",
                    "summary_translation": "现有的针对使用工具的 LLM agents（大语言模型智能体）的基准主要报告单次运行成功率，而忽视了生产环境所需的可靠性属性。我们介绍了 \\textbf{ReliabilityBench}，这是一个从三个维度评估 agent（智能体）可靠性的基准：(i) 使用 $\\mathrm{pass}^k$（通过率）指标衡量的重复执行下的一致性，(ii) 在强度 $ε$ 下对语义等价任务扰动（perturbations）的鲁棒性，以及 (iii) 在强度 $λ$ 下受控工具/API 故障（failures）下的容错性。ReliabilityBench 提供了一个统一的可靠性曲面 $R(k,ε,λ)$，定义了 \\textit{action metamorphic relations}（动作蜕变关系），即通过终态等价性而非文本相似度来定义正确性，并引入了一个混沌工程风格的故障注入框架（包括超时、速率限制、部分响应、模式漂移）。我们在四个领域（日程安排、旅行、客户支持、电子商务）的 1,280 个回合中，对两个模型和两种 agent architectures（智能体架构）进行了评估。仅引入扰动（perturbations）就使成功率从 $ε=0$ 时的 96.9% 下降至 $ε=0.2$ 时的 88.1%。在消融实验中，速率限制是最具破坏性的故障。在综合压力下，ReAct 表现出比 Reflexion 更强的鲁棒性，且 Gemini 2.0 Flash 以低得多的成本实现了与 GPT-4o 相当的可靠性。ReliabilityBench 为评估 LLM agents 的生产就绪度提供了一个系统化的框架。",
                    "inspiration_trace": "基于论文《ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions》，以下是对作者产出该文章核心方法的逻辑链推演：\n\n### 1. 宏观观察：基准与现实的错位\n**逻辑起点：** 作者观察到 LLM Agent 正从实验室原型加速走向生产环境（如客服、自动化操作），但现有的评估体系存在严重的“脱节”。\n*   **现象：** 现有的主流基准（如 ToolBench, AgentBench）主要关注“单次运行成功率”。\n*   **矛盾：** 生产环境的核心诉求不是“能不能做”，而是“能不能稳定地做 1000 次”。在实验室里跑通一次和在真实网络环境下面对各种干扰跑通，是完全两回事。\n*   **初步结论：** 传统的 `pass@1` 指标过于乐观，掩盖了 Agent 在实际部署中的脆弱性。我们需要一个新的评估视角，即“生产就绪度”。\n\n### 2. 问题解构：什么是“可靠性”？\n为了填补上述差距，作者没有直接提出新测试集，而是先对“可靠性”这一概念进行了三维度的解构，试图定义生产环境到底包含哪些挑战：\n*   **维度一：一致性。** 受 τ-bench 启发，作者意识到 LLM 的随机性导致即使输入相同，多次运行结果也可能不同。生产环境要求的是“次次成功”，而非“偶尔成功”。\n*   **维度二：鲁棒性。** 真实用户不会按标准模板说话。他们会改写指令、插入无关信息、中途纠正。Agent 需要理解语义的等价性，而非死板的文本匹配。\n*   **维度三：容错性。** 真实的基础设施是不完美的。API 会超时、限流、返回残缺数据。Agent 需要具备“抗打击”和恢复能力。\n\n**思考演进：** 作者意识到这三个维度不是独立的，而是相互交织的。一个 Agent 可能很稳定（一致性高），但一遇到 API 报错就崩溃（容错性低）。因此，评估必须是一个多维度的综合体系。\n\n### 3. 方法论构建：跨学科思想的引入与适配\n有了定义，接下来的核心问题是：**如何量化这三个维度？** 作者在此处引入了两个关键的外部领域思想，并针对 Agent 场景进行了改造。\n\n*   **针对“鲁棒性”的解法：引入“变形测试”。**\n    *   *传统困境：* 对于 Agent 任务，输出文本可能千差万别（路径不同），用文本相似度判断对错很难。\n    *   *创新点：* 提出 **Action Metamorphic Relations（动作变形关系）**。核心逻辑是：只要输入的语义变化不改变任务目标，那么最终的**系统状态**必须一致。例如，指令从“订机票”变为“我要飞去...”，只要最终订票状态一致，就算通过。这解决了“非标准输入”的验证难题。\n\n*   **针对“容错性”的解法：引入“混沌工程”。**\n    *   *传统困境：* 静态数据集无法模拟动态故障。\n    *   *创新点：* 借鉴 Netflix 的 Chaos Monkey，提出 **Chaos Engineering for Agents**。不再等待故障发生，而是主动在工具调用层注入故障（如超时、限流、Schema 漂移）。这模拟了真实生产环境的“压力测试”。\n\n### 4. 统一框架：从点到面的升维\n有了具体的测试手段（变形关系、故障注入），作者需要一个数学框架来统一这些指标。\n*   **逻辑推演：** 既然可靠性有三个维度（k, ε, λ），那么评估结果就不应该是一个单一的分数，而应该是一个“函数”或“曲面”。\n*   **产出：** 提出了 **Reliability Surface $R(k, \\epsilon, \\lambda)$**。这个三维曲面能够直观地展示 Agent 在不同压力组合下的表现。例如，它能回答“当故障率增加时，Agent 对用户指令改写的敏感度是如何变化的？”这一复杂问题。\n\n### 5. 实证与反思：复杂度的悖论\n最后，作者通过实验验证假设，并得出了反直觉的结论，完善了整个思考闭环。\n*   **假设：** 更复杂的架构（如 Reflexion，带有自我反思机制）应该更可靠。\n*   **实验发现：** 在压力条件下，简单的 ReAct 架构反而表现更好。\n*   **逻辑修正：** 作者意识到，复杂的反思机制在遇到故障或干扰时，可能会引入更多的错误传播或无效循环，反而降低了稳定性。这进一步强化了论文的核心观点：**生产环境下的可靠性不等于模型能力的堆砌，而是对压力的稳健性。**\n\n---\n\n**总结：**\n作者的思考路径是从**“评估指标的失效”**出发，通过**“解构生产环境挑战”**定义了三个核心维度，进而**“跨界融合”**了软件测试的变形思想和 SRE 的混沌工程思想，最终构建了一个**“多维度的可靠性曲面”**框架，并揭示了**“简单架构在压力下的优势”**。整个过程体现了从现象观察、理论抽象到方法创新、实证修正的完整学术逻辑。"
                },
                {
                    "title": "LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
                    "arxiv_id": "2601.06111",
                    "authors": "Aayush Gupta, Farahan Raza Sheikh",
                    "summary": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis. We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于LLM的社会数字孪生框架，其中LLM充当个体智能体的认知引擎，用于模拟群体行为。这属于多智能体系统的研究范畴。尽管使用了COVID-19作为案例研究，但论文的核心贡献是通用的智能体框架架构，而非纯医疗应用，因此符合筛选条件。",
                    "summary2": "本文旨在解决预测人口对政策干预反应的挑战。针对政策响应预测中传统模型缺乏机制可解释性的问题，我们提出了一种基于LLM的Social Digital Twins框架，利用LLM作为Agent的认知引擎生成多维行为概率，并通过校准层映射到观测数据。在COVID-19大流行响应数据集上，通过RMSE指标验证了其有效性，相比Gradient Boosting基线，宏观平均预测误差降低了20.7%。",
                    "summary_translation": "预测人群如何响应政策干预是计算社会科学和公共政策领域的一个根本性挑战。传统方法依赖于聚合统计模型，这些模型虽然能够捕捉历史相关性，但缺乏机制可解释性，且难以应对新颖的政策场景。我们提出了一个构建社会数字孪生的通用框架——即虚拟人口副本，其中大语言模型作为个体智能体的认知引擎。每个智能体由人口统计学和心理特征学属性表征，接收政策信号并输出多维行为概率向量。一个校准层将聚合的智能体响应映射到可观测的群体层面指标，从而能够利用真实世界数据进行验证，并用于反事实政策分析。我们在大流行应对领域实例化了该框架，以拥有丰富观测数据的COVID-19作为案例研究。在保留测试期内，我们校准后的数字孪生在六个行为类别上，相较于梯度提升基线，在宏平均预测误差上实现了20.7%的改进。反事实实验展示了针对政策变化的单调且有界的响应，确立了行为的合理性。该框架是领域无关的：同样的架构适用于交通政策、经济干预、环境法规，或任何政策影响人群行为的场景。我们讨论了该框架对政策模拟的影响、方法的局限性，以及将基于大语言模型的数字孪生扩展到大流行应对之外的方向。",
                    "inspiration_trace": "基于论文《LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 第一阶段：宏观问题识别与现有方法的痛点分析\n**思考起点：** 政策制定者面临的核心困境——如何预测人群对未实施政策的反应？\n**逻辑推演：**\n1.  **观察现实需求：** 政府在推行碳税、封锁或福利改革前，需要预判公众行为（如：人们会减少开车吗？会遵守居家令吗？）。\n2.  **审视现有工具箱：**\n    *   **传统统计模型（如回归、时间序列）：** 擅长捕捉历史数据中的相关性，但本质是“黑箱”或“相关性归纳”。当遇到从未发生过的新颖政策（Novel Policy Scenarios）时，模型无法外推，且无法解释“为什么”会发生（缺乏机制可解释性）。\n    *   **传统基于主体的模型（ABM）：** 具备机制可解释性，能模拟个体决策。但其致命弱点在于“知识瓶颈”——必须由专家手动编写决策规则。如果人类自身都不完全理解某种复杂行为，就无法编写规则，导致模型难以泛化。\n3.  **核心矛盾提炼：** 我们需要一种既能像ABM那样具备**个体层面的机制推理能力**，又能像统计模型那样**易于构建且适应复杂场景**的新范式。\n\n### 第二阶段：技术机遇捕捉与核心假设提出\n**思考转折：** 大语言模型（LLM）的涌现能力是否提供了破局的关键？\n**逻辑推演：**\n1.  **观察LLM特性：** LLM不仅是在生成文本，它们在海量人类语料上训练，实际上习得了隐性的“人类推理模型”、“偏好”和“决策模式”（即“硅基采样” Silicon Sampling）。\n2.  **提出核心假设：** 如果LLM能模拟调查问卷回答、参与经济博弈，那么它本质上是一个通用的**人类行为模拟器**。\n3.  **范式转换构想：** 用LLM替换ABM中手工编写的规则引擎。\n    *   **输入：** 给LLM设定一个人设（年龄、职业、价值观）和一个政策背景。\n    *   **输出：** 让LLM基于其“常识”推理出该人设的行为概率。\n    *   **优势：** 无需针对每个领域硬编码规则，利用LLM的泛化能力处理新颖政策。\n\n### 第三阶段：框架构建——从“直觉”到“科学”\n**思考深化：** 仅靠LLM生成文本是不够的，如何将其转化为严谨的科学预测工具？\n**逻辑推演：**\n1.  **定义架构：** 提出“社会数字孪生”概念。这不仅是调用API，而是一个包含四个组件的系统：\n    *   **代理人口：** 必须构建符合真实人口统计学分布的合成人设，以保证群体的异质性。\n    *   **LLM认知引擎：** 负责将“人设+政策”映射为“行为概率向量”。\n2.  **解决“幻觉”与“对齐”问题（关键创新点）：**\n    *   **观察：** LLM输出的概率（如0.7）往往是主观的，不能直接对应现实世界的宏观指标（如客流量百分比）。\n    *   **引入校准层：** 必须建立一个数学映射层 $f(p; \\theta)$，将LLM输出的原始概率校准为可观测的现实指标。这相当于用历史数据去“锚定”LLM的直觉，使其具备预测精度。\n3.  **确立验证逻辑：** 强调严格的时空分割，防止信息泄露，确保模型是在真正“预测”而非“记忆”。\n\n### 第四阶段：实证策略与案例选择\n**思考落地：** 如何证明这个框架真的有效？\n**逻辑推演：**\n1.  **选择测试场：** 为什么选COVID-19？\n    *   **数据丰富度：** 有高频的谷歌移动数据和牛津政策追踪数据。\n    *   **自然实验属性：** 疫情期间政策变化剧烈且频繁，是测试模型应对“新颖/极端场景”的完美压力测试。\n    *   **行为多维性：** 涵盖工作、休闲、购物等多种行为，能全面测试模型。\n2.  **设定对比基线：** 与梯度提升树（GBM）等强统计模型对比。目的是验证：在处理“语义理解”和“决策逻辑”时，LLM是否优于纯数据驱动的统计模型。\n\n### 第五阶段：结果反思与定位修正\n**思考升华：** 实验结果揭示了什么？该方法论的边界在哪里？\n**逻辑推演：**\n1.  **结果分析：**\n    *   **成功之处：** 在工作场所、零售等“决策驱动型”行为上，LLM大幅超越统计模型。这证明了LLM理解政策语义（如“封锁”意味着“居家”）的能力。\n    *   **失败之处：** 在居住等“惯性驱动型”行为上，LLM不如统计模型。这说明LLM缺乏对日常习惯和惯性的记忆。\n2.  **方法论定位：**\n    *   明确该框架不是要取代所有统计模型，而是填补**“政策语义理解”与“机制推理”**的空白。\n    *   强调其**领域无关性**：COVID-19只是验证数据集，同样的架构可以无缝迁移到交通、经济、环保等领域，因为LLM已经学习了跨领域的人类行为逻辑。\n\n---\n\n**总结：**\n作者的思考路径是从**政策预测的现实困境**出发，敏锐地捕捉到**LLM作为通用认知引擎**的潜力，通过引入**校准层**解决了从“文本生成”到“科学预测”的跨越，最后通过**疫情案例**验证了其在处理复杂决策行为上的优越性，从而确立了一套通用的社会数字孪生方法论。"
                },
                {
                    "title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning",
                    "arxiv_id": "2601.06098",
                    "authors": "Nicholas X. Wang, Neel V. Parpia, Aaryan D. Parikh, Aggelos K. Katsaggelos",
                    "summary": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“多智能体LLM架构”，其中包含专门的智能体负责图寻路、推理、验证和输出等特定任务，这些智能体通过协作来减少幻觉并生成高质量问题。这符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决LLM在自动问题生成中的幻觉问题，以支持直觉学习。针对STEM教育场景，我们提出了一种结合Causal Graph引导的Chain-of-Thought推理与Multi-agent LLM架构的框架，并在Stellar在线学习平台上通过Flesch-Kincaid Grade Level、Key Points和Solution Quality等指标验证了其有效性，结果显示质量提升高达70%。",
                    "summary_translation": "直觉学习对于培养深层概念理解至关重要，尤其是在 STEM（科学、技术、工程和数学）教育领域，学生往往难以掌握抽象且相互关联的概念。自动问题生成已成为实现个性化学习和自适应学习的有效策略。然而，其有效性受到大语言模型中“幻觉”现象的制约，这可能导致生成事实错误、语义模糊或教学不一致的问题。为解决这一问题，我们提出了一种新颖的框架，该框架结合了因果图引导的思维链推理与多智能体 LLM 架构。该方法确保生成准确、有意义且符合课程要求的问题。因果图提供了领域知识的显式表示，而 CoT 推理则促进了对相关概念的结构化、逐步遍历。专用的 LLM 智能体被分配了图路径查找、推理、验证和输出等特定任务，所有任务均在领域约束范围内执行。一种在概念阶段和输出阶段实施的双重验证机制，极大地减少了幻觉现象。实验结果表明，与基准方法相比，该方法在质量上提升了高达 70%，并在主观评估中取得了极为理想的结果。",
                    "inspiration_trace": "基于论文《Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观愿景与教育痛点\n**（从“直觉学习”的理想出发）**\n\n1.  **观察现状**：在STEM教育中，传统的死记硬背已不足以应对抽象概念的学习，教育界正转向“直觉学习”——即通过自然认知过程、探索和逐步推理来建立深层理解。\n2.  **技术机遇**：生成式AI（特别是LLM）为实现个性化、自适应的“直觉学习”提供了可能，其中“自动问题生成（AQG）”是核心抓手，它能实时提供符合学生水平的挑战。\n3.  **核心矛盾**：虽然LLM具备强大的生成能力，但在教育场景下存在致命缺陷——**“幻觉”**。LLM会生成事实错误、逻辑不清或不符合教学大纲的问题，这会误导学生，破坏学习体验，违背了直觉学习追求“概念清晰”的初衷。\n\n### 第二阶段：问题诊断与归因\n**（深入分析LLM在教育场景失效的本质）**\n\n1.  **归因分析**：为什么LLM会产生幻觉？因为LLM本质上是基于概率预测的文本生成器，缺乏对领域知识**显性结构**的约束。它不知道概念A必须是概念B的前提（例如：不知道“牛顿第二定律”是推导“能量守恒”的基础）。\n2.  **需求明确**：要解决这一问题，不能仅靠微调模型，必须引入一种机制，能够：\n    *   显式表示知识的依赖关系（结构）。\n    *   强制生成过程遵循逻辑步骤（推理）。\n\n### 第三阶段：理论假设与融合\n**（提出“因果图 + 思维链”的结合点）**\n\n1.  **引入“因果图”**：作者意识到，因果图能完美映射学科中的概念依赖（如：力 $\\rightarrow$ 加速度 $\\rightarrow$ 速度）。它提供了**“是什么”**和**“什么顺序”**的知识骨架，解决了结构缺失问题。\n2.  **引入“思维链”**：CoT推理能模拟人类解决问题的逐步思考过程。它提供了**“如何”**连接这些概念的逻辑流。\n3.  **核心假设**：如果将因果图作为“导航地图”，将CoT作为“行驶路径”，让LLM沿着因果图的路径进行CoT推理，就能生成既符合学科逻辑又具备教学深度的题目。\n\n### 第四阶段：方法论构建与抗噪设计\n**（从理论假设落地为可执行的系统架构）**\n\n1.  **架构设计：多智能体协作**：单一的Prompt难以同时处理图遍历、逻辑推理和文本生成。作者受软件工程启发，决定采用**多智能体架构**，将复杂任务拆解：\n    *   *寻路智能体*：负责在因果图中找到正确的概念路径。\n    *   *推理智能体*：负责基于路径生成CoT。\n    *   *生成与输出智能体*：负责最终题目的产出。\n2.  **抗噪机制：双重验证**：为了专门针对第二阶段发现的“幻觉”问题，作者设计了**双重验证**机制：\n    *   *概念层验证*：在生成前，检查寻路智能体找到的路径是否逻辑自洽。\n    *   *输出层验证*：在生成后，检查最终题目是否准确、无歧义。\n    *   *逻辑闭环*：通过这两道“安检”，确保输出严格受限于因果图的结构约束。\n\n### 第五阶段：验证与价值确认\n**（通过实验反馈闭环验证思想）**\n\n1.  **评估维度设定**：为了证明该方法优于普通LLM（如ChatGPT），作者设定了不仅关注“可读性”，更关注“关键点覆盖”和“解题步骤质量”的指标。这直接呼应了第一阶段“直觉学习”对深度理解的要求。\n2.  **结果反馈**：实验显示，该方法在题目深度和逻辑性上显著优于基线模型（提升70%），且用户反馈题目“自然”、“符合推理习惯”。\n3.  **结论升华**：这证明了**结构化知识（因果图）与结构化推理（CoT）的结合**，是解决教育领域LLM幻觉问题的有效范式。\n\n---\n\n**总结：作者的思考路径是从教育理念（直觉学习）出发，遭遇技术瓶颈（LLM幻觉），通过引入外部结构（因果图）和内部逻辑（CoT）进行约束，最终通过工程化手段（多智能体+双重验证）将理论落地，从而实现了高质量的教育内容生成。**"
                },
                {
                    "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
                    "arxiv_id": "2601.07122",
                    "authors": "Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu",
                    "summary": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个分层多智能体框架，其中上层LLM智能体明确使用了ReAct规划、长短期记忆和工具集成（符合单智能体标准），并与下层RL智能体进行协作（符合多智能体标准）。尽管应用于网络防御领域，但论文的核心贡献在于LLM智能体的架构设计（LLM与RL的结合），而非单纯的应用或AI安全对齐研究。",
                    "summary2": "本文旨在解决云网络防御在动态环境下的适应性和鲁棒性问题。针对云网络结构、规模及攻击策略动态变化的场景，我们提出了一种名为CyberOps-Bots的分层多智能体强化学习框架，该框架结合了LLM的高层战术规划与底层RL智能体的原子动作执行。在AWS企业云数据集和Yawning Titan仿真环境中，通过网络可用性和Jumpstart性能等指标验证了其有效性，实现了无需重训练的高效自适应防御。",
                    "summary_translation": "虽然虚拟化和资源池化为云网络赋予了结构灵活性和弹性可扩展性，但它们不可避免地扩大了攻击面，并挑战了网络弹性。基于强化学习的防御策略已被开发出来，用于在对抗条件下优化资源部署和隔离策略，旨在通过维持和恢复网络可用性来增强系统弹性。然而，现有方法缺乏鲁棒性，因为它们需要重新训练以适应网络结构、节点规模、攻击策略和攻击强度的动态变化。此外，缺乏人在回路支持限制了可解释性和灵活性。为了解决这些局限性，我们提出了 CyberOps-Bots，这是一个由大语言模型赋能的分层多智能体强化学习框架。受 MITRE ATT&CK 的战术-技术模型启发，CyberOps-Bots 具有双层架构：(1) 上层 LLM 智能体包含四个模块——ReAct 规划、基于 IPDRR 的感知、长短期记忆以及动作/工具集成——负责执行全局感知、人类意图识别和战术规划；(2) 下层 RL 智能体通过异构分离预训练开发，在局部网络区域内执行原子防御动作。这种协同作用在确保可靠的 RL 执行的同时，保留了 LLM 的适应性和可解释性。在真实云数据集上的实验表明，与最先进的算法相比，CyberOps-Bots 在不重新训练的情况下切换场景时，维持的网络可用性高出 68.5%，并实现了 34.7% 的启动性能增益。据我们所知，这是首个建立具有 HITL 支持的鲁棒 LLM-RL 框架用于云防御的研究。我们将向社区发布我们的框架，以促进云网络中鲁棒且自主防御的发展。",
                    "inspiration_trace": "基于论文《Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题与现状悖论\n**1. 观察现象：云原生环境的“双刃剑”特性**\n作者首先观察到云网络的核心矛盾：虚拟化和弹性伸缩赋予了系统极大的灵活性，但同时也导致了网络拓扑、节点规模和攻击面的高度动态变化。\n*   **思考：** 传统的静态防御策略无法应对这种瞬息万变的环境。\n\n**2. 现有方案的尝试与局限**\n学术界和工业界开始尝试使用强化学习（RL）来自动化防御决策，因为RL擅长通过交互学习最优策略。\n*   **深入分析：** 作者发现现有的RL方法存在致命的“脆弱性”。当网络结构（A1）、规模（A2）、攻击策略（A3）或攻击强度（A4）发生变化时，RL模型往往失效，必须重新训练。\n*   **根本原因定位：**\n    *   **表征僵化：** RL依赖固定维度的状态向量（如邻接矩阵），一旦网络规模或拓扑改变，输入层结构就不匹配了。\n    *   **泛化缺失：** RL是基于模式匹配的，而非语义理解。面对未见过的攻击阶段或并发攻击，它无法举一反三。\n    *   **人机断层：** 纯算法模型缺乏可解释性，无法支持安全专家在紧急情况下进行有效干预（HITL）。\n\n---\n\n### 第二阶段：范式转换与假设提出\n**3. 引入新视角：从“数值计算”转向“语义推理”**\n为了解决泛化性和人机交互问题，作者将目光投向了大语言模型（LLM）。\n*   **假设：** LLM具备强大的语义理解、逻辑推理和零样本泛化能力，能够理解复杂的网络态势和人类指令，从而弥补RL在高层认知上的不足。\n\n**4. 识别新技术的短板**\n然而，作者敏锐地意识到LLM并非万能：\n*   **短板：** LLM在精确的数值计算（如计算最短路径）和生成低层级的精确控制指令（如具体的流表修改命令）方面存在“幻觉”和不稳定性。\n*   **结论：** 单纯依靠LLM无法保证防御执行的可靠性。\n\n---\n\n### 第三阶段：方法论融合与架构设计\n**5. 核心思想：分层协同的“战术-技术”解耦**\n受MITRE ATT&CK框架（战术与技术的分层）启发，作者提出了一个融合假设：**将“大脑”（LLM）与“手脚”（RL）结合**。\n*   **逻辑推演：**\n    *   **上层（LLM）：** 负责宏观感知、战术规划和意图理解。利用自然语言处理能力，将动态的网络状态抽象为文本，从而解耦对特定网络结构的依赖。\n    *   **下层（RL）：** 负责微观执行。利用RL在特定动作空间内的精确控制能力，执行具体的原子防御操作。\n\n**6. 解决动态适应性的具体机制设计**\n针对前述的四个动态挑战（A1-A4），作者在架构中嵌入了对应的解决方案：\n\n*   **针对A1（结构变化）与A2（规模变化）：自然语言状态抽象**\n    *   *思考：* 如何让模型不关心网络具体有多少个节点？\n    *   *方案：* 设计一个感知模块，将高维、结构化的网络状态转化为自然语言描述。因为LLM处理文本不受长度限制，这天然解决了状态空间爆炸和维度不匹配的问题，实现了“零样本”适应新拓扑。\n\n*   **针对A3（攻击策略变化）：异构分离预训练**\n    *   *思考：* 如何应对不同类型的攻击（如DDoS vs 渗透）？\n    *   *方案：* 不训练一个全能的RL智能体，而是训练一组功能单一的“专家”RL智能体（如隔离专家、补丁专家）。LLM作为指挥官，根据当前的攻击语义，动态调度不同的专家组合。这比单一模型更具灵活性。\n\n*   **针对A4（攻击强度/并发性）：长短时记忆机制**\n    *   *思考：* 面对多阶段、并发的攻击链，如何保持连贯性？\n    *   *方案：* 赋予LLM记忆模块（LTM/STM）。通过存储和检索历史攻击链，LLM能够识别攻击意图的演变，从而进行长期的防御规划，而不是短视的反应。\n\n---\n\n### 第四阶段：增强可靠性与人机协同\n**7. 引入ReAct范式与HITL支持**\n为了解决LLM的“幻觉”问题并增强信任度：\n*   **ReAct（推理+行动）：** 强制LLM在输出行动前先生成推理链。这不仅提高了决策的准确性，还提供了天然的可解释性日志。\n*   **人在回路（HITL）：** 允许安全专家通过自然语言直接干预LLM的规划层。这使得系统不仅是自动化的，更是可审计、可修正的。\n\n---\n\n### 第五阶段：逻辑闭环与验证\n**8. 最终产出：CyberOps-Bots框架**\n作者将上述思考整合为一个三层架构：环境层（模拟动态对抗）、LLM层（语义规划）、RL层（原子执行）。\n\n**9. 验证逻辑：**\n*   **实验设计：** 不再测试静态环境，而是专门设计场景动态切换（如从30节点跳到450节点，攻击策略从侦察变为渗透）。\n*   **核心指标：** 关注“Jumpstart性能”（即在新环境下无需重新训练的初始表现）和“网络可用性”。\n*   **结论验证：** 实验证明，这种分层架构确实在无需重训的情况下，适应了A1-A4的所有动态变化，且性能优于传统RL算法。\n\n---\n\n**总结：**\n作者的思考路径是从**“RL在动态环境下的失效”**这一痛点出发，通过**引入LLM的语义泛化能力**作为破局点，进而通过**分层架构（LLM规划+RL执行）**规避了LLM的精确性短板，最终利用**自然语言抽象和异构智能体调度**实现了对云网络动态特性的鲁棒适应。"
                },
                {
                    "title": "Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation",
                    "arxiv_id": "2601.06877",
                    "authors": "Donghuo Zeng, Roberto Legaspi, Kazushi Ikeda",
                    "summary": "Effective persuasive dialogue agents adapt their strategies to individual users, accounting for the evolution of their psychological states and intentions throughout conversations. We present a personality-aware reinforcement learning approach comprising three main modules: (1) a Strategy-Oriented Interaction Framework, which serves as an agenda-based strategy controller that selects strategy-level actions and generate responses via Maximal Marginal Relevance (MMR) retrieval to ensure contextual relevance, diversity, and scalable data generation; (2) Personality-Aware User Representation Learning, which produces an 81-dimensional mixed-type embedding predicted at each turn from recent exchanges and appended to the reinforcement learning state; and (3) a Dueling Double DQN (D3QN) model and Reward Prediction, in which the policy is conditioned on dialogue history and turn-level personality estimates and trained using a composite reward incorporating agreement intent, donation amount, and changeof-mind penalties. We use an agenda-based LLM simulation pipeline to generate diverse interactions, from which personality estimation is inferred from the generated utterances. Experiments on the PersuasionForGood (P4G) dataset augmented with simulated dialogues reveal three main findings: (i) turn-level personality conditioning improves policy adaptability and cumulative persuasion rewards; (ii) LLM-driven simulation enhances generalization to unseen user behaviors; and (iii) incorporating a change-of-mind penalty reduces post-agreement retractions while slightly improving donation outcomes. These results demonstrate that structured interaction, dynamic personality estimation, and behaviorally informed rewards together yield more effective persuasive policies.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个基于强化学习的说服对话智能体，包含策略规划（议程式策略控制器）、记忆与状态表征（个性化用户表征学习）以及利用LLM进行环境模拟，符合单智能体的研究范围。",
                    "summary2": "本文旨在解决说服性对话中用户心理状态动态变化难以捕捉的问题。针对多轮交互场景，我们提出了一种Personality-Aware Reinforcement Learning方法，集成Strategy-Oriented Interaction Framework、动态Personality-Aware User Representation及D3QN模型。我们在PersuasionForGood (P4G)数据集及LLM仿真环境中，通过累积说服奖励等指标验证了其有效性。",
                    "summary_translation": "高效的 persuasive dialogue agents（说服对话代理）能够针对个体用户调整策略，并考量其在对话过程中心理状态和意图的演变。我们提出了一种 personality-aware reinforcement learning（人格感知强化学习）方法，该方法包含三个主要模块：(1) Strategy-Oriented Interaction Framework（面向策略的交互框架），作为一个基于议程的策略控制器，用于选择策略级动作，并通过 Maximal Marginal Relevance (MMR)（最大边际相关性）检索生成响应，以确保上下文相关性、多样性及可扩展的数据生成；(2) Personality-Aware User Representation Learning（人格感知用户表征学习），生成一个81维的混合类型嵌入，该嵌入在每一轮对话中根据最近的交流进行预测，并附加到强化学习状态中；(3) Dueling Double DQN (D3QN)（决斗双深度Q网络）模型和 Reward Prediction（奖励预测），其中策略以对话历史和轮级人格估计为条件，并利用包含同意意图、捐赠金额和 change-of-mind penalty（改变主意惩罚）的复合奖励进行训练。我们采用基于议程的 LLM（大语言模型）模拟流水线生成多样化的交互，并据此从生成的言语中推断人格估计。在通过模拟对话增强的 PersuasionForGood (P4G) 数据集上进行的实验揭示了三个主要发现：(i) turn-level personality conditioning（轮级人格条件化）提高了策略适应性和累积说服奖励；(ii) LLM-driven simulation（大语言模型驱动的模拟）增强了对未见用户行为的泛化能力；(iii) 引入 change-of-mind penalty（改变主意惩罚）减少了达成协议后的撤回行为，同时略微改善了捐赠结果。这些结果表明，结构化的交互、动态的人格估计以及基于行为的奖励共同产生了更有效的 persuasive policies（说服策略）。",
                    "inspiration_trace": "基于论文《Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 1. 宏观问题：现有劝说系统的“行为落地”困境\n**起点：** 作者首先关注到，尽管大语言模型（LLM）在对话流畅度上表现优异，但在**劝说**这一特定任务中，它们往往缺乏稳定的“行为基础”。\n*   **核心矛盾：** 劝说不仅仅是生成通顺的文本，而是一个长期的、旨在改变用户信念和行为的策略过程。现有的通用LLM缺乏对个体用户心理状态的深层建模和长线策略规划能力。\n\n### 2. 观察与痛点分析：从静态到动态，从匮乏到仿真\n在确立宏观问题后，作者深入剖析了现有研究的两个主要局限性：\n\n*   **观察一：用户画像的“静态性”缺陷。**\n    *   **现象：** 传统的基于强化学习（RL）的对话系统通常假设用户具有固定的“静态人格”。\n    *   **推论：** 真实的劝说过程是动态博弈，用户的心理状态和意图会随着对话的进行而实时演变。如果仅依赖静态画像，策略选择将无法适应当前的对话情境，导致次优的劝说效果。\n\n*   **观察二：训练数据的“稀缺性”与模拟器的“机械性”。**\n    *   **现象：** 高质量的人工标注劝说数据（如P4G数据集）非常昂贵且覆盖面有限。而传统的基于规则或模板的用户模拟器过于死板，无法模拟出真实人类复杂、微妙的反应。\n    *   **推论：** 训练一个鲁棒的RL策略需要大量、多样化的交互数据。虽然LLM具备模拟人类的潜力，但如果缺乏结构化约束，容易产生幻觉或行为漂移，难以保证训练数据的可靠性。\n\n### 3. 假设形成：动态感知与结构化仿真的协同\n基于上述痛点，作者提出了三个核心假设，构成了本文的方法论基石：\n\n*   **假设一（动态性）：** 如果将用户人格建模从“静态预设”转变为“逐轮预测”，并将这种动态特征作为RL状态的一部分，策略的适应性将显著提升。\n*   **假设二（可控性）：** 如果利用LLM作为模拟器，但通过“议程”机制进行结构化约束，就能在保证行为多样性的同时，生成符合逻辑且高质量的训练轨迹。\n*   **假设三（稳定性）：** 劝说的成功不仅在于达成口头协议，更在于防止用户事后反悔。如果在奖励函数中引入“反悔惩罚”，可以引导策略产生更稳固的承诺。\n\n### 4. 方法论构建：模块化架构的设计\n为了验证上述假设，作者设计了一个三层递进的架构：\n\n*   **第一步：构建“策略导向交互框架”（解决数据与控制问题）。**\n    *   **思路：** 为了解决LLM模拟的不稳定性，作者没有直接让LLM自由生成，而是设计了一个基于“议程”的控制器。\n    *   **逻辑：** 系统先选择高层策略（如“逻辑诉求”），再通过检索（MMR算法）生成具体回复。对于用户模拟，利用LLM但强制其遵循特定的行为模式。这样既利用了LLM的生成能力，又保证了数据的结构化和多样性。\n\n*   **第二步：实现“人格感知的用户表征”（解决动态建模问题）。**\n    *   **思路：** 将用户的混合型特征（25个连续变量 + 7个类别变量）编码为一个紧凑的81维向量。\n    *   **逻辑：** 关键在于“逐轮预测”。作者训练了一个预测器，在每一轮对话中根据最近的交互实时更新这个81维向量，并将其拼接到RL的状态输入中。这使得Agent能“看到”用户当前的心理轨迹。\n\n*   **第三步：设计“复合奖励与D3QN优化”（解决策略学习问题）。**\n    *   **思路：** 使用Dueling Double DQN（D3QN）来处理状态-价值估计。\n    *   **逻辑：** 在奖励函数设计上，除了常规的“同意意图”和“捐赠金额”，创新性地加入了“反悔惩罚”。这直接对应了假设三，迫使Agent不仅要说服用户，还要巩固用户的承诺，减少“口头答应但事后反悔”的情况。\n\n### 5. 逻辑闭环：实验验证与发现\n最后，作者通过实验验证了这一思考链条的有效性：\n*   **验证动态性：** 实验表明，包含逐轮人格特征的策略确实获得了更高的累积奖励。\n*   **验证仿真：** 基于LLM的仿真数据增强了模型对未见用户行为的泛化能力。\n*   **验证稳定性：** 引入反悔惩罚后，用户的反悔率确实下降，且捐赠结果略有提升。\n\n**总结：**\n作者的思考路径是从**“通用LLM缺乏策略性”**这一宏观洞察出发，通过**“动态人格”**和**“结构化仿真”**两个切入点，将心理学建模与强化学习紧密结合，最终构建了一个既能适应实时心理变化，又能产生稳定劝说效果的闭环系统。"
                },
                {
                    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
                    "arxiv_id": "2601.06789",
                    "authors": "Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang",
                    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究代码智能体，核心贡献是MemGovern框架，旨在通过将GitHub数据转化为可操作的经验记忆来增强智能体能力，属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决Code Agents因“封闭世界”限制而无法有效利用GitHub历史经验的问题。针对GitHub上非结构化且碎片化的Issue和PR数据，我们提出了MemGovern框架，通过Experience Governance将原始数据转化为结构化的Experience Cards，并引入Agentic Experience Search机制实现逻辑驱动的检索。在SWE-bench Verified上通过Resolution Rate验证了其有效性，平均提升了4.65%。",
                    "summary_translation": "尽管 autonomous software engineering (SWE) agents（自主软件工程智能体）正在重塑编程范式，但目前它们仍受限于“closed-world”限制：即试图从零开始修复 bug 或仅依赖 local context（局部上下文），而忽视了 GitHub 等平台上蕴藏的丰富历史人类经验。获取这种 open-world experience（开放世界经验）的过程，受到现实世界中 issue-tracking data（问题跟踪数据）非结构化和碎片化特性的阻碍。在本文中，我们介绍了 MemGovern，这是一个旨在对原始 GitHub 数据进行治理，并将其转化为智能体可用的 actionable experiential memory（可操作经验记忆）的 framework（框架）。MemGovern 采用 experience governance（经验治理）将人类经验转化为 agent-friendly（智能体友好）的 experience cards（经验卡片），并引入了一种 agentic experience search strategy（智能体经验搜索策略），从而实现了对 human expertise（人类专业知识）的 logic-driven retrieval（逻辑驱动检索）。通过生成 135K 个治理后的 experience cards（经验卡片），MemGovern 实现了显著的 performance boost（性能提升），将 SWE-bench Verified 上的 resolution rates（解决率）提高了 4.65%。作为一种 plug-in approach（插件式方法），MemGovern 为构建 agent-friendly memory infrastructure（智能体友好的记忆基础设施）提供了有效的解决方案。",
                    "inspiration_trace": "基于对论文《MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察与问题定义：从“封闭”到“开放”的鸿沟\n\n*   **现象观察**：当前的自主软件工程代理在解决代码问题时，往往表现得像是一个“孤胆英雄”。它们倾向于从零开始尝试修复Bug，或者仅依赖当前代码库的局部上下文。\n*   **人类对标**：在现实世界的软件工程实践中，资深开发者很少从零开始。面对复杂问题，他们习惯于在GitHub等协作平台上搜索历史记录，借鉴前人解决类似问题的专家推理和修复模式。\n*   **核心假设**：如果代码代理能够像人类一样，利用GitHub上海量的“开放世界”历史经验，其推理深度和修复准确性应该能得到显著提升。\n*   **现实瓶颈**：虽然GitHub蕴含了巨大的知识宝库，但直接将其用于Agent存在巨大的“语义鸿沟”。原始的Issue和PR讨论充满了社交闲聊、非标准术语和碎片化信息，噪声极大且高度异构。直接检索这些数据会导致“记忆污染”，难以实现跨仓库的知识迁移。\n\n### 2. 思考转折：从“数据检索”到“数据治理”\n\n*   **思维突破**：既然原始数据不可用，那么问题的核心就不在于“如何更好地检索”，而在于“如何将混乱的人类经验转化为Agent友好的知识”。\n*   **治理理念**：作者意识到必须引入一个中间层，即“经验治理”。这不仅仅是清洗数据，而是要进行知识蒸馏。\n*   **结构化重构**：为了解决跨仓库的异构性问题，作者提出将非结构化的讨论重构为标准化的“经验卡片”。\n*   **关键洞察（解耦）**：为了实现有效的知识迁移，必须将“检索信号”与“修复逻辑”解耦。\n    *   **索引层**：提取通用的故障症状（如异常类型、错误签名），用于跨仓库的广泛匹配。\n    *   **解析层**：封装可复用的修复逻辑（如根因分析、修复策略），用于具体的代码生成。\n    *   *逻辑推演*：这种分层设计使得Agent能够基于症状找到相似案例，再根据抽象的修复策略应用到当前的具体上下文中，从而实现了从“形似”到“神似”的跨越。\n\n### 3. 交互设计：从“静态注入”到“智能搜索”\n\n*   **对现有方法的批判**：传统的检索增强生成（RAG）通常采用“一次性检索+上下文注入”的模式。这就像把整本教科书扔给学生，不仅消耗上下文窗口，还容易引入噪声，干扰Agent的推理。\n*   **人类行为模拟**：人类查阅资料时是动态的——先搜索目录，筛选出相关章节，再深入阅读细节。\n*   **机制创新**：作者提出了“Agent式经验搜索”。\n    *   **双原语接口**：设计了“搜索”和“浏览”两个工具。搜索用于广度发现（基于索引层），浏览用于深度挖掘（基于解析层）。\n    *   **渐进式推理**：允许Agent根据当前解决问题的状态，自主决定是扩大搜索范围还是深入某个具体案例。这种机制让Agent具备了主动筛选和验证信息的能力，避免了被动接受噪声。\n\n### 4. 逻辑闭环与验证：质量即性能\n\n*   **质量控制的必要性**：考虑到自动化提取可能产生幻觉或遗漏，作者引入了基于检查表的质量控制机制，并设计了“优化循环”，确保进入记忆库的每张卡片都是经过验证的高质量知识。\n*   **最终假设验证**：如果上述逻辑成立，那么经过治理的经验配合渐进式搜索，应该能显著优于直接使用原始数据或传统RAG方法。\n*   **实验反馈**：通过在SWE-bench上的实验，证实了“治理后的经验”比“原始数据”更有效，且“Agent式搜索”比“静态RAG”更具鲁棒性。这反向验证了作者最初的假设：**高质量的结构化记忆 + 类人的搜索策略 = 更强的代码Agent**。\n\n### 总结\n\n作者的思考路径遵循了 **“发现人类行为优势 -> 识别数据应用瓶颈 -> 引入治理机制进行结构化转化 -> 模拟人类认知过程设计交互 -> 实验验证逻辑闭环”** 的完整链条。其核心贡献在于将“数据治理”引入了Agent的记忆构建过程，并证明了结构化的知识表示比单纯的数据量更重要。"
                },
                {
                    "title": "CEDAR: Context Engineering for Agentic Data Science",
                    "arxiv_id": "2601.06606",
                    "authors": "Rishiraj Saha Roy, Chris Hinze, Luzian Hahn, Fabian Kuech",
                    "summary": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个用于自动化数据科学的智能体框架（CEDAR），涉及规划（生成计划块）、工具使用（函数调用和代码生成）、记忆与上下文管理（智能历史渲染），符合单智能体及多智能体的研究范围。",
                    "summary2": "本文旨在解决利用 LLMs 自动化数据科学任务时面临的上下文限制、数据隐私及任务复杂性等问题。针对 Kaggle 竞赛等数据科学场景，我们提出了一种名为 CEDAR 的代理系统，采用结构化提示、多代理编排及智能历史渲染等上下文工程技术。我们在 canonical Kaggle challenges 上验证了其有效性，展示了其自动化解决初级数据科学任务的能力。",
                    "summary_translation": "我们展示了CEDAR，这是一个利用agentic setup（智能体架构）来自动化数据科学（DS）任务的应用程序。利用LLMs（大语言模型）解决数据科学问题是一个尚待深入探索但具有巨大市场价值的领域。其面临的挑战是多方面的，包括任务复杂性、数据规模、计算限制以及上下文限制。我们表明，通过有效的context engineering（上下文工程）可以缓解这些挑战。我们首先通过数据科学特定的输入字段为初始prompt（提示词）引入结构，这些字段作为智能体系统的指令。随后，解决方案被呈现为由独立的LLM agents（大语言模型智能体）生成的、交替的计划和代码块的枚举序列，从而在工作流的任何步骤都为上下文提供可读的结构。用于生成这些中间文本及相应Python代码的function calls（函数调用），确保数据保留在本地，仅有聚合统计信息及相关指令被注入到LLMs的prompt（提示词）中。我们通过迭代代码生成和智能历史渲染引入了容错机制和上下文管理。最后，我们利用典型的Kaggle挑战赛验证了该智能体数据科学家的可行性。",
                    "inspiration_trace": "基于论文《CEDAR: Context Engineering for Agentic Data Science》，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观行业痛点到微观技术实现的思维演进。\n\n---\n\n### 1. 宏观观察与问题定义\n**思考起点：** 数据科学（DS）工作流高度依赖人工，繁琐且重复，而现代大语言模型（LLM）具备自动化这些任务的潜力。然而，现有的通用LLM工具（如ChatGPT Advanced Data Analysis）在处理真实DS任务时表现不佳。\n\n**核心矛盾识别：**\n作者观察到，虽然LLM能力强大，但在DS领域存在“五大鸿沟”：\n1.  **任务复杂性：** 真实DS项目无法通过单次Prompt解决，需要多步推理。\n2.  **计算能力：** LLM的数学计算能力不可靠。\n3.  **数据规模：** 企业级数据往往超过上传限制。\n4.  **隐私安全：** 敏感数据无法上传至云端模型。\n5.  **上下文混乱：** 随着步骤增加，指令、代码、数据、错误信息混杂，导致上下文超出长度限制且逻辑不可读。\n\n### 2. 核心假设提出\n**思维转折：** 作者意识到，单纯提升模型智商并不能解决上述所有问题。真正的瓶颈在于**“上下文工程”**。\n\n**假设：** 如果能设计一套机制，在LLM推理过程中动态地优化、结构化并压缩上下文，同时确保数据不离开本地环境，就能构建一个高效、透明且安全的自动化数据科学系统。\n\n### 3. 方法论的逻辑演进\n基于上述假设，作者开始构建CEDAR系统，其逻辑演进遵循以下步骤：\n\n#### 第一阶段：输入的结构化（解决“指令不清”）\n*   **思考：** 用户往往不知道如何写完美的Prompt。DS任务有固定的元数据（如数据位置、评价指标、任务描述）。\n*   **决策：** 放弃自由文本输入，设计**结构化表单**。强制用户填写任务描述、数据路径、指标等字段。\n*   **逻辑：** 将非结构化的自然语言需求转化为结构化的机器指令，作为系统的初始上下文。\n\n#### 第二阶段：输出的结构化与可读性（解决“过程黑箱”）\n*   **思考：** 直接让模型输出最终结果（如一个准确率数值）既不可信也不可复用。人类数据科学家的工作方式是“计划+代码”交替进行（类似Jupyter Notebook）。\n*   **决策：** 强制模型输出**交错的文本和代码块**。每一步包含“自然语言计划”和“可执行代码”。\n*   **逻辑：** 模拟人类思维过程，让工作流透明化，便于人类审查和纠错。\n\n#### 第三阶段：智能体分工与路由（解决“任务复杂性”）\n*   **思考：** 让一个LLM同时负责规划、写代码、写解释、判断是否结束，负担太重，容易出错。\n*   **决策：** 引入**多智能体架构**。\n    *   **Orchestrator（编排器）：** 只负责决策，即“下一步该写文本还是写代码，或者结束”。\n    *   **Text Agent：** 专门负责写解释和分析。\n    *   **Code Agent：** 专门负责写Python代码。\n*   **逻辑：** 职责分离。利用**函数调用**和**结构化输出**（JSON Schema）约束编排器的行为，防止其产生幻觉，确保指令准确传递给子代理。\n\n#### 第四阶段：本地化执行与容错（解决“计算与隐私”）\n*   **思考：** LLM不擅长数学，且数据不能上传。\n*   **决策：** **代码即工具**。LLM只生成代码，代码在本地Docker容器中执行。\n*   **逻辑：**\n    *   **数据隐私：** 数据永远不离开本地，只有统计摘要进入Prompt。\n    *   **计算准确性：** 用Python解释器替代LLM进行数学运算。\n    *   **容错机制：** 如果代码执行报错，将错误信息回传给Code Agent进行迭代修复，而不是直接崩溃。\n\n#### 第五阶段：智能历史渲染（解决“上下文膨胀”）\n*   **思考：** 随着步骤增加，历史记录会无限增长，撑爆上下文窗口。直接截断会丢失关键信息。\n*   **决策：** 开发**History Rendering（历史渲染）模块**，对上下文进行“有损压缩”。\n*   **逻辑：**\n    *   **保留全量：** 用户的指令、生成的文本和代码本身通常不长，全量保留。\n    *   **智能截断：** 代码的输出往往很长。只保留成功输出的“头部”（关键信息）和失败输出的“尾部”（错误堆栈）。\n    *   **滑动窗口：** 如果总长度仍超限，只保留最近的N个字符。\n*   **逻辑：** 确保LLM在任何时刻看到的都是最相关、最精简的信息，从而维持推理连贯性。\n\n### 4. 最终系统形态\n通过上述层层递进的思考，作者最终形成了CEDAR系统的核心逻辑：\n**一个基于上下文工程的智能体系统，它通过结构化输入引导，利用编排器路由文本与代码生成代理，在本地执行代码以保障隐私与计算准确性，并通过智能的历史压缩机制，在有限的上下文窗口内完成复杂的数据科学任务。**\n\n---\n\n**总结：**\n作者的思考路径并非从“如何设计一个复杂的Agent”出发，而是从“如何管理信息流”出发。**CEDAR的本质不是算法创新，而是信息架构的创新**——通过精心设计什么信息应该进入Prompt、以什么形式进入、以及在何时被修剪，从而释放LLM在复杂任务中的潜力。"
                },
                {
                    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
                    "arxiv_id": "2601.06487",
                    "authors": "Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha",
                    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一种名为ArenaRL的强化学习范式，旨在通过锦标赛式相对排序来提升LLM智能体在开放式任务（如复杂旅行规划）中的表现。研究涉及智能体的自我演化（通过反馈自我完善）和规划能力，属于单智能体研究范畴，且侧重于算法改进而非纯应用或基础设施。",
                    "summary2": "本文旨在解决开放性Agent任务中强化学习因点式评分导致的判别性崩溃问题。针对缺乏客观真值的复杂规划场景，我们提出了一种ArenaRL框架，通过基于锦标赛的相对排名机制替代不稳定的标量评分，并利用带种子的单败淘汰赛实现高效优势估计。我们在Open-Travel和Open-DeepResearch基准上，通过胜率和多维度评估指标验证了其有效性，显著优于现有RL基线。",
                    "summary_translation": "强化学习 已显著提升了 LLM agents (大语言模型智能体) 在具有 verifiable outcomes (可验证结果) 的任务上的表现，但在具有 vast solution spaces (巨大解空间) 的 open-ended agent tasks (开放式智能体任务)（例如复杂的旅行规划）中仍然面临挑战。由于这些任务缺乏 objective ground-truth (客观真值)，当前的 RL algorithms (强化学习算法) 主要依赖于对 individual responses (单个响应) 分配 scalar scores (标量分数) 的 reward models (奖励模型)。我们认为这种 pointwise scoring (逐点打分) 存在固有的 discrimination collapse (判别性崩溃)：reward model (奖励模型) 难以区分不同 trajectories (轨迹) 之间的 subtle advantages (细微优势)，导致组内的分数被压缩到一个狭窄的范围内。因此，有效的 reward signal (奖励信号) 被 reward model (奖励模型) 的噪声所主导，导致 optimization stagnation (优化停滞)。为了解决这个问题，我们提出了 ArenaRL，这是一种从 pointwise scalar scoring (逐点标量打分) 转向 intra-group relative ranking (组内相对排序) 的 reinforcement learning paradigm (强化学习范式)。ArenaRL 引入了一种 process-aware pairwise evaluation mechanism (过程感知成对评估机制)，采用 multi-level rubrics (多级评分标准) 为 trajectories (轨迹) 分配 fine-grained relative scores (细粒度相对分数)。此外，我们构建了一个 intra-group adversarial arena (组内对抗竞技场) 并设计了一种 tournament-based ranking scheme (基于锦标赛的排序方案) 来获取稳定的 advantage signals (优势信号)。Empirical results (实证结果) 证实，构建的 seeded single-elimination scheme (种子单败淘汰赛方案) 在仅具有 O(N) 复杂度的情况下，实现了与具有 O(N^2) 复杂度的 full pairwise comparisons (全成对比较) 几乎等效的 advantage estimation accuracy (优势估计精度)，在效率和精度之间取得了最佳平衡。此外，为了解决缺乏针对 open-ended agents (开放式智能体) 的 full-cycle benchmarks (全周期基准) 的问题，我们构建了 Open-Travel 和 Open-DeepResearch，这两个高质量的 benchmarks (基准) 具有涵盖 SFT (监督微调)、RL training (强化学习训练) 和 multi-dimensional evaluation (多维评估) 的 comprehensive pipeline (全流程管道)。Extensive experiments (广泛实验) 表明，ArenaRL 明显优于 standard RL baselines (标准强化学习基线)，使 LLM agents (大语言模型智能体) 能够为复杂的现实世界任务生成更 robust (鲁棒) 的解决方案。",
                    "inspiration_trace": "基于论文《ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking》的内容，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观问题的确立与现状的困境\n**思考起点：** 强化学习（RL）在数学、代码等有明确“标准答案”的任务上极其成功，但在旅行规划、深度研究等**开放式任务**上却举步维艰。\n**核心矛盾：** 开放式任务没有客观的Ground Truth（标准答案）。现有的解决方案通常采用“LLM作为裁判”给模型的输出打一个标量分数（例如0-10分）。\n**初步假设：** 如果能训练一个准确的奖励模型来给这些开放式轨迹打分，就能像数学题一样进行RL优化。\n\n### 第二阶段：现象观察与核心痛点识别\n**深入观察：** 作者在实验中发现了一个反直觉的现象：随着模型能力的提升，RL优化反而停滞甚至退化。\n**归因分析：** 作者将此命名为**“判别性崩溃”**。\n1.  **信号压缩：** 当模型变强后，生成的轨迹质量都很高，且分布趋同。裁判很难区分“好”和“更好”，给出的分数被压缩在一个极窄的区间（如0.8-0.9）。\n2.  **信噪比（SNR）恶化：** 裁判本身存在随机噪声（如位置偏差、长度偏好）。当分数之间的差异（信号）小于裁判的随机误差（噪声）时，优化过程实际上是在拟合噪声，而非提升能力。\n**结论：** 在开放式任务中，**点式标量打分**存在根本性缺陷，无法提供有效的梯度信号。\n\n### 第三阶段：范式转移——从“绝对分数”到“相对排序”\n**理论借鉴：** 借鉴决策理论，人类在判断模糊事物时，相对比较（A比B好）比绝对量化（A是8.5分）更稳定、更准确。\n**核心假设：** 放弃给单个轨迹打绝对分，转而在**组内**进行轨迹之间的两两比较，构建相对排名。\n**预期收益：** 相对比较能放大细微的质量差异，避免陷入绝对分数的“高分段压缩”陷阱，从而获得更纯净的优势信号。\n\n### 第四阶段：工程落地的挑战——效率与精度的权衡\n**新问题：** 虽然两两比较（Round-Robin，循环赛）能提供最准确的排名，但其计算复杂度是 $O(N^2)$。对于需要大规模采样的RL训练来说，这是不可接受的昂贵成本。\n**朴素尝试与失败：**\n1.  **锚点法：** 只让所有样本与一个锚点（如贪婪解码结果）比较。复杂度降为 $O(N)$，但分辨率太低，无法区分两个都比锚点好但互有优劣的样本。\n2.  **标准淘汰赛：** 随机两两对决，胜者晋级。虽然快，但随机性太大。两个高质量的样本可能在第一轮就相遇，导致其中一个被过早淘汰，损失了信息。\n\n### 第五阶段：结构创新——带种子的单败淘汰赛\n**逻辑推演：** 为了在 $O(N)$ 的线性复杂度下保持接近循环赛的精度，必须解决“过早相遇”的问题。\n**解决方案：** 提出**带种子的单败淘汰赛**。\n1.  **预排序：** 先利用低成本的“锚点法”对所有样本进行一轮快速评估，得到一个粗略的初始排名（种子）。\n2.  **结构化对决：** 按照种子排布对阵（例如：第1名对最后一名，第2名对倒数第二名）。这保证了强样本在早期不会相遇，只有到了决赛圈才强强对话。\n**结果：** 这种设计既保留了线性复杂度的高效，又通过先验信息保证了排名的保真度，实现了效率与精度的最佳平衡。\n\n### 第六阶段：评估维度的深化——过程感知\n**最后一步：** 既然是Agent任务，评价标准不能只看最终答案。\n**补充逻辑：** 引入**过程感知的成对评估**。裁判不仅看结果，还要审查思维链的逻辑连贯性和工具调用的有效性。这确保了RL优化的方向是提升Agent的内在推理能力，而不是仅仅学会生成漂亮的最终文本。\n\n---\n\n**总结：**\n作者的思考路径是从**“开放式任务缺乏客观标准”**这一痛点出发，通过**“判别性崩溃”**否定了现有的标量打分范式，进而提出**“相对排序”**的理论转向。为了解决该理论带来的计算开销，作者通过**“带种子的淘汰赛”**这一精巧的结构设计，成功在计算效率和信号质量之间找到了最优解，最终形成了ArenaRL的方法论闭环。"
                },
                {
                    "title": "Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users",
                    "arxiv_id": "2601.06301",
                    "authors": "Arth Bhardwaj, Nirav Diwan, Gang Wang",
                    "summary": "Web scraping has historically required technical expertise in HTML parsing, session management, and authentication circumvention, which limited large-scale data extraction to skilled developers. We argue that large language models (LLMs) have democratized web scraping, enabling low-skill users to execute sophisticated operations through simple natural language prompts. While extensive benchmarks evaluate these tools under optimal expert conditions, we show that without extensive manual effort, current LLM-based workflows allow novice users to scrape complex websites that would otherwise be inaccessible. We systematically benchmark what everyday users can do with off-the-shelf LLM tools across 35 sites spanning five security tiers, including authentication, anti-bot, and CAPTCHA controls. We devise and evaluate two distinct workflows: (a) LLM-assisted scripting, where users prompt LLMs to generate traditional scraping code but maintain manual execution control, and (b) end-to-end LLM agents, which autonomously navigate and extract data through integrated tool use. Our results demonstrate that end-to-end agents have made complex scraping accessible - requiring as little as a single prompt with minimal refinement (less than 5 changes) to complete workflows. We also highlight scenarios where LLM-assisted scripting may be simpler and faster for static sites. In light of these findings, we provide simple procedures for novices to use these workflows and gauge what adversaries could achieve using these.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确研究并基准测试了“端到端LLM智能体”，重点评估了智能体的“工具使用”和“自主导航”能力，符合单智能体的研究范围。",
                    "summary2": "本文旨在评估LLM对网络爬虫的民主化影响及非专家用户的实际能力。针对35个跨越5个安全层级的网站，我们提出了两种工作流：LLM-assisted Scripting (LAS) 和 End-to-end LLM Agent (ELA)，并在这些网站上通过Extraction Success Rate (ESR)、Execution Time和Manual Effort Required (MER)验证了其有效性。",
                    "summary_translation": "历史上，Web scraping (网络爬虫) 一直需要掌握 HTML parsing (HTML解析)、session management (会话管理) 和 authentication circumvention (身份验证绕过) 等技术专长，这使得大规模数据提取仅限于熟练的开发者。我们认为，large language models (LLMs，大语言模型) 已经普及了 Web scraping，使低技能用户能够通过简单的 natural language prompts (自然语言提示) 执行复杂的操作。尽管现有的广泛基准测试是在最佳专家条件下评估这些工具的，但我们表明，在无需大量人工投入的情况下，当前的 LLM-based workflows (基于LLM的工作流) 能够使 novice users (新手用户) 抓取原本无法访问的复杂网站。我们针对 35 个跨越五个 security tiers (安全层级) 的网站（包括 authentication (身份验证)、anti-bot (反机器人) 和 CAPTCHA controls (验证码控制)），系统性地评估了日常用户利用 off-the-shelf LLM tools (现成的LLM工具) 所能实现的效果。我们设计并评估了两种截然不同的 workflows (工作流)： LLM-assisted scripting (LLM辅助脚本编写)，即用户提示 LLM 生成传统的抓取代码，但保留手动执行控制权；以及 end-to-end LLM agents (端到端LLM智能体)，即通过 integrated tool use (集成工具使用) 自主导航并提取数据。我们的结果表明，end-to-end agents (端到端智能体) 已使复杂的抓取任务变得易于实现——仅需一个提示配合 minimal refinement (微调，少于5次修改) 即可完成整个 workflows (工作流)。我们还强调了在某些场景下，对于 static sites (静态网站)，LLM-assisted scripting (LLM辅助脚本编写) 可能更为简单快捷。基于这些发现，我们为 novice users (新手用户) 提供了使用这些 workflows (工作流) 的简易流程，并评估了 adversaries (攻击者) 利用这些技术可能达到的效果。",
                    "inspiration_trace": "基于论文《Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 第一阶段：宏观观察与问题提出\n**——从“技术壁垒”到“技术民主化”的范式转移**\n\n1.  **观察现象**：\n    *   **过去**：网络爬虫是一项高门槛技术，需要掌握HTML解析、会话管理、反爬虫绕过等专业技能，这构成了天然的“技术过滤器”，限制了大规模数据提取仅限于熟练开发者。\n    *   **现在**：大语言模型（LLM）和智能体框架的出现，使得用户仅凭自然语言提示就能执行复杂的爬虫操作。\n\n2.  **提出核心问题**：\n    *   LLM是否真正实现了网络爬虫的“民主化”？\n    *   换言之，缺乏深厚技术背景的“日常用户”，是否真的能利用现成的LLM工具，完成以前只有专家才能做到的复杂数据提取？\n\n### 第二阶段：识别研究空白\n**——现有评估与真实场景的脱节**\n\n1.  **批判现有文献**：\n    *   作者注意到，现有的基准测试（如AgentBench, OSWorld）大多关注“最佳实践”。\n    *   这些测试通常假设在**理想条件**下进行：拥有专家指导、经过优化的配置、复杂的提示工程。\n\n2.  **锁定现实差距**：\n    *   **真实用户画像**：非专家用户通常使用默认设置，缺乏深度调试技能，且受限于时间和预算。\n    *   **研究盲区**：学术界缺乏对“非专家用户在现实约束下，利用现成工具到底能做到什么程度”的实证评估。\n\n3.  **确立研究目标**：\n    *   不再评估“工具的上限（专家能做什么）”，而是评估“工具的下限（新手能做什么）”。\n    *   量化这种“民主化”对网络安全防御（反爬虫）的实际影响。\n\n### 第三阶段：假设构建与变量设计\n**——如何模拟“真实世界”的复杂性？**\n\n1.  **定义威胁模型**：\n    *   为了建立保守的基线，作者将研究对象设定为“低技能行为者”。假设他们只会运行Python脚本、使用LLM，但不了解爬虫库的深层细节，也不使用高级提示技巧。\n\n2.  **构建难度梯度**：\n    *   为了全面测试，作者认为不能只测静态页面。必须模拟网站防御的升级过程。\n    *   **逻辑推演**：从最简单的静态页面，逐步增加难度，直到传统工具完全失效。\n    *   **最终分类**：确立了5个难度层级（简单HTML -> 复杂HTML -> 简单认证 -> 复杂认证 -> CAPTCHA）。\n\n### 第四阶段：方法论形成\n**——对比两种截然不同的“人机协作模式”**\n\n1.  **模式抽象**：\n    *   作者意识到，用户使用LLM爬虫主要有两种思维模式，这构成了实验的核心对比维度：\n    *   **模式 A：LLM辅助脚本编写 (LAS)**。\n        *   *思维逻辑*：用户仍想掌控代码执行，只是把LLM当作“高级程序员”来生成代码（如BeautifulSoup/Scrapy脚本），然后自己运行。\n        *   *代表场景*：传统开发者的提效工具。\n    *   **模式 B：端到端LLM智能体 (ELA)**。\n        *   *思维逻辑*：用户完全不想写代码，只给目标，让智能体像人一样操作浏览器（如Claude, Simular.ai）。\n        *   *代表场景*：完全不懂代码的小白用户。\n\n2.  **确立评估指标**：\n    *   除了传统的“成功率”（能不能做），作者引入了“易用性指标”（好不好做）。\n    *   **关键指标**：手动干预程度。这直接反映了“民主化”的程度——如果需要频繁手动调试，说明门槛依然存在。\n\n### 第五阶段：实证推演与结果验证\n**——验证“易用性”与“能力”的权衡**\n\n1.  **预期假设**：\n    *   对于静态网站，传统代码（LAS）应该更快、更高效。\n    *   对于复杂网站（登录、验证码），智能体（ELA）应该具有压倒性优势，因为它们能模拟人类行为。\n\n2.  **实验验证与发现**：\n    *   **发现1**：ELA确实让复杂爬虫变得触手可及（单次提示即可），证明了民主化的真实性。\n    *   **发现2**：但在简单任务上，ELA效率低下（慢10-20倍），属于“杀鸡用牛刀”。\n    *   **发现3**：LAS在遇到认证和反爬时彻底失效，而ELA虽然慢但能行得通。\n\n### 第六阶段：结论与启示\n**——从“二元对立”到“场景互补”**\n\n1.  **逻辑升华**：\n    *   作者的思考并没有停留在“谁更好”，而是上升到了“适用场景”。\n    *   **核心结论**：不存在万能的工具，存在的是“效率”与“可访问性”的权衡。\n\n2.  **未来展望**：\n    *   基于实验结果，作者进一步推演出未来的理想形态：**混合模式**。\n    *   *新思路*：利用智能体（ELA）去搞定最难的“登录/绕过”环节，获取会话权限，然后交给传统脚本（LAS）进行高效的数据提取。这结合了智能体的“灵活性”和脚本的“高效性”。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“现象观察 -> 差距识别 -> 模型构建 -> 对比实验 -> 场景化结论”**的学术逻辑。其核心创新点在于将评估视角从“技术能力的极限”转向了“普通用户的可达性”，并通过对两种工作流（LAS vs ELA）的精细划分，精准地刻画了LLM时代网络爬虫技术的新版图。"
                },
                {
                    "title": "Automated QoR improvement in OpenROAD with coding agents",
                    "arxiv_id": "2601.06268",
                    "authors": "Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee",
                    "summary": "EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.",
                    "category": "cs.AI",
                    "filter_reason": "论文提出了一个名为AuDoPEDA的自主编码系统，明确使用了“coding agents”这一术语。该系统具备单智能体的核心特征：自主性、规划（提出研究方向）、工具使用（读取代码库、提交可执行差异）以及闭环反馈机制，完全符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决EDA开发受限于专家资源稀缺及代码库复杂的问题，实现利用LLM自主改进OpenROAD的QoR。针对OpenROAD多语言、大规模的代码仓库，我们提出了AuDoPEDA系统，该系统集成了图结构文档生成、基于文献的DSPy规划及具有QoR反馈的自主执行代理。在ASAP7、SKY130HD和Nangate45 benchmark上，通过routed wirelength和effective clock period验证，实现了线长降低5.9%和时钟周期减少10.0%的显著效果。",
                    "summary_translation": "EDA（电子设计自动化）的开发与创新一直受到专家工程资源稀缺的制约。尽管领先的 LLMs（大语言模型）在代码编写和科学推理任务中表现优异，但其在推动 EDA 技术本身发展方面的能力尚未得到充分验证。我们提出了 AuDoPEDA，这是一个构建于 OpenAI 模型和 Codex 类智能体之上的自主式、基于代码仓库的编码系统。该系统能够读取 OpenROAD（开源自动化设计工具），提出研究方向，将其扩展为实施步骤，并提交可执行的 diffs（差异补丁）。我们的主要贡献包括：(i) 一个用于 EDA 代码修改的闭环 LLM 框架；(ii) 一套面向 PPA（功耗、性能、面积）优化的 OpenROAD 任务集及评估协议；以及 (iii) 仅需极少量人工监督的端到端演示。在 OpenROAD 上进行的实验表明，布线线长最多降低了 5.9%，有效时钟周期最多缩短了 10.0%。",
                    "inspiration_trace": "基于论文《Automated QoR improvement in OpenROAD with coding agents》，以下是对作者提出AuDoPEDA方法核心逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程。\n\n---\n\n### 1. 宏观问题：EDA创新的资源瓶颈与LLM的潜力错位\n**观察：**\nEDA（电子设计自动化）工具的发展严重依赖资深专家。这些专家需要跨越庞大的代码库（数百万行C++、Tcl、Python等）和复杂的迭代流程进行推理。然而，这种人力资源极其稀缺，限制了EDA技术的迭代速度。\n\n**矛盾：**\n另一方面，以GPT-4、Codex为代表的大语言模型（LLM）在代码生成和科学推理任务上表现出色。但在EDA领域，LLM的应用多停留在辅助脚本编写或RTL生成层面，尚未触及核心物理设计（PD）算法的改进。\n\n**核心问题：**\n能否让LLM驱动的智能体像人类专家一样，自主地对工业级EDA代码库进行修改，并直接提升芯片设计的质量（QoR，如功耗、性能、面积）？\n\n---\n\n### 2. 深入分析：通用代码代理在EDA领域的“水土不服”\n**挑战识别：**\n作者意识到，直接将通用的代码生成模型（如GitHub Copilot）应用于OpenROAD这样的EDA项目会面临三个致命障碍：\n1.  **上下文稀释：** OpenROAD代码库规模巨大、语言混杂（C++核心+Tcl脚本+Python工具），且文档稀疏。通用模型无法在有限的上下文窗口中理解跨模块的隐式接口和不变量。\n2.  **领域知识缺失：** 优化物理设计不仅仅是写代码，更需要结合EDA领域的学术文献（如布局、布线算法）。单纯的代码补全无法产生“研究级”的改进思路。\n3.  **验证闭环困难：** 软件工程的正确性通常通过单元测试判断，但EDA的改进必须通过物理设计流程（RTL-to-GDS）来验证，指标是PPA（功耗、性能、面积）。这是一个高成本、长周期的反馈过程。\n\n---\n\n### 3. 核心假设：模拟人类专家的“入职”过程\n**思维转折：**\n作者提出，与其试图训练一个懂EDA的超级模型，不如模拟人类专家的学习路径。人类专家在接手OpenROAD时，并不是直接阅读源码，而是先阅读文档、理解架构、查阅文献，然后提出假设，最后修改代码并跑流验证。\n\n**假设：**\n如果构建一个系统，能够为LLM智能体提供“文档优先”的入职环境，使其能够像人类一样结构化地获取代码知识、结合文献进行规划，并在真实的QoR反馈下迭代，那么它就能实现自主的代码改进。\n\n---\n\n### 4. 方法论构建：四阶段逻辑演进\n基于上述假设，作者将复杂的任务解构为四个逻辑严密的阶段，形成了一个闭环系统。\n\n#### 第一阶段：结构化理解（S0）—— 解决“看不懂”的问题\n**思考：**\n原始代码库太乱，直接喂给LLM效果差。必须先进行预处理，提取出机器可读的“知识图谱”。\n**逻辑：**\n利用Tree-sitter解析多语言代码，构建属性图（DAG），将函数调用、依赖关系显式化。然后，通过自底向上的遍历，自动生成“文档卡片”，总结每个模块的API、前置/后置条件。这相当于为智能体编写了一部动态更新的“操作手册”。\n\n#### 第二阶段：文献引导的规划（S1）—— 解决“没思路”的问题\n**思考：**\n光懂代码结构不够，还需要知道“改什么能提升性能”。这需要领域知识。\n**逻辑：**\n将规划过程视为一个声明式的程序（利用DSPy框架）。智能体结合“代码文档”（S0产物）和“EDA文献库”（外部知识），通过检索增强生成（RAG），合成出高层的研究计划。例如：“根据文献X，调整布局阶段的拥塞惩罚权重可能减少线长”。\n\n#### 第三阶段：计划定位与颗粒化（S2）—— 解决“落地难”的问题\n**思考：**\n高层计划（如“调整拥塞权重”）不能直接执行，必须映射到具体的代码修改点，且必须保证修改是安全的。\n**逻辑：**\n将高层计划投影到代码图上，找到具体的修改位置（文件、函数）。同时，将计划转化为“颗粒化计划”，包含具体的Diff意图、预检查（编译、测试）、监控指标和回滚条件。这一步将抽象的“研究思路”变成了可执行的“工程任务单”。\n\n#### 第四阶段：自主执行与QoR反馈（S3）—— 解决“验证慢”的问题\n**思考：**\n代码修改后，必须跑通EDA流程才能知道好坏。如何保证自动化且不破坏系统？\n**逻辑：**\n构建一个基于Codex的执行智能体，应用Diff、编译、运行OpenROAD流程。关键在于引入“QoR门控”：如果修改导致DRC违规或时序恶化，系统自动回滚。智能体通过爬山算法，在指标反馈的引导下不断尝试，直到找到最优解。\n\n---\n\n### 5. 总结：从“辅助工具”到“自主研究员”的范式转变\n**逻辑闭环：**\n整个思考过程从解决“资源稀缺”出发，通过分析EDA代码的特殊性，提出了“模拟人类专家学习”的核心假设，并最终落地为一个集成了**知识图谱构建（S0）**、**文献推理（S1）**、**工程映射（S2）**和**闭环验证（S3）**的完整系统。\n\n**最终贡献：**\n作者不仅仅是在用LLM写代码，而是构建了一个能够**阅读文献、提出假设、修改算法、并在真实芯片设计流程中验证效果**的自主科研智能体。这标志着EDA工具的优化模式从“人工驱动”转向了“AI自主驱动”。"
                },
                {
                    "title": "Latent Space Communication via K-V Cache Alignment",
                    "arxiv_id": "2601.06123",
                    "authors": "Lucio M. Dery, Zohar Yahav, Henry Prior, Qixuan Feng, Jiajun Shen, Arthur Szlam",
                    "summary": "Solving increasingly complex problems with large language models (LLMs) necessitates a move beyond individual models and towards multi-model systems that can effectively collaborate. While text has traditionally served as the medium for inter-model communication, a richer and more efficient exchange is possible if models can access each other's internal states directly. In this paper, we propose learning a shared representation space that aligns the k-v caches of multiple models, creating a high-bandwidth channel for collaboration without altering the underlying pre-trained parameters. We do so by augmenting each model with adapters to translate its state into and out of this shared space. Via a suite of experiments with Gemma-2 models, we demonstrate that this approach not only enables seamless inter-model communication but also improves individual model performance. We also show that the shared space allows for the direct transfer of learned skills, such as soft prompts, between different models. Our work represents a significant step towards a future where models can fluidly share knowledge and capabilities.",
                    "category": "cs.AI",
                    "filter_reason": "论文主要研究多模型系统之间的协作与通信机制，通过K-V缓存对齐实现模型间的高效信息交换，符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决多模型协作中通信带宽低及潜在空间不兼容的问题。针对不同训练条件下的LLM，我们提出了一种通过学习共享k-v cache潜在空间并利用adapters进行翻译对齐的方法，在Gemma-2模型及多语言C4数据集上通过语言建模损失验证了其有效性。",
                    "summary_translation": "利用大型语言模型解决日益复杂的问题，要求我们超越单一模型，转向能够有效协作的多模型系统。尽管文本传统上一直作为模型间通信的媒介，但如果模型能够直接访问彼此的内部状态，则可以实现更丰富、更高效的交互。在本文中，我们提出学习一个共享表示空间，该空间对齐多个模型的 k-v caches (键值缓存)，从而在不改变底层预训练参数的情况下，为协作创建一个高带宽通道。我们通过为每个模型增加 adapters (适配器) 来实现这一点，用于将其状态转换进出该共享空间。通过一系列基于 Gemma-2 模型的实验，我们证明了该方法不仅实现了无缝的模型间通信，还提升了单个模型的性能。我们还展示了该共享空间允许在不同模型之间直接迁移习得的技能，例如 soft prompts (软提示)。我们的工作代表了迈向模型能够灵活共享知识和能力未来的重要一步。",
                    "inspiration_trace": "基于论文《Latent Space Communication via K-V Cache Alignment》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n### 1. 宏观背景：单体模型的局限与协作的必要性\n**思考起点**：随着LLM能力边界的扩展，单一模型（无论是通用模型还是领域专家）在解决极其复杂的问题时往往力不从心。\n**逻辑推演**：未来的趋势必然是从“单体智能”走向“群体智能”。我们需要构建一个多模型系统，让不同特长的模型能够协同工作。然而，这就引出了一个核心问题：**这些模型之间应该如何高效地交换信息？**\n\n### 2. 现状瓶颈：文本通信的低带宽\n**观察现状**：目前多模型协作（如Agent系统、级联模型）主要依赖自然语言文本进行通信。\n**痛点分析**：文本是一种“有损”且低带宽的媒介。模型内部丰富的推理链、上下文细节和隐含状态很难被完全压缩进几个token的文本中。这种通信方式就像两个人只能通过纸条交流，效率极低且信息丢失严重。\n**思考方向**：如果模型能绕过文本，直接读取彼此的“思维过程”，协作效率将产生质的飞跃。\n\n### 3. 核心洞察：K-V Cache作为高带宽载体\n**技术聚焦**：Transformer架构中的Key-Value (K-V) Cache 实际上存储了模型处理输入时的内部状态（注意力机制的历史记录）。\n**假设提出**：K-V Cache 是模型内部状态的丰富表征。如果模型A能直接访问模型B的K-V Cache，就相当于直接读取了B的“记忆”和“推理路径”。这提供了一种比文本高得多的通信带宽。\n\n### 4. 关键障碍：潜在空间的异构性\n**现实挑战**：虽然想法很美好，但现实很骨感。不同模型（不同架构、不同训练数据、不同随机初始化）的K-V Cache所在的潜在空间是完全不同的。\n**深层原因**：由于参数不同，同一个token在不同模型中产生的条件依赖和向量表征是截然不同的，且这种差异会随着网络深度呈指数级放大。直接混用会导致模型“听不懂”对方的内部状态。\n\n### 5. 理论假设：引入“中间语”共享空间\n**灵感借鉴**：借鉴机器翻译中的“中间语”概念。在翻译多种语言时，不直接进行两两互译，而是先将所有语言映射到一个抽象的语义空间，再从该空间映射到目标语言。\n**核心构想**：构建一个**全局共享的潜在空间（$\\Sigma$）**。这个空间充当所有模型的“通用语言”。每个模型只需要学会两件事：如何把自己的K-V Cache“翻译”进这个共享空间，以及如何从共享空间“翻译”回自己的私有空间。\n\n### 6. 方法构建：基于Adapter的非线性映射\n**设计约束**：为了保持模型的原始能力并降低成本，不能修改预训练模型的参数。\n**架构设计**：为每个模型配备轻量级的“适配器”。\n*   **映射方向**：$T[\\text{Model} \\to \\Sigma]$（编码）和 $T[\\Sigma \\to \\text{Model}]$（解码）。\n*   **非线性选择**：由于不同模型间的几何关系可能高度复杂且非线性，简单的线性映射可能不够。作者选择了基于交叉注意力的小型Transformer作为适配器架构，以捕捉复杂的层级依赖关系。\n*   **扩展性**：这种设计使得参数量仅随模型数量线性增长，且新模型加入时无需重训练整个系统。\n\n### 7. 优化目标：从“形似”到“神似”\n**训练信号的选择**：如何训练这些适配器？\n*   **初级尝试（重建损失）**：强制让翻译后的Cache看起来像目标模型原本的Cache。但这可能过于严格，且受限于目标模型本身的能力上限。\n*   **进阶思考（功能对齐）**：我们不需要Cache完全一样，只需要它们产生的**结果**一样。\n*   **最终方案（后缀语言建模损失）**：使用源模型的前缀Cache翻译给目标模型，看目标模型能否准确预测后续的文本。这是一种“功能主义”的训练目标，只要能帮助模型完成任务，Cache长什么样并不重要。实验证明，这种方法甚至能通过共享空间“蒸馏”出更好的特征，提升单体模型性能。\n\n### 8. 价值延伸：技能的即插即用\n**逻辑推演**：既然存在一个共享的潜在空间，那么在这个空间中的任何表征（如软提示 Soft Prompts、前缀微调 Prefix Tuning）本质上都变成了一种“通用资源”。\n**应用场景**：在一个模型上学到的特定技能（如某种写作风格或编程能力），可以通过共享空间直接“移植”给另一个模型，而无需对目标模型进行额外训练。这实现了从“模型协作”到“技能复用”的跨越。\n\n---\n\n**总结**：\n作者的思考路径是从**解决多模型协作效率低下的宏观痛点**出发，通过**挖掘K-V Cache的高带宽价值**，针对**模型异构性这一核心障碍**，借鉴**机器翻译的中间语思想**，提出了**基于共享潜在空间和Adapter映射的解决方案**，并最终通过**功能对齐的训练目标**和**技能迁移的验证**，完成了从理论构想到方法论的闭环。"
                },
                {
                    "title": "Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation",
                    "arxiv_id": "2601.06034",
                    "authors": "Dudekula Kasim Vali",
                    "summary": "Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.",
                    "category": "cs.AI",
                    "filter_reason": "该论文提出了一个“Autonomous QA Agent”，利用RAG（检索增强生成）作为记忆机制，并生成Selenium脚本（工具使用），属于单智能体研究范畴（记忆、工具使用），且不属于排除的纯应用领域（如医疗/金融）或纯推理研究。",
                    "summary2": "本文旨在解决LLM生成Selenium脚本时因缺乏应用上下文而产生幻觉的问题。针对自然语言需求和HTML DOM结构，我们提出了一种Autonomous QA Agent，这是一种基于RAG的多模态框架，通过检索文档与HTML上下文生成脚本。在自定义电商应用的20个测试场景上，通过语法有效性、元素解析率和执行成功率验证了其有效性，实现了90%的执行成功率。",
                    "summary_translation": "软件测试在软件开发生命周期中至关重要，然而将需求转化为可执行测试脚本的过程仍主要依赖人工，且容易出错。尽管大语言模型能够生成代码，但它们经常产生幻觉，编造出不存在的UI元素。我们提出了自主QA代理，这是一种检索增强生成系统，它将Selenium脚本生成基于特定项目的文档和HTML结构之上。通过将多种格式导入向量数据库，我们的系统在生成代码之前会检索相关的上下文信息。针对20个电商测试场景的评估表明，我们的RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95%置信区间：[85%, 95%]，p < 0.001）的执行成功率，而标准LLM生成的成功率仅为30%。尽管我们的评估仅限于单一领域，但我们的方法通过将生成过程基于实际的DOM结构，显著减少了幻觉现象，展示了RAG在自动化UI测试中的潜力。",
                    "inspiration_trace": "基于论文《Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：QA环节的效率瓶颈\n**思考起点：** 在敏捷开发和DevOps主导的现代软件工程中，开发迭代速度极快，但软件测试（QA）成为了明显的瓶颈。\n**核心痛点：** QA工程师花费40%-50%的时间在做“翻译”工作——将自然语言描述的功能需求（PRD）手动转化为机器可执行的自动化测试脚本（如Selenium）。这个过程不仅枯燥，而且容易出错（如选错元素ID、忽略边界情况）。\n**初步设想：** 能否利用代码生成能力强大的大语言模型（LLM）来自动完成这个“翻译”过程？\n\n### 2. 尝试与失败：LLM的“盲写”困境\n**尝试：** 直接使用标准的LLM（如GPT-4, Llama），输入自然语言需求（如“生成一个添加购物车的脚本”），让其编写Selenium代码。\n**观察到的现象：** LLM生成的代码语法通常没问题，但一运行就报错。\n**失败原因分析：** LLM患有一种“盲写症”。它通晓通用的编程语法，但它**看不见**被测应用（AUT）的具体结构。\n**具体表现：** LLM会凭空捏造UI元素。例如，它可能会猜测登录按钮的ID是 `#login-btn`，但实际开发人员写的是 `#btn-submit-login`。这种“幻觉”导致生成的脚本无法定位元素，执行失败。\n\n### 3. 深度诊断：语义鸿沟与上下文缺失\n**问题定义：** 核心问题在于“人类需求”与“机器执行”之间存在语义鸿沟。要生成一个可运行的脚本，不仅需要逻辑（做什么），还需要精确的定位信息（在哪里做）。\n**现有方案的局限：**\n*   **传统MBT（基于模型的测试）：** 构建成本太高，维护困难。\n*   **通用代码RAG：** 现有的检索增强生成多用于检索“相似的代码片段”。但在UI测试中，检索别人的代码对定位当前页面的特定DOM元素帮助不大。\n**关键洞察：** 要解决幻觉，必须让LLM“看见”真实的界面结构。LLM缺失的上下文不是代码示例，而是**应用的实际DOM结构**。\n\n### 4. 策略转折：从“代码检索”到“结构检索”\n**核心假设：** 如果在生成脚本之前，先给LLM提供被测应用的真实HTML文档和需求文档，它就能基于真实的结构编写准确的定位器，从而消除幻觉。\n**方法论创新：** 提出一种专门针对QA领域的RAG架构。\n*   **传统RAG：** 检索通用知识库。\n*   **本论文RAG：** 检索**双模态上下文**。\n    1.  **功能性上下文：** 需求文档（Markdown/PDF），告诉LLM“要测什么”。\n    2.  **结构性上下文：** 原始HTML文件，告诉LLM“元素在哪里”。\n\n### 5. 架构构建：多模态摄入与上下文融合\n**逻辑推演：** 为了实现上述假设，系统需要具备以下能力：\n1.  **知识库构建：** 必须能够“吃进”多种格式的数据。不仅要处理文本需求，还要解析HTML标签，提取出ID、Class等关键属性，并存入向量数据库。\n2.  **精准检索：** 当用户提问时，系统需要同时从文档库中找到相关需求，并从HTML库中找到对应的页面结构片段。\n3.  **提示工程约束：** 在生成阶段，必须强制LLM使用检索到的真实ID，而不是自己编造。通过Prompt明确指令：“仅使用提供的HTML结构中的ID”。\n\n### 6. 验证与结论：Grounding（接地气）的有效性\n**实验设计：** 对比“标准LLM（无上下文）”与“RAG Agent（含HTML上下文）”。\n**结果验证：**\n*   标准LLM：虽然语法正确，但因元素定位错误，执行成功率仅为30%。\n*   RAG Agent：通过将生成过程“锚定”在真实的DOM结构上，执行成功率提升至90%。\n**最终结论：** 证明了在UI自动化测试中，**结构化的上下文（HTML）比通用的代码知识更重要**。通过RAG技术将LLM与实际应用状态连接，是解决测试脚本生成中“幻觉”问题的有效路径。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现瓶颈 -> 尝试新技术（LLM） -> 识别新技术缺陷（幻觉/盲写） -> 引入特定领域知识（DOM结构） -> 设计专用架构（多模态RAG） -> 验证有效性”** 的完整逻辑闭环。其核心创新点在于意识到UI测试不仅仅是代码生成任务，更是一个需要精确空间感知（DOM结构）的任务。"
                },
                {
                    "title": "AI-Assisted Authoring for Transparent, Data-Driven Documents",
                    "arxiv_id": "2601.06027",
                    "authors": "Alfonso Piscitelli, Cristina David, Mattia De Rosa, Ali Mohammed, Federico Nanni, Jacob Pake, Roly Perera, Jessy Sodimu, Chenyiqiu Zheng",
                    "summary": "We introduce _transparent documents_, interactive web-based scholarly articles which allow readers to explore the relationship to the underlying data by hovering over fragments of text, and present an LLM-based tool for authoring transparent documents, building on recent developments in data provenance for general-purpose programming languages. As a target platform, our implementation uses Fluid, an open source programming language with a provenance-tracking runtime. Our agent-based tool supports a human author during the creation of transparent documents, identifying fragments of text which can be computed from data, such as numerical values selected from records or computed by aggregations like sum and mean, comparatives and superlatives like _better than_ and _largest_, trend-adjectives like _growing_, and similar quantitative or semi-quantitative phrases, and then attempts to synthesise a suitable Fluid query over the data which generates the target string. The resulting expression is inserted into the article's web page, turning the static text fragment into an interactable data-driven element able to reveal the data that underwrites the natural language claim. We evaluate our approach on a subset of SciGen, an open source dataset consisting of tables from scientific articles and their corresponding descriptions, which we extend with hand-generated counterfactual test cases to evaluate how well machine-generated expressions generalise. Our results show that gpt4o is often able to synthesise compound expressions extensionally compatible with our gold solutions.",
                    "category": "cs.AI",
                    "filter_reason": "论文明确提出了一个“基于智能体的工具”，利用LLM（GPT-4o）辅助人类作者进行文档创作。该智能体具备工具使用能力，能够识别文本片段并合成Fluid查询与外部系统交互，符合单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在解决学术文档中数据声明难以追溯至底层数据的问题。针对科学论文中的定量描述，我们提出了一种基于LLM的AI辅助编写工具，结合Fluid编程语言的溯源运行时，将静态文本转化为可交互的数据驱动元素，并在SciGen数据集上通过成功率及反事实测试验证了其有效性。",
                    "summary_translation": "我们介绍了“透明文档”，这是一种交互式的基于网络的学术文章，允许读者通过将鼠标悬停在文本片段上来探索其与底层数据的关系。基于通用编程语言在 data provenance（数据溯源）方面的最新进展，我们提出了一种基于 LLM 的工具，用于创作此类透明文档。在目标平台方面，我们的实现采用了 Fluid，这是一种具有 provenance-tracking runtime（具有溯源跟踪功能的运行时）的开源编程语言。我们的 agent-based（基于智能体）的工具在透明文档的创作过程中为人类作者提供支持。该工具能够识别那些可以从数据中计算得出的文本片段，例如：从记录中选取的数值，或通过 sum（求和）和 mean（平均）等 aggregations（聚合操作）计算得出的数值；“better than”（优于）和“largest”（最大）等 comparatives and superlatives（比较级和最高级）；“growing”（增长）等 trend-adjectives（趋势形容词）；以及类似的 quantitative or semi-quantitative phrases（定量或半定量短语）。随后，工具会尝试合成一个合适的 Fluid query（Fluid 查询），以生成目标字符串。生成的表达式被插入到文章的网页中，将静态文本片段转化为 interactable data-driven element（可交互的数据驱动元素），从而能够揭示支撑该 natural language claim（自然语言陈述）的数据。我们在 SciGen 数据集的一个子集上对该方法进行了评估。SciGen 是一个由科学文章中的表格及其对应描述组成的开源数据集。我们通过手工生成的 counterfactual test cases（反事实测试用例）对该数据集进行了扩展，以评估机器生成表达式的 generalise（泛化）能力。结果表明，gpt4o 通常能够生成与我们的 gold solutions（黄金标准）在 extensionally compatible（外延兼容）的 compound expressions（复合表达式）。",
                    "inspiration_trace": "基于论文《AI-Assisted Authoring for Transparent, Data-Driven Documents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法产出的思考过程：\n\n### 1. 宏观观察：科学交流中的“可追溯性鸿沟”\n**思考起点：**\n作者首先关注到学术出版和科学写作中的一个核心痛点——**信任与验证的困难**。\n*   **现象：** 在学术论文或数据报告中，充斥着大量基于数据的断言（如“系统X比系统Y更快”）。这些断言以静态的自然语言形式存在。\n*   **问题：** 读者（或审稿人）很难直接从文本追溯到支撑该断言的具体数据点。这种“断言”与“证据”之间的脱节，导致了验证困难，甚至因数据管理错误导致论文撤稿。\n*   **现有技术的局限：**\n    *   **数据可视化工具（如Tableau, D3.js）：** 虽然图表是动态的，且部分工具支持溯源，但它们无法处理占据论文主体的自然语言。\n    *   **大语言模型（LLM）：** 擅长理解和生成文本，甚至能进行事实核查，但其输出通常是黑盒的，缺乏将文本片段直接链接到底层数据源的交互式基础设施。\n\n### 2. 核心假设：将“自然语言”视为“计算输出”\n**思维跃迁：**\n为了解决上述鸿沟，作者提出一个颠覆性的假设：**论文中的定量陈述不应是静态的字符串，而应是数据查询的计算结果。**\n*   **类比思维：** 就像Excel中的图表会随数据变化而更新一样，论文中的文字（如“增长率为5%”）也应该是动态生成的。\n*   **概念定义：** 作者提出了“透明文档”的概念。这种文档允许读者通过鼠标悬停在文本上，触发“溯源查询”，直接看到生成该文本的数据来源。\n*   **关键挑战：** 如果要求作者手动编写代码来生成每一个句子（例如写SQL或Python代码来输出“better than”），这在科学写作工作流中是不现实的，门槛太高。\n\n### 3. 方法论构建：寻找“语义理解”与“程序化溯源”的结合点\n**解决方案的合成：**\n作者意识到，要实现上述假设，必须结合两个领域的最新进展，形成互补：\n1.  **LLM的语义理解能力：** 负责将自然语言（如“显著提高”）转化为形式化的逻辑意图。\n2.  **溯源编程语言（Fluid）的基础设施：** 负责执行逻辑并自动维护数据流向，提供交互能力。\n\n**逻辑推演：**\n*   *为什么选Fluid？* 普通语言（如Python）只能计算数据，无法自动追踪数据来源并支持用户交互（悬停查询）。Fluid特有的溯源运行时是“透明性”的技术保障。\n*   *为什么用LLM？* 只有LLM能理解复杂的学术语言并自动生成代码，从而降低作者的使用门槛。\n\n### 4. 实现策略：从“全自动”转向“人机协同”\n**工作流设计：**\n在具体实现路径上，作者没有追求完全自动化的“一键生成”，而是基于对LLM局限性的认知，设计了**人机协同**的迭代工作流。\n*   **思考逻辑：** LLM可能会产生幻觉或生成错误的代码。如果完全自动化，生成的文档将不可信。\n*   **Agent分工：**\n    *   **SuggestionAgent：** 充当“助手”，识别哪些文本片段是可以被数据化的（如数值、比较级）。\n    *   **InterpretationAgent：** 充当“翻译官”，尝试将文本片段编译为Fluid代码。\n*   **闭环验证机制：** 作者设计了一个“生成-验证-修正”的闭环。系统生成代码后，必须在Fluid环境中实际运行，检查输出字符串是否与原文完全匹配。如果不匹配，利用错误信息反馈给LLM进行重试。\n*   **人的角色：** 作者保留最终决定权。只有当作者在网页上交互验证（悬停查看数据）无误后，才会确认替换原文。这确保了科学严谨性。\n\n### 5. 评估视角：从“准确率”到“泛化性与鲁棒性”\n**验证逻辑的深化：**\n在评估方法时，作者不仅关注LLM能否“猜对”代码，更关注这种方法的**鲁棒性**。\n*   **思考：** 如果LLM只是死记硬背了数据，那么当数据发生变化时，生成的代码就会失效。\n*   **反事实测试：** 作者引入了反事实测试用例，故意修改底层数据，观察生成的代码是否能正确反映新的数据状态（例如，数据变了，文本是否自动从“增长”变为“下降”）。\n*   **意义：** 这证明了生成的代码不仅仅是字符串匹配，而是真正捕捉到了文本背后的**语义逻辑**。\n\n### 总结：思想演进脉络\n1.  **发现问题：** 学术文本是静态的，缺乏数据溯源，难以验证。\n2.  **提出愿景：** 让文本像图表一样，成为数据的动态视图（透明文档）。\n3.  **技术选型：** 利用LLM解决“写代码难”的问题，利用Fluid解决“溯源交互”的问题。\n4.  **流程设计：** 采用人机协同的闭环生成，平衡自动化效率与科学准确性。\n5.  **价值验证：** 通过反事实测试，确保系统真正理解了语言与数据的逻辑关系，而非简单的文本替换。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 36,
            "papers": [
                {
                    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
                    "arxiv_id": "2601.06021",
                    "authors": "Jiajie Zhang, Xin Lv, Ling Feng, Lei Hou, Juanzi Li",
                    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究基于LLM的深度搜索智能体，提出了一种强化学习框架来训练智能体进行证据链构建和推理，属于单智能体的工具使用与自我演化范畴，且不涉及被排除的纯应用或纯推理内容。",
                    "summary2": "本文旨在解决深度搜索智能体在强化学习中因依赖二元结果奖励而导致的捷径利用和幻觉问题。针对多跳问答场景，我们提出了一种Citation-aware Rubric Rewards (CaRR) 框架及C-GRPO算法，通过细粒度的引用感知规则奖励来评估推理的全面性和事实性。在BrowseComp、GAIA等多个深度搜索基准上，通过准确率验证了该方法能有效提升智能体的鲁棒性和推理质量。",
                    "summary_translation": "强化学习 (Reinforcement Learning, RL) 已成为提升基于大语言模型 (Large Language Model, LLM) 的深度搜索智能体性能的关键技术。然而，现有方法主要依赖二元结果奖励，这无法捕捉智能体推理过程的全面性和事实性，且往往导致捷径利用和幻觉等不良行为。为解决这些局限性，我们提出了 **引文感知评分标准奖励**，这是一种针对深度搜索智能体的细粒度奖励框架，强调推理的全面性、事实依据以及证据的连接性。CaRR 将复杂问题分解为可验证的单跳评分标准，并要求智能体通过显式识别隐藏实体、利用正确引文予以支持，以及构建连接至预测答案的完整证据链来满足这些标准。我们进一步引入了 **引文感知组相对策略优化**，该方法结合了 CaRR 和结果奖励，用于训练稳健的深度搜索智能体。实验结果表明，在多个深度搜索基准测试中，C-GRPO 始终优于标准的基于结果的 RL 基线模型。我们的分析还验证了 C-GRPO 能够有效抑制捷径利用，促进全面且基于证据的推理，并在开放式深度研究任务中表现出强大的泛化能力。我们的代码和数据可在 https://github.com/THUDM/CaRR 获取。",
                    "inspiration_trace": "基于论文《Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards》，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 1. 宏观背景与痛点观察\n**逻辑起点：** 深度搜索代理的兴起与强化学习（RL）的引入。\n*   **现状观察：** 当前主流的深度搜索代理训练主要依赖RL，且普遍使用**二元结果奖励**——即只看最终答案是否与Ground Truth匹配。\n*   **问题发现：** 作者敏锐地观察到这种“唯结果论”的奖励机制存在严重缺陷。在长链路、多跳的复杂推理任务中，代理可以通过“走捷径”（只关注最后几跳，忽略题目中的其他约束）或“幸运的幻觉”（猜对答案但推理过程错误）来获得高分。\n*   **核心矛盾：** 我们的目标是训练一个**鲁棒**、**诚实**且**全面**的推理代理，但现有的奖励信号却在鼓励“投机取巧”的行为。这种错位导致了模型在面对长上下文或更复杂任务时泛化能力差。\n\n### 2. 核心假设与切入点\n**思维转折：** 从关注“答案对不对”转向关注“过程好不好”。\n*   **灵感来源：** 作者注意到现有的合成多跳QA数据集具有天然的**结构化特征**。一个复杂问题可以被拆解为多个中间步骤，每个步骤都包含特定的“隐藏实体”。\n*   **核心假设：** 如果我们将这些中间步骤视为必须通过的“检查点”，那么一个完美的推理轨迹应该满足所有检查点，而不仅仅是终点。\n*   **切入点定义：** 引入**细粒度奖励**来评估推理过程。这个奖励必须包含三个维度：\n    1.  **全面性：** 是否覆盖了所有必要的中间实体？\n    2.  **事实性：** 每个结论是否有引用来源支持？\n    3.  **连通性：** 这些证据是否逻辑相连，最终指向答案？\n\n### 3. 方法论构建：从“结果”到“证据链”\n**逻辑展开：** 如何将上述假设转化为可执行的评估框架？\n*   **第一步：分解。**\n    *   既然问题是由多跳构成的，那么在训练前，先利用LLM将复杂问题拆解为一系列原子化的**单跳规则**。每个规则对应一个需要被发现的隐藏实体。\n*   **第二步：验证。**\n    *   仅仅找到实体还不够，代理必须证明它。作者引入了**引用感知**机制。\n    *   **实体识别：** 检查代理的最终回答中是否明确指出了这些隐藏实体。\n    *   **引用校验：** 检查这些实体的描述是否有对应的网页内容支持，防止幻觉。\n*   **第三步：连通。**\n    *   为了防止代理堆砌无关的正确事实，作者引入了**证据连通性检查**。通过构建图结构，确保被满足的规则能够通过实体关系最终连接到答案节点，形成一条完整的证据链。\n*   **产出：** 形成了**Citation-aware Rubric Rewards (CaRR)** 框架，将原本模糊的“推理质量”量化为“被满足规则的比率”。\n\n### 4. 算法落地：混合奖励机制\n**逻辑闭环：** 如何将新的评估框架融入现有的RL训练流程？\n*   **权衡思考：** 如果完全抛弃结果奖励，只看过程奖励，可能会导致模型陷入“为了找证据而找证据”的误区，偏离“回答问题”的最终目标。\n*   **策略设计：** 提出了**Citation-aware Group Relative Policy Optimization (C-GRPO)**。\n    *   **混合策略：** 保留结果奖励作为基础（保证答案正确），但在答案正确的前提下，叠加过程奖励（鼓励推理更好）。\n    *   **加权机制：** 仅对那些答案正确的轨迹给予额外的过程奖励加权。这样既锁定了正确方向，又激励了更优的路径。\n*   **优化目标：** 引导模型从“只要对就行”进化到“既要对，又要证据确凿、逻辑严密”。\n\n### 5. 验证与反思\n**逻辑验证：** 这种方法真的有效吗？\n*   **实验预期：** 作者预期C-GRPO训练出的模型在长上下文（128k）下表现更好，因为它学会了彻底验证而非走捷径。\n*   **结果分析：** 实验数据证实了这一点。相比标准GRPO，C-GRPO在长上下文下性能提升显著，且在开放性研究任务中展现了更强的泛化能力。\n*   **最终结论：** 通过引入细粒度的、基于引用的规则奖励，成功解决了深度搜索代理中的“捷径利用”和“幻觉”问题，实现了从“投机性智能”向“鲁棒性智能”的转变。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“发现问题 -> 提出假设 -> 构建框架 -> 算法融合 -> 实验验证”**的学术逻辑链条。其核心创新在于**利用合成数据的结构化特征，将不可见的“推理过程”转化为可见的、可验证的“证据链”**，从而解决了RL训练中奖励信号稀疏且误导的痛点。"
                },
                {
                    "title": "Distilling Feedback into Memory-as-a-Tool",
                    "arxiv_id": "2601.05960",
                    "authors": "Víctor Gallego",
                    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了包含“基于文件的记忆系统”和“智能体控制的工具调用”的框架，涉及单智能体的记忆机制、工具使用以及通过反馈进行自我完善，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决推理时自修正计算成本高且无法持久化的问题。针对基于rubric的反馈学习场景，我们提出了一种Memory-as-a-Tool框架，利用文件系统工具调用将瞬时反馈转化为可检索的抽象指南。在Rubric Feedback Bench数据集上，通过Judge scores和成本分析验证了其有效性。实验表明该方法能快速匹配Self-Critique性能并大幅降低推理成本。",
                    "summary_translation": "我们提出了一种框架，通过基于文件的内存系统和代理控制的工具调用，将瞬态批评转化为可检索的指南，从而摊销 inference-time reasoning (推理时推理) 的成本。我们在 Rubric Feedback Bench (基于量规的反馈基准) 上对该方法进行了评估，这是一个用于基于量规学习的新颖数据集。实验表明，我们的增强型 LLMs 能够迅速达到 test-time refinement pipelines (测试时细化流水线) 的性能水平，同时大幅降低推理成本。",
                    "inspiration_trace": "基于论文《Distilling Feedback into Memory-as-a-Tool》，以下是对作者核心方法论形成逻辑链的系统性推演：\n\n### 1. 宏观观察：System 2 的繁荣与代价\n**起点：** 作者首先关注到了当前大模型（LLM）领域的一个核心趋势——“System 2”扩展（如思维链、自我修正、搜索）。\n**现象：** 通过在推理时增加计算量，模型能够显著超越零样本表现，展现出强大的逻辑和生成能力。\n**痛点：** 这种高性能的代价极其昂贵。每次推理都需要重新进行“思考”过程，且这种思考是“片段式”的——一旦上下文窗口关闭，模型就会“忘记”刚才的推理过程。面对新任务时，它必须从头开始推导相同的结论，造成了巨大的计算冗余。\n\n### 2. 核心矛盾：持久性与灵活性的两难\n**深入分析：** 作者对比了现有的两种解决方案，发现了中间的空白地带：\n*   **推理时修正：** 灵活性高，能适应特定任务，但计算成本高，且无法持久化知识。\n*   **微调：** 能持久化知识，推理成本低，但训练成本高，且缺乏快速适应新用户自定义规则（如特定写作风格）的灵活性。\n**问题聚焦：** 如何既保留推理时修正的**灵活适应性**，又能像微调一样实现**低成本的持久化**？\n\n### 3. 核心假设：将“反馈”转化为“记忆”以摊销成本\n**逻辑跃迁：** 作者意识到，在自我修正循环中，模型生成的“批评”或“反馈”本质上是一个高价值的学习信号。\n*   **传统视角：** 反馈仅用于修正当前的输出，用完即弃。\n*   **作者视角：** 反馈应当被蒸馏并存储。\n**假设提出：** 如果能将这种短暂的“批评”转化为持久的、可检索的“指南”，那么在未来的任务中，模型就可以直接调用这些指南，而无需重复昂贵的自我修正循环。这就是**“摊销推理成本”**的核心思想。\n\n### 4. 机制设计：从“被动存储”到“主动工具”\n**实现挑战：** 如何存储这些知识？传统的向量数据库（RAG）通常存储原始数据，缺乏抽象能力，且检索过程是被动的。\n**设计思路：** 作者提出了一种**“记忆即工具”**的范式，强调记忆的主动性和语义性：\n*   **抽象化：** 记忆不应是原始的对话日志，而应是“经验教训”。模型需要将具体的反馈（如“第2段缺乏通感语言”）抽象为通用的原则（如“优先使用通感修辞”）。\n*   **工具化交互：** 不使用黑盒的向量检索，而是将文件系统作为工具。模型必须通过 `ls`（列举）、`read`（读取）、`write`（写入）等工具调用来管理记忆。\n    *   *逻辑：* 这迫使模型在写入时进行**语义命名**（为了以后能找到），在读取时进行**主动推理**（判断哪个文件相关）。这模拟了人类整理笔记的过程。\n\n### 5. 验证与闭环：构建基准测试\n**验证需求：** 为了证明这种方法不仅省钱，还能保持高性能，作者需要一个能测试“从反馈中学习”的环境。\n**方案：** 构建了“Rubric Feedback Bench”。\n*   **逻辑：** 该基准包含复杂的、多维度的评分标准，迫使模型必须通过反馈学习特定的风格或规则（如“混乱写作风格”或“义务论伦理框架”）。\n*   **预期结果：** 实验应证明，经过几轮反馈后，使用记忆的模型在后续任务中能直接生成高质量答案，其性能接近每次都做自我修正的模型，但成本大幅降低。\n\n### 总结：思想演进脉络\n作者从**“System 2 推理的高冗余”**这一宏观问题出发，通过**“摊销计算成本”**的经济学视角，提出了**“将反馈蒸馏为持久记忆”**的解决方案。为了实现这一点，作者摒弃了传统的被动检索，转而采用**基于文件系统的主动工具调用**，强迫模型进行知识的抽象与结构化，最终在**性能与成本**之间找到了帕累托最优的平衡点。"
                },
                {
                    "title": "Can We Predict Before Executing Machine Learning Agents?",
                    "arxiv_id": "2601.05930",
                    "authors": "Jingsheng Zheng, Jintian Zhang, Yujie Luo, Yuren Mao, Yunjun Gao, Lun Du, Huajun Chen, Ningyu Zhang",
                    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了名为 \"FOREAGENT\" 的智能体，旨在解决自主机器学习智能体中的“执行瓶颈”问题。该研究通过引入“预测-验证”循环来优化智能体的工作流，属于单智能体机制（规划与执行优化）的研究范畴。",
                    "summary2": "本文旨在解决自主ML智能体面临的“执行瓶颈”，将数小时的物理执行压缩为秒级的逻辑推理。针对机器学习任务中的算法解决方案选择场景，我们提出了一种基于“隐式世界模型”的预测框架，利用“Verified Data Analysis Report”使LLM在不执行代码的情况下预测方案优劣，并构建了FORE AGENT采用“Predict-then-Verify”循环。在包含18,438对比较的自建语料库及MLE-bench上，通过Pairwise Accuracy、Beat Ratio和Speedup等指标验证了其有效性，实现了6倍加速及6%的性能提升。",
                    "summary_translation": "自主机器学习智能体彻底变革了科学发现，但它们仍受限于 Generate-Execute-Feedback paradigm（生成-执行-反馈范式）。先前的方法面临严重的 Execution Bottleneck（执行瓶颈），因为假设评估严格依赖于昂贵的物理执行。为了绕过这些物理约束，我们借鉴 World Models（世界模型）的思想，内化 execution priors（执行先验），用即时预测推理替代昂贵的运行时检查。在这项工作中，我们形式化定义了 Data-centric Solution Preference（以数据为中心的解偏好）任务，并构建了一个包含 18,438 个成对比较的综合语料库。我们证明，当以 Verified Data Analysis Report（验证过的数据分析报告）为提示时，LLMs（大语言模型）表现出显著的预测能力，达到了 61.5% 的准确率和鲁棒的置信度校准。最后，我们在 FOREAGENT 中实例化了该框架，这是一个采用 Predict-then-Verify loop（预测-验证循环）的智能体，实现了 6 倍的收敛加速，同时性能超过基于执行的基线 6%。我们的代码和数据集将很快在 https://github.com/zjunlp/predict-before-execute 公开。",
                    "inspiration_trace": "基于论文《Can We Predict Before Executing Machine Learning Agents?》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n---\n\n### 第一阶段：宏观观察与痛点识别\n**——从“试错法”的效率困境出发**\n\n1.  **观察现状**：\n    *   当前的自主机器学习智能体（如 AIDE, AutoMind）在解决科学发现等复杂任务时，普遍遵循“生成-执行-反馈”的范式。\n    *   这种范式本质上是**暴力试错**：智能体生成代码 -> 在物理环境（如GPU集群）中运行数小时 -> 获得结果 -> 迭代。\n\n2.  **锁定核心瓶颈**：\n    *   **执行瓶颈**：物理执行极其昂贵且缓慢（例如在 MLE-Bench 上单次运行需9小时）。\n    *   **探索受限**：由于时间成本高昂，智能体无法广泛探索多样化的解决方案，只能进行有限的线性尝试。\n\n3.  **提出根本性问题**：\n    *   人类专家在编写代码前，会先在脑海中“模拟”算法的适用性，剔除明显不合适的方案。\n    *   **核心追问**：我们能否将“数小时的物理执行”压缩为“数秒的逻辑推理”？即，智能体能否在运行代码之前，就预测出哪个方案更好？\n\n---\n\n### 第二阶段：概念迁移与假设提出\n**——引入“世界模型”思想**\n\n1.  **跨域灵感**：\n    *   借鉴强化学习中的**世界模型**概念：智能体通过内部模拟环境动力学来预测行动结果，而非依赖外部真实交互。\n\n2.  **形成核心假设**：\n    *   大语言模型（LLM）是否可以充当机器学习任务中的**隐式世界模型**？\n    *   **假设**：LLM 不需要实际运行代码，仅通过“阅读”任务描述、数据特征和代码逻辑，就能通过推理预测出两个解决方案的相对优劣。\n\n---\n\n### 第三阶段：关键挑战与认知鸿沟\n**——发现“数据理解”的缺失**\n\n1.  **识别障碍**：\n    *   要预测代码好坏，光看代码逻辑是不够的，必须看**数据**（Data-centric）。\n    *   例如：一个复杂的深度学习模型在小样本数据上会过拟合，而简单的树模型可能表现更好。这种判断依赖于对数据分布的理解。\n\n2.  **直面 LLM 的局限性**：\n    *   LLM 存在**数值盲区**：直接将成千上万行的原始数据或统计日志喂给 LLM，它们无法有效处理，且容易产生幻觉。\n    *   **鸿沟**：原始数据的“数值空间”与 LLM 擅长的“语义空间”之间存在断层。\n\n---\n\n### 第四阶段：方法论构建与语义桥接\n**——从“数值”到“语义”的转化**\n\n1.  **任务形式化**：\n    *   将问题定义为**以数据为中心的解决方案偏好**任务。\n    *   不要求预测具体的准确率数值（太难），而是进行**成对比较**：给定方案 A 和方案 B，判断谁更好。\n\n2.  **核心创新：验证式数据分析报告**：\n    *   为了解决 LLM 看不懂数据的痛点，作者提出了一种**“语义化”策略**。\n    *   **逻辑**：不直接给数据，而是让 LLM 生成一段脚本来分析数据，然后将分析结果（统计特征）转化为**自然语言描述**。\n    *   **转化示例**：将“数据集包含5000条样本，类别分布极度不均”转化为一段关于“小样本与类别不平衡风险”的语义叙述。\n    *   **作用**：这相当于给 LLM 提供了一个“数据说明书”，使其能够基于数据特性进行逻辑推理，而非仅仅依赖代码复杂度（如“模型越大越好”的偏见）。\n\n3.  **构建验证基准**：\n    *   收集真实智能体的执行轨迹，构建包含 18,438 对比较的大规模数据集，用于验证上述假设。\n\n---\n\n### 第五阶段：系统验证与机制洞察\n**——证明“推理”替代“执行”的可行性**\n\n1.  **实验验证**：\n    *   实验证明，DeepSeek-V3.2 等推理能力强的模型在阅读了“数据报告”后，预测准确率达到 61.5%，显著优于随机猜测和基于代码复杂度的启发式规则。\n    *   **结论**：LLM 确实具备隐式世界模型的能力，能够通过语义理解捕捉算法与数据的匹配度。\n\n2.  **机制分析**：\n    *   发现**语义报告**是关键：仅提供代码或原始数字效果不佳，只有转化为语义叙述，LLM 的推理能力才被激活。\n    *   发现**置信度校准**：模型对自己判断的信心与实际准确率高度相关，这意味着可以用它来做“过滤器”。\n\n---\n\n### 第六阶段：最终应用与范式革新\n**——从“预测”到“智能体加速”**\n\n1.  **闭环整合**：\n    *   既然预测有效，就将其嵌入到智能体的工作流中。\n    *   提出 **FORE AGENT** 框架，将传统的“生成-执行-反馈”改造为**“预测-验证”循环**。\n\n2.  **逻辑流程**：\n    *   **并行生成**：一次性生成多个候选方案（不执行）。\n    *   **预测筛选**：利用上述的“隐式世界模型”在几秒钟内对所有方案进行推理打分，剔除低置信度的方案。\n    *   **物理验证**：仅对筛选出的 Top-k 方案进行昂贵的物理执行。\n\n3.  **最终收益**：\n    *   **解耦探索与执行**：用低成本的推理（秒级）替代高成本的执行（小时级）。\n    *   **结果**：实现了 6 倍的收敛加速，并在相同时间内探索了更广的搜索空间，最终性能提升了 +6%。\n\n---\n\n### 总结：逻辑演进全貌\n\n1.  **痛点**：物理执行太慢，限制了智能体的探索效率。\n2.  **灵感**：用 LLM 做“世界模型”，以推理代替执行。\n3.  **障碍**：LLM 读不懂原始数据，无法判断算法与数据的适配性。\n4.  **突破**：将数据统计特征转化为**语义报告**，激活 LLM 的逻辑推理能力。\n5.  **验证**：证明了 LLM 能基于语义报告准确预测方案优劣。\n6.  **落地**：构建“预测-验证”循环，用推理做过滤器，大幅提升智能体效率。"
                },
                {
                    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
                    "arxiv_id": "2601.05808",
                    "authors": "Xiaoshuai Song, Haofei Chang, Guanting Dong, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen",
                    "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于构建用于LLM智能体的工具交互环境，旨在通过程序化合成生成多样化的环境来训练智能体，提升其在复杂场景下的多轮、多工具交互能力，属于单智能体中的“工具使用”研究范畴。",
                    "summary2": "本文旨在解决LLM智能体训练中缺乏可扩展、高质量工具交互环境的问题。针对受限的真实系统和不可靠的模拟环境场景，我们提出了EnvScaler框架，通过SkelBuilder构建环境骨架，利用ScenGenerator生成任务场景及基于规则的验证函数。在Qwen3系列模型上应用SFT和RL训练后，于BFCL-v3 Multi-Turn、Tau-Bench和ACEBench-Agent基准上，通过Overall Score等指标验证了其显著提升模型解决复杂多轮多工具交互任务能力的有效性。",
                    "summary_translation": "人们期望将大语言模型 (LLMs) 训练为在各种现实世界环境中运作的智能体，但这一过程依赖于丰富多样的工具交互沙箱。然而，获取真实系统的权限通常受限；由 LLM 模拟的环境容易出现幻觉和不一致性问题；而手动构建的沙箱难以扩展规模。在本文中，我们提出了 EnvScaler，这是一个利用程序合成技术实现可扩展工具交互环境的自动化框架。EnvScaler 由两个组件组成。首先，SkelBuilder 通过主题挖掘、逻辑建模和质量评估构建多样化的环境骨架。随后，ScenGenerator 为每个环境生成多个任务场景以及基于规则的轨迹验证函数。借助 EnvScaler，我们合成了 191 个环境和约 7K 个场景，并将其应用于 Qwen3 系列模型的监督微调 (SFT) 和强化学习 (RL)。三个基准测试的结果表明，EnvScaler 显著提升了 LLM 在涉及多轮、多工具交互的复杂环境中解决任务的能力。我们在 https://github.com/RUC-NLPIR/EnvScaler 发布了我们的代码和数据。",
                    "inspiration_trace": "基于论文《EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体解决方案产出的思考过程。\n\n---\n\n### 1. 宏观观察：智能体的“数据饥渴”与环境瓶颈\n**思考起点：**\n作者首先观察到LLM智能体的发展趋势——从单纯的对话者转向能够执行复杂任务的行动者（如电商后台操作、航班改签等）。\n**核心矛盾：**\n要训练这样的智能体，必须让其在大量的、多样化的环境中进行交互学习（无论是模仿学习SFT还是强化学习RL）。\n**现实困境：**\n*   **真实环境：** 访问受限，隐私风险高，无法大规模获取。\n*   **模拟环境：** 现有的LLM模拟环境容易产生幻觉，状态不一致（即“前脚说有这个文件，后脚找不到了”）。\n*   **人工构建：** 虽然稳定，但成本极高，无法扩展。\n\n**初步结论：** 现有的环境供给方式无法满足智能体训练对“大规模、高质量、多样化”环境的需求。\n\n---\n\n### 2. 深入分析：寻找“一致性”与“可扩展性”的平衡点\n**对比分析：**\n作者对比了三种环境类型（Table 1），发现“程序化构建”是唯一能同时满足“可扩展、一致、可控、稳定”的路径。\n**现有方案的缺陷：**\n*   现有的程序化工作大多依赖于**先验知识**（如已有的API文档、已有的轨迹数据）。\n*   这意味着它们只能“复现”或“重组”已知的世界，无法创造全新的、未见过的环境，限制了智能体的泛化能力。\n\n**关键假设：** 如果我们能自动化地**从零合成**可执行的环境代码，而不是依赖现有的API或轨迹，就能打破数据来源的限制，实现真正的环境扩展。\n\n---\n\n### 3. 范式转移：从“LLM作为模拟器”到“LLM作为程序员”\n**逻辑跃迁：**\n既然LLM直接模拟环境状态容易产生幻觉（不可控），那么不如利用LLM强大的代码生成能力，让它编写**环境的逻辑代码**。\n**核心洞察：**\n*   **代码即规则：** Python代码是确定性的，执行逻辑是严谨的，这天然解决了LLM模拟时的“状态不一致”问题。\n*   **LLM作为架构师：** 让LLM去设计环境的状态空间、工具接口和业务逻辑，而不是直接模拟每一次交互的反馈。\n\n**方法论雏形：** 构建一个自动化流水线，输入是文本描述，输出是可执行的Python环境类。\n\n---\n\n### 4. 质量控制：解决“代码生成不可靠”的挑战\n**新问题：**\n虽然代码比文本模拟更严谨，但LLM生成的代码可能包含逻辑错误或Bug。如果环境本身是错的，智能体就会学到错误的策略。\n**解决方案构思：**\n需要一个自动化的“测试-验收”机制。\n**双智能体闭环：**\n*   **测试智能体：** 扮演“黑盒测试者”，随机或针对性地调用工具，试图找出环境漏洞（如输入非法参数、调用不存在的ID）。\n*   **检查智能体：** 扮演“代码审查员”，检查源代码、执行结果和状态变化，判断是否符合预期逻辑。\n\n**逻辑演进：** 通过这种对抗性的闭环测试，只有通过率高的环境才会被保留，从而保证了合成环境的质量。\n\n---\n\n### 5. 场景构建：从“空壳”到“实战”\n**进一步思考：**\n仅有环境代码（骨架）是不够的，智能体需要具体的任务和数据来训练。\n**数据生成的逻辑：**\n*   **状态先行：** 任务必须依赖于环境的具体状态。例如，不能“取消一个不存在的订单”。因此，必须先生成环境的初始状态数据。\n*   **任务反推：** 基于生成的初始状态和可用工具，设计出具有挑战性且可解的任务。\n*   **评估革新：** 传统的评估往往依赖与标准轨迹的匹配（死板）。作者提出基于**最终状态**的规则验证。只要最终环境状态符合规则（如订单状态变为“已取消”），无论中间用了什么工具，都算成功。这更符合真实世界的多解性。\n\n---\n\n### 6. 最终方法论形成：EnvScaler\n**逻辑闭环：**\n将上述思考整合为一个完整的自动化框架：\n1.  **SkelBuilder（骨架构建）：**\n    *   *挖掘：* 从现有任务中反推环境主题（解决“灵感来源”）。\n    *   *建模：* LLM编写环境代码（解决“一致性”）。\n    *   *评估：* 双智能体测试（解决“质量”）。\n2.  **ScenGenerator（场景生成）：**\n    *   *生成：* 生成初始状态和任务（解决“训练数据”）。\n    *   *验证：* 生成基于状态的检查函数（解决“评估灵活性”）。\n\n**总结：**\n作者的思考路径是从**“缺乏训练环境”**这一痛点出发，通过**“程序化合成”**解决一致性问题，通过**“双智能体测试”**解决代码质量问题，最后通过**“状态驱动”**的任务生成解决训练数据的实用性，最终形成了一套无需依赖真实系统即可无限扩展高质量训练环境的自动化方案。"
                },
                {
                    "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat",
                    "arxiv_id": "2601.05657",
                    "authors": "Hao Yang, Hongyuan Lu, Dingkang Yang, Wenliang Yang, Peng Sun, Xiaochuan Zhang, Jun Xiao, Kefan He, Wai Lam, Yang Liu, Xinhua Zeng",
                    "summary": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了一个具有分步决策能力的对话智能体，能够主动决定发送消息还是等待，并模拟思考时间，涉及单智能体的决策机制以及双智能体系统的交互，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有逐步式AI聊天系统缺乏主动等待机制及消息节奏不自然的问题。针对即时通讯社交聊天场景，我们提出了一种名为Stephanie2的逐步决策对话智能体，引入主动等待和消息节奏适应机制，将延迟建模为思考时间与打字时间之和。我们在基于Persona-Chat生成的伪对话数据上，通过自然度、参与度等指标及角色识别测试验证了其有效性。",
                    "summary_translation": "即时通讯中的人类社交聊天通常通过一系列短消息序列进行。现有的 step-by-step AI chatting systems（逐步式 AI 聊天系统）通常将 one-shot generation（一次性生成）拆分为多条消息并顺序发送，但它们缺乏 active waiting mechanism（主动等待机制），且表现出不自然的 message pacing（消息节奏）。为了解决这些问题，我们提出了 Stephanie2，一种新颖的下一代 step-wise decision-making dialogue agent（逐步决策对话代理）。通过 active waiting（主动等待）和 message-pace adaptation（消息节奏适应），Stephanie2 在每一步明确决定是发送还是等待，并将 latency（延迟）建模为 thinking time（思考时间）和 typing time（打字时间）的总和，从而实现更自然的节奏。我们进一步引入了一种 time-window-based dual-agent dialogue system（基于时间窗口的双代理对话系统），用于生成 pseudo dialogue histories（伪对话历史）以进行人工和自动评估。实验结果表明，Stephanie2 在 naturalness（自然度）和 engagement（参与度）等指标上明显优于 Stephanie1，并且在 role identification Turing test（角色识别图灵测试）的人工评估中获得了更高的通过率。",
                    "inspiration_trace": "基于论文《Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat》，以下是对作者核心方法产出逻辑链的系统推演：\n\n### 1. 宏观观察：从“单步生成”到“分步交互”的范式错位\n*   **现象**：现有的主流LLM对话系统遵循“单步范式”，即用户输入一句，AI回复一大段长文本。\n*   **现实**：人类的即时通讯（IM）社交是“分步范式”的——人们倾向于将一个想法拆解为多条短消息发送，并根据对方的反应实时调整措辞或话题。\n*   **初步结论**：为了模拟真实社交体验，AI必须从“生成一段长文本”转变为“生成一系列连续的短消息”。（这是前作Stephanie1的基础，也是本文的起点）。\n\n### 2. 问题诊断：Stephanie1的机械性与“失聪”\n作者在肯定前作Stephanie1（通过分隔符生成多段消息）的基础上，敏锐地发现了其依然存在的两个核心缺陷，这构成了本文的突破口：\n\n*   **缺陷一：缺乏“主动等待”机制**\n    *   *观察*：Stephanie1虽然把消息切短了，但它倾向于一股脑地把所有切好的消息发出去。\n    *   *后果*：当用户正在连续表达（如倾诉情绪、补充细节）时，AI往往会因为急于输出而打断用户，破坏了对话的自然流和情感连贯性。\n    *   *本质*：AI不懂“倾听”，它只知道“输出”。\n\n*   **缺陷二：消息节奏的建模过于简化**\n    *   *观察*：现有的分步系统通常仅根据消息长度（模拟打字速度）来计算发送延迟。\n    *   *后果*：短消息回得太快（显得轻率、像机器），长消息回得太慢（破坏对话流）。\n    *   *本质*：忽略了人类交流中的“思考时间”。在真实对话中，停顿往往代表思考，而不仅仅是打字耗时。\n\n### 3. 核心假设：对话即“决策”而非单纯的“生成”\n基于上述诊断，作者的思想发生了质的飞跃：**对话不应被视为文本生成的任务，而应被视为一系列微观决策的序列。**\n\n*   **假设**：一个拟人化的AI在每一步都应该面临一个二元选择：**“现在发送”** 还是 **“继续等待”**。\n*   **推论**：为了做出正确的选择，AI必须具备“认知”能力，即显式地思考当前的语境（对方说完了吗？我表达完整了吗？）。\n*   **延展**：既然引入了“思考”，那么“思考”本身应当消耗时间。因此，**延迟 = 思考时间 + 打字时间**，这样才能还原真实的对话节奏。\n\n### 4. 方法构建：Stephanie2的“思考-决策”闭环\n为了验证上述假设，作者构建了Stephanie2系统，其逻辑演进如下：\n\n*   **第一步：显式思维链**\n    *   强制模型在输出内容前，先输出一段 `"
                },
                {
                    "title": "GIFT: Games as Informal Training for Generalizable LLMs",
                    "arxiv_id": "2601.05633",
                    "authors": "Nuoyan Lyu, Bingbing Xu, Weihao Meng, Yige Yuan, Yang Zhang, Zhiyong Huang, Tat-Seng Chua, Huawei Shen",
                    "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "论文利用游戏（如矩阵博弈、井字棋、谁是卧底）作为环境，通过强化学习（GRPO）训练LLM的战略创造力和社会推理能力，涉及多智能体博弈与通过反馈自我完善，符合“多智能体：博弈”及“自我演化”的研究范围。",
                    "summary2": "本文旨在解决LLMs缺乏实践智慧及多任务训练中的性能退化问题。针对数学和游戏环境，我们提出了一种Nested Training Framework，将游戏作为非正式学习环境，通过顺序组合子任务将优化目标从“OR”转变为“AND”。我们在Qwen2.5模型上，通过MATH500、MMLU、CommonGen等基准验证了其有效性，显著提升了模型的泛化能力。",
                    "summary_translation": "虽然 Large Language Models (LLMs，大型语言模型) 在数学和代码生成等 formal learning tasks (形式化学习任务) 中取得了显著成就，但在人类认知所特有的“practical wisdom” (实践智慧) 和 generalizable intelligence (可泛化智能) —— 例如 strategic creativity (战略创造力) 和 social reasoning (社会推理) —— 方面仍面临挑战。这种差距源于缺乏 informal learning (非正式学习)，后者依赖于 interactive feedback (交互式反馈) 而非 goal-oriented instruction (目标导向型指令)。本文提出将 Games (游戏) 作为 LLM informal learning (非正式学习) 的主要环境，利用其 intrinsic reward signals (内在奖励信号) 和 abstracted complexity (抽象复杂性) 来培养多样化的 competencies (能力)。为解决 multi-task learning (多任务学习) 中观察到的 performance degradation (性能下降) 问题，我们引入了一种 Nested Training Framework (嵌套训练框架)。与优化隐式“OR”目标的 naive task mixing (朴素任务混合) 不同，我们的框架采用 sequential task composition (顺序任务组合) 来强制执行显式“AND”目标，迫使模型同时掌握多种能力以获得最大奖励。通过在 Matrix Games (矩阵博弈)、TicTacToe (井字棋) 和 Who's the Spy (谁是卧底) 游戏中使用基于 GRPO 的 reinforcement learning (强化学习)，我们证明了整合 game-based informal learning (基于游戏的非正式学习) 不仅能够防止 task interference (任务干扰)，还能显著增强模型在广泛的 ability-oriented benchmarks (能力导向型基准) 上的 generalization (泛化能力)。该框架和实现代码已公开可用。",
                    "inspiration_trace": "基于论文《GIFT: Games as Informal Training for Generalizable LLMs》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体方法论的思考过程：\n\n### 1. 宏观观察：LLM 的“偏科”现象\n**思考起点：** 现有的 LLM 在数学、代码等“正式学习”任务上表现卓越，但在策略创造力、社会推理等体现“实践智慧”的通用智能上仍显不足。\n**核心洞察：** 人类智能源于“正式学习”与“非正式学习”的互补。正式学习侧重结构化知识，而非正式学习侧重在交互中通过反馈获取隐性知识。LLM 的短板在于缺乏后者——即缺乏在非结构化、交互式环境中通过试错来积累经验的能力。\n\n### 2. 假设提出：寻找 LLM 的“非正式学习”环境\n**问题转化：** 既然 LLM 缺乏非正式学习，那么什么环境适合作为 LLM 的非正式学习场所？\n**假设形成：** **游戏** 是最佳载体。\n**逻辑支撑：**\n*   **交互性：** 游戏通过内在规则和奖励信号提供反馈，无需人工标注数据。\n*   **抽象性：** 游戏是现实世界复杂交互的高度抽象（如博弈、规划、社交）。\n*   **多样性：** 不同类型的游戏可以对应不同的认知能力（如矩阵游戏对应抽象推理，多回合游戏对应规划，多人游戏对应心智理论）。\n**初步方案：** 将数学任务作为“正式学习”环境，将多种游戏作为“非正式学习”环境，结合训练。\n\n### 3. 问题识别：朴素多任务学习的“陷阱”\n**尝试与失败：** 作者尝试将数学与游戏任务进行简单的混合训练。\n**观察到的现象：** 这种“朴素混合”导致了性能退化，模型往往顾此失彼。\n**深度归因：**\n*   **优化视角：** 朴素混合实际上是在优化一个隐式的 **“OR” 目标**。只要模型在任意一个子任务上表现好，总奖励就会增加。\n*   **后果：** 模型会倾向于“偷懒”，专注于优化容易获得高奖励的单一任务，而忽略其他任务。这导致梯度信号被主导任务垄断，其他任务无法得到有效学习，最终损害了泛化能力。\n\n### 4. 方法创新：从“OR”到“AND”的逻辑重构\n**核心突破：** 如何强迫模型必须同时掌握所有能力，而不是只掌握其中之一？\n**概念转换：** 将优化目标从隐式的 **“OR”** 转换为显式的 **“AND”**。\n**具体方案：** 提出 **嵌套训练框架**。\n*   **机制：** 不再随机混合任务，而是将多个子任务按顺序串联成一个复合任务。\n*   **约束：** 模型只有在连续完成所有子任务（如：先解出数学题，再赢得游戏）时，才能获得最大奖励。\n*   **效果：** 这种结构迫使模型在一个轨迹中必须同时调用多种能力。部分成功无法满足目标，从而保持了更高的探索熵和梯度的稳定性，避免了单一任务的梯度主导。\n\n### 5. 逻辑闭环：通用智能的涌现\n**最终验证：** 通过这种“正式+非正式”结合且强制“AND”逻辑的嵌套训练，模型不仅在游戏和数学任务上表现良好，更重要的是在 MMLU、SocialIQA 等通用能力基准上取得了显著提升。\n**结论：** 游戏作为非正式学习环境是有效的，而嵌套训练框架解决了多任务干扰问题，二者结合成功赋予了 LLM 更强的泛化智能。"
                },
                {
                    "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation",
                    "arxiv_id": "2601.05548",
                    "authors": "Jeonghyun Kang, Hongjin Kim, Harksoo Kim",
                    "summary": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.",
                    "category": "cs.CL",
                    "filter_reason": "论文专注于长期对话系统中的记忆更新机制，提出了动态生成整合记忆的方法。记忆是LLM智能体的核心组件之一（属于单智能体研究范围中的“记忆”），该研究旨在提升智能体在长期交互中跟踪用户状态和情感语境的能力，符合筛选条件。",
                    "summary2": "本文旨在解决长期对话中记忆更新导致的信息冲突及缺乏情感理解的问题。针对多会话对话场景，我们提出了一种基于生成且融合情感与因果关系的KEEM数据集，通过动态生成整合性记忆来替代传统的操作式更新。我们在KEEM数据集上通过人工评估、关键词召回率及多种长期对话模型的Perplexity指标验证了其有效性。",
                    "summary_translation": "本文介绍了 Keep Emotional and Essential Memory (KEEM) 数据集，这是一个新颖的 generation-based dataset (基于生成的数据集)，旨在增强 long-term conversational systems (长期对话系统) 中的 memory updates (记忆更新)。与现有的依赖简单累积或 operation-based methods (基于操作的方法) 不同——这些方法往往导致 information conflicts (信息冲突) 并难以准确 tracking a user's current state (跟踪用户当前状态)——KEEM 能够动态生成 integrative memories (整合记忆)。这一过程不仅保留了 essential factual information (基本事实信息)，还融入了 emotional context (情感语境) 和 causal relationships (因果关系)，从而实现了对 user interactions (用户交互) 更 nuanced understanding (细致入微的理解)。通过利用 emotional and essential data (情感与基本数据) 无缝更新系统记忆，我们的方法促进了 deeper empathy (深层共情)，并增强了系统在 open-domain conversations (开放域对话) 中做出 meaningful response (有意义回应) 的能力。",
                    "inspiration_trace": "基于对论文《Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation》的深入分析，以下是作者产出该文章核心思想的逻辑演进过程推演：\n\n### 第一阶段：宏观观察与问题定位\n**（从“长期对话”的需求到“记忆管理”的瓶颈）**\n\n1.  **观察现象**：随着开放域对话系统的发展，多轮、跨会话的长期对话成为趋势。在真实的人类交互中，用户的状态（如健康状况、位置、情绪）是随时间动态变化的。\n2.  **识别核心矛盾**：现有的长期对话系统大多关注如何生成回复，却忽视了**记忆管理**。系统往往只是简单地累积信息，导致记忆库臃肿且充满过时信息，无法准确反映用户的“当前状态”。\n3.  **初步聚焦**：作者意识到，要实现真正像人类一样的长期对话，关键不在于“记住更多”，而在于“如何有效地更新记忆”。\n\n### 第二阶段：对现有范式的批判性分析\n**（从“简单操作”到“信息丢失”的反思）**\n\n1.  **批判“累积法”**：传统的做法是将新会话的摘要直接追加到旧记忆中。\n    *   *逻辑漏洞*：这会导致信息冲突（例如：记忆中既有“我在欧洲”，又有“我回韩国了”），且无法区分历史事实与当前状态。\n2.  **批判“操作法”**：以 CareCallmem 为代表的方法引入了 PASS, APPEND, DELETE, REPLACE 四种操作来处理新旧记忆的关系。\n    *   *逻辑漏洞*：作者发现这种非黑即白的操作会导致**关键信息的语义丢失**。\n        *   *案例反思*：当用户从“我在欧洲”变为“我回韩国了”，REPLACE 操作会删除“我在欧洲”这一历史事实；当用户从“感冒”变为“痊愈”，DELETE 操作会让系统彻底忘记用户曾生过病。\n3.  **形成假设**：记忆更新不应是简单的“选择”或“删除”，而应是**信息的融合与重构**。我们需要一种能保留“历史本质”同时反映“当前状态”的更新机制。\n\n### 第三阶段：概念跃迁——从“操作式”到“生成式”\n**（提出“生成式更新”的核心思想）**\n\n1.  **思想转变**：作者提出放弃基于标签的操作（如 REPLACE），转而采用**生成式**的方法。\n2.  **逻辑推演**：面对新旧记忆冲突，系统应具备理解能力，生成一个新的句子来整合两者。\n    *   *例子*：旧记忆“我在欧洲” + 新信息“我回韩国了” -> 生成新记忆“我之前去了欧洲，现在已经回韩国了”。\n3.  **确立核心优势**：这种方法既能消除冲突，又能保留用户经历的时间线完整性，从而更准确地追踪用户状态。\n\n### 第四阶段：维度的深化——引入“情感与因果”\n**（从“事实记忆”到“共情记忆”的扩展）**\n\n1.  **发现新缺口**：在分析现有数据集（如 MSC, CareCallmem）时，作者发现记忆内容多局限于客观事实摘要，缺乏情感维度。\n2.  **逻辑推演**：人类对话中的共情不仅需要知道用户“是什么情绪”，更需要知道“为什么产生这种情绪”。\n    *   *假设*：如果记忆中只包含“我很伤心”，系统只能给予泛泛的安慰；如果记忆包含“因为工作失误而感到羞愧”，系统就能提供更有针对性的建议。\n3.  **整合目标**：理想的记忆更新必须同时包含**情感**及其**因果原因**，以支持深度的认知共情。\n\n### 第五阶段：方法论构建与验证\n**（利用 LLM 构建 KEEM 数据集以验证假设）**\n\n1.  **数据策略**：由于缺乏现成的、包含情感因果且支持生成式更新的数据集，作者决定利用 ChatGPT-4 对现有的 KMSC 数据集进行重构。\n2.  **实施逻辑**：\n    *   **步骤一（情感注入）**：指令 LLM 从对话中提取情感及其原因，重写摘要，解决“情感缺失”问题。\n    *   **步骤二（生成式更新）**：指令 LLM 对比旧记忆与新摘要，生成整合后的新记忆，解决“操作式丢失”问题。\n3.  **闭环验证**：通过人工评估和模型下游任务测试（如 Perplexity、冲突率分析），证明这种“生成式+情感反思”的记忆更新方法，在信息保留量、冲突减少率和对话质量上均优于传统的累积法和操作法。\n\n---\n\n**总结：**\n作者的思考路径是从**长期对话的动态性需求**出发，通过批判现有方法导致的信息冲突与丢失，提出了**生成式更新**的范式转变；进而为了实现更深层次的共情，引入了**情感与因果**维度，最终通过构建 KEEM 数据集将这一方法论落地并验证其有效性。"
                },
                {
                    "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems",
                    "arxiv_id": "2601.05520",
                    "authors": "Xuemei Tang, Chengxi Yan, Jinghang Gu, Chu-Ren Huang",
                    "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个名为CHisAgent的多智能体LLM框架，包含Inducer、Expander和Enricher三个具有特定角色的智能体，它们通过协作（自底向上、自顶向下、证据引导）来完成分类法构建任务。这完全符合“多智能体：协作”的研究范围，且核心贡献在于智能体框架本身而非单纯的历史领域应用。",
                    "summary2": "本文旨在解决LLMs在非英语历史文化推理中能力有限及手动构建分类法成本高昂的问题。针对中国古代文化系统，我们提出了一种名为CHisAgent的多智能体LLM框架，通过Inducer、Expander和Enricher三个阶段协同构建事件分类法。我们在二十四史数据集上，通过Path Granularity、CSC、Coverage Rate及Node Recall等指标验证了其有效性，并展示了其在跨文化对齐中的优越性。",
                    "summary_translation": "尽管在众多任务上表现出色，但大型语言模型在历史与文化推理方面的能力仍然有限，特别是在中国历史等非英语语境中。分类体系结构为组织历史知识和提升理解提供了有效机制。然而，人工构建分类体系成本高昂且难以扩展。因此，我们提出了 \\textbf{CHisAgent}，一个面向中国古代语境历史分类体系构建的多智能体 LLM 框架。CHisAgent 将分类体系构建分解为三个角色专门化的阶段：自下而上的 *Inducer*（归纳器），负责从原始历史语料库中推导出初始层次结构；自上而下的 *Expander*（扩展器），利用 LLM 的世界知识引入缺失的中间概念；以及证据引导的 *Enricher*（丰富器），通过整合外部结构化历史资源来确保内容的忠实性。基于《二十四史》，我们构建了一个大规模的、领域感知的事件分类体系，涵盖了中国古代的政治、军事、外交和社会生活。广泛的无参考和有参考评估表明，该方法在结构连贯性和覆盖范围上均有提升，进一步分析显示，生成的分类体系能够支持跨文化对齐。",
                    "inspiration_trace": "基于论文《CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到方法论构建的思考过程：\n\n---\n\n### 1. 宏观问题观察：LLM的文化“失语”与知识结构化需求\n**思考起点：**\n作者首先观察到一个核心矛盾：尽管大语言模型（LLMs）在通用任务上表现优异，但在**历史与文化推理**方面存在显著局限，特别是在**非英语语境**（如中国古代史）中。\n*   **现象：** LLMs往往只能捕捉文本表面的模式，无法深入理解深层的文化结构和历史逻辑。\n*   **推论：** 单纯的文本建模不足以支撑历史理解。历史知识需要显性的结构化组织，而**分类体系**正是组织这种知识、提升模型理解能力的有效机制。\n\n### 2. 现实瓶颈：人工构建的不可行性与现有方法的局限\n**问题聚焦：**\n既然分类体系如此重要，为何目前缺乏大规模、系统化的中国古代历史分类法？\n*   **障碍：** 传统的分类体系构建高度依赖专家，成本高昂且难以扩展。\n*   **现有技术缺陷：** 虽然已有利用LLM自动构建分类法的研究，但作者发现它们存在两极分化的问题：\n    *   **纯自底向上：** 仅依赖语料聚类，虽然忠实于文本，但往往缺乏抽象的中间概念，结构松散。\n    *   **纯自顶向下：** 依赖模型先验知识生成，虽然结构连贯，但容易产生幻觉，且对特定历史语料的覆盖率不足。\n\n### 3. 核心假设：混合策略与多智能体协作\n**逻辑转折：**\n为了解决“忠实度”与“结构完整性”之间的矛盾，作者提出了一种**辩证的构建思路**：\n*   **假设：** 一个完美的历史分类体系应当同时具备**数据驱动的颗粒度**（来自原始文献）和**知识驱动的逻辑性**（来自专家/模型先验），并最终通过**外部证据**进行校验。\n*   **方法论选择：** 单一模型难以同时胜任这些相互冲突的任务。因此，必须采用**多智能体框架**，将复杂的构建任务分解为不同角色的协作流程。\n\n### 4. 方法论构建：三阶段逻辑闭环\n基于上述假设，作者设计了一个包含三个专门化阶段的演进逻辑，形成了CHisAgent框架：\n\n#### 第一阶段：归纳者—— 数据驱动的“基石”\n*   **思考：** 必须先从最原始的史料（《二十四史》）中挖掘真实存在的实体。\n*   **逻辑：** 采用**自底向上**策略。从海量历史文本中提取事件实例，通过聚类形成初步的层级。\n*   **目的：** 确保分类体系扎根于真实的历史语料，解决“空对空”的问题。\n\n#### 第二阶段：扩展者—— 知识驱动的“骨架”\n*   **思考：** 仅靠数据挖掘的体系往往存在“断层”，缺乏人类专家眼中的中间抽象概念（如从“战争”直接跳到“具体战役”，缺失了“战术”或“战略”等中间层）。\n*   **逻辑：** 引入**自顶向下**策略。利用LLM的世界知识和专家角色，识别并填补缺失的中间节点，修正层级结构。\n*   **目的：** 提升分类体系的结构连贯性和逻辑完整性。\n\n#### 第三阶段：丰富者—— 证据导向的“校验”\n*   **思考：** 扩展阶段虽然补全了结构，但可能引入了不符合历史事实的节点，或者遗漏了语料中隐含的重要事件。\n*   **逻辑：** 引入外部结构化知识（如CBDB人物数据库）和主题模型作为“证据源”。将高频事件、潜在主题和外部关系映射回分类树中。\n*   **目的：** 确保最终结果的**历史忠实度**和覆盖广度。\n\n### 5. 总结：从“单点突破”到“系统演进”\n**最终产出：**\n作者的思考过程并非简单的技术堆砌，而是针对历史领域特殊性（古汉语、文化特异性、时间跨度大）的定制化演进：\n1.  **发现问题：** LLM不懂历史深层逻辑。\n2.  **寻找抓手：** 用分类体系结构化知识。\n3.  **克服困难：** 人工太慢，单一AI方法太偏（要么太散，要么太假）。\n4.  **提出方案：** 用多智能体模拟人类专家的工作流——先**归纳**事实，再**演绎**逻辑，最后**考证**证据。\n\n这一逻辑链体现了作者将**数据挖掘、知识推理与事实校验**有机结合的系统性思维。"
                },
                {
                    "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse",
                    "arxiv_id": "2601.05505",
                    "authors": "Yubo Hou, Zhisheng Chen, Tao Wan, Zengchang Qin",
                    "summary": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了FlashMem框架，旨在解决LLM智能体在长期自主任务中缺乏动态上下文保存机制的问题，属于单智能体研究中的“记忆”范畴。虽然涉及推理延迟优化，但其核心在于通过计算重用提取内在记忆以增强智能体的持久认知能力，而非单纯的基础设施部署优化。",
                    "summary2": "本文旨在解决LLM无状态架构导致的历史信息冗余处理及现有潜在记忆方法的架构分离问题。针对长时程自主代理任务，我们提出了一种FlashMem框架，利用Shared-KV Consolidator直接复用主干网络的冻结缓存提取记忆，并通过基于注意熵的Cognitive Monitor自适应触发记忆整合。我们在GSM8K、MATH等六个基准数据集上，通过准确率、ROUGE-1及推理延迟验证了其有效性，结果显示其在匹配重型基线性能的同时，将推理延迟降低了5倍。",
                    "summary_translation": "大型语言模型 的 stateless architecture (无状态架构) 本质上缺乏保存动态上下文的机制，迫使智能体 冗余地重新处理历史记录以维持 long-horizon autonomy (长期自主性)。尽管 latent memory (潜在记忆) 提供了一种解决方案，但当前方法受限于 architectural segregation (架构分离)，依赖于将记忆与 reasoning backbone (推理骨干网络) 解耦的 auxiliary encoders (辅助编码器)。我们提出了 FlashMem，这是一个通过 computation reuse (计算复用) 直接从 transient reasoning states (瞬时推理状态) 中蒸馏 intrinsic memory (内在记忆) 的框架。利用 internal representations (内部表示) 唯一编码 input trajectories (输入轨迹) 的特性，FlashMem 将 last hidden state (最后一个隐藏状态) 识别为交互历史的 sufficient statistic (充分统计量)。这使得 Shared-KV Consolidator (共享键值整合器) 能够通过直接关注 backbone's frozen cache (骨干网络的冻结缓存) 来合成记忆，从而消除 redundant re-parameterization (冗余的重新参数化)。此外，一个无参数的 Cognitive Monitor (认知监视器) 利用 attention entropy (注意力熵)，仅在检测到高 epistemic uncertainty (认知不确定性) 时自适应地触发 consolidation (整合)。实验表明，FlashMem 在将 inference latency (推理延迟) 降低 5 倍的同时，达到了 heavy baselines (重型基线) 的性能，有效地弥合了效率与 persistent cognition (持久认知) 之间的差距。",
                    "inspiration_trace": "基于论文《FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观困境与现状反思\n**——从“无状态”架构的局限出发**\n\n1.  **观察现象**：现有的LLM本质上是“无状态”的，它们将输入映射到输出，但在交互之间不保留持久的内部状态。\n2.  **面临挑战**：对于需要长期自主性的智能体，这种无状态性导致了一个严重的瓶颈——为了维持上下文连贯性，智能体必须在每一步推理中**冗余地重新处理历史信息**。\n3.  **现有方案的局限**：虽然“潜在记忆”被提出作为解决方案（将上下文压缩为密集向量），但作者发现现有方法存在根本性的**结构低效**。它们通常采用“分离式架构”，即依赖独立的编码器或适配器来生成记忆，这与主推理骨干是割裂的。\n\n### 第二阶段：痛点诊断与核心假设\n**——识别“计算冗余”与“架构隔离”的根源**\n\n1.  **深入分析**：为什么现有的潜在记忆方案效率低下？作者意识到，这是因为它们引入了**辅助参数**来重新编码历史。\n2.  **逻辑推演**：\n    *   LLM在推理过程中已经计算过一次历史信息，这些信息蕴含在内部的KV Cache（键值缓存）和隐藏状态中。\n    *   现有方法却丢弃这些现成的计算结果，转而使用另一个独立的模块从头开始处理原始文本。这不仅是存储上的浪费，更是**计算上的重复**。\n3.  **提出核心假设**：**“内在性”假设**。LLM的内部表示（特别是最后一层的隐藏状态）已经唯一且充分地编码了输入轨迹。因此，我们不需要外部编码器，可以直接从模型现有的推理状态中“蒸馏”出记忆。\n\n### 第三阶段：范式转移——从“分离”到“内在”\n**——确立“计算复用”的设计哲学**\n\n1.  **思维转变**：从“如何设计一个更好的外部记忆编码器”转变为“如何直接复用骨干网络的计算成果”。\n2.  **理论支撑**：利用LLM表示的**单射性**，即输入轨迹与内部表示是一一对应的。这意味着**最后一个隐藏状态是交互历史的充分统计量**。\n3.  **方法论雏形**：提出**计算复用**的概念。记忆生成过程不应是一个独立的编码Pass，而应是一个直接读取骨干网络冻结KV Cache的“读取”操作。\n\n### 第四阶段：机制细化——何时记忆与如何记忆\n**——解决动态触发与轻量化实现的矛盾**\n\n1.  **子问题一：何时生成记忆？（动态触发）**\n    *   **思考**：并非每一步推理都需要记忆，频繁生成会带来巨大开销。我们需要一个“认知监控器”。\n    *   **洞察**：模型的不确定性与注意力机制的熵高度相关。当模型困惑时，注意力分布趋于分散（高熵）。\n    *   **方案**：设计一个**无参数的认知监控器**，基于注意力熵来实时检测模型的“认知困惑”。只有当熵超过阈值（即模型不确定时）才触发记忆固化，避免在简单问题上浪费算力。\n\n2.  **子问题二：如何高效生成记忆？（轻量化读取）**\n    *   **思考**：既然要复用KV Cache，那么记忆生成模块就不应该有庞大的参数。\n    *   **方案**：设计**共享KV整合器**。\n        *   **输入**：直接使用骨干网络当前的隐藏状态作为初始Query。\n        *   **操作**：通过交叉注意力机制，直接对骨干网络的冻结KV Cache进行查询。\n        *   **去重**：摒弃传统的Key/Value投影矩阵，只保留Query的投影，实现极低的参数开销。\n\n### 第五阶段：逻辑闭环与系统成型\n**——FlashMem框架的最终确立**\n\n1.  **整合逻辑**：\n    *   **感知层**：利用注意力熵监控模型的不确定性，决定“何时”介入。\n    *   **提取层**：利用Shared-KV Consolidator，直接从骨干网络的现有状态中提取信息，解决“如何”高效提取。\n    *   **反馈层**：生成的潜在记忆向量被软注入回骨干网络的输入流，作为高密度的认知线索。\n2.  **最终愿景**：FlashMem不再是一个外挂的辅助系统，而是一个与骨干网络深度耦合的**内在记忆机制**。它消除了架构隔离，通过复用计算资源，在保持高性能推理的同时，实现了极低的推理延迟（5倍提升）。\n\n---\n\n**总结**：作者的思考路径是从**“无状态架构的缺陷”**出发，通过批判**“现有分离式架构的冗余”**，提出了**“内在记忆与计算复用”**的核心假设，并最终通过**“熵触发机制”**和**“共享KV设计”**将这一假设落地为一个高效、轻量的智能体记忆框架。"
                },
                {
                    "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards",
                    "arxiv_id": "2601.05488",
                    "authors": "Zhiyu Shen, Ziming Wu, Fuming Lai, Shaobing Lian, Yanghui Rao",
                    "summary": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的核心组件——长期记忆构建。它提出利用强化学习框架来训练模型构建多维记忆，属于单智能体研究中的“记忆”范畴，且涉及通过反馈进行自我完善，符合筛选标准。",
                    "summary2": "本文旨在解决LLM在长期对话中保持一致性的挑战。针对长期对话场景，我们提出MemBuilder强化学习框架，利用Attributed Dense Rewards Policy Optimization (ADRPO) 优化多维记忆构建。我们在LoCoMo、LongMemEval和PerLTQA数据集上通过QA准确率验证了其有效性，使4B参数模型超越了SOTA闭源基线。",
                    "summary_translation": "在长期对话中保持一致性仍是 LLMs (Large Language Models，大语言模型) 面临的一项基本挑战，因为标准的 retrieval mechanisms (检索机制) 往往无法捕捉 historical states (历史状态) 的 temporal evolution (时间演变)。尽管 memory-augmented frameworks (记忆增强框架) 提供了一种结构化的替代方案，但现有系统要么依赖于对 closed-source models (闭源模型) 的 static prompting (静态提示)，要么受困于 sparse rewards (稀疏奖励) 导致的低效 training paradigms (训练范式)。我们提出了 MemBuilder，这是一个 reinforcement learning (强化学习) 框架，旨在训练模型利用 attributed dense rewards (归因密集奖励) 来 orchestrate (编排) multi-dimensional memory construction (多维记忆构建)。MemBuilder 解决了两个关键挑战：(1) Sparse Trajectory-Level Rewards (稀疏轨迹级奖励)：我们采用 synthetic session-level question generation (合成会话级问题生成) 来在 extended trajectories (扩展轨迹) 中提供 dense intermediate rewards (密集中间奖励)；(2) Multi-Dimensional Memory Attribution (多维记忆归因)：我们引入了 contribution-aware gradient weighting (贡献感知梯度加权)，根据每个组件的 downstream impact (下游影响) 来缩放 policy updates (策略更新)。实验结果表明，MemBuilder 能够使一个 4B-parameter model (40亿参数模型) 的性能超越 state-of-the-art (SOTA，最先进的) closed-source baselines (闭源基线)，并在 long-term dialogue benchmarks (长期对话基准) 上展现出强大的 generalization (泛化) 能力。",
                    "inspiration_trace": "基于论文《MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards》，以下是对作者核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察：现有记忆机制的“盲点”\n**起点：** 长期对话的一致性是LLM的核心痛点。\n**观察：**\n*   **RAG的局限：** 传统的检索增强生成（RAG）将信息视为静态、独立的切片，无法捕捉信息的“时间演化”（例如用户喜好的改变）。\n*   **Prompting框架的局限：** 现有的记忆增强框架（如MemGPT, Mem0）主要依赖静态提示词和昂贵的闭源模型。它们处于“开环”状态——只管写入记忆，却不知道这些记忆是否真的对下游任务有帮助。\n\n### 2. 问题聚焦：从“开环”到“闭环”的挑战\n**假设：** 能否训练一个轻量级模型，通过直接监督来构建记忆，从而替代昂贵的闭源模型？\n**现状分析：** 虽然已有尝试（如Memory-R1, Mem-α）使用强化学习（RL）来训练记忆构建，但效果不佳。作者诊断出两个核心瓶颈：\n*   **瓶颈一：奖励过于稀疏。** 在跨越数十个会话的长期对话中，仅在轨迹末端给出一个奖励，模型无法分辨是哪一个会话的记忆操作导致了最终的成功或失败，导致梯度更新噪声极大。\n*   **瓶颈二：归因过于粗糙。** 记忆通常是多维度的（如核心记忆、情景记忆）。现有方法对所有维度的记忆操作共享一个全局奖励，无法区分是哪一类记忆对回答问题做出了实际贡献。\n\n### 3. 核心思路：将“模糊反馈”转化为“精准信号”\n为了解决上述瓶颈，作者提出了**“归因化密集奖励”**的思路，逻辑演进如下：\n\n#### 3.1 解决稀疏性：从“终点奖励”到“过程奖励”\n**思考：** 既然无法在真实对话中获得每一步的反馈，能否“模拟”反馈？\n**方案：** 引入**合成会话级问答**。\n*   在每个会话结束后，利用专家模型基于当前会话和历史记忆生成一组QA对。\n*   立即用这些QA对测试当前构建的记忆质量。\n*   **逻辑：** 这样就将原本在对话结束才给出的稀疏奖励，变成了每个会话都能获得的密集奖励，极大加快了学习收敛速度。\n\n#### 3.2 解决归因性：从“全局平均”到“贡献加权”\n**思考：** 既然不同类型的记忆（如情景记忆 vs 语义记忆）在回答问题时的作用不同，奖励信号也应有所区分。\n**方案：** 引入**贡献感知的梯度加权**。\n*   在计算奖励时，记录回答问题过程中检索到了哪一类记忆。\n*   如果某类记忆被频繁检索并用于正确回答，那么该类记忆对应的操作策略应获得更强的梯度更新。\n*   **逻辑：** 这解决了“多任务共享奖励”时的信用分配难题，让模型学会优先优化那些真正有用的记忆维度。\n\n### 4. 方法论落地：ADRPO框架\n基于上述思考，作者构建了完整的训练流程：\n\n1.  **架构设计：** 采用多维记忆架构（Core, Episodic, Semantic, Procedural），为精细化的归因提供物理基础。\n2.  **冷启动（SFT）：** 利用专家模型（Claude）收集轨迹进行监督微调，先教会模型“怎么写”（格式正确），解决RL探索初期的无效动作问题。\n3.  **强化学习（ADRPO）：**\n    *   利用**合成QA**提供密集的会话级奖励。\n    *   利用**检索计数**对梯度进行加权，实现归因化更新。\n    *   最终目标：让模型学会构建能够最大化下游QA效用的记忆。\n\n### 5. 总结：逻辑链全景\n**发现问题**（现有记忆系统昂贵且盲目） $\\rightarrow$ **提出假设**（用RL训练小模型构建记忆） $\\rightarrow$ **诊断失败**（现有RL奖励太稀疏、归因太粗糙） $\\rightarrow$ **提出解法**（合成QA实现密集奖励 + 检索统计实现归因加权） $\\rightarrow$ **验证效果**（小模型超越大模型Prompting方案）。\n\n这就是作者从观察到方法论的完整思考路径。"
                },
                {
                    "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models",
                    "arxiv_id": "2601.05366",
                    "authors": "Zheng Luo, T Pranav Kutralingam, Ogochukwu N Okoani, Wanpeng Xu, Hua Wei, Xiyang Hu",
                    "summary": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.",
                    "category": "cs.CL",
                    "filter_reason": "该论文研究了大语言模型作为智能体调用外部工具的能力，属于单智能体研究中的“工具使用”范畴。虽然涉及多语言鲁棒性，但其核心是评估和改进智能体的工具调用能力，而非纯应用或纯推理。",
                    "summary2": "本文旨在解决大型语言模型在多语言场景下工具调用的鲁棒性问题。针对多语言用户查询与英语执行接口的冲突，我们提出了MLCL诊断基准，通过控制查询语言组成和语义扰动来隔离执行级错误。我们在MLCL数据集上，通过细粒度错误分类法验证了参数值语言不匹配是主要失败模式，并评估了推理时缓解策略的有效性。",
                    "summary_translation": "大语言模型正日益被部署为智能体，通过结构化函数调用（structured function calls）来调用外部工具。尽管近期的研究报告显示，在以英语为中心的标准评估中，模型表现出了强大的工具调用（tool-calling）能力，但在多语言用户交互场景下，工具调用的鲁棒性（robustness）仍有待探索。在本研究中，我们引入了 MLCL（一个诊断基准），并对中文、印地语以及低资源语言伊博语（Igbo）的多语言工具调用进行了系统性评估。通过细粒度的错误分析，我们发现即便模型正确理解了意图并选择了工具，仍会出现许多失败情况。我们将参数值语言不匹配（parameter value language mismatch）确定为主要的一种失败模式，即模型虽然生成了语义上恰当的参数值，但这些值使用的是用户的语言，从而违反了语言不变的执行约定（language-invariant execution conventions）。我们进一步评估了几种推理时系统策略（inference-time system strategies），结果发现，尽管这些策略显著减少了由语言引起的执行错误，但没有任何一种策略能够完全恢复到英语水平的性能。",
                    "inspiration_trace": "基于论文《Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models》的内容，以下是对作者产出该文章核心方法论的逻辑链推演：\n\n### 1. 宏观背景与问题切入\n**逻辑起点：** LLM正在从单纯的对话系统演变为能够调用外部工具的智能体。\n**观察现状：** 现有的工具调用评估主要在英语环境下进行，且表现优异。然而，现实世界的用户是多语言的，而底层的工具接口（API）通常是语言无关的（通常是英语）。\n**提出问题：** 当用户使用非英语与LLM交互时，LLM的工具调用能力是否依然稳健？这种跨语言的交互是否会引入新的、未被现有基准测试发现的失败模式？\n\n### 2. 现象观察与初步假设\n**深入观察：** 作者通过初步测试发现，当用户输入中文、印地语或伊博语时，模型经常生成“语义正确但无法执行”的工具调用。\n**典型案例：** 用户问“纽约的天气”，模型理解了意图，选择了正确的函数，但在参数`location`中填入了“纽约市”（中文）而非“New York City”。\n**形成假设：** 这种失败并非源于模型“没听懂”（语义理解失败），而是源于模型“说错了话”（执行层面的语言不匹配）。模型倾向于模仿用户的语言来生成参数值，从而违反了底层代码要求英语参数的硬性约束。\n\n### 3. 核心概念定义\n**概念提炼：** 作者将这种失败模式定义为**“参数值语言不匹配”**。\n**逻辑推演：** 如果假设成立，那么只要解决了语言格式问题，模型的工具调用表现应该就能恢复。这意味着，多语言工具调用的瓶颈可能不在于模型的跨语言推理能力，而在于自然语言与程序化接口之间的对齐问题。\n\n### 4. 诊断性基准的设计\n为了验证上述假设，作者需要剥离干扰变量，设计一个受控的实验环境（即MLCL基准）。\n\n*   **变量控制：** 保持工具接口（函数名、参数定义）严格为英语，仅改变用户查询的语言。\n*   **维度一：语言构成**\n    *   *设计思路：* 为了区分“理解能力”和“格式习惯”，作者引入了“部分翻译”。\n    *   *逻辑：* 如果在部分翻译（保留关键参数为英文）的情况下，模型表现恢复，就证明模型理解了非英语指令，只是在全翻译环境下习惯性地复制了用户的语言。\n*   **维度二：语义扰动**\n    *   *设计思路：* 引入改写和同义词替换。\n    *   *逻辑：* 测试模型对表面形式变化的鲁棒性，观察在严格匹配要求下，语义噪声是否会加剧执行错误。\n*   **语言选择：** 选取中文（高资源）、印地语（中资源）、伊博语（低资源），以探究不同语系和资源条件下的表现差异。\n\n### 5. 实验验证与归因分析\n**执行验证：** 在MLCL基准上测试多个主流模型。\n**结果分析：**\n*   **全翻译（FT）导致错误激增：** 证实了“参数值语言不匹配”是主导错误模式。\n*   **部分翻译（PAR）显著改善：** 证实了模型确实具备跨语言意图理解能力，失败主要发生在“语言-执行”边界。\n*   **低资源语言（如伊博语）的特殊性：** 发现模型较少直接复制低资源语言词汇（可能因为训练数据少），因此语言不匹配错误少，但语义理解错误多。这进一步细化了结论：高资源语言的失败主要是“接口规范”问题，低资源语言则包含“理解能力”问题。\n\n### 6. 解决方案探索与反思\n**尝试修复：** 既然问题是语言不匹配，能否通过简单的推理时策略解决？\n**策略测试：**\n*   **提示词干预：** 明确要求输出英语参数。\n*   **预翻译：** 先把用户问话翻译成英语再调用工具。\n*   **后翻译：** 生成参数后再翻译回英语。\n**发现局限：** 虽然这些策略能减少语言不匹配，但无法完全恢复到英语水平。原因在于翻译过程会引入“语义漂移”（如“Queen-size bed”被翻译成“King-size bed”），导致新的执行错误。\n\n### 7. 最终结论与贡献\n**逻辑闭环：** 作者得出结论，多语言工具调用的鲁棒性不仅仅是一个模型训练问题，更是一个**系统级设计挑战**。\n**核心产出：**\n1.  揭示了“参数值语言不匹配”这一被忽视的失败模式。\n2.  提出了MLCL这一诊断性基准，将语义理解错误与执行接口错误解耦。\n3.  指出现有的轻量级修复方案存在权衡，未来的Agent系统需要在自然语言交互与代码执行规范之间做更深层的对齐。"
                },
                {
                    "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization",
                    "arxiv_id": "2601.05475",
                    "authors": "Jiefu Ou, Sapana Chaudhary, Kaj Bostrom, Nathaniel Weir, Shuai Zhang, Huzefa Rangwala, George Karypis",
                    "summary": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于强化学习搜索框架的代码优化方法，核心在于LLM通过执行反馈进行迭代细化和自我完善。其中集成了自然语言批判模型进行自我反思，并利用推理时搜索算法进行规划，符合单智能体中关于工具使用、自我反思和自我演化的定义。",
                    "summary2": "本文旨在解决LLM在代码优化中面临的复杂性与性能指标解读难题。针对CUDA和C++代码优化场景，我们提出了一种MaxCode最大奖励强化学习框架，引入自然语言评论模型和最佳折扣奖励以增强观察空间，并在KernelBench和PIE基准上通过绝对加速比和相对加速排名验证了其有效性。",
                    "summary_translation": "大型语言模型在通用编码任务中展现出强大的能力，但在代码优化方面面临两个关键挑战：（i）编写优化代码（例如高性能 CUDA 内核和竞赛级 CPU 代码）的复杂性，需要具备系统、算法及特定语言的专业知识；（ii）除了二进制正确性之外，还需要对执行时间和设备利用率等性能指标进行解读。在本研究中，我们探索了推理时搜索算法，该算法引导 LLM 基于执行反馈通过迭代改进来发现更优的解决方案。我们的方法 MaxCode 将现有的搜索方法统一在最大奖励强化学习框架下，使得观测函数和动作价值函数模块化，便于进行修改。为了增强观测空间，我们集成了一个自然语言评论模型，将原始执行反馈转化为关于错误和性能瓶颈的诊断性洞察，以及迄今为止观测到的最佳折扣奖励。这些信息共同为代码提议函数提供了更丰富的输入。为了改善搜索过程中的探索能力，我们利用推演产生的动作值训练了一个生成性回报模型，以对潜在解决方案进行重排序。在 KernelBench (CUDA) 和 PIE (C++) 优化基准上的测试表明，与基线相比，MaxCode 提升了优化代码的性能，在绝对加速比和相对加速排名上分别实现了 20.3% 和 10.1% 的相对提升。",
                    "inspiration_trace": "基于论文《MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization》，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观视角：代码优化的特殊性\n**起点：** 作者首先观察到，虽然大语言模型（LLM）在通用代码生成上表现出色，但在**代码优化**（如编写高性能CUDA内核或竞赛级C++代码）这一特定任务上仍面临巨大挑战。\n**核心矛盾：**\n*   **高门槛：** 优化代码需要深厚的系统级知识（算法权衡、内存访问模式、硬件架构），这超出了通用LLM的常规训练分布。\n*   **模糊的反馈：** 与“代码能否运行”这种二元反馈不同，优化任务关注的是“运行时间”或“资源利用率”。原始的性能指标（如“慢了20%”）是非诊断性的，它无法告诉LLM*为什么*慢以及*如何*改进。\n\n### 2. 痛点分析：现有搜索方法的局限\n**观察：** 现有的基于推理时搜索的方法（如迭代优化、基于执行反馈的修正）虽然有效，但存在两个根本性的逻辑缺陷：\n*   **目标错位：** 传统的强化学习（RL）通常最大化“累积奖励”。但在代码优化中，我们只关心**最终找到的那个最好解**（Max Reward），中间过程的累积性能毫无意义。如果找到了一个极快的解，即使之前尝试了很多次失败的解，结果也是成功的。\n*   **信息利用率低：** 现有的搜索算法往往只将上一步的执行结果作为输入，缺乏对历史轨迹的深度利用，且难以理解复杂的性能指标。\n\n### 3. 理论重构：从“累积奖励”到“最大奖励”\n**假设：** 如果将代码优化过程重新定义为一个**最大奖励强化学习**问题，而不是标准RL，就能更准确地匹配优化任务的目标。\n**逻辑推演：**\n*   在标准RL中，Agent试图最大化长期回报的总和。\n*   在代码优化中，Agent应该最大化**轨迹中出现的最大奖励**。\n*   **推论：** 为了保持马尔可夫性质，状态空间必须显式包含一个辅助变量 $u$，代表**“迄今为止见到的最大折扣奖励”**。这样，LLM在生成新代码时，就能明确知道“目前的最好成绩是多少”，从而以此为基准进行超越。\n\n### 4. 信息增强：从“数值反馈”到“自然语言诊断”\n**问题：** 即使有了“最大奖励”的概念，LLM面对原始的执行反馈（如编译错误日志或具体的运行时间）仍然难以直接转化为有效的代码修改策略。\n**灵感：** 借鉴“自我反思”和“自然语言批评”的研究。\n**解决方案：** 引入一个**批评模型**。\n*   **作用：** 将原始的执行反馈（数字、错误码）转化为**自然语言的诊断洞察**（例如：“内存带宽是瓶颈”或“存在线程同步问题”）。\n*   **逻辑：** 这种“翻译”过程极大地丰富了观察空间，将冷冰冰的指标变成了LLM可以理解并据此采取行动的“建议”。\n\n### 5. 效率优化：引入价值预测模型\n**新挑战：** 代码优化的评估成本极高（需要编译、运行、测试）。在有限的计算预算下，无法无限制地探索所有可能的代码变体。\n**假设：** 如果能训练一个模型来预测某个搜索路径的“潜力”，就可以提前剪枝，避免在无希望的分支上浪费计算资源。\n**方法：** 训练一个**生成式价值/奖励预测模型**。\n*   **逻辑：** 该模型预测在当前状态下，未来能获得的最大奖励是多少。\n*   **应用：** 在搜索过程中，先生成多个候选代码，先用价值模型筛选出最有希望的几个，再送去执行环境评估。这实现了“以小博大”，提高了搜索效率。\n\n### 6. 最终框架：MaxCode 的统一\n**综合：** 作者将上述思考整合为一个统一的框架——MaxCode。\n*   **形式化：** 定义了包含初始代码、当前代码、执行反馈、自然语言批评以及历史最佳奖励的MDP。\n*   **算子化：** 提出了“最大奖励推理算子”，将现有的搜索方法（如Effi-Learner, CUDA-LLM）统一在这个框架下进行重写。\n*   **闭环：** 通过“批评”增强理解，通过“最大奖励”明确目标，通过“价值模型”提升效率。\n\n**总结：**\n作者的思考路径是从**任务的特殊性**（优化难、反馈模糊）出发，通过**理论视角的转换**（Max-Reward RL）重新定义目标，利用**自然语言作为中间媒介**解决理解难题，最后引入**学习型价值函数**解决计算成本问题，从而构建出一套完整的代码优化方法论。"
                },
                {
                    "title": "Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring",
                    "arxiv_id": "2601.05256",
                    "authors": "Eirini Baltzi, Tilemachos Moumouris, Athena Psalta, Vasileios Tsironis, Konstantinos Karantzalos",
                    "summary": "Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了 NAIAD 系统，明确描述了其作为“agentic AI assistant”的架构，涉及 LLM 推理、外部工具编排和智能体反思，符合单智能体中“工具使用”和“自我反思”的研究范围。尽管应用于内陆水监测领域，但其核心贡献在于智能体系统的设计与实现，而非单纯的应用效果展示。",
                    "summary2": "本文旨在解决内陆水域监测中现有方法缺乏集成且对非专家不友好的问题。针对内陆水域质量监测场景，我们提出了一种名为NAIAD的Agentic AI系统，利用LLMs和RAG技术，通过动态构建DAGs来编排外部工具。在希腊三个湖泊构建的数据集上，通过正确率和输出相关性验证了其有效性，结果显示Qwen2.5模型表现最佳。",
                    "summary_translation": "内陆水体监测对于保障公共健康和生态系统至关重要，能够实现及时干预以降低风险。现有方法通常分别解决孤立的子问题，如蓝藻、叶绿素或其他水质指标。NAIAD 提出了一种智能体 AI 助手，利用大语言模型和外部分析工具，基于对地观测数据为内陆水体监测提供整体解决方案。NAIAD 专为专家和非专家设计，提供了一个单指令界面，能够将自然语言查询转化为可执行的洞察。通过检索增强生成、大语言模型推理、外部工具编排、计算图执行和智能体反思，该系统从精选数据源中检索并综合知识，以生成定制化报告。该系统集成了多种工具，用于处理气象数据、Sentinel-2 影像、遥感指数计算（如 NDCI）、叶绿素a 估算，并集成了 CyFi 等成熟平台。性能评估使用了正确性和相关性指标，在涵盖多种用户专业水平的专用基准测试中，分别达到了 77% 和 85% 以上。初步结果显示了该系统在不同查询类型下具有很强的适应性和鲁棒性。针对大语言模型基座的消融实验进一步表明，Gemma 3 (27B) 和 Qwen 2.5 (14B) 在计算效率和推理性能之间提供了最佳平衡。",
                    "inspiration_trace": "基于论文《NAIAD: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring》的内容，以下是对作者构建该系统核心思想的逻辑链推演：\n\n### 1. 宏观观察与问题定义\n**逻辑起点：环境监测的紧迫性与技术落地的脱节**\n*   **观察**：内陆水体（湖泊、河流）对生态系统和公共健康至关重要，面临富营养化、藻华等威胁。\n*   **现状**：虽然已有大量技术手段（如卫星遥感、原位传感器、AI模型），但它们通常是**碎片化**的。\n*   **核心矛盾**：现有的AI解决方案多针对单一任务（如只测叶绿素、只测浊度）。要获得全面的水质评估，专家必须手动整合多个孤立工具的结果。\n*   **痛点**：对于非专家（环境从业者、决策者），技术门槛过高，缺乏一个能够将复杂EO（对地观测）数据转化为直观决策支持的统一入口。\n\n### 2. 假设提出与范式转移\n**逻辑推演：从“专用工具”到“通用智能助手”**\n*   **假设**：如果能够利用大语言模型（LLM）强大的推理与自然语言理解能力，是否可以构建一个“中间人”或“代理人”，自动理解用户意图并调度这些碎片化工具？\n*   **范式转移**：从传统的“用户手动操作特定软件”转向“用户自然语言提问，系统自动执行复杂工作流”。\n*   **目标定位**：构建一个**通用目的**的内陆水监测系统，而非单一功能的模型。\n\n### 3. 关键挑战的识别\n**逻辑深化：如何让LLM“靠谱”地执行科学任务？**\n*   **挑战一：领域知识缺失**。通用LLM不懂遥感指数（如NDCI）或特定水体的生化特性。\n    *   *应对思路*：引入**检索增强生成（RAG）**，将专业知识库注入LLM，确保其理解的专业性。\n*   **挑战二：工具调度的逻辑性**。水质分析往往涉及多步骤依赖（例如：必须先下载卫星图像 -> 计算光谱指数 -> 估算叶绿素 -> 结合气象数据 -> 生成报告）。简单的线性推理容易出错。\n    *   *应对思路*：需要一种结构化的方式来表示任务流程，确保步骤之间的依赖关系清晰。\n\n### 4. 核心创新点的诞生\n**逻辑突破：动态计算图（DAG）的引入**\n*   **灵感来源**：现有的Agent框架要么是多智能体协作（过于复杂），要么是简单的链式调用（缺乏灵活性）。\n*   **核心思想**：将用户的查询转化为一个**有向无环图（DAG）**。\n    *   **节点**：代表具体的工具（如Sentinel-2下载、NDCI计算、天气API）。\n    *   **边**：代表数据流向和依赖关系（例如，NDCI计算节点的输入必须来自图像下载节点的输出）。\n*   **优势**：LLM不再只是生成文本，而是充当“编译器”，根据查询动态“编译”出一张执行蓝图。这不仅保证了逻辑的正确性，还提供了可解释性和容错能力（如节点重试）。\n\n### 5. 方法论的系统化构建\n**逻辑整合：单智能体架构与反思机制**\n*   **架构决策**：选择**单智能体**架构而非多智能体。作者认为，对于内陆水监测这一特定垂直领域，一个具备强大工具编排能力的单智能体比多智能体系统更高效、更易于部署和维护。\n*   **流程闭环**：\n    1.  **理解**：用户输入自然语言 -> LLM重写查询。\n    2.  **规划**：利用RAG获取背景知识 -> LLM构建DAG（规划工作流）。\n    3.  **执行**：按DAG顺序调用外部工具（卫星、气象、模型）。\n    4.  **反思**：引入**Agentic Reflection**机制，系统自我审查输出结果，修正偏差，确保报告的相关性和准确性。\n*   **工具生态**：集成异构工具（Sentinel-2数据、气象API、CyFi预测平台），通过统一的元数据描述，使LLM能够灵活调用。\n\n### 6. 验证与迭代\n**逻辑验证：从模拟到现实的跨越**\n*   **评估策略**：不同于以往仅做模拟，作者强调在真实地理环境（希腊三个湖泊）上进行测试。\n*   **模型选择**：通过消融研究，在开源模型（如Qwen2.5, Gemma 3）中寻找“推理能力”与“计算成本”的最佳平衡点，证明了该架构不依赖于昂贵的闭源模型（如GPT-4），具有实际部署的可行性。\n\n---\n\n**总结：作者的思考路径**\n从**“监测手段碎片化”**的现实痛点出发，提出**“LLM作为统一调度中枢”**的假设。为了解决科学计算中复杂的**逻辑依赖问题**，创新性地引入**动态DAG构建机制**，并结合**RAG**与**反思机制**确保专业性与准确性，最终形成了一个**垂直领域、可落地、高鲁棒性**的智能体系统。"
                },
                {
                    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
                    "arxiv_id": "2601.07782",
                    "authors": "Wei Fang, James Glass",
                    "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体的**工具使用**和**规划**能力。它提出了TOOLQP框架，通过将指令分解为子任务并动态生成查询来改进工具检索，直接属于单智能体的研究范围。",
                    "summary2": "本文旨在解决大规模动态工具库中复杂请求检索困难的问题。针对用户意图与工具文档间的语义鸿沟及组合性挑战，我们提出了一种TOOL QP框架，将检索建模为迭代查询规划过程，通过任务分解和动态查询生成与检索器交互。在ToolRet基准测试上，通过nDCG@K和Completeness@K等指标验证了其有效性，显著提升了检索精度和下游执行成功率。",
                    "summary_translation": "运行于大规模、动态工具库之上的 LLM agents（大语言模型智能体）依赖于有效检索，然而标准的 single-shot dense retrievers（单次密集检索器）在应对复杂请求时往往力不从心。这些检索失败主要归因于抽象用户目标与技术文档之间的脱节，以及固定大小 embeddings（嵌入向量）在建模组合式工具组合方面的能力局限。为应对上述挑战，我们提出了 TOOLQP，这是一个将检索过程建模为 iterative query planning（迭代式查询规划）的轻量级框架。不同于单次匹配，TOOLQP 将指令分解为若干子任务，并动态生成查询与检索器进行交互；通过针对组合所需的具体子任务，该方法有效地弥合了语义鸿沟。我们利用合成查询轨迹对 TOOLQP 进行训练，随后通过 Reinforcement Learning with Verifiable Rewards (RLVR，可验证奖励强化学习) 进行优化。实验结果表明，TOOLQP 实现了 state-of-the-art（最先进）的性能，展现出卓越的 zero-shot generalization（零样本泛化）能力、在不同检索器间的鲁棒性，以及在 downstream agentic execution（下游智能体执行）方面的显著提升。",
                    "inspiration_trace": "基于论文《Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning》，以下是对作者产出核心方法 **TOOL QP** 的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM智能体”到“工具检索的必要性”）**\n\n1.  **观察趋势**：随着大语言模型（LLM）向智能体演进，解决复杂任务（如数学、编程、推理）越来越依赖于外部工具（API、数据库）。\n2.  **现实瓶颈**：工具库的规模正在从几十个手工挑选的函数，爆炸式增长到数万个动态API。\n3.  **核心冲突**：由于上下文窗口的限制，无法将所有工具的文档和说明一次性塞入LLM。因此，**工具检索**成为了连接用户意图与海量工具库的必经之路。\n\n### 第二阶段：现有方案的深度诊断\n**（为什么传统的“单次检索”会失效？）**\n\n作者首先审视了当前主流的解决方案——即直接套用标准信息检索（IR）技术，使用密集嵌入进行单次语义匹配。通过分析，作者发现了三个根本性的结构性缺陷：\n\n1.  **语义鸿沟**：\n    *   *现象*：用户的表达通常是抽象的、高层的（如“让这段录音音质变好”），而工具文档是技术的、底层的（如“IIR滤波器参数”）。\n    *   *诊断*：单次嵌入试图在一个向量空间内强行对齐这两种完全不同的语言体系，往往导致匹配失败。\n\n2.  **组合性瓶颈**：\n    *   *现象*：现实任务是组合性的，往往需要同时调用多个不同的工具（如“分析降雨如何影响零售销量”需要天气API+股票数据库）。\n    *   *诊断*：单次查询生成的固定维度向量，本质上是一个“词袋”，缺乏表达“多个离散工具组合”的容量。它无法编码工具之间的逻辑关系和组合多样性。\n\n3.  **缺乏交互性**：\n    *   *现象*：工具之间存在依赖关系（如工具A需要工具B的输出作为参数），且工具库是动态变化的。\n    *   *诊断*：传统检索将工具库视为静态数据库，只能“查一次”，无法像人类一样通过“试错”或“反馈”来发现隐含的依赖关系。\n\n### 第三阶段：范式转换与核心假设\n**（从“静态匹配”转向“动态规划”）**\n\n基于上述诊断，作者意识到问题的根源在于**试图用一次性的静态映射来解决动态的、多步骤的推理问题**。\n\n*   **思维跃迁**：如果人类面对复杂任务时会先“制定计划”，再分步执行，为什么不让检索器也这样做？\n*   **核心假设**：工具检索不应是“Query -> Result”的单跳匹配，而应是一个“Goal -> Plan -> Sub-goals -> Queries -> Results”的**迭代规划过程**。\n*   **新视角**：将底层的检索器视为一个可交互的“环境”，而不是一个静态的索引库。\n\n### 第四阶段：方法论构建\n**（如何实现“查询规划”？）**\n\n为了验证上述假设，作者设计了 **TOOL QP** 框架，将检索过程拆解为三个逻辑阶段：\n\n1.  **任务分解**：\n    *   *逻辑*：为了解决语义鸿沟，不能直接用用户原始查询去检索。\n    *   *方案*：先将复杂的用户指令拆解为一系列逻辑上的子任务。这相当于在抽象意图和具体工具之间架设了一座“中间层桥梁”。\n\n2.  **交互式查询生成**：\n    *   *逻辑*：为了解决组合性和依赖性问题，需要分步检索。\n    *   *方案*：针对每个子任务生成特定的搜索查询。关键在于引入**反馈机制**——每一步检索后，模型会观察结果，并动态调整下一步的查询策略（例如，发现缺少某个前置工具，下一步就去专门搜那个工具）。\n\n3.  **检索聚合**：\n    *   *逻辑*：多步检索会产生多个列表，如何合并？\n    *   *方案*：放弃复杂的加权融合，采用“峰值排名”策略——即取每个工具在所有检索步骤中获得的最高排名。这避免了某些子任务因为查询次数多而主导最终结果的偏差。\n\n### 第五阶段：训练策略的演进\n**（如何在没有标注数据的情况下训练规划器？））\n\n框架设计好了，但面临一个现实难题：现有的数据集只有（用户查询，相关工具），没有中间的“规划轨迹”或“子任务标注”。\n\n1.  **数据合成**：\n    *   *思路*：利用强模型（如GPT-4）作为“教师”，反向合成数据。\n    *   *过程*：让教师模型根据最终的正确工具，反推并生成能够找到这些工具的“规划路径”和“中间查询”。这为模型提供了模仿学习的样本。\n\n2.  **强化学习优化（RLVR）**：\n    *   *思路*：单纯的模仿学习（SFT）只能学会教师的风格，不一定能最大化检索成功率。\n    *   *过程*：引入强化学习（RLVR），直接以检索指标（如nDCG、Recall）作为奖励信号。这迫使模型跳出模仿的局限，自主探索能真正提高检索准确率的查询策略。\n\n---\n\n### 总结：作者的思考路径图\n\n1.  **起点**：LLM智能体需要处理海量工具库 -> 必须检索。\n2.  **痛点**：单次密集检索在复杂任务上表现糟糕。\n3.  **归因**：语义错位、组合性限制、缺乏交互反馈。\n4.  **顿悟**：检索应该是一个**规划**过程，而非简单的匹配。\n5.  **方案**：分解任务 -> 迭代查询 -> 动态反馈 -> 结果聚合。\n6.  **落地**：利用合成数据教模型“怎么想”，利用强化教模型“怎么做得更好”。\n\n这一逻辑链条清晰地展示了作者如何从对现有技术缺陷的敏锐观察，上升到对问题本质的重新定义（从IR到Planning），最终构建出一套完整的解决方案。"
                },
                {
                    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
                    "arxiv_id": "2601.07711",
                    "authors": "Pietro Ferrazzi, Milica Cvjeticanin, Alessio Piraccini, Davide Giannuzzi",
                    "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了“Agentic RAG”，其中LLM作为智能体自主编排整个过程（决定动作、时机、迭代），这直接涉及单智能体的规划、工具使用和自我反思能力，符合研究范围。",
                    "summary2": "本文旨在比较Enhanced RAG与Agentic RAG的性能与成本权衡。针对FIQA、NQ、FEVER和CQADupStack-English数据集，我们对比了基于固定模块的Enhanced RAG与LLM自主编排的Agentic RAG。通过F1、NDCG@10及LLM-as-a-judge等指标验证，发现Agentic RAG在查询重写上表现更优，而Enhanced RAG在文档重排和成本控制上更具优势。",
                    "summary_translation": "检索增强生成 (RAG) 系统通常定义为生成器与检索组件的组合，其中检索组件负责从知识库中提取文本上下文，以回答用户查询。然而，此类基础实现存在若干局限性，包括检索结果存在噪声或非最优、对超出范围的查询误用检索机制、查询与文档匹配度低，以及生成器带来的波动性或成本问题。这些缺陷推动了“增强型” RAG 的发展，即在流程中引入专用模块以解决特定的薄弱环节。近期，大型语言模型日益增强的自我反思能力催生了一种新范式，我们将其称为“代理型” RAG。在该方法中，LLM 统筹整个流程——决定执行何种操作、何时执行以及是否进行迭代——从而减少对固定的人工设计模块的依赖。尽管这两种范式已得到快速普及，但在何种条件下哪种方法更具优势尚不明确。在本研究中，我们针对增强型和代理型 RAG，在多种场景和维度上进行了广泛的实证驱动评估。我们的研究结果揭示了这两种范式之间的权衡关系，并综合考虑成本与性能，为在现实应用中选择最有效的 RAG 设计提供了指导。",
                    "inspiration_trace": "基于论文《Is Agentic RAG worth it? An experimental comparison of RAG approaches》，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：RAG 范式的分化与演进\n**思考起点：** 作者首先观察到 RAG（检索增强生成）技术已从最初的“朴素 RAG”（Naïve RAG，即简单的检索+生成）演进出两条截然不同的发展路径：\n*   **增强型 RAG (Enhanced RAG)：** 传统的工程化优化路径。通过在固定流程中增加特定模块（如路由器、查询重写、重排序器）来修补朴素 RAG 的已知缺陷。这是一种“流水线”式的思维。\n*   **代理型 RAG (Agentic RAG)：** 新兴的智能化路径。利用大语言模型（LLM）的反思和规划能力，让 LLM 作为“大脑”自主决定何时检索、如何重写、是否迭代。这是一种“动态循环”式的思维。\n\n**核心冲突：** 社区和业界对 Agentic RAG 的热情高涨，认为其灵活性代表了未来。然而，这种灵活性是否真的带来了性能的提升？还是仅仅增加了不必要的复杂度和成本？\n\n### 2. 问题聚焦：实证缺失与决策困境\n**痛点识别：** 尽管已有理论上的定义和分类，但缺乏严格的**实证对比**。现有的文献多停留在概念探讨或单一架构的优化上。\n**核心问题：** 在实际应用中，Agentic RAG 相比于精心设计的 Enhanced RAG，究竟是“物有所值”还是“徒增开销”？在什么场景下应该选择哪一种？\n\n### 3. 假设提出：基于“缺陷修复”的维度拆解\n**逻辑推演：** 为了公平对比，不能笼统地比较“整体好坏”，而应该回到 RAG 的根本痛点上。作者假设：Agentic 和 Enhanced 两种范式在解决 RAG 的不同缺陷时，可能各有优劣。\n**维度构建：** 作者将朴素 RAG 的缺陷拆解为四个核心维度，并针对每个维度提出了对比假设：\n1.  **用户意图处理：** Enhanced RAG 依赖显式的分类器（路由），而 Agentic RAG 依赖 LLM 的自主判断。假设：在复杂意图下，Agentic 可能更灵活，但在简单任务上可能过度思考。\n2.  **查询-文档对齐：** Enhanced RAG 使用固定的重写技术（如 Hyde），而 Agentic RAG 可以动态调整查询。假设：Agentic 的动态重写可能更能适应不同文档格式。\n3.  **检索结果精炼：** Enhanced RAG 使用专门的重排序模型，而 Agentic RAG 通过多次迭代检索来优化。假设：专门的重排序模型可能比 LLM 的迭代检索更精准。\n4.  **底层模型质量的影响：** 两种架构对 LLM 能力的敏感度是否不同？\n\n### 4. 方法论设计：控制变量的“擂台赛”\n**实验设计思路：** 为了验证上述假设，作者设计了一个“头对头”的对比实验框架。\n*   **数据选择：** 选取了涵盖问答（QA）和信息检索/提取（IR/E）的四个数据集（FIQA, NQ, FEVER, CQADupStack），以覆盖不同领域和任务类型。\n*   **架构对齐：**\n    *   **Enhanced 端：** 选用当前最先进的 SOTA 组件（如 Semantic Router, Hyde 查询重写, Cross-encoder 重排序）构建最强流水线。\n    *   **Agentic 端：** 构建一个基于图的最小化代理框架，赋予其调用 RAG 工具、重写查询和迭代的能力，但不预设固定步骤。\n*   **评估指标：** 除了传统的性能指标（F1, NDCG），作者特别引入了**成本分析**（Token 消耗、端到端延迟），因为“Is it worth it”的核心在于性价比。\n\n### 5. 结果分析与洞察：打破迷思\n**逻辑推演与发现：** 通过实验数据，作者得出了反直觉或精细化的结论，修正了最初的假设：\n*   **关于意图：** Agentic RAG 在狭窄领域（如金融）表现出色，但在广泛或嘈杂领域（如 FEVER）反而不如 Enhanced RAG 的显式路由器可靠。这表明 LLM 的自主判断在边界模糊时容易失效。\n*   **关于重写：** Agentic RAG 确实表现更好，证明了动态适应性的价值。\n*   **关于精炼：** 专门的重排序模型显著优于 Agentic 的迭代检索。这揭示了 LLM 在“从一堆文档中挑出最好的”这一具体任务上，不如专门的微调模型。\n*   **关于成本：** Agentic RAG 的成本高出数倍（最高 3.6 倍），且延迟更高。\n\n### 6. 结论形成：权衡与指导\n**最终思考：** 并不存在“银弹”。\n*   **Agentic RAG 的价值：** 在于处理模糊的意图和需要动态适应查询格式的场景。\n*   **Enhanced RAG 的价值：** 在于处理需要高精度检索（重排序）和高效率的场景。\n*   **核心建议：** 不要盲目追求 Agentic 的新颖性。如果业务场景对成本敏感、对检索精度要求极高，经过优化的 Enhanced RAG 依然是更优选择；只有在需要高度灵活性和自主决策的复杂场景下，Agentic RAG 的额外成本才“值得”。"
                },
                {
                    "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
                    "arxiv_id": "2601.07696",
                    "authors": "Nick Ferguson, Alan Bundy, Kwabena Nuamah",
                    "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了LLM在多跳问答任务中的工具使用和规划能力（将问题分解为步骤），重点分析了工具选择和工具调用输出，符合单智能体中“工具使用”和“规划”的研究范围。",
                    "summary2": "本文旨在探索大语言模型的元级推理能力。针对多跳表格问答任务，我们提出了一种基于工具的评估框架，通过比较模型工具调用与预设的“essential actions”来分析推理过程。我们在基于世界银行数据的自定义数据集上，通过最终答案准确率、精确率和召回率验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）的最新进展日益聚焦于“推理”能力，这一概念在LLM相关讨论中存在诸多重叠的定义。我们采用一种更具结构化的方法，将元级推理（meta-level reasoning，指代为解决任务所需的中间步骤进行推理的过程）与对象级推理（object-level reasoning，涉及上述步骤的底层执行）区分开来。我们设计了一项新颖的问答任务，该任务基于不同国家在不同年份的地缘政治指标数值。这些问题需要分解为中间步骤、进行数据检索以及对检索到的数据执行数学运算。我们通过考察模型为回答问题而选择合适工具的情况，来分析LLMs的元级推理能力。为了超越单纯的最终答案准确率，对LLMs进行更深入的分析，我们的任务中包含了“必要动作”，通过将LLMs的工具调用输出与这些动作进行比对，从而推断其推理能力的强弱。我们发现，LLMs在我们的任务中表现出了良好的元级推理能力，但在任务理解的某些方面仍存在缺陷。研究发现，n-shot提示（n-shot prompting）对准确率影响甚微；遇到的错误信息通常不会导致性能下降；此外，我们还提供了LLMs数理能力低下的进一步证据。最后，我们讨论了这些发现在其他任务领域的泛化性及其局限性。",
                    "inspiration_trace": "基于对论文《Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task》的深入分析，以下是作者产出该文章的系统性逻辑链推演：\n\n### 1. 宏观观察：现有“推理”评估的模糊性\n**思考起点：** 当前学术界对大语言模型（LLM）能力的讨论高度集中在“推理”这一概念上。然而，作者观察到“推理”一词在LLM语境下定义重叠且模糊（如数学推理、常识推理等）。\n**核心痛点：** 现有的基准测试（如GSM8K, MATH）大多仅关注**最终答案的准确性**。这种“黑盒”评估方式存在严重缺陷：如果模型答错了，我们无法区分是模型**没想对步骤**（规划失败），还是**算错了数**（执行失败）。\n**初步假设：** 为了真正理解LLM的推理能力，必须将“规划做什么”与“实际去做”这两个层面解耦。\n\n### 2. 理论引入：经典AI视角的二元划分\n**理论溯源：** 为了解决上述模糊性，作者回顾了符号AI和自动定理证明领域的经典理论（特别是Bundy, 1983的工作）。\n**概念界定：** 引入**元级推理**与**对象级推理**的严格区分：\n*   **元级推理：** 关于“如何解决问题”的思考，即高层规划、任务分解、步骤选择。\n*   **对象级推理：** 具体执行上述步骤的过程，如数据检索、算术计算、符号操作。\n**逻辑演进：** 作者意识到，将这一经典框架应用于LLM评估，可以将原本混在一起的“推理能力”拆解为两个可独立分析的维度，从而提供比单纯准确率更深层的诊断。\n\n### 3. 方法论构建：将思维过程“外化”\n**关键挑战：** LLM的推理过程通常隐藏在模型内部的隐状态或生成的自然语言中，难以量化评估。如何让“元级推理”变得可观测？\n**解决方案：** 利用**工具使用**范式。\n*   **逻辑支点：** 当LLM调用一个工具（如`search_indicator`或`calculate_mean`）时，它实际上是在显式地展示其“计划”。工具调用序列就是元级推理的**中间表征**。\n*   **任务设计：** 选择**多跳表格QA任务**（基于世界银行数据）。该任务天然需要将复杂问题分解为“检索数据”和“数学运算”两个子步骤，完美契合元级（规划检索与运算顺序）与对象级（实际检索与计算）的二元框架。\n\n### 4. 评估创新：从“结果导向”转向“过程导向”\n**评估困境：** 传统的QA评估只有“对/错”两种状态。但在工具使用场景下，模型可能选对了工具（元级强），但工具参数填错或计算出错（对象级弱）。\n**核心创新：** 提出**“必要动作”**的概念。\n*   **定义：** 针对每个问题，定义一组必须执行的工具调用集合。这不是唯一的“黄金路径”，而是解决问题的核心动作集。\n*   **指标构建：** 不再只看Final Answer，而是将模型生成的工具调用序列与“必要动作”进行对比，计算**精确率**和**召回率**。\n    *   **高精确率：** 模型知道该用什么工具（元级推理强）。\n    *   **低召回率：** 模型遗漏了必要步骤（规划有漏洞）。\n    *   **最终答案错误：** 可能是对象级计算错误，而非元级规划错误。\n\n### 5. 实验验证与发现：诊断模型的能力边界\n**实验设计意图：** 作者并不旨在设计一个让模型得高分的系统，而是利用这个环境作为“显微镜”来观察模型。\n**逻辑推演与验证：**\n*   **验证元级能力：** 实验发现模型在工具选择的精确率上表现良好，证明LLM具备较强的**高层规划能力**（即知道“先做什么后做什么”）。\n*   **验证对象级缺陷：** 当移除数学工具，强制模型自己计算时，性能大幅下降。这证实了LLM在**底层执行（特别是算术）**上的固有缺陷。\n*   **验证鲁棒性：** 通过引入错误信息，观察模型是否能自我修正。这进一步测试了元级推理中的“动态调整”能力。\n\n### 6. 总结：逻辑链的闭环\n作者的思考过程完成了一个闭环：\n从**现象**（LLM推理定义模糊、评估单一）出发 -> 引入**理论**（元级/对象级二分法） -> 寻找**载体**（工具调用作为思维外化的接口） -> 设计**度量**（基于必要动作的过程评估） -> 最终得出**结论**（LLM是优秀的“规划者”，但仍是蹩脚的“计算器”）。\n\n这篇文章的本质不仅仅是发布了一个数据集，而是提供了一套**解剖LLM推理能力的思维框架和手术刀（工具调用评估）**。"
                },
                {
                    "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments",
                    "arxiv_id": "2601.07606",
                    "authors": "Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",
                    "summary": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了一个用于评估“基于智能体的研究判断”的基准，重点比较了“使用工具的智能体”与非智能体基线，涉及工具使用和交互预算，符合单智能体（工具使用）的研究范围。",
                    "summary2": "本文旨在评估模型对科学思想的判断能力及预测其未来影响力。针对科学思想评估缺乏可扩展验证方法的问题，我们提出了一种名为 Proof of Time (PoT) 的半可验证基准框架，通过冻结截止时间前的证据并在离线沙箱中预测未来结果。我们在涵盖四个领域的 30K+ 实例上，通过准确率和测试时计算缩放分析验证了其有效性，发现增加交互预算能提升智能体性能，且工具使用的效果高度依赖于任务类型。",
                    "summary_translation": "大语言模型正日益被用于评估和预测研究思路，然而，我们目前缺乏可扩展的方法来衡量模型对这些科学想法的判断质量。为实现这一目标，我们提出了 PoT，这是一个半可验证的基准测试框架，它将科学想法的判断与随后可观察到的下游信号（例如引用和研究人员议程的转变）联系起来。PoT 在离线沙箱中冻结截止前的证据快照，并要求模型预测截止后的结果，这使得在真实情况出现时能够进行可验证的评估，在无需详尽专家标注的情况下实现可扩展的基准测试，并能够针对同行评审奖项等信号分析人类与模型之间的不一致性。此外，PoT 为评估科学想法的基于智能体的研究判断提供了一个受控测试平台，能够在提示消融和预算缩放的条件下，对比使用工具的智能体与非智能体基线。在跨越四个基准领域的 30,000 多个实例中，我们发现，与非智能体基线相比，更高的交互预算通常能提升智能体的性能，而使用工具的收益则高度依赖于具体任务。通过将按时间划分的、未来可验证的目标与用于工具使用的离线沙箱相结合，PoT 支持对面向未来的科学想法判断任务中的智能体进行可扩展评估。",
                    "inspiration_trace": "基于论文《Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观观察到具体方法设计的思考过程：\n\n### 第一阶段：宏观观察与核心矛盾\n**思考起点：科学评价的“时效性错位”**\n1.  **现象观察**：科学界评价研究创意（Idea）的主要机制（同行评审、静态基准测试）通常发生在“当下”，且高度依赖主观判断。\n2.  **核心矛盾**：真正衡量一个科学创意价值的标准是“时间的检验”（如引用量、奖项、后续研究方向的改变），但这些信号具有滞后性，无法在决策当下立即获取。\n3.  **现有困境**：大语言模型（LLM）正被用于辅助科研评价，但我们缺乏一种可扩展、客观的方法来评估模型判断“未来影响力”的能力。如果仅用静态数据集评估，无法反映模型对科学演进的预测能力。\n\n### 第二阶段：概念突破——将“时间”转化为验证机制\n**核心假设：利用历史数据模拟未来预测**\n1.  **思维转换**：既然无法真的等待未来，不如利用“过去”来模拟“未来”。如果我们将时间轴切分，设定一个截止点 $t_0$，那么对于 $t_0$ 之后的 $t_1$ 时刻，其结果在当下已经是已知的客观事实。\n2.  **方法论雏形**：\n    *   **冻结证据**：只给模型提供 $t_0$ 时刻之前的“快照”信息（如论文摘要、作者历史）。\n    *   **预测未来**：要求模型预测 $t_1$ 时刻才会发生的信号（如 $t_1$ 时刻的引用数、获奖情况）。\n    *   **事后验证**：利用现实中已经发生的 $t_1$ 结果作为“金标准”进行评分。\n3.  **优势确立**：这种方法解决了“可验证性”（标签是客观事实而非主观打分）和“可扩展性”（无需专家人工标注，数据可自动更新）的问题。\n\n### 第三阶段：控制变量——解决“智能体”评估的污染问题\n**进阶思考：如何公平地评估工具使用能力？**\n1.  **新挑战**：当前流行使用“工具调用智能体”来处理复杂任务。但现有评估往往混淆了“推理能力”与“信息获取能力”。如果允许智能体联网，它可能只是直接查到了答案，而非基于证据进行了判断。\n2.  **隔离设计**：为了纯粹测试模型基于有限证据进行推理和判断的能力，作者引入了**“离线沙盒”**概念。\n3.  **逻辑闭环**：\n    *   将智能体关在一个“断网”的房间里。\n    *   房间里只有 $t_0$ 时刻的冻结证据和本地工具（如Python、文本编辑器）。\n    *   智能体表现出的任何提升，必须归因于其对有限证据的挖掘和推理能力，而非外部信息检索。\n\n### 第四阶段：维度拆解——定义“科学创意判断”的具体内涵\n**操作化定义：从抽象概念到具体任务**\n1.  **问题细化**：“科学创意判断”是一个抽象概念，需要将其拆解为可量化的具体维度。\n2.  **四个维度的构建**：\n    *   **影响力预测**：预测未来的引用量（量化指标）。\n    *   **价值评估**：预测同行评审奖项（定性共识）。\n    *   **研究演进**：预测教授未来的研究方向（连续性与漂移）。\n    *   **技术前沿**：预测基准测试的SOTA轨迹（技术极限）。\n3.  **任务设计逻辑**：这些任务覆盖了从个人（教授）、群体（会议奖项）到领域（SOTA）不同层面的科学判断，且均符合“时间可验证”原则。\n\n### 第五阶段：实验假设与验证——探索“智能体”的边际效应\n**实证探究：智能体何时才值得？**\n1.  **对比基准**：设置“零样本”与“智能体”模式的对比，旨在验证增加工具和推理步骤是否真的有效。\n2.  **成本-收益分析**：引入“消息预算”概念，模拟测试时的计算成本。\n3.  **假设验证**：\n    *   智能体并非在所有任务上都优于直接生成。\n    *   在需要深度证据挖掘的任务（如Faculty任务）上，智能体优势明显。\n    *   在结构化预测或简单任务上，增加智能体步骤可能只是浪费算力。\n4.  **结论导向**：通过实验揭示模型在处理“未来导向”任务时的失败模式（如检索失败、推理循环），为未来改进提供方向。\n\n---\n\n**总结：作者的逻辑演进路径**\n从**“科学评价需要时间检验”**的哲学观察出发，通过**“时间切片”**的技术手段将未来预测转化为离线验证，进而引入**“离线沙盒”**以排除信息干扰，纯粹考察模型的**“证据推理能力”**，最终构建了一个多维度、可扩展的基准，回答了“AI能否判断科学创意的未来价值”这一核心问题。"
                },
                {
                    "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents",
                    "arxiv_id": "2601.07582",
                    "authors": "Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",
                    "summary": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.",
                    "category": "cs.CL",
                    "filter_reason": "论文研究了长期对话智能体的记忆机制，提出了基于事件分割的分层记忆架构，属于单智能体研究范围中的“记忆”方向。",
                    "summary2": "本文旨在解决长期对话智能体中记忆粒度僵化及检索缺乏结构感知的问题。针对长期交互场景，我们提出了一种基于Event Segmentation Theory的ES-Mem框架，结合动态事件分割与边界锚定的分层记忆架构。我们在LoCoMo和LongMemEval-S基准上通过F1、BLEU-1和Accuracy等指标验证了其有效性。",
                    "summary_translation": "记忆对于对话代理在长期交互中维持连贯性并实现持续适应至关重要。尽管现有的记忆机制具备基本的存储与检索能力，但它们主要受限于两个方面：(1) 僵化的记忆粒度往往破坏语义完整性，导致记忆单元碎片化且缺乏连贯性；(2) 主流的扁平化检索范式仅依赖于表层语义相似度，忽视了在导航和定位特定情景语境时所必需的话语结构线索。为克服上述局限，受事件分割理论的启发，我们提出了ES-Mem框架，该框架包含两个核心组件：(1) 动态事件分割模块，用于将长期交互划分为具有清晰边界的语义连贯事件；(2) 分层记忆架构，通过构建多层记忆并利用边界语义来锚定特定的情景记忆，从而实现精确的语境定位。在两个记忆基准测试上的评估表明，ES-Mem相较于基线方法取得了持续的性能提升。此外，所提出的事件分割模块在对话分割数据集上也展现出了稳健的适用性。",
                    "inspiration_trace": "基于论文《ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents》的内容，以下是对作者产出该文章核心方法逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法论构建的思考过程。\n\n---\n\n### 1. 宏观问题：长程对话中的“记忆断层”\n**思考起点：**\n作者首先关注到大语言模型（LLM）在对话智能体应用中的一个根本性矛盾：虽然LLM生成能力极强，但其固有的“上下文窗口”限制了处理超长对话的能力。\n**核心挑战：**\n为了实现真正的个性化与连续适应，智能体必须具备“长期记忆”。然而，现有的记忆机制（如简单的RAG或向量数据库）在面对成百上千轮的复杂对话时，往往表现不佳，导致智能体“遗忘”或“胡言乱语”。\n\n### 2. 深入观察：现有记忆机制的两大“结构性缺陷”\n作者对现有的主流记忆方法（如MemGPT, MemoryBank等）进行了深入剖析，发现它们普遍存在两个深层次的逻辑漏洞，这构成了文章的切入点：\n\n*   **缺陷一：记忆粒度的“机械性”。**\n    *   *观察：* 现有方法大多采用固定粒度（如按“轮次”Turn或固定Token数）切分对话。\n    *   *问题：* 真实的对话流是语义连续的。机械切分往往会打断一个完整的语义事件（例如，讨论“生日礼物”的想法跨越了3轮，却被切成了两半）。这导致存储的记忆单元本身是“碎片化”和“语义不完整”的。\n*   **缺陷二：检索范式的“扁平化”。**\n    *   *观察：* 现有检索大多基于向量相似度的“扁平检索”，把所有记忆块当作孤立的文本片段进行匹配。\n    *   *问题：* 这种方式忽略了对话的“篇章结构”。当用户问“为什么我们后来放弃了那个园艺工具？”时，关键信息不在于“园艺工具”这个词本身，而在于话题**转换**的那个瞬间。扁平检索很难定位这种结构性的转折点。\n\n### 3. 跨学科灵感：引入认知心理学中的“事件分割理论”\n**思考转折：**\n为了解决上述“语义碎片”和“结构缺失”的问题，作者跳出纯计算机科学的视角，转向认知心理学寻找答案。\n**理论引入：**\n作者引入了**事件分割理论**。该理论指出，人类并非连续地感知世界，而是将经验流解析为离散的、有意义的事件单元。\n**关键洞察：**\n人类记忆中，**事件边界**尤为重要。边界处是注意力最集中的时刻，起到了“认知锚点”的作用，帮助人类高效地索引和回忆过去的经历。\n*假设：* 如果让AI像人类一样，按“事件”来组织记忆，并利用“边界”作为检索的锚点，就能解决现有方法的痛点。\n\n### 4. 核心假设形成：从“存储文本”转向“结构化事件”\n基于EST理论，作者提出了核心假设：\n*   **关于存储：** 记忆的粒度不应是固定的轮次，而应是动态的“语义事件”。\n*   **关于检索：** 检索不应是全局的文本匹配，而应是先定位“边界锚点”，再展开细节的“由粗到细”过程。\n\n### 5. 方法论构建：ES-Mem框架的逻辑落地\n为了验证上述假设，作者设计了ES-Mem框架，其逻辑演进分为三个步骤：\n\n**第一步：如何定义“事件”？（动态分割模块）**\n*   *思考：* 机器如何知道一个话题结束了？\n*   *策略：* 采用“统计信号+语义验证”的两阶段法。\n    1.  **粗筛：** 利用互信息计算话题连贯性，当语义连贯性骤降时，标记为潜在边界。\n    2.  **精修：** 引入意图识别，判断这是话题的“转换”还是内容的“细化”。只有真正的意图转换才被确认为边界。\n\n**第二步：如何利用“边界”？（分层记忆架构）**\n*   *思考：* 既然边界是锚点，那么记忆的结构就不能是扁平的。\n*   *策略：* 构建三层金字塔结构。\n    *   **Level 1（顶层）：精炼边界。** 专门描述“话题A是如何转换到话题B的”。这是检索时的“路标”。\n    *   **Level 2（中层）：事件摘要。** 用于快速匹配内容。\n    *   **Level 3（底层）：原始上下文。** 用于最终生成细节。\n    *   *创新点：* 显式地将“边界”建模为一种可检索的信息索引，而不仅仅是切分点。\n\n**第三步：如何模拟人类回忆？（由粗到细检索）**\n*   *思考：* 人类回忆时，先想“那是哪段时间的事？”，再想“具体说了什么？”。\n*   *策略：* 模仿这一认知过程。\n    1.  **边界扫描：** 先在Level 1（边界层）搜索，找到最相关的“话题转换时刻”。\n    2.  **区间扩展：** 以该边界为中心，向前后扩展一个时间窗口，锁定相关的记忆区间。\n    3.  **摘要重排：** 在锁定的区间内，利用Level 2（摘要）进行精细打分，选出最准确的上下文。\n\n### 6. 逻辑闭环与验证\n**最终产出：**\n作者通过这一系列思考，将传统的“静态、扁平”的记忆系统，重构为“动态、结构化、认知驱动”的记忆框架。\n**验证逻辑：**\n在实验中，作者不仅验证了ES-Mem在长程记忆任务上的性能提升，还专门验证了“事件分割”模块本身的鲁棒性。这证明了：**模仿人类认知结构（事件分割+边界锚定）确实是解决长程对话记忆难题的有效路径。**\n\n---\n\n**总结：**\n作者的思考路径是从**“现有技术无法处理长程语义连贯性”**这一工程问题出发，通过**“认知心理学的事件分割理论”**获得理论指引，最终通过**“动态分割+分层存储+锚点检索”**的技术手段，实现了对人类记忆机制的工程化复现。"
                },
                {
                    "title": "GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap",
                    "arxiv_id": "2601.07375",
                    "authors": "Farzad Shami, Subhrasankha Dey, Nico Van de Weghe, Henrikki Tenkanen",
                    "summary": "The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent's execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at https://anonymous.4open.science/r/groke.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于LLM的分层框架（GROKE），用于执行导航指令评估。它涉及单智能体的核心能力——规划（子指令规划）和导航（拓扑图导航），符合“单智能体：规划”的研究范围。论文明确为“无视觉”，避开了多模态/视觉的排除项，且重点在于智能体的架构设计与执行能力，而非纯应用或基础设施优化。",
                    "summary2": "本文旨在解决视觉依赖评估中的成本与感知误差问题。针对 Map2Seq 数据集，我们提出了一种基于 OpenStreetMap 的无视觉分层 LLM 框架 GROKE，结合子指令规划与结构化 JSON 表示进行图推理。我们在 Map2Seq 上通过 Navigation Error (NE)、Success Rate (SR) 等指标验证了其有效性，导航误差降低了 68.5%。",
                    "summary_translation": "导航指令的评估仍然是视觉语言导航 (VLN) 研究中一个长期存在的挑战。传统的基于参考的指标，如 BLEU 和 ROUGE，无法捕捉空间指令的功能效用，特别是无法衡量指令是否成功引导导航者到达预定目的地。尽管现有的 VLN 智能体可以作为评估器，但它们对高保真视觉模拟器的依赖带来了许可限制和计算成本，且感知误差进一步干扰了语言质量评估。本文介绍了 GROKE (Graph-based Reasoning over OSM Knowledge for instruction Evaluation)，这是一个基于分层大语言模型 (LLM) 的无视觉、无需训练的框架，用于利用 OpenStreetMap 数据评估导航指令。通过系统的消融实验，我们证明了空间信息的结构化 JSON 和文本格式显著优于基于网格和视觉图的表示。我们的分层架构结合了子指令规划与拓扑图导航，在 Map2Seq 数据集上，与启发式和采样基线相比，将导航误差降低了 68.5%。智能体的执行成功率、轨迹保真度和决策模式作为功能可导航性的代理指标（基于 OSM 可见地标和拓扑结构），建立了一种无视觉依赖的可扩展且可解释的评估范式。代码和数据可在 https://anonymous.4open.science/r/groke 获取。",
                    "inspiration_trace": "基于对论文《GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观问题：如何准确评估导航指令的“功能性”质量？\n**思考起点：**\n在视觉语言导航（VLN）领域，传统的评估指标（如BLEU、ROUGE）存在根本性缺陷。这些指标基于文本相似度（n-gram重叠），无法捕捉导航指令的核心价值——即“能否引导用户到达目的地”。\n*   **反例：** “在银行左转”与“在银行右转”文本相似度极高，但功能截然相反。\n*   **结论：** 评估必须从“文本相似度”转向“功能效用性”。\n\n### 2. 现状批判：现有“务实评估”路径的痛点\n**演进逻辑：**\n为了解决上述问题，学术界引入了“Agent-as-Judge”范式，即训练一个智能体在模拟器中执行指令，通过成功率来反推指令质量。\n**观察到的瓶颈：**\n这种方法严重依赖高保真的视觉模拟器（如Matterport3D、Google Street View），引入了新的噪声：\n1.  **混淆变量：** 智能体失败可能是因为视觉识别能力差（看不清“红砖墙”），而非指令本身写得不好。这导致评估结果混杂了视觉感知误差。\n2.  **成本与壁垒：** 视觉数据昂贵、版权受限、计算量大，限制了评估的可扩展性。\n\n### 3. 核心假设：能否剥离视觉，仅基于“语义与拓扑”进行评估？\n**思维跃迁：**\n导航的本质是空间推理，而非像素识别。人类在阅读导航指南（如地图）时，依赖的是地标（POI）、方向和拓扑连接，而非实景照片。\n**假设提出：**\n如果我们将环境抽象为符号化的地图数据（如OpenStreetMap），构建一个“无视觉”的评估智能体，是否既能保留功能性评估的优势，又能消除视觉噪声和成本问题？\n*   **数据基础：** Map2Seq数据集提供了OSM数据（节点、边、POI），为这一假设提供了实验土壤。\n\n### 4. 方法论探索：如何让大语言模型（LLM）“看懂”地图？\n**技术挑战：**\n既然决定使用LLM作为推理核心，如何将图结构的空间数据转化为LLM能高效理解的输入？\n**实验与试错（Ablation Studies驱动的设计）：**\n作者对比了四种空间表征形式，试图寻找最优解：\n1.  **网格/矩阵：** 模仿视觉像素。结果发现LLM难以解析这种高密度的ASCII字符，效果最差。\n2.  **可视化图：** 使用Graphviz风格。虽然直观，但LLM处理箭头和图形符号的推理能力不如处理结构化数据。\n3.  **纯文本描述：** 自然语言描述连接关系。效果尚可，但在复杂路径上信息密度不足，导致认知负荷过高。\n4.  **结构化JSON（最终选择）：** 将节点、边、POI组织为层级化的JSON。\n    *   **逻辑判断：** JSON格式既保留了机器可读的结构，又符合LLM预训练数据中的代码/结构化文本模式，能显著提升推理效率和准确性。\n\n### 5. 架构优化：如何处理长程导航的复杂性？\n**问题分解：**\n直接让LLM根据整段长指令在地图上一步步走，容易迷失目标或产生累积误差。\n**灵感来源：** 人类认知习惯——将复杂任务拆解为子目标。\n**架构设计：**\n提出**分层架构**：\n1.  **子指令代理：** 负责高层规划，将长指令拆解为原子动作（如“直走”、“左转”）并提取关键地标。\n2.  **导航代理：** 负责底层执行，仅关注当前子目标在局部地图（可见区域）内的实现。\n*   **逻辑优势：** 这种解耦降低了单次推理的复杂度，使得智能体能更专注于当前的局部决策，同时保持全局目标的一致性。\n\n### 6. 最终验证：这种“无视觉”评估是否有效？\n**闭环思考：**\n如果智能体没有眼睛，它的成功是否真的代表了指令的质量？\n**验证逻辑：**\n通过相关性分析，将GROKE的导航指标（如导航误差NE、成功率SR）与人类对指令清晰度的评分进行对比。\n*   **结果：** 两者呈现显著相关性。证明了一个基于逻辑和拓扑的智能体，足以作为指令质量的可靠代理指标，从而建立了一种**可扩展、可解释且无视觉依赖**的评估新范式。\n\n---\n\n**总结：**\n作者的思考路径是从**评估指标的失效**出发，批判了**视觉依赖的局限性**，提出了**基于OSM图推理的“无视觉”假设**，并通过**对比实验确定了JSON作为最优的空间表征**，最终利用**分层代理架构**实现了高效、准确的指令评估。"
                },
                {
                    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
                    "arxiv_id": "2601.07348",
                    "authors": "Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang",
                    "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了“受控自我演化”（CSE）框架，通过迭代“生成-验证-优化”循环实现自我完善，符合“自我演化”的研究范围。同时，文中包含“多样化规划初始化”和“分层进化记忆”等组件，涉及单智能体的规划与记忆机制。",
                    "summary2": "本文旨在解决现有自进化方法在算法代码优化中探索效率低下的问题。针对代码生成任务，我们提出了一种Controlled Self-Evolution (CSE)框架，通过多样化规划初始化、遗传进化及分层进化记忆提升搜索效率。在EffiBench-X基准上，通过Execution-Time ratio (ET)、Memory-Peak ratio (MP)和Memory-Integral ratio (MI)指标验证了其有效性，CSE在多种LLM主干网络上均表现出更优的算法优化能力。",
                    "summary_translation": "自进化方法通过迭代的“生成-验证-优化”循环来增强代码生成，然而现有方法存在探索效率低下的问题，无法在有限的预算内发现具有更优复杂度的解决方案。这种低效性源于初始化偏差导致进化陷入劣质解区域、缺乏反馈引导的不可控随机操作，以及跨任务经验利用不足。为解决这些瓶颈，我们提出了受控自进化，该方法包含三个关键组件。多样化规划初始化生成结构各异的算法策略，以实现广泛的解空间覆盖。遗传进化用反馈引导机制替代随机操作，从而实现定向突变和组合交叉。分层进化记忆在任务间和任务内层面捕获成功与失败的经验。在 EffiBench-X 上的实验表明，CSE 在各种 LLM backbones (大语言模型骨干) 上均持续优于所有 baselines (基线模型)。此外，CSE 在早期代即展现出更高的效率，并在整个进化过程中保持持续改进。我们的代码已在 https://github.com/QuantaAlpha/EvoControl 公开。",
                    "inspiration_trace": "基于论文《Controlled Self-Evolution for Algorithmic Code Optimization》，以下是对作者产出该核心方法的逻辑链推演与思考过程还原：\n\n### 1. 宏观观察：从“功能正确”到“算法最优”的鸿沟\n*   **现象**：现有的LLM在代码生成任务上已经表现出色，能够通过单次生成解决许多编程问题，实现“功能正确”。\n*   **问题**：在算法竞赛或高性能计算场景下，仅仅“正确”是不够的。代码的执行效率（时间复杂度、空间复杂度）至关重要。现有的模型往往生成的是“正确但低效”的代码。\n*   **初步思考**：如何让模型不仅能写出能跑通的代码，还能像人类专家一样不断优化代码，逼近算法的最优解？\n\n### 2. 现有范式分析：自进化的潜力与瓶颈\n*   **现有方案**：学术界引入了“自进化”范式，即通过“生成-验证-修正”的迭代循环来优化代码。\n*   **深入观察**：虽然理论上只要迭代次数足够多，随机搜索总能找到最优解，但在实际应用中，计算资源和推理预算是有限的。\n*   **核心矛盾**：**探索效率低下**。现有的自进化方法在有限的预算内，很难跳出局部最优，发现具有更优复杂度的解。它们浪费了大量的预算在低质量的解空间中。\n\n### 3. 病因诊断：效率低下的三大根源\n作者深入剖析了为什么现有的自进化方法“瞎忙活”，归纳出三个核心痛点：\n\n*   **痛点一：初始化偏差**\n    *   *观察*：传统方法通常从一个或少数几个初始解开始进化。如果初始解处于解空间的“贫瘠区域”（例如算法思路本身是低效的），后续的微调很难从根本上改变算法结构，导致进化陷入局部最优。\n    *   *思考*：起点决定了起跑线，如果起跑线就选错了，后面跑得再快也没用。我们需要在起跑时就覆盖不同的算法思路。\n\n*   **痛点二：无控制的随机进化**\n    *   *观察*：现有的变异和交叉操作往往是随机的、黑盒的。模型不知道哪里错了，只是盲目地修改代码或拼接文本。这种“无向探索”导致生成的变体大多无效，无法利用验证反馈来指导搜索方向。\n    *   *思考*：进化不能靠“猜”，必须靠“反馈”。我们需要一种机制，能精准定位代码中的“病灶”，并进行“手术式”的修复，而不是盲目重写。\n\n*   **痛点三：进化经验的浪费**\n    *   *观察*：模型在进化过程中经常重复犯同样的错误（无论是同一个任务内的重复失败，还是不同任务间忽略了通用的优化技巧）。现有的方法缺乏记忆机制，无法积累和复用成功的经验。\n    *   *思考*：人类专家之所以强，是因为他们记住了之前的教训和套路。我们需要给模型装上“短期记忆”（避免重蹈覆辙）和“长期记忆”（复用通用优化策略）。\n\n### 4. 核心假设：从“随机搜索”转向“受控进化”\n*   **假设提出**：如果我们将进化过程从“无控制的随机操作”转变为“受反馈引导的精细化操作”，并辅以多样化的起点和记忆机制，就能大幅提升探索效率。\n*   **方法论构建**：基于上述三个痛点，提出 **Controlled Self-Evolution (CSE)** 框架，对应设计三个关键组件来逐一击破。\n\n### 5. 方法论演进：三大组件的逻辑构建\n\n*   **针对痛点一（初始化偏差） -> 多样化规划初始化**\n    *   *设计思路*：不要直接生成代码，先生成“策略草图”。强制模型在生成具体代码前，先规划出多种结构上截然不同的算法策略（如贪心 vs 动态规划 vs 位运算）。\n    *   *逻辑*：通过策略层面的多样性，确保初始种群覆盖了解空间中多个有潜力的区域，降低了陷入局部最优的风险。\n\n*   **针对痛点二（无控制进化） -> 遗传进化机制**\n    *   *设计思路*：引入“功能分解”概念。将代码拆解为独立的功能模块（如I/O、核心逻辑、边界处理）。\n    *   *受控变异*：利用反馈定位导致性能低下的具体模块，只对该模块进行“靶向再生”，保留表现良好的部分。\n    *   *组合交叉*：模仿人类专家，从不同父代中提取优势模块（如A的算法核心 + B的优化技巧），在逻辑层面进行结构化重组，而非简单的文本拼接。\n\n*   **针对痛点三（经验浪费） -> 分层进化记忆**\n    *   *设计思路*：建立双层记忆系统。\n    *   *局部记忆（任务内）*：记录当前任务中哪些修改带来了提升（成功经验），哪些导致了倒退（失败教训），实时指导后续迭代，避免走回头路。\n    *   *全局记忆（跨任务）*：将不同任务中的通用优化模式（如特定的I/O加速技巧、数据结构替换规则）提炼出来，存入向量数据库。遇到新任务时，主动检索相关经验作为先验知识。\n\n### 6. 逻辑闭环与验证\n*   **综合**：将“多样化起点”作为基础，通过“受控的遗传操作”在解空间中高效导航，并利用“分层记忆”作为导航的指南针。\n*   **预期结果**：这种方法不仅能更快地找到高质量解（早期效率高），而且能在整个进化过程中持续改进（持续优化），不会过早陷入停滞。\n*   **实验验证**：在EffiBench-X上的实验结果证实了CSE在不同LLM骨干网络上均优于基线方法，且消融实验证明了三个组件缺一不可，形成了协同效应。\n\n---\n\n**总结**：作者的思考路径是从**发现LLM代码效率不足**这一现象出发，通过分析现有自进化方法**“盲目搜索”**的本质缺陷，提出了**“受控引导”**的核心思想，并最终通过**策略多样化、操作精细化、经验分层化**三个维度的创新，构建了一套完整的算法代码优化方法论。"
                },
                {
                    "title": "Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation",
                    "arxiv_id": "2601.07338",
                    "authors": "Yanzhi Tian, Cunxiang Wang, Zeming Liu, Heyan Huang, Wenbo Yu, Dawei Song, Jie Tang, Yuhang Guo",
                    "summary": "Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了 RATE，一个基于智能体的翻译评估框架，其中包含一个具有自我反思能力的核心智能体，并能动态调用专门的子智能体。这涉及了单智能体的自我反思、工具使用以及多智能体协作，符合 LLM 智能体的研究范围。",
                    "summary2": "本文旨在解决非字面翻译评估中现有指标失效的问题。针对包含俚语、隐喻等复杂语言现象的场景，我们提出了一种名为 RATE 的 Reflective Agentic Translation Evaluation 框架，通过 Core Agent 动态调用子代理获取外部知识并校准分数。我们在构建的 MENT 数据集上通过 Meta Score 验证了其有效性，结果显示 RATE 显著优于现有指标。",
                    "summary_translation": "大语言模型显著推动了机器翻译的发展，并将其应用于语言复杂的领域——如社交网络服务、文学等。在这些场景中，翻译往往需要处理非字面表达，从而导致机器翻译指标的不准确。为了系统地研究机器翻译指标的可靠性，我们首先构建了一个专注于非字面翻译的元评估数据集，即 MENT。MENT 涵盖了四个非字面翻译领域，包含源句子与来自不同机器翻译系统的译文配对，并附带 7,530 个人工标注的翻译质量分数。实验结果揭示了传统机器翻译指标的不准确性，以及大语言模型作为评判者的局限性，特别是知识截止和评分不一致的问题。为了缓解这些局限性，我们提出了 RATE，这是一种新颖的基于智能体的翻译评估框架，其核心是一个能够动态调用专门子智能体的反思性核心智能体。实验结果表明了 RATE 的有效性，与当前指标相比，其元分数至少提升了 3.2 分。进一步的实验证明，RATE 在通用领域的机器翻译评估中也具有鲁棒性。代码和数据集可在以下地址获取：https://github.com/BITHLP/RATE。",
                    "inspiration_trace": "基于论文《Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation》，以下是对作者产出该文章核心方法（RATE）的逻辑链推演与思考过程还原：\n\n### 第一阶段：宏观观察与问题定位\n**（从“LLM很强大”到“评估标准失效”）**\n\n1.  **现象观察**：作者首先注意到大语言模型（LLMs）极大地推动了机器翻译（MT）的发展，使其应用场景从传统的新闻领域扩展到了社交媒体（SNS）、文学、跨文化内容等复杂领域。\n2.  **核心矛盾**：在这些新场景中，翻译的核心难点不再是字面意义的转换，而是对**非字面表达**（如网络俚语、文化隐喻、诗歌意象）的处理。\n3.  **现有工具的失效**：作者发现，传统的MT评估指标（如BLEU、COMET）依赖于字面重叠或形式化文本匹配，无法理解深层语义，导致评估结果与人类判断严重错位。\n4.  **初步假设**：现有的评估体系已经无法适应LLM时代的非字面翻译需求，必须重新审视评估的可靠性。\n\n### 第二阶段：假设验证与基准构建\n**（从“怀疑指标”到“量化失效”）**\n\n1.  **验证策略**：为了系统性地证明“现有指标不可靠”，作者需要一个专门的测试集。然而，现有的Meta-evaluation数据集（如WMT）多基于新闻或维基百科，缺乏非字面内容。\n2.  **构建MENT数据集**：作者决定构建一个专注于非字面翻译的Meta-evaluation数据集（MENT）。\n    *   **覆盖范围**：选取了四个最具代表性的非字面领域（SNS、跨文化、诗歌、文学）。\n    *   **数据质量**：通过严格的筛选和人工标注，确保数据集包含高难度的语言学挑战。\n3.  **实验验证**：在MENT上测试传统指标和新兴的“LLM-as-a-Judge”方法。\n4.  **发现新问题**：实验证实了传统指标确实失效。虽然“LLM-as-a-Judge”表现较好，但作者敏锐地发现了其两个致命缺陷：\n    *   **知识截止**：LLM无法理解最新的网络流行语或生僻的文化典故。\n    *   **评分不一致**：LLM在打分时存在主观性和波动性，缺乏校准机制。\n\n### 第三阶段：根因分析与思维转向\n**（从“静态评估”到“动态反思”）**\n\n1.  **根因诊断**：作者意识到，单纯依赖LLM的内部参数知识（静态）和单次Prompt（被动）是无法解决上述问题的。\n    *   针对“知识截止”，评估者必须具备**外部检索能力**。\n    *   针对“评分不一致”，评估者必须具备**自我反思与校准能力**。\n2.  **思维跃迁**：作者不再将评估视为一个简单的“输入文本-输出分数”的函数，而是将其建模为一个**需要多步推理、工具调用和决策的智能过程**。这自然引出了“Agent（智能体）”的概念。\n\n### 第四阶段：方法论设计\n**（从“单一模型”到“多智能体协作框架 RATE”）**\n\n为了解决上述根因，作者设计了 **RATE (Reflective Agentic Translation Evaluation)** 框架，其设计逻辑遵循“分而治之”与“动态编排”：\n\n1.  **核心架构设计**：需要一个“大脑”来统筹全局，而不是固定的流水线。因此设计了 **Core Agent（核心智能体）**，采用OODA（观察-调整-决策-行动）循环，根据当前状态动态决定下一步动作。\n2.  **解决“知识截止” -> Search Agent**：\n    *   *思考*：当Core Agent遇到未知俚语或文化背景时，不应瞎猜，而应去查。\n    *   *实现*：设计 **Search Agent**，负责调用搜索引擎获取实时外部知识，并将背景信息回传给Core Agent。\n3.  **解决“评分不一致” -> Comparison Agent**：\n    *   *思考*：绝对分数（如3.5分）很难把握，但相对好坏（A比B好）更容易判断。\n    *   *实现*：设计 **Comparison Agent**，通过将当前译文与历史锚点进行成对比较，来校准分数，将主观判断转化为相对排序。\n4.  **基础评估 -> Evaluation Agent**：\n    *   *思考*：仍需要一个基础模块来执行具体的打分任务。\n    *   *实现*：设计 **Evaluation Agent**，结合Core Agent提供的背景知识，进行初步打分并标记置信度。\n\n### 第五阶段：验证与泛化\n**（从“特定领域”到“通用鲁棒性”）**\n\n1.  **闭环验证**：在MENT数据集上测试RATE。逻辑是：如果RATE确实解决了知识截止和评分不一致，那么它在非字面翻译上的表现应显著优于所有Baseline。实验结果证实了这一点（Meta score提升至少3.2）。\n2.  **鲁棒性检验**：作者进一步思考：这种复杂的Agent框架是否只适用于刁钻的非字面场景？在通用领域（如WMT23）是否会“杀鸡用牛刀”甚至性能下降？\n3.  **结论**：实验证明，由于Core Agent的动态调度能力，RATE在通用领域也能保持与SOTA相当的性能，证明了该方法的普适性和鲁棒性。\n\n---\n\n**总结：作者的思考路径**\n从**发现LLM应用场景下沉带来的评估错位**出发，通过**构建MENT数据集量化了传统方法和静态LLM的缺陷**，进而**诊断出“知识缺失”和“主观波动”两大痛点**，最终**跳出单一模型的思维定式，利用Agent技术构建了一个具备反思、检索和校准能力的动态评估框架（RATE）**，完成了从问题发现到方法创新的全逻辑闭环。"
                },
                {
                    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
                    "arxiv_id": "2601.07264",
                    "authors": "Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya",
                    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究基于LLM的“tool-use agents”（工具使用智能体），分析了工具集成智能体工作流中的校准问题，并提出了通过强化学习微调来构建具有自我意识的智能体。这完全符合“单智能体：工具使用”的研究范围。",
                    "summary2": "本文旨在解决Tool-use agents中的miscalibration问题。针对Evidence tools导致overconfidence的场景，我们提出了Calibration Agentic RL (CAR)框架，利用Margin-Separated Calibration Reward (MSCR)联合优化任务准确性与校准。我们在NQ、HotpotQA、SimpleQA-verified及AIME、MATH-500数据集上，通过Accuracy、ECE、Brier Score和AUROC验证了其有效性，显著提升了模型的校准能力与泛化性。",
                    "summary_translation": "基于大语言模型 (LLMs) 的自主代理正在快速发展以处理多轮任务，但确保其可信度仍然是一个关键挑战。这种可信度的一个基本支柱是校准，它指的是代理表达能够可靠反映其实际性能的置信度的能力。尽管校准在静态模型中已有深入研究，但其在集成工具的代理工作流中的动态变化仍未被充分探索。在这项工作中，我们系统地调查了工具使用代理中的语言化校准，揭示了由工具类型驱动的基本置信度二分法。具体而言，我们的试点研究表明，证据工具（如 web search）由于检索信息中固有的噪声，会系统性地导致严重的过度自信，而验证工具（如 code interpreters）可以通过确定性反馈来锚定推理并减轻校准偏差。为了在不同工具类型间稳健地提升校准性能，我们提出了一种强化学习 (RL) 微调框架，该框架联合优化任务准确性和校准性能，并得到了全面的奖励设计基准的支持。我们证明，经过训练的代理不仅实现了卓越的校准性能，而且表现出从本地训练环境到嘈杂的网络设置以及数学推理等不同领域的稳健泛化能力。我们的结果强调了针对工具使用代理采用特定领域校准策略的必要性。更广泛地说，这项工作为构建能够在高风险的现实世界部署中可靠地传达不确定性的自我感知代理奠定了基础。",
                    "inspiration_trace": "基于论文《The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents》，以下是对作者核心方法论产出逻辑链的系统推演：\n\n### 1. 宏观背景与核心矛盾：从“能力”到“可信度”\n**思考起点：**\n随着大语言模型（LLM）向智能体演进，工具使用（如搜索、代码解释器）极大地扩展了模型的能力边界。然而，作者敏锐地捕捉到了一个被忽视的关键问题：**信任危机**。\n**逻辑推演：**\n*   现有研究多关注智能体“能不能做”，而忽略了“知不知道自己能不能做”。\n*   在高风险场景下，智能体的**校准**能力——即其表达的置信度与实际表现的一致性——是可信度的基石。\n*   **初步观察：** 现有文献指出，引入工具后，智能体往往比静态模型表现出更严重的过度自信。这引发了一个根本性的疑问：**工具使用本身是否就是导致校准失效的元凶？**\n\n### 2. 深入探究与假设提出：打破“工具”的刻板印象\n**思考转折：**\n作者没有接受“工具导致过度自信”这一笼统结论，而是试图解构“工具”这一概念。\n**逻辑推演：**\n*   **假设：** 并非所有工具都对校准产生相同影响。工具的**性质**（反馈机制、输出确定性）可能决定了其对置信度的不同影响。\n*   **分类维度：** 作者将工具划分为两类典型范式：\n    1.  **证据工具：** 如网络搜索。特征是输出开放、充满噪声、缺乏明确的负向反馈（搜索总是有结果的，无论是否相关）。\n    2.  **验证工具：** 如代码解释器。特征是输出确定、提供执行反馈（代码会报错），能提供逻辑上的“落地”。\n\n### 3. 验证与发现：揭示“置信度二分法”\n**思考过程：**\n通过设计对比实验（直接提示 vs. 工具使用 vs. RL微调），作者验证了上述假设，发现了核心现象——**置信度二分法**。\n**逻辑推演：**\n*   **证据工具的陷阱：** 在使用网络搜索时，智能体表现出严重的过度自信。原因在于“检索行为”本身被模型误认为是“尽职调查”，且检索到的噪声信息被误认为确凿证据，导致虚假的确定性。\n*   **验证工具的锚定：** 在使用代码解释器时，智能体的校准度反而提升。因为确定性的执行反馈（如报错信息）为推理过程提供了现实约束，抑制了盲目的自信。\n*   **结论：** 校准失效并非工具使用的普遍后果，而是特定于**证据工具**带来的噪声干扰。这指明了后续研究的靶心：**如何修复证据工具导致的过度自信？**\n\n### 4. 方法论构建：从“提示工程”到“内在校准”\n**思考转折：**\n既然证据工具的噪声无法完全消除，且简单的提示工程无法解决根本问题（实验表明Prompting-based策略依然失效），作者转向通过训练来改变模型的内在置信度生成机制。\n**逻辑推演：**\n*   **技术选型：** 采用强化学习（RL）进行微调，因为智能体本身就是通过RL训练来使用工具的，这能保持任务能力的连贯性。\n*   **核心挑战：** 如何设计奖励函数？传统的奖励仅关注任务准确性，这往往鼓励模型“瞎猜”或过度自信。引入校准项（如Brier Score）虽然能惩罚置信度偏差，但存在一个隐患：**激励重叠**。\n\n### 5. 核心创新：解决“安全失败”的激励冲突\n**思考深化：**\n作者深入分析了现有校准奖励（如RLCR）的缺陷，发现了一个逻辑漏洞：如果对“低置信度的错误回答”给予过高的奖励（因为它诚实），模型可能会学会“安全失败”——即为了获得校准分而故意降低置信度，甚至放弃尝试正确回答。\n**逻辑推演：**\n*   **设计原则：** 必须建立严格的优先级。**“做对”必须永远优于“做错”**，无论置信度如何。\n*   **方案提出：** **边际分离校准奖励**。\n    *   **机制：** 强制将奖励空间划分为两个互不重叠的区域。所有正确答案的奖励下限，必须高于所有错误答案的奖励上限。\n    *   **效果：** 这消除了模型通过“诚实但错误”来投机取巧的动机，迫使模型在追求正确性的前提下，再去优化置信度的表达。\n\n### 6. 验证与泛化：从实验室到现实世界\n**思考闭环：**\n为了证明CAR框架不仅仅是过拟合训练数据，作者设计了更具挑战性的验证场景。\n**逻辑推演：**\n*   **环境泛化：** 从干净的本地检索环境迁移到充满噪声的真实API环境（如Serper API）。结果证明，模型学到的不是死记硬背的特定置信度值，而是一种对不确定性的感知能力。\n*   **领域泛化：** 将该方法应用于数学推理（验证工具场景）。虽然验证工具本身有助于校准，但CAR框架依然能带来额外提升，证明了该方法的通用性。\n\n### 总结：思想演进脉络\n1.  **观察：** 智能体越强，越容易盲目自信（可信度危机）。\n2.  **质疑：** 是所有工具都导致盲目自信吗？\n3.  **发现：** 只有“证据工具”（如搜索）因噪声导致过度自信，而“验证工具”（如代码）反而能锚定置信度（二分法）。\n4.  **定位：** 重点解决证据工具场景下的校准问题。\n5.  **洞察：** 现有的校准训练方法存在“安全失败”的漏洞，可能鼓励模型“躺平”。\n6.  **解决：** 提出CAR框架与MSCR奖励，通过严格分离正确与错误的奖励边界，迫使模型在追求准确的同时学会表达不确定性。"
                },
                {
                    "title": "ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity A REM-Inspired System Design for Emergent Creative Ideation",
                    "arxiv_id": "2601.07121",
                    "authors": "Makoto Sato",
                    "summary": "Large language models (LLMs) are used not only for problem solving but also for creative ideation; however, eliciting serendipitous insights that are both novel and internally coherent remains difficult. While stochastic sampling promotes novelty, it often degrades consistency. Here, we propose ReMIND, a REM-inspired modular framework for ideation. ReMIND consists of four stages: wake, which generates a stable low-temperature semantic baseline; dream, which performs high-temperature exploratory generation; judge, which applies coarse evaluation to filter incoherent outputs and extract candidate ideas; and re-wake, which re-articulates selected ideas into coherent final outputs. By instantiating each stage as an independent LLM, ReMIND enables functional separation between exploration and consolidation. Parameter sweeps show that ReMIND reliably induces semantic exploration while preserving downstream stability. Embedding-based analyses confirm substantial semantic displacement during the dream phase, whereas external evaluations reveal that high-quality ideas emerge sporadically rather than as extrema along any single metric. These results suggest that serendipitous ideation in LLMs is a rare-event process best approached through system level design that shapes the conditions under which valuable ideas can emerge and be stabilized. ReMIND provides a general framework for studying the computational basis of serendipity and illustrates how modular LLM orchestration can bridge exploration and stabilization.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了ReMIND框架，通过将四个独立的LLM实例化为不同的模块（Wake, Dream, Judge, Re-wake）来协同完成创意构思任务。这属于多智能体协作（角色分工）和自我反思（Judge阶段评估与过滤）的研究范畴，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决LLM难以生成兼具新颖性与连贯性的Serendipitous insights的问题。针对Creative ideation场景，我们提出了一种受REM睡眠启发的模块化框架ReMIND，通过Wake、Dream、Judge和Re-wake四个阶段实现探索与巩固的功能分离。在多种Conceptual pair prompts上，通过外部LLM评估和Embedding-based similarity analysis验证了其有效性。",
                    "summary_translation": "大语言模型不仅用于问题解决，还用于创造性构思；然而，要诱导出既新颖又内部连贯的意外洞察仍然困难重重。虽然随机采样有助于提升新颖性，但往往会损害一致性。在此，我们提出了 ReMIND，这是一种受 REM (快速眼动睡眠) 启发的模块化构思框架。ReMIND 包含四个阶段：wake (清醒)，用于生成稳定的低温度语义基线；dream (做梦)，用于执行高温度的探索性生成；judge (评判)，用于应用粗粒度评估以过滤不连贯的输出并提取候选想法；以及 re-wake (再清醒)，用于将选定的想法重新阐述为连贯的最终输出。通过将每个阶段实例化为一个独立的 LLM (大语言模型)，ReMIND 实现了探索与巩固之间的功能分离。参数扫描表明，ReMIND 能够可靠地诱导语义探索，同时保持下游稳定性。基于嵌入的分析证实，在 dream (做梦) 阶段存在显著的语义位移，而外部评估显示，高质量想法是零星出现的，而非作为任何单一指标的极值而存在。这些结果表明，LLM 中的意外构思是一个稀有事件过程，最好通过系统级设计来应对，这种设计塑造了有价值想法涌现并得以稳定的条件。ReMIND 为研究机缘巧合的计算基础提供了一个通用框架，并阐明了模块化 LLM (大语言模型) 编排如何桥接探索与稳定。",
                    "inspiration_trace": "基于对论文《ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity》的深度分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 第一阶段：问题识别与核心矛盾\n**（从宏观现象出发）**\n\n1.  **观察现状**：大语言模型（LLMs）已被广泛应用于创意生成，但面临一个根本性瓶颈——难以同时兼顾“新颖性”与“连贯性”。\n2.  **剖析矛盾**：\n    *   **高随机性**：提高采样温度虽然能增加新颖性，但往往导致输出语无伦次、事实错误。\n    *   **低随机性**：降低温度保证了逻辑连贯，但输出多为陈词滥调，缺乏真正的洞察力。\n3.  **现有方法的局限**：目前的创意生成主要依赖“单次生成”，试图在同一个模型实例中平衡探索与约束。作者意识到，这种“单体模型”的范式本质上将创造力视为一种参数调优的权衡，而非一种可被系统化设计的认知过程。\n\n### 第二阶段：跨学科启发与理论假设\n**（引入生物学视角）**\n\n1.  **寻找灵感**：作者将目光转向人类认知科学，特别是关于“顿悟”产生机制的研究。\n2.  **关键隐喻——REM睡眠**：心理学和神经科学研究表明，人类的创造性洞察常发生在快速眼动（REM）睡眠期间。\n    *   **机制**：在REM阶段，大脑的海马体进行广泛的联想探索（记忆重组、概念松绑），而负责逻辑判断的前额叶皮层活动减弱（去抑制）。\n    *   **后续**：这种不受约束的探索之后，必须经历一个“稳定化”阶段，将碎片化的洞察重新整合进清醒时的逻辑框架中。\n3.  **提出假设**：如果人工系统的创造力也遵循这一机制，那么解决LLM创意瓶颈的关键不在于优化单一模型的参数，而在于**功能解耦**——将“探索”与“巩固”在时间和计算上分离开来。\n\n### 第三阶段：方法论构建与架构设计\n**（从理论到系统设计）**\n\n1.  **设计原则**：构建一个受REM启发的模块化框架，明确划分“探索”、“评估”和“巩固”三个阶段。\n2.  **模块定义与功能映射**：\n    *   **Wake（清醒/锚点）**：使用低温度采样，生成一个稳定、符合逻辑的基线输出。其作用不是产生创意，而是作为语义锚点，确保后续生成不偏离主题。\n    *   **Dream（做梦/探索）**：使用高温度采样，故意放松逻辑约束，进行疯狂的语义跳跃和概念重组。这一阶段模拟REM睡眠，允许产生看似荒谬但可能蕴含潜力的组合。\n    *   **Judge（评判/筛选）**：作为一个独立的过滤器，不参与生成，仅评估Dream输出的连贯性，并提取出有潜力的“候选想法”。这模拟了大脑对记忆痕迹的初步筛选。\n    *   **Re-wake（再清醒/巩固）**：这是最关键的一步。重新调用Wake模型，将Judge筛选出的“碎片化创意”进行重述和润色。\n    *   **逻辑闭环**：通过Re-wake，系统利用低温度模型的逻辑能力，将高温度探索出的“狂野想法”驯化为人类可理解的、连贯的最终输出。\n3.  **核心创新点**：作者意识到，**同一个模型在不同阶段扮演不同角色**。Wake在第一阶段是“锚点”，在最后阶段变成了“稳定器/压缩器”。\n\n### 第四阶段：实验验证与现象洞察\n**（通过实证修正认知）**\n\n1.  **验证策略**：如何证明这种方法真的产生了“有意义的意外”？\n    *   **量化语义位移**：使用嵌入向量计算Wake输出与Dream输出的余弦相似度。数据证实，Dream阶段确实导致了显著的语义漂移（探索发生）。\n    *   **外部评估**：使用更强的外部模型（如GPT-5.2）对最终输出进行评分。\n2.  **关键发现**：\n    *   高质量创意并非均匀分布，而是**稀疏出现的**。\n    *   即使在相同参数下，也只有部分运行产生了极具价值的洞察。\n3.  **理论修正**：这一发现促使作者将“意外创意”重新定义为一种**“稀有事件过程”**。这意味着我们无法通过确定性算法“制造”创意，但可以通过系统设计**提高其涌现的概率**。\n\n### 第五阶段：哲学升华与范式转移\n**（最终结论）**\n\n1.  **总结范式**：作者最终提出，通往人工创造力的路径不是单纯扩大模型规模或增加数据量，而是**“思维的功能编排”**。\n2.  **系统观**：ReMIND不仅仅是一个提示技巧，它代表了一种新的系统设计哲学——**BiMoLLM（脑启发模块化LLM）**。即通过模块间的交互涌现出高阶智能，而非依赖单一模型的万能性。\n3.  **最终产出**：文章产出了一套可复现的、将生物学认知过程转化为计算架构的工程框架，证明了通过分离探索与巩固，可以在保持逻辑连贯性的同时，显著提升LLM产生意外洞察的能力。"
                },
                {
                    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
                    "arxiv_id": "2601.06966",
                    "authors": "Haonan Bian, Zhiyuan Yao, Sen Hu, Zishan Xu, Shaolei Zhang, Yifu Guo, Ziliang Yang, Xueran Han, Huacan Wang, Ronghao Chen",
                    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究LLM作为自主通用智能体的记忆机制（属于单智能体核心能力），并在数据构建中使用了多智能体对话生成，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决现有LLM记忆基准难以评估长期项目导向交互的问题。针对现实世界中动态演进的项目场景，我们提出了RealMem基准及其包含项目基础构建、多智能体对话生成及记忆日程管理的三阶段合成管道。我们在包含11个场景、2000+跨会话对话的RealMem数据集上，通过Recall@k、NDCG@k及QA Score等指标验证了其有效性，揭示了现有系统在动态状态管理上的不足。",
                    "summary_translation": "随着 Large Language Models (LLMs，大语言模型) 从静态对话接口演变为 autonomous general agents (自主通用智能体)，有效的 memory (记忆) 对于确保 long-term consistency (长期一致性) 至关重要。然而，现有的 benchmarks (基准测试) 主要关注 casual conversation (日常闲聊) 或 task-oriented dialogue (任务导向对话)，未能捕捉到 **“long-term project-oriented” (长期项目导向)** 的交互，在此类交互中，agents (智能体) 必须跟踪 evolving goals (不断演进的目标)。为了弥合这一差距，我们介绍了 **RealMem**，这是首个 grounded in realistic project scenarios (基于现实项目场景) 的 benchmark (基准测试)。RealMem 包含跨越 11 个场景的 2,000 多个 cross-session dialogues (跨会话对话)，并利用 natural user queries (自然用户查询) 进行评估。我们提出了一个 synthesis pipeline (合成流程)，该流程整合了 Project Foundation Construction (项目基础构建)、Multi-Agent Dialogue Generation (多智能体对话生成) 以及 Memory and Schedule Management (记忆与日程管理)，以模拟 memory (记忆) 的 dynamic evolution (动态演变)。实验表明，当前的 memory systems (记忆系统) 在管理现实世界项目中固有的 long-term project states (长期项目状态) 和 dynamic context dependencies (动态上下文依赖) 方面面临重大挑战。我们的代码和数据集可在 [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench) 获取。",
                    "inspiration_trace": "基于论文《RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction》，以下是对作者核心方法逻辑链的系统性推演，旨在还原其从宏观观察到具体产出的思考过程：\n\n### 1. 宏观观察：从“对话”到“代理”的范式转变\n**思考起点**：作者首先注意到大语言模型（LLMs）的发展趋势正在发生质变。模型不再仅仅是简单的“聊天机器人”，而是正在演变为能够长期协作的“自主智能体”。\n**核心洞察**：在这种新范式下，**“记忆”** 成为了决定性因素。没有有效的记忆，智能体就无法在长期交互中保持一致性，也无法实现真正的个性化与通用人工智能（AGI）。\n\n### 2. 问题聚焦：现有基准的“静态”局限\n**观察现状**：作者审视了现有的记忆评估基准（如 LoCoMo, LongMemEval, HaluMem），发现它们大多存在一个共同的缺陷：**过于“静态”和“孤立”**。\n*   **LoCoMo** 仅关注社交闲聊。\n*   **LongMemEval** 类似于“大海捞针”测试，侧重于孤立的事实检索。\n**提出质疑**：这些基准测试的是“你记住了某个事实吗？”，而不是“你能否在复杂、变化的环境中利用记忆推进项目？”。它们无法反映真实世界中**长期、跨会话、目标导向**的交互逻辑。\n\n### 3. 核心假设：真实交互是“项目导向”的\n**定义新范式**：作者提出，真实世界的记忆驱动交互应当属于第三种范式——**“长期项目导向交互”**。\n**提炼特征**：为了构建这一新范式，作者抽象出了四个关键特征，这也是后续方法设计的指导原则：\n1.  **内生性查询**：问题源于任务进展，而非孤立的事实核查。\n2.  **交错分布**：对话在多个项目间穿插（如健身与旅行计划交替进行）。\n3.  **动态状态演化**：环境非静止，记忆需随状态（如受伤、计划变更）同步更新。\n4.  **主动上下文对齐**：智能体需利用记忆主动推断模糊意图，而非被动应答。\n\n### 4. 方法构建：如何模拟“动态演化”？\n**面临的挑战**：如何获取包含数千次跨会话对话、且具有复杂逻辑一致性的真实数据？显然，人工标注不现实，现有数据集也不存在。\n**解决思路**：作者决定采用**合成数据**的方法，但必须解决“长期生成容易逻辑崩塌”的问题。为此，设计了一个**三阶段合成流水线**：\n\n*   **阶段一：项目基础构建**\n    *   *思考*：先搭骨架，再填血肉。\n    *   *逻辑*：先定义用户画像和项目目标，再生成“蓝图”和“事件列表”。这确保了全局逻辑的连贯性，防止后续对话跑偏。\n\n*   **阶段二：多智能体对话生成**\n    *   *思考*：模拟真实博弈，而非单向生成。\n    *   *逻辑*：引入“用户智能体”和“助手智能体”。用户智能体只能看到当前会话摘要（模拟人类遗忘），助手智能体拥有完整记忆。这种不对称信息设置迫使模型必须依赖记忆机制来维持对话。\n\n*   **阶段三：记忆与日程管理**\n    *   *思考*：形成闭环反馈，确保记忆“活着”。\n    *   *逻辑*：对话生成后，通过专门的代理提取记忆点、更新日程表、去重。这些更新后的记忆又会作为下一轮对话的上下文输入。这模拟了记忆随时间动态演化的过程。\n\n### 5. 评估洞察：从“检索”到“状态管理”\n**重新定义评估标准**：作者意识到，传统的检索指标（如 Recall）不足以衡量项目导向任务。\n**逻辑推演**：在复杂项目中，**精确度**比**召回率**更重要。如果检索到了大量相关但充满噪音的信息，反而会干扰模型决策。\n**新指标设计**：因此，作者引入了基于 LLM 的语义评估（如 Mem Recall, Mem Helpful）和 QA Score，重点考察模型是否正确利用了**动态状态**，而不仅仅是生成了流畅的文本。\n\n### 6. 最终产出：RealMem 的诞生\n**结论验证**：通过实验，作者发现现有的 SOTA 记忆系统（如 Mem0, MemoryOS）在处理动态更新和主动对齐时依然表现不佳，证明了该基准的有效性和挑战性。\n**价值定位**：RealMem 不仅仅是一个数据集，它是一个**诊断工具**，揭示了当前智能体在处理长期、复杂、动态项目时的核心瓶颈，迫使社区从“静态知识库”向“动态状态管理器”转变。\n\n---\n\n**总结**：作者的思考路径是从**“智能体需要长期记忆”**这一宏观趋势出发，通过批判现有基准的**“静态性”**，提出了**“项目导向”**的动态交互假设，进而通过**“分层合成+闭环反馈”**的方法论解决了数据构建难题，最终建立了一套能够真实反映智能体动态记忆管理能力的评估体系。"
                },
                {
                    "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
                    "arxiv_id": "2601.06922",
                    "authors": "Tianhua Zhang, Kun Li, Junan Li, Yunxiang Li, Hongyin Luo, Xixin Wu, James Glass, Helen Meng",
                    "summary": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究“Agentic RAG”，将问答视为推理与信息检索（工具使用）之间的多步交互。它提出了一种基于树的强化学习框架来优化智能体的决策过程，属于单智能体和自我演化的研究范畴。",
                    "summary2": "本文旨在解决Agentic RAG中仅结果监督的强化学习面临的信用分配难题。针对多步推理与检索交互场景，我们提出了一种名为TREE PS-RAG的在线树结构强化学习框架，该方法将推理过程建模为树，利用蒙特卡洛估计从后代结果中推导步骤优势，无需中间标注。在七个QA基准上通过Exact Match指标验证，其性能显著优于现有基线。",
                    "summary_translation": "代理式检索增强生成将问答任务构建为推理与信息检索之间的多步交互过程，并近期通过基于结果的监督强化学习得到了推进。尽管行之有效，但仅依赖稀疏的最终奖励限制了逐步信用分配，且对中间推理和动作的指导作用较弱。近期的研究探索了过程级监督，但通常依赖于离线构建的训练数据（存在分布偏移的风险），或者需要高昂成本的中间标注。本文提出了 TreePS-RAG，这是一种用于代理式 RAG 的在线、基于树的强化学习框架，能够在保留标准仅基于结果奖励的同时实现逐步信用分配。我们的核心思想是将代理式 RAG 的推理过程建模为一棵推演树，其中每个推理步骤自然地映射为一个节点。这种树结构允许通过对其后代结果进行蒙特卡洛估计来估算步骤效用，从而在无需中间标签的情况下获得细粒度的过程优势。为了使该范式具有实用性，我们引入了一种高效的在线树构建策略，能够在受限的计算预算下保持探索多样性。在与 Search-R1 等强基线相当的推演成本下，在多个模型规模的七个多跳和通用问答基准上进行的实验表明，TreePS-RAG 始终显著优于基于结果监督和领先的过程监督强化学习方法。",
                    "inspiration_trace": "基于论文《TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG》的内容，以下是对作者核心方法论产出过程的逻辑推演与还原：\n\n### 第一阶段：问题锚定——从“结果导向”到“过程黑箱”的困境\n\n**1. 宏观观察：**\n作者首先关注到 Agentic RAG（智能体检索增强生成）已成为解决复杂多跳问答的主流范式。为了优化这一过程，学术界引入了强化学习（RL），特别是像 Search-R1 这样的方法，利用最终答案的正确性作为奖励信号来训练智能体。\n\n**2. 痛点识别：**\n然而，作者敏锐地发现这种仅依赖“结果监督”的方法存在一个核心缺陷：**信用分配难题**。\n*   **逻辑推演：** 在一个多步推理的轨迹中，如果最终答案错误，RL 算法通常会对所有步骤进行“连坐”惩罚。但实际上，可能只有中间某一步的检索或推理是致命的，其他步骤可能是正确的。\n*   **结论：** 稀疏且滞后的最终奖励无法提供细粒度的指导，限制了智能体学习高效搜索和推理策略的能力。\n\n### 第二阶段：路径探索——理想与现实的博弈\n\n**1. 提出假设：**\n既然结果监督太粗糙，那么“过程监督”显然是更好的选择。即，对每一个中间步骤（如搜索查询、推理片段）都给出一个即时奖励。\n\n**2. 现实阻碍：**\n作者审视了现有的过程监督方案，发现了两个不可忽视的障碍：\n*   **标注成本高：** 获取高质量的中间步骤标注（如每一步的子问题是否正确）极其昂贵。\n*   **分布偏移：** 许多方法（如 ReasonRAG）依赖离线构建的数据集进行训练。这意味着智能体是在“静态”的过去数据上学习，而非在“动态”的在线交互中学习，导致模型在面对新环境时泛化能力下降。\n\n**3. 核心矛盾：**\n我们需要**细粒度的过程信号**，但我们必须在**无中间标注**且**在线**的约束下获得它。\n\n### 第三阶段：核心洞察——将“树”转化为“自监督工具”\n\n**1. 思维跃迁：**\n如何在不依赖外部标注者的情况下评估一个中间步骤的好坏？作者借鉴了蒙特卡洛树搜索（MCTS）的思想，提出了一个关键假设：\n*   **假设：** 如果一个中间步骤（节点）是好的，那么从该步骤出发，通过多次随机探索（rollout），最终得到正确答案的概率应该很高。\n\n**2. 结构化建模：**\n基于上述假设，作者将 Agentic RAG 的推理过程重新定义为**树结构**，而非线性的轨迹。\n*   **逻辑映射：** 每一个推理步骤对应树上的一个节点。从根节点到叶节点的路径代表一条完整的推理轨迹。\n*   **价值反推：** 不需要人为给中间步骤打分。只需看该节点下的所有“子孙”叶节点（最终结果）的平均奖励。如果后代大多答对了，那么这个中间节点的价值就高。\n\n**3. 解决矛盾：**\n这种方法巧妙地绕过了“标注”和“离线”的障碍：\n*   **无标注：** 价值估计完全基于易于获取的最终答案。\n*   **在线性：** 树是在训练过程中实时构建和探索的，完全符合在线 RL 的范式。\n\n### 第四阶段：工程落地——在有限预算下驯服“指数爆炸”\n\n**1. 新的挑战：**\n虽然树结构在理论上完美，但在实际计算中，随着深度增加，节点数量会呈指数级爆炸。如果无限制地展开树，计算成本将不可接受。\n\n**2. 约束设定：**\n作者设定了一个硬性约束：**计算成本必须与传统的线性采样方法（如 Search-R1）相当**。即，总采样节点数 $N$ 必须固定。\n\n**3. 策略优化：**\n为了在固定预算 $N$ 下最大化树的效用，作者引入了两个关键机制：\n*   **动态分支控制：** 不再平均用力，而是根据当前层的节点数量动态分配下一层的分支数，确保总节点数维持在预算 $N$ 附近。\n*   **语义剪枝：** 作者意识到，如果两个搜索步骤检索到的文档高度重合，那么它们就是冗余的。为了在有限预算下探索更多可能性，必须去除冗余。\n    *   **逻辑：** 利用检索文档的 Jaccard 相似度来衡量节点间的语义距离，通过聚类保留多样化的路径，剔除重复探索。\n\n### 第五阶段：方法闭环——从树结构到 RL 优化\n\n**1. 信号生成：**\n通过上述构建的树，作者计算出了每个节点的“过程优势”。\n*   **全局优势：** 当前步骤相对于根节点（整体平均水平）的提升。\n*   **局部优势：** 当前步骤相对于其父节点（上一步）的提升。\n\n**2. 训练整合：**\n最后，将这些树结构推导出的细粒度优势值，无缝集成到标准的策略梯度算法（如 GRPO/PPO）中。\n*   **逻辑：** 模型不再只对最终答案负责，而是对树中每一个经过的推理步骤负责。这使得模型能够精确地学习到“哪一步检索是关键的”、“哪一步推理是多余的”。\n\n---\n\n**总结：**\n作者的思考路径是从**发现稀疏奖励的局限性**出发，试图引入过程监督但受限于**标注成本和分布偏移**，最终通过**树结构建模**将“最终结果”转化为“中间步骤的价值估计”，并利用**剪枝策略**解决了计算复杂度问题，从而实现了一种无需标注、在线且高效的 Agentic RAG 训练框架。"
                },
                {
                    "title": "AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents",
                    "arxiv_id": "2601.06818",
                    "authors": "Xuannan Liu, Xiao Yang, Zekun Li, Peipei Li, Ran He",
                    "summary": "As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于LLM智能体，提出了针对智能体工作流中幻觉归因的基准测试。研究内容明确涉及智能体的核心能力，如规划、工具使用和多步推理，属于单智能体研究范畴。虽然涉及幻觉（可靠性），但重点在于评估智能体轨迹而非被排除的安全对齐或纯推理问题。",
                    "summary2": "本文旨在解决LLM-based agents在多步工作流中难以定位和解释幻觉起源的问题。针对多步Agent轨迹，我们提出了一种**automated hallucination attribution**新任务，并构建了包含693条高质量轨迹及系统化分类法的**AgentHallu** benchmark。我们在该数据集上评估了13个主流LLM，通过**step localization accuracy**和**G-EVAL scores**验证了其有效性。实验表明，即使是顶尖模型（如Gemini-2.5-Pro）在步骤定位上也仅达到41.1%的准确率，凸显了该任务的挑战性。",
                    "summary_translation": "由于基于大语言模型的智能体在执行顺序多步推理时，中间步骤产生的幻觉存在沿轨迹传播的风险，从而降低整体可靠性。与单轮响应中的幻觉检测不同，诊断多步工作流中的幻觉需要识别出导致初始偏差的具体步骤。为填补这一空白，我们提出了一项新的研究任务——基于大语言模型的智能体的自动幻觉归因，旨在识别导致幻觉的步骤并解释其原因。为支持该任务，我们引入了 AgentHallu，这是一个综合基准，包含：(1) 693 条涵盖 7 个智能体框架和 5 个领域的高质量轨迹；(2) 一个包含 5 个大类（规划 Planning、检索 Retrieval、推理 Reasoning、人机交互 Human-Interaction 和工具使用 Tool-Use）及 14 个子类别的幻觉分类体系；(3) 涵盖二分类标签、致幻步骤及因果解释的人工策划多级标注。我们评估了 13 个领先模型，结果表明，即使是顶级模型（如 GPT-5、Gemini-2.5-Pro），该任务也极具挑战性。表现最佳的模型仅实现了 41.1% 的步骤定位准确率，其中工具使用幻觉最为困难，准确率仅为 11.6%。我们相信 AgentHallu 将促进未来关于开发鲁棒、透明且可靠的智能体系统的研究。",
                    "inspiration_trace": "基于对论文《AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：从“单轮对话”到“多步智能体”的范式转移\n**思考起点：**\n随着大语言模型（LLM）的发展，研究热点已从简单的单轮问答转向了复杂的**LLM-based Agents（智能体）**。智能体具备规划、检索、工具调用、多步推理等能力，能够解决长链路任务。\n\n**核心洞察：**\n在单轮对话中，幻觉通常表现为“生成内容与事实不符”。但在智能体的**多步工作流**中，问题变得复杂：中间某一步的错误（如规划失误、检索错误）会像滚雪球一样**向后传播**，导致最终结果错误。\n*   **关键矛盾：** 仅仅判断“最终答案是否错误”（二分类）对于智能体来说远远不够。如果不知道错误是在哪一步产生的，就无法修复智能体，也无法提升其可靠性。\n\n### 2. 问题聚焦：从“检测”到“归因”的认知升级\n**现有局限：**\n作者回顾了现有的幻觉检测基准（如HaluEval, FELM等），发现它们都局限于**单轮响应**的**二分类判断**（是/非幻觉）。这些基准无法回答两个关键问题：\n1.  **Where（在哪里）：** 错误最早出现在轨迹的哪一步？\n2.  **Why（为什么）：** 这一步为什么会出错？\n\n**假设提出：**\n为了构建可靠的智能体系统，必须提出一个新的研究任务——**自动化幻觉归因**。这个任务的目标不仅仅是发现错误，而是要像调试程序一样，**定位导致错误的“源代码行”（步骤）并解释原因**。\n\n### 3. 方法论构建：如何定义和量化“归因”？\n**思考难点：**\n在多步轨迹中，错误往往具有连锁反应。例如，第1步规划错了，导致第3步工具调用错了，最后第5步答案错了。究竟哪一步才是“负责”的？\n\n**逻辑定义（因果对齐）：**\n作者引入了因果推断的思想来定义“负责步骤”：\n*   **反事实推理：** 如果修正了某一步 $u_t$，并重新执行后续步骤，最终答案变正确了，那么 $u_t$ 就是幻觉的根源。\n*   **最小化原则：** 如果有多个步骤都满足上述条件，取最早的那一步（即错误的源头）。\n\n### 4. 数据构建：如何设计基准以覆盖智能体的复杂性？\n**思考路径：**\n既然要评估“归因”，数据集就不能只有问答对，必须包含完整的**思维-行动-观察**轨迹。同时，智能体的幻觉类型是多样的，不能一概而论。\n\n**分类学构建：**\n作者没有凭空想象类别，而是通过**扎根理论**分析数据，归纳出智能体特有的5大幻觉类别，对应智能体的核心能力模块：\n1.  **Planning（规划）：** 目标分解错误。\n2.  **Retrieval（检索）：** 查询或上下文错误。\n3.  **Reasoning（推理）：** 逻辑或计算错误。\n4.  **Human-Interaction（人机交互）：** 误解人类反馈。\n5.  **Tool-Use（工具使用）：** 工具参数或调用错误。\n\n**数据筛选策略：**\n为了保证基准的挑战性，作者制定了严格的过滤标准：\n*   **排除非欺骗性失败：** 剔除那些直接报错、崩溃的简单案例（太容易检测）。\n*   **保留“ plausible but wrong”：** 专注于那些看起来逻辑通顺、但结果错误的轨迹，这才是归因的难点所在。\n\n### 5. 评估验证：证明任务的必要性与难度\n**逻辑闭环：**\n如果现有的顶尖模型（如GPT-5, Gemini-2.5-Pro）能轻松完成这个任务，那么这个基准就没有价值。\n\n**实验设计：**\n作者在这些模型上测试了两种Prompting策略（标准Prompt vs. 逐步Prompt）。\n*   **预期结果：** 即使是最强的模型，在步骤定位上的准确率也很低（约41%），特别是在工具使用幻觉上（仅11.6%）。\n*   **结论：** 这证实了“幻觉归因”确实是一个尚未解决的难题，从而确立了AgentHallu基准的学术价值——它为未来的研究指明了方向（即如何让模型具备自我诊断和因果解释的能力）。\n\n---\n\n**总结：作者的思考链条**\n1.  **观察现象：** 智能体的多步特性导致错误传播，单轮检测失效。\n2.  **提出假设：** 需要从“判断对错”升级为“定位源头+解释原因”。\n3.  **形式化定义：** 利用反事实推理定义“负责步骤”。\n4.  **工程实现：** 构建包含多维度分类和细粒度标注的AgentHallu数据集。\n5.  **验证价值：** 通过实验证明现有SOTA模型在此任务上的不足，确立研究基准。"
                },
                {
                    "title": "IDRBench: Interactive Deep Research Benchmark",
                    "arxiv_id": "2601.06676",
                    "authors": "Yingchaojie Feng, Qiang Huang, Xiaoya Xie, Zhaorui Yang, Jun Yu, Wei Chen, Anthony K. H. Tung",
                    "summary": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了针对LLM驱动的深度研究智能体的基准，涉及多智能体框架、Web探索（工具使用）以及通过交互反馈进行动态调整（自我反思/演化），符合多智能体协作及单智能体工具使用的研究范围。",
                    "summary2": "本文旨在解决现有深度研究基准忽略交互动态评估的问题。针对未明确指定的查询场景，我们提出了IDRBench，包含交互式多代理框架、基于参考的User Simulator及交互感知评估套件。我们在引入模糊性注入的数据集上，通过Report Similarity、LLM-ACS及Interaction Turns等指标，验证了交互能显著提升研究质量与鲁棒性。",
                    "summary_translation": "由大语言模型驱动的深度研究代理能够执行多步推理、网络探索和长篇报告生成。然而，大多数现有系统以自主模式运行，假设用户意图已完全明确，且仅评估最终输出。在实践中，研究目标往往定义不足且在探索过程中不断演变，因此持续的交互对于实现鲁棒对齐至关重要。尽管交互至关重要，但现有的深度研究基准大多未将其纳入考量，既未对动态用户反馈进行建模，也未量化交互成本。我们介绍了IDRBench，这是首个用于系统性评估交互式深度研究的基准。IDRBench结合了具备按需交互功能的模块化多代理研究框架、可扩展的基于参考的用户模拟器，以及一个交互感知评估套件；该套件能够联合衡量交互收益（质量与对齐度）和交互成本（交互轮次与令牌数）。针对七个最先进大语言模型的实验表明，交互能够持续提升研究质量和鲁棒性，其效果往往能超越模型能力差异带来的影响，同时也揭示了在交互效率方面存在显著的权衡。",
                    "inspiration_trace": "基于论文《IDRBench: Interactive Deep Research Benchmark》的内容，以下是对作者核心方法论产出过程的逻辑链推演。这一过程展现了从宏观趋势观察到具体痛点识别，再到方法论创新与验证的完整思考路径。\n\n---\n\n### 1. 宏观观察与趋势捕捉\n**思考起点：** 作者首先关注到大语言模型（LLM）在信息获取领域的演进。\n*   **现象：** LLM的能力已从单轮问答进化为能够进行多步推理、网页探索和长报告生成的“深度研究智能体”。\n*   **现状：** 现有的主流系统（如DeepResearcher等）大多采用**自主模式**，即用户给出初始指令，系统独立完成全过程，最后仅评估生成的报告质量。\n\n### 2. 现实痛点与核心假设\n**深入思考：** 作者敏锐地发现了“自主模式”与“真实研究场景”之间的巨大鸿沟。\n*   **问题识别：**\n    1.  **意图模糊性：** 现实中的用户需求往往是未充分定义的，用户在研究开始时并不清楚自己到底想要什么。\n    2.  **意图漂移：** 在长周期的推理过程中，智能体容易偏离用户初衷，产生幻觉或跑题，缺乏纠偏机制。\n*   **核心假设：** 深度研究不应是“独角戏”，而应是**“交互式协作”**。引入用户反馈可以显著提升研究质量和对齐度，甚至可能弥补模型本身能力的不足。\n\n### 3. 评估盲区的发现\n**关键转折：** 作者意识到，虽然“交互”很重要，但现有的评估体系完全忽略了这一点。\n*   **盲区分析：**\n    *   现有的基准测试都是**静态**的（Query + Reference Document），只看最终结果，不看中间过程。\n    *   这种评估方式无法区分“运气好答对”和“通过交互修正错误”的智能体。\n    *   更重要的是，它们忽略了交互的**成本**（打扰用户的次数、Token消耗）。\n*   **推论：** 要推动交互式研究的发展，必须建立一套能够量化“交互收益”与“交互成本”的新型基准。\n\n### 4. 方法论构建：从概念到落地\n为了验证上述假设并填补评估盲区，作者设计了IDRBench，其构建逻辑遵循以下步骤：\n\n#### A. 数据构建：如何逼真地模拟“需要交互”的场景？\n*   **挑战：** 现有的高质量数据集（如DeepResearch Bench）中的Query往往非常详细，智能体直接执行即可，不需要交互。\n*   **创新思路（模糊性注入）：** 作者决定人为制造“信息差”。通过LLM将原本详细的Query进行压缩（摘要化），保留核心意图但移除具体细节。\n*   **逻辑：** 只有当任务变得“模糊”时，智能体才被迫主动提问，从而触发交互行为。\n\n#### B. 用户模拟：如何实现大规模、可重复的评估？\n*   **挑战：** 真实的人类交互成本高昂且不可控（主观性强、不一致），无法作为大规模Benchmark的组件。\n*   **创新思路（基于参考的模拟器）：** 构建一个基于参考文档的“用户模拟器”。\n*   **逻辑：** 将参考文档视为“上帝视角”的真理。模拟器被设定为：像人类一样简洁回答，提供宏观指导，且拒绝错误选项。这样既保证了反馈的合理性，又实现了评估的标准化。\n\n#### C. 评估体系：如何定义“好的交互”？\n*   **思路：** 交互是一把双刃剑，必须建立多维度的评估指标。\n*   **维度拆解：**\n    1.  **收益：** 交互是否提升了质量？（语义相似度、结构覆盖度、意图满足度）。\n    2.  **成本：** 交互是否太烦人？（交互轮数、消耗的Token数）。\n*   **逻辑：** 只有同时考察这两个维度，才能判断一个智能体是否具备高效的“交互智能”。\n\n#### D. 框架设计：如何让智能体具备交互能力？\n*   **思路：** 基于现有的多智能体架构（规划、研究、生成），嵌入“交互模块”。\n*   **机制设计：**\n    *   **评估器：** 决定“何时”提问（权衡信息增益与打扰成本）。\n    *   **提问器：** 决定“问什么”（生成针对性的澄清问题）。\n*   **逻辑：** 交互不应是随机的，而应是基于当前上下文不确定性的理性决策。\n\n### 5. 实验验证与洞察提炼\n**最终验证：** 通过在多个SOTA模型上的实验，作者验证了最初的假设并发现了更深层的规律。\n*   **发现一：** 交互确实能普遍提升质量，且**交互能力有时比模型本身的原始智力更重要**（例如，开启交互的弱模型可能超过自主运行的强模型）。\n*   **发现二：** 存在**边际递减效应**。强模型通过交互获得的提升较小，而弱模型提升巨大。\n*   **发现三：** 交互策略存在差异。有的模型倾向于“频繁短问”，有的倾向于“少量长问”，这揭示了不同模型在交互效率上的权衡。\n\n### 总结\n作者的思考路径是一个典型的**“观察现象 -> 识别缺陷 -> 提出假设 -> 构建工具（Benchmark） -> 验证假设”**的学术闭环。\n\n其核心贡献不在于发明了一个新的聊天机器人，而在于**重新定义了深度研究的评估范式**——从“静态的结果导向”转向了“动态的过程导向”，并巧妙地通过“模糊性注入”和“用户模拟”解决了交互式系统难以量化评估的难题。"
                },
                {
                    "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
                    "arxiv_id": "2601.06636",
                    "authors": "Wenting Chen, Zhongrui Zhu, Guolin Huang, Wenxuan Wang",
                    "summary": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了名为 ECR-Agent 的智能体架构，该架构包含“Critic-Driven Graph and Memory Evolution (CGME)”组件，涉及记忆存储和迭代完善，符合“单智能体”中的记忆机制以及“自我演化”的研究范围。尽管论文应用于医疗领域，但其核心贡献在于智能体的架构设计（动态因果推理、记忆演化），而非单纯的应用部署。",
                    "summary2": "本文旨在解决医疗大模型在临床诊断中因依赖统计捷径而产生的Einstellung Effect（思维定势效应）。针对医疗诊断场景，我们提出了MedEinst基准测试及ECR-Agent框架。该框架通过Dynamic Causal Inference (DCI)和Critic-Driven Graph & Memory Evolution (CGME)实现基于循证医学的结构化因果推理。我们在MedEinst数据集上通过Bias Trap Rate和Robust Accuracy等指标验证了其有效性，显著降低了模型的误判率。",
                    "summary_translation": "尽管在医学基准测试中取得了高准确率，LLMs（大语言模型）在临床诊断中表现出 Einstellung Effect（定势效应）——即依赖统计捷径而非患者特异性证据，导致在非典型病例中出现误诊。现有的基准测试未能检测到这种关键的失效模式。我们提出了 MedEinst，这是一个包含 49 种疾病共 5,383 对临床病例的反事实基准。每一对病例包含一个对照病例和一个“陷阱”病例，后者通过改变鉴别性证据从而翻转诊断结果。我们通过 Bias Trap Rate（偏差陷阱率）来衡量易感性——即在正确诊断对照病例的情况下误诊陷阱病例的概率。对 17 个 LLMs 的广泛评估表明，前沿模型虽然达到了很高的基线准确率，但存在严重的偏差陷阱率。因此，我们提出了 ECR-Agent，通过两个组件将 LLM 推理与 Evidence-Based Medicine（循证医学）标准对齐：(1) Dynamic Causal Inference (DCI)（动态因果推理）通过双通路感知、跨越三个层次（关联、干预、反事实）的动态因果图推理以及用于最终诊断的证据审计来执行结构化推理；(2) Critic-Driven Graph and Memory Evolution (CGME)（批评驱动的图与记忆演化）通过将验证过的推理路径存储在范例库中并将疾病特异性知识整合到演化的疾病图中，来迭代地优化系统。源代码即将发布。",
                    "inspiration_trace": "基于论文《MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis》，以下是对作者产出该文章核心方法的逻辑链推演与思考过程还原：\n\n---\n\n### 第一阶段：宏观观察与问题定义（从“高分低能”现象切入）\n\n**1. 观察现象：基准测试成绩与临床实战能力的错位**\n作者首先观察到一个矛盾现象：尽管当前的医学大语言模型在USMLE等标准医学基准测试上取得了极高的准确率，但在处理非典型或复杂的临床病例时，仍频繁误诊。\n*   **思考：** 为什么模型通过了“考试”，却在“看病”时失败？\n\n**2. 归因分析：定势效应的发现**\n作者将这种失败归因为心理学中的“定势效应”。即模型倾向于依赖统计捷径——即训练数据中最常见的症状与疾病的关联模式，而不是针对患者特异性证据进行逻辑推理。\n*   **核心洞察：** 模型在做“概率匹配”而非“因果诊断”。当遇到表面症状符合常见病（如流感），但关键细节指向罕见病（如肺栓塞）的病例时，模型会被先验概率“绑架”，忽略关键的反证证据。\n\n---\n\n### 第二阶段：评估工具的缺失与重构（从“静态知识”到“反事实推理”）\n\n**3. 现有工具的局限性分析**\n作者审视了现有的医学基准（如MedQA, DDXPlus），发现它们大多基于独立同分布（I.I.D.）的样本或典型病例。\n*   **逻辑推演：** 在这些数据集上，统计捷径往往能带来正确答案。因此，现有基准无法检测出模型是否真正具备“推翻直觉、依据证据下结论”的能力。我们需要一种能“诱骗”模型暴露其认知偏见的测试工具。\n\n**4. 构建新基准的假设：MedEinst的设计逻辑**\n为了捕捉定势效应，作者提出必须引入“反事实”思维。\n*   **设计思路：** 构建“对照组”与“陷阱组”病例对。\n    *   **对照组：** 典型病例，符合统计直觉。\n    *   **陷阱组：** 在对照组基础上进行最小化修改，仅替换关键的鉴别证据，使得诊断翻转。\n*   **核心指标：** 提出“偏差陷阱率”。即模型能做对对照组（证明有基础能力），却在陷阱组中坚持对照组诊断（证明被偏见误导）的概率。这成功将“推理能力”与“记忆力”解耦。\n\n---\n\n### 第三阶段：深层原因探究（从“概率拟合”到“循证医学”）\n\n**5. 失败模式的微观剖析**\n通过实验，作者发现即便是GPT-5等前沿模型，在陷阱病例上也表现出极高的错误率。进一步分析发现，模型的思维链存在三种缺陷：盲目（忽略关键证据）、思考不足（未深入分析）和过度思考（为错误结论找借口）。\n*   **思考：** 现有的“思维链”只是线性地合理化直觉，而非真正的验证过程。模型缺乏医生临床决策中的核心框架——循证医学（EBM）。\n\n**6. 理论对标：从相关性到因果性**\n作者意识到，要解决定势效应，必须让模型从Pearl因果层级的第一层（关联/Association）上升到第二层（干预/Intervention）和第三层（反事实/Counterfactual）。\n*   **逻辑演进：** 医生的诊断不是简单的“症状->诊断”映射，而是“症状->证据验证->诊断”的结构化过程。因此，新的方法论必须强制模型执行显式的证据鉴别。\n\n---\n\n### 第四阶段：方法论构建（ECR-Agent的诞生）\n\n**7. 架构设计：模拟EBM认知流程**\n基于上述分析，作者提出了ECR-Agent，旨在将LLM的推理过程与EBM标准对齐。其设计逻辑包含两个核心模块：\n\n*   **模块一：动态因果推理（DCI）—— 解决“怎么想”的问题**\n    *   **双通道感知：** 强制分离“直觉通道”（生成假设）和“分析通道”（提取客观事实），防止直觉过早封闭分析路径。\n    *   **三层因果图推理：**\n        *   *关联层：* 建立初步假设。\n        *   *干预层：* 主动检索鉴别证据，模拟“如果我去检查这个指标会怎样”。\n        *   *反事实层：* 引入“影子节点”，检查“如果这个诊断成立，应该有哪些证据缺失了？”，以此惩罚不完整的推理。\n\n*   **模块二：评论驱动的图与记忆演化（CGME）—— 解决“怎么学”的问题**\n    *   仅仅推理是不够的，系统需要像医生一样积累经验。通过评论模型反馈，将验证过的推理路径存储为范例，并将疾病知识固化为不断进化的疾病图谱。\n\n---\n\n### 第五阶段：验证与结论（从“规模定律”到“结构变革”）\n\n**8. 实验验证与反直觉发现**\n作者在MedEinst上测试了多种模型，结果证实：模型规模的扩大（Scaling Laws）并没有降低偏差陷阱率，甚至更强的模型因为更自信于统计先验，反而更容易掉进陷阱（“更强的先验，更强的盲目”）。\n*   **最终结论：** 解决医学LLM的定势效应，不能仅靠扩大参数规模，必须进行架构层面的范式转移——从基于统计的概率生成，转向基于证据的因果验证。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“现象观察（高分低能）→ 问题定性（定势效应）→ 工具创新（反事实基准MedEinst）→ 机制归因（缺乏EBM因果推理）→ 方法构建（ECR-Agent结构化验证）”** 的完整闭环。其核心贡献在于指出了LLM在医疗领域“概率拟合”的局限性，并引入因果推理框架作为破局的关键。"
                },
                {
                    "title": "Structured Episodic Event Memory",
                    "arxiv_id": "2601.06411",
                    "authors": "Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang, Baotian Hu",
                    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确提出了针对自主智能体的结构化情景事件记忆（SEEM）框架，旨在解决智能体在长期交互中的记忆组织和动态关联问题，属于单智能体研究中的“记忆”范畴。",
                    "summary2": "本文旨在解决LLM在长期交互中因静态RAG导致的检索分散和缺乏结构依赖的问题。针对连续的交互流，我们提出了一种名为SEEM的分层框架，该框架协同了用于关系事实的Graph Memory Layer和用于叙事进展的Episodic Memory Layer，并引入了Episodic Event Frames (EEFs) 和Reverse Provenance Expansion (RPE) 机制。我们在LoCoMo和LongMemEval benchmarks上通过F1、BLEU-1和Accuracy等指标验证了其有效性，结果显示SEEM显著优于现有基线。",
                    "summary_translation": "目前，大型语言模型中的记忆方法主要依赖于静态的检索增强生成（RAG），这种方法往往导致检索结果零散，且无法捕捉复杂推理所需的结构依赖关系。对于自主代理而言，这些被动且扁平的架构缺乏必要的认知组织能力，难以对长期交互的动态性和联想性进行建模。为解决这一问题，我们提出了结构化情节事件记忆（SEEM），这是一个分层框架，协同整合了用于存储关系事实的图记忆层和用于处理叙事进展的动态情节记忆层。基于认知框架理论，SEEM 将交互流转化为结构化的情节事件框架（EEFs），并通过精确的溯源指针进行锚定。此外，我们引入了一种代理式联想融合机制和反向溯源扩展（RPE）机制，旨在从碎片化证据中重构连贯的叙事语境。在 LoCoMo 和 LongMemEval 基准测试上的实验结果表明，SEEM 显著优于基线模型，使代理能够保持卓越的叙事连贯性和逻辑一致性。",
                    "inspiration_trace": "基于论文《Structured Episodic Event Memory (SEEM)》，以下是对作者构建该方法论的逻辑链推演，旨在还原其从宏观问题观察到微观机制设计的思考过程：\n\n### 1. 宏观问题：智能体的“失忆”与“碎片化”困境\n**观察起点：**\n随着大语言模型（LLM）向自主智能体演进，它们需要处理长期的、动态的交互。然而，LLM 受限于有限的上下文窗口，且缺乏稳定的外部长期记忆系统。\n\n**现有方案的缺陷（痛点）：**\n为了解决记忆问题，业界普遍采用检索增强生成（RAG）。但作者敏锐地观察到，现有的 RAG 系统（无论是基于向量的还是基于图谱的）存在一个核心缺陷——**“碎片化检索”**。\n*   **现象：** 当智能体需要回答复杂问题时，检索到的往往是零散的文本片段或孤立的事实节点。\n*   **后果：** 这些片段缺乏上下文连贯性，无法支撑需要理解事件全貌、时间顺序和因果关系的复杂推理。智能体“只见树木，不见森林”，难以维持叙事的一致性。\n\n### 2. 认知科学假设：模拟人脑的双重记忆机制\n**思维转折：**\n为了解决“碎片化”问题，作者跳出纯计算机视角，转向认知心理学寻找灵感。人脑在处理记忆时并非单一存储，而是存在明确的分工：\n*   **语义记忆：** 存储客观事实、概念和关系（如“巴黎是法国首都”）。\n*   **情景记忆：** 存储特定时间、地点下的个人经历和事件流（如“去年夏天我在巴黎做了什么”）。\n\n**核心假设：**\n如果让智能体也具备这种分层记忆结构——即用**静态的关系图谱**来存储事实，用**动态的情景结构**来存储叙事流——就能从根本上解决上下文断裂的问题。\n\n### 3. 结构化创新：从“文本片段”到“认知框架”\n**具体化挑战：**\n虽然有了分层假设，但如何具体实现“情景记忆”？直接存储原始对话记录依然混乱。作者引入了认知框架理论。\n\n**方法论构建：**\n作者提出将连续的交互流转化为结构化的**情景事件框架**。\n*   **逻辑：** 一个事件不仅仅是文本，它包含参与者、动作、时间、地点、原因等多维属性。\n*   **设计：** 将非结构化的文本解析为具有明确语义槽位的结构化单元（EEF）。这就像把散乱的文字变成了填好的“案件调查表”，使得机器能像人类一样理解事件的要素。\n\n**动态融合机制：**\n现实中的对话是断续的（例如：A问了一半，B回答，A补充）。为了防止记忆碎片化，作者设计了**“联想融合”**机制。如果新的事件与旧的事件在语义上相关（如同一话题的不同轮次），系统会将它们合并为一个连贯的“场景”。这模拟了人类记忆中会将相关经历整合的心理过程。\n\n### 4. 检索机制革新：逆向溯源与上下文重构\n**解决“检索断层”：**\n即使有了结构化的记忆，如何确保检索时不漏掉关键信息？传统的检索是基于关键词匹配的，容易遗漏那些没有直接关键词但属于同一事件上下文的信息。\n\n**逻辑闭环：**\n作者提出了**逆向溯源扩展（RPE）**机制。\n*   **思考路径：** 当用户提问时，系统首先在“图谱层”找到相关的静态事实节点。但这只是线索。\n*   **关键动作：** 利用这些节点作为锚点，反向追踪到它们所属的“情景事件框架（EEF）”。\n*   **最终效果：** 一旦激活了某个事件框架，系统就会把该框架下关联的所有原始文本片段（通过溯源指针）全部召回。这确保了智能体看到的不是孤立的句子，而是整个事件的完整起承转合。\n\n### 5. 总结：逻辑演进的全景图\n作者的思考路径遵循了从**现象观察**（RAG的碎片化） -> **理论借鉴**（认知心理学的双重记忆） -> **结构化建模**（EEF与分层架构） -> **机制完善**（联想融合与逆向溯源）的完整闭环。\n\n**核心思想演进：**\n1.  **发现问题：** 现有记忆是平面的、静态的，导致推理断裂。\n2.  **提出假设：** 记忆需要分层，区分“事实”与“故事”。\n3.  **构建模型：** 用图谱存事实，用框架存故事，并用指针连接两者。\n4.  **优化检索：** 从“找相似文本”转变为“找事件线索，再还原全貌”。\n\n这一过程体现了作者试图赋予 AI 智能体类似人类的“叙事能力”和“长期连贯性”的深层动机。"
                },
                {
                    "title": "Value of Information: A Framework for Human-Agent Communication",
                    "arxiv_id": "2601.06407",
                    "authors": "Yijiang River Dong, Tiancheng Hu, Zheng Hui, Caiqi Zhang, Ivan Vulić, Andreea Bobu, Nigel Collier",
                    "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个基于信息价值（VoI）的决策理论框架，用于解决LLM智能体在信息不足时是直接行动还是向用户提问的决策问题。这属于单智能体的决策与交互机制研究，符合LLM智能体的研究范围。",
                    "summary2": "本文旨在解决LLM智能体在处理未指定请求时，如何在行动与提问间取得平衡的问题。针对未指定的用户查询，我们提出了一种基于决策论中Value of Information (VoI)的框架，动态权衡信息增益与用户认知成本。在20 Questions、Flight Recommendation和Ambiguous WebShop等四个领域上，通过总效用验证了其有效性，该方法无需超参数调整即可达到最优性能。",
                    "summary_translation": "部署用于现实世界任务的 Large Language Model (LLM) agents（大语言模型智能体）面临一个根本性的两难困境：用户的请求往往是信息不足的，但智能体必须决定是依据不完整的信息采取行动，还是打断用户以寻求澄清。现有方法要么依赖于需要针对特定任务进行调优的脆弱 confidence thresholds（置信度阈值），要么未能考虑到不同决策所涉及的不同利害关系。我们引入了一个 decision-theoretic framework（决策理论框架），该框架通过 Value of Information (VoI)（信息价值）解决了这一权衡问题，使智能体能够动态地权衡提问带来的 expected utility gain（预期效用增益）与给用户带来的 cognitive cost（认知成本）。我们的 inference-time（推理时）方法无需 hyperparameter tuning（超参数调优），并且能够跨场景无缝适应——从休闲游戏到医疗诊断。在四个不同领域（20 Questions、医疗诊断、航班预订和电子商务）的实验表明，VoI 始终匹配或超过最佳手动调优的 baselines（基线），在高成本设置中实现了高达 1.36 的效用点提升。这项工作提供了一个用于 adaptive agent communication（自适应智能体通信）的 parameter-free（无参数）框架，该框架明确平衡了 task risk（任务风险）、query ambiguity（查询歧义）和 user effort（用户努力）。",
                    "inspiration_trace": "基于论文《Value of Information: A Framework for Human-Agent Communication》，以下是对作者核心方法论产出逻辑链的系统性推演。这一过程展现了作者如何从现实痛点出发，通过批判性分析现有技术，最终引入决策理论解决人机交互中的根本矛盾。\n\n---\n\n### 第一阶段：宏观困境的识别——“模糊性”与“两难”\n**（观察与问题定义）**\n\n作者的思考始于对现实世界LLM智能体应用场景的观察。作者发现，尽管LLM在执行任务上能力强大，但在面对真实用户时存在一个根本性的**“信息缺口”**：\n*   **用户请求的天然模糊性**：用户的指令往往是欠规范的（如“订一张去伦敦的机票”），隐含了未知的偏好（预算、时间、转机容忍度）。\n*   **智能体的两难困境**：\n    *   **行动**：在信息不全时直接行动，可能导致结果与用户意图不符（任务失败风险）。\n    *   **询问**：通过提问澄清信息，但会打断用户，增加认知负担（用户流失风险）。\n\n**核心思考**：现有的智能体大多假设指令是清晰的，或者仅仅关注“如何执行”，而忽略了“何时该沟通”这一前置决策。作者意识到，**解决这一两难困境是智能体从“工具”进化为“合作伙伴”的关键。**\n\n---\n\n### 第二阶段：对现有范式的批判——“置信度”的失效\n**（假设验证与否定）**\n\n在寻找解决方案时，作者首先审视了学术界和工业界的主流做法，并发现了其逻辑漏洞：\n1.  **固定轮次策略**：无论任务难易都问固定数量的问题。这显然是愚蠢的，因为它忽略了上下文。\n2.  **基于置信度的阈值**：这是目前最先进的自适应方法。当模型对答案的“自信度”低于某个阈值（如0.9）时，就提问。\n\n**作者的批判性洞察**：\n*   **置信度 $\\neq$ 价值**：模型对“猜动物”有90%的把握，和对“诊断癌症”有90%的把握，其含义截然不同。\n*   **缺乏风险感知**：置信度方法只关注“我知道多少”（信息论视角），却忽略了“如果错了后果有多严重”（决策论视角）。在低风险任务（猜动物）中，90%的置信度可能已经足够；但在高风险任务（医疗诊断）中，90%可能意味着致命风险，必须继续提问。\n\n**结论**：单纯依赖模型内部的不确定性估计是片面的，必须引入对**任务风险**和**决策后果**的考量。\n\n---\n\n### 第三阶段：理论视角的转换——从“信息获取”到“理性决策”\n**（理论引入与框架构建）**\n\n为了解决上述缺陷，作者将视角从计算机科学转向了认知科学与决策理论，提出了核心假设：\n*   **沟通即决策**：提问不应仅仅是为了获取信息，而应被视为一种“行动”。这种行动有成本（认知负荷），也有收益（提升决策质量）。\n*   **理性言语行为**：借鉴RSA框架，智能体应当是“理性”的，即只有当提问带来的**预期效用提升**大于**提问成本**时，才应该进行沟通。\n\n**逻辑推演**：\n我们需要一个数学工具来量化“提问到底值不值”。作者引入了经典的**信息价值**理论。\n*   **定义**：VoI = (获得信息后的预期效用) - (当前信息下的预期效用)。\n*   **决策规则**：如果 $VoI > \\text{提问成本}$，则提问；否则，直接行动。\n\n这一转换将问题从“我不确定吗？”（模糊逻辑）变成了“值得去弄清楚吗？”（经济逻辑）。\n\n---\n\n### 第四阶段：方法论的落地——LLM驱动的贝叶斯模拟\n**（从理论到实践的映射）**\n\n有了VoI理论框架，接下来的挑战是如何让LLM在推理时计算出这个值。作者设计了一套无需训练的推理时算法：\n\n1.  **信念分布**：\n    *   *思考*：LLM通常只输出一个确定答案，但计算VoI需要概率。\n    *   *方案*：强制LLM输出对用户潜在意图（如偏好、疾病类别）的概率分布 $b(\\theta)$。\n\n2.  **前瞻性模拟**：\n    *   *思考*：在问出问题前，智能体需要预判“如果我问了，用户可能怎么答，以及回答后我的效用会变多少”。\n    *   *方案*：利用LLM的生成能力进行“反事实模拟”。针对候选问题 $q$，枚举可能的回答 $y$，模拟更新信念分布 $b(\\theta|y)$，并计算对应的效用。\n\n3.  **动态权衡**：\n    *   *思考*：如何整合风险和成本？\n    *   *方案*：在VoI公式中显式引入任务风险（通过效用函数 $U(\\theta, a)$ 的量级体现）和认知成本（常数 $c$）。\n\n---\n\n### 第五阶段：验证逻辑——自适应性与零参数优势\n**（实验设计与预期验证）**\n\n最后，作者通过实验验证这一逻辑链条的有效性，其验证逻辑紧扣之前的批判：\n*   **跨场景泛化**：选择“猜动物”（低风险）、“医疗诊断”（高风险）、“订票”（多属性偏好）等不同场景，证明VoI能自动适应不同的风险等级。\n*   **对比基线**：专门对比“置信度阈值”方法。\n    *   *预期结果*：置信度方法需要针对每个任务手动调整阈值（脆弱），而VoI方法无需调参（鲁棒）。\n    *   *逻辑闭环*：在高风险场景下，VoI会因为潜在收益巨大而倾向于多问；在低风险或高沟通成本场景下，VoI会自动停止提问。\n\n---\n\n### 总结：作者的思维演进图谱\n\n1.  **痛点**：用户指令模糊，智能体在“瞎猜”和“烦人”之间进退维谷。\n2.  **反思**：现有的“自信度”机制只看不确定性，不看后果，无法区分“猜错猫”和“误诊癌症”的区别。\n3.  **升维**：引入决策论，将沟通视为一种投资，必须计算ROI（投资回报率）。\n4.  **工具**：采用**信息价值**作为核心指标，量化“提问”带来的预期收益。\n5.  **实现**：利用LLM自身的推理能力进行信念估计和未来模拟，实现无需训练的动态决策。\n\n这一逻辑链条展示了作者如何从具体的交互体验出发，通过跨学科的理论融合，最终构建出一个既符合人类直觉又具备数学严谨性的通用框架。"
                },
                {
                    "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning",
                    "arxiv_id": "2601.06282",
                    "authors": "Yue Zhou, Xiaobo Guo, Belhassen Bayar, Srinivasan H. Sengamedu",
                    "summary": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",
                    "category": "cs.CL",
                    "filter_reason": "该论文专注于解决长期对话智能体的记忆问题，提出了通过智能体推理构建结构化记忆（情景记忆和语义记忆）的框架。这属于单智能体研究中的“记忆”范畴，符合筛选条件。",
                    "summary2": "本文旨在解决长期对话代理中现有记忆框架缺乏连贯性且计算成本高昂的问题。针对长对话场景，我们提出了一种名为Amory的工作记忆框架，通过主动构建情景叙事、动量感知整合和语义化外围事实来组织记忆，并在LOCOMO benchmark上通过LLM-as-a-Judge分数和响应延迟验证了其有效性。",
                    "summary_translation": "随着交互时间的延长，长期对话代理面临着一个根本的可扩展性挑战：重复处理整个对话历史在计算上变得不可行。目前的解决方案试图通过记忆框架来解决这一问题，这些框架主要将对话分割为孤立的 embeddings（嵌入向量）或 graph representations（图表示），并以 RAG（检索增强生成）的方式检索相关信息。尽管这些方法在计算上效率较高，但它们往往对记忆形成过程的处理过于简单，无法捕捉人类记忆的微妙之处和连贯性。我们提出了 Amory，这是一个 working memory（工作记忆）框架，它通过在 offline time（离线时间）期间增强 agentic reasoning（智能体推理）来主动构建结构化的记忆表示。Amory 将对话片段组织成 episodic narratives（情景叙事），利用 momentum（动量）巩固记忆，并将 peripheral facts（外围事实）转化为 semantic memory（语义记忆）。在检索阶段，系统在叙事结构上采用 coherence-driven reasoning（连贯性驱动推理）。在针对长期推理的 LOCOMO benchmark（基准测试）上的评估表明，Amory 相比于先前的 state-of-the-art（最先进水平）取得了显著改进，其性能与 full context reasoning（全上下文推理）相当，同时将响应时间缩短了 50%。分析表明，momentum-aware consolidation（动量感知巩固）显著提升了响应质量，而 coherence-driven retrieval（连贯性驱动检索）相比基于 embeddings 的方法提供了更优越的 memory coverage（记忆覆盖率）。",
                    "inspiration_trace": "基于论文《Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题到具体解决方案的思考过程：\n\n---\n\n### 1. 宏观问题：长对话中的“质量-效率”悖论\n**观察起点：**\n随着大语言模型（LLM）在长对话场景中的应用，一个根本性的瓶颈浮出水面：**上下文窗口的有限性与对话历史无限增长之间的矛盾**。\n*   **现状：** 如果每次都处理全量历史，计算成本极高且响应缓慢。\n*   **现有解法（RAG范式）：** 为了解决效率问题，主流方法采用检索增强生成（RAG），将对话切片存入向量数据库或图结构，按需检索。\n*   **痛点识别：** 作者发现，这种“碎片化”的存储方式虽然快，但丢失了对话的**上下文连贯性**。它将记忆视为孤立的“数据点”，而非有逻辑的“体验”，导致模型难以进行复杂的推理（如多跳问题或时间推理）。\n\n### 2. 认知视角的引入：从“存储”转向“体验”\n**理论假设：**\n为了解决碎片化问题，作者将目光转向认知科学，试图寻找人类记忆的运作机制作为灵感。\n*   **核心洞察：** 人类记忆不是简单的关键词索引，而是基于**叙事**的。\n    *   **情景记忆：** 记住的是“故事”，包含情节、人物和因果链条。\n    *   **语义记忆：** 提取出的去语境化事实。\n    *   **记忆巩固：** 记忆不是静态的，而是随着时间推移从不稳定状态重组为稳定结构。\n*   **推论：** 如果AI能像人类一样，将对话碎片组织成连贯的“故事”，并在非活跃时间进行“巩固”，就能在保持效率的同时，大幅提升记忆的可用性和推理深度。\n\n### 3. 关键转折：利用“离线智能体推理”构建结构\n**技术难点：**\n要构建复杂的叙事结构，需要LLM进行深度的逻辑推理。然而，在用户提问的“在线”阶段进行这种推理会带来不可接受的延迟。\n*   **策略选择：** 作者提出了一个关键的时间维度分离策略——**“离线构建，在线检索”**。\n*   **核心假设：** 利用对话的自然间隙（离线时间），让智能体主动去“思考”和“整理”记忆。这样既利用了LLM的推理能力，又不影响实时响应速度。\n\n### 4. 方法论构建：动态演进的叙事记忆\n基于上述假设，作者设计了一套动态的记忆构建流程，模拟人类认知的三个阶段：\n\n*   **阶段一：叙事化组织**\n    *   *思考：* 对话不是杂乱无章的，而是围绕特定主题展开的。\n    *   *设计：* 将对话片段绑定到“情景记忆”中，形成层级结构（主情节 -> 子情节 -> 片段）。这解决了碎片化问题，赋予了记忆骨架。\n\n*   **阶段二：动量感知的巩固**\n    *   *思考：* 对话有“热度”。当一个话题被反复讨论时（活跃态），不应急于总结；当话题转移后（非活跃态），才是重组记忆的最佳时机。\n    *   *设计：* 引入“对话动量”概念。仅在记忆进入非活跃状态时，触发LLM对情节进行重组和概括（Consolidation）。这模拟了人类在事后反思并固化记忆的过程。\n\n*   **阶段三：语义化剥离**\n    *   *思考：* 并非所有信息都属于故事。有些是琐碎的事实（如“某人住在哪”），它们不需要上下文即可被理解。\n    *   *设计：* 将与主情节逻辑关联不大的边缘事实，提取为结构化的三元组存入“语义记忆”。这实现了叙事与事实的分离，提高了检索的精准度。\n\n### 5. 检索范式革新：连贯性驱动\n**最后一步：**\n既然记忆是结构化的叙事，检索方式也必须升级。\n*   *批判：* 传统的向量相似度检索无法理解逻辑关系（例如，用户问“John为什么喜欢篮球？”，向量检索可能只匹配到“篮球”这个词，而忽略了“职业发展”这个潜在情节）。\n*   *设计：* 采用**连贯性推理检索**。让LLM基于情节标题和人物关系进行逻辑判断，而非简单的向量匹配。这确保了检索到的不仅是“相似”的内容，更是“逻辑相关”的上下文。\n\n### 6. 总结：逻辑链的闭环\n作者的思考路径完成了一个闭环：\n1.  **发现问题：** 现有RAG方法虽然快，但记忆太碎，推理能力差。\n2.  **寻找灵感：** 人类通过叙事和巩固来形成高质量记忆。\n3.  **提出假设：** 利用LLM的推理能力，在离线阶段主动构建叙事结构。\n4.  **细化机制：** 通过“绑定-巩固-语义化”三步走，动态管理记忆的演进。\n5.  **验证效果：** 实验证明，这种方法在保持低延迟的同时，显著提升了长对话中的推理质量，接近全量上下文的效果。\n\n这一过程体现了作者从**工程痛点**出发，借鉴**认知科学理论**，最终通过**巧妙的时空分离设计（离线推理/在线检索）**实现了方法论落地的完整逻辑演进。"
                },
                {
                    "title": "Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms",
                    "arxiv_id": "2601.06039",
                    "authors": "Yueze Liu, Ajay Nagi Reddy Kumdam, Ronit Kanjilal, Hao Yang, Yichi Zhang",
                    "summary": "Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja",
                    "category": "cs.CL",
                    "filter_reason": "该论文明确提出了VEJA框架，旨在通过改进数据策览来增强角色扮演智能体的内部状态（价值观、经历、判断、能力）和推理能力。这属于单智能体研究范畴，涉及智能体的记忆、自我反思和行为建模，旨在提升智能体的深度和叙事连续性。",
                    "summary2": "本文旨在解决现代角色扮演模型缺乏真实角色深度及内心冲突推理的问题。针对现有训练范式的局限性，我们提出了一种VEJA框架（Values, Experiences, Judgments, Abilities），用于指导数据策划。我们在基于角色Makise Kurisu的数据集上，通过LLM-as-judge的盲A/B测试验证了其有效性，结果显示VEJA策划的数据在角色一致性和叙事连续性上显著优于合成基线。",
                    "summary_translation": "现代角色扮演模型日益精密，但始终难以捕捉可信且引人入胜角色的本质。我们认为，这一缺陷归因于训练范式忽视了角色内心世界的动态相互作用。当前的方法，包括检索增强生成、基于事实的提示、基于文学的学习以及合成数据生成，在建模定义人类互动的深思熟虑且充满价值冲突的推理方面，均表现出反复出现的局限性。在本文中，我们确定了对于角色真实性至关重要的四个核心概念：价值观、经历、判断和能力。我们提出 VEJA 框架作为一种新的数据策展范式，旨在解决这些系统性局限。为了展示本框架所能达到的质量上限，我们进行了一项试点研究，将人工策展的基于 VEJA 的数据集与最先进的合成基线进行了比较。利用大语言模型评判法，我们的研究结果显示出显著的质量差距，这表明转向以概念为基础的数据策展（如 VEJA 所体现的那样），对于创建具有真正深度和叙事连贯性的角色扮演智能体是必要的。完整数据集可在 https://github.com/HyouinKyoumaIRL/Operation-Veja 获取。",
                    "inspiration_trace": "基于论文《Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms》，以下是对作者产出该文章核心方法（VEJA框架）的逻辑链推演：\n\n### 第一阶段：现象观察与核心痛点识别\n**（从“模型能说话”到“模型没有灵魂”）**\n\n1.  **宏观观察**：作者发现，尽管现代角色扮演模型越来越复杂，能够生成流畅的对话，但它们始终缺乏“令人信服的、引人入胜的角色本质”。\n2.  **具体案例触发**：作者在尝试构建高保真角色（如《命运石之门》中的牧濑红莉栖）时发现，现有模型无法复刻其核心特质——即“求知欲”与“社交戒备心”之间的冲突。模型的反应仅仅是条件反射式的，缺乏内在的驱动力。\n3.  **核心假设提出**：作者认为，问题的根源不在于模型参数不够大，而在于**训练范式**忽视了角色内心世界的动态相互作用。人类互动不是检索“正确”答案，而是**冲突价值的协商**。\n\n### 第二阶段：对现有范式的批判性解构\n**（为什么当前主流方法都失效了？）**\n\n为了验证假设，作者系统性地解剖了当时四种主流的角色建模方法，试图找出它们共同的缺陷：\n\n1.  **检索增强生成（RAG）的局限**：RAG擅长处理事实，但人类的价值体系是组合爆炸的。试图用检索列表来穷举一个角色在所有情境下的价值判断是不可能的。\n2.  **基于事实的价值预设的局限**：为了通过基准测试，现有方法倾向于将价值简化为孤立的公式（如“对陌生人开放”）。这导致模型在对话中过度索引单一特征，忽略了语境和平衡，显得机械且缺乏分寸。\n3.  **基于文学生成的局限**：文学名著虽然包含深度，但对话只“暗示”了思维过程，而非“显式”展示。模型无法从对话文本中反向推导出角色复杂的内心 deliberation（ deliberative reasoning）。此外，文学中的“经验”通常通过旁白而非对话传递，导致模型难以学会“以史为鉴”。\n4.  **合成数据生成的死循环**：这是最致命的陷阱。试图用现有的强模型（如GPT-4）生成高质量角色数据是行不通的，因为**生成者本身就不具备处理复杂价值冲突的能力**。这导致了一个递归的质量天花板。\n\n### 第三阶段：理论重构与VEJA框架的诞生\n**（回归戏剧艺术，重建角色的“因果逻辑”）**\n\n在否定了现有技术路径后，作者转向经典戏剧理论（如斯坦尼斯拉夫斯基体系），试图从第一性原理出发定义什么是“真实的角色”。\n\n1.  **寻找基本单元**：作者认为，要模拟角色的深度，必须显式地建模其内心逻辑。通过数据整理过程中的观察，作者提炼出四个核心概念：\n    *   **Values (价值观)**：行为的根本动机（Why）。\n    *   **Experiences (经历)**：塑造价值观和判断的过去事件（Evidence）。\n    *   **Judgments (判断)**：价值观经过经历过滤后形成的具体观点（Output）。\n    *   **Abilities (能力)**：表达上述特质的知识和技能工具（Toolkit）。\n\n2.  **建立因果链条**：这四个要素不是孤立的标签，而是一个严密的**因果闭环**：\n    *   经历塑造价值观；\n    *   价值观与经历共同产出判断；\n    *   判断通过能力表达出来。\n    *   *逻辑演进点*：作者意识到，只有强制数据遵循这个因果链，才能让模型学会“像人一样思考”，即基于过去（E）和动机（V）来形成当下的观点（J），而不仅仅是模仿语气。\n\n### 第四阶段：验证与范式转移\n**（证明“人+框架”优于“纯模型合成”）**\n\n1.  **实验设计的逻辑**：既然现有模型无法生成高质量数据，那么“人类作者”是否就是答案？为了验证这一点，作者设计了一个对比实验：**纯模型生成** vs. **VEJA框架指导的人类写作**。\n2.  **结果解读**：实验结果显示，VEJA指导的人类数据显著优于SOTA合成数据。这证明了作者的核心论点：**当前的技术瓶颈不在于算力，而在于数据的“概念深度”**。\n3.  **最终结论**：作者提出，社区需要从“构建更好的鹦鹉”（模仿表面）转向“创造真正的数字心智”（模拟内在）。VEJA不仅仅是一个数据标注框架，更是一种新的训练范式，它要求我们在数据构建阶段就必须显式地包含角色的内心冲突和推理过程。\n\n---\n\n**总结：**\n作者的思考路径是从**“体验到的肤浅感”**出发，经过**“对技术路径的证伪”**，回归**“对人性和戏剧艺术的本体论思考”**，最终提炼出**“VEJA因果模型”**，并通过实验确立了**“概念驱动数据”**优于**“纯合成数据”**的新范式。"
                },
                {
                    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
                    "arxiv_id": "2601.07779",
                    "authors": "Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",
                    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了一个名为 OS-Symphony 的计算机使用智能体框架，核心研究内容包括智能体的记忆机制、自我反思、工具使用以及多智能体协作，完全符合 LLM 智能体的研究范围。虽然涉及视觉模型，但重点在于智能体架构而非视觉模型本身。",
                    "summary2": "本文旨在解决计算机使用代理在长时程任务中鲁棒性不足及新领域泛化能力差的问题。针对复杂的桌面自动化场景，我们提出了一种名为OS-Symphony的整体框架，该框架集成了利用里程碑驱动长期记忆的Reflection-Memory Agent和采用See-Act范式的Multimodal Searcher。并在OSWorld、WindowsAgentArena和MacOSArena基准上通过Step Success Rate验证了其有效性，实现了SOTA性能。",
                    "summary_translation": "尽管 Vision-Language Models (VLMs，视觉语言模型) 显著推动了 Computer-Using Agents (CUAs，计算机使用代理) 的发展，但现有框架在长时程工作流的鲁棒性以及在新领域的泛化能力方面仍面临挑战。这些局限性主要归因于对历史视觉上下文筛选缺乏细粒度控制，以及缺乏视觉感知的教程检索机制。为弥合这些差距，我们提出了 OS-Symphony，这是一个包含 Orchestrator (编排器) 的整体框架，该编排器协调两项关键创新以实现鲁棒的自动化：(1) Reflection-Memory Agent (反思记忆代理)，利用里程碑驱动的长期记忆实现轨迹级自我修正，有效缓解长时程任务中的视觉上下文丢失；(2) Versatile Tool Agents (多功能工具代理)，其特色在于包含一个 Multimodal Searcher (多模态搜索器)，该搜索器采用 SeeAct 范式在基于浏览器的沙箱中进行导航，以合成实时的、视觉对齐的教程，从而解决未见场景中的保真度问题。实验结果表明，OS-Symphony 在不同模型规模下均带来了显著的性能提升，在三个在线基准测试中确立了新的 State-of-the-Art (SOTA，最先进) 结果，尤其在 OSWorld 上达到了 65.84% 的成绩。",
                    "inspiration_trace": "基于论文《OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent》的内容，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方案构建的思考过程：\n\n---\n\n### 第一阶段：宏观观察与问题界定\n**思考起点：** 尽管视觉语言模型（VLMs）推动了计算机代理（CUA）的发展，但现有的代理框架在实际应用中仍存在显著的脆弱性。\n**核心矛盾：** 当前的CUA在两个关键维度上表现不佳：\n1.  **长时程任务的鲁棒性：** 在需要多步骤、跨应用的复杂工作流中，代理容易迷失方向或陷入死循环。\n2.  **新领域的泛化能力：** 面对未见过的软件或环境（OOD场景），代理缺乏必要的知识储备，无法有效执行任务。\n\n### 第二阶段：微观诊断与归因分析\n**思考深入：** 为什么现有的模块化或端到端框架无法解决上述矛盾？作者通过分析现有架构的局限性，识别出两个具体的“技术断层”：\n\n**断层一：视觉上下文的“失忆”**\n*   **观察：** 现有的记忆机制（如简单的滑动窗口或文本摘要）缺乏对历史视觉信息的精细化管理。\n*   **推论：** 在长任务中，屏幕截图包含大量冗余信息，直接存储会撑爆上下文窗口，而简单丢弃又会丢失关键状态。这种“视觉上下文丢失”导致代理无法回溯历史，从而无法识别意图漂移或循环行为等错误，失去了自我纠错的基础。\n\n**断层二：检索增强的“视觉盲区”**\n*   **观察：** 为了解决泛化问题，现有方法引入了检索增强生成（RAG）。但这些方法多依赖纯文本检索或静态知识库。\n*   **推论：** GUI任务本质上是视觉的。纯文本检索无法捕捉界面布局、图标样式等视觉语义，导致检索到的教程与当前屏幕状态不匹配（保真度低）。此外，静态知识库更新成本高，难以适应新软件的快速迭代。\n\n### 第三阶段：核心假设与策略提出\n**思考转折：** 要解决上述断层，必须从“被动处理”转向“主动感知与压缩”。作者提出了两个核心假设：\n\n1.  **关于记忆的假设：** 如果能设计一种机制，只保留具有里程碑意义的关键截图，并基于这些视觉证据生成轨迹级的反思，就能在压缩上下文的同时保留纠错能力。\n2.  **关于泛化的假设：** 如果代理能像人类一样，在遇到不懂的操作时主动打开浏览器进行“视觉搜索”，通过实际浏览网页来合成与当前环境视觉对齐的教程，就能解决静态知识库的滞后和文本检索的盲区。\n\n### 第四阶段：方法论构建与系统设计\n**思考落地：** 基于上述假设，作者构建了 **OS-Symphony** 这一整体框架，其逻辑架构体现了“分工协作”的思想：\n\n**1. 设计“指挥官”：**\n*   **逻辑：** 系统需要一个核心大脑来负责任务理解和动作调度，同时协调其他模块。\n*   **角色：** Orchestrator（编排器）。它只关注短期记忆（最近K步）和来自其他模块的高级指令，保持决策的敏捷性。\n\n**2. 构建“反思者与记忆库”：**\n*   **逻辑：** 针对“视觉上下文丢失”，需要一个专门的模块来管理长期记忆和进行错误审计。\n*   **方案：** **Reflection-Memory Agent (RMA)**。\n    *   **里程碑机制：** 不存储所有截图，而是通过算法判断哪些步骤是“里程碑”（如状态发生重大改变），只保留这些关键帧。\n    *   **结构化反思：** RMA 审计历史轨迹，通过结构化的消息协议向 Orchestrator 反馈状态（如：On-track, Off-track, GUI Error, Lack of Tutorial），从而实现轨迹级的自我纠正。\n\n**3. 打造“全能工具箱”：**\n*   **逻辑：** 针对“视觉盲区”和执行效率问题，需要专门的工具来处理特定类型的任务。\n*   **方案：** **Versatile Tool Agents**。\n    *   **多模态搜索者：** 这是一个核心创新。它采用“See-Act”范式，在一个隔离的浏览器沙箱中自主导航，阅读网页并合成包含视觉描述的教程。这解决了传统RAG缺乏视觉感知的问题。\n    *   **定位器与编码器：** 分别负责UI元素的精确定位和系统级的代码操作，弥补纯GUI操作的不足。\n\n### 第五阶段：逻辑闭环与验证\n**思考总结：** 整个框架形成了一个闭环：\n*   Orchestrator 执行任务；\n*   遇到困难或错误时，RMA 通过视觉审计发现并反馈；\n*   如果是知识缺失，Searcher 主动上网寻找视觉教程；\n*   最终完成任务并更新记忆。\n\n**结论：** 这种设计通过**精细化的视觉记忆管理**解决了长时程任务的鲁棒性问题，通过**主动的视觉搜索**解决了新领域的泛化问题，从而在多个基准测试中实现了SOTA性能。\n\n---\n\n**总结：** 作者的思考路径是从**宏观的能力缺失**（鲁棒性与泛化性）出发，深入到**微观的信息处理缺陷**（视觉记忆丢失与检索视觉盲区），进而提出**主动化与结构化**的解决策略（里程碑记忆与视觉搜索），最终通过**多智能体协作**的架构实现了逻辑落地。"
                },
                {
                    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
                    "arxiv_id": "2601.07641",
                    "authors": "Jiaxuan Lu, Ziyu Kong, Yemin Wang, Rong Fu, Haiyuan Wan, Cheng Yang, Wenjie Lou, Haoran Sun, Lilong Wang, Yankai Jiang, Xiaosong Wang, Xiao Sun, Dongzhan Zhou",
                    "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
                    "category": "cs.CL",
                    "filter_reason": "该论文提出了Test-Time Tool Evolution (TTE)框架，旨在解决LLM智能体在科学推理任务中依赖静态工具的问题。它属于单智能体研究范畴，重点探讨了智能体的工具使用和自我演化（工具演化）能力，符合筛选条件中关于单智能体和自我演化的定义。",
                    "summary2": "本文旨在解决静态工具库在科学推理中覆盖不足和适应性差的问题。针对开放式科学计算任务，我们提出了一种Test-Time Tool Evolution (TTE)框架，通过动态合成、验证和演化可执行工具来增强智能体能力。我们在SciEvo、SciBench等benchmark上通过Accuracy和Tool Reuse Rate验证了其有效性，显著提升了推理准确率和工具复用效率。",
                    "summary_translation": "AI for Science（科学智能）的核心挑战不仅在于单纯的推理，更在于在开放式的科学世界中创造计算方法的能力。现有的 LLM-based agents（基于大语言模型的智能体）依赖于静态的、预定义的工具库，这种范式在工具稀疏、异构且本质上不完整的科学领域中根本无法奏效。在本文中，我们提出了 Test-Time Tool Evolution (TTE，测试时工具演化)，这是一种新的范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，TTE 克服了静态工具库的僵化性和长尾局限性。为了进行严格的评估，我们引入了 SciEvo，这是一个包含 1,590 个科学推理任务的 benchmark（基准），并由 925 个自动演化的工具提供支持。大量实验表明，TTE 在准确率和工具效率方面均达到了最先进的性能，同时实现了计算工具的有效 cross-domain adaptation（跨域适应）。代码和 benchmark 已在 https://github.com/lujiaxuan0520/Test-Time-Tool-Evol 发布。",
                    "inspiration_trace": "基于论文《Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从观察到创新的思考过程：\n\n### 1. 宏观观察：AI for Science 的“严谨性鸿沟”\n**思考起点：** 作者首先审视了当前大语言模型（LLM）在科学领域的应用现状。\n*   **现象：** LLM 拥有强大的推理能力，但在处理科学问题时，其概率性的本质往往导致“幻觉”，缺乏科学研究所必须的精确计算和严谨逻辑。\n*   **现有解法：** 业界通用的做法是给 LLM 配备外部工具（如计算器、API），即“工具增强”。\n*   **初步质疑：** 这种“LLM + 工具”的模式虽然解决了通用领域的部分问题，但在真正的科学研究中，是否足够？\n\n### 2. 问题诊断：静态工具库的“长尾困境”\n**深入分析：** 作者进一步剖析了现有工具增强范式在科学领域的根本缺陷。\n*   **核心矛盾：** 科学世界是开放、无边界的，而现有的工具库是**静态**且**预定义**的。\n*   **两大瓶颈：**\n    1.  **稀疏性与异构性：** 科学计算工具分散且非标准化，无法像通用 API 那样通过爬取构建一个“全知”的静态库。\n    2.  **不可预知性：** 科学探索往往涉及新颖的问题，需要全新的计算原语。静态库无法包含尚未被定义的工具。\n*   **结论：** 依赖静态工具库，本质上将 AI 限制在“被动选择者”的角色，无法应对开放的科学问题。这是一个**范式层面**的局限，而非工程细节问题。\n\n### 3. 核心假设：从“工具检索”到“工具进化”\n**范式转换：** 为了解决上述矛盾，作者提出了一个颠覆性的假设。\n*   **假设：** 一个真正的科学智能体，不应该只是从仓库里拿工具，而应该具备**在推理过程中即时创造和演化工具**的能力。\n*   **核心概念：** **Test-Time Tool Evolution (TTE，测试时工具进化)**。\n*   **逻辑推演：** 如果工具库是不完整的，那么它就不应该是固定的资源，而应该是**问题驱动的产物**。工具应该在解决问题的过程中被动态合成、验证并积累。\n\n### 4. 方法论构建：闭环进化机制\n**具体化思考：** 如何实现“工具进化”？作者构建了一个闭环逻辑，将科学方法论的迭代性引入 AI 系统。\n*   **第一步：结构化分解。** 面对复杂问题，不能直接生成代码，而应先将其拆解为原子化的子目标。这是为了精准定位需要什么样的工具。\n*   **第二步：动态检索与合成。** 先看库里有没有，没有就现场写一个。这里的关键是**“按需合成”**。\n*   **第三步：验证与原子化。** 生成的工具不能直接入库，必须经过严格的验证（语法、执行、领域逻辑）。更重要的是，要将复杂的工具拆解为**原子工具**。\n    *   *思考逻辑：* 只有原子化的工具才能被未来不同的问题复用，避免生成大量“一次性脚本”。\n*   **第四步：更新与修剪。** 库不能无限膨胀，需要基于使用频率进行优胜劣汰，保持工具库的高效和紧凑。\n\n### 5. 验证与拓展：零起点与跨域适应\n**场景推演：** 为了证明 TTE 的普适性，作者设定了两个极端的验证场景。\n*   **场景一：TTE-Zero（白板起家）。** 模拟人类科学家从零开始探索。初始工具库为空，看智能体能否在解决问题的过程中，自我演化出一套完整的科学计算工具集。\n*   **场景二：TTE-Adapt（跨域迁移）。** 模拟知识迁移。给智能体一个“材料科学”的工具库，让它去解决“化学”问题。看它能否通过进化，保留通用工具，淘汰不适用工具，并生成新领域的专用工具。\n*   **预期结果：** 如果 TTE 成立，它不仅能解决问题，还能演化出高复用率的核心科学原语。\n\n### 6. 最终愿景：定义“科学智能体”的新标准\n**思想升华：** 作者的思考最终落脚于对 AI 智能体的重新定义。\n*   **总结：** 科学推理的核心不在于参数知识的多寡，而在于**创造计算方法的能力**。\n*   **产出：** 这篇文章不仅仅是提出了一个算法框架，更是确立了“动态工具进化”作为下一代科学 AI 的核心范式。智能体从被动的工具使用者，进化为了主动的方法创造者。\n\n---\n\n**逻辑链总结：**\n**严谨性鸿沟** $\\rightarrow$ **静态工具库的局限性** $\\rightarrow$ **提出“测试时进化”假设** $\\rightarrow$ **构建“分解-合成-验证-原子化”闭环** $\\rightarrow$ **验证零起点与跨域能力** $\\rightarrow$ **确立主动创造的科学智能体范式**。"
                },
                {
                    "title": "LRAS: Advanced Legal Reasoning with Agentic Search",
                    "arxiv_id": "2601.07296",
                    "authors": "Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",
                    "summary": "While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on \"closed-loop reasoning\" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric \"closed-loop thinking\" to dynamic and interactive \"Active Inquiry\". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.",
                    "category": "cs.CL",
                    "filter_reason": "论文提出了“Legal Reasoning with Agentic Search (LRAS)”框架，旨在将模型从静态推理转变为动态的“Active Inquiry”（主动询问）。这涉及智能体行为（搜索/询问）和自我反思（Introspective Imitation Learning），符合单智能体（工具使用、自我反思）的研究范围，尽管应用于法律领域，但其核心贡献在于智能体框架而非单纯的应用。",
                    "summary2": "本文旨在解决现有法律大模型因缺乏知识边界自省而在复杂场景下推理脆弱的问题。针对法律推理任务，我们提出了一种LRAS框架，通过Introspective Imitation Learning和Difficulty-aware Reinforcement Learning实现从“闭环思维”到“主动探究”的转变。我们在LexEval、LawBench等基准上通过准确率验证了其有效性，性能提升达8.2%-32%。",
                    "summary_translation": "尽管 Large Reasoning Models (LRMs，大型推理模型) 在数学领域展现了卓越的逻辑能力，但其在法律领域的应用仍受限于程序严谨性及遵循法律逻辑的严格要求。现有的 legal LLMs（法律大语言模型）依赖于仅源自内部参数化知识的“closed-loop reasoning”（闭环推理），往往缺乏对自身知识边界的认知，从而导致“自信但错误”的结论。为应对这一挑战，我们提出了 Legal Reasoning with Agentic Search (LRAS，基于智能体搜索的法律推理)，这是首个旨在将 legal LLMs 从静态且参数化的“closed-loop thinking”（闭环思维）转变为动态且交互式的“Active Inquiry”（主动探究）的框架。通过整合 Introspective Imitation Learning（内省模仿学习）和 Difficulty-aware Reinforcement Learning（难度感知强化学习），LRAS 赋能 LRMs 识别知识边界并应对法律推理的复杂性。实证结果表明，LRAS 的性能超越 state-of-the-art baselines（最先进的基线模型）8.2-32%，其中在需要基于可靠知识进行深度推理的任务中，提升幅度最为显著。我们将很快公开发布我们的数据和模型，以供进一步探索。",
                    "inspiration_trace": "基于论文《LRAS: Advanced Legal Reasoning with Agentic Search》的内容，以下是对作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观观察与问题界定\n**起点：** 大型推理模型（LRMs）在数学和符号逻辑领域表现出色，但在法律领域却遭遇瓶颈。\n**思考：** 法律领域不同于数学，它不仅需要逻辑，更要求极高的程序严谨性和对法律逻辑的严格遵循。现有的法律大模型大多依赖“闭环推理”，即仅利用模型内部的参数知识进行推断。\n**核心痛点：** 这种“闭卷考试”式的思维模式导致模型缺乏对自身知识边界的认知，经常在不知道答案时依然自信地输出错误结论（即“幻觉”），这在容错率极低的法律场景中是不可接受的。\n\n### 2. 深度诊断与假设验证\n**假设：** 既然模型内部知识不足，引入外部检索（RAG）是否就能解决问题？\n**实验观察：**\n*   **现象一（内省缺失）：** 在模型出错的案例中，虽然有搜索工具可用，但超过70%的情况下模型并未触发搜索。这说明主要问题不在于“缺乏知识”，而在于“缺乏自知之明”——模型不知道自己什么时候该去查资料。\n*   **现象二（复杂场景下的脆弱性）：** 在简单的法律任务上，静态检索（Full RAG）有效；但在需要深度推理的复杂任务上，静态检索的提升非常有限。这表明面对复杂案情，被动地接收检索结果是不够的，模型需要具备主动规划和多步探索的能力。\n\n**结论：** 仅仅给模型“喂”更多数据或简单的检索工具是不够的。必须从根本上改变模型的思维范式，从静态的“闭环思考”转向动态的“主动探究”。\n\n### 3. 范式转移与核心思路\n**核心思想：** 构建一个具有“代理搜索”能力的法律推理框架（LRAS），让模型像人类律师一样：先思考，发现知识盲区，主动检索，验证，再思考。\n**逻辑拆解：** 为了实现这一范式转移，需要解决两个递进的核心问题：\n1.  **“是否搜索”：** 解决内省缺失，让模型学会识别知识边界。\n2.  **“如何搜索”：** 解决复杂场景下的脆弱性，让模型学会在难题中自主规划多步搜索策略。\n\n### 4. 方法论构建\n基于上述逻辑，作者设计了双机制学习架构：\n\n*   **第一阶段：内省式模仿学习**\n    *   **目标：** 解决“是否搜索”的问题。\n    *   **逻辑：** 既然模型不知道自己不知道，那就通过专家示范来教它。通过合成包含“思考-搜索-验证”轨迹的高质量数据，训练模型模仿专家的行为——只有在遇到模糊或关键法律内容时，才主动触发搜索。这赋予了模型“内省”能力。\n\n*   **第二阶段：难度感知强化学习**\n    *   **目标：** 解决“如何搜索”的问题。\n    *   **逻辑：** 模仿学习只能教会基本的动作模式，但在模型依然做不出来的“硬骨头”案例上，需要更强的自主探索能力。作者筛选出SFT模型通过率低的困难样本，利用强化学习（GRPO）进行训练。通过奖励机制，鼓励模型在复杂场景下进行多轮探索和证据综合，从而从被动的“事实核查”进化为主动的“深度推理”。\n\n### 5. 逻辑闭环与验证\n**最终产出：** LRAS框架。\n**验证逻辑：** 实验结果显示，LRAS在需要深度推理的任务上提升显著（8.2%-32%），且在复杂案例中能主动进行多轮搜索并准确区分细微的法律概念（如“负责”与“报告工作”的区别）。这证明了从“被动接收”到“主动探究”的范式转移是提升法律AI推理能力的关键路径。"
                },
                {
                    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
                    "arxiv_id": "2601.07226",
                    "authors": "Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",
                    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
                    "category": "cs.CL",
                    "filter_reason": "论文明确研究了智能体AI系统和智能体工作流，重点评估了工具使用任务和RAG场景下的鲁棒性。它分析了智能体如何处理噪声工具输出，并旨在构建鲁棒的、具备推理能力的智能体，符合单智能体（工具使用、推理）的研究范围。",
                    "summary2": "本文旨在解决推理模型在噪声环境下的鲁棒性问题。针对包含随机文档、无关聊天记录和困难负例的噪声上下文，我们提出了NoisyBench基准和RARE（Rationale-Aware Reward）奖励函数，并在NoisyBench的11个数据集上通过准确率验证了其有效性。",
                    "summary_translation": "推理模型和智能体 AI 系统的最新进展导致了对多样化外部信息的依赖日益增加。然而，这种转变引入了本质上包含噪声的输入上下文，而当前经过净化的基准未能捕捉到这一现实。我们介绍了 NoisyBench，这是一个综合基准，针对包括随机文档、无关聊天历史和困难负样本干扰项在内的多种噪声类型，系统地评估了模型在 RAG (检索增强生成)、推理、对齐和工具使用任务中跨越 11 个数据集的鲁棒性。我们的评估显示，当面临上下文干扰项时，最先进的模型会出现高达 80% 的灾难性性能下降。关键在于，我们发现智能体工作流通常通过过度信任含噪工具输出来放大这些错误，并且即使没有对抗性意图，干扰项也能触发涌现性不对齐。我们发现提示工程、上下文工程、SFT (监督微调) 和仅基于结果奖励的 RL (强化学习) 都无法确保鲁棒性；相比之下，我们提出的 Rationale-Aware Reward (RARE) (理由感知奖励) 通过激励识别噪声中的有用信息，显著增强了韧性。最后，我们发现了一种逆向缩放趋势，即在噪声环境中，增加测试时计算会导致性能下降，并通过注意力可视化证明模型过度关注干扰项标记，这为构建下一代鲁棒且具备推理能力的智能体提供了重要见解。",
                    "inspiration_trace": "基于论文《Lost in the Noise: How Reasoning Models Fail with Contextual Distractors》，以下是对作者核心方法论产出过程的逻辑链推演：\n\n### 第一阶段：宏观观察与问题定义（从“理想”到“现实”）\n\n1.  **观察现状**：\n    *   作者注意到AI范式正从单纯的“对话模型”转向“智能体系统”。这些系统严重依赖外部工具（如RAG、搜索、计算器）和长上下文来解决复杂任务。\n2.  **发现缺口**：\n    *   **理想 vs. 现实**：现有的学术基准测试大多是在“无菌”的清洁数据下进行的。然而，现实世界中的智能体面临的是充满噪声的环境（错误的检索结果、无关的聊天历史、工具输出错误）。\n    *   **假设**：当前SOTA模型在清洁环境下的高分可能掩盖了其在真实噪声环境下的脆弱性。这种脆弱性可能不仅仅是效率问题，而是根本性的推理崩溃。\n\n### 第二阶段：现象验证与深度剖析（“有多脆弱？”）\n\n1.  **构建验证工具**：\n    *   为了验证假设，作者构建了 **NoisyBench**。这是一个系统性的基准，涵盖了RAG、推理、对齐和工具使用四大类任务，并人为注入了三种噪声：随机文档、无关聊天历史、硬负样本。\n2.  **关键发现**：\n    *   **灾难性下降**：即使是顶尖模型（如Gemini-2.5-Pro），在面对噪声时性能也出现了高达80%的断崖式下跌。这证明了“清洁性能强 $\\neq$ 抗噪能力强”。\n    *   **智能体的悖论**：引入智能体工作流（使用工具）在清洁环境下能提升性能，但在噪声环境下反而**放大**了错误。这是因为智能体倾向于“过度信任”工具输出和上下文，导致错误在多步规划中累积。\n    *   **反向缩放定律**：在噪声环境下，增加推理步骤（测试时计算）反而导致性能下降。模型花费更多token去“思考”噪声，结果越想越错。\n\n### 第三阶段：解决方案的试错与迭代（“常规方法为何失效？”）\n\n在确认问题严重性后，作者尝试了现有的主流修复方案，但均遭遇失败：\n\n1.  **尝试一：提示工程与上下文工程**\n    *   **逻辑**：通过优化Prompt或重新组织上下文来引导模型忽略噪声。\n    *   **结果**：**失败**。模型无法通过简单的指令区分信号与噪声，上下文工程本身也容易受到噪声干扰。\n2.  **尝试二：监督微调（SFT）**\n    *   **逻辑**：在包含噪声的数据集（NoisyInstruct）上进行训练，让模型适应噪声。\n    *   **结果**：**失败**。导致了“灾难性遗忘”，模型失去了原有的推理能力，且并未真正学会抗噪。\n3.  **尝试三：基于结果的强化学习（Outcome-based RL）**\n    *   **逻辑**：只对最终答案的正确性进行奖励，让模型自己探索如何在噪声中得出正确答案。\n    *   **结果**：**部分有效但局限**。虽然比SFT好，但模型往往通过“作弊”或依赖内部记忆来得分，而不是真正学会从噪声中提取信息。它无法区分“答对了”是因为“抗噪成功”还是“碰巧蒙对”。\n\n### 第四阶段：核心洞察与方法论形成（从“结果导向”转向“过程导向”）\n\n1.  **核心洞察**：\n    *   作者意识到，单纯奖励“最终答案”是不够的。模型失败的根本原因在于**推理过程**被噪声劫持（注意力机制分析显示模型在错误预测时过度关注干扰项）。\n    *   因此，必须显式地奖励模型在推理过程中**识别并锚定有用信息**的行为，而不仅仅是奖励最终结果。\n2.  **方法论提出：RARE (Rationale-Aware Reward)**\n    *   **逻辑转变**：从“Reward the Outcome”转变为“Reward the Process”。\n    *   **具体机制**：设计一个新的奖励函数，不仅检查最终答案，还检查模型的思维链中是否正确引用或提取了上下文中的**有效参考信息**。\n    *   **作用原理**：通过奖励模型在噪声中“抓取”正确线索的行为，强迫模型学会过滤干扰项。这就像训练学生不仅要写出正确答案，还要在草稿纸上圈出解题依据。\n3.  **最终验证**：\n    *   实验证明，RARE 显著降低了模型被干扰的比例，同时提高了最终准确率。更重要的是，这种方法不仅提升了抗噪性，在清洁环境下也没有性能损失，实现了鲁棒性与通用性的双赢。\n\n---\n\n**总结**：\n作者的思考路径是从**现实应用场景的落差**出发，通过**基准测试量化了“噪声脆弱性”这一现象**，在排除了**提示工程和传统训练方法**的无效性后，抓住了**“推理过程被干扰”这一本质原因**，最终通过**引入过程级奖励（RARE）**，成功引导模型学会了在噪声中“去伪存真”的推理能力。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 7,
            "papers": [
                {
                    "title": "Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting",
                    "arxiv_id": "2601.05606",
                    "authors": "Chen Han, Jin Tan, Bohan Yu, Wenzhen Zheng, Xijin Tang",
                    "summary": "Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.",
                    "category": "cs.MA",
                    "filter_reason": "该论文明确研究了LLM多智能体系统（MAS）中的社会交互、从众动态、网络拓扑以及决策范式（集中式与分布式），属于多智能体协作与通信的研究范畴。",
                    "summary2": "本文旨在研究网络拓扑和自我-社会权重如何影响LLM多智能体系统中的从众动态。针对虚假信息检测任务，我们提出了一种置信度归一化池化规则，通过参数$\\alpha$平衡自我依赖与社会影响，并在Snopes25数据集上通过Central Accuracy、Final Accuracy等指标验证了其有效性。",
                    "summary_translation": "Large Language Models (LLMs，大语言模型) 越来越多地被实例化为 multi-agent systems (MAS，多智能体系统) 中的交互智能体，其中集体决策是通过社会互动而非独立推理产生的。这一过程中一个基本但尚未被充分探索的机制是 conformity (从众)，即智能体将其判断与主流群体意见保持一致的倾向。本文通过一个 misinformation detection task (虚假信息检测任务)，系统研究了 network topology (网络拓扑结构) 如何塑造 LLM-based MAS (基于LLM的多智能体系统) 中的 conformity dynamics (从众动态)。我们引入了一种 confidence-normalized pooling rule (置信度归一化池化规则)，用于控制 self-reliance (自主性) 与 social influence (社会影响) 之间的权衡，从而能够对两种典型的决策范式进行比较：Centralized Aggregation (集中式聚合) 和 Distributed Consensus (分布式共识)。实验结果表明，network topology (网络拓扑结构) 关键性地决定了 collective judgments (集体判断) 的 efficiency (效率) 和 robustness (鲁棒性)。Centralized structures (集中式结构) 能够实现即时决策，但对 hub competence (枢纽节点能力) 敏感，并且表现出 same-model alignment biases (同模型对齐偏差)。相比之下，distributed structures (分布式结构) 促进了更稳健的共识，而 network connectivity (网络连接性) 的增加虽然加快了 convergence (收敛) 速度，但也加剧了 wrong-but-sure cascades (错误但确定的级联) 的风险，即智能体以高置信度收敛于错误决策。这些发现刻画了 LLM-based MAS (基于LLM的多智能体系统) 中的 conformity dynamics (从众动态)，阐明了 network topology (网络拓扑结构) 和 self-social weighting (自我-社会权重) 如何共同塑造集体决策的 efficiency (效率)、robustness (鲁棒性) 和 failure modes (失效模式)。",
                    "inspiration_trace": "基于论文《Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting》，以下是对作者核心方法论产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题界定\n**从“单体智能”到“群体涌现”的视角转变**\n\n1.  **现象观察**：随着LLM的发展，研究热点正从单一模型的推理能力转向多智能体系统（MAS）的协作。现有的MAS研究多关注任务完成的效率（如如何通过辩论提升准确率），却忽略了其中的**社会动力学机制**。\n2.  **核心痛点**：人类群体决策中存在“从众心理”，即个体倾向于调整判断以符合群体意见。在LLM构成的MAS中，这种“从众”是如何发生的？它是有助于消除噪声，还是会引发错误的信息级联？\n3.  **研究问题确立**：作者不再将LLM视为孤立的推理器，而是将其视为社会网络中的节点。核心问题转化为：**网络拓扑结构（谁和谁连接）与自我-社会权重（听自己的还是听别人的）如何共同塑造LLM群体的从众动态？**\n\n### 第二阶段：理论假设与机制抽象\n**将社会心理学概念转化为可计算模型**\n\n1.  **借鉴经典理论**：作者回顾了经典的舆论动力学模型（如DeGroot模型），但指出这些模型缺乏对LLM语义推理能力的刻画。\n2.  **关键变量提取**：\n    *   **置信度**：LLM不仅能给出判断（真/假），还能给出置信度。作者认为置信度是量化影响力的天然指标——越自信的智能体，对邻居的影响应越大。\n    *   **自我-社会权衡**：智能体在更新观点时，面临两难选择：是坚持己见（自我依赖），还是采纳邻居意见（社会影响）。\n3.  **方法论创新（核心公式）**：为了量化这一过程，作者提出了**置信度归一化池化规则**。\n    *   *逻辑推演*：需要一个参数 $\\alpha$ 来控制“自我”与“社会”的比重。同时，为了避免数值不稳定并模拟真实的信念更新，必须利用置信度 $p$ 对邻居的判断进行加权。\n    *   *结果*：这构建了一个通用的更新机制，使得从众行为不再是黑盒，而是可调节、可观测的数学过程。\n\n### 第三阶段：实验设计与拓扑解构\n**通过结构对比隔离变量**\n\n1.  **拓扑作为控制变量**：为了探究结构的影响，作者选取了两种极端的决策范式进行对比：\n    *   **中心化聚合**：模拟“独裁”或“专家咨询”模式（如星型网络）。假设是：决策快，但极度依赖中心节点的能力。\n    *   **分布式共识**：模拟“民主”或“去中心化”模式（如环状到全连接网络）。假设是：决策慢，但通过多轮交互可能达成更稳健的共识。\n2.  **任务选择**：选择**二分类虚假信息检测**任务。原因在于该任务有明确的真伪标准，便于量化群体决策的准确性，且容易触发“少数服从多数”的从众现象。\n\n### 第四阶段：实证发现与逻辑修正\n**从“效率-鲁棒性”权衡到“错误级联”的发现**\n\n1.  **验证假设**：实验证实了网络结构的关键作用。中心化结构下，Hub的能力决定了上限；分布式结构下，连接越紧密，收敛越快。\n2.  **意外发现（深层洞察）**：作者发现从众是一把双刃剑。\n    *   *正面*：适度的从众（$\\alpha=0.75$）能有效过滤个别智能体的噪声，提升整体准确率。\n    *   *反面*：在高连接度（全连接网络）且初始信号错误的情况下，群体会迅速达成**“错误但确信”的共识**。这揭示了LLM MAS的一个致命弱点：**回声室效应**。\n3.  **异质性分析**：进一步引入模型异质性（如GPT-4o与GPT-3.5混合），发现中心节点倾向于听取与其同源的模型意见（同源偏差），这进一步丰富了从众动态的内涵。\n\n### 第五阶段：理论升华\n**构建LLM MAS的设计原则**\n\n1.  **总结规律**：作者将实验现象上升为理论——LLM MAS中的从众动力学受拓扑和权重的联合调控。\n2.  **指导意义**：研究最终落脚于系统设计建议。没有绝对完美的结构，设计者必须在**收敛速度（效率）**与**抗级联能力（鲁棒性）**之间做权衡。\n3.  **逻辑闭环**：从最初的社会学观察（从众），到数学建模（置信度池化），再到实验验证（拓扑效应），最终回归到工程实践（如何设计更可靠的MAS），形成了一个完整的学术闭环。\n\n---\n\n**总结**：作者的思考路径是从**社会学的直觉**出发，利用**控制论的方法**（更新规则）进行建模，通过**网络科学的视角**（拓扑结构）进行实验剖析，最终揭示了LLM群体智能中**“盲目共识”的风险**，为构建更可靠的多智能体系统提供了理论依据。"
                },
                {
                    "title": "EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting",
                    "arxiv_id": "2601.05487",
                    "authors": "Huanxiang Lin, Qianyue Wang, Jinwu Hu, Bailin Chen, Qing Du, Mingkui Tan",
                    "summary": "Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \\textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \\textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \\textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了EvidFuse，这是一个用于文本-图表生成的多智能体框架。它涉及两个协作组件（智能体）：数据增强分析智能体和实时证据构建编写器，展示了智能体协作、规划（大纲规划）和工具使用（访问原始表格）等核心智能体特征，符合多智能体协作的研究范围。",
                    "summary2": "本文旨在解决现有数据驱动报告中图表与文本不一致及洞察冻结的问题。针对多数据表和用户分析请求，我们提出了一种名为EvidFuse的训练无关多智能体框架，通过Data-Augmented Analysis Agent和Real-Time Evidence Construction Writer实现写作时按需构建视觉证据。我们在Tableau、OWID和USAFacts数据集上，通过LLM-as-a-judge和人工评估验证了其在图表质量、文本图表对齐和报告有用性方面的有效性。",
                    "summary_translation": "数据驱动报告通过将叙述文本与基于底层表格的图表紧密交织，从而传达决策相关的见解。然而，当前的基于大语言模型的系统通常在分阶段流水线中生成叙述和可视化内容，遵循“先文本后图表”或“先图表后文本”的范式。这些设计往往导致图文不一致和见解冻结，即中间证据空间变得固定，模型无法随着叙述的演变检索或构建新的视觉证据，从而导致分析浅显且流于预设。为了解决这些局限性，我们提出了 **EvidFuse**，这是一个免训练的多智能体框架，能够在数据驱动报告的写作过程中实现文本与图表的交织生成。EvidFuse 通过两个协作组件将可视化分析与长文本撰写解耦：一个是配备了探索性数据分析（EDA）衍生知识并拥有原始表格访问权限的 **数据增强分析智能体**，另一个是负责规划大纲并起草报告，同时间歇性发出细粒度分析请求的 **实时证据构建编写器**。这种设计允许在叙述需要的确切时刻构建并整合视觉证据，直接约束后续论点，并实现证据空间的按需扩展。实验表明，在图表质量、图文对齐以及报告级实用性方面，EvidFuse 在大模型评判和人类评估中均获得了最高排名。",
                    "inspiration_trace": "基于论文《EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting》的内容，以下是对作者产出该核心方法逻辑链的系统性推演：\n\n### 1. 宏观观察：数据报告生成的核心矛盾\n作者首先关注到一个宏观问题：高质量的数据驱动报告（如商业分析、政策报告）不仅仅是文本，而是**叙事文本与可视化图表的紧密交织**。\n*   **现状**：虽然大语言模型（LLM）在长文本生成上表现优异，但在生成这种“文图交织”的报告时，往往面临**一致性**（文本说的和图表画的不一样）和**深度**（仅停留在表面描述，缺乏决策洞察）的双重挑战。\n\n### 2. 问题诊断：现有范式的“时空错位”\n作者深入分析了现有的解决方案，发现它们大多遵循**分阶段流水线**，并指出了其根本缺陷：\n*   **范式 A：先文后图**。先写完故事，再插入图表。\n    *   *缺陷*：文本生成时并没有看到真实的图表，导致文本往往是“幻觉”或与最终生成的图表不匹配。\n*   **范式 B：先图后文**。先生成一组图表，再基于图表写故事。\n    *   *缺陷*：叙事被限制在预先生成的固定图表集合中。随着故事的发展，如果需要新的证据视角，模型无法回溯去生成新图。\n*   **核心症结**：作者将这一现象抽象为**“证据空间冻结”**。无论是先文还是先图，证据（图表）和叙事（文本）在时间上是分离的，导致两者无法在生成过程中相互动态约束。\n\n### 3. 假设提出：从“分阶段”到“写作时交织”\n为了解决“证据空间冻结”的问题，作者提出了一个核心假设：**证据的构建应该发生在写作的过程中，而不是写作之前或之后。**\n*   **新范式**：写作时证据构建。\n*   **逻辑推演**：如果模型在写到一个需要数据支撑的观点时，能够暂停，去生成一个精确的图表，拿到图表后再继续写接下来的文字，那么：\n    1.  文本将严格基于刚生成的真实图表（解决一致性问题）。\n    2.  叙事可以随时触发新的图表生成，不再受限于预设集合（解决深度和灵活性问题）。\n\n### 4. 方法设计：解耦与协作的双智能体架构\n为了实现上述“写作时交织”的理想状态，作者意识到让一个模型同时处理“复杂的数据分析/绘图”和“连贯的长文写作”会导致认知过载和上下文混乱。因此，逻辑演进转向了**任务解耦**：\n\n*   **角色一：数据增强分析代理**\n    *   *职责*：专门负责脏活累活。它需要懂探索性数据分析（EDA），能访问原始表格，能写代码画图。\n    *   *作用*：作为一个“工具”，随时响应具体的分析请求，产出带标注的图表。\n*   **角色二：实时证据构建写手**\n    *   *职责*：专门负责讲故事。它先规划大纲，然后分段写作。\n    *   *关键机制*：它具备“元认知”能力，知道何时需要证据。当它写到需要图表支撑的地方时，会发出一个特定的请求信号（如 `<visualization>`），然后**暂停生成**，等待分析代理返回图表结果，将图表注入上下文后，再恢复写作。\n\n### 5. 逻辑闭环：动态演进的证据空间\n通过上述设计，作者构建了一个动态闭环：\n*   **Writer** 发起请求 -> **Agent** 生成图表 -> **Writer** 基于图表继续写 -> 触发新请求...\n*   这种设计使得证据空间不再是静态的，而是随着叙事的深入不断**按需扩展**。文本约束了图表的内容（通过请求），图表约束了文本的描述（通过上下文注入），从而实现了真正的“文图一致”和“深度洞察”。\n\n### 总结\n作者的思考路径是从**发现现有方法“时空分离”导致的不一致性**出发，提出**“写作时构建证据”的范式转变**，进而通过**双智能体分工（Writer负责叙事流，Agent负责数据流）**来落地这一想法，最终实现了一个能够动态、按需生成高质量数据报告的框架。"
                },
                {
                    "title": "DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems",
                    "arxiv_id": "2601.07248",
                    "authors": "Shuyu Zhang, Yujie Liu, Xinru Wang, Cheng Zhang, Yanmin Zhu, Bin Li",
                    "summary": "Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了DarwinTOD框架，核心研究内容包括LLM驱动的终身自我演化（符合自我演化标准）以及在线多智能体对话执行与同行评审机制（符合多智能体标准），属于Agentic AI的研究范畴。",
                    "summary2": "本文旨在解决任务导向对话系统无法在部署后实现终身自主进化和适应新领域的问题。针对动态环境下的对话交互，我们提出了一种名为DarwinTOD的终身自进化框架，该框架集成了进化计算与LLM驱动的策略优化，通过维护可进化策略库（ESB）及双循环机制（在线多智能体执行与离线结构化进化）实现无人工干预的持续优化。我们在MultiWOZ和SGD数据集上通过Inform、Success、BLEU及Combine指标验证了其有效性。",
                    "summary_translation": "传统的任务型对话系统无法从持续的交互中进行演化，也无法在部署后适应新领域，这是其在现实世界动态环境中的一个关键局限。持续学习方法依赖于基于人工策划数据的阶段性重训练，未能实现自主的终身改进。尽管进化计算和 LLM (Large Language Model, 大语言模型) 驱动的自我改进为对话优化提供了有前景的机制，但它们缺乏一个用于全面、迭代策略优化的统一框架。为了弥合这一差距，我们提出了 DarwinTOD，一个终身自演化对话框架，该框架系统性整合了这两种范式，从而能够在无需特定任务微调的情况下，从零样本基础开始实现持续的策略优化。DarwinTOD 维护一个 Evolvable Strategy Bank (可演化策略库)，并通过双环过程运行：包含同伴评议的在线多智能体对话执行，以及利用累积反馈优化策略库的离线结构化进化操作。这种闭环设计使得无需人工干预即可实现自主的持续改进。大量实验表明，DarwinTOD 优于以往最先进的方法，并在整个演化过程中展现出持续的性能提升。我们的工作为构建具有终身自演化能力的对话系统提供了一个新颖的框架。",
                    "inspiration_trace": "基于对论文《DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems》的深度分析，以下是作者产出该核心方法的逻辑推演过程，还原了从宏观观察到具体方法论的思考链条：\n\n### 第一阶段：宏观问题识别——从“静态系统”到“动态世界”的矛盾\n\n**1. 现实观察：**\n作者首先观察到现实世界是动态变化的。用户的偏好、对话的领域以及任务的目标都在不断演进。然而，现有的任务型对话系统（TOD）在部署后本质上是“静态”的——一旦训练完成，其能力就被冻结，无法从后续的交互中学习或适应新领域。\n\n**2. 核心痛点提炼：**\n这导致了“研究原型”与“可部署系统”之间的巨大鸿沟。学术界通常在静态基准上评估模型，而工业界需要的是一个能在开放、动态环境中长期运行的智能体。因此，核心问题不再是“如何让模型在测试集上表现好”，而是**“如何让系统具备终身自我进化的能力，实现完全自主的持续改进？”**\n\n### 第二阶段：现有范式的批判与局限分析\n\n**1. 审视传统方案：**\n作者逐一分析了现有技术路线，发现它们都无法解决上述核心问题：\n*   **流水线架构：** 虽然模块化，但存在级联错误传播，且难以适应新领域，缺乏灵活性。\n*   **端到端大模型（LLM）：** 虽然泛化能力强，但本质上仍是基于初始指令的静态执行，缺乏从经验中学习的机制。\n*   **持续学习：** 虽然试图增量更新，但严重依赖人工整理的数据和周期性的重训练，无法实现真正的“自主”和“终身”进化。\n\n**2. 寻找突破口：**\n作者意识到，要实现真正的自主进化，必须摆脱对“人工标注数据”和“模型参数微调”的依赖，转而寻找一种能够利用系统自身交互经验进行自我优化的机制。\n\n### 第三阶段：理论融合——进化计算与大模型的互补性思考\n\n**1. 两个孤立的方向：**\n作者注意到了两个有潜力但各自为政的研究方向：\n*   **进化计算：** 擅长基于种群的优化，能通过选择、变异等机制寻找最优解，但缺乏语义理解能力，通常只用于优化孤立的提示词。\n*   **LLM驱动的自我改进：** 擅长推理和反思，能通过多智能体协作解决问题，但往往缺乏结构化的长期策略管理机制，容易陷入单轮优化的局部视角。\n\n**2. 逻辑跃迁（核心假设）：**\n**“如果将LLM作为进化算法的‘大脑’，利用其强大的语义理解和推理能力来驱动对话策略的进化，会发生什么？”**\n作者认为，LLM可以作为智能的“进化算子”，而进化算法提供了结构化的“优化框架”。两者的结合可以解决各自的短板：进化算法提供了终身迭代的框架，LLM提供了语义层面的策略生成与评估能力。\n\n### 第四阶段：方法论构建——从“单点优化”到“种群进化”\n\n**1. 核心概念定义：**\n基于上述假设，作者提出了**“可进化策略库”**的概念。\n*   **思维转变：** 传统的Prompt Engineering是在寻找一个“最好的”提示词。而DarwinTOD转向维护一个“多样化的策略种群”。这些策略在交互中竞争、优胜劣汰。\n\n**2. 闭环机制设计：**\n为了实现终身进化，作者设计了一个**“双循环”架构**，将理论落地：\n*   **在线执行循环：** 模拟真实环境。作者没有使用单一的端到端Agent，而是保留了**多智能体流水线（DST, DP, NLG）**。为什么？因为模块化不仅能防止错误级联，更重要的是，它允许每个模块拥有独立的策略，从而实现更细粒度的进化。\n*   **引入“同伴批判”：** 为了获得比单纯的“任务成功/失败”更密集的反馈信号，作者让智能体之间互相批判。这不仅能实时纠错，还能为离线进化提供高质量的反思数据。\n\n**3. 离线进化循环：**\n这是系统的“大脑”部分。作者设计了四种受进化论启发的操作算子，直接作用于策略库：\n*   **Genesis（创生）：** 针对新领域，利用LLM的零样本能力从无到有生成策略。\n*   **Mutation（变异）：** 针对失败的对话，利用LLM分析失败原因并修改策略。\n*   **Consolidation（整合）：** 利用LLM合并相似的策略，保持种群精简。\n*   **Pruning（剪枝）：** 淘汰低适应度的策略，控制计算成本。\n\n### 第五阶段：鲁棒性思考——应对噪声与不确定性\n\n**1. 潜在风险识别：**\n作者意识到，LLM生成的批判和变异可能包含噪声或偏见。如果系统盲目信任每一次反馈，可能会导致策略退化。\n\n**2. 解决方案设计：**\n为了解决这个问题，作者引入了**“适应度函数”**和**“玻尔兹曼选择”**机制。\n*   **长期统计：** 不依赖单次反馈，而是基于长期的历史表现（正负反馈计数）来计算策略的适应度。\n*   **概率选择：** 即使策略当前适应度低，也有一定概率被选中（探索），防止过早收敛到局部最优。\n*   **逻辑闭环：** 这种设计使得系统具有“抗噪性”，即使偶尔有错误的批判，长期的大数定律和种群选择机制也能过滤掉噪声，确保进化方向是向上的。\n\n### 总结：逻辑演进的全景图\n\n作者的思考路径是从**现实世界的动态需求**出发，批判了现有技术的静态本质，通过**融合进化计算的结构化优势与LLM的语义优势**，创造性地提出了**基于种群策略进化的新范式**。最终，通过**双循环架构**和**抗噪的进化机制**，将这一理论转化为一个无需人工干预、能够终身自我进化的对话系统。"
                },
                {
                    "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
                    "arxiv_id": "2601.07152",
                    "authors": "Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan",
                    "summary": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了一个名为“Agents of Diffusion”的框架，明确采用了多智能体架构，包含“提示优化智能体”和“判别智能体”。这些智能体通过协作和自然语言反馈来引导生成过程，符合“多智能体：协作、通信”的研究范围。",
                    "summary2": "本文旨在解决生成高质量结构化数据时难以兼顾语义丰富性与严格模式一致性的挑战。针对结构化文本生成任务，我们提出了Agents of Diffusion (AoD)框架，利用多智能体强化学习通过自然语言反馈迭代引导冻结的Diffusion Language Models (DLMs)。在MultiWOZ、Super-NaturalInstructions等数据集上，通过Task Success Rate (TSR)和Field Overlap等指标验证了其有效性，实现了优于基线模型的结构保真度与语义多样性平衡。",
                    "summary_translation": "生成高质量的结构化数据（例如 JSON 记录）仍然是大语言模型面临的一项基本挑战，尤其是在必须兼顾语义丰富性与严格模式遵守的情况下。尽管自回归大语言模型具备强大的结构一致性，但它们往往难以应对语义变化和输出多样性方面的要求。相比之下，扩散语言模型引入了实现语义丰富性和双向解码的强大机制，却缺乏可靠保持结构所需的归纳偏置。我们提出了 Agents of Diffusion (AoD)，这是一个新颖的框架，通过语言介导的强化学习，将扩散语言模型的生成灵活性与自回归模型的推理能力统一起来。AoD 将结构化文本生成构建为一个多智能体对齐过程，其中提示优化智能体与评判智能体协作，利用自然语言反馈迭代指导扩散语言模型。这种方法实现了可控且符合模式规范的生成，而无需修改模型参数或依赖人工设计的约束。AoD 证明了扩散模型在协作智能体的监督下能够同时实现高语义新颖性和结构保真度，从而推进了可控生成领域的发展。在多个结构化数据基准测试中，AoD 始终优于扩散模型和自回归模型的基线，为结构感知且多样性增强的文本合成开辟了一条新路径。",
                    "inspiration_trace": "基于论文《Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体方法创新的思考过程。\n\n---\n\n### 1. 宏观问题与现状观察：结构化生成的“两难困境”\n**思考起点：** 作者首先关注到生成高质量结构化数据（如JSON）是当前LLM应用的一个核心痛点。\n*   **观察现象：** 现有的两大生成范式存在明显的优缺点互补，但无法兼得。\n    *   **自回归模型（AR-LLMs）：** 具有极强的结构一致性和因果逻辑（因为是从左到右生成），容易符合Schema。但缺点是语义单一、容易陷入重复模式，缺乏多样性。\n    *   **扩散语言模型：** 具有双向去噪机制，语义丰富、多样性高。但缺乏位置先验，很难严格保持复杂的嵌套结构（如JSON的括号匹配、字段完整性）。\n*   **核心矛盾：** 我们既想要扩散模型的“语义多样性”，又想要自回归模型的“结构严谨性”。现有的方法要么微调模型（成本高），要么使用硬性规则（缺乏灵活性）。\n\n### 2. 核心假设：用“推理”驾驭“生成”\n**思考转折：** 既然重新训练一个完美的模型很难，能否通过“外部控制”来弥补内部缺陷？\n*   **假设提出：** 能否利用自回归模型强大的逻辑推理能力，来“监督”或“引导”扩散模型的生成过程？\n*   **关键洞察：** 不需要修改扩散模型的参数（保持其生成多样性），而是通过改变其输入条件来控制输出。\n*   **控制接口：** 最直接的控制接口就是**提示词**。如果能让提示词动态进化，就能在不改动模型权重的情况下，引导模型生成符合结构要求的内容。\n\n### 3. 方法论演进：从静态提示到动态强化学习\n**思考深化：** 传统的提示工程是静态的（写一次，固定用），无法应对生成过程中的随机性和错误。如何实现动态控制？\n*   **机制选择：** 引入**强化学习（RL）**。将“提示词的修改”看作是一个动作，将“生成结果的质量”看作是奖励。\n*   **反馈信号的困境：** 传统的RL通常使用标量奖励（如一个分数）。但在结构化生成中，一个分数很难解释具体的错误（例如：“缺少字段”还是“格式错误”）。\n*   **创新点：** **自然语言反馈**。既然是语言模型，为什么不直接用语言来作为奖励信号？语言反馈比标量数字包含更丰富的信息，且更容易被LLM理解和执行。\n\n### 4. 架构构建：多智能体分工协作\n**具体化：** 如何将上述理论落地？作者设计了一个基于角色的多智能体系统，将任务拆解。\n*   **角色分工：**\n    *   **生成者：** 冻结的扩散模型。负责提供多样化的候选内容（探索者）。\n    *   **评判者：** 自回归LLM。负责检查生成内容的结构完整性和语义准确性，并输出自然语言反馈（批评家）。\n    *   **优化者：** 另一个自回归LLM。负责根据评判者的反馈，修改提示词（决策者）。\n*   **闭环逻辑：**\n    1.  优化者给出初始提示。\n    2.  扩散模型根据提示生成JSON。\n    3.  评判者检查JSON，给出具体建议（如：“缺少date字段，请修正”）。\n    4.  优化者根据建议修改提示词（如：“确保包含YYYY-MM-DD格式的date字段”）。\n    5.  循环往复，直到生成完美结果。\n\n### 5. 理论保障与最终形态\n**逻辑闭环：** 为什么这个系统是稳定且有效的？\n*   **解决“漂移”问题：** 多智能体系统常面临对话发散的问题。作者通过将扩散模型作为“环境锚点”，所有智能体的交互都围绕具体的生成样本展开，从而保证了交互的稳定性。\n*   **无参数化优势：** 整个过程不需要梯度回传更新扩散模型，完全通过语言层面的交互实现优化。这使得该方法可以即插即用于各种开源或闭源模型。\n\n---\n\n**总结：作者的思考路径**\n从**“结构 vs 多样性”**的矛盾出发 $\\rightarrow$ 提出**“用AR推理控制DLM生成”**的假设 $\\rightarrow$ 选择**“提示词”**作为控制抓手 $\\rightarrow$ 引入**“强化学习+自然语言反馈”**实现动态优化 $\\rightarrow$ 最终构建**“生成-评判-优化”**的多智能体协作闭环。"
                },
                {
                    "title": "SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models",
                    "arxiv_id": "2601.07252",
                    "authors": "Chunwei Yang, Yankai Wang, Jianxiang Tang, Haojie Qu, Ziqiang Zou, YuLiu, Chunrui Deng, Zhifang Qiu, Ming Ding",
                    "summary": "Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.",
                    "category": "cs.MA",
                    "filter_reason": "该论文提出了一个基于多种大语言模型的多智能体系统，专注于智能体之间的协作、智能纠错（自我反思）以及检索增强生成（RAG），符合研究范围中关于“多智能体：协作”和“工具使用”的定义。尽管论文涉及CFD领域应用和多模态输入，但其核心贡献在于构建智能体框架而非单纯的应用落地或多模态模型开发。",
                    "summary2": "本文旨在解决现有CFD多智能体系统在处理复杂几何形状时的局限性。针对包含图像和文本的多模态输入场景，我们提出了一种名为SwarmFoam的多智能体框架，集成了多模态感知、首错优先智能纠错及RAG机制。我们在25个涵盖多种物理问题的测试用例上，通过Pass Rate、Token Usage等指标验证了其有效性，总体通过率达到84%。",
                    "summary_translation": "数值模拟是科学研究中的主流方法之一，通常由专业工程师执行。随着多智能体技术的进步，利用协作智能体模拟人类行为，在智能计算流体力学 (CFD) 模拟方面展现出巨大潜力。目前已提出了一些基于大语言模型的多智能体系统。然而，在处理复杂几何结构时，这些系统表现出显著的局限性。本文介绍了一种新的多智能体模拟框架——SwarmFoam。SwarmFoam 集成了 Multi-modal perception (多模态感知)、Intelligent error correction (智能纠错) 和 Retrieval-Augmented Generation (检索增强生成) 等功能，旨在通过对图像和高级指令的双重解析来实现更复杂的模拟。实验结果表明，SwarmFoam 对不同模态的模拟输入具有良好的适应性。在 25 个测试用例中，整体通过率为 84%，其中自然语言输入和多模态输入用例的通过率分别为 80% 和 86.7%。SwarmFoam 所展示的工作将进一步推动 CFD 智能体方法的发展。",
                    "inspiration_trace": "基于论文《SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models》的内容，以下是对作者产出该核心方法的逻辑链推演，旨在还原其从宏观观察到微观实现的思考过程：\n\n---\n\n### 第一阶段：宏观观察与趋势研判\n**（从“AI for Science”到“智能体自动化”的范式转移）**\n\n1.  **背景洞察**：作者首先观察到计算流体力学（CFD）是科研和工程的核心工具，但高度依赖专业工程师，门槛高、流程繁琐。\n2.  **技术趋势**：随着生成式AI的爆发，利用大语言模型（LLM）和多智能体系统来模拟人类专家行为、自动化执行复杂任务已成为新趋势。\n3.  **现状评估**：作者回顾了2024-2025年的前沿工作（如MetaOpenFOAM、OpenFOAMGPT、Foam-Agent），确认了“基于LLM的智能体自动化OpenFOAM仿真”这一技术路线的可行性。\n4.  **核心痛点识别**：尽管现有系统（如Foam-Agent）已经能通过自然语言驱动仿真，但作者敏锐地发现了一个关键局限——**“模态缺失”**。在真实的工程场景中，几何结构和物理条件往往通过图纸、图像传递，仅靠自然语言描述复杂几何极其困难且不准确。\n\n### 第二阶段：问题聚焦与假设提出\n**（突破“文本单一性”与“纠错低效性”的双重瓶颈）**\n\n1.  **针对输入模态的假设**：\n    *   *思考*：如果给智能体加上“眼睛”，让它能像人类工程师一样看懂几何图纸，是否能大幅提升仿真的准确性和适用范围？\n    *   *假设*：引入多模态感知能力，结合图像和文本输入，能解决复杂几何描述不清的问题。\n\n2.  **针对多模态处理策略的假设**：\n    *   *思考*：有了图像能力后，是直接把图像丢给写配置文件的智能体，还是先专门解析？\n    *   *假设*：将“图像理解”与“网格生成”解耦。先由一个专门的智能体把图像信息转化为结构化的几何/物理文本描述，再传递给后续智能体，比直接端到端生成效果更好（避免信息丢失）。\n\n3.  **针对纠错机制的假设**：\n    *   *思考*：现有系统（如Foam-Agent）在仿真失败时，倾向于一次性分析所有错误日志并批量修改。这既浪费Token，又可能因为修改了由“根源错误”导致的“衍生错误”而陷入混乱。\n    *   *假设*：采用“首错优先”策略。假设第一个报错是根源，只修复它，然后重试。这种迭代方式虽然可能增加轮次，但能大幅降低单次推理成本，并避免无效修改。\n\n### 第三阶段：方法论构建与架构设计\n**（从“假设”到“SwarmFoam”系统架构的落地）**\n\n1.  **引入“观察者”**：\n    *   为了验证多模态假设，作者设计了**Observer Agent**。它的核心任务是“看图说话”，将用户输入的图像和自然语言，解析为标准的几何参数（如顶点坐标）和物理条件（如边界类型）。\n\n2.  **构建协作流水线**：\n    *   为了实现全流程自动化，作者将人类工程师的仿真工作流拆解为六个角色，形成流水线：\n        *   **Observer**（感知）：解析图文。\n        *   **Architect**（规划）：决定需要哪些文件，规划目录结构。\n        *   **InputWriter**（执行）：利用RAG（检索增强生成）技术，参考历史案例编写具体的配置文件。\n        *   **Runner**（运行）：执行OpenFOAM命令。\n        *   **Reviewer**（纠错）：实施“首错优先”策略，定位并反馈第一个错误。\n        *   **ParaMaster**（后处理）：自动调用ParaView生成可视化结果。\n\n3.  **强化知识支撑（RAG）**：\n    *   考虑到LLM可能产生幻觉或不懂OpenFOAM的特定语法，作者引入了RAG系统，将官方文档、参考案例等作为“外挂大脑”，确保生成的配置文件符合规范。\n\n### 第四阶段：验证与迭代优化\n**（通过消融实验验证核心假设）**\n\n1.  **验证多模态策略**：\n    *   *实验设计*：对比“预解析图像”（Method 1）与“直接利用图像”（Method 2）。\n    *   *结果反馈*：实验发现直接利用图像会导致大量几何信息丢失，通过率大幅下降。这证实了作者在第二阶段的判断——**解耦图像解析与文件生成是必要的**。\n\n2.  **验证纠错策略**：\n    *   *实验设计*：对比开启/关闭智能纠错机制，以及与基线模型（Foam-Agent）的Token消耗对比。\n    *   *结果反馈*：虽然“首错优先”可能增加迭代次数，但显著降低了Token使用量（-83.85%），证明了其在经济性和逻辑清晰度上的优势。\n\n3.  **综合评估**：\n    *   通过25个涵盖单相流、多相流、燃烧等不同物理问题的测试用例，最终验证了SwarmFoam在多模态输入下的高通过率（86.7%），确立了其作为新一代智能CFD仿真系统的有效性。\n\n---\n\n**总结：**\n作者的思考路径遵循了典型的**“观察-假设-设计-验证”**科研闭环：\n从**“现有智能体看不懂图纸”**这一具体痛点出发，提出**“引入多模态感知”**和**“首错优先纠错”**的创新假设，进而通过精细化的**Agent角色分工**和**RAG技术**构建了SwarmFoam系统，最后通过严格的**消融实验**证实了其设计思路的正确性。"
                },
                {
                    "title": "Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents",
                    "arxiv_id": "2601.06490",
                    "authors": "Wenyu Mao, Haosong Tan, Shuchang Liu, Haoyang Liu, Yifan Xu, Huaxiang Ji, Xiang Wang",
                    "summary": "Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了一个名为Bi-Mem的智能体框架，利用归纳智能体和反思智能体来构建和校准分层记忆。这属于单智能体研究范畴中的“记忆”和“自我反思”能力，符合筛选条件。",
                    "summary2": "本文旨在解决个性化LLM分层记忆中因噪声和幻觉导致的局部记忆与全局画像不一致问题。针对长期个性化对话场景，我们提出了一种名为Bi-Mem的智能体框架，通过归纳-反思双向构建机制校准记忆，并引入联想检索。我们在LoCoMo数据集上通过F1和BLEU-1指标验证了其有效性。",
                    "summary_translation": "从用户的长期对话中构建记忆，能够克服大语言模型的上下文限制，从而实现个性化交互。近期的研究侧重于层次化记忆，旨在通过聚类和聚合历史对话来建模用户的多粒度行为模式。然而，对话噪声和记忆幻觉可能在聚类过程中被放大，导致局部聚合记忆与用户的全局人设不一致。为缓解这一问题，我们提出了Bi-Mem，这是一个通过双向构建来确保层次化记忆保真度的智能体框架。具体而言，我们部署了一个归纳智能体来构建层次化记忆：该智能体从原始对话中提取事实信息以形成事实级记忆，利用图聚类将其聚合为主题场景（即局部场景级记忆），并推断用户画像作为全局人设级记忆。同时，我们设计了一个反思智能体，利用从人设级记忆中导出的全局约束来校准局部场景级记忆，从而强化全局-局部对齐。为实现连贯的记忆回忆，我们提出了一种联想检索机制：除了初始的层次搜索外，扩散激活过程允许事实激活上下文场景，而场景级匹配则检索显著的支撑性事实信息。实证评估表明，Bi-Mem在长期个性化对话任务的问答性能方面取得了显著提升。",
                    "inspiration_trace": "基于论文《Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到微观方法设计的思考过程：\n\n---\n\n### 第一阶段：宏观背景与现有范式的演进\n**（从“个性化需求”到“分层记忆”的必然性）**\n\n1.  **观察起点：** LLM的上下文窗口有限，无法容纳用户的长期对话历史。为了实现个性化交互（如记住用户偏好、回忆共同经历），必须引入外部记忆机制。\n2.  **现有方案的局限：** 早期的“扁平化记忆”仅存储孤立的事实或摘要，缺乏对事实间关联和用户高层行为模式的捕捉能力。\n3.  **趋势演进：** 研究自然转向了“分层记忆”，即通过聚类将原子事实聚合为场景，再提炼为用户画像。这种结构模仿了人类认知，从细粒度到粗粒度，看似完美解决了信息碎片化问题。\n\n### 第二阶段：关键问题的发现与诊断\n**（从“单向聚合”到“级联错误”的洞察）**\n\n1.  **深入审视：** 作者观察到，现有的分层记忆构建过程大多是**单向的**，即纯粹的自底向上聚合。\n2.  **核心痛点：** 在自底向上的过程中，原始对话中的“噪声”（如无关闲聊）和提取过程中的“幻觉”会被聚类算法放大。\n3.  **逻辑矛盾：** 这种放大的噪声会导致局部聚合的场景记忆与用户的全局画像发生冲突。\n    *   *案例思考：* 用户平时口味清淡（全局画像），但偶尔陪朋友吃了一次辣（局部场景）。单纯的聚类会错误地将“吃辣”归纳为用户的局部习惯，导致后续推荐出错。\n4.  **归纳假设：** 问题的根源在于缺乏“全局约束”。局部记忆的生成缺乏对全局一致性的校验，导致了“级联错误”的积累。\n\n### 第三阶段：核心假设与方法论的提出\n**（从“单向构建”到“双向闭环”的突破）**\n\n1.  **解决思路：** 为了解决局部与全局的冲突，记忆构建不能只是单向的归纳，必须引入反向的反思机制。\n2.  **框架设计：** 提出 **Bi-Mem** 框架，将记忆构建过程拆解为两个互补的智能体：\n    *   **归纳智能体：** 负责传统的自底向上构建（事实 -> 场景 -> 画像）。这是为了从数据中提取信息。\n    *   **反思智能体：** 负责自顶向下的校准。利用生成的全局画像作为“约束条件”，去检查和修正下层的场景记忆。\n3.  **逻辑闭环：** 通过这种“双向构建”，确保了局部细节（场景）始终服务于并服从于全局特征（画像），消除了记忆中的逻辑矛盾。\n\n### 第四阶段：记忆利用机制的优化\n**（从“静态检索”到“动态关联”的完善）**\n\n1.  **新问题：** 虽然记忆结构被修正了，但在检索时，如果仅按层级独立检索（如只查场景或只查画像），可能会割裂事实与上下文的联系。\n2.  **联想机制：** 作者引入了心理学中的“扩散激活”概念。\n3.  **检索逻辑：** 检索不应是孤立的。\n    *   检索到一个“事实”时，应自动激活其所属的“场景”。\n    *   检索到一个“场景”时，应回溯其包含的关键“事实”。\n4.  **最终形态：** 形成了**联想检索机制**，在初始检索后进行跨层级的扩散，确保模型在生成回答时能同时获得宏观的上下文和微观的证据支持。\n\n---\n\n### 总结：作者的思维演进路径\n\n1.  **宏观需求：** LLM需要长期记忆来实现个性化。\n2.  **技术选型：** 分层记忆优于扁平记忆。\n3.  **批判性观察：** 现有的分层记忆是单向的，容易因噪声放大导致“局部-全局”不一致。\n4.  **核心创新：** 引入“反思”机制，构建双向闭环（归纳+校准），用全局画像约束局部场景。\n5.  **应用落地：** 设计联想检索，打通层级间的壁垒，实现连贯的记忆召回。"
                },
                {
                    "title": "DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation",
                    "arxiv_id": "2601.06373",
                    "authors": "Yutong Song, Jiang Wu, Kazi Sharif, Honghui Xu, Nikil Dutt, Amir Rahmani",
                    "summary": "Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.",
                    "category": "cs.MA",
                    "filter_reason": "论文提出了 DemMA，一个用于模拟痴呆症患者的单智能体系统。它涉及智能体核心能力，如记忆（构建人格）和行动（模拟非语言行为），并提出了特定的智能体训练框架（CoT 蒸馏），而不仅仅是将现有智能体应用于医疗任务。",
                    "summary2": "本文旨在解决痴呆症模拟中数据稀缺及缺乏医学严谨性的挑战。针对多轮对话场景，我们提出了一种名为DemMA的专家引导推理与动作模拟框架。该方法通过临床人格构建和多智能体工作流生成数据，并利用CoT蒸馏技术将推理、语言和动作生成整合到单个LLM中。我们在DemMA-Dialogue数据集上，通过人格一致性、医学一致性等指标验证了其有效性，显著优于基线模型。",
                    "summary_translation": "利用大语言模型模拟 dementia patients (痴呆症患者) 具有挑战性，因为需要在长对话过程中对 cognitive impairment (认知障碍)、emotional dynamics (情绪动态) 和 nonverbal behaviors (非语言行为) 进行联合建模。我们提出了 DemMA，这是一个专家引导的 dementia dialogue agent (痴呆症对话智能体)，旨在实现高保真的 multi-turn patient simulation (多轮患者模拟)。DemMA 通过整合病理信息、人格特质以及由临床专家指导的特定亚型 memory-status personas (记忆状态人格)，构建了基于临床的 dementia personas (痴呆症人格)。为了突破纯文本模拟的局限，DemMA 对 nonverbal behaviors (非语言行为)（包括动作、面部表情和声音线索）进行了显式建模。我们进一步引入了一个 Chain-of-Thought (思维链) 蒸馏框架，该框架训练单个 LLM 在一次前向传播中联合生成 reasoning traces (推理轨迹)、患者话语以及对齐的行为动作，从而无需 multi-agent inference (多智能体推理) 即可实现高效部署。与专家、医学生及 LLM 评判者进行的广泛评估表明，DemMA 在多项指标上均显著优于现有的强 baselines (基线模型)。",
                    "inspiration_trace": "基于论文《DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation》，以下是对作者核心方法论逻辑链的系统性推演，旨在还原其从宏观问题观察到具体技术方案产出的思考过程：\n\n---\n\n### 第一阶段：宏观问题与痛点观察\n**思考起点：数据荒漠与模拟困境**\n作者首先观察到痴呆症研究和护理培训领域存在一个根本性的结构瓶颈：**高质量互动数据的极度稀缺**。\n*   **现实约束**：由于隐私和伦理限制，真实的患者数据（尤其是包含面部表情、语音语调等多模态信息的数据）几乎无法获取。\n*   **现有缺陷**：现有的模拟手段要么依赖僵化的脚本，无法捕捉真实互动的异质性；要么直接使用通用的对话模型，但这在医疗场景下极不可靠。\n\n### 第二阶段：深入剖析与假设提出\n**思考深入：通用大模型为何失效？**\n作者意识到，直接将LLM作为痴呆症患者模拟器存在三个核心矛盾，这构成了后续方法设计的假设前提：\n1.  **医学严谨性缺失**：通用模型缺乏临床依据，可能生成不安全或医学上不准确的建议。\n2.  **“过度完美”悖论**：LLM倾向于生成流畅、礼貌的回复，但这恰恰掩盖了痴呆症患者特有的认知衰退标志（如重复、犹豫、逻辑断裂）。这种“人格漂移”会导致模拟失真。\n3.  **模态缺失**：痴呆症沟通是多通道的（语言、情感、行为），纯文本模型丢失了非语言线索（如动作、神态），而这些随着语言能力下降变得愈发重要。\n\n**核心假设**：要实现高保真模拟，必须从**“通用对话生成”**转向**“临床病理驱动的行为建模”**。\n\n### 第三阶段：方法论演进与逻辑构建\n为了验证上述假设，作者分三个步骤构建了解决方案：\n\n#### 步骤一：构建临床锚点——从“角色扮演”到“病理分层”\n**思考**：如何防止模型生成随机的“疯言疯语”，而是生成符合特定痴呆症亚型的症状？\n**逻辑推演**：患者的人格不应是随机的，而应是病理学的产物。\n*   **创新点**：提出了**分层人格构建范式**。\n    *   不再使用单一的Prompt，而是将患者解构为三个依赖层：**背景层**（人口统计学+亚型病理）、**性格层**（基于ICF标准的心理功能）、**记忆层**（长/短期记忆状态）。\n    *   **目的**：通过这种结构化约束，确保生成的“遗忘”或“混乱”是特定病理（如阿尔茨海默症 vs. 额颞叶痴呆）的临床表现，而非模型的随机幻觉。\n\n#### 步骤二：解决数据与质量控制——多智能体流水线\n**思考**：既然没有真实数据，如何合成高质量数据？同时，如何解决长对话中的逻辑一致性问题？\n**逻辑推演**：单一模型难以同时兼顾记忆分析、对话规划和动作生成。需要“分而治之”。\n*   **创新点**：设计了**多智能体LLM工作流**。\n    *   引入专门的**记忆分析智能体**（判断当前哪些记忆可访问）、**对话规划智能体**（决定情感轨迹和内容）、**生成智能体**（产出语言）、**动作标注智能体**（补充非语言行为）以及**验证智能体**。\n    *   **目的**：通过将推理过程外显化，不仅生成了首个合成数据集，还确保了每一步都有临床逻辑支撑，解决了长对话的一致性问题。\n\n#### 步骤三：解决落地效率——思维链蒸馏\n**思考**：多智能体系统虽然质量高，但推理延迟大，无法满足实时护理培训的需求。如何保留“专家级推理”的同时，实现“单模型高效推理”？\n**逻辑推演**：多智能体的过程本质上是生成了丰富的“思维链”。如果能让一个模型学会这些思维过程，就不需要在推理时调用多个模型。\n*   **创新点**：提出了**CoT蒸馏多任务训练框架**。\n    *   将多智能体流水线产生的推理轨迹作为中间监督信号，训练一个单一模型同时完成“推理（规划）+ 说话（文本）+ 行动（多模态标签）”。\n    *   **目的**：将复杂的系统级逻辑内化为单模型的参数，实现了低延迟下的高保真模拟。\n\n### 第四阶段：最终方案合成\n**思考总结**：DemMA不仅仅是一个聊天机器人，而是一个**“临床 grounded 的多模态行为模拟器”**。\n\n**逻辑闭环**：\n1.  **输入端**：通过分层人格模块注入临床病理知识。\n2.  **训练端**：利用多智能体生成的高质量合成数据，通过CoT蒸馏，教会单模型如何像专家一样分析记忆状态、规划对话并匹配非语言行为。\n3.  **输出端**：在一个前向传播中，同时输出符合病理特征的语言、显式的推理逻辑以及对应的动作标签（Motion/Face/Sound），从而在文本界面中补偿了非语言信息的缺失。\n\n---\n\n**总结**：作者的思考路径是从**“数据稀缺”**的现实出发，识别出**“通用模型不适用”**的本质矛盾，进而通过**“结构化病理建模”**确立内容准确性，利用**“多智能体外显推理”**保证数据质量，最后通过**“知识蒸馏”**解决工程效率问题，最终实现了DemMA这一高保真、可落地的痴呆症模拟系统。"
                },
            ]
        },
    ],
    "2026-01-09": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                    "arxiv_id": "2601.04861",
                    "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
                    "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 OI-MAS 的新型多智能体框架。它不是将现有的智能体简单应用到某个垂直领域，而是针对多智能体系统（MAS）中存在的计算效率低下问题，提出了改进的架构和方法论。这符合“构建、改进或演化 LLM智能体”的核心目标。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确属于 `Multi-Agent Systems (MAS)`，关注多智能体协作。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并提出了动态选择智能体角色和模型尺度的机制，这是对多智能体协作机制的直接改进。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   虽然论文提到了“计算效率”和“成本降低”，但这属于智能体框架的优化和改进，而非底层的基础设施（如硬件加速）或单纯的部署优化，因此不应被排除。 综上所述，该论文通过引入置信度感知路由和自适应模型选择策略，实质性地改进了多智能体协作的效率和性能，是关于 LLM 智能体架构优化的高质量研究。",
                    "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。",
                    "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。"
                },
            ]
        },
    ],
    "2026-01-08": [
        {
            "name": "Artificial Intelligence",
            "count": 9,
            "papers": [
                {
                    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
                    "arxiv_id": "2601.05187",
                    "authors": "Yanchang Liang, Xiaowei Zhao",
                    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，主要基于以下判断： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文的核心不仅仅是将LLM应用于Simulink这一特定领域，而是提出了一种新的智能体架构和训练方法。具体而言，它提出了 **Reflection-GRPO (ReGRPO)**，这是一种结合了**自我反思**机制的强化学习算法。该算法通过自我反思痕迹提供中间反馈，解决长视距任务中的稀疏奖励问题，从而加速收敛并提高鲁棒性。这直接对应了筛选标准中的“自我演化”和“自我反思”机制。 2.  **具备明确的Agentic特征**： 论文描述了一个轻量级的 **“计划-执行”架构**，并明确提到该架构赋予智能体低级工具技能和高级设计推理能力。这符合筛选标准中关于“规划”和“工具使用”的正面指标。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是Simulink建模（特定工程领域），但根据筛选标准第四步第2点，如果论文的核心是提出一种新的“自我演化”机制（即ReGRPO和两阶段课程学习），即使它被应用在特定领域，也应该保留。本文的重点在于智能体如何通过反思和强化学习进行自我完善和迭代，而非单纯的应用。 综上所述，该论文在构建智能体架构、引入自我反思机制以及通过强化学习实现自我演化方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决LLM在图形化Simulink建模中的应用难题。针对Simulink建模任务，我们提出了SimuAgent框架，采用轻量级Python字典表示模型，并引入Reflection-GRPO (ReGRPO)算法结合两阶段训练策略。我们在新发布的SimuBench数据集（5300个任务）上进行了实验，通过建模准确率验证了其有效性。结果显示，SimuAgent比标准RL收敛更快，且超越了GPT-4o。",
                    "summary_translation": "大语言模型彻底变革了基于文本的代码自动化，但它们在面向图形的工程工作流中的潜力仍未得到充分探索。我们介绍了SimuAgent，这是一个专为Simulink定制的、由LLMs（大语言模型）驱动的建模与仿真智能体。SimuAgent用简洁的字典风格Python表示法取代了冗长的XML（可扩展标记语言），大幅减少了token（词元）数量，提高了可解释性，并实现了快速的进程内仿真。一种轻量级的plan-execute（规划-执行）架构，分两个阶段进行训练，使智能体既具备低级工具技能，又具备高级设计推理能力。为了解决长视界任务中的稀疏奖励问题，我们提出了Reflection-GRPO (ReGRPO)，它通过自我反思轨迹增强了Group Relative Policy Optimization (GRPO)（群体相对策略优化），这些轨迹提供了丰富的中间反馈，从而加速收敛并提高鲁棒性。在我们新发布的包含5300个多领域建模任务的基准SimuBench上的实验表明，使用SimuAgent微调的Qwen2.5-7B模型比标准RL（强化学习）基线收敛更快，建模精度更高，并且在同一基准上使用few-shot prompting（少样本提示）进行评估时，甚至超越了GPT-4o。消融实验证实，两阶段课程和抽象-重构数据增强进一步提高了泛化能力。SimuAgent完全在本地使用适度的硬件进行训练和运行，为工业模型驱动工程提供了一个保护隐私且具有成本效益的解决方案。SimuAgent弥合了LLMs与图形建模环境之间的差距，为工业环境下的AI辅助工程设计提供了实用的解决方案。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
                    "arxiv_id": "2601.05107",
                    "authors": "Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng",
                    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向下的核心研究。 1.  **核心贡献判断 (第一步)**: *   论文的核心贡献是提出了 **SteeM (Steerable Memory Agent)**，这是一个用于LLM智能体的新框架。 *   它旨在解决长期人机交互中智能体如何使用记忆的问题，即如何平衡“记忆锚定”与“创新”。这属于对LLM智能体核心组件（记忆机制）的构建和改进，而非简单的应用或基础设施研究。 2.  **符合核心关注点 (第二步)**: *   论文直接对应我的研究焦点中的 **“单智能体”** 类别。 *   具体而言，它涉及智能体的关键能力指标：**`Memory` (记忆)**。论文探讨了如何量化记忆依赖度以及如何动态调节记忆的使用，这是提升智能体在长期任务中表现的关键技术。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了“个性化人机协作”，但这只是应用场景，其核心在于提出一种可控的记忆机制框架，而非单纯解决特定领域的业务问题。 综上所述，该论文致力于改进LLM智能体的记忆机制，属于构建和演化LLM智能体的核心方法论研究，因此予以保留。",
                    "summary2": "本文旨在解决长期人机交互中智能体过度依赖历史记忆导致的“Memory Anchoring”问题，实现用户对记忆依赖度的动态控制。针对Research和Tutoring场景，我们提出了一种名为SteeM的框架，通过偏好对齐的数据生成、SFT和GRPO训练，使智能体能根据用户偏好动态调整记忆使用程度。我们在合成的长期交互数据集上，通过alignment error和reward score验证了其有效性，结果显示SteeM优于传统提示方法和记忆掩码策略。",
                    "summary_translation": "随着 LLM-based agents (基于大语言模型的智能体) 在长期交互中的应用日益广泛，cumulative memory (累积记忆) 对于实现个性化以及保持风格一致性至关重要。然而，大多数现有系统在记忆使用上采取了一种 all-or-nothing approach (全有或全无的方法)：纳入所有相关的过往信息可能导致 Memory Anchoring (记忆锚定)，即智能体受困于过去的交互；而完全排除记忆则会导致 under-utilization (利用不足) 以及重要交互历史的丢失。我们表明，智能体的 reliance on memory (记忆依赖) 可以被建模为一个显式的且用户可控的维度。我们首先引入了一种 memory dependence (记忆依赖度) 的 behavioral metric (行为指标)，用于量化过往交互对当前输出的影响。随后，我们提出了 **Stee**rable **M**emory Agent (可操控记忆智能体)，即 \\texttt{SteeM}，这是一个允许用户动态调节 reliance on memory (记忆依赖) 的框架，其调节范围涵盖促进创新的 fresh-start mode (全新开始模式) 到紧密遵循交互历史的 high-fidelity mode (高保真模式)。在不同场景下的实验表明，我们的方法始终优于 conventional prompting (传统提示) 和 rigid memory masking strategies (僵化的记忆掩码策略)，为个性化的人机协作提供了更为 nuanced (细致的) 且有效的控制手段。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
                    "arxiv_id": "2601.04888",
                    "authors": "Tongyu Wen, Guanting Dong, Zhicheng Dou",
                    "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心判断**: *   论文的核心贡献是提出了 **SmartSearch**，这是一个专门用于改进 **LLM-based search agents**（基于LLM的搜索智能体）的新框架。 *   它不是简单地将智能体作为工具应用到特定垂直领域（如医疗或金融），而是专注于解决智能体本身在执行搜索任务时的核心缺陷（即中间搜索查询质量不高），属于对智能体能力的底层构建和改进。 2.  **符合研究焦点**: *   **单智能体**: 论文明确针对搜索智能体的架构进行优化，涉及智能体的 **Tool Use**（使用搜索工具）和 **Planning**（生成查询的推理过程）。 *   **自我演化**: 论文引入了“过程奖励”和“查询优化”机制，并设计了一个三阶段的课程学习框架（模仿 -> 对齐 -> 泛化）。这种机制旨在让智能体通过反馈信号，逐步内化并自我改进其生成查询的能力。这符合“自我演化”中通过环境反馈进行自我完善和迭代的标准。 3.  **排除标准检查**: *   论文不涉及安全、对齐、可解释性或水印等排除主题。 *   论文不涉及多模态或视觉技术。 *   论文不涉及知识图谱或图神经网络。 综上所述，该论文通过提出新的方法论来提升LLM智能体的工具使用效率和自我优化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决基于LLM的搜索代理中中间搜索查询质量低下的问题。针对知识密集型任务，我们提出了一种结合过程奖励和查询优化的SmartSearch框架，并设计了三阶段课程学习策略。我们在2WikiMQA、HotpotQA等六个基准上，通过EM和F1分数验证了其有效性，显著提升了搜索效率和查询质量。",
                    "summary_translation": "基于大语言模型 (LLM) 的搜索智能体通过结合信息检索能力，在解决知识密集型问题方面展现出了巨大的潜力。现有工作主要集中在优化搜索智能体的推理范式，然而推理过程中中间搜索查询的质量却往往被忽视。结果是，生成的查询往往不准确，导致检索结果不理想，最终限制了搜索智能体的整体效能。为了缓解这一问题，我们提出了 SmartSearch，这是一个基于两个关键机制的框架：(1) 过程奖励，它通过双层信用评估为每个中间搜索查询的质量提供细粒度的监督；(2) 查询精炼，通过选择性精炼低质量搜索查询并基于这些精炼结果重新生成后续搜索轮次，从而促进查询生成的优化。为了使搜索智能体能够在过程奖励的指导下逐步内化提高查询质量的能力，我们设计了一个三阶段的课程学习框架。该框架引导智能体经历从模仿，到对齐，最终到泛化的递进过程。实验结果表明，SmartSearch 始终优于现有基线，额外的定量分析进一步证实了其在搜索效率和查询质量方面的显著提升。代码可在 https://github.com/MYVAE/SmartSearch 获取。",
                    "inspiration_trace": "基于对论文《SmartSearch: Process Reward-Guided Query Refinement for Search Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：搜索智能体的“阿喀琉斯之踵”\n**起点：** 作者首先关注到 LLM 搜索智能体在解决知识密集型任务时的巨大潜力。现有的研究范式（如 ReAct）已经证明了通过迭代调用搜索工具可以弥补模型的知识盲区。\n**发现盲区：** 尽管学术界在优化智能体的“推理范式”（如思维链设计、强化学习策略）上投入巨大，但作者敏锐地发现了一个被忽视的环节：**中间搜索查询的质量**。\n**直觉判断：** 智能体的推理能力再强，如果它向搜索引擎发出的指令是模糊、错误或冗余的，那么检索回来的信息就是无用的，后续的推理自然建立在沙堆之上。\n\n### 2. 问题聚焦：从“结果导向”到“过程诊断”\n**现象分析：** 作者通过案例分析（如文中提到的 Kevin McCarthy 演员与政治家混淆的例子）发现，一个微小的查询偏差（如漏掉“Actor”关键词）会导致检索结果完全跑偏，进而导致整个推理轨迹崩塌。\n**现有方法的局限：** 传统的强化学习训练通常只关注“结果奖励”，即最终答案是否正确。这种反馈是稀疏且滞后的。模型即使最终答对了，中间可能走了很多弯路；或者答错了，模型也不知道具体是哪一步的查询出了问题。\n**核心假设：** 如果能对每一个中间步骤的搜索动作进行细粒度的评估和反馈，就能从根本上提升智能体的信息获取能力。\n\n### 3. 机制设计一：构建“过程奖励”作为质检员\n**思考：** 如何定义一个“好的搜索查询”？作者意识到这不能仅靠单一指标。\n**逻辑拆解：**\n*   **新颖性：** 查询不应重复。如果这一步搜到的内容和上一步完全一样，那就是浪费算力。这可以通过规则（文档重叠度）低成本解决。\n*   **有用性：** 查询必须服务于最终答案。这需要语义理解。这一步的意图是否必要？检索结果是否包含预期信息？这需要模型来判断。\n**方案形成：** 结合两者，提出了“双层信用评估”。规则层负责查重（效率），模型层负责语义评估（质量），从而输出具体的分数和文本反馈。\n\n### 4. 机制设计二：引入“查询重写”作为修正员\n**思考：** 仅仅给低分查询打分是不够的，模型需要知道“什么是更好的”。\n**灵感来源：** 类似于人类写作时的修改过程。如果发现某一步查询质量低，为什么不直接修改它，然后基于修改后的查询重新跑一遍后续流程？\n**逻辑闭环：**\n1.  生成原始轨迹。\n2.  利用过程奖励找出“坏”的查询步骤。\n3.  基于反馈文本，利用模型重写该查询。\n4.  从重写点开始重新生成后续步骤，形成一条“修正后的轨迹”。\n**价值：** 这不仅生成了更好的数据，还天然构成了用于对比学习的成对数据。\n\n### 5. 训练策略：三阶段课程学习\n**思考：** 直接让模型学会上述所有能力太难，需要一个循序渐进的过程。\n**逻辑演进：**\n*   **阶段一（模仿）：** 先让模型看“好榜样”。利用过程奖励筛选出那些不仅答案正确、且中间查询也高质量的轨迹进行监督微调（SFT）。这确立了基准能力。\n*   **阶段二（对齐）：** 让模型学会“辨别好坏”。利用上一阶段的“查询重写”机制，构造“原始轨迹”和“修正轨迹”的对比对。通过 DPO（直接偏好优化），让模型偏好那些查询质量更高的路径。\n*   **阶段三（泛化）：** 让模型在实战中“探索”。在强化学习阶段，将查询重写机制融入 Rollout 过程。如果模型生成了坏查询，系统会自动修正并继续，让模型在探索中不断内化“如何写出好查询”的策略。\n\n### 6. 总结：逻辑链的终点\n作者最终构建的 SmartSearch 框架，其核心思想并非单纯堆砌算力或模型参数，而是通过**引入细粒度的过程监督**和**主动的轨迹修正机制**，解决了搜索智能体中“工具使用不当”这一核心瓶颈。从发现中间查询的重要性，到设计评估标准，再到利用修正数据驱动训练，形成了一个完整的逻辑闭环。"
                },
                {
                    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                    "arxiv_id": "2601.04861",
                    "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu",
                    "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（符合）**：论文的本质是构建和改进 LLM 智能体框架。作者提出了 OI-MAS（Orchestrating Intelligence），这是一个新颖的多智能体系统框架。它并非将智能体作为工具应用于特定垂直领域（如医疗或金融），而是针对多智能体系统本身的“计算效率低下”这一痛点，提出了新的架构解决方案（自适应模型选择策略和状态依赖路由机制）。这属于对智能体框架的改进和优化。 2.  **正面指标（强匹配）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并引入了机制来动态选择智能体角色和模型尺度。 *   **优化机制**：虽然涉及效率，但其核心是通过智能体层面的“路由”和“置信度感知”来实现的，这是提升多智能体系统性能的关键技术，属于构建智能体方法论的一部分。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉或图技术，也不是单纯的基础设施研究。 综上所述，该论文致力于解决多智能体协作中的效率与资源分配问题，是对 LLM 智能体架构的直接改进，符合“构建、改进或演化 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。",
                    "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。",
                    "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。"
                },
                {
                    "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search",
                    "arxiv_id": "2601.04703",
                    "authors": "Yiqun Chen, Lingyong Yan, Zixuan Yang, Erhan Zhang, Jiashu Zhao, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao",
                    "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **M-ASK (Multi-Agent Search and Knowledge)**，这是一个新的**多智能体框架**。 *   它旨在解决现有单体智能体在 Agentic Search 任务中的结构性瓶颈，而非仅仅将现有模型应用到一个特定领域。因此，它属于构建和改进 LLM 智能体的方法论研究，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **多智能体**: 论文将智能体解耦为两个互补的角色：`Search Behavior Agents`（负责规划和执行搜索动作）和 `Knowledge Management Agents`（负责聚合、过滤和维护内部上下文）。这体现了智能体间的协作与分工。 *   **智能体能力**: 涉及 `Planning`（规划搜索动作）、`Tool Use`（工具使用）以及 `Memory`（通过知识管理智能体维护内部上下文）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文在 Multi-hop QA（多跳问答）基准测试上进行了实验，但这只是为了验证框架的有效性，其核心在于提出了一种新的多智能体协作架构来优化 Agentic Search，而非单纯的应用型研究。 综上所述，该论文通过构建多智能体系统来改进 LLM 智能体的搜索与推理能力，精准契合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在解决单体 Agentic Search 架构因无约束输出、稀疏奖励和搜索噪声导致的训练不稳定及信用分配难题。针对复杂的多跳问答场景，我们提出了一种 M-ASK 多智能体框架，通过解耦 Search Behavior Agents 和 Knowledge Management Agents，并利用 turn-level dense rewards 实现联合优化。在 HotpotQA、2Wiki、Musique 等多跳问答基准上，通过 F1 Score 验证了其有效性和稳定性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "基于论文《Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题定位\n**——从“单体智能”的繁荣中看到结构性隐患**\n\n1.  **观察现象**：\n    *   随着 Agentic Search（智能体搜索）的兴起，LLM 被赋予了使用工具（如搜索引擎）进行迭代推理的能力。\n    *   现有的 SOTA 方法（如 Search-r1）大多采用**单体架构**，即由一个单一的 LLM 承担所有任务：规划、搜索、信息整合和最终回答。\n\n2.  **发现问题**：\n    *   作者发现这种“全能型”单体智能体在训练中表现出极不稳定的特征，容易崩溃。\n    *   虽然端到端的强化学习（RL）能提升性能，但在复杂的多跳问答任务中，单体模型往往难以收敛。\n\n### 第二阶段：深度诊断与归因分析\n**——解构“长视界信用分配”的死结**\n\n作者并未止步于“不稳定”的现象，而是深入剖析了单体架构背后的三个互为因果的致命缺陷，构成了一个**“毒性三角”**：\n\n1.  **无约束的输出长度**：\n    *   单体模型倾向于生成冗长的推理链。这不仅增加了计算成本，更重要的是拉长了决策轨迹，使得“最终结果”与“早期决策”之间的距离过远。\n2.  **稀疏的奖励信号**：\n    *   通常只有在任务结束时（回答正确与否）才有反馈。在长达数十步的推理中，模型无法知道哪一步是对的，哪一步是错的。\n3.  **搜索噪声**：\n    *   外部工具（搜索引擎）会引入无关或错误的信息。在单体架构中，这些噪声会直接累积在上下文中，干扰后续推理。\n\n**核心洞察**：\n上述三者共同导致了**长视界信用分配难题**。当一条冗长、充满噪声的轨迹最终只得到一个简单的对错反馈时，优化算法根本无法将奖励归因到具体的某个 Token 或动作上，导致训练梯度发散，模型崩溃。\n\n### 第三阶段：战略假设与范式转移\n**——从“分身乏术”到“术业专攻”**\n\n为了打破上述死结，作者提出了一个核心假设：**如果将“搜索行为”与“知识管理”解耦，就能从根本上隔离噪声并压缩轨迹。**\n\n1.  **角色解耦**：\n    *   不再让一个大脑既负责“找信息”又负责“记信息”。\n    *   **搜索行为代理**：只负责决策（搜什么、何时停）。\n    *   **知识管理代理**：只负责记忆（过滤噪声、更新状态）。\n2.  **预期效果**：\n    *   通过 KMA 的过滤，进入上下文的信息是高密度的，解决了“噪声”问题。\n    *   通过 SBA 的专注，每次生成的动作更短更精准，解决了“输出长度”问题。\n\n### 第四阶段：机制设计与优化逻辑\n**——构建“协作-反馈”闭环**\n\n有了架构假设，作者进一步思考：如何让这两个独立的团队协同工作并稳定训练？\n\n1.  **通信机制设计**：\n    *   设计了一个**结构化知识状态**。这不仅是共享内存，更是两个团队交互的唯一接口。SBA 写入查询，KMA 更新状态，双方通过这个紧凑的状态进行异步协作。\n\n2.  **解决信用分配难题（关键创新）**：\n    *   既然全局奖励太稀疏，那就引入**Turn-level Dense Rewards（轮级密集奖励）**。\n    *   **状态奖励**：给负责输出的 Agent（如 Answer Agent）基于当前答案质量的绝对分数。\n    *   **边际奖励**：这是最精妙的一笔。给负责迭代的 Agent（Search, Summary, Update）分配“边际增益”（$F1_{current} - F1_{previous}$）。\n    *   **逻辑**：如果这一轮搜索和更新让答案变好了，大家都有奖；如果没变好或变差了，大家都要负责。这迫使搜索团队必须找对信息，知识团队必须滤对噪声。\n\n### 第五阶段：验证与理论闭环\n**——从“假设”到“定律”**\n\n最后，作者通过实验验证了这一逻辑链条的完整性：\n*   **消融实验**：移除知识管理模块（KMA）导致性能下降，证明了“过滤噪声”的必要性；移除轮级奖励导致多跳任务崩溃，证明了“密集反馈”在长链路中的关键作用。\n*   **稳定性分析**：对比单体架构 Search-r1 的高崩溃率，M-ASK 实现了 0% 崩溃，证实了“解耦”确实解决了长视界训练的不稳定性。\n\n---\n\n**总结：作者的思考路径**\n从**单体架构的不稳定性**出发 $\\rightarrow$ 诊断出**长视界信用分配**是核心病灶 $\\rightarrow$ 提出通过**角色解耦**来切断噪声与长度的累积 $\\rightarrow$ 利用**边际奖励机制**将全局反馈转化为局部指导 $\\rightarrow$ 最终构建出一个既分工明确又利益绑定的多智能体协作系统。"
                },
                {
                    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
                    "arxiv_id": "2601.04620",
                    "authors": "Di Zhang",
                    "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 **AgentDevel**，这是一个将 LLM 智能体的自我改进过程重构为“发布工程”的新框架。它通过迭代运行智能体、生成质量信号、诊断故障并合成新版本，实现了智能体的自我演化。这完全符合“构建、改进或演化 LLM 智能体”的目标，不属于非演化型应用、非Agentic的基础推理或基础设施研究。 2.  **研究焦点匹配**: 论文直接对应研究焦点的第三点 **“自我演化”**。它提出了一种新的演化机制，即通过外部化的回归感知发布管道来迭代改进智能体，这与传统的内部自我反思或基于种群的搜索不同，强调了演化的稳定性和可审计性。 3.  **正面指标**: 论文明确涉及 `Self-Evolving`、`Self-Improvement`、`Iterative Improvement`、`Agentic AI` 以及 `LLM-based Agents` 等核心范式和能力。 4.  **排除标准**: 论文不涉及安全对齐、多模态技术或特定领域的垂直应用（如医疗、金融），也不属于基础设施优化。 5.  **结论**: 该论文为 LLM 智能体的演化提供了新的方法论视角，属于前沿研究，应予以保留。",
                    "summary2": "本文旨在解决自进化 LLM Agent 改进过程不稳定且难以审计的问题。针对执行密集型任务，我们提出了一种名为 AgentDevel 的发布工程管道，该管道包含实现无关 Critic、可执行诊断及翻转中心 Gating 机制。我们在 SWE-bench、WebArena 和 StableToolBench 上通过 Resolved rate、Success rate 及回归率等指标验证了其有效性，实现了性能显著提升且回归更少的稳定改进。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
                    "arxiv_id": "2601.04544",
                    "authors": "Jiuzhou Zhao, Chunrong Chen, Chenqi Qiao, Lebin Zheng, Minqi Han, Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang",
                    "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **TCAndon-Router (TCAR)**，这是一个用于多智能体协作的自适应推理路由器。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统（MAS）中的关键组件——路由器——进行了构建和改进。 *   论文设计了一个“协作执行流水线”，其中包括一个专门的“Refining Agent”来聚合和优化响应，这属于构建新的多智能体交互机制。 2.  **正面指标（第二步）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体特性**：涉及 `Collaboration`（协作执行流水线）、`Communication`（通过路由器进行任务分配）以及 `Agent Society`（支持动态智能体接入）。 *   **智能体能力**：利用 `Reasoning`（生成自然语言推理链）来辅助路由决策。 3.  **排除标准（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **综合结论**： 该论文致力于解决多智能体系统中的任务分配与协作效率问题，提出了新的路由机制和协作框架，直接对应研究课题中的“多智能体”方向，因此应当保留。",
                    "summary2": "本文旨在解决多智能体系统中现有路由策略难以动态接入新智能体及处理能力重叠导致的路由冲突问题。针对企业级应用中模糊或跨域的查询场景，我们提出了一种名为TCAndon-Router (TCAR) 的自适应推理路由器，通过生成自然语言推理链选择候选智能体，并利用Refining Agent聚合多智能体响应。在CLINC150、HWU64等公共数据集及腾讯云私有数据集上，通过Accuracy和F1等指标验证了其有效性。",
                    "summary_translation": "多智能体系统已成为构建高性能智能应用的有力范式。在这些系统中，负责确定哪些专家智能体应处理给定查询的路由器在整体性能中起着至关重要的作用。现有的路由策略通常分为两类：性能路由，它在不同规模的模型之间平衡延迟和成本；以及任务路由，它将查询分配给特定领域的专家以提高准确性。在现实世界的应用程序中，任务路由更为适用；然而，大多数现有方法依赖于静态单标签决策，这引入了两个主要局限性：随着业务领域的扩展，难以无缝集成新的智能体；以及由智能体能力重叠引起的路由冲突，最终降低了准确性和鲁棒性。\n\n为了解决这些挑战，我们提出了 TCAndon-Router (TCAR)：一种用于多智能体协作的自适应推理路由器。与传统路由器不同，TCAR 支持动态智能体接入，并在预测能够处理查询的候选智能体集合之前，首先生成一条自然语言推理链。此外，我们设计了一个协作执行管道，其中选定的智能体独立生成响应，随后由专门的精炼智能体将这些响应聚合并精炼为单一的高质量响应。\n\n在公共数据集和真实企业数据上的实验表明，TCAR 显著提高了路由准确性，减少了路由冲突，并在模糊场景中保持了鲁棒性。我们已在 https://huggingface.co/tencent/TCAndon-Router 发布了 TCAR，以支持未来关于可解释和协作多智能体路由的研究。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery",
                    "arxiv_id": "2601.04500",
                    "authors": "Yifei Gao, Jiang Wu, Xiaoyi Chen, Yifan Yang, Zhe Cui, Tianyi Ma, Jiaming Zhang, Jitao Sang",
                    "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **GUITester**，这是一个**多智能体框架**。它不仅仅是将现有的LLM智能体作为工具应用，而是构建了一个新的架构来解决特定问题。 *   该框架包含两个关键模块：**规划-执行模块 (PEM)** 和 **层次化反思模块 (HRM)**。这直接对应了研究焦点中的 **\"Agentic\" (规划)** 和 **\"自我演化\" (自我反思 Self-Reflection / 自我修正 Self-Correction)** 方向。 2.  **涉及多智能体与自我反思机制 (第二步 & 第四步)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   论文提出的 **HRM (Hierarchical Reflection Module)** 通过分析交互历史来解决归因歧义，这是一种典型的 **自我反思** 和 **自我修正** 机制。虽然它应用于GUI测试，但其核心在于改进智能体处理反馈和自我完善的能力，符合“自我演化”中通过反思进行迭代的定义。 3.  **特殊情况的正确处理 (第三步 & 第四步)**: *   **应用 vs. 方法论**: 虽然论文的应用场景是GUI测试（特定领域），但其核心在于提出了解决“目标导向掩蔽”和“执行偏差归因”的**新方法论**（即解耦导航与验证的框架），而非单纯的应用。 *   **多模态**: 论文虽然使用了多模态大模型 (MLLM) 来感知GUI界面，但视觉仅作为智能体感知环境的工具，研究的核心并非视觉模型的改进，而是智能体的决策与反思框架，因此符合“除非它们被用作智能体感知环境的工具”的例外条款。 综上所述，该论文在构建多智能体框架、引入规划机制以及实现自我反思方面做出了实质性贡献，完全符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有GUI智能体在探索性测试中难以自主发现缺陷的问题。针对目标导向掩蔽和执行偏差归因两大挑战，我们提出了一种名为GUITester的多智能体框架，通过规划执行模块（PEM）主动探测缺陷，并利用分层反思模块（HRM）解决归因歧义。我们在首个交互式benchmark GUITestBench上通过F1-score（Pass@3）验证了其有效性，结果显示GUITester达到48.90%，显著优于现有基线。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements",
                    "arxiv_id": "2601.04235",
                    "authors": "Hong Su",
                    "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种“主动获取反馈”的模型。这不仅仅是应用现有智能体解决具体问题，而是构建了一种新的智能体交互与评估机制，使智能体能够在没有预设测量的情况下，通过主动与环境交互来获取反馈。这属于构建和改进LLM智能体方法论的研究。 2.  **高度匹配核心关注点 (第二步)**: *   **单智能体**: 论文详细探讨了智能体如何进行“自主行动评估”、“与环境交互”以及“自主规划和调整行动”，这直接对应了Agentic AI中的规划、工具使用和环境交互能力。 *   **自我演化**: 论文引入了“自触发机制”，允许智能体根据内部目标（如准确性、效率）自主调整行动。这种基于反馈的自我调整和迭代优化，正是“自我演化”和“自我修正”的关键体现。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉核心或图技术等排除项。 *   它不是非演化型应用，而是提出了一种通用的智能体能力增强机制。 综上所述，该论文通过解决智能体如何在开放环境中自主获取反馈并自我完善的问题，为Agentic AI的演化和能力提升提供了新的方法论，因此应当保留。",
                    "summary2": "本文旨在解决AI智能体在开放环境中依赖预定义测量评估行动的问题。针对无预设测量的动态环境，我们提出了一种Actively Feedback Getting model，利用action-induced environmental differences进行反馈检测与主动干预。在文本场景和模拟环境上，通过语义相似度和LLM查询数量验证了其有效性。",
                    "summary_translation": "从环境中获取可靠反馈是 Intelligent agents (智能体) 评估其行为正确性并积累可复用知识的基本能力。然而，大多数现有方法依赖于 predefined measurements (预定义度量) 或 fixed reward signals (固定奖励信号)，这限制了它们在开放式和动态环境中的适用性，因为在这些环境中，新的行为可能需要以前未知的反馈形式。为了解决这些局限性，本文提出了一种 Actively Feedback Getting model (主动反馈获取模型)，其中 AI agent 主动与环境交互以发现、筛选和验证反馈，而无需依赖 predefined measurements (预定义度量)。该方法不假设显式的反馈定义，而是利用 action-induced environmental differences (动作引起的环境差异) 来识别未预先指定的目标反馈，这是基于动作不可避免地在环境中产生可测量变化这一观察。此外，本文引入了一种由提高准确度、精确度和效率等 internal objectives (内部目标) 驱动的 self-triggering mechanism (自触发机制)，以自主规划和调整行为，从而在没有 external commands (外部指令) 的情况下实现更快、更聚焦的反馈获取。实验结果表明，所提出的主动方法显著提高了 factor identification (因子识别) 的效率和鲁棒性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation",
                    "arxiv_id": "2601.04516",
                    "authors": "Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu",
                    "summary": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于 **Multi-Agent (多智能体)** 方向的前沿研究。具体判断依据如下： 1.  **核心判断 (符合)**: *   论文的核心贡献是提出了一种名为 **LinguaGame** 的新范式，这是一种基于博弈论的多智能体对话生成框架。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统的 **交互过程** 本身进行了改进，旨在通过建模意图和策略来提升智能体间的通信效率。这符合“构建、改进 LLM智能体”的核心目标。 2.  **研究焦点匹配 (多智能体)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   它的核心关注点是多智能体之间的 **Communication (通信)** 和 **Interaction (交互)**。 *   论文涉及智能体如何推断彼此的意图和策略，这属于多智能体协作与博弈中的高级认知能力。 3.  **排除标准检查 (通过)**: *   **非单纯应用**: 虽然论文在模拟法庭和辩论中进行了评估，但其核心贡献是通用的交互机制，而非解决特定领域的业务问题。 *   **非安全/对齐/多模态**: 论文不涉及安全对齐、视觉或多模态内容，纯粹关注语言层面的智能体交互逻辑。 4.  **正面指标**: *   包含核心范式关键词：`Multi-Agent Systems (MAS)`, `Game-Theoretic`。 *   包含多智能体能力关键词：`Communication`, `Negotiation` (隐含在辩论场景中), `Agent Society` (隐含在多智能体环境中)。 综上所述，该论文提出了一种改进多智能体通信机制的新方法，属于 Agentic AI 中多智能体交互的重要进展，因此予以保留。",
                    "summary2": "本文旨在解决基于LLM的Multi-Agent Systems中沟通效率低下的问题。针对模拟法庭和辩论场景，我们提出了一种名为LinguaGame的基于语言学的博弈论范式。该方法将对话建模为关于交际意图和策略的Signalling Game，并利用无训练的均衡近似算法在推理时优化决策。在模拟法庭和辩论数据集上，通过人工专家评估（涵盖清晰度、简洁性、论证和策略等指标），验证了该方法能显著提升对话质量和沟通效率。",
                    "summary_translation": "大语言模型推动了多智能体系统的发展，在该系统中，智能体通过自然语言进行交互，以解决复杂任务或模拟多方对话。现有关于基于大语言模型的多智能体系统的研究主要集中在架构设计方面，例如角色分配和工作流编排。与之不同，本文聚焦于交互过程本身，旨在通过帮助智能体利用语言更有效地传达其预期含义，从而提升通信效率。为此，我们提出了 LinguaGame，这是一种基于语言学的博弈论范式，用于多智能体对话生成。我们的方法将对话建模为基于交际意图和策略的信号博弈，并采用一种无训练的均衡近似算法进行求解，以实现推理时的决策调整。与先前的博弈论多智能体系统不同，后者的博弈设计往往与特定任务目标紧密耦合，而我们的框架依赖于基于语言学的推理，仅与特定任务存在最小程度的耦合。具体而言，该框架将对话视为一种意图性和策略性的通信，要求智能体推断他人旨在实现的目标（意图）以及其追求这些目标的方式（策略）。我们在模拟法庭庭审和辩论场景中对该框架进行了评估，人类专家的评估结果显示通信效率得到了显著提升。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-07": [
        {
            "name": "Artificial Intelligence",
            "count": 6,
            "papers": [
                {
                    "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
                    "arxiv_id": "2601.04035",
                    "authors": "Yilin Cao, Yufeng Zhong, Zhixiong Zeng, Liming Zheng, Jing Huang, Haibo Qiu, Peng Shi, Wenji Mao, Wan Guanglu",
                    "summary": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献在于提出了 **MobileDreamer**，这是一个基于世界模型的前瞻框架，旨在装备和增强移动 GUI 智能体。它不仅仅是将现有的智能体框架应用到一个新领域，而是针对现有 GUI 智能体“反应式”的局限性，提出了一种新的架构（包含文本草图世界模型和推演想象策略），以提升智能体的决策能力。这属于构建和改进 LLM 智能体的范畴。 2.  **正面指标（符合）**： *   **Agentic AI**: 论文明确聚焦于 GUI Agent 的构建。 *   **Planning**: 论文的核心在于通过世界模型预测行动结果，从而支持智能体进行更好的决策制定和行动选择，这属于智能体规划能力的增强。 3.  **排除标准（未触发）**： *   **多模态与视觉**: 虽然论文涉及处理屏幕图像和生成草图，但这属于“智能体感知环境的工具”。论文的核心贡献不是提出一种新的视觉算法或图像生成模型，而是利用视觉转换来构建辅助智能体规划的“世界模型”。因此，符合“除非它们被用作智能体感知环境的工具”这一例外条款。 4.  **特殊与模糊情况（符合）**： *   **推理/规划**: 论文通过引入世界模型，让智能体能够进行“前瞻”和“想象”，这属于智能体在复杂任务中进行多步推理和规划的高级形式，符合保留条件。 综上所述，该论文通过引入世界模型机制显著增强了单智能体的规划与决策能力，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决移动GUI代理在长视界任务中缺乏前瞻性规划的问题。针对移动设备屏幕交互场景，我们提出了一种基于文本草图世界模型的高效前瞻框架MobileDreamer。该方法通过文本草图世界模型预测未来状态，并利用推演想象策略构建预测树以优化动作选择。在Android World基准测试上，通过任务成功率（SR）验证了其有效性，显著提升了代理的性能。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
                    "arxiv_id": "2601.03555",
                    "authors": "Yuxuan Jiang, Francis Ferraro",
                    "summary": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance. Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions. Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断 (第一步)**: *   该论文的核心贡献是提出了 **SCRIBE**，这是一个用于训练**工具增强型智能体** 的强化学习框架。 *   论文明确指出其目标是解决训练可靠智能体时的“信用分配”难题，并致力于提升智能体的“高层规划”和“低层执行”能力。 *   这完全符合“构建、改进或演化 LLM智能体”的核心目标，属于 Agentic AI 的范畴，因此应予以保留。 2.  **正面指标匹配 (第二步)**: *   **智能体能力**: 论文重点涉及 `Tool Use` (工具使用) 和 `Planning` (规划)，特别是区分了高层规划与低层执行。 *   **演化机制**: 摘要中明确提到了“co-evolution across abstraction levels”（跨抽象层的协同演化）以及“emergence of effective high-level planning behaviors”（有效高层规划行为的涌现），这与研究焦点中的“自我演化”高度契合。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除领域。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文虽然涉及推理，但它是通过强化学习框架来训练智能体在多步任务中进行规划和工具使用，属于智能体的架构与训练方法，而非单纯提升LLM基础Token预测能力的数学或逻辑微调，因此符合保留条件。 **结论**: 该论文提出了一种新的训练框架来改进LLM智能体的工具使用和规划能力，并探讨了技能与规划之间的协同演化机制，精准契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决工具增强模型在多步推理中因信用分配困难导致的训练不可靠问题。针对多步推理和复杂工具交互场景，我们提出了一种SCRIBE框架，利用Skill Prototypes库进行中间层抽象，将开放式LLM评估转化为基于原型的约束验证任务。并在MATH、AIME25和BFCL V4基准上通过准确率和成功率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
                    "arxiv_id": "2601.04060",
                    "authors": "Jinwei Su, Qizhen Lan, Zeyu Wang, Yinghui Xia, Hairu Wen, Yiqun Duan, Xi Xiao, Tianyu Shi, Yang Jingsong, Lewei He",
                    "summary": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究范围，核心依据如下： 1.  **核心贡献符合 Agentic AI 定义 (第一步 & 第二步)**: *   论文明确提出了 \"ComfySearch, an **agentic framework**\"（一个智能体框架）。这不仅仅是将现有的LLM作为工具简单应用，而是构建了一个新的智能体架构来解决特定问题。 *   该框架的核心能力涉及 **Autonomous Exploration**（自主探索）和 **Reasoning**（推理），这直接对应了筛选标准中的“单智能体”方向，特别是 **Planning**（规划）和 **Tool Use**（工具使用，即操作ComfyUI组件）。 2.  **属于智能体规划与构建范畴 (第四步)**: *   论文解决的问题是如何在复杂的图约束下构建长视距的工作流。这属于智能体如何在复杂任务中进行多步规划和决策的研究范畴，而非单纯的LLM基础推理能力提升（如数学或逻辑题）。它关注的是智能体如何通过 \"validation-guided workflow construction\"（验证引导的工作流构建）来完成任务，这是一种典型的 Agentic 行为模式。 3.  **非简单的应用型论文 (第一步 & 第四步)**: *   虽然论文的应用场景是 ComfyUI（一个视觉生成平台），但论文的核心贡献在于**提出了一种能够自主探索组件空间并生成功能性工作流的智能体机制**，而不是仅仅展示LLM在生成图片上的效果。根据第四步的规则，只要核心是提出新的智能体机制（在此处是探索和构建机制），即使应用在特定领域，也应保留。 4.  **排除标准检查 (第三步)**: *   虽然涉及视觉内容生成，但论文的研究焦点不在于改进视觉模型本身，而在于控制生成流程的智能体框架，因此不属于被排除的“多模态与视觉”核心研究。 *   不涉及安全、对齐或基础设施问题。 综上所述，该论文提出了一种新的智能体框架来解决复杂的规划和构建问题，属于 Agentic AI 的核心研究范畴。",
                    "summary2": "本文旨在解决ComfyUI工作流生成中因组件复杂和图约束严格导致的低通过率和质量问题。针对ComfyUI工作流构建场景，我们提出了一种名为ComfySearch的智能体框架，采用Reasoning-as-action范式，结合State-aware validation和In-place repair确保结构正确性，并引入Entropy-adaptive branching处理长视距不确定性。我们在ComfyBench和GenEval上通过%Pass、%Resolve及GenEval分数验证了其有效性。",
                    "summary_translation": "AI生成内容已从单体模型演进至模块化工作流，特别是在 ComfyUI 等平台上，使用户能够定制复杂的创意流水线。然而，ComfyUI 中组件数量庞大，且在严格的图约束下难以保持长视距结构一致性，这往往导致通过率低下且生成的工作流质量有限。为解决上述局限性，我们提出了 ComfySearch，这是一个智能体框架，能够通过验证引导的工作流构建，有效探索组件空间并生成功能完备的 ComfyUI 流水线。实验表明，在复杂和创意任务中，ComfySearch 的性能显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决率以及更强的泛化能力。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Architecting Agentic Communities using Design Patterns",
                    "arxiv_id": "2601.03624",
                    "authors": "Zoran Milosevic, Fethi Rabhi",
                    "summary": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献在于提出了一种用于构建“Agentic Communities”（智能体社区）的架构方法和设计模式。这不仅仅是将LLM作为工具应用，而是构建了一个包含LLM智能体、Agentic AI实体和人类的复杂多智能体系统框架。它定义了智能体社区的组织结构、角色和协议，属于构建和改进LLM智能体系统的方法论。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确涉及 `Agentic AI` 和 `Multi-Agent Systems (MAS)`，特别是提出了“Agentic Communities”这一概念。 *   **多智能体能力**：摘要中详细讨论了智能体之间以及智能体与人类之间的 `Coordination`（协调）、`Communication`（通信）、`Negotiation`（谈判）以及 `Governance`（治理）机制。这些都是多智能体系统研究中的核心议题。 3.  **排除标准检查（第三步）**： *   虽然论文提到了“法律和伦理规则”以及“问责机制”，但其主要贡献是**架构框架**，而非单纯的安全算法或对齐技术研究。 *   虽然使用了“临床试验匹配”作为案例研究，但这仅用于验证框架的有效性，论文本质并非解决医疗领域的特定问题，而是提供通用的系统架构指导。 综上所述，该论文为构建复杂的多智能体协作系统提供了新的架构视角和设计模式，直接契合研究课题中关于“多智能体”的焦点，因此予以保留。",
                    "summary2": "本文旨在为构建生产级Agentic AI系统提供系统的架构指导。针对多智能体协调与治理的挑战，我们提出了一种基于ODP Enterprise Language (ODP-EL) 形式主义的设计模式框架，涵盖LLM Agents、Agentic AI及Agentic Communities三类模式。我们在临床试验匹配案例中验证了其有效性，实现了可验证的治理属性与问责机制。",
                    "summary_translation": "大语言模型 (Large Language Models, LLM) 及随后的 Agentic AI（智能体 AI）技术的快速演进，为构建复杂的、生产级系统提出了系统化架构指导的需求。本文提出了一种架构此类系统的方法，该方法采用了源于企业分布式系统标准、形式化方法和行业实践的设计模式。我们将这些模式划分为三个层级：LLM Agents（大语言模型智能体，即特定任务的自动化）、Agentic AI（智能体 AI，即自适应的目标寻求者）以及 Agentic Communities（智能体社区，即 AI 智能体和人类参与者通过正式角色、协议和治理结构进行协调的组织框架）。我们重点关注 Agentic Communities——这是一种包含 LLM Agents、Agentic AI 实体和人类的协调框架——其与企业及工业应用最为相关。借鉴分布式系统中既定的协调原则，我们将这些模式建立在一个形式框架之上，该框架明确了协作协议，规定了 AI 智能体和人类如何在受治理的生态系统中承担特定角色。该方法不仅提供了实用指导，还具备形式化验证能力，能够通过问责机制来表达组织、法律和伦理规则，从而确保对智能体间通信、谈判和意图建模进行可操作且可验证的治理。我们通过一项临床试验匹配的案例研究验证了该框架。我们的目标是为从业者提供切实可行的指导，同时保持企业部署在动态多智能体生态系统中所必需的形式严谨性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization",
                    "arxiv_id": "2601.03359",
                    "authors": "Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner",
                    "summary": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种“新颖的多智能体工作流”，用于优化提示词指令。这属于构建和改进 LLM 智能体系统的方法论研究，而非将现有智能体简单应用到特定垂直领域（如医疗、金融）。 2.  **正面指标匹配**： *   **多智能体**：标题和摘要中明确提到了“Multi-Agentic Workflow”，涉及多个智能体协同工作。 *   **自我演化/自我完善**：论文描述了使用“定量分数作为反馈来迭代地重写和改进”提示词，这符合自我演化中的“迭代改进”和“自我修正”机制。 *   **智能体能力**：该工作流体现了智能体的规划（解耦任务描述与约束）和工具使用（利用评估分数）能力。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然目标是“指令遵循”，但其手段是构建一个多智能体系统，这属于 Agentic AI 的范畴，而非单纯的非 Agentic 推理或数据集构建。 综上所述，该论文通过构建多智能体协作框架来实现任务的自我迭代优化，精准契合我对“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决 LLMs 难以严格遵守输出约束的问题。针对 InfoBench 数据集，我们提出了一种 evaluation-driven multi-agentic workflow，将任务描述与约束解耦，并利用定量反馈迭代优化约束。我们在 Llama 3.1 8B 和 Mixtral-8x 7B 上通过 compliance scores 验证了其有效性，显著提升了模型的指令遵循能力。",
                    "summary_translation": "大语言模型 (Large Language Models, LLMs) 虽然常能生成实质相关的内容，但往往未能遵守形式约束 (formal constraints)，从而导致输出结果在概念上正确，但在程序上存在缺陷。传统的提示词优化 (prompt refinement) 方法主要侧重于改写大语言模型需执行的主要任务描述，而忽视了那些作为响应验收标准 (acceptance criteria) 的细粒度约束 (granular constraints)。我们提出了一种新颖的多智能体工作流 (multi-agentic workflow)，该工作流将主要任务描述的优化与其约束解耦，并利用定量分数 (quantitative scores) 作为反馈，对二者进行迭代重写和改进。我们的评估表明，该方法生成的修订提示词能够使 Llama 3.1 8B 和 Mixtral-8x 7B 等模型产生显著更高的合规分数 (compliance scores)。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Evolving Programmatic Skill Networks",
                    "arxiv_id": "2601.03509",
                    "authors": "Haochen Shi, Xingdi Yuan, Bang Liu",
                    "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合“自我演化”**: 论文提出了 Programmatic Skill Network (PSN) 框架，其核心在于智能体如何通过经验构建、细化和重用技能库。摘要中明确指出技能库是“evolves through experience”（通过经验演化），并包含“progressive optimization”（渐进式优化）和“structural refactoring”（结构重构）等机制，这直接对应了筛选标准中的“Self-Evolving”、“Self-Improvement”和“Iterative Improvement”。 2.  **具备核心智能体能力**: 论文详细描述了智能体的自我反思和自我修正能力。具体而言，它利用 LLM 实例化了“REFLECT”机制进行结构化故障定位，以及“成熟度感知更新门控”来平衡技能的稳定性与可塑性。这符合筛选标准中的“Self-Correction”、“Self-Reflection”和“Memory”（技能库作为长期记忆）。 3.  **符合特殊处理规则**: 尽管论文是在 MineDojo 和 Crafter（具身/游戏环境）中进行实验，但根据第四步的规则，只要论文的核心贡献是提出一种新的“自我演化”机制（即 PSN 框架），即使应用在特定领域，也应予以保留。该论文并非单纯将 LLM 应用于游戏，而是提出了一套通用的技能演化算法。 综上所述，该论文不仅涉及 LLM 智能体的构建，更深入探讨了智能体如何通过反思和反馈实现自我演化和技能积累，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决开放具身环境中智能体持续获取、细化和重用技能的问题。针对开放任务流，我们提出了一种Programmatic Skill Network (PSN)框架，通过REFLECT机制进行结构化故障定位、成熟度感知更新门控及规范化结构重构，实现技能网络的持续演化。并在MineDojo和Crafter环境上通过技术树掌握迭代次数、累积奖励及技能保持率验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
                    "arxiv_id": "2601.03785",
                    "authors": "Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang",
                    "summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** 论文的核心贡献是提出了一种名为 \"Membox\" 的分层记忆架构，旨在解决 LLM 智能体在长程记忆中保持主题连续性的问题。这属于对 LLM 智能体核心组件（记忆系统）的构建与改进，而非将智能体作为工具应用到特定领域（非演化型应用），也非基础设施或基础模型推理能力的提升。 2.  **正面指标匹配（第二步）：** *   **核心范式：** 论文明确聚焦于 `LLM-based Agents`。 *   **智能体能力：** 论文的核心贡献点在于 `Memory`（记忆）。它通过引入 \"Topic Loom\" 和 \"Trace Weaver\" 等机制，优化了智能体存储和检索对话历史的能力，这直接对应了筛选标准中单智能体方向下的“记忆”子方向。 3.  **排除标准检查（第三步）：** 论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此未触犯任何排除规则。 4.  **总结：** 该论文致力于通过改进记忆机制来增强 LLM 智能体的连贯性和效率，属于单智能体研究中对基础能力（记忆）的深化，完全符合“构建、改进 LLM 智能体”的核心目标。",
                    "summary2": "本文旨在解决现有LLM Agent记忆系统因碎片化存储导致的话题连续性丢失问题。针对长对话场景，我们提出了一种名为Membox的分层记忆架构，利用Topic Loom将连续同话题对话分组为“记忆盒”，并通过Trace Weaver链接宏观话题以恢复事件时间线。我们在LoCoMo数据集上通过F1和BLEU指标验证了其有效性，在Temporal Reasoning任务上实现了高达68%的F1提升，且显著降低了上下文Token消耗。",
                    "summary_translation": "人机对话通常表现出话题连续性——一种通过时间上相邻的交互演变的稳定主题框架——然而大多数大语言模型 (LLM) 智能体记忆系统未能保留这一特性。现有设计遵循一种碎片化-补偿范式：它们首先将对话流分解为孤立的话语进行存储，然后试图通过基于嵌入的检索来重建连贯性。这一过程不可逆地破坏了叙事和因果流，同时导致检索偏向于词汇相似性。我们介绍了 membox，这是一种以 Topic Loom (话题编织器) 为中心的分层记忆架构，它以滑动窗口的方式持续监控对话，在存储时将连续的同一话题轮次分组为连贯的“记忆盒”。封装后的记忆盒随后由 Trace Weaver (轨迹编织器) 链接成长程事件时间轴轨迹，从而跨越对话间隔恢复宏观话题的重现。在 LoCoMo 数据集上的实验表明，Membox 在时序推理任务上实现了高达 68% 的 F1 值提升，优于竞争性基线模型（如 Mem0, A-MEM）。值得注意的是，Membox 在仅使用现有方法所需的一小部分上下文 token 的情况下取得了这些提升，突显了其在效率与有效性之间取得了卓越的平衡。通过显式建模话题连续性，Membox 为增强 LLM 智能体的连贯性和效率提供了一种受认知启发的机制。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
                    "arxiv_id": "2601.03641",
                    "authors": "Zheng Wu, Xingyu Lou, Xinbei Ma, Yansi Li, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang",
                    "summary": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心论文。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 **Agent-Dice**，这是一个专门用于解决 **LLM智能体持续学习** 问题的参数融合框架。它旨在解决智能体在动态环境中学习新任务时遇到的“灾难性遗忘”和“稳定性-可塑性困境”。这直接对应了我研究目标中的“自我演化”，即智能体通过经验、反思或环境反馈进行自我完善和迭代。这不是将智能体作为工具的应用，而是对智能体底层学习机制的改进。 2.  **符合正面指标 (第二步)**: *   **核心范式**: 论文明确关注 `LLM-based Agents`。 *   **演化机制**: 论文的核心主题是 `Continual Learning`（持续学习），这是 `Self-Evolving`（自我演化）的重要组成部分。它涉及 `Knowledge Updates`（知识更新）和 `Iterative Improvement`（迭代改进）。 *   **智能体能力**: 论文在 `GUI agents` 和 `tool-use agents` 领域进行了验证，体现了智能体与环境的交互能力。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐或水印问题。 *   虽然提到了 GUI agents（可能涉及视觉），但视觉仅作为智能体感知环境的工具，而非研究的核心（核心是参数融合和持续学习算法），因此不违反多模态排除规则。 *   不涉及图技术。 4.  **特殊情况处理 (第四步)**: 论文提出了一种新的“自我演化”机制（通过几何共识和曲率加权实现持续学习），即使它在特定的 GUI 或工具使用领域进行了实验，根据规则“如果论文的核心是提出一种新的‘自我演化’机制……也应该保留”，这篇论文应当被保留。 综上所述，Agent-Dice 通过改进智能体的知识更新机制，实现了智能体在连续任务中的自我演化，精准契合“LLM智能体及其演化”这一课题。",
                    "summary2": "本文旨在解决LLM智能体持续学习中的稳定性-可塑性困境。针对GUI和工具使用智能体场景，我们提出了一种基于方向一致性评估的参数融合框架Agent-Dice，通过几何一致性过滤和基于曲率的重要性加权解耦知识更新。在AITZ、AndroidControl、GUI-Odyssey和ToolACE数据集上，通过平均Z-score等指标验证了其有效性。",
                    "summary_translation": "基于 Large Language Model (LLM) 的 agents（智能体）通过与动态环境交互，显著扩展了 LLMs 的效用。然而，使 agents 能够在不发生 catastrophic forgetting（灾难性遗忘）的情况下持续学习新任务，仍然是一个关键挑战，即所谓的 stability-plasticity dilemma（稳定性-可塑性困境）。在这项工作中，我们认为这一困境根本源于未能明确区分跨任务共享的 common knowledge（通用知识）与由 task-specific interference（任务特定干扰）引入的 conflicting knowledge（冲突知识）。为解决这一问题，我们提出了 Agent-Dice，这是一种基于 directional consensus evaluation（方向一致性评估）的 parameter fusion framework（参数融合框架）。具体而言，Agent-Dice 通过两阶段过程解耦知识更新：利用 geometric consensus filtering（几何一致性过滤）剪枝 conflicting gradients（冲突梯度），并利用 curvature-based importance weighting（基于曲率的重要性加权）增强 shared semantics（共享语义）。我们提供了严格的理论分析，确立了所提出的 fusion scheme（融合方案）的有效性，并深入探讨了 stability-plasticity dilemma（稳定性-可塑性困境）的起源。在 GUI agents（图形用户界面智能体）和 tool-use agent（工具使用智能体）领域的大量实验表明，Agent-Dice 展现出了卓越的 continual learning performance（持续学习性能），且仅需极少的 computational overhead（计算开销）和 parameter updates（参数更新）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing",
                    "arxiv_id": "2601.03261",
                    "authors": "Shuo Lu, Yinuo Xu, Jianjie Cheng, Lingxiao He, Meng Wang, Jian Liang",
                    "summary": "Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**: 论文明确以 \"Deep Research agents\"（深度研究智能体）为研究对象，提出了 DeepResearch-Slice 这一神经符号框架。这属于构建和改进 LLM 智能体的方法论，符合“单智能体”的研究范畴。 2.  **正面指标**: 论文涉及智能体的核心能力，特别是工具使用后的信息处理。它解决了智能体在嘈杂环境中利用检索证据的瓶颈，改进了智能体的推理鲁棒性，属于对智能体内部机制的优化。 3.  **排除标准**: 论文不涉及安全、对齐、多模态或图技术；也不是针对特定垂直领域（如生物、金融）的单纯应用，而是提出了通用的改进框架。 4.  **特殊处理**: 虽然涉及推理，但这是在智能体框架下对“检索-利用”过程的改进，而非单纯提升模型的基础数学或逻辑预测能力，因此不属于“非Agentic的推理”。",
                    "summary2": "本文旨在解决Deep Research中模型因上下文盲区无法利用已检索证据的Retrieval-Utilization Gap问题。针对噪声环境下的检索文档，我们提出了一种名为DeepResearch-Slice的神经符号框架，通过预测精确的span索引执行显式文本切片和硬过滤。在六个基准测试上，通过任务准确率验证了其有效性，在冻结骨干网络上实现了73%的相对性能提升。",
                    "summary_translation": "Deep Research agents (深度研究智能体) 主要致力于优化 search policies (搜索策略)，以最大化 retrieval probability (检索概率)。然而，我们发现了一个关键瓶颈：retrieval-utilization gap (检索-利用差距)，即由于 noisy environments (噪声环境) 中的 context blindness (上下文盲区)，模型即使在成功检索到 gold evidence (黄金证据) 后，仍无法有效利用这些证据。为弥合这一差距，我们提出了 DeepResearch-Slice，这是一个简单而有效的 neuro-symbolic framework (神经符号框架)。与 implicit attention (隐式注意力) 机制不同，我们的方法通过预测精确的 span indices (片段索引)，在 reasoning (推理) 过程之前执行 deterministic hard filter (确定性硬过滤)。在六个 benchmarks (基准测试) 上进行的广泛评估表明，该方法带来了显著的 robustness gains (鲁棒性提升)。将该方法应用于 frozen backbones (冻结骨干网络) 时，实现了 73% 的相对性能提升（从 19.1% 提升至 33.0%），有效缓解了噪声干扰，且无需对 reasoning model (推理模型) 进行 parameter updates (参数更新)。这些结果凸显了在 open-ended research (开放式研究) 中引入 explicit grounding mechanisms (显式定位机制) 的必要性。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
                    "arxiv_id": "2601.03905",
                    "authors": "Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, Heng Ji, Heng Ji",
                    "summary": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献在于实证研究LLM智能体如何利用“世界模型”作为外部工具来进行“前瞻”规划。这直接属于“单智能体”研究范畴中的“工具使用”和“规划”能力。虽然论文主要揭示了当前智能体的局限性（即未能有效利用），但其目的是为了指导未来构建具有更强认知能力的智能体系统，属于对智能体构建和改进机制的探索，而非单纯的应用。 2.  **正面指标匹配（第二步）**：论文高度符合核心关注点。 *   **Agentic AI**: 明确以“Agents”为研究对象。 *   **Tool Use / Tool Augmentation**: 研究的核心是智能体如何将生成式世界模型作为“外部模拟器”工具来使用。 *   **Planning**: 论文聚焦于“anticipating future states”（前瞻）和“foresight”，这是智能体规划能力的高级形式。 3.  **排除标准与特殊情况处理（第三、四步）**： *   **多模态问题**: 尽管摘要提到了“vision-language models”和“visual question answering”，但视觉内容在此处是智能体感知的环境或工具（世界模型）的一部分，而非论文的研究核心。论文的核心在于智能体与该工具的交互机制，而非改进视觉模型本身，因此符合“作为工具使用”的例外情况。 *   **推理/规划**: 论文讨论的是智能体如何通过模拟进行多步推理和决策，属于Agentic层面的规划，而非单纯的LLM内部逻辑推理优化。 综上所述，该论文深入探讨了单智能体在工具使用和前瞻规划方面的关键问题，对构建和改进LLM智能体具有直接的指导意义，完全符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在探究当前智能体利用世界模型进行前瞻认知的能力。针对多样化的智能体决策和视觉问答任务，我们提出了一种“世界模型即工具”的评估框架，并在FrozenLake、Navigation等任务及VQA基准上通过成功率、准确率和调用率验证了其有效性。研究发现当前智能体很少调用世界模型，且强制使用会降低性能，核心瓶颈在于前瞻治理机制。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "LLM-Enabled Multi-Agent Systems: Empirical Evaluation and Insights into Emerging Design Patterns & Paradigms",
                    "arxiv_id": "2601.03328",
                    "authors": "Harri Renney, Maxim N Nethercott, Nathan Renney, Peter Hayes",
                    "summary": "This paper formalises the literature on emerging design patterns and paradigms for Large Language Model (LLM)-enabled multi-agent systems (MAS), evaluating their practical utility across various domains. We define key architectural components, including agent orchestration, communication mechanisms, and control-flow strategies, and demonstrate how these enable rapid development of modular, domain-adaptive solutions. Three real-world case studies are tested in controlled, containerised pilots in telecommunications security, national heritage asset management, and utilities customer service automation. Initial empirical results show that, for these case studies, prototypes were delivered within two weeks and pilot-ready solutions within one month, suggesting reduced development overhead compared to conventional approaches and improved user accessibility. However, findings also reinforce limitations documented in the literature, including variability in LLM behaviour that leads to challenges in transitioning from prototype to production maturity. We conclude by outlining critical research directions for improving reliability, scalability, and governance in MAS architectures and the further work needed to mature MAS design patterns to mitigate the inherent challenges.",
                    "category": "cs.MA",
                    "filter_reason": "1.  **核心判断 (符合)**: 论文的核心贡献在于形式化了LLM驱动的多智能体系统（MAS）的设计模式和范式，并定义了包括智能体编排、通信机制和控制流策略在内的关键架构组件。这属于“构建”和“改进”LLM智能体（特别是多智能体系统）的方法论研究，符合第一步中关于保留“构建LLM智能体”或“多智能体系统”方法论的要求。 2.  **正面指标 (匹配)**: 论文明确涉及 `Multi-Agent Systems (MAS)` 这一核心范式，并深入探讨了 `Agent Orchestration`（智能体编排）和 `Communication mechanisms`（通信机制），这些都是多智能体研究中的关键能力和正面指标。 3.  **排除标准 (未触发)**: *   **非演化型应用**: 尽管论文使用了电信、遗产管理等领域的案例研究，但其主要目的不是为了解决这些领域的具体业务问题，而是为了评估MAS架构和设计模式的实用性及开发效率。因此，它不属于仅将LLM作为工具应用到特定领域的“非演化型应用”。 *   **安全与对齐**: 论文虽然提到了治理和可靠性，但这并非其核心贡献，核心在于系统架构和设计模式。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 4.  **综合结论**: 该论文提供了关于如何构建、设计和评估多智能体系统的实证见解和架构框架，直接服务于“LLM智能体及其演化”中关于多智能体方向的研究目标。",
                    "summary2": "本文旨在形式化LLM赋能的多智能体系统（MAS）的设计模式并评估其实用性。针对电信安全、国家遗产资产管理和公用事业客户服务等真实场景，我们提出了一种基于ReAct智能体和动态编排的MAS设计范式，并在三个受控的容器化试点项目中通过开发周期、利益相关者反馈及UAT评分验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "When Numbers Start Talking: Implicit Numerical Coordination Among LLM-Based Agents",
                    "arxiv_id": "2601.03846",
                    "authors": "Alessio Buscemi, Daniele Proverbio, Alessandro Di Stefano, The Anh Han, German Castignani, Pietro Liò",
                    "summary": "LLMs-based agents increasingly operate in multi-agent environments where strategic interaction and coordination are required. While existing work has largely focused on individual agents or on interacting agents sharing explicit communication, less is known about how interacting agents coordinate implicitly. In particular, agents may engage in covert communication, relying on indirect or non-linguistic signals embedded in their actions rather than on explicit messages. This paper presents a game-theoretic study of covert communication in LLM-driven multi-agent systems. We analyse interactions across four canonical game-theoretic settings under different communication regimes, including explicit, restricted, and absent communication. Considering heterogeneous agent personalities and both one-shot and repeated games, we characterise when covert signals emerge and how they shape coordination and strategic outcomes.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的本质是研究 LLM 智能体在多智能体环境中的行为机制。它不是将智能体作为工具去解决生物、医疗等特定领域的应用问题，而是深入探讨智能体之间如何进行“隐式协调”和“隐蔽通信”。这属于构建和理解多智能体系统（Multi-Agent Systems）的方法论研究，因此应予以保留。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确涉及 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Communication`（特别是非显式的、隐蔽的通信）和 `Collaboration`（协调）。 *   **博弈与互动**：通过博弈论设置分析智能体间的战略互动，这属于多智能体研究中的社会行为和博弈范畴。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图技术等排除项。 **总结**：该论文的核心贡献在于揭示了 LLM 智能体在缺乏显式语言通道时，如何通过行动（如数字信号）进行隐式协调。这直接拓展了对于多智能体通信机制和协作行为的理解，符合“多智能体”这一研究焦点。",
                    "summary2": "本文旨在探究LLM智能体在多智能体环境中的隐式通信与协调机制。针对四种经典博弈论场景（Prisoner's Dilemma等），我们提出了一种基于数值序列的隐式通信机制，并在FAIRGAME框架下通过GPT-4o智能体进行了实验。通过分析不同通信条件下的合作水平与熵值，验证了隐式信号能产生结构化模式并有效影响战略结果。",
                    "summary_translation": "基于LLMs（大语言模型）的智能体越来越多地在需要strategic interaction（策略互动）和协调的multi-agent environments（多智能体环境）中运行。尽管现有工作主要集中在个体智能体或共享explicit communication（显式通信）的交互智能体上，但对于交互智能体如何进行隐式协调的研究尚不充分。特别是，智能体可能会进行covert communication（隐蔽通信），即依赖于嵌入在其行动中的间接或非语言信号，而非显式消息。本文对LLM驱动的多智能体系统中的隐蔽通信进行了game-theoretic（博弈论）研究。我们分析了在不同communication regimes（通信机制）下，包括显式、受限和缺失通信，四种典型博弈论设定中的互动。考虑到heterogeneous agent personalities（异构智能体人格）以及one-shot games（单次博弈）和repeated games（重复博弈），我们刻画了隐蔽信号何时涌现，以及它们如何塑造协调和策略结果。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-06": [
        {
            "name": "Artificial Intelligence",
            "count": 7,
            "papers": [
                {
                    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
                    "arxiv_id": "2601.03236",
                    "authors": "Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li",
                    "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 MAGMA 的“智能体记忆架构”。这属于构建和改进 LLM 智能体的方法论范畴，旨在解决智能体在长视界推理中的记忆检索和表示问题。它不是将现有智能体简单应用于特定领域，也不是单纯的基础设施优化，而是直接针对智能体核心组件（记忆）的架构创新。 2.  **包含核心关注点 (第二步)**: 论文明确涉及 `Agentic AI` 和 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它探讨了如何通过多图结构（语义、时间、因果、实体）来优化智能体的记忆存储和检索，从而提升推理能力。这完全符合单智能体方向中关于“记忆”机制的子方向。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 虽然摘要提到了“alignment between query intent and retrieved evidence”（查询意图与检索证据的对齐），但这指的是信息检索层面的语义匹配，而非 AI 安全领域的“对齐”。论文主要贡献不在于 Safety 或 Alignment。 *   **多模态与视觉**: 论文未涉及视觉或多模态内容。 *   **图**: 尽管论文使用了“Multi-Graph”（多图）技术，但这是作为实现智能体记忆架构的**手段**，而非研究图算法本身。论文的主题是“Agentic Memory Architecture”，属于智能体研究，因此不应被排除。 综上所述，这篇论文通过改进智能体的记忆机制来提升其性能，属于构建和演化 LLM 智能体的核心研究范围，符合筛选标准。",
                    "summary2": "本文旨在解决现有Memory-Augmented Generation系统因依赖单一语义相似度而限制长时程推理准确性和可解释性的问题。针对长上下文推理场景，我们提出了一种名为MAGMA的多图智能体记忆架构，通过语义、时间、因果和实体四个正交关系图解耦记忆表示，并采用策略引导的图遍历进行检索。在LoCoMo和LongMemEval数据集上通过LLM-as-a-Judge、F1及系统延迟等指标验证了其有效性。",
                    "summary_translation": "Memory-Augmented Generation (MAG，记忆增强生成) 通过引入外部记忆扩展了大语言模型，以支持长上下文推理。然而，现有方法主要依赖于单一整体记忆存储上的语义相似度，导致时间、因果和实体信息相互纠缠。这种设计限制了可解释性以及查询意图与检索证据之间的对齐，从而导致推理准确率不理想。在本文中，我们提出了 MAGMA，这是一种多图智能体记忆架构，它在正交的语义、时间、因果和实体图中表示每个记忆项。MAGMA 将检索过程构建为在这些关系视图上的策略引导遍历，从而实现查询自适应的选择和结构化上下文构建。通过将记忆表示与检索逻辑解耦，MAGMA 提供了透明的推理路径以及对检索过程的细粒度控制。在 LoCoMo 和 LongMemEval 数据集上的实验表明，MAGMA 在长视界推理任务中始终优于最先进的智能体记忆系统。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
                    "arxiv_id": "2601.02950",
                    "authors": "Xuan Yang, Furong Jia, Roy Xie, Xiong Xi, Hengwei Bian, Jian Li, Monica Agrawal",
                    "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（符合 Agentic AI 范畴）**： 虽然论文标题侧重于“推理”，但其核心贡献不仅仅是提出一种新的思维链变体，而是构建了一个名为 **BoT-R (Batch-of-Thought with Reflection)** 的架构。摘要明确指出，该方法是在 **\"multi-agent reflection architecture\"（多智能体反思架构）** 中实例化的。这表明论文的本质是构建了一个包含特定角色（Reflector 智能体）的智能体系统，而非单纯的模型推理能力提升。 2.  **符合正面指标（多智能体与自我反思）**： *   **多智能体**：论文明确提到了 \"multi-agent reflection architecture\"，涉及智能体之间的协作与信息交互（Reflector 执行联合评估以解锁互信息增益）。 *   **自我反思/修正**：论文的核心机制涉及 \"reflection\"（反思）和 \"detects errors\"（检测错误），这属于智能体的自我反思和自我修正能力范畴。 *   **Agentic 框架**：BoT-R 是一种新的 Agentic 框架，用于处理复杂任务，符合“构建、改进 LLM 智能体”的目标。 3.  **排除标准检查**： *   该论文不属于特定领域的非演化型应用（如医疗、法律），而是一种通用的推理架构改进。 *   虽然涉及推理，但它通过多智能体架构来实现，不属于“非 Agentic 的推理”排除项。 *   不涉及安全、对齐、多模态或图等排除领域。 综上所述，该论文通过引入多智能体反思架构来增强 LLM 的推理能力，属于 Agentic AI 和 Multi-Agent Systems 的研究范畴，符合筛选要求。",
                    "summary2": "本文旨在解决现有LLM推理系统独立处理查询导致跨实例信号丢失的问题。针对相关查询批次，我们提出了一种名为Batch-of-Thought (BoT) 的免训练方法，利用Reflector进行联合评估以实现跨实例学习。在六个基准和三个模型家族上，通过准确率、置信度校准和Token成本验证了其有效性，实现了性能提升和高达61%的成本降低。",
                    "summary_translation": "当前的 Large Language Model (大语言模型) 推理系统独立处理查询，忽略了诸如共享推理模式和一致性约束等有价值的 cross-instance signals (跨实例信号)。我们提出了 Batch-of-Thought (BoT)，这是一种 training-free (无需训练) 的方法，通过联合处理相关查询来实现 cross-instance learning (跨实例学习)。通过在批次间执行 comparative analysis (比较分析)，BoT 能够识别高质量的 reasoning templates (推理模板)，通过 consistency checks (一致性检查) 检测错误，并分摊 computational costs (计算成本)。我们在 multi-agent reflection architecture (多智能体反思架构) 中实例化了 BoT (BoT-R)，其中 Reflector (反思者) 执行 joint evaluation (联合评估)，以释放 isolated processing (孤立处理) 中无法获得的 mutual information gain (互信息增益)。在三个模型系列和六个基准测试上的实验表明，BoT-R 在将 inference costs (推理成本) 降低高达 61% 的同时，持续提高了 accuracy (准确性) 和 confidence calibration (置信度校准)。我们的理论和实验分析揭示了 batch-aware reasoning (批感知推理) 在何时以及为何能使 Large Language Model (大语言模型) 系统受益。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
                    "arxiv_id": "2601.02702",
                    "authors": "Shuhaib Mehri, Priyanka Kargupta, Tal August, Dilek Hakkani-Tür",
                    "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”与“自我演化”的交叉领域。 1.  **核心贡献符合要求**：论文的核心在于构建了一种“长期协作智能体”，并提出了相应的基准测试。这不仅仅是应用现有模型，而是提出了一种新的智能体框架，旨在解决智能体如何适应环境（用户）的问题。 2.  **符合“单智能体”方向**：论文明确聚焦于智能体的关键能力——**记忆**和**自我反思**。它详细描述了如何通过持久化的记忆来存储用户偏好，以及如何通过反思机制来更新记忆，这直接对应筛选标准中的“Agentic: 记忆、自我反思”。 3.  **符合“自我演化”方向**：论文强调智能体随着交互经验的积累，能够不断细化用户偏好并改进协作质量。这种通过经验反馈进行迭代改进和自我完善的过程，正是“自我演化”的核心体现。 4.  **排除标准检查**：论文不属于特定领域的非演化型应用（其核心是智能体机制的通用改进），也不涉及安全对齐、多模态或图技术等排除领域。 综上所述，该论文在构建具备记忆和自我演化能力的LLM智能体方面做出了实质性贡献，应予保留。",
                    "summary2": "本文旨在解决对话代理在多会话协作中学习并利用用户偏好以提升协作质量的问题。针对多会话协作问题求解场景，我们提出了一种配备持久化记忆的长期协作代理及基于强化学习的记忆更新框架，并在MULTI SESSION COLLAB基准上通过Task Success、User Effort和Conversation Length验证了其有效性。",
                    "summary_translation": "随着 conversational agents (对话智能体) 积累与用户协作的经验，适应用户偏好对于建立长期关系以及随着时间的推移提升协作质量至关重要。我们介绍了 MultiSessionCollab，这是一个 benchmark (基准测试)，旨在评估智能体在跨多个会话的过程中学习用户偏好并利用这些偏好提升协作质量的能力。为了开发能够在此类场景中取得成功的智能体，我们提出了 long-term collaborative agents (长期协作智能体)，该智能体配备了 memory (记忆)，能够随着交互经验的积累而持久保存并细化用户偏好。此外，我们证明了可以从 MultiSessionCollab 中的 user simulator (用户模拟器) 行为中提取 learning signals (学习信号)，从而训练智能体生成更全面的 reflections (反思内容) 并更有效地更新其 memory (记忆)。大量实验表明，为智能体配备 memory (记忆) 能够改善长期协作效果，从而带来更高的任务成功率、更高效的交互以及更低的用户负担。最后，我们进行了一项人类用户研究，结果表明 memory (记忆) 有助于在真实场景中提升用户体验。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents",
                    "arxiv_id": "2601.02643",
                    "authors": "Mehmet Kurmaz",
                    "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向中的“工具使用”和“规划”子方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献在于提出了一种新的方法论（偏好感知查询修复）和一个基准（AWARE-US），旨在解决“工具调用智能体”在查询结构化数据库时遇到的“不可行性”问题。 *   这不是将智能体简单应用于特定领域（如医疗或金融）的应用型论文，而是针对智能体在工具使用过程中的通用能力缺陷（查询失败处理）进行改进和构建。 *   它不属于基础设施优化，也不是非Agentic的基础推理提升。 2.  **正面指标匹配 (第二步)**: *   论文明确涉及 `LLM-based Agents` 和 `Tool Use / Tool Augmentation`。 *   它关注智能体如何根据对话上下文推断用户意图，并动态调整查询策略（放松约束），这属于智能体的 `Planning` 和决策能力范畴。 3.  **排除标准检查 (第三步)**: *   虽然论文提到了“preference alignment”（偏好对齐），但这里的对齐是指智能体在执行任务时对用户具体约束条件的偏好（例如用户更看重价格还是速度），而非AI安全、伦理或价值观层面的“对齐”。因此，不应被排除。 *   论文不涉及多模态视觉、图技术或安全防御机制。 4.  **特殊处理 (第四步)**: *   论文关于智能体如何处理工具调用失败并进行自我修正（查询修复），属于Agentic的推理与规划范畴，符合保留条件。 综上所述，该论文致力于改进LLM智能体的工具使用鲁棒性和交互规划能力，符合“构建、改进LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决Tool-calling agents在查询结构化数据库时面临的infeasibility问题。针对基于personas的对话场景，我们提出了一种Preference-Aware Resolution框架，包含local weighting、global one-shot weighting和pairwise ranking三种约束重要性推断方法。我们在AWARE-US benchmark上通过Relax match和Car match等指标验证了其有效性，结果显示Local weighting方法在汽车推荐中与用户偏好一致性达到48%。",
                    "summary_translation": "调用工具查询结构化数据库的对话代理通常面临两个相互关联的问题：underspecification（约束不足，即缺乏运行精确查询所需的约束）和 infeasibility（不可行性，即完全指定的查询返回空集，因为没有项目满足所有约束）。现有研究通常以“无结果”作为回应，或利用 ad hoc rules（特设规则）放宽约束，这可能会因丢弃用户最关心的需求而违背用户意图。我们将 infeasibility handling（不可行性处理）构建为一个 preference-aware query repair（感知偏好的查询修复）问题：当查询不可满足时，代理应当放宽对用户而言重要性最低的约束。我们提出了三种基于 LLM（大语言模型）的方法，用于从对话中推断相对约束重要性：(1) local weighting（局部加权），(2) global one-shot weighting（全局一次性加权），以及 (3) pairwise ranking（成对排序）。实验结果表明，local weighting（局部加权）实现了最佳的 preference alignment（偏好对齐），而 global weighting（全局加权）在正确的 constraint relaxation（约束放宽）方面表现最佳。我们还介绍了 AWARE-US，这是一个包含 persona-grounded queries（基于人设的查询）的基准数据集，要求代理通过对话消除请求歧义，并以与 persona-implied preferences（人设隐含偏好）一致的方式解决 infeasibility（不可行性）。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
                    "arxiv_id": "2601.02553",
                    "authors": "Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao",
                    "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **SimpleMem**，这是一个专门为 **LLM智能体** 设计的高效记忆框架。 *   它属于 **构建/改进 LLM智能体** 的范畴，旨在解决智能体在长期交互中的记忆管理问题，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或硬件加速的研究。 2.  **正面指标匹配（第二步）**： *   论文直接涉及 **Agentic AI** 和 **LLM-based Agents** 的核心范式。 *   论文重点解决了智能体的 **Memory（记忆）** 能力，这是单智能体方向的关键子方向之一。摘要中提到的“管理历史经验”、“语义结构化压缩”和“递归记忆整合”都是为了增强智能体的记忆机制，使其能更好地支持长期交互。 3.  **排除标准检查（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 综上所述，该论文致力于改进LLM智能体的核心组件（记忆系统），属于单智能体研究范畴，符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决LLM Agent在长期交互中因上下文限制和冗余信息导致的记忆管理低效问题。针对复杂环境下的长上下文交互场景，我们提出了一种基于语义无损压缩的SimpleMem框架，包含语义结构化压缩、递归记忆整合和自适应查询感知检索三阶段流程。我们在LoCoMo benchmark上通过F1 score和Token Cost验证了其有效性，实现了平均F1提升26.4%且推理Token消耗降低30倍。",
                    "summary_translation": "为了在复杂环境中支持可靠的长期交互，LLM agents（大语言模型智能体）需要能够高效管理历史经验的 memory systems（记忆系统）。现有方法要么通过 passive context extension（被动上下文扩展）保留完整的交互历史，导致大量冗余，要么依赖 iterative reasoning（迭代推理）来过滤噪声，从而产生高昂的 token costs（Token 成本）。为了应对这一挑战，我们提出了 SimpleMem，这是一种基于 semantic lossless compression（语义无损压缩）的高效 memory framework（记忆框架）。我们提出了一个旨在最大化 information density（信息密度）和 token utilization（Token 利用率）的 three-stage pipeline（三阶段流水线）：(1) \\textit{Semantic Structured Compression}（语义结构化压缩），该阶段应用 entropy-aware filtering（熵感知过滤）将非结构化交互提炼为紧凑的 multi-view indexed memory units（多视图索引记忆单元）；(2) \\textit{Recursive Memory Consolidation}（递归记忆整合），这是一个异步过程，将相关单元整合为 higher-level abstract representations（高层抽象表示）以减少冗余；以及 (3) \\textit{Adaptive Query-Aware Retrieval}（自适应查询感知检索），该阶段根据 query complexity（查询复杂度）动态调整 retrieval scope（检索范围），以高效构建精确的上下文。在 benchmark datasets（基准数据集）上的实验表明，我们的方法在准确性、retrieval efficiency（检索效率）和 inference cost（推理成本）方面始终优于 baseline approaches（基线方法），实现了平均 26.4% 的 F1 提升，同时将 inference-time token consumption（推理时 Token 消耗）减少了多达 30 倍，展示了性能与效率之间的卓越平衡。代码可在 https://github.com/aiming-lab/SimpleMem 获取。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices",
                    "arxiv_id": "2601.02732",
                    "authors": "Lingzhe Zhang, Tong Jia, Yunpeng Zhai, Leyi Pan, Chiming Duan, Minghua He, Mengxi Jia, Ying Li",
                    "summary": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“构建LLM智能体”的目标**： 尽管论文的应用场景是微服务中的根因定位（特定领域），但其核心贡献并非简单地将现有LLM作为工具应用，而是提出了一个新的框架 **AMER-RCL**。该框架包含两个关键的智能体组件： *   **Recursive Reasoning RCL engine**：明确被定义为一个**多智能体框架**，用于执行递归推理。 *   **Agentic Memory**：一种智能体记忆机制，用于增量累积和重用推理过程。 2.  **高度匹配“正面指标”**： *   **多智能体**：论文明确提出了多智能体框架来处理告警和细化候选原因。 *   **智能体能力**：涉及 `Memory`（智能体记忆）、`Reasoning`（递归推理）以及 `Multi-Agent` 协作。 *   **核心范式**：论文标题和摘要中多次强调 \"Agentic Memory\" 和 \"Agentic\"，完全符合 Agentic AI 的研究焦点。 3.  **通过“排除标准”和“特殊情况”检查**： *   虽然涉及特定领域（微服务），但根据第四步的“自我演化的应用”逻辑（此处虽非演化，但同理），如果论文的核心是提出一种新的智能体机制（如这里的递归推理+记忆机制），即使应用在特定领域，也应保留。这区别于仅仅调用API解决领域问题的“非演化型应用”。 *   论文不涉及安全、对齐、多模态视觉或图神经网络等排除内容。 综上所述，该论文在构建多智能体框架和智能体记忆机制方面做出了实质性贡献，属于 Agentic AI 和 Multi-Agent Systems 的前沿研究。",
                    "summary2": "本文旨在解决现有LLM方法在微服务根因定位中推理浅显及缺乏跨告警复用的问题。针对微服务系统中的多告警场景，我们提出了一种名为AMER-RCL的智能体记忆增强递归推理框架。该框架利用递归推理引擎进行深度分析，并通过智能体记忆复用历史推理结果。在AIOPS 2022、Train-Ticket和FAMOS-Mall数据集上，通过Recall@k、MRR及推理延迟验证了其有效性，显著优于现有方法。",
                    "summary_translation": "随着当代微服务系统日益普及且日趋复杂——通常由数百甚至数千个细粒度、相互依赖的子系统组成——其故障发生频率也随之增加。因此，确保系统可靠性需要准确的根因定位。尽管针对该任务已探索了许多传统的基于图和深度学习的方法，但它们往往严重依赖预定义模式，难以适应不断演变的运行环境。因此，近期提出了多种基于大语言模型的方法。然而，这些方法仍面临两大主要局限：一是浅层的、以症状为中心的推理损害了准确性；二是缺乏跨告警重用，导致推理冗余和高延迟。本文对站点可靠性工程师如何定位故障根因进行了全面研究，汲取了来自多个组织的专业人士的见解。调查结果显示，专家根因分析表现出三个关键特征：递归性、多维扩展和跨模态推理。基于这些发现，我们提出了 AMER-RCL，一种用于微服务根因定位的智能体记忆增强递归推理框架。AMER-RCL 采用了递归推理 RCL 引擎，这是一个多智能体框架，通过对每个告警执行递归推理来逐步细化候选原因；同时，智能体记忆增量地积累并重用时间窗口内先前告警的推理结果，以减少冗余探索并降低推理延迟。实验结果表明，AMER-RCL 在定位准确性和推理效率方面均持续优于最先进的方法。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance",
                    "arxiv_id": "2601.02454",
                    "authors": "Saba Naqvi, Mohammad Baqar, Nawaz Ali Mohammad",
                    "summary": "Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“多智能体”与“自我演化”方向**： 论文的核心贡献是提出了一种“智能体多模型测试框架”。这不仅仅是一个应用，而是一个新的**多智能体系统（MAS）**架构。该系统包含三个具有不同角色的智能体（测试生成、执行与分析、审查与优化），它们通过协作完成任务，这直接对应了筛选标准中的“多智能体”方向。 2.  **具备明确的“自我演化”机制**： 摘要中明确提到这是一个“闭环、自我修正系统”，利用“沙箱执行”和“强化信号”进行“迭代再生或修补”。这种通过环境反馈（执行结果、覆盖率指标）来引导智能体自主改进和迭代的过程，完全符合“自我演化”中关于自我完善、自我修正和迭代改进的定义。 3.  **属于Agentic AI的构建而非单纯应用**： 虽然论文的应用场景是软件测试（特定领域），但根据筛选标准第四步的“自我演化的应用”例外规则，只要论文的核心是提出一种新的“自我演化”机制或Agentic框架，即使应用在特定领域也应保留。本文重点在于构建了一个能够自主规划、协作和反思的智能体框架，而非简单地将现有LLM作为工具生成测试代码。 综上所述，该论文在多智能体协作、自我修正机制以及闭环演化方面具有明确的方法论贡献，符合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决现有AI测试生成缺乏执行反馈导致测试无效的问题。针对微服务应用，我们提出了一种Agentic Testing Architecture (ATA)，即包含Test Generation Agent、Execution and Analysis Agent和Review and Optimization Agent的多智能体闭环协作框架。在开源及企业应用数据集上，通过代码覆盖率、无效测试率和人工工作量等指标验证了其有效性，实现了无效测试减少60%及覆盖率提升30%。",
                    "summary_translation": "软件测试正迈向智能自动化，但现有的 AI-based test generators（基于AI的测试生成器）仍受限于静态、单次输出的模式。由于缺乏 execution aware feedback（执行感知反馈），这些工具常生成无效、冗余或不可执行的测试用例。本文提出了一种 agentic multi-model testing framework（代理多模型测试框架），这是一个闭环、自纠正系统。在该系统中，Test Generation Agent（测试生成代理）、Execution and Analysis Agent（执行与分析代理）以及 Review and Optimization Agent（审查与优化代理）协同工作，生成、执行、分析并完善测试，直至达到收敛状态。通过利用 sandboxed execution（沙箱执行）、详细的 failure reporting（失败报告）以及对失败测试的 iterative regeneration（迭代重新生成）或 patching（修补），该框架能够自主提升测试质量并扩大覆盖率。该框架集成于 CI/CD-compatible pipeline（兼容CI/CD的流水线）中，利用来自 coverage metrics（覆盖率指标）和 execution outcomes（执行结果）的 reinforcement signals（强化信号）来指导测试的完善过程。针对 microservice based applications（基于微服务的应用程序）进行的实证评估表明，与 single-model baselines（单模型基线）相比，该框架将无效测试减少了高达 60%，覆盖率提升了 30%，并显著降低了人工投入。这证明了 multi-agent（多代理）、feedback-driven loops（反馈驱动循环）能够将软件测试演变为一个自主的、持续学习的 quality assurance ecosystem（质量保证生态系统），以支持 self-healing（自愈）和 high-reliability codebases（高可靠性代码库）。",
                    "inspiration_trace": "基于论文《The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance》，以下是对作者产出该核心方法逻辑链的系统性推演，旨在还原其从宏观观察到微观实现的思考过程：\n\n### 1. 宏观背景：软件复杂性与自动化瓶颈\n**观察：**\n现代软件工程已转向云原生、微服务架构和DevOps模式，代码迭代速度极快，依赖关系复杂。传统的手工测试或基于规则的自动化脚本已无法跟上这种交付节奏，成为效率瓶颈。\n\n**思考：**\n必须引入更高阶的智能化手段。近年来，大语言模型（LLMs）在代码理解和生成方面表现出色，似乎为解决这一瓶颈提供了契机。学术界和工业界开始尝试利用LLMs自动生成测试用例。\n\n### 2. 核心痛点：LLM的“静态生成”与“执行盲区”\n**深入观察：**\n尽管现有的基于LLM的测试生成工具（如Codex, GPT-4）能够快速产出代码，但作者发现了一个关键问题：**这些生成是“一次性”的**。LLM像是在真空中写作，它不知道生成的代码在真实环境中能否运行。\n\n**逻辑断层：**\n实证数据显示，超过40%的LLM生成的测试用例在首次执行时就会失败（如缺少依赖、语法错误、逻辑不匹配）。这是因为现有方法缺乏“执行感知”的反馈机制。LLM无法从自己的错误中学习，导致产生了大量无效、冗余的垃圾代码，反而增加了人工清理的成本。\n\n**假设提出：**\n如果能让测试生成系统具备“自我反省”和“自我修正”的能力，即像人类测试人员一样——写代码 -> 运行 -> 报错 -> 修改 -> 再运行，那么测试的质量和有效性将大幅提升。\n\n### 3. 概念跃迁：从“单点工具”到“多智能体协作”\n**灵感来源：**\n作者观察到AI领域正在兴起“Agentic AI”（智能体AI）和多智能体系统（如AutoGen, SWE-Agent）。这些系统通过让多个专门的AI角色相互协作、对话来解决复杂任务，而非依赖单一模型。\n\n**类比推理：**\n软件测试本身就是一个团队协作过程：有人写测试，有人执行测试，有人分析结果。为什么不让AI也模仿这种社会分工？\n\n**方法论雏形：**\n不再使用一个单一的LLM模型完成所有工作，而是设计一个**多智能体系统**。将测试流程拆解为不同的专业角色，让它们各司其职，形成一个流水线。\n\n### 4. 方法论构建：闭环反馈与收敛机制\n**架构设计：**\n为了实现上述假设，作者构建了三个核心智能体，形成了一个闭环：\n1.  **生成者：** 负责根据需求编写初始测试。\n2.  **执行者：** 负责在沙盒环境中运行测试，并收集覆盖率数据和报错信息。\n3.  **审查与优化者：** 负责分析执行结果，诊断失败原因，并指导生成者进行修复。\n\n**逻辑核心：**\n这个系统的核心不在于单个智能体的能力，而在于它们之间的**交互循环**。作者引入了“收敛”的概念：系统不是无限循环，而是设定了明确的停止条件（如覆盖率>95%，失败率<2%）。这将其从简单的“尝试”转变为一种“优化过程”。\n\n**关键创新点：**\n作者意识到，只有当“执行反馈”被转化为“自然语言指令”并重新输入给LLM时，真正的自我修正才发生。因此，必须建立一个共享的记忆库，让智能体能记住之前的错误，避免重蹈覆辙。\n\n### 5. 最终愿景：迈向自主演进的QA生态系统\n**验证与结论：**\n通过实验，作者验证了这种多智能体闭环模式确实能显著降低无效测试比例，提高覆盖率。这证明了“反馈驱动”优于“静态生成”。\n\n**思想升华：**\n最终，作者将这一方法定义为“Agentic Testing”。这不仅仅是一个工具，而是一个**自主的质量保证生态系统**。它标志着软件测试从“人类辅助AI”转向了“AI自主协作”，测试系统具备了类似生物的“自愈”和“适应”能力，能够随着代码库的演变而自动演进。\n\n---\n\n**总结：**\n作者的思考路径遵循了 **“发现问题（静态生成无效） -> 寻找类比（人类协作流程） -> 引入范式（多智能体系统） -> 构建机制（闭环反馈与收敛） -> 实现愿景（自主QA）”** 的完整逻辑链条。其核心洞察在于：**没有反馈的生成是盲目的，只有引入执行反馈和多智能体协作，AI测试才能真正落地。**"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 4,
            "papers": [
                {
                    "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
                    "arxiv_id": "2601.03192",
                    "authors": "Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen, Zhiyu Li, Feiyu Xiong, Yutao Qi, Bo Tang, Muning Wen",
                    "summary": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。具体判断依据如下： 1.  **核心判断（符合）**： 论文的核心贡献是提出了 **MemRL**，这是一个旨在实现 **Self-Evolving Agents（自我演化智能体）** 的框架。它解决的是智能体如何通过经验进行自我完善和迭代的问题，而非仅仅是将LLM作为工具应用到特定领域。这直接对应了研究目标中的“自我演化”方向。 2.  **正面指标（高度匹配）**： *   **核心范式**：论文标题和摘要中明确提到了 `Self-Evolving Agents` 和 `Reinforcement Learning`，属于核心关注点。 *   **智能体能力**：论文重点研究了 `Memory`（情景记忆 Episodic Memory）和 `Self-Improvement`（通过环境反馈持续改进）。 *   **演化机制**：论文提出了一种非参数强化学习机制，通过环境反馈在运行时迭代改进 Q-values（效用），从而实现智能体的持续演化，符合 `Iterative Improvement` 和 `Self-Refine` 的定义。 3.  **排除标准（未触发）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然论文在 HLE、BigCodeBench 等基准上进行了实验，但这些是用于验证智能体能力的通用任务，不属于“非演化型应用”的排除范畴。 4.  **特殊与模糊情况处理**： *   论文属于典型的 **自我演化** 机制研究。它提出了一种新的“在情景记忆上进行运行时强化学习”的方法，使智能体能够在不更新模型权重的情况下解决稳定性-可塑性困境并实现持续改进。这完全符合“保留：如果论文的核心是提出一种新的‘自我演化’机制”的规则。 综上所述，该论文专注于构建新的框架以实现LLM智能体的自我演化和记忆优化，是“LLM智能体及其演化”课题下的高质量相关论文。",
                    "summary2": "本文旨在解决LLM在部署后难以持续自我进化及避免灾难性遗忘的问题。针对需要Runtime Continuous Learning的场景，我们提出了一种MemRL框架，通过非参数强化学习优化情节记忆，采用基于Q值的Two-Phase Retrieval机制区分高价值策略与噪声。并在HLE、BigCodeBench、ALFWorld及Lifelong Agent Bench上通过Last Epoch Accuracy和Cumulative Success Rate验证了其有效性。",
                    "summary_translation": "人类智能的显著特征在于通过 Constructive Episodic Simulation（建设性情景模拟）掌握新技能的能力——即检索过往经验以综合解决新颖任务的方案。尽管 Large Language Models（大语言模型）拥有强大的推理能力，但它们难以模拟这种自我进化：Fine-tuning（微调）计算成本高昂且容易导致 Catastrophic Forgetting（灾难性遗忘），而现有的基于记忆的方法依赖于被动语义匹配，往往会检索到噪声。为了应对这些挑战，我们提出了 MemRL，这是一个通过在 Episodic Memory（情景记忆）上进行 Non-parametric Reinforcement Learning（非参数强化学习）来使智能体实现自我进化的框架。MemRL 明确地将冻结 LLM 的稳定推理与具有可塑性且不断进化的记忆分离开来。与传统方法不同，MemRL 采用了一种 Two-Phase Retrieval（两阶段检索）机制，首先根据语义相关性过滤候选，然后基于学习到的 Q-values（效用）进行选择。这些效用通过环境反馈以试错的方式不断优化，使智能体能够区分高价值策略与相似的噪声。在 HLE、BigCodeBench、ALFWorld 和 Lifelong Agent Bench 上进行的广泛实验表明，MemRL 显著优于 State-of-the-art（最先进的）基线。我们的分析实验证实，MemRL 有效调和了 Stability-Plasticity Dilemma（稳定性-可塑性困境），能够在不进行权重更新的情况下实现持续的运行时改进。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents",
                    "arxiv_id": "2601.02845",
                    "authors": "Kai Li, Xuanqing Yu, Ziyi Ni, Yi Zeng, Yao Xu, Zheqing Zhang, Xin Li, Jitao Sang, Xiaogang Duan, Xuelei Wang, Chengbao Liu, Jie Tan",
                    "summary": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心贡献符合第一步判断**：论文的核心是提出了一种名为 TiMem 的时间-分层记忆框架，旨在解决长视界对话智能体在管理不断增长的交互历史时面临的上下文窗口限制问题。这属于“构建、改进或演化 LLM智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等）的非演化型应用。 2.  **精准命中第二步正面指标**：我的研究焦点明确包含“单智能体”方向下的“记忆”能力。该论文专门针对智能体的“记忆”机制进行了创新，提出了时间记忆树（TMT）和语义引导的整合机制，这正是对智能体核心组件的改进。 3.  **不触犯第三步排除标准**：论文主要关注智能体的记忆架构，不涉及安全与对齐、多模态视觉技术或知识图谱等被明确排除的领域。 综上所述，该论文致力于改进LLM智能体的记忆能力，属于单智能体研究的关键子方向，符合筛选要求。",
                    "summary2": "本文旨在解决长周期对话代理中记忆碎片化及个性化不稳定的问题。针对不断增长的交互历史，我们提出了一种TiMem框架，该框架利用Temporal Memory Tree实现时间分层记忆整合与复杂度感知检索，并在LoCoMo和LongMemEval-S基准上通过准确率和记忆长度验证了其有效性。",
                    "summary_translation": "长程对话智能体必须管理不断增长的交互历史，这些历史很快就会超过大语言模型（large language models, LLMs，大语言模型）的有限上下文窗口。现有的记忆框架对跨层级的时间结构化信息支持有限，往往导致记忆碎片化和不稳定的长程个性化。我们提出了 TiMem，一个时间-层级记忆框架，它通过时间记忆树（Temporal Memory Tree, TMT，时间记忆树）组织对话，实现了从原始对话观察到逐步抽象的人设表征的系统性记忆整合。TiMem 具有三个核心特征：（1）通过 TMT 进行的时间-层级组织；（2）语义引导的整合，能够在无需微调的情况下实现跨层级的记忆整合；（3）复杂度感知的记忆召回，在不同复杂度的查询中平衡精确度和效率。在一致的评估设置下，TiMem 在两个基准测试中都达到了最先进的准确率，在 LoCoMo 上达到 75.30%，在 LongMemEval-S 上达到 76.88%。它优于所有评估的基线模型，同时在 LoCoMo 上将召回记忆长度减少了 52.20%。流形分析表明，在 LoCoMo 上存在清晰的人设分离，在 LongMemEval-S 上离散度有所降低。总的来说，TiMem 将时间连续性视为对话智能体长程记忆的首要组织原则。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
                    "arxiv_id": "2601.02744",
                    "authors": "Hanqi Jiang, Junhao Chen, Yi Pan, Ling Chen, Weihang You, Yifan Zhou, Ruidong Zhang, Yohannes Abate, Tianming Liu",
                    "summary": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断 (第一步)**: 该论文的核心贡献是提出了一种名为 Synapse 的统一记忆架构，旨在解决 LLM 智能体在长期记忆中的“不连贯”问题。这属于对 LLM 智能体核心组件（记忆）的构建与改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型推理能力的提升。因此，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: 论文直接涉及 `LLM-based Agents` 的核心能力——`Memory`（记忆）。它引入了基于认知科学的扩散激活机制来优化智能体的记忆检索，这直接提升了智能体处理复杂时序和多跳推理任务的能力，属于单智能体方向的关键技术突破。 3.  **排除标准检查 (第三步)**: *   **安全与对齐**: 论文不涉及安全、对齐或可解释性问题。 *   **多模态**: 论文专注于文本记忆架构，不涉及视觉或多模态内容。 *   **图技术**: 尽管论文中提到了“动态图”和“图遍历”，但其核心目的是构建智能体的记忆系统，而非研究图神经网络（GNN）或知识图谱算法本身。图在这里是作为智能体内部存储和检索信息的机制，服务于智能体的功能，因此不应被排除。 4.  **综合结论**: 该论文通过改进智能体的记忆机制，直接增强了 LLM 智能体的自主性和任务处理能力，完全符合“单智能体”方向中关于“记忆”的研究焦点。",
                    "summary2": "本文旨在解决LLM智能体长期记忆中的上下文隔离问题。针对长对话中的复杂推理场景，我们提出了一种基于Spreading Activation的SYNAPSE架构，构建Unified Episodic-Semantic Graph并结合Lateral Inhibition实现动态检索。我们在LoCoMo benchmark上通过F1 Score和Token消耗验证了其有效性，显著提升了多跳推理精度并降低了95%的Token使用量。",
                    "summary_translation": "尽管 Large Language Models (LLMs，大型语言模型) 在 generalized reasoning (泛化推理) 方面表现出色，但标准的 retrieval-augmented approaches (检索增强方法) 未能解决 long-term agentic memory (长期智能体记忆) 的割裂特性。为弥合这一差距，我们提出了 Synapse (Synergistic Associative Processing Semantic Encoding，协同联想处理语义编码)，这是一种超越 static vector similarity (静态向量相似度) 的 unified memory architecture (统一记忆架构)。借鉴 cognitive science (认知科学)，Synapse 将记忆建模为一个 dynamic graph (动态图)，其中相关性源于 spreading activation (扩散激活) 而非 pre-computed links (预计算链接)。通过整合 lateral inhibition (侧抑制) 和 temporal decay (时间衰减)，该系统能够动态突显相关的 sub-graphs (子图)，同时过滤干扰信息。我们实施了一种 Triple Hybrid Retrieval strategy (三重混合检索策略)，该策略将 geometric embeddings (几何嵌入) 与 activation-based graph traversal (基于激活的图遍历) 相融合。在 LoCoMo benchmark (LoCoMo 基准测试) 上的全面评估表明，Synapse 在复杂的 temporal and multi-hop reasoning tasks (时序和多跳推理任务) 中显著优于 state-of-the-art methods (最先进方法)，为 \"Contextual Tunneling\" (上下文隧道) 问题提供了稳健的解决方案。我们的代码和数据将在论文录用后公开发布。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
                    "arxiv_id": "2601.02880",
                    "authors": "Abhishek HS, Pavan C Shekar, Arpit Jain, Ashwanth Krishnan",
                    "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心判断**: *   论文的核心贡献是提出了 **ReTreVal** 这一混合框架，旨在构建和改进 LLM 智能体的多步推理能力。 *   它不是简单的应用型论文，而是提出了一种新的方法论，结合了 Tree-of-Thoughts (ToT)、自我细化和 Reflexion 记忆机制。 *   它不属于非演化型应用，也不属于基础设施或基础模型推理能力的微调，而是专注于智能体的架构设计。 2.  **符合研究焦点**: *   **单智能体**: 论文深入探讨了智能体的核心能力，包括 **Planning** (通过构建推理树进行结构化探索)、**Memory** (Reflexion memory buffer 用于持久化存储见解) 和 **Self-Reflection** (self-critique and refinement)。 *   **自我演化**: 论文明确提到了 \"persistent learning across problems\"（跨问题的持久学习）和 \"cross-problem memory\"（跨问题记忆）。智能体通过存储成功路径和失败模式的见解，利用这些经验在后续问题中进行改进，这完全符合“通过经验、反思进行自我完善”的自我演化定义。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态或图技术等排除项。 *   虽然在数学和创意写作任务上进行了评估，但这只是验证框架有效性的实验场景，论文的核心在于框架本身的机制（验证、剪枝、记忆），而非特定领域的应用。 4.  **特殊规则处理**: *   根据“推理/规划”规则，该论文属于保留范畴。它不仅仅是提出一种新的 CoT 变体，而是构建了一个包含验证、记忆和树搜索的完整 **Agentic 框架**，与 ReAct 和 Reflexion 等经典 Agentic 方法进行对比和改进。 综上所述，该论文通过引入记忆机制和结构化探索，增强了 LLM 智能体的规划和自我演化能力，高度契合“LLM智能体及其演化”的研究课题。",
                    "summary2": "本文旨在解决大语言模型在复杂多步推理任务中的局限性。针对数学和创意写作场景，我们提出了一种名为 ReTreVal 的混合框架，该框架集成了 Tree-of-Thoughts 探索、Self-Refinement、LLM-based critique scoring 和 reflexion memory。我们在 500 个数学问题和创意写作任务上，基于 Qwen 2.5 7B 模型，通过准确性和 GPT-4o mini 评分验证了其有效性。",
                    "summary_translation": "多步推理仍然是大型语言模型面临的一个关键挑战，特别是在数学和创意写作等复杂领域。尽管包括 ReAct、Reflexion 和 Self-Refine 在内的近期方法通过迭代优化和反思改进了推理能力，但它们往往缺乏对替代解题路径的结构化探索以及跨问题的持久学习能力。我们提出了 ReTreVal (Reasoning Tree with Validation，带验证的推理树)，这是一个集成了思维树探索、自我优化、基于 LLM 的批评评分以及反思记忆的混合框架，旨在实现有界且经过验证的多步推理。ReTreVal 构建了一个具有基于问题复杂度的自适应深度的结构化推理树，其中每个节点都在明确的 LLM 生成反馈指导下，经历迭代的自我批评和优化。双重验证机制评估每个节点的推理质量、连贯性和正确性，同时将来自成功推理路径的见解和失败模式持久地存储在反思记忆缓冲区中，从而实现跨问题学习。基于批评的剪枝策略在每一层仅保留得分最高的 top-k 个节点，在控制计算成本的同时保留了高质量的解题路径。我们使用 Qwen 2.5 7B 作为底层 LLM，在 500 个数学问题和创意写作任务上对 ReTreVal 与 ReAct、Reflexion 和 Self-Refine 进行了评估，结果表明 ReTreVal 通过结合结构化探索、批评驱动的优化以及跨问题记忆，始终优于现有方法，使其在需要探索性推理、严格验证和知识迁移的任务中尤为有效。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 1,
            "papers": [
                {
                    "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
                    "arxiv_id": "2601.02997",
                    "authors": "Waleed Khalid, Dmitry Ignatov, Radu Timofte",
                    "summary": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合筛选标准，属于“自我演化”和“Agentic AI”的核心研究范畴。 1.  **核心判断（符合自我演化机制）**： 论文的核心贡献在于构建了一个“闭环合成框架”，让LLM能够通过环境反馈（执行结果和性能评估）进行自我完善。论文详细描述了LLM经过22个监督微调周期的演化过程，通过迭代微调，模型内化了经验性的架构先验，从而提升了生成质量和性能。这完全符合“自我演化”中定义的“智能体通过经验、反思或环境反馈进行自我完善和迭代”。 2.  **符合Agentic AI特征**： 论文将LLM定位为一个“自主的、性能驱动的神经设计者”。它不仅仅是生成代码，而是通过“验证-评估-过滤-更新”的循环，展示了智能体利用工具（代码执行）和反馈机制来优化自身输出的能力，这属于Agentic AI的高级形态。 3.  **特殊情况处理（自我演化的应用）**： 虽然论文的应用场景是“神经架构设计”（NAS），属于特定领域的应用，但根据第四步的规则，只要论文的核心是提出一种新的“自我演化”机制，即使应用在特定领域也应保留。本文的重点在于证明LLM如何通过反馈循环“从记忆走向创造”，其演化机制具有通用性和研究价值，而非单纯的应用落地。 综上所述，该论文在自我演化和智能体框架构建上具有显著贡献，符合研究课题要求。",
                    "summary2": "本文旨在探索LLM能否通过迭代微调成为自主的神经网络架构设计师。针对CIFAR-10图像分类任务，我们提出了一种基于闭环合成框架的方法，结合了MinHash-Jaccard新颖性过滤和低保真性能信号（单轮准确率）。在22个微调周期中，我们使用DeepSeek-Coder-7B-Instruct-v1.5模型，通过有效生成率、首轮准确率和结构新颖性等指标验证了其有效性。结果显示模型性能显著提升，且保持了结构多样性。",
                    "summary_translation": "大型语言模型在程序合成方面表现出色，但其在自主导航神经架构设计——即在句法可靠性、性能和结构新颖性之间取得平衡——方面的能力仍有待探索。我们通过将面向代码的LLM置于闭环合成框架中来解决这一问题，并分析了其在22个监督微调周期中的演变过程。该模型合成PyTorch卷积网络，这些网络经过验证，通过低保真性能信号（单轮准确率）进行评估，并使用MinHash-Jaccard标准进行过滤以防止结构冗余。高性能且新颖的架构被转换为提示-代码对，用于通过参数高效的LoRA适配进行迭代微调，该过程初始化自LEMUR数据集。在各个周期中，LLM内化了经验性架构先验，成为了一个鲁棒生成器。有效生成率稳定在50.6%（峰值达到74.5%），而平均首轮准确率从28.06%上升到50.99%，准确率超过40%的候选者比例从2.04%增长到96.81%。分析证实，该模型超越了复制现有模式的阶段，合成了455个原始语料库中不存在的高性能架构。通过将代码合成建立在执行反馈的基础上，这项工作提供了一个可扩展蓝图，用于将随机生成器转化为自主的、性能驱动的神经设计器，确立了LLM能够内化经验性的、非文本奖励从而超越其训练数据的结论。",
                    "inspiration_trace": "基于论文《From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures》，以下是对作者核心方法论产出过程的逻辑推演：\n\n### 第一阶段：宏观观察与问题定位\n**逻辑起点：LLM的代码能力与NAS的高昂成本之间的矛盾。**\n1.  **观察现状**：传统神经架构搜索（NAS）虽然能自动化设计网络，但依赖强化学习或进化算法，计算成本极高（通常需要训练大量模型）。\n2.  **技术契机**：大型语言模型（LLM）在代码合成方面表现出色，能够生成复杂的PyTorch代码。\n3.  **提出核心问题**：现有的LLM辅助NAS研究多将LLM视为静态的“代码生成器”，仅关注最终搜索结果的精度。作者试图转换视角——**如果将LLM视为一个可进化的“智能体”，通过自我迭代，它能否从单纯的代码生成者转变为具备设计直觉的“架构设计师”？**\n\n### 第二阶段：假设形成与关键挑战\n**逻辑推演：从“死记硬背”到“举一反三”的可能性。**\n1.  **核心假设**：如果让LLM在一个闭环中不断生成架构、接收反馈并基于自身的高质量产出进行微调，它将逐渐内化出一种“经验性的架构先验”，而不仅仅是记忆训练数据中的代码片段。\n2.  **面临的挑战**：\n    *   **可靠性**：生成的代码往往无法运行（语法错误、维度不匹配）。\n    *   **性能评估**：如果对每个生成的模型都进行完整训练，计算成本将不可接受。\n    *   **模式崩塌**：模型可能会陷入局部最优，反复生成相似的“高分”架构，失去探索新结构的能力。\n\n### 第三阶段：策略性简化与代理指标设计\n**逻辑转折：为了可行性，必须降低评估成本。**\n1.  **引入低保真代理**：为了解决计算成本问题，作者决定不追求模型的最终收敛精度，而是采用**“单轮训练后的准确率”**作为性能代理。\n    *   *思考逻辑*：一个优秀的架构通常在训练初期就能快速学习。这个指标既便宜又能反映架构的“学习潜力”。\n2.  **定义目标函数**：将优化目标从“最终精度”转化为“初始学习速度”与“语法有效性”的结合。\n\n### 第四阶段：闭环机制与多样性约束\n**逻辑构建：如何确保“进化”而非“退化”？**\n1.  **构建闭环**：设计了一个“生成-评估-筛选-微调”的迭代循环。\n    *   *生成*：LLM产出PyTorch代码。\n    *   *评估*：检查代码可执行性，并跑一轮训练获取代理分数。\n    *   *筛选*：这是关键步骤，必须同时满足三个条件：**代码可运行**、**代理分数达标**、**结构新颖**。\n2.  **引入新颖性过滤**：为了防止模型陷入重复生成已知好架构的陷阱（即过拟合），作者引入了**MinHash-Jaccard相似度**作为去重机制。\n    *   *思考逻辑*：只有那些“既好用又没见过”的架构，才有资格被加入下一轮的微调数据集。这迫使LLM不断探索设计空间的新区域，而不是在舒适区里打转。\n\n### 第五阶段：验证与结论\n**逻辑闭环：从假设到现实的映射。**\n1.  **实验验证**：通过22个周期的迭代，观察LLM的行为变化。\n2.  **结果解读**：\n    *   **可靠性提升**：有效生成率从低位稳定在50%以上，说明LLM学会了如何写“能跑的代码”。\n    *   **性能提升**：单轮准确率显著提高，说明LLM学会了什么样的结构更容易学习。\n    *   **创造力涌现**：发现了大量不在原始数据集中的高性能架构，证明了模型超越了简单的记忆，具备了设计能力。\n\n**总结：**\n作者的思考路径是从**利用LLM写代码**（工具属性），进化到**利用反馈机制训练LLM**（进化属性），最后通过**约束多样性**（探索属性），成功将一个通用的代码模型重塑为一个专用的、具备创造力的神经网络设计专家。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
                    "arxiv_id": "2601.02695",
                    "authors": "Guibin Zhang, Haiyang Yu, Kaiming Yang, Bingli Wu, Fei Huang, Yongbin Li, Shuicheng Yan",
                    "summary": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“自我演化”方向。 1.  **核心贡献符合第一步判断**: 论文的核心贡献是提出了 **EvoRoute**，这是一种**自我演化**的模型路由范式。它不是将现有的智能体简单应用到某个垂直领域（非演化型应用），也不是单纯的基础设施优化，而是针对LLM智能体系统本身提出了一种新的架构和演化机制，旨在解决智能体系统中的性能、成本和延迟之间的权衡问题。 2.  **精准命中“自我演化”与“Agentic AI”焦点**: *   **自我演化**: 论文明确提到这是一个“self-evolving”范式，并且详细描述了其机制：利用不断扩大的先验经验知识库，通过环境反馈持续完善其自身的选择策略。这完全符合筛选标准中关于“智能体通过经验、反思或环境反馈进行自我完善和迭代”的定义。 *   **Agentic AI**: 论文的研究对象是“Complex agentic AI systems”，涉及LLM、工具和记忆模块的协调集成，这直接对应了研究课题中的LLM智能体构建。 3.  **符合正面指标**: 论文包含了大量核心关键词，如 `Self-Evolving`、`Agentic AI`、`LLM-based Agents`、`Iterative Improvement`（通过反馈持续完善）以及 `Tool and memory modules`。 4.  **排除标准检查**: 论文不涉及安全对齐、多模态视觉核心研究或图技术。虽然它关注了成本和延迟（通常属于基础设施范畴），但其解决手段是通过智能体的自我演化策略来实现的，因此属于智能体架构的改进，而非单纯的底层硬件或部署优化。 综上所述，该论文提出了一种新的智能体自我演化机制，属于构建和改进LLM智能体的前沿研究，应予以保留。",
                    "summary2": "本文旨在解决Agent System Trilemma，即性能、成本与效率之间的权衡问题。针对复杂的多轮LLM Agent系统，我们提出了一种名为EvoRoute的自进化模型路由范式，利用多方面检索和帕累托最优过滤动态选择LLM。我们在GAIA和BrowseComp+等基准上通过性能、成本和延迟指标验证了其有效性，在维持高性能的同时将成本降低了80%，延迟降低了70%。",
                    "summary_translation": "由大语言模型、工具和记忆模块的协调集成所驱动的复杂智能体 AI 系统，在复杂的多轮任务中展现出了卓越的能力。然而，这一成就伴随着高昂的经济成本和严重的延迟，从而揭示了一个关键但尚未被充分探索的权衡问题。我们将这一挑战形式化为 **Agent System Trilemma**（智能体系统三难困境）：即在实现最先进的性能、最小化货币成本以及确保快速任务完成这三者之间存在的内在张力。为了打破这一三难困境，我们提出了 EvoRoute，这是一种超越静态、预定义模型分配的自进化模型路由范式。EvoRoute 利用不断扩展的先验经验知识库，在每一步动态选择 Pareto-optimal（帕累托最优）的 LLM backbones（大语言模型骨干网络），在准确性、效率和资源使用之间取得平衡，同时通过环境反馈持续优化其自身的选择策略。在 GAIA 和 BrowseComp+ 等具有挑战性的智能体基准上进行的实验表明，当将 EvoRoute 集成到现成的智能体系统中时，它不仅保持甚至提升了系统性能，还将执行成本降低了高达 80%，延迟降低了超过 70%。",
                    "inspiration_trace": ""
                },
                {
                    "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
                    "arxiv_id": "2601.03204",
                    "authors": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
                    "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **InfiAgent**，这是一个用于通用自主智能体的**新框架**。 *   它解决的是 LLM 智能体在长视界任务中面临的上下文增长和误差累积问题，属于**构建和改进 LLM 智能体**的方法论研究。 *   它不是将现有智能体简单应用到特定领域（如医疗、金融），而是提出了底层的架构改进（状态外部化），因此不属于“非演化型应用”。 *   它关注的是智能体的系统架构和状态管理，而非模型的基础设施或硬件加速。 2.  **正面指标匹配（第二步）：** *   **核心范式**：明确属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文重点解决了智能体的 **`Memory`**（通过文件中心的状态抽象外部化持久状态）和 **`Planning`**（在长视界任务中保持推理稳定性）能力。这是单智能体研究中的关键子方向。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态或图技术，未触发任何排除标准。 4.  **特殊情况处理（第四步）：** *   论文关于长视界任务中的推理稳定性，属于智能体如何在复杂任务中进行规划和多步推理的范畴，符合保留条件。 **总结**：该论文通过引入显式的状态外部化机制，改进了 LLM 智能体处理长视界任务的能力，是对智能体架构和记忆机制的重要创新，直接契合“单智能体”方向中的规划与记忆研究焦点。",
                    "summary2": "本文旨在解决LLM agents在长视界任务中因上下文无界增长导致的稳定性问题。针对长视界推理场景，我们提出了一种InfiAgent框架，采用File-Centric State Abstraction将持久状态外部化，并结合分层代理架构保持上下文有界。在DeepResearch benchmark和80篇论文综述任务上，通过综合评分和覆盖率指标验证了其有效性，证明其能以20B开源模型达到与大型专有系统相当的性能。",
                    "summary_translation": "LLM agents (大语言模型智能体) 虽然具备推理和使用工具的能力，但在长视界任务中，往往因上下文的无界增长和误差累积而失效。常见的解决方案，如上下文压缩或 retrieval-augmented prompting (检索增强提示)，往往在信息保真度与推理稳定性之间引入了权衡。我们提出了 InfiAgent，这是一个通用框架，通过将持久状态外部化到 file-centric state abstraction (以文件为中心的状态抽象) 中，无论任务持续时间长短，都能将智能体的推理上下文严格限制在固定范围内。在每一步中，智能体通过结合 workspace state snapshot (工作区状态快照) 和近期操作的固定窗口来重构上下文。在 DeepResearch 数据集和一项包含 80 篇论文的文献综述任务上的实验表明，无需进行 task-specific fine-tuning (针对特定任务的微调)，配备 20B 开源模型的 InfiAgent 的性能可与更大的专有系统相媲美，并且相比 context-centric baselines (以上下文为中心的基线方法)，其长视界覆盖率显著更高。这些结果验证了 explicit state externalization (显式状态外部化) 作为构建稳定长视界智能体的实用基础的可行性。\n\nGithub 代码库：https://github.com/ChenglinPoly/infiAgent",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-05": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
                    "arxiv_id": "2601.01569",
                    "authors": "Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sihui Han, Bo An, Yike Guo, Jun Song",
                    "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断**: *   论文的核心贡献是提出了 **CaveAgent**，这是一个新的 **LLM智能体框架**。它旨在解决现有智能体系统在处理长时程任务时面临的上下文漂移和脆弱依赖问题。 *   这不是一篇将现有智能体简单应用到特定领域的应用论文，而是对智能体底层架构和运行机制的改进。 *   它不属于基础设施优化或非Agentic的基础推理研究。 2.  **正面指标匹配**: *   **Agentic AI / LLM-based Agents**: 论文明确致力于改进基于LLM的智能体系统。 *   **Memory (记忆)**: 论文的核心创新点之一是 **Stateful Runtime Management**（有状态运行时管理）。通过注入、操作和检索持久的Python对象（如DataFrames），它构建了一个高保真的 **外部记忆** 机制，以消除上下文漂移和灾难性遗忘。这直接对应了单智能体研究中的“记忆”能力。 *   **Tool Use / Tool Augmentation**: 框架利用代码生成作为工具，通过Python Runtime流来执行任务，超越了传统的JSON函数调用，属于工具使用能力的增强。 *   **Planning (规划)**: 论文提到利用代码生成高效解决相互依赖的子任务（如循环、条件判断），这涉及智能体在复杂任务中的多步规划和执行能力。 3.  **排除标准检查**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 **结论**: 该论文通过引入双流上下文架构和有状态运行时管理，显著增强了LLM智能体的记忆保持和任务执行能力，是对单智能体架构的重要改进，完全符合“构建、改进或演化 LLM智能体”的核心目标。",
                    "summary2": "本文旨在解决传统基于JSON的LLM Agent在长周期任务中面临的上下文漂移和效率低下问题。针对复杂的工具调用和数据处理场景，我们提出了一种名为CaveAgent的框架，采用双流上下文架构和有状态运行时管理，实现对象级交互。我们在Tau 2-bench、BFCL及数据密集型任务上通过成功率和Token消耗验证了其有效性，显著提升了任务表现并降低了成本。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
                    "arxiv_id": "2601.01885",
                    "authors": "Yi Yu, Liuyi Yao, Yuexiang Xie, Qingquan Tan, Jiaqi Feng, Yaliang Li, Libing Wu",
                    "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心判断（第一步）**：论文的核心贡献是构建了一个名为 \"Agentic Memory (AgeMem)\" 的新框架，旨在解决LLM智能体在长程推理中的记忆管理问题。这属于对LLM智能体内部机制的构建和改进，而非将智能体作为工具应用到特定领域（如医疗、金融），也非基础设施或基础模型能力的提升。 2.  **正面指标匹配（第二步）**： *   **核心范式**：明确属于 `LLM-based Agents` 和 `Agentic AI`。 *   **智能体能力**：论文的核心焦点是 `Memory`（记忆），这是智能体的关键能力之一。同时，它将记忆操作（存储、检索、更新等）定义为 `Tool Use / Tool Augmentation`（基于工具的动作），使智能体能自主决策。 *   **演化机制**：论文提出了一种三阶段渐进式强化学习（RL）策略来训练智能体的记忆管理策略，这涉及智能体通过环境反馈进行 `Self-Improvement`（自我完善）和策略优化。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉核心或图技术，因此不在排除之列。 综上所述，该论文通过引入统一的记忆管理框架和强化学习训练方法，显著增强了LLM智能体的自主性和适应性，是对Agentic AI核心能力的直接贡献，因此予以保留。",
                    "summary2": "本文旨在解决LLM智能体在长视界推理中因有限上下文窗口导致的记忆管理限制问题。针对LTM和STM分离且依赖启发式规则的现状，我们提出了一种名为Agentic Memory (AgeMem)的统一框架，通过基于工具的动作接口实现端到端的记忆管理，并采用三阶段渐进式RL策略进行训练。在ALFWorld、SciWorld等五个长视界基准测试上，通过Success Rate和Memory Quality等指标验证了其有效性，显著提升了任务性能和记忆质量。",
                    "summary_translation": "大语言模型智能体由于上下文窗口有限，在长程推理方面面临根本性局限，这使得有效的记忆管理变得至关重要。现有方法通常将长期记忆和短期记忆视为独立的组件进行处理，并依赖于启发式方法或辅助控制器，这限制了系统的适应性和端到端优化能力。在本文中，我们提出了 Agentic Memory (AgeMem，智能体记忆)，这是一个将长期记忆和短期记忆管理直接整合到智能体策略中的统一框架。AgeMem 将记忆操作呈现为基于工具的动作，使大语言模型智能体能够自主决定存储、检索、更新、总结或丢弃何种信息以及何时执行这些操作。为了训练这种统一行为，我们提出了一种三阶段渐进式强化学习策略，并设计了分步 GRPO，以解决由记忆操作引起的奖励稀疏和不连续问题。在五个长程基准测试上的实验表明，AgeMem 在多种大语言模型骨干网络上始终优于强大的记忆增强基线模型，实现了更优的任务性能、更高质量的长期记忆以及更高效的上下文利用率。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2026-01-02": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "An Agentic Framework for Neuro-Symbolic Programming",
                    "arxiv_id": "2601.00743",
                    "authors": "Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi",
                    "summary": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
                    "category": "cs.AI",
                    "filter_reason": "1.  **核心贡献符合 Agentic AI 范畴**：论文明确提出了一个名为 \"AgenticDomiKnowS (ADS)\" 的框架，其核心在于构建一个基于 LLM 的智能体工作流。这完全符合第一步中关于“构建 LLM 智能体（Agentic LLM）的方法论或新框架”的保留标准。 2.  **具备单智能体核心能力**：论文描述的智能体工作流涉及将自由形式的任务描述分解并转化为具体的程序组件，这体现了智能体的**规划**能力；同时，工作流包含“创建和测试”组件的过程，体现了**工具使用**和执行能力。这些均属于第二步正面指标中的“单智能体”核心关注点。 3.  **不属于排除项**： *   虽然论文涉及“神经符号编程”这一特定领域，但其核心贡献并非仅仅是应用 LLM 解决该领域问题，而是提出了一种通用的智能体框架来辅助编程，因此不属于“非演化型应用”。 *   尽管神经符号编程可能涉及逻辑或图结构，但论文的研究焦点是智能体框架本身，而非图神经网络或知识图谱算法的改进，因此不触犯“图”相关的排除标准。 *   论文不涉及安全、对齐或多模态视觉等排除领域。 综上所述，该论文的核心在于提出一个新的 Agentic 框架来解决复杂的编程构建任务，符合“单智能体”方向的研究目标。",
                    "summary2": "本文旨在解决神经符号编程门槛高、编写困难的问题。针对自然语言任务描述，我们提出了一种 AgenticDomiKnowS (ADS) 智能体框架，采用多智能体协作与RAG检索的分阶段工作流生成代码。在涵盖NLP、视觉和CSP的12个数据集及人类评估实验中，通过图生成正确率和开发时间验证了其有效性，显著提升了开发效率。",
                    "summary_translation": "将 symbolic constraints（符号约束）集成到 deep learning models（深度学习模型）中，能够提升模型的鲁棒性、可解释性以及数据效率。然而，这一过程依然耗时且充满挑战。现有框架（如 DomiKnowS）通过提供 high-level declarative programming interface（高级声明式编程接口）辅助这一集成过程，但它们仍预设用户已熟练掌握该库的特定语法。为消除这一依赖，我们提出了 AgenticDomiKnowS (ADS)。ADS 利用一种 agentic workflow（智能体工作流），将 free-form task descriptions（自由形式的任务描述）转化为完整的 DomiKnowS 程序，该工作流会对每个 DomiKnowS component（组件）分别进行创建和测试。该工作流支持可选的 human-in-the-loop intervention（人在回路干预），使熟悉 DomiKnowS 的用户能够优化 intermediate outputs（中间输出）。我们展示了 ADS 如何使经验丰富的 DomiKnowS 用户及非用户均能快速构建 neuro-symbolic programs（神经符号程序），将开发时间从数小时缩短至 10-15 分钟。",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-31": [
        {
            "name": "Artificial Intelligence",
            "count": 1,
            "papers": [
                {
                    "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
                    "arxiv_id": "2512.24613",
                    "authors": "Zheyu Shi, Dong Qiu, Shanlong Yu",
                    "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“多智能体”和“自我演化”方向的交叉研究。 1.  **核心贡献符合多智能体系统 (Multi-Agent)**: 论文的核心是提出了一种“面向群体商议的多智能体对话模型”。它构建了一个包含三个不同角色的智能体架构：意见生成智能体、证据验证智能体和一致性仲裁智能体。这直接对应了我筛选标准中的“多智能体”方向，特别是智能体间的协作、通信和角色分工。 2.  **包含自我演化机制**: 论文中明确引入了“自我博弈机制”来扩展多路径推理轨迹，并设计了复合奖励函数结合改进的近端策略优化（PPO）策略进行协作训练。这种通过反馈和强化学习进行迭代优化的过程，符合“自我演化”中关于智能体通过环境反馈进行自我完善和迭代的标准。 3.  **具备智能体关键能力**: 论文模型包含检索增强模块，体现了“工具使用”能力；同时，其通过多智能体商议解决复杂推理任务的过程，涉及了复杂的“规划”和“多步推理”。 4.  **排除标准检查**: 该论文不涉及安全对齐、多模态视觉或图技术等排除项。虽然论文在HotpotQA等数据集上进行实验，但其核心贡献在于提出了新的智能体框架和训练机制，而非单纯将现有模型应用于特定领域。 综上所述，该论文在构建多智能体协作框架及引入自我博弈演化机制方面做出了实质性贡献，符合筛选要求。",
                    "summary2": "本文旨在优化单一语言模型在复杂推理任务中的局限性。针对多跳问答等场景，我们提出了一种Group Deliberation Multi-Agent Dialogue Model，构建“Generation-Verification-Integration”三级角色分工架构，引入Self-game mechanism和Retrieval Enhancement Module。在HotpotQA、2WikiMultihopQA和MeetingBank数据集上，通过Multi-hop reasoning accuracy和Consistency index验证了其有效性，显著提升了推理准确性与一致性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 1,
            "papers": [
                {
                    "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory",
                    "arxiv_id": "2512.24684",
                    "authors": "Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu",
                    "summary": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断**：论文明确提出了 R-Debater，这是一个用于生成多轮辩论的 **\"agentic framework\"**（智能体框架）。其核心贡献在于构建了一个结合检索增强和论证记忆的智能体架构，而非仅仅将现有模型作为工具简单应用于辩论领域。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合研究焦点**：该研究完全符合 **单智能体** 方向。 *   **记忆**：论文的核心创新点是 \"Argumentative Memory\"（论证记忆），使智能体能够回忆和调整先前的论点。 *   **工具使用**：集成了辩论知识库进行检索，作为智能体获取证据的工具。 *   **规划**：摘要中提到 \"structured planning\"（结构化规划），用于在多轮对话中保持立场一致性和连贯性。 3.  **排除标准检查**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然应用场景是辩论，但其重点在于智能体如何通过记忆和规划机制来完成任务，属于智能体能力的构建，而非单纯的非演化型应用。 综上所述，该论文提出了新的智能体框架并深入探讨了记忆与规划机制，符合筛选要求。",
                    "summary2": "本文旨在解决LLM在多轮辩论生成中缺乏事实依据、立场一致性差及跨轮次连贯性弱的问题。针对ORCHID数据集上的多轮辩论场景，我们提出了一种名为R-Debater的框架，该框架集成了检索增强生成与基于角色的智能体，通过检索和重构论证记忆来生成高质量辩论。我们在ORCHID数据集上通过InspireScore和Debatrix指标验证了其有效性，实验表明其在逻辑连贯性和事实准确性上显著优于强基线模型。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-29": [
        {
            "name": "Machine Learning",
            "count": 2,
            "papers": [
                {
                    "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
                    "arxiv_id": "2512.22733",
                    "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
                    "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心方法论研究。 1.  **核心判断（第一步）：** *   论文的核心贡献是提出了 **FoldAct** 这一新框架，旨在解决 **Long-horizon search agents**（长视界搜索智能体）在强化学习训练中面临的上下文无限增长和非平稳观测分布问题。 *   这属于 **构建和改进 LLM 智能体** 的范畴。它关注的是智能体如何处理记忆（上下文折叠）以及如何在长任务中保持稳定性和效率，这是智能体架构和训练机制的关键部分，而非简单的应用或基础设施优化。 2.  **正面指标匹配（第二步）：** *   **核心范式**：论文明确针对 **LLM-based Agents**（特别是搜索智能体）。 *   **智能体能力**：论文重点解决了智能体的 **Memory**（通过上下文折叠压缩交互历史）和 **Planning/Search**（长视界搜索）能力。它提出的“分离损失计算”和“全上下文一致性损失”是为了优化智能体的训练过程，使其具备更好的长程任务处理能力。 3.  **排除标准检查（第三步）：** *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了计算效率（5.19倍加速），但这属于算法层面的优化（选择性片段训练），而非硬件或部署基础设施的研究。 4.  **特殊情况处理（第四步）：** *   论文涉及 **Reasoning/Planning**（长视界搜索），且其核心是改进智能体在执行过程中的状态管理和训练稳定性，符合保留条件。 综上所述，FoldAct 提出了一种改进智能体记忆机制和训练稳定性的新方法，直接贡献于 LLM 智能体的构建与优化，因此被保留。",
                    "summary2": "本文旨在解决长视距强化学习中上下文折叠导致的非平稳观测分布及训练不稳定问题。针对长视距搜索代理场景，我们提出了一种FoldAct框架，通过分离损失计算、全上下文一致性损失和选择性片段训练来优化上下文折叠。在Local RAG和Web Search基准上，通过F1、EM及Pass@1等指标验证了其有效性，并实现了5.19倍的训练加速。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Memento-II: Learning by Stateful Reflective Memory",
                    "arxiv_id": "2512.22716",
                    "authors": "Jun Wang",
                    "summary": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合您的研究范围，属于“自我演化”和“单智能体”方向的核心研究。 1.  **核心贡献符合第一步判断**： 论文的核心贡献是提出了一种名为“Memento-II”的理论框架，旨在解决LLM智能体的持续学习和经验学习问题。这直接对应了您筛选条件中的“构建、改进或演化 LLM智能体”以及“自我演化”的方法论。它不是将智能体作为工具应用到特定领域，而是研究智能体本身如何学习和适应的底层机制。 2.  **高度匹配正面指标**： *   **自我演化**：论文明确提出了“continual and experiential learning”（持续和经验学习），并强调智能体可以在“without back propagation or model fine tuning”（无需反向传播或模型微调）的情况下通过交互进行适应。这是典型的自我演化特征，即通过经验而非参数更新来迭代改进。 *   **智能体能力**：论文重点研究了“Memory”（情景记忆）和“Self-Reflection”（反思），将其作为智能体适应环境的关键机制。这直接命中了您关注的核心能力列表。 *   **核心范式**：该研究属于“LLM-based Agents”和“Agentic AI”范畴，特别是关于“Retrieval-based language model agents”（基于检索的语言模型智能体）。 3.  **不涉及排除标准**： 论文主要关注智能体的学习机制和理论框架，不涉及安全对齐、多模态视觉处理或图神经网络等排除领域。 综上所述，该论文通过引入“有状态反思决策过程”和结合强化学习，为LLM智能体在不更新模型参数的情况下实现自我完善和持续适应提供了理论基础，精准契合您关于“自我演化”和“单智能体”的研究目标。",
                    "summary2": "本文旨在解决LLM智能体无需微调即可实现持续学习的问题。针对开放式长视野任务，我们提出了一种Stateful Reflective Decision Process (SRDP) 框架，通过Read-Write Reflective Learning机制将 episodic memory 与 soft policy iteration 相结合。我们在软件测试、自动化数据科学和深度研究代理等场景上，通过理论收敛性证明和实证表现验证了其有效性。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
                    "arxiv_id": "2512.22322",
                    "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
                    "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向的核心研究。 1.  **核心贡献符合构建与改进 LLM 智能体的目标**： 论文提出了 **SmartSnap** 这一新范式，并定义了一种新型的 **Self-Verifying Agent（自验证智能体）**。这不仅仅是应用现有智能体去解决特定问题，而是对智能体架构和机制的实质性改进。它解决了 Agentic RL 中任务验证的瓶颈问题，将验证从“被动、事后”转变为“主动、原位”。 2.  **符合“单智能体”与“自我反思”的核心关注点**： 论文的核心在于赋予智能体双重使命：完成任务 + 证明完成。这直接对应筛选标准中的 **自我反思** 和 **自我修正** 能力。智能体通过主动寻找证据来验证自身行为，这是一种高级的元认知能力，属于 Agentic AI 的关键能力范畴。 3.  **不属于排除项**： *   虽然论文在 GUI 和移动任务上进行了实验，但其核心贡献是通用的“自验证”机制，而非针对特定领域的应用解决方案，因此不属于“非演化型应用”。 *   虽然涉及 GUI，但视觉仅作为智能体感知环境的一部分，并非研究视觉模型本身，因此不违反多模态排除规则。 *   论文关注的是智能体的任务完成验证机制，而非安全对齐或基础设施优化。 综上所述，该论文通过提出新的智能体框架来增强智能体的自主性和可靠性，是关于 LLM 智能体构建与演化的高质量研究。",
                    "summary2": "本文旨在解决Agentic RL中被动验证成本高且不可靠的瓶颈。针对复杂的GUI任务，我们提出了一种SmartSnap范式，引入了基于3C原则的Self-Verifying Agent，使其主动收集并提交关键快照证据。我们在AndroidLab基准上通过Success Rate等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-26": [
        {
            "name": "Computation and Language",
            "count": 2,
            "papers": [
                {
                    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
                    "arxiv_id": "2512.21919",
                    "authors": "KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He",
                    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”和“单智能体”方向的交叉研究。 1.  **核心贡献符合第一步（核心判断）**： *   论文的核心贡献是提出了 **SWE-RM**，一种用于软件工程智能体的奖励模型。 *   它不是简单地将智能体作为工具应用，而是致力于**改进智能体本身**的反馈机制。论文旨在解决现有基于执行的反馈（如单元测试）存在的稀疏性和依赖性问题，提出了一种免执行的反馈机制。 2.  **高度契合第二步（正面指标）与第三步（自我演化）**： *   **自我演化机制**：论文明确指出该模型旨在支持 **Reinforcement Learning (RL)** 和 **Test-time scaling (TTS)**。RL 是智能体通过环境反馈进行自我完善和迭代的核心技术，属于典型的“自我演化”范畴。TTS 则涉及智能体在推理时的搜索和规划。 *   **智能体能力**：论文讨论了如何通过奖励模型提供更细粒度的信号，帮助智能体进行 **Self-Correction**（自我修正）和 **Trajectory Selection**（轨迹选择，即规划的一部分）。 3.  **符合第四步（特殊和模糊情况）**： *   虽然论文的应用场景是软件工程（SWE），但其核心在于提出一种新的“反馈/奖励机制”，这种机制是智能体实现自我演化和改进的关键组件。根据第四步中关于“自我演化的应用”的例外规则，只要核心是提出新的演化/改进机制，即使应用在特定领域，也应保留。 综上所述，SWE-RM 提供了一种让智能体通过更高质量的反馈进行自我学习和优化的新方法，直接贡献于 LLM 智能体的演化能力，因此予以保留。",
                    "summary2": "本文旨在解决软件工程代理中基于执行的反馈稀疏且不可靠的问题。针对SWE任务中的多轮轨迹，我们提出了一种名为SWE-RM的免执行奖励模型，该模型采用混合专家架构，并强调AUC和校准能力。我们在SWE-Bench Verified数据集上通过Resolve Rate、AUC和ECE验证了其有效性。实验表明，SWE-RM在测试时扩展和强化学习中均显著提升了代理性能，达到了开源模型中的SOTA水平。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
                {
                    "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management",
                    "arxiv_id": "2512.21567",
                    "authors": "Changzhi Sun, Xiangyu Chen, Jixiang Luo, Dell Zhang, Xuelong Li",
                    "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.",
                    "category": "cs.CL",
                    "filter_reason": "1.  **核心判断符合 (第一步)**: 该论文的核心贡献是提出了一种名为 DAM (Decision-theoretic Agent Memory) 的框架，用于解决 LLM 智能体中的“记忆管理”问题。这直接属于“构建、改进 LLM 智能体”的范畴，而非将智能体作为工具应用到特定领域（如医疗、金融等），也不是关于基础设施或基础模型推理能力的提升。 2.  **命中核心关注点 (第二步)**: *   **单智能体能力**: 论文明确聚焦于智能体的 **Memory (记忆)** 机制。记忆是您列出的单智能体核心能力之一（规划、记忆、工具使用、自我反思）。 *   **Agentic AI**: 论文将记忆管理视为“不确定性下的序列决策问题”，这属于 Agentic AI 的核心方法论，旨在提升智能体在长期交互中的表现。 3.  **无排除项 (第三步)**: 论文不涉及安全与对齐、多模态视觉核心研究或图技术，因此不在排除范围内。 4.  **综合结论**: 该论文致力于改进 LLM 智能体的关键组件（记忆），提出了新的决策理论框架来替代传统的启发式方法，从而提升智能体的长期交互能力。这完全符合您的研究课题中关于“单智能体”及其能力演化的方向。",
                    "summary2": "总结生成失败",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
    "2025-12-24": [
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "Reflection-Driven Self-Optimization 6G Agentic AI RAN via Simulation-in-the-Loop Workflows",
                    "arxiv_id": "2512.20640",
                    "authors": "Yunhao Hu, Xinchen Lyu, Chenshan Ren, Keda Chen, Qimei Cui, Xiaofeng Tao",
                    "summary": "The escalating complexity of sixth-generation (6G) networks demands unprecedented levels of autonomy beyond the capabilities of traditional optimization-based and current AI-based resource management approaches. While agentic AI has emerged as a promising paradigm for autonomous RAN, current frameworks provide sophisticated reasoning capabilities but lack mechanisms for empirical validation and self-improvement. This article identifies simulation-in-the-loop validation as a critical enabler for truly autonomous networks, where AI agents can empirically verify decisions and learn from outcomes. We present the first reflection-driven self-optimization framework that integrates agentic AI with high-fidelity network simulation in a closed-loop architecture. Our system orchestrates four specialized agents, including scenario, solver, simulation, and reflector agents, working in concert to transform agentic AI into a self-correcting system capable of escaping local optima, recognizing implicit user intent, and adapting to dynamic network conditions. Extensive experiments validate significant performance improvements over non-agentic approaches: 17.1\\% higher throughput in interference optimization, 67\\% improved user QoS satisfaction through intent recognition, and 25\\% reduced resource utilization during low-traffic periods while maintaining service quality.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“多智能体”方向**： 论文的核心贡献是提出了一种“反思驱动的自我优化框架”，这直接对应研究焦点中的“自我演化”和“自我反思”。论文构建了一个包含四个专门智能体（场景、求解器、模拟、反思智能体）的闭环架构，旨在通过经验验证和反思机制实现系统的自我修正和迭代改进，这属于构建新的Agentic框架和方法论。 2.  **符合“自我演化的应用”例外规则**： 根据筛选标准第四步第2点，虽然论文的应用领域是特定的6G网络（RAN），但其核心在于提出了一种新的“自我演化”机制（Simulation-in-the-Loop + Reflection-driven），而不仅仅是将现有智能体作为工具应用。因此，符合“保留”的例外条件。 3.  **包含关键正面指标**： 论文明确涉及了 `Multi-Agent Systems`（多智能体协作）、`Self-Reflection`（反思智能体）、`Self-Correction`（自我修正）和 `Self-Optimization`（自我优化）等核心关键词。 综上所述，该论文在特定领域（6G）中验证了具有自我演化能力的多智能体框架，符合关于“LLM智能体及其演化”的研究课题要求。",
                    "summary2": "本文旨在解决6G网络中现有Agentic AI缺乏经验验证与自我改进机制的问题。针对多小区RAN场景，我们提出了一种Reflection-Driven Self-Optimization Framework，通过Simulation-in-the-Loop工作流协调Scenario、Solver、Simulation和Reflector四个Agent。我们在Sionna平台上通过吞吐量、QoS满意度和资源利用率等指标验证了其有效性，实现了显著的性能提升。",
                    "summary_translation": "翻译失败",
                    "inspiration_trace": "生成灵感溯源时发生错误"
                },
            ]
        },
    ],
};

const availableDates = ["2026-01-15", "2026-01-14", "2026-01-13", "2026-01-12", "2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24", "2025-12-23", "2025-12-22", "2025-12-19", "2025-12-18", "2025-12-17", "2025-12-16", "2025-12-12", "2025-12-11", "2025-12-10", "2025-12-09", "2025-12-05", "2025-12-03", "2025-12-02", "2025-11-28", "2025-11-26", "2025-11-25", "2025-11-21", "2025-11-20", "2025-11-19", "2025-11-18", "2025-11-14", "2025-11-13", "2025-11-12", "2025-11-10", "2025-11-07", "2025-11-06", "2025-11-05", "2025-11-04", "2025-10-31", "2025-10-30", "2025-10-29", "2025-10-28", "2025-10-24", "2025-10-23", "2025-10-22", "2025-10-21", "2025-10-20", "2025-10-17", "2025-10-16", "2025-10-15", "2025-10-14", "2025-10-10", "2025-10-09", "2025-10-08", "2025-10-07", "2025-10-06", "2025-10-03", "2025-10-02", "2025-10-01", "2025-09-30", "2025-09-29", "2025-09-26", "2025-09-25", "2025-09-24", "2025-09-23"];
const loadedDates = new Set(["2026-01-15", "2026-01-14", "2026-01-13", "2026-01-12", "2026-01-09", "2026-01-08", "2026-01-07", "2026-01-06", "2026-01-05", "2026-01-02", "2025-12-31", "2025-12-29", "2025-12-26", "2025-12-24"]);
const LOAD_MORE_DAYS = 7;

const dailyOverviewsRaw = {
    "2026-01-15": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-15)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: peer closed connection without sending complete message body (incomplete chunked read)",
    "2026-01-14": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-14)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u63ed\u793a\u4e86AI\u667a\u80fd\u4f53\u7814\u7a76\u6b63\u4ece\u5355\u4e00\u6a21\u578b\u7684\u80fd\u529b\u5c55\u793a\uff0c\u5411\u66f4\u590d\u6742\u7684**\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u4ee5\u53ca\u63a8\u7406\u6548\u7387\u4f18\u5316**\u6df1\u5ea6\u6f14\u8fdb\u3002\u4e00\u65b9\u9762\uff0c\u7814\u7a76\u8005\u4eec\u8bd5\u56fe\u901a\u8fc7\u5f15\u5165**\u4e16\u754c\u6a21\u578b**\u548c**\u524d\u77bb\u7b56\u7565**\u6765\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u89c4\u5212\u80fd\u529b\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u5173\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684**\u6210\u672c\u6548\u76ca**\u548c**\u534f\u4f5c\u6709\u6548\u6027**\u51fa\u73b0\u4e86\u53cd\u601d\u6027\u7814\u7a76\uff0c\u6311\u6218\u4e86\u201c\u8d8a\u590d\u6742\u8d8a\u597d\u201d\u7684\u5047\u8bbe\u3002\u6b64\u5916\uff0c**\u4e2a\u6027\u5316\u8bb0\u5fc6**\u4e0e**\u73af\u5883\u7406\u89e3**\u6210\u4e3a\u667a\u80fd\u4f53\u843d\u5730\u5e94\u7528\u7684\u5173\u952e\u7a81\u7834\u53e3\u3002\n\n---\n\n### \u534f\u4f5c\u7684\u6096\u8bba\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u5c40\u9650\n\n\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4eca\u65e5\u7684\u7814\u7a76\u4e0d\u4ec5\u63d0\u51fa\u4e86\u65b0\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u4e5f\u5bf9\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6709\u6548\u6027\u63d0\u51fa\u4e86\u8d28\u7591\u3002\n\n*   **[Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning]**\n    \u63d0\u51fa\u4e86 **MATTRL** \u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u7ed3\u6784\u5316\u7684\u6587\u672c\u7ecf\u9a8c\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u4e13\u5bb6\u56e2\u961f\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\u5e76\u8fbe\u6210\u5171\u8bc6\uff0c\u5728\u533b\u5b66\u548c\u6570\u5b66\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u51c6\u786e\u7387\u3002\n    (ArXiv ID 2601.09667 [cs.AI])\n\n*   **[DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols]**\n    \u8fd9\u9879\u7814\u7a76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4ee4\u4eba\u610f\u5916\u7684\u8d1f\u9762\u7ed3\u679c\uff1a\u5728\u63a7\u5236\u5b9e\u9a8c\u4e2d\uff0c\u7b80\u5355\u7684\u201c\u4ece\u6c60\u4e2d\u9009\u62e9\u6700\u4f73\u54cd\u5e94\u201d\u7b56\u7565\uff08Best-of-N\uff09\u4ee5 **6.0\u500d** \u7684\u4f18\u52bf\u51fb\u8d25\u4e86\u590d\u6742\u7684\u591aLLM\u5ba1\u8bae\u534f\u8bae\u3002\u8fd9\u8868\u660e\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u6027\u5e76\u4e0d\u4e00\u5b9a\u80fd\u63d0\u5347\u8d28\u91cf\uff0c\u4e14\u5f80\u5f80\u4f34\u968f\u7740\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002\n    (ArXiv ID 2601.08835 [cs.AI])\n\n*   **[SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration]**\n    \u53d7\u793e\u4f1a\u8d44\u672c\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51fa\u4e86 **SC-MAS** \u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6709\u5411\u56fe\u6765\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u667a\u80fd\u4f53\u89d2\u8272\u4e4b\u95f4\u7684\u5f02\u6784\u534f\u4f5c\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u4e0e\u6027\u80fd\u7684\u66f4\u597d\u5e73\u8861\u3002\n    (ArXiv ID 2601.09434 [cs.MA])\n\n*   **[Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants]**\n    \u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u591a\u533a\u57df\u653f\u7b56\u5236\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u6570\u636e\u3001\u75ab\u60c5\u6a21\u62df\u5668\u548c\u7ed3\u6784\u5316\u901a\u4fe1\uff0c\u5b9e\u73b0\u4e86\u8de8\u533a\u57df\u7684\u534f\u8c03\u9632\u63a7\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u611f\u67d3\u548c\u6b7b\u4ea1\u4eba\u6570\uff0c\u9a8c\u8bc1\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u516c\u5171\u653f\u7b56\u6a21\u62df\u4e2d\u7684\u4ef7\u503c\u3002\n    (ArXiv ID 2601.09264 [cs.AI])\n\n*   **[Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework]**\n    \u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u667a\u80fd\u4f53\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u77e5\u8bc6\u5408\u6210\uff08**RKS**\uff09\u6765\u5206\u6790\u591a\u6a21\u578b\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002\u8be5\u67b6\u6784\u5229\u7528\u8bed\u4e49\u751f\u6210\u3001\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u900f\u660e\u5ea6\u5ba1\u8ba1\u7684\u5faa\u73af\u4ea4\u4e92\uff0c\u5728\u65e0\u9700API\u7684\u516c\u5f00\u90e8\u7f72\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002\n    (ArXiv ID 2601.08839 [cs.CL])\n\n*   **[MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability]**\n    \u9488\u5bf9**\u65f6\u7a7a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027**\u74f6\u9888\uff0c\u63d0\u51fa\u4e86 **MACRO-LLM** \u6846\u67b6\uff0c\u5305\u542bCoProposer\u3001Negotiator\u548cIntrospector\u4e09\u4e2a\u6a21\u5757\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u9884\u6d4b\u63a8\u6f14\u548c\u5747\u503c\u573a\u7edf\u8ba1\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u96be\u9898\u3002\n    (ArXiv ID 2601.09295 [cs.MA])\n\n---\n\n### \u4ece\u53cd\u5e94\u5230\u9884\u6f14\uff1a\u667a\u80fd\u4f53\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u65b0\u8303\u5f0f\n\n\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u201c\u77ed\u89c6\u201d\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86\u4e16\u754c\u6a21\u578b\u3001\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u4ee3\u7801\u6267\u884c\u7b49\u673a\u5236\uff0c\u8d4b\u4e88\u667a\u80fd\u4f53\u66f4\u5f3a\u7684\u524d\u77bb\u6027\u548c\u89c4\u5212\u80fd\u529b\u3002\n\n*   **[Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models]**\n    \u63d0\u51fa\u4e86 **ITP** \u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u524d\u77bb\u6027\u7684\u201c\u60f3\u8c61\u201d\u800c\u975e\u76f4\u63a5\u4e0e\u73af\u5883\u4ea4\u4e92\u3002\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u524d\u77bb\u673a\u5236\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u8fdb\u5ea6\u52a8\u6001\u8c03\u6574\u60f3\u8c61\u6b65\u957f\uff0c\u4ece\u800c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u60f3\u8c61\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5236\u5b9a\u66f4\u4f18\u7b56\u7565\u3002\n    (ArXiv ID 2601.08955 [cs.AI])\n\n*   **[EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines]**\n    \u4e3a\u4e86\u89e3\u51b3\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86 **EvoFSM**\uff0c\u901a\u8fc7\u8fdb\u5316\u663e\u5f0f\u7684**\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09**\u800c\u975e\u81ea\u7531\u5f62\u5f0f\u7684\u4ee3\u7801\u91cd\u5199\u3002\u8be5\u65b9\u6cd5\u5c06\u4f18\u5316\u7a7a\u95f4\u89e3\u8026\u4e3a\u5b8f\u89c2\u6d41\u7a0b\u548c\u5fae\u89c2\u6280\u80fd\uff0c\u5728\u4fdd\u8bc1\u884c\u4e3a\u8fb9\u754c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u7684\u53ef\u63a7\u8fdb\u5316\u3002\n    (ArXiv ID 2601.09465 [cs.AI])\n\n*   **[Programming over Thinking: Efficient and Robust Multi-Constraint Planning]**\n    \u63d0\u51fa\u4e86 **SCOPE** \u6846\u67b6\uff0c\u4e3b\u5f20\u201c\u7f16\u7a0b\u4f18\u4e8e\u601d\u8003\u201d\uff0c\u5c06\u7279\u5b9a\u67e5\u8be2\u7684\u63a8\u7406\u4e0e\u901a\u7528\u4ee3\u7801\u6267\u884c\u5206\u79bb\u3002\u901a\u8fc7\u751f\u6210\u53ef\u91cd\u7528\u7684\u6c42\u89e3\u5668\u51fd\u6570\uff0c\u8be5\u65b9\u6cd5\u5728TravelPlanner\u7b49\u4efb\u52a1\u4e0a\u5927\u5e45\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u591a\u7ea6\u675f\u89c4\u5212\u7684\u6210\u529f\u7387\u3002\n    (ArXiv ID 2601.09097 [cs.AI])\n\n*   **[MAXS: Meta-Adaptive Exploration with LLM Agents]**\n    \u5f15\u5165\u4e86 **MAXS** \u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u524d\u77bb\u7b56\u7565\u4f30\u8ba1\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u503c\uff0c\u5e76\u7ed3\u5408\u8f68\u8ff9\u6536\u655b\u673a\u5236\u6765\u5e73\u8861\u63a8\u7406\u6548\u679c\u4e0e\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u524d\u7ec8\u6b62\u4e00\u81f4\u6027\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5de5\u5177\u63a8\u7406\u4e2d\u7684\u5c40\u90e8\u77ed\u89c6\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002\n    (ArXiv ID 2601.09259 [cs.AI])\n\n*   **[Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning]**\n    \u7814\u7a76\u4e86\u57fa\u4e8e\u8fed\u4ee3\u7684\u603b\u7ed3\u63a8\u7406\u6846\u67b6\uff0c\u53d1\u73b0\u901a\u8fc7\u5d4c\u5165\u5f0f\u7684\u8bed\u4e49\u7f13\u5b58\u68c0\u7d22\u5148\u524d\u7684\u63a8\u7406\u6a21\u5f0f\u53ef\u4ee5\u5f15\u5165**\u65b9\u5411\u6027\u5438\u5f15\u5b50**\u3002\u8fd9\u79cd\u673a\u5236\u867d\u7136\u80fd\u63d0\u5347\u7ed3\u6784\u5316\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u4f46\u4e5f\u53ef\u80fd\u5728\u5f02\u6784\u9886\u57df\u5bfc\u81f4\u6027\u80fd\u9000\u5316\uff0c\u63ed\u793a\u4e86\u76f8\u4f3c\u6027\u8bb0\u5fc6\u5728\u63a8\u7406\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\u3002\n    (ArXiv ID 2601.08846 [cs.AI])\n\n---\n\n### \u8bb0\u5fc6\u4e0e\u4e2a\u6027\u5316\uff1a\u6784\u5efa\u957f\u671f\u4ea4\u4e92\u7684\u57fa\u77f3\n\n\u667a\u80fd\u4f53\u8981\u771f\u6b63\u878d\u5165\u4eba\u7c7b\u751f\u6d3b\uff0c\u5fc5\u987b\u5177\u5907\u957f\u671f\u8bb0\u5fc6\u3001\u7406\u89e3\u9690\u5f0f\u610f\u56fe\u4ee5\u53ca\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u80fd\u529b\u3002\u4eca\u65e5\u7684\u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u8ba9\u667a\u80fd\u4f53\u201c\u8bb0\u4f4f\u201d\u5e76\u201c\u7406\u89e3\u201d\u7528\u6237\u4e0e\u73af\u5883\u3002\n\n*   **[PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records]**\n    \u63d0\u51fa\u4e86 **PersonalAlign** \u4efb\u52a1\u548c **HIM-Agent**\uff0c\u65e8\u5728\u5229\u7528\u957f\u671f\u7528\u6237\u8bb0\u5f55\u6765\u89e3\u51b3GUI\u667a\u80fd\u4f53\u4e2d\u7684\u9690\u5f0f\u610f\u56fe\u5bf9\u9f50\u95ee\u9898\u3002\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u7528\u6237\u504f\u597d\u548c\u4f8b\u884c\u7a0b\u5e8f\uff0c\u8be5\u667a\u80fd\u4f53\u5728\u5904\u7406\u6a21\u7cca\u6307\u4ee4\u548c\u63d0\u4f9b\u4e3b\u52a8\u5efa\u8bae\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\n    (ArXiv ID 2601.09636 [cs.AI])\n\n*   **[Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments]**\n    \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7**\u610f\u56fe\u6761\u4ef6\u76d1\u63a7**\u548c**\u4e8b\u4ef6\u89e6\u53d1\u8ddf\u8fdb**\u6765\u4e3b\u52a8\u7ef4\u62a4\u957f\u671f\u610f\u56fe\u3002\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728ChronosBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4efb\u52a1\u4ea4\u4e92\u4e2d\u7684\u6709\u6548\u6027\u3002\n    (ArXiv ID 2601.09382 [cs.AI])\n\n*   **[What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding]**\n    \u63d0\u51fa\u4e86 **Task2Quiz (T2Q)** \u8303\u5f0f\uff0c\u5c06\u4efb\u52a1\u6267\u884c\u4e0e\u73af\u5883\u72b6\u6001\u7406\u89e3\u89e3\u8026\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4efb\u52a1\u6210\u529f\u5f80\u5f80\u662f\u73af\u5883\u7406\u89e3\u7684\u7cdf\u7cd5\u4ee3\u7406\u6307\u6807\uff0c\u5f53\u524d\u7684\u8bb0\u5fc6\u673a\u5236\u5e76\u672a\u80fd\u6709\u6548\u5e2e\u52a9\u667a\u80fd\u4f53\u83b7\u5f97\u57fa\u4e8e\u4e8b\u5b9e\u7684\u73af\u5883\u6a21\u578b\uff0c\u6307\u51fa\u4e86\u4e3b\u52a8\u63a2\u7d22\u548c\u7ec6\u7c92\u5ea6\u72b6\u6001\u8868\u793a\u7684\u91cd\u8981\u6027\u3002\n    (ArXiv ID 2601.09503 [cs.AI])\n\n*   **[The AI Hippocampus: How Far are We From Human Memory?]**\n    \u8fd9\u662f\u4e00\u7bc7\u5173\u4e8eLLM\u548cMLLM\u4e2d\u8bb0\u5fc6\u673a\u5236\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u6784\u5efa\u4e86\u5305\u542b**\u9690\u5f0f\u8bb0\u5fc6**\u3001**\u663e\u5f0f\u8bb0\u5fc6**\u548c**\u667a\u80fd\u4f53\u8bb0\u5fc6**\u7684\u5206\u7c7b\u4f53\u7cfb\u3002\u6587\u7ae0\u6df1\u5165\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u5728\u589e\u5f3a\u63a8\u7406\u3001\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u8bb0\u5fc6\u6574\u5408\u9762\u4e34\u7684\u6311\u6218\u3002\n    (ArXiv ID 2601.09113 [cs.AI])\n\n---\n\n### \u8bc4\u4f30\u4e0e\u4f18\u5316\uff1a\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5bfb\u627e\u8fb9\u754c\n\n\u968f\u7740\u667a\u80fd\u4f53\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5982\u4f55\u51c6\u786e\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u5176\u90e8\u7f72\u6210\u672c\uff0c\u6210\u4e3a\u4e86\u7814\u7a76\u70ed\u70b9\u3002\n\n*   **[The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments]**\n    \u5728\u771f\u5b9e\u7684\u7535\u5546RL\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u524d\u6cbf\u6a21\u578b\uff0c\u603b\u7ed3\u51fa\u4e86\u4e00\u4e2a**\u667a\u80fd\u4f53\u80fd\u529b\u5c42\u7ea7**\uff1a\u5de5\u5177\u4f7f\u7528\u3001\u89c4\u5212\u4e0e\u76ee\u6807\u5f62\u6210\u3001\u9002\u5e94\u6027\u3001\u57fa\u7840\u6027\u548c\u5e38\u8bc6\u63a8\u7406\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u6700\u597d\u7684\u6a21\u578b\u5728\u73b0\u5b9e\u5de5\u4f5c\u573a\u6240\u4efb\u52a1\u4e2d\u4ecd\u6709\u7ea640%\u7684\u5931\u8d25\u7387\uff0c\u4e14\u5931\u8d25\u6a21\u5f0f\u5448\u73b0\u51fa\u660e\u663e\u7684\u5c42\u7ea7\u7279\u5f81\u3002\n    (ArXiv ID 2601.09032 [cs.AI])\n\n*   **[DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation]**\n    \u4ecb\u7ecd\u4e86 **DeepResearchEval**\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u5e76\u8fdb\u884c\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u6846\u67b6\u3002\u5b83\u5305\u542b\u81ea\u9002\u5e94\u70b9\u5f0f\u8d28\u91cf\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u6838\u67e5\u7ec4\u4ef6\uff0c\u80fd\u591f\u52a8\u6001\u751f\u6210\u8bc4\u4f30\u7ef4\u5ea6\u5e76\u9a8c\u8bc1\u5f15\u7528\u7f3a\u5931\u7684\u62a5\u544a\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u96be\u7684\u95ee\u9898\u3002\n    (ArXiv ID 2601.09688 [cs.CL])\n\n*   **[UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning]**\n    \u63d0\u51fa\u4e86 **UserLM-R1**\uff0c\u4e00\u79cd\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u7528\u6237\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u4f5c\u4e3a\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u7684\u7406\u60f3\u4ea4\u4e92\u73af\u5883\u3002\u901a\u8fc7\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff0c\u8be5\u6a21\u578b\u4e0d\u4ec5\u9002\u5e94\u4e0d\u540c\u573a\u666f\uff0c\u8fd8\u5177\u5907\u6218\u7565\u601d\u7ef4\uff0c\u80fd\u6709\u6548\u6311\u6218\u6216\u8ba8\u4ef7\u8fd8\u4ef7\uff0c\u9632\u6b62\u88ab\u667a\u80fd\u4f53\u64cd\u7eb5\u3002\n    (ArXiv ID 2601.09215 [cs.CL])\n\n*   **[LLMs can Compress LLMs: Adaptive Pruning by Agents]**\n    \u63d0\u51fa\u4e86\u7531\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a**\u81ea\u9002\u5e94\u526a\u679d\u667a\u80fd\u4f53**\u7684\u65b0\u65b9\u6cd5\uff0c\u6307\u5bfcLLM\u7684\u526a\u679d\u8fc7\u7a0b\u3002\u8be5\u667a\u80fd\u4f53\u7ed3\u5408\u6743\u91cd-\u6fc0\u6d3b\u6307\u6807\u548c\u68af\u5ea6\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u8fed\u4ee3\u4f18\u5316\u526a\u679d\u7b56\u7565\uff0c\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u526a\u679d\u540e\u6a21\u578b\u7684\u4e8b\u5b9e\u77e5\u8bc6\u4fdd\u7559\u7387\u3002\n    (ArXiv ID 2601.09694 [cs.AI])\n\n*   **[LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach]**\n    \u63d0\u51fa\u4e86 **LEAN-LLM-OPT**\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u4f18\u5316\u6a21\u578b\u3002\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5de5\u4f5c\u6d41\u5e76\u5c06\u673a\u68b0\u6027\u6570\u636e\u5904\u7406\u5378\u8f7d\u7ed9\u8f85\u52a9\u5de5\u5177\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u52a0\u5761\u822a\u7a7a\u7b49\u5b9e\u9645\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5efa\u6a21\u80fd\u529b\u3002\n    (ArXiv ID 2601.09635 [cs.AI])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n*   **\u98a0\u8986\u6027\u53d1\u73b0\uff1a\u591a\u667a\u80fd\u4f53\u5ba1\u8bae\u53ef\u80fd\u5e76\u4e0d\u5212\u7b97**\n    *DeliberationBench* \u7684\u7814\u7a76\u7ed3\u679c\u4ee4\u4eba\u6df1\u601d\uff0c\u5b83\u7528\u6570\u636e\u8bc1\u660e\u4e86\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u667a\u80fd\u4f53\u5ba1\u8bae\u534f\u8bae\u4e0d\u4ec5\u8ba1\u7b97\u6210\u672c\u9ad8\u51fa\u6570\u500d\uff0c\u5176\u8868\u73b0\u751a\u81f3\u4e0d\u5982\u7b80\u5355\u7684\u201c\u9009\u6700\u4f73\u201d\u7b56\u7565\u3002\u8fd9\u63d0\u793a\u793e\u533a\uff0c\u5728\u8ffd\u6c42\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u5fc5\u987b\u91cd\u65b0\u5ba1\u89c6\u201c\u4eba\u591a\u529b\u91cf\u5927\u201d\u8fd9\u4e00\u5047\u8bbe\u5728AI\u9886\u57df\u7684\u6709\u6548\u6027\u3002\n\n*   **\u63a8\u7406\u8303\u5f0f\u7684\u8f6c\u79fb\uff1a\u4ece\u201c\u601d\u7ef4\u94fe\u201d\u5230\u201c\u4e16\u754c\u6a21\u578b\u201d**\n    \u591a\u7bc7\u8bba\u6587\uff08\u5982 *Imagine-then-Plan*, *MAXS*, *SCOPE*\uff09 \u5171\u540c\u6307\u5411\u4e86\u4e00\u4e2a\u8d8b\u52bf\uff1a\u667a\u80fd\u4f53\u6b63\u5728\u4ece\u5355\u7eaf\u7684\u6587\u672c\u63a8\u7406\uff08CoT\uff09\u8f6c\u5411\u5229\u7528**\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u9884\u6f14**\u6216**\u4ee3\u7801\u6267\u884c**\u3002\u8fd9\u79cd\u201c\u5148\u60f3\u8c61\u540e\u884c\u52a8\u201d\u6216\u201c\u7f16\u7a0b\u4f18\u4e8e\u601d\u8003\u201d\u7684\u6a21\u5f0f\uff0c\u6b63\u5728\u6210\u4e3a\u89e3\u51b3\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u591a\u7ea6\u675f\u95ee\u9898\u7684\u5173\u952e\u8def\u5f84\u3002\n\n*   **\u8bb0\u5fc6\u673a\u5236\u7684\u201c\u77e5\u884c\u5206\u79bb\u201d**\n    *Task2Quiz* \u7684\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u5c34\u5c2c\u7684\u73b0\u5b9e\uff1a\u667a\u80fd\u4f53\u53ef\u80fd\u6210\u529f\u5b8c\u6210\u4e86\u4efb\u52a1\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u5b83\u4eec\u771f\u6b63\u201c\u7406\u89e3\u201d\u4e86\u6240\u5904\u7684\u73af\u5883\u3002\u8fd9\u4e3a\u672a\u6765\u7684\u667a\u80fd\u4f53\u7814\u7a76\u6572\u54cd\u4e86\u8b66\u949f\u2014\u2014\u4ec5\u4ec5\u4f18\u5316\u4efb\u52a1\u6210\u529f\u7387\u662f\u4e0d\u591f\u7684\uff0c\u5982\u4f55\u6784\u5efa\u624e\u5b9e\u3001\u53ef\u8fc1\u79fb\u7684\u4e16\u754c\u72b6\u6001\u6a21\u578b\u624d\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u5fc5\u7ecf\u4e4b\u8def\u3002\n\n*   **\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u7684\u65b0\u65b9\u5411\uff1a\u7ed3\u6784\u5316\u4f18\u4e8e\u81ea\u7531\u5f0f**\n    *EvoFSM* \u548c *LLM can Compress LLMs* \u5c55\u793a\u4e86\u667a\u80fd\u4f53\u81ea\u6211\u4f18\u5316\u7684\u65b0\u601d\u8def\u3002\u4e0e\u5176\u8ba9\u667a\u80fd\u4f53\u81ea\u7531\u5730\u91cd\u5199\u4ee3\u7801\u6216Prompt\uff08\u5bb9\u6613\u5bfc\u81f4\u5931\u63a7\uff09\uff0c\u4e0d\u5982\u9650\u5236\u5176\u5728**\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09**\u6216**\u526a\u679d\u7b56\u7565**\u7b49\u7ed3\u6784\u5316\u7a7a\u95f4\u5185\u8fdb\u884c\u8fdb\u5316\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u63a7\u6027\u3002",
    "2026-01-13": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-13)\n\n\u4eca\u5929\u7684\u8bba\u6587\u96c6\u63ed\u793a\u4e86AI\u7814\u7a76\u4ece\u5355\u7eaf\u8ffd\u6c42\u6a21\u578b\u89c4\u6a21\u5411\u6784\u5efa**\u590d\u6742\u3001\u9c81\u68d2\u4e14\u5177\u5907\u957f\u671f\u8ba4\u77e5\u80fd\u529b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf**\u7684\u6df1\u523b\u8f6c\u578b\u3002\u7814\u7a76\u91cd\u70b9\u96c6\u4e2d\u5728\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff1a\u4e00\u662f\u5bf9**Agentic RAG\uff08\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09**\u7684\u6548\u80fd\u8fdb\u884c\u53cd\u601d\u4e0e\u67b6\u6784\u4f18\u5316\uff0c\u8bd5\u56fe\u5728\u6210\u672c\u4e0e\u6027\u80fd\u95f4\u5bfb\u627e\u5e73\u8861\uff1b\u4e8c\u662f**\u8bb0\u5fc6\u673a\u5236**\u7684\u7206\u53d1\u5f0f\u521b\u65b0\uff0c\u7814\u7a76\u8005\u4eec\u6b63\u8bd5\u56fe\u4e3aAI\u690d\u5165\u7c7b\u4f3c\u4eba\u7c7b\u6d77\u9a6c\u4f53\u7684\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u4ee5\u652f\u6301\u957f\u671f\u4ea4\u4e92\uff1b\u4e09\u662f**\u81ea\u6211\u8fdb\u5316\u4e0e\u8bc4\u4f30**\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u667a\u80fd\u4f53\u5728\u65e0\u6570\u636e\u6216\u5c11\u6570\u636e\u73af\u5883\u4e0b\u81ea\u6211\u8fed\u4ee3\uff0c\u5e76\u5f00\u53d1\u66f4\u4e25\u82db\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u66b4\u9732\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8106\u5f31\u6027\u3002\n\n---\n\n### \u667a\u80fd\u4f53\u67b6\u6784\u5347\u7ea7\uff1a\u4eceRAG\u8fdb\u5316\u5230\u81ea\u4e3b\u89c4\u5212\n\n\u968f\u7740LLM\u5411Agent\u6f14\u8fdb\uff0c\u4f20\u7edf\u7684\u9759\u6001RAG\u548c\u7b80\u5355\u7684\u5de5\u5177\u8c03\u7528\u5df2\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u9700\u6c42\uff0c\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u89c4\u5212\u3001\u53cd\u601d\u548c\u5de5\u5177\u8fdb\u5316\u6765\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u80fd\u529b\u3002\n\n*   **TOOLQP** \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u68c0\u7d22\u5efa\u6a21\u4e3a**\u8fed\u4ee3\u5f0f\u67e5\u8be2\u89c4\u5212**\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u52a8\u6001\u751f\u6210\u67e5\u8be2\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u76ee\u6807\u4e0e\u6280\u672f\u6587\u6863\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5de5\u5177\u5e93\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002 (2601.07782 [cs.CL])\n*   **Is Agentic RAG worth it?** \u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u5bf9\u6bd4\u4e86\"\u589e\u5f3a\u578bRAG\"\u4e0e\"Agentic RAG\"\uff0c\u63ed\u793a\u4e86\u4e24\u8005\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6743\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u5728**\u6210\u672c\u3001\u6027\u80fd\u4e0e\u5ef6\u8fdf**\u4e4b\u95f4\u9009\u62e9\u6700\u5408\u9002\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u3002 (2601.07711 [cs.CL])\n*   **TreePS-RAG** \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u7684\u5728\u7ebfRL\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53RAG\u7684\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a**\u5c55\u5f00\u6811**\uff0c\u5229\u7528\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5b9e\u73b0\u4ec5\u57fa\u4e8e\u6700\u7ec8\u5956\u52b1\u7684**\u6b65\u9aa4\u7ea7\u4fe1\u7528\u5206\u914d**\uff0c\u5728\u65e0\u9700\u4e2d\u95f4\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002 (2601.06922 [cs.CL])\n*   **Beyond Static Tools (TTE)** \u63d0\u51fa\u4e86**\u6d4b\u8bd5\u65f6\u5de5\u5177\u8fdb\u5316**\u8303\u5f0f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5408\u6210\u3001\u9a8c\u8bc1\u5e76\u8fdb\u5316\u53ef\u6267\u884c\u5de5\u5177\uff0c\u4ece\u800c\u514b\u670d\u4e86\u9759\u6001\u5de5\u5177\u5e93\u5728\u79d1\u5b66\u9886\u57df\u56fa\u6709\u7684\u521a\u6027\u548c\u957f\u5c3e\u5c40\u9650\u6027\u3002 (2601.07641 [cs.CL])\n*   **Task-Decoupled Planning (TDP)** \u9488\u5bf9\u957f\u89c6\u754c\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd**\u4efb\u52a1\u89e3\u8026\u89c4\u5212**\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u5e76\u9650\u5236\u63a8\u7406\u8303\u56f4\uff0c\u6709\u6548\u9632\u6b62\u4e86\u9519\u8bef\u4f20\u64ad\u5e76\u5927\u5e45\u964d\u4f4e\u4e86Token\u6d88\u8017\u3002 (2601.07577 [cs.AI])\n*   **JudgeFlow** \u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u8bc4\u4f30-\u5224\u65ad-\u4f18\u5316-\u66f4\u65b0\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u590d\u7528\u7684\u903b\u8f91\u5757\u548c\u4e13\u95e8\u7684**Judge\u6a21\u5757**\u6765\u68c0\u67e5\u6267\u884c\u8f68\u8ff9\u5e76\u5b9a\u4f4d\u95ee\u9898\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5bf9\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002 (2601.07477 [cs.AI])\n*   **OS-Symphony** \u662f\u4e00\u4e2a\u901a\u7528\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7**\u53cd\u601d-\u8bb0\u5fc6\u667a\u80fd\u4f53**\u548c**\u591a\u529f\u80fd\u5de5\u5177\u667a\u80fd\u4f53**\u7684\u534f\u540c\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u754c\u5de5\u4f5c\u6d41\u4e2d\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u65b0\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u3002 (2601.07779 [cs.CL])\n*   **ToolGym** \u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b5,571\u4e2a\u7edf\u4e00\u683c\u5f0f\u5de5\u5177\u7684**\u5f00\u653e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u73af\u5883**\uff0c\u5e76\u5f15\u5165\u4e86\u4efb\u52a1\u521b\u5efa\u5f15\u64ce\u548c\u72b6\u6001\u63a7\u5236\u5668\u6765\u538b\u529b\u6d4b\u8bd5\u667a\u80fd\u4f53\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7ea6\u675f\u9075\u5faa\u65b9\u9762\u7684\u5f31\u70b9\u3002 (2601.06328 [cs.AI])\n\n---\n\n### \u8bb0\u5fc6\u67b6\u6784\u91cd\u6784\uff1a\u6784\u5efa\u5177\u5907\"\u6d77\u9a6c\u4f53\"\u80fd\u529b\u7684\u667a\u80fd\u4f53\n\n\u4e3a\u4e86\u652f\u6301\u957f\u671f\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\uff0c\u4eca\u65e5\u6d8c\u73b0\u4e86\u5927\u91cf\u5173\u4e8e\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u8d8b\u52bf\u662f\u4ece\u6241\u5e73\u7684RAG\u8f6c\u5411\u5206\u5c42\u3001\u4e8b\u4ef6\u9a71\u52a8\u4e14\u5177\u5907\u65f6\u95f4\u611f\u77e5\u7684\u52a8\u6001\u8bb0\u5fc6\u67b6\u6784\u3002\n\n*   **ES-Mem** \u53d7**\u4e8b\u4ef6\u5206\u5272\u7406\u8bba**\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u52a8\u6001\u4e8b\u4ef6\u5206\u5272\u6a21\u5757\u548c\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u8bed\u4e49\u951a\u5b9a\u7279\u5b9a\u7684\u60c5\u666f\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u3002 (2601.07582 [cs.CL])\n*   **RealMem** \u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u4e8e\u73b0\u5b9e\u9879\u76ee\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03**\"\u957f\u671f\u9879\u76ee\u5bfc\u5411\"**\u7684\u4ea4\u4e92\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ba1\u7406\u52a8\u6001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u957f\u671f\u9879\u76ee\u72b6\u6001\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002 (2601.06966 [cs.CL])\n*   **HiMem** \u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7**\u4e3b\u9898\u611f\u77e5\u7684\u4e8b\u4ef6-\u60ca\u8bb6\u53cc\u901a\u9053\u5206\u5272**\u6784\u5efa\u60c5\u666f\u8bb0\u5fc6\uff0c\u5e76\u63d0\u53d6\u7a33\u5b9a\u7684\u7b14\u8bb0\u8bb0\u5fc6\uff0c\u652f\u6301\u6df7\u5408\u68c0\u7d22\u548c\u51b2\u7a81\u611f\u77e5\u7684**\u8bb0\u5fc6\u518d\u5de9\u56fa**\u3002 (2601.06377 [cs.AI])\n*   **TeleMem** \u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u957f\u65f6\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7**\u53d9\u4e8b\u52a8\u6001\u63d0\u53d6**\u548c\u7ed3\u6784\u5316\u5199\u5165\u7ba1\u7ebf\uff0c\u5728\u4fdd\u6301\u7528\u6237\u753b\u50cf\u8fde\u8d2f\u6027\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u5b58\u50a8\u6548\u7387\u5e76\u52a0\u901f\u4e86\u8bb0\u5fc6\u64cd\u4f5c\u3002 (2601.06037 [cs.CL])\n*   **Amory** \u662f\u4e00\u4e2a\u5de5\u4f5c\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u65f6\u95f4\u7684**\u667a\u80fd\u4f53\u63a8\u7406**\u4e3b\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u8868\u793a\uff0c\u5c06\u5bf9\u8bdd\u7247\u6bb5\u7ec4\u7ec7\u4e3a\u60c5\u666f\u53d9\u4e8b\uff0c\u5e76\u5229\u7528\u52a8\u91cf\u8fdb\u884c\u8bb0\u5fc6\u5de9\u56fa\uff0c\u5728LOCOMO\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6027\u80fd\u3002 (2601.06282 [cs.CL])\n*   **Structured Episodic Event Memory (SEEM)** \u7ed3\u5408\u4e86\u56fe\u8bb0\u5fc6\u5c42\u548c\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\uff0c\u57fa\u4e8e\u8ba4\u77e5\u6846\u67b6\u7406\u8bba\u5c06\u4ea4\u4e92\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684**\u60c5\u666f\u4e8b\u4ef6\u5e27**\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u6eaf\u6e90\u6269\u5c55\u673a\u5236\u91cd\u5efa\u8fde\u8d2f\u7684\u53d9\u4e8b\u4e0a\u4e0b\u6587\u3002 (2601.06411 [cs.CL])\n*   **Temporal Semantic Memory (TSM)** \u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5efa\u6a21\u4e0a\u7684\u4e0d\u51c6\u786e\u6027\u548c\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa**\u8bed\u4e49\u65f6\u95f4\u7ebf**\u548c\u6301\u7eed\u8bb0\u5fc6\uff0c\u652f\u6301\u57fa\u4e8e\u65f6\u95f4\u610f\u56fe\u7684\u68c0\u7d22\uff0c\u63d0\u4f9b\u4e86\u65f6\u95f4\u6709\u6548\u4e14\u6301\u7eed\u4e00\u81f4\u7684\u4e0a\u4e0b\u6587\u3002 (2601.07468 [cs.AI])\n*   **Bi-Mem** \u901a\u8fc7**\u5f52\u7eb3-\u53cd\u601d\u667a\u80fd\u4f53**\u7684\u53cc\u5411\u6784\u5efa\u673a\u5236\uff0c\u5229\u7528\u5168\u5c40\u4eba\u8bbe\u7ea6\u675f\u6821\u51c6\u5c40\u90e8\u573a\u666f\u8bb0\u5fc6\uff0c\u786e\u4fdd\u4e86\u5206\u5c42\u8bb0\u5fc6\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u901a\u8fc7\u8054\u60f3\u68c0\u7d22\u673a\u5236\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7684\u8bb0\u5fc6\u53ec\u56de\u3002 (2601.06490 [cs.MA])\n\n---\n\n### \u6253\u7834\"\u6e29\u5ba4\u6548\u5e94\"\uff1a\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u4e0e\u9c81\u68d2\u6027\n\n\u7814\u7a76\u8005\u4eec\u6b63\u81f4\u529b\u4e8e\u5f00\u53d1\u66f4\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u667a\u80fd\u4f53\u5728\u9762\u5bf9\u566a\u58f0\u3001\u5e72\u6270\u548c\u957f\u671f\u538b\u529b\u65f6\u7684\u53ef\u9760\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e25\u91cd\u7f3a\u9677\u3002\n\n*   **AgentHallu** \u9488\u5bf9\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u51fa\u4e86**\u81ea\u52a8\u5e7b\u89c9\u5f52\u56e0**\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b693\u4e2a\u9ad8\u8d28\u91cf\u8f68\u8ff9\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u9876\u5c16\u6a21\u578b\uff08\u5982GPT-5\uff09\u5728\u5b9a\u4f4d\u5e7b\u89c9\u6b65\u9aa4\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a41.1%\u3002 (2601.06818 [cs.CL])\n*   **NoisyBench** \u7cfb\u7edf\u8bc4\u4f30\u4e86\u6a21\u578b\u5728RAG\u3001\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u5bf9**\u4e0a\u4e0b\u6587\u5e72\u6270\u9879**\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0SOTA\u6a21\u578b\u5728\u9762\u5bf9\u566a\u58f0\u65f6\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe80%\uff0c\u4e14**\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u589e\u52a0\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d**\u3002 (2601.07226 [cs.CL])\n*   **ReliabilityBench** \u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u53ef\u9760\u6027\u8868\u9762 $R(k,\u03b5,\u03bb)$\uff0c\u4ece\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u667a\u80fd\u4f53\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u8bed\u4e49\u6270\u52a8\u4e5f\u4f1a\u5bfc\u81f4\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\u3002 (2601.06112 [cs.AI])\n*   **IDRBench** \u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30**\u4ea4\u4e92\u5f0f\u6df1\u5ea6\u7814\u7a76**\u7684\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u6309\u9700\u4ea4\u4e92\u548c\u7528\u6237\u6a21\u62df\u5668\uff0c\u91cf\u5316\u4e86\u4ea4\u4e92\u5e26\u6765\u7684\u8d28\u91cf\u63d0\u5347\u4e0e\u6210\u672c\u6743\u8861\uff0c\u8868\u660e\u4ea4\u4e92\u5f80\u5f80\u6bd4\u6a21\u578b\u80fd\u529b\u7684\u5dee\u5f02\u66f4\u80fd\u51b3\u5b9a\u7814\u7a76\u8d28\u91cf\u3002 (2601.06676 [cs.CL])\n*   **MedEinst** \u901a\u8fc7**\u53cd\u4e8b\u5b9e\u8bca\u65ad**\u57fa\u51c6\u6d4b\u8bd5\u4e86\u533b\u7597LLM\u4e2d\u7684**\u5b9a\u52bf\u6548\u5e94**\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u867d\u7136\u5728\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u9762\u5bf9\u9677\u9631\u75c5\u4f8b\u65f6\u6781\u6613\u4ea7\u751f\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u6001\u56e0\u679c\u63a8\u7406\u7684ECR-Agent\u8fdb\u884c\u7f13\u89e3\u3002 (2601.06636 [cs.CL])\n*   **The Confidence Dichotomy** \u63ed\u793a\u4e86\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u4e2d\u5b58\u5728\u7684**\u7f6e\u4fe1\u5ea6\u4e8c\u5206\u6cd5**\uff1a\u8bc1\u636e\u5de5\u5177\uff08\u5982\u641c\u7d22\uff09\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c\u9a8c\u8bc1\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u5219\u80fd\u901a\u8fc7\u786e\u5b9a\u6027\u53cd\u9988\u7f13\u89e3\u6821\u51c6\u8bef\u5dee\u3002 (2601.07264 [cs.CL])\n\n---\n\n### \u6570\u636e\u532e\u4e4f\u65f6\u4ee3\u7684\u81ea\u6211\u8fdb\u5316\uff1aRL\u4e0e\u667a\u80fd\u4f53\u7684\u5171\u751f\n\n\u968f\u7740\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u65e5\u76ca\u7a00\u7f3a\uff0c\u5982\u4f55\u8ba9\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u5916\u90e8\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u6216\u6a21\u578b\u5408\u5e76\u6765\u63d0\u5347\u80fd\u529b\uff0c\u6210\u4e3a\u4e86\u4eca\u65e5\u7684\u7814\u7a76\u70ed\u70b9\u3002\n\n*   **Dr. Zero** \u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684**\u81ea\u6211\u8fdb\u5316\u641c\u7d22\u667a\u80fd\u4f53**\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u8bae\u8005\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\u6765\u8bad\u7ec3\u6c42\u89e3\u8005\uff0c\u5e76\u5229\u7528**\u8df3\u8dc3\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08HRPO\uff09**\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5339\u914d\u7684\u6027\u80fd\u3002 (2601.07055 [cs.AI])\n*   **PRISM** \u57fa\u4e8e\u56fe\u5f0f\u7406\u8bba\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u529b\u5b66\u611f\u77e5\u7684\u6570\u636e\u4ef2\u88c1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u7684\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u5c06\u5f15\u53d1\u9ad8\u8ba4\u77e5\u51b2\u7a81\u7684\u6570\u636e\u8def\u7531\u81f3RL\u8fdb\u884c\u7ed3\u6784\u91cd\u7ec4\uff0c\u5c06\u4f4e\u51b2\u7a81\u6570\u636e\u8def\u7531\u81f3SFT\u8fdb\u884c\u6a21\u5f0f\u5de9\u56fa\uff0c\u5b9e\u73b0\u4e86Pareto\u6539\u8fdb\u3002 (2601.07224 [cs.AI])\n*   **ECHO (Evolving Critic for Hindsight-Guided Optimization)** \u63d0\u51fa\u4e86\u4e00\u79cd**\u534f\u540c\u8fdb\u5316\u5faa\u73af**\uff0c\u901a\u8fc7\u7ea7\u8054\u5c55\u5f00\u673a\u5236\u548c\u9971\u548c\u611f\u77e5\u589e\u76ca\u6574\u5f62\u76ee\u6807\uff0c\u786e\u4fdd\u8bc4\u8bba\u5bb6\u7684\u53cd\u9988\u4e0e\u4e0d\u65ad\u6f14\u8fdb\u7684\u7b56\u7565\u4fdd\u6301\u540c\u6b65\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u8bc4\u8bba\u5bb6\u5728\u5728\u7ebfRL\u4e2d\u53cd\u9988\u5931\u6548\u7684\u95ee\u9898\u3002 (2601.06794 [cs.AI])\n*   **ArenaRL** \u9488\u5bf9\u5f00\u653e\u7aef\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u533a\u5206\u7ec6\u5fae\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u70b9\u5f0f\u6807\u91cf\u8bc4\u5206\u8f6c\u5411**\u7ec4\u5185\u76f8\u5bf9\u6392\u540d**\uff0c\u5229\u7528\u9526\u6807\u8d5b\u5f0f\u6392\u540d\u65b9\u6848\u5728\u4fdd\u6301O(N)\u590d\u6742\u5ea6\u7684\u540c\u65f6\u83b7\u5f97\u4e86\u7a33\u5b9a\u7684\u4f18\u52bf\u4fe1\u53f7\u3002 (2601.06487 [cs.AI])\n*   **Controlled Self-Evolution (CSE)** \u9488\u5bf9\u7b97\u6cd5\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u74f6\u9888\uff0c\u5f15\u5165\u4e86\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u548c**\u9057\u4f20\u8fdb\u5316**\u673a\u5236\uff0c\u901a\u8fc7\u53cd\u9988\u5f15\u5bfc\u7684\u7a81\u53d8\u548c\u7ec4\u5408\u4ea4\u53c9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u66f4\u4f18\u89e3\u7684\u80fd\u529b\u3002 (2601.07348 [cs.CL])\n*   **ARM (Agent-Role Merging)** \u63d0\u51fa\u4e86\u4e00\u79cd**\u89d2\u8272\u6761\u4ef6\u795e\u7ecf\u5143\u79fb\u690d**\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6fc0\u6d3b\u6a21\u5f0f\u6765\u5408\u5e76\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u65e0\u9700\u68af\u5ea6\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u667a\u80fd\u4f53\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u7684\u76ee\u6807\u3002 (2601.07309 [cs.AI])\n\n---\n\n### \u4eca\u65e5\u770b\u70b9\n\n1.  **Agentic RAG \u7684\"\u795b\u9b45\"\u4e0e\u52a1\u5b9e\u9009\u62e9**\uff1a\u4eca\u65e5\u6700\u5f15\u4eba\u6df1\u601d\u7684\u8bba\u6587\u4e4b\u4e00\u662f *Is Agentic RAG worth it?*\u3002\u5728\u4e1a\u754c\u72c2\u70ed\u8ffd\u6367Agentic Workflow\u4e4b\u9645\uff0c\u8be5\u7814\u7a76\u51b7\u9759\u5730\u6307\u51fa\u4e86\u5176\u6210\u672c\u4e0e\u5ef6\u8fdf\u7684\u4ee3\u4ef7\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u9009\u62e9\"\u589e\u5f3a\u578bRAG\"\u6216\"Agentic RAG\"\u7684\u5b9e\u8bc1\u4f9d\u636e\u3002\u8fd9\u6807\u5fd7\u7740Agent\u7814\u7a76\u5f00\u59cb\u4ece\"\u80fd\u4e0d\u80fd\u505a\"\u8f6c\u5411\"\u503c\u4e0d\u503c\u5f97\u505a\"\u7684\u5de5\u7a0b\u5316\u6df1\u6c34\u533a\u3002\n2.  **\u8bb0\u5fc6\uff1a\u667a\u80fd\u4f53\u7684\"\u7b2c\u4e8c\u5927\u8111\"**\uff1a\u4ece *ES-Mem* \u5230 *HiMem*\uff0c\u518d\u5230 *TeleMem*\uff0c\u4eca\u65e5\u5927\u91cf\u9ad8\u8d28\u91cf\u8bba\u6587\u96c6\u4e2d\u5728\u8bb0\u5fc6\u67b6\u6784\u4e0a\u3002\u8fd9\u8868\u660e\u4e1a\u754c\u5df2\u8fbe\u6210\u5171\u8bc6\uff1a\u5355\u7eaf\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7a97\u53e3\u662f\u4e0d\u591f\u7684\uff0c\u672a\u6765\u7684\u667a\u80fd\u4f53\u5fc5\u987b\u5177\u5907\u7ed3\u6784\u5316\u3001\u5206\u5c42\u4e14\u80fd\u81ea\u6211\u66f4\u65b0\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u8fd9\u53ef\u80fd\u662f\u5b9e\u73b0\u957f\u671f\u4e2a\u6027\u5316\u4ea4\u4e92\u7684\u5173\u952e\u7a81\u7834\u53e3\u3002\n3.  **\u81ea\u6211\u8fdb\u5316\uff1a\u6253\u7834\u6570\u636e Scaling Law \u7684\u8bc5\u5492**\uff1a*Dr. Zero* \u548c *Beyond Static Tools (TTE)* \u7b49\u7814\u7a76\u5c55\u793a\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u65b9\u5411\u2014\u2014\u667a\u80fd\u4f53\u4e0d\u518d\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u7684\"\u6b7b\u6570\u636e\"\uff0c\u800c\u662f\u901a\u8fc7\u81ea\u6211\u535a\u5f08\u3001\u5de5\u5177\u8fdb\u5316\u548c\u53cd\u601d\u751f\u6210\"\u6d3b\u6570\u636e\"\u3002\u8fd9\u79cd\"\u65e0\u6570\u636e\u81ea\u6211\u8fdb\u5316\"\u8303\u5f0f\u53ef\u80fd\u662f\u89e3\u51b3\u9ad8\u8d28\u91cf\u6570\u636e\u67af\u7aed\u95ee\u9898\u7684\u7ec8\u6781\u65b9\u6848\u3002\n4.  **\u9c81\u68d2\u6027\u5371\u673a\uff1a\u566a\u58f0\u662f\u63a8\u7406\u6a21\u578b\u7684\u963f\u5580\u7409\u65af\u4e4b\u8e35**\uff1a*NoisyBench* \u7684\u53d1\u73b0\u4ee4\u4eba\u8b66\u9192\uff1aSOTA\u6a21\u578b\u5728\u9762\u5bf9\u4e0a\u4e0b\u6587\u5e72\u6270\u9879\u65f6\u6027\u80fd\u4f1a\u66b4\u8dcc80%\uff0c\u4e14\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u53cd\u800c\u53ef\u80fd\u6076\u5316\u7ed3\u679c\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8eCoT\u548cAgentic\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u672c\u8d28\u4e0a\u53ef\u80fd\u5bf9\u566a\u58f0\u6781\u5176\u654f\u611f\uff0c\u672a\u6765\u7684\u7814\u7a76\u5fc5\u987b\u4ece\u5355\u7eaf\u7684\"\u63d0\u5347\u63a8\u7406\u6df1\u5ea6\"\u8f6c\u5411\"\u63d0\u5347\u6297\u566a\u9c81\u68d2\u6027\"\u3002",
    "2026-01-12": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-12)\n\n#### \u4e8c\u3001 \u5f00\u7bc7\u5bfc\u8bed\n\u4eca\u65e5\u7684\u7814\u7a76\u5448\u73b0\u51fa\u667a\u80fd\u4f53\u5411\u66f4\u6df1\u5c42\u6b21\u8ba4\u77e5\u4e0e\u66f4\u9ad8\u6548\u6267\u884c\u6f14\u8fdb\u7684\u660e\u663e\u8d8b\u52bf\u3002\u6838\u5fc3\u7126\u70b9\u96c6\u4e2d\u5728\u5229\u7528**\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09**\u91cd\u5851\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\u7b80\u5355\u7684\u5956\u52b1\u4fe1\u53f7\u8f6c\u5411\u7ec6\u7c92\u5ea6\u7684\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u53cd\u9988\u673a\u5236\u3002\u540c\u65f6\uff0c**\u8bb0\u5fc6\u673a\u5236**\u6b63\u4ece\u9759\u6001\u5b58\u50a8\u5411\u52a8\u6001\u3001\u60c5\u611f\u5316\u4e14\u5177\u5907\u8ba1\u7b97\u590d\u7528\u80fd\u529b\u7684\u65b9\u5411\u8fdb\u5316\u3002\u6b64\u5916\uff0c\u7814\u7a76\u754c\u5f00\u59cb\u9ad8\u5ea6\u91cd\u89c6\u667a\u80fd\u4f53\u7684**\u793e\u4f1a\u5c5e\u6027**\u4e0e**\u6267\u884c\u6548\u7387**\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u591a\u8f6e\u5bf9\u8bdd\u3001\u5371\u673a\u7ba1\u7406\u53ca\u590d\u6742\u5de5\u5177\u8c03\u7528\u4e2d\uff0c\u901a\u8fc7\u9884\u6d4b\u63a8\u7406\u548c\u7b56\u7565\u6027\u6a21\u7cca\u6765\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\n\n---\n\n#### \u4e09\u3001 \u4e3b\u9898\u5206\u7c7b\u4e0e\u8bba\u6587\u901f\u89c8\n\n**\u4e3b\u9898\u4e00\uff1a\u667a\u80fd\u4f53\u67b6\u6784\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b0\u8303\u5f0f**\n*\u8be5\u677f\u5757\u805a\u7126\u4e8e\u5982\u4f55\u901a\u8fc7\u521b\u65b0\u7684RL\u7b97\u6cd5\u548c\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002*\n\n*   **[Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards]**\n    \u63d0\u51fa\u4e86 **Citation-aware Rubric Rewards (CaRR)** \u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5f15\u7528\u611f\u77e5\u5956\u52b1\u66ff\u4ee3\u4f20\u7edf\u7684\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u5f3a\u8c03\u63a8\u7406\u7684\u5168\u9762\u6027\u548c\u4e8b\u5b9e\u4f9d\u636e\u3002\u7ed3\u5408 **C-GRPO** \u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6377\u5f84\u5229\u7528\u548c\u5e7b\u89c9\u884c\u4e3a\uff0c\u5728\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n    (2601.06021 [cs.CL])\n\n*   **[GIFT: Games as Informal Training for Generalizable LLMs]**\n    \u5c06\u6e38\u620f\u73af\u5883\u4f5c\u4e3aLLM\u7684**\u975e\u6b63\u5f0f\u5b66\u4e60** \u573a\u6240\uff0c\u5229\u7528\u6e38\u620f\u5185\u5728\u7684\u5956\u52b1\u4fe1\u53f7\u57f9\u517b\u7b56\u7565\u521b\u9020\u529b\u7b49\u901a\u7528\u667a\u80fd\u3002\u5f15\u5165**\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6** \u89e3\u51b3\u591a\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u7684 \"AND\" \u76ee\u6807\u8feb\u4f7f\u6a21\u578b\u540c\u65f6\u638c\u63e1\u591a\u79cd\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5e7f\u6cdb\u80fd\u529b\u57fa\u51c6\u4e0a\u7684\u6cdb\u5316\u6027\u3002\n    (2601.05633 [cs.CL])\n\n*   **[MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards]**\n    \u5f15\u5165 **MemBuilder** \u6846\u67b6\uff0c\u5229\u7528**\u5c5e\u6027\u5bc6\u96c6\u5956\u52b1** \u8bad\u7ec3\u6a21\u578b\u6784\u5efa\u591a\u7ef4\u5ea6\u7684\u957f\u671f\u8bb0\u5fc6\u3002\u901a\u8fc7\u5408\u6210\u4f1a\u8bdd\u7ea7\u95ee\u9898\u63d0\u4f9b\u5bc6\u96c6\u4e2d\u95f4\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u8d21\u732e\u611f\u77e5\u7684\u68af\u5ea6\u52a0\u6743\uff0c\u4f7f4B\u53c2\u6570\u6a21\u578b\u5728\u957f\u671f\u5bf9\u8bdd\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86SOTA\u95ed\u6e90\u6a21\u578b\u3002\n    (2601.05488 [cs.CL])\n\n*   **[MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization]**\n    \u63d0\u51fa\u4e86 **MaxCode**\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6700\u5927\u5956\u52b1RL\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5bfcLLM\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53d1\u73b0\u9ad8\u6027\u80fd\u4ee3\u7801\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u7136\u8bed\u8a00\u6279\u5224\u6a21\u578b\u548c\u751f\u6210\u6027\u5956\u52b1\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u89c2\u5bdf\u7a7a\u95f4\u548c\u63a2\u7d22\u6548\u7387\uff0c\u5728CUDA\u548cC++\u4f18\u5316\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\n    (2601.05475 [cs.CL])\n\n*   **[From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation]**\n    \u9488\u5bf9GUI\u667a\u80fd\u4f53\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86 **BEPA** \u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u7b56\u7565\u5c06\u9759\u6001\u4e13\u5bb6\u8f68\u8ff9\u8f6c\u5316\u4e3a\u4e0e\u7b56\u7565\u5bf9\u9f50\u7684\u6307\u5bfc\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e13\u5bb6\u8f68\u8ff9\u4e0e\u5b66\u4e60\u8005\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefGUI\u667a\u80fd\u4f53\u5728OSWorld-Verified\u7b49\u57fa\u51c6\u4e0a\u7684\u6210\u529f\u7387\u3002\n    (2601.05787 [cs.AI])\n\n*   **[PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering]**\n    \u63d0\u51fa\u4e86 **PRISMA** \u6846\u67b6\uff0c\u91c7\u7528 **Plan-Retrieve-Inspect-Solve-Memoize** \u67b6\u6784\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5d29\u6e83\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002\u901a\u8fc7**\u4e24\u7ea7GRPO** \u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u5f15\u5bfc\u7684\u534f\u4f5c\uff0c\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002\n    (2601.05465 [cs.AI])\n\n*   **[KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits]**\n    \u63d0\u51fa\u4e86 **KP-Agent**\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528**\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a** \u6846\u67b6\u89e3\u51b3\u8d5e\u52a9\u641c\u7d22\u5e7f\u544a\u4e2d\u7684\u5173\u952e\u8bcd\u4fee\u526a\u95ee\u9898\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u4ee3\u7801\u7247\u6bb5\u6765\u4f18\u5316\u5173\u952e\u8bcd\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u5c06\u7d2f\u8ba1\u5229\u6da6\u63d0\u5347\u9ad8\u8fbe49.28%\u3002\n    (2601.05257 [cs.AI])\n\n**\u4e3b\u9898\u4e8c\uff1a\u8bb0\u5fc6\u673a\u5236\u4e0e\u957f\u671f\u63a8\u7406**\n*\u8be5\u677f\u5757\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u3001\u5185\u90e8\u72b6\u6001\u84b8\u998f\u548c\u60c5\u611f\u5efa\u6a21\uff0c\u8d4b\u4e88\u667a\u80fd\u4f53\u6301\u4e45\u4e14\u8fde\u8d2f\u7684\u8ba4\u77e5\u80fd\u529b\u3002*\n\n*   **[Distilling Feedback into Memory-as-a-Tool]**\n    \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u77ac\u65f6\u7684\u6279\u8bc4\u53cd\u9988\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u6307\u5357\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6587\u4ef6\u7684\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5de5\u5177\u8c03\u7528\u644a\u9500\u63a8\u7406\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u5728 **Rubric Feedback Bench** \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684LLM\u80fd\u8fc5\u901f\u5339\u914d\u6d4b\u8bd5\u65f6\u4f18\u5316\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002\n    (2601.05960 [cs.CL])\n\n*   **[Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation]**\n    \u5f15\u5165\u4e86 **KEEM** \u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u5f0f\u7684\u8bb0\u5fc6\u66f4\u65b0\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u4fe1\u606f\u51b2\u7a81\u548c\u72b6\u6001\u8ddf\u8e2a\u96be\u9898\u3002\u8be5\u6570\u636e\u96c6\u4e0d\u4ec5\u4fdd\u7559\u4e8b\u5b9e\u4fe1\u606f\uff0c\u8fd8\u878d\u5408\u4e86**\u60c5\u611f\u8bed\u5883** \u548c\u56e0\u679c\u5173\u7cfb\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u66f4\u5177\u540c\u7406\u5fc3\u5730\u8fdb\u884c\u5f00\u653e\u57df\u5bf9\u8bdd\u3002\n    (2601.05548 [cs.CL])\n\n*   **[FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse]**\n    \u63d0\u51fa\u4e86 **FlashMem**\uff0c\u901a\u8fc7**\u8ba1\u7b97\u590d\u7528** \u76f4\u63a5\u4ece\u77ac\u6001\u63a8\u7406\u72b6\u6001\u4e2d\u63d0\u70bc\u5185\u5728\u8bb0\u5fc6\uff0c\u6d88\u9664\u4e86\u5bf9\u8f85\u52a9\u7f16\u7801\u5668\u7684\u4f9d\u8d56\u3002\u5229\u7528 **Shared-KV Consolidator** \u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u71b5\u7684 **Cognitive Monitor**\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e865\u500d\u3002\n    (2601.05505 [cs.CL])\n\n*   **[StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management]**\n    \u63d0\u51fa\u4e86 **StackPlanner**\uff0c\u4e00\u4e2a\u5177\u6709\u663e\u5f0f\u8bb0\u5fc6\u63a7\u5236\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u534f\u8c03\u4e0e\u5b50\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u5b66\u4e60\u53ef\u91cd\u7528\u7684\u534f\u8c03\u7ecf\u9a8c\uff0c\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u81a8\u80c0\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\n    (2601.05890 [cs.AI])\n\n**\u4e3b\u9898\u4e09\uff1a\u793e\u4f1a\u4ea4\u4e92\u4e0e\u5782\u76f4\u9886\u57df\u5e94\u7528**\n*\u8be5\u677f\u5757\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5728\u6a21\u62df\u4eba\u7c7b\u793e\u4f1a\u884c\u4e3a\uff08\u5982\u5408\u4f5c\u3001\u8fa9\u8bba\u3001\u5371\u673a\u516c\u5173\uff09\u53ca\u5904\u7406\u7279\u5b9a\u9886\u57df\uff08\u5982\u5386\u53f2\u3001\u79d1\u5b66\u5b9e\u9a8c\u3001\u591a\u8bed\u8a00\uff09\u4efb\u52a1\u65f6\u7684\u6700\u65b0\u8fdb\u5c55\u3002*\n\n*   **[Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat]**\n    \u63d0\u51fa\u4e86 **Stephanie2**\uff0c\u4e00\u79cd\u5177\u5907**\u4e3b\u52a8\u7b49\u5f85** \u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u80fd\u529b\u7684\u9010\u6b65\u51b3\u7b56\u5bf9\u8bdd\u667a\u80fd\u4f53\u3002\u5b83\u901a\u8fc7\u663e\u5f0f\u51b3\u5b9a\u53d1\u9001\u6216\u7b49\u5f85\uff0c\u5e76\u5c06\u5ef6\u8fdf\u5efa\u6a21\u4e3a\u601d\u8003\u65f6\u95f4\u548c\u6253\u5b57\u65f6\u95f4\u7684\u603b\u548c\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u8282\u594f\uff0c\u5728\u56fe\u7075\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n    (2601.05657 [cs.CL])\n\n*   **[CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems]**\n    \u63d0\u51fa\u4e86 **CHisAgent**\uff0c\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u6587\u5316\u4e8b\u4ef6\u5206\u7c7b\u5b66\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u5f52\u7eb3\u3001\u81ea\u4e0a\u800c\u4e0b\u7684\u6269\u5c55\u548c\u8bc1\u636e\u5f15\u5bfc\u7684\u4e30\u5bcc\u5316\u4e09\u4e2a\u9636\u6bb5\uff0c\u8be5\u7cfb\u7edf\u6210\u529f\u6784\u5efa\u4e86\u8986\u76d6\u653f\u6cbb\u3001\u519b\u4e8b\u7b49\u9886\u57df\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u652f\u6301\u8de8\u6587\u5316\u5bf9\u9f50\u3002\n    (2601.05520 [cs.CL])\n\n*   **[Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring]**\n    \u4ecb\u7ecd\u4e86 **NAIAD**\uff0c\u4e00\u4e2a\u5229\u7528LLM\u548c\u5916\u90e8\u5206\u6790\u5de5\u5177\u8fdb\u884c\u5185\u9646\u6c34\u76d1\u6d4b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u901a\u8fc7RAG\u3001\u5de5\u5177\u7f16\u6392\u548c\u8ba1\u7b97\u56fe\u6267\u884c\uff0c\u8be5\u7cfb\u7edf\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u5728\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u67e5\u8be2\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6b63\u786e\u6027\u548c\u76f8\u5173\u6027\u3002\n    (2601.05256 [cs.CL])\n\n*   **[Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models]**\n    \u5f15\u5165\u4e86 **Crisis-Bench**\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53POMDP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4f01\u4e1a\u5371\u673a\u4e2d\u7684**\u6218\u7565\u6a21\u7cca\u6027** \u548c\u58f0\u8a89\u7ba1\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u90e8\u5206\u6a21\u578b\u80fd\u591f\u4e3a\u4e86\u7a33\u5b9a\u6a21\u62df\u80a1\u4ef7\u800c\u8868\u73b0\u51fa\u9a6c\u57fa\u96c5\u7ef4\u5229\u5f0f\u7684\u5408\u6cd5\u4fe1\u606f\u4fdd\u7559\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\"\u7ae5\u5b50\u519b\"\u5f0f\u9053\u5fb7\u7edd\u5bf9\u4e3b\u4e49\u3002\n    (2601.05570 [cs.AI])\n\n*   **[Effects of personality steering on cooperative behavior in Large Language Model agents]**\n    \u7814\u7a76\u4e86**\u4eba\u683c\u5f15\u5bfc** \u5bf9LLM\u667a\u80fd\u4f53\u5728\u91cd\u590d\u56da\u5f92\u56f0\u5883\u4e2d\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5b9c\u4eba\u6027\u662f\u4fc3\u8fdb\u5408\u4f5c\u7684\u4e3b\u5bfc\u56e0\u7d20\uff0c\u800c\u660e\u786e\u7684\u4eba\u683c\u4fe1\u606f\u867d\u7136\u80fd\u589e\u52a0\u5408\u4f5c\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u88ab\u5265\u524a\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u65e9\u671f\u6a21\u578b\u4e2d\u3002\n    (2601.05302 [cs.AI])\n\n*   **[Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting]**\n    \u7cfb\u7edf\u7814\u7a76\u4e86\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u5982\u4f55\u5851\u9020LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684**\u4ece\u4f17\u52a8\u6001**\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e2d\u5fc3\u5316\u7ed3\u6784\u51b3\u7b56\u5feb\u4f46\u6613\u53d7\u67a2\u7ebd\u80fd\u529b\u5f71\u54cd\uff0c\u800c\u5206\u5e03\u5f0f\u7ed3\u6784\u5171\u8bc6\u66f4\u7a33\u5065\uff0c\u4f46\u9ad8\u8fde\u901a\u6027\u53ef\u80fd\u5bfc\u81f4\"\u9519\u8bef\u4f46\u786e\u4fe1\"\u7684\u7ea7\u8054\u6548\u5e94\u3002\n    (2601.05606 [cs.MA])\n\n*   **[PRISM: Protocol Refinement through Intelligent Simulation Modeling]**\n    \u63d0\u51fa\u4e86 **PRISM** \u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5b9e\u9a8c\u534f\u8bae\u7684\u8bbe\u8ba1\u3001\u9a8c\u8bc1\u548c\u6267\u884c\u3002\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u6b65\u9aa4\uff0c\u5e76\u5728NVIDIA Omniverse\u6570\u5b57\u5b6a\u751f\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4ece\u8bed\u8a00\u751f\u6210\u5230\u673a\u5668\u4eba\u6267\u884c\u7684\u65e0\u7f1d\u8854\u63a5\u3002\n    (2601.05356 [cs.AI])\n\n*   **[EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting]**\n    \u63d0\u51fa\u4e86 **EvidFuse**\uff0c\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u62a5\u544a\u4e2d\u7684**\u5199\u4f5c\u65f6\u6587\u672c-\u56fe\u8868\u4ea4\u9519\u751f\u6210**\u3002\u901a\u8fc7\u89e3\u8026\u53ef\u89c6\u5316\u5206\u6790\u4e0e\u957f\u6587\u8d77\u8349\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u5728\u53d9\u8ff0\u9700\u8981\u65f6\u5373\u65f6\u6784\u5efa\u89c6\u89c9\u8bc1\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d41\u6c34\u7ebf\u4e2d\u7684\u56fe\u8868\u4e0d\u4e00\u81f4\u548c\u6d1e\u5bdf\u51bb\u7ed3\u95ee\u9898\u3002\n    (2601.05487 [cs.MA])\n\n*   **[Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models]**\n    \u5f15\u5165\u4e86 **MLCL** \u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5de5\u5177\u8c03\u7528\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c**\u53c2\u6570\u503c\u8bed\u8a00\u4e0d\u5339\u914d** \u662f\u4e3b\u8981\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e86\u8bed\u4e49\u6b63\u786e\u4f46\u8bed\u8a00\u4e0d\u7b26\u5408\u6267\u884c\u7ea6\u5b9a\u7684\u53c2\u6570\u503c\uff0c\u73b0\u6709\u7684\u63a8\u7406\u65f6\u7b56\u7565\u5c1a\u65e0\u6cd5\u5b8c\u5168\u6062\u590d\u82f1\u8bed\u6c34\u5e73\u7684\u6027\u80fd\u3002\n    (2601.05366 [cs.CL])\n\n**\u4e3b\u9898\u56db\uff1a\u641c\u7d22\u89c4\u5212\u4e0e\u6548\u7387\u4f18\u5316**\n*\u8be5\u677f\u5757\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u9884\u6d4b\u63a8\u7406\u3001\u73af\u5883\u5408\u6210\u548c\u8fa9\u8bba\u673a\u5236\uff0c\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u641c\u7d22\u548c\u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u74f6\u9888\u4e0e\u540c\u8d28\u5316\u95ee\u9898\u3002*\n\n*   **[Can We Predict Before Executing Machine Learning Agents?]**\n    \u63d0\u51fa\u4e86 **FOREAGENT**\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u6267\u884c\u5148\u9a8c\u77e5\u8bc6\uff0c\u7528\u77ac\u65f6\u9884\u6d4b\u63a8\u7406\u66ff\u4ee3\u6602\u8d35\u7684\u7269\u7406\u8fd0\u884c\uff0c\u4ece\u800c\u89e3\u51b3**\u6267\u884c\u74f6\u9888**\u3002\u8be5\u6846\u67b6\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\u504f\u597d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e866\u500d\u7684\u6536\u655b\u52a0\u901f\uff0c\u5e76\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6267\u884c\u7684\u57fa\u7ebf\u3002\n    (2601.05930 [cs.CL])\n\n*   **[EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis]**\n    \u63d0\u51fa\u4e86 **EnvScaler**\uff0c\u4e00\u4e2a\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u6269\u5c55\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u7684\u6846\u67b6\u3002\u5b83\u5305\u542b\u6784\u5efa\u73af\u5883\u9aa8\u67b6\u7684 **SkelBuilder** \u548c\u751f\u6210\u573a\u666f\u7684 **ScenGenerator**\uff0c\u5408\u6210\u4e86191\u4e2a\u73af\u5883\u548c\u7ea67K\u4e2a\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u591a\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002\n    (2601.05808 [cs.CL])\n\n*   **[DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation]**\n    \u5f15\u5165\u4e86 **DynaDebate**\uff0c\u901a\u8fc7**\u52a8\u6001\u8def\u5f84\u751f\u6210\u4e0e\u5206\u914d** \u548c\u4ee5\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u8fa9\u8bba\u673a\u5236\uff0c\u6253\u7834\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u540c\u8d28\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u667a\u80fd\u4f53\u91c7\u7528\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u907f\u514d\u4e86\u7b80\u5355\u7684\u591a\u6570\u6295\u7968\u9000\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002\n    (2601.05746 [cs.AI])\n\n*   **[Over-Searching in Search-Augmented Large Language Models]**\n    \u7cfb\u7edf\u8bc4\u4f30\u4e86\u641c\u7d22\u589e\u5f3aLLM\u4e2d\u7684**\u8fc7\u5ea6\u641c\u7d22** \u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u4e0d\u5fc5\u8981\u7684\u60c5\u51b5\u4e0b\u8c03\u7528\u641c\u7d22\u5de5\u5177\u3002\u7814\u7a76\u5f15\u5165\u4e86 **Tokens Per Correctness (TPC)** \u6307\u6807\u6765\u8861\u91cf\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6743\u8861\uff0c\u5e76\u53d1\u73b0\u8fc7\u5ea6\u641c\u7d22\u5728\u590d\u6742\u63a8\u7406\u6a21\u578b\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002\n    (2601.05503 [cs.AI])\n\n---\n\n#### \u56db\u3001 \u4eca\u65e5\u770b\u70b9\n\n*   **RL\u6b63\u5728\u63a5\u7ba1\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\"\u6700\u540e\u4e00\u516c\u91cc\"**\uff1a\u4eca\u65e5\u591a\u7bc7\u8bba\u6587\uff08\u5982CaRR, MemBuilder, MaxCode, PRISMA\uff09\u4e0d\u7ea6\u800c\u540c\u5730\u91c7\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u667a\u80fd\u4f53\u7684\u7279\u5b9a\u884c\u4e3a\u3002\u8fd9\u8868\u660e\uff0c\u5355\u7eaf\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5df2\u4e0d\u8db3\u4ee5\u652f\u6491\u590d\u6742\u7684Agent\u4efb\u52a1\uff0cRL\u6b63\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6df1\u5ea6\u3001\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u548c\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u7684\u6807\u51c6\u914d\u7f6e\u3002\n*   **\"\u9884\u6d4b\u4f18\u4e8e\u6267\u884c\"\u6210\u4e3a\u6548\u7387\u4f18\u5316\u7684\u65b0\u5171\u8bc6**\uff1a\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u63a2\u7d22\"\u5148\u9884\u6d4b\u540e\u9a8c\u8bc1\"\uff08FOREAGENT\uff09\u6216\u8bc6\u522b\"\u8fc7\u5ea6\u641c\u7d22\"\uff08Over-Searching\uff09\u7684\u673a\u5236\u3002\u8fd9\u79cd\u8d8b\u52bf\u6807\u5fd7\u7740Agent\u7814\u7a76\u4ece\u5355\u7eaf\u7684\"\u80fd\u529b\u63d0\u5347\"\u8f6c\u5411\u4e86\"\u80fd\u529b\u4e0e\u6210\u672c\u7684\u5e73\u8861\"\uff0c\u8bd5\u56fe\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002\n*   **\u667a\u80fd\u4f53\u5f00\u59cb\u5177\u5907\"\u793e\u4f1a\u6027\"\u4e0e\"\u57ce\u5e9c\"**\uff1aCrisis-Bench\u7684\u7814\u7a76\u6781\u5177\u542f\u53d1\u6027\uff0c\u5b83\u63ed\u793a\u4e86LLM\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u9700\u8981\u5177\u5907\"\u6218\u7565\u6a21\u7cca\u6027\"\uff08Strategic Ambiguity\uff09\uff0c\u5373\u4e3a\u4e86\u8fbe\u6210\u76ee\u6807\uff08\u5982\u80a1\u4ef7\u7a33\u5b9a\uff09\u800c\u5b66\u4f1a\u6492\u8c0e\u6216\u9690\u7792\u4fe1\u606f\u3002\u7ed3\u5408\u5173\u4e8e\u4eba\u683c\u5f15\u5bfc\u548c\u4ece\u4f17\u6548\u5e94\u7684\u7814\u7a76\uff0c\u8bf4\u660eAgent\u6b63\u4ece\u51b7\u51b0\u51b0\u7684\u8ba1\u7b97\u5668\u5411\u5177\u6709\u590d\u6742\u793e\u4f1a\u5c5e\u6027\u548c\u884c\u4e3a\u7b56\u7565\u7684\"\u6570\u5b57\u4eba\"\u6f14\u53d8\u3002\n*   **\u8bb0\u5fc6\u673a\u5236\u7684\u5185\u5377\u5316\u4e0e\u60c5\u611f\u5316**\uff1aFlashMem\u901a\u8fc7\u8ba1\u7b97\u590d\u7528\u5c06\u8bb0\u5fc6\u5185\u5316\u5230\u6a21\u578b\u5185\u90e8\u72b6\u6001\uff0c\u800cKEEM\u5219\u5f3a\u8c03\u8bb0\u5fc6\u4e2d\u7684\u60c5\u611f\u7ef4\u5ea6\u3002\u8fd9\u8868\u660e\u672a\u6765\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5c06\u4e0d\u518d\u4ec5\u4ec5\u662f\u5916\u6302\u7684\u5411\u91cf\u6570\u636e\u5e93\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u6df1\u5ea6\u8026\u5408\u3001\u4e14\u80fd\u7406\u89e3\u4e0a\u4e0b\u6587\u60c5\u611f\u8272\u5f69\u7684\u52a8\u6001\u8ba4\u77e5\u7ec4\u4ef6\u3002",
    "2026-01-09": "### \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-09)\n\n#### \u4e8c\u3001 \u5f00\u7bc7\u5bfc\u8bed\n\u4eca\u65e5\u7684\u7814\u7a76\u805a\u7126\u4e8e\u901a\u8fc7\u667a\u80fd\u5316\u7684\u8d44\u6e90\u8c03\u5ea6\u6765\u7a81\u7834\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u74f6\u9888\u3002\u6838\u5fc3\u8d8b\u52bf\u663e\u793a\uff0c\u7814\u7a76\u91cd\u5fc3\u6b63\u4ece\u5355\u7eaf\u8ffd\u6c42\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u8f6c\u5411\u5982\u4f55\u66f4\u7cbe\u7ec6\u5730\u201c\u7f16\u6392\u201d\u73b0\u6709\u667a\u80fd\u4f53\uff0c\u4ee5\u5b9e\u73b0\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6700\u4f73\u5e73\u8861\u3002\u6211\u4eec\u770b\u5230\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u8f6c\u53d8\uff1a\u5373\u5229\u7528\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u81ea\u9002\u5e94\u5730\u5206\u914d\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002\u8fd9\u6807\u5fd7\u7740AI\u7cfb\u7edf\u67b6\u6784\u6b63\u671d\u7740\u66f4\u52a0\u5f02\u6784\u3001\u9ad8\u6548\u548c\u8d44\u6e90\u611f\u77e5\u7684\u65b9\u5411\u6f14\u8fdb\u3002\n\n#### \u4e09\u3001 \u4e3b\u9898\u5206\u7c7b\u4e0e\u8bba\u6587\u901f\u89c8\n\n##### \u6548\u7387\u4e0e\u7f16\u6392\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u667a\u80fd\u8def\u7531\n\n*   **[Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models]**\n    \u8be5\u7814\u7a76\u63d0\u51fa\u4e86 **OI-MAS** \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u4e25\u91cd\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165 **\u72b6\u6001\u4f9d\u8d56\u8def\u7531** \u548c **\u7f6e\u4fe1\u5ea6\u611f\u77e5\u673a\u5236**\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e00\u4e2a\u5f02\u6784\u7684\u591a\u5c3a\u5ea6LLM\u6c60\u4e2d\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u667a\u80fd\u4f53\u89d2\u8272\u548c\u6a21\u578b\u89c4\u6a21\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u5c06\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12.88%\u7684\u540c\u65f6\uff0c\u6210\u529f\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u8fd180%\u3002(ArXiv ID: 2601.04861 [cs.AI])\n\n#### \u56db\u3001 \u4eca\u65e5\u770b\u70b9\n\n*   **\u6253\u7834\u201c\u5927\u6a21\u578b\u5168\u5305\u201d\u7684\u8ff7\u601d**\uff1a\u7814\u7a76\u6709\u529b\u5730\u8bc1\u660e\u4e86\u5e76\u975e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u89d2\u8272\u90fd\u9700\u8981\u9876\u914d\u7684\u5927\u6a21\u578b\u3002\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u8def\u7531\u7b56\u7565\uff0c\u8ba9\u5c0f\u6a21\u578b\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff0c\u5927\u6a21\u578b\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u662f\u5b9e\u73b0\u9ad8\u6548AI\u534f\u4f5c\u7684\u5173\u952e\u3002\n*   **\u6781\u81f4\u7684\u6027\u4ef7\u6bd4\u63d0\u5347**\uff1a\u5728\u51c6\u786e\u7387\u63d0\u5347\u7684\u540c\u65f6\u5b9e\u73b0\u8fd180%\u7684\u6210\u672c\u524a\u51cf\uff0c\u8fd9\u4e00\u6210\u679c\u5bf9\u4e8e\u4f01\u4e1a\u7ea7AI\u5e94\u7528\u843d\u5730\u5177\u6709\u91cd\u5927\u610f\u4e49\uff0c\u5b83\u5c55\u793a\u4e86\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6761\u4ef6\u4e0b\u90e8\u7f72\u9ad8\u6027\u80fd\u7684\u590d\u6742\u63a8\u7406\u7cfb\u7edf\u3002\n*   **\u4ece\u9759\u6001\u5206\u5de5\u5230\u52a8\u6001\u7f16\u6392**\uff1a**OI-MAS** \u4ee3\u8868\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u7684\u65b0\u8d8b\u52bf\u2014\u2014\u5373\u4ece\u56fa\u5b9a\u7684\u667a\u80fd\u4f53\u5206\u5de5\uff0c\u8f6c\u5411\u57fa\u4e8e\u5b9e\u65f6\u72b6\u6001\u548c\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7f16\u6392\uff0c\u8fd9\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u667a\u80fd\u7684\u201cAI\u7ec4\u7ec7\u201d\u63d0\u4f9b\u4e86\u6280\u672f\u84dd\u56fe\u3002",
    "2026-01-08": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-08)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-07": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-07)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-06": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-06)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-05": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-05)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2026-01-02": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2026-01-02)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-31": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-31)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Error code: 522",
    "2025-12-29": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-29)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-26": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-26)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
    "2025-12-24": "# \u4eca\u65e5AI\u8bba\u6587\u901f\u89c8 (2025-12-24)\n\n\u751f\u6210\u6bcf\u65e5\u901f\u89c8\u65f6\u53d1\u751f\u9519\u8bef: Connection error.",
};
const dailyOverviews = {};
for (const date in dailyOverviewsRaw) {
    dailyOverviews[date] = dailyOverviewsRaw[date];
}


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文
        let isLoadingMore = false; // 是否正在加载更多

        // 获取未加载的日期
        function getUnloadedDates() {
            return availableDates.filter(date => !loadedDates.has(date));
        }

        // 加载更多日期的数据
        async function loadMoreDates() {
            if (isLoadingMore) return;

            const unloadedDates = getUnloadedDates();
            if (unloadedDates.length === 0) {
                showSimpleToast('已加载全部数据');
                return;
            }

            isLoadingMore = true;
            const loadBtn = document.getElementById('load-more-btn');
            if (loadBtn) {
                loadBtn.disabled = true;
                loadBtn.innerHTML = '<span class="animate-spin inline-block mr-2">⏳</span>加载中...';
            }

            const datesToLoad = unloadedDates.slice(0, LOAD_MORE_DAYS);
            let loadedCount = 0;

            for (const date of datesToLoad) {
                try {
                    const response = await fetch(`data/${date}.json`);
                    if (!response.ok) continue;

                    const dateData = await response.json();

                    // 将数据添加到 allPapers
                    allPapers[date] = dateData.categories;

                    // 添加每日速览
                    if (dateData.overview) {
                        dailyOverviews[date] = dateData.overview;
                    }

                    loadedDates.add(date);
                    loadedCount++;
                } catch (e) {
                    console.error(`加载 ${date} 数据失败:`, e);
                }
            }

            isLoadingMore = false;

            if (loadedCount > 0) {
                renderPapers();
                showSimpleToast(`已加载 ${loadedCount} 天的数据`);
            }

            updateLoadMoreButton();
        }

        // 更新"加载更多"按钮状态
        function updateLoadMoreButton() {
            const loadBtn = document.getElementById('load-more-btn');
            const unloadedCount = getUnloadedDates().length;

            if (loadBtn) {
                if (unloadedCount === 0) {
                    loadBtn.style.display = 'none';
                } else {
                    loadBtn.style.display = 'inline-flex';
                    loadBtn.disabled = false;
                    loadBtn.innerHTML = `📥 加载更多 (还有 ${unloadedCount} 天)`;
                }
            }
        }

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            const listItem = paperEl.closest('li');
            const categoryContent = paperEl.closest('.category-content');
            const sectionEl = paperEl.closest('section[data-date-section]');
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                if (listItem) {
                    listItem.remove();
                } else {
                    paperEl.remove();
                }

                updateCategoryView(categoryContent);
                updateDateSection(sectionEl);
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        function updateCategoryView(categoryContent) {
            if (!categoryContent) return;
            const listEl = categoryContent.querySelector('ul');
            if (!listEl) return;

            const paperItems = listEl.querySelectorAll('.paper-item').length;
            let placeholder = listEl.querySelector('.empty-category-placeholder');

            if (paperItems === 0) {
                if (!placeholder) {
                    placeholder = document.createElement('li');
                    placeholder.className = 'empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400';
                    placeholder.textContent = '此分类下暂无论文。';
                    listEl.appendChild(placeholder);
                }
            } else if (placeholder) {
                placeholder.remove();
            }

            const toggle = document.querySelector(`.category-toggle[data-target="${categoryContent.id}"]`);
            if (toggle) {
                const countBadge = toggle.querySelector('.category-count');
                if (countBadge) {
                    countBadge.textContent = paperItems;
                }
            }
        }

        function updateDateSection(sectionEl) {
            if (!sectionEl) return;
            const totalPapers = sectionEl.querySelectorAll('.paper-item').length;
            const header = sectionEl.querySelector('[data-date-heading]');

            if (header) {
                const dateLabel = header.dataset.dateHeading || header.textContent.split(' ')[0];
                header.textContent = `${dateLabel} (${totalPapers} 篇论文)`;
            }

            if (totalPapers === 0) {
                sectionEl.remove();
            }
        }

        // 可折叠功能
        function toggleCollapsible(header) {
            const content = header.nextElementSibling;
            const isOpen = header.classList.contains('open');
            
            if (isOpen) {
                header.classList.remove('open');
                content.classList.remove('open');
            } else {
                header.classList.add('open');
                content.classList.add('open');
            }
        }

        // 渲染所有 Markdown 内容
        function renderAllMarkdown() {
            // 配置 marked 选项
            if (typeof marked !== 'undefined') {
                marked.setOptions({
                    breaks: true,
                    gfm: true,
                    headerIds: false,
                    mangle: false
                });
                
                // 遍历所有灵感溯源的容器并渲染 Markdown
                for (const date in allPapers) {
                    const categories = allPapers[date];
                    categories.forEach(category => {
                        if (category.papers) {
                            category.papers.forEach(paper => {
                                if (paper.inspiration_trace) {
                                    const elementId = `inspiration-${paper.arxiv_id}`;
                                    const element = document.getElementById(elementId);
                                    if (element) {
                                        try {
                                            element.innerHTML = marked.parse(paper.inspiration_trace);
                                        } catch (e) {
                                            console.error('Markdown 渲染失败:', e);
                                            // 如果渲染失败，使用纯文本显示
                                            element.textContent = paper.inspiration_trace;
                                        }
                                    }
                                }
                            });
                        }
                    });
                }
            }
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-3 sm:mb-4">
                        <div class="flex items-start space-x-2 sm:space-x-3 flex-1 min-w-0">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5 sm:h-6 sm:w-6" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-base sm:text-lg font-semibold text-black dark:text-white leading-tight break-words">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-2 sm:ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-3 sm:mb-4">
                        <div class="flex flex-wrap items-center gap-2 sm:gap-4 text-xs sm:text-sm text-slate-600 dark:text-slate-400">
                            <span class="break-all"><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300 whitespace-nowrap">
                                ${paper.category}
                            </span>
                            <span class="whitespace-nowrap">${date}</span>
                        </div>
                        <div class="text-xs sm:text-sm text-black dark:text-white break-words">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-3 sm:mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50 w-4 h-4">
                            <span class="ml-2 text-xs sm:text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">筛选原因</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.filter_reason}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">AI总结</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words">
                                        ${paper.summary2}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 (默认展开) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header open text-sm sm:text-base" onclick="toggleCollapsible(this)">原始摘要</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-3 sm:p-4 rounded-r-lg">
                                    ${paper.summary_translation ? `
                                    <div class="chinese-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: block;">
                                        ${paper.summary_translation}
                                    </div>
                                    ` : ''}
                                    ${paper.summary ? `
                                    <div class="english-summary text-xs sm:text-sm text-black dark:text-white leading-relaxed break-words" style="display: none;">
                                        ${paper.summary}
                                    </div>
                                    ` : ''}
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.inspiration_trace ? `
                    <!-- 灵感溯源 (默认折叠) -->
                    <div class="mb-3 sm:mb-4">
                        <div class="collapsible-header text-sm sm:text-base" onclick="toggleCollapsible(this)">灵感溯源</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-red-50/70 dark:bg-red-950/20 border-l-3 border-red-300 p-3 sm:p-4 rounded-r-lg">
                                    <div class="text-xs sm:text-sm text-black dark:text-white leading-relaxed markdown-content break-words" id="inspiration-${paper.arxiv_id}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors whitespace-nowrap">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors whitespace-nowrap">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-2.5 py-1.5 sm:px-3 sm:py-2 text-xs sm:text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors whitespace-nowrap">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="empty-category-placeholder pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-2 sm:p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-2 sm:space-x-3 min-w-0 flex-1">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform flex-shrink-0" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400 text-sm sm:text-base truncate">${category.name}</span>
                        </div>
                        <span class="category-count text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5 ml-2 flex-shrink-0">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-2 sm:ml-4">
                        <ul class="space-y-3 sm:space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-6 sm:mb-8" data-date-section="${date}">
                        <h2 class="text-base sm:text-lg font-medium text-slate-500 dark:text-slate-400 mb-3 sm:mb-4" data-date-heading="${date}">${date} (${dateVisibleTotal} 篇论文)</h2>
                `;
                
                // 添加该日期的AI论文速览（如果存在）
                if (dailyOverviews[date]) {
                    html += `
                        <div class="mb-3 sm:mb-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md p-3 sm:p-5">
                            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                                <svg class="w-4 h-4 sm:w-5 sm:h-5 mr-2 text-blue-600 dark:text-blue-400 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path>
                                </svg>
                                <span class="font-semibold text-slate-900 dark:text-white text-sm sm:text-base">今日AI论文速览</span>
                            </div>
                            <div class="collapsible-content">
                                <div class="inner">
                                    <div class="markdown-content text-slate-700 dark:text-slate-200 text-xs sm:text-sm" id="overview-${date}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }
                
                html += `
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-3 sm:p-4 lg:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }

            // 添加"加载更多"按钮
            const unloadedCount = getUnloadedDates().length;
            if (unloadedCount > 0) {
                html += `
                    <div class="text-center py-6">
                        <button id="load-more-btn" onclick="loadMoreDates()"
                            class="inline-flex items-center px-6 py-3 text-base font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg shadow-md transition-all duration-200 hover:shadow-lg">
                            📥 加载更多 (还有 ${unloadedCount} 天)
                        </button>
                    </div>
                `;
            }

            mainContent.innerHTML = html;
            updateStats();
            
            // 渲染所有日期的 Markdown 速览内容
            for (const date in dailyOverviews) {
                const overview = dailyOverviews[date];
                const elementId = `overview-${date}`;
                const element = document.getElementById(elementId);
                if (element && overview) {
                    try {
                        if (typeof marked !== 'undefined') {
                            element.innerHTML = marked.parse(overview);
                        } else {
                            element.textContent = overview;
                        }
                    } catch (e) {
                        console.error('Markdown 渲染失败:', e);
                        element.textContent = overview;
                    }
                }
            }
            
            // 渲染所有论文的 Markdown 内容
            renderAllMarkdown();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>