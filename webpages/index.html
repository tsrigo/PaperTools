<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MyArxiv - 学术论文集合</title>
    <!-- 引入 Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 引入 Marked.js 用于 Markdown 渲染 -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* 微软雅黑字体 */
        body {
            font-family: "Microsoft YaHei", "微软雅黑", sans-serif;
            -ms-overflow-style: none;  /* IE and Edge */
            scrollbar-width: none;  /* Firefox */
        }
        body::-webkit-scrollbar {
            display: none;
        }
        /* 星标样式 */
        .star-button {
            transition: color 0.2s ease-in-out;
        }
        .star-button.starred {
            color: #fbbf24;
        }
        .star-button:not(.starred) {
            color: #9ca3af;
        }
        .star-button:hover {
            color: #fbbf24;
        }
        /* 删除按钮样式 */
        .delete-button {
            transition: all 0.2s ease-in-out;
        }
        .delete-button:hover {
            color: #ef4444;
            transform: scale(1.1);
        }
        /* 论文项目样式 */
        .paper-item {
            transition: all 0.3s ease-in-out;
        }
        .paper-item.hidden-paper {
            opacity: 0.3;
            transform: scale(0.98);
        }
        /* 平滑过渡 */
        .rotate-90-transition {
            transition: transform 0.2s ease-in-out;
        }
        
        /* 可折叠部分样式 */
        .collapsible-header {
            cursor: pointer;
            display: flex;
            align-items: center;
            font-weight: 600;
            padding: 8px 0;
            user-select: none;
            color: #1e40af;
            transition: all 0.2s ease-in-out;
        }
        .dark .collapsible-header {
            color: #60a5fa;
        }
        .collapsible-header:hover {
            opacity: 0.8;
        }
        .collapsible-header::before {
            content: "▶";
            margin-right: 8px;
            transition: transform 0.3s ease;
            font-size: 0.8em;
        }
        .collapsible-header.open::before {
            transform: rotate(90deg);
        }
        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .collapsible-content.open {
            max-height: none;
        }
        .collapsible-content .inner {
            padding-top: 8px;
        }
        
        /* Markdown 内容样式 */
        .markdown-content {
            line-height: 1.6;
        }
        .markdown-content h1 {
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 1em;
            margin-bottom: 0.5em;
            color: #1e40af;
        }
        .dark .markdown-content h1 {
            color: #60a5fa;
        }
        .markdown-content h2 {
            font-size: 1.3em;
            font-weight: bold;
            margin-top: 0.8em;
            margin-bottom: 0.4em;
            color: #1e40af;
        }
        .dark .markdown-content h2 {
            color: #60a5fa;
        }
        .markdown-content h3 {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 0.6em;
            margin-bottom: 0.3em;
            color: #1e40af;
        }
        .dark .markdown-content h3 {
            color: #60a5fa;
        }
        .markdown-content h4 {
            font-size: 1em;
            font-weight: bold;
            margin-top: 0.5em;
            margin-bottom: 0.25em;
            color: #2563eb;
        }
        .dark .markdown-content h4 {
            color: #93c5fd;
        }
        .markdown-content p {
            margin-bottom: 0.8em;
        }
        .markdown-content ul, .markdown-content ol {
            margin-left: 1.5em;
            margin-bottom: 0.8em;
        }
        .markdown-content ul {
            list-style-type: disc;
        }
        .markdown-content ol {
            list-style-type: decimal;
        }
        .markdown-content li {
            margin-bottom: 0.3em;
        }
        .markdown-content code {
            background-color: #f1f5f9;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.9em;
        }
        .dark .markdown-content code {
            background-color: #334155;
        }
        .markdown-content pre {
            background-color: #f1f5f9;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 0.8em;
        }
        .dark .markdown-content pre {
            background-color: #334155;
        }
        .markdown-content pre code {
            background-color: transparent;
            padding: 0;
        }
        .markdown-content blockquote {
            border-left: 3px solid #cbd5e1;
            padding-left: 1em;
            margin-left: 0;
            margin-bottom: 0.8em;
            color: #64748b;
        }
        .dark .markdown-content blockquote {
            border-left-color: #475569;
            color: #94a3b8;
        }
        .markdown-content strong {
            font-weight: 600;
        }
        .markdown-content em {
            font-style: italic;
        }
    </style>
    <script>
        // Tailwind CSS 暗色模式配置
        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark')
        } else {
            document.documentElement.classList.remove('dark')
        }
    </script>
</head>
<body class="bg-slate-50 dark:bg-slate-900 font-sans text-slate-800 dark:text-slate-200">

    <!-- 撤销删除的Toast -->
    <div id="undo-toast" class="fixed top-4 right-4 bg-red-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 hidden">
        <div class="flex items-center space-x-2">
            <span id="toast-message">已删除</span>
            <span id="countdown" class="text-sm opacity-75"></span>
            <button id="undo-btn" class="ml-2 px-2 py-1 bg-white text-red-500 rounded text-sm hover:bg-gray-100">撤销</button>
        </div>
    </div>

    <div class="container mx-auto w-3/5 max-w-none p-4 sm:p-6">
        <!-- 头部导航栏 -->
        <header class="flex justify-between items-center mb-6">
            <h1 class="text-3xl font-bold text-slate-900 dark:text-white">MyArxiv</h1>
            <div class="flex items-center space-x-4">
                <!-- 统计信息 -->
                <div class="text-sm text-slate-600 dark:text-slate-400">
                    总计 <span id="total-papers">0</span> 篇论文
                </div>
                <!-- 筛选按钮 -->
                <div class="flex items-center space-x-2">
                    <button id="filter-starred" class="px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors">
                        只看收藏
                    </button>
                    <button id="filter-all" class="px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors">
                        显示全部
                    </button>
                </div>
                <!-- 中英文摘要切换按钮 -->
                <button id="summary-toggle" class="px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors">
                    中文摘要
                </button>
                <button id="theme-toggle" class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500">
                    <!-- 太阳图标 (浅色模式) -->
                    <svg id="theme-icon-light" class="h-6 w-6 text-slate-600 dark:text-slate-300" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
                    </svg>
                    <!-- 月亮图标 (深色模式) -->
                    <svg id="theme-icon-dark" class="h-6 w-6 text-slate-600 dark:text-slate-300 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
                    </svg>
                </button>
                <!-- GitHub 图标按钮 -->
                <a href="https://github.com/tsrigo/PaperTools" target="https://github.com/tsrigo/PaperTools" title="GitHub 项目主页"
                   class="p-2 rounded-full hover:bg-slate-200 dark:hover:bg-slate-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500">
                    <svg class="h-6 w-6 text-slate-700 dark:text-slate-200" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                        <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.021c0 4.428 2.865 8.184 6.839 9.504.5.092.682-.217.682-.483 0-.237-.009-.868-.014-1.703-2.782.605-3.369-1.342-3.369-1.342-.454-1.155-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.004.07 1.532 1.032 1.532 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.339-2.221-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.254-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.025A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.295 2.748-1.025 2.748-1.025.546 1.378.202 2.396.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.847-2.337 4.695-4.566 4.944.359.309.678.919.678 1.852 0 1.336-.012 2.417-.012 2.747 0 .268.18.579.688.481C19.138 20.2 22 16.447 22 12.021 22 6.484 17.523 2 12 2z" clip-rule="evenodd"/>
                    </svg>
                </a>
            </div>
        </header>

        <!-- 主要内容区域 -->
        <main class="space-y-8" id="main-content">
            <!-- 加载提示 -->
            <div id="loading" class="text-center py-8">
                <div class="inline-flex items-center px-4 py-2 font-semibold leading-6 text-sm shadow rounded-md text-slate-500 bg-white dark:bg-slate-800 transition ease-in-out duration-150">
                    <svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-slate-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                        <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                        <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    加载中...
                </div>
            </div>
        </main>
    </div>

    <script>
        const allPapers = {
    "2025-09-30": [
        {
            "name": "Artificial Intelligence",
            "count": 17,
            "papers": [
                {
                    "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
                    "arxiv_id": "2509.25004",
                    "authors": "Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo",
                    "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出CLPO（课程学习与策略优化相结合）的新算法，旨在增强大语言模型的通用推理能力。论文通过创建动态教学反馈循环，利用模型自身性能进行实时难度评估，构建在线课程，从而提高LLM在数学和通用推理任务上的表现。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。论文关注的是基础能力改进和新训练范式，而非将LLM作为工具应用于特定领域。同时，论文包含了多个正面指标，如大语言模型、推理能力、强化学习方法等，且不涉及任何排除标准中的领域。因此，该论文非常符合研究范围。",
                    "summary2": "本文旨在解决LLM推理中RLVR方法因统一训练样本处理导致的学习效率低下问题。针对不同难度问题，我们提出了一种CLPO（Curriculum-guided Learning for Policy Optimization）方法，通过在线课程学习和自适应问题重构机制实现引导式自我进化，并在八个数学和通用推理基准测试上通过pass@1指标验证了其有效性，平均提升6.96%。",
                    "summary_translation": "最近，在线可验证奖励强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）已成为提升大语言模型（Large Language Models, LLMs）推理能力的关键范式。然而，现有方法通常统一对待所有训练样本，忽视了问题难度相对于模型当前能力的巨大差异。这种统一的训练策略导致对模型已掌握问题的低效探索，同时缺乏对最具挑战性问题的有效指导，限制了学习效率和上限性能。为解决此问题，我们提出CLPO（课程引导的策略优化学习，Curriculum-guided Learning for Policy Optimization），这是一种在策略优化过程中创建动态教学反馈循环的新型算法。CLPO的核心利用模型自身的推理（rollout）性能进行实时难度评估，从而构建在线课程（Online Curriculum）。该课程进而引导自适应问题重构（Adaptive Problem Restructuring）机制，使模型成为自己的老师：它多样化中等难度问题以促进泛化，并简化具有挑战性的问题使其更易达成。我们的方法将静态训练过程转变为与模型能力共同进化的动态过程。实验表明，CLPO在八个具有挑战性的数学和通用推理基准测试中实现了最先进的性能，相比其他方法平均pass@1提升了6.96%，展示了其更高效训练更强推理能力模型的潜力。",
                    "inspiration_trace": "# CLPO方法逻辑链分析：从问题观察到方法创新\n\n## 一、宏观问题：LLM推理能力提升的瓶颈\n\n### 1.1 核心观察\n作者首先观察到大型语言模型(LLM)在复杂推理任务上仍面临显著挑战，尤其是数学问题解决、科学探究和代码生成等需要多步推理的领域。虽然传统监督微调(SFT)可以初步对齐模型能力，但其依赖静态数据集，成本高昂且难以覆盖所有推理路径。\n\n### 1.2 现有强化学习方法的局限\n作者进一步观察到，新兴的强化学习与可验证奖励(RLVR)方法虽然允许模型从自身生成中学习，但存在关键缺陷：\n- **统一训练策略**：现有方法(如GRPO)对所有训练样本一视同仁，忽略了问题难度相对于模型当前能力的巨大差异\n- **低效探索**：对模型已掌握问题的过度探索和对挑战性问题缺乏有效指导\n- **性能瓶颈**：导致策略多样性下降和策略熵崩溃，限制长期改进\n\n## 二、问题聚焦：探索效率与质量的平衡\n\n### 2.1 现有改进路径的分析\n作者梳理了两类现有改进方法及其局限：\n\n**路径一：优化算法改进**\n- 代表工作：DAPO、GFPO等\n- 思路：通过提高探索数量或维持更高熵来增强模型探索能力\n- 局限：探索质量比数量更关键，高熵并不总能保证高效探索\n\n**路径二：引入外部指导**\n- 代表工作：Critique-GRPO、LUFFY\n- 思路：利用外部批评模型或专家轨迹提供精确指导\n- 局限：依赖昂贵的外部资源，未解决内源性学习问题\n\n### 2.2 核心假设形成\n基于以上分析，作者提出核心假设：\n> \"能否在不依赖外部指导的情况下，实现高效、有针对性的内源性学习？\"\n\n这一假设引导作者思考如何将模型自身rollout信息从简单的奖励计算信号提升为更高级的学习指导机制。\n\n## 三、理论突破：从静态学习到动态课程\n\n### 3.1 关键洞察\n作者获得关键洞察：**问题难度评估是优化学习效率的核心**。不同难度的问题对模型学习的价值不同，需要区分对待：\n- 已掌握问题：继续探索收益低\n- 中等难度问题：处于学习边缘，适合泛化训练\n- 困难问题：当前无法解决，需要简化处理\n\n### 3.2 理论框架转变\n作者提出从GRPO的\"静态学习范式\"转向\"动态课程学习范式\"：\n- **GRPO范式**：模型被动接收静态分布D中的问题，仅优化响应空间\n- **CLPO范式**：模型主动构建与自身能力共同进化的动态课程，同时优化问题空间和响应空间\n\n## 四、方法构建：CLPO三大核心机制\n\n### 4.1 在线课程学习(Online Curriculum Learning)\n**问题**：如何实时评估问题难度？\n**解决方案**：\n- 利用模型自身rollout性能计算经验准确率\n- 基于预定义难度阈值(τ_hard, τ_med)动态划分问题难度\n- 构建实时难度评估机制，为后续处理提供精确信号\n\n### 4.2 自适应问题重构(Adaptive Problem Restructuring)\n**问题**：如何根据难度调整问题以促进学习？\n**解决方案**：\n- 模型作为自己的\"教师\"，通过特定提示重构问题\n- 对中等难度问题：应用多样化策略，生成语义等价但表述变化的问题\n- 对困难问题：应用简化策略，降低认知复杂度\n- 保持原问题答案不变，确保学习目标一致性\n\n### 4.3 难度感知策略优化(Difficulty-aware Policy Optimization)\n**问题**：如何根据问题难度调整优化策略？\n**解决方案**：\n- 引入动态KL正则化机制\n- 对困难问题：减小KL约束(λ_hard < λ_non-hard)，扩大探索范围\n- 对非困难问题：增大KL约束，确保稳定性\n- 实现探索与利用的动态平衡\n\n## 五、整合与验证：CLPO框架形成\n\n### 5.1 方法整合\n作者将三大机制整合为CLPO(Curriculum-guided Learning for Policy Optimization)框架，形成\"引导自我进化\"(Guided Self-Evolution)的闭环：\n1. 评估问题难度 → 2. 自适应重构问题 → 3. 难度感知优化 → 1. 重新评估难度...\n\n### 5.2 实验验证\n通过八个数学和通用推理基准测试验证CLPO有效性：\n- 平均pass@1性能提升6.96%\n- 在最具挑战性的AIME2024基准上实现SOTA\n- 消融研究确认各组件贡献\n- 测试时间缩放分析证明模型探索路径多样性提升\n\n## 六、逻辑演进总结\n\n作者的创新思路遵循了清晰的逻辑链条：\n1. **观察现象**：LLM推理能力受限，现有RLVR方法效率低下\n2. **分析局限**：统一训练策略忽略问题难度差异，探索质量不足\n3. **形成假设**：能否通过内源性机制实现高效、有针对性的学习\n4. **理论突破**：从静态学习转向动态课程学习范式\n5. **方法构建**：设计三大核心机制解决难度评估、问题重构和优化调整\n6. **整合验证**：形成CLPO框架并通过实验验证有效性\n\n这一思路体现了从宏观问题到具体解决方案的系统思考，将课程学习理念与强化学习优化深度融合，实现了LLM推理能力训练的范式创新。"
                },
                {
                    "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
                    "arxiv_id": "2509.25148",
                    "authors": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu",
                    "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data synergy.We evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(UniAPL)来改进LLM的基础能力。论文针对的是LLM后训练对齐问题，提出统一对抗偏好学习框架，将监督微调(SFT)和强化学习(RL)两种模态统一在一个框架中，解决分布不匹配问题。这明显属于改进LLM基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文包含多个相关主题： - 核心概念：明确关注LLMs，使用Qwen3系列模型进行实验 - 训练方法：涉及强化学习(RL)作为偏好学习的一种模态，并将其与SFT统一 - 能力方向：虽然不直接针对推理能力，但关注指令遵循能力(instruction-following)，这是一种重要的通用能力，对模型的整体表现和推理能力有基础性影响 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。虽然提到了\"beneficial and safe\"，但这只是作为背景，论文核心是提出统一训练框架。 最后，论文提出的UniAPL框架通过动态对齐策略分布与专家分布，解决了传统SFT后跟RL的顺序流程中的分布不匹配问题，这种方法论创新有望提升LLM的通用能力，包括其推理和问题解决能力。论文的实验结果也表明，UniAPL能够显著提升模型性能，甚至使小模型达到大模型的效果。 因此，尽管论文没有直接针对推理能力进行研究，但它提出的训练范式创新属于提升LLM基础能力的研究范畴，符合筛选标准。",
                    "summary2": "本文旨在解决大型语言模型后训练对齐过程中的分布不匹配问题。针对静态专家数据与动态策略之间的分布差异，我们提出了一种统一对抗偏好学习(UniAPL)框架，并在多个指令跟随基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "# 中文翻译\n\n塑造强大且有益安全的LLMs（大型语言模型）是AI对齐（AI alignment）的核心。我们认为后训练对齐（post-training alignment）本质上是一个统一的偏好学习（Preference Learning）问题，涉及两种模式：演示偏好（demonstrated preferences，例如监督微调Supervised Fine-Tuning, SFT）和比较偏好（comparative preferences，例如强化学习Reinforcement Learning, RL）。标准的顺序流程——先进行SFT再进行RL——由于关键的分布不匹配（distributional mismatch）而存在缺陷：SFT使用静态专家数据，但随着策略（policy）的演变，其生成分布会发生漂移，导致SFT知识变得脆弱。随后的RL在没有直接访问专家演示中丰富的真实知识（ground-truth knowledge）的情况下进行探索，导致效率低下且缺乏基础的更新。这种分离阻止了数据源之间的相互正则化（mutual regularization）。为解决此问题，我们将对齐重新定义为约束优化问题（constrained optimization problem），并提出统一对抗偏好学习（Unified Adversarial Preference Learning, UniAPL），这是一个新颖的框架，能够动态地将策略的分布与专家的分布对齐。UniAPL实现了单阶段统一训练目标（single-stage unified training objective），从SFT和偏好数据的混合批次中联合学习。在每个梯度步骤（gradient step）中，密集的专家演示直接为在线探索提供基础并进行正则化，本质上解决了分布不匹配问题并最大化数据协同效应（data synergy）。我们使用Qwen3-235B-Instruct-2507作为教师模型（teacher model），在指令跟随任务（instruction-following tasks）上评估UniAPL。我们的模型匹配或超过了强大的GRPO基线（GRPO baselines）：在Qwen3-0.6B上提升5.77%（匹配32B模型），在Qwen3-4B上提升3.75%，甚至超越了教师模型。对响应长度和对数概率分布（log-probability distributions）的分析证实，UniAPL的输出紧密模仿专家演示，同时实现了更强的性能和更好的行为对齐（behavioral alignment）。",
                    "inspiration_trace": "# UniAPL方法逻辑链推演\n\n## 一、宏观问题：AI对齐的核心挑战\n\n作者从AI安全与对齐的根本问题出发：如何确保强大的大型语言模型(LLMs)行为既有益又安全。他们观察到，后训练对齐过程本质上是一个统一的**偏好学习(Preference Learning)**挑战，包含两种不同模式：\n- 从示范偏好学习（如监督微调SFT）\n- 从比较偏好学习（如强化学习RL）\n\n## 二、关键观察：现有方法的根本缺陷\n\n作者深入分析了当前行业标准的\"SFT-then-RL\"流水线，发现其存在**分布不匹配(distributional mismatch)**的根本缺陷，导致两个相互关联的问题：\n\n### 1. 离线学习问题（模仿的脆弱性）\n- SFT在固定的专家数据分布(P_expert)上进行离线学习\n- 随着策略自身生成分布(P_policy)的漂移，学到的知识变得脆弱且难以泛化\n\n### 2. 在线学习问题（无基础的探索）\n- 后续RL阶段探索超出静态专家数据以提高泛化能力\n- 但缺乏对专家演示中丰富真实知识的实时访问，导致探索效率低下且易发生灾难性遗忘\n\n这种根本性分离使两种数据源无法协同正则化，导致对齐过程低效且不稳定。\n\n## 三、理论假设：重新概念化对齐问题\n\n作者提出核心假设：对齐不应被视为简单的信息衰减问题，而应重新概念化为**约束优化问题**。最优对齐策略必须同时满足来自两个来源的约束：\n\nπ_aligned = arg max_πθ E[R_ψ(y|x)] subject to D(πθ || π*) ≤ ε\n\n其中，R_ψ是来自偏好信息的奖励函数，约束条件强制策略保持对专家策略的忠诚度。\n\n## 四、解决方案：构建对抗性桥梁\n\n基于上述假设，作者提出了**统一对抗偏好学习(UniAPL)**框架，通过引入对抗性目标来动态弥合策略分布与专家分布之间的差距。\n\n### 1. 对抗性梯度信号\n引入判别器D_φ，给定提示x、学生响应y_s和教师响应y_t，输出相似性得分。学生策略被训练以最大化此分数，产生对抗性梯度g_ADV，将学生策略拉向教师的语义流形。\n\n### 2. 对齐组件的梯度公式\n将每个对齐阶段视为其对整体策略更新的梯度贡献：\n\n- **对抗性监督微调(A-SFT)**：结合模仿梯度和对抗梯度\n  g_A-SFT = g_SFT + λ_adv g_ADV\n\n- **对抗性群体相对策略优化(A-GRPO)**：结合偏好寻求梯度和对抗梯度\n  g_A-GRPO = g_GRPO + λ_adv g_ADV\n\n### 3. 统一训练目标\n最终表达为单阶段统一训练目标，其梯度是四个基本学习信号的协同组合：\n\ng_Unified = α∇θ L_SFT（模仿信号）\n+ (1-α)∇θ L_GRPO（偏好信号）\n+ λ_adv(α∇θ L_ADV|SFT + (1-α)∇θ L_ADV|PREF)（全局分布正则化）\n\n## 五、优势与验证\n\n这种统一范式直接解决了顺序范式的核心挑战，具有三个显著优势：\n1. **固有防止无基础策略漂移**：专家分布成为动态正则器，不断锚定策略\n2. **协同数据利用增强泛化**：每个梯度更新同时平衡模仿和偏好优化\n3. **简化高效训练流程**：将复杂多阶段训练替换为单一连续训练运行\n\n作者通过在指令跟踪任务上的实验验证了方法的有效性，证明UniAPL不仅实现了更强的性能，还生成了更接近专家演示的输出。\n\n## 六、结论：统一对齐的新范式\n\nUniAPL通过将对齐重新概念化为单阶段约束优化问题，提供了一个原则性框架，有效解决了顺序SFT-then-RL对齐中的分布不匹配问题。这一方法不仅具有理论意义，还通过显著的经验增益和简化的训练流程得到了验证，为整合更多样化的监督信号和开发更强大可扩展的AI系统引导方法奠定了基础。"
                },
                {
                    "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning",
                    "arxiv_id": "2509.24765",
                    "authors": "Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song",
                    "summary": "Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMs' logical performance.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的基础能力——具体来说是逻辑推理能力。论文提出了LogicAgent，这是一种基于符号学的多视角推理框架，旨在增强LLM处理抽象命题、模糊语境和冲突立场的能力，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于逻辑推理(logical reasoning)，这是通用推理能力的重要组成部分；新兴范式上提出了基于LLM的智能体(LLM-based agents)框架。 第三，论文不涉及任何排除标准中的领域：没有研究多模态与视觉问题，没有将LLM应用于特定领域（虽然使用了哲学概念构建数据集，但这只是为了评估逻辑推理能力，而非应用焦点），也没有主要关注模型可靠性的应用层面问题。 在特殊和模糊情况处理上，论文提出的LogicAgent是一种通用的智能体框架，旨在提升LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种新的方法论来增强LLM的逻辑推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大语言模型在处理抽象命题、模糊语境和冲突立场时的逻辑推理问题。针对语义复杂性和逻辑复杂性交织的场景，我们提出了一种基于符号学方格的多视角推理框架LogicAgent，并在RepublicQA等数据集上通过准确率等指标验证了其有效性。该方法通过语义结构化、逻辑推理和反思验证三个阶段，结合一阶逻辑和三值决策方案，实现了比强基线平均高6.25-7.05%的性能提升。",
                    "summary_translation": "逻辑推理是大型语言模型（large language models, LLMs）的基本能力。然而，现有研究在很大程度上忽视了逻辑复杂性与语义复杂性之间的相互作用，导致现有方法难以处理涉及抽象命题、模糊语境和冲突立场等具有挑战性的场景，而这些场景正是人类推理的核心。针对这一空白，我们提出了LogicAgent，这是一个由符号方阵（semiotic square）引导的框架，旨在共同解决逻辑复杂性和语义复杂性问题。LogicAgent在一阶逻辑（first-order logic, FOL）中明确执行多视角演绎，同时通过存在性导入检查（existential import checks）来缓解空洞推理，该检查采用三值决策方案（True, False, Uncertain）以更真实地处理边界情况。此外，为了克服现有数据集的语义简单性和低逻辑复杂性，我们引入了RepublicQA，这是一个达到大学难度水平（FKGL = 11.94）的基准测试，其词汇和结构多样性显著高于之前的基准测试。RepublicQA基于哲学概念，具有抽象命题和系统组织的对立关系与矛盾关系，使其成为评估逻辑推理的语义最丰富的资源。实验表明，LogicAgent在RepublicQA上实现了最先进的性能，比强基线（strong baselines）平均提高了6.25%，并且能有效泛化到主流逻辑推理基准测试，包括ProntoQA、ProofWriter、FOLIO和ProverQA，额外实现了7.05%的平均提升。这些结果突显了我们基于符号学的多视角推理在提升大型语言模型逻辑性能方面的显著效果。",
                    "inspiration_trace": "# 从模糊到判决：LogicAgent方法论的逻辑演进\n\n## 一、宏观问题：LLM逻辑推理的语义与逻辑双重复杂性\n\n作者观察到，尽管大语言模型(LLM)在逻辑推理方面取得了进展，但它们在处理真实世界推理场景时仍面临根本性挑战。这些场景往往涉及**抽象命题**、**模糊语境**和**冲突立场**，而现有方法未能充分考虑**语义复杂性**与**逻辑复杂性**之间的相互作用。\n\n语义复杂性表现为表达式允许多种解释或表面形式，而逻辑复杂性则要求在上下文前提和语义交互上进行推理。作者识别出关键问题：现有方法将这两者割裂处理，导致在面对哲学抽象、概念对立和深层逻辑关系时表现不佳。\n\n## 二、问题分解：现有方法的局限性分析\n\n作者系统分析了现有方法的不足：\n\n1. **线性推理路径的局限**：如Chain-of-Thought和Tree-of-Thought等方法虽改进了逐步推理，但主要遵循单一线性路径，难以捕捉语义对立和矛盾关系。\n\n2. **符号增强方法的不足**：Logic-LM等符号增强框架虽结合了LLM和一阶逻辑求解器，但缺乏对语义结构的系统处理，难以应对模糊语境。\n\n3. **基准数据集的简单性**：ProntoQA、ProofWriter等现有基准主要基于简单模板和语义直接的内容，缺乏抽象概念和系统性组织的对立关系，无法充分评估真实世界推理的复杂性。\n\n这一分析揭示了研究空白：需要一种能同时处理语义复杂性和逻辑复杂性的框架。\n\n## 三、理论启发：符号学方阵的逻辑迁移\n\n作者从Greimas的符号学方阵(Greimas' Semiotic Square)中获得关键启发。这一结构主义语义框架将二元对立扩展为四元素结构，包含：\n\n- **对立关系(Contraries)**：如S1与S2，两者不能同时为真，但可能同时为假\n- **矛盾关系(Contradictions)**：如S1与¬S1，两者不能同时为真或假\n- **语义蕴含关系**：如S1 ⇒ ¬S2和S2 ⇒ ¬S1\n\n作者洞察到，这一语义框架可以迁移到一阶逻辑中，为处理语义复杂性和逻辑复杂性提供统一的理论基础。这形成了方法论的核心假设：通过符号学方阵引导的多视角推理，可以系统性地处理语义模糊性和逻辑复杂性。\n\n## 四、核心假设：多视角推理与反思验证\n\n基于理论启发，作者形成了四个核心假设：\n\n1. **符号学-逻辑整合假设**：将Greimas符号学方阵整合到一阶逻辑中，可以同时处理语义复杂性和逻辑复杂性。\n\n2. **多视角推理假设**：通过同时考虑原命题、其矛盾命题和对立命题，可以提高推理的鲁棒性，特别是在模糊场景下。\n\n3. **三值决策假设**：引入{True, False, Uncertain}三值决策方案，可以更真实地处理边界情况，避免二元决策的局限性。\n\n4. **反思验证假设**：通过结构化的反思验证机制，可以检测和纠正推理过程中的不一致性，提高推理的准确性。\n\n这些假设共同构成了LogicAgent方法的理论基础。\n\n## 五、方法论设计：三阶段推理框架\n\n基于核心假设，作者设计了LogicAgent框架，包含三个相互连接的阶段：\n\n### 1. 语义结构化阶段\n- **符号学方阵构建**：为输入命题生成其矛盾命题(¬S1)、对立命题(S2)和对立命题的矛盾(¬S2)\n- **一阶逻辑转换**：为每个命题生成对应的FOL表示\n- **验证机制**：通过真值表评估、CFG语法验证和LLM语义验证确保命题的一致性\n\n### 2. 逻辑推理阶段\n- **转换器**：将自然语言前提转换为一阶逻辑，确保逻辑结构的准确性\n- **规划器**：构建推理蓝图，确定评估目标、选择相关前提和推理规则\n- **求解器**：执行符号推理，生成中间结论和最终分类\n\n### 3. 反思验证阶段\n- **直接解析**：当S1和¬S1产生互补结果时采用\n- **快速反思**：当任一命题结果为不确定时触发，分析推理内部一致性\n- **深度反思**：当S1和¬S1产生相同结果时，利用符号学方阵中的语义蕴含关系进行验证\n\n这一设计实现了从语义结构化到符号推理，再到反思验证的完整推理循环，模拟了人类在模糊情境中的多视角推理过程。\n\n## 六、评估基准：RepublicQA的设计\n\n为验证方法有效性，作者设计了RepublicQA基准数据集，基于柏拉图《理想国》的哲学概念构建：\n\n- **语义复杂性**：包含抽象哲学命题和多样化的语境前提\n- **逻辑复杂性**：系统组织的对立和矛盾关系，要求深层逻辑推理\n- **语言复杂性**：达到大学级阅读难度(FKGL=11.94)，词汇和结构多样性显著超过现有基准\n\nRepublicQA的设计反映了作者对真实世界推理挑战的理解：语义丰富、逻辑复杂、概念抽象。这为评估LogicAgent处理语义模糊性和概念复杂性的能力提供了理想平台。\n\n## 七、实验验证：从假设到证据\n\n通过系统实验，作者验证了核心假设：\n\n1. **多视角推理的有效性**：在RepublicQA上，LogicAgent超过强基线平均6.25%，证明符号学引导的多视角推理能有效处理语义复杂性。\n\n2. **泛化能力验证**：在四个主流逻辑推理基准上，LogicAgent实现平均7.05%的提升，表明方法不仅适用于哲学推理，还能泛化到多种逻辑推理任务。\n\n3. **组件贡献分析**：消融实验表明，移除符号学方阵导致性能下降最大(-8.16%)，验证了多视角推理的核心作用；移除反思验证阶段也导致显著下降(-4.36%)，证实了反思机制的重要性。\n\n4. **语义-符号整合优势**：对比实验显示，在语义丰富的RepublicQA上，同时使用自然语言和FOL表示效果最佳；而在其他基准上，FOL表示更为关键，证明了双模态表示的互补价值。\n\n## 八、结论：从模糊到判决的完整路径\n\nLogicAgent的演进体现了从问题识别到方法解决的完整逻辑链条：\n\n1. **问题识别**：LLM逻辑推理面临语义与逻辑双重复杂性挑战\n2. **理论启发**：符号学方阵为处理语义对立和逻辑关系提供了结构化框架\n3. **假设形成**：多视角推理、三值决策和反思验证可增强LLM的逻辑性能\n4. **方法设计**：三阶段推理框架实现了语义结构化、逻辑推理和反思验证的整合\n5. **验证评估**：通过专门设计的基准和系统实验，验证了方法的有效性和泛化能力\n\n这一演进路径展示了如何从观察到的实际问题出发，通过理论启发形成创新假设，进而设计系统解决方案，最终通过严格实验验证其有效性。LogicAgent不仅提高了LLM在逻辑推理任务上的性能，更重要的是，它为处理自然语言中的语义模糊性和逻辑复杂性提供了一种新的范式，弥合了符号推理与神经推理之间的鸿沟。"
                },
                {
                    "title": "Query Circuits: Explaining How Language Models Answer User Prompts",
                    "arxiv_id": "2509.24808",
                    "authors": "Tung-Yu Wu, Fazl Barez",
                    "summary": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出\"查询电路\"(query circuits)方法，用于解释语言模型如何将特定输入映射到输出，追踪模型内部的信息流。论文不仅提出了这一新方法，还开发了评估指标(NDF)和高效识别电路的采样方法。虽然论文不直接提出新的训练范式或直接增强模型推理能力的方法，但它通过提供模型决策的局部、输入级别解释，有助于深入理解LLM的内部推理机制。这种对模型推理过程的理解是提升模型通用推理能力的重要基础，符合\"增强模型内在可解释性，从而提升模型的通用可靠性和推理质量\"的标准。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或应用层面的模型可靠性问题。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决语言模型决策解释问题，特别是解释模型如何回答特定用户查询。针对单个输入查询的解释需求，我们提出了一种query circuits方法，通过在模型内部直接追踪信息流来识别关键子网络，并在IOI、arithmetic、MMLU和ARC等benchmark上通过NDF指标验证了其有效性。实验表明，仅使用1.3%的模型连接即可恢复约60%的MMLU问题性能。",
                    "summary_translation": "解释语言模型为何产生特定输出需要局部的、输入级别的解释。现有方法揭示了全局能力电路（global capability circuits）（例如，间接对象识别），但未能解释模型为何以特定方式回答特定输入查询。我们引入了查询电路（query circuits），它直接追踪模型内部将特定输入映射到输出的信息流动。与基于代理的方法（surrogate-based approaches）（例如，稀疏自编码器）不同，查询电路是在模型内部识别的，从而产生更忠实且计算上可访问的解释。为使查询电路实用化，我们解决了两个挑战。首先，我们引入了标准化偏差忠实度（Normalized Deviation Faithfulness, NDF），这是一个稳健的指标，用于评估发现的电路在多大程度上能够恢复模型对特定输入的决策，并且广泛适用于我们设置之外的电路发现。其次，我们开发了基于采样的方法，以高效识别那些稀疏但忠实描述模型行为的电路。在多个基准测试（IOI、算术、MMLU和ARC）中，我们发现模型内存在极其稀疏的查询电路，可以恢复其在单个查询上的大部分性能。例如，一个仅覆盖模型连接1.3%的电路可以恢复约60%的MMLU问题性能。总体而言，查询电路为忠实、可扩展地解释语言模型如何处理单个输入提供了一个步骤。",
                    "inspiration_trace": "# 从宏观问题到创新方法：Query Circuits的逻辑演进\n\n## 1. 宏观问题：语言模型的可解释性需求\n\n论文从语言模型在高风险领域（医疗、自动驾驶）的应用需求出发，提出一个根本性问题：**如何解释语言模型为何对特定用户查询产生特定输出？**\n\n- 当医疗AI决定患者是否需要手术时，其推理过程必须可解释\n- 当自动驾驶系统选择错误控制动作时，其失败模式必须可解释\n- 现有方法主要解释模型的全局能力，而非特定输入-输出映射\n\n## 2. 现有方法的局限性分析\n\n作者系统评估了三类主流可解释性方法，揭示其核心局限：\n\n### 2.1 能力电路(Capability Circuits)\n- **功能**：识别实现特定技能（如间接对象识别）的子网络\n- **局限**：只能解释模型的全局算法能力，无法解释为何对特定输入产生特定输出\n- **例证**：能解释模型如何进行间接对象识别，但不能解释对特定句子的具体回答\n\n### 2.2 基于代理模型的方法（如稀疏自编码器）\n- **功能**：在代理模型中发现电路\n- **局限**：\n  - 代理模型可能无法忠实重构原始模型激活\n  - 可能未捕捉LLM的真实机制\n  - 训练成本高，可访问性受限\n\n### 2.3 其他方法\n- 视觉模型电路发现和LLM输入依赖特征分析\n- **共同局限**：不提供LLM在提示级别的电路解释\n\n## 3. 问题聚焦：查询电路发现\n\n基于上述局限，作者提出新研究任务：**查询电路发现(Query Circuit Discovery)**\n\n### 3.1 核心定义\n- **目标**：识别LLM内部驱动其对单个输入查询做出决策的特定电路\n- **与能力电路的本质区别**：\n  - 能力电路：全局解释（模型如何实现特定算法技能）\n  - 查询电路：局部解释（模型如何处理特定输入并产生输出）\n\n### 3.2 技术挑战识别\n\n作者通过实验发现三个关键挑战：\n\n#### 挑战1：评估指标不稳定性\n- **现象**：广泛使用的标准化忠实度分数(NFS)在通用数据集(如MMLU)上表现极不稳定\n- **证据**：NFS值常超过1或低于0，无法可靠指示电路何时开始捕获模型行为\n- **影响**：无法有效评估电路质量和监控发现进度\n\n#### 挑战2：能力电路方法在查询设置中性能下降\n- **现象**：直接应用能力电路方法发现查询电路效果差\n- **证据**：在IOI数据集上，查询电路平均恢复不到50%性能，而能力电路恢复约65%\n- **原因**：\n  - 特征归因受梯度噪声影响\n  - 间接效应计算忽略边缘间组合效应\n  - 单个查询中传输无关特征的边缘可能显示非零梯度，但组合时贡献很小\n\n#### 挑战3：复杂查询需要高边缘预算\n- **现象**：处理复杂查询需要大量边缘才能形成有效电路\n- **证据**：在MMLU天文学上，EAP-IG需约100k边缘(25.9%)才能超过随机基线\n- **问题**：是复杂查询确实需要更多边缘，还是现有方法无法识别有效电路？\n\n## 4. 解决方案：创新方法设计\n\n针对上述挑战，作者提出两个核心创新：\n\n### 4.1 新评估指标：标准化偏差忠实度(NDF)\n\n#### 设计动机\n- NFS在通用数据集上不稳定，需要更稳健的评估指标\n\n#### 核心定义\n```\nNDF(Cq) = 1 - min( [L(M(q)) - L(Cq(q))] / [L(M(q)) - L(M(q'))], 1 )\n```\n\n#### 关键特性\n- **对称性**：围绕原始模型性能对称惩罚上下偏差\n- **有界性**：严格限制在[0,1]区间\n- **解释性**：NDF=1表示电路与原模型性能相同；NDF=0表示性能偏差超过原始与损坏查询间的差距\n\n#### 优势\n- 在通用数据集上提供稳定可靠的评估\n- 能有效跟踪发现进度\n\n### 4.2 新发现方法：Best-of-N (BoN)采样\n\n#### 设计动机与关键观察\n- **观察**：基于原始查询发现的电路可能不忠实，但基于其释义发现的电路可以成功\n- **假设**：由于梯度噪声和忽略组合效应，单个查询的边缘评分只能捕获粗略模式，不足以一致选择形成忠实电路的边缘集\n- **类比**：发现忠实电路类似\"彩票假设\"——原始查询和其释义发现的电路是\"彩票\"，成功恢复模型性能的是\"中奖彩票\"\n\n#### 方法流程\n1. 生成原始查询q的p个释义{q1, ..., qp}\n2. 使用{q, q1, ..., qp}计算边缘重要性分数，形成边缘分数矩阵{S, S1, ..., Sp}\n3. 利用这些矩阵形成p+1个电路\n4. 测量忠实度分数，选择最优电路\n\n#### 效率优化变体\n为提高效率，作者提出两种变体：\n\n##### 4.2.1 插值BoN (iBoN)\n- **原理**：在两个先前发现的忠实电路间插值，无需额外LLM前向传播\n- **方法**：从较小电路开始，添加来自较大电路的高分边缘，直到达到目标边缘数\n\n##### 4.2.2 带约束自适应分数矩阵的BoN (BoN-CSM)\n- **原理**：利用所有先前发现的电路建立分数矩阵和层级矩阵\n- **方法**：按层级（优先来自较小电路的边缘）和分数排序边缘，选择前N个形成电路\n\n## 5. 实验验证与结果\n\n作者在多个基准测试上验证方法有效性：\n\n### 5.1 数据集\n- IOI（间接对象识别）\n- 算术加法和乘法\n- MMLU（多个类别）\n- ARC Challenge\n\n### 5.2 主要结果\n- **有效性**：BoN、iBoN和BoN-CSM一致优于基线方法\n- **稀疏性**：在MMLU上，仅使用5k边缘（1.3%）的电路可实现平均0.6的NDF\n- **对比**：普通EAP-IG需要约200k边缘（51.7%）才能达到相同水平\n- **意义**：证明即使在复杂查询中，LLM内的紧凑子网络仍可恢复大部分模型行为\n\n### 5.3 消融研究\n- **释义数量**：BoN性能随释义数量增加而单调提高，但收益递减\n- **运行效率**：BoN运行时间略长于EAP-IG（500步），但性能显著优于EAP-IG（即使1000步）\n\n## 6. 结论与贡献\n\n### 6.1 核心贡献\n1. **任务定义**：提出查询电路发现任务，区分于能力电路发现和基于代理模型的方法\n2. **挑战解决**：\n   - 提出NDF解决评估指标不稳定问题\n   - 提出BoN采样及其变体解决发现方法效果差问题\n3. **实证验证**：证明模型内的小电路可解释单个查询上的大部分行为\n\n### 6.2 研究意义\n- 为忠实、可扩展的提示级别LLM决策解释提供了实用路径\n- 推进了从输入依赖激活稀疏性到电路稀疏性的研究\n- 为高风险场景中LLM决策的可解释性提供了新思路\n\n这一逻辑演进展示了作者从宏观可解释性需求出发，通过批判性分析现有方法局限，聚焦到具体研究问题，系统识别技术挑战，并提出创新解决方案的完整思维过程。"
                },
                {
                    "title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs",
                    "arxiv_id": "2509.24377",
                    "authors": "Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu",
                    "summary": "Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://github.com/reml-group/PRISM.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力，特别是数学推理能力。论文提出了PRISM框架，将数学推理分为策略规划和定向执行两个阶段，这是一种新的训练/推理范式，旨在增强LLM的通用推理能力，而非将LLM应用于特定领域。 其次，论文包含多个正面指标：核心概念方面明确关注Large Language Models (LLMs)；能力方向专注于mathematical reasoning和planning；虽然未明确提及强化学习或自我进化，但提出了训练轻量级策略适配器的方法，属于训练方法创新。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。数学推理被视为LLM的基础能力之一，而非特定领域应用。 在特殊情况下，虽然论文提到\"tool-integrated reasoning\"作为现有方法之一，但其核心贡献是提出一种通用的策略路由框架，而非将工具应用在特定领域。PRISM框架是一种通用方法论，适用于各种数学推理问题，旨在增强LLM的通用问题解决能力。 综上所述，这篇论文的核心贡献是提出了一种新的框架来增强LLM的数学推理能力，通过动态选择最适合特定问题的推理策略来提高模型性能，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决LLMs在数学推理中单一策略无法适应不同问题需求及忽略效率-有效性权衡的问题。针对数学推理任务，我们提出了一种PRISM框架，将推理解耦为策略规划和执行两个阶段，并在五个数学推理基准上通过Pass@1和Pass@5准确率验证了其有效性。",
                    "summary_translation": "现有方法通常利用固定策略，如自然语言推理（natural language reasoning）、代码增强推理（code-augmented reasoning）、工具集成推理（tool-integrated reasoning）或基于集成的推理（ensemble-based reasoning），来指导大型语言模型（Large Language Models, LLMs）执行数学推理。我们的分析表明，单一策略无法适应特定问题的需求，因此忽略了有效性和效率之间的权衡。为解决这些问题，我们提出了通过实例特定建模进行规划和路由（Planning and Routing through Instance-Specific Modeling, PRISM），这是一个将数学推理分解为两个阶段的新框架：策略规划和定向执行。具体而言，我们首先整理了一个多策略偏好数据集，我们称之为MathStrat，捕获每个问题-策略对的正确性、过程质量和计算效率。然后，我们基于该数据集训练了一个轻量级的策略适配器（Strategy Adapter），以获得在上述四种推理策略上的置信度分布。在推理时，自适应路由策略（adaptive routing policy）基于预测器置信度动态调整推理方法。它指导模型对高置信度预测使用单策略执行，对竞争性场景使用双策略验证，对不确定情况使用全面的多策略探索。在五个数学推理基准测试上的大量实验表明，PRISM始终优于单个策略和集成基线，在不同基础模型上实现了0.9%到7.6%的改进。自适应路由方法对各种模型架构的数学推理任务显示出特别强大的优势。我们的代码已在https://github.com/reml-group/PRISM上发布。",
                    "inspiration_trace": "# 从问题到解决方案：PRISM框架的逻辑演进\n\n## 一、宏观问题：提升LLMs的数学推理能力\n\n作者从提升大型语言模型(LLMs)在数学推理任务中的性能这一宏观问题出发。数学推理需要精确的逻辑推理、符号操作和多步问题解决，是一个长期存在的挑战。现有方法主要依赖固定的推理策略，如自然语言推理(NLR)、代码增强推理(CAR)、工具集成推理(TIR)或基于集成的推理(EBR)。\n\n## 二、关键观察：现有方法的局限性\n\n通过系统性实验分析，作者发现了两个核心挑战：\n\n### 挑战1：单一策略无法适应所有问题\n作者评估了四种推理策略在不同数学问题类型上的性能（如图1所示），发现没有单一策略在所有问题类别上都表现最佳。例如，某些策略在代数问题上表现出色，但在几何问题上表现较差。这表明僵化地坚持固定推理范式无法充分释放LLMs的潜力。\n\n### 挑战2：忽略了效率与效果之间的权衡\n作者进一步评估了不同策略的推理效率，发现没有单一策略在所有情况下都实现最佳效率。现有方法通常忽略计算成本、延迟和资源效率，导致大量计算开销并未带来相应的准确性提升。\n\n## 三、核心假设：问题感知的策略选择\n\n基于以上观察，作者形成了核心假设：\n**如果能够根据具体问题动态选择最适合的推理策略，而不是固定使用单一策略，那么可以在保持计算效率的同时提高数学推理的整体性能。**\n\n这一假设包含两个关键点：\n1. 不同问题适合不同的推理策略\n2. 动态选择策略可以平衡效果和效率\n\n## 四、解决方案设计：PRISM框架\n\n为验证这一假设，作者提出了PRISM（Planning and Routing through Instance-Specific Modeling）框架，将数学推理解耦为两个核心阶段：\n\n### 阶段1：策略规划\n- **目标**：为每个问题实例预测最适合的推理策略\n- **实现方法**：训练一个轻量级的策略适配器(Strategy Adapter)\n\n#### 数据构建\n作者设计了一个创新的多策略性能分析方法：\n1. 在标准基准问题上执行四种推理策略\n2. 使用多维度评分函数评估每个解决方案：\n   - 正确性：是否产生正确解决方案\n   - 过程质量：是否遵循数学上有效的推理步骤\n   - 计算效率：时间和资源消耗\n3. 将原始分数转换为软目标分布，形成约13,000个实例的MathStrat数据集\n\n#### 策略适配器训练\n- 训练策略适配器预测策略概率分布\n- 使用KL散度损失匹配目标分布\n- 添加辅助交叉熵损失强化顶级策略学习\n\n### 阶段2：目标执行\n- **目标**：根据策略适配器的预测，动态选择最适合的执行路径\n- **实现方法**：基于置信度的自适应路由策略\n\n作者设计了三种执行模式：\n1. **自信路由**：高置信度且偏好明显时，仅执行单一最佳策略\n2. **审议路由**：置信度高但策略排名接近时，执行前两个策略并通过多数投票聚合结果\n3. **探索路由**：预测器置信度不足时，执行所有可用策略并通过多数投票选择最终答案\n\n这种基于置信度的编排机制实现了战略灵活性和计算效率的平衡。\n\n## 五、验证与评估\n\n作者通过广泛实验验证了PRISM的有效性：\n\n1. **主要结果**：在五个数学推理基准上，PRISM一致优于单一策略和集成基线，在MATH基准上达到53.2%的准确率，比最佳单一策略基线提高3.1%。\n\n2. **消融研究**：逐步添加路由组件的结果表明，完整的自适应路由策略对实现最佳性能至关重要。\n\n3. **性能-效率权衡**：PRISM在提高准确率的同时实现了更好的计算效率。\n\n4. **可扩展性**：在不同规模的模型上评估表明，PRISM在不同模型容量下都能提供一致的改进。\n\n## 六、逻辑演进总结\n\n作者的思维链条展现了从宏观问题到具体解决方案的系统性演进：\n\n1. **观察现象**：现有数学推理方法使用固定策略，无法适应不同类型的问题\n2. **发现问题**：单一策略无法适应所有问题，且忽略了效率与效果之间的权衡\n3. **提出假设**：动态选择最适合的推理策略可以提高整体性能\n4. **设计解决方案**：提出PRISM框架，解耦策略规划和目标执行\n5. **实现方法**：构建多策略偏好数据集，训练策略适配器，设计自适应路由策略\n6. **验证假设**：通过广泛实验证明PRISM的有效性、优越性和可扩展性\n\n这一逻辑链条体现了科学研究中的系统性思维，从问题识别到解决方案设计的完整思考过程，为数学推理任务提供了一个创新且有效的方法论。"
                },
                {
                    "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling",
                    "arxiv_id": "2509.24460",
                    "authors": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Likang Xiao, Yanwei Ren, Quan Chen, Xianglong Liu",
                    "summary": "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力的。它提出了一种名为ContextPRM的新方法，将过程奖励模型(PRM)的学习目标从验证领域特定知识转变为建模领域无关的逻辑流，通过关注思维链(CoT)步骤之间的上下文连贯性来增强模型的通用推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确讨论大语言模型(LLMs)；能力方向上聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning)和逻辑推理；训练方法上涉及过程奖励模型(PRM)，这与强化学习相关；新兴范式上采用了思维链(CoT)方法。 第三，论文不主要聚焦于排除标准中的任何领域。虽然论文在法律、历史、哲学等非数学领域进行了测试，但这些是用来验证模型泛化能力的测试领域，而非论文的主要研究焦点。论文的核心是提出一种通用的推理方法，而非针对特定应用领域。 最后，论文不涉及需要特殊判断的模糊情况。它没有将LLM作为工具应用于特定领域，而是直接提升LLM本身的通用推理能力，使其能够在多个领域表现出色。 综上所述，这篇论文完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决Process Reward Models (PRMs)在非数学领域泛化能力有限的问题。针对多领域推理任务，我们提出了一种ContextPRM方法，通过专注于思维链步骤间的上下文连贯性而非领域特定知识来建模领域无关的逻辑流程。在MMLU-Pro的九个非数学领域上，通过加权多数投票评估，ContextPRM实现了比基线平均6.5%的准确率提升，显著优于先前最先进方法。",
                    "summary_translation": "过程奖励模型（Process reward models, PRMs）已证明通过利用测试时扩展（test-time scaling, TTS）在增强大型语言模型（large language models, LLMs）的数学推理能力方面具有显著效果。然而，虽然大多数PRMs在数学领域表现出实质性提升，但特定领域训练数据的稀缺性和基于知识的学习模式限制了它们在面对其他领域时的泛化能力。为解决这一限制，我们将学习目标从验证特定领域知识转向建模领域无关的逻辑流。我们的方法以思维链（chain-of-thought, CoT）步骤之间的上下文连贯性为中心，通过一种新颖的数据注释和训练框架实现，增强了模型在不同领域的泛化能力。例如，我们得到的模型ContextPRM在MMLU-Pro的九个非数学领域（包括法律、历史和哲学）中，通过加权多数投票实现了比多数投票基线显著提高6.5%的平均准确率，大幅超越了VersaPRM的2.2%提升和其他专注于数学的PRMs的0.5%提升，展示了在数学和非数学领域的一致性能。",
                    "inspiration_trace": "# ContextPRM核心方法的逻辑链推演\n\n## 1. 宏观问题：大语言模型(LLM)的推理能力提升\n\n**观察**：LLM在复杂推理任务上仍有局限性，需要提升其推理能力。  \n**聚焦**：测试时扩展(TTS)作为一种利用额外计算资源提高性能的策略，其效果严重依赖于指导推理过程的验证器质量。  \n**关键问题**：如何构建有效的验证器来提升LLM的推理能力？\n\n## 2. 问题聚焦：过程奖励模型(PRM)的领域局限性\n\n**观察**：PRM作为验证器的一种，在数学推理任务上表现出色，但在其他领域泛化能力有限。  \n**证据**：VersaPRM研究表明，数学PRM难以迁移到非数学任务。  \n**深入分析**：\n- 现有PRM主要关注领域特定知识的验证\n- 不同学科有异质性推理风格：从科学领域的形式化符号推导到人文学科的细微论证\n- 这种异质性使得基于领域特定知识的PRM难以跨领域泛化\n\n## 3. 核心假设：从领域特定知识到领域无关逻辑流\n\n**洞察**：不同领域虽然知识内容不同，但逻辑推理结构可能有共通之处。  \n**假设**：如果将学习目标从验证领域特定知识转变为建模领域无关的逻辑流，可能会提高PRM的泛化能力。  \n**理论依据**：逻辑流是跨领域通用的，而领域知识是特定的；通过关注思维链步骤之间的上下文连贯性，可以捕捉这种领域无关的逻辑流。\n\n## 4. 方法设计：ContextPRM的两大创新组件\n\n### 4.1 上下文感知训练方法\n\n**传统PRM局限**：将每个步骤视为独立正确性评估，随着上下文增长，难以诊断当前步骤失败的真正原因。  \n**创新设计**：\n- 构建复合上下文表示，明确考虑连续推理步骤之间的关系\n- 为每个步骤提供前一步骤作为直接上下文，通过特殊格式化函数标记\n- 将k步CoT转换为k个独立训练样本，每个样本评估步骤间的逻辑转换有效性\n- 损失函数从评估步骤正确性转变为评估步骤间连贯性\n\n### 4.2 基于上下文连贯性的CoT标注标准\n\n**传统标注局限**：主要识别事实错误，无法识别孤立正确但在推理链中逻辑有缺陷的步骤。  \n**创新标注标准**：\n- 三级分类：Good(正确、上下文适当、有贡献)、Okay(正确但冗余或进展最小)、Bad(包含错误、误解、逻辑谬误或误导)\n- 识别\"正确但无逻辑依据\"的步骤\n- 一旦发现第一个\"Bad\"步骤，所有后续步骤自动标记为不正确\n- 实验证明此标注标准能更精确地定位推理链中的初始错误(修改率达42.82%)\n\n## 5. 验证与结果：从假设到实证\n\n**实验设计**：在MMLU-Pro-CoT-Eval测试集上评估多领域性能，比较WMV和BoN采样方法。  \n**关键结果**：\n- ContextPRM在非数学领域实现6.5%的平均准确率提升，显著优于VersaPRM的2.2%\n- 消融研究证明上下文感知训练和上下文连贯性标注具有协同效应\n- 单领域训练研究显示，即使在单一领域(如法律、心理学)上训练，ContextPRM也表现出显著的多领域泛化能力\n- 逻辑密集型领域(如哲学、心理学)的训练比知识密集型领域(如历史、物理)产生更好的泛化效果\n\n## 6. 理论贡献与启示\n\n**核心洞见**：通过将学习目标从验证孤立正确性转变为建模领域无关的上下文连贯性，成功提高了PRM的跨领域泛化能力。  \n**方法论贡献**：建立了标注标准与训练方法的紧密耦合，确保训练目标与标注逻辑一致。  \n**更广泛启示**：关注推理的结构而非仅仅是其内容，是构建更通用、更强大的多领域测试时扩展过程奖励模型的有前景方向。"
                },
                {
                    "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision",
                    "arxiv_id": "2509.24351",
                    "authors": "Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu",
                    "summary": "The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at https://github.com/reml-group/AMCS.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种自适应蒙特卡洛搜索(AMCS)框架，用于改进过程奖励模型(PRM)的训练数据质量，从而增强大语言模型的复杂数学推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文的核心贡献是将数据生成从固定的静态搜索转变为自适应的动态搜索，在节点值估计和路径扩展层面进行了创新，这是一种提升LLM通用推理能力的新方法。 从正面指标看，论文包含了多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上使用了过程奖励模型(PRM)，这与强化学习框架相关。 论文不符合任何排除标准：不涉及多模态与视觉内容，不专注于特定应用领域(数学推理被视为LLM的通用能力而非特定领域应用)，也不关注模型可靠性的应用层面问题。 论文也不涉及需要特殊处理的模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文直接致力于提高大语言模型的通用推理能力(特别是数学推理)，提出了新的训练范式和方法，完全符合我的研究范围。",
                    "summary2": "本文旨在解决Process Reward Model (PRM)训练中过程监督数据生成的效率低下和灵活性不足问题。针对数学推理任务，我们提出了一种Adaptive Monte Carlo Search (AMCS)框架，通过不确定性驱动的自适应采样和时间自适应策略的路径扩展，将数据生成从静态转变为动态过程。在MATH、AIME等四个数学推理基准上，AMCS训练的PRM实现了高达76.2%的准确率，显著优于基线方法，且在小参数模型监督下能超越大参数模型，展现出强大的泛化能力。",
                    "summary_translation": "过程数据的质量在训练过程奖励模型（Process Reward Model, PRM，过程奖励模型）中起着关键作用，该模型能够增强大语言模型的复杂数学推理能力。现有方法基于固定预算采样策略（fixed-budget sampling strategy，固定预算采样策略）来估计推理步骤的质量，并在自动数据生成过程中导航庞大的搜索空间以执行路径扩展，导致其效率低下且缺乏灵活性。为解决这些问题，我们提出了自适应蒙特卡洛搜索（Adaptive Monte Carlo Search, AMCS，自适应蒙特卡洛搜索）框架，该框架在节点值估计和路径扩展层面上将数据生成从固定的静态搜索转变为自适应的动态搜索。一方面，AMCS通过为不确定的推理步骤分配更多样本，同时为那些较易估计的步骤使用较少样本，从而自适应地优化估计。另一方面，它通过具有时间自适应策略的蒙特卡洛算法（Monte Carlo algorithm，蒙特卡洛算法）增强路径扩展，该策略从广泛探索开始，并逐渐转向利用最有前景的方向。借助AMCS，我们构建了一个包含约20万个过程监督示例的大规模数据集MathSearch-200K，用于训练PRMs。为验证我们方法的有效性，我们在四个数学推理基准测试上进行了广泛的实验。实验结果表明，Qwen2.5-Math-7B-PRM-AMCS在MATH500上与GLM-4-9B配合使用时达到了高达76.2%的准确率，优于所有基线PRMs。值得注意的是，由Qwen2.5-Math-7B-PRM-AMCS监督的7B模型超越了具有较弱监督的72B模型。此外，Qwen2.5-Math-7B-PRM-AMCS在分布外问题（out-of-distribution problems，分布外问题）上保持了一致的优势，展示了强大的泛化能力。我们的代码可在https://github.com/reml-group/AMCS获取。",
                    "inspiration_trace": "# 从静态到动态：自适应蒙特卡洛搜索的逻辑演进\n\n## 宏观问题：提升LLMs的数学推理能力\n\n大型语言模型(LLMs)在复杂数学推理任务中面临挑战，需要有效方法增强其推理能力。研究表明，过程奖励模型(PRM)通过评估推理过程中的每个步骤，能够显著提升LLMs的数学推理表现。然而，PRM的效果高度依赖于高质量的过程监督数据。\n\n## 问题聚焦：过程监督数据生成的瓶颈\n\n获取高质量过程监督数据面临两大挑战：\n1. 人工标注每个推理步骤需要大量专业知识和努力\n2. 现有自动化方法（如基于蒙特卡洛的方法）存在效率和灵活性不足\n\n## 关键观察：现有方法的局限性\n\n通过深入分析，作者发现现有方法在两个层面存在根本性问题：\n\n### 节点价值估计层面\n- **现象**：现有方法对所有推理节点采用固定预算采样策略（如统一分配16个样本）\n- **问题**：忽略了不同节点扩展的难度差异，导致资源分配不合理\n- **后果**：简单节点过度采样，复杂节点采样不足，整体效率低下\n\n### 路径扩展层面\n- **现象**：现有方法在搜索阶段采用固定的探索和利用策略\n- **问题**：无法根据搜索进展动态调整探索与利用的平衡\n- **后果**：难以准确定位错误推理步骤，数据生成过程缺乏灵活性\n\n## 假设形成：自适应是关键\n\n基于上述观察，作者提出核心假设：\n1. 如果能根据推理步骤的不确定性动态调整采样资源，将提高节点价值估计效率\n2. 如果能在路径扩展过程中自适应地平衡探索和利用，将提高数据生成灵活性\n\n## 方法设计：自适应蒙特卡洛搜索(AMCS)\n\n为验证假设，作者设计了AMCS框架，包含两个核心创新：\n\n### 1. 不确定性驱动的自适应采样\n- **初始采样与聚类**：将多样化的初始rollouts按生成置信度和解决方案复杂度特征分组\n- **迭代优化**：对不确定性高的集群分配更多采样资源，对已收敛集群减少采样\n- **终止条件**：设置节点置信度、总预算和集群收敛三重标准，确保计算效率\n\n### 2. 自适应路径扩展\n- **利用价值**：基于自适应MC估计的节点级成功概率\n- **探索奖励**：采用UCT风格奖励鼓励访问较少节点\n- **动态权衡**：随时间指数衰减调整探索权重，从初始广泛探索逐渐转向最有希望方向的利用\n\n## 实验验证：从数据到模型\n\n作者通过完整实验链验证AMCS有效性：\n1. **数据生成**：使用AMCS构建包含20万标注推理轨迹的MathSearch-200K数据集\n2. **模型训练**：基于数据集训练PRM模型Qwen2.5-Math-7B-PRM-AMCS\n3. **性能评估**：在四个数学推理基准测试中，AMCS显著优于现有方法\n4. **深入分析**：验证监督数据分布特性和自适应资源分配模式\n\n## 结论：动态自适应的价值\n\nAMCS通过将数据生成从固定的、静态的转变为自适应的、动态的搜索策略，解决了现有方法在效率和灵活性方面的瓶颈。实验证明，高质量的PRM不仅能提升LLMs的数学推理能力，甚至能使小模型超越弱监督的大模型，展示了过程监督质量相对于模型规模的重要性。这一研究为自动生成高质量过程监督数据提供了新范式。"
                },
                {
                    "title": "Humanline: Online Alignment as Perceptual Loss",
                    "arxiv_id": "2509.24207",
                    "authors": "Sijia Liu, Niklas Muennighoff, Kawin Ethayarajh",
                    "summary": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的训练方法，特别是对齐(alignment)技术。论文提出了一个新的理论框架，解释为什么在线对齐(如GRPO)比离线对齐(如DPO)表现更好，并基于此创建了\"humanline\"变体。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而不是将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文虽然未直接提及\"Large language models\"，但讨论的DPO、GRPO、PPO等技术都是LLM训练中的核心对齐方法。在训练方法方面，论文明确讨论了强化学习方法(RLHF, RL)，特别是PPO/GRPO等在线对齐方法，这是提高LLM通用能力的重要技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的理论框架和训练方法，通过模拟人类感知来改进模型的对齐过程，从而提高模型在可验证和不可验证任务上的性能。虽然论文没有直接讨论推理、规划等具体能力，但改进对齐方法本质上是提高模型整体性能和通用性的重要途径，这与提高LLM通用推理能力的目标是一致的。论文中提到的\"可验证和不可验证任务\"很可能包括需要推理能力的任务，因此这篇论文对研究大语言模型的通用推理能力具有重要参考价值。",
                    "summary2": "本文旨在解释在线对齐方法（如GRPO）为何比离线方法（如DPO）表现更好。基于行为经济学的展望理论，我们提出了一种humanline设计模式，将人类感知偏差纳入对齐目标，通过humanline syncing和humanline clipping实现。在AlpacaEval2和MATH500上的实验表明，humanline变体使用离线数据时能达到与在线方法相当的性能，同时训练速度提升6倍，在数学推理任务中采样频率可降低64倍而不损失性能。",
                    "summary_translation": "在线对齐（Online alignment，例如GRPO）通常比离线对齐（Offline alignment，例如DPO）表现更佳——但原因是什么？借鉴行为经济学中的前景理论（Prospect theory），我们提出了一个以人为中心的解释。我们证明了在线策略采样（Online on-policy sampling）能更好地近似人类对模型可生成内容的感知分布，而PPO/GRPO风格的裁剪（clipping）——最初引入只是为了稳定训练——恢复了人类感知概率的一种感知偏差。从这个意义上说，PPO/GRPO已经起到了感知损失（Perceptual losses）的作用。\n\n我们的理论进一步表明，在线/离线二分法本身对于最大化人类效用是偶然的，因为我们可以通过以模仿人类感知的方式选择性训练任何数据来实现相同效果，而不局限于在线策略数据。这样做将使我们能够更快速、更经济、更灵活地进行后训练（Post-train），同时不牺牲性能。为此，我们提出了一种设计模式，将概率的感知扭曲（Perceptual distortions）明确纳入DPO/KTO/GRPO等目标中，创建了它们的\"humanline\"变体。令人惊讶的是，我们发现这些humanline变体，即使使用离线非策略数据（Offline off-policy data）训练，也能在可验证（Verifiable）和不可验证（Unverifiable）任务上匹配其在线对应方法的性能。",
                    "inspiration_trace": "# 从观察到方法：Humanline核心方法的逻辑演进\n\n## 一、问题起源：在线对齐方法为何更优？\n\n**初始观察**：在线对齐方法（如GRPO）通常比离线对齐方法（如DPO）表现更好，但原因不明。\n\n**现有解释不足**：\n- 在线方法数据覆盖更好\n- 在线方法更强调生成而非判别\n- 在线方法搜索空间更简单\n\n作者认为这些解释未触及本质，提出核心问题：如果目标是最大化人类效用，那么在线/离线二分法本身可能是偶然的。\n\n## 二、跨学科视角：引入前景理论\n\n**理论迁移**：作者引入行为经济学中的前景理论（Prospect Theory），该理论解释了人类为何做出不最大化期望值的决策。\n\n**核心概念**：\n1. **价值函数**：映射结果到主观价值，表现为风险规避和损失规避\n2. **权重函数**：替代客观概率，反映人类对概率的主观感知（高估极端结果，低估典型结果）\n\n**创新扩展**：将前景理论应用于生成模型对齐，将输出的\"surprisal\"（log[πθ(y|x)/πref(y|x)]）视为结果。\n\n## 三、核心假设：人类感知概率分布的关键作用\n\n**关键洞见**：人类效用最大化不仅取决于价值函数，还取决于人类如何感知模型输出的概率分布。\n\n**理论支持**：作者证明在线在策略采样比离线离策略采样更好地近似人类感知的分布（图2）：\n- 从较差模型采样时，隐含容量函数比人类容量函数饱和更快\n- 从较好模型采样时，隐含容量函数饱和更慢\n\n## 四、重大发现：裁剪操作与感知偏差的联系\n\n**意外发现**：PPO/GRPO中的裁剪操作（最初仅为稳定训练）实际上恢复了人类感知概率的感知偏差。\n\n**理论证明**：作者证明了PPO/GRPO的裁剪组件是人类线采样（humanline sampling）在极限条件下的特殊情况。这一发现揭示了最先进的对齐方法之所以成功，是因为它们在某种程度上已经是感知损失。\n\n## 五、方法论创新：人类线变体（Humanline Variants）\n\n**设计思路**：既然成功源于模拟人类感知，那么可以显式地将感知偏差纳入任何对齐目标，而不必限制于在线在策略数据。\n\n**核心设计模式**：\n1. **人类线同步（Humanline Syncing）**：每k步将参考模型权重与策略模型同步\n2. **人类线裁剪（Humanline Clipping）**：在损失计算前不对称裁剪token-wise似然比到[εP, εR]范围\n\n**理论优势**：这种方法允许从任何来源（在线、离线、在策略、离策略）获取数据，并以反映人类感知的方式选择性地使用它。\n\n## 六、实验验证与意义\n\n**验证平台**：\n1. 不可验证奖励：指令跟随任务\n2. 可验证奖励：数学推理任务\n\n**关键结果**：\n- 离线数据+人类线变体匹配在线方法性能（图1）\n- 数学推理中，人类线GRPO允许数据采样频率降低64倍而不损失性能（图6）\n\n**核心意义**：在线/离线二分法对最大化人类效用是偶然的，关键在于数据是否反映人类感知的模型结果分布。人类线变体实现了更快、更便宜、更灵活的对齐，同时不牺牲性能。\n\n这一逻辑链条从观察现象出发，通过跨学科理论引入，形成核心假设，发现意外联系，最终提出创新方法论，并通过实验验证其有效性，展现了完整的科学思维过程。"
                },
                {
                    "title": "Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback",
                    "arxiv_id": "2509.24159",
                    "authors": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu",
                    "summary": "Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是提出了一种新的训练范式\"鲁棒偏好优化\"(RPO)。论文关注的是如何更好地将模型与人类价值观对齐，这是提升模型通用能力的重要方面。对齐是LLM推理和行为的基础，更好的对齐可以提升模型在各种任务上的表现，包括推理任务。 其次，从正面指标分析： - 核心概念：论文明确关注大语言模型(LLMs)的对齐问题，提到了Mistral和Llama 3模型。 - 训练方法：论文明确讨论了RLHF（基于人类反馈的强化学习），并提出了改进方法，这符合强化学习优化的指标。 - 能力方向：虽然论文没有直接讨论推理、规划或问题解决，但对齐方法的改进可以间接提升模型在这些方面的表现。 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于任何特定应用领域 - 不是从应用层面讨论水印、安全或安全问题，而是从基础方法层面改进对齐算法 最后，论文提出的是一种通用的训练框架，可以应用于多种对齐算法（DPO, IPO, SimPO,和CPO），提升它们的鲁棒性和效果。这种基础性的方法改进有助于提升大语言模型的通用能力，符合研究目标。 因此，尽管论文没有直接讨论推理能力，但其提出的对齐方法改进可以间接提升模型的推理表现，且属于基础能力改进的范畴，故应保留。",
                    "summary2": "本文旨在解决LLM对齐中人类偏好数据噪声问题。针对噪声和多元化的偏好数据场景，我们提出了一种基于EM算法的Latent Collective Preference Optimization (LCPO)框架，通过推断标签正确性并作为自适应权重校准训练损失，并在Mistral和Llama 3模型上通过AlpacaEval 2和Arena-Hard基准测试的win rate指标验证了其有效性，实现了高达7.0%的性能提升。",
                    "summary_translation": "标准的人类偏好对齐方法，如人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF），是将大型语言模型（Large Language Models, LLMs）与人类价值观对齐的基石技术。然而，这些方法都建立在一个关键但有缺陷的假设之上：人类偏好是同质的（代表单一、统一的偏好），且收集的数据是无噪声的（无错误的）。实际上，这两点都不成立，因为人类偏好是多元的，且标注者可能会犯错。这导致了记录数据与真实偏好之间的差异，可能会误导模型并降低其性能。\n\n为应对这一挑战，我们提出了鲁棒偏好优化（Robust Preference Optimization, RPO）。RPO采用期望最大化（Expectation-Maximization, EM）算法来推断每个标签正确性的后验概率，并利用该概率在训练损失中自适应地重新加权每个数据点，以减轻噪声影响。我们通过建立任意偏好损失与其相应概率模型之间的理论联系，进一步推广了这一方法。这种推广使得现有对齐算法能够系统地转化为其鲁棒对应版本，从而将RPO从一个特定算法提升为鲁棒偏好对齐的元框架。\n\n理论上，我们证明了在完美校准模型的条件下，RPO保证能够收敛到数据集的真实噪声水平。我们的实验证明了RPO作为元框架的有效性，它持续增强了四种最先进的对齐算法（DPO、IPO、SimPO和CPO）。当应用于Mistral和Llama 3模型时，RPO增强的方法在AlpacaEval 2和Arena-Hard上实现了显著的胜率提升，分别提高了7.0%和5.4%。",
                    "inspiration_trace": "# LCPO核心方法逻辑链分析\n\n## 1. 宏观问题：LLM对齐中的根本挑战\n\n论文从大型语言模型(LLM)与人类价值观对齐的核心挑战出发：\n- **问题背景**：LLM对齐是开发安全可靠AI系统的关键前提\n- **主流方法**：基于人类反馈的强化学习(RLHF)及其简化变体(如DPO)\n- **关键缺陷**：现有方法都基于一个有问题的假设——人类偏好是同质的且数据无噪声\n\n## 2. 问题观察：现实与假设的差距\n\n作者敏锐观察到现实与理论假设之间的鸿沟：\n- **观察1**：人类偏好本质上是多元的(pluralistic)，不同标注者有不同观点\n- **观察2**：标注过程存在错误，导致数据中包含显著噪声\n- **影响量化**：研究表明20%-40%的偏好对可能是噪声，仅10%的噪声率增加就能导致模型胜率下降30%\n\n## 3. 问题分析：现有方法的局限性\n\n深入分析传统方法的根本缺陷：\n- **硬标签问题**：将人类反馈视为确定性的二元选择，无法表达不确定性\n- **平等权重问题**：每个标签被赋予同等置信度，无法区分可靠反馈和噪声\n- **脆弱性根源**：标准损失函数会导致模型过度拟合损坏的标签，性能随噪声增加而急剧下降\n\n## 4. 核心假设：重新框架化问题\n\n基于以上分析，作者提出两个创新性假设重新定义问题：\n\n**假设1：潜在集体偏好**\n- 存在一个不可观测的\"潜在集体偏好\"yw ≻∗ yl，代表理想化共识\n- 观察到的偏好yw ≻k yl只是这一真实值的噪声信号\n- 引入二元潜在变量zi表示标签是否正确，用参数ηk建模标注者可靠性\n\n**假设2：偏好的通用概率模型**\n- 任何偏好损失函数Lpref都可转换为概率分布\n- 偏好概率与指数化负损失成正比：p(yw ≻∗ yl|x, θ) = σ(Lpref(x, yl ≻ yw; θ) - Lpref(x, yw ≻ yl; θ))\n- 这建立了损失函数与概率模型的统一联系\n\n## 5. 方法推导：期望最大化(EM)算法\n\n基于上述假设，作者推导出LCPO算法：\n\n**E步：推断标签正确性**\n- 计算每个标签正确的后验概率wi作为\"软标签\"\n- wi = p(yw,i ≻∗ yl,i |xi, θ)ηki / [p(yw,i ≻∗ yl,i |xi, θ)ηki + p(yl,i ≻∗ yw,i |xi, θ)(1-ηki)]\n- 这相当于估计每个标注者的可靠性\n\n**M步：加权参数更新**\n- 使用wi作为权重更新策略参数θ，形成加权损失函数\n- 更新标注者可靠性ηk为其所有标签的平均置信度\n- 实现策略优化和可靠性估计的相互促进\n\n## 6. 实际实现：小批量训练优化\n\n为平衡效率与性能，作者提出实用实现方案：\n- 使用指数移动平均(EMA)进行在线可靠性更新\n- ηk ← (1-α)ηk + α·(∑wi)/Nk,B\n- 这使算法能够适应大规模训练场景\n\n## 7. 理论分析：收敛性保证\n\n作者提供严格理论证明：\n- **引理4.1**：在完美校准模型下，真实标注者可靠性是EM更新的不动点\n- **定理4.2**：EM迭代保证收敛到真实可靠性η⋆k\n- 这为LCPO提供了坚实的理论基础\n\n## 8. 实验验证：实证效果\n\n通过广泛实验验证LCPO的有效性：\n- **元框架验证**：LCPO一致增强四种主流对齐算法(DPO、IPO、SimPO、CPO)\n- **性能提升**：在AlpacaEval 2和Arena-Hard上实现最高7.0%的胜率提升\n- **理论验证**：受控实验证实LCPO能准确估计标注者可靠性\n\n## 逻辑链总结\n\n作者从LLM对齐的宏观挑战出发，通过观察现实与假设的差距，分析现有方法的根本缺陷，提出创新的核心假设，推导出基于EM算法的解决方案，并通过理论分析和实验验证其有效性，最终形成一个通用且强大的LLM对齐框架。这一完整逻辑链展示了从问题识别到方法创新的系统性思考过程。"
                },
                {
                    "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge",
                    "arxiv_id": "2509.24276",
                    "authors": "Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan",
                    "summary": "Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出G-reasoner框架，通过集成图和语言基础模型来增强LLMs对图结构知识的推理能力。从本质上看，论文致力于解决LLMs在处理结构化知识方面的局限性，提出了一种新的方法（QuadGraph抽象和图基础模型）来增强LLMs的通用推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning\"这一核心能力方向，并讨论了如何通过结合图结构知识来提升LLMs的推理性能。虽然涉及图结构知识，但这是一种通用的知识表示方法，不是针对特定应用领域的研究。论文提出的框架具有通用性，旨在提升LLMs的基础推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在图结构知识上推理能力有限的问题。针对多样化的图结构知识场景，我们提出了一种G-reasoner框架，结合QuadGraph标准化表示和图基础模型进行联合推理，并在六个基准数据集上通过EM、F1和ACC等指标验证了其有效性。",
                    "summary_translation": "大语言模型（Large language models, LLMs）在复杂推理方面表现出色，但仍受到静态和不完整参数化知识（parametric knowledge）的限制。检索增强生成（Retrieval-augmented generation, RAG）通过引入外部知识来缓解这一问题，然而现有的RAG方法由于信息碎片化和知识结构建模能力弱，在知识密集型任务中表现不佳。图（Graphs）为知识内部关系建模提供了一种自然方式，但大语言模型本质上是非结构化的，无法有效处理图结构数据进行推理。近期的图增强检索生成（Graph-enhanced RAG, GraphRAG）试图通过构建定制化图并使大语言模型能够在其上进行推理来弥合这一差距。然而，这些方法通常依赖于临时图设计（ad-hoc graph designs）、启发式搜索（heuristic search）或昂贵的智能体管道（agent pipelines），这阻碍了其可扩展性和泛化能力。\n\n为应对这些挑战，我们提出了G-reasoner，这是一个统一框架，集成了图基础模型（graph foundation models）和语言基础模型（language foundation models），用于对多样化的图结构知识进行推理。我们方法的核心是QuadGraph，这是一种标准化的四层抽象（four-layer abstraction），能够将异构知识源（heterogeneous knowledge sources）统一为共同的图表示。在此基础上，我们引入了一个3400万参数的图基础模型（Graph foundation model, GFM），该模型共同捕捉图拓扑结构（graph topology）和文本语义（textual semantics），并与大语言模型集成以增强下游应用（downstream applications）中的推理能力。为确保可扩展性和效率，我们实现了混合精度训练（mixed-precision training）和分布式消息传递（distributed message-passing），以支持使用更多GPU来扩展图基础模型。在六个基准测试（benchmarks）上的广泛实验表明，G-reasoner始终优于最先进的基线方法（state-of-the-art baselines），显著增强大语言模型的推理能力，并实现了高效率和跨图泛化（cross-graph generalization）能力。",
                    "inspiration_trace": "# G-reasoner核心方法的逻辑推演\n\n## 1. 宏观问题：LLMs在知识密集型推理中的局限性\n\n**观察**：\n- 大型语言模型(LLMs)展现出色推理能力，但受限于静态和不完整的参数化知识\n- LLMs难以获取最新和领域特定知识，限制了在真实场景中的应用\n\n**核心问题**：\n如何使LLMs能够有效利用外部知识进行复杂推理，特别是在需要理解知识结构和关系的知识密集型任务中？\n\n## 2. 中观问题：RAG方法的局限性\n\n**观察**：\n- 检索增强生成(RAG)通过引入外部知识缓解了LLMs的知识限制\n- 现有RAG方法在知识密集型任务上表现不佳，因为：\n  * 相关信息分散，难以有效关联\n  * 对知识结构的建模能力弱\n\n**关键发现**：\n- 图是自然的知识结构建模方式，能有效捕捉知识间关系\n- 但LLMs本质上是非结构化的，无法有效处理图结构数据\n\n**核心问题**：\n如何弥合LLMs与图结构知识之间的鸿沟，使LLMs能够有效推理图结构知识？\n\n## 3. 具体问题：现有GraphRAG方法的局限性\n\n**观察**：\n- 现有图增强RAG(GraphRAG)方法主要关注两方面：\n  1. 图构建：设计特定图结构组织知识关系\n  2. 图增强推理：提升LLMs在图结构上的推理能力\n\n**关键局限**：\n- 依赖特定图结构，泛化性差\n- 基于图搜索的方法未充分利用基础模型推理能力\n- 基于代理的方法计算成本高、延迟大\n- GNN-based方法(如GFM-RAG)仍局限于特定图类型\n\n**核心问题**：\n如何开发统一方法，能适应各种图结构并有效推理，同时保证可扩展性和泛化性？\n\n## 4. 假设形成与解决思路\n\n**核心假设**：\n1. 若能将异构图结构统一为标准化表示，则可开发通用推理模型\n2. 若能同时建模图拓扑和文本语义，则可增强推理能力\n3. 若能实现高效训练推理机制，则可支持大规模图处理\n\n**解决思路**：\n- 设计统一图接口，标准化异构知识表示\n- 开发图基础模型，联合推理拓扑结构和语义信息\n- 优化训练推理效率，支持大规模应用\n\n## 5. 方法设计：G-reasoner框架\n\n### 5.1 统一图表示：QuadGraph\n\n**设计动机**：\n现有图结构(知识图、文档图、层次图)各具特点但互不兼容，阻碍了通用推理模型的开发\n\n**解决方案**：\n设计四层图结构统一表示：\n1. **属性层**：捕获节点共同属性\n2. **知识图层**：表示实体及关系三元组\n3. **文档层**：包含非结构化文本信息\n4. **社区层**：基于语义/结构相似性分组节点\n\n**创新点**：\n通过层次化设计，QuadGraph能统一现有各种图结构知识，为通用推理奠定基础\n\n### 5.2 图基础模型推理\n\n**设计动机**：\n现有方法要么忽略文本语义，要么无法处理多样化节点类型，推理能力有限\n\n**解决方案**：\n- 采用查询依赖GNN作为模型主干\n- 融合节点文本语义与图拓扑结构\n- 设计类型特定预测器，支持多样化节点预测\n\n**关键公式**：\n```\nh_l^v = Update(h_{l-1}^v, Agg({Msg(h_{l-1}^v, h_l^r, h_{l-1}^{v'})|(v,r,v')∈E}))\n```\n\n**创新点**：\n首次实现图拓扑与文本语义的联合推理，支持任意节点类型的预测\n\n### 5.3 语言基础模型增强\n\n**设计动机**：\n图推理结果需有效融入LLMs以提升下游任务性能\n\n**解决方案**：\n- GFM预测节点相关性得分\n- 选择各类型top-k相关节点\n- 构建提示模板，整合查询与图推理结果\n- LLM基于增强信息生成最终答案\n\n### 5.4 大规模训练与推理优化\n\n**设计动机**：\n图数据规模庞大，传统方法难以高效处理\n\n**解决方案**：\n- **混合精度训练**：提高吞吐量2.1倍，减少GPU内存17.5%\n- **分布式消息传递**：图分割+本地聚合+跨设备交换，支持大规模图处理\n\n**创新点**：\n首次在图基础模型中实现高效分布式训练，显著提升可扩展性\n\n## 6. 实验验证与效果评估\n\n**验证思路**：\n- 六个基准数据集全面评估\n- 对比非结构化与图增强方法\n- 测试跨图结构泛化能力\n- 消融研究关键组件贡献\n- 分析训练推理效率\n\n**关键结果**：\n- 在所有数据集上显著超越SOTA方法\n- 在不同图结构上展现强大泛化能力\n- 消融实验证实各组件必要性\n- 推理速度提升10倍以上，效率显著\n\n## 7. 逻辑链条总结\n\n从宏观问题到具体解决方案，G-reasoner的逻辑演进遵循以下路径：\n\n```\nLLMs知识限制 → RAG方法局限 → 图结构知识优势 → 现有GraphRAG不足 → 统一表示需求 → \nQuadGraph设计 → 图语义联合推理 → GFM模型开发 → 效率优化 → 实验验证\n```\n\n这一逻辑链条体现了作者从问题本质出发，通过系统性分析和创新设计，最终提出G-reasoner这一统一框架的完整思考过程，为图结构知识与语言模型的融合提供了新范式。"
                },
                {
                    "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL",
                    "arxiv_id": "2509.23870",
                    "authors": "Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu",
                    "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究智能体强化学习(Agentic RL)中的奖励校准问题，提出了一种改进训练方法来增强自主智能体的决策能力。论文核心不是将LLM作为工具应用到特定领域，而是研究如何改进LLM作为智能体的基础训练机制，特别是奖励机制和梯度耦合问题，这直接关系到提升LLM的通用推理能力，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：提到\"autonomous agents\"，虽然未直接使用\"LLMs\"术语，但\"Agentic RL\"通常指基于LLM的智能体强化学习 - 能力方向：关注解决\"long-horizon, real-world tasks\"，这隐含了推理和规划能力 - 训练方法：明确涉及\"reinforcement learning (RL)\"，这是提升LLM能力的关键方法 - 新兴范式：讨论\"agentic RL\"，与\"llm-based agents\"直接相关 第三步：排除标准 论文不符合任何排除标准： - 未涉及多模态与视觉内容 - 未聚焦于医疗、化学、生物等特定应用领域 - 未讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理 论文讨论的是通用的智能体强化学习方法，而非将智能体应用在特定领域。它研究的是如何通过改进奖励机制和解决梯度耦合问题来提升智能体的决策质量，这属于通用推理能力的提升范畴。 核心贡献：论文揭示了智能体强化学习中奖励校准和梯度耦合的关键问题，并提出了一种训练actor分类好/坏动作的方法，以分离好/坏动作的嵌入并减轻梯度干扰，从而提升智能体的决策能力和推理质量。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决智能体强化学习中GRPO算法的奖励校准问题。针对高度相似的训练样本导致梯度耦合问题，我们提出了一种生成式分类解耦（GCD）方法，训练actor同时作为分类器来区分好坏动作，并在ALFWorld和ScienceWorld数据集上通过成功率等指标验证了其有效性。",
                    "summary_translation": "构建能够解决长期视野、现实世界任务的自主智能体（autonomous agents）已获得显著的研究关注。但基于结果的奖励可能导致奖励校准不当（reward miscalibration），这意味着它可能错误地向有缺陷的中间步骤分配正向奖励，这被视为训练过程中不良行为被强化的关键原因。然而，我们揭示基于结果的奖励确保了那些有缺陷的中间步骤具有预期的负优势（negative advantage），这意味着有缺陷的行为应在训练过程中受到惩罚。即使考虑到\"挤压效应\"（squeezing effect），良好行为的概率质量（probability mass）也应该增加，并且行为体（actor）应该逐渐摆脱有害行为。这表明有缺陷的行为应在训练过程中受到惩罚。\n\n我们进一步确定相似样本之间的梯度耦合（gradient coupling）是智能体强化学习（agentic RL）中的一个关键问题，输入提示（input prompt）极其相似且输出动作空间（output action space）有限，因此在训练过程中，来自表现良好样本的梯度可能由于相似的输入观察和输出动作而无意中强化次优或不正确的动作。我们表明，在梯度耦合存在的情况下，一些有缺陷的行为可能会被增强。为解决此问题，我们提出训练行为体（actor）来区分好坏动作，以分离好坏动作的嵌入（embedding）并减轻梯度干扰，大量实验证明了其有效性。",
                    "inspiration_trace": "# 从问题到解决方案：GRPO在智能体强化学习中奖励校准的重新思考\n\n## 1. 宏观问题：智能体强化学习的长时程任务挑战\n\n**观察起点**：当前大型语言模型驱动的自主智能体在解决复杂长时程任务时表现不佳。监督微调泛化能力差，而现有强化学习方法主要关注单轮响应，难以处理需要多步交互的长时程任务。\n\n**主流观点**：基于结果的强化学习方法（如GRPO）在数学推理等领域表现良好，但在多轮交互智能体任务中表现不佳，常导致重复、无效动作等失败模式。\n\n**主流解释**：这种失败归因于\"奖励校准问题\"——在长轨迹中，有缺陷的中间动作仍可能导致成功结果，从而被错误地强化。因此，研究者们尝试通过引入步骤级奖励来提供更细粒度的反馈。\n\n## 2. 挑战主流：重新思考奖励校准问题\n\n**质疑与理论分析**：作者挑战了这一主流观点，通过数学证明（Lemma 3.1）表明基于结果的方法如GRPO原则上能够惩罚有害行为——对于有风险的动作（r > 0），其期望优势是负的，这意味着GRPO应该自然地阻止这类动作。\n\n**核心问题**：如果理论上GRPO应该能够惩罚有缺陷动作，为什么在训练后仍然出现并持续存在有缺陷行为（如回声陷阱或重复响应）？\n\n**深入分析**：考虑\"挤压效应\"（在DPO训练中，所选样本的概率实际上也会下降），作者证明这种效应并不阻碍GRPO的整体收敛：高奖励（好）动作的概率质量应该增加，而坏动作的可能性逐渐减少。\n\n## 3. 新假设：梯度耦合问题\n\n**观察**：作者发现根本原因不在于奖励信号，而在于智能体任务数据固有的高度相似性。在智能体任务中，连续轮次仅因一个新的观察而不同，且受限的动作空间导致相似的思维过程和输出。\n\n**数据分析**：通过实验证明ALFWorld中的样本间相似度明显高于GSM8K数学推理任务（图3a）。不同样本之间的高度相似性直接导致其梯度的相似性。\n\n**新假设**：梯度耦合——对一个样本执行梯度下降可能会不自觉地影响相似样本的可能性。实验验证显示，使用一个样本进行单步梯度下降会导致配对样本的概率发生明显变化（图3b）。\n\n## 4. 学习动态分析：安全区域与危险区域\n\n**建模分析**：将来自相似正样本的梯度耦合建模为对有缺陷动作的恒定正优势c，与动作的内在自我纠正优势A = qr(q - 1)竞争。\n\n**关键发现**：自我纠正机制形成两个不同的动态区域：\n- **安全区域（q < 0.5）**：任何不希望的有缺陷动作概率增加都会遇到更强的自我纠正惩罚，形成稳定的负反馈循环。\n- **危险区域（q > 0.5）**：随着概率增加，自我纠正惩罚减弱，动作极易受到外部梯度耦合的影响，导致概率失控增加。\n\n**冷启动重要性**：良好初始化的模型可以确保有缺陷动作以低概率开始，将它们牢牢置于\"安全区域\"。\n\n## 5. 长期动态与收敛条件\n\n**共存动作分析**：考虑好的动作S1（概率p1，期望优势A1 > 0）和相似的有缺陷动作S2（概率p2，期望优势A2 < 0）的共同进化。\n\n**关键发现**：如果|A1| ≫ |A2|，S1的强正优势可能会泄漏并对S2产生正推动，导致两个概率最初都上升。有缺陷动作的概率p2只有在好动作的概率p1变得足够高以产生克服梯度耦合的抑制效应后才会开始下降。\n\n**实验验证**：图5显示，随着训练进行，一些有缺陷动作的概率下降，但其他一些有缺陷动作的一致性可能会增加，只有当正动作的一致性相对较高时，有缺陷动作的一致性才会开始下降。\n\n## 6. 解决方案：生成式分类解耦（GCD）\n\n**核心洞察**：为了减轻梯度干扰，目标是削弱样本间的影响δ，确保好动作被强化而有缺陷动作被抑制。\n\n**现有方法不足**：如Deng et al. (2025a)尝试通过识别负样本中的特定标记并应用惩罚来限制它们对正样本的影响，但这种方法不适用于基于智能体的强化学习，因为在智能体任务中，相似性主要来自重叠的动作序列和详细推理中的共享结构。\n\n**新方法思路**：如果两个相似样本有不同的结果（一个导致成功，一个导致失败），关键是要区分它们的表示以避免有害的梯度干扰。\n\n**具体方案**：生成式分类解耦（GCD），即训练智能体同时充当分类器。\n- 引入一个辅助任务，让模型学习判断给定动作是好是坏。\n- 通过对结果标签进行监督，迫使模型学习区分性的表示。\n- 对于两个输入输出对(x1, y1)和(x2, y2)，其中y1成功而y2不成功，分类目标将它们的隐藏嵌入在表示空间中拉开。\n- 总体训练目标：L = LGRPO + LGCD，其中LGCD是应用于分类任务的GRPO风格损失。\n\n## 7. 辅助策略：基于提示的纠正\n\n**动机**：虽然基于评论家的训练减弱了梯度耦合，但不能完全消除它。当有缺陷动作的概率很高（\"危险区域\"）时，剩余的耦合最危险。\n\n**解决方案**：基于提示的纠正。\n- 在训练过程中收集模型自己生成的评论，突出显示其容易犯的特定错误。\n- 然后综合这些常见错误，并将它们作为明确的指令注入到后续任务的提示中。\n- 这作为一种强大的、有针对性的干预，将有缺陷动作的概率拖出\"危险区域\"并进入\"安全区域\"。\n- 一旦概率降低，自然的自我纠正机制可以有效地接管并继续在强化学习期间抑制有缺陷行为。\n\n## 8. 实验验证与结论\n\n**主要结果**：在ALFWorld和ScienceWorld任务上验证了所提方法的有效性。\n- 当使用冷启动时，不同方法在域内测试数据上的性能接近，但在域外数据上测试时，作者的方法大大提高了性能。\n- 消融研究证明通过训练模型作为生成式判断器来分类动作是否良好，可以有效分离好动作和坏动作的嵌入，从而帮助减少梯度耦合并提高性能。\n- 梯度耦合分析显示，作者的方法有效地分离了正负样本的嵌入，相同类别样本和不同类别样本之间的影响差距远大于原始GRPO。\n\n**最终贡献**：\n1. 诊断了基于结果的强化学习在智能体任务中的失败，并将其归因于样本相似性导致的梯度干扰而非奖励校准问题。\n2. 展示了训练过程如何进行，揭示了有缺陷动作概率可能增加而非减少的原因和时间，并展示了冷启动在智能体强化学习中的重要性。\n3. 提出了一种新的训练范式，其中智能体同时学习充当评论家，有效解耦有害梯度，增强模型的区分能力，并显著提高性能。"
                },
                {
                    "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance",
                    "arxiv_id": "2509.23730",
                    "authors": "Siyao Song, Cong Ma, Zhihao Cheng, Shiye Lei, Minghao Li, Ying Zeng, Huaixiao Tou, Kai Jia",
                    "summary": "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种新的强化学习框架EAPO（Expert-Assisted Policy Optimization），旨在通过外部专家辅助增强LLM的推理能力，这明显属于改进LLM基础能力和通用推理能力的范畴，而非将LLM作为工具应用于特定领域。 其次，论文包含多个关键正面指标：明确涉及大型语言模型(LLMs)这一核心概念；专注于reasoning能力（特别是数学推理）；采用强化学习(RL)作为训练方法；并包含多轮交互与专家咨询的智能体协作元素。 第三，论文不涉及任何排除标准领域：没有多模态与视觉内容，不专注于特定应用领域（数学推理被视为通用推理能力的一部分），也不关注模型基础设施或应用层面的可靠性。 最后，在特殊情况下，论文中提到的与外部专家的多轮交互和咨询机制，是一种通用的智能体协作框架，目的是增强LLM的通用推理能力，而非应用于特定领域。 论文的核心贡献是提出了一种通过外部专家辅助来增强LLM推理能力的新训练范式，这种方法可以提升模型固有的推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在强化学习优化中面临的探索效率低下和奖励稀疏问题。针对复杂推理任务，我们提出了一种EAPO（Expert-Assisted Policy Optimization）方法，将专家咨询设计为可学习动作，使策略模型能自适应决定何时咨询专家，并在数学推理基准（AIME 2024/2025、AIMO 2025）上通过Pass@32和Var指标验证了其有效性，平均比自我探索模型提高5个百分点。",
                    "summary_translation": "大语言模型（Large language models, LLMs）最近在可验证奖励下通过强化学习（reinforcement learning, RL）优化后，在推理能力方面取得了显著进展。现有方法主要依赖基于结果的监督（outcome-based supervision）来增强大语言模型的内部推理，但往往导致探索效率低下和奖励稀疏的问题。为缓解这一问题，我们提出了专家辅助策略优化（Expert-Assisted Policy Optimization, EAPO），这是一种新颖的强化学习框架，通过在训练过程中融入与外部专家的多轮交互（multi-turn interactions）来增强探索能力。与以往策略孤立推理的方法不同，EAPO激励策略自适应地确定何时以及如何咨询专家，从而产生更丰富的奖励信号和更可靠的推理轨迹（reasoning trajectories）。外部协助最终将专家知识内化到策略模型（policy model）中，放大了模型固有的推理能力。在评估阶段，策略模型已经过充分优化，能够独立解决问题，生成改进的推理路径和更准确的解决方案。在数学推理基准测试（包括AIME 2024、AIME 2025和AIMO 2025）上的实验表明，EAPO一致性地优于专家辅助工作流（expert-assisted workflow）、专家蒸馏模型（expert-distilled models）和强化学习基线（RL baselines），相比自探索模型（self-exploratory models）平均提高了5分。",
                    "inspiration_trace": "# EAPO方法逻辑演进分析\n\n## 一、问题识别：稀疏奖励与低效探索\n\n作者首先观察到大型语言模型(LLMs)在复杂推理任务中面临的核心挑战：\n\n1. **稀疏奖励问题**：现有方法主要依赖基于结果的监督，只有在推理过程结束时才能获得奖励信号，导致探索效率低下。\n   > \"Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards.\"\n\n2. **独立探索的局限性**：传统强化学习方法要求策略模型独立探索广阔的搜索空间，缺乏有效指导，优化过程不稳定。\n   > \"existing reinforcement learning (RL) algorithms rely on policy model to explore and exploit on its own to produce long chains of thought, resulting in inefficient exploration and suboptimal optimization.\"\n\n3. **现有方法不足**：RLHF存在奖励黑客、过度优化问题；测试时扩展方法(如Tree-of-Thoughts)通信成本高且不提升模型本身能力。\n\n## 二、核心假设：专家指导的价值\n\n基于上述问题，作者提出三个关键假设：\n\n1. **专家知识可缓解稀疏奖励**：引入外部专家指导可提供中间步骤反馈，密集化奖励信号。\n   > \"By incorporating external expert assistance, EAPO increases the likelihood of producing partially correct intermediate states and ultimately correct answers, densifying the reward signal...\"\n\n2. **自适应咨询可行**：策略模型能学习何时需要专家帮助，而非盲目依赖或完全忽略。\n   > \"EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals...\"\n\n3. **知识可内化**：通过适当训练，模型能逐渐内化专家知识，最终独立表现优异。\n   > \"External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities.\"\n\n## 三、方法设计：EAPO框架构建\n\n基于假设，作者设计了EAPO框架，包含四个关键组件：\n\n1. **专家咨询作为可学习动作**：将\"咨询专家\"添加到动作空间，使模型能选择独立推理或寻求帮助。\n   > \"the agent's action space is augmented with a special consult experts action, which enables agent improve its reasoning strategy when external assistance is needed.\"\n\n2. **多专家并行推理**：允许模型并行咨询多个专家，提高信息覆盖面。\n   > \"To broaden information coverage within a single turn while keeping computation manageable, we instantiate up to K replicas of the expert model.\"\n\n3. **渐进式减少依赖**：通过课程学习设计，随训练进展逐渐降低专家咨询频率。\n   > \"With a curriculum-like optimization design, the agent is allowed to consult experts freely without in the early stages of training... As training progresses, the model is incentivized to consult less frequently...\"\n\n4. **咨询惩罚机制**：引入惩罚项，进一步鼓励模型减少对专家的依赖。\n   > \"To further strengthen the model's own capability, a penalty term on consultation is introduced.\"\n\n## 四、理论解释：EAPO有效性机制\n\n作者进一步提供了EAPO有效性的理论解释：\n\n1. **缓解稀疏奖励**：专家指导提供中间步骤反馈，增加部分正确状态产生概率。\n   > \"EAPO enables the policy model to consult expert for assistance at critical steps during training, supplying cues of problem-solving from the external experts.\"\n\n2. **信息增益**：专家协助作为外部信号增强上下文，降低决策不确定性。\n   > \"EAPO can be viewed as a lightweight mechanism of information injection: expert assistance acts as external signals that augment the historical context...\"\n\n3. **隐式课程学习**：专家指导引导模型更快进入高奖励区域，为后续学习奠定基础。\n   > \"Under these conditions, EAPO acts as an implicit curriculum, guiding the model more rapidly into regions with high reward...\"\n\n## 五、实验验证：性能与内化证据\n\n通过数学推理基准测试验证EAPO有效性：\n\n1. **性能提升**：在AIME和AIMO数据集上，EAPO平均比自我探索模型提高5个百分点。\n   > \"Experiments on mathematical reasoning benchmarks... show that EAPO consistently outperforms... with an average gain of 5 points over self-exploratory models.\"\n\n2. **稳定性增强**：EAPO训练模型在测试集上表现更低的方差，推理更稳定。\n   > \"training with assistance from experts, policy model achieves lower variance on test sets, indicating greater stability and consistency...\"\n\n3. **内化证据**：随训练进行，模型咨询专家频率逐渐降低，表明知识内化。\n   > \"The policy model calls experts frequently in early training to obtain solution cues, but the number of calls declines markedly as training proceeds.\"\n\n## 六、演进逻辑总结\n\nEAPO方法的逻辑演进可概括为：从观察到LLMs在复杂推理中面临稀疏奖励和低效探索问题，假设引入专家指导可缓解这些问题，设计将专家咨询作为可学习动作的EAPO框架，通过渐进式减少依赖实现知识内化，最终使模型在评估时能独立推理且性能优异。这一过程体现了从问题识别到假设验证，再到方法设计与理论解释的完整科研逻辑链。"
                },
                {
                    "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
                    "arxiv_id": "2509.23619",
                    "authors": "Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, Qiang Xu",
                    "summary": "The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心原因如下： 首先，从本质上看，论文的核心贡献是提出一种名为\"Reasoning Scaffolding\"（推理支架）的新框架，用于提升语言模型的通用推理能力。论文明确指出当前主流方法（从文本理由中进行行为克隆）的局限性，即只教会小模型模仿表面模式而非思维的底层算法结构。作者提出的解决方案是将推理重构为结构化生成过程，通过多任务训练使模型内化连贯推理的计算模式。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，论文包含了多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理(reasoning)，特别是逻辑推理；虽然论文没有涉及强化学习等特定训练方法，但它提出了一种创新的多任务训练范式来提升推理能力。 第三，论文不符合任何排除标准：它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，论文提出的\"推理支架\"方法通过将思维过程抽象为可解释的语义信号，不仅提升了模型的推理能力，还间接增强了模型的可解释性和逻辑一致性，这与提升模型内在可靠性的目标是一致的。 总之，这篇论文直接针对大语言模型的通用推理能力提升，提出了一种新的训练范式，完全符合我的研究范围。",
                    "summary2": "本文旨在解决从大型语言模型(LLMs)向小型语言模型(SLMs)蒸馏推理能力时存在的逻辑鲁棒性不足问题。针对文本理由的行为克隆这一传统方法，我们提出了一种Reasoning Scaffolding框架，将推理过程抽象为离散语义信号的结构化生成过程，并在GSM8K、StrategyQA等多个推理基准测试上通过Pass@1指标验证了其有效性。",
                    "summary_translation": "从大型语言模型（Large Language Models, LLMs）中蒸馏推理的主流方法——即从文本理由中进行行为克隆（behavioral cloning）——存在根本性局限。这种方法教导小型语言模型（Small Language Models, SLMs）模仿表面模式，而非思想的底层算法结构，导致逻辑鲁棒性（logical robustness）严重不足。我们认为，蒸馏过程不应克隆文本，而应直接转移这种算法结构。我们提出了推理支架（Reasoning Scaffolding）框架，该框架将推理重新构建为一个结构化的生成过程。我们的方法首先将教师模型的思想过程抽象为一系列离散的、可解释的语义信号（例如，对比[Contrast]、添加[Addition]），这些信号充当支架。随后，学生模型通过多任务目标进行训练，既要（1）预测下一个语义信号，预期推理流程，又要（2）基于该信号生成相应的步骤。这种多任务方案作为一种强大的正则化器（regularizer），迫使学生模型内化连贯推理的计算模式。在一系列具有挑战性的推理基准测试（reasoning benchmarks）中，我们的方法在准确性和逻辑一致性方面均显著优于最先进的蒸馏技术（state-of-the-art distillation），为创建真正具有推理能力而不仅是流利模仿者的小型模型提供了路径。",
                    "inspiration_trace": "# Reasoning Scaffolding方法论逻辑推演\n\n## 一、宏观问题发现：推理蒸馏的根本缺陷\n\n作者首先观察到当前大型语言模型(LLMs)向小型语言模型(SLMs)转移推理能力的主流方法存在根本性限制。现有方法主要是通过行为克隆从文本理由(Chain-of-Thought rationales)中提取推理能力，这本质上是一种表面模仿。\n\n**核心问题**：传统方法让小型模型学习\"写什么\"，而不是\"如何思考\"。这导致学生模型只是学会了表面文本模式，而非底层的算法思维结构，在面对新问题时表现出逻辑脆弱性。\n\n## 二、深入分析：现象背后的本质原因\n\n作者进一步剖析了这一现象的根本原因：\n\n1. **文本模仿的局限性**：行为克隆将推理视为文本模仿任务，强制小型模型进行一种机械记忆，而非真正的推理学习。\n\n2. **算法结构的缺失**：现有方法无法捕捉和转移教师模型思维过程中的算法结构，只是复制了表面的语言风格。\n\n3. **逻辑一致性的缺乏**：结果是学生模型在面对新问题时容易产生逻辑不一致或无意义的论证。\n\n这一分析揭示了问题的本质：我们需要的是思维结构的转移，而非文本的克隆。\n\n## 三、关键洞察：推理过程的结构化抽象\n\n通过观察教师模型的推理过程，作者获得了关键洞察：\n\n**核心发现**：教师模型的推理过程可以从冗长的文本中抽象为结构化的\"蓝图\"。这个蓝图由离散的、可解释的语义信号组成（如\"对比\"、\"补充\"、\"结论\"等），这些信号控制着连贯论证的流程。\n\n作者观察到在推理轨迹中存在特定的关键词（如\"wait\"、\"but\"、\"ok\"、\"in addition\"），这些词自然地标志着推理中的转换点，可以归类为7种语义信号。\n\n## 四、形成假设：结构化推理转移的可能性\n\n基于上述洞察，作者形成了核心假设：\n\n**核心假设**：如果我们能够将教师模型的推理过程抽象为结构化的语义信号，并让小型模型同时学习这些信号和相应的推理步骤，那么小型模型将能够内化推理的计算模式，而不仅仅是克隆文本。\n\n这一假设将问题从\"如何模仿文本\"转变为\"如何转移思维结构\"。\n\n## 五、解决方案设计：Reasoning Scaffolding框架\n\n基于核心假设，作者提出了\"Reasoning Scaffolding\"（推理支架）框架，该框架包含三个关键组件：\n\n### 1. 逻辑表示蒸馏（Logic Representation Distillation）\n\n**目标**：将教师模型的推理过程抽象为结构化的语义信号。\n\n**方法**：\n- 查询大型推理模型获取详细的推理轨迹\n- 将推理轨迹分割为单个步骤\n- 通过两阶段方法为每个步骤分配语义信号：\n  - 第一阶段：基于关键词匹配进行初始标记\n  - 第二阶段：使用强大的LLM进行语义验证和修正\n\n**创新点**：将冗长的推理文本抽象为7种离散、可解释的语义信号，形成推理的\"结构蓝图\"。\n\n### 2. 推理提议者和语义信号预测器的联合训练\n\n**目标**：让小型模型同时学习预测推理流程和生成具体内容。\n\n**方法**：\n- 设计双分支架构：一个分支用于生成下一步推理，另一个分支用于预测语义信号\n- 通过多任务目标训练小型模型：\n  - 任务1：预测下一个语义信号，预期推理流程\n  - 任务2：基于该信号生成相应的推理步骤\n- 总损失函数：L(t) = L(t)_token + L(t)_signal\n\n**创新点**：使用信号预测任务作为逻辑一致性的强大正则化器，迫使学生模型内化推理的计算模式。\n\n### 3. 语义信号引导的推理\n\n**目标**：在推理过程中利用学习到的支架指导模型思维。\n\n**方法**：\n- 在推理过程中，每一步推理都由预测的语义信号引导\n- 采用自适应策略，计算预测信号的置信度\n- 如果置信度超过阈值，使用该信号指导下一步推理\n- 如果低于阈值，终止推理轨迹并生成结论\n\n**创新点**：通过可解释的语义信号引导推理过程，使小型模型能够模仿教师模型的思维结构，而非简单复制文本。\n\n## 六、验证与优化：实证检验与改进\n\n作者通过实验验证了方法的有效性，并进行了一系列优化：\n\n### 实验验证\n- 在多个推理基准测试上评估方法（GSM8K、StrategyQA等）\n- 结果显示，与最先进的蒸馏方法相比，该方法在准确性和逻辑一致性方面都有显著提高\n- 特别是在小型模型上，提升更为明显，证明了方法的有效性和普适性\n\n### 优化策略\n1. **自适应信号预测**：基于置信度阈值决定是否使用预测信号，确保推理可靠性\n2. **推理轨迹优化**：通过保留关键结论步骤并修剪其他步骤，显著减少令牌使用量，同时保持推理性能\n\n## 七、反思与展望：局限性与未来方向\n\n作者也客观评估了方法的局限性，并提出了未来研究方向：\n\n### 局限性\n1. 主要提取高级话语标记，而非形式化的算法或逻辑操作\n2. 注释方法结合了启发式技术和基于LLM的验证，引入了对教师格式和外部模型的依赖\n\n### 未来方向\n1. 结合更细粒度的逻辑或算法信号，弥合话语级和形式推理之间的差距\n2. 开发更鲁棒、自监督的信号提取方法，减少对昂贵预言机的依赖\n\n## 总结：从问题到解决方案的逻辑演进\n\n作者的思想演进展现了一条清晰的逻辑链：从发现现有方法的表面模仿问题，到分析其逻辑脆弱性的本质原因，再到洞察推理过程的结构化特征，形成结构化推理转移的核心假设，最终设计出\"Reasoning Scaffolding\"框架，通过多任务训练让小型模型真正内化推理的计算模式。这一方法论不仅解决了当前推理蒸馏的根本缺陷，为创建真正具备推理能力的小型模型提供了新路径，也为语言模型的知识转移研究开辟了新方向。"
                },
                {
                    "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks",
                    "arxiv_id": "2509.23537",
                    "authors": "Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang",
                    "summary": "We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究多轮多智能体编排框架，通过多个LLM智能体之间的交互和投票机制来提高问题解决能力，这属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。其次，论文满足多个正面指标：核心概念明确涉及大型语言模型(LLMs)；能力方向上，虽然摘要未直接提及\"reasoning\"，但多智能体通过交互和投票达成共识的过程本质上是一种推理和问题解决，且在MuSR（多步推理）等基准上进行测试；新兴范式方面，论文明确研究\"multi-turn multi-agent orchestration\"，属于多智能体系统的新兴范式。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面研究。最后，论文提出的是通用的多智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用。因此，这篇论文的核心贡献——通过多智能体协作框架提升LLM的通用推理能力——完全符合研究目标。",
                    "summary2": "本文旨在探索多智能体协作是否能超越最强单一LLM的性能。针对多个LLM在复杂推理任务上的表现，我们提出了一种多轮多智能体编排框架，让多个LLM智能体通过迭代提出答案或投票直到达成共识，并在GPQA-Diamond、IFEval和MuSR三个基准测试上通过准确率指标验证了其有效性。",
                    "summary_translation": "我们研究了多轮多智能体编排(multi-turn multi-agent orchestration，多轮多智能体协同工作)，其中多个大型语言模型(large language model, LLM)智能体通过多轮交互，迭代提出答案或进行投票，直至达成共识。我们在GPQA-Diamond、IFEval和MuSR三个数据集上使用四种LLM模型（Gemini 2.5 Pro、GPT-5、Grok 4和Claude Sonnet 4）进行了两项实验：(i)将编排方法与单一LLM基线进行基准测试；以及(ii)在GPQA-Diamond上进行消融实验(ablations，消融研究)，改变智能体是否能看到答案作者以及是否能观察正在进行的投票。编排方法匹配或超过了最强的单一模型，并且始终优于其他模型。对最佳可实现编排性能的分析显示出进一步改进的潜力。消融实验表明，揭示作者身份会增加自我投票和平局情况，而显示正在进行的投票会放大羊群效应(herding，群体跟随行为)，这加快了收敛速度，但有时可能导致过早达成共识。",
                    "inspiration_trace": "# 从单一模型局限到多智能体编排：逻辑演进分析\n\n## 宏观问题：LLM能力的边界与突破\n\n论文的出发点源于对一个根本性问题的思考：**如何突破单一大型语言模型(LLM)的性能天花板？** 作者观察到，尽管LLM能力迅速提升，但没有任何一个模型能在所有任务上表现卓越，这引发了一个更深层的问题：是否存在一种方法能整合不同模型的优势，实现超越单一最强模型的性能？\n\n## 观察与聚焦：多智能体协作的潜力\n\n作者通过三个关键观察将问题聚焦：\n\n1. **模型互补性现象**：不同LLM在不同类型任务上各有所长，表明模型间存在潜在互补性。例如，Gemini 2.5 Pro在GPQA-Diamond上表现最佳(85.9%)，而GPT-5在IFEval上领先(87.4%)。\n\n2. **集体智慧的启示**：人类解决复杂问题时常通过协作、辩论达成更优决策，这一机制可能在AI系统中同样有效。\n\n3. **研究空白识别**：现有多智能体协作研究缺乏与强单一模型的直接比较，且未系统考察协调策略对共识形成的影响。\n\n由此，核心问题聚焦为：**多轮多智能体编排能否有效整合不同LLM的优势，超越最强单一模型的性能？**\n\n## 假设形成：理论基础构建\n\n基于聚焦的问题，作者提出三个核心假设：\n\n1. **互补性假设**：不同LLM的优势可通过适当协调机制结合，产生超越任何单一模型的集体性能。\n\n2. **共识形成假设**：多轮交互(答案提案与投票)能使智能体逐步修正错误，达成比初始判断更准确的共识。\n\n3. **协调机制影响假设**：协调策略(如身份透明度、投票可见性)将显著影响共识质量和形成过程。\n\n## 方法设计：多轮多智能体编排框架\n\n为验证假设，作者设计了一个结构化的三阶段编排框架：\n\n1. **智能体行动阶段**：智能体异步操作，可选择生成新答案或对现有答案投票。引入\"动态重启\"机制——当有新答案提出时，中断当前投票，使所有智能体重新评估，防止过早共识。\n\n2. **共识阶段**：所有智能体完成答案生成和投票后，获多数票的答案被选为共识。动态重启后仅统计重启后的投票，确保基于最新信息决策。\n\n3. **最终呈现阶段**：获胜智能体整合所有参与者的见解和推理，生成综合最终答案。\n\n这一设计的关键创新在于动态重启机制和结构化投票流程，旨在平衡效率与全面性。\n\n## 实验验证：系统测试与消融研究\n\n作者设计两组互补实验验证方法：\n\n1. **基准比较实验**：在GPQA-Diamond、IFEval和MuSR三个基准上，比较编排系统与四个单一LLM(Gemini 2.5 Pro、GPT-5、Grok 4、Claude Sonnet 4)的性能。\n\n2. **协调策略消融实验**：在GPQA-Diamond上，控制两个关键变量：\n   - 投票身份披露(匿名vs实名)\n   - 投票计数可见性(隐藏vs显示)\n\n通过测量自我投票率、首轮答案选择率和共识僵局率等指标，量化协调策略的影响。\n\n## 发现与意义：验证与启示\n\n实验结果证实了核心假设，并带来重要启示：\n\n1. **编排系统有效性**：编排系统在三个基准中的两个取得最高准确率，平均表现(81.2%)超越所有单一模型，证实了多智能体协作的价值。\n\n2. **协调策略的关键影响**：\n   - 身份披露增加自我投票(如GPT-5从81.0%升至88.4%)和共识僵局(从14.1%升至23.2%)\n   - 投票可见性引发羊群效应，首轮答案选择率从54.1%升至67.8%\n\n3. **改进空间识别**：分析显示，即使系统出错，参与智能体中常有正确答案(如GPQA-Diamond上64%的错误案例中至少有一个智能体正确)，表明协调机制仍有优化空间。\n\n## 逻辑演进总结\n\n从宏观问题到具体方法，作者的逻辑演进呈现清晰路径：从观察单一LLM的局限性→识别多智能体协作潜力→形成互补性、共识形成和协调机制影响假设→设计结构化编排框架→通过系统实验验证→揭示协调策略的关键作用→指出未来改进方向。这一研究不仅证明了多轮多智能体编排的有效性，更重要的是揭示了协调机制设计对系统性能的决定性影响，为未来多智能体系统研究提供了重要理论基础和方法指导。"
                },
                {
                    "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning",
                    "arxiv_id": "2509.23285",
                    "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou",
                    "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进LLM的通用推理能力，具体聚焦于\"Tool-Integrated Reasoning (TIR)\"，即通过集成外部工具来增强大语言模型的内部推理能力。这明显属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标的主题：核心概念上明确研究LLMs；能力方向上专注于reasoning（推理）；训练方法上提出了\"Self-Evolved Preference Learning\"和\"Self-Evolved Direct Preference Optimization (DPO)\"，属于self-evolve（自我进化）的训练方法；新兴范式上关注tool use（工具使用），这些都是高度相关的主题。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 在特殊和模糊情况处理上，论文提出的是一种通用的工具使用框架（Tool-Light）来增强LLM的通用推理能力，而不是将工具应用在特定领域，这符合保留标准。论文通过提高推理效率和质量，间接有助于提升模型的通用推理能力。 论文的核心贡献是提出了Tool-Light框架，通过数据集构建和多阶段微调（包括SFT和Self-Evolved DPO）来鼓励LLM高效准确地执行工具集成推理，这直接服务于提升LLM通用推理能力的研究目标。因此，这篇论文完全符合我的研究范围。",
                    "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR)模型中工具使用不当和过度思考的问题。针对多工具调用场景，我们提出了一种基于信息熵分析的Tool-Light框架，包含熵引导采样策略和两阶段训练方法，并在10个推理任务数据集上通过Efficiency和Necessity指标验证了其有效性。",
                    "summary_translation": "工具集成推理（Tool-Integrated Reasoning, TIR）使大型语言模型（Large Language Models, LLMs）能够通过集成外部工具来提高其内部推理能力。然而，采用TIR的模型常常表现出次优行为，如工具使用不足或过度，以及在工具调用后过度思考。如何激励LLMs高效准确地执行TIR，同时稳定推理过程，仍然是一个悬而未决的问题。在本文中，我们首先从信息熵的角度探讨工具调用对模型推理的影响。我们的研究结果表明，工具调用结果导致后续推理的信息熵发生明显变化，推理链的整体熵值根据工具调用的次数而变化。基于这些见解，我们提出了Tool-Light，这是一个旨在鼓励LLMs高效准确地执行TIR的框架。我们的框架包括数据集构建和多阶段微调。在数据集构建方面，我们采用使用微调模型的连续自我进化采样，结合了原始采样（vanilla sampling）和熵引导采样（entropy-guided sampling）。此外，我们在采样过程中建立了选择正负对（positive-negative pairs）的严格标准。训练过程采用两阶段方法，包括监督微调（Supervised Fine-Tuning, SFT）和自我进化直接偏好优化（Self-Evolved Direct Preference Optimization, DPO）。在10个数据集上的实验结果证明了Tool-Light的有效性，显著提高了模型执行TIR任务的效率。",
                    "inspiration_trace": "# Tool-Light方法逻辑推演：从问题观察到方法创新\n\n## 1. 宏观问题：工具集成推理的效率与准确性挑战\n\n作者从大型语言模型(LLMs)的核心局限性出发：尽管LLMs在多种推理任务上表现出色，但当面临复杂任务（如深度信息检索和精确计算）时，仅依靠内部推理能力往往不足。Tool-Integrated Reasoning (TIR)方法应运而生，允许模型在推理过程中调用外部工具以弥补内部知识或能力的不足。\n\n然而，作者观察到现有TIR模型存在三个关键问题：\n- **工具调用不合理**：模型表现出工具使用不足或过度使用的次优行为\n- **工具后过度思考**：当获得低质量工具调用结果时，模型容易出现过度思考甚至分析瘫痪\n- **推理过程不稳定**：缺乏平衡工具使用与内部推理的有效机制\n\n这些问题构成了作者研究的出发点：如何激励LLMs高效准确地执行TIR，同时稳定推理过程？\n\n## 2. 现有方法局限性分析\n\n作者系统审视了现有解决方案，发现其局限性：\n\n1. **强化学习方法**：主要针对单个工具优化，难以泛化到多工具调用场景\n2. **多工具推理优化**：通常只关注工具过度使用问题，忽视了工具使用不足以及工具调用结果对后续推理过程的影响\n3. **全面性不足**：现有工作未能全面解决\"不正确工具调用\"问题，包括过度使用、使用不足和工具后推理效率低下\n\n这些局限性表明，需要一种更全面的方法来优化TIR过程，不仅关注工具使用的数量，还要关注工具使用的质量和推理效率。\n\n## 3. 新视角引入：从信息熵角度探索TIR过程\n\n作者创新性地引入信息熵理论来分析TIR过程。受先前研究启发（推理链的高熵部分往往决定推理方向），作者进行了初步实验，分析TIR任务中的信息熵特征。\n\n关键发现：\n1. **工具调用后的熵变模式**：当模型接收到工具调用结果后，其输出信息熵先上升，然后波动，最后在下一个工具调用到来前急剧下降\n2. **工具调用数量与熵关系**：对于同一样本，低熵链往往涉及较少的工具调用，且随着推理进行，高熵链和低熵链之间的工具调用差异越来越明显\n\n这些发现揭示了工具调用与推理过程信息熵之间的内在联系，为优化TIR提供了新思路。\n\n## 4. 核心假设形成\n\n基于上述观察，作者形成了三个核心假设：\n\n1. **熵-效能关联假设**：工具调用的效率与推理过程中的信息熵变化密切相关\n2. **优化路径假设**：通过优化信息熵分布，可以减少不必要的工具调用，避免过度思考，同时确保在必要时进行工具调用\n3. **训练框架假设**：结合信息熵指导和偏好学习，可以设计出更有效的TIR训练框架\n\n这些假设构成了Tool-Light方法设计的理论基础。\n\n## 5. 方法设计：Tool-Light框架\n\n基于核心假设，作者设计了Tool-Light框架，从数据构建和算法两个角度优化模型的TIR能力。\n\n### 5.1 数据构建：熵引导采样策略\n\n作者设计了一种创新的熵引导采样方法：\n1. **主链生成**：首先生成一个主推理链\n2. **高熵分支**：从最高熵位置分支创建多个路径（基于高熵位置更可能产生多样性输出的观察）\n3. **正负对筛选**：应用严格标准选择高质量的正负对\n4. **混合采样**：将熵引导采样与直接采样相结合，确保数据多样性和平衡\n\n这种方法将计算复杂度从O(mn)降低到O(n log m)，显著提高了采样效率。\n\n### 5.2 算法设计：两阶段TIR训练范式\n\n作者提出了\"两阶段TIR训练\"流程：\n\n1. **监督微调(SFT)**：使用现有SFT数据训练基础模型，帮助模型快速获取完成TIR任务的能力\n\n2. **自进化DPO训练**：\n   - **预对齐DPO训练**：增强模型推理能力，同时减少冗余工具调用\n   - **自进化DPO对齐**：交替采样和训练过程，使模型学习必要的工具调用，同时根据模型当前熟练程度动态调整训练数据复杂性\n\n关键创新点在于自进化机制：模型通过自身生成的数据进行迭代改进，不断优化工具调用策略。\n\n## 6. 实验验证与结果分析\n\n作者在10个具有挑战性的推理任务上验证了Tool-Light的有效性，包括知识密集型和数学推理任务。\n\n关键结果：\n1. **性能提升**：Tool-Light在多个任务上达到最先进水平，平均性能显著优于基线方法\n2. **工具使用效率**：在效率和必要性指标上表现最佳，表明模型能够平衡工具使用与内部推理\n3. **熵分布优化**：Tool-Light的输出序列具有较低的熵分布，证实了熵引导策略的有效性\n4. **推理简化**：相比基线方法，Tool-Light产生了更短的输出序列，同时保持高准确性，有效减轻了过度思考现象\n\n## 7. 逻辑链条总结\n\n作者的研究思路形成了一个完整的逻辑链条：\n\n1. **问题识别**：从TIR的效率和准确性挑战出发，识别工具调用不合理和过度思考等关键问题\n2. **视角创新**：引入信息熵理论分析TIR过程，发现工具调用与熵变化的关联模式\n3. **假设形成**：基于熵观察形成核心假设，提出通过优化熵分布来改善TIR效能的思路\n4. **方法设计**：设计Tool-Light框架，包括创新的熵引导采样策略和两阶段自进化训练范式\n5. **实验验证**：通过多任务实验验证方法有效性，证实了熵-效能关联假设和优化路径假设\n\n这一研究不仅解决了TIR中的实际问题，还为理解和优化LLMs的工具使用行为提供了新的理论视角，体现了从问题观察到方法创新的完整科研思路。"
                },
                {
                    "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers",
                    "arxiv_id": "2509.23178",
                    "authors": "Tian Qin, Yuhan Chen, Zhiwei Wang, Zhi-Qin John Xu",
                    "summary": "Transformers are able to perform reasoning tasks, however the intrinsic mechanism remains widely open. In this paper we propose a set of information propagation rules based on Transformers and utilize symbolic reasoning tasks to theoretically analyze the limit reasoning steps. We show that the limit number of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with $L$ attention layers in a single-pass.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一套基于Transformer的信息传播规则，并利用符号推理任务来理论上分析推理步骤的极限。论文研究了具有L个注意力层的模型在一次前向传播中能够执行的推理步骤数量限制，这直接关注大语言模型的基础推理能力，特别是多步推理能力的理论分析。根据筛选标准的第一步，这篇论文是关于改进LLM基础能力的研究，符合保留条件。论文明确涉及\"reasoning tasks\"和\"symbolic multi-step reasoning tasks\"，这与我们的研究目标高度相关。同时，论文不涉及任何需要排除的特定应用领域或多模态研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在分析Transformer在符号多步推理任务中的推理能力极限。针对符号多步推理任务，我们提出了一套基于Transformer的信息传播规则，并在合成数据集上通过推理准确率和模型维度等指标验证了其有效性。理论分析表明，具有L层注意力块的Transformer在单次前向传播中能够执行的推理步数极限在O(2^{L-1})和O(3^{L-1})之间，实验结果支持了这一理论预测。",
                    "summary_translation": "Transformers (Transformer模型) 能够执行推理任务，但其内在机制(intrinsic mechanism)在很大程度上仍未明确。在本文中，我们提出了一套基于Transformers的信息传播规则(information propagation rules)，并利用符号推理任务(symbolic reasoning tasks)来理论分析极限推理步骤(limit reasoning steps)。我们表明，对于具有L个注意力层(attention layers)的模型，在单次传递(single-pass)中，极限推理步骤数介于$O(3^{L-1})$和$O(2^{L-1})$之间。",
                    "inspiration_trace": "# 从观察到理论：Transformer推理极限的分析逻辑链\n\n## 1. 宏观问题的提出\n\n**观察起点**：Transformer模型在逻辑推理任务中展现出惊人能力，甚至能在国际数学奥林匹克竞赛中达到与顶尖人类选手相当的水平。然而，常用的推理策略如Chain-of-Thought (CoT)等存在\"过度思考\"现象，消耗大量计算资源。\n\n**核心问题**：Transformer模型的内在单次推理能力极限是什么？具体而言，在不依赖迭代提示或外部脚手架的情况下，模型在单次前向传播中能执行多少步有效推理？\n\n## 2. 初步假设与研究方向\n\n**假设形成**：Transformer的推理能力可能源于其架构特性，特别是注意力机制和层叠结构中的信息传播方式。模型可能通过某种机制在单次前向传播中并行执行多步推理。\n\n**研究方向确定**：\n- 研究Transformer架构中的信息传播机制\n- 使用符号推理任务进行理论分析\n- 探索推理步数的理论极限\n\n## 3. 深入分析与机制识别\n\n**关键机制发现**：通过分析大量关于上下文学习和归纳头的研究，识别出三个核心机制：\n\n1. **缓冲机制**：Transformer通过线性变换将多条信息存储到不同子空间，使每个位置能同时保存多个令牌的信息。\n\n2. **相邻位置匹配**：模型利用位置编码在相邻令牌间建立连接，类似于人类依赖前一个词预测下一个词的行为模式。\n\n3. **相同令牌匹配**：归纳头内的基本机制，使包含相同信息的节点能够相互关注，赋予模型强大的分布外泛化能力。\n\n**关键观察**：传统观点认为每层只执行一步推理，但这远未达到Transformer的上限。相邻位置匹配和相同令牌匹配可在单层内多次发生，使浅层模型也能执行多步推理——作者将此现象称为\"并行推理\"。\n\n**问题聚焦**：给定只有相邻位置匹配和相同令牌匹配，具有L层注意力块的Transformer能执行的并行推理步数的上下界是什么？\n\n## 4. 理论框架构建\n\n**符号推理任务定义**：为进行理论分析，作者精确定义了推理对、推理链、推理序列等概念，建立了形式化的推理任务描述框架。\n\n**信息传播规则提取**：基于Transformer行为，提出五条核心规则：\n1. 初始设置规则：定义第0层节点的构造方式\n2. 掩码条件规则：注意力只能从前向节点传播到后向节点\n3. 相邻位置匹配规则：奇数位置节点的信息可传播到后续偶数位置节点\n4. 相同令牌匹配规则：具有相同值集的节点可以相互传播信息\n5. 残差连接规则：确保信息在层间传递时不丢失\n\n**理论分析**：通过数学归纳法，作者证明了在信息传播规则下，对于L层Transformer，推理步数的极限满足：\n- 下界：O(2^(L-1))\n- 上界：O(3^(L-1))\n\n这一结果源于两个关键架构特性：(i)令牌在每层内并行执行推理；(ii)每个嵌入可编码来自不同子线性空间的多个令牌信息。\n\n## 5. 实验验证与结果\n\n**实验设计**：作者构建了与理论分析一致的符号推理任务，训练不同层数和维度的Transformer模型，测试其多步推理能力。\n\n**关键发现**：\n- 3层Transformer能完美解决3步推理问题，但需要较大的隐藏维度\n- 测试准确度随隐藏维度dm单调增加，最终接近100%\n- 对于4步推理，模型达到46.1%准确度；5步推理时降至25.1%\n- 当推理步数超过理论上限(3^(L-1)-1)/2时，模型性能显著下降\n\n**因果验证**：通过干预实验，作者证实当模型正确执行推理时，其信息流动符合预设的推理规则；而当推理步数超过理论极限时，模型依赖记忆而非真正推理。\n\n## 6. 结论与意义\n\n**最终结论**：具有L层注意力块的Transformer在单次前向传播中能执行的最大推理步数Sp满足：\n2^(L-1) - 1 ≤ Sp ≤ (3^(L-1) - 1)/2\n\n**理论意义**：这一分析首次揭示了Transformer在单次前向传播中的推理能力极限，为理解大型语言模型的内在机制提供了理论基础，也为设计更高效的推理策略指明了方向。\n\n**实践启示**：结果解释了为什么复杂推理任务需要多层架构或迭代推理策略，同时也表明通过优化信息传播方式，有可能提升模型的推理效率。"
                },
                {
                    "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models",
                    "arxiv_id": "2509.23108",
                    "authors": "Morgan McCarty, Jorge Morales",
                    "summary": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)如何通过命题推理(propositional reasoning)来完成传统上被认为需要视觉心理意象的任务。论文创建了认知心理学中的经典心理意象任务的新项目，测试了几个最先进的LLMs，发现它们的表现显著高于人类平均水平，特别是当模型分配更多的推理token时表现更强。这表明LLMs具有一种新兴的认知能力，能够通过纯文本推理完成依赖意象的任务。论文关注的是LLMs的基础推理能力和认知机制，而不是将LLM作为工具应用到特定领域，也不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，特别是关于逻辑推理和问题解决能力的研究。",
                    "summary2": "本文旨在探索大型语言模型(LLMs)是否能完成传统上需要视觉心理意象(mental imagery)的任务。针对60个心理意象任务指令集(48个全新创建和12个来自经典研究)，我们提出了一种基于命题推理(propositional reasoning)的测试方法，并在多个最先进的LLMs与100名人类受试者上通过加权评分系统验证了其有效性。实验发现，最佳LLMs(GPT-5和o3系列)表现显著超过人类平均水平(高9.4%-12.2%)，且图像辅助处理反而降低性能，表明LLMs可能通过纯语言处理完成了依赖意象的任务。",
                    "summary_translation": "本研究提供了一种基准测试人工系统中复杂认知行为的新方法。几乎普遍情况下，大型语言模型（Large Language Models, LLMs）在可能包含在其训练数据中且仅使用自然语言即可完成的任务上表现最佳，这限制了我们对它们新兴的复杂认知能力的理解。在这项工作中，我们创建了几十个来自认知心理学（cognitive psychology）的经典心理意象任务（mental imagery task）的新项目。传统上，认知心理学家认为这类任务只能通过视觉心理意象（visual mental imagery）解决（即仅靠语言是不够的）。大型语言模型是测试这一假设的理想选择。首先，我们通过向纯文本模型提供书面指令，并要求它们在执行上述任务中的转换后报告结果对象，从而测试了几个最先进的大型语言模型。然后，我们通过测试100名人类受试者完成完全相同的任务来创建基线。我们发现，最佳的大型语言模型的表现显著高于人类平均水平。最后，我们测试了设置为不同推理水平的推理模型（reasoning models），发现当模型分配更多推理令牌（reasoning tokens）时表现最强。这些结果提供了证据，表明最佳的大型语言模型可能具有完成依赖意象任务的能力，尽管其架构具有非图像性质。我们的研究不仅展示了大型语言模型在执行新任务时的一种新兴认知能力，还为该领域提供了一个新任务，为其他已经高度能力的模型留下了大量改进空间。最后，我们的发现重新点燃了关于人类视觉意象表征形式的争论，表明命题推理（propositional reasoning）（或至少是非意象推理）可能足以完成长期以来被认为依赖意象的任务。",
                    "inspiration_trace": "# 从心理意象本质到LLMs认知能力：论文核心方法的逻辑推演\n\n## 1. 宏观问题：心理意象的本质表征\n\n论文的出发点源于认知心理学中一个持续数十年的根本性争论：**心理意象的本质是什么？**\n- **意象理论(Pictorial view)**：心理意象以类似图像的形式表征，类似于视觉体验\n- **命题理论(Propositional view)**：心理意象可通过命题描述（语言元素）捕捉，无需真正图像表征\n\n这一争论触及人类思维的基本机制，影响我们对认知过程的理解。\n\n## 2. 关键观察：理论与现实的不一致性\n\n作者注意到一个与主流意象理论不符的现象：\n- **无意象人群(Aphantasics)**：约1-4%人口报告没有有意识的心理意象\n- **矛盾发现**：这些人在传统上被认为需要心理意象的任务中表现与常人相当\n\n如果意象理论正确，无意象人群应无法完成此类任务，但事实并非如此，暗示可能有其他机制（如命题推理）支持任务完成。\n\n## 3. 研究机会：LLMs作为理想测试平台\n\n作者敏锐地识别到LLMs提供了一个独特的研究机会：\n- **LLMs的特性**：基于语言训练和处理，无已知视觉意象能力\n- **研究价值**：若LLMs能完成传统上需要视觉意象的任务，将支持命题理论\n- **创新视角**：将AI系统作为认知科学理论的测试平台，跨越学科界限\n\n## 4. 核心假设：命题推理支持的心理意象\n\n基于上述观察，作者提出具体假设：\n**LLMs可通过纯命题推理（语言处理）完成传统上被认为需要视觉心理意象的任务**\n\n若假设成立，将表明：\n1. 命题理论在某些情境下可行\n2. LLMs具有新兴认知能力\n3. 心理意象任务可能不严格依赖视觉表征\n\n## 5. 方法选择：经典任务的创新适应\n\n为验证假设，作者选择并改进了Finke等(1989)的经典心理意象任务：\n- **任务特点**：按指令对想象中的字母/形状进行转换，最终识别物体\n- **改进创新**：\n  * 创建48个全新指令集（避免训练数据污染）\n  * 增加认知负荷（允许每步最多两个字母）\n  * 不限制最终图像只有一种规范形式\n  * 更新语言表述，提高清晰度\n\n## 6. 实验设计：多维度比较验证\n\n作者设计了全面的实验框架：\n- **人类基线**：100名受试者建立性能基准\n- **LLM测试**：多种最先进模型（Claude、Gemini、OpenAI）\n- **图像辅助条件**：测试图像生成是否提升性能\n- **评估系统**：结合专家评分和众包评分处理答案主观性\n\n这一设计允许直接比较LLMs和人类在心理意象任务上的表现，验证核心假设。\n\n## 7. 扩展验证：推理与图像的对比实验\n\n为进一步验证，作者设计了对比实验：\n- **推理令牌测试**：不同推理级别对性能的影响\n- **图像辅助推理**：强制模型生成并修改图像，而非仅依靠想象\n- **上下文测试**：单上下文与多上下文比较，测试上下文学习效果\n\n这些实验设计提供了多角度验证假设的机会。\n\n## 8. 结果分析与理论解释\n\n实验结果支持了作者的核心假设：\n- **主要发现**：最佳LLMs（GPT-5和o3系列）显著超越人类表现（高出9.4%-12.2%）\n- **关键证据**：添加图像反而降低最佳模型性能，支持命题推理而非视觉处理\n- **推理效应**：推理令牌数量与性能正相关，表明认知处理的重要性\n\n作者提出**空间意象与物体意象的区分**作为解释框架，可能调和不同理论观点。\n\n## 9. 理论贡献与未来方向\n\n研究不仅验证了假设，还提出了更广泛的理论意义：\n- **认知科学**：重新点燃心理意象表征形式的辩论\n- **人工智能**：揭示LLMs新兴的认知能力\n- **跨学科价值**：为两个领域提供新研究范式和方法\n\n未来方向包括：研究无意象人群策略、探索更复杂任务、分析LLMs内部表征机制等。\n\n---\n\n这一逻辑链展示了作者如何从宏观的认知科学问题出发，通过观察理论与现实的矛盾，识别LLMs作为研究平台的新机会，提出具体假设，设计创新实验，最终通过实证数据验证假设并提出理论解释。整个过程体现了科学研究的典型逻辑：从问题到假设，再到实验验证和理论解释，同时展现了跨学科研究的创新价值。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 87,
            "papers": [
                {
                    "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
                    "arxiv_id": "2509.25189",
                    "authors": "Gongrui Zhang, Jialiang Zhu, Ruiqi Yang, Kai Qiu, Miaosen Zhang, Zhirong Wu, Qi Dai, Bei Liu, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Yuan Zhang, Xin Li, Zhaoyi Liu, Xin Geng, Baining Guo",
                    "summary": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出InfoAgent，一个通过创新的数据合成管道和协调的网络搜索工具驱动的深度研究智能体，旨在增强大语言模型的通用信息寻求和推理能力。论文符合我的研究目标，原因如下：1）论文本质上是关于改进LLM的基础能力，特别是通过工具使用和信息寻求来增强其推理能力；2）论文包含多个正面指标，如LLM、推理、规划、问题解决、强化学习、智能体、工具使用和深度研究等主题；3）论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面；4）论文提出的智能体/工具使用方法是通用的，旨在增强LLM的通用问题解决能力，而不是应用在特定领域。论文通过两阶段训练方法（监督微调和强化学习）显著提高了模型推理驱动的工具使用能力，这正是我所关注的提高LLM通用推理能力的研究方向。",
                    "summary2": "本文旨在提高自主信息搜索代理的性能。针对复杂多步信息检索任务，我们提出了一种InfoAgent框架，结合创新的数据合成流程和自托管搜索工具，并在BrowseComp、BrowseComp-ZH和Xbench-DS等基准上通过准确率验证了其有效性。",
                    "summary_translation": "构建通过与外部工具交互来扩展能力的大语言模型（Large Language Model）代理代表了人工智能研究和应用的新前沿。在本文中，我们介绍了InfoAgent，这是一个由创新的数据合成（data synthesis）管道和协调的网络（orchestrated web）搜索工具驱动的深度研究（deep research）代理。为了构建具有挑战性且难以找到的查询，我们构建了实体树（entity trees）并应用带有实体模糊化（entity fuzzification）的子树采样（sub-tree sampling）来系统地增加问题难度。与先前严重依赖商业搜索工具的工作不同，我们开发了一个专用的自托管（self-hosted）搜索基础设施，增强了代理环境的透明度，并促进了代理能力的进一步发展。我们通过测量正确回答一个问题所需的平均工具调用（tool calls）次数来评估我们数据管道的有效性，并表明我们的代理在配备我们的工具时能产生更好的性能。我们的InfoAgent是从Qwen3-14B通过两阶段方法进行后训练（post-trained）的：冷启动（cold-start）监督微调以灌输长期视野（long-horizon）搜索行为，随后进行强化学习（reinforcement learning），显著提高了推理驱动（reasoning-driven）的工具使用。通过我们的方法，InfoAgent在BrowseComp上达到15.3%的准确率，在BrowseComp-ZH上达到29.2%，在Xbench-DS上达到40.4%，优于先前的开源深度研究代理，如WebSailor-72B和DeepDive-32B。",
                    "inspiration_trace": "# InfoAgent核心方法的逻辑演进分析\n\n## 一、宏观问题：深度研究智能体的能力瓶颈\n\n作者从互联网知识获取工具发展不均的宏观现象出发，观察到大型语言模型(LLM)通过工具交互扩展能力的趋势，特别是深度研究智能体(DRA)的潜力。DRA能自主规划、推理并执行多步骤信息获取行动，完成复杂研究任务，已成为下一代信息平台的定义特征。然而，作者发现现有DRA存在明显的能力瓶颈，无法充分发挥其潜力。\n\n## 二、问题识别：两大核心挑战\n\n通过分析现有研究，作者识别出实现高效DRA面临的两个关键挑战：\n\n1. **数据合成瓶颈**：现有开源DRA通常执行浅层搜索，主要因为它们在相对简单的数据上训练。这些数据缺乏足够的复杂性和多样性，无法迫使智能体进行长时程检索和连接推理。\n\n2. **交互环境限制**：许多智能体依赖商业搜索API或模拟环境，前者限制了透明度和可控性，后者在面对真实世界复杂问题时表现不佳。DRA的强化学习基础设施需要能处理高并发搜索调用并提供一致结果的工具。\n\n## 三、假设形成：突破瓶颈的可能路径\n\n基于上述问题，作者形成了两个核心假设：\n\n1. **数据复杂性假设**：如果能够构建更具挑战性的训练数据，系统性地增加问题难度，将迫使智能体执行长时程检索和连接推理，从而提高其能力上限。\n\n2. **环境透明度假设**：如果能够建立一个专用的、自托管的搜索基础设施，而非依赖商业搜索API，将提供对输出的细粒度控制和透明的实验环境，促进智能体能力的进一步发展。\n\n## 四、方法论形成：InfoAgent的核心创新\n\n基于这些假设，作者设计了InfoAgent的方法论，包含三个关键创新：\n\n### 1. 创新的数据合成流程\n\n作者设计了一个两阶段的流程来自动合成复杂的、多实体的搜索问题：\n\n- **实体树构建**：从维基百科实体集出发，构建实体树结构，每个节点包含实体名称、URL和提取的事实。通过命名实体识别识别子实体，递归扩展形成实体树。\n\n- **实体模糊化**：通过三阶段过程对事实进行模糊化处理：(1)实体替换（如\"Albert Einstein\"变为\"a famous physicist\"）；(2)数字/日期范围化（如\"1992\"变为\"early 1990s\"）；(3)语义重述。这使问题更难通过内部知识或直接搜索解决。\n\n- **子树采样与QA生成**：从实体森林中采样子树，为根节点生成关于其属性的问题，而非根节点本身，增加问题难度。使用高级模型(如o3)进行多轮测试，过滤出高难度且可解决的问题。\n\n### 2. 自托管搜索基础设施\n\n作者开发了专用的搜索和浏览工具，提供高质量、高吞吐量的信息访问：\n\n- **搜索功能**：不仅返回URL和快照，还通过爬虫获取完整网页内容，使用BM25、嵌入模型和重排序模型提取与查询最相关的文本块，最后用LLM生成简洁摘要。\n\n- **浏览功能**：给定URL，返回网页的语义文档，通过智能文本块选择过滤噪声内容，提供深度信息探索能力。\n\n这一设计解决了商业API的局限性，提供了透明、可控的实验环境。\n\n### 3. 两阶段训练策略\n\n作者采用两阶段方法对基础模型(Qwen3-14B)进行后训练：\n\n- **冷启动监督微调(SFT)**：在合成的长轨迹数据上进行微调，灌输长时程搜索行为，使模型学会规划、信息检索和回溯等复杂能力。\n\n- **强化学习(RL)**：采用GRPO(Generalized Relative Preference Optimization)进一步增强模型的推理驱动工具使用能力。选择适当难度的问题进行训练，提高效率和稳定性。\n\n## 五、验证与优化：实验驱动的迭代\n\n作者在多个深度研究基准上评估InfoAgent，包括BrowseComp、BrowseComp-ZH和Xbench-DS等。实验结果验证了方法论的有效性：\n\n1. InfoAgent(14B参数)在多个基准上超过了更大规模的开源模型(如WebSailor-72B和DeepDive-32B)。\n2. 消融实验证实了SFT冷启动的重要性、自托管工具相对于传统检索器的优势，以及长轨迹训练数据的价值。\n3. 工具调用分析显示，作者合成的数据集需要平均20.3次工具调用，显著高于现有工作(ASearcher: 5.4次，DeepDive: 9.5次)，证明了其更高的复杂性。\n\n## 六、逻辑演进总结\n\nInfoAgent的核心方法形成了一条清晰的逻辑链条：从深度研究智能体的能力瓶颈观察出发，识别数据合成和交互环境两大核心挑战，形成通过增加数据复杂度和提高环境透明度来突破瓶颈的假设，最终设计出创新的数据合成流程、自托管搜索基础设施和两阶段训练策略的完整方法论。这一过程体现了从问题识别到假设形成，再到方法论设计和实验验证的系统化创新思路，为自主信息获取智能体的发展提供了新的方向。"
                },
                {
                    "title": "An empirical study on the limitation of Transformers in program trace generation",
                    "arxiv_id": "2509.25073",
                    "authors": "Simeng Sun",
                    "summary": "We study Transformers on the task \\emph{program trace generation} (PTG), where models produce step-by-step execution traces for synthetic programs. Unlike existing algorithmic problems, PTG externalizes reasoning through long traces where each step is trivial. We train small Transformers with diverse modifications, including alternative position encodings, softmax replacements, hybrid model, and short convolutions. While these models achieve strong in-distribution accuracy, they exhibit systematic failures when generalizing to various factors (e.g., program length, trace steps), though some designs significantly improve generalization.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。具体分析如下： 第一步：核心判断 这篇论文的本质是研究Transformer架构在程序跟踪生成(PTG)任务上的局限性及其改进方法。程序跟踪生成要求模型为合成程序生成逐步执行跟踪，这本质上是一种逻辑推理任务，需要模型理解程序逻辑并按照步骤执行推理。论文探讨了通过修改模型架构（如替代位置编码、softmax替换、混合模型和短卷积）来提高模型的泛化能力，这属于\"改进LLM的基础能力\"和\"增强其逻辑推理能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文涉及多个正面指标： - 能力方向：论文核心研究的是reasoning（推理），特别是logical reasoning（逻辑推理），因为程序跟踪生成本质上是一种逻辑推理过程。 - 虽然论文没有明确提到\"Large language models\"或\"LLMs\"，但研究的Transformer架构是LLM的基础，且程序跟踪生成是评估模型推理能力的重要任务。 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 虽然研究的是程序跟踪生成，但这更准确地说是一个评估模型推理能力的基准任务，而非针对特定领域（如医疗、化学等）的应用研究 - 没有涉及模型可靠性方面的水印、安全性等问题 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。 综上所述，这篇论文通过研究Transformer在程序跟踪生成任务上的表现和改进方法，探索了如何增强模型的逻辑推理和泛化能力，符合提高大语言模型通用推理能力的研究目标。因此，我认为这篇论文应该被保留。",
                    "summary2": "本文旨在研究Transformers在程序跟踪生成任务中的局限性。针对合成程序和输入数据，我们提出了多种Transformers架构修改（包括替代位置编码、softmax替换等），并在控制四个因素（程序长度、跟踪步骤、变量数量和输入大小）的数据集上通过whole-trace accuracy验证了其有效性。",
                    "summary_translation": "我们研究了Transformers（变换器）在\\emph{program trace generation}（程序轨迹生成，PTG）任务上的表现，该任务要求模型为合成程序（synthetic programs）生成逐步的执行轨迹（execution traces）。与现有的算法问题（algorithmic problems）不同，PTG通过长轨迹将推理过程（reasoning）外化（externalizes），其中每个步骤都是微不足道的。我们训练了经过多样化修改的小型Transformers，包括替代位置编码（alternative position encodings）、softmax替代函数（softmax replacements）、混合模型（hybrid model）和短卷积（short convolutions）。尽管这些模型在分布内准确率（in-distribution accuracy）方面表现出色，但在向各种因素（例如程序长度、轨迹步骤）泛化（generalizing）时表现出系统性失效，不过某些设计显著改善了泛化能力。",
                    "inspiration_trace": "# 从宏观问题到方法论：Transformers在程序跟踪生成中的局限性研究逻辑链\n\n## 1. 宏观问题：Transformers在程序执行任务中的能力边界\n\n作者研究的出发点是探索Transformers架构在程序执行和推理任务中的能力边界。作为现代大语言模型的基础架构，Transformers在多种任务上表现出色，但它们在需要精确规则遵循和长序列推理的任务中的能力仍不完全清楚，特别是在模拟计算机程序执行方面。\n\n## 2. 现有方法观察：算法任务的局限性\n\n作者观察到现有算法任务研究（如字符串反转、模加法、排列组合等）存在两个关键局限：\n- 这些任务要求模型在训练期间将固定算法内化到参数中\n- 任务复杂性与输入大小紧密耦合\n\n这种设计无法充分评估模型处理任意程序和长序列推理的能力，也无法区分模型是真正学会了算法还是仅仅记忆了输入-输出映射。\n\n## 3. 新任务设计：程序跟踪生成(PTG)\n\n为解决上述局限，作者提出程序跟踪生成(PTG)任务，其核心特点是：\n- 在上下文中明确提供算法，模型无需内化算法到参数\n- 输入和算法解耦，程序有独立于输入大小的复杂度\n\nPTG要求模型为给定程序和输入生成完整的逐步执行跟踪，将推理过程外部化为长而结构化的跟踪，迫使模型像计算机一样执行可靠顺序操作。\n\n## 4. 研究问题聚焦：从内部执行到外部跟踪\n\n基于PTG任务，作者将研究问题从：\n> \"Transformers能否在给定未见/更长输入的情况下内部执行固定程序？\"\n\n转变为：\n> \"Transformers能否在给定任意程序的情况下，在长序列上一致地执行简单操作？\"\n\n这一转变使研究更关注模型的上下文灵活性和执行任意程序的能力，而非仅处理固定程序和更大输入的能力。\n\n## 5. 控制变量设计：四个关键因素\n\n为精确评估程序执行能力，作者设计了四个关键因素进行控制：\n- **程序长度**：测试当\"指令内存\"增加时的模型行为\n- **跟踪步骤**：测试当\"数据内存\"增加时的模型行为\n- **变量数量**：测试处理更多变量的能力\n- **输入大小**：测试将算法应用于更大实体的能力\n\n这种设计使作者能系统评估Transformers在不同维度上的泛化能力。\n\n## 6. 假设形成：架构选择对泛化性能的影响\n\n基于对Transformers架构的理解，作者假设不同架构选择（如位置编码、注意力机制等）会对PTG任务的泛化性能产生显著影响，特别是某些修改可能更适合处理长序列和精确规则遵循。\n\n## 7. 实验设计：多样化架构修改\n\n为验证假设，作者比较了标准Transformers与多种修改版本：\n- 替代位置编码方法（NoPE, ALiBi, NaPE, Fox, PaTH）\n- Softmax替换（STB, α-entmax）\n- 短1D卷积（Canon）\n- 混合模型（SWAN）\n\n作者训练约154M参数的小型Transformers，在分布内数据上达到接近完美性能，然后评估分布外条件下的表现。\n\n## 8. 实验结果与分析\n\n实验结果显示：\n- 所有模型在分布内数据上实现了强准确性\n- 但在泛化到各种因素时表现出系统性失败\n- 架构选择对分布外性能有显著影响\n- NaPE（混合NoPE和ALiBi头部）平均优于其他修改\n\n特别是，作者发现可靠生成长跟踪仍具挑战性，模型在处理更复杂/更长的指令时表现不佳，处理更大实体大小的能力受位置编码严重影响。\n\n## 9. 结论与启示\n\n基于实验结果，作者得出结论：现有Transformers在逐步执行程序方面存在局限性，这对推理任务和精确指令遵循至关重要。某些架构选择（如NaPE）可显著改善泛化性能，为未来改进Transformers架构提供了方向。\n\n这一研究通过系统性的实验设计，揭示了Transformers在程序执行任务中的具体局限性，并为改进模型架构提供了实证基础。"
                },
                {
                    "title": "Scaling Generalist Data-Analytic Agents",
                    "arxiv_id": "2509.25084",
                    "authors": "Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
                    "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出DataMind，一种可扩展的数据合成和智能体训练方法，用于构建通用的数据分析智能体。论文主要关注增强LLM在数据分析领域的通用推理能力，特别是处理多格式、大规模数据文件和长时程、多步推理的能力。根据筛选标准，该论文符合研究目标，原因如下： 1. 核心判断：论文的本质是改进LLM的基础能力和提出新的训练范式，特别是增强其多步推理能力。DataMind方法结合了SFT和RL的训练目标，旨在提升模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 2. 正面指标：论文包含了所有关键正面指标： - 核心概念：训练了DataMind-14B和DataMind-7B等LLM模型 - 能力方向：明确关注\"long-horizon, multi-step reasoning\"（长时程、多步推理） - 训练方法：使用了结合SFT和RL损失的动态调整训练目标 - 新兴范式：核心是构建\"generalist data-analytic agents\"（通用数据分析智能体） 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 4. 特殊情况处理：虽然论文关注数据分析，但它提出的是一种通用的智能体训练框架，而不是针对特定领域的应用。论文的核心是增强LLM的通用推理能力，这与研究目标高度一致。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，特别是在数据分析领域的多步推理能力，完全符合研究范围。",
                    "summary2": "本文旨在构建开源通用数据分析智能体，解决现有模型在处理多样化格式、大规模数据文件和长周期多步推理方面的不足。针对数据分析任务场景，我们提出了一种DATA MIND可扩展数据合成和智能体训练方法，结合细粒度任务分类、知识增强轨迹采样、动态SFT-RL训练目标及稳定代码执行框架，并在多个数据分析基准上通过准确率指标验证了其有效性，DATA MIND-14B达到71.16%平均得分，超越了专有模型DeepSeek-V3.1和GPT-5。",
                    "summary_translation": "数据分析代理（Data-analytic agents，指能够自动进行数据分析的智能系统）正成为自动化科学发现和创新人工智能（Innovating AI，指具有创新能力的人工智能）愿景的关键催化剂。然而，当前方法严重依赖于专有模型（proprietary models，指闭源的商业化模型）的提示工程（prompt engineering，指设计输入提示以优化模型输出的技术），而开源模型（open-source models）则难以应对现实世界分析所需的多格式、大规模数据文件以及长期（long-horizon，指需要长时间规划的任务）多步推理（multi-step reasoning）。\n\n本文介绍了DataMind，一种可扩展的数据合成（data synthesis）和代理训练方法（recipe），旨在构建通用型（generalist）数据分析代理。DataMind解决了构建开源数据分析代理的三个关键挑战，包括数据资源不足、训练策略不当以及基于代码的多轮交互（multi-turn rollout，指代理与环境的多次交互过程）不稳定。具体而言，DataMind采用了：1) 细粒度任务分类（fine-grained task taxonomy）和递归的由易到难任务组合机制，以增加合成查询的多样性和难度；2) 知识增强的轨迹采样（trajectory sampling）策略，随后进行基于模型和基于规则的过滤；3) 结合SFT（Supervised Fine-Tuning，监督微调）和RL（Reinforcement Learning，强化学习）损失的动态可调整训练目标；4) 内存节约且稳定的基于代码的多轮交互框架。\n\n基于DataMind，我们精心策划了DataMind-12K，这是一个涵盖多个领域、任务类别和数据文件格式的高质量轨迹集（trajectory set，指代理执行任务的过程记录）。在DataMind-12K上训练后，我们的DataMind-14B模型在多个数据分析基准测试上以71.16%的平均分达到了最先进水平（state-of-the-art），超越了最强的专有基线模型DeepSeek-V3.1和GPT-5。我们的DataMind-7B模型也以68.10%的得分在所有开源模型中表现最佳。我们还将探索性试验中获得的一些经验见解（empirical insights）纳入分析实验，旨在为社区提供关于代理训练（agentic training，指训练具有代理能力的模型）的可操作见解。我们将向社区发布DataMind-12K和DataMind-7B、14B模型，以支持未来的研究。",
                    "inspiration_trace": "# DATA MIND方法论逻辑链推演\n\n## 1. 宏观问题：开源数据分析智能体的局限性\n\n**观察**：当前数据分析智能体领域存在明显的不平衡。专有模型通过提示工程或多智能体框架在数据分析任务上表现良好，但开源模型在处理现实世界数据分析需求时存在显著差距。\n\n**核心问题**：开源模型难以应对多样化格式、大规模数据文件和长视野、多步推理，而这些正是实际数据分析场景的核心需求。\n\n## 2. 问题分解：三大关键挑战\n\n### 挑战一：数据资源不足\n**观察**：训练高质量数据分析智能体需要大规模、多样化的任务及解决方案轨迹，但现有公开基准仅提供有限的测试集，缺乏详细的逐步轨迹注释。\n\n**问题本质**：开源社区缺乏可用于训练的高质量数据分析任务轨迹数据集，无法支撑通用数据分析智能体的训练需求。\n\n### 挑战二：训练策略不当\n**观察**：现有智能体训练遵循\"SFT-then-RL\"范式，但在数据分析这一新场景中，如何稳定长视野训练以及平衡SFT与RL的权重分配尚不明确。\n\n**问题本质**：缺乏针对数据分析智能体特点的优化训练策略，导致训练不稳定或效果不佳。\n\n### 挑战三：不稳定的基于代码的多轮展开\n**观察**：数据分析涉及大量代码执行和文件I/O操作，在有限内存资源下，并行智能体展开和多轮代码生成容易导致环境崩溃。\n\n**问题本质**：缺乏内存友好且稳定的代码执行环境，阻碍了大规模训练和部署。\n\n## 3. 假设形成：解决方案的初步构想\n\n### 假设一：数据合成假设\n**假设**：如果能设计一个自动化流程，生成多样化、高质量的数据分析任务及解决方案轨迹，就能解决训练数据不足的问题。\n\n**推论**：通过细粒度任务分类和递归任务组合机制，可以合成覆盖广泛领域和难度梯度的数据分析任务。\n\n### 假设二：动态训练假设\n**假设**：如果能动态调整SFT与RL的权重比例，就能在训练初期充分利用专家知识，后期鼓励模型探索，从而稳定训练并提升性能。\n\n**推论**：SFT可以作为训练的稳定器，而RL则有助于模型发现新的推理模式，两者动态结合能达到最佳效果。\n\n### 假设三：稳定执行假设\n**假设**：如果能优化代码执行环境，实现异步交互和内存高效管理，就能解决多轮代码展开的稳定性问题。\n\n**推论**：通过分块代码维护和严格的环境隔离，可以在有限资源下实现大规模并行训练。\n\n## 4. 方法设计：DATA MIND的构建\n\n### 组件一：数据合成与查询生成\n**实现路径**：\n1. 构建细粒度任务分类法（18个类别），确保查询多样性\n2. 设计递归的从易到难任务组合机制，通过链接多个任务类型逐步提升难度\n3. 从互联网和开放社区收集多样化数据文件，并进行严格筛选\n\n**创新点**：通过任务分类和递归组合，系统性地增加了合成查询的多样性和难度，覆盖了现实世界数据分析的复杂需求。\n\n### 组件二：专家轨迹采样与过滤\n**实现路径**：\n1. 引入知识增强的轨迹采样策略，使用高级工作流引导模型生成轨迹\n2. 采用自洽性过滤，通过多个独立轨迹的一致性验证确保答案质量\n3. 应用基于规则的过滤（格式合规性、长度控制、语言完整性）\n\n**创新点**：自洽性过滤确保了轨迹质量，而反馈机制使模型能够从失败中学习，不断改进推理路径。\n\n### 组件三：动态训练目标\n**实现路径**：\n1. 结合SFT损失和RL损失，使用动态系数γ调度两者权重\n2. 设计无效轮次过滤机制，屏蔽包含无效步骤的轨迹\n3. 实现冷启动策略，为RL训练提供良好初始状态\n\n**创新点**：动态平衡SFT与RL，在训练初期侧重知识吸收，后期鼓励探索，解决了传统训练策略的稳定性问题。\n\n### 组件四：内存友好的多轮展开框架\n**实现路径**：\n1. 异步交互：解耦模型生成和代码执行，避免资源争用\n2. 分块代码维护：仅生成当前步骤所需代码，减少内存占用\n3. 安全控制：隔离执行环境，限制资源使用，确保稳定性\n\n**创新点**：通过异步交互和分块维护，在有限资源下实现了高效稳定的多轮代码执行。\n\n## 5. 实验验证：假设检验与洞察发现\n\n### 验证一：数据合成效果\n**结果**：成功构建DATA MIND-12K高质量轨迹集，覆盖多样化任务类别和数据格式。\n**结论**：数据合成假设得到验证，自动化生成的数据集质量足以支持高性能智能体训练。\n\n### 验证二：动态训练策略效果\n**结果**：DATA MIND-14B在多个基准上超越专有模型，平均得分71.16%。\n**结论**：动态训练假设得到验证，SFT与RL的动态平衡确实能提升训练稳定性和模型性能。\n\n### 验证三：稳定执行框架效果\n**结果**：成功实现大规模并行训练，环境崩溃率显著降低。\n**结论**：稳定执行假设得到验证，内存友好的多轮展开框架确实能解决代码执行稳定性问题。\n\n### 关键洞察\n1. **自洽性过滤比最佳轨迹选择更重要**：轨迹多样性比单一\"最佳\"路径更能提升模型推理能力。\n2. **SFT损失的双面性**：既是RL训练的稳定器，也可能是过度拟合的源头。\n3. **RL的局限性**：能缩小不同基础模型间的差距，但难以逆转其固有能力排序。\n\n## 6. 结论：从问题到解决方案的完整逻辑链\n\nDATA MIND的提出源于对开源数据分析智能体局限性的观察，通过分解为三大核心挑战（数据资源不足、训练策略不当、执行环境不稳定），形成三个关键假设（数据合成假设、动态训练假设、稳定执行假设），最终设计出四个相互支撑的解决方案组件（数据合成与查询生成、专家轨迹采样与过滤、动态训练目标、内存友好的多轮展开框架）。\n\n这一逻辑链展现了从宏观问题到具体解决方案的系统思考过程，不仅解决了开源数据分析智能体的关键问题，还为社区提供了可复现的训练框架和有价值的经验洞察。DATA MIND的成功验证了\"数据+算法+工程\"协同创新的强大力量，为构建更通用的AI智能体提供了重要参考。"
                },
                {
                    "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes",
                    "arxiv_id": "2509.24945",
                    "authors": "Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra",
                    "summary": "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是探索并挑战了关于语言模型推理能力的两个假设：(1)推理能力只在足够大的模型中出现，(2)这种能力需要在大规模数据集上训练。论文通过精心策划和重采样数据集，证明了可以用更少的数据（约2T tokens）在小型语言模型（sub-billion-parameter）上实现强大的推理能力。这完全符合研究目标，因为论文的本质是改进LLM的基础推理能力，提出新的训练范式（数据策划和重采样），增强其通用推理能力。论文在多个推理基准上测试了模型性能，如AIME数学推理测试，证明其方法的有效性。论文没有将LLM作为工具应用于特定领域，也没有关注多模态、特定应用或模型可靠性等排除标准中的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在探索sub-billion参数语言模型的推理能力极限。针对大规模数据训练的假设，我们提出了一种benchmark-free、自我进化的数据优化方法，通过精心策划和重采样约2T tokens的高质量开源数据，开发了MobileLLM-R1系列模型。在GSM8K、HumanEval、MATH、AIME和LCBv6等多个推理benchmark上，通过accuracy和pass@1等指标验证了其有效性，MobileLLM-R1-950M在AIME上取得15.5分，远超同类开源模型，且仅用Qwen3训练数据的11.7%就达到相当性能。",
                    "summary_translation": "大型语言模型(large language models, LLMs)从本能反应到思维链(chain-of-thought, CoT)推理的范式转变，助长了两个普遍假设：(1)推理能力仅在足够大的模型中才会出现，(2)这种能力需要在海量数据集上进行训练。尽管第一个假设已经被最近的十亿参数级以下的推理模型所挑战，如Qwen3-0.6B和DeepSeek蒸馏变体，但第二个假设在很大程度上仍未受到质疑。在这项工作中，我们重新审视了推理能力出现所需扩展到极大规模语料库(>10T tokens)的必要性。通过精心筛选和重新采样我们根据设计的指标确定为有益的开源数据集，我们证明了强大的推理能力可以用少得多的数据出现。具体而言，我们表明仅约2T tokens的高质量数据就足够了，并且在这约2T tokens重新采样的数据集上进行4.2T tokens的预训练，随后采用既定的后训练(post-training)程序，使得MobileLLM-R1的开发成为可能，这是一系列十亿参数级以下的推理模型，其性能显著优于在完全开源数据上训练的先前模型。例如，MobileLLM-R1-950M在AIME评分上达到15.5，而OLMo-2-1.48B仅为0.6，SmolLM-2-1.7B仅为0.3。值得注意的是，尽管与Qwen3专有的36T-token预训练语料库相比，MobileLLM-R1-950M仅使用了11.7%的tokens进行训练，但它在多个推理基准测试(benchmarks)中匹配或超越了Qwen3-0.6B。为了促进这一方向的进一步研究，我们已经发布了完整的训练方案、数据来源、数据混合比例和模型检查点(model checkpoints)，以及在整个研究过程中获得的关键见解。",
                    "inspiration_trace": "# 从宏观问题到方法论：MobileLLM-R1的逻辑演进\n\n## 宏观问题：如何在资源受限设备上实现强大的语言模型推理能力？\n\n### 观察与背景分析\n- **现实需求**：大型语言模型(LLMs)已展示显著推理能力，但给资源受限设备带来压力；未来个人助手、智能家居和机器人将依赖设备上推理处理复杂任务。\n- **技术瓶颈**：长上下文推理加剧内存使用，KV缓存增长急剧增加内存占用。\n- **主流假设**：(1)推理能力只在足够大模型中出现；(2)这种能力需要在大规模数据集(>10T tokens)上训练。\n\n### 挑战主流假设\n- **假设1已被部分挑战**：Qwen3-0.6B和DeepSeek蒸馏变体等亚十亿参数推理模型表明小模型也能具备推理能力。\n- **假设2仍存疑**：推理能力是否真的需要极大规模数据集？这一问题尚未得到充分探索。\n\n### 核心研究问题聚焦\n在严格容量限制下，如何最有效地赋予小型推理模型强大能力并释放其隐藏潜力？更基础地，如何首先赋予预训练模型推理的潜在能力？\n\n### 关键观察与洞见\n1. **小模型的敏感性**：小型语言模型对数据质量极为敏感，噪声容易淹没其有限容量。\n2. **知识编码冲突**：模型缩小时，神经元必须编码更多重叠知识，增加干扰和冲突风险。\n3. **推理能力本质**：推理能力出现可理解为标记概率空间的系统性转变——从一般用途LLM到推理专门化模型的转变。\n\n### 形成假设\n1. **假设1**：通过精心策划的高质量数据，可以在小规模模型中培养强大推理能力。\n2. **假设2**：不需要极大规模数据集，较少的高质量数据(约2T tokens)足以实现强大推理能力。\n3. **假设3**：数据策划和混合策略对小模型推理能力培养至关重要。\n\n### 方法论形成\n\n#### 1. 训练课程设计\n借鉴教育心理学原理，设计完整训练课程，从一般知识到专业推理能力逐步引导模型：\n\n- **预训练阶段**：接触多样化语料库，建立语言和世界知识基础，同时获得基本数学和推理能力。\n- **中期训练阶段**：战略性地将数据分布转向推理丰富领域(数学、编码、结构化问题解决)。\n- **后训练阶段**：通过监督微调(SFT)使模型与人类偏好行为保持一致。\n\n#### 2. 数据策划与优化\n- **无基准自我进化数据优化**：利用跨领域影响定制数据混合，无需暴露基准数据。\n- **数据-模型共同进化策略**：适应中期训练过程中模型容量的快速变化。\n- **分层拒绝采样**：结合FineWeb-Edu分类器和Ask-LLM范式，构建代表性数据子集。\n- **影响力分析**：使用留一法(LOO)评估数据集贡献，利用影响力分数指导标记重新加权。\n\n#### 3. 影响力计算与应用\n- **自我影响力与交叉影响力**：计算训练样本对验证损失的影响，量化与目标能力的连接强度。\n- **数据集级别加权**：根据影响力分数为每个数据集分配采样权重，优先选择对多种能力有积极贡献的数据集。\n- **迭代压缩**：在中期训练中动态移除负影响力样本，调整数据采样比例，直到大多数样本影响力为零或负值。\n\n### 实验验证与发现\n- **模型规模**：在140M、360M、950M参数模型上验证方法。\n- **比较基准**：与完全开源模型(OLMo、SmolLM)和部分开源模型(Qwen、Gemma、LLaMA)比较。\n- **关键发现**：\n  - MobileLLM-R1-950M仅使用Qwen3的11.7%训练token，匹配或超过Qwen3-0.6B在多个推理基准的表现。\n  - 在MATH准确性上比Olmo 1.24B高5倍，比SmolLM2 1.7B高2倍。\n  - 证明了数据质量和策划比数据规模更重要。\n\n### 结论与启示\n提出了以数据为中心的框架，在有限参数和标记情况下最大化小型语言模型推理能力。通过数据质量、标记效率和原则性数据策划，成功在小模型中实现强大推理能力，挑战了推理能力需要极大数据集的主流假设，为未来研究提供了完全开源的训练方案。"
                },
                {
                    "title": "Circuit Distillation",
                    "arxiv_id": "2509.25002",
                    "authors": "Somin Wadhwa, Silvio Amir, Byron C. Wallace",
                    "summary": "Model distillation typically focuses on behavioral mimicry, where a student model is trained to replicate a teacher's output while treating its internal computations as a black box. In this work we propose an alternative approach: Distilling the underlying computational mechanisms implemented by a teacher model. Specifically, we propose circuit distillation, which introduces an objective to align internal representations between analogous circuit components in teacher and student models. We propose a method to match ``functionally correspondent'' circuit components and introduce a loss reflecting similarities between the representations that these induce. We evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks using models from the Llama3 family. Our results demonstrate that circuit distillation outperforms standard distillation, successfully transferring algorithmic capabilities by adjusting only a small, targeted subset of student model parameters. This work establishes the feasibility of transferring mechanisms, which may in turn allow for efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的模型蒸馏方法——\"电路蒸馏\"(Circuit Distillation)，其核心目标是转移模型内部的计算机制而非仅仅模仿输出行为。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，因为它关注如何通过理解和转移模型内部的计算机制来提升模型能力，特别是算法能力的转移。 其次，从正面指标分析，论文明确使用了Llama3模型家族进行评估，包含了\"Large language models, LLMs\"这一核心概念。同时，论文在实体跟踪(entity tracking)和心理理论(ToM)任务上进行了评估，这两个任务都与推理能力密切相关，尤其是心理理论(ToM)涉及高级认知推理能力，属于\"reasoning\"能力方向。 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 最后，在特殊和模糊情况处理上，论文提到了\"interpretable and controllable internal student mechanisms\"，这涉及到模型的可解释性，属于通过增强模型内在可解释性来提升模型通用推理质量的情况，符合保留标准。 论文的核心贡献是提出了一种新的蒸馏方法，通过匹配和转移模型内部的\"电路\"组件，成功实现了算法能力的转移，这种方法有望通过可解释和可控的内部机制来高效蒸馏目标能力，这与提升大语言模型的通用推理能力直接相关。",
                    "summary2": "本文旨在解决传统模型蒸馏仅关注行为模仿而忽视内部计算机制转移的问题。针对实体跟踪和心理理论任务，我们提出了一种电路蒸馏方法，通过匹配功能对应的电路组件并引入表示相似性损失函数来对齐教师和学生模型的内部计算机制，并在Llama3系列模型上通过任务准确率和电路性能指标验证了其有效性。",
                    "summary_translation": "模型蒸馏（model distillation）通常专注于行为模仿（behavioral mimicry），即训练学生模型（student model）复制教师模型（teacher model）的输出，同时将其内部计算视为黑盒（black box）。在本研究中，我们提出了一种替代方法：蒸馏教师模型所实现的基础计算机制。具体而言，我们提出了电路蒸馏（circuit distillation），该方法引入了一个目标，用于对齐教师模型和学生模型中类似电路组件（circuit components）之间的内部表征。我们提出了一种匹配\"功能对应\"（functionally correspondent）电路组件的方法，并引入了一种损失函数（loss），反映这些组件所诱导的表征之间的相似性。我们使用Llama3系列的模型，在实体跟踪（entity tracking）和心理理论（theory of mind, ToM）任务上评估了电路蒸馏的效果。我们的结果表明，电路蒸馏优于标准蒸馏，通过仅调整学生模型参数中的一小部分目标子集，就能成功地转移算法能力。这项工作证明了转移机制的可行性，这反过来可能通过可解释和可控的学生模型内部机制，实现针对教师模型特定能力的高效蒸馏。",
                    "inspiration_trace": "# Circuit Distillation 的逻辑演进：从宏观问题到方法论创新\n\n## 1. 宏观问题：模型蒸馏的根本局限\n\n模型蒸馏旨在将大型教师模型的能力转移到小型学生模型中，以提高效率。然而，传统蒸馏方法存在一个根本性瓶颈：学生模型只能从教师输出中学习，必须自行推断如何执行任务。这种\"行为模仿\"范式限制了知识转移的深度和效率。\n\n## 2. 关键观察：内部机制的缺失\n\n作者观察到传统蒸馏方法忽视了教师模型的内部计算机制。尽管这些方法能有效转移任务特定知识，但无法传递教师模型内部的算法实现。这引发了一个核心问题：如果能让学生学习教师模型的内部算法，而非仅仅模仿输出，是否能实现更有效的知识转移？\n\n## 3. 洞察与假设：机械可解释性的启示\n\n机械可解释性领域的最新进展揭示了Transformer模型内存在人类可理解的算法子图，称为\"电路\"(circuits)。这些电路是模型实现特定功能的可识别组件。基于此，作者提出了核心假设：\n\n**通过在组件级别强制功能对齐，不仅可以蒸馏教师的知识，还可以蒸馏其内部算法。**\n\n如果学生模型能够模仿教师模型中特定电路的计算机制，那么它可能会更有效地学习任务，并且可能只需要调整更少的参数。\n\n## 4. 技术挑战与解决路径\n\n要实现这一假设，必须解决两个关键技术挑战：\n\n### 挑战1：不同规模模型间的组件对应关系\n\n如何在不同大小的教师和学生模型之间建立功能上对应的组件关系？\n\n**解决方案：基于消融影响的映射策略**\n- 测量每个组件(注意力头)移除时对任务性能的影响\n- 将具有相似功能重要性的组件进行映射\n- 这种基于功能而非结构的方法适用于不同规模的模型\n\n具体实现：\n- 计算学生头hs的消融影响：ΔPs(hs) = Ps_base - Ps_abl(hs)\n- 计算教师头ht的消融影响：ΔPt(ht) = Ps_base - Pt_abl(ht)\n- 映射最小化|ΔPs(hs) - ΔPt(ht)|的头对\n\n### 挑战2：表示对齐的度量与优化\n\n如何设计目标函数来强制对应电路之间的表示对齐？\n\n**解决方案：中心核对齐(CKA)损失**\n- CKA对正交变换和各向同性缩放不变，适合比较不同架构的表示\n- 通过测量表示诱导的相似性模式来比较功能相似性\n- 将CKA转化为损失项：L_CKA(Ks, Kt) = 1 - CKA(Ks, Kt)\n\n## 5. 方法论形成：电路蒸馏框架\n\n结合上述解决方案，作者提出了\"电路蒸馏\"的完整框架：\n\n1. **识别目标电路**：利用已有的机械可解释性方法(如路径补丁)识别教师模型中与任务相关的电路组件\n\n2. **建立组件映射**：使用消融影响相似性将学生组件映射到教师组件\n\n3. **复合损失函数**：\n   L_total = L_task(y, ŷ_s) + λ Σ L_CKA(K_s^(c), K_t^(c))\n   \n   其中：\n   - L_task是传统任务损失\n   - L_CKA是CKA-based电路相似性损失\n   - λ平衡任务性能和机械对齐目标\n\n4. **参数高效训练**：仅调整映射到的电路组件参数，而非整个模型\n\n## 6. 实验验证与效果确认\n\n作者在两个认知任务上验证了电路蒸馏的有效性：\n\n1. **实体追踪任务**：测试模型追踪和更新实体信息的能力\n2. **心理理论(ToM)任务**：测试模型在部分可观察条件下对信念进行推理的能力\n\n实验结果表明：\n- 电路蒸馏优于标准蒸馏方法\n- 通过仅调整11-15%的注意力头参数，成功转移了算法能力\n- 电路蒸馏不仅提高了性能，还提供了效率增益和收敛速度提升\n\n## 7. 理论贡献与实践意义\n\n电路蒸馏的贡献在于：\n\n1. **概念创新**：将蒸馏焦点从输出行为转向内部机制\n2. **方法创新**：提供了组件功能映射和表示对齐的具体方法\n3. **效率提升**：实现了参数高效的知识转移\n4. **可解释性增强**：产生内部计算更可控的学生模型\n\n这一工作为构建更高效、更可解释的模型提供了新思路，证明了不仅知识可以蒸馏，产生知识的算法也可以蒸馏。"
                },
                {
                    "title": "Expanding Computation Spaces of LLMs at Inference Time",
                    "arxiv_id": "2509.24884",
                    "authors": "Yoonna Jang, Kisu Yang, Isabelle Augenstein",
                    "summary": "Chain-of-thought (CoT) rationale enables language models to use additional task-related text for problem-solving, benefiting not only from detailed reasoning steps but also from the expanded computational space of longer inputs. Prior work has trained filler or special tokens to serve as additional computation spaces. In this study, we investigate whether language models can leverage artificially inserted sequences of filler tokens solely at inference. We first identify effective token types, numbers, and insertion locations, then examine at what stage of training models begin to exploit the expanded computation space, and finally analyze dynamics within these spaces via attention maps. Experiments on models ranging from 1.7B to 32B across open-domain QA and math tasks show that appropriate token types and counts vary, but placing filler tokens directly before the final 'Answer:' token is most effective. Smaller models benefit most, up to 12.372 percentage points in SmolLM2-1.7B-Instruct, indicating that these spaces act as additional computational capacity rather than redundant input. Attention maps reveal that expanded spaces often continue the original attention mechanism and sometimes focus on questions or answer options, suggesting meaningful computation for problem-solving.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，其核心是改进大语言模型的基础推理能力。论文提出在推理时通过插入填充令牌来扩展LLM的计算空间，这是一种增强模型通用问题解决能力的新方法。从核心判断来看，论文本质上是研究如何通过扩展计算空间来提升LLM的推理能力，类似于思维链(CoT)的扩展，属于改进LLM基础能力的研究。从正面指标看，论文明确涉及LLMs核心概念，并在开放域问答和数学任务上验证了推理能力提升。论文不符合任何排除标准，它不关注多模态、特定应用领域或模型可靠性等排除领域。虽然论文没有涉及智能体、工具使用等新兴范式，但其核心贡献——通过扩展推理时的计算空间来增强LLM的通用推理能力——直接与研究目标\"提高大语言模型的通用推理能力\"高度一致。实验表明这种方法特别有利于提升小模型的推理能力，进一步证明其通用价值。",
                    "summary2": "本文旨在探索语言模型在推理时如何利用额外计算空间提升性能。针对多种规模的LLMs，我们提出了一种在输入中插入填充标记（如空格、句号等）扩展计算空间的方法，并在MMLU、ARC、GSM8K和MATH-500数据集上通过准确率和精确匹配率验证了其有效性。实验表明，该方法特别使小型模型（如SmolLM2-1.7B）性能提升达12.372个百分点，且无需额外训练。",
                    "summary_translation": "思维链（Chain-of-thought, CoT）原理使语言模型能够使用额外的任务相关文本来解决问题，不仅受益于详细的推理步骤，还受益于更长输入所带来的扩展计算空间。先前的研究已经训练了填充符（filler）或特殊标记（tokens）作为额外的计算空间。在本研究中，我们探讨了语言模型是否能够仅在推理阶段利用人工插入的填充符序列。我们首先确定了有效的标记类型、数量和插入位置，然后检查模型在训练的哪个阶段开始利用扩展的计算空间，最后通过注意力图（attention maps）分析这些空间内的动态变化。在开放域问答（open-domain QA）和数学任务上，对从1.7B到32B规模模型的实验表明，适当的标记类型和数量各不相同，但将填充符直接放在最终的'Answer:'标记前最为有效。较小的模型获益最大，在SmolLM2-1.7B-Instruct模型中提升高达12.372个百分点，表明这些空间充当的是额外的计算能力，而非冗余输入。注意力图（attention maps）显示，扩展的空间通常会延续原始的注意力机制，有时会聚焦于问题或答案选项，这表明这些空间进行了有意义的问题解决计算。",
                    "inspiration_trace": "# 从思维链到计算空间扩展：LLM推理能力提升的逻辑推演\n\n## 一、宏观问题起点：思维链有效性的本质探究\n\n**观察现象**：思维链(CoT)提示显著提升了语言模型的推理性能，使模型能够逐步分解问题并显式化推理轨迹。\n\n**核心疑问**：CoT的有效性究竟来源于什么？是详细的推理步骤内容，还是仅仅是更长的输入提供了更大的计算空间？\n\n**前期研究基础**：已有工作通过训练模型使用特殊标记或填充字符作为额外计算空间，证实了扩展空间的价值。但这些方法都需要在预训练或微调阶段进行专门训练。\n\n## 二、关键假设形成：无需训练的计算空间扩展\n\n**假设提出**：当前的语言模型可能能够在无需专门训练的情况下，仅通过在推理时插入填充标记来利用扩展的计算空间。\n\n**假设依据**：\n1. 模型在训练过程中已经接触过各种常见标记（如句号、空格等）\n2. 初步研究表明，即使没有特定形式的训练，提供额外标记作为扩展计算空间也能增强模型性能\n\n**研究聚焦**：将宏观问题具体化为三个研究问题：\n- RQ1: 什么类型和数量的标记有效，插入在何处最有效？\n- RQ2: 模型在训练的哪个阶段开始利用扩展计算空间？\n- RQ3: 扩展的标记空间如何与原始输入交互并影响答案预测？\n\n## 三、方法论构建：系统探索计算空间扩展\n\n### 1. 填充标记选择\n**逻辑**：选择模型在训练语料中自然见过的标记，确保模型熟悉这些标记\n**实施**：选择六种字符作为填充标记：空格(' ')、换行符('\\n')、制表符('\\t')、句号('.')、<pad>标记、破折号('-')\n\n### 2. 实验设计\n**变量控制**：\n- **数量维度**：从16到8192个填充标记，探索\"多少是足够，多少是过多\"\n- **位置维度**：在\"Answer:\"标记前后插入，确定最佳位置\n- **模型维度**：选择1.7B到32B参数范围的模型，探索模型大小的影响\n- **任务维度**：选择开放域问答(MMLU、ARC)和数学任务(GSM8K、MATH-500)\n\n### 3. 分析方法\n**多层次验证**：\n- **性能评估**：测量准确率变化，量化效果\n- **训练阶段分析**：通过中间检查点追踪能力发展\n- **注意力机制分析**：通过注意力图揭示内部计算过程\n\n## 四、实验发现与理论完善\n\n### 1. 基础发现\n**现象确认**：模型确实可以利用填充标记作为扩展计算空间，无需明确训练\n**关键证据**：SmolLM2-1.7B-Instruct模型性能提升高达12.372个百分点\n\n### 2. 优化条件发现\n**标记类型与数量**：\n- 不同模型有不同最优标记类型（如SmolLM在句号上表现最佳）\n- 小模型能从更多标记中受益（最多256个），而大模型受益较少\n- 超过1024个标记会导致性能下降，出现\"lost-in-the-middle\"现象\n\n**插入位置**：\n- 将填充标记直接放在\"Answer:\"标记前效果最佳\n- 放在\"Answer:\"后会导致模型预测更多填充标记而非答案\n\n### 3. 机制解释\n**计算空间本质**：\n- 扩展空间不是冗余输入，而是提供额外计算容量\n- 小模型受益更多，表明这些空间补偿了其有限的参数容量\n\n**注意力模式分析**：\n- 扩展空间继续原始注意力机制\n- 填充标记有时直接关注问题或答案选项，参与有意义计算\n- 在中间层，填充标记参与解释问题和选项；在后期层，影响答案选择\n\n### 4. 能力发展轨迹\n**训练阶段分析**：\n- 模型在预训练后期逐渐获得利用扩展空间的能力\n- 指令微调阶段进一步增强了这种能力\n- 表明这是一种自然出现的能力，不需要专门训练\n\n## 五、理论贡献与实践意义\n\n### 1. 核心理论贡献\n**计算空间扩展理论**：证明了语言模型可以利用推理时插入的填充标记作为额外计算空间，这种空间不是简单冗余，而是进行有意义计算的容量。\n\n**能力发展理论**：模型在预训练过程中自然获得利用扩展计算空间的能力，无需专门训练。\n\n### 2. 实践意义\n**小模型增强策略**：为资源受限的小模型提供了一种简单有效的性能提升方法\n**推理优化思路**：开辟了通过扩展计算空间而非仅增加参数来提升模型性能的新途径\n**模型理解深化**：通过注意力分析，揭示了模型内部计算过程的更多细节\n\n这一研究从对思维链有效性的本质探究出发，通过假设无需训练即可扩展计算空间，系统设计实验验证，最终形成了完整的理论框架，为理解和提升语言模型推理能力提供了新视角。"
                },
                {
                    "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space",
                    "arxiv_id": "2509.24771",
                    "authors": "Guibin Zhang, Fanci Meng, Guancheng Wan, Zherui Li, Kun Wang, Zhenfei Yin, Lei Bai, Shuicheng Yan",
                    "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the reasoning capabilities of Large Language Models (LLMs) during the inference phase without altering model parameters. However, existing TTS methods are largely independent, implying that LLMs have not yet evolved to progressively learn how to scale more effectively. With the objective of evolving LLMs to learn ``how to scale test-time computation,'' we propose LatentEvolve, a self-evolving latent TTS framework inspired by the complementary learning system (CLS) theory. Analogous to the human brain's dual system of a fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve comprises two evolutionary components: \\textit{daytime scaling}, which rapidly retrieves historical latent representations to better guide current LLM reasoning; and \\textit{nighttime scaling}, which integrates past latent optimizations in a manner akin to the human brain's consolidation of experiences during sleep. The alternation of daytime and nighttime processes facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive dynamics in a fully unsupervised manner. Extensive experiments across eight benchmarks and five model backbones demonstrate that our LatentEvolve surpasses state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and exhibits exceptional cross-domain and cross-backbone generalization.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出LatentEvolve，一种自我进化的潜在测试时扩展(TTS)框架，旨在增强大语言模型的推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用推理能力。 从正面指标看，论文明确包含多个相关主题：核心概念方面直接研究Large Language Models (LLMs)；能力方向专注于reasoning能力的提升；训练方法涉及self-evolve（自我进化）这一创新范式。论文在八个基准测试上验证了其方法的有效性，表明其关注的是通用推理能力而非特定领域应用。 论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它提出了一种受人类互补学习系统理论启发的通用框架，通过\"daytime scaling\"和\"nighttime scaling\"两个组件的交替，实现了LLM推理能力的自我进化，这种创新方法直接服务于提升LLM的通用推理能力这一核心目标。",
                    "summary2": "本文旨在解决LLMs在测试时计算扩展(TTS)中缺乏自我进化能力的问题。针对测试时推理任务，我们提出了一种LatentEvolve自我进化潜在空间框架，通过daytime scaling和nighttime scaling两个互补组件实现快速适应和慢速整合，并在8个基准测试和5个模型骨干上通过准确率等指标验证了其有效性，超越了现有SOTA方法最高达13.33%。",
                    "summary_translation": "测试时缩放（Test-time Scaling, TTS）已被证明能够显著增强大型语言模型（Large Language Models, LLMs）在推理阶段的推理能力，而无需改变模型参数。然而，现有的TTS方法大多是独立的，这意味着LLMs尚未进化到能够逐步学习如何更有效地进行缩放。为了使LLMs进化到能够学习\"如何缩放测试时计算\"（how to scale test-time computation），我们提出了LatentEvolve，这是一个受互补学习系统（complementary learning system, CLS）理论启发的自我进化潜在TTS框架。类似于人脑中快速回忆的海马体（hippocampus）和缓慢巩固的新皮层（neocortex）的双系统，LatentEvolve包含两个进化组件：\\textit{daytime scaling}（白天缩放），它快速检索历史潜在表示以更好地指导当前LLM推理；以及\\textit{nighttime scaling}（夜间缩放），它以类似于人脑在睡眠期间巩固经验的方式整合过去的潜在优化。白天和夜间过程的交替促进了LLM TTS的快速和缓慢进化，以完全无监督的方式反映了人类认知动态。在八个基准测试（benchmarks）和五个模型骨干（model backbones）上的广泛实验表明，我们的LatentEvolve超越了最先进的TTS方法，如LatentSeek和TTRL，最高提升达$13.33\\%$，并表现出卓越的跨领域和跨骨干泛化能力。",
                    "inspiration_trace": "# LatentEvolve核心方法的逻辑链推演\n\n## 1. 宏观问题识别：测试时间缩放的自进化困境\n\n作者首先观察到大型语言模型(LLMs)的测试时间缩放(TTS)领域存在一个根本性问题：\n> \"现有TTS方法大多是相互独立的，意味着LLMs尚未进化到能够逐步学习如何更有效地进行缩放。\"\n\n这一观察揭示了核心问题：**当前TTS方法无法从解决问题的经验中学习并进化，每个查询的推理过程被视为独立事件，缺乏跨任务的知识积累与能力提升机制**。这种独立性限制了TTS范式通过与环境的持续交互而逐步进化的潜力。\n\n## 2. 现有方法分析：独立处理的局限性\n\n作者系统分析了现有TTS方法的局限性：\n\n- **并行缩放**：生成多个候选响应并聚合，但成功选择策略不传递到未来任务\n- **顺序缩放**：迭代改进解决方案，但成功反思策略局限于特定实例\n- **混合与内部化缩放**：虽然更复杂，但仍缺乏经验积累机制\n\n关键发现是：**无论具体形式如何，现有TTS方法都缺乏\"自进化\"能力**，无法将解决一个问题的经验转化为提升未来问题解决效率的知识。\n\n## 3. 观察与启发：互补学习系统的启示\n\n作者从认知科学领域获得关键启发：\n> \"我们提出了LatentEvolve，一个受互补学习系统(CLS)理论启发的自进化潜在TTS框架。\"\n\nCLS理论描述了人类大脑的双系统学习机制：\n- **海马体**：快速学习特定情景记忆\n- **新皮层**：慢速整合这些经验为一般知识，尤其在睡眠期间\n\n这一观察为解决TTS自进化问题提供了新思路：**可以设计一个模拟人类认知系统的双阶段学习框架，实现快速适应与慢速整合的互补**。\n\n## 4. 核心假设：双阶段进化机制\n\n基于CLS理论，作者形成核心假设：\n\n> \"LatentEvolve通过双阶段进化运行：白天缩放用于快速的、情景性的适应；夜间缩放用于慢速的程序性整合。\"\n\n这一假设可表述为：**通过模拟海马体-新皮层互补系统，设计一个交替进行快速实例适应(白天)和慢速知识整合(夜间)的框架，可以实现LLMs在测试时间的自进化能力**。\n\n这一假设的关键创新点在于：**将原本独立的TTS过程转变为相互关联、经验积累的进化过程**。\n\n## 5. 方法论构建：从假设到具体实现\n\n作者将核心假设转化为具体的方法论框架：\n\n### 5.1 白天缩放：快速情景适应\n\n模拟海马体的快速记忆功能，实现三个关键机制：\n\n1. **关联检索**：维护情景缓冲区M，存储高质量经验三元组(上下文嵌入、初始潜在序列、优化潜在序列)，对新查询检索相关经验\n\n2. **知情潜在初始化**：不仅利用最终优化状态，更捕获从初始到优化的\"动量\"(Δzj = z∗j - zbase,j)，通过加权动量转移构建更优初始化\n\n3. **自监督细化与归档**：采用自奖励策略迭代优化潜在序列，将高置信度经验存入缓冲区\n\n### 5.2 夜间缩放：慢速知识整合\n\n模拟新皮层在睡眠中的知识整合功能：\n\n1. **潜在编织者**：引入小型LLM作为知识整合模型，学习从上下文和初始状态预测优化潜在序列\n\n2. **经验回放整合**：定期使用缓冲区中的高置信度经验更新编织者参数，将分散经验整合为程序知识\n\n### 5.3 双阶段进化循环\n\n将白天和夜间缩放整合为循环进化过程：\n- 每个查询先通过潜在编织者获得改进初始化\n- 白天缩放进行快速适应并存储经验\n- 定期触发夜间缩放整合经验\n- 形成交替变换：(M, Wψ) → M' → (M', W'ψ)\n\n## 6. 验证与完善：实验驱动的迭代\n\n作者通过多维度实验验证和完善方法：\n\n1. **性能验证**：在八个基准测试和五个模型骨干上验证，LatentEvolve超越现有方法高达13.33%\n\n2. **泛化能力验证**：证明跨域和跨骨干泛化能力，如MATH上的训练可提升GPQA和JAMA性能\n\n3. **持续学习验证**：证明在新域上学习不会降低原域性能，甚至可提升\n\n4. **组件必要性验证**：通过消融研究证明白天和夜间组件的互补必要性\n\n5. **参数敏感性分析**：研究关键参数如潜在维度L'和进化周期T的影响，优化框架配置\n\n## 逻辑链总结\n\nLatentEvolve的核心逻辑链展现了从宏观问题到创新解决方案的完整思考路径：\n\n**问题观察** → **跨领域启发** → **核心假设** → **方法论构建** → **实验验证**\n\n这一路径从\"TTS方法缺乏自进化能力\"的宏观问题出发，通过人类认知系统的CLS理论获得启发，形成\"双阶段进化可实现TTS自进化\"的核心假设，进而构建白天缩放(快速适应)与夜间缩放(慢速整合)的具体方法论，最终通过广泛实验验证其有效性、泛化能力和持续学习潜力。\n\n这一创新不仅解决了TTS领域的自进化困境，更为LLMs实现类似人类认知的持续学习能力提供了新路径。"
                },
                {
                    "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models",
                    "arxiv_id": "2509.24781",
                    "authors": "Jun Rao, Yunjie Liao, Xuebo Liu, Zepeng Lin, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang",
                    "summary": "Existing alignment methods for preference optimization of large language models (LLMs) aim to enhance model performance by utilizing pairs of positive and negative samples. However, due to the limited capacity of models in scoring or generating responses, the quality of positive and negative samples may become similar during training, which complicates optimization for preference learning. To address this issue, we introduce SeaPO, a Strategic Error Amplification method that leverages three error types commonly occurring in LLMs to introduce specific error patterns into the model Preference Optimization. This strategy ensures that negative samples are more erroneous than positive samples and preference-based training is employed to mitigate the occurrence of these errors, thereby enhancing model performance. Evaluations across five capability dimensions and different model scales (1.5B to 14B) demonstrate that the generated data significantly improved overall model performance, particularly in terms of truthfulness, with improvements of 5-10 percentage points observed. Further analysis reveals that task performance varies depending on the error types introduced. Injecting the most common error types improves performance in related tasks, while a mix of error types leads to a broader performance enhancement: most tasks show stable improvements, while a few tasks exhibit significant gains.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出SeaPO（Strategic Error Amplification）方法，用于大语言模型的偏好优化。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础能力，提出了一种新的训练范式来增强模型的偏好学习。它通过策略性地放大错误来确保负样本比正样本更有错误性，从而提高模型的整体性能和真实性，这属于改进LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念明确聚焦于大语言模型(LLMs)；训练方法涉及偏好优化，这与强化学习人类反馈(RLHF)相关；虽然未直接提及reasoning等术语，但提高模型真实性是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，虽然论文没有涉及智能体/工具使用，但它关注于通过新方法提高模型的真实性，这与减少幻觉相关，属于提升模型内在可靠性和推理质量的方法，符合研究目标。 综上所述，SeaPO论文致力于通过改进训练范式来提高LLM的通用推理能力，特别是真实性方面，因此符合研究范围。",
                    "summary2": "本文旨在解决大型语言模型偏好优化中正负样本质量相似导致的优化困难问题。针对LLMs在评分或生成响应时的能力限制，我们提出了一种SeaPO策略性错误放大方法，通过引入三种常见错误类型（正确性、逻辑和幻觉）来构造负样本，并在多个能力维度和不同模型规模（1.5B到14B）上通过准确率等指标验证了其有效性，尤其在真实性任务上实现了5-10个百分点的性能提升。",
                    "summary_translation": "现有的针对大型语言模型(LLMs)偏好优化的对齐方法旨在通过利用正负样本对来提高模型性能。然而，由于模型在评分或生成响应方面的能力有限，正负样本的质量在训练过程中可能变得相似，这使偏好学习的优化变得复杂。为解决这一问题，我们提出了SeaPO，一种策略性错误放大(Strategic Error Amplification)方法，该方法利用大型语言模型中常见的三种错误类型，在模型偏好优化(Preference Optimization)中引入特定的错误模式。这一策略确保负样本比正样本包含更多错误，并采用基于偏好的训练来减轻这些错误的发生，从而提高模型性能。在五个能力维度和不同模型规模(1.5B至14B)上的评估表明，生成的数据显著提高了整体模型性能，特别是在真实性(truthfulness)方面，观察到5-10个百分点的提升。进一步分析显示，任务性能根据引入的错误类型而有所不同。注入最常见的错误类型可提高相关任务的性能，而混合错误类型则带来更广泛的性能提升：大多数任务表现出稳定的改进，而少数任务则显示出显著增益。",
                    "inspiration_trace": "# SeaPO方法逻辑链分析：从问题观察到方法论形成\n\n## 一、宏观问题：LLM偏好优化的样本质量困境\n\n在大型语言模型(LLMs)的偏好优化过程中，研究者面临一个根本性挑战：**如何确保正负样本之间存在足够的质量差异以实现有效学习**。现有方法（如DPO、KTO等）依赖于高质量的正负样本对，但实际应用中存在一个悖论——如果模型本身能力有限，它可能难以生成或识别真正高质量的正负样本，导致两者质量相近，使偏好优化效果大打折扣。\n\n## 二、问题观察：现有方法的局限性\n\n作者通过深入分析，识别出三个关键观察：\n\n### 1. 模型能力限制导致样本质量模糊\n- 评分模型可能误判正负样本，特别是在复杂任务中\n- 拒绝采样产生的\"负样本\"有时可能与\"正样本\"质量相当，甚至更好\n- 这种质量模糊性使偏好学习信号变得微弱，模型难以区分\n\n### 2. 传统负样本构建方法效率低下\n- 需要多次采样和强大的评分模型来识别正负样本\n- 计算开销大，且对模型能力要求高\n- 缺乏对错误类型的明确控制，导致负样本多样性不足\n\n### 3. 现有研究关注点不平衡\n- 大多数工作专注于生成更正确的答案，忽视了提高错误样本的质量\n- 缺乏系统性分析和利用LLMs常见错误类型的研究\n\n## 三、核心假设：战略性错误的作用机制\n\n基于上述观察，作者提出了一系列假设，形成了SeaPO的理论基础：\n\n### 1. 错误类型可定义性假设\nLLMs的错误并非随机分布，而是可以被系统性地分类为几种主要类型。作者将这些错误定义为\"战略性错误\"(Strategic Errors)：\n- **正确性错误**：与事实准确性或计算相关的问题（如错误事实、计算错误）\n- **逻辑错误**：推理或论证存在缺陷（如矛盾、不支持的结论）\n- **幻觉错误**：生成完全捏造或虚假的信息\n\n### 2. 错误放大假设\n通过在负样本中故意引入这些特定类型的错误，可以：\n- 确保负样本比正样本更容易出错\n- 创建明确的学习信号，增强偏好学习效果\n- 减少对复杂评分模型的依赖\n\n### 3. 错误减少假设\n基于这些故意引入错误的负样本进行偏好优化训练，可以：\n- 减少模型在实际应用中产生这些类型错误的概率\n- 提高模型在相关任务上的表现\n- 实现更全面的性能提升\n\n## 四、方法论构建：SeaPO的设计逻辑\n\n基于上述假设，作者设计了SeaPO方法，包含两个核心组件：\n\n### 1. 错误注入的负样本生成\n**设计逻辑**：如果传统方法难以获取高质量负样本，为什么不主动构建具有明确错误特征的负样本？\n\n**实现方式**：\n- 定义战略性错误类型及其特征（如表1所示）\n- 使用目标模型本身对原始正确答案进行后编辑，故意注入指定比例的错误\n- 形式化表示：aerror = Injector(x, y, e)，其中e为特定错误类型\n\n**创新点**：不再依赖采样和评分模型获取负样本，而是通过明确的错误注入过程生成负样本，降低了计算开销和对强大模型的依赖。\n\n### 2. 错误聚焦的偏好优化\n**设计逻辑**：既然我们定义了特定错误类型，优化目标应该直接针对减少这些错误的发生概率。\n\n**实现方式**：\n- 选择KTO作为基础优化算法（因为它不需要匹配的数据对）\n- 优化目标：减少负样本概率，从而最小化引入的战略错误的发生概率\n- 数学表达：LKTO(πθ, πref) = Ex,y∼D[λy - v(rθ(x,y))]，其中rθ(x,y)是输出的奖励\n\n**创新点**：将优化目标明确指向减少特定错误类型，而非一般的偏好对齐。\n\n## 五、实验验证：假设检验与效果评估\n\n作者通过一系列实验验证了SeaPO的有效性和假设的正确性：\n\n### 1. 主要结果验证\n- SeaPO在不同模型规模（1.5B到14B）和任务上持续优于基线方法\n- 在错误丰富的数据集（如MATH和TruthfulQA）上表现尤为突出\n- 例如，Qwen2.5-7B在MATH上提高10个百分点，在TruthfulQA上提高12个百分点\n\n### 2. 分析实验深化理解\n- **错误类型影响分析**：不同错误类型对不同任务有不同影响，验证了错误类型与任务关联的假设\n- **错误注入质量分析**：生成的负样本确实比原始负样本包含更多错误，验证了错误放大的假设\n- **错误严重程度分析**：中等错误程度的样本效果最佳，说明错误需要可识别但不过度\n- **优化目标分析**：SeaPO在不同学习算法中均表现良好，证明方法的鲁棒性\n\n## 六、逻辑演进总结\n\nSeaPO方法的逻辑演进展现了一个清晰的问题解决路径：\n\n1. **从宏观问题出发**：识别LLM偏好优化中正负样本质量相似的根本挑战\n2. **深入观察现象**：分析现有方法的局限性，特别是负样本构建的问题\n3. **形成核心假设**：提出战略性错误的概念及其在偏好优化中的潜在作用\n4. **设计创新方法**：构建包含错误注入和错误聚焦优化的SeaPO框架\n5. **系统验证假设**：通过广泛的实验验证方法的有效性和假设的正确性\n\n这一逻辑链条不仅解决了一个具体的技术问题，还为LLM的偏好优化提供了一个新的思路：通过主动控制和利用错误，而非被动接受错误，来提升模型性能。这种方法论的创新在于将\"错误\"从需要避免的对象转变为可以利用的工具，体现了问题解决思维的转变。"
                },
                {
                    "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents",
                    "arxiv_id": "2509.24704",
                    "authors": "Guibin Zhang, Muxin Fu, Shuicheng Yan",
                    "summary": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \\textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \\textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，提出了一种新的记忆框架来增强LLM驱动智能体的推理能力。论文提出的MemGen框架通过\"记忆触发器\"和\"记忆编织器\"实现了记忆与推理的紧密交织，这属于增强LLM通用推理能力的方法论研究，特别是关于智能体框架和自我进化的研究方向。 其次，论文包含多个正面指标：明确涉及\"Large Language Model (LLM)-powered agents\"这一核心概念；关注推理能力的提升(\"enrich its reasoning\")；涉及自我进化(\"Self-Evolving Agents\")；并且研究LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容；没有专注于特定应用领域；也没有关注模型可靠性方面的应用问题。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体记忆框架，而非针对特定领域的应用。实验在\"八个基准测试\"上进行，展示了其跨领域泛化能力，进一步证明其通用性。 论文的核心贡献是提出了一种动态生成记忆框架，使LLM驱动的智能体能够通过记忆与推理的交织循环实现自我进化，自发演化出类似人类的记忆能力，从而提升通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决LLM agent记忆机制无法实现推理与记忆流畅交织的问题。针对agent与环境交互过程中的经验学习场景，我们提出了一种MemGen动态生成式记忆框架，通过memory trigger和memory weaver实现记忆与推理的紧密交织，并在8个基准测试上通过任务完成率和准确率等指标验证了其有效性。实验表明MemGen超越了现有外部记忆系统，表现出强大跨领域泛化能力，且能自发演化出类似人类的记忆层级结构。",
                    "summary_translation": "代理记忆塑造了大型语言模型（LLM）驱动的代理如何类似人脑那样通过环境交互逐步自我完善。现有范式仍然受限：参数化记忆（parametric memory）强制调整模型参数，而基于检索的记忆（retrieval-based memory）将经验外化到结构化数据库中，但两者都未能捕捉到人类认知背后推理和记忆的流动交织。为弥补这一差距，我们提出了MemGen，一种动态生成记忆框架，为代理配备了类似人类的认知能力。它由一个\\textit{记忆触发器}（memory trigger）和一个\\textit{记忆编织器}（memory weaver）组成，前者监控代理的推理状态以决定显式记忆调用，后者将代理的当前状态作为刺激，构建潜在令牌序列作为机器原生记忆来丰富其推理。通过这种方式，MemGen使代理能够在整个推理过程中回忆和增强潜在记忆，产生记忆与认知紧密交织的循环。在八个基准测试上的大量实验表明，MemGen比领先的外部记忆系统如ExpeL和AWM高出最多$38.22\\%$，超过GRPO最多$13.44\\%$，并展现出强大的跨领域泛化能力。更重要的是，我们发现没有显式监督的情况下，MemGen自发演化出不同的类似人类记忆能力，包括规划记忆（planning memory）、程序性记忆（procedural memory）和工作记忆（working memory），这表明机器认知正朝着更自然的形式涌现的轨迹。",
                    "inspiration_trace": "# MemGen核心方法的逻辑推演\n\n## 1. 宏观问题：智能体如何实现自我进化？\n\n论文从智能体自我进化的根本问题出发。智能体需要通过与环境的交互不断学习和改进，而记忆机制是这一过程的核心。正如人类大脑通过记忆不断 refine 自身能力，LLM驱动的智能体也需要类似的记忆机制来实现持续进化。\n\n> \"Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions.\"\n\n## 2. 现有范式的局限性分析\n\n作者系统分析了现有两种主要记忆范式及其根本缺陷：\n\n### 2.1 参数化记忆的困境\n- **工作原理**：通过直接更新模型参数内化经验\n- **核心局限**：灾难性遗忘(catastrophic forgetting)——新知识获取导致原有知识丢失\n- **本质问题**：记忆与推理能力耦合，无法独立进化\n\n### 2.2 检索式记忆的不足\n- **工作原理**：将经验外化到结构化数据库，任务执行时检索\n- **核心局限**：缺乏与推理的流畅交织，依赖刚性执行流程\n- **本质问题**：记忆是外部的、静态的，无法与认知过程动态互动\n\n## 3. 与人类认知的关键差距\n\n通过对比人类认知，作者识别出两个关键差距：\n\n1. **缺乏无缝交织**：人类大脑中，推理和记忆形成\"连续的思想流\"，而现有方法仅在任务开始时检索一次记忆\n2. **缺乏生成性重构**：现有方法通过嵌入相似性检索记忆，而非生成性地重构为新颖、连贯的见解\n\n这引出了核心研究问题：\n> \"How can we architect agent memory as a dynamic cognitive faculty, capable of fluid, reconstructive processes that interweave seamlessly with reasoning?\"\n\n## 4. 潜在记忆的探索与局限\n\n作者考察了潜在记忆(Latent Memory)作为替代方案，但发现现有方法仍有局限：\n\n1. **KV缓存方法**：主要解决长上下文问题，而非记忆重构\n2. **潜在令牌嵌入**：仍依赖侵入式参数更新，无法避免灾难性遗忘\n\n这些方法未能实现记忆与推理的动态交织和生成性重构。\n\n## 5. 核心假设：动态生成式记忆框架\n\n基于以上分析，作者提出核心假设：设计一个动态生成式记忆框架，使智能体具备类人认知能力。这一框架应该：\n\n1. **动态监控**：持续监控智能体的认知状态\n2. **按需生成**：在推理过程中的关键时刻动态调用记忆生成\n3. **无缝集成**：将记忆与推理紧密交织，形成递归对话\n\n## 6. 方法论设计：MemGen框架\n\n基于假设，作者设计了MemGen框架，包含两个协同组件：\n\n### 6.1 记忆触发器(Memory Trigger)\n- **功能**：作为元认知监控器，识别记忆调用的适当时机\n- **实现**：RL训练的轻量级LoRA适配器，监控推理器的隐藏状态\n- **关键创新**：仅在语义显著边界(如句子结束)激活，平衡记忆效用与计算效率\n- **训练目标**：确保关键记忆被调用，同时避免不必要的调用\n\n### 6.2 记忆编织器(Memory Weaver)\n- **功能**：以当前状态为刺激，生成机器原生的潜在记忆\n- **实现**：另一个LoRA适配器，接受隐藏状态作为\"钩子\"，生成潜在令牌序列\n- **关键创新**：不是简单检索，而是生成性地重构记忆\n- **训练目标**：仅更新编织器参数，保持核心推理器不变，避免灾难性遗忘\n\n## 7. 关键创新与突破\n\nMemGen的核心创新在于实现了三个关键突破：\n\n1. **动态记忆调用**：不同于传统方法的一次性检索，可在推理过程中任何关键时刻调用记忆\n2. **生成式记忆重构**：根据当前状态生成性重构记忆，而非静态检索\n3. **推理-记忆无缝交织**：形成递归对话，而非线性展开\n\n## 8. 实验验证与意外发现\n\n通过广泛实验，作者不仅验证了MemGen的性能优势，还发现了一个重要现象：\n\n**自发演化的类人记忆层次**：在没有明确监督的情况下，MemGen自发演化出三种类人记忆功能：\n- 规划记忆：支持高级任务规划\n- 程序记忆：捕获任务特定操作知识\n- 工作记忆：维护长上下文的一致性和理解\n\n这一发现表明，MemGen不仅解决了技术问题，还朝着更自然的机器认知形式迈出了重要一步。\n\n## 9. 结论：从技术方案到认知范式\n\nMemGen代表了一种从技术方案到认知范式的转变。通过将记忆视为动态认知能力而非静态存储，MemGen实现了记忆与推理的无缝交织，为自我进化智能体的发展提供了新路径。这一工作不仅解决了现有记忆机制的技术局限，还为构建更接近人类认知的智能系统开辟了新方向。"
                },
                {
                    "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
                    "arxiv_id": "2509.24726",
                    "authors": "Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, Linfeng Zhang",
                    "summary": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是提出Socratic-Zero框架，通过三个智能体（Teacher、Solver和Generator）的共同进化来生成高质量训练数据，从而提高大语言模型的推理能力。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划和多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究large language models (LLMs) - 能力方向：专注于reasoning，特别是math reasoning，在多个数学推理基准测试上评估 - 训练方法：涉及evolution和self-evolve概念，通过智能体共同进化实现自我改进 - 新兴范式：提出了llm-based agents和multi-agent systems框架 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（虽然使用数学推理作为评估基准，但方法是通用的） - 不讨论模型可靠性的应用层面问题 第四步特殊和模糊情况：论文提出的是通用的智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过智能体共同进化来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理能力提升对大规模高质量人工标注数据的依赖问题。针对数学推理任务，我们提出了一种基于三个智能体（Teacher、Solver和Generator）共同进化的Socratic-Zero框架，并在七个数学推理基准测试上通过准确率等指标验证了其有效性。实验表明，该方法从仅100个种子问题开始，实现了平均+20.2百分点的性能提升，且生成的合成数据使学生模型超越了多个最先进商业模型。",
                    "summary_translation": "近期大语言模型（large language models, LLMs）在推理任务（reasoning tasks）上的突破严重依赖大规模、高质量的数据集——这些数据集通常需要人工标注，因此难以扩展。尽管数据合成（data synthesis）或蒸馏（distillation）提供了一种有前景的替代方案，但现有方法在数据质量不一致和无法动态适应模型不断发展的能力方面存在困难，导致训练信号次优。为解决这些局限性，我们提出了Socratic-Zero（苏格拉底-零），这是一个完全自主的框架，通过三个智能体的共同进化——教师（Teacher）、解决者（Solver）和生成者（Generator）——从最少的种子示例中生成高质量训练数据。解决者（Solver）通过学习成功和失败轨迹（trajectories）上的偏好反馈（preference feedback）不断完善其推理能力；教师（Teacher）根据解决者的弱点自适应地设计越来越具有挑战性的问题；而生成者（Generator）则蒸馏（distills）教师的问题设计策略，以实现可扩展、高保真的课程（curriculum）生成。这个闭环系统产生了一个自我改进的课程——无需预先存在的任务或标签。值得注意的是，仅从100个种子问题开始，我们的Socratic-Solver-8B在七个数学推理基准测试（benchmarks）（AMC23、AIME24-25、Olympiad、MATH-500、Minerva和GSM8K）上比先前的数据合成方法平均提高了20.2个百分点，并且在Qwen3和GLM4系列模型上都有一致的提升。更令人惊讶的是，来自Socratic-Generator-32B的合成数据使学生大语言模型在这些基准测试上能够实现比其他最先进（state-of-the-art, SOTA）商业大语言模型更优越的性能，包括Qwen3-235B-A22B、DeepSeek-V3.1-671B、GPT-5、Gemini-2.5-Pro、Grok-4和Claude-4.1-Opus。",
                    "inspiration_trace": "# Socratic-Zero方法论逻辑推演\n\n## 1. 宏观问题：数据依赖困境\n\n**观察**：当前大型语言模型(LLMs)在推理任务上的突破严重依赖大规模、高质量数据集，而这些数据集通常需要人工标注，难以扩展。\n\n**核心问题**：如何在不依赖大规模人工标注数据的情况下，提升LLMs的数学推理能力？\n\n## 2. 问题细化：三大关键挑战\n\n**挑战1：静态范式限制**\n- 现有方法中，数据集在收集后被冻结，课程预先设计，模型在固定问题分布上训练\n- 无法适应训练过程中模型能力的变化\n- 无法利用反馈信号针对特定弱点\n\n**挑战2：现有合成数据方法缺陷**\n- 缺乏有效的质量控制机制\n- 重复使用低价值样本严重影响效果\n- 无法动态适应模型不断发展的能力\n\n**挑战3：知识传递低效**\n- 学生模型被动接受教师反馈而不评估可靠性\n- 当指导不理想时，学习质量下降\n- 无法根据学生能力动态调整内容\n\n## 3. 关键观察与启发\n\n**哲学启发**：苏格拉底式学习方法\n- 教师(苏格拉底)通过提问引导思考，而非直接给出答案\n- 实践者(亚里士多德)通过被引导沿推理路径学习\n- 学徒教师(柏拉图)通过观察内化教学方法\n\n**技术观察**：\n- 数据合成方法(如LLM2LLM)引入缺陷感知机制\n- 自主自玩范式(如Absolute Zero)探索连续任务生成\n- 偏好学习方法(如DPO)直接优化偏好，提高效率\n\n**核心洞察**：需要一个闭环系统，能够产生自我改进的课程，不需要预先存在的任务或标签。\n\n## 4. 核心假设形成\n\n**假设1：协同进化假设**\n如果设计一个系统，其中解题器、教师和生成器三个智能体协同进化，可以创造自我改进循环，从最少种子数据开始自主提升推理能力。\n\n**假设2：动态课程假设**\n如果课程能根据解题器表现动态调整难度，保持在解题器能力前沿(既不太简单也不太困难)，学习效率将最大化。\n\n**假设3：知识蒸馏假设**\n如果生成器能学习并内化教师的问题生成策略，就能以可扩展方式产生高质量课程，减少对昂贵教师模型的依赖。\n\n**假设4：偏好学习假设**\n如果解题器不仅从正确解决方案学习，还从成功和失败轨迹的偏好反馈中学习，可以更有效地改进推理能力。\n\n## 5. 方法论设计：Socratic-Zero框架\n\n### 5.1 三智能体架构\n\n**Solver(解题器)**：\n- 策略πθS，将问题q映射到解决方案轨迹y\n- 通过偏好学习改进推理能力\n\n**Teacher(教师)**：\n- 固定高容量LLM，提供两个确定性函数：\n  - 验证函数V(q, y) → {0, 1}：判断解决方案正确性\n  - 问题细化函数G(q, yfail) → (q', y'ref)：基于失败解决方案创建新问题\n\n**Generator(生成器)**：\n- 策略πθG，学习模仿教师的问题细化策略\n- 通过价值加权监督微调(WSFT)训练\n\n### 5.2 协同进化机制\n\n**课程动态扩展**：\n1. 收集解题器在当前课程Dt上的失败集合Ft\n2. 教师细化每个失败案例，创建新问题-解决方案对\n3. 扩充课程：Dt+1 = Dt ∪ Dnew\n\n**解题器训练**：\n- 生成多个解决方案尝试，分为\"获胜\"和\"失败\"集合\n- 使用直接偏好优化(DPO)更新参数，最大化首选解决方案可能性\n\n**生成器训练**：\n- 定义效用函数U(q'|πθS)，以目标成功率0.5为中心的高斯函数\n- 通过加权监督微调(WSFT)学习生成高效用问题\n\n### 5.3 动态课程分区\n\n根据解题器表现将问题分为三区：\n- **已掌握区域**：解题器始终能正确解决的问题\n- **学习区域**：解题器间歇性能解决的问题\n- **太难区域**：解题器始终无法解决的问题\n\n新问题主要从已掌握和学习区域生成，确保课程扩展保持在解题器的\"最近发展区\"内。\n\n## 6. 实验验证与结果\n\n**解题器效果**：\n- Socratic-Solver-8B在七个数学推理基准上平均准确率56.1%，比静态增强方法提高15.4个百分点\n- 跨架构泛化：在GLM4-9B和Qwen3-14B上也表现一致改进\n- 数学推理能力转移到更广泛认知能力，平均提高6.02个百分点\n\n**生成器效果**：\n- Socratic-Generator-32B实现95.6%有效率，显著优于基础模型(89.1%)\n- 产生的合成数据使学生模型达到37.72%准确率，优于包括Qwen3-235B-A22B(37.13%)等在内的最先进商业模型\n\n**消融研究**：\n- 初始监督微调(SFT)至关重要：无SFT版本仅达11.98%，SFT初始化模型达28.02%\n- 高斯奖励函数N(μ=0.5, σ=0.2)表现最佳，优于其他参数设置\n\n## 7. 结论与展望\n\n**核心贡献**：\n- 提出多智能体协同进化框架，将推理改进形式化为自适应课程学习问题\n- 实现从最小种子数据(仅100个问题)开始的自主推理提升\n- 在数学推理基准上取得最先进结果，展示强泛化能力\n\n**未来方向**：\n- 建立多智能体动态的理论基础，包括收敛分析\n- 将框架扩展到科学发现、现实世界决策等更广泛领域\n- 探索分层扩展，多个解题器-生成器对在不同难度水平运行\n\nSocratic-Zero成功将苏格拉底方法的哲学对话转化为协同进化的计算框架，通过三个智能体的动态交互，实现了从最少数据开始的自主推理能力提升，为解决数据依赖问题提供了新思路。"
                },
                {
                    "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training",
                    "arxiv_id": "2509.24494",
                    "authors": "Hongcheng Wang, Yinuo Huang, Sukai Wang, Guanghui Ren, Hao Dong",
                    "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM推理能力的训练方法。具体来说，它提出了GRPO-MA方法来优化GRPO算法，以更稳定和高效地训练大语言模型的链式思维(CoT)推理能力。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的标准。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于reasoning，特别是Chain-of-Thought推理，并在数学和代码任务上验证 - 训练方法：使用强化学习(GRPO)进行训练优化 第三步排除标准：虽然论文提到了VLMs和\"多样化的多模态任务\"，但多模态并非论文的主要焦点，而是作为评估方法的一种应用场景。论文核心是提出一种通用的训练方法改进，而非专注于多模态技术研究。论文也不涉及医疗、化学等特定应用领域，或水印、安全等模型可靠性问题。 第四步特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。关于多模态方面，论文只是将多模态任务作为评估场景之一，而非研究核心。 综合判断：论文的核心贡献是提出GRPO-MA方法来改进强化学习训练过程，以增强大语言模型的链式思维推理能力。这是一种通用推理能力的提升方法，符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决GRPO算法在Chain-of-Thought训练中的梯度耦合、稀疏奖励和不稳定优势估计问题。针对LLMs和VLMs的推理训练，我们提出GRPO-MA方法，通过为每个思维生成多个答案优化训练，并在数学、代码和多模态任务上通过准确率和训练效率验证了其有效性。",
                    "summary_translation": "最近的进展，如DeepSeek-R1，表明GRPO算法（一种强化学习(Reinforcement Learning, RL)方法）能够有效地训练大型语言模型(Large Language Models, LLMs)和视觉语言模型(Vision-Language Models, VLMs)中的思维链(Chain-of-Thought, CoT)推理。在本文中，我们分析了GRPO面临的三个挑战：思想与答案之间的梯度耦合(gradient coupling)、由有限并行采样导致的稀疏奖励信号(sparse reward signals)以及不稳定的优势估计(advantage estimation)。为缓解这些挑战，我们提出了GRPO-MA，这是一个简单但具有理论基础的方法，它利用每个思想过程的多答案生成(multi-answer generation)，从而实现更稳健和高效的优化。理论上，我们证明了思想优势(thought advantage)的方差随着每个思想生成的答案数量增加而减少。实证上，我们的梯度分析(gradient analysis)证实了这一效果，表明GRPO-MA相比GRPO减少了梯度尖峰(gradient spikes)。在数学、代码和多样化多模态任务(multimodal tasks)上的实验表明，GRPO-MA显著提高了性能和训练效率。我们的消融研究(ablation studies)进一步揭示，增加每个思想的答案数量持续提升模型性能。",
                    "inspiration_trace": "# GRPO-MA核心方法的逻辑推演\n\n## 宏观问题：如何提高基于强化学习的链式思考(CoT)训练的稳定性和效率？\n\n### 第一步：观察现有方法(GRPO)的表现与局限\n\n作者首先观察到GRPO算法在训练CoT推理方面的成功应用，如DeepSeek-R1模型所示。然而，通过深入分析，作者识别出GRPO面临的三个关键挑战：\n\n1. **梯度耦合问题**：思考和答案的梯度在GRPO中是耦合的。这导致一个根本矛盾：推理过程可能有效但最终答案错误，或推理有缺陷却产生正确答案。这种不一致性会扭曲梯度方向，削弱训练效果。\n\n2. **稀疏奖励信号**：GRPO需为每个问题采样多个完整CoT-答案对，计算成本高。在有限采样下，难以捕获罕见成功结果，导致全零奖励组。由于GRPO通过组内归一化计算优势，全零奖励集会崩溃为统一零优势，消除信息梯度，阻碍优化。\n\n3. **不稳定优势估计**：从概率角度看，\"好\"思考应增加生成\"好\"答案的概率。评估思考质量的最稳健方法是评估其产生的多个答案的总体分布。GRPO基于单个答案估计思考优势，结合高温采样的随机性，导致优势估计方差增大。\n\n### 第二步：形成核心假设\n\n基于上述观察，作者提出三个关键假设：\n\n1. 若能减少思考与答案间的梯度耦合，可能提高训练效果。\n2. 若能增加奖励信号密度而不显著增加计算成本，可解决稀疏奖励问题。\n3. 若能通过多答案样本估计思考优势，可降低优势估计方差，提高训练稳定性。\n\n### 第三步：探索解决方案方向\n\n针对这些假设，作者探索了可能的解决路径：\n\n1. **解耦思考与答案评估**：分别评估思考过程和最终答案质量，而非将它们视为整体。这可通过为每个思考生成多个答案，基于这些答案的平均奖励评估思考质量。\n\n2. **增强奖励信号密度**：通过为每个思考生成多个答案，在不生成大量完整推理响应的情况下增加奖励信号密度，避免全零奖励导致的优势崩溃。\n\n3. **稳定优势估计**：基于多个答案的平均奖励估计思考优势，降低估计方差。理论上，随着每个思考的答案数量增加，思考优势的方差应减少。\n\n### 第四步：形成GRPO-MA方法论\n\n基于上述探索，作者形成了GRPO-MA（GRPO with Multi-Answer）方法论：\n\n1. **核心创新**：对于K个思考中的每一个，采样M个答案。思考的价值是其M个答案的平均奖励，用于推导其相对于其他思考的优势。同时，每个K×M答案也接收自己的优势。这两个优势分别用于更新思考和答案标记。\n\n2. **理论支持**：基于delta方法的理论分析表明，随着M增加，思考优势的方差单调递减至零。相比之下，增加K只能将方差减少到一个非零常数。\n\n3. **实现框架**：\n   - 给定提示p，生成K个思考{th₁, ..., thₖ}\n   - 对每个思考thᵢ，生成M个答案{ansᵢ,₁, ..., ansᵢ,ₘ}\n   - 计算思考价值V(thᵢ) = (1/M)∑ⱼRᵢ,ⱼ，并归一化得到思考优势A(thᵢ)\n   - 计算每个答案的优势A(ansᵢ,ⱼ)\n   - 结合两个层次的优势进行更新：基于思考优势更新思考，基于答案优势更新答案\n\n### 第五步：验证与评估\n\n作者通过多维度实验验证GRPO-MA的有效性：\n\n1. **实验范围**：在数学、代码和多种多模态任务（目标检测、可供性预测、轨迹预测等）以及基于模拟器的视觉操作任务上评估。\n\n2. **关键发现**：\n   - 与GRPO基线相比，GRPO-MA在训练时间略微增加的情况下产生明显性能提升\n   - 与生成K×M个响应的基线相比，GRPO-MA仅用约60%训练时间实现相似或更好性能\n   - 在稀疏奖励任务中，GRPO-MA显著优于标准GRPO\n   - 消融研究表明增加M通常提高模型性能，且思考优势估计的稳定性比奖励信号丰富性更关键\n\n3. **理论验证**：梯度分析证实GRPO-MA比GRPO减少了梯度尖峰，验证了理论预测。\n\n### 第六步：总结与展望\n\n作者总结GRPO-MA的核心贡献：\n\n1. 提出GRPO-MA算法，一种简单有效且通用的GRPO改进策略\n2. 提供理论分析，证明该方法可提高优势估计稳定性，产生更稳定梯度\n3. 在多个不同任务上验证GRPO-MA始终优于基线GRPO\n\n同时，作者也指出研究局限性，如计算约束、思考值独立性假设的简化，以及缺乏通用奖励模型等，为未来研究指明方向。\n\n这一逻辑推演展示了从观察到问题识别，从假设形成到解决方案探索，最终形成方法论并验证的完整研究思路，体现了严谨的学术思维过程。"
                },
                {
                    "title": "Alternatives To Next Token Prediction In Text Generation - A Survey",
                    "arxiv_id": "2509.24435",
                    "authors": "Charlie Wyatt, Aditya Joshi, Flora Salim",
                    "summary": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是探讨替代传统下一个词元预测(NTP)范式的方法，以解决大语言模型在长期规划、错误累积和计算效率等方面的弱点。论文明确聚焦于改进LLM的基础能力，特别是通过\"Plan-then-Generate\"和\"Latent Reasoning\"等方法来增强模型的规划和推理能力，这与研究目标中\"提高大语言模型本身的通用推理能力\"高度一致。论文没有涉及任何特定应用领域（如医疗、化学等），也不是将LLM作为工具应用于特定问题，而是专注于改进LLM的核心生成机制和推理能力。作为一篇综述，它系统地分类了五种改进LLM推理能力的方法家族，包括多词元预测、先规划后生成、潜在推理等，这些都是直接针对提升LLM通用推理能力的研究方向。因此，这篇论文完全符合筛选标准，应该被纳入研究范围。",
                    "summary2": "本文旨在解决Next Token Prediction (NTP)在文本生成中的局限性问题，包括长期规划能力差、错误累积和计算效率低等。针对现有LLMs的NTP范式，我们提出了一种系统性的分类框架，将替代方法分为五大类：Multi-Token Prediction、Plan-then-Generate、Latent Reasoning、Continuous Generation Approaches和Non-Transformer Architectures，并通过综合分析各类代表性研究成果，验证了这些方法在提升生成质量和效率方面的有效性。",
                    "summary_translation": "下一词元预测(Next Token Prediction, NTP)范式推动了大型语言模型(Large Language Models, LLMs)前所未有的成功，但同时也是其最持久弱点的根源，如长期规划能力差、错误累积和计算效率低下。鉴于探索NTP替代方案的兴趣日益增长，本综述描述了NTP替代方案的新兴生态系统。我们将这些方法分为五大类：(1) 多词元预测(Multi-Token Prediction)，目标是预测未来的一组词元而非单个词元；(2) 先规划后生成(Plan-then-Generate)，预先创建全局高级计划以指导词元级解码；(3) 潜在推理(Latent Reasoning)，将自回归过程本身转移到连续潜在空间；(4) 连续生成方法(Continuous Generation Approaches)，通过扩散、流匹配或基于能量的方法，用迭代并行细化替代顺序生成；(5) 非Transformer架构(Non-Transformer Architectures)，通过其固有的模型结构规避NTP。通过综合这些方法的见解，本综述提供了一个分类法，以指导针对词元级生成已知局限性模型的研究，从而为自然语言处理开发新的变革性模型。",
                    "inspiration_trace": "# 从NTP局限到替代方案：逻辑推演链\n\n## 1. 宏观问题：NTP范式的根本性矛盾\n\n**观察起点**：下一个词预测(NTP)范式驱动了大型语言模型(LLMs)的空前成功，但同时也成为其最持久弱点的根源。尽管LLMs在流畅性方面表现出色，却经常表现出脆弱行为：幻觉事实、长输出中失去连贯性、在需要长期规划的任务中挣扎。\n\n**核心矛盾**：NTP在实现局部流畅性的同时，牺牲了全局连贯性和推理能力。即使模型规模、上下文长度和训练数据不断增长，这些局限性依然存在，表明问题不是偶然的，而是NTP框架本身的结构性问题。\n\n## 2. 深入观察：NTP的多维度局限性\n\n### 2.1 推理时失败：贪婪生成与错误累积\n- **现象**：NTP的自回归特性鼓励短视优化，优先考虑短期标记可能性而非长期连贯性\n- **问题**：早期生成中的微小错误会复合成后期序列中的显著不连贯或事实漂移\n- **证据**：即使在大海捞针检索任务中表现近乎完美，LLMs在生成长文档摘要时仍表现不佳\n\n### 2.2 粒度不匹配：标记与想法的鸿沟\n- **现象**：NTP在子词标记上操作，而人类推理在想法、句子和话语结构上展开\n- **问题**：子词标记既非语言上自然，也非语义上有意义，导致在简单字符级任务上失败\n- **证据**：LLMs难以回答\"strawberry中有多少个r\"等简单问题，因为无法预先规划整体响应\n\n### 2.3 计算效率低下：顺序生成的代价\n- **现象**：Transformer在每个层和时间步计算全序列注意力，导致O(n²)复杂度\n- **问题**：即使适度长度文档也需要数百或数千解码步骤，每步都涉及完整前向传递\n- **证据**：现代NLP架构的能源消耗和环境影响已成为关注焦点\n\n### 2.4 训练时失败：教师强制的误导\n- **现象**：模型使用教师强制训练，学习在给定真实前文的情况下预测下一个标记\n- **问题**：导致模型学习虚假捷径(\"Clever Hans\"效应)，利用输出前缀猜测合理延续而非理解任务\n- **证据**：在路径查找等前瞻任务中，训练准确率100%的模型在测试时完全失败\n\n### 2.5 关键发现：Transformer内部已存在高级表示\n- **现象**：尽管输出是顺序标记流，Transformer内部潜在状态表现出显著复杂性和前瞻性\n- **问题**：NTP限制可能不在于模型表示复杂想法的能力，而在于必须一次一个子词表达它们\n- **证据**：研究发现LLMs内部形成\"大于标记\"的抽象，特定神经元对应于多标记概念\n\n## 3. 形成核心假设：NTP的根本缺陷\n\n基于上述观察，形成核心假设：**NTP的根本问题在于其标记级、顺序生成的本质，而非Transformer架构本身**。\n\n这一假设基于以下推理链：\n1. NTP鼓励短视局部决策，而非全局规划\n2. 子词标记与人类自然思考单位(想法、句子)存在粒度不匹配\n3. 顺序生成导致计算效率低下，无法并行处理\n4. 教师强制训练使模型学习虚假捷径，而非真正理解\n5. Transformer内部已存在更高级别表示，但被约束为一次输出一个标记\n\n## 4. 探索解决方案：超越NTP的新范式\n\n基于核心假设，探索应满足以下目标的新范式：\n- 支持更全局的规划和推理\n- 在更合适的粒度上操作(想法、句子或概念级别)\n- 实现更高效的并行生成\n- 避免教师强制训练的陷阱\n- 充分利用Transformer内部已存在的高级表示\n\n## 5. 提出五类替代方法：从观察到系统化\n\n通过对现有文献的系统回顾，将替代NTP的方法归纳为五大家族：\n\n### 5.1 多标记预测(MTP)\n- **核心思想**：将预测目标从单个标记扩展到一块k个未来标记\n- **逻辑基础**：解决NTP的短视问题，使模型能够\"向前看\"多个步骤\n- **实现方式**：使用共享上下文表示(\"主干\")馈送多个并行输出头\n- **优势**：提高计算效率，鼓励短期规划，部分并行化\n- **局限**：通常只关注短期未来，缺乏真正的全局规划机制\n\n### 5.2 先计划后生成(PtG)\n- **核心思想**：引入高级规划阶段指导后续标记生成\n- **逻辑基础**：人类写作和思考通常先有全局规划再填充细节\n- **实现方式**：两阶段过程——先生成高级抽象计划，再基于计划生成输出序列\n- **优势**：引入全局连贯性，减轻下一个标记解码的短视性\n- **局限**：符号规划器缺乏可扩展性，潜在规划器引入可解释性挑战\n\n### 5.3 潜在推理(LR)\n- **核心思想**：用潜在状态上的自回归替换标记级生成\n- **逻辑基础**：解决子词标记与高级语义之间的粒度不匹配\n- **实现方式**：生成潜在向量序列再解码为文本，或在发出标记前细化潜在状态\n- **优势**：在句子、思想或概念级别操作，减少序列长度\n- **局限**：生成过程不透明，训练复杂，潜在对齐问题\n\n### 5.4 连续生成方法(CG)\n- **核心思想**：通过迭代转换同时初始化和细化整个输出\n- **逻辑基础**：人类创作过程常涉及反复修改和全局优化\n- **实现方式**：定义从噪声到连贯文本的轨迹，通过扩散、流匹配或能量模型实现\n- **优势**：全局规划和修订，减轻错误累积，双向细化\n- **局限**：与自回归模型相比质量差距，推理成本高，固定长度限制\n\n### 5.5 非Transformer架构(NTA)\n- **核心思想**：通过改变模型架构本身绕过NTP\n- **逻辑基础**：不同序列建模机制可能更适合语言本质\n- **实现方式**：使用状态空间模型、联合嵌入预测架构等替代架构\n- **优势**：线性时间复杂度，隐式潜在空间抽象\n- **局限**：难以适应生成任务，表示漂移，评估不匹配\n\n## 6. 形成统一方法论：超越NTP的框架\n\n最终形成统一的方法论框架，将替代NTP的方法组织为五大家族，每类方法包含：\n- 清晰的定义和形式化描述\n- 代表性论文和实现方式\n- 失败模式和局限性分析\n- 讨论和未来研究方向\n\n这一框架不仅帮助理解当前替代NTP的方法，还为研究人员提供了探索语言模型如何超越标记级生成的路线图，朝着能够更像人类一样规划、推理和交流的模型发展。\n\n## 结论：从问题到解决方案的逻辑演进\n\n整个逻辑演进过程展现了从观察到假设再到解决方案的科学研究思维：\n1. **问题识别**：发现NTP范式下的根本性矛盾\n2. **现象分析**：系统观察NTP在多个维度的局限性\n3. **假设形成**：提出NTP的根本问题在于标记级顺序生成的本质\n4. **方向探索**：确定新范式应满足的关键目标\n5. **方法分类**：系统化五类替代方法，每类针对NTP的不同局限\n6. **框架构建**：形成统一的方法论框架，指导未来研究\n\n这一逻辑链不仅解释了当前NTP替代方案的研究现状，也为语言模型的未来发展提供了清晰路径。"
                },
                {
                    "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model",
                    "arxiv_id": "2509.24389",
                    "authors": "Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen",
                    "summary": "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步：核心判断——这篇论文的本质是提出了一种新的模型架构LLaDA-MoE，结合了Mixture-of-Experts (MoE)架构和扩散模型技术。论文的核心贡献在于提高大语言模型的计算效率（保持7B参数容量但只激活1.4B参数），同时保持或提升其在多种通用任务上的性能。这属于改进LLM的基础能力和架构的研究，符合\"改进LLM的基础能力\"的研究目标。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language diffusion model\"，属于LLMs范畴 - 能力方向：特别提到了模型在\"数学推理\"(mathematical reasoning)方面的表现 - 新兴范式：提到了模型在\"智能体\"(agent)任务上的表现 第三步：排除标准——论文不主要聚焦于任何排除领域： - 虽然提到了\"Diffusion Language Model\"，但这是应用于语言模型的扩散技术，而非视觉或多模态领域的扩散模型 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况——论文提到模型在智能体任务上的表现，但主要是评估而非提出新的智能体框架；没有明确讨论幻觉、可解释性或安全性问题。 综合判断：这篇论文的核心是改进LLM的基础架构和效率，同时评估了其在数学推理等通用推理能力上的表现。虽然主要关注点是计算效率，但其目的是提升模型在多种通用任务上的性能，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决扩散语言模型的计算效率问题。针对大规模语言模型训练和推理的高计算开销场景，我们提出了一种基于稀疏MoE架构的扩散语言模型LLaDA-MoE，并在多个基准测试上通过准确率、性能比较等指标验证了其有效性。该模型在推理时仅激活1.4B参数，却超越了之前的8B参数扩散语言模型，并实现了与Qwen2.5-3B-Instruct相当的性能。",
                    "summary_translation": "我们介绍了LLaDA-MoE，这是一种采用Mixture-of-Experts (MoE，专家混合)架构的大型语言扩散模型(language diffusion model)，该模型从零开始在大约20T个token（标记）上进行了训练。LLaDA-MoE在保持70亿参数容量的同时，在推理(inference)过程中仅激活14亿参数，从而显著降低了计算开销(computational overhead)，实现了具有竞争力的性能。我们的实证评估(empirical evaluation)表明，LLaDA-MoE在具有更大参数的扩散语言模型中实现了最先进的(state-of-the-art)性能，在多个基准测试(benchmark)中超越了之前的扩散语言模型LLaDA、LLaDA 1.5和Dream。经过指令微调(instruct-tuned)的模型LLaDA-MoE-7B-A1B-Instruct在知识理解、代码生成、数学推理、智能体(agent)和对齐(alignment)任务上展现出与Qwen2.5-3B-Instruct相当的能力，尽管使用了更少的激活参数。我们的研究结果表明，将稀疏MoE架构(sparse MoE architecture)整合到掩码扩散语言模型(masked diffusion language model)的训练目标中，仍然能在少量激活参数的高效推理条件下发挥MoE的优势，并为扩散语言模型的进一步探索开辟了广阔空间。LLaDA-MoE模型已在Huggingface上发布。",
                    "inspiration_trace": "# LLaDA-MoE核心方法的逻辑推演\n\n## 宏观问题：语言模型的效率与性能平衡\n\n大型语言模型(LLMs)快速发展并广泛应用于各类任务，但面临一个核心挑战：如何在保持强大性能的同时提高计算效率？当前主流的自回归(AR)模型虽然表现出色，但计算开销巨大，尤其是随着模型规模增长。这引出一个关键问题：是否存在一种既能保持高性能又能提高计算效率的语言模型范式？\n\n## 观察一：扩散语言模型的潜力\n\n作者首先观察到掩码扩散模型(MDMs)作为一种替代性建模范式正受到关注。MDMs通过迭代细化部分掩码序列来生成文本，与AR模型相比具有相当的扩展性和性能。最新研究表明，如LLaDA等8B参数的MDMs已能媲美LLaMA3 8B的性能，证明扩散语言建模是一个有前景的方向。\n\n**关键观察**：MDMs提供了与AR模型不同的技术路径，但现有MDMs主要依赖密集的Transformer架构，计算效率仍有提升空间。\n\n## 观察二：MoE架构在AR模型中的成功\n\n作者进一步观察到，稀疏专家混合(MoE)架构在AR模型中已得到广泛验证。MoE通过为每个token只激活一小部分参数，实现了与更大密集模型相当的性能，同时大幅降低了计算开销。这种\"稀疏激活\"机制为解决语言模型的效率问题提供了有效途径。\n\n**关键观察**：MoE架构在AR模型中已证明能有效平衡性能与效率，但在MDMs领域尚未探索。\n\n## 假设：MoE与MDMs的结合潜力\n\n基于以上观察，作者提出核心假设：将稀疏MoE架构整合到掩码扩散语言模型的训练目标中，可以在保持高效推理（激活少量参数）的同时，发挥MoE的优势，从而实现更高效的MDMs。\n\n这一假设建立在两个关键洞见之上：\n1. MDMs的迭代掩码预测机制与MoE的稀疏激活机制可能存在天然契合点\n2. MoE的参数效率优势可能使MDMs在更大规模上更具可行性\n\n## 方法论形成：LLaDA-MoE的设计\n\n为验证这一假设，作者设计了LLaDA-MoE模型，具体方法论形成过程如下：\n\n### 1. 架构设计决策\n\n**决策点**：如何在MDMs中有效整合MoE架构？\n- 采用稀疏MoE架构，总参数量7B，但推理时仅激活1.4B参数\n- 设置64个专家网络，每个token通过路由机制选择top-8专家\n- 保留MDMs的核心组件（双向注意力、掩码预测），同时用MoE替换密集前馈网络\n\n**创新点**：首次将MoE架构与扩散语言模型结合，实现\"稀疏激活的扩散建模\"。\n\n### 2. 训练策略优化\n\n**决策点**：如何确保MoE-MDMs的有效训练？\n- 设计多阶段训练流程：预训练(20T tokens)→退火(1T tokens)→SFT\n- 在预训练阶段2增加数学和代码数据的比例，提升模型在这些领域能力\n- 采用变量长度训练(1%步骤)减少训练-推理分布不匹配\n- 引入辅助损失函数(L_Z和L_LB)解决MoE训练中的专家负载不平衡问题\n\n**创新点**：针对MoE-MDMs的特性定制训练策略，确保稳定收敛和性能优化。\n\n### 3. 推理机制设计\n\n**决策点**：如何在保持MDMs特性的同时实现高效推理？\n- 采用标准MDMs的迭代去掩码过程\n- 引入半自回归采样策略，将序列分块并行处理\n- 实现低置信度重掩码机制，提高生成质量\n\n**创新点**：结合MoE的稀疏激活与MDMs的迭代生成，实现参数高效的推理。\n\n## 实验验证与结论\n\n通过在多个基准测试上的评估，作者验证了LLaDA-MoE的有效性：\n1. 仅激活1.4B参数的情况下，超越了之前的8B参数扩散语言模型\n2. 经过指令微调后，性能与Qwen2.5-3B-Instruct相当，尽管激活参数更少\n3. 证明了稀疏MoE架构是构建更高效MDMs的可行路径\n\n这一研究不仅验证了初始假设，还为扩散语言模型开辟了新的设计空间，展示了如何通过架构创新实现语言模型的效率与性能平衡。"
                },
                {
                    "title": "Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining",
                    "arxiv_id": "2509.24356",
                    "authors": "Matthew Theodore Roque, Dan John Velasco",
                    "summary": "Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究在数据受限情况下如何通过课程学习和文本简化来优化语言模型的预训练过程，提高模型的表示质量和性能。根据筛选标准的第一步，这属于改进LLM基础能力的研究，提出了新的训练范式（课程学习结合文本简化），旨在增强模型的语言理解、常识推理等通用能力。论文评估了模型在\"常识推理\"等能力上的表现，虽然不是直接针对推理能力的研究，但其方法可以增强模型的通用推理能力。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在解决数据受限预训练环境中的优化问题。针对有限预训练数据场景，我们提出了一种结合文本简化和课程学习的预训练方法，并在人类撰写段落与LLM简化版本组成的平行语料库上，通过微调性能和零样本任务准确率验证了其有效性。",
                    "summary_translation": "大多数关于语言模型预训练（language model pretraining）的研究都集中在大型数据集上，而在数据受限（data-constrained）环境中的优化问题仍存在未解之谜。在这些情况下，训练数据顺序（training data order）的影响以及包含同一文本的替代版本（alternative versions）的效果仍未得到充分探索。我们通过研究预训练中的课程学习（curriculum learning）来解决这一问题，重点关注文本复杂度排序（text-complexity ordering）和通过简化（simplification）进行数据增强（data augmentation）。我们提出以下问题：(1) 简化文本是否比重复使用原始数据更能提高表示质量（representation quality）？(2) 按文本复杂度排序数据是否能产生更好的表示？为回答这些问题，我们基于一组平行语料库（parallel corpora），其中人工撰写的段落与大型语言模型（LLM）简化的变体对齐，并测试了四种数据调度（data schedules）方式：重复暴露（repeated exposure）、从低到高复杂度（low-to-high complexity）、从高到低复杂度（high-to-low）和交错式（interleaved）。我们通过微调（fine-tuning）从样本效率（sample efficiency）角度分析模型的表示质量，以及其在语言知识（linguistic knowledge）、实体跟踪（entity tracking）、世界知识（world knowledge）和常识推理（commonsense reasoning）方面的零样本（zero-shot）性能。我们的研究结果表明，添加简化数据相比重复暴露基线（repeated-exposure baseline）提高了微调和零样本性能：较小模型受益于从低到高复杂度的排序，而较大模型在交错式排序下表现更好。",
                    "inspiration_trace": "# 从宏观问题到方法创新：文本简化与课程学习的逻辑推演\n\n## 1. 宏观问题观察：数据受限预训练的挑战\n\n**起点：现有研究的盲区**\n- 作者观察到，大多数语言模型预训练研究聚焦于大规模数据集场景\n- 现实中，预训练常面临数据受限环境，不可避免需要重复使用相同语料库\n- 在数据受限条件下，两个关键因素被忽视：(1)训练数据呈现顺序；(2)同一文本的简化版本使用\n\n**核心问题提出**：在数据有限的情况下，如何优化预训练效果，超越简单的数据重复？\n\n## 2. 问题聚焦：两个关键假设\n\n**假设1：文本简化优于重复暴露**\n- 推理：重复相同数据可能导致收益递减，而简化版本可能提供新的语言模式\n- 问题：在数据受限环境中，用简化文本替代重复暴露是否能提高表示质量？\n\n**假设2：文本复杂度排序影响学习效果**\n- 推理：人类学习遵循从简单到复杂的课程，机器学习可能类似\n- 问题：按文本复杂度排序（简单到复杂vs复杂到简单vs混合）是否会产生更好的表示？\n\n## 3. 方法构建：从理论到实验设计\n\n**理论基础构建**：\n- 借鉴课程学习理论(Bengio et al., 2009)：逐渐增加复杂度的训练顺序可能提升学习效率\n- 整合文本简化研究：简化文本保留语义内容但降低表面复杂度\n\n**实验设计**：\n- 数据基础：构建平行语料库，人工撰写段落(HW)与LLM简化版本(SIMP)对齐\n- 变量控制：保持架构、分词器、上下文长度、优化器等变量恒定，仅改变数据顺序\n- 四种数据调度策略：\n  1. BASELINE：两轮HW（模拟数据受限场景）\n  2. INTERLEAVED：HW和SIMP均匀混合\n  3. SIMP→HW：先简化后原始文本（标准课程）\n  4. HW→SIMP：先原始后简化文本（反课程）\n\n## 4. 验证框架：多维度评估\n\n**评估维度设计**：\n- 微调评估：测试表示质量在下游任务中的迁移能力\n- 零样本评估：测试模型在未见任务上的泛化能力\n- 样本效率分析：通过不同规模的微调数据集测试预训练效果\n\n**任务选择**：\n- 语言知识：BLiMP、BLiMP Supplement\n- 实体跟踪：Entity Tracking\n- 世界知识：MMLU、EWoK\n- 常识推理：ARC、HellaSwag、PIQA等\n\n## 5. 结果发现与理论修正\n\n**假设1验证**：\n- 发现：简化数据确实优于重复原始数据\n- 修正：效果与模型规模相关，124M模型有适度提升，256M模型提升更明显\n- 解释：更大模型能更好吸收释义变化，混合暴露产生一致改进\n\n**假设2验证**：\n- 发现：课程效果依赖于模型规模\n- 修正：较小模型(124M)受益于简单到复杂课程(SIMP→HW)，较大模型(256M)偏好平衡暴露(INTERLEAVED)\n- 解释：较小模型需要渐进式学习，而较大模型能同时处理两种复杂度\n\n## 6. 方法论贡献：数据受限预训练的新路径\n\n**核心贡献**：\n- 提供首个关于文本简化和课程调度在数据受限预训练中交互作用的对照研究\n- 证明在数据有限情况下，文本简化可作为重复暴露的有效补充\n- 揭示模型规模是选择最佳数据调度策略的关键因素\n\n**实践意义**：\n- 为数据受限环境提供实用优化方法，无需收集新数据\n- 提供针对不同模型规模的预训练策略指导\n- 开辟数据增强新方向：通过文本变换而非简单重复扩展数据效用\n\n这一逻辑链条从宏观问题出发，通过观察现有研究盲区，聚焦具体假设，设计严谨实验，验证并修正理论，最终形成针对数据受限预训练的创新方法论，体现了从问题到解决方案的系统性思考过程。"
                },
                {
                    "title": "Reinforcement Mid-Training",
                    "arxiv_id": "2509.24375",
                    "authors": "Yijun Tian, Shaoyu Chen, Zhichao Xu, Yawei Wang, Jinhe Bi, Peng Han, Wei Wang",
                    "summary": "The development of state-of-the-art large language models is commonly understood as a two-stage process involving pre-training and post-training. We point out the need for an additional intermediate stage called reinforcement mid-training with potential for strong performance gains. In this paper, we formally define the problem and identify three key challenges: (1) inefficient training due to excessive reasoning steps, (2) disregard of the imbalanced token entropy distribution, and (3) underutilization of token information. To address these challenges, we propose RMT, a framework for efficient, adaptive, and unified reinforcement mid-training with various innovative components. In particular, we first introduce a dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking. Next, we design a curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens. Finally, we present a dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information. Extensive experiments demonstrate the superiority of RMT over state-of-the-art methods, achieving up to +64.91% performance improvement with only 21% of the reasoning length in language modeling. We also show that checkpoints obtained after reinforcement mid-training can benefit the subsequent post-training, yielding up to +18.76% improvement in the mathematical domain.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"强化中训练\"(RMT)的新训练范式，用于提高大语言模型的推理能力和训练效率。从核心判断来看，论文本质上是改进LLM的基础能力，特别是通过动态token预算机制减少不必要的推理步骤，通过基于课程的自适应采样方法促进从易到难的学习轨迹，以及通过结合强化学习和下一个token预测的双重训练策略来充分利用token信息。这些方法都是直接提升LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。论文在数学领域展示了改进，但这只是作为通用推理能力的一个示例，而不是论文的主要焦点。从正面指标来看，论文包含了LLM核心概念、推理能力（特别是数学推理）和强化学习方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合研究目标，即筛选出那些致力于提高大语言模型本身的通用推理能力的论文。",
                    "summary2": "本文旨在 [解决大型语言模型训练过程中的效率问题]。针对 [预训练和后训练之间的中间阶段]，我们提出了一种 [强化中训练框架RMT，包含动态token预算和自适应采样策略]，并在 [语言建模任务] 上通过 [准确率和推理长度] 验证了其有效性。",
                    "summary_translation": "# 中文翻译\n\n最先进的大型语言模型（state-of-the-art large language models）的发展通常被理解为一个包含预训练（pre-training）和后训练（post-training）的两阶段过程。我们指出需要增加一个称为强化中间训练（reinforcement mid-training）的额外中间阶段，该阶段具有强大的性能提升潜力。在本文中，我们正式定义了该问题并确定了三个关键挑战：(1) 由于推理步骤过多导致的训练效率低下，(2) 忽略了不平衡的令牌熵（token entropy）分布，以及 (3) 令牌（token）信息的利用不足。\n\n为应对这些挑战，我们提出了RMT，一个高效、自适应且统一的强化中间训练框架，包含多种创新组件。具体而言，我们首先引入了一种动态令牌预算（dynamic token budget）机制，该机制限制了不必要的推理步骤并减轻了模型过度思考（overthinking）问题。接下来，我们设计了一种基于课程的自适应采样（curriculum-based adaptive sampling）方法，该方法促进了从简单到复杂令牌的渐进式学习轨迹。最后，我们提出了一种双重训练（dual training）策略，将强化学习（reinforcement learning）与下一令牌预测（next-token prediction）相结合，确保对关键令牌进行有针对性的学习并充分利用所有令牌信息。\n\n大量实验证明了RMT相对于最先进方法的优越性，在语言建模中仅用21%的推理长度就实现了高达+64.91%的性能提升。我们还表明，在强化中间训练后获得的检查点（checkpoints）可以使后续的后训练受益，在数学领域带来高达+18.76%的改进。",
                    "inspiration_trace": "# 从观察到创新：RMT框架的逻辑演进\n\n## 1. 宏观问题：LLM训练流程的优化空间\n\n作者首先观察到大型语言模型(LLM)的发展通常被简化为两阶段过程：预训练(pre-training)和后训练(post-training)。预训练赋予模型基础知识和语言能力，后训练则使模型与人类目标对齐。然而，这种二分法可能忽略了模型能力发展的连续性和中间阶段的优化空间。\n\n## 2. 关键发现：被忽视的中间阶段\n\n通过分析现有研究，作者发现了一个被忽视的中间阶段——中期训练(mid-training)。这一阶段利用预训练数据，但采用比一般预训练更有针对性的目标，系统性地增强模型在特定领域(如数学推理)的能力，为后续后训练做更好准备。与后训练不同，中期训练不依赖带标签或奖励信号的领域特定数据，而是利用无标签预训练数据。\n\n## 3. 现有方法的局限性分析\n\n作者深入分析了现有强化中期训练方法(如RPT)面临的三个关键挑战：\n\n### 挑战1：低效训练与过度思考\n- **观察**：现有方法生成过长的推理链，导致计算开销剧增\n- **问题**：如图1(a)所示，现有方法需要显著更多token进行推理，却不能保证性能提升\n\n### 挑战2：忽略不平衡的token熵分布\n- **观察**：token的熵值(不确定性和学习难度)各不相同，但现有方法不加区分地采样\n- **问题**：在训练早期，模型能力不足时过度暴露于高难度token，导致学习不稳定\n\n### 挑战3：token信息利用不足\n- **观察**：如图1(b)所示，大多数token表现出低熵，但现有方法只关注高熵token\n- **问题**：忽略大量低熵token导致信息损失和错失学习机会\n\n## 4. 核心假设形成\n\n基于上述观察，作者形成了核心假设：\n- 在预训练和后训练之间引入专门的中期训练阶段能显著提升LLM性能\n- 通过解决三个关键挑战，可设计出高效、自适应和统一的强化中期训练框架\n\n## 5. 解决方案设计：RMT框架\n\n为验证假设并解决三个挑战，作者设计了RMT框架，包含三个创新组件：\n\n### 组件1：动态token预算机制（解决挑战1）\n- **设计思路**：引入随训练进度衰减的token预算，约束推理长度\n- **实现**：Bt = max(Bmin, ⌊B0 · γ^(t/T)⌋)，配合三角形长度奖励函数\n- **预期效果**：使模型随能力成熟自适应减少推理开销，提高效率\n\n### 组件2：基于课程的自适应采样（解决挑战2）\n- **设计思路**：根据token熵值分类难度，动态调整采样概率\n- **实现**：将token分为简单、中等、困难三类，通过分段线性插值实现课程调度\n- **预期效果**：从易到难渐进学习，确保训练稳定性\n\n### 组件3：双重训练策略（解决挑战3）\n- **设计思路**：结合token选择性强化学习与token包容性下一token预测\n- **实现**：对高熵token应用强化学习，对低熵token应用下一token预测，统一损失函数\n- **预期效果**：兼顾关键token的针对性学习和所有token信息的充分利用\n\n## 6. 实验验证与结果\n\n作者通过多维度实验验证RMT框架：\n\n### 语言建模任务\n- **结果**：RMT-Q3实现+64.91%性能提升，推理长度仅为基线的21%\n\n### 持续后训练\n- **结果**：RMT检查点在后续后训练中实现+18.76%性能提升\n\n### 效率分析\n- **结果**：响应长度显著减少，生成时间大幅降低\n\n### 消融研究\n- **结果**：各组件均有贡献，移除任一组件都会降低性能\n\n## 7. 结论与意义\n\nRMT框架成功解决了强化中期训练中的关键挑战，实现了高效、自适应和统一的训练。这一工作不仅为LLM生命周期引入了有价值的新阶段，也为未来研究开辟了新方向。作者通过系统性的观察、假设形成、解决方案设计和实验验证，构建了一个完整的逻辑链条，展示了从问题发现到方法创新的科学思维过程。"
                },
                {
                    "title": "Prompt and Parameter Co-Optimization for Large Language Models",
                    "arxiv_id": "2509.24245",
                    "authors": "Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong",
                    "summary": "Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文本质上是关于改进LLM基础能力的研究，提出了一种名为MetaTuner的新框架，联合优化提示和参数来提升LLM性能。这是一种新的训练范式，属于增强LLM通用能力的方法论研究，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确关注\"Large Language Models (LLMs)\"这一核心概念；虽然未直接提及reasoning等词，但提示优化和参数微调都是提高LLM通用推理能力的方法；同时，论文提出了一种创新的训练方法，符合训练方法这一正面指标。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性等应排除的内容。 论文的核心贡献是提出了一种联合优化提示和参数的新框架，通过两个神经网络分别生成提示和参数，并允许它们共享底层编码层以实现知识共享。这种方法通过结合提示优化（显式自然语言）和微调（隐式参数更新）两种互补的方法，提升了LLM的通用能力，因此完全符合我筛选\"致力于提高大语言模型通用推理能力\"论文的研究目标。",
                    "summary2": "本文旨在解决大语言模型中提示优化和微调被独立研究而忽略协同潜力的问题。针对提示优化（离散优化）和微调（连续优化）的互补特性，我们提出了一种MetaTuner框架，通过共享元编码器联合生成提示和参数，并设计监督正则化损失解决混合优化挑战。在MATH、GSM8K、HotpotQA和CosmosQA四个基准测试上，通过准确率等指标验证了其有效性，实验结果表明MetaTuner始终优于现有基线方法。",
                    "summary_translation": "提示优化(prompt optimization)和微调(fine-tuning)是提升大型语言模型(Large Language Models, LLMs)性能的两种主要方法。它们从互补的角度增强LLMs的能力：前者通过显式的自然语言，后者通过隐式的参数更新。然而，先前的研究通常孤立地研究这两种方法，导致它们的协同潜力在很大程度上未被充分探索。为了弥合这一差距，在本文中，我们提出了MetaTuner，这是一种联合集成提示优化和微调用于LLMs训练的新型框架。具体而言，我们引入了两个神经网络分别生成提示(prompts)和参数(parameters)，同时允许它们共享一个公共的底层编码层(bottom encoding layer)以实现知识共享。在最终监督信号(supervised signals)的引导下，我们的框架被优化以发现提示和参数之间的最佳组合。考虑到提示学习(prompt learning)涉及离散优化，而微调在连续参数空间(continuous parameter space)中操作，我们设计了一种监督正则化损失(supervised regularization loss)来有效训练我们的框架。在多个不同基准测试(benchmarks)上的大量实验表明，我们的方法始终优于基线方法(baselines)。",
                    "inspiration_trace": "# MetaTuner方法论逻辑演进分析\n\n## 一、宏观问题：LLM性能优化的二元路径\n\n论文从大语言模型(LLMs)性能优化的宏观视角出发，观察到存在两条并行但分离的研究路径：\n- **提示优化(Prompt Optimization)**：通过显式自然语言引导模型行为\n- **参数微调(Fine-tuning)**：通过隐式参数更新调整模型内部表示\n\n这两种方法从互补角度提升LLM能力，但学术界通常孤立研究它们，忽视了潜在的协同效应。\n\n## 二、关键观察：各自局限性与相互依赖\n\n通过初步实验(图1)，作者发现两个关键现象：\n\n1. **性能差异**：微调方法在MATH和HotpotQA数据集上平均表现优于提示优化策略\n2. **敏感性依赖**：微调效果高度依赖于所选提示，次优提示会导致性能显著下降，甚至低于纯提示优化方法\n\n这揭示了核心问题：两种方法各自存在明显局限性——提示优化难以适应复杂任务数据模式，而微调则受限于手动设计的提示可能不是最优的。\n\n## 三、核心假设：协同优化的可能性\n\n基于观察，作者提出核心假设：\n> 提示和参数可视为两个互补维度，共同影响从输入到输出的预测过程。通过统一框架优化它们，可以找到最佳组合，相互弥补弱点。\n\n这一假设引出了统一优化目标：\n```\nmin θ,pi Σ L(Mθ(pi, xi), yi)\n```\n其中提示被视为\"特殊参数\"，与模型参数共同优化。\n\n## 四、面临挑战：混合优化难题\n\n实现上述假设面临三大挑战：\n\n1. **机制差异**：提示优化是外部优化(寻找合适输入上下文)，微调是内部优化(修改参数拟合数据分布)\n2. **空间异质**：提示优化是离散优化问题，微调在连续参数空间操作\n3. **梯度障碍**：离散-连续混合优化导致非可微梯度问题，难以端到端训练\n\n## 五、解决方案：MetaTuner框架设计\n\n为解决上述挑战，作者提出MetaTuner框架，其核心设计包含三个关键创新：\n\n### 1. 提示生成连续化\n- **思路**：用神经网络生成提示，将离散优化转化为连续任务\n- **实现**：从初始提示出发，利用LLM重写生成优化提示\n- **公式化**：`pi = Gφ(˜p, xi)`，将离散问题转化为连续参数优化\n\n### 2. 联合生成架构\n- **思路**：设计共享-私有参数结构，实现提示和参数的协同生成\n- **实现**：引入共享元编码器，连接提示解码器和参数解码器\n- **公式化**：`min φs,φp,φq Σ L(MF(φs,φq)(˜p,xi)(G(φs,φp)(˜p, xi), xi), yi)`\n- **优势**：共享参数实现知识共享和相互正则化，私有参数保持各自灵活性\n\n### 3. 监督正则化优化\n- **思路**：设计特殊损失函数解决离散-连续混合优化问题\n- **实现**：将目标分为两项——固定提示参数的任务损失和提示生成的监督正则化损失\n- **公式化**：\n  ```\n  min φs,φp,φq Σ L(MF(φs,φq)(˜p,xi)(G(φs,φ'p)(˜p, xi), xi), yi) \n               + Σ α·L(G(φs,φp)(˜p, xi), pi)\n  ```\n- **优势**：避免Gumbel-Softmax等近似方法带来的梯度偏差，直接在离散空间优化\n\n## 六、实验验证：从理论到实践\n\n作者通过系统实验验证MetaTuner的有效性：\n\n1. **整体性能**：在四个数据集上显著优于基线方法，证明联合优化的优势\n2. **消融研究**：验证各组件贡献，确认共享结构和监督正则化的必要性\n3. **参数共享分析**：探索不同共享比例对性能的影响，发现模型大小与最佳共享比例相关\n4. **监督正则化分析**：确认优于Gumbel-Softmax等替代方案，并研究关键超参数影响\n5. **泛化能力**：证明在未见数据集上的优越表现，验证方法的鲁棒性\n\n## 七、逻辑演进总结\n\nMetaTuner的提出体现了清晰的逻辑演进链：\n```\n宏观问题(二元路径分离) → 关键观察(各自局限) → 核心假设(协同可能) → \n面临挑战(混合优化) → 解决方案(三重创新) → 实验验证(系统评估)\n```\n\n这一演进过程展示了作者从现象观察到理论假设，再到方法创新和实验验证的完整科学思维路径，成功地将提示优化和微调两个分离的研究方向统一到一个框架中，实现了\"1+1>2\"的协同效果。"
                },
                {
                    "title": "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight",
                    "arxiv_id": "2509.24169",
                    "authors": "Haolin Yang, Hakaze Cho, Kaize Ding, Naoya Inoue",
                    "summary": "Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of \"key heads\" most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)机制，并提出了一种新的方法——学习任务向量(LTVs)来增强这一能力。从本质上看，论文关注的是改进LLM的基础能力而非将其作为工具应用到特定领域，这符合第一步的核心判断标准。论文明确研究LLMs的内部工作机制，提出了直接训练任务向量的新范式，而不是从模型中提取，这可以被视为一种新的训练方法来增强LLM的能力。虽然论文没有直接针对推理或规划，但上下文学习是LLM的一项基础能力，与通用问题解决相关。论文深入分析了任务向量如何通过注意力头的OV电路引导预测，以及它们在Transformer中的传播方式，这种机制层面的理解可能对改进LLM的通用推理能力有重要启示。论文不符合任何排除标准，没有涉及多模态、特定应用领域或应用层面的模型可靠性。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究范围。",
                    "summary2": "本文旨在解决大型语言模型中任务向量(TVs)提取方法复杂低效且缺乏机制解释的问题。针对多种分类和生成任务场景，我们提出了一种直接训练学习任务向量(LTVs)的方法，并在七个数据集上通过准确率等指标验证了其有效性。实验表明，LTVs不仅优于提取的TVs，还揭示了TVs主要通过注意力头OV电路影响预测，以及其在模型中传播的线性机制。",
                    "summary_translation": "大型语言模型 (Large Language Models, LLMs) 能够通过上下文示例来执行新任务，这种现象被称为上下文学习 (in-context learning, ICL)。最近的研究表明，这些示例被压缩成任务向量 (task vectors, TVs)，这是LLMs用于预测的紧凑任务表示。然而，先前的研究通常使用繁琐且不透明的方法从模型输出或隐藏状态中提取TVs，并且很少阐明TVs影响计算的机制。在这项工作中，我们解决了这两个局限性。首先，我们提出直接训练学习型任务向量 (Learned Task Vectors, LTVs)，它们在准确性上超过提取的TVs，并表现出卓越的灵活性——能够在任意层、位置甚至与ICL提示一起有效工作。其次，通过系统分析，我们研究了TVs的机制作用，表明在低层次上，它们主要通过注意力头OV电路 (attention-head OV circuits) 来引导预测，其中一小部分\"关键头\" (key heads) 最具决定性。在更高层次上，我们发现尽管Transformer (Transformer) 存在非线性，TV的传播在很大程度上是线性的：早期的TV被旋转到与任务相关的子空间以提高相关标签的logits值 (logits)，而后期的TV主要在幅度上进行缩放。总而言之，LTVs不仅为获取有效的TVs提供了实用方法，还为了解ICL的机制基础提供了一个有原则的视角。",
                    "inspiration_trace": "# 从观察到创新：任务向量学习方法的逻辑演进\n\n## 1. 宏观问题：大型语言模型如何实现上下文学习？\n\n**观察现象**：大型语言模型(LLMs)展现出一种非凡能力——仅通过输入中提供的演示示例就能学习并执行新任务，这一现象被称为上下文学习(ICL)。这与传统机器学习范式形成鲜明对比，因为模型没有经过显式训练就能适应新任务。\n\n**核心疑问**：ICL的内部机制是什么？LLMs如何从演示示例中提取信息并用于新查询的预测？\n\n## 2. 中观聚焦：任务向量在ICL中的角色\n\n**现有发现**：先前研究(Hendel et al., 2023)提出，LLMs利用演示的方式是将它们压缩成任务向量(TVs)——演示所示例任务的简洁向量表示。这些TVs可以被注入到零样本提示的隐藏状态中，以实现ICL级别的性能。\n\n**识别局限**：现有TV研究存在两个关键缺陷：\n1. **方法效率低下**：现有方法依赖于复杂的过滤或优化来从模型表示中构建TVs，过程繁琐且不透明\n2. **机制理解不足**：大多数研究仅表明注入TVs提高了性能，但未阐明LLMs如何利用TVs做出正确预测的核心机制\n\n## 3. 具体问题：如何改进TV获取并揭示其工作机制？\n\n**方法分析**：现有TV提取方法存在明显不足：\n- **Vanilla TV**：定义为ICL与零样本提示的隐藏状态差异，准确性低且对注入层选择高度敏感\n- **Function Vector (FV)**：从选定的注意力头输出中构建，依赖于头选择过程，忽略了头间相互关联\n\n**关键假设**：\n1. 如果直接训练TVs而非从模型表示中提取，可能会获得更有效的TVs，不受模型表示质量限制\n2. 通过系统分析TVs与模型组件的相互作用，可以揭示TVs有效性的工作机制\n\n## 4. 方法创新：学习任务向量(LTVs)\n\n**核心思路**：摒弃提取范式，转而直接训练TVs。具体方法是将向量θ添加到特定层的隐藏状态，并通过梯度下降优化它，最小化预测损失：-log p(yq|xq, θ, L, P)\n\n**突破优势**：\n- 消除了对ICL隐藏状态的依赖，不受表示质量限制\n- 注入位置和层更加灵活，不限于最后位置或单层\n- 可同时训练和注入多个TVs，提高性能上限\n\n## 5. 机制探索：TVs如何影响模型计算？\n\n### 低层次机制\n**假设**：TVs主要通过注意力头的OV电路影响预测。\n\n**验证方法**：\n1. 通过OV电路重建TV效果：计算TV与所有后续层注意力头OV电路的乘积和，将其作为重建TV注入\n2. 识别关键注意力头：计算各头显著性分数，消融关键头与随机头对比\n\n**发现**：注入的TVs主要通过注意力头的OV电路被利用，一小部分\"关键头\"最具决定性，这些头主要分布在注入层后和最后层，且较少受注意力下沉影响。\n\n### 高层次机制\n**假设**：尽管Transformer存在非线性，TV传播可能主要是线性的。\n\n**验证方法**：分析TV注入后隐藏状态的层动态，跟踪三个指标：\n1. Logit Lens准确率：中间层隐藏状态解码准确率\n2. Logit差异：正确与错误标签间的logit差距\n3. 任务对齐：隐藏状态与标签unembeddings的平均余弦相似度\n\n**发现**：TV传播确实主要是线性的，可分解为旋转和缩放两个操作：\n- 早期TVs：被旋转到与任务相关子空间对齐，提高相关标签的logits\n- 后期TVs：主要在幅度上被缩放，旋转效应减弱\n\n## 6. 统一理论：旋转-缩放动态\n\n**综合理解**：TVs的工作机制可统一为旋转-缩放动态：\n- 在低层次，TVs通过注意力头OV电路实现其效果\n- 在高层次，后续层对TVs进行线性变换，早期以旋转为主，后期以缩放为主\n- 这种动态为不同深度的TVs如何塑造最终预测提供了统一解释\n\n**理论意义**：LTVs不仅提供了获取有效TVs的实用方法，还为理解ICL的机制基础提供了原则性视角。\n\n这一逻辑链展示了从观察ICL现象，到关注TVs作用，发现现有方法局限，提出创新方法，最终深入揭示机制的完整思考过程，体现了科学研究从现象到本质的演进路径。"
                },
                {
                    "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
                    "arxiv_id": "2509.24193",
                    "authors": "Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang",
                    "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。AceSearcher提出了一个合作式自我对弈框架，训练单个大语言模型在分解器和求解器两个角色间切换，这是一种新的训练范式，旨在增强LLM的复杂推理能力。论文结合了监督微调和强化微调，优化最终答案准确性，这些都是提升LLM通用推理能力的方法论研究。 其次，论文包含多个正面指标主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于推理能力(reasoning)，特别是复杂推理任务和多步推理 - 训练方法：使用强化学习(reinforcement learning)和自我对弈(self-play)框架 - 新兴范式：提出了类似智能体的框架，让同一模型在不同角色间切换 第三，论文不主要聚焦于任何排除标准中的领域。虽然提到了在金融推理任务上的实验，但这只是作为评估模型性能的一个应用场景，而非论文核心焦点。论文的核心是提出一种通用的推理和搜索增强框架。 最后，在特殊和模糊情况处理上，论文提出的自我对弈框架可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用在特定领域，因此符合保留标准。 综上所述，AceSearcher论文的核心贡献是提出了一种通过强化自我对弈来增强LLM推理和搜索能力的通用框架，完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决搜索增强型LLM在复杂推理任务中多跳检索效率低且推理能力有限的问题。针对复杂推理场景，我们提出了一种AceSearcher协作式自我博弈框架，训练单个LLM在分解器和求解器两个角色间切换，并通过两阶段微调（SFT和RFT）优化模型。在10个数据集上的三个推理密集型任务中，通过精确匹配等指标验证了其有效性，平均提升7.6%，且AceSearcher-32B以不到DeepSeek-V3 5%的参数达到相当性能。",
                    "summary_translation": "搜索增强型大语言模型（Search-augmented LLMs）通常在复杂推理任务上表现不佳，这是由于无效的多跳检索（multi-hop retrieval）和有限的推理能力所致。我们提出了AceSearcher，一种合作自我对弈框架（cooperative self-play framework），该框架训练单个大语言模型（LLM）在两个角色之间交替：一个是分解器（decomposer），负责分解复杂查询；另一个是求解器（solver），负责整合检索到的上下文以生成答案。AceSearcher将在搜索、推理和分解任务的多样化混合数据集上的监督微调（supervised fine-tuning）与针对最终答案准确性优化的强化微调（reinforcement fine-tuning）相结合，消除了对中间标注的需求。在10个数据集上的三个推理密集型任务（reasoning-intensive tasks）的广泛实验表明，AceSearcher优于最先进的基线模型（state-of-the-art baselines），实现了平均精确匹配（exact match）提升7.6%。值得注意的是，在文档级金融推理任务（document-level finance reasoning tasks）上，AceSearcher-32B仅使用不到DeepSeek-V3模型5%的参数，就达到了与之相当的性能。即使在较小规模（1.5B和8B参数）下，AceSearcher也常常超越参数量多达其9倍的现有搜索增强型大语言模型，凸显了其在处理复杂推理任务方面的卓越效率和有效性。我们的代码将发布于https://github.com/ritaranx/AceSearcher和https://huggingface.co/AceSearcher。",
                    "inspiration_trace": "# AceSearcher核心方法的逻辑推演\n\n## 1. 宏观问题：搜索增强LLMs的复杂推理困境\n\n作者从观察到一个核心问题出发：**现有搜索增强大型语言模型(LLMs)在复杂推理任务上表现不佳**。具体表现为：\n- 难以有效进行多跳检索(multi-hop retrieval)\n- 在需要整合多段信息进行推理的任务上能力有限\n- 现有方法大多局限于简单问题，无法处理真实世界中的复杂推理需求\n\n## 2. 现有方法的局限性分析\n\n作者系统分析了三类主流方法的缺陷：\n\n### 2.1 传统RAG方法的局限\n- **单步检索限制**：大多数RAG系统仅考虑单步检索，无法处理需要多跳信息收集的复杂问题\n- **简单问题偏向**：主要设计用于可通过单次检索解决的简单问题\n- **长尾知识处理不足**：在处理长尾或动态变化的知识时表现不佳\n\n### 2.2 多步搜索方法的局限\n- **依赖强大闭源模型**：迭代提示方法通常需要强大的闭源LLMs作为基础\n- **推理延迟增加**：树搜索算法虽能提高效果，但显著增加了推理时间\n- **推理能力要求高**：这些方法假设模型已具备强推理能力，但未专门针对此进行优化\n\n### 2.3 强化学习方法的局限\n- **内存密集型**：现有RL框架内存消耗大，不适合资源受限环境部署\n- **数据依赖性强**：过度依赖QA数据集进行监督，限制了在更广泛任务上的应用\n- **中间标注需求**：通常需要中间步骤的标注，获取成本高\n\n## 3. 核心假设与思路形成\n\n基于上述分析，作者提出了三个关键假设：\n\n### 3.1 人类问题解决策略假设\n**核心假设**：人类解决复杂问题的策略——将复杂任务分解为更简单的子问题——可以应用于LLMs。\n\n作者观察到人类在面对复杂问题时，会自然地将问题分解为更小、更易管理的子问题，然后逐一解决。这种策略被认为是解决复杂推理的有效方法。\n\n### 3.2 分解质量与答案质量关联假设\n**关键洞见**：更好的问题分解会导致更准确的答案。\n\n作者假设，如果能够有效地将复杂问题分解为合适的子问题，那么基于这些子问题检索到的相关信息将更加相关和全面，从而提高最终答案的准确性。\n\n### 3.3 单一模型多角色协同假设\n**创新思路**：单一LLM可以通过扮演不同角色来协同完成复杂任务。\n\n作者提出，不需要多个专门的模型，而是训练一个LLM在适当的时候扮演不同的角色——分解器和求解器——通过自我协作来完成复杂推理任务。\n\n## 4. AceSearcher框架设计\n\n基于上述假设，作者设计了AceSearcher框架：\n\n### 4.1 双角色定义\n- **分解器(Decomposer, ρ)**：将原始问题q分解为子问题序列z = (z₁, z₂, ..., zₙ)\n- **求解器(Solver, π)**：逐步生成中间答案w = (w₁, w₂, ..., wₙ)和最终答案a'\n\n### 4.2 联合学习目标\n作者定义了联合学习目标：\n```\npθ(a|q) = Σz pθ(z|q) Σw pθ(a|q,z,w) pθ(w|q,z)\n```\n这一目标反映了分解器和求解器的协同作用：分解器生成子问题，求解器基于子问题和检索到的上下文生成答案。\n\n### 4.3 两阶段训练框架\n作者设计了两阶段训练框架来解决不同层面的挑战：\n\n**第一阶段：监督微调(SFT)**\n- **目标**：建立基础能力\n- **数据策略**：混合三类数据\n  - 上下文丰富的QA数据（增强上下文利用能力）\n  - 问题分解数据（提升问题分解能力）\n  - 思维链数据（增强多步推理能力）\n- **训练方法**：标准下一个标记预测目标\n\n**第二阶段：强化微调(RFT)**\n- **目标**：在缺乏中间标注的情况下优化性能\n- **核心创新**：仅使用最终答案准确率作为奖励信号\n- **环境设计**：\n  - RAG环境：处理多跳QA和事实验证\n  - 上下文推理环境：处理表格、段落等复杂推理\n- **奖励设计**：结合精确匹配(EM)和格式正确性\n- **优化方法**：迭代偏好优化(Iterative DPO)\n\n## 5. 方法实现的关键创新\n\n### 5.1 无需中间标注的强化学习\n作者面临的核心挑战是如何在没有中间步骤标注的情况下训练模型。他们的解决方案是：\n\n1. **假设验证**：通过实验验证\"更好的分解导致更准确答案\"的假设\n2. **奖励设计**：仅基于最终答案的准确性设计奖励函数\n3. **联合优化**：同时优化分解器和求解器，使分解器产生有利于求解器生成正确答案的子问题\n\n### 5.2 偏好数据构建策略\n为解决稀疏奖励信号下的高方差问题，作者设计了创新的偏好数据构建方法：\n\n1. **多轨迹采样**：对每个问题生成多个候选分解和解决方案\n2. **基于期望奖励的排序**：根据对应解决方案的期望奖励对分解进行排序\n3. **多层次偏好对**：构建分解偏好对、子问题偏好对和最终答案偏好对\n4. **统一优化**：将三类偏好对合并，统一优化分解器和求解器\n\n### 5.3 实际效率优化\n作者还考虑了实际部署的效率问题：\n\n1. **推理策略优化**：对中间问题直接生成答案，仅对最终答案生成完整推理过程\n2. **上下文长度控制**：限制每个子问题的文档数量，防止输入过长\n3. **迭代优化平衡**：采用两轮迭代DPO，平衡性能与效率\n\n## 6. 验证与完善逻辑链\n\n作者通过大量实验验证了AceSearcher的有效性，完善了整个逻辑链：\n\n### 6.1 整体性能验证\n- 在10个数据集上的三个任务（多跳QA、多跳事实验证、文档级推理）上评估\n- 结果显示AceSearcher平均精确匹配提高7.6%，显著优于现有方法\n- 证明了方法在不同任务和领域的通用性\n\n### 6.2 参数效率验证\n- AceSearcher-32B使用不到DeepSeek-V3模型5%的参数就达到匹配性能\n- 小规模模型(1.5B和8B)常超过参数多达9倍的现有搜索增强LLMs\n- 证明了方法的高效性和可扩展性\n\n### 6.3 消融研究验证\n- 验证了SFT和RFT两个阶段的必要性\n- 验证了分解器(ρ)和求解器(π)两个组件的互补作用\n- 验证了SFT数据混合中搜索和推理数据的重要性\n- 比较了不同RL算法的效果，确认了所提方法的优势\n\n### 6.4 效率研究验证\n- 数据效率：仅用2K SFT示例就能匹配强基线，经过RFT后超越它们\n- 推理效率：虽然比标准RAG有更高延迟，但实现了显著性能提升\n- 证明了方法在资源受限环境中的实用性\n\n## 7. 逻辑链总结\n\nAceSearcher核心方法的逻辑推演展现了从宏观问题到具体解决方案的完整思考过程：\n\n1. **问题观察**：搜索增强LLMs在复杂推理任务上的局限性\n2. **局限分析**：系统分析现有方法在多跳检索和复杂推理上的不足\n3. **假设形成**：基于人类问题解决策略提出三个核心假设\n4. **框架设计**：提出双角色协作的AceSearcher框架和两阶段训练策略\n5. **方法实现**：解决无中间标注的强化学习挑战，设计创新的偏好数据构建策略\n6. **验证完善**：通过全面实验验证方法有效性、效率和各组件贡献\n\n这一逻辑链不仅解决了搜索增强LLMs在复杂推理任务上的核心挑战，还提供了一种高效、可扩展的解决方案，为未来相关研究提供了新的思路和方法论。"
                },
                {
                    "title": "Can Large Language Models Express Uncertainty Like Human?",
                    "arxiv_id": "2509.24202",
                    "authors": "Linwei Tao, Yi-Fan Yeh, Bo Kai, Minjing Dong, Tao Huang, Tom A. Lamb, Jialin Yu, Philip H. S. Torr, Chang Xu",
                    "summary": "Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究大语言模型如何像人类一样表达不确定性，通过语言置信度(Linguistic Confidence, LC)的方式提升模型表达可靠不确定性的能力。这不是将LLM作为工具应用到特定领域的研究，而是专注于提升LLM本身的一种基础能力——准确评估和表达自己的知识边界和置信度。这种能力对于模型的推理质量和可靠性至关重要，因此符合保留标准。 第二步：正面指标 论文明确涉及\"Large language models, LLMs\"这一核心概念。虽然未直接提及数学推理、逻辑推理或规划，但不确定性表达与推理质量密切相关，可靠的置信度估计能增强推理任务的准确性和可信度，因此与能力方向间接相关。 第三步：排除标准 论文不符合任何排除标准。它不涉及多模态与视觉研究，不是针对特定应用领域（如医疗、化学等）的研究，也不主要关注模型可靠性的应用层面（如水印、安全等）。 第四步：特殊和模糊情况 论文提出了一种新方法（语言置信度LC和相关的微调框架）来增强模型表达不确定性的能力，这可以视为提升模型内在可靠性和推理质量的方法。通过更准确地表达不确定性，模型可以减少过度自信导致的误导性回答，从而间接减少幻觉和提高推理质量，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种提升大语言模型表达不确定性能力的方法，这属于改进LLM基础能力的研究，与提高模型通用推理能力的研究目标一致，因此应该被保留。",
                    "summary2": "本文旨在解决大语言模型如何像人类一样自然表达不确定性的问题。针对大语言模型在高风险场景中的不确定性表达需求，我们提出了一种基于语言置信度(Linguistic Confidence)的方法，通过模糊限制语表达不确定性，并在SimpleQA、PopQA和NQ-Open等QA基准上通过校准误差(ECE)和AUROC指标验证了其有效性。",
                    "summary_translation": "大语言模型（LLMs）越来越多地被用于高风险场景，在这些场景中，过度自信的回应可能会误导用户。可靠的置信度估计（confidence estimation）已被证明能够增强信任度和任务准确性。然而，现有方法面临实际障碍：logits（模型输出层的原始分数）通常被隐藏，多重采样（multi-sampling）计算成本高，而口头表达的数值不确定性（verbalized numerical uncertainty）（例如给出0-100的分数）偏离了自然交流方式。我们重新审视语言置信度（linguistic confidence, LC），其中模型通过模糊限制语（hedging language）（例如\"probably\"、\"might\"）表达不确定性，提供了一种轻量级且以人为中心的替代方案。为推进这一方向，我们（1）发布了首个多样化、大规模的模糊限制语数据集，其中包含人工标注的置信度分数；并且（2）提出了一种轻量级映射器（mapper），能够以几乎零成本将模糊限制语转换为置信度分数。基于这些资源，我们（3）对现代LLMs和QA（问答）基准测试中的LC进行了首次系统性研究，结果表明，尽管大多数LLMs在表达可靠LC方面表现不佳，但精心设计的提示（prompting）能够实现具有竞争力的校准性（calibration）和区分度（discriminability）。最后，我们（4）引入了一种微调（fine-tuning）框架，进一步提高了LC的可靠性。总而言之，我们的工作将语言置信度定位为一种可扩展、高效且符合人类思维的（human-aligned）LLMs不确定性估计方法，并呼吁对这一有前景但尚未充分探索的方向进行更深入的研究。",
                    "inspiration_trace": "# 从问题到方法：大型语言模型表达不确定性的逻辑演进\n\n## 一、宏观问题：LLMs如何可靠地表达不确定性？\n\n### 观察现实需求\n大型语言模型(LLMs)在教育、医疗、法律等高风险领域应用日益广泛，但存在一个根本矛盾：模型可能产生幻觉和事实错误，而人类用户往往过度依赖其输出。例如，有律师使用ChatGPT生成的虚假案例导致专业制裁。这引出一个核心问题：**如何让LLMs可靠地表达不确定性，以支持人类决策？**\n\n### 现有方法的局限性分析\n作者系统考察了三类主流不确定性表达方法，发现各自存在明显缺陷：\n\n1. **基于token概率的方法**（如perplexity）：\n   - 优势：简单、计算成本低\n   - 局限：需要访问模型内部logits，而商业LLM API通常不提供这些信息\n\n2. **多次采样的方法**（如语义熵、自洽性）：\n   - 优势：通常效果较好\n   - 局限：计算成本高，需要多次模型调用或辅助网络，实用性受限\n\n3. **口头化数值不确定性**（如直接给出0-100分数）：\n   - 优势：直接明了\n   - 局限：偏离自然交流方式，用户不会自然地以这种方式提问\n\n这些观察揭示了一个研究空白：**需要一种既高效又符合人类交流习惯的不确定性表达方法**。\n\n## 二、提出假设：语言置信度(LC)作为替代方案\n\n### 从人类交流中获取灵感\n作者观察到人类在表达不确定性时通常使用模糊语言（如\"probably\"、\"might\"、\"I am not entirely sure\"）。这引发了一个关键假设：**语言置信度(Linguistic Confidence, LC)——通过自然语言表达不确定性——可能是一个理想的替代方案**。\n\n### 假设的理论优势\nLC方法具有三个潜在优势：\n1. **无缝集成**：自然融入回应中，不需额外结构\n2. **计算高效**：几乎不需要额外计算开销\n3. **人类对齐**：模仿人类自然传达不确定性的方式\n\n### 现有研究的不足\n作者发现相关研究（如Yona等人2024年的工作）存在明显局限：\n- 仅在小数据集（仅18个不确定性短语）上验证\n- 未在标准不确定性估计指标（如校准和区分性）下评估\n- 使用LLM进行置信度映射，计算成本高（约$3/评估轮次）\n\n这些不足表明，**LC作为不确定性表达方法尚未得到充分探索**，需要更系统的研究。\n\n## 三、验证假设：构建数据集和评估方法\n\n### 关键挑战识别\n为验证LC的有效性，作者识别出两个核心挑战：\n1. **缺乏大规模、多样化的模糊表达数据集**\n2. **缺乏高效可靠的置信度映射方法**\n\n### 解决方案一：构建人类标注的数据集\n作者设计了一个五步流程构建高质量数据集：\n\n1. **收集不确定表达**：\n   - 从SimpleQA采样200个问题\n   - 使用4个LLMs生成不同置信度水平的表达（高、中、低、最低、完全不确定）\n   - 获得40,000个表达，从中采样10,000个\n\n2. **获取人类标注**：\n   - 通过Amazon Mechanical Turk平台\n   - 每个表达由5人标注0-100的置信度分数\n   - 包含5个专家预先标注的验证项用于质量控制\n\n3. **筛选可靠标注者**：\n   - 识别遵循指示的标注者（如对拒绝回答的表达给出0分）\n   - 使用这些可靠标注者的数据作为筛选基准\n\n4. **确定有效分数范围**：\n   - 对每个置信度水平，计算平均值±1标准差的范围\n   - 将此范围作为有效注释的上下限\n\n5. **构建最终数据集**：\n   - 保留至少有3个有效注释的表达\n   - 使用平均分数作为最终置信度标注\n\n### 解决方案二：开发轻量级置信度映射器\n为解决LLM-based映射方法的高成本问题，作者开发了轻量级模型：\n\n1. **模型设计**：\n   - 使用DistilRoBERTa作为编码器\n   - 后接带sigmoid激活的线性层\n   - 输出0-1之间的置信度分数\n\n2. **训练策略**：\n   - 在LLM生成的句子数据集上训练（标签：完全不确定=0，最低=0.25，低=0.5，中等=0.75，高=1）\n   - 在人类标注的表达上训练（之前因注释不足被丢弃的数据）\n\n3. **评估结果**：\n   - MSE显著低于LLM-based基线（50.68 vs 183.23-385.40）\n   - 计算成本接近零，延迟极低（1.32秒 vs 551-678秒）\n\n这一步验证了**可以高效可靠地将模糊语言映射为数值置信度**，为后续系统性研究奠定了基础。\n\n## 四、系统性研究：评估LLMs的语言置信度\n\n### 研究设计\n作者在多个QA基准（SimpleQA、NQ-Open、PopQA）上评估了多种现代LLMs的LC性能：\n\n1. **评估维度**：\n   - **校准(Calibration)**：预测置信度与实际正确性的对齐程度（使用ECE评估）\n   - **区分性(Discriminability)**：区分正确与错误答案的能力（使用AUROC评估）\n\n2. **对比方法**：\n   - 口头化数值置信度(VNC)\n   - 语义不确定性(SU)\n   - 自评估方法P(True)和困惑度（仅开源模型）\n\n3. **提示策略**：\n   - LC：普通QA提示\n   - LC+：明确指示模型在不确定时使用模糊语言\n\n### 关键发现\n实验结果揭示了几个重要现象：\n\n1. **普通LC表现不佳**：\n   - 在默认提示下，大多数模型的LC校准和区分性都很差\n   - AUROC仅略高于50%，几乎无法区分知道与不知道的内容\n\n2. **明确提示显著改善LC+性能**：\n   - LC+在校准和区分性上大幅提升\n   - 在某些模型（如Qwen3-235b）上达到最佳校准性能\n   - 与语义不确定性等强基线相当\n\n3. **推理过程的积极影响**：\n   - 增加推理预算不仅提高了数值置信度校准\n   - 也显著增强了语言置信度的表达\n\n这些发现表明：**LLMs确实有能力通过语言表达不确定性，但需要适当的提示或推理才能可靠地实现**。\n\n## 五、改进方法：微调框架以提高LC可靠性\n\n### 问题识别\n虽然LC+提示改善了性能，但它需要额外指令，偏离了自然交互。更理想的情况是模型在直接回答问题时就能自然表达不确定性。\n\n### 解决方案：微调框架\n作者设计了一个四步微调框架：\n\n1. **计算语义不确定性作为监督信号**：\n   - 使用基础模型为每个问题生成10个答案\n   - 计算语义不确定性作为不确定性代理\n   - 将其离散化为五个水平（完全不确定、最低、低、中等、高）\n\n2. **生成不确定句子**：\n   - 使用SOTA LLMs（如GPT-5）生成与指定置信度水平一致的表达\n   - 预构建不确定性句子数据库以简化实现\n\n3. **构建微调数据集**：\n   - 从SimpleQA采样200个问题\n   - 为每个问题检索40个LLM生成的回应\n   - 形成8,000个问题-答案对\n\n4. **微调基础模型**：\n   - 采用LoRA进行高效微调\n   - 保持大多数预训练权重冻结，降低计算成本\n\n### 实验结果\n在Qwen3-8B模型上的评估显示：\n\n1. **一致的性能提升**：\n   - 与基础模型相比，微调模型在校准和区分性上均有改进\n   - 在NQ-Open上超越了VNC和SU等强基线\n\n2. **更自然的表达**：\n   - 微调后的模型在回答时自然使用模糊语言\n   - 不需要额外提示就能表达不确定性\n\n这些结果表明，**通过适当微调，可以使LLMs在自然交互中可靠地表达语言置信度**。\n\n## 六、结论与展望：语言置信度的定位与未来\n\n### 核心贡献总结\n作者将语言置信度定位为LLMs不确定性估计的可扩展、高效且与人类对齐的方法，主要贡献包括：\n\n1. **首个大规模、多样化的模糊表达数据集**\n2. **轻量级置信度映射器**，实现高效可靠的LC评估\n3. **对现代LLMs的LC进行首次系统性研究**\n4. **微调框架**，提高LC的可靠性\n\n### 局限性与未来方向\n作者也坦诚研究的局限性：\n\n1. **主观性挑战**：模糊语言的感知因人而异，将平均分数作为真值不是最优解\n2. **默认性能不足**：LC并非固有可靠，需要明确提示或微调才能发挥作用\n3. **应用范围有限**：研究主要集中在QA任务，未来需扩展到推理和多模态场景\n\n### 逻辑链总结\n从宏观问题到具体方法，作者的思考路径清晰可见：\n\n**现实需求→现有方法局限→提出LC假设→构建评估工具→系统性验证→方法改进→定位与展望**\n\n这一完整逻辑链条展示了如何从观察问题到提出假设，再到验证假设并改进方法，最终形成一个系统的研究框架。作者不仅解决了LLMs不确定性表达的实际问题，还为这一领域开辟了新的研究方向，体现了学术研究的创新性和系统性。"
                },
                {
                    "title": "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis",
                    "arxiv_id": "2509.24164",
                    "authors": "Haolin Yang, Hakaze Cho, Naoya Inoue",
                    "summary": "We investigate the mechanistic underpinnings of in-context learning (ICL) in large language models by reconciling two dominant perspectives: the component-level analysis of attention heads and the holistic decomposition of ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel framework based on Task Subspace Logit Attribution (TSLA) to identify attention heads specialized in TR and TL, and demonstrate their distinct yet complementary roles. Through correlation analysis, ablation studies, and input perturbations, we show that the identified TR and TL heads independently and effectively capture the TR and TL components of ICL. Using steering experiments with geometric analysis of hidden states, we reveal that TR heads promote task recognition by aligning hidden states with the task subspace, while TL heads rotate hidden states within the subspace toward the correct label to facilitate prediction. We further show how previous findings on ICL mechanisms, including induction heads and task vectors, can be reconciled with our attention-head-level analysis of the TR-TL decomposition. Our framework thus provides a unified and interpretable account of how large language models execute ICL across diverse tasks and settings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究大语言模型(LLM)的上下文学习(ICL)机制，通过分析注意力头来理解任务识别(TR)和任务学习(TL)的内部工作机制。论文提出了TSLA框架来识别专门用于TR和TL的注意力头，并揭示了它们在ICL过程中的不同作用。这属于对LLM基础能力和内部工作机制的研究，特别是关注上下文学习这种通用推理能力的机制分析，而不是将LLM作为工具应用到特定领域。因此，根据第一步的标准，应该保留。 第二步：正面指标 - 核心概念：论文明确研究大语言模型(LLMs)的上下文学习机制，符合这一指标。 - 能力方向：上下文学习(ICL)本身就是一种通用推理能力的体现，论文研究的是ICL的机制，涉及到任务识别和任务学习，这与推理能力直接相关，符合这一指标。 - 训练方法：论文没有明确提到强化学习、进化或自我进化等训练方法，不符合这一指标。 - 新兴范式：论文没有直接讨论基于LLM的智能体、多智能体系统、工具使用或深度研究等新兴范式，不符合这一指标。 第三步：排除标准 论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步：特殊和模糊情况 论文关注的是LLM内部工作机制的可解释性，通过分析注意力头来理解上下文学习的机制。这属于\"增强模型内在的可解释性\"的情况，从而提升我们对模型推理能力的理解。根据标准，应该保留。 最终决策： 这篇论文的核心贡献是提供了一个统一且可解释的框架，用于理解大语言模型如何执行上下文学习。这种研究有助于我们深入理解LLM的通用推理机制，从而可能指导未来改进LLM推理能力的方法开发。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在揭示大型语言模型中上下文学习(ICL)的机制基础。针对ICL中的任务识别(TR)和任务学习(TL)组件，我们提出了一种基于任务子空间Logit归因(TSLA)的新框架，用于识别专门负责TR和TL的注意力头。在多个数据集上通过相关性分析、消融研究和输入扰动验证了这些头的独立作用，并通过几何分析揭示了TR头对齐隐藏状态与任务子空间，而TL头在子空间内旋转状态朝向正确标签的机制。",
                    "summary_translation": "我们通过协调两种主流观点来研究大语言模型中上下文学习(in-context learning, ICL)的机制基础：注意力头(attention heads)的组件级分析(component-level analysis)以及将ICL整体分解(holistic decomposition)为任务识别(Task Recognition, TR)和任务学习(Task Learning, TL)。我们提出了一个基于任务子空间Logit归因(Task Subspace Logit Attribution, TSLA)的新框架，用于识别专门负责TR和TL的注意力头，并展示了它们不同但互补的作用。通过相关性分析(correlation analysis)、消融研究(ablation studies)和输入扰动(input perturbations)，我们证明了所识别的TR和TL头能够独立且有效地捕捉ICL的TR和TL组件。通过结合隐藏状态(hidden states)几何分析(geometric analysis)的引导实验(steering experiments)，我们揭示了TR头通过将隐藏状态与任务子空间(task subspace)对齐来促进任务识别，而TL头则在子空间内将隐藏状态旋转至正确标签以促进预测。我们进一步展示了先前关于ICL机制的发现，包括归纳头(induction heads)和任务向量(task vectors)，如何与我们对TR-TL分解的注意力头级分析相协调。因此，我们的框架为大语言模型如何在各种任务和设置中执行ICL提供了一个统一且可解释的说明。",
                    "inspiration_trace": "# 推演作者核心方法的逻辑链\n\n## 1. 宏观问题识别：ICL机制理解的范式鸿沟\n\n作者首先观察到大型语言模型(LLMs)的上下文学习(ICL)能力存在两种主导研究范式：\n- **内省范式**：关注模型内部组件(如注意力头)的作用，能定位特定组件但缺乏功能解释\n- **整体范式**：将ICL分解为任务识别(TR)和任务学习(TL)两个功能组件，提供功能视图但无法追溯到具体组件\n\n**核心问题**：两种范式各自存在局限性，缺乏一个既能提供机制精确性又能提供功能清晰度的统一框架。\n\n## 2. 问题聚焦：连接功能组件与具体注意力头\n\n作者将问题聚焦于一个关键挑战：如何将ICL的功能分解(TR和TL)与具体的模型组件(注意力头)联系起来？\n\n具体需要回答：\n- 哪些注意力头专门负责TR和TL功能？\n- 这些头如何协同工作实现完整ICL？\n- 既有发现(如归纳头、任务向量)如何与TR-TL框架协调？\n\n## 3. 现有方法的局限性分析\n\n作者深入分析了识别TR/TL头的现有方法(Lieberum等, 2023)的缺陷：\n\n**TR头识别问题**：\n- 现有方法仅关注对特定演示标签(如positive/negative)的logit贡献\n- 但标签是任意超参数，改变标签(如改为favourable/unfavourable)不改变任务本质\n- 因此，现有方法无法捕捉超越表面标签的任务语义\n\n**TL头识别问题**：\n- 现有方法仅关注提升正确标签logit的头\n- 忽略了标签间的竞争：提升正确标签的头可能同时提升错误标签\n- 无法区分真正的任务映射头与普遍提升所有标签的头\n\n**核心洞察**：需要一种能(a)捕捉任务语义本质和(b)评估标签间相对贡献的新方法。\n\n## 4. 几何视角的核心假设\n\n基于LLMs将相关语义编码为子空间的特性，作者形成了关键假设：\n\n**TR头假设**：\n- TR头应使其输出与任务子空间(由演示标签的unembedding向量张成)对齐\n- 这种对齐应独立于具体标签选择，反映任务语义的本质\n\n**TL头假设**：\n- TL头应在任务子空间内增加正确与错误标签间的logit差异\n- 几何上表现为在子空间内将隐藏状态朝正确标签方向旋转\n\n这些假设将功能概念(TR/TL)转化为可测量的几何性质。\n\n## 5. 方法论创新：任务子空间Logit归因(TSLA)\n\n基于上述假设，作者提出了TSLA方法：\n\n**TR分数设计**：\n```\nTR_score = ∥Proj_{W_Y^U} a_{lN,k}∥²\n```\n- 测量头输出在任务子空间上的投影范数\n- 捕捉对任务相关语义的整体贡献，不依赖具体标签选择\n- 提供理论保证：高TR分数意味着头输出最佳匹配任务子空间\n\n**TL分数设计**：\n```\nTL_score = Ave_{y'∈Y/{y*}}(a_{l,N,k}^⊤(W_{y*}^U - W_{y'}^U)) / ∥Proj_{W_Y^U} a_{lN,k}∥²\n```\n- 分子测量头创造的标签间logit差异\n- 分母归一化，评估在任务子空间内的相对贡献\n- 几何上表示投影头输出与正确-错误标签差异的对齐程度\n\n## 6. 多层次验证策略\n\n作者设计了全面的验证策略，从多个角度检验方法：\n\n**头特化验证**：\n- 分析TR、TL头与归纳头(IH)的重叠和相关性\n- 研究不同层中这些头的分布模式\n\n**功能独立性验证**：\n- 引入新指标\"任务识别比\"(TR ratio)分离TR和TL贡献\n- 通过消融实验测试TR和TL头的独立作用\n- 在输入扰动(打乱文本、重标记)下验证独立性\n\n**几何机制验证**：\n- 通过转向实验检验TR/TL头作为任务向量的效果\n- 分析TR/TL输出对隐藏状态几何性质的影响\n- 层面分析验证TR/TL头对几何变化的驱动作用\n\n## 7. 理论整合与统一框架建立\n\n通过实验结果，作者整合了既有发现并建立统一框架：\n\n**关键发现**：\n- TR头与IH高度重叠，表明IH主要通过增强TR影响ICL\n- 零样本预测失败主要源于隐藏状态与任务子空间弱对齐\n- TR输出对齐任务子空间，TL输出在子空间内旋转朝正确标签\n\n**统一框架**：\n- TR头负责将隐藏状态与任务子空间对齐，实现任务识别\n- TL头在子空间内旋转隐藏状态朝向正确标签，实现任务学习\n- 这种机制共同解释了ICL如何在不同任务和设置中执行\n\n这一框架不仅解释了ICL的基本机制，还自然调和了归纳头、任务向量等既有发现，提供了组件级与功能级视角的统一理解。"
                },
                {
                    "title": "Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE",
                    "arxiv_id": "2509.24130",
                    "authors": "Guancheng Wan, Lucheng Fu, Haoxin Liu, Yiqiao Jin, Hui Yi Leong, Eric Hanchen Jiang, Hejia Geng, Jinhe Bi, Yunpu Ma, Xiangru Tang, B. Aditya Prakash, Yizhou Sun, Wei Wang",
                    "summary": "The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础能力，具体关注提示词工程的鲁棒性问题。论文提出的TARE框架通过减少提示词在语义相同但表达不同情况下的性能波动，增强了LLM的稳定性和可靠性。提示词工程是直接影响LLM推理能力和输出质量的关键因素，因此这属于改进LLM基础能力的范畴，应当保留。 第二步：正面指标 - 核心概念：论文明确聚焦于Large Language Models (LLMs)，符合这一指标。 - 能力方向：虽然论文没有直接讨论推理能力，但提示词的鲁棒性直接影响LLM在推理任务中的表现。稳定的提示词可以确保LLM在面对不同表述的相同问题时保持一致的推理能力。 - 训练方法：论文提到了\"evolutionary search\"作为现有方法之一，并提出了\"Textual Sharpness-Aware Evolving\"(TARE)框架，涉及进化方法，部分符合这一指标。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况 论文关注提示词鲁棒性，通过减少提示词微小变化导致的性能波动，间接提高了模型输出的稳定性和可靠性。这可以视为提升模型内在可靠性的方法，有助于提高LLM的通用推理质量，因此应当保留。 综合判断：该论文提出的TARE框架通过优化提示词的鲁棒性，提高了LLM在面对语义相同但表达不同的问题时的稳定性，这直接关系到LLM的通用推理能力和问题解决能力，符合研究范围的核心目标。",
                    "summary2": "本文旨在解决大型语言模型提示优化中的脆弱性问题，即提示在语义保留的释义下表现不稳定。针对提示景观的文本锐度问题，我们提出了一种TARE（Textual Sharpness-Aware Evolving）框架，通过内部对抗性搜索和外部鲁棒性选择来优化提示的鲁棒性，并在Big Bench Hard和GSM8K数据集上通过准确率指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的性能取决于精心设计的提示（prompts）。然而，现有的提示优化方法，从启发式编辑（heuristic edits）和强化学习（reinforcement learning）到进化搜索（evolutionary search），主要针对点状准确率（point-wise accuracy）。这些方法很少强制执行释义不变性（paraphrase invariance）或搜索稳定性（searching stability），因此无法在实践中解决这种脆弱性问题。自动提示搜索仍然脆弱：微小的、保持语义的释义（semantically preserving paraphrases）常常导致性能大幅波动。我们将这种脆弱性识别为提示景观（prompt landscape）的文本锐度（textual sharpness）。\n\n在这项工作中，我们首次对提示在离散语义空间（discrete, semantic space）中的文本锐度进行了形式化处理，并提出了一个在语义邻域（semantic neighborhood）上的可操作鲁棒性标准（operational robustness criterion）；该设计是黑盒（black-box）或仅API（API-only）的，不需要梯度来更新模型参数。然后，我们介绍了TARE（Textual Sharpness-Aware Evolving，文本锐度感知进化），这是一个无导数框架（derivative-free framework），它在内部基于采样的对抗搜索（sampling-based adversarial search）和外部鲁棒选择（robust selection）之间交替进行，前者通过困难的释义（hard paraphrases）对提示施加压力，后者则倾向于选择那些邻域仍然保持强大的候选者。我们进一步提出了ATARE，它学习各向异性权重（anisotropic weights）来塑造语义邻域，并随时间调整其半径以平衡探索（exploration）和保真度（fidelity）。\n\n多样化的任务评估了我们的方法，其最小化文本锐度差距（textual sharpness gap）的设计使得提示在释义下保持准确率，优于仅追求准确率的提示搜索，同时保持了计算上的实用性。",
                    "inspiration_trace": "# 从提示词脆弱性到文本锐度感知优化：TARE方法的逻辑演进\n\n## 1. 宏观问题：LLMs的提示词脆弱性\n\n**观察现象**：大型语言模型(LLMs)的性能高度依赖于精心设计的提示词，但现有提示词优化方法存在一个根本缺陷——它们主要追求点状准确性(point-wise accuracy)，却忽略了一个关键问题：小的、语义保持的改写往往会导致性能大幅波动。\n\n**核心问题**：这种脆弱性严重限制了LLM系统在实际应用中的可靠性。一个在特定输入上表现优异的提示词，在面对轻微改写或重新表述时可能完全失效。\n\n## 2. 问题形式化：文本锐度概念的提出\n\n**关键洞察**：作者将这种脆弱性定义为\"提示词景观的文本锐度\"(textual sharpness of the prompt landscape)，即提示词性能在语义空间中的变化剧烈程度。\n\n**理论联系**：作者注意到深度学习领域已有研究表明，模型在损失景观中收敛到平坦最小值时具有更好的泛化性能。锐度感知最小化(SAM)等技术在连续参数空间中已取得成功，但将其原理应用于离散、语义丰富的文本提示词空间仍是一个未被探索的挑战。\n\n**研究问题聚焦**：\n1. 如何在离散、语义空间中正式定义和量化提示词的\"锐度邻域\"？\n2. 如何设计实用算法在离散景观中导航，找到既有效又对语义扰动稳健的提示词？\n\n## 3. 理论基础：语义邻域与文本锐度的形式化\n\n**语义邻域定义**：作者首先构建了提示词空间的语义不相似性度量，定义各向同性邻域：\n```\nB(p, ρ_text) := {p' ∈ P : d_text(p, p') ≤ ρ_text}\n```\n\n**各向异性扩展**：为捕捉提示词不同组件的异质性敏感性，引入各向异性度量和椭球邻域，允许对不同语义组件施加不同程度的扰动。\n\n**文本锐度量化**：基于语义邻域，将文本锐度定义为局部最坏情况风险：\n```\nLS(p, ρ_text) := max_{p'∈B(p,ρ_text)} LD(p')\n```\n\n**稳健优化目标**：将问题转化为最小化文本锐度感知风险：\n```\nmin_{p∈P} LS(p, ρ_text)\n```\n\n## 4. 方法设计：TARE框架的构建\n\n**设计原则**：基于黑盒、无导数优化，语义邻域保留任务意图，内部对抗搜索暴露弱点，外部稳健更新选择候选。\n\n**核心算法**：\n1. **内部对抗搜索**：在语义邻域内采样候选变体，评估性能并识别最坏情况。\n2. **外部稳健更新**：基于当前提示词和其最差邻居生成改进候选，选择在语义邻域内表现最稳健的提示词。\n\n**与SAM的类比**：TARE在离散文本空间中镜像了SAM的逻辑——内部搜索发现最坏扰动，外部更新向更平坦区域移动，共同优化任务性能和稳健性。\n\n## 5. 方法改进：ATARE的各向异性与自适应\n\n**局限性识别**：各向同性邻域平等对待所有提示词组件，但实际敏感性是异质的——核心约束、方法指导和风格元素具有不同的重要性。\n\n**ATARE创新**：\n1. **敏感性估计**：计算每个组件的对抗性增益，动态调整权重。\n2. **各向异性采样**：根据组件敏感性进行差异化扰动，对敏感组件精细探索，对稳健组件广泛探索。\n3. **自适应半径调度**：根据验证结果动态调整邻域大小，平衡探索与保真度。\n\n## 6. 优化器实现：LATO的景观感知设计\n\n**优化挑战**：外部稳健更新需要智能的优化器来生成改进候选。\n\n**LATO设计**：景观感知文本优化器通过分析当前提示词和其最差邻居的性能差异，直接感知局部景观几何形状，引导提示词向更平坦的语义盆地移动。\n\n**核心机制**：LATO不仅修正当前错误，还从邻居的失败模式中学习，预先解决最脆弱的方面，使提示词本质上对类似扰动更加稳健。\n\n## 7. 实验验证与扩展应用\n\n**多维度验证**：通过优越性、有效性、韧性和敏感性四个维度全面评估方法，证明TARE和ATARE在多个推理任务上显著优于现有基线。\n\n**扩展应用**：将框架原则扩展到解决方案优化领域，利用ATARE的各向异性特性处理逻辑推理链中不同组件的异质性敏感性，保持正确推理骨架不变的同时针对性改进逻辑缺陷。\n\n## 8. 逻辑演进总结\n\n从观察到方法论的完整逻辑链：\n1. **问题识别**：提示词优化的脆弱性现象\n2. **理论构建**：引入文本锐度概念，形式化语义邻域\n3. **方法设计**：TARE框架实现离散空间中的锐度感知优化\n4. **方法改进**：ATARE引入各向异性和自适应机制\n5. **实现优化**：LATO提供景观感知的优化能力\n6. **验证扩展**：实验证明有效性并拓展应用场景\n\n这一演进过程展示了如何将连续优化理论成功适配到离散语义空间，解决了提示词优化的稳健性问题，为LLMs的可靠应用提供了新思路。"
                },
                {
                    "title": "Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems",
                    "arxiv_id": "2509.24116",
                    "authors": "Minsoo Kim, Seung-won Hwang",
                    "summary": "LLM-based agents have seen promising advances, yet they are still limited in \"hard-exploration\" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出GLoW（双尺度世界模型），一种用于增强LLM智能体在\"hard-exploration\"任务中表现的新方法。根据筛选标准，这篇论文符合我的研究目标，原因如下： 首先，从本质上看，论文专注于提升LLM本身的基础能力，特别是探索和问题解决能力，而不是将LLM作为工具应用到特定领域。论文提出的双尺度世界模型和Multi-path Advantage Reflection机制，旨在增强LLM通过探索学习新知识的能力，这属于提升LLM通用推理能力的范畴。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM-based agents\"；能力方向上涉及\"hard-exploration\"问题，这与问题解决能力直接相关；方法上提出了一种从试错中学习的机制，与强化学习有相似之处；并且明确关注LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准。虽然使用了文本游戏作为评估基准，但这只是作为测试通用探索能力的平台，而不是专注于游戏领域本身。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。 最后，论文提出的是一种通用的智能体框架来增强LLM的探索能力，而不是将智能体应用在特定领域，这符合特殊情况的判断标准。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（特别是探索和问题解决能力），符合我的研究范围。",
                    "summary2": "本文旨在解决LLM agents在hard-exploration问题中的局限性。针对具有大状态-动作空间、欺骗性局部最优和稀疏奖励的文本游戏环境，我们提出了一种双尺度世界模型框架GLoW，并在Jericho benchmark上通过游戏得分和样本效率验证了其有效性。",
                    "summary_translation": "基于LLM的智能体（LLM-based agents）已经取得了令人瞩目的进展，但它们在需要通过探索学习新知识的\"硬探索\"（hard-exploration）任务中仍然存在局限。我们提出了GLoW，一种利用双尺度世界模型（dual-scale world models）的新方法，该方法在全局尺度上维持高价值发现的轨迹前沿（trajectory frontier），同时通过多路径优势反思机制（Multi-path Advantage Reflection mechanism）从探索中的局部试错中学习，该机制推断基于优势的进展信号以指导探索。为了评估我们的框架在硬探索任务上的表现，我们应对了基于文本游戏的Jericho基准测试套件（Jericho benchmark suite），其中GLoW在基于LLM的方法中取得了最新的最先进性能（state-of-the-art performance）。与最先进的基于强化学习的方法（state-of-the-art RL-based methods）相比，我们的方法取得了相当的性能，同时所需的环境交互减少了100-800倍。",
                    "inspiration_trace": "# GLoW方法逻辑链分析：从宏观问题到双尺度世界模型\n\n## 一、宏观问题：LLM智能体在硬探索任务上的局限性\n\n**观察现象**：尽管LLM智能体在利用预训练知识的任务（如机器人规划、软件工程）中表现出色，但在需要通过探索学习新知识的\"硬探索\"任务中表现不佳。\n\n**问题定义**：硬探索问题具有三大特征：\n- 大规模状态-动作空间（如文本游戏中的组合爆炸）\n- 欺骗性局部最优（容易陷入无法逃脱的局部区域）\n- 稀疏奖励（成功前几乎没有反馈信号）\n\n**核心挑战**：LLM智能体面临两个关键瓶颈：\n1. **全局学习**：如何维护探索过程中有价值发现的长期知识\n2. **局部试错**：如何从稀疏环境反馈中快速细化探索策略\n\n## 二、现有方法分析与局限\n\n**观察**：当前LLM智能体方法（如ReAct、Reflexion）支持局部试错，但缺乏长期知识积累机制。\n\n**启发**：Go-Explore算法通过维护已发现状态存档并在\"选择有希望状态\"和\"从该状态继续探索\"之间交替，在硬探索问题上取得突破。\n\n**局限识别**：\n- 原始Go-Explore使用手工启发式方法进行状态选择，随机动作采样进行探索\n- 后续改进（如IGE）虽引入LLM推理，但选择标准仍不明确，探索能力有限\n\n## 三、核心洞察：双尺度学习的必要性\n\n**关键洞察**：选择和探索都需要从过去经验中学习，但在不同尺度上：\n1. **全局尺度**：需要维护完整轨迹上下文，理解如何达到高价值状态及进展停滞原因\n2. **局部尺度**：需要从同一状态的多条探索路径中比较，识别关键决策点的优势信号\n\n**理论支持**：\n- 全局层面受UCB价值分解启发，平衡利用与探索\n- 局部层面借鉴优势函数思想，减少稀疏奖励环境中的方差\n\n## 四、方法构建：全局世界模型\n\n**设计思路**：超越孤立状态存档，维护轨迹前沿以保留完整上下文\n\n**核心组件**：\n1. **价值排序轨迹前沿**：维护k个最高价值轨迹F = {τ₁, τ₂, ..., τₖ}，按最大累积奖励排序\n2. **LLM生成轨迹分析**：通过g_LLM分析前沿轨迹，提取关键状态及其双重价值：\n   - W_global = {(s₁, v₁, v'₁), (s₂, v₂, v'₂), ..., (sₖ, vₖ, v'ₖ)}\n   - vᵢ：已实现价值，v'ᵢ：未来价值潜力估计\n\n**状态选择机制**：通过align_LLM评估存档状态与W_global的匹配度，自然平衡利用（与已证明高奖励区域相似）与探索（优先接近已识别瓶颈的高潜力状态）。\n\n## 五、方法构建：局部世界模型\n\n**设计思路**：从Q值到优势的转换，通过多路径比较减少稀疏奖励环境中的方差\n\n**核心组件**：\n1. **多路径优势反思(MAR)**：从同一状态采样n条轨迹，比较不同结果，产生伪密集优势信号\n2. **语义优势表示**：不仅记录哪些动作有益，还解释为什么有效及适用条件\n   - W_local = {(s*₁, A_s*₁), ..., (s*_k, A_s*_k)}\n\n**探索策略**：π_explore(a|s_t, h_t) = Agent_LLM(s_t, h_t, W_local, T_s, F)\n结合局部优势学习、当前探索历史和全局成功策略指导探索\n\n## 六、实验验证与效果\n\n**实验设置**：在Jericho文本游戏基准测试上评估，该测试具有部分可观察性和组合状态-动作空间挑战\n\n**关键结果**：\n1. 在7/10游戏中达到LLM方法新SOTA\n2. 与需要100-800倍更多环境交互的RL方法相比，性能相当\n3. 显著优于直接可比的IGE方法（8/10游戏）\n\n**消融研究**：\n- 移除MAR导致性能显著下降，证明优势学习的有效性\n- 移除全局价值分析或轨迹前沿均降低性能，确认全局模型贡献\n- 组件间存在明显协同效应，整体性能源于互补设计\n\n## 七、逻辑演进总结\n\n从\"LLM智能体在硬探索任务中表现不佳\"的宏观问题出发，通过分析现有方法缺乏长期知识积累和有效局部学习的局限，形成了\"需要在不同尺度上进行结构化学习\"的核心洞察，进而设计了双尺度世界模型框架：全局模型维护轨迹前沿和潜在价值分析用于状态选择，局部模型通过多路径优势反思指导探索。整个逻辑链条从问题识别到方法设计再到验证，形成了一个完整、严谨的研究思路，最终实现了在保持样本效率的同时显著提升LLM智能体在硬探索任务中的性能。"
                },
                {
                    "title": "Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics",
                    "arxiv_id": "2509.24102",
                    "authors": "Guangliang Liu, Xi Chen, Bocheng Chen, Xitong Zhang, Kristen Johnson",
                    "summary": "Moral reasoning has emerged as a promising research direction for Large Language Models (LLMs), yet achieving generalization remains a central challenge. From a linguistic standpoint, this difficulty arises because LLMs are adept at capturing distributional semantics, which fundamentally differs from the morals which operate at the pragmatic level. This paper investigates how LLMs can achieve generalized moral reasoning despite their reliance on distributional semantics. We propose pragmatic inference methods grounded in moral foundations theory, which leverage contextual information at each step to bridge the pragmatic gap and guide LLMs in connecting moral foundations with moral reasoning objectives. Experimental results demonstrate that our approach significantly enhances LLMs' generalization in moral reasoning, providing a foundation for future research grounded in moral foundations theory.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它的核心是提升大语言模型(LLMs)在道德推理方面的通用能力。根据筛选标准分析：首先，论文本质上是改进LLM的基础推理能力，提出了一种基于道德基础理论的语用推理方法，这属于\"增强其逻辑、多步推理等通用能力\"的范畴。道德推理可以被视为通用推理能力的一个重要子集，涉及逻辑判断和原则应用。其次，论文包含正面指标中的核心概念\"Large language models\"和能力方向\"reasoning\"。第三，论文不符合排除标准，虽然涉及道德推理，但并未将其作为特定应用领域（如社会学）的研究，而是将其视为提升LLM通用推理能力的一种途径。论文提出的语用推理方法旨在弥合LLM依赖的分布语义与道德推理所需的语用层面之间的差距，这实质上是增强模型内在推理机制的研究，符合\"提高大语言模型本身的通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决大型语言模型在道德推理任务中泛化能力不足的问题。针对LLMs依赖分布语义而道德推理本质上是语用层面的差异，我们提出了一种基于道德基础理论的语用推理方法，并在Moral Integration Corpus (MIC)基准上通过预测准确率验证了其有效性。",
                    "summary_translation": "道德推理已成为大型语言模型（LLMs）的一个有前景的研究方向，然而实现泛化仍然是一个核心挑战。从语言学的角度来看，这一困难的出现是因为LLMs擅长捕捉分布语义学（distributional semantics），而道德则是在语用（pragmatic）层面运作的，两者存在根本差异。本文探讨了LLMs如何在依赖分布语义学的情况下实现泛化的道德推理。我们提出了基于道德基础理论（moral foundations theory）的语用推理（pragmatic inference）方法，该方法在每一步都利用上下文信息来弥合语用差距，并引导LLMs将道德基础与道德推理目标联系起来。实验结果表明，我们的方法显著增强了LLMs在道德推理方面的泛化能力，为未来基于道德基础理论的研究奠定了基础。",
                    "inspiration_trace": "# 论文核心方法逻辑链分析：从语用差距到道德推理泛化\n\n## 一、宏观问题：LLMs道德推理的泛化困境\n\n**观察现象**：大型语言模型(LLMs)在道德推理任务中表现不佳，尤其是泛化能力有限。即使经过大量数据训练，模型在面对新情境时仍难以做出合理的道德判断。\n\n**问题定位**：这一挑战并非简单的数据不足或模型规模问题，而是源于LLMs的工作机制与道德推理本质之间的根本性不匹配。\n\n## 二、问题根源：分布语义与语用学的鸿沟\n\n**理论分析**：\n- LLMs基于**分布语义**(distributional semantics)工作，遵循Firth(1957)的分布假说——语言单位的含义由其出现的上下文决定\n- 道德推理本质上是**语用学**(pragmatics)层面的任务，涉及隐含意义、社会规范和行为后果等\"未言明\"的变量\n\n**核心发现**：存在\"语用差距\"(pragmatic gap)——即\"所说的\"和\"道德上隐含的\"之间的差距。正如论文所述：\"metaphorically speaking, the gap between what is said and what is morally implied\"。\n\n**理论支持**：引用Spencer-Oatey和Xing(2019)的研究，说明道德推理需要从语言使用、道德规范和行为后果的隐含意义中推导结论，而这正是LLMs所缺乏的能力。\n\n## 三、核心假设：通过语用推理弥合差距\n\n**假设形成**：如果能够将道德推理中通常未被言明的变量(如社会规范、元语用评价等)**文本化**(textualizing)，使LLMs能够学习这些变量，就可以弥合分布语义与语用学之间的差距。\n\n**理论框架选择**：采用**道德基础理论**(Moral Foundations Theory, MFT)作为框架，因为：\n1. MFT提供了一个原则性的方式来注释、分析和建模语言中的道德维度\n2. 六个道德基础(关怀、公平、自由、忠诚、权威、神圣)可以作为连接具体情境和抽象道德原则的桥梁\n3. 这些基础是\"common ground which underlie all interactions\"，为道德推理提供了统一的理论基础\n\n## 四、方法设计：三种语用推理框架\n\n基于上述假设，作者设计了三种逐步复杂的语用推理框架，形成从简单到复杂的逻辑演进：\n\n### 框架1：道德基础分类(MFC)\n- **输入**：经验法则(Rule of Thumb, RoT)和六个道德基础的定义\n- **推理步骤**：\n  1. 识别RoT中包含的判断和相关行动\n  2. 推断该行动的后果(行为后果)\n  3. 将行动和后果与相关道德基础联系起来，通过文本化它们的元语用链接\n- **目标**：从抽象规则中提取共同原则，建立RoT与MFs之间的联系\n\n### 框架2：道德判断\n- **输入**：提示-回复对和潜在的道德基础\n- **推理步骤**：\n  1. 解释道德基础的定义\n  2. 从回复中得出结论\n  3. 解释回复的结论如何维护或违反道德基础\n- **目标**：基于具体情境和道德基础做出道德判断，处理语境特定的解释\n\n### 框架3：联合道德基础分类和道德判断(MFC-Judgment)\n- **输入**：提示-回复对和六个道德基础的定义\n- **推理步骤**：\n  1. 基于提示，从回复中得出结论\n  2. 解释回复的结论与道德基础的关联\n  3. 做出道德判断，并解释结论如何遵守或违反道德基础\n- **目标**：自动推断道德基础并做出相应的道德判断，更接近现实场景\n\n**设计逻辑**：这三个框架形成了一个从抽象到具体、从简单到复杂的渐进过程，逐步逼近真实世界的道德推理场景。\n\n## 五、实验验证：方法有效性的实证检验\n\n**实验设计**：\n- 使用MIC(Moral Integration Corpus)数据集\n- 采用Llama3.2-1B和3B作为基础模型\n- 设计三种数据规模(5K、10K、23.5K样本)\n- 与基线方法(base和base+)进行比较\n\n**关键结果**：\n1. 在所有任务和数据规模上，提出的语用推理方法均显著优于基线方法\n2. 随着道德基础数量增加，基线方法性能显著下降，而提出的方法性能下降较小\n3. 干预实验表明，该方法确实促进了LLMs有效利用道德基础，而非仅依赖表面相关性\n4. 语言建模困惑度(perplexity)分析显示，提出的方法导致更低的困惑度，表明更好的语言建模能力\n\n## 六、深入分析：方法机制与局限性\n\n**道德基础依赖性分析**：通过替换模型自动选择的道德基础为真实标签，发现性能显著提升，证明模型确实依赖于道德基础进行推理。\n\n**个别道德基础分析**：发现模型在常见道德基础(如关怀)上表现更好，而在罕见道德基础(如神圣)上表现较差，表明数据不平衡是一个瓶颈。\n\n**方法泛化潜力**：讨论了将语用推理过程文本化的概念可能广泛适用于其他非逻辑推理任务，如理解隐喻、幽默和讽刺。\n\n## 七、结论与展望：从问题到解决方案的完整闭环\n\n**核心贡献**：通过语用推理方法弥合了分布语义与道德推理的语用学本质之间的差距，显著提高了LLMs在道德推理任务中的泛化能力。\n\n**理论意义**：证明了将道德推理中通常未被言明的变量文本化的有效性，为解决LLMs在语用学层面的局限性提供了新思路。\n\n**实践意义**：提出的三种语用推理框架为构建更具道德推理能力的AI系统提供了具体方法。\n\n**未来方向**：\n1. 探索不依赖道德基础的推理路径\n2. 解决数据不平衡问题\n3. 将方法扩展到多语言环境以考察文化差异\n\n这一逻辑链清晰地展示了作者从观察问题、分析根源、形成假设、设计方法到验证效果的完整思考过程，形成了一个从宏观问题到具体解决方案的严密逻辑演进。"
                },
                {
                    "title": "Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?",
                    "arxiv_id": "2509.24090",
                    "authors": "Matteo Boffa, Jiaxuan You",
                    "summary": "Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)处理大量约束的能力，提出了\"大规模约束生成\"(LSCG)这一新问题，并开发了FoCusNet方法来提升LLMs在这一任务上的表现。从本质上看，论文关注的是LLMs的基础能力——约束解析和遵循能力，这属于逻辑推理和问题解决的通用能力范畴，而不是将LLMs作为工具应用到特定领域。论文评估了不同模型特性和技术（包括思维链）对性能的影响，这些都是提升LLM通用推理能力的研究方向。论文不涉及多模态、特定应用领域或模型基础设施等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)处理大量约束的能力问题。针对大规模细粒度通用约束场景，我们提出了一种FoCusNet辅助模型，用于将原始大量约束解析为相关子集，并在Words Checker任务上通过准确率、精确率和召回率等指标验证了其有效性。实验表明，随着约束数量增加，传统方法性能显著下降，而FoCusNet实现了8-13%的准确率提升，证明了其在约束聚焦方面的优势。",
                    "summary_translation": "近期研究探讨了大型语言模型 (Large Language Models, LLMs) 在少量特定任务要求的明确提示下的约束生成能力。相比之下，我们提出了大规模约束生成 (Large-Scale Constraint Generation, LSCG)，这是一个新问题，用于评估LLMs是否能够解析大量细粒度的通用约束列表。为了检验LLMs处理日益增多的约束的能力，我们创建了一个LSCG的实际实例，称为Words Checker。在Words Checker中，我们评估了模型特征（例如，大小、系列）和引导技术（例如，简单提示、思维链、N选最佳）对性能的影响。我们还提出了FoCusNet，一个小型专用模型，它将原始约束列表解析为更小的子集，帮助LLM专注于相关约束。实验表明，随着约束数量的增加，现有解决方案的性能显著下降，而FoCusNet则显示出8-13%的准确率提升。",
                    "inspiration_trace": "# 从观察到解决方案：FoCusNet的逻辑演进\n\n## 1. 宏观问题发现：现实约束与当前研究的差距\n\n作者首先观察到现实世界中存在大量需要遵循众多约束的场景，如旅行指南、编程文档等。然而，现有研究主要聚焦于LLMs处理少量特定任务约束的能力，缺乏对大规模通用约束处理能力的探索。这形成了一个研究空白：**LLMs能否有效处理数百甚至数千个细粒度通用约束？**\n\n## 2. 问题框架定义：从具体到抽象\n\n基于上述观察，作者定义了\"大规模约束生成\"(LSCG)的新问题框架：\n- 核心特征：约束数量大(C ≥ 100)、细粒度但通用\n- 关键挑战：模型需从大量约束中识别与特定任务相关的部分\n- 典型场景：如\"如何在伊斯兰国家做一个好游客\"配合长篇旅行指南\n\n这一框架将抽象的现实需求转化为可研究的技术问题。\n\n## 3. 实验设计：Words Checker的创建\n\n为系统研究LSCG，作者设计了Words Checker作为具体测试实例：\n- 任务：判断句子是否包含禁用词列表中的任何词\n- 特点：简单明确，无需复杂推理，便于隔离约束数量的影响\n- 变量控制：创建不同规模的禁用词列表(100、500、1000)\n\n这一设计使作者能够精确测量约束数量增加对LLM性能的影响。\n\n## 4. 现有方法评估：发现性能瓶颈\n\n通过Words Checker实验，作者揭示了现有方法的显著局限：\n- 所有模型在约束数量增加时性能均大幅下降(~30%准确率损失)\n- 传统引导策略(如思维链、最佳N次)不仅无法缓解问题，反而导致过度思考和幻觉\n- 模型倾向于逐个处理约束，失去焦点，混淆推理过程与实际任务\n\n这些发现表明，简单扩展现有方法无法解决大规模约束问题。\n\n## 5. 核心假设形成：从人类认知获取灵感\n\n基于实验发现，作者提出关键假设：**通过预先筛选相关约束，可帮助LLM专注于重要信息，从而提高大规模约束下的性能**。这一假设受到两方面启发：\n- 人类认知策略：面对大量信息时先筛选再深入处理\n- 检索增强生成(RAG)的成功经验：通过检索相关信息辅助生成\n\n## 6. 方法设计：FoCusNet的架构创新\n\n为验证假设，作者设计了FoCusNet作为解决方案：\n- 定位：轻量级辅助模型，非基础模型修改\n- 功能：将原始约束集解析为相关子集，缩小LLM注意力范围\n- 训练：针对特定任务进行二元分类训练，识别约束相关性\n- 协作：与LLM形成互补关系，FoCusNet负责筛选，LLM负责最终判断\n\n这一设计在指令微调和简单测试引导之间提供了平衡点。\n\n## 7. 实验验证：假设证实与效果量化\n\n通过Words Checker实验，作者验证了FoCusNet的有效性：\n- FoCusNet本身达到90%的约束检测准确率\n- 成功缩小搜索空间(平均从1000词筛选至30个可疑词)\n- 与传统方法相比，实现8-13%的准确率提升\n- 即使在1000个约束的情况下，仍保持相对稳定的性能\n\n实验结果证实了作者的初始假设：约束预筛选确实能显著提升LLM在大规模约束场景下的表现。\n\n## 8. 贡献总结与未来展望\n\n最后，作者将研究定位为LLM约束处理领域的新探索方向：\n- 理论贡献：提出LSCG问题框架，扩展约束生成研究边界\n- 实践贡献：提供Words Checker测试基准和FoCusNet解决方案\n- 社区贡献：开源代码和数据集，促进领域发展\n\n同时，作者也指出了当前工作的局限性，为未来研究指明方向，如多模态约束处理、更通用架构设计等。\n\n这一完整逻辑链展现了从问题观察到方法创新再到实验验证的系统性研究过程，体现了作者在LLM约束处理领域的深入思考和创新能力。"
                },
                {
                    "title": "GEAR: A General Evaluation Framework for Abductive Reasoning",
                    "arxiv_id": "2509.24096",
                    "authors": "Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen",
                    "summary": "Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出GEAR评估框架和基于动量的课程学习策略，专门用于评估和提升大语言模型的溯因推理能力。溯因推理(abductive reasoning)是一种重要的通用逻辑推理能力，涉及生成合理假设来解释观察到的现象，这直接符合研究目标中\"增强LLM逻辑推理能力\"的要求。 从筛选标准分析： 1. 核心判断：论文本质是改进LLM的基础推理能力，提出了新的评估框架和训练方法（基于动量的课程学习），而非将LLM作为工具应用于特定领域。 2. 正面指标：论文明确包含核心概念\"Large language models\"，能力方向聚焦于\"reasoning\"（特别是溯因推理这种逻辑推理），并提出了新的训练方法。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型基础设施等排除内容。 4. 特殊情况：论文关注提升模型生成可靠假设的能力，这与减少幻觉、增强模型内在可靠性相关，属于提升通用推理质量的范畴。 论文通过GEAR框架评估和改进LLM的溯因推理能力，这种能力是通用推理能力的重要组成部分，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大语言模型溯因推理能力评估不足的问题。针对多种溯因推理任务，我们提出了一种GEAR评估框架，通过一致性、泛化性和多样性三个维度评估假设质量，并在9个LLMs和4个流行benchmark上通过自动化指标验证了其有效性。",
                    "summary_translation": "自大型语言模型（large language models, LLMs，大型语言模型）出现以来，研究主要集中在指令遵循和演绎推理（deductive reasoning，演绎推理）上。一个核心问题仍然存在：这些模型能否发现新知识，以及我们如何评估这种能力？我们通过研究溯因推理（abductive reasoning，溯因推理）——即生成合理解释观察结果的假设的过程——来解决这个问题，并引入GEAR（General Evaluation for Abductive Reasoning，溯因推理通用评估），这是一个通用、全自动、透明且无标签的评估范式。GEAR通过三个指标对假设集进行评分：一致性（consistency，每个假设都能解释观察结果）、泛化性（generalizability，一致的假设能在未见输入上做出有意义的预测）和多样性（diversity，假设集涵盖不同的预测和模式）。以这种方式构建，GEAR具有可扩展性（无需人工标准答案）、可靠性（确定性评分与经典溯因一致）和开放性（只有当模型产生新的合理假设时分数才会提高，不像静态基准在准确率高时会饱和）。使用GEAR，我们对九个LLM在四个包含1,500个问题的溯因基准测试上进行了细粒度研究，生成了超过50,000个候选假设，并揭示了被标准答案或纯人工评估所掩盖的模型差异。我们进一步提出了一种基于动量的课程（momentum-based curriculum，基于动量的课程），通过学习速度调整GEAR衍生的训练数据：它从模型快速学习的内容开始，一旦模型在基础目标上变得自信，就转向更难的目标，如生成多样化的假设。在没有标准标签监督的情况下，这一策略改善了所有GEAR目标，并且这些改进转移到了已建立的溯因推理基准测试上。总体而言，GEAR提供了一个评估溯因的原则性框架，并提供无标签、可扩展的训练信号，帮助LLM产生更多样化和可靠的假设。",
                    "inspiration_trace": "# GEAR框架提出逻辑链分析\n\n## 1. 宏观问题：AI知识发现能力的评估困境\n\n论文从一个根本性问题出发：大型语言模型(LLMs)在指令遵循和演绎推理方面取得了显著进展，但一个核心问题仍未解决——**这些模型是否能够真正发现新知识，以及如何评估这种能力？** 这一宏观问题指向了AI研究中一个关键缺口：缺乏对模型创造性、发现性思维的系统评估方法。\n\n## 2. 问题聚焦：溯因推理的核心地位\n\n作者将问题聚焦到**溯因推理**(Abductive Reasoning)上，即生成合理解释观察结果的假设的能力。这种推理方式与科学发现、创新思维等高阶认知活动密切相关，却相比演绎推理受到的关注少得多。溯因推理成为评估AI知识发现能力的理想切入点，因为它直接体现了\"从观察到假设\"的核心认知过程。\n\n## 3. 现有方法批判：评估范式的不匹配\n\n作者深入分析了现有溯因推理评估方法的两大流派及其根本缺陷：\n\n### 基于黄金答案(Gold-answer)的评估\n- **理论缺陷**：采用Harman的\"最佳解释推理\"(IBE)定义，但\"最佳\"标准模糊不清，因情境而异\n- **实践缺陷**：依赖单一\"黄金\"假设，忽视了科学解释的多元性；约80%同样合理的假设被错误标记为不正确\n- **哲学缺陷**：违背了溯因推理的\"证据不确定性\"本质，即给定有限数据，多个假设可能同样合理\n\n### 基于人类的评估\n- **可扩展性问题**：昂贵、耗时、难以大规模应用\n- **主观性问题**：结果高度依赖评估者的专业背景和判断标准\n- **一致性问题**：不同评估者间的一致性低，结果难以复现\n\n这些批判揭示了现有评估范式与溯因推理本质之间的根本不匹配：**评估应该测试模型提出多种、新颖且合理的解释性假设的能力，而非测试与单一\"黄金\"解释的一致性。**\n\n## 4. 理论重构：回归溯因推理的本质\n\n为克服现有方法的局限性，作者回归Peirce的原始定义，将溯因推理重新概念化为\"从给定观察中生成假设的更一般任务\"。基于此，作者提出了评估科学假设的三个经典标准，构建了GEAR框架的理论基础：\n\n### 一致性(Consistency)\n- **核心思想**：假设必须与观察到的事实不矛盾\n- **形式化定义**：生成的假设f与观察集O一致，当且仅当∀(in_i, out_i) ∈ O, f(in_i) = out_i\n- **理论依据**：科学解释的基本要求，确保与现有证据兼容\n\n### 泛化性(Generalizability)\n- **核心思想**：好的假设应超越观察数据，对未见案例做出可测试预测\n- **形式化定义**：假设f1比f2更一般，如果其输入域D1大于D2（M(D1) > M(D2)）\n- **理论依据**：Popper的可证伪性理论，更好的假设承载更高的经验内容，做出更精确、风险更大的预测\n\n### 多样性(Diversity)\n- **核心思想**：假设集应提供多种不同视角，避免过早收敛到单一解释\n- **形式化定义**：通过γ-多样性（平均唯一预测数）和β-多样性（预测模式分散度）量化\n- **理论依据**：Chamberlin的\"多重工作假说\"和Platt的\"强推理\"，强调竞争性解释对科学进步的重要性\n\n## 5. 方法论构建：GEAR框架的设计\n\n基于上述理论基础，作者设计了GEAR(General Evaluation for Abductive Reasoning)框架，将抽象理论转化为可操作评估方法：\n\n### 技术实现路径\n1. **问题形式化**：将观察表示为输入-输出对集合O = {(in_i, out_i)}，假设表示为函数f\n2. **样本空间构建**：为每个任务定义适当的样本空间S，用于评估泛化性和多样性\n3. **度量方法设计**：\n   - 一致性：直接检查假设是否与所有观察匹配\n   - 泛化性：测量假设在样本空间S上的覆盖率\n   - 多样性：通过γ-多样性和β-多样性量化假设集的差异程度\n\n### 框架优势\n- **可扩展性**：完全自动化，无需人类黄金答案\n- **可靠性**：所有指标透明定义和计算，不依赖黑盒模型\n- **开放性**：分数仅在模型产生新的、合理的假设时提高，不会像静态基准那样饱和\n\n## 6. 实验验证：框架的有效性检验\n\n作者通过大规模实验验证GEAR框架的有效性：\n\n### 实验设计\n- **数据集**：四个流行的溯因推理基准(MINI-ARC, ACRE, LIST FUNCTIONS, ARC-2025)\n- **模型**：九个LLMs，包括API模型和开源模型\n- **规模**：1,500个问题，生成50,340个候选假设\n\n### 关键发现\n1. **一致性挑战**：即使是70B级别的模型，也只产生约20%一致的假设\n2. **观察数量影响**：增加初始观察数量会降低多样性，但对一致性影响有限\n3. **模型规模与能力**：模型大小与溯因多样性弱相关，表明数据和训练方法可能比原始规模更重要\n4. **黄金答案评估缺陷**：约80%同样合理的假设被现有基准错误标记为不正确\n\n这些发现不仅验证了GEAR框架的敏感性，还揭示了先前评估方法未能捕捉的重要洞见。\n\n## 7. 应用扩展：从评估到训练\n\n作者进一步将GEAR从评估框架扩展为训练信号，形成完整的解决方案：\n\n### 训练信号构建\n1. **偏好数据准备**：从生成的假设中构建偏好对，分三个阶段分配偏好：\n   - 指令遵循/格式合规性\n   - 一致性\n   - GEAR分数（泛化性和多样性）\n\n2. **基于动量的课程学习**：\n   - **核心思想**：先学习提高最快且最容易的内容，然后转向更困难的信号\n   - **实现方法**：使用指数加权移动平均(EWMA)跟踪每种偏好类型的损失，将最近的改进转化为采样权重\n   - **优势**：避免手动调整固定比例，让每个基础模型自然趋向其首选的混合比例\n\n### 训练效果\n- 提高了所有三个GEAR目标——一致性、泛化性和多样性\n- 这些增益转移到已建立的溯因推理基准测试中\n- 基于动量的课程学习策略始终优于固定比例基线\n\n## 8. 理论与实践的闭环：GEAR的完整价值\n\n作者通过GEAR框架实现了理论与实践的闭环：\n\n1. **理论贡献**：重新概念化了溯因推理评估，强调了多元假设的重要性\n2. **方法贡献**：提供了可操作、可扩展、可靠的评估框架\n3. **实践贡献**：将评估转化为训练信号，提高了模型的溯因推理能力\n4. **应用前景**：框架不仅限于编程领域，还可扩展到自然语言设置（需解决语义表示等挑战）\n\nGEAR框架不仅解决了\"如何评估溯因推理\"的问题，还通过\"如何利用评估改进模型\"形成了完整的解决方案，为AI系统的知识发现能力评估与提升提供了新范式。"
                },
                {
                    "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering",
                    "arxiv_id": "2509.23982",
                    "authors": "Lucio La Cava, Andrea Tagarelli",
                    "summary": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为PaLRS的新方法，用于改进大语言模型的基础能力。论文提出了一种训练-free的偏好对齐方法，通过利用LLM残差流中编码的偏好信号，提取轻量级转向向量来增强模型性能。这明显属于改进LLM本身的基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调了数学推理能力的提升，这正是通用推理能力的核心组成部分 - 训练方法：讨论了与RLHF等传统偏好对齐方法的对比，并提出了一种新的更高效的方法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文提到了数学推理和代码生成，但这是作为评估模型通用推理能力的基准，而不是针对特定领域的应用。 特别值得注意的是，论文明确指出PaLRS方法\"保留了基线的通用性能\"，并在\"数学推理和代码生成基准上取得了一致的改进\"，这直接表明其研究目标是提升LLM的通用推理能力，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、高效的偏好对齐方法来增强大语言模型的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型偏好对齐过程中需要大量数据和计算资源的问题。针对偏好对齐任务，我们提出了一种基于残差流的模型转向方法PALRS，并在多个开源LLMs上通过数学推理(GSM8K)和代码生成(HumanEval)基准测试验证了其有效性。PALRS仅需少量偏好对即可提取轻量级转向向量，在推理时应用，无需更新模型参数，实现了高效灵活的偏好对齐。",
                    "summary_translation": "偏好对齐（Preference alignment）是使大型语言模型（Large Language Models, LLMs）变得有用并与（人类）偏好保持一致的关键步骤。现有方法，如人类反馈强化学习（Reinforcement Learning from Human Feedback）或直接偏好优化（Direct Preference Optimization），通常需要精心策划的数据和针对数十亿参数的昂贵优化，最终导致持久的特定任务模型。在这项工作中，我们介绍了通过残差引导（Residual Steering）实现大型语言模型偏好对齐（Preference alignment of Large Language Models via Residual Steering, PaLRS）的方法，这是一种无训练（training-free）的方法，利用了编码在大型语言模型残差流（residual streams）中的偏好信号。仅需少至一百个偏好对（preference pairs），PaLRS就能提取轻量级、即插即用（plug-and-play）的引导向量（steering vectors），可在推理时（inference time）应用，推动模型朝向期望的行为发展。我们在各种中小型开源大型语言模型上评估了PaLRS，结果表明经过PaLRS对齐的模型在数学推理和代码生成基准测试上取得了一致的性能提升，同时保持了基线的通用性能。此外，与经过直接偏好优化（Direct Preference Optimization, DPO）对齐的模型相比，它们表现更好且节省了大量时间。我们的研究结果表明，PaLRS为标准偏好优化流程提供了一种有效、更高效且更灵活的替代方案，它提供了一种无训练、即插即用的对齐机制，仅需最少的数据。",
                    "inspiration_trace": "# 从问题到解决方案：PALRS方法的逻辑推演\n\n## 一、宏观问题：LLM偏好对齐的困境\n\n大型语言模型(LLMs)的偏好对齐是使其有用且符合人类偏好的关键步骤。然而，现有方法如RLHF和DPO面临三大核心挑战：\n- **数据依赖性**：需要大量精心策划的偏好数据集\n- **计算密集性**：需要对数十亿参数进行昂贵的优化\n- **缺乏灵活性**：一旦对齐，模型难以适应新的偏好设置\n\n这些限制使得偏好对齐成为资源密集且不够灵活的过程。\n\n## 二、关键观察：残差流中的可操控特征\n\n作者注意到一个新兴研究方向：LLMs的残差流激活编码了上下文丰富且可线性化的特征。这些特征可用于在不改变权重或不需要额外训练的情况下操控模型行为。已有研究表明残差干预能有效：\n- 缓解拒绝行为\n- 擦除概念\n- 诱导角色行为\n- 提高事实性\n\n这引出一个核心问题：能否利用残差流的特性来解决偏好对齐的挑战？\n\n## 三、核心假设：偏好方向的可提炼性\n\n作者提出假设：对于特定领域(如数学或编码)的问题，选择和拒绝响应的残差流激活之间的差异可以被提炼成引导方向，用于通过轻量级推理时间干预来诱导模型中的期望行为。\n\n为支持这一假设，论文提供了关键证据(图1)：\n1. **幅度证据**：选择和拒绝的激活在残差空间中相距较远(欧几里得距离大)，意味着可以定义有效的引导向量\n2. **方向一致性**：选择和拒绝激活之间的差异在方向上大多一致(余弦角度小)，表明可以提炼出可泛化的聚合引导向量\n\n这两个观察共同表明：残差流中确实编码了可用于偏好对齐的线性可访问信息。\n\n## 四、方法论构建：PALRS的形成\n\n基于上述观察和假设，作者构建了PALRS(Preference alignment of Large Language Models via Residual Steering)方法，其核心逻辑链如下：\n\n### 4.1 残差流激活的利用\n- 残差流激活是标记在模型中流动时累积的隐藏表示，编码了所有上下文信息\n- 这些激活是线性可解释的，因为子层输出被添加到残差流中\n- 残差流与模型的预测分布相关联，因为最终层将残差流投影到标记上\n\n### 4.2 候选偏好方向的提取\n采用差异均值方法计算选择和拒绝响应的残差流激活的平均值：\n```\nμ(+)i,ℓ = 1/|D| ∑ x_i,ℓ(t(+))\nμ(-)i,ℓ = 1/|D| ∑ x_i,ℓ(t(-))\n```\n候选偏好方向定义为两者之差：\n```\nr_i,ℓ = μ(+)i,ℓ - μ(-)i,ℓ\n```\n\n### 4.3 位置和层的选择策略\n- **位置选择**：关注指令后的标记位置，确保模型已处理给定文本并开始生成输出\n- **层选择**：专注于中后层(0.3L-0.9L)，因为早期层主要处理句法和结构特征，中期层出现语义和推理过程，后期层直接影响输出\n\n### 4.4 最优引导方向的选择\n选择与选择响应的平均残差流激活最严格对齐的向量：\n```\nr* = arg max_(i,ℓ)∈C |r_i,ℓ · μ(+)ℓ|\n```\n然后重新缩放r*使其范数与||μ(+)ℓ*||匹配，以便更好地控制激活添加步骤中的乘法因子效果。\n\n### 4.5 引导方向的应用\n通过激活添加将选定的偏好方向添加到模型生成的新响应的残差流激活中：\n```\nx'ℓ* := xℓ*(t) + αˆr*\n```\n其中α是控制引导效果强度的系数。\n\n## 五、方法验证：从理论到实践\n\n作者通过一系列实验验证了PALRS的有效性：\n1. **目标任务性能**：在数学推理(GSM8K)和代码生成(HumanEval)基准测试上，PALRS一致提升了模型性能\n2. **通用能力保持**：在其他领域的基准测试上，PALRS-aligned模型保持了基线的一般用途性能\n3. **与DPO的比较**：PALRS-aligned模型在性能上优于DPO-aligned模型，同时节省了大量时间\n4. **参数敏感性**：通过调整引导系数α，可以控制引导效果的强度，避免过度引导\n\n## 六、逻辑链总结\n\nPALRS方法的形成遵循了清晰的逻辑演进：\n1. **问题识别**：现有偏好对齐方法存在数据、计算和灵活性方面的限制\n2. **现象观察**：残差流激活编码了可线性化的特征，可用于操控模型行为\n3. **假设形成**：选择和拒绝响应的残差激活差异可提炼成有效的引导方向\n4. **方法构建**：设计了一套完整的残差引导框架，包括方向提取、选择和应用\n5. **实验验证**：在多个任务和模型上验证了方法的有效性和效率\n\n这一逻辑链从宏观问题出发，通过观察现象形成假设，进而构建系统方法论，最终通过实验验证，展现了一个完整的研究思路演进过程。PALRS的核心创新在于将残差流操控技术应用于偏好对齐领域，提供了一种无需训练、即插即用的对齐机制。"
                },
                {
                    "title": "Assessing Large Language Models in Updating Their Forecasts with New Information",
                    "arxiv_id": "2509.23936",
                    "authors": "Zhangdie Yuan, Zifeng Ding, Andreas Vlachos",
                    "summary": "Prior work has largely treated future event prediction as a static task, failing to consider how forecasts and the confidence in them should evolve as new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework for evaluating whether large language models appropriately revise their predictions in response to new information. In particular, EVOLVECAST assesses whether LLMs adjust their forecasts when presented with information released after their training cutoff. We use human forecasters as a comparative reference to analyze prediction shifts and confidence calibration under updated contexts. While LLMs demonstrate some responsiveness to new information, their updates are often inconsistent or overly conservative. We further find that neither verbalized nor logits-based confidence estimates consistently outperform the other, and both remain far from the human reference standard. Across settings, models tend to express conservative bias, underscoring the need for more robust approaches to belief updating.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)如何根据新信息更新预测的能力，属于对LLM通用推理能力的评估和研究。论文提出了EVOLVECAST框架来评估LLMs在面对新证据时如何调整预测和置信度，这直接涉及到模型的推理能力，特别是信念更新(belief updating)这一通用推理能力。从第一步核心判断来看，论文关注的是改进LLM的基础能力，而非将其作为工具应用到特定领域。论文满足正面指标中的核心概念(Large language models)和能力方向(reasoning)两个关键指标。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型基础设施等内容。虽然论文主要是评估性的而非提出新的改进方法，但它对LLM推理能力的深入理解和评估对于进一步改进LLM的通用推理能力具有重要意义，符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型如何根据新信息动态更新预测的问题。针对未来事件预测场景，我们提出了E VOLVE CAST框架，用于评估LLMs在面对训练截止后新信息时的预测更新能力。在Metaculus平台构建的1,613个问题-新闻对数据集上，通过MDA、MSE、SMSPE和Brier Score等指标验证了模型的方向一致性、幅度对齐和置信度校准。实验发现，尽管LLMs能对新信息做出响应，但更新常不一致或过于保守，且基于语言表达和logits的置信度估计均未达到人类预测者水平。",
                    "summary_translation": "先前的研究大多将未来事件预测视为静态任务，未能考虑随着新证据出现，预测及其置信度应如何演变。为填补这一空白，我们提出了EVOLVECAST，一个用于评估大型语言模型（large language models）是否能够根据新信息适当修正其预测的框架。具体而言，EVOLVECAST评估当大型语言模型（LLMs）面对训练截止日期后发布的信息时，是否会调整其预测。我们以人类预测者作为比较参考，分析在更新情境下的预测变化和置信度校准（confidence calibration）。虽然大型语言模型（LLMs）对新信息表现出一定的响应性，但它们的更新往往不一致或过于保守。我们进一步发现，无论是语言化（verbalized）还是基于logits的置信度估计，都没有一种能持续优于另一种，且两者都远未达到人类参考标准。在各种设置中，模型往往表现出保守偏差（conservative bias），这凸显了对更稳健的信念更新（belief updating）方法的迫切需求。",
                    "inspiration_trace": "# 从静态到动态：EVOLVECAST框架的逻辑演进\n\n## 一、宏观问题：静态评估与现实需求的脱节\n\n作者首先观察到，现实世界中的决策本质上是动态的——决策者需要不断根据新信息调整对未来事件的预测。然而，当前大型语言模型(LLMs)的评估方法存在根本性局限：\n\n- **评估视角的静态性**：大多数评估关注模型对训练截止日期前已知信息的回忆和推理能力\n- **预测任务的动态性**：实际预测需要基于不完整证据不断调整信念，而非一次性判断\n- **能力盲区**：现有框架无法评估模型如何随时间演变更新其预测和置信度\n\n这引出了一个核心问题：**如何评估LLMs在动态环境中根据新信息更新预测的能力？**\n\n## 二、问题聚焦：预测中的信念更新机制\n\n从宏观问题出发，作者将焦点缩小到预测任务中的信念更新过程：\n\n1. **预测的本质特征**：\n   - 预测是时间性和动态的，不同于事实检索\n   - 需要基于不完整或不确定证据进行推理\n   - 要求在新信号出现时调整已有信念\n\n2. **现有研究的局限**：\n   - 即使考虑置信度的概率预测研究，也主要关注单次静态预测\n   - 缺乏对模型如何根据新信息调整信念的考察\n   - 置信度校准问题尚未在动态背景下得到充分研究\n\n3. **核心研究问题**：\n   - LLMs能否像人类预测者那样，根据新信息适当更新其预测和置信度？\n   - 这种更新在方向、幅度和校准方面与人类表现有何差异？\n\n## 三、关键假设：基于观察的理论构建\n\n作者通过文献观察和现实分析，形成了三个关键假设：\n\n1. **响应性假设**：LLMs可能能够根据新信息调整其预测，但这种调整可能与人类预测者的调整方式存在系统性差异。\n\n2. **校准假设**：无论是语言表达的置信度还是基于logits的置信度，都可能无法准确反映预测的不确定性，导致校准不良。\n\n3. **保守性假设**：LLMs在更新预测时可能表现出保守性，即对新信息的反应不足，更新幅度小于人类预测者。\n\n这些假设构成了后续实验验证的理论基础，也为评估框架的设计提供了方向。\n\n## 四、方法论构想：动态评估框架的初步设计\n\n基于上述假设，作者开始构思评估框架的核心要素：\n\n1. **动态场景模拟**：需要构建一个随时间演变的信息环境，模拟真实预测场景\n2. **变化测量机制**：需要设计方法来捕捉和量化模型预测和置信度的变化\n3. **人类参考标准**：需要将模型行为与人类预测者的行为进行比较，建立评估基准\n\n这一阶段的核心挑战是如何将抽象的\"信念更新\"概念转化为可操作、可测量的评估指标。\n\n## 五、EVOLVECAST框架：从构想到具体实现\n\n作者将初步构想发展为完整的EVOLVECAST框架，包含三个关键组成部分：\n\n### 1. 任务形式化\n- 将动态预测任务定义为：评估LLMs是否以与参考标准相当的方式根据新信息修改其预测和置信度\n- 形式化表示：对于预测问题q，在时间t出现新信息x_t，测量模型信念p_t = P(Yes|x_t)的变化Δp = p_t - p_0\n\n### 2. 多维度评估标准\n为全面评估信念更新质量，作者设计了三个互补的评估维度：\n\n- **方向一致性**：使用平均方向准确性(MDA)评估模型和参考预测是否在同一方向更新信念\n- **幅度一致性**：通过均方误差(MSE)和对称均方百分比误差(SMSPE)衡量更新幅度的匹配程度\n- **置信度校准**：使用Brier分数评估模型置信度与参考标准的一致性\n\n### 3. 数据集构建\n- 从Metaculus预测平台获取问题和人类预测数据\n- 通过监控评论流检测新信息出现的时间点\n- 使用Google Search API检索相关新闻文章，并通过语义相似度选择最相关的更新\n- 最终构建包含1,613个问题-新闻对的数据集\n\n## 六、实验验证：从理论到实证\n\n作者设计了系统的实验来验证假设并评估框架有效性：\n\n### 1. 模型选择\n- 评估三个开源推理模型及其基础对应模型\n- 覆盖不同架构(Qwen和LLaMA)和参数规模(1.5B/7B/8B)\n- 目的是分离推理风格后训练的效果\n\n### 2. 实验条件\n- **基础条件**：仅提供预测问题，测量初始预测\n- **更新条件**：提供预测问题+新闻更新，测量更新后预测\n- **累积条件**：提供完整序列的新闻更新，测试 richer 上下文的影响\n\n### 3. 置信度提取方法\n- **黑盒方法**：模型明确表达1-10尺度的置信度，归一化到[0,1]\n- **白盒方法**：从模型输出概率中计算平均token概率\n\n## 七、结果分析与理论完善\n\n实验结果证实了作者的初始假设，并揭示了更深层次的发现：\n\n1. **响应性验证**：LLMs确实能够根据新信息调整预测，但更新往往不一致或过于保守\n\n2. **校准问题**：两种置信度提取方法均未显示出明显优势，且都远未达到人类参考标准\n\n3. **保守性确认**：模型普遍表现出保守偏差，对新信息的反应幅度小于人类预测者\n\n4. **意外发现**：\n   - 累积新闻上下文并未改善性能，反而可能导致下降\n   - 直接方向性提示比基于概率的方法产生了更高的方向一致性\n   - 即使提供人类预测作为参考锚点，模型也无法有效利用\n\n这些发现不仅验证了初始假设，还揭示了LLMs在建模信念动态方面的基本困难，强调了需要超越静态预测准确性的评估框架。\n\n## 八、逻辑演进总结\n\n从宏观问题到具体方法，作者的思考过程展现了一个清晰的逻辑演进：\n\n**现实需求(动态决策) → 研究空白(静态评估) → 核心问题(信念更新能力) → 关键假设(响应性、校准、保守性) → 方法构想(动态评估框架) → 具体实现(EVOLVECAST) → 实验验证 → 理论完善**\n\n这一逻辑链不仅展示了作者如何从观察现实世界需求出发，识别研究空白，形成假设，设计评估框架，并通过实验验证假设的完整思考过程，也体现了学术研究从问题识别到方法构建的标准路径。最终，EVOLVECAST框架填补了评估LLMs动态预测能力的研究空白，为未来的研究提供了新的方向和方法论基础。"
                },
                {
                    "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs",
                    "arxiv_id": "2509.23967",
                    "authors": "Ken Deng, Zizheng Zhan, Wen Xiang, Wenqiang Zhu, Tianhao Peng, Xinping Lei, Weihao Li, Jingxuan Xu, Kun Wu, Yifan Yao, Haoyang Huang, Huaixi Tang, Kepeng Lei, Zhiyi Lai, Songwei Yu, Zongxian Feng, Zuchen Gao, Weihao Xie, Chenchen Zhang, Yanan Wu, Yuanxing Zhang, Lecheng Huang, Yuqun Zhang, Jie Liu, Zhaoxiang Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu",
                    "summary": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off). Specifically, HiPO combines a hybrid data pipelineproviding paired Think-on and Think-off responseswith a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning. Experiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy. Finally, we hope HiPO a can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是改进大语言模型的基础推理能力，提出了混合策略优化(HiPO)这一新框架，使LLM能够自适应地选择推理策略（何时进行详细推理，何时直接响应）。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的标准。 其次，论文包含多个正面指标：明确关注Large Language Models (LLMs)这一核心概念；专注于reasoning能力（特别是在数学和编码基准测试中验证）；采用了reinforcement learning作为训练方法（混合强化学习奖励系统）。 第三，论文不符合任何排除标准：不涉及多模态与视觉内容；不聚焦于特定应用领域（虽然使用数学和编码基准测试，但这些是通用推理能力的评估，而非特定领域应用）；不关注模型可靠性层面的水印、安全等问题。 论文的核心贡献是提出了一种能动态控制推理过程的新方法，在保持或提高准确性的同时显著提高推理效率，这直接服务于提升LLM的通用推理能力这一研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决大型语言模型中过度思考问题，导致不必要的token使用和推理成本增加。针对复杂推理任务场景，我们提出了一种HiPO(Hybrid Policy Optimization)框架，使LLMs能够有选择地决定何时进行详细推理(Think-on)，何时直接响应(Think-off)。在数学和编码基准测试上通过准确率和token长度等指标验证了其有效性，显著减少了token长度同时保持或提高了准确性。",
                    "summary_translation": "大型语言模型(LLMs)越来越依赖思维链(CoT, chain-of-thought)推理来提高在复杂任务上的准确性。然而，总是生成冗长的推理过程是低效的，导致过多的令牌(token)使用和更高的推理成本。本文介绍了混合策略优化(Hybrid Policy Optimization, 即HiPO)，这是一个自适应推理控制框架，使大型语言模型能够有选择地决定何时进行详细推理(Think-on)以及何时直接响应(Think-off)。具体而言，HiPO结合了一个混合数据管道（提供成对的Think-on和Think-off响应）和一个混合强化学习(reinforcement learning)奖励系统，该系统在平衡准确性和效率的同时，避免过度依赖详细推理。在数学和编码基准测试(benchmarks)中的实验表明，HiPO可以在保持或提高准确性的同时，显著减少令牌长度。最后，我们希望HiPO能成为一种高效自适应推理的原则性方法，推动面向推理的大型语言模型在现实世界资源敏感环境中的部署。",
                    "inspiration_trace": "# HiPO方法逻辑链分析：从问题洞察到解决方案\n\n## 一、宏观问题：LLM推理的效率与准确性矛盾\n\n**起点观察**：大型语言模型(LLMs)在复杂任务上取得成功，关键在于思维链(CoT)推理——通过分步解析提升准确性。\n\n**问题识别**：CoT导致\"过度思考\"(overthinking)现象——即使是简单问题也生成冗长推理链，造成token使用量激增、延迟提高、效率下降，形成推理质量与计算成本的根本张力。\n\n## 二、现有方法分析与局限\n\n**方法分类**：作者系统梳理两类自适应推理控制方法：\n1. 基于训练的自适应推理：RL/SFT通过长度惩罚或简洁奖励鼓励简洁推理\n2. 外部控制：手工提示或动态指令限制推理\n\n**局限洞察**：现有方法存在三重缺陷：\n- 监督信号粗糙，无法精确区分问题复杂度\n- 单调激励阻碍模型对困难问题进行深入推理\n- 缺乏准确性、延迟与token效率间的原则性权衡机制\n\n## 三、核心假设形成\n\n**关键洞察**：需要一种动态平衡机制，使模型能自主决定何时\"深度思考\"(Think-on)、何时\"直接回答\"(Think-off)。\n\n**双假设构建**：\n1. **混合数据假设**：提供成对的Think-on/Think-off响应数据，能让模型学习问题复杂度与推理深度的映射关系\n2. **混合奖励假设**：设计平衡两种模式的奖励系统，可防止过度依赖冗长推理，同时确保模式选择与实际性能增益一致\n\n## 四、方法论设计：HiPO框架\n\n### 1. 混合数据构建管道\n\n**数据生成逻辑**：\n- 为每个查询在两种模式下各采样N个响应，验证正确性\n- 选择通过率更高的模式；若差异小于阈值δ，则倾向Think-off（鼓励简洁）\n- 保留获胜模式中最短的正确响应，强制1%查询随机分配模式以增加多样性\n- 生成模式选择解释，增强模型对决策理由的理解\n\n**格式设计**：使用特殊标记(<judge>、<think_on/off>、<answer>)清晰分隔判断、推理和答案，形成结构化训练信号\n\n### 2. 混合强化学习奖励系统\n\n**奖励机制设计**：\n- **基本奖励**：结合答案正确性(ACC)和格式正确性(FORMAT)\n- **偏差调整**：动态规范化模式贡献，防止过度拟合Think-on模式\n- **双优势函数**：\n  * 判断优势(Ajudge)：全局视角评估模式选择价值\n  * 答案优势(Aanswer)：模式内视角评估响应质量\n- **令牌级分配**：根据令牌所属段(判断/答案)分配相应优势值\n\n**训练范式**：两阶段策略——冷启动阶段建立基础能力，强化学习阶段精细调优模式选择\n\n## 五、验证与优化\n\n**实验验证**：在数学和编程多基准测试中证明HiPO能显著减少token长度(平均30%)，同时保持或提高准确性(平均提升6.2%)\n\n**消融研究**：系统验证各组件贡献，包括：\n- 最短响应选择策略的有效性\n- 全局/局部优势函数的必要性\n- 超参数(γ、N、ω)对性能的影响\n\n**动态分析**：追踪训练过程中Think-on/Think-off激活比例变化，证明模型确实学会根据任务难度自适应调整推理深度\n\n## 六、逻辑链条总结\n\n从\"过度思考\"问题观察→现有方法局限分析→动态平衡需求洞察→混合数据与混合奖励双假设→HiPO框架设计→实验验证优化，形成完整闭环。作者通过系统性思考，将LLM推理的效率-准确性矛盾转化为可计算的自适应控制问题，创新性地结合数据构建与强化学习，实现了对推理深度的原则性控制。"
                },
                {
                    "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step",
                    "arxiv_id": "2509.23924",
                    "authors": "Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao",
                    "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了解决Masked Diffusion Language Models (MDLMs)训练和推理不一致性的新方法，包括EOSER和ASS解码调度器以及CJ-GRPO强化学习算法。从第一步核心判断来看，论文本质上是关于改进LLM的基础能力和提出新的训练范式的研究，特别是增强模型的推理能力，这符合保留标准。从第二步正面指标看，论文涉及大语言模型(MDLMs)、推理能力(明确在数学和规划基准测试上实验)以及强化学习方法(CJ-GRPO)，满足多个正面指标。从第三步排除标准看，论文虽然标题中提到\"Diffusion\"，但这是指语言模型架构而非视觉领域的扩散模型，且论文没有聚焦于特定应用领域或模型可靠性的应用层面问题。综合分析，这篇论文致力于提高大语言模型本身的通用推理能力，通过新的解码策略和强化学习算法来优化模型性能，完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在优化掩码扩散语言模型(MDLMs)的解码效率和强化学习训练。针对MDLMs在推理过程中存在的<EOS>陷阱和轨迹不一致问题，我们提出了EOSER、ASS解码调度器和CJ-GRPO算法，并在数学和规划推理任务上通过准确率等指标验证了其有效性。实验结果表明，所提方法能显著减少解码步骤（从L/2到log₂L），同时保持或提高模型性能，特别是在规划任务上表现突出。",
                    "summary_translation": "掩码扩散语言模型（Masked diffusion language models, MDLMs）最近作为一种自回归（autoregressive, AR）语言模型的有前景的替代方案而出现，提供了并行解码（parallel decoding）、灵活生成顺序（flexible generation orders）以及更少推理步骤（inference steps）的潜力等特性。尽管有这些优势，但专门为MDLMs量身定制的解码策略和强化学习（reinforcement learning, RL）算法仍然研究不足。一种朴素的方法是将AR模型中成熟的技术直接转移到MDLMs上。然而，这引发了一个直接的问题：这种朴素的转移真的是最优的吗？例如，1）分块（Block-wise）和半自回归（semi-AR）解码策略在MDLMs的训练过程中并未被使用，那么为什么它们在推理过程中会优于完整的扩散式解码呢？2）将为AR模型设计的RL算法直接应用于MDLMs表现出训练-推理不一致性，因为MDLM解码是非因果（non-causal）的（并行的）。这导致了推演轨迹（rollout trajectory）和优化轨迹（optimization trajectory）之间的不一致。\n\n为了应对这些挑战，我们提出了EOS早期拒绝（EOS Early Rejection, EOSER）和递增步长（Ascending Step-Size, ASS）解码调度器，它们释放了MDLMs执行完整扩散式解码的潜力，在更少的解码步骤中实现了有竞争力的性能。此外，我们引入了一致性轨迹组相对策略优化（Consistency Trajectory Group Relative Policy Optimization, CJ-GRPO）来驯服MDLMs，它强调了推演轨迹和优化轨迹之间的一致性，并减少了由跳步优化（skip-step optimization）引起的优化错误。我们使用LLaDA-8B-Instruct在推理任务上进行了广泛的实验，如数学和规划基准测试。结果表明，所提出的EOSER和ASS机制，以及CJ-GRPO，在有效且高效地驯服MDLMs方面显示出巨大的潜力。\n\n代码：https://github.com/yjyddq/EOSER-ASS-RL。",
                    "inspiration_trace": "# 从问题观察到方法构建：MDLMs优化逻辑链分析\n\n## 一、宏观问题：掩码扩散语言模型的未解之谜\n\n掩码扩散语言模型(MDLMs)作为自回归模型的潜在替代方案，展现出并行解码、灵活生成顺序和更少推理步骤的潜力。然而，一个根本性问题悬而未决：**直接将自回归模型中成熟的技术迁移到MDLMs是否是最优解？** 这一宏观问题引出了两个关键挑战。\n\n## 二、具体挑战：解码策略与强化学习的不一致性\n\n### 挑战1：解码策略的悖论\n作者观察到：在MDLMs训练中未使用的block-wise和semi-AR解码策略，在推理时却意外地优于全扩散式解码。这引发了一个核心问题：**为什么未经专门训练的解码策略反而表现更好？**\n\n### 挑战2：强化学习的轨迹不一致\n直接将AR模型设计的RL算法应用于MDLMs会导致训练-推理不一致，因为MDLM解码是非因果的(并行的)，这导致rollout轨迹和优化轨迹之间存在根本性不匹配。\n\n## 三、关键观察：揭示MDLMs的行为模式\n\n通过实验，作者发现了三个关键现象：\n\n1. **token置信度演化规律**：在去噪过程中，平均token置信度在初始步骤较低，但随着去噪进行急剧增加。\n   \n2. **<EOS>置信度异常**：<EOS>的平均置信度显著高于<non-EOS>，尤其在早期步骤。\n\n3. **解码策略性能差异**：semi-AR(block-wise)解码优于全扩散解码，因为它通过块约束避免了在早期步骤解码序列末尾的<EOS>。\n\n这些观察揭示了一个核心问题：**全扩散解码存在\"<EOS>陷阱\"**——在早期低置信度阶段，模型过早生成序列结束符，导致偏离正确的去噪轨迹。\n\n## 四、假设形成：从现象到理论解释\n\n基于观察，作者形成了三个核心假设：\n\n1. **<EOS>陷阱假设**：全扩散解码性能较差是因为在早期低置信度阶段，模型对<EOS>的置信度过高，导致过早生成序列结束符。\n\n2. **非均匀调度假设**：由于token置信度从低到急剧增加的变化趋势，均匀的步骤大小解码调度器可能是次优的。早期应谨慎解码(解码较少token)，后期应更积极(解码更多token)。\n\n3. **轨迹一致性假设**：由于MDLM的非因果性质，直接应用AR模型的RL算法会导致rollout轨迹和优化轨迹不一致，从而引入优化误差。\n\n## 五、方法论构建：三管齐下的解决方案\n\n### 1. EOS Early Rejection (EOSER)\n**目标**：解决全扩散解码中的\"<EOS>陷阱\"问题。\n\n**方法**：引入随去噪步骤s变化的衰减系数γ，早期抑制<EOS>的置信度，随去噪进行逐渐恢复其概率。\n\n```\nγ = γmin + (γmax - γmin) * s/(S-1)\n```\n\n**核心思想**：通过动态调整<EOS>的置信度，防止模型在早期过早结束序列，同时允许在适当时候生成序列结束符。\n\n### 2. Ascending Step-Size (ASS) Decoding Scheduler\n**目标**：利用token置信度变化趋势，减少解码步骤数量。\n\n**方法**：用2的幂次方调度替代固定步骤大小，早期解码较少token，后期解码更多token。\n\n```\nAscending Step-Size Scheduler(s) = {2^(s+1), if s = S-1; 2^s, otherwise}\n```\n\n**核心思想**：将解码步骤从L/2减少到log₂L，时间复杂度从O(L)降低到O(log₂L)，实现更高效的推理。\n\n### 3. Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\n**目标**：确保rollout和优化轨迹的一致性，减少优化误差。\n\n**方法**：记录每个去噪步骤的中间状态(包括token置信度和解码位置)，利用这些状态指导优化轨迹。\n\n```\npθ(x^s_g | x^{s-1}_g) = πθ(·| x^{s-1}_g) · pos^s_g\n```\n\n**核心思想**：通过保持rollout和优化轨迹的一致性，解决MDLM非因果性质导致的RL优化不一致问题。\n\n## 六、实验验证：从理论到实践\n\n作者在数学和规划任务上验证了方法的有效性：\n\n1. **EOSER**显著提高了全扩散解码的性能，有效缓解了\"<EOS>陷阱\"。\n2. **ASS调度器**在保持性能的同时大幅减少了解码步骤，实现了O(log₂L)的时间复杂度。\n3. **CJ-GRPO**与EOSER和ASS调度器结合，在更少解码步骤下实现了与更多步骤相当甚至更好的性能。\n\n特别值得注意的是，作者还发现了任务特性与解码策略的匹配关系：规划任务(如Countdown和Sudoku)更适合并行推理的全扩散解码，而数学任务(如GSM8K和MATH500)更适合顺序推理的semi-AR解码。这一发现为未来混合扩散式和自回归式推理提供了方向。\n\n## 七、逻辑链总结\n\n从宏观问题到具体方法论的完整逻辑链：\n\n**宏观问题** → **具体挑战** → **关键观察** → **核心假设** → **方法论构建** → **实验验证**\n\n这一逻辑链展示了作者如何从\"MDLMs技术迁移是否最优\"的宏观问题出发，通过观察\"<EOS>陷阱\"等现象，形成关于解码策略和强化学习一致性的假设，最终构建出EOSER、ASS和CJ-GRPO三管齐下的解决方案，并通过实验验证其有效性。这一系统性思考过程为MDLMs的优化提供了新的思路和方法。"
                },
                {
                    "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models",
                    "arxiv_id": "2509.23863",
                    "authors": "Ziyi Yang, Weizhou Shen, Ruijun Chen, Chenliang Li, Fanqi Wan, Ming Yan, Xiaojun Quan, Fei Huang",
                    "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出SPELL（一种多角色自我博弈强化学习框架）来提高大语言模型的长上下文推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力（长上下文推理），提出了新的训练范式（自我博弈强化学习），属于增强LLM通用推理能力的研究。从第二步正面指标看，论文包含了所有关键主题：核心概念（LLMs）、能力方向（reasoning）、训练方法（reinforcement learning, self-evolve）和新兴范式（multi-agent systems）。论文不符合第三步的任何排除标准，因为它不涉及多模态、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的多角色框架是一种通用的智能体协作方法，用于增强LLM的通用问题解决能力，而非针对特定领域。总之，SPELL框架通过自我博弈和强化学习直接提升了LLM的通用推理能力，与研究目标高度一致。",
                    "summary2": "本文旨在解决长上下文语言模型推理能力提升的问题。针对长文本处理困难及人工标注稀缺的场景，我们提出了一种SPELL多角色自我博弈强化学习框架，集成了提问者、响应者和验证者三个循环角色，并在六个长上下文基准测试上通过pass@k等指标验证了其有效性，实现了平均7.6点的性能提升。",
                    "summary_translation": "大型语言模型（LLMs，大型语言模型）在长上下文推理（long-context reasoning，长上下文推理）方面的进展一直落后于其他最近的进展。这一差距不仅源于处理长文本的固有难度，还源于可靠的人工注释（human annotations，人工注释）和可编程验证的奖励信号（programmatically verifiable reward signals，可编程验证的奖励信号）的稀缺。在本文中，我们提出了SPELL，一个多角色自我博弈强化学习（multi-role self-play reinforcement learning，多角色自我博弈强化学习）框架，能够实现长上下文推理的可扩展、无标签优化（scalable, label-free optimization，可扩展、无标签优化）。SPELL在单个模型中集成了三个循环角色——提问者（questioner，提问者）、响应者（responder，响应者）和验证者（verifier，验证者），以实现持续的自我改进。提问者从原始文档生成问题并配以参考答案；响应者学习基于文档解决这些问题；验证者评估响应者输出与提问者参考答案之间的语义等价性（semantic equivalence，语义等价性），产生奖励信号以指导持续训练。为了稳定训练，我们引入了一个自动课程（automated curriculum，自动课程），逐渐增加文档长度，以及一个奖励函数（reward function，奖励函数），根据模型不断发展的能力调整问题难度。在六个长上下文基准测试（long-context benchmarks，长上下文基准测试）上的广泛实验表明，SPELL在各种LLMs上持续提高性能，并且优于在大规模注释数据上微调的同等规模模型。值得注意的是，SPELL在强大的推理模型Qwen3-30B-A3B-Thinking上实现了pass@8指标平均7.6分的提升，提高了其性能上限，并显示出扩展到更强大模型的潜力。",
                    "inspiration_trace": "# SPELL方法逻辑链推演\n\n## 宏观问题：长上下文推理能力的发展瓶颈\n\n作者从观察到的一个核心现象出发：**大语言模型(LLMs)的长上下文推理能力发展明显滞后于其他能力**。这一滞后不仅源于长文本处理的固有难度，更因为两个关键瓶颈：\n- 人工标注成本高昂且不可靠（在LongBench-V2上人类准确率仅25.1%）\n- 缺乏可编程验证的奖励信号（传统RLVR方法难以应用于长上下文场景）\n\n## 问题聚焦：外部监督的局限性\n\n作者进一步观察到，随着模型能力接近或超越人类水平，外部监督成为根本性瓶颈：\n- 现有长上下文对齐方法（LongAlign、LongPO等）均依赖某种形式的外部监督\n- 上下文长度增加时，可靠标注获取难度和成本呈指数级增长\n- 监督多样性随上下文长度增加而减少\n\n## 核心假设：自博弈强化学习的潜力\n\n基于上述观察，作者提出核心假设：**自博弈强化学习可以让模型通过生成和解决自己的任务来实现自我进化，无需人类标签**。但这一假设面临关键挑战：\n- 长上下文推理中，答案可能语义正确但表达方式差异很大\n- 简单字符串匹配或多数投票无法提供可靠奖励信号\n\n## 初步思路：多角色自我博弈框架\n\n作者提出初步解决方案：**单一模型扮演三个互补角色**：\n1. **提问者**：从原始文档生成问题及参考答案\n2. **回答者**：基于文档尝试解决这些问题\n3. **验证者**：评估回答者输出与参考答案的语义等价性\n\n这一设计形成闭环系统，使模型能够自主生成训练数据和奖励信号。\n\n## 关键创新：解决具体挑战\n\n作者进一步聚焦并解决了三个关键挑战：\n\n### 1. 稳定奖励信号生成\n**问题**：如何验证语义等价性？  \n**解决方案**：验证者通过在可验证任务上训练自一致性，使用多数投票聚合多个判断，减少方差。  \n**创新点**：结合规则检查和验证者共识，`rres_i = max(Rrule(yi, a), vver_i)`，避免语义正确但表达不同导致的误判。\n\n### 2. 自适应课程学习\n**问题**：如何确保问题难度与模型能力匹配？  \n**解决方案**：引入历史记忆机制，逐步增加文档长度和问题复杂度。  \n**创新点**：高斯形状奖励函数，以0.5成功概率为中心，`rque = exp(-(rres-μ)²/2σ²)`，确保问题既不太简单也不过于困难。\n\n### 3. 训练稳定性保障\n**问题**：如何平衡不同角色的贡献？  \n**解决方案**：角色特定动态采样策略，防止验证者样本主导更新。  \n**创新点**：统一策略优化，联合优化所有三个角色的GRPO目标，`JGRPO(θ) = Jque_GRPO(θ) + Jres_GRPO(θ) + Jver_GRPO(θ)`。\n\n## 最终方法：SPELL框架\n\n通过上述逻辑链，作者最终形成了SPELL（Self-Play Reinforcement Learning for Evolving Long-Context Language Models）框架，这是一个：\n- **自给自足的闭环系统**：单一模型通过扮演三个互补角色实现自我进化\n- **无需外部监督**：不依赖人工标注或可编程验证的奖励信号\n- **自适应课程学习**：根据模型能力动态调整任务难度\n- **训练稳定高效**：通过角色特定奖励设计和动态采样策略确保训练稳定性\n\n这一方法在12个不同架构和大小的模型上验证，一致提升了长上下文推理能力，甚至在某些情况下超越了依赖大量人工标注的指令微调模型，证明了自博弈强化学习在长上下文推理领域的巨大潜力。"
                },
                {
                    "title": "Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions",
                    "arxiv_id": "2509.23782",
                    "authors": "Yoonah Park, Haesung Pyun, Yohan Jo",
                    "summary": "Large Language Models (LLMs) often fail on multiple-choice questions (MCQs) despite demonstrating correct knowledge in other contexts, such as free-form generation. To investigate the mechanism underlying this knowledge-prediction gap on MCQs and alleviate it, we conduct a probing analysis and find that residual streams in certain layers contain a subspace spanned by two important bases: a \\emph{knowledge basis} that encodes the probability of the ground-truth answer for a given MCQ and a \\emph{prediction basis} that encodes the probability of the answer choice predicted by the model. We observe that incorrect predictions arise from a misalignment of the model's hidden states along these two bases. Hence, we introduce \\textbf{KAPPA} (Knowledge-Aligned Prediction through Projection-based Adjustment), a parameter-free intervention that transforms the hidden states to align the prediction coordinate with the knowledge coordinate within this subspace. Experiments on binary-choice reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA substantially improves accuracy and consistently outperforms baselines. While optimal subspaces differ across tasks, subspaces generalize to some extent, as supported by cross-dataset experiments. Moreover, KAPPA extends its effectiveness to free-form questions beyond MCQs. Our work provides a new geometric understanding of the knowledge-prediction gap and offers a practical method for better aligning model behavior with its latent knowledge.",
                    "category": "cs.CL",
                    "filter_reason": "我按照筛选标准对这篇论文进行了全面分析，判断其符合研究目标。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLMs)在多选题场景中的\"知识-预测差距\"问题，并提出了解决方案。论文的核心贡献是发现LLM内部存在知识基和预测基两个重要维度，并提出了KAPPA方法来对齐这两个维度，从而提高模型的推理准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确研究Large language models (LLMs)，并聚焦于reasoning和problem-solving能力，特别是解决多选题这类需要推理的任务。虽然论文没有涉及强化学习、智能体等新兴范式，但已足够表明其与研究目标相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于医疗、化学等特定应用领域，也不讨论水印、安全等模型可靠性问题。 最后，论文提出的KAPPA方法是一种通用的干预方法，用于对齐模型的预测与其潜在知识，这可以被视为提升LLM通用推理能力的方法，而非针对特定领域的应用。论文还指出该方法的效果可以扩展到多选题以外的自由形式问题，进一步证明其通用性。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，符合研究目标的核心要求。",
                    "summary2": "本文旨在解决大语言模型在多项选择题上存在的知识-预测差距问题。针对LLMs在MCQs中无法正确表达其内部知识的现象，我们提出了一种基于投影调整的无参数干预方法KAPPA，通过在残差流中识别知识-预测子空间并对齐坐标来弥合差距。在Big-Bench-Hard和ARC-Challenge的二项选择重构数据集上，通过准确率指标验证了其有效性，显著提升了模型性能并展现出跨数据集泛化能力。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）尽管在其他情境（如自由形式生成）中展现出正确知识，但在多项选择题（multiple-choice questions, MCQs）上常常表现不佳。为探究MCQs中这种知识-预测差距背后的机制并缓解该问题，我们进行了探测分析，发现特定层的残差流（residual streams）包含一个由两个重要基（bases）张成的子空间（subspace）：一个编码给定MCQ正确答案概率的\\emph{知识基}（knowledge basis）和一个编码模型预测答案选择概率的\\emph{预测基}（prediction basis）。我们观察到，错误预测源于模型隐藏状态（hidden states）在这两个基上的不对齐。因此，我们提出了\\textbf{KAPPA}（Knowledge-Aligned Prediction through Projection-based Adjustment，基于投影调整的知识对齐预测），这是一种无参数的干预方法，通过变换隐藏状态来使该子空间内的预测坐标与知识坐标对齐。在Big-Bench-Hard和ARC-Challenge的二选一重构任务上的实验表明，KAPPA显著提高了准确性，并持续优于基线方法。虽然最优子空间因任务而异，但子空间在一定程度上具有泛化性，跨数据集实验支持了这一点。此外，KAPPA的有效性不仅限于MCQs，还扩展到了自由形式问题。我们的工作为知识-预测差距提供了新的几何理解，并提供了一种实用方法来更好地使模型行为与其潜在知识（latent knowledge）对齐。",
                    "inspiration_trace": "# 从观察到干预：KAPPA方法的逻辑演进\n\n## 1. 宏观问题：知识-预测差距的发现\n\n**起点现象**：大型语言模型(LLMs)在多选题(MCQ)评估中表现不佳，尽管它们在其他情境(如自由形式生成)中展示了正确的知识。这引发了一个核心问题：为什么模型拥有知识却无法在特定任务中正确表达？\n\n**关键观察**：\n- 模型可能在自由形式设置中生成正确答案，但在同一问题的MCQ格式中选择错误选项\n- 即使LLMs选择了错误答案，其内部表示中仍编码了正确答案，可通过线性探针检测到\n\n## 2. 假设形成：几何错位理论\n\n**核心假设**：知识-预测差距源于模型隐藏状态沿着两个重要基的几何错位：\n- 知识基(knowledge basis)：编码给定MCQ的正确答案概率\n- 预测基(prediction basis)：编码模型预测的答案选择概率\n\n**理论框架**：当模型的隐藏状态沿着这两个基对齐时，预测通常是正确的；而当它们错位时，预测往往是错误的。这表明问题不在于知识缺失，而在于知识表达与预测机制之间的不协调。\n\n## 3. 验证方法：探测分析与子空间识别\n\n**验证策略**：\n1. **探测分析**：在模型残差流中寻找是否存在由知识基和预测基张成的子空间\n2. **数据构建**：为每个输入提示存储真实标签和模型预测，创建两个数据集\n3. **线性探针训练**：训练两个独立的逻辑回归分类器，一个预测真实标签，一个预测模型输出\n4. **子空间构建**：使用训练好的探针权重作为两个基，构建知识-预测子空间\n\n**关键发现**：\n- 在某些层的残差流中确实存在由这两个功能不同的基张成的子空间\n- 知识探针的准确率通常超过模型自身的预测准确率，证实了模型内部编码了正确知识\n- 导致正确预测的隐藏状态激活通常沿着两个基很好地对齐，而错位通常导致错误预测\n\n## 4. 解决方案：KAPPA干预方法\n\n**方法设计**：基于上述发现，提出KAPPA（Knowledge-Aligned Prediction through Projection-based Adjustment）方法，这是一种基于测试时的干预，通过几何变换调整隐藏状态。\n\n**核心机制**：\n1. **目标**：通过最小化扰动，使预测坐标与知识坐标对齐\n2. **数学形式**：将问题表述为约束优化问题，推导出闭式解：\n   ```\n   h' = h + (ũ_knowledgeᵀh̃ - ũ_predictionᵀh̃)/||u_prediction||² * u_prediction\n   ```\n3. **几何解释**：该变换仅在预测方向上修改隐藏状态，保留所有正交分量不变\n\n**方法扩展**：引入更一般化的对齐约束，通过超参数w和β参数化目标映射，以适应更广泛的知识-预测关系：\n```\nũ_predictionᵀh̃' = w·ũ_knowledgeᵀh̃ + β·sign(ũ_knowledgeᵀh̃)\n```\n\n## 5. 实验验证与效果评估\n\n**主要发现**：\n1. **显著性能提升**：在BBH-Binary上，KAPPA将Llama-2-7B-Chat的准确率从50.9%提升至70.8%\n2. **跨数据集泛化**：在一个数据集上训练的探针可有效应用于其他数据集，表明子空间反映了基本的模型属性\n3. **超越MCQ**：KAPPA提高了自由形式生成的准确性，证明其不仅限于多选题场景\n4. **不影响一般能力**：在vicuna-eval上，KAPPA在知识和推理密集型任务上表现优异，同时保持生成连贯响应的能力\n\n## 6. 理论贡献与实践意义\n\n**理论贡献**：\n- 提供了知识-预测差距的几何理解框架\n- 揭示了LLMs内部表示的结构化特性\n- 证明了模型失败往往源于表达机制而非知识缺失\n\n**实践意义**：\n- 提供了一种无需参数更新的轻量级干预方法\n- 为提高模型忠实度提供了新思路\n- 为理解和改进LLMs的内部工作机制开辟了新途径\n\n这一完整的逻辑链条展示了作者如何从宏观问题出发，通过观察、假设形成、方法设计、实验验证，最终提出一个既有理论深度又有实用价值的解决方案，成功弥合了LLMs中的知识-预测差距。"
                },
                {
                    "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality",
                    "arxiv_id": "2509.23765",
                    "authors": "Junliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, Haifeng Wang",
                    "summary": "Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model's internal knowledge boundaries, exacerbating the so-called \"hallucination tax\". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model's expressed knowledge and the base model's parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model's internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是减少幻觉和提高事实性，这直接关联到LLM的通用推理能力。论文提出了\"Knowledge-Level Consistency Reinforcement Learning Framework (KLCF)\"这一新的训练范式，通过强化学习方法优化模型的事实表达，属于提升LLM核心能力的研究。 从正面指标看，论文明确涉及大语言模型(LLMs)这一核心概念，使用了强化学习(RL)作为训练方法，虽然未直接提及\"reasoning\"一词，但减少幻觉和提高事实性是推理能力的重要组成部分，特别是在长文本生成中保持事实一致性需要强大的推理能力。 论文不符合排除标准，它没有聚焦于多模态与视觉、特定应用领域或应用层面的模型可靠性研究。相反，它针对的是LLM的通用问题——幻觉和事实性缺陷。 在特殊和模糊情况处理上，论文提出了减少幻觉的新方法，通过知识一致性强化学习来提升模型的内在可靠性和推理质量，这正是提升LLM通用推理能力的关键方向。 综上所述，这篇论文的核心贡献是提出了一种新的强化学习框架来提高LLM的事实性和减少幻觉，这直接关联到提升LLM的通用推理能力，因此符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在长文本生成中的幻觉和事实性缺陷问题。针对长文本生成场景，我们提出了一种知识级一致性强化学习框架(KLCF)，通过双事实对齐机制(Dual-Fact Alignment)共同优化事实召回和精确度，并在FActScore、LongWiki等多个长文本基准测试上通过FActScore、Recall@K、Precision等指标验证了其有效性。",
                    "summary_translation": "幻觉（hallucination）和事实性缺陷（factuality deficits）仍然是大型语言模型（large language models, LLMs）在长文本生成（long-form generation）中可靠性的关键障碍。现有的人类反馈强化学习（reinforcement learning from human feedback, RLHF）框架主要依赖偏好奖励（preference rewards），然而它们常常忽视模型的内部知识边界（internal knowledge boundaries），加剧了所谓的\"幻觉税\"（hallucination tax）。为应对这一挑战，我们提出了知识级一致性强化学习框架（Knowledge-Level Consistency Reinforcement Learning Framework, KLCF），这是一个新颖的框架，专注于策略模型（policy model）的表达知识与基础模型（base model）的参数知识（parametric knowledge）之间的一致性，并引入了双事实对齐（Dual-Fact Alignment）机制来共同优化事实召回率（factual recall）和精确度（precision）。具体而言，KLCF利用预训练知识边界（pretrained knowledge boundaries）构建事实检查清单（fact checklist），指导在线强化学习（online reinforcement learning）以提高事实覆盖率和召回率；同时，它基于基础模型的内部知识训练一个自我评估模块（self-assessment module），以增强生成过程中的事实精确度。与依赖外部检索（external retrieval）或繁重验证（heavy verification）的先前方法不同，我们的奖励设计完全无需外部知识（external-knowledge-free）且轻量级（lightweight），使KLCF高效且易于扩展到大规模训练。实验结果表明，KLCF在多个长文本基准测试（long-form benchmarks）上显著提高了事实性指标（factuality metrics），并有效缓解了模型幻觉（model hallucinations）。",
                    "inspiration_trace": "# KLCF方法逻辑推演：从问题观察到解决方案\n\n## 一、宏观问题识别：长文本生成中的幻觉困境\n\n**核心观察**：\n1. 大型语言模型(LLMs)在长文本生成中存在严重的幻觉问题，即生成与既定事实相冲突的内容\n2. 幻觉在长文本中尤为危险，因为早期错误会通过\"滚雪球效应\"级联放大，逐渐破坏整个回应的可信度\n3. 现有RLHF框架主要依赖偏好奖励，但忽略了模型的内部知识边界，导致模型可能捏造超出其知识范围的事实\n\n**关键问题**：如何在提高长文本生成的事实性的同时，避免模型过度保守而牺牲信息覆盖率？\n\n## 二、深入分析：知识表达与参数化知识的错位\n\n**核心洞见**：\n1. LLMs的参数化知识(Parametric Knowledge)来源于预训练，而对齐模型学习如何表达这些知识\n2. 长文本事实性问题的本质是\"表达知识\"(Expressed Knowledge)与\"参数化知识\"之间的不一致\n3. 现有方法主要关注事实精确度(precision)，而忽略了事实召回(recall)，导致模型倾向于保守回应\n\n**形成假设**：\n1. 如果能够增加对齐模型的\"表达知识\"与基础模型的\"参数化知识\"之间的一致性，可以减少幻觉\n2. 需要同时优化两个目标：最大化从参数化知识中的事实召回，同时最小化超出知识边界的内容生成\n3. 有可能仅通过模型内部知识提供奖励信号来实现这种对齐，无需依赖外部检索\n\n## 三、方法论设计：双事实对齐机制\n\n基于上述分析，作者设计了知识层面一致性强化学习框架(KLCF)，核心是双事实对齐机制：\n\n### 1. 离线数据准备阶段\n- **知识边界探测**：从基础模型采样，探测其知识边界\n- **结构化提取**：将非结构化文本转换为可验证的原子声明集\n- **事实验证**：通过本地知识索引验证声明，获得可靠性标签\n- **资源构建**：准备事实清单和真实性奖励模型训练数据\n\n### 2. 双奖励机制设计\n**基于清单的一致性奖励(Checklist Reward)**：\n- 目的：提高事实覆盖率和召回率\n- 机制：监督生成与预构建清单的一致性\n- 计算：结合事实召回奖励和事实精确度奖励的加权调和平衡\n\n**基于置信度的真实性奖励(Truthfulness Reward)**：\n- 目的：防止生成超出知识边界的错误陈述\n- 机制：使用自我评估模块进行细粒度的真实性评估\n- 计算：评估生成声明为真的概率，取平均值作为奖励\n\n### 3. 辅助奖励与优化\n- **辅助奖励**：一般奖励、格式奖励和长度惩罚，确保整体生成质量\n- **策略优化**：使用组相对策略优化(GRPO)算法，实现高效训练\n\n## 四、关键创新与优势\n\n**核心创新**：\n1. 首次提出\"知识层面一致性\"作为长文本事实性对齐的核心目标\n2. 设计双事实对齐机制，同时优化事实召回和精确度\n3. 完全不需要外部知识的轻量级奖励系统，实现高效可扩展的在线RL训练\n\n**优势体现**：\n1. **效率**：比传统方法快3-4倍，无需外部检索\n2. **平衡**：同时优化召回和精确度，避免幻觉-保守主义权衡\n3. **可扩展**：适用于不同模型规模，从7B到32B均表现一致\n\n## 五、验证与启示\n\n**实验验证**：\n- 消融研究证实双奖励机制的必要性：单独使用任一奖励都会导致召回与精确度的失衡\n- 扩展性研究显示方法在不同模型规模上一致有效\n- 效率分析证明方法显著降低计算开销\n\n**理论启示**：\n1. 知识表达一致性是解决长文本幻觉问题的关键方向\n2. 内部知识边界可以作为有效的奖励信号源\n3. 双目标优化机制能够平衡事实性与全面性的矛盾\n\n这一逻辑链条从宏观问题出发，通过深入分析形成核心假设，最终设计出创新的双事实对齐机制，为解决长文本生成中的幻觉问题提供了全新思路。"
                },
                {
                    "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs",
                    "arxiv_id": "2509.23657",
                    "authors": "Shulin Huang, Yiran Ding, Junshu Pan, Yue Zhang",
                    "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs) attracts widespread attention. While reinforcement learning (RL) has shown superior performance for improving complex reasoning, its impact on cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains unexplored. We present the first systematic investigation into cross-lingual reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation model, we conduct experiments on diverse multilingual reasoning benchmarks, including math reasoning, commonsense reasoning, and scientific reasoning. Our investigation yields two significant findings: (1) Tuning with RL not only achieves higher accuracy but also demonstrates substantially stronger cross-lingual generalization capabilities compared to SFT. (2) RL training on non-English data yields better overall performance and generalization than training on English data, which is not observed with SFT. Furthermore, through comprehensive mechanistic analyses, we explore the underlying factors of RL's superiority and generalization across languages. Our results provide compelling evidence that RL enables the model with more robust reasoning strategies, offering crucial guidance for more equitable and effective multilingual reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)来提高大语言模型(LLM)的推理能力，特别是跨语言推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力。 其次，论文包含多个正面指标：核心概念上明确研究Large language models (LLMs)；能力方向上聚焦于reasoning，特别是math reasoning、commonsense reasoning和scientific reasoning；训练方法上使用reinforcement learning作为主要技术手段。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献在于系统研究了强化学习相比监督微调在提升LLM跨语言推理能力方面的优势，并发现RL训练能使模型获得更强大的推理策略。这直接服务于提升大语言模型通用推理能力的研究目标，因此应该被保留。",
                    "summary2": "本文旨在探索强化学习(RL)如何改善大语言模型的跨语言推理能力。针对多语言推理场景，我们提出了一种比较RL与监督微调(SFT)在跨语言泛化能力差异的方法，并在MGSM、MMath500、MMLU-ProX-Lite等多语言推理基准上通过准确率和泛化分数验证了其有效性。研究发现RL不仅优于SFT，且使用非英语数据训练效果更佳，为多语言推理提供了新思路。",
                    "summary_translation": "提升大语言模型(Large Language Models, LLMs)的复杂推理能力引起了广泛关注。尽管强化学习(Reinforcement Learning, RL)在提升复杂推理方面表现出优越性能，但与监督微调(Supervised Fine-Tuning, SFT)相比，其对跨语言泛化的影响尚未被探索。我们首次对RL和SFT的跨语言推理泛化进行了系统性研究。我们以Qwen2.5-3B-Base为基础模型，在多种多样的多语言推理基准上进行了实验，包括数学推理、常识推理和科学推理。我们的研究得出了两个重要发现：(1) 与SFT相比，使用RL进行调优不仅实现了更高的准确率，还表现出明显更强的跨语言泛化能力。(2) 在非英语数据上进行RL训练比在英语数据上训练能获得更好的整体性能和泛化效果，而这一现象在SFT中并未观察到。此外，通过全面的机制分析，我们探讨了RL在不同语言中表现优越和泛化的潜在因素。我们的结果提供了有力证据，证明RL能够使模型具备更稳健的推理策略，为更公平和有效的多语言推理提供了重要指导。",
                    "inspiration_trace": "# 强化学习提升LLM跨语言推理能力的逻辑链分析\n\n## 一、宏观问题：多语言推理的挑战与不平衡\n\n论文从多语言在人类社会和AI发展中的重要性出发，指出全球7000多种语言各具特色，而现有LLM在英语与其他语言间存在显著性能差距。作者观察到，尽管LLM在推理能力上取得进展，但跨语言推理仍面临重大挑战：模型不仅需要理解不同语言的语义内容，还需在多样化语言环境中执行逻辑推理和问题解决。\n\n## 二、现有方法的局限性\n\n作者系统梳理了现有研究路径的不足：\n\n1. **提示策略的局限**：当前多语言推理研究主要依赖\"先翻译后解决\"等提示策略，这些方法本质上仍依赖英语作为中介，而非提升模型内在的跨语言泛化能力。\n\n2. **监督微调(SFT)的缺陷**：SFT通过模仿专家思维链示例增强推理能力，但其学习范式基于模仿和记忆给定轨迹，导致模型对训练数据中的语言和模式产生过拟合，限制了跨语言迁移能力。\n\n## 三、关键观察与假设形成\n\n作者通过文献分析发现强化学习(RL)在推理任务中的独特优势：\n\n1. **RL在推理任务中的成功**：研究表明RL在数学和逻辑推理等复杂任务上表现优越，通过奖励引导机制使模型获得更稳健、可泛化的推理策略。\n\n2. **RL的跨任务泛化能力**：RL不仅提高模型性能，还展现出更强的跨任务泛化能力，这引发作者思考：RL是否也具备跨语言泛化的优势？\n\n基于这些观察，作者提出核心假设：**RL可能通过学习不依赖特定语言的推理策略，实现比SFT更强的跨语言泛化能力**。\n\n## 四、实验验证与意外发现\n\n为验证假设，作者设计了系统性实验，比较RL和SFT在多语言推理基准上的性能：\n\n### 发现1：RL的跨语言泛化优势\n实验结果表明，RL不仅准确性高于SFT，还展现出显著更强的跨语言泛化能力。RL训练使模型能更有效地将一种语言中学到的推理能力迁移到另一种语言。\n\n### 发现2：非英语数据的意外优势\n与传统认知相反，使用非英语数据(如中文和德语)进行RL训练比使用英语数据产生更好的整体性能和泛化能力。这一现象在SFT中未观察到，挑战了\"以英语为中心\"的训练范式。\n\n## 五、深入分析：探索RL优势的潜在机制\n\n为解释上述发现，作者从三个维度进行深入分析：\n\n### 1. 语言不一致性分析\n- **观察**：RL训练的模型在推理过程中不严格使用训练语言，而是采用多语言或混合语言进行推理。\n- **验证**：通过强制语言一致性的实验(添加语言控制提示和一致性奖励)，发现约束语言使用会显著降低RL模型的跨语言性能。\n- **结论**：语言灵活性使模型能够利用预训练中建立的更强大的多语言推理模块，而非局限于特定语言模式。\n\n### 2. 采样机制分析\n- **方法**：引入拒绝采样微调(RFT)作为SFT和RL之间的中间基线，比较不同采样策略的效果。\n- **发现**：从Base→SFT→RFT→RL，性能逐步提升，表明模型对解决路径的探索程度与推理能力正相关。\n- **解释**：RL的在线策略优化过程通过探索多样化解决路径，使模型学习更稳健、可泛化的推理策略，超越简单模仿学习。\n\n### 3. 语义特征转移分析\n- **方法**：分析模型训练前后语义表示的变化，使用PCA可视化最终层隐藏状态的转移模式。\n- **发现**：RL(特别是使用非英语数据)训练导致的语义表示转移最小，更接近预训练模型。\n- **解释**：预训练建立了对跨语言迁移至关重要的多语言推理结构，RL通过最小化对这些结构的干扰，保持了更强的泛化能力。\n\n## 六、方法论形成：RL跨语言推理优势的整合框架\n\n基于以上分析，作者形成了关于RL在跨语言推理中优势的整合方法论：\n\n1. **语言灵活性机制**：RL允许模型在推理过程中自由切换语言，这种灵活性使模型能够激活和利用预训练中建立的多语言推理模块，而非局限于特定语言模式。\n\n2. **探索-优化平衡机制**：RL通过采样多样化解决路径并进行在线策略优化，使模型能够发现更稳健、可泛化的推理策略，而非简单模仿给定解决方案。\n\n3. **语义空间稳定性机制**：RL(特别是使用非英语数据)能够更好地保持预训练中建立的多语言推理结构，最小化语义空间的偏移，从而实现更好的跨语言泛化。\n\n## 七、结论与意义\n\n作者通过系统研究揭示：RL相比SFT不仅提高了LLM的推理准确性，还显著增强了跨语言泛化能力；反直觉地，使用非英语数据进行RL训练比英语数据更有效。这些发现挑战了以英语为中心的训练范式，为构建更公平、有效的多语言推理系统提供了新思路。\n\n这一研究逻辑链条从宏观问题出发，通过观察现有方法局限性，形成创新假设，通过实验验证获得意外发现，进而深入分析潜在机制，最终形成系统方法论，展现了完整的科学探究过程。"
                },
                {
                    "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models",
                    "arxiv_id": "2509.23653",
                    "authors": "Zemin Huang, Yuhang Wang, Zhiyang Chen, Guo-Jun Qi",
                    "summary": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect tokens: once a token is generated, it typically remains fixed. The key challenge is to identify potential errors in the inputs. In this paper, we propose \\emph{\\underline{Rem}asking-\\underline{e}nabled \\underline{Di}ffusion Language Model (RemeDi}, a mask-based DLM that introduces \\emph{remasking} as another fundamental mechanism, enabling more flexible text refinement in diffusion-based text generation. To achieve this, RemeDi jointly predicts token distributions and per-token confidence scores at each step. The confidence scores determine which tokens to be unmasked after the current step, allowing the model to identify tokens with low quality and remask them. These remasked tokens can be resampled with richer context in subsequent steps. We design a remask-aware pipeline to train this ability, including supervised fine-tuning which teaches the model to detect and remask incorrect tokens in addition to predict mask tokens, and reinforcement learning which optimizes full generation trajectories toward higher rewards. Experiments show that RemeDi achieves the state-of-the-art results among open-source DLMs on multiple datasets.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"RemeDi\"的扩散语言模型，通过引入\"重新掩码\"(remasking)机制，使模型能够识别低质量token并进行重新采样，从而增强模型的自我修正和推理能力。论文设计了一个包含监督微调和强化学习的训练管道来优化这种能力。这种方法本质上是提升大语言模型的基础能力和通用推理能力，特别是通过自反思机制来改进输出质量，符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的要求。论文使用了强化学习作为训练方法，这是正面指标之一。虽然论文没有明确讨论数学或逻辑推理，但其提出的自我修正机制可以提升模型的问题解决能力。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。相反，论文提出的方法可以看作是提升模型内在可靠性和推理质量的新方法，这符合保留标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决基于掩码的扩散语言模型无法修订已生成不正确token的问题。针对文本生成场景，我们提出了一种RemeDi模型，引入自我反思的重新掩码机制，使模型能够识别低质量token并重新掩码以便后续重采样。在GSM8K、MATH、HumanEval等多个benchmark上通过准确率等指标验证了其有效性，取得了开源DLMs中最先进的性能。",
                    "summary_translation": "基于掩码的扩散语言模型(Mask-based Diffusion Language Models, DLMs)难以修正不正确的标记(token)：一旦生成了标记，它通常会保持固定。关键挑战在于识别输入中的潜在错误。在本文中，我们提出了\\emph{\\underline{重}掩码-\\underline{启}用的\\underline{扩}散语言模型(RemeDi)}，这是一种基于掩码的DLM，它引入了\\emph{重掩码}(remasking)作为另一种基本机制，能够在基于扩散的文本生成中实现更灵活的文本精炼。为实现这一目标，RemeDi在每个步骤中联合预测标记(token)分布和每个标记的置信度得分。置信度得分决定了在当前步骤后哪些标记将被取消掩码，使模型能够识别低质量的标记并对其进行重新掩码。这些被重新掩码的标记可以在后续步骤中利用更丰富的上下文进行重新采样。我们设计了一个感知重掩码的管道来训练这种能力，包括监督微调(supervised fine-tuning)，它教导模型除了预测掩码标记外，还能检测并重新掩码不正确的标记；以及强化学习(reinforcement learning)，它优化完整生成轨迹以获得更高的奖励。实验表明，在多个数据集上，RemeDi在开源DLMs中实现了最先进(state-of-the-art)的结果。",
                    "inspiration_trace": "# 从问题到解决方案：RemeDi的逻辑演进\n\n## 1. 宏观问题：扩散语言模型的固有局限\n\n**观察现象**：扩散语言模型(DLMs)作为自回归模型的替代方案，具有并行生成和灵活顺序的优势，特别是基于掩码的DLMs已成为主流变体。\n\n**核心问题**：现有DLMs存在一个根本性局限——一旦token被生成(unmasked)，就会保持固定不变，无法在后续步骤中修正。这导致早期生成的错误token会一直保留到生成结束，即使随着更多上下文的出现，这些错误变得明显。\n\n**关键挑战**：如何让模型能够识别已生成但可能错误的token，并提供一种机制来修正这些错误？\n\n## 2. 现有解决方案的不足\n\n**文献调研发现**：作者分析了两类现有解决方案：\n1. **推理时修正方法**：如predictor-corrector采样器，在推理时随机重新掩码部分token。这些方法缺乏识别哪些token实际错误的机制，效率低下且难以优化。\n2. **修改扩散过程方法**：如结合掩码扩散与均匀扩散或编辑扩散。这些方法要么缺乏确保掩码token数量单调减少的机制，要么没有提供原则性的错误检测方法。\n\n**洞察**：现有方法都没有解决核心问题——如何让模型自我反思并识别需要修正的token。\n\n## 3. 核心假设：自我反思的重新掩码\n\n**关键洞察**：如果模型能够评估每个已生成token的质量，并有选择地重新掩码低质量token，就能在后续步骤中用更丰富的上下文重新采样，从而提高生成质量。\n\n**直观例子**：模型最初生成\"left for the pies\"，但后来发现语义不匹配，应改为\"used for the pies\"。传统DLM无法修正这种错误，但如果模型能识别\"left\"是低质量token并重新掩码它，就能在后续步骤中生成正确的\"used\"。\n\n**核心假设**：通过联合预测token分布和每个token的置信度分数，模型可以自我反思并决定哪些token需要被重新掩码和重新生成。\n\n## 4. 方法设计：RemeDi架构\n\n**架构创新**：基于上述假设，作者设计了双流transformer架构：\n1. **Token预测流(TPS)**：负责预测掩码token的概率分布。\n2. **Unmasking策略流(UPS)**：负责输出每个token的置信度分数，决定哪些token应被unmask或重新掩码。\n\n**工作机制**：在每个扩散步骤中，模型首先通过UPS预测每个位置的置信度分数，然后基于这些分数决定哪些token保持unmask、哪些被重新掩码。与现有DLM不同，RemeDi允许已经生成的token被重新掩码，实现真正的自我修正能力。\n\n**关键保证**：设计噪声调度确保掩码token数量单调减少，维持扩散模型的基本特性。\n\n## 5. 训练策略：两阶段学习流程\n\n**挑战**：如何训练模型具备自我反思的重新掩码能力？\n\n**解决方案**：作者设计了两个互补的训练阶段：\n\n### 阶段1：Remask SFT（监督微调）\n**目标**：教会模型检测和重新掩码不正确的token。\n**方法**：\n- 构建包含两类噪声的训练样本：掩码token和随机替换的不正确token。\n- 设计标签策略：干净token标记为保持unmask，不正确token标记为重新掩码，掩码token标记为基于预测概率的软标签。\n- 结合扩散损失和置信度预测损失进行联合训练。\n\n### 阶段2：Remask RL（强化学习）\n**目标**：优化整个生成轨迹，提高最终输出质量。\n**方法**：\n- 将生成过程建模为马尔可夫决策过程，每个步骤包含unmasking策略和token预测策略。\n- 使用GRPO算法优化生成轨迹，奖励基于任务类型设计（可验证正确性或奖励模型评估）。\n- 通过强化学习微调整体生成策略，提高模型在复杂任务上的表现。\n\n## 6. 实验验证与效果分析\n\n**验证方法**：作者在多个基准数据集上评估RemeDi，包括数学问题、代码生成和一般任务。\n\n**关键发现**：\n1. **性能提升**：RemeDi在所有评估任务上实现了最先进的性能，显著超过现有DLMs。\n2. **重新掩码行为分析**：\n   - 代码生成中重新掩码最频繁，其次是数学推理，一般任务中最少。\n   - 随着问题难度增加，重新掩码频率也增加，表明迭代细化对复杂问题尤为重要。\n3. **消融研究**：\n   - Remask SFT优于传统SFT，特别是在数学和代码任务上。\n   - Remask RL比现有RL方法收敛更快且性能更好。\n\n**可视化验证**：通过具体例子展示了重新掩码如何实现多种修正操作，包括纠正计算错误、优化文本质量、合并/拆分token、插入和删除token等。\n\n## 7. 逻辑演进总结\n\n从观察到解决方案，作者展现了清晰的逻辑链条：\n\n1. **问题识别**：发现DLMs无法修正已生成token的根本局限。\n2. **现有方法分析**：识别现有解决方案的不足，缺乏原则性的错误检测机制。\n3. **核心洞察**：提出自我反思的重新掩码概念，允许模型评估并修正低质量token。\n4. **架构设计**：创建双流transformer架构，分离token预测和unmasking决策。\n5. **训练策略**：设计两阶段学习流程，先教会模型检测错误，再优化整体生成轨迹。\n6. **实验验证**：通过严格实验证明方法有效性，并深入分析机制行为。\n\n这一逻辑演进展示了作者如何从一个观察到的局限性出发，通过系统分析和创新思考，最终提出一个有效的解决方案，为扩散语言模型的发展开辟了新方向。"
                },
                {
                    "title": "Fast Thinking for Large Language Models",
                    "arxiv_id": "2509.23633",
                    "authors": "Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tianwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang, Hongyang He",
                    "summary": "Reasoning-oriented Large Language Models (LLMs) often rely on generating explicit tokens step by step, and their effectiveness typically hinges on large-scale supervised fine-tuning or reinforcement learning. While Chain-of-Thought (CoT) techniques substantially enhance performance on complex reasoning tasks, they remain inefficient, requiring long reasoning traces that increase latency and token usage. In this work, we introduce Latent Codebooks for Fast Thinking, a framework that uses concise CoT sketches only during training to learn a codebook of discrete strategy priors. At inference, the model conditions on a handful of continuous thinking vectors distilled from the codebook in a single pass, enabling strategy-level guidance without producing explicit reasoning tokens. To complement this design, we propose GainRouter, a lightweight routing mechanism that adaptively switches between fast codebook guided inference and slow explicit reasoning, thereby suppressing overthinking and reducing unnecessary token generation. Experiments across multiple reasoning benchmarks show that our approach achieves competitive or superior accuracy while substantially lowering inference cost, offering a practical path toward efficient and controllable reasoning in large language models.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了\"Latent Codebooks for Fast Thinking\"框架，旨在解决传统思维链(CoT)技术效率低下的问题。这明显属于改进LLM通用推理能力的研究，而非将LLM应用于特定领域。 其次，论文符合多个正面指标：核心概念上明确关注\"Reasoning-oriented Large Language Models (LLMs)\"；能力方向上专注于推理(reasoning)，并在多个推理基准上进行了实验验证；虽然未直接使用强化学习等方法，但提到了与这些方法的对比。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，没有将LLM应用于特定领域，也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的训练和推理范式，通过在训练阶段使用简洁的CoT草图学习离散策略先验的码本，在推理阶段使用少量连续思维向量进行策略级引导，从而提高推理效率。这种方法直接增强了LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型中Chain-of-Thought推理效率低下的问题。针对推理任务场景，我们提出了一种Latent Codebooks for Fast Thinking框架，通过学习离散策略先验码本并在推理时使用少量连续思考向量引导单次推理，并在数学推理和编程基准测试上通过准确性和生成token数量验证了其有效性。",
                    "summary_translation": "面向推理的大型语言模型 (Reasoning-oriented Large Language Models, LLMs) 通常依赖于逐步生成显式标记，其有效性通常取决于大规模监督微调 (supervised fine-tuning) 或强化学习 (reinforcement learning)。尽管思维链 (Chain-of-Thought, CoT) 技术显著提升了复杂推理任务的性能，但它们仍然效率低下，需要长的推理轨迹，这增加了延迟和标记使用量。在这项工作中，我们介绍了快速思考的潜在码本 (Latent Codebooks for Fast Thinking)，这是一个仅在训练期间使用简洁的思维链草图来学习离散策略先验码本的框架。在推理时，模型基于从码本中蒸馏出的少量连续思考向量（单次通过），实现策略级指导，而无需生成显式推理标记。为了补充这一设计，我们提出了增益路由器 (GainRouter)，一种轻量级路由机制，它能在快速码本引导推理和慢速显式推理之间自适应切换，从而抑制过度思考并减少不必要的标记生成。跨多个推理基准的实验表明，我们的方法在大幅降低推理成本的同时，实现了具有竞争力或更优的准确性，为大型语言模型中的高效且可控推理提供了一条实用路径。",
                    "inspiration_trace": "# 从问题到解决方案：LC-FT方法的逻辑演进\n\n## 1. 宏观问题：LLM推理的效率困境\n\n作者首先观察到大型语言模型(LLMs)在复杂推理任务中面临的核心矛盾：\n- **推理效率与质量的权衡**：Chain-of-Thought (CoT)技术显著提升了复杂推理性能，但需要生成冗长的推理轨迹，导致高延迟和大量token消耗\n- **过度思考问题**：在简单实例上，扩展推理可能引入不必要步骤，甚至增加错误率\n- **训练成本障碍**：强大的多步推理通常依赖过程监督或强化学习优化，增加了大规模部署的复杂性\n\n这一核心问题构成了研究的出发点：**如何在保持推理质量的同时，大幅提升LLM的推理效率？**\n\n## 2. 核心洞察：人类快速思维的启发\n\n作者从人类认知过程中获得关键灵感：\n- **人类问题解决模式**：许多看似复杂的问题可通过快速回忆和应用先验解决方案模式来解决，无需详尽推导\n- **\"识别-行动\"机制**：人类专家能快速\"识别情况\"并提前承诺高概率解决方案\n\n这引导作者提出核心假设：**LLM能否内化一套紧凑的、可重用的推理策略先验，并在推理时仅获取少量针对性指导，从而实现快速思考？**\n\n## 3. 方法假设：从显式到隐式的推理转变\n\n基于上述洞察，作者形成三个关键假设：\n1. **码本假设**：可以通过学习离散策略先验的码本，将冗长CoT压缩为紧凑表示\n2. **隐式推理假设**：推理时仅需少量连续思考向量指导解码，无需生成显式推理token\n3. **自适应路由假设**：可设计轻量级机制，在快速码本引导和慢速显式推理间智能切换\n\n这些假设指向一个统一目标：**将显式的、token密集的\"慢思考\"转换为隐式的、向量驱动的\"快思考\"**\n\n## 4. 方法设计：LC-FT框架的构建\n\n### 4.1 潜在码本快速思考(LC-FT)\n\n作者设计了一个两阶段学习框架：\n\n**阶段1：简洁CoT数据构建与码本学习**\n- 创建仅用于训练信号的简洁教师推理数据集\n- 设计可学习码本C ∈ R^(M×H)，存储M个原型推理策略\n- 通过注意力机制从码本提取K个思考令牌向量T\n\n**阶段2：思考令牌注入与模型微调**\n- 在Transformer第L层注入思考令牌，在此之前掩码，之后可见\n- 使用LoRA适配器从注入层开始进行参数高效微调\n- 通过对齐损失和标准监督损失优化模型\n\n**创新点**：将冗长文本CoT替换为紧凑抽象策略先验，推理时不增加输出长度\n\n### 4.2 GainRouter：智能路由机制\n\n为解决\"何时快思考、何时慢思考\"的问题，作者设计：\n\n**特征提取与决策机制**\n- 提取问题向量和思考令牌向量，通过注意力聚合证据\n- 结合问题特征和码本不确定性信号计算自适应阈值\n- 默认使用快速模式，仅在预测失败风险高时切换到CoT推理\n\n**创新点**：抑制过度思考，减少不必要token生成，提供可控的准确率-成本权衡\n\n## 5. 实验验证：效率与质量的平衡\n\n作者在数学推理(AIME、OlympiadBench)和编程任务(MBPP、HumanEval)上验证方法：\n\n**关键发现**：\n- LC-FT在保持竞争力的准确率同时，显著降低推理成本\n- GainRouter匹配Qwen3-Thinking准确率，但减少33-65%的token使用\n- 码本激活模式呈现稀疏性和任务特异性，证实学习到可重用策略原型\n- 消融实验验证码本层、精炼器和LoRA适配的各自贡献\n\n**核心结论**：LC-FT实现了\"快速思考优先\"的推理范式，在保持高质量推理的同时大幅提升效率\n\n## 6. 逻辑演进总结\n\n作者的方法论形成了一条清晰的逻辑链：\n\n**问题识别** → CoT推理效率低下、过度思考、训练成本高  \n**核心洞察** → 人类快速思维启发：通过先验模式快速解决问题  \n**假设形成** → 可学习离散策略码本，用少量连续向量指导解码  \n**方法设计** → LC-FT框架(码本学习、思考令牌注入) + GainRouter(自适应路由)  \n**实验验证** → 在数学推理和编程任务上验证效率与有效性  \n**最终贡献** → 为LLM提供高效可控推理的实用路径\n\n这一逻辑演进展示了作者如何从实际问题出发，通过跨领域类比形成创新假设，设计出系统解决方案，并通过严谨实验验证其有效性，最终为LLM推理效率问题提供了一个兼具理论创新和实用价值的解决路径。"
                },
                {
                    "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales",
                    "arxiv_id": "2509.23574",
                    "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang",
                    "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election \\textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进语言模型的基础推理能力，它聚焦于思维链(CoT)蒸馏技术，提出了一种新的训练范式MoRSD（模型导向的推理选择蒸馏），旨在通过选择高质量的推理过程来增强小型语言模型的推理能力。这直接符合我们对提高LLM通用推理能力的研究目标。 其次，论文包含多个正面指标：核心概念涉及语言模型(SLMs和大型教师模型)，能力方向明确关注推理(reasoning)特别是多步推理能力，这些都是我们研究范围的核心要素。 第三，论文不涉及任何排除标准领域，它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种高效的CoT蒸馏方法，通过控制推理过程的准确性、多样性和难度，使用更少的高质量推理就能显著提升小型模型的推理能力。这种方法论研究直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域，因此完全符合我们的研究目标。",
                    "summary2": "本文旨在 [解决CoT蒸馏中忽视推理质量而过度关注数量的问题]。针对 [小型语言模型需要从大型教师模型获取推理能力的场景]，我们提出了一种 [MoRSD方法，通过自引导推理选择器选择高质量推理过程]，并在 [七个数据集覆盖三个不同任务] 上通过 [准确率] 验证了其有效性。",
                    "summary_translation": "思维链(Chain-of-thought, CoT)蒸馏旨在通过从更大的教师模型(teacher models)转移多步推理能力，来增强小型语言模型(small language models, SLMs)的推理能力。然而，现有工作低估了基本原理(rationale)质量，主要关注数据数量，这可能会将噪声或错误信息传递给学生模型(student model)。为解决上述问题，我们提出了面向模型的基本原理选择蒸馏(Model-Oriented Rationale Selection Distillation, MoRSD)，该方法能够识别并选择高质量的基本原理进行蒸馏，以进一步提高性能。我们进一步提出了基本原理难度(Rationale Difficulty, RD)指标，用于衡量学生模型在给定基本原理下生成正确答案的能力。与基线相比，我们通过控制基本原理的准确性、多样性和难度，在三个任务的七个数据集上实现了4.6%的平均提升，同时使用了更少的基本原理。我们的结果表明，一小部分高质量的基本原理比整个数据集更能增强学生模型的推理能力。我们的方法有望成为高效CoT蒸馏的一种可能解决方案。我们的代码将在https://github.com/Leon221220/MoRSD上发布。",
                    "inspiration_trace": "# 从问题到方法：MoRSD的逻辑演进链\n\n## 一、宏观问题：推理能力迁移的效率挑战\n\n**出发点**：大型语言模型(LLMs)通过思维链(CoT)展现出强大的推理能力，但其巨大规模限制了实际部署。小型语言模型(SLMs)计算效率高，却缺乏复杂推理能力。因此，核心问题是：**如何高效地将LLMs的推理能力迁移到SLMs上？**\n\n## 二、问题聚焦：CoT蒸馏的质量困境\n\n作者观察到现有CoT蒸馏方法存在三大局限：\n\n1. **数量导向偏见**：现有方法主要关注增加推理链数量(从1增加到8个)，而忽视质量差异\n2. **噪声传递风险**：不加选择地使用所有生成的推理链，可能将错误或噪声信息传递给学生模型\n3. **模型特异性缺失**：大多数方法忽略学生模型的特定能力和限制，采用\"一刀切\"的蒸馏策略\n\n这引导作者聚焦到更具体的问题：**如何选择一小部分高质量的、针对特定学生模型的推理链，以实现高效有效的蒸馏？**\n\n## 三、核心假设：质量优于数量的理论基石\n\n基于上述观察，作者形成四个关键假设：\n\n1. **质量假设**：一小部分精心挑选的高质量推理链可能比整个数据集更能增强学生模型的推理能力\n2. **特异性假设**：推理链的质量评估应考虑学生模型的特定能力，不同学生模型可能需要不同的推理链\n3. **多维质量假设**：推理链质量是多维概念，包括准确性、多样性和难度\n4. **难度假设**：推理链对学生模型的难度是衡量其对蒸馏贡献的重要指标\n\n## 四、方法设计：MoRSD的四阶段框架\n\n基于这些假设，作者设计Model-Oriented Rationale Selection Distillation (MoRSD)方法，包含四个逻辑相连的阶段：\n\n### 1. 推理链生成：获取原始数据\n- 提示教师模型生成多个推理链\n- 使用固定模板确保格式一致性\n- 构建完整数据集D<sub>full</sub>\n\n### 2. 自我评估：量化推理链价值\n- **创新点**：提出推理链难度(RD)指标\n- **核心思想**：RD衡量学生在给定推理链下生成正确答案的能力\n- **计算方法**：RD = PPL(答案|推理链,问题) / PPL(答案|问题)\n- **逻辑**：RD值越低，表明该推理链对学生理解问题更有帮助\n\n### 3. 推理链选择：三维质量过滤\n作者设计了一个三级过滤流程，体现了多维质量假设：\n\n- **准确性选择**：过滤数据集，确保达到预设准确率阈值δ\n- **多样性选择**：使用N-gram和Jaccard相似度消除冗余推理链\n- **难度选择**：选择RD值最低的k个推理链，体现模型特异性假设\n\n### 4. 蒸馏：高效知识迁移\n- 使用筛选后的高质量推理链数据集D<sub>selected</sub>微调学生模型\n- 目标函数：最小化学生模型在生成正确答案和推理链上的损失\n\n## 五、实验验证：假设检验与效果评估\n\n作者通过多角度实验验证了方法的有效性：\n\n### 1. 整体性能验证\n- 在7个数据集上平均提高4.6%准确率\n- 使用更少推理链实现更好性能，验证了质量假设\n\n### 2. 各维度影响分析\n- **准确性影响**：性能随数据集准确率提高而提高，但超过阈值后收益平稳\n- **多样性影响**：性能随推理链多样性增加而提高\n- **难度影响**：RD值越低的推理链，蒸馏效果越好\n\n### 3. 消融研究\n移除任何选择组件(准确性、多样性或难度选择)都会导致性能显著下降，验证了多维质量假设和各组件的必要性。\n\n### 4. 推理链质量分析\n使用ChatGPT作为裁判评估推理链质量，结果显示低RD推理链质量显著高于高RD推理链，验证了难度假设和RD指标的有效性。\n\n## 六、结论与贡献：从理论到实践\n\n通过上述逻辑链条，作者得出结论：MoRSD通过自主选择高质量的推理链，实现了用更少的数据获得更好的性能，为高效CoT蒸馏提供了新思路。\n\n主要贡献体现为：\n1. 提出了一种简单有效的方法，证明\"少即是多\"在CoT蒸馏中的适用性\n2. 提出了模型特定的RD指标，使学生模型能根据自身需求定制数据\n3. 通过广泛实验验证了方法的有效性和各组件的重要性\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过观察、假设形成、方法设计、实验验证到结论的系统性思考过程，体现了科研工作的严谨性和创新性。"
                },
                {
                    "title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank",
                    "arxiv_id": "2509.23595",
                    "authors": "Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong",
                    "summary": "Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Timber\"的无训练方法，通过微调权重差异来部分地将Instruct模型恢复到Base模型的状态，以增强Instruct模型的探索能力(exploration capability)同时保持其利用能力(exploitation)。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，这篇论文是关于改进LLM基础能力的研究，属于后训练优化范畴，而不是将LLM作为工具应用到特定领域。论文提出的Timber方法直接针对LLM的内在能力进行改进，符合\"改进LLM的基础能力\"这一核心判断标准。 其次，论文明确涉及大语言模型（Llama和Qwen系列）这一核心概念，并通过增强模型的探索能力来提升其整体性能。虽然论文没有直接提及reasoning、planning或problem-solving等具体能力方向，但探索能力的提升往往与模型的推理能力和问题解决能力密切相关，特别是在处理复杂任务时。 第三，论文不涉及任何排除标准中提到的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，虽然论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，但其提出的无训练方法为改进LLM的通用能力提供了新的视角，这与研究目标\"提高大语言模型（LLM）本身的通用推理能力\"是一致的。论文中提到的Pass@k性能提升也表明该方法可能对模型的推理能力有积极影响。 综上所述，这篇论文符合研究范围，应该被保留。",
                    "summary2": "本文旨在解决大型语言模型后训练过程中探索能力受限的问题。针对Instruct模型与Base模型之间的权重差异，我们提出了一种基于有效秩(effective rank)的无训练精炼方法Timber，通过部分恢复Instruct模型至Base状态来增强探索能力。在Llama和Qwen系列模型上，通过Pass@k等指标验证了Timber能显著提升模型性能，特别是在多采样场景下。",
                    "summary_translation": "后训练（Post-training），即将预训练的基础模型（Base model）转化为相应的指令模型（Instruct model）的过程，被广泛认为是表面的。在这项工作中，我们首先通过提供权重层面的新颖定量证据来强化这一假设，即有效秩（effective rank, eRank）的变化微乎其微。然而，这种表面性也存在一个关键的权衡，即以限制探索能力（exploration）为代价来提高利用能力（exploitation）。为解决这一问题，我们提出了Timber，一种简单而有效的免训练方法，能够在保持利用能力的同时增强指令模型的探索能力。其关键洞见是通过微妙而有针对性的权重增量（weight deltas）细化，将指令模型部分恢复至配对的基础模型。在Llama和Qwen系列上的广泛实验表明，Timber持续改进了原始指令模型，尤其在Pass@k性能方面表现突出。我们的发现为权重层面的后训练阶段提供了新的见解，并提出了无需训练即可优化指令模型的实用策略。",
                    "inspiration_trace": "# Timber方法逻辑链分析：从观察到方法论\n\n## 1. 宏观问题：大型语言模型训练的局限性\n\n作者从大型语言模型(LLMs)训练的基本流程出发，识别出一个核心问题：LLMs通常采用两阶段训练流程（预训练→后训练），但后训练阶段存在根本性局限。这一阶段旨在将Base模型转化为具有指令遵循能力的Instruct模型，但作者质疑这一过程的深度和效果。\n\n## 2. 观察现象：后训练的\"表面性\"\n\n通过深入研究，作者发现后训练过程是\"表面的\"(superficial)——它仅利用预训练期间Base模型已获得的知识模式，而不引入新的表征能力。为提供定量证据，作者引入有效秩(effective rank, eRank)作为分析工具：\n\n- **eRank定义**：衡量权重矩阵奇异值分布的均匀性，反映矩阵的有效维度\n- **关键发现**：Base和Instruct模型对应层的eRank几乎完全相同（图1）\n- **结论**：后训练对模型的有效维度影响微乎其微，从权重层面证实了其表面性\n\n## 3. 问题聚焦：利用与探索的权衡\n\n基于上述观察，作者进一步识别出一个关键权衡问题：\n\n- **利用能力(Exploitation)增强**：Instruct模型在Pass@1指标上表现优异\n- **探索能力(Exploration)受限**：但在Pass@k（k较大时）指标上表现不如Base模型\n- **根本原因**：表面性后训练过度优化高奖励路径，抑制了采样空间的多样性\n\n这一权衡成为作者要解决的核心问题：如何在不损害利用能力的前提下增强探索能力？\n\n## 4. 解决思路：部分恢复Base状态\n\n作者提出一个创新思路：利用Base模型增强Instruct模型。这一思路基于以下洞察：\n\n- Base模型包含几乎所有知识，Instruct模型仅激发部分高奖励思维模式\n- 后训练的表面性表明，Base和Instruct模型在权重层面高度相似\n- 模型合并相关研究表明，无需训练的权重整合策略是可行的\n\n因此，解决方案是将Instruct模型部分恢复到其Base状态，以重新获得部分探索能力。\n\n## 5. 方法形成：基于eRank的权重增量细化\n\n为实现上述思路，作者设计了Timber方法。关键挑战是如何精确地\"部分恢复\"：\n\n- **初步尝试**：简单线性缩放权重增量效果不佳，因后训练修改具有脆弱性\n- **关键洞察**：eRank可作为自适应阈值，有效分离奇异值谱的主要成分（图3）\n- **方法设计**：\n  1. 计算权重增量：W∆ = WI - WB\n  2. SVD分解：SVD(W∆) → UΣV^T\n  3. 基于eRank设置阈值：K := ⌈eRank(W∆)⌉\n  4. 细化奇异值：\n     - Timber-L：移除尾部奇异值（置零）\n     - Timber：衰减尾部奇异值（缩放因子λ）\n  5. 构建细化权重：W+I = WB + U refine(Σ)V^T\n\n## 6. 实验验证：方法的有效性与鲁棒性\n\n作者通过广泛实验验证Timber方法：\n\n- **测试范围**：Llama和Qwen系列，模型规模从0.6B到30B\n- **评估指标**：多个主流基准测试，特别关注Pass@k指标\n- **关键结果**：\n  - Timber一致地优于原始Instruct模型（表2）\n  - Pass@k性能显著提升，证明探索能力增强（图5）\n  - 对衰减因子λ具有鲁棒性（图4）\n\n这一完整的逻辑链从宏观问题出发，通过定量观察识别本质问题，聚焦核心权衡，提出创新解决思路，形成具体方法，并通过实验验证有效性，展现了清晰的科学研究思维过程。"
                },
                {
                    "title": "The Impact of Role Design in In-Context Learning for Large Language Models",
                    "arxiv_id": "2509.23501",
                    "authors": "Hamidreza Rouzegar, Masoud Makrehchi",
                    "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究提示工程中的角色设计如何影响大语言模型在上下文学习(ICL)中的表现。论文不是将LLM作为工具应用到特定领域，而是研究如何通过改进提示设计来提升LLM的基础能力。论文特别评估了角色设计对模型在数学推理等任务上的影响，这直接关联到提升LLM的通用推理能力。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念: 明确研究Large Language Models (LLMs)，包括GPT-3.5、GPT-4o、Llama2等模型 - 能力方向: 特别提到了math reasoning（数学推理），这正是通用推理能力的核心组成部分 虽然论文没有涉及reinforcement learning、llm-based agents等新兴范式，但它包含了足够的核心正面指标。 第三步：排除标准 论文不聚焦于任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），虽然评估了多种任务，但这些是通用任务而非特定领域应用 - 不讨论模型可靠性方面的watermarking、safety等问题 第四步：特殊和模糊情况 论文研究的是通过角色设计这种提示工程方法来增强LLM的通用能力，特别是推理能力。这与\"智能体/工具使用\"的情况类似，都是提出一种通用方法来增强LLM的问题解决能力，因此应该保留。 综合判断：这篇论文的核心贡献是研究角色设计如何提升LLM在多种任务（包括数学推理）上的表现，这直接关联到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True。",
                    "summary2": "本文旨在研究角色设计在大型语言模型上下文学习中的影响。针对多种NLP任务和数学推理问题，我们提出了五种不同的角色配置提示设计，并在五个数据集上通过F1分数和结构准确性验证了其有效性。实验表明，包含系统、用户和助手角色的FewSUA配置在大多数NLP任务中表现最佳，而数学推理任务则受益于允许解释的提示设计。",
                    "summary_translation": "上下文学习(In-context learning, ICL)使大型语言模型(Large Language Models, LLMs)能够基于提示(prompts)生成预测，而无需额外的微调(fine-tuning)。尽管提示工程(prompt engineering)已被广泛研究，但提示中角色设计(role design)的影响仍未得到充分探索。本研究使用来自OpenAI的GPT-3.5和GPT-4o以及来自Meta的Llama2-7b和Llama2-13b，考察了角色配置(role configurations)在零样本(zero-shot)和少样本(few-shot)学习场景中的影响。我们评估了模型在多个数据集上的表现，重点关注情感分析(sentiment analysis)、文本分类(text classification)、问答(question answering)和数学推理(math reasoning)等任务。我们的研究结果表明，基于角色的提示结构(role-based prompt structuring)有潜力提升大型语言模型(LLM)的性能。",
                    "inspiration_trace": "# 角色设计在大型语言模型上下文学习中的影响：逻辑链推演\n\n## 1. 宏观问题：提示工程的盲区\n\n**起点观察**：大型语言模型(LLMs)通过上下文学习(ICL)展现出强大能力，无需额外微调即可适应多样化任务。然而，现有研究主要集中在提示内容选择和结构优化上，忽视了一个关键维度——角色设计。\n\n**问题识别**：作者发现，尽管提示工程已被广泛研究，但提示中角色分配(系统、用户、助手)对模型行为的影响机制尚未被系统探索。这构成了研究领域的明显空白。\n\n## 2. 中观聚焦：角色设计的潜在价值\n\n**假设形成**：基于初步观察，作者提出核心假设——明确的角色区分可能会显著影响LLMs在ICL中的表现，且这种影响可能因任务类型和模型架构而异。\n\n**理论支撑**：作者引用相关研究表明，角色扮演提示能增强LLMs的推理能力，但这些研究主要局限于零样本设置，缺乏对不同学习范式的系统比较。\n\n## 3. 微观实验：系统性验证\n\n**实验设计**：为验证假设，作者构建了严谨的实验框架：\n- **模型选择**：采用GPT-3.5、GPT-4o、Llama2-7b和Llama2-13b四种代表性LLMs\n- **任务覆盖**：选择情感分析、文本分类、问答和数学推理等多样化任务\n- **提示配置**：设计五种基本角色设计变体，从简单到复杂逐步增加角色区分度\n\n**核心方法演进**：\n1. **基础角色设计**：从ZeroU(无角色区分)到FewSUA(完整系统-用户-助手角色)\n2. **复杂任务适配**：针对数学推理任务，设计Reasoning-First等高级提示策略\n3. **评估维度创新**：引入\"结构准确性\"指标，评估模型对预期提示结构的遵循程度\n\n## 4. 发现与验证：角色设计的差异化效应\n\n**关键发现**：\n- **简单NLP任务**：FewSUA配置表现最佳，表明明确角色区分和示例对提高模型性能至关重要\n- **复杂推理任务**：允许解释的配置(如Reasoning-First)优于严格结构遵循，表明在复杂任务中，鼓励模型表达思维过程更有效\n- **模型差异**：较小模型(如Llama2)在结构遵循方面表现较差，需要更明确的角色设计\n\n**理论修正**：基于实验结果，作者修正了初始假设，形成了更精细的方法论——角色设计效果存在任务和模型依赖性，不存在\"一刀切\"的最优策略。\n\n## 5. 方法论形成：角色设计的指导原则\n\n**核心贡献**：作者提出了关于角色设计在ICL中的系统方法论：\n- **任务适配原则**：简单NLP任务受益于明确角色区分(FewSUA)，复杂推理任务需要更精细设计(Reasoning-First)\n- **模型考量原则**：较小模型需要更明确的角色设计来遵循结构要求\n- **结构-性能权衡原则**：在复杂推理任务中，结构准确性与任务性能可能存在权衡\n\n**实践意义**：为研究人员和从业者提供了针对不同任务和模型优化提示设计的具体指导，填补了提示工程中角色设计研究的空白。\n\n这一逻辑链从宏观问题出发，通过观察、假设形成、实验验证，最终形成了系统性的方法论，展现了严谨的学术思维过程。"
                },
                {
                    "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models",
                    "arxiv_id": "2509.23441",
                    "authors": "Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li",
                    "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit harmful behaviors. Current alignment strategies typically embed safety into model weights, making these controls implicit, static, and difficult to modify. This paper introduces Cognition-of-Thought (CooT), a novel decoding-time framework that equips LLMs with an explicit cognitive self-monitoring loop. CooT couples a standard text Generator with a cognitive Perceiver that continuously monitors the unfolding sequence. The Perceiver uses a structured, precedence-based hierarchy of principles (e.g., safety over obedience) to detect potential misalignments as they arise. When violations are flagged, CooT intervenes by rolling back the generation to the point of error and regenerating under injected guidance that combines universal social priors with context-specific warnings. CooT thus transforms alignment from a fixed property into an explicit, dynamic, and auditable process active during inference, allowing for flexible policy updates without retraining the model. Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出Cognition-of-Thought (CooT)框架，这是一种新的解码时推理框架，旨在增强LLM的认知自我监控能力。论文的核心贡献不是将LLM作为工具应用到特定领域，而是改进LLM本身的基础推理能力，特别是社会对齐推理能力。这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并直接针对推理能力(reasoning)，特别是社会对齐推理(social-aligned reasoning)。虽然不是传统意义上的训练方法，但CooT框架代表了一种新兴的推理范式，通过自我监控循环增强模型的推理过程。 第三，论文不符合排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（如医疗、化学等），虽然涉及安全性，但是从提升模型内在推理质量的角度出发，而非单纯的应用层面研究。 在处理特殊和模糊情况时，虽然论文涉及安全性，但它是通过提出一种新的推理框架(CooT)来增强模型的社会对齐推理能力，从而提升模型的通用推理质量，这符合\"提出新方法来增强模型内在可靠性，从而提升模型的通用推理质量\"的保留标准。 综上所述，这篇论文的核心贡献是提出一种新的推理框架来增强LLM的通用推理能力，特别是社会对齐推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决大型语言模型在推理过程中可能产生有害行为的问题，同时使alignment控制更加显式、动态和可审计。针对LLMs在生成过程中可能出现的潜在风险，我们提出了一种Cognition-of-Thought (CooT)框架，通过Generator和Perceiver的耦合架构实现实时监控和干预，并在AIR-Bench 2024和SocialEval等基准测试上通过安全合规率和社会推理性能指标验证了其有效性。",
                    "summary_translation": "大语言模型（Large language models, LLMs）擅长复杂推理，但仍可能表现出有害行为。当前的对齐策略（alignment strategies）通常将安全性嵌入模型权重中，使这些控制变得隐式、静态且难以修改。本文提出了思维认知（Cognition-of-Thought, CooT），一种新颖的解码时（decoding-time）框架，为大语言模型配备了明确的认知自我监控循环。CooT将一个标准文本生成器（Generator）与一个持续监控展开序列的认知感知器（Perceiver）相结合。感知器（Perceiver）使用一种结构化的、基于优先级的原则层次结构（例如，安全优先于服从）来检测出现的潜在不对齐（misalignments）情况。当检测到违规时，CooT通过将生成回滚到错误点，并在注入的指导下重新生成内容来进行干预，该指导结合了普遍的社会先验（social priors）和特定情境的警告。因此，CooT将对齐从一种固定属性转变为在推理（inference）过程中活跃的显式、动态且可审计的过程，允许在不重新训练模型的情况下灵活更新策略。在多个基准测试（benchmarks）和模型系列上的广泛实验证实，CooT一致性地提高了安全性和社会推理性能。",
                    "inspiration_trace": "# 从问题到方案：CooT方法的逻辑演进\n\n## 宏观问题：大型语言模型的安全与对齐困境\n\n### 观察阶段：当前对齐策略的根本局限\n作者首先观察到大型语言模型(LLMs)的核心矛盾：它们在复杂推理任务上表现出色，却仍可能产生有害行为。深入分析主流对齐策略(如RLHF、DPO等)后发现一个共同问题：**对齐被嵌入为模型权重的隐式属性**，导致三大局限：\n- 隐式性：规范优先级被\"烘焙\"到参数中，推理时不可见\n- 静态性：部署后难以修改或更新\n- 不灵活性：无法适应不同上下文或政策需求\n\n这引出一个关键问题：能否将对齐从静态属性转变为动态过程？\n\n### 借鉴阶段：人类认知的启发\n作者转向人类认知过程寻找解决方案，发现：\n1. 心理学研究表明可靠推理建立在**实时自我监控与调节**能力上(Flavell, 1979)\n2. 道德心理学强调这种调节由**优先级层次结构**引导，如避免伤害优先于服从(Kohlberg, 1963)\n3. 人类日常交流中自然交织语义表达与认知对齐，如说话者意识到可能冒犯他人时会重新表述\n\n这形成核心假设：**如果为LLMs构建显式认知自我监控循环，将对齐转变为动态过程，可实现更有效的安全与社会对齐**。\n\n### 构建阶段：CooT框架的设计\n基于这一假设，作者设计了Cognition-of-Thought(CooT)框架，包含两个核心组件：\n\n#### 1. Generator-Perceiver耦合架构\n- **Generator**：负责语义生成，类似标准语言模型\n- **Perceiver**：作为\"内部批评者\"，持续监控生成序列并评估是否符合规范\n\n这种设计模拟人类认知中的\"内心独白\"机制，实现生成与认知的并行运作。\n\n#### 2. 认知状态系统\n为使Perceiver有效监控，作者构建结构化状态系统：\n- 借鉴阿西莫夫机器人三定律，定义优先级层次：**安全 > 利他主义 > 自我主义**\n- 设计状态向量：y_t = (y_t^(S), y_t^(A), y_t^(E)) ∈ {-1, 0, 1}^3\n- 定义可行状态空间，确保状态向量符合优先级约束\n\n这种设计使Perceiver能检测原则违反及优先级冲突，如当模型为服从指令而可能造成伤害时。\n\n#### 3. 干预机制\n当检测到不对齐时，触发双重干预：\n- **因果回滚**：通过注意力分析识别不安全轨迹起源点，回滚到错误开始前的位置\n- **思想干预**：注入双重指导\n  * 通用社会指导：基于行为、情感和社交技能清单(BESSI)\n  * 上下文相关指导：针对特定风险生成纠正措施\n\n### 验证阶段：实验证明\n作者通过多维度实验验证CooT有效性：\n1. **安全对齐评估**：在AIR-Bench 2024上，CooT平均合规率达0.80，比基础模型提高13%\n2. **社会智能评估**：在SocialEval上，CooT提高亲社会行为9.02%，同时减少反社会行为\n3. **消融研究**：证明每个组件(回滚、指导注入、优先级感知状态)都至关重要\n\n### 贡献与意义\nCooT的核心创新在于将对齐从静态、隐式的模型权重属性转变为动态、可审计、可编辑的推理过程。这种方法不仅提高了安全性和社会推理能力，还使对齐决策变得透明可解释，且无需重新训练即可灵活更新政策，为LLMs的安全对齐提供了全新范式。\n\n这一演进过程展示了从问题观察到灵感借鉴，从假设形成到框架设计，再到实验验证的完整科学思维路径，体现了作者对LLMs对齐问题的深刻洞察与创新解决方案。"
                },
                {
                    "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization",
                    "arxiv_id": "2509.23371",
                    "authors": "Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng",
                    "summary": "Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式MetaAPO（Meta-Weighted Adaptive Preference Optimization），用于优化大语言模型与人类价值观的对齐过程。这属于改进LLM基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，从正面指标看，论文明确包含\"Large language models, LLMs\"这一核心概念，并且讨论的\"preference optimization\"与强化学习方法（如RLHF）密切相关，是提升LLM性能的重要训练范式。虽然论文没有直接提及推理能力，但通过改进模型对齐过程，可能间接提升模型的整体推理和问题解决能力。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的是一种通用的优化框架，旨在解决模型训练中的数据分布不匹配问题，这属于基础模型研究的范畴，与提高LLM通用能力的目标一致。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决大型语言模型对齐中数据生成与偏好优化间的分布不匹配问题。针对预收集离线偏好数据与模型策略间的分布差距，我们提出了一种MetaAPO框架，通过轻量级元学习器动态耦合数据生成与模型训练。在AlpacaEval 2、Arena-Hard和MT-Bench上，通过胜率(WR)、长度控制胜率(LC)和平均多轮得分(Score)等指标验证了其有效性，MetaAPO性能优于现有方法，同时减少42%在线标注成本。",
                    "summary_translation": "偏好优化（Preference optimization）对于使大型语言模型（LLMs）与人类价值观和意图保持一致至关重要。此过程中的一个重大挑战是预先收集的离线偏好数据与不断发展的模型策略之间的分布不匹配（distribution mismatch）。现有方法尝试使用静态启发式方法或解耦的在线采样策略来缩小这一差距，但它们往往无法适应模型的动态学习状态。为了弥合这一差距，我们提出了元加权自适应偏好优化（Meta-Weighted Adaptive Preference Optimization, MetaAPO），这是一个将数据生成与模型训练动态耦合的新框架。MetaAPO采用了一个轻量级元学习器（meta-learner）作为\"对齐差距估计器\"（alignment gap estimator），以评估在线策略采样（on-policy sampling）相对于离线数据的潜在效益。这指导了有针对性的在线生成，并为优化目标分配样本级别的元权重（meta-weights），动态平衡在线和离线数据的质量和分布。在AlpacaEval 2、Arena-Hard和MT-Bench上的实验表明，MetaAPO在各种设置下始终优于现有的偏好优化方法，同时将在线注释成本降低了42%。",
                    "inspiration_trace": "# 从宏观问题到创新方法：MetaAPO的逻辑演进\n\n## 1. 宏观问题：大型语言模型与人类价值观的对齐挑战\n\n### 观察现象：LLM对齐中的关键矛盾\n- **对齐需求**：大型语言模型(LLMs)需要与人类价值观和意图对齐，确保其输出有帮助、诚实且无害\n- **现有方法局限**：强化学习从人类反馈(RLHF)虽有效但计算成本高且训练不稳定；离线偏好优化方法(如DPO、SimPO)简化了流程却面临分布不匹配问题\n- **核心矛盾**：静态的离线偏好数据与动态演化的模型策略之间存在分布差距，导致对齐效果受限\n\n## 2. 问题聚焦：分布不匹配的本质与现有解决方案的不足\n\n### 深入观察：离线与在线数据的权衡困境\n- **离线数据特点**：人类标注的离线数据质量高、多样性丰富，但与当前模型策略存在分布不匹配\n- **在线数据特点**：模型生成的在线数据更符合当前策略分布，但往往缺乏多样性和质量，可能引入噪声\n- **现有方法局限**：依赖静态启发式或解耦的在线采样策略，无法适应模型的动态学习状态\n\n### 关键假设：需要动态耦合机制\n- **假设1**：数据采样与偏好优化过程应紧密耦合，而非独立处理\n- **假设2**：需要一种自适应机制，能根据模型当前状态动态平衡离线与在线数据的贡献\n- **假设3**：元学习方法可能提供解决方案，通过学习\"对齐差距估计\"来指导数据采样和优化\n\n## 3. 方法论形成：MetaAPO框架的构建\n\n### 核心思想1：元加权自适应在线采样\n- **问题**：如何确定哪些离线样本需要在线增强？\n- **解决方案**：\n  1. 使用偏好评分函数评估离线样本与当前模型的对齐程度\n  2. 引入轻量级元学习器将偏好分数映射为权重w∈[0,1]\n  3. 低权重样本(对齐较差)有更高概率被选中进行在线生成\n  4. 通过奖励模型评估生成响应，构建在线偏好对\n\n### 核心思想2：元加权偏好优化\n- **问题**：如何有效结合离线与在线数据进行优化？\n- **解决方案**：\n  1. 设计元加权损失函数：L(θ) = w·ℓ_offline + (1-w)·ℓ_online\n  2. 元权重w动态平衡离线与在线数据的贡献\n  3. 高权重强化从可靠离线数据学习，低权重增加对在线反馈的依赖\n\n### 核心思想3：元学习器作为对齐差距估计器\n- **问题**：如何确保权重分配的合理性？\n- **解决方案**：\n  1. 元学习器与策略模型交替训练\n  2. 最小化元损失：L_meta(ϕ) = -E[h_ϕ(ℓ_off)·ℓ_off + (1-h_ϕ(ℓ_off))·ℓ_on]\n  3. 梯度分析表明：当在线样本优于离线样本时，降低离线权重；反之则提高离线权重\n  4. 提供理论保证：随着元缓冲区增大，学习到的元学习器风险收敛到最优函数风险\n\n## 4. 实验验证与效果确认\n\n### 实验设计\n- **评估基准**：AlpacaEval 2、Arena-Hard和MT-Bench\n- **基础模型**：Llama-3.1-8B和Qwen2.5-7B\n- **对比方法**：离线(DPO、SimPO)、在线(Online DPO、PPO)和混合(ADPO、SELM)方法\n\n### 关键发现\n- **性能优势**：MetaAPO在所有基准测试上显著优于基线方法\n- **效率提升**：减少42%的在线注释成本，整体训练时间减少50%以上\n- **动态行为**：训练过程展现\"探索-整合\"行为，有效识别并解决对齐差距\n- **组件重要性**：消融实验证实元学习器、自适应采样和加权机制都是成功的关键因素\n\n## 5. 逻辑演进总结\n\n从宏观的LLM对齐挑战出发，作者观察到分布不匹配是核心问题，特别是离线数据与动态模型策略之间的差距。通过深入分析离线与在线数据的权衡困境，提出需要一种动态耦合机制的关键假设。基于这些假设，构建了MetaAPO框架，通过元学习器实现数据采样与模型优化的紧密耦合，动态平衡离线与在线数据的贡献。最终，通过理论分析和实验验证，确认了该方法的有效性和效率。\n\n这一逻辑演进体现了从问题观察到假设形成，再到方法论构建和验证的完整科研思维过程，成功解决了LLM对齐中的关键挑战。"
                },
                {
                    "title": "Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces",
                    "arxiv_id": "2509.23291",
                    "authors": "Joseph Marvin Imperial, Harish Tayyar Madabushi",
                    "summary": "Policy compliance assessment is a fundamental task of evaluating whether an input case strictly complies with a set of human-defined rules, more generally known as policies. In practice, human experts follow a systematic, step-by-step process to identify violations with respect to specific stipulations outlined in the policy. However, such documentation of gold-standard, expert-level reasoning processes is costly to acquire. In this paper, we introduce Policy Reasoning Traces (PRT), a form of specialized generated reasoning chains that serve as a reasoning bridge to improve an LLM's policy compliance assessment capabilities. Our empirical evaluations demonstrate that the use of PRTs for both inference-time and training-time scenarios significantly enhances the performance of open-weight and commercial models, setting a new state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also highlight how PRTs can improve an LLM's ability to accurately cite policy clauses, as well as influence compliance decisions through their high utilization from the raw chains of thought.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出\"Policy Reasoning Traces (PRT)\"，一种专门的生成推理链形式，作为推理桥梁来提高LLM的政策合规评估能力。这本质上是一种改进LLM推理能力的新方法，类似于思维链(CoT)的变体，专门针对政策合规任务。论文关注的是提升LLM的基础推理能力，而非将LLM作为工具应用到特定领域，因此符合保留标准。 第二步正面指标：论文明确包含多个正面指标： - 核心概念：研究的是语言模型(LLMs)的政策合规评估能力 - 能力方向：直接涉及推理(reasoning)能力，特别是政策合规评估中的逻辑推理(logical reasoning)和问题解决(problem-solving) - 论文提出的PRT方法可以视为一种增强模型推理能力的新范式 第三步排除标准：虽然论文提到了HIPAA和GDPR政策，可能看似涉及特定应用领域，但论文的核心不是解决医疗或数据保护领域的具体问题，而是提高LLM在政策合规评估这一通用推理任务上的能力。政策合规评估本身是一种通用推理任务，类似于逻辑推理，因此不应被排除。 第四步特殊和模糊情况：论文提出的PRT方法能提高LLM\"准确引用政策条款\"的能力，这与增强模型内在的可解释性和推理质量相关，符合保留标准。 综合判断：这篇论文的核心贡献是提出一种新的推理方法(PRT)来增强LLM的通用推理能力，特别是在政策合规评估任务上。虽然论文使用HIPAA和GDPR作为评估案例，但其研究目标是提升LLM的通用推理能力，而非专注于特定应用领域。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在提高大型语言模型在政策合规性评估任务中的表现。针对复杂的政策解读场景，我们提出了一种Policy Reasoning Traces (PRT)方法，作为连接政策特定约束与合规判断的推理桥梁。在HIPAA、GDPR和ModelSpec三个多领域政策数据集上，通过准确率指标验证了PRT在推理时和训练时的有效性，显著提升了开放权重和商业模型的性能，并为HIPAA和GDPR设立了新的最先进水平。",
                    "summary_translation": "政策合规评估（Policy compliance assessment）是一项基础任务，用于评估一个输入案例是否严格遵守一套人类定义的规则，这些规则通常被称为政策（policies）。在实践中，人类专家遵循一个系统的、循序渐进的过程，以识别与政策中概述的具体规定相关的违规行为。然而，获取这种黄金标准（gold-standard）的专家级推理过程文档成本高昂。在本文中，我们介绍了政策推理轨迹（Policy Reasoning Traces, PRT），这是一种专门的生成推理链形式，作为推理桥梁来提高大型语言模型（LLM）的政策合规评估能力。我们的实证评估表明，在推理时间（inference-time）和训练时间（training-time）场景中使用PRT显著增强了开源权重（open-weight）和商业模型的性能，为HIPAA（健康保险可携性和责任法案）和GDPR（通用数据保护条例）政策设定了新的最先进水平（state-of-the-art）。除了准确性提升外，我们还强调了PRT如何提高大型语言模型准确引用政策条款的能力，以及通过高度利用原始思维链（raw chains of thought）来影响合规决策。",
                    "inspiration_trace": "# 政策推理追踪(PRT)方法的逻辑演进分析\n\n## 1. 宏观问题：政策合规性评估的自动化挑战\n\n**观察起点**：在医疗、法律、安全等高风险领域，经常需要评估特定案例是否符合既定政策（如法规、规则）。例如，法院需审查医疗交易是否符合数据隐私法规(GDPR)，企业需确保操作符合行业规范等。\n\n**核心问题**：如何使大型语言模型(LLM)能像人类专家一样，系统化地评估案例是否符合复杂政策，并给出准确判断？\n\n## 2. 深入分析：人类专家的推理过程\n\n**关键观察**：人类专家进行政策合规评估时，遵循系统化、逐步的推理过程：\n- 识别案例中的关键要素\n- 将其与政策中的具体条款对应\n- 分析是否符合/违反规定\n- 基于分析得出结论\n\n**核心障碍**：获取这种专家级推理过程的详细记录成本高昂且耗时，难以大规模获取作为训练数据。\n\n## 3. 灵感来源：法律领域的先例推理\n\n**领域洞察**：在法律领域，法院经常参考先例或先前案例的推理过程，识别适用的法律条款，然后作出裁决。这种基于案例的推理方式为解决复杂政策合规问题提供了思路。\n\n**核心假设**：如果能为LLM提供类似专家推理过程的示例，作为\"推理桥梁\"，可能显著提升其政策合规评估能力。\n\n## 4. 创新突破：利用LLM生成伪专家推理\n\n**关键思考**：既然真实专家推理过程难以获取，是否可以利用现有的高级推理LLM来生成\"伪专家\"推理过程？\n\n**方法构想**：使用前沿推理模型(如DeepSeek-R1)，基于已有的案例-裁决对，生成详细的推理过程，模拟专家的思考路径。\n\n## 5. 方法形成：政策推理追踪(PRT)\n\n**概念定义**：PRT是一种专门的生成推理链，作为连接政策具体约束与合规判断的\"推理桥梁\"。\n\n**生成方式**：\n```\nPRT_i = M_E(c_i, P, v_i)\n```\n其中M_E是专家推理模型，c_i是案例，P是政策，v_i是黄金标准裁决。\n\n**应用方式**：\n- **推理时**：作为上下文学习中的少样本示例\n- **训练时**：作为监督微调的编译数据集\n\n## 6. 验证设计：多维度评估框架\n\n**验证假设**：如果PRT有效，应能：\n1. 提高多种模型在多种政策上的评估准确性\n2. 增强模型准确引用政策条款的能力\n3. 展示跨政策泛化能力\n4. 被模型实际用作推理过程的一部分\n\n**实验设计**：\n- **政策选择**：HIPAA(医疗)、GDPR(数据隐私)、ModelSpec(模型安全)\n- **模型测试**：覆盖开源和商业模型\n- **评估方法**：标准提示、少样本学习、自我反馈、添加PRT等对比\n\n## 7. 实证发现：效果验证与优化\n\n**核心发现**：\n1. PRT显著提升模型性能，HIPAA上提升50-100%\n2. 在GDPR上建立新的最先进水平\n3. 提高政策条款引用准确性(+2-13.7%)\n4. 展示跨政策泛化能力\n5. 模型确实将PRT用作推理桥梁(利用率>80%)\n\n**方法优化**：比较通用专家模型(DeepSeek-R1)与领域专家模型(SAUL LM-54B)生成的PRT，发现通用模型通常表现更好。\n\n## 8. 反思与局限：方法的边界认知\n\n**局限认识**：\n1. PRT不是完美监督，可能包含不一致性和幻觉\n2. 对已针对特定政策优化的模型(如OpenAI模型在ModelSpec上)收益有限\n3. 增加上下文长度，需要更多推理预算\n\n**未来方向**：探索使用偏好调优优化更高质量的PRT，但可能需要领域专家的广泛注释工作。\n\n## 逻辑演进总结\n\n从\"政策合规评估自动化\"这一宏观问题出发，通过分析人类专家推理过程的关键作用，识别出缺乏专家推理数据的核心障碍，进而创新性地提出利用LLM自身生成伪专家推理的解决方案，最终形成PRT方法并通过多维度实验验证其有效性。整个逻辑链条体现了从问题观察到假设形成、方法设计、实验验证的完整科研思维过程。"
                },
                {
                    "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning",
                    "arxiv_id": "2509.23330",
                    "authors": "Peng Yu, Zeyuan Zhao, Shao Zhang, Luoyi Fu, Xinbing Wang, Ying Wen",
                    "summary": "Large language models (LLMs) have achieved significant advancements in reasoning capabilities through reinforcement learning (RL) via environmental exploration. As the intrinsic properties of the environment determine the abilities that LLMs can learn, the environment plays a important role in the RL finetuning process. An ideal LLM reasoning environment should possess three core characteristics: scalability, generalizable reasoning, and verifiability. However, existing mathematical and coding environments are difficult to scale due to heavy reliance on expert annotation, while the skills learned in game-based environments are too specialized to generalize. To bridge this gap, we introduce the \\textbf{S}tructured \\textbf{I}n-context \\textbf{E}nvironment (SIE) framework. SIE achieves scalability by automatically constructing reasoning environments from large-scale structured data, where the rich compositional patterns naturally support generalizable reasoning. Moreover, the explicit schemas and reasoning chains in structured data provide a foundation for rule-based verifiability. Experimental results show that SIE framework not only achieves substantial improvements in in-domain structured reasoning, but also enables the learned compositional reasoning skills to generalize effectively to out-of-domain mathematical and logical reasoning tasks. We further explored learning in information-limited partial SIEs and found that LLMs can infer the missing information through exploring the environment, leading to robust reasoning improvements and generalization performance.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。以下是我的详细判断过程： 第一步：核心判断——论文的核心是关于改进LLM的推理能力。论文提出了SIE框架，这是一种新的训练/微调范式，通过强化学习和环境设计来增强LLM的推理能力。论文明确关注的是提升模型的逻辑推理和数学推理等通用能力，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个关键正面指标： - 核心概念：明确提及\"Large language models (LLMs)\" - 能力方向：多次强调\"reasoning capabilities\"、\"generalizable reasoning\"、\"mathematical and logical reasoning\" - 训练方法：核心方法为\"reinforcement learning (RL)\"，通过环境探索进行微调 - 新兴范式：虽然未直接提及智能体，但\"environmental exploration\"概念与智能体交互环境的思想一致 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未专注于医疗、化学、生物等特定应用领域 - 未主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理——论文中的环境探索概念是为了提升通用推理能力，而非应用于特定领域，符合保留条件。 核心贡献分析：论文提出的SIE框架通过结构化上下文环境和强化学习来提升LLM的推理能力，使模型能够学习可泛化的组合推理技能，并有效应用于数学和逻辑推理任务。这一贡献直接针对提升LLM的通用推理能力，完全符合研究目标。论文不仅关注了推理能力的提升，还通过实验验证了这些能力的泛化性，这正是\"通用推理能力\"研究的核心。",
                    "summary2": "本文旨在解决LLM推理环境中可扩展性、可泛化性和可验证性的挑战。针对结构化数据场景，我们提出了一种结构化上下文环境(SIE)框架，通过从大规模知识图谱自动构建可扩展推理环境，并在WebQSP、CWQ和GrailQA等数据集上通过pass@1指标验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）通过环境探索的强化学习（Reinforcement Learning, RL）在推理能力方面取得了显著进展。由于环境的内在特性决定了LLMs可以学习的能力，环境在RL微调过程中扮演着重要角色。一个理想的LLM推理环境应具备三个核心特征：可扩展性（scalability）、可泛化推理（generalizable reasoning）和可验证性（verifiability）。然而，现有的数学和编码环境由于严重依赖专家标注而难以扩展，而在基于游戏的环境中学习的技能过于专业化而难以泛化。为了弥补这一差距，我们提出了结构化上下文环境（Structured In-context Environment, SIE）框架。SIE通过从大规模结构化数据中自动构建推理环境来实现可扩展性，其中丰富的组合模式自然支持可泛化推理。此外，结构化数据中的显式模式和推理链为基于规则的可验证性提供了基础。实验结果表明，SIE框架不仅在域内结构化推理方面实现了显著改进，还使学习到的组合推理技能能够有效泛化到域外数学和逻辑推理任务。我们进一步探索了在信息受限的部分SIE中的学习，发现LLMs可以通过探索环境来推断缺失信息，从而带来稳健的推理改进和泛化性能。",
                    "inspiration_trace": "# 从问题到方法：SIE框架的逻辑演进\n\n## 宏观问题：如何有效提升LLM的推理能力\n\n作者首先观察到，通过强化学习(RL)进行环境探索已成为提升大语言模型(LLM)推理能力的主要范式。然而，现有研究主要聚焦于算法优化，忽视了训练环境的关键作用。这引发了一个核心问题：**环境的内在特性如何决定LLM能够学习哪些能力？**\n\n## 理想环境的特征定义\n\n通过分析现有环境的局限性，作者提出了理想LLM推理环境应具备的三个核心特征：\n\n1. **可扩展性**：能以自动化、低成本方式从大规模数据源构建高质量训练环境\n2. **可泛化推理**：学到的推理策略能转移到其他通用推理领域\n3. **可验证性**：具备清晰、客观的规则来验证答案正确性\n\n## 现有环境的局限性分析\n\n作者识别出两类主要环境及其缺陷：\n\n- **内部化规则环境**（如数学）：依赖昂贵专家标注，难以扩展\n- **外部化规则环境**（如游戏）：技能过于专业化，泛化能力差\n\n这引出了一个关键问题：**如何构建同时满足可扩展性、可泛化性和可验证性的推理环境？**\n\n## 结构化数据的潜力发现\n\n作者观察到结构化数据（如知识图谱）具有独特优势：\n\n1. **丰富资源**：存在大量现实世界结构化数据，支持自动化环境构建\n2. **知识浓缩**：结构化数据是人类经验和领域知识的高度浓缩，推理模式泛化潜力强\n3. **显式约束**：内置模式和约束支持基于规则的验证\n\n这形成了一个核心假设：**从大规模结构化数据自动构建推理环境，可能平衡可扩展性和可泛化性**。\n\n## SIE框架的设计与实现\n\n基于上述假设，作者提出了结构化上下文环境(SIE)框架，其核心创新在于：\n\n1. **环境表示**：将环境动态编码为结构化上下文，作为LLM提示中的软约束\n2. **自动化构建管道**：\n   - 种子子图检索（双向检索提高效率）\n   - 支持子图提取（最短路径算法）\n   - 干扰子图过滤（两阶段语义过滤）\n   - 部分SIE构建（控制信息比例模拟不同难度）\n3. **RL训练机制**：使用GRPO算法，结合答案奖励和格式奖励指导模型学习组合推理策略\n\n## 实验验证与发现\n\n作者通过系统实验验证了SIE框架的有效性：\n\n1. **结构化推理提升**：在WebQSP、CWQ等任务上，RL w/ SIE比无上下文RL平均提升50%以上\n2. **训练效率优势**：相比传统监督微调(SFT w/ SRD)，SIE中的RL训练效率高出40%以上\n3. **跨域泛化能力**：学到的推理策略成功泛化到数学(GSM8K、MATH500)和逻辑(KK-easy、KK-hard)任务\n4. **信息受限鲁棒性**：即使在信息不完整的部分SIE中，模型仍能通过环境探索推断缺失信息\n\n## 逻辑演进总结\n\n从\"如何提升LLM推理能力\"的宏观问题出发，作者通过识别理想环境的特征、分析现有方法的局限、发现结构化数据的潜力，逐步形成了SIE框架的核心思想。这一框架成功解决了环境构建的可扩展性、推理能力的可泛化性和结果验证的可靠性三大挑战，为提升LLM推理能力提供了新范式。整个逻辑链条体现了从问题观察到假设形成，再到方法设计和实验验证的完整科研思维过程。"
                },
                {
                    "title": "PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness",
                    "arxiv_id": "2509.23206",
                    "authors": "Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, pengxin, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen",
                    "summary": "Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：论文的核心是关于改进LLM在多轮对话中调用函数的能力，提出了PARL-MT框架，通过引入进度感知(progress awareness)来增强模型的规划和多步推理能力。这属于提高LLM本身的通用推理能力的研究，特别是规划和多步推理方面，因此应该保留。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确讨论了大型语言模型(LLMs) - 能力方向：强调了\"progress awareness\"（进度感知），涉及总结过去交互和规划未来行动的能力，直接对应planning和problem-solving能力 - 训练方法：提出了\"Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm\"，使用了强化学习方法 - 新兴范式：讨论了\"function calling\"（函数调用），属于tool use的范畴 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然提到\"travel planning or multi-stage data analysis\"等应用场景，但只是作为例子，论文本身是提出通用框架而非针对特定领域 - 不主要聚焦于模型可靠性方面的研究 第四步特殊和模糊情况：论文讨论的\"function calling\"属于工具使用范畴，且PARL-MT是一种通用框架，用于增强LLM在多轮对话中调用函数的能力，属于通用的工具使用方法，而非针对特定领域的应用，因此应该保留。 综合判断：这篇论文的核心贡献是提出了一种增强LLM通用推理能力（特别是规划和多步推理）的新方法，通过引入进度感知和强化学习来改进多轮函数调用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多轮对话中缺乏进度感知能力的问题。针对多轮函数调用场景，我们提出了一种PARL-MT框架，通过Progress Awareness Generation管道和Progress Awareness-Guided Reinforcement Learning算法，将进度感知整合到模型训练中。在BFCL-V3和τ-Bench两个公共基准上，通过Executable Function Accuracy等指标验证了该方法显著优于现有策略，有效提升了模型在多轮对话中的函数调用性能。",
                    "summary_translation": "大型语言模型(LLMs, Large Language Models)在单轮函数调用方面已经取得了令人瞩目的成功，然而现实世界的应用（如旅行规划或多阶段数据分析）通常在多轮对话中展开。在这些场景中，LLMs不仅需要在每一步发出准确的函数调用，还必须保持进度意识(progress awareness)，即总结过去交互并规划未来行动的能力，以确保连贯的、长周期的任务执行。然而，现有方法要么将多轮训练简化为孤立的单轮样本（忽视了任务级规划），要么采用端到端强化学习(RL, Reinforcement Learning)，这种方法存在冗余问题且缺乏对进度意识的明确整合。为克服这些局限性，我们提出了PARL-MT框架，该框架明确地将进度意识整合到用于多轮函数调用的LLMs训练中。PARL-MT结合了(i)进度意识生成(PAG, Progress Awareness Generation)流程，该流程自动构建将对话摘要与未来任务规划相结合的数据集；以及(ii)进度意识引导的强化学习(PAG-RL, Progress Awareness-Guided Reinforcement Learning)算法，该算法将进度意识整合到RL训练中，以减少上下文冗余并提高局部行动与全局任务完成之间的一致性。在两个公共基准上的实证结果表明，PARL-MT显著优于现有方法，凸显了进度意识在实现稳健高效的多轮函数调用方面的有效性。",
                    "inspiration_trace": "# PARL-MT方法论逻辑推演分析\n\n## 一、宏观问题：多轮对话中的函数调用挑战\n\n**起点**：现实世界应用（如旅行规划、企业工作流）通常需要多轮对话完成，而非孤立交互。在这些场景中，LLMs不仅需要在每一步准确调用函数，还必须具备\"进度感知\"能力——即总结历史交互并规划未来行动，以确保任务连贯执行。\n\n**核心矛盾**：现有方法无法有效处理多轮对话中的长期依赖关系，导致重复调用函数、省略参数等问题，成为性能瓶颈。\n\n## 二、问题观察与现有方法分析\n\n### 1. 现有方法局限性观察\n\n**单轮训练方法的缺陷**：\n- 将多轮对话降级为孤立单轮样本，忽视任务级规划\n- 训练中排除未来对话信息，阻碍模型规划能力发展\n- 过度关注单轮准确性，忽略对未来有用的历史信息\n\n**端到端强化学习的缺陷**：\n- 随对话增长，输入冗余加剧，决策效率低下\n- 缺乏进度感知的显式整合，难以对齐局部行动与全局任务完成\n\n### 2. 关键洞察：进度感知的核心作用\n\n作者观察到，成功执行多轮任务需要两种关键能力：\n- **历史总结**：准确总结交互历史，减少冗余，辅助决策\n- **未来规划**：系统规划未来执行步骤，确保任务连贯性\n\n**核心假设**：显式地将进度感知纳入LLMs训练，可以显著提升多轮函数调用性能。\n\n## 三、方法论形成：从假设到设计\n\n基于上述洞察，作者提出PARL-MT框架，包含两个核心组件：\n\n### 1. 进度感知生成管道(PAG)\n\n**设计逻辑**：若要模型具备进度感知能力，首先需要高质量的数据集。\n\n**实现路径**：\n- **轨迹分割**：将多轮对话在函数调用处分割，保留关键上下文\n- **感知生成**：使用LLM生成包含历史总结、未来计划和基本原理的紧凑感知文档\n- **模型感知验证**：确保生成的感知包含足够信息重建原始函数调用\n- **多样性保持增强**：通过语义变换增强鲁棒性，避免过拟合\n- **模型预热**：使用增强数据集进行初步训练，建立基础进度感知能力\n\n**关键创新**：首次构建了将对话总结与任务规划相结合的显式进度感知数据集。\n\n### 2. 进度感知引导强化学习(PAG-RL)\n\n**设计逻辑**：将进度感知整合到强化学习中，解决上下文冗余和局部-全局对齐问题。\n\n**实现路径**：\n- **感知引导滚动**：每步先生成进度感知，再基于感知生成动作，减少冗余\n- **复合奖励设计**：结合结构有效性、模式正确性、任务成功和执行效率\n- **优化程序**：采用GRPO算法，将归一化优势均匀分配给轨迹中所有标记\n\n**关键创新**：首次将进度感知显式整合到多轮强化学习训练中，提供更高效的决策指导。\n\n## 四、验证与效果评估\n\n**实验设计**：在BFCL-V3和τ-Bench两个多轮函数调用基准测试上验证，使用多种骨干LLMs。\n\n**核心发现**：\n1. PARL-MT在所有基准测试和模型上一致优于现有方法\n2. 消融研究证实PAG和PAG-RL都对性能提升有贡献\n3. 进度感知能力评估显示模型在总结和规划方面显著提升\n4. 实际案例证明PARL-MT在准确性和效率上均优于基线\n\n## 五、逻辑链总结\n\n从宏观问题到最终方法，作者形成了清晰的逻辑链条：\n\n1. **问题识别**：现实多轮对话需要进度感知能力，现有方法无法满足\n2. **洞察发现**：进度感知（历史总结+未来规划）是解决多轮函数调用瓶颈的关键\n3. **假设形成**：显式整合进度感知可显著提升多轮函数调用性能\n4. **方法设计**：\n   - PAG：构建高质量进度感知数据集并预热模型\n   - PAG-RL：将进度感知整合到强化学习中，减少冗余并提供高效决策\n5. **实验验证**：在多个基准测试上验证方法有效性，证明假设成立\n\n这一逻辑链条从实际问题出发，通过观察、分析、假设形成、方法设计和实验验证，形成了一个完整的研究思路，核心创新在于识别并形式化了\"进度感知\"概念，并设计了专门方法将其整合到LLMs训练中。"
                },
                {
                    "title": "Pretraining LLM with Latent Thoughts in Continuous Space",
                    "arxiv_id": "2509.23184",
                    "authors": "Boyi Zeng, He Li, Shixiang Song, Yixuan Wang, Ziwei He, Xinbing Wang, Zhouhan Lin",
                    "summary": "The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts. Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, ours-1.4B (Pythia Arch), pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出一种新的预训练方法论\"Pretraining Language Models with Latent Thoughts\"，该方法通过在预训练阶段引入中间\"潜在思维\"来增强语言模型的推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究，直接受到思维链(CoT)的启发，旨在提升模型的通用推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标主题。核心概念方面明确聚焦于大语言模型(LLMs)；能力方向方面，论文直接关联推理能力，特别是通过形成\"类似于CoT的链\"来增强模型推理；训练方法方面，提出了一种新的预训练范式，虽然不是强化学习或进化方法，但同样是改进模型训练的创新方法。 第三步排除标准：论文完全不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)等排除领域，纯粹关注提升LLM本身的通用能力。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 核心贡献是提出了一种在连续空间中使用潜在思维预训练LLM的新方法，通过增加计算步骤来增强模型的推理能力，这直接符合提高大语言模型通用推理能力的研究目标。实验证明该方法能显著提升模型性能，且增加潜在思维数量可持续改善模型表现，进一步证实了其对推理能力的增强作用。",
                    "summary2": "本文旨在解决传统语言模型扩展方法面临收益递减的问题。针对语言模型预训练，我们提出了一种在连续空间中使用潜在思想的预训练方法，让模型在预测每个token前先生成中间潜在思想，并在Pile数据集和多个下游任务上通过perplexity和准确率等指标验证了其有效性。",
                    "summary_translation": "思维链(Chain-of-Thought, CoT)的显著成功——通过在测试时扩展生成步骤来提升性能——启发我们提出一个问题：我们能否在预训练期间利用类似的计算步骤扩展来改进每个单独令牌的生成？为解决这一问题，我们提出了一种新颖的预训练方法：使用潜在思维预训练语言模型(Pretraining Language Models with Latent Thoughts)。我们的方法预训练一个语言模型(LM，语言模型)首先生成一个中间潜在思维——即当前位置的最后一个隐藏状态——然后将其用作输入来预测实际的后续令牌。这一额外的计算步骤使语言模型能够在无约束的连续空间(unconstrained continuous space)中优化其预测。我们的实验表明，在相同的推理成本(identical inference cost)下，每个令牌生成一个额外潜在思维的语言模型优于参数数量翻倍的标准模型。例如，我们的模型ours-1.4B (Pythia Arch，Pythia架构)，在来自Pile数据集的3000亿个令牌上预训练，在语言建模和一系列通用下游任务上显著超过了使用相同数据训练的原始Pythia-2.8B模型。此外，在每个实际令牌之前增加生成的潜在思维数量——形成类似于CoT的链——持续地提升了模型的性能。",
                    "inspiration_trace": "# 从宏观问题到创新方法：预训练连续空间潜在思想的逻辑推演\n\n## 1. 宏观问题：语言模型能力提升的新路径\n\n论文始于对语言模型发展现状的观察：传统的提升方法——扩大参数规模和数据规模——正面临收益递减的困境。具体表现为：\n- 数据稀缺性（data scarcity）限制了高质量数据的获取\n- 扩展定律（scaling laws）逐渐饱和，增加参数带来的性能提升越来越小\n- 训练开销（training overheads）变得 prohibitive，成本急剧上升\n\n这引发了一个根本性问题：除了传统的\"向上扩展\"（scaling up）外，是否存在其他途径可以有效提升语言模型的能力？\n\n## 2. 关键观察：Chain-of-Thought的启示\n\n作者将目光投向了Chain-of-Thought (CoT)方法的显著成功。CoT通过在测试时扩展生成步骤（生成更长的推理链）来提高性能，这一现象启发了一个关键思考：\n\n**如果测试时的计算步骤扩展能够提升性能，那么在预训练期间是否可以通过类似的计算步骤扩展来提高每个token的生成质量？**\n\n这一观察将问题从\"如何扩大模型\"转向\"如何深化每个生成步骤的计算\"，标志着思维的重要转变。\n\n## 3. 问题聚焦：预训练阶段的计算步骤扩展\n\n基于CoT的启发，作者将问题聚焦为：如何在预训练阶段实现计算步骤的扩展，以提高每个token的生成质量？\n\n这个问题进一步分解为三个子问题：\n- 如何在预训练阶段引入额外的计算步骤？\n- 如何确保这种扩展在计算上是高效的？\n- 如何避免对专门数据集的依赖？\n\n## 4. 现有方法的局限性分析\n\n作者系统分析了现有相关方法的局限性：\n\n**垂直扩展（Vertical Scaling）方法**：如PonderLM，通过重复使用参数来加深网络。但这种方法导致训练不稳定性，且在可比推理预算下往往无法优于标准密集模型。\n\n**离散token空间方法**：如插入\"暂停\"token或规划token。这些方法受限于离散词汇空间，且通常需要专门的数据或复杂训练方案。\n\n**CoT及其变体**：如Coconut，在CoT数据上微调语言模型，使用\"连续思想链\"模拟推理步骤。但这种方法通常仅在问题提出后应用，且需要专门的CoT数据。\n\n这些分析揭示了研究空白：缺乏一种在预训练阶段、在连续空间中、无需专门数据的计算步骤扩展方法。\n\n## 5. 核心假设：连续空间中的潜在思想\n\n基于以上分析，作者提出了核心假设：**如果允许模型在生成每个实际token之前，先生成一个中间的潜在思想（在连续的隐藏状态空间中），然后将这个潜在思想作为输入来预测实际的后续token，那么模型将能够在无约束的连续空间中细化其预测，从而提高性能。**\n\n这一假设的关键创新点在于：\n- 使用连续的隐藏状态空间而非离散的token空间\n- 在生成每个token之前引入额外的计算步骤\n- 通过潜在思想实现预测的细化\n- 在预训练阶段学习这种能力，而非仅在测试时或微调阶段\n\n## 6. 方法设计：潜在思想的生成与利用\n\n基于核心假设，作者设计了具体的方法框架：\n\n**推理过程**：对于每个要生成的token，模型首先计算其对应的最后一个隐藏状态（即潜在思想），然后将这个隐藏状态作为下一个token生成步骤的输入嵌入，模拟递归思考过程。\n\n**训练过程**：由于推理是顺序的，纯自回归训练计算成本过高。为解决此问题，作者采用Jacobi迭代来近似真实的自回归隐藏状态，实现并行训练：\n1. 初始隐藏状态估计（迭代0）\n2. 通过Jacobi迭代并行更新状态\n3. 损失计算\n\n**位置嵌入**：隐藏状态被反馈到模型作为输入时，继承与其对应的原始token嵌入相同的位置编码。\n\n这一设计巧妙地解决了三个子问题：通过潜在思想引入额外计算步骤；通过Jacobi迭代确保计算效率；通过标准预训练目标避免对专门数据的依赖。\n\n## 7. 实验验证：从假设到证据\n\n作者通过一系列精心设计的实验验证了方法的有效性：\n\n**大规模预训练**：在300B-token的Pile数据集上预训练，证明方法在参数效率和数据效率方面均表现出优越扩展特性。\n\n**下游任务评估**：在一般基准测试和指令跟随任务上评估，证明预训练模型不仅优于类似大小模型，甚至超过参数数量翻倍的模型。\n\n**基线方法比较**：与Looped Transformer、Pause Token、Pondering LLM等方法比较，证明该方法在所有语言建模基准上实现最低困惑度，在下游任务上获得最高平均准确率。\n\n**现成模型应用**：在LLaMA-3-3B上进行持续预训练，验证方法对现有大规模基础模型的有效性。\n\n**消融研究**：研究关键组件影响，包括Jacobi迭代次数、位置嵌入策略和潜在思想数量，证明增加潜在思想数量（形成类似CoT的链）可持续提高性能。\n\n## 8. 结论与意义：新的扩展维度\n\n作者得出结论：使用潜在思想预训练的语言模型在相同推理成本下，始终优于参数数量翻倍的对应模型，以及之前的相关方法，即使这些方法使用双倍推理预算。\n\n这项工作的核心贡献在于引入了一个新的语言模型能力扩展维度——**水平扩展（horizontal scaling）**，与传统的垂直扩展（加深网络）形成对比。通过在每个token生成前引入连续空间中的潜在思想，模型能够在预训练阶段学习\"思考\"能力，为提升语言模型性能提供了全新思路。\n\n这一逻辑链条展示了从观察现象、分析问题、提出假设、设计方法到验证效果的完整科学研究过程，体现了作者敏锐的观察力、系统的分析能力和创新的思维方法。"
                },
                {
                    "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs",
                    "arxiv_id": "2509.23196",
                    "authors": "Haonan Wang, Weida Liang, Zihang Fu, Nie Zheng, Yifan Zhang, Yao Tong, Tongyao Zhu, Hao Jiang, Chuang Li, Jiaying Wu, Kenji Kawaguchi",
                    "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME'25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight-refine-solve framework.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的推理能力，而非将其应用于特定领域。论文提出了Insight-to-Solve (I2S)方法，这是一种测试时程序，将上下文示例转化为明确的、可重用的见解，并推导出针对特定问题的推理轨迹，直接针对提升LLM的通用推理能力。 从正面指标来看，论文明确涉及\"reasoning LLMs (RLMs)\"这一核心概念，并聚焦于reasoning能力（特别是在AIME和GPQA等数学和逻辑推理基准测试上）。同时，论文提到了\"verifier-based reinforcement learning\"和\"self-refined\"等与强化学习和自我进化相关的训练方法。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文处理了模型推理过程中的问题（语义误导和策略转移失败），但这是从提升模型内在推理质量的角度出发，而非对这些现象的社会学研究或应用层面讨论。 综上所述，这篇论文的核心贡献是提出了一种新的方法来增强LLM的通用推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决推理大模型(RLMs)在使用上下文推理示例时表现下降的问题。针对高质量推理示例反而降低模型准确性的现象，我们提出I2S(Insight-to-Solve)方法，将示例转化为可重用见解并推导目标推理轨迹，在AIME'25、GPQA等benchmark上通过准确率指标验证了其有效性，GPT-4.1在AIME'25上提升14.0%。",
                    "summary_translation": "近期的推理型大型语言模型(RLMs, reasoning LLMs)，特别是那些通过基于验证器的强化学习(verifier-based reinforcement learning)训练的模型，在使用少样本思维链(few-shot CoT, chain-of-thought)时往往表现不如直接回答(direct answering)。我们使用DeepSeek-R1的高质量推理轨迹(reasoning traces)作为示例(demonstrations)重新审视这一悖论，发现即使示例是最优的，增加更多范例(exemplars)也会持续降低准确性。详细分析揭示了这种下降背后的两个机制：(i) 语义误导(semantic misguidance)，即高文本相似性导致模型将目标视为与范例相同，并逐字复制中间步骤；以及(ii) 策略迁移失败(strategy transfer failure)，即模型难以提取有用的推理策略并将其应用于目标问题。\n\n基于这些发现，我们提出了洞察到解决(Insight-to-Solve, I2S)，这是一种顺序测试时程序(test-time procedure)，将示例转化为明确、可重用的洞察(reusable insights)，并推导出针对目标的推理轨迹；此外，推理过程可以进行自我优化(self-refined)以提高连贯性和正确性(I2S+)。在多样化基准测试(benchmarks)上的广泛实验表明，I2S和I2S+在开源和闭源模型(open- and closed-source models)上均持续优于直接回答和测试时扩展(test-time scaling)基线方法(baselines)。即使对于GPT模型，我们的方法也有帮助：在AIME'25上，GPT-4.1提升了+14.0%，o1-mini在AIME上提高了+2.7%，在GPQA上提高了+1.7%，这表明上下文示例(in-context demonstrations)可以通过洞察-优化-解决框架(insight-refine-solve framework)得到有效利用。",
                    "inspiration_trace": "# 从危害到帮助：推理示例转化为推理大模型资产的逻辑演进\n\n## 一、宏观问题：推理大模型的上下文学习悖论\n\n**起点观察**：现代推理大模型(RLMs)展现出反直觉现象——使用少样本思维链(CoT)提示时，其性能反而不如直接回答。这与传统大语言模型的行为形成鲜明对比，传统LLM通常从CoT示例中获益。\n\n**现象普遍性**：这一现象不仅存在于开源模型中，也被DeepSeek和OpenAI等组织观察到。DeepSeek特别指出RLMs对提示的敏感性，标准少样本CoT提示可能损害模型性能；OpenAI的推理模型手册也明确建议避免使用CoT提示。\n\n## 二、初始假设与验证：示例质量并非根本原因\n\n**初始假设**：传统CoT示例是为早期、较弱的模型设计的，现在可能落后于RLMs自身生成的推理轨迹，从而限制而非增强其能力。\n\n**验证实验**：使用DeepSeek-R1生成的高质量推理轨迹作为示例，这些问题与目标问题紧密匹配。在多个模型(Qwen系列和DeepSeek-R1蒸馏模型)和基准测试(AIME'25和GPQA)上进行评估。\n\n**意外发现**：即使使用高质量、紧密匹配的演示，准确率相对于直接推理仍然持续下降，且随着添加更多示例，下降更加明显。这推翻了初始假设，表明问题不仅在于示例质量。\n\n## 三、深入分析：揭示失败机制\n\n为了理解为什么对人类有帮助的演示对RLMs有害，作者进行了深入研究，识别出两个关键失败机制：\n\n### 1. 语义误导(Semantic Misguidance)\n- **表现形式**：当演示和目标问题共享高词汇重叠时，模型将它们视为等价物，直接复制中间步骤甚至最终结论。\n- **根本原因**：模型过度关注表面相似性，忽略问题的深层结构差异。演示充当\"语义诱饵\"，强调表面线索而掩盖问题结构。\n- **实例佐证**：在数学问题中，共享的\"digits 1-8\"短语导致模型错误地将演示中的\"分成两组\"和\"总和约18\"的策略应用到需要不同结构的目标问题上。\n\n### 2. 策略转移失败(Strategy Transfer Failure)\n- **表现形式**：即使演示编码了有用的结构见解，模型也无法适当地提取并应用这些策略到目标问题。\n- **根本原因**：模型倾向于错误地提取(跳过关键步骤)和错误地应用(使用不相关策略)，将复杂约束简化为简单规则。\n- **实例佐证**：在座位排列问题中，模型未能将演示中\"列举相邻对\"的方法转移到目标问题上，而是错误地将三元组约束简化为成对约束。\n\n## 四、初步尝试：两步推理\n\n**设计思路**：基于分析，作者尝试将答案生成与推理分离——首先使用演示引出推理轨迹，然后丢弃演示，仅使用问题和引出的轨迹生成最终答案。\n\n**实验结果**：这种方法确实带来了一致的改进，但仍未达到直接推理设置的性能。然而，这些结果是有希望的，并推动作者研究如何提高推理轨迹的质量。\n\n## 五、方法设计：从洞察到解决方案(I2S)\n\n基于前面的发现，作者提出了I2S (Insight-to-Solve)方法，这是一个测试时间推理流程，将推理示例转化为抽象指导，然后应用于目标问题。\n\n### 设计原则\n1. **提取结构见解并应用于目标**：专注于提取嵌入在示例中的高级推理策略，而不是原始CoT内容，鼓励模型内化可转移的解决方案模式。\n2. **补充而非覆盖内部推理**：现代RLMs已经拥有强大的潜在推理能力；演示应充当提示，而不是抑制这种能力的刚性模板。\n\n### 结构化推理转移流程\n1. **比较生成**：模型生成演示问题和目标问题之间的结构化比较，突出相似性和差异，帮助识别哪些方面是相关的。\n2. **分析推导**：基于比较和演示推理轨迹，模型生成分析，过滤掉无关细节，保留可转移策略。\n3. **推理生成**：模型通过将分析应用于目标问题来生成自己的推理轨迹，防止逐字重用演示步骤。\n\n## 六、方法扩展：迭代自我完善(I2S+)\n\n对于特别困难的问题，作者引入了I2S+，在基础流程上集成了迭代自我完善循环，使模型能够逐步改进其推理。每次完善迭代包括三个阶段：\n\n1. **建议**：模型生成一组候选修改或扩展建议，如替代中间步骤、完整性检查或错误纠正。\n2. **审查**：模型通过生成评估每个建议质量的检查语句来审查剩余建议。\n3. **完善**：模型采用最佳评级的建议，并使用它来修改当前的推理轨迹，提高一致性和正确性。\n\n## 七、实验验证：有效性证明\n\n作者在多个基准测试上验证了I2S和I2S+的有效性：\n\n- **封闭式基准测试**：在AIME'25和GPQA上，I2S+相对于直接推理带来显著提升(如R1-Distill-Qwen-7B在AIME'25上提升+9.33%)。\n- **开放式推理任务**：在通用推理任务上，I2S和I2S+也优于直接推理和其他基线。\n- **闭源模型**：即使对于GPT模型，该方法也有帮助——在AIME'25上，GPT-4.1提高+14.0%，o1-mini在AIME上提高+2.7%，在GPQA上提高+1.7%。\n\n## 八、效率分析：计算成本与收益\n\n作者研究了额外测试时间计算如何有效地转化为准确性：\n\n- 与并行测试时间扩展基线(如majority@N)相比，I2S在适度预算下更有效地将计算转化为收益。\n- 在相同数量的问题条件调用下，I2S在AIME'25和GPQA上始终优于majority@3。\n- I2S+与使用相当数量问题条件调用的majority@32相比，在GPQA上表现更好或相当。\n\n## 九、结论：从危害到帮助的范式转变\n\n论文得出结论，推理演示并非本质上对RLMs有害；当正确使用时，它们可以改善而不是降低性能。通过洞察-完善-解决框架，可以有效地利用上下文演示，将它们从潜在的危害转变为有价值的资产。\n\n这一研究不仅解释了RLMs中观察到的反直觉现象，还提供了一个实用的测试时间扩展方法，在各种模型和任务上始终优于现有方法，为推理大模型的有效利用开辟了新途径。"
                },
                {
                    "title": "Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models",
                    "arxiv_id": "2509.23146",
                    "authors": "Zichao Yu, Ming Li, Wenyi Zhang, Weiguo Gao",
                    "summary": "Tree search has recently emerged as a powerful framework for aligning generative models with task-specific rewards at test time. Applying tree search to Masked Diffusion Language Models, however, introduces two key challenges: (i) parallel unmasking yields highly correlated branches, limiting exploration, and (ii) reward evaluation via sampled completions produces high-variance estimates, making pruning unstable. We propose TReASURe, a tree-search test-time alignment method that addresses these issues. It introduces (i) UnmaskBranch, a branching strategy based on first-hitting unmasking that diversifies both token content and reveal order with a single model call per parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic resubstitution to score partially masked sequences with low-variance proxy completions. Theoretically, we quantify branching efficiency gains in NFEs (number of function evaluations), show that the scoring rule approximates the true reward with error bounded by predictive uncertainty, and prove improvements with larger tree widths. Empirically, TReASURe achieves state-of-the-art results on perplexity, linguistic acceptability, and control of sentiment and toxicity, outperforming prior methods under matched compute budgets, with especially strong gains in low-NFE regimes.",
                    "category": "cs.CL",
                    "filter_reason": "我按照筛选标准对这篇论文进行了详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为TReASURe的树搜索测试时间对齐方法，用于改进掩码扩散语言模型的生成能力。论文的核心贡献是解决树搜索应用于掩码扩散语言模型时的两个关键挑战：(1)并行去掩码导致分支高度相关，限制探索；(2)通过采样完成进行奖励评估产生高方差估计，使修剪不稳定。论文提出的UnmaskBranch分支策略和ResubstituteScore修剪规则，旨在增强语言模型的生成质量和控制能力，这属于改进LLM基础能力和推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 - 核心概念：论文明确研究\"Masked Diffusion Language Models\"，属于LLM范畴。 - 能力方向：树搜索是一种规划方法，与problem-solving相关，论文通过改进树搜索来增强模型的推理和规划能力。 - 训练方法：论文提到\"reward-aligned\"（奖励对齐），这与强化学习中的奖励概念相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉领域，虽然提到\"Diffusion\"，但指的是语言模型中的扩散模型，而非视觉扩散模型。 - 不聚焦于特定应用领域，而是关注通用语言模型的改进。 - 虽然提到\"control of sentiment and toxicity\"，但这只是作为评估指标，不是论文主要焦点。 第四步：特殊和模糊情况 论文没有明确讨论智能体/工具使用，也没有专门针对幻觉、可解释性或安全性的改进，因此不适用于这些特殊情况的判断。 最终决策：这篇论文符合研究范围，因为它致力于通过改进树搜索方法来增强大语言模型本身的通用推理能力和生成质量，而非将LLM作为工具应用到特定领域。论文提出的TReASURe方法是一种通用的测试时间对齐技术，能够提升语言模型的基础能力，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决在Masked Diffusion Language Models中应用树搜索进行测试时对齐的两个关键挑战：并行去掩码产生高度相关分支和采样奖励评估的高方差问题。针对MDLMs的离散特性，我们提出了一种名为TR EASUR E的树搜索方法，引入了UNMASK BRANCH分支策略和RESUBSTITUTE SCORE剪枝规则。在困惑度、语言可接受性、情感和毒性控制等多个可控文本生成任务上，通过奖励函数和NFE指标验证了其有效性，在匹配计算预算下实现了最先进的性能。",
                    "summary_translation": "树搜索(Tree search)最近已成为一种强大的框架，用于在测试时间(test time)将生成模型(generative models)与任务特定奖励(task-specific rewards)对齐。然而，将树搜索应用于掩码扩散语言模型(Masked Diffusion Language Models)会带来两个关键挑战：(i) 并行解掩(parallel unmasking)产生高度相关的分支(correlated branches)，限制了探索(exploration)；(ii) 通过采样完成(sampled completions)进行奖励评估(reward evaluation)会产生高方差估计(high-variance estimates)，使剪枝(pruning)不稳定。我们提出了TReASURe，一种解决这些问题的树搜索测试时间对齐(tree-search test-time alignment)方法。它引入了(i) UnmaskBranch，一种基于首次命中解掩(first-hitting unmasking)的分支策略，通过每个父节点(parent node)的单次模型调用来多样化标记内容(token content)和揭示顺序(reveal order)；以及(ii) ResubstituteScore，一种使用确定性重新替换(deterministic resubstitution)来评估部分掩码序列(partially masked sequences)的剪枝规则(pruning rule)，采用低方差代理完成(low-variance proxy completions)。理论上，我们量化了在NFEs (number of function evaluations，函数评估次数)方面的分支效率提升，表明评分规则(scoring rule)以预测不确定性(predictive uncertainty)为界的误差逼近真实奖励，并证明了更大树宽度(tree widths)带来的改进。实证上(Empirically)，TReASURe在困惑度(perplexity)、语言可接受性(linguistic acceptability)以及情感(sentiment)和毒性(toxicity)控制方面取得了最先进的(state-of-the-art)结果，在匹配的计算预算(compute budgets)下优于先前方法，在低NFE机制(low-NFE regimes)下尤其表现出强劲的提升。",
                    "inspiration_trace": "# 从观察到创新：TReASURe方法的逻辑演进\n\n## 1. 宏观问题：生成模型的测试时对齐\n\n生成模型（如自回归和扩散模型）在预训练后往往需要根据特定任务目标进行对齐。传统方法通过微调实现，但这需要大量计算资源和训练数据。测试时对齐(TTA)提供了一种替代方案，允许在推理时引导模型输出朝向任务特定目标，无需重新训练。\n\n## 2. 中观聚焦：MDLMs中的测试时对齐挑战\n\n掩码扩散语言模型(MDLMs)作为自回归模型的新兴替代方案，从全掩码token开始，通过离散去噪步骤逐步揭示token。MDLMs具有灵活的采样调度和条件模式，特别适合可控生成任务。\n\n然而，将树搜索方法（在连续扩散模型中表现出色）应用于MDLMs时，作者发现两个关键挑战：\n\n**挑战1：并行去掩码导致高度相关分支**\n- MDLMs中所有掩码位置同时更新，导致紧密耦合的token分布\n- 重复采样产生高度相似候选，探索空间受限\n- 本地重采样很少改变下一个提交token，轨迹多样性不足\n\n**挑战2：奖励评估高方差导致修剪不稳定**\n- MDLMs输出分类分布，估计价值函数需采样完整序列\n- 小扰动可导致奖励大变化，使修剪决策不可靠\n\n## 3. 微观观察：问题具体表现\n\n通过实验，作者验证了这些挑战：\n\n**观察1：分支效率低下**\n- 实验显示，即使使用更宽的beam，大多数节点只折叠到一两个有效分支\n- 表明并行采样产生大量冗余分支，浪费计算资源\n\n**观察2：奖励估计波动大**\n- 去噪步骤中奖励值波动显著，中间步骤尤其不稳定\n- 高方差使得基于这些估计的修剪不可靠\n\n## 4. 核心假设：解决问题的关键点\n\n基于观察，作者形成两个关键假设：\n\n**假设1：仅在提交事件时分支可提高多样性**\n- 只在实际发生去掩码事件时扩展搜索，避免浪费更新\n- 同时实现去掩码顺序和token内容多样化\n- 每个父节点只需一次模型调用，保持效率\n\n**假设2：确定性重新填充可降低方差**\n- 重用分支时获得的概率，通过确定性填充构建临时完成\n- 单一代理完成用奖励评分一次，无需额外模型调用\n- 提供低方差、确定性评分，使修剪更稳定\n\n## 5. 方法论构建：TReASURe设计\n\n基于假设，作者开发了TReASURe方法，包含两个核心组件：\n\n**组件1：UNMASK BRANCH**\n- 使用首次命中采样(FHS)直接跳到下一个去掩码时间\n- 评估模型一次，通过均匀选择掩码索引和枚举top-b(n)个token生成子节点\n- 在去掩码顺序和token内容上引入多样性，每个父节点只需一次模型调用\n\n**组件2：RESUBSTITUTE SCORE**\n- 重用分支概率，通过确定性填充掩码位置构建临时完成\n- 已提交token保持固定，掩码位置填充当前头部预测\n- 单一代理完成用奖励评分一次，修剪成本与父节点数量成线性比例\n\n## 6. 理论验证：方法有效性保障\n\n作者提供三个理论支持：\n\n**理论1：UNMASK BRANCH效率优势**\n- 证明与朴素并行采样相比，显著减少模型评估次数\n- 朴素方法需b(n)/(1-exp(-nh))次评估，而UNMASK BRANCH只需一次\n\n**理论2：RESUBSTITUTE SCORE准确性**\n- 证明在Hamming-Lipschitz奖励假设下，重新填充奖励与真实期望奖励差距由模型预测不确定性限定\n- 当模型对顶部预测高置信度时，重新填充奖励接近期望奖励\n\n**理论3：树宽度增加的奖励单调性**\n- 证明增加树宽度m(n)总是改善最终奖励\n- 为使用更宽搜索提供理论保证\n\n## 7. 实验验证：方法优越性证明\n\n通过全面实验验证：\n\n**实验1：与基线方法比较**\n- TReASURe在所有任务(CoLA、Toxicity、Sentiment和Gen. PPL)上实现最先进性能\n- 在低NFE制度下已超过基线在更高NFE下的结果\n- 随NFE增加性能稳步提高，符合理论预测\n\n**实验2：奖励轨迹可视化**\n- 去噪过程中奖励稳步上升，表明TReASURe引导中间状态朝向更理想区域\n\n**实验3：组件消融研究**\n- RESUBSTITUTE SCORE优于匹配NFE下的前一步评分\n- 以远少于真实后验评分的评估次数实现相当性能\n- 增加beam宽度不保证更好奖励，而增加树宽度则单调改善性能\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过观察和假设形成，最终构建出创新方法论的系统性思考过程。"
                },
                {
                    "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs",
                    "arxiv_id": "2509.23166",
                    "authors": "Chenxing Wei, Hong Wang, Ying He, Fei Yu, Yao Shu",
                    "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。具体分析如下： 第一步：核心判断——论文的本质是改进LLM的基础能力。论文提出了测试时策略自适应(T2PAM)和ROSA算法，使LLM能够在多轮交互中根据用户反馈实时调整自己的策略，实现对话中的自我修正。这明显是关于提升LLM基础推理和适应能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确关注Large Language Models (LLMs) - 能力方向: 涉及reasoning和problem-solving，特别是复杂任务的多轮交互推理能力 - 训练方法: 利用用户反馈作为奖励信号来优化模型策略，与强化学习思想一致 - 新兴范式: 关注LLM在多轮交互中的表现，与智能体交互和自我进化相关 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于医疗、化学、生物等特定应用领域 - 不主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文提出的是一种通用的自适应机制，用于增强LLM在多轮交互中的推理和适应能力，而不是针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种新的范式(T2PAM)和算法(ROSA)，使LLM能够在测试时根据用户反馈进行策略自适应，从而提升其在复杂多轮交互中的推理能力和自我修正能力。这直接关系到提升LLM的通用推理能力，完全符合我的研究目标。",
                    "summary2": "本文旨在解决LLMs在多轮交互中性能下降的问题。针对多轮对话场景，我们提出了一种测试时策略适应(T2PAM)范式及ROSA算法，在多个推理任务数据集上通过准确率和纠错提升率验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）采用多轮交互（multi-turn interaction）作为完成复杂任务的基本范式。然而，在扩展交互（extended interactions）中，它们的性能通常会下降，因为它们通常在静态、单轮数据（single-turn data）上进行训练，这阻碍了它们适应实时用户反馈的能力。为了解决这一限制，我们首先提出了一种新范式：多轮交互的测试时策略适应（Test-Time Policy Adaptation for Multi-Turn Interactions, T2PAM），它利用正在进行的交互中的用户反馈作为奖励信号，来估计与用户偏好一致的潜在最优策略（latent optimal policy），然后更新一小部分参数以引导模型朝向这一策略，最终实现高效的对话内自我修正（in-conversation self-correction）。接着，我们介绍了最优参考的一步适应（Optimum-Referenced One-Step Adaptation, ROSA），这是一个实现T2PAM的轻量级算法。ROSA在单个、高效的更新步骤中引导模型参数朝向理论最优策略，避免了昂贵的迭代基于梯度的优化（gradient-based optimization），并最小化了计算开销（computational overhead）。我们提供了严格的理论分析（theoretical analysis），保证随着交互次数的增加，ROSA的策略会收敛（converges）到用户的偏好。在具有挑战性的基准测试（benchmark）上的大量实验表明，ROSA在任务有效性和效率方面都取得了显著改进。",
                    "inspiration_trace": "# 从问题观察到方法创新：ROSA算法的逻辑演进\n\n## 1. 宏观问题识别：多轮交互中的性能退化\n\n作者首先观察到一个核心问题：**大型语言模型(LLMs)在多轮对话中表现不佳**。具体表现为：\n- LLMs主要在静态、单轮数据上训练，但实际应用中多轮交互是完成复杂任务的基本范式\n- 随着对话轮次增加，模型性能逐渐下降，难以适应实时用户反馈\n- 现有模型在多轮场景中表现出能力减弱和不稳定性\n\n这一观察揭示了**训练范式与实际应用场景之间的根本不匹配**。\n\n## 2. 现有方法分析：识别局限性\n\n作者系统分析了现有方法的不足，为创新奠定基础：\n\n**传统训练方法**：\n- 监督微调(SFT)和基于人类反馈的强化学习(RLHF)主要依赖单轮数据\n- 多轮训练策略成本高昂，受限于长上下文序列训练和数据收集难度\n\n**推理时方法**：\n- 提示工程：作为上下文学习，难以在少数轮次内实现有效偏好对齐\n- 检索增强生成(RAG)：增加显著推理开销，性能依赖外部数据库质量\n- 模型编辑：结构上不适合编码细粒度用户偏好\n- 现有测试时方法：主要针对单轮任务，计算成本高\n\n这一分析揭示了**现有方法在效率、适应性和计算成本之间的权衡困境**。\n\n## 3. 核心洞察：范式转换的必要性\n\n基于上述分析，作者形成关键洞察：**需要将模型对齐从静态、离线训练阶段转移到动态、在线推理过程**。\n\n这一洞察引出核心假设：如果能够设计一种方法，在推理时利用对话中的用户反馈作为奖励信号，实时更新模型参数，使模型能够动态适应用户偏好，那么就可以提高多轮交互的效果。\n\n## 4. 新范式提出：T2PAM\n\n基于核心洞察，作者提出了**测试时多轮交互策略适应(T2PAM)**新范式，其核心思想是：\n- 将用户反馈视为主动信号，引导实时参数更新\n- 在对话过程中实现自我纠正，使策略逐步演进并与用户偏好对齐\n- 保持计算轻量级，不引起用户感知的延迟或内存开销\n\nT2PAM范式形式化为三个步骤：\n1. **生成**：模型根据对话上下文生成响应\n2. **反馈**：用户提供反馈，映射为标量奖励信号\n3. **适应**：基于反馈在线更新模型参数\n\n## 5. 实现挑战与解决方案：ROSA算法\n\n要实现T2PAM范式，作者面临两个主要挑战：\n\n**挑战1：如何高效利用反馈更新策略？**\n- 传统RLHF方法依赖迭代梯度优化，计算成本高\n- 需要避免昂贵的迭代优化，实现轻量级更新\n\n**解决方案**：作者提出**最优参考一步适应(ROSA)**算法，核心创新是：\n- 利用RLHF目标函数，但直接求解其闭式最优解\n- 将用户反馈转化为最优策略的解析估计\n- 在单一步骤中将模型参数引导向这个最优策略\n\n**挑战2：如何保证方法有效性？**\n- 需要理论保证随着交互增加，模型策略会收敛到用户偏好\n\n**解决方案**：作者提供了严格的理论分析，证明：\n- 每次反馈都能保证减少与用户策略的误差（单调错误减少）\n- 随着交互次数增加，模型策略会累积收敛到用户偏好\n- 统一误差界限平衡了学习速度与稳定性\n\n## 6. ROSA算法设计细节\n\nROSA算法的具体设计遵循三个关键步骤：\n\n**步骤1：基于RLHF目标的轮次适应**\n- 建立最大化期望奖励同时惩罚与前一策略偏离的优化目标\n- 引入KL散度正则化确保稳定受控的更新\n\n**步骤2：从理论最优到实用一步更新**\n- 利用RLHF目标的闭式解直接确定最优策略\n- 针对实践中仅观察到单个响应反馈的情况，构建实用目标\n- 通过指数重加权调整错误响应的概率，指导模型减少生成错误\n\n**步骤3：通过线性化优化进行高效参数更新**\n- 使用一阶泰勒展开近似策略函数\n- 采用共轭梯度算法高效求解参数更新，避免显式构建海森矩阵\n- 实现计算和内存高效的在线适应\n\n## 7. 实验验证与效果评估\n\n作者通过广泛实验验证ROSA的有效性：\n\n**有效性验证**：\n- 在数学推理、一般推理、代码生成和多语言推理四个领域评估\n- 结果显示ROSA在所有基准数据集和模型上一致优于基线方法\n- 特别是在\"纠正提升\"指标上表现突出，证明其强大的自我纠正能力\n\n**效率验证**：\n- 更新过程可异步执行，对用户基本不可感知\n- GPU内存开销仅适度增加，适合现有硬件部署\n- 时间-准确率曲线显示ROSA在更短时间内达到更高准确率\n\n## 8. 逻辑演进总结\n\n作者提出核心方法的完整逻辑链可概括为：\n\n**问题观察** → **方法分析** → **洞察形成** → **范式提出** → **算法设计** → **理论保证** → **实验验证**\n\n这一演进过程体现了从实际问题出发，通过系统分析现有方法局限性，形成创新性洞察，提出新范式，设计具体算法，提供理论保证，最终通过实验验证的完整科学研究路径。ROSA算法的核心创新在于将模型适应从训练阶段转移到推理阶段，实现了高效、轻量级的在线策略适应，为解决LLMs在多轮交互中的性能退化问题提供了新思路。"
                },
                {
                    "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts",
                    "arxiv_id": "2509.23188",
                    "authors": "Guancheng Wan, Leixin Sun, Longxu Dou, Zitong Shi, Fang Wu, Eric Hanchen Jiang, Wenke Huang, Guibin Zhang, Hejia Geng, Xiangru Tang, Zhenfei Yin, Yizhou Sun, Wei Wang",
                    "summary": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种全栈框架（Diagnose, Localize, Align）来解决LLM多智能体系统在指令冲突下的可靠性问题。从本质上看，论文专注于改进LLM多智能体系统的基础能力，特别是协作推理和指令遵从性，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。 论文包含了多个正面指标：核心概念上明确关注LLM驱动的多智能体系统；能力方向上涉及协作推理；新兴范式上专注于多智能体系统和工具使用。论文提出的CRAS评分、注意力漂移分析和SAIL对齐方法都是为了增强模型对指令层级结构的理解，从而提高其在复杂任务中的推理能力。 虽然论文在实验中使用了MedQA作为评估基准，但这只是验证方法有效性的手段，而不是论文的主要焦点。论文的核心是提出一种通用的框架，可以应用于各种多智能体系统，而不是针对特定领域的问题。因此，这篇论文符合研究范围，应该被保留。",
                    "summary2": "本文旨在解决LLM多智能体系统在指令冲突下的可靠性问题。针对系统-用户和对等-对等指令冲突场景，我们提出了一种三阶段全栈框架（Diagnose-Localize-Align），包括上下文化角色adherence评分（CRAS）、注意力漂移分析和指令层外科手术对齐（SAIL）。在多个基准测试和MAS框架上，通过准确率和CRAS指标验证了其有效性，显著提高了指令层次遵从性（如AutoGen在MedQA上提升5.60%），无需全模型微调。",
                    "summary_translation": "由大型语言模型(Large Language Model, LLM)驱动的多智能体系统(multi-agent systems, MAS)已迅速推进了复杂任务中的协作推理、工具使用和角色专业化协调。然而，对可靠性要求高的部署仍然受到一种系统性故障模式的阻碍：在指令冲突(instruction conflicts，系统-用户、对等-对等)下的层级合规性(hierarchical compliance)，即智能体在存在竞争性需求时错误地优先处理系统级规则。此外，广泛使用的宏观层面指标(macro-level metrics，如pass@k)掩盖了这些微观层面的违规行为，并为修复提供了很少可行的指导。在这项工作中，我们提出了一个全栈的三阶段框架：(1) 诊断(Diagnose) - 上下文化角色遵守度得分(Contextualized Role Adherence Score, CRAS)，一种针对每个查询的、上下文感知的评分指标，将角色遵守度分解为四个可测量的维度；(2) 定位(Localize) - 注意力漂移(attention drift)分析，揭示指令冲突是通过主要集中在中层的注意力头(attention heads)解决的；(3) 对齐(Align) - 指令层的外科手术式对齐(Surgical Alignment of Instruction Layers, SAIL)，该方法仅在定位的焦点层上安装LoRA，并优化一个基于token加权的DPO风格偏好目标(DPO-style preference objective)，该目标根据token的焦点注意力贡献给予奖励。在标准基准测试和MAS框架中，我们的外科手术式方法提高了指令层级合规性(instruction hierarchy compliance)(例如，在MedQA上使用AutoGen提高了5.60%)，而无需进行全模型微调(full-model finetuning)。",
                    "inspiration_trace": "# 从问题观察到解决方案：LLM多智能体系统指令冲突处理框架的逻辑演进\n\n## 1. 宏观问题观察：LLM多智能体系统的可靠性瓶颈\n\n### 现象观察\n作者首先观察到LLM驱动的多智能体系统(MAS)虽然在协作推理、工具使用和角色专业化协调方面取得了快速进展，但在可靠性关键场景的部署中存在一个系统性故障模式：**指令冲突下的层级合规性问题**。具体表现为：\n\n- 每个智能体同时受到高级别系统指令（身份、约束）和低级别用户/对等请求的约束\n- 当系统指令与用户指令或对等指令之间出现冲突时，智能体会错误地优先级处理，偏离角色、违反约束\n- 现有宏观指标（如任务成功率、pass@k）掩盖了这些微观故障，无法提供干预指导\n\n### 核心问题提出\n基于以上观察，作者提出第一个关键问题：**如何量化智能体在交互中是否忠实地遵循其角色和约束？**\n\n## 2. 诊断阶段：从宏观到微观的评估\n\n### 思考路径\n作者认识到现有宏观指标无法捕捉微观层面的角色遵循失败，需要一个能够上下文感知、细粒度评估角色遵循的方法。\n\n### 解决方案：CRAS评分系统\n作者提出**上下文角色遵循评分(CRAS)**，这是一个查询级别的、基于规则的诊断工具，将角色遵循分解为四个互补维度：\n\n1. **目标对齐(GA)**：行动和中间步骤是否持续推进任务目标\n2. **角色一致性(RC)**：语言、推理风格和方法选择是否与角色一致\n3. **知识边界遵循(KBA)**：主张是否保持在预定知识范围内\n4. **约束合规性(CC)**：是否违反明确约束条件\n\n### 实现机制\n- 为每个查询上下文编程实例化评估规则\n- 使用独立评估器将轨迹映射到各维度分数\n- 聚合分数获得CRAS总分\n\n### 预期效果\n- 从宏观成功评估提升到上下文感知的遵循评估\n- 提供稳定、可复现的信号用于有针对性修复\n- 为后续定位和对齐阶段奠定基础\n\n## 3. 定位阶段：深入模型内部寻找问题根源\n\n### 现象观察\n通过CRAS驱动的诊断，作者发现在冲突情况下，角色遵循在系统和用户指令碰撞时确实会下降，尽管一般能力保持完整。这表明存在一个**局部仲裁机制**，而非全局弱点。\n\n### 核心问题提出\n这引发第二个关键问题：**指令仲裁在模型的哪个部分发生？**\n\n### 思考路径\n作者假设如果存在局部仲裁机制，应该可以通过分析模型在冲突和非冲突输入下的行为差异来定位。注意力机制可能是这种仲裁的关键，因为它决定了模型如何权衡不同信息。\n\n### 解决方案：注意力漂移分析\n作者提出通过对比冲突和非冲突输入下的注意力行为来定位冲突敏感区域：\n\n1. 构建程序化冲突数据集，包含仅在指令兼容性上不同的匹配输入对\n2. 沿三个互补轴量化每个头的变化：\n   - 幅度(∆mag)：注意力强度的变化\n   - 方向(∆dir)：注意力模式的方向性重新定位\n   - 分布(∆dist)：注意力分布的重塑\n3. 结合指标获得头级别漂移分数：S(l,h) = λmag∆mag(l,h) + λdir∆dir(l,h) + λdist∆dist(l,h)\n4. 选择前k%的头作为焦点头，收集它们的层形成冲突敏感层集\n\n### 关键发现\n冲突敏感模块表现出明显的行为变化，且**集中在中间层**（如Qwen2.5-7B的19-23层，LLaMA3.1-8B的18-22层）。这与只有部分注意力头在功能上关键的证据相呼应。\n\n## 4. 对齐阶段：针对性的解决方案\n\n### 现象观察\n通过诊断和定位，作者已确定了角色遵循失败的量化方法(CRAS)和指令仲裁发生的模型位置（中间层）。\n\n### 核心问题提出\n这引出第三个关键问题：**是否可以仅通过焦点对齐加强指令层级合规性，而不影响一般能力？**\n\n### 思考路径\n作者推测既然已定位仲裁机制，应该可以只针对这些特定区域优化，而不需调整整个模型。这种\"外科手术式\"方法可最大限度减少对模型一般能力的干扰。\n\n### 解决方案：SAIL外科手术对齐\n作者提出**指令层的外科手术对齐(SAIL)**，具体包括：\n\n1. **偏好数据构建**：使用CRAS为每个冲突上下文构建偏好对\n2. **相对注意力贡献**：定义每个token的焦点头注意力贡献比例作为权重\n3. **SAIL损失**：设计token加权的偏好优化损失，根据焦点注意力贡献加权token级学习\n4. **焦点层适配器**：仅在焦点层的注意力投影上安装LoRA适配器，冻结非焦点参数\n\n### 预期效果\n- 集中学习信号在仲裁发生的确切位置和时间\n- 提高指令层级合规性，同时保持一般能力\n\n## 5. 整体框架：Diagnose-Localize-Align\n\n### 整合思考\n作者将三个阶段整合为全栈框架：\n- **诊断(CRAS)**：提供细粒度评估信号，识别问题\n- **定位(注意力漂移分析)**：揭示问题在模型内部的位置\n- **对齐(SAIL)**：提供针对性解决方案，仅针对问题区域优化\n\n### 框架优势\n- **全栈性**：从评估到定位到修复的完整流程\n- **高效性**：不需要全模型微调，仅更新焦点层\n- **精确性**：通过CRAS的细粒度评估和注意力漂移的精确定位，确保干预针对性\n- **可解释性**：每个阶段都提供可解释信号和洞察\n\n## 6. 实验验证\n\n### 实验设计\n作者在四个数据集（MMLU、SciBench、GPQA、MedQA）和四个多智能体系统（Dylan、MacNet、AutoGen、SelfConsistency）上评估，使用两个指令调整模型作为基础架构：LLaMA3.1-8B-Instruct和Qwen2.5-7B-Instruct。\n\n### 主要结果\n- SAIL在多数情况下提高性能，尤其在复杂推理基准上\n- 例如，在AutoGen框架下，MedQA准确率提高5.60%\n- 消融研究验证了token级奖励机制和冲突驱动的层选择的有效性\n\n### 鲁棒性和敏感性分析\n- 方法在不同训练阶段表现出稳定性能提升\n- 对学习率和LoRA秩的敏感性分析表明，适当超参数选择对方法有效性至关重要\n\n## 逻辑链总结\n\n作者从观察到解决方案的逻辑演进可概括为：\n\n1. **问题识别**：观察到LLM多智能体系统在指令冲突下的可靠性问题，现有宏观指标无法捕捉微观角色遵循失败\n\n2. **诊断需求**：需要量化智能体在交互中是否忠实遵循角色和约束的方法\n\n3. **诊断解决方案**：提出CRAS，多维度、上下文感知的角色遵循评分系统\n\n4. **深入分析**：通过CRAS发现角色遵循在指令冲突时下降，表明存在局部仲裁机制\n\n5. **定位需求**：确定模型中指令仲裁发生的精确位置\n\n6. **定位解决方案**：通过注意力漂移分析，发现冲突敏感模块集中在中间层\n\n7. **对齐需求**：是否可通过焦点对齐加强指令层级合规性，而不影响一般能力？\n\n8. **对齐解决方案**：提出SAIL，外科手术式对齐方法，仅在焦点层安装LoRA适配器\n\n9. **整体框架**：整合诊断、定位和对齐三个阶段，形成全栈框架\n\n10. **实验验证**：在多个基准和MAS框架上验证方法有效性，证明其能提高指令层级合规性而不影响一般能力\n\n这一逻辑链展示了作者从观察到假设，再到解决方案的系统性思考过程，体现了科学研究的严谨性和创新性，为LLM多智能体系统在指令冲突下的可靠性问题提供了一个全面而精确的解决框架。"
                },
                {
                    "title": "Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning",
                    "arxiv_id": "2509.23140",
                    "authors": "Song Jin, Juntian Zhang, Yong Liu, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan",
                    "summary": "Recent advancements have endowed Large Language Models (LLMs) with impressive general reasoning capabilities, yet they often struggle with personalization reasoning - the crucial ability to analyze user history, infer unique preferences, and generate tailored responses. To address this limitation, we introduce TagPR, a novel training framework that significantly enhances an LLM's intrinsic capacity for personalization reasoning through a tagging the thought approach. Our method first develops a data-driven pipeline to automatically generate and semantically label reasoning chains, creating a structured dataset that fosters interpretable reasoning. We then propose a synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on this tagged data to establish foundational reasoning patterns, followed by a multi-stage reinforcement learning (RL) process. This RL phase is guided by a unique composite reward signal, which integrates tag-based constraints and a novel Personalization Reward Model with User Embeddings (PRMU) to achieve fine-grained alignment with user-specific logic. Extensive experiments on the public LaMP benchmark and a self-constructed dataset demonstrate that our approach achieves state-of-the-art results, delivering an average improvement of 32.65% over the base model across all tasks. Our work validates that structured, interpretable reasoning is a highly effective pathway to unlocking genuine personalization capabilities in LLMs.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为TagPR的新训练框架，用于增强大语言模型(LLM)的个性化推理能力。从筛选标准来看，该论文符合我的研究目标，理由如下： 首先，在核心判断层面，论文的本质是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。论文提出了\"标记思维\"的新方法，结合监督微调和多阶段强化学习过程来增强模型的推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)这一核心概念；聚焦于推理能力(reasoning)，特别是个性化推理；采用了强化学习(RL)作为关键训练方法；通过创建结构化数据集促进可解释推理，这与提升模型内在能力的方向一致。 在排除标准方面，论文没有涉及多模态与视觉、特定应用领域(如医疗、化学等)或模型可靠性的应用层面问题。虽然论文关注\"个性化推理\"，但这应被视为通用推理能力的一个特定方面，而非针对特定领域的应用研究。 在特殊和模糊情况处理上，论文提出的\"标记思维\"方法增强了模型的可解释性，这属于\"增强模型内在的可解释性\"的情况，应当保留。这种方法通过提升推理的可解释性来提高模型的推理质量，而非仅进行社会学研究或应用层面的讨论。 综上所述，该论文致力于通过新方法提升LLM的通用推理能力，符合我的研究范围。",
                    "summary2": "本文旨在解决大型语言模型在个性化推理方面的不足。针对用户历史分析和偏好推断任务，我们提出了一种TagPR框架，通过\"标记思维\"方法和多阶段强化学习训练策略，并在LaMP基准和自构建数据集上通过准确率、F1分数、ROUGE等指标验证了其有效性，平均性能提升32.65%。",
                    "summary_translation": "最近的技术进步使大型语言模型（Large Language Models, LLMs）具备了令人印象深刻的通用推理能力，然而它们在个性化推理（personalization reasoning）方面常常表现不佳——个性化推理是分析用户历史、推断独特偏好并生成定制化回应的关键能力。为解决这一局限性，我们提出了TagPR，一种新颖的训练框架，通过标记思维（tagging the thought）方法显著增强了大型语言模型的个性化推理内在能力。我们的方法首先开发了一个数据驱动的流程，自动生成并语义标记推理链（reasoning chains），创建一个促进可解释推理的结构化数据集。随后，我们提出了一种协同训练策略，首先在标记数据上进行监督微调（Supervised Fine-Tuning, SFT）以建立基础推理模式，然后进行多阶段强化学习（reinforcement learning, RL）过程。该强化学习阶段由独特的复合奖励信号（composite reward signal）引导，该信号整合了基于标签的约束和一种新颖的用户嵌入个性化奖励模型（Personalization Reward Model with User Embeddings, PRMU），以实现与用户特定逻辑的细粒度对齐。在公开的LaMP基准测试和自构建数据集上的大量实验表明，我们的方法取得了最先进（state-of-the-art）的成果，在所有任务上相比基础模型平均提升了32.65%。我们的工作验证了结构化、可解释的推理是释放大型语言模型真正个性化能力的一条高效途径。",
                    "inspiration_trace": "# 从问题到解决方案：TagPR方法的逻辑演进\n\n## 1. 宏观问题：LLMs的个性化推理困境\n\n**观察现象**：大型语言模型(LLMs)在数学、编程等通用推理任务上表现出色，但在个性化推理方面存在明显不足。即使最强大的推理型LLMs也难以有效分析用户历史、推断独特偏好并生成定制化响应。\n\n**问题本质**：通用推理模型倾向于优先考虑内部通用逻辑，而非用户特定上下文，导致响应过于通用或与用户偏好相矛盾。现有方法如R2P和RPM主要作为外部支架，未从根本上增强模型内在的个性化推理能力。\n\n## 2. 核心洞察：个性化推理的结构化本质\n\n**关键观察**：个性化推理不是单一直觉行为，而是结构化的多步骤过程——分析用户历史、识别重复模式、将模式应用到新情境。\n\n**假设形成**：标准LLMs的不透明、自由形式推理不适合这种程序性任务。如果能够将推理过程外部化为离散、可解释的步骤，每个步骤标记语义标签，就能释放模型的个性化潜力。\n\n## 3. 方法论构思：\"标记思想\"框架\n\n**核心思想**：通过\"标记思想\"(Tagging the Thought)将复杂个性化任务转化为明确、可管理的过程，使用语义标签作为认知路标。\n\n**框架设计**：TagPR框架包含两个关键组成部分：\n1. 数据驱动的管道：自动生成和语义标记推理链，创建结构化数据集\n2. 协同训练策略：监督微调(SFT)建立基础推理模式，多阶段强化学习(RL)进行精细调整\n\n## 4. 具体实现：从数据构建到训练策略\n\n**数据构建管道**：\n- 原始推理链生成：使用强大推理模型生成多样化候选\n- 两阶段过滤：准确性过滤和LLM质量评估确保数据完整性\n- 两阶段标记：探索性标记生成广泛标签，限制性标记确保一致性\n\n**训练策略**：\n- 基础SFT：在标记数据集上微调，建立结构化推理基础\n- 引导RL阶段：使用复合奖励信号（标签约束+个性化奖励模型PRMU）\n- 探索RL阶段：简化奖励信号，鼓励模型自由探索策略空间\n\n**个性化奖励模型(PRMU)**：引入可学习用户嵌入捕获个人偏好，提供细粒度奖励信号，实现与用户特定逻辑的精细对齐。\n\n## 5. 实验验证与效果评估\n\n**全面实验**：在LaMP基准和自构建数据集上评估，与多种基线方法比较。\n\n**关键结果**：\n- TagPR在所有任务上取得最先进结果，平均提升32.65%\n- 8B参数模型超越更大的专有模型（如GPT-4o、Gemini-2.5-Pro）\n- 消融研究验证各组件的必要性和协同效应\n\n**泛化能力**：在跨语言新基准上展示卓越零样本泛化性能，证明学习到可转移的个性化技能。\n\n## 6. 结论与启示\n\n**核心结论**：训练LLMs生成结构化、可解释的推理是释放真正个性化能力的有效途径。TagPR通过\"标记思想\"方法显著增强了模型内在个性化推理能力。\n\n**研究意义**：为构建更复杂、更符合用户需求的智能系统铺平道路，验证了结构化推理在个性化任务中的价值，为未来研究提供新方向。"
                },
                {
                    "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents",
                    "arxiv_id": "2509.23040",
                    "authors": "Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang",
                    "summary": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出ReMemR1（一种带有回调增强记忆的记忆增强LLM智能体）和多级奖励强化学习(RLMLR)训练方法，旨在解决LLM在长上下文推理中的挑战。从第一步核心判断来看，论文本质上是改进LLM的基础推理能力，特别是长上下文推理能力，而不是将LLM作为工具应用到特定领域。论文提出的记忆机制允许非线性推理和重新审视早期证据，这直接增强了LLM的通用推理能力。从第二步正面指标看，论文包含多个相关主题：大型语言模型(LLMs)、推理能力(reasoning)、强化学习(RL)和LLM智能体(llm-based agents)。从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的是一种通用的智能体框架来增强LLM的推理能力，而非应用于特定领域。综合分析，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在长上下文问答中面临的证据分散和信息丢失问题。针对数百万token的长文档场景，我们提出了一种ReMemR1方法，通过回调增强内存机制实现历史记忆检索和非线性推理，并在HotpotQA和2WikiMultiHopQA数据集上通过准确性指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models）在长上下文问答（long-context question answering）中面临挑战，因为查询的关键证据可能分散在数百万个标记（tokens）中。现有研究为大型语言模型配备了记忆语料库（memory corpus），该语料库在单次文档扫描过程中动态更新，这种方法也被称为\"边读边记\"（\"memorize while reading\"）方法。虽然这种方法能够高效扩展，但它存在不可逆的单向处理、通过覆盖导致的信息丢失以及稀疏的强化学习（reinforcement learning）信号等问题。\n\n为应对这些挑战，我们提出了ReMemR1，一种具有回调增强记忆（callback-enhanced memory）的记忆增强代理（memory-augmented agent），它允许从整个记忆历史中进行选择性检索，并支持非线性推理和重新审视早期证据。为进一步加强训练，我们提出了多级奖励强化学习（Reinforcement Learning with Multi-Level Rewards, RLMLR），该方法将最终答案奖励与密集的步骤级信号相结合，以指导有效使用记忆。总体而言，这些贡献减轻了信息退化，改进了监督，并支持多跳记忆利用（multi-hop memory utilizing）。在长文档问答（long-document QA）上的实验表明，与现有的基于记忆的方法相比，我们的方法取得了显著改进，这验证了ReMemR1作为长上下文推理代理（long-context reasoning agents）的有效解决方案。",
                    "inspiration_trace": "# 从宏观问题到创新方法：ReMemR1的逻辑演进\n\n## 宏观问题：长上下文推理的瓶颈\n\n大型语言模型(LLMs)在处理长文档推理任务时面临根本性挑战。在现实场景中（如法律文档分析或科学文献综述），回答单个查询的关键证据可能分散在数百万个token中，这使得模型难以有效跟踪长距离依赖关系并将分散信息综合成连贯答案。\n\n## 问题分解：现有方法的局限性\n\n作者首先审视了主流解决方案——\"边读边记\"(memorize while reading)范式，并识别出三个关键局限：\n\n### 1. 不可逆的仅向前处理\n**观察**：在多跳推理任务中，不同跳的证据可能需要同时使用。例如，在寻找第一跳证据时可能遇到第二跳证据，但由于问题尚未解决，代理无法识别其重要性。\n**问题**：随着记忆更新，这种未被识别的关键证据可能被永久遗忘，限制了代理整合分散信息的能力。\n\n### 2. 记忆覆盖导致的信息渐进丢失\n**观察**：固定长度记忆缓冲区需要不断压缩信息，早期关键细节（如\"Dr Aris Thorne曾在芝加哥做博士后\"）在多次更新后不可避免地丢失。\n**问题**：这种记忆退化使模型难以维持完整上下文，阻碍了合成分散在文档远端证据的能力。\n\n### 3. 稀疏且延迟的监督信号\n**观察**：现有方法通常仅使用最终答案正确性作为单一奖励信号。\n**问题**：这种稀疏奖励对长序列中间记忆更新提供有限指导，导致优化效率低下和次优的记忆管理策略。\n\n## 假设形成：突破性思考\n\n基于上述观察，作者提出三个核心假设：\n\n1. **非线性推理假设**：如果允许代理在需要时回溯和访问历史记忆，可以打破线性处理限制，实现更灵活的推理路径。\n\n2. **智能记忆管理假设**：通过设计更智能的记忆检索机制，可以减少关键信息的丢失，维持长期推理所需的关键事实。\n\n3. **密集监督假设**：如果在训练过程中提供更细粒度的中间步骤监督，可以更有效地优化记忆管理策略，加速学习过程。\n\n## 方法论构建：ReMemR1的诞生\n\n### 1. 突破状态表示限制\n**核心创新**：将传统MDP中的状态st = mt扩展为st = (mt, qt)，其中qt是回调查询。\n**实现机制**：代理不仅更新记忆mt，还生成回调查询qt+1来检索历史记忆{mi}i≤t，使状态转换变为：\n```\nst+1 = (mt+1, qt+1) = πθ(Q, ct, mt, E({mi}i≤t, qt))\n```\n**解决的问题**：使代理能够构建非线性推理路径，选择性重新整合早期证据，打破不可逆向前处理的限制。\n\n### 2. 设计检索增强的记忆机制\n**核心创新**：引入检索函数E，从历史记忆中选择与回调查询最相关的内容。\n**实现机制**：使用基于词重叠的启发式检索方法，E(X, b) = argmaxx∈X recall(b, x)，其中recall(a, b)表示a中出现在b中的词的比例。\n**解决的问题**：使代理能够在需要时检索和重新整合可能被覆盖的关键信息，减轻信息渐进丢失问题。\n\n### 3. 多级奖励强化学习框架\n**核心创新**：提出强化学习与多级奖励(RLMLR)框架，结合轨迹级结果奖励和密集的步骤级状态奖励。\n**实现机制**：\n- **轨迹级结果奖励**：基于最终答案正确性，Rout(g) = maxy∈Y I(ŷ(g) = y)\n- **步骤级状态奖励**：包括三部分\n  - 记忆更新信息增益：rmemory,t(g) = maxy∈Y recall(mt(g), y) - maxy∈Y recall(mt-1(g), y)\n  - 回调检索奖励：rcallback,t(g) = maxy∈Y recall(y, E({mi(g)}i≤t, qt(g)) ∪ mt(g) ∪ ct) - maxy∈Y recall(y, mt(g) ∪ ct)\n  - 格式奖励：确保正确使用<callback>和<memory>标签\n- **综合优势计算**：Ât(g) = αÂout(g) + (1-α)Âstate,t(g)\n\n**解决的问题**：提供密集的细粒度监督，缓解传统仅结果RL的稀疏性问题，有效塑造中间行为。\n\n## 实验验证与确认\n\n通过系统性实验，作者验证了ReMemR1的有效性：\n\n1. **性能验证**：在HotpotQA和2WikiMultiHopQA数据集上，ReMemR1在所有模型规模和上下文长度上均优于现有方法，特别是在超长上下文(6400文档)情况下仍保持稳定性能。\n\n2. **消融研究**：\n   - RLMLR的有效性：α=0.8时性能最佳，证明结合结果奖励和步骤级奖励的必要性\n   - RL驱动vs规则回调：RL驱动的回调明显优于使用问题本身作为固定查询的规则方法\n\n3. **远距离证据挑战**：在特意设计的远距离证据设置下，ReMemR1显著优于基线，证实了其非线性文档利用能力。\n\n## 结论：从问题到解决方案的完整逻辑链\n\n作者从长上下文推理的宏观挑战出发，通过系统分析现有方法的局限性，形成了三个核心假设，并逐步构建出ReMemR1这一创新解决方案。这一方法通过引入历史增强的状态表示、智能记忆检索机制和多级奖励强化学习框架，有效解决了不可逆向前处理、信息渐进丢失和稀疏监督信号三大关键挑战，为长上下文LLM代理提供了新的发展方向。整个逻辑演进体现了从问题观察到假设形成，再到方法论构建和实验验证的完整科学研究过程。"
                },
                {
                    "title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents",
                    "arxiv_id": "2509.23071",
                    "authors": "Muzhi Li, Jinhu Qi, Yihong Wu, Minghao Zhao, Liheng Ma, Yifan Li, Xinyu Wang, Yingxue Zhang, Ho-fung Leung, Irwin King",
                    "summary": "Retrieval-augmented generation agents development is hindered by the lack of process-level supervision to effectively guide agentic capabilities like task decomposition, retriever invocation, and stepwise decision-making. While reinforcement learning offers a potential solution, it suffers from sparse rewards and the limited reasoning capabilities of large language models (LLMs). Meanwhile, existing data synthesis methods only produce chain-of-thought rationales and fail to model environmental interactions. In this paper, we propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent development. EviPath comprises: (i) Abductive Subtask Planning, which decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; (ii) Faithful Sub-question Answering, which uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question; and (iii) Conversational Fine-Tuning, which formats the complete agent-environment interaction trajectory into a dialogue format suitable for Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data. Extensive experiments on widely-used question-answering benchmarks show that an 8B parameter model trained with EviPath-synthesized data significantly and consistently outperforms state-of-the-art baselines with a double-digit absolute EM gain of 14.7% in open-domain question answering.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，该论文的本质是提出一种新的训练范式(EviPath)来增强大语言模型的推理能力和工具使用能力。论文关注的是如何通过合成数据和特定的训练方法来提升LLM的通用推理能力，包括任务分解、检索器调用和逐步决策等智能体能力。这明显是关于改进LLM基础能力和通用推理能力的研究，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标主题： - 核心概念：明确讨论large language models (LLMs) - 能力方向：聚焦于溯因推理(Abductive Reasoning)、推理路径合成、任务分解和逐步决策等推理和规划能力 - 新兴范式：研究检索增强生成智能体(RAG agents)和工具使用能力(tool-use capabilities) 第三，论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等），而是提出通用方法 - 不讨论模型可靠性方面的水印、安全等问题 最后，关于智能体/工具使用的特殊处理，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此符合保留标准。 综上所述，该论文的核心贡献是提出一种新的证据锚定推理路径合成范式，用于提升大语言模型的通用推理能力和工具使用能力，完全符合研究目标。",
                    "summary2": "本文旨在解决RAG智能体开发中缺乏过程级监督的问题。针对多跳问答任务，我们提出了一种基于溯因推理的EviPath框架，通过证据锚定的推理路径合成来训练智能体。在HotpotQA、MuSiQue和2WikiMultihopQA等基准测试上，通过EM和F1指标验证了其有效性，8B参数模型实现了14.7%的绝对EM增益，显著优于现有基线方法。",
                    "summary_translation": "检索增强生成（Retrieval-augmented generation）智能体开发受到缺乏过程级监督（process-level supervision）的阻碍，这种监督能够有效指导任务分解（task decomposition）、检索器调用（retriever invocation）和逐步决策（stepwise decision-making）等智能体能力。虽然强化学习（reinforcement learning）提供了一个潜在的解决方案，但它面临稀疏奖励（sparse rewards）和大型语言模型（large language models, LLMs）推理能力有限的问题。与此同时，现有的数据合成（data synthesis）方法仅产生思维链（chain-of-thought）推理过程，而无法对环境交互（environmental interactions）进行建模。\n\n在本文中，我们提出了EviPath，一种用于RAG智能体开发的证据锚定推理路径合成（evidence-anchored reasoning path synthesis）范式。EviPath包括：（i）溯因式子任务规划（Abductive Subtask Planning），它将问题分解为子问题，并根据它们之间的依赖关系迭代规划最优解决方案路径；（ii）忠实子问题回答（Faithful Sub-question Answering），它使用支持证据构建代理环境（proxy environment）以生成每个子问题的推理思路和答案；以及（iii）对话式微调（Conversational Fine-Tuning），它将完整的智能体-环境交互轨迹（agent-environment interaction trajectory）格式化为适合监督微调（Supervised Fine-Tuning）的对话格式。EviPath使大型语言模型（LLMs）能够直接从合成数据中学习复杂推理和工具使用（tool-use）能力。\n\n在广泛使用的问答基准（question-answering benchmarks）上进行的大量实验表明，使用EviPath合成数据训练的8B参数模型在开放域问答（open-domain question answering）任务上显著且一致地优于最先进的基线方法（state-of-the-art baselines），实现了14.7%的双位数绝对EM（Exact Match，精确匹配）增益。",
                    "inspiration_trace": "# 从问题到解决方案：EviPath方法的逻辑演进\n\n## 1. 宏观问题识别：RAG智能体的过程监督缺失\n\n**观察现象**：检索增强生成(RAG)智能体在解决复杂多跳问题时表现不佳，尽管它们具备自主收集外部知识的能力。\n\n**核心问题**：现有数据集只提供最终答案和支持事实，缺乏**过程级监督信号**，无法有效训练智能体的关键能力：\n- 任务分解\n- 检索器调用\n- 逐步决策制定\n\n这一根本性限制导致RAG智能体在实际应用中无法提供可靠性能。\n\n## 2. 现有方法分析：两种主流范式的局限性\n\n### 2.1 强化学习方法的问题\n\n**假设**：通过结果奖励优化决策过程可以训练RAG智能体。\n\n**发现局限**：\n- 奖励信号稀疏且延迟，难以将功劳分配给单个决策\n- RAG智能体包含检索器等不可微分组件，端到端梯度反向传播不可行\n- 严重依赖模型内在推理能力，缺乏先验知识时无法发现正确动作\n\n### 2.2 数据合成方法的问题\n\n**假设**：合成思维链(CoT)理由可以提供足够的训练信号。\n\n**发现局限**：\n- 生成的路径通常是事后解释，而非真实的问题解决过程\n- 无法模拟与外部环境的交互，缺乏智能体核心能力训练\n\n## 3. 新视角提出：溯因推理的引入\n\n**关键洞察**：将推理路径合成重新定义为**溯因推理问题**——从观察到的结果(答案)反向推导出最可能的推理过程。\n\n**理论依据**：\n- 溯因推理适合从结果推断原因\n- 可以利用最终答案和支持证据作为约束，反向工程出最优推理轨迹\n- 这种方法能生成结构化的、交互式的、面向目标的推理路径\n\n**创新点**：首次将RAG智能体的推理路径合成表述为溯因推理问题，为生成过程监督信号提供了新思路。\n\n## 4. 方法设计：EviPath框架的构建\n\n基于溯因推理视角，设计了与RAG智能体Planner-Executor架构一致的三阶段框架：\n\n### 4.1 溯因子任务规划(ASP)\n\n**核心思想**：从答案反向推导任务分解和执行路径。\n\n**实现机制**：\n- 分析最终答案和证据间依赖关系，反向工程推理图\n- 将推理图线性化为子问题序列，创建\"黄金\"计划\n- 模拟智能体迭代执行过程，生成思想和检索查询\n\n### 4.2 忠实子问题回答(FSA)\n\n**核心思想**：构建模拟环境确保推理忠实性。\n\n**实现机制**：\n- 利用完整支持事实作为稳定知识库，避免实际检索错误\n- 三步过程：推导中间答案→识别黄金证据→生成基于证据的推理思想\n- 在包含噪声的上下文中训练，增强模型的抗干扰能力\n\n### 4.3 对话微调(CFT)\n\n**核心思想**：将完整轨迹转换为适合监督学习的格式。\n\n**实现机制**：\n- 将推理路径格式化为用户-助手对话形式\n- 同时训练规划者(高级规划)和执行者(子问题回答)能力\n- 通过单一优化目标联合学习所有组件\n\n## 5. 验证与意义：从理论到实践\n\n**实验验证**：\n- 在三个多跳问答数据集上，8B参数模型显著超越所有基线\n- 实现14.7%的绝对EM增益，证明方法有效性\n- 消融研究确认各组件的必要性\n\n**核心贡献**：\n- 建立了数据驱动的RAG智能体开发新范式\n- 解决了数据稀缺和依赖复杂强化学习的双重挑战\n- 证明高质量过程监督信号比模型规模和学习算法更关键\n\n## 6. 逻辑演进总结\n\n从识别RAG智能体缺乏过程监督的根本问题出发，通过分析现有方法的局限性，引入溯因推理的新视角，最终设计出EviPath框架，实现了从证据到轨迹的完整推理路径合成。这一演进过程体现了从问题观察到理论创新，再到方法设计和实验验证的完整科研逻辑，为RAG智能体开发提供了新的有效路径。"
                },
                {
                    "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks",
                    "arxiv_id": "2509.23067",
                    "authors": "Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo",
                    "summary": "The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是提出一种名为\"语义投票\"(Semantic Voting)的新方法，用于大语言模型在不可验证的开放性任务上的自我改进。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。论文关注的是LLM的通用能力提升，特别是通过自我改进机制来增强模型性能，符合保留标准。 第二步正面指标：论文明确包含核心概念\"Large language models, LLMs\"；虽然未直接强调推理能力，但开放性任务通常需要推理能力，且论文提出的语义投票方法可以提升LLM在这些任务上的表现；论文关注自我改进(self-improvement)，这与自我进化(self-evolve)概念相关，属于训练方法的范畴。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，完全避开了这些排除标准。 第四步特殊和模糊情况：论文提到自我评估机制的\"过度自信问题\"和\"内在偏差\"，这与幻觉问题相关。论文提出的方法旨在减轻这些问题，从而提升模型的通用可靠性和推理质量，符合保留标准。 最终决策：论文的核心贡献是提出了一种通用的自我改进方法，通过语义投票机制提升LLM在开放性任务上的表现，减轻了传统自我评估方法的计算负担和内在偏差。这直接关系到提升LLM的通用推理能力，特别是在处理需要复杂推理的开放性任务时。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大型语言模型在不可验证开放任务上自我改进的计算开销大和过度自信问题。针对翻译和摘要等开放性任务，我们提出了一种语义投票方法，通过轻量级句子嵌入模型实现软匹配，替代传统的自我评估机制。在WMT24++翻译数据集和CNN/Dailymail、PubMed摘要数据集上，通过BLEU、n-MQM、Rouge-L和BLEURT等指标验证了其有效性，实现了计算效率显著提升(仅为自我评估方法的0.1%-5%)的同时保持或提高了模型性能。",
                    "summary_translation": "获取监督数据（supervised data）的成本不断上升，推动了对大型语言模型（large language models, LLMs）自我改进（self-improvement）的浓厚兴趣。像多数投票（majority voting）这样简单的无监督信号（unsupervised signals）已被证明在生成可验证任务（verifiable tasks）的伪标签（pseudo-labels）方面有效，而它们对于不可验证任务（unverifiable tasks）（如翻译）的适用性则受到响应开放性特征的限制。因此，自我评估机制（self-evaluation mechanisms）（如自我判断（self-judging）和熵最小化（entropy minimization））主要用于推导伪标签。然而，依赖大型语言模型（LLMs）的自我评估通常会导致高计算开销，并由于内在偏差（intrinsic biases）而引入过度自信（overconfidence）问题。为应对这些挑战，我们提出了一种针对不可验证任务的新型无自我评估（self-evaluation-free）方法，专为轻量级但有效的自我改进而设计。受可验证任务中常用的多数投票启发，我们提出了语义投票（semantic voting）作为一种新机制，该机制将硬匹配（hard matching）（即精确匹配（exact matching））的原则放宽为软匹配（soft matching）（即语义相似性（semantic similarity））。软匹配通过利用轻量级句子嵌入模型（sentence embedding model）来量化语义相似性，从而减轻了自我评估的过度计算负担和与内在偏差相关的限制。综合实验表明，我们的方法在计算效率方面取得了显著提升，并在多种模型架构和任务上整体表现优于自我评估方法。",
                    "inspiration_trace": "# 从问题到解决方案：语义投票的逻辑演进\n\n## 宏观问题：监督数据的高昂成本\n\n**起点：** 获取高质量监督数据的成本不断攀升，专家注释的推理轨迹、细粒度人类反馈等方法需要大量人力投入，且泛化性有限。\n\n**观察：** 这促使研究人员探索LLM自我改进的可能性，通过无监督或自监督方式生成伪标签，减少对外部昂贵监督的依赖。\n\n## 现有方法的局限与启示\n\n### 多数投票的有效性与局限性\n\n**观察：** 多数投票在可验证任务(如算术问题、选择题)上表现优异，通过选择最频繁出现的答案作为伪标签。\n  \n**局限：** 多数投票依赖精确匹配(hard matching)，无法适用于翻译、摘要等不可验证的开放性任务，这些任务存在多个语义等效但表达不同的答案。\n\n### 自我评估方法的缺陷\n\n**观察：** 为解决不可验证任务的挑战，研究者开发了自我评估方法，主要包括：\n- Self-judging：利用\"LLM-as-a-Judge\"范式，模型评估自身生成的回答\n- Entropy Minimization：通过Shannon熵等指标估计输出置信度，优先选择低熵(高置信度)输出\n\n**局限：** \n- 计算开销大：需要额外推理步骤\n- 引入偏差：自我偏好偏差导致过度自信问题\n- 不一致性：在不同模型和任务上表现不稳定\n\n## 核心洞察与假设\n\n**关键观察：** 尽管自我评估方法试图构建精细的伪反馈，但研究表明LLM即使从噪声标签中也能有效学习，这引发了对精细伪反馈必要性的质疑。\n\n**核心假设：** \n1. 语义共识可替代精确匹配作为质量指标\n2. 在多个生成的回答中，与其他回答语义更相似的回答质量更高\n3. 轻量级句子嵌入模型足以捕捉语义相似性，无需昂贵的自我评估\n\n## 方法创新：语义投票\n\n### 从硬匹配到软匹配\n\n**创新点：** 放宽多数投票的精确匹配要求，转向基于语义相似性的软匹配(soft matching)。\n\n**实现：** \n```\nS_sv(a_j^i|A_i) = 1/|A_i| * Σ f_sim(a_j^i, a_k^i)\n```\n其中f_sim使用句子嵌入的余弦相似度计算语义相似性。\n\n### 解决噪声与异常值问题\n\n**观察：** 随机生成的候选回答可能包含语义偏离的异常值，影响投票结果。\n\n**解决方案：** 在语义投票前引入聚类过滤步骤，仅保留最大聚类内的回答：\n```\nC_max^i = arg max_k |C_k^i|\nS_sv(a_j^i|C_max^i) = 1/|C_max^i| * Σ f_sim(a_j^i, a_k^i)\n```\n\n### 构建训练信号\n\n**实现：** 选择语义投票得分最高和最低的回答构建偏好对，使用DPO(直接偏好优化)训练模型。\n\n**选择理由：** DPO对噪声标签具有鲁棒性，适合处理不可避免的伪信号噪声。\n\n## 实验验证与优化\n\n### 多维度验证\n\n**任务选择：** 翻译(WMT24++)和摘要(CNN/Dailymail, PubMed)\n**评估指标：** 结合词汇指标(BLEU, ROUGE-L)和语义指标(n-MQM, BLEURT)\n**基线对比：** 与Self-judging和Entropy Minimization方法全面比较\n\n### 关键实验设计\n\n1. **反转实验：** 验证语义投票信号的有效性，排除\"虚假奖励\"可能性\n2. **扩展性实验：** 测试与其他训练范式(GRPO)的兼容性\n3. **消融实验：** 验证聚类和语义投票各组件的必要性\n4. **超参数研究：** 探索生成参数和聚类参数的影响\n\n## 结论与贡献\n\n**核心贡献：** 提出一种高效、无需自我评估的LLM自我改进方法，在不可验证的开放性任务上实现与自我评估方法相当或更好的性能，同时将计算开销降低至原来的0.1%-5%。\n\n**方法论意义：** 证明语义共识是开放性任务中有效的质量指标，为LLM自我改进提供了简单、有效且可扩展的新范式。"
                },
                {
                    "title": "HEART: Emotionally-driven test-time scaling of Language Models",
                    "arxiv_id": "2509.22876",
                    "authors": "Gabriela Pinto, Palash Goyal, Yiwen Song, Souradip Chakraborty, Zifeng Wang, Tomas Pfister, Hamid Palangi",
                    "summary": "Test-time scaling has shown considerable success in improving the performance of language models on complex reasoning tasks without requiring fine-tuning. However, current strategies such as self-reflection primarily focus on logical or structural refinement. They do not leverage the guiding potential of affective feedback. Inspired by psychological research showing that emotions can modulate cognitive performance, we introduce HEART--a novel framework that uses emotionally-driven prompts for iterative self-correction. HEART provides feedback on a model's incorrect response using a curated set of concise, emotionally charged phrases based on the six universal emotions categorized by Dr. Paul Ekman. By systematically varying the emotional tone of the feedback across iterations, our method guides the model to escape flawed reasoning paths and explore more promising alternatives. We evaluate our framework on challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam, and SimpleQA. Our results reveal a significant new phenomenon: when guided by an oracle verifier, this affective iteration protocol unlocks significantly deeper reasoning, leading to consistent and substantial increases in accuracy over state-of-the-art baselines with the same verifier. However, we also identify a critical bottleneck for practical deployment. In a verifier-free setting, it struggles to harness these gains consistently, highlighting as a key challenge for future work. Our findings suggest that the next frontier in machine reasoning may lie not just in refining logic, but also in understanding and leveraging the `HEART' of the models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为HEART的新框架，通过情感驱动的提示进行迭代自我修正，以提高大语言模型在复杂推理任务上的表现。该方法基于心理学研究，使用情感化的反馈来引导模型摆脱有缺陷的推理路径，探索更有希望的替代方案。论文在多个具有挑战性的推理基准上评估了该方法，结果表明当由验证器引导时，这种情感迭代协议能解锁更深层次的推理，显著提高准确性。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为论文关注的是改进LLM的基础推理能力，提出了一种新的测试时扩展范式，而不是将LLM作为工具应用到特定领域。论文的主题与正面指标中的\"大语言模型\"和\"推理能力\"高度吻合，同时不涉及任何排除标准中的领域。",
                    "summary2": "本文旨在解决语言模型测试时扩展中缺乏情感反馈的问题。针对复杂推理任务，我们提出了HEART框架，利用情感提示进行迭代自我修正，并在多个推理基准上通过准确性验证了其有效性。",
                    "summary_translation": "测试时扩展(Test-time scaling)在无需微调的情况下，显著提高了语言模型在复杂推理任务上的性能，已取得相当大的成功。然而，当前策略如自我反思(self-reflection)主要集中于逻辑或结构上的优化，并未利用情感反馈(affective feedback)的指导潜力。受心理学研究启发——研究表明情绪能够调节认知表现——我们提出了HEART（一种利用情感驱动提示进行迭代自我修正的新型框架）。HEART使用一套精心策划的简洁情感短语，基于Paul Ekman博士分类的六种普遍情绪，为模型的错误响应提供反馈。通过在迭代过程中系统性地改变反馈的情感基调，我们的方法引导模型摆脱有缺陷的推理路径，探索更有前景的替代方案。\n\n我们在具有挑战性的推理基准测试上评估了我们的框架，包括OlympiadBench、Humanity's Last Exam和SimpleQA。我们的结果揭示了一个重要的新现象：当由oracle verifier（神谕验证器）引导时，这种情感迭代协议能够解锁显著更深入的推理能力，导致在使用相同验证器的情况下，准确度相对于最先进的基线方法有了持续且大幅的提升。然而，我们也识别出了实际部署中的一个关键瓶颈。在无验证器(verifier-free)的设置中，该方法难以稳定地利用这些增益，这突显了未来工作的一个关键挑战。我们的研究结果表明，机器推理的下一个前沿可能不仅在于完善逻辑，还在于理解和利用模型的\"HEART\"（情感核心）。",
                    "inspiration_trace": "# HEART方法逻辑链推演：从问题洞察到方法论创新\n\n## 1. 宏观问题：语言模型推理能力的局限性\n\n作者从大型语言模型(LLMs)的一个根本性问题出发：尽管LLMs展示了显著能力，但在复杂推理任务中实现可靠、准确的推理仍是一个核心挑战。这一观察构成了研究的起点，引导作者思考如何提升模型的推理能力。\n\n## 2. 现有方法的双向观察与局限分析\n\n作者系统考察了提升LLMs推理能力的两类主流方法，并识别出它们的互补性局限：\n\n### 2.1 结构化推理方法\n- **代表技术**：Chain-of-Thought (CoT)及其变体\n- **核心机制**：通过外部化推理过程，在模型输出上施加逻辑框架\n- **关键优势**：程序上稳健，提供清晰的逻辑路径\n- **根本局限**：作者敏锐地将其描述为\"affectively sterile\"（情感上贫乏），缺乏驱动高质量人类推理的动机背景\n\n### 2.2 情感提示方法\n- **代表技术**：EmotionPrompt等\n- **核心机制**：通过点燃模型的\"认知状态\"和引导其焦点来提升性能\n- **关键优势**：动机上强大，能激发模型潜能\n- **根本局限**：结构上不精确，作为\"一次性\"全局刺激，缺乏引导多步自我修正过程所需的有针对性指导\n\n## 3. 关键洞察：情感与认知的整合视角\n\n作者从认知科学研究中获得关键启示：情感不是认知的障碍，而是不可或缺的组成部分，塑造注意力、动机和问题解决。这一洞察成为连接两类方法的桥梁，启发了一个核心问题：如何将情感反馈整合到迭代自我修正过程中？\n\n理论基础包括：\n- Paul Ekman的六种基本情绪理论（快乐、悲伤、惊讶、愤怒、恐惧、厌恶）\n- 对立过程理论(opponent-process theory)：情感可以触发纠正性认知状态\n\n## 4. 核心假设的形成\n\n基于以上观察和洞察，作者形成了核心假设：**有针对性的情感反馈可以触发纠正性认知状态**，帮助模型放弃有缺陷的推理路径并尝试新的方法。例如，表达对答案\"失望\"的提示可以激励模型放弃其有缺陷的推理，类似于人类如何利用不满来重新开始解决问题的努力。\n\n这一假设将情感从简单的性能增强器转变为推理过程的引导机制，标志着研究思路的重要转变。\n\n## 5. 方法设计：HEART框架的构建\n\n基于核心假设，作者设计了HEART框架，包含两个主要组件：\n\n### 5.1 情感提示构建（Affective Cue Prompts）\n- **理论基础**：基于Ekman的六种基本情绪\n- **设计方法**：精心设计30个情感提示，每种情绪5个不同表达\n- **质量控制**：先由强大LLM生成，再由人类研究人员手动优化，确保情感表达的多样性和自然性\n\n### 5.2 情感迭代协议（HEART Protocol）\n- **迭代机制**：从标准CoT响应开始，如果初始响应不正确，启动一系列修正尝试\n- **动态情感序列**：受对立过程理论启发，在迭代中交替使用积极和消极情感组\n- **候选生成**：在每个迭代中，使用当前激活的情感组中的每个AC-Prompt作为反馈生成新的候选答案\n- **候选解析**：设计两种场景\n  - S1（Oracle Selection）：理想化场景，假设访问全知验证器\n  - S2（Generative Synthesis）：现实场景，使用生成式集成器合成新答案\n\n## 6. 实验验证与关键发现\n\n作者通过精心设计的实验验证了HEART的有效性，并获得了重要发现：\n\n### 6.1 Oracle-guided场景下的显著效果\n- HEART一致且显著地优于所有基线方法\n- 证明动态情感提示在引导模型生成正确解决方案方面非常有效\n- 例如，在Humanity's Last Exam上，Deepseek-R1与HEART达到84.16%的准确率，显著高于基线\n\n### 6.2 Verifier-free场景下的关键瓶颈\n- HEART在无验证器场景下表现不如Oracle场景\n- 揭示了实际瓶颈：模型在合成过程中选择正确推理路径的能力有限\n- 这一发现指明了未来研究方向：改进自主选择机制\n\n### 6.3 动态情感序列的关键作用\n- 通过消融研究，作者发现动态情感序列是性能提升的主要驱动因素\n- 交替使用积极和消极提示提供了更强大的动机循环，防止模型陷入单一思维模式\n\n## 7. 理论贡献与实践意义\n\n### 7.1 理论贡献\n- 首次将情感反馈整合到迭代自我修正过程中\n- 提供了动态、迭代的情感提示可以显著改善推理和自我修正的第一个有力证据\n- 确定了选择机制作为关键瓶颈，为未来研究指明方向\n\n### 7.2 实践意义\n- 为更自然、协作的人机系统铺平道路\n- 可能在个性化教育、创意协作写作等领域解锁新能力\n- 建议未来研究方向应超越纯逻辑，走向更全面、与人类对齐的模型\n\n## 8. 伦理考量与未来方向\n\n作者也考虑了伦理问题，承认使用苛刻语言的含义，并强调这些提示严格作为诊断工具，目的是理解模型机制，而不是认可或规范有害的交互模式。\n\n未来工作方向包括：\n- 增强核心算法：用自适应选择模型替换预定义的情感序列\n- 扩展应用范围：将框架扩展到多模态LLMs，探索更广泛的评估领域\n\n## 总结\n\nHEART方法的逻辑演进展现了从宏观问题（LLMs推理能力限制）到具体方法论的完整思考过程。作者通过观察现有方法的互补性局限，从认知科学中获取灵感，形成核心假设，设计并验证了创新框架。这一过程不仅解决了技术问题，还开辟了将情感与认知整合的新研究方向，体现了跨学科思维在AI研究中的价值。"
                },
                {
                    "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?",
                    "arxiv_id": "2509.22947",
                    "authors": "Mohammed Sabry, Anya Belz",
                    "summary": "Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它专注于提升大语言模型的基础通用推理能力，而非将LLM作为工具应用于特定领域。 从第一步核心判断来看，论文的本质是研究如何通过预训练数据的设计来增强大语言模型的上下文学习(ICL)能力。ICL是大语言模型的一种核心通用推理能力，允许模型通过少量示例学习新任务。论文提出了Bi-Induct方法，通过在预训练流中注入目标合成数据来研究归纳电路的发展，这属于改进LLM基础能力的研究范畴，而非将LLM应用于特定领域。 从第二步正面指标看，论文明确包含核心概念\"Large language models\"，并研究\"reasoning\"能力方向中的上下文学习(ICL)，这是大语言模型进行多步推理和问题解决的基础能力。 从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，因此不符合任何排除标准。 论文的核心贡献在于探索了预训练数据设计对LLM通用推理能力(ICL)的影响，挑战了早期归纳电路激活直接改善ICL的假设，并提出了机制感知的预训练诊断方法。这些研究直接关注如何提升大语言模型本身的通用推理能力，符合我的研究目标。",
                    "summary2": "本文旨在解决在相同计算预算下，自然文本预训练与加入目标合成示例哪种更有利于上下文学习(ICL)的问题。针对Transformer模型的归纳头(induction heads)激活，我们提出了一种Bi-Induct方法，在预训练流中注入前向或后向复制片段，并在0.13B到1B参数模型上通过ICL基准性能、头级别遥测和语言建模困惑度验证了其有效性。实验发现，尽管Bi-Induct加速了小规模模型归纳头的出现，但并未一致提升ICL性能，而自然文本训练产生了更具功能必要性的归纳电路。",
                    "summary_translation": "在预训练过程中显式训练归纳电路(induction circuit，归纳推理电路)是否能改善上下文学习(in-context learning, ICL)，或者在计算量保持不变(iso-FLOPs，等浮点运算数)的情况下，自然文本是否足够？为了测试目标合成数据能否加速归纳头(induction heads，归纳推理头)的出现并增强ICL，我们引入了Bi-Induct，这是一种轻量级课程，将前向复制(Induction，归纳)、后向复制(Anti，反归纳)或平衡混合注入预训练流。我们在iso-FLOPs条件下训练了从0.13B到1B参数的模型，评估了(i)少样本ICL基准，(ii)头部级别遥测(head-level telemetry)，和(iii)保留语言建模困惑度(held-out language modeling perplexity)。我们的发现挑战了早期归纳电路激活直接改善ICL的假设。虽然Bi-Induct在小规模上加速了归纳头的出现，但这并不总能产生更强的泛化能力。在标准LM基准上，Bi-Induct与仅自然文本训练表现相当；在函数式ICL探测(function-style ICL probes)上，1B的仅自然文本模型表现最佳。压力测试(如标签置换、HITS@1 vs. HITS@3、1 vs. 10 shots)保持了这些趋势。遥测显示较大的仅自然文本模型在没有显式归纳模式的情况下发展出更广泛、更早的归纳头。反归纳数据未能引发有意义的激活。合成数据带来的困惑度惩罚随规模增大而缩小，表明更大的模型能够以最小代价吸收非自然模式。关键的是，消融(ablating)顶部2%的归纳头比随机消融更能降低ICL性能，特别是对于仅自然文本模型，表明存在更集中、承重的电路。Bi-Induct变体表现出更多冗余的归纳活动，意味着不同的电路利用方式。总体而言，诱导激活并不足够：ICL的增益取决于这些电路变得功能上必要。这些结果强调了机制感知的预训练诊断和能够培养承重结构而不仅仅是存在结构的数据混合。",
                    "inspiration_trace": "# 从宏观问题到方法创新：Bi-Induct的逻辑演进\n\n## 1. 宏观问题的提出\n\n**核心问题**：在有限计算预算下，如何最有效地提升大型语言模型的上下文学习(ICL)能力？\n\n这一问题源于实际约束与理论挑战的交汇点：ICL能力对Transformer模型至关重要，但其发展通常需要海量计算资源和训练数据。在现实场景中，计算预算往往是固定的，因此如何在相同计算条件下最大化ICL性能成为一个关键研究问题。\n\n## 2. 关键观察与背景分析\n\n### 观察一：ICL的神经机制基础\n- 研究发现Transformer模型会发展出特定的\"感应头\"(induction heads)结构\n- 这些感应头形成两头部 motif，实现重复模式匹配：当token A在上下文中重现时，模型会增加预测之前A之后出现的token的概率\n- 这种机制被证实与ICL能力密切相关，特别是在模式匹配任务上\n\n### 观察二：感应头发展的延迟性\n- 感应头通常在训练后期才出现，需经过数十亿token训练\n- 这一延迟表现为训练损失的第一个平台期后的相变过程\n- 这种延迟意味着计算资源的低效利用——模型在前期训练中并未发展关键能力\n\n### 观察三：现有方法的局限性\n- 已有尝试通过修改目标函数(如多token预测)或使用特定任务数据来缩短ICL平台期\n- 但这些研究要么专注于纯合成任务训练，要么依赖架构/目标级别的干预\n- 缺乏在自然语言预训练环境中，通过数据级别干预的系统研究\n\n## 3. 核心假设的形成\n\n基于以上观察，作者提出核心假设：\n\n**假设**：在相同计算预算(iso-FLOPs)下，通过在预训练数据中有针对性地注入合成示例来明确锻炼感应电路，相比仅使用自然文本预训练，能更有效地加速感应头出现并增强ICL性能。\n\n这一假设基于以下推理链：\n1. 如果感应头是ICL的关键机制，那么直接训练模型执行感应任务应加速这些结构形成\n2. 数据级别干预比架构/目标级别干预更容易大规模部署\n3. 精心设计的合成示例可在最小化自然语言建模能力负面影响的同时，提供明确的感应信号\n\n## 4. 方法设计：Bi-Induct的诞生\n\n### 设计原则\n- **针对性**：合成示例应直接锻炼感应机制的核心功能\n- **轻量级**：仅替换小部分自然token，保持自然分布主导\n- **可扩展**：方法应易于在不同模型规模上实现和比较\n- **可控性**：引入明确的对照组以分离变量效应\n\n### 具体实现\n\n#### 4.1 合成片段构造\n设计三种类型的定向复制片段：\n- **前向复制(Induction)**：序列后跟相同序列(如\"ABCDE ABCDE\")\n- **后向复制(Anti-induction)**：序列后跟其反转(如\"ABCDE EDCBA\")\n- **平衡混合(Balanced)**：随机选择前向或后向复制\n\n这种设计直接针对感应电路的核心功能，同时提供控制条件(后向复制作为对照)。\n\n#### 4.2 课程安排策略\n- **线性退火注入**：注入概率随时间从初始值线性衰减至零\n- **前期加载**：在训练早期集中注入复制提示，对应感应头通常出现的阶段\n- **保持主导**：确保自然文本在整体训练数据中保持主导地位\n\n#### 4.3 实验验证框架\n设计多维度评估体系：\n1. **行为评估**：标准ICL基准和函数式探针的少样本性能\n2. **机制评估**：头部级别遥测技术，测量感应和反感应复制分数\n3. **质量保障**：保留数据集上的困惑度评估\n4. **消融研究**：通过消融顶部感应头验证其对ICL的贡献\n\n## 5. 预期与发现的对比\n\n### 预期结果\n1. Bi-Induct将加速感应头出现，特别是在较小规模模型中\n2. 这种加速将转化为更好的ICL性能\n3. 合成数据注入不会显著损害整体语言建模质量\n\n### 实际发现\n1. **部分验证**：Bi-Induct确实加速了感应头出现，在0.13B和0.5B模型中更明显\n2. **关键差异**：早期激活并不一致转化为更好的少样本泛化能力\n3. **规模效应**：在1B模型上，纯自然文本基线在函数式ICL探针上表现最佳\n4. **机制差异**：自然数据发展出\"承载负载\"的感应头，而Bi-Induct产生更分布式、冗余的感应活动\n5. **鲁棒性**：压力测试(标签置换、不同shot数、HITS@1 vs HITS@3)保持了这些趋势\n\n## 6. 理论演进与最终洞见\n\n### 理论调整\n基于实验发现，作者对初始理论进行了关键修正：\n\n**初始观点**：加速感应头出现是提升ICL的关键\n**修正观点**：感应头变得\"功能上必要\"比\"早期出现\"更重要\n\n### 核心洞见\n1. **存在性与必要性的区别**：感应头的存在不等于它们对模型功能至关重要\n2. **电路质量优于数量**：自然训练发展出更集中、承载负载的感应电路，而合成数据诱导出更分散、冗余的电路\n3. **规模依赖性**：较小模型从合成数据中受益更多，而较大模型能更有效地从自然文本中提取关键模式\n4. **路径转移**：随着模型规模增大，预测质量越来越多地通过FFN/残差路径路由，减少对局部高分数感应头的依赖\n\n## 7. 方法论贡献与启示\n\n### 方法论贡献\nBi-Induct作为一种机制感知的数据重写方法，提供了：\n- 一种轻量级、可扩展的预训练干预策略\n- 将机制解释学见解转化为实际训练方法的范例\n- 系统评估合成数据对特定电路发展影响的框架\n\n### 研究启示\n1. **超越加速**：未来工作应关注如何使诱导的电路变得功能上必要，而不仅仅是早期出现\n2. **机制感知诊断**：需要发展更精细的工具来评估电路的功能必要性，而非仅测量其存在\n3. **数据设计原则**：预训练数据混合应 fostering 承载负载的结构，而非仅促进存在性结构\n\n这一逻辑演进展示了从宏观问题到具体方法，再到理论修正的完整科研思维过程，体现了假设驱动研究与实证发现的有机结合。"
                },
                {
                    "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
                    "arxiv_id": "2509.22887",
                    "authors": "EunJeong Hwang, Yuwei Yin, Giuseppe Carenini, Peter West, Vered Shwartz",
                    "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是提升LLM的通用推理能力，特别是社交推理能力。论文提出ToMAgent (ToMA)，通过将心智理论(ToM)与对话前瞻配对来训练模型，这是一种新的训练范式，旨在增强LLM的基础推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确聚焦于\"LLM-based social agents\" - 能力方向：涉及推理能力，特别是社交推理和目标导向的推理行为 - 新兴范式：涉及LLM智能体框架 第三步排除标准：论文不符合任何排除标准。虽然涉及社交智能，但这是在提升LLM的通用推理能力，而非将LLM应用到特定社会学研究领域。社交智能是人类智能的核心组成部分，提升LLM的社交推理能力属于提升其通用推理能力的范畴。 第四步特殊情况处理：论文提出的ToMAgent是一种通用的智能体框架，专注于提升LLM的社交推理能力，符合\"提出通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况。 综合分析，论文的核心贡献是通过引入心智理论和新的训练范式来提升LLM的社交推理能力，这是一种通用推理能力的提升，符合研究目标。",
                    "summary2": "本文旨在解决LLM社交代理缺乏心智理论(ToM)的问题。针对社交对话场景，我们提出了一种ToMAgent (TOMA)方法，通过将ToM与对话前瞻配对来训练，产生对实现对话目标最有用的心理状态。在Sotopia交互式社交评估基准上，通过目标达成、关系维护和知识获取等指标验证了其有效性，相比基线模型实现了高达18.9%的性能提升。",
                    "summary_translation": "心理理论(Theory of Mind, ToM)——即对他人心理状态的理解——是人类社交智能的一个关键方面，然而，聊天机器人和基于大语言模型(LLM)的社交智能体通常并未整合这一能力。在这项工作中，我们证明了明确使用心理理论(ToM)的大语言模型(LLM)在对话方面表现更佳，能够更有效地实现目标。在证明仅仅提示模型在对话轮次之间生成心理状态已经能带来显著益处之后，我们进一步介绍了ToMAgent (ToMA)，一种专注于心理理论(ToM)的对话智能体。ToMA通过将心理理论(ToM)与对话前瞻(dialogue lookahead)配对进行训练，以生成对实现对话目标最有用的心理状态。在Sotopia互动社交评估基准上的实验表明，我们的方法相较于一系列基线方法具有有效性。综合分析显示，ToMA表现出更具战略性、目标导向的推理行为，这使得它能够进行长期视野(long-horizon)的适应，同时与对话伙伴保持更好的关系。我们的研究结果为整合心理理论(ToM)以构建具有社交智能的大语言模型(LLM)智能体迈出了一步。",
                    "inspiration_trace": "# 推演ToMAgent方法的逻辑链\n\n## 1. 宏观问题：社交智能的缺失\n\n**观察起点**：人类社交互动的成功不仅取决于表达自己的意图，还取决于理解对话伙伴的心理状态。这种理解他人心理状态的能力被称为\"心智理论\"(Theory of Mind, ToM)，是人类社交智能的核心组成部分。\n\n**问题识别**：当前的聊天机器人和基于LLM的社交代理通常没有整合ToM能力，这限制了它们在复杂社交场景中的表现。尽管这些系统被部署在理解用户至关重要的环境中（如工作面试、客户服务），但它们往往缺乏真正理解用户心理状态的能力。\n\n## 2. 现状分析：LLM的ToM能力与局限\n\n**研究发现**：LLM是否已经具备ToM能力存在争议。虽然一些研究显示LLM在ToM基准测试上表现良好，但其他研究表明这种能力不一致且表面化。\n\n**现有方法评估**：改进LLM ToM能力的方法包括思维链提示、神经符号方法、贝叶斯逆向规划和推理时假设生成等。然而，这些方法通常在静态和人工的ToM基准上评估，要求模型作为观察者而非动态环境中的参与者回答问题，因此无法充分反映ToM在真实社交互动中的价值。\n\n**研究缺口**：现有研究忽视了明确心理状态建模在社交互动中的作用，而专注于训练模型生成导致成功对话的话语。\n\n## 3. 核心假设：ToM能提升社交代理性能\n\n**假设提出**：明确使用ToM的LLM在对话中会表现得更好，更有效地实现目标。\n\n**初步验证**：通过简单提示模型在对话轮次之间生成心理状态，已经能显著提高目标实现。这表明心理状态建模对社交互动有积极影响。\n\n**深入假设**：通过将ToM与对话前瞻相结合，可以产生对实现对话目标最有用的心理状态。这种方法可以让模型学习哪些心理状态对成功社交互动最有价值。\n\n## 4. 方法设计：ToMAgent框架\n\n**核心思路**：创建一个训练框架，使代理能够生成关于他人心理状态的假设，模拟这些假设导致的对话结果，并选择最有利于实现目标的心理状态-话语对进行训练。\n\n**关键组件设计**：\n\n1. **心理状态假设生成**：针对每个对话上下文，生成K个关于他人心理状态的假设，涵盖信念、欲望、意图、情感和知识等至少三个维度。\n\n2. **话语生成**：基于每个心理状态假设，生成J个可能的回应话语。\n\n3. **对话前瞻模拟**：对每个心理状态-话语对，模拟最多四轮后续对话，评估对话结果。\n\n4. **效用评估**：计算每个模拟对话的目标达成得分，保留平均得分≥9的配对（或得分最高的配对）。\n\n5. **模型微调**：使用筛选出的高质量心理状态-话语对训练模型，使其能够联合学习心理状态预测和话语生成。\n\n**数学表达**：训练目标为最小化联合损失：\nLCE(ϕ) = -log Pϕ(m*|H) - log Pϕ(u*|H, m*)\n\n## 5. 实验验证：ToM的实证效果\n\n**验证环境**：使用Sotopia交互式社交评估基准，包含合作、谈判、说服和冲突等多种目标导向社交场景。\n\n**评估指标**：目标实现程度(Goal)、关系维护(Relationship)和知识获取(Knowledge)。\n\n**对比设置**：\n- 基础模型(Base)：未经微调的原始语言模型\n- 基础+心理状态(Base+MS)：先生成心理状态再生成话语的两步提示方法\n- 仅微调话语(FT+Uttr)：仅微调话语生成，消融心理状态监督\n- 仅微调心理状态(FT+MS)：仅微调心理状态生成，消融话语生成监督\n- ToMAgent(FT+MS+Uttr)：联合微调心理状态和话语生成\n\n**实验结果**：ToMAgent在所有指标上均优于基线模型，表现出更具战略性、目标导向的推理行为，同时能更好地维护与伙伴的关系。特别是在长对话中，ToMAgent表现出更强的适应能力，随着对话轮次增加，目标完成度不断提高。\n\n## 6. 深入分析：ToM的作用机制\n\n**策略分析**：ToMAgent采用更多战略性策略（如妥协、适应、提供解决方案），而基础模型更依赖人际策略（如建立融洽关系）和直接目标追求方法（如持续请求、直接请求）。\n\n**心理状态分析**：ToMAgent生成更多关于意图的假设，较少依赖情感，同时生成更多一阶信念（关于他人心理状态的推理），表明其更好地推断他人心理状态。\n\n**场景适应性**：ToMAgent在各种社交场景类型（合作、谈判、说服、冲突）中均优于基础模型，尤其在冲突场景中优势更明显，表明ToM在复杂社交互动中的价值。\n\n## 结论：从问题到方法的逻辑演进\n\n从识别社交智能中ToM的缺失，到分析现有方法的局限，提出ToM能提升社交代理性能的核心假设，最终设计出通过对话前瞻训练框架整合ToM的ToMAgent方法，并通过实证研究验证了其有效性。这一逻辑演进展示了如何从宏观问题出发，通过观察、假设和验证，形成一套有效的方法论，为构建具有社交智能的LLM代理提供了新思路。"
                },
                {
                    "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing",
                    "arxiv_id": "2509.22854",
                    "authors": "Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang",
                    "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是\"in-context learning\"(上下文学习)能力。论文提出的\"In-Context Routing (ICR)\"方法通过在注意力对数级别内化可泛化的ICL模式，增强模型的通用推理能力，这属于改进LLM本身基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，论文包含正面指标中的核心概念\"Large language models, LLMs\"，且研究的in-context learning与推理能力密切相关，因为上下文学习本质上涉及模型如何从给定信息中学习和推理。虽然论文没有直接提到reasoning、planning等关键词，但改进ICL能力实际上是在提升模型的通用推理能力。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性问题。虽然论文在多个领域的真实世界数据集上进行了评估，但其目的是测试方法的泛化能力，而非解决特定领域问题。 最后，论文的核心贡献是提出了一种新的训练/推理范式来增强LLM的通用上下文学习能力，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。ICR方法通过改进注意力机制，使模型能够更好地从上下文中学习和推理，从而提升了LLM的通用推理能力。",
                    "summary2": "本文旨在解决隐式上下文学习(ICL)泛化性不足的问题。针对大型语言模型(LLMs)的上下文学习场景，我们提出了一种基于注意力路由的上下文路由(ICR)方法，通过在注意力logits层面提取和利用可泛化的ICL模式。在12个跨领域数据集和多个LLMs上，通过准确率等指标验证了ICR的有效性，结果显示ICR在域内和域外任务上均优于现有隐式ICL方法，且无需特定任务的检索或训练。",
                    "summary_translation": "# 中文翻译\n\n隐式上下文学习(Implicit in-context learning, ICL)作为一种新兴的有前景的范式，在大型语言模型(Large Language Models, LLMs)的表示空间中模拟ICL行为，旨在以零样本成本实现少样本性能。然而，现有方法主要依赖于将偏移向量注入残差流中，而这些偏移向量通常是从标记的演示或特定任务对齐中构建的。此类设计未能充分利用ICL底层的结构机制，并且泛化能力有限。为解决这一问题，我们提出了上下文路由(In-Context Routing, ICR)，这是一种新颖的隐式ICL方法，在注意力对数(attention logits)层面内化了可泛化的ICL模式。该方法提取ICL过程中出现的可重用结构方向，并采用可学习的输入条件路由器(input-conditioned router)相应地调节注意力对数，从而实现一次训练、多次重用的框架。我们在涵盖多个领域和多种大型语言模型的12个真实世界数据集上对ICR进行了评估。结果表明，ICR始终优于需要特定任务检索或训练的先前隐式ICL方法，同时在对现有方法而言具有挑战性的域外任务上展现出强大的泛化能力。这些发现使ICR有望推动ICL实用价值的边界扩展。",
                    "inspiration_trace": "# 从问题到方法：ICR的逻辑演进分析\n\n## 1. 宏观问题：零样本上下文学习的困境\n\n大型语言模型(LLMs)的上下文学习(ICL)能力允许模型通过少量示例适应新任务，无需参数更新。然而，传统ICL面临两个根本性挑战：\n- **效率瓶颈**：插入上下文示例显著增加序列长度和推理成本\n- **稳定性问题**：性能对示例顺序和格式微小变化高度敏感\n\n这引出了核心问题：**如何在零样本设置下获得ICL的能力，同时克服传统方法的局限性？**\n\n## 2. 现有方法观察：残差流干预的局限\n\n作者观察到现有隐式ICL方法主要采用\"向量注入\"范式，即将ICL示例转化为密集向量并注入模型残差流。然而，这种方法存在根本性缺陷：\n\n- **泛化能力有限**：向量通常从特定任务示例构建，难以跨任务迁移\n- **信息容量受限**：固定大小向量只能编码有限信息，添加新知识需重新构建\n- **机制理解不足**：缺乏模型无关、输入无关的理论基础\n- **模拟而非内化**：促使模型\"模拟\"ICL行为而非真正\"内化\"ICL机制\n\n## 3. 关键发现：多任务ICL中的潜在模式\n\n通过多任务ICL实验(图1)，作者发现了一个重要现象：来自不同任务的ICDs有时能提升性能，有时却会降低性能。这表明：\n- 不同任务的ICDs可能嵌入了**潜在的跨任务ICL模式**\n- 显式提示引入的噪声可能**掩盖**这种潜在模式\n- 现有方法无法有效提取和利用这种**可泛化的结构模式**\n\n## 4. 核心假设：注意力空间中的ICL本质\n\n基于上述观察，作者提出了一个关键假设：**更具泛化性的ICL模式存在于注意力空间而非残差流中**。具体而言：\n\n- 注意力机制是transformer中学习ICL的核心组件\n- ICL的本质在于查询如何通过注意力路径被路由到相关信息\n- 注意力logits是查询-键交互的直接体现，是挖掘ICL模式的自然入口点\n\n这一假设引导作者从\"后验残差干预\"转向\"注意力空间调制\"。\n\n## 5. 方法推导：从假设到ICR\n\n### 5.1 提取跨任务ICL模式\n\n作者首先设计了一种提取可泛化ICL模式的方法：\n1. 在多个领域执行显式ICL，收集最后一个token的Q和K投影\n2. 构建跨领域的ICL基础，通过PCA提取\"主ICL方向\"(PIDs)\n3. 理论分析表明，这些PIDs捕获了跨领域共享的ICL结构，而领域特定变化在聚合中趋于平均\n\n### 5.2 设计注意力路由机制\n\n基于提取的PIDs，作者提出了\"注意力路由\"范式：\n- 使用路由向量α为PIDs分配权重，形成低秩调制\n- 在零样本推理中，将注意力动态偏向提取的PIDs\n- 这种方法在注意力logits级别直接控制信息流，而非后验干预\n\n### 5.3 构建查询条件路由器\n\n为使方法适应不同输入，作者设计了可学习的路由器：\n- 使用冻结文本编码器处理查询，生成输入表示\n- 通过两个MLP分支生成路由矩阵α(x)和门控矩阵γ(x)\n- α(x)根据查询语义自适应调制PIDs，γ(x)调节各注意力头贡献\n- 实现了\"一次训练，重复使用\"的框架\n\n### 5.4 多目标训练策略\n\n为确保训练效果，作者设计了综合训练目标：\n- 监督交叉熵：提供语义监督\n- 置信度对齐：确保路由预测不降低模型置信度\n- 稀疏路由：鼓励稀疏调制，特别是对后层施加更大稀疏性惩罚\n\n## 6. 逻辑链条总结\n\n从宏观问题到最终方法，作者的逻辑演进可概括为：\n\n**问题**：零样本ICL的效率与稳定性挑战  \n↓  \n**观察**：现有向量注入方法泛化能力有限  \n↓  \n**发现**：多任务ICL中存在潜在跨任务模式  \n↓  \n**假设**：ICL本质存在于注意力空间而非残差流  \n↓  \n**方法1**：通过PCA提取跨任务PIDs  \n↓  \n**方法2**：设计注意力路由机制调制注意力logits  \n↓  \n**方法3**：构建查询条件路由器实现自适应调制  \n↓  \n**方法4**：设计多目标训练策略确保效果  \n↓  \n**最终方法**：In-Context Routing (ICR)\n\n这一逻辑演进展示了作者从问题本质出发，通过深入观察和理论分析，提出创新假设，并逐步推导出一种在注意力空间内化ICL模式的新方法，实现了高效、稳定且可泛化的零样本上下文学习。"
                },
                {
                    "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions",
                    "arxiv_id": "2509.22750",
                    "authors": "Jeonghyun Park, Ingeol Baek, Seunghyun Yoon, Haeun Jang, Aparna Garimella, Akriti Jain, Nedim Lipka, Hwanhee Lee",
                    "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is inseparable from the reasoning process itself. This ambiguity creates a distinct challenge, where multiple reasoning paths emerge from a single question, each requiring independent resolution. Since each sub-question is ambiguous, the model must resolve ambiguity at every step. Thus, answering a single question requires handling multiple layers of ambiguity throughout the reasoning chain. We find that current Large Language Models (LLMs) struggle in this setting, typically exploring wrong reasoning paths and producing incomplete answers. To facilitate research on multi-hop ambiguity, we introduce MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE), a benchmark designed to analyze and evaluate this challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM verification pipeline. Our experiments reveal that even state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and significant challenge. To establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning and InstructiON (CLARION), a multi-agent framework that significantly outperforms existing approaches on MIRAGE, paving the way for more adaptive and robust reasoning systems.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：论文的核心是关于改进LLM在多跳推理(multi-hop reasoning)中处理模糊性(ambiguity)的能力。它提出了MIRAGE基准测试和CLARION多智能体框架，旨在增强LLM的逻辑推理能力，特别是处理复杂、模糊的多步推理问题。这属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确讨论Large Language Models (LLMs)在推理任务中的表现 - 能力方向：专注于reasoning，特别是multi-hop reasoning（多跳推理），属于逻辑推理范畴 - 新兴范式：提出了CLARION多智能体框架(multi-agent framework)，属于llm-based agents和multi-agent systems的研究 第三步排除标准：论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的CLARION框架是一种通用的多智能体协作方法，用于增强LLM处理模糊推理的能力，属于提升通用推理能力的范畴，应予以保留。 最终决策：这篇论文的核心贡献是提出了一种新的评估基准和多智能体框架来提升LLM在模糊多跳推理任务中的表现，这直接符合\"提高大语言模型本身通用推理能力\"的研究目标。论文关注的是LLM的基础推理能力提升，而非特定领域应用，因此应被纳入研究范围。",
                    "summary2": "本文旨在解决多跳问答中的歧义性问题。针对现实世界中多跳问答存在的多重推理路径歧义，我们提出了MIRAGE基准和CLARION多智能体框架，并在包含1,142个高质量多跳歧义问题的MIRAGE基准上通过STR-EM、Disambig-F1和LLM-as-a-Judge等指标验证了其有效性。",
                    "summary_translation": "现实世界中的多跳问答（Multi-hop Question Answering, QA）通常涉及与推理过程本身密不可分的模糊性。这种模糊性带来了一种独特的挑战，即从单个问题中会产生多条推理路径，每条路径都需要独立解决。由于每个子问题都是模糊的，模型必须在每一步都解决模糊性。因此，回答单个问题需要在整个推理链中处理多层次的模糊性。我们发现当前的大型语言模型（Large Language Models, LLMs）在这种设置下表现不佳，通常会探索错误的推理路径并产生不完整的答案。\n\n为促进多跳模糊性的研究，我们提出了MIRAGE（MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions，用于幻觉问题的多跳推理与模糊性评估），这是一个旨在分析和评估模糊性解释与多跳推理这一具有挑战性交叉领域的基准测试。MIRAGE包含1,142个高质量的多跳模糊问题示例，这些示例根据句法、一般和语义模糊性的分类法进行分类，并通过严格的多LLM验证流程精心筛选。我们的实验表明，即使是最新模型在MIRAGE上也表现不佳，这证实了解决模糊性与多步推理相结合是一个独特且重大的挑战。\n\n为建立一个稳健的基线，我们提出了CLARION（CLarifying Ambiguity with a Reasoning and InstructiON，通过推理和指令澄清模糊性），这是一个多智能体框架，在MIRAGE上显著优于现有方法，为开发更具适应性和稳健性的推理系统铺平了道路。",
                    "inspiration_trace": "# 从问题观察到方法论形成：CLARION框架的逻辑演进\n\n## 一、宏观问题发现：多跳问答中的歧义挑战\n\n**观察现象**：现实世界中的多跳问答(Multi-hop QA)存在一个根本性挑战——问题本身包含与推理过程不可分割的歧义性。这种歧义导致从单个问题中产生多个推理路径，每条路径都需要独立解决。\n\n**实证支持**：\n- 实验数据表明，在真实用户查询中，48.4%的问题是歧义的，17.7%涉及多跳推理，13.3%同时具有这两种特性\n- 现有LLMs在处理这类问题时表现不佳，通常探索错误的推理路径并产生不完整答案\n\n**核心问题**：当前模型倾向于只解决单个子问题的歧义，过早地修剪掉其他推理路径，导致答案不完整且无法满足用户的完整信息需求。\n\n## 二、问题分类与分析：多跳歧义的本质\n\n作者将多跳歧义系统性地分为三类，每类具有不同的特征和处理需求：\n\n### 1. 多跳句法歧义\n- **定义**：问题允许多个格式正确的句法解析，导致不同的推理链\n- **典型触发**：介词短语附着、代词共指、协调范围(\"and/or\")、量词范围\n- **处理需求**：需要模型进行句法解析(Resolve)\n\n### 2. 多跳一般歧义\n- **定义**：查询范围过于狭窄，导致早期承诺和修剪其他有效推理路径\n- **典型触发**：过度指定约束、过于精细的实体/区域粒度、限制性修饰词\n- **处理需求**：需要模型进行范围调整和意图重聚焦(Generalize)\n\n### 3. 多跳语义歧义\n- **定义**：相同词语映射到多种含义或实体，产生不同的多跳推理路径\n- **典型触发**：歧义名称、首字母缩略词、品牌-普通名词重叠\n- **处理需求**：需要模型进行意义/实体选择(Interpret)\n\n## 三、假设形成：解决多跳歧义的关键思路\n\n基于问题分析，作者形成三个核心假设：\n\n1. **独特挑战假设**：解决与多步推理过程深度集成的歧义是一个独特的、很大程度上未解决的挑战\n\n2. **系统性处理假设**：有效解决多跳歧义问答需要系统性地检测、消除歧义并回答问题，而非简单应用现有方法\n\n3. **两阶段处理假设**：将任务分为规划和执行两个阶段可能更有效——先分析问题并消除歧义，再执行推理计划\n\n## 四、方法论设计：CLARION框架的提出\n\n基于上述假设，作者设计了CLARION(CLarifying Ambiguity with a Reasoning and InstructiON)框架，采用两阶段多智能体方法：\n\n### 4.1 规划智能体(Planning Agent)\n在任何检索或回答之前分析输入问题，执行三个顺序操作：\n\n1. **歧义检测**：确定问题是否包含歧义，无歧义则直接传递给执行智能体\n\n2. **歧义类型分类**：将歧义问题分为句法、一般或语义三类\n\n3. **问题澄清**：基于检测到的类型，将原始问题重写为消除歧义的变体，同时保留信息需求\n\n### 4.2 执行智能体(Acting Agent)\n通过ReAct风格提示方案执行推理计划，展开为思想→行动→观察循环：\n\n1. **搜索**：当需要额外证据时检索外部文档\n\n2. **规划**：如果当前计划不足，重新调用检测、类型分类或澄清\n\n3. **回答**：一旦收集到足够证据，合成最终输出\n\n**设计特点**：\n- 所有行动采用JSON格式确保可靠解析和自动执行\n- 限制最多五次迭代防止无限循环\n- 达到限制时强制执行Answer行动\n\n## 五、验证与评估：方法论的有效性证明\n\n作者在MIRAGE基准测试上验证CLARION框架，与三个强基线比较：\n\n1. **无检索**：仅LLM推理，无外部上下文\n2. **朴素RAG**：标准检索-然后-阅读流水线\n3. **DIVA**：三阶段多样化-验证-适应RAG框架\n\n**实验结果**：\n- CLARION在所有评估指标上显著优于基线方法\n- 消融研究证实每个组件的贡献，其中问题澄清组件最为关键\n- 结果支持了作者的假设：识别歧义并明确重写问题对实现稳健的多跳歧义问答至关重要\n\n## 六、逻辑演进总结\n\nCLARION框架的提出体现了清晰的科研思路演进：\n\n1. **从现象到问题**：观察到多跳问答中的歧义挑战，识别出当前方法的不足\n\n2. **从问题到分类**：系统分析多跳歧义的本质，将其分为三类并明确每类的处理需求\n\n3. **从分析到假设**：基于问题本质形成三个核心假设，指明解决方向\n\n4. **从假设到方法**：设计两阶段多智能体框架，系统化处理多跳歧义问题\n\n5. **从方法到验证**：通过严格实验验证方法有效性，证明假设的正确性\n\n这一逻辑链条从宏观问题出发，逐步聚焦到具体解决方案，展现了从观察、假设到形成最终方法论的完整思考过程，为解决多跳歧义问答这一复杂挑战提供了创新且有效的思路。"
                },
                {
                    "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning",
                    "arxiv_id": "2509.22824",
                    "authors": "Chi Ruan, Dongfu Jiang, Yubo Wang, Wenhu Chen",
                    "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Critique Reinforcement Learning (CRL)\"的新训练范式，旨在通过让模型学会评论和反思来增强其推理能力。论文本质上是改进LLM的基础能力，提出新的训练范式来增强其逻辑推理能力，完全符合第一步的\"保留\"标准。从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)，特别是在逻辑推理任务上表现出色，并提出了基于强化学习(RL)的新训练方法。论文强调其方法不仅提高了代码生成能力，还增强了通用推理能力，这种能力可以迁移到广泛的任务中，这正是研究目标所关注的\"通用推理能力\"。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了代码生成，但这是作为增强通用推理能力的手段而非特定领域应用。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在增强代码生成模型的推理和批判能力，解决标准强化学习缺乏明确批判或反思机制的问题。针对代码生成和一般推理任务，我们提出了一种Critique Reinforcement Learning (CRL)方法，通过奖励模型对预测批判的正确性进行反馈，并开发了CRITIQUE-CODER模型。在LiveCodeBench、EvalPlus等代码基准测试及BBEH逻辑推理基准上，通过准确率、Pass@1等指标验证了其有效性，CRITIQUE-CODER-8B在LiveCodeBench上达到60.8%，超越了DeepCoder-14B和GPT-o1等模型。",
                    "summary_translation": "强化学习(Reinforcement Learning, RL)已成为一种流行的训练范式，特别是与推理模型(reasoning models)配对时。虽然有效，但它主要关注生成响应，缺乏明确促进批评(critique)或反思(reflection)的机制。几项最近的研究，如批评微调(Critique-Fine-Tuning, CFT)和批评引导蒸馏(Critique-Guided-Distillation, CGD)，已经展示了明确教授大语言模型(Large Language Models, LLMs)如何批评的好处。受这些研究启发，我们提出了批评强化学习(Critique Reinforcement Learning, CRL)，在该方法中，模型被要求为给定的(问题，解决方案)对生成批评。奖励仅取决于生成批评的最终判断标签$c \\in \\{\\texttt{True}, \\texttt{False}\\}$是否与真实判断$c^*$一致。基于这一点，我们引入了\\textsc{Critique-Coder}，它通过用CRL数据替代20%的标准RL数据，在RL和CRL的混合数据上进行训练。我们对多个模型(\\textsc{Critique-Coder})进行微调，并在不同基准测试上评估它们，以展示它们相对于仅使用RL的模型的优势。我们表明，\\textsc{Critique-Coder}在所有评估的基准测试上始终优于仅使用RL的基线模型。值得注意的是，我们的\\textsc{Critique-Coder-8B}在LiveCodeBench (v5)上可以达到超过60%的准确率，优于其他推理模型，如DeepCoder-14B和GPT-o1。除了代码生成，\\textsc{Critique-Coder}还展示了增强的一般推理能力，这在其在BBEH数据集的逻辑推理任务上的更好表现中得到证明。这表明在代码数据集上应用CRL增强了通用推理和批评能力，这些能力可以在广泛范围内转移到各种任务中。因此，我们相信CRL作为标准RL对LLM推理的极好补充。",
                    "inspiration_trace": "# 从问题到方法论：Critique Reinforcement Learning的逻辑演进\n\n## 1. 宏观问题：如何提升大型语言模型的推理能力？\n\n在当代AI研究中，提升大型语言模型(LLM)的推理能力是一个核心挑战。特别是在代码生成、数学问题求解和逻辑推理等复杂任务中，模型需要展现出超越简单模式匹配的深度思考能力。研究者们已经探索了多种路径，但如何系统性地增强模型的批判性思维和自我反思能力，仍是一个开放性问题。\n\n## 2. 现有方法及其局限性分析\n\n### 2.1 强化学习与思维链结合的现状\n\n论文首先观察到，强化学习(RL)与思维链(CoT)的结合已成为提升模型推理能力的主流范式。这种方法使模型能够迭代优化中间推理步骤，在代码生成等领域取得了显著进展。然而，作者敏锐地指出这一范式的根本局限：\n\n> \"虽然有效，但它主要关注生成响应，缺乏明确促进批判或反思的机制。\"\n\n标准RLVR(带可验证奖励的强化学习)虽然能提高问题解决能力，但难以激发模型对现有解决方案的内部批判或反思行为。模型被训练为直接生成正确答案，却很少被要求评估和判断已有解决方案的质量。\n\n### 2.2 批判学习的新兴研究\n\n与此同时，作者注意到另一条研究路线正在兴起：Critique-Fine-Tuning (CFT)和Critique-Guided-Distillation (CGD)等方法表明，明确教导LLM如何进行批判可以有效释放其推理潜力。这些方法通过让模型学习评判和反思，展示了提升推理能力的新途径。\n\n## 3. 关键洞察与假设形成\n\n### 3.1 核心洞察\n\n通过分析现有研究，作者形成了两个关键洞察：\n\n1. **互补性洞察**：RL和批判学习具有天然互补性——RL专注于提高问题解决能力，而批判学习则培养批判性思维和推理能力。\n\n2. **机制缺失洞察**：标准RL框架中缺乏对批判和反思的明确激励机制，这限制了模型推理能力的进一步提升。\n\n### 3.2 核心假设\n\n基于这些洞察，作者提出了一个大胆而创新的假设：\n\n> \"将批判机制整合到RL范式中，通过奖励模型对解决方案的准确判断，可以同时优化问题解决技能和批判能力，并且这些能力可能转移到广泛的任务中。\"\n\n这一假设暗示，如果能够让模型在生成解决方案的同时，也学习如何评判解决方案的正确性，可能会产生更强大的推理能力。\n\n## 4. 方法论设计：Critique Reinforcement Learning (CRL)\n\n### 4.1 核心思想\n\n为了验证假设，作者设计了Critique Reinforcement Learning (CRL)，其核心思想简洁而有力：\n\n- 模型被提示为给定的(问题,解决方案)对生成批判\n- 奖励仅基于生成的批判的最终判断标签c ∈ {True, False}是否与真实判断c*一致\n- 这与标准RLVR形成鲜明对比，后者只激励模型为给定问题生成正确解决方案\n\nCRL的创新在于它将批判学习融入RL框架，通过明确的奖励信号引导模型发展批判性思维能力。\n\n### 4.2 实现方案：CRITIQUE-CODER\n\n基于CRL范式，作者进一步开发了CRITIQUE-CODER模型，采用混合训练策略：\n\n- 用20%的CRL数据替代标准RL数据\n- CRL数据帮助模型发展批判性思维和推理能力\n- RL数据专注于提高问题解决性能\n- 两种数据类型结合，使模型能够整合两种范式的优势\n\n这种设计反映了作者的深入思考：CRL不应完全替代RL，而应作为其补充，因为CRL训练主要关注评判问题-解决方案对，而不生成实际解决方案。\n\n## 5. 实验验证与结果分析\n\n### 5.1 数据集构建与训练策略\n\n作者精心设计了数据集构建过程：\n\n- 从rStar-Coder的人类种子RL数据集构建CRL数据集\n- 通过过滤测试用例提高效率和一致性\n- 生成候选解决方案并根据测试用例通过率确定其判断\n- 采用两阶段训练策略，逐步增加上下文长度以充分利用长推理链的潜力\n\n### 5.2 评估结果\n\n实验结果有力地支持了作者的假设：\n\n1. **性能提升**：CRITIQUE-CODER在所有评估基准上均优于仅使用RL的模型\n   - CRITIQUE-CODER-8B在LiveCodeBench (v5)上达到60%以上，优于DeepCoder-14B和GPT-o1等推理模型\n   - 在Qwen3-4B上，LiveCodeBench得分从54.2提升到59.0(+4.8)\n\n2. **能力迁移**：CRL不仅提高了代码生成能力，还增强了通用推理能力\n   - 在BIG-Bench Extra Hard逻辑推理基准上，CRITIQUE-CODER超越了基线和RL训练模型\n   - 平均比基线模型提高+6.1分，证明批判能力可以转移到其他领域\n\n3. **最佳比例**：消融研究确认20%的CRL数据混合比例是最佳实践\n   - 完全用CRL替代RL会导致性能下降，证实了CRL作为RL补充而非替代的价值\n\n## 6. 结论与意义\n\n通过这一系列严谨的推理和实验，作者得出结论：CRL作为一种新颖的强化学习训练框架，成功地将批判学习整合到RL范式中，解决了标准RL框架中缺乏批判和反思激励的问题。\n\n这一工作的意义不仅在于提出了一种新的训练方法，更在于它揭示了批判性思维与问题解决能力之间的深层联系，为未来提升LLM推理能力的研究指明了新方向。正如作者所言：\n\n> \"CRL通过赋予模型更强的批判和推理能力来增强标准RL——这些能力不仅在编码任务中表现出来，还能有效转移到更广泛的推理领域。\"\n\n这一完整的逻辑链条——从宏观问题出发，分析现有方法局限性，形成创新假设，设计具体方法，并通过实验验证——展示了学术研究的严谨思维过程，也为如何系统性地解决复杂AI问题提供了范例。"
                },
                {
                    "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression",
                    "arxiv_id": "2509.25176",
                    "authors": "Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang",
                    "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression），一种针对大型推理模型(LRMs)的强化学习方法。该方法通过在训练过程中交替压缩和扩展推理预算，动态调整最大展开长度，从而提高模型的推理效率和准确性。这明显是关于改进LLM基础推理能力的研究，属于强化学习优化的方法论研究，符合保留标准。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确研究大型推理模型(LRMs)，即具有推理能力的大语言模型 - 能力方向：专注于推理能力(reasoning)，特别是在数学推理任务(AIME24)上评估 - 训练方法：核心是强化学习(Reinforcement Learning)方法，通过迭代训练提升模型性能 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（虽然使用数学竞赛作为评估基准，但目的是提升通用推理能力） - 不关注模型基础设施、部署优化或硬件加速 第四步特殊和模糊情况：论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出一种新的强化学习训练范式来增强大语言模型的推理能力和效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型推理模型(LRMs)在强化学习训练中产生冗余思考模式导致的效率与性能权衡问题。针对数学推理任务，我们提出了一种名为SIRI的迭代强化学习方法，通过交替压缩和扩展推理预算来动态调整输出长度。在DeepSeek-R1-Distill-Qwen模型上，通过AIME24、AIME25等数据集的Pass@1准确率和token使用量评估，验证了SIRI在提高推理准确率的同时显著减少冗余token的有效性。",
                    "summary_translation": "我们介绍SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression，交错压缩的迭代强化学习扩展），这是一种简单而有效的大型推理模型（Large Reasoning Models, LRMs）强化学习方法，能够实现更高效和准确的推理。现有研究已经观察到LRMs中存在重复性思维模式，而减少这些模式的尝试往往以性能为代价。在本文中，我们表明这种权衡可以通过一种训练机制来克服，该机制通过在训练过程中动态调整最大推演长度（rollout length），迭代地在压缩和扩展推理预算之间交替。压缩阶段减少了推演长度，迫使模型在有限的上下文中做出精确且有价值的决策，从而有效减少冗余标记（tokens）并提高推理密度。扩展阶段则放宽长度限制，为模型在长时程（long-horizon）设置中进行探索和规划提供空间。值得注意的是，我们发现经过每个压缩-扩展周期后，模型的性能即使在其输出长度减少的情况下也有所提高，稳步推动其更接近性能-效率权衡中的帕累托前沿（Pareto frontier）。在DeepSeek-R1-Distill-Qwen-1.5B上进行训练，SIRI-low在三次迭代后将AIME24上的性能提高了43.2%，同时减少了46.9%的标记使用量，而SIRI-high相比所有其他方法达到了最高准确率（图1）。我们的研究结果揭示了在训练过程中周期性振荡LRM的输出截断长度的潜力，以动态平衡推理中的探索和效率，从而收敛到两者之间的最优\"甜点\"（sweet spot）。我们的模型已公开发布。",
                    "inspiration_trace": "# SIRI方法逻辑推演：从问题到解决方案\n\n## 宏观问题：大型推理模型的效率与性能困境\n\n大型推理模型(LRMs)通过强化学习提升推理能力时，展现出显著的性能提升，但同时也产生了一个严重问题：模型输出中包含大量冗余推理token，导致训练和推理时间大幅增加。这引发了一个核心挑战：**如何在保持或提高推理准确性的同时，显著减少冗余token，提升推理效率？**\n\n## 现象观察：性能与效率的权衡困境\n\n作者通过观察现有方法发现：\n\n1. **重复思维模式**：LRMs在强化学习后表现出明显的重复思考、不必要的回溯和非推理填充内容\n2. **现有方法的局限性**：先前尝试减少冗余的方法（如引入长度惩罚、长度截断、混合推理策略等）无一例外地导致模型性能下降或停滞\n3. **关键发现**：DeepScaleR的8K训练阶段显示，尽管响应长度急剧下降，模型性能却有所提升，表明模型能将关键推理步骤压缩到更短上下文中，为后续探索释放能力\n\n## 假设形成：压缩-扩展周期的潜力\n\n基于上述观察，作者提出核心假设：**通过周期性地交替压缩和扩展推理预算，可以同时实现性能提升和效率优化**。具体假设包括：\n\n1. **压缩阶段**：强制模型在有限上下文中做出精确有价值的决策，减少冗余token，提高推理密度\n2. **扩展阶段**：放松长度限制，为模型提供探索和规划的空间，特别是在长视野设置中\n3. **周期性迭代**：每个压缩-扩展周期后，模型性能应提升而输出长度减少，逐步逼近性能-效率权衡的帕累托前沿\n\n## 方法设计：SIRI框架的构建\n\n基于假设，作者设计了SIRI(Scaling Iterative Reinforcement Learning with Interleaved Compression)框架，包含三个关键组件：\n\n### 1. 奖励塑造机制\n采用长度限制奖励函数：\n```\nR(y) = {1, 如果可以从clip(y, L)中提取正确答案; 0, 其他情况}\n```\n这种方法在组内响应多样化时特别有效，能引导模型保留正确且密集的推理模式，同时修剪低效或错误的模式。\n\n### 2. 长度调度器设计\n设计控制压缩和探索行为的调度器，需满足：\n- 压缩阶段防止性能下降\n- 扩展阶段鼓励探索\n- 确保模型在扩展阶段结束前生成长度达到平稳\n\n提出三种调度器变体：\n- **阶梯调度器**：直接在压缩阶段将最大生成长度从Lmax降至Lmin，扩展阶段再恢复\n- **余弦调度器**：使用余弦函数平滑地调整长度，使压缩和恢复过程更平缓\n- **阶梯-余弦调度器**：结合前两者优点，确保过程平滑性的同时，在Lmax处充分探索，在Lmin处充分利用\n\n### 3. 迭代训练框架\n通过多个压缩-扩展周期的迭代，逐步提升模型性能和效率，形成螺旋上升的优化轨迹。\n\n## 验证与优化：实验驱动的改进\n\n通过实验验证假设并优化方法：\n\n1. **有效性验证**：SIRI在减少token使用的同时提高性能。例如，在DeepSeek-R1-Distill-Qwen-1.5B上，SIRI-low在三次迭代后AIME24性能提高43.2%，token使用减少46.9%\n\n2. **调度器优化**：\n   - 发现周期较长的调度器(640周期)表现最佳，压缩阶段更平滑，扩展阶段更持久\n   - 余弦调度器减轻压缩过程中的性能损失，阶梯调度器最大化扩展过程中的性能提升\n\n3. **行为机制分析**：\n   - 压缩-扩展方案主要影响模型的回溯和验证行为\n   - \"wait\"等代表回溯验证的标记频率在压缩和扩展过程中显著变化\n   - 熵值在压缩阶段降低，扩展阶段增加，但保持在稳定范围内而非崩溃\n\n## 结论：突破性能-效率权衡的新范式\n\nSIRI方法通过周期性地在训练过程中交替压缩和扩展token预算，成功地突破了大型推理模型中性能与效率的权衡困境。这一发现揭示了通过动态调整输出截断长度来平衡探索和效率的潜力，为大型推理模型的训练提供了新的研究方向。"
                },
                {
                    "title": "The Era of Real-World Human Interaction: RL from User Conversations",
                    "arxiv_id": "2509.25137",
                    "authors": "Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston",
                    "summary": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的强化学习范式\"Reinforcement Learning from Human Interaction (RLHI)\"，该方法使大语言模型能够直接从真实用户对话中学习，而非依赖预标注的专家反馈。这完全符合我的研究目标，因为它专注于改进LLM的基础能力，提出了新的训练范式来增强模型性能。 具体分析如下： 1. 核心判断：论文本质上是关于改进LLM基础能力的研究，提出了新的强化学习方法(RLHI)来优化模型的对齐能力和指令跟踪能力。论文明确提到这些方法还能\"enhances performance on reasoning benchmarks\"，表明其对推理能力的提升作用。 2. 正面指标：论文符合多个正面指标，包括： - 核心概念：针对对话模型(属于LLM范畴) - 能力方向：明确提及对推理基准性能的提升 - 训练方法：核心是提出新的强化学习方法(RLHI)，与RLHF相关但更先进 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性(应用层面)。 4. 特殊情况：论文虽然涉及用户交互，但不是将LLM作为工具应用于特定领域，而是提出通用方法来增强模型的基础能力，因此应该保留。 综上所述，这篇论文通过提出新的强化学习范式来改进LLM的通用能力(包括推理能力)，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决当前对话模型依赖预注释专家反馈而非真实用户交互的问题。针对真实用户对话场景，我们提出了一种Reinforcement Learning from Human Interaction (RLHI)方法，包括User-Guided Rewrites和User-Based Rewards两种互补技术，并在WildChat数据集上通过个性化、指令遵循和推理能力指标验证了其有效性。",
                    "summary_translation": "我们主张，为了实现持续模型改进和多方面对齐（multifaceted alignment），未来的模型必须从自然人类交互中学习。当前的对话模型（conversational models）是使用预先标注的、专家生成的人类反馈（expert-generated human feedback）进行对齐的。在这项工作中，我们介绍了Reinforcement Learning from Human Interaction (RLHI，人类交互强化学习)，这是一种直接从真实环境中的用户对话（in-the-wild user conversations）中学习的范式。我们开发了两种互补方法：(1) RLHI with User-Guided Rewrites（用户引导重写的人类交互强化学习），该方法基于用户的自然语言后续回应修改不满意的模型输出；(2) RLHI with User-Based Rewards（基于用户奖励的人类交互强化学习），该方法通过一个基于用户长期交互历史（称为persona，角色）的奖励模型（reward model）进行学习。这些方法共同通过persona-conditioned preference optimization（角色条件偏好优化）将长期用户角色与轮次级偏好（turn-level preferences）联系起来。在基于WildChat数据集的对话训练中，两种RLHI变体在个性化（personalization）和指令遵循（instruction-following）方面均优于强基线，且类似的反馈增强了在推理基准测试（reasoning benchmarks）上的性能。这些结果表明，有机人类交互（organic human interaction）为个性化对齐（personalized alignment）提供了可扩展且有效的监督（effective supervision）。",
                    "inspiration_trace": "# 从宏观问题到方法论：RLHI的逻辑演进链\n\n## 宏观问题：如何实现语言模型的持续改进与多面对齐？\n\n### 观察一：当前对齐方法的局限性\n- **现状**：现有模型主要依赖静态的专家标注数据进行对齐（预标注问题、固定演示、排名或评分）\n- **问题**：\n  1. 反映标注者非自然场景下的观点，而非真实用户的真实多元长期目标\n  2. 捕获静态、无上下文的判断，而非动态、情境化的需求\n  3. 受标注预算限制，而非随实际使用和用户多样性扩展\n\n### 观察二：人类学习方式的启示\n- **核心洞察**：人类通过与环境和他人的持续互动学习，接收反馈并调整行为\n- **映射到AI**：语言模型的丰富监督来源已存在于真实世界中——模型与真实用户的自然互动\n- **关键发现**：这种互动揭示长期历史中的隐藏用户偏好和动态上下文需求\n\n## 假设形成：真实人类互动可作为有效监督信号\n\n### 假设核心\n真实人类互动能提供可扩展、有效的个性化对齐监督，并具有三个关键特性：\n\n1. **上下文基础**：\n   - 出现在任务或对话流程中\n   - 直接与用户情境需求和模型先前输出相关\n   - 被用户个性化知识（历史、偏好）塑造\n\n2. **演化分布**：\n   - 反映随时间变化的目标和环境\n   - 提供与人类需求真实分布时间相关的监督\n\n3. **多样监督信号**：\n   - 包含显式高带宽信号（修正、澄清）和隐式线索（不参与、沮丧）\n   - 可能包含风格、情感基调甚至对抗性输入\n\n## 假设验证：真实人类互动数据分析\n\n### 数据分析（WildChat-1M数据集）\n1. **用户反馈模式**：\n   - 26.51%用户消息为\"带反馈的重新尝试\"\n   - 对话后期阶段，此类反馈占比高达83.15%\n   - 这些反馈虽短（平均272字符），但语义密集\n\n2. **上下文多样性**：\n   - 真实用户互动上下文多样性(0.865)显著高于HH-RLHF(0.751)和HelpSteer2(0.848)\n   - 表明真实用户互动反映更广泛的真实日常需求\n\n3. **用户偏好多样性**：\n   - 用户personas呈现明显差异（如专业知识水平、信息密度、语调、结构偏好）\n   - 存在独特偏好子集（如反复请求类比或角色扮演）\n   - 用户需求可能跨领域变化或随时间演变\n\n## 方法论构建：RLHI框架\n\n### 核心挑战\n如何从有机互动中提取有效信号，将长期用户偏好与轮级响应关联？\n\n### 解决方案：双路径方法\n\n#### 路径一：RLHI with User-Guided Rewrites\n**逻辑**：利用用户自然语言反馈直接改进不满意输出\n- **步骤**：\n  1. 识别\"带反馈的重新尝试\"\n  2. 基于用户反馈修改原始响应\n  3. 形成偏好对（修改版 vs 原始版）\n  4. 整合用户persona（从长期历史提取）\n  5. 应用基于persona的条件DPO训练\n\n**关键创新**：将用户自然语言反馈转化为具体改进信号，而非简化为二元标签\n\n#### 路径二：RLHI with User-Based Rewards\n**逻辑**：针对无明确反馈的请求，利用用户长期偏好指导响应选择\n- **步骤**：\n  1. 为每个请求生成多个候选响应\n  2. 使用条件于用户persona的奖励模型评分\n  3. 选择最高分和最低分响应形成偏好对\n  4. 应用基于persona的条件偏好优化\n\n**关键创新**：将长期用户偏好转化为响应选择的指导信号\n\n### 统一框架：Persona-Conditioned Preference Optimization\n两种方法通过基于persona的条件偏好优化，将长期用户personas与轮级偏好联系起来，实现个性化对齐。\n\n## 实验验证与发现\n\n### 关键实验结果\n1. **个性化与指令遵循**：两种RLHI变体均显著优于基线\n2. **标准基准**：User-Based Rewards在AlpacaEval 2.0达到77.9%长度控制胜率\n3. **推理能力**：User-Guided Rewrites将四个推理基准平均准确率从26.5提升至31.8\n\n### 关键发现\n1. 用户引导重写优于从头生成（利用上下文反馈）\n2. 基于用户的奖励能捕捉长期偏好，实现更强对齐\n3. 强化学习优于监督微调（利用偏好信号）\n4. 人类互动数据嘈杂，需要质量过滤\n5. 用户多样性对RLHI效果至关重要\n\n## 结论：从静态标注到动态互动的范式转变\n\n论文最终论证了有机人类互动为个性化对齐提供了可扩展、有效的监督，标志着从静态专家标注向动态真实用户互动的范式转变，为构建能随时间改进的个性化助手指明了方向。"
                },
                {
                    "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
                    "arxiv_id": "2509.25123",
                    "authors": "Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng",
                    "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究LLM如何通过强化学习(RL)组合已有技能来获取新技能，这直接涉及到改进LLM的基础推理能力和提出新的训练范式。论文探讨了LLM的函数组合推理能力，即从已知的f(x)和g(x)推理出未见过的f(g(x))，这属于逻辑推理和多步推理的核心研究。 其次，论文包含多个正面指标：核心概念明确是LLMs；能力方向聚焦于推理能力，特别是逻辑推理和问题解决；训练方法采用强化学习(RL)；虽然未直接讨论智能体或工具使用，但研究的技能组合机制与这些新兴范式的底层能力相关。 第三，论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题，而是专注于通用推理机制的研究。 最后，论文的核心贡献是揭示了LLM通过RL能够真正获取新技能（而非仅仅激活现有技能），并通过组合已有技能实现更复杂的推理能力，这种能力还能泛化和迁移到不同任务。这些发现对于提升LLM的通用推理能力具有重要意义，完全符合研究目标。",
                    "summary2": "本文旨在解决强化学习(RL)是否真正教会大型语言模型(LLMs)新技能的争议问题。针对LLMs在RL训练中技能获取的机制，我们提出了一种基于组合性技能的合成框架，通过字符串转换任务定义原子技能和组合技能，并在Llama-3.1-8B-Instruct模型上通过准确率和泛化能力验证了其有效性。实验证明RL能使模型通过组合已有技能获得新技能，且这些技能能泛化到更复杂问题和跨任务场景。",
                    "summary_translation": "强化学习(RL)是教授大语言模型(LLMs)真正的新技能，还是仅仅激活其已有的能力？这个问题是关于强化学习在大语言模型后训练中作用的持续争论的核心。一方面，即使没有先前的监督微调(supervised finetuning)，强化学习也能取得强有力的实证结果；另一方面，批评者认为强化学习除了重新加权现有的推理策略外，贡献甚微。本研究提供了具体证据，表明大语言模型可以通过组合现有技能在强化学习过程中获得真正的新技能，这反映了人类获取新认知技能的核心机制之一。为了减轻数据污染(data contamination)和其他混杂因素(confounding factors)的影响，并允许对任务复杂性进行精确控制，我们为研究开发了一个合成框架(synthetic framework)。具体而言，我们将技能定义为给定x时推断字符串转换函数(string transformation function) f(x)输出的能力。当大语言模型在强化学习之前已经学习了f和g时，我们的实验表明，强化学习使其能够学习它们未见过的组合h(x)=g(f(x))。此外，这种组合能力(compositional ability)可以泛化到更困难的问题，例如在强化学习训练期间未见过的超过2个函数的组合。令人惊讶的是，我们的实验表明，在源任务(source task)上获得的组合技能能够迁移到不同的目标任务(target task)。这种迁移甚至在没有对目标任务进行组合训练的情况下也能发生，只需要对目标任务的原子技能(atomic skills)有先验知识。我们的定性分析(qualitative analysis)表明，强化学习从根本上改变了模型的推理行为。相比之下，使用相同数据的下一个词元预测训练(next-token training)并未产生这些发现。我们的系统性实验为大语言模型学习提供了新的见解，表明首先构建具有基本技能的基础模型，然后使用强化学习来激励复杂问题的高级、可泛化技能的价值。",
                    "inspiration_trace": "# 从宏观问题到核心方法：论文逻辑链推演\n\n## 宏观问题：RL是否真正教会LLM新技能？\n\n论文始于一个根本性问题：强化学习(RL)在大型语言模型(LLM)后训练中是真正教授新技能，还是仅仅激活了模型已有的能力？这一问题触及了当前对RL在LLM中作用的激烈争论核心。\n\n## 矛盾观察：实证成功与理论质疑的对立\n\n作者首先识别了两个看似矛盾的观察：\n1. **实证成功**：RL即使在没有先前监督微调的情况下也能取得显著成果，甚至能直接基于基础模型进行训练。\n2. **理论质疑**：有观点认为RL的贡献仅限于重新加权现有推理策略，而非真正学习新能力。\n\n这种矛盾表明，我们需要更深入地理解RL在LLM中的实际作用机制。\n\n## 理论启发：从人类认知技能获取中寻找答案\n\n作者转向认知科学理论，特别是Anderson(1982)的研究，该研究表明人类通过组合和内化现有技能来获得新认知技能。这一理论启发作者提出一个核心假设：LLM可能也通过类似机制，即组合现有技能来获得真正的新技能。\n\n## 假设形成：RL组合性假说\n\n基于上述理论启发，作者提出了\"RL组合性假说\"：\n> 一旦模型通过下一个词元预测(NTP)训练获得了任务所需的原子不可分解技能，通过适当激励的RL可以教会模型通过组合原子技能来学习新技能。\n\n这一假说将研究焦点从\"RL是否教授新技能\"转向\"RL如何通过组合现有技能教授新技能\"。\n\n## 方法设计：构建受控实验框架\n\n为验证假说，作者设计了一个受控合成框架，解决现有研究中的关键问题：\n\n### 1. 解决数据污染问题\n- 设计字符串转换预测任务，使用无意义函数标识符(如func_16)\n- 在RL期间隐藏函数定义，确保任务只能通过原子技能获取训练解决\n\n### 2. 精确定义技能层次\n- **原子技能**：单一、不可分解的转换，如func_16(x)\n- **组合技能**：原子技能的嵌套组合，如func_15(func_16(x))\n- **难度控制**：通过嵌套深度控制任务难度，Level n涉及n个函数组合\n\n### 3. 两阶段训练协议\n- **阶段1**：通过RFT学习原子技能，模型观察函数定义\n- **阶段2**：通过RL或RFT学习组合技能，模型仅观察函数名称和组合\n\n## 实验验证：系统性测试假说\n\n作者通过一系列针对性实验验证假说的各个方面：\n\n### 实验1：RL教授新组合技能\n- **方法**：比较三种RL配置(仅原子技能、仅Level-2组合、混合)\n- **发现**：包含组合任务的RL使模型能泛化到更复杂问题，而仅原子技能的RL则不能\n- **结论**：RL在组合数据上教授了可泛化的新技能\n\n### 实验2：组合数据是关键激励\n- **方法**：比较RL与RFT在相同组合数据上的效果\n- **发现**：RL显著优于RFT，后者几乎无法泛化\n- **结论**：RL本身是学习可泛化组合技能的重要因素，而非仅有组合数据\n\n### 实验3：组合技能的跨任务迁移\n- **方法**：测试字符串任务上学习的组合技能能否迁移到Countdown任务\n- **发现**：在字符串任务上接受RL训练的模型在Countdown任务上表现显著更好\n- **结论**：RL学习的组合技能可作为元技能，增强模型在不同领域使用原子技能的能力\n\n### 实验4：突破\"重排序幻觉\"\n- **方法**：分析不同难度级别上的pass@k性能\n- **发现**：在简单问题上，RL与基础模型差距随k增加而缩小；但在困难问题上，差距反而扩大\n- **结论**：先前认为RL仅重排序的结论源于评估和训练模型已表现良好的任务，RL确实能学习新技能\n\n### 实验5：RL转变推理行为\n- **方法**：分析不同模型在组合问题上的失败模式\n- **发现**：RL训练的模型完全消除了\"忽略组合\"错误，主要失败模式变为原子预测错误\n- **结论**：RL从根本上改变了模型的行为，使其能正确理解和处理组合结构\n\n## 最终方法论：技能获取视角的LLM开发\n\n基于以上发现，作者提出了一个更广泛的LLM开发方法论：\n1. **基础模型阶段**：构建具有必要基本技能的基础模型\n2. **后训练阶段**：通过RL和适当激励获取更高级技能，这些技能能更好地泛化到复杂和跨域问题\n\n这一方法论强调了基础模型开发与后训练策略之间的协调，从技能获取的角度重新思考LLM的发展路径。\n\n## 总结：从问题到方法的逻辑链条\n\n论文展现了清晰的逻辑演进：从宏观问题(RL是否教授新技能)→矛盾观察→理论启发→假说形成→方法设计→实验验证→最终方法论。通过精心设计的受控实验，作者不仅回答了初始问题，还揭示了RL在LLM中作用的更深层次机制，为LLM的后训练提供了新的理论指导。"
                },
                {
                    "title": "Rethinking Entropy Regularization in Large Reasoning Models",
                    "arxiv_id": "2509.25133",
                    "authors": "Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, Jing Shao",
                    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise in enhancing the reasoning abilities of large reasoning models (LRMs). However, it suffers from a critical issue: entropy collapse and premature convergence. Naive entropy regularization, a common approach for encouraging exploration in the traditional RL literature, fails to address this problem in the context of LRM. Our analysis reveals that this failure stems from the vast action space and long trajectories in LRMs, which easily trigger a global entropy explosion as the model indiscriminately explores all possible actions and states. To address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method that confines exploration to a meaningful subset of actions and states. SIREN achieves this through a two-step entropy masking mechanism, consisting of a top-p mask and a peak-entropy mask. In addition, regularization is transformed into a self-anchored form to stabilize training. Across five mathematical benchmarks, SIREN attains superior average performance over previous entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes greater response diversity and maintains entropy at an appropriate level, which helps to preserve the validation pass@k throughout training. This effectively mitigates the premature convergence problem common in RLVR for LRM.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标和筛选标准。从核心判断来看，论文的本质是关于改进大型推理模型(LRMs)的基础能力，提出了一种新的强化学习训练范式(SIREN)来增强模型的推理能力。论文明确关注解决熵崩溃和过早收敛问题，这些都是提升模型推理能力的关键技术挑战。 在正面指标方面，论文包含了多个相关主题：核心概念上提到了\"Large reasoning models (LRMs)\"；能力方向上明确关注\"reasoning abilities\"，特别是在数学推理方面；训练方法上讨论了\"Reinforcement learning with verifiable rewards (RLVR)\"，这与强化学习方法论直接相关。 从排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然论文在数学基准测试上评估了方法，但数学推理被视为通用推理能力的一部分，而非特定应用领域。论文的核心是改进推理模型的一般训练方法，而不是解决特定领域的问题。 论文的核心贡献是提出SIREN方法，通过选择性熵正则化来解决大型推理模型在强化学习训练中遇到的熵崩溃和过早收敛问题，从而提升模型的通用推理能力。这完全符合我的研究目标，即筛选致力于提高大语言模型本身通用推理能力的论文。",
                    "summary2": "本文旨在解决大型推理模型(LRMs)中强化学习与可验证奖励(RLVR)的熵崩溃和过早收敛问题。针对LRMs巨大的动作空间和长轨迹场景，我们提出了一种选择性熵正则化方法SIREN，通过top-p掩码和峰值熵掩码将探索限制在有意义的子集内，并采用自锚定正则化稳定训练。在五个数学基准测试上，SIREN实现了显著优于基线方法的性能，如AIME24/25上+6.6 maj@k的提升，有效促进了响应多样性并维持了适当熵水平。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)在提升大推理模型(Large Reasoning Models, LRM)的推理能力方面展现出巨大潜力。然而，该方法面临一个关键问题：熵崩溃(entropy collapse)和过早收敛(premature convergence)。朴素熵正则化(naive entropy regularization)是传统强化学习文献中鼓励探索的常用方法，但在大推理模型(LRM)的背景下无法解决这一问题。我们的分析表明，这种失败源于大推理模型(LRM)中巨大的动作空间(action space)和长轨迹(long trajectories)，当模型不加选择地探索所有可能的动作和状态时，容易引发全局熵爆炸(global entropy explosion)。\n\n为解决此问题，我们提出了SIREN(SelectIve entRopy rEgulatioN，选择性熵正则化)，该方法将探索限制在有意义的动作和状态子集上。SIREN通过两步熵掩码机制(entropy masking mechanism)实现这一目标，该机制包括top-p掩码(top-p mask)和峰值熵掩码(peak-entropy mask)。此外，正则化被转换为自锚定形式(self-anchored form)以稳定训练过程。在五个数学基准测试(mathematical benchmarks)中，SIREN相比以往与熵相关的RLVR方法取得了更优的平均性能，以在Qwen2.5-Math-7B模型上AIME24/25测试中maj@k指标提升6.6%为例。进一步分析证实，SIREN促进了更大的响应多样性(response diversity)，并将熵维持在适当水平，这有助于在整个训练过程中保持验证pass@k指标。这有效缓解了大推理模型(LRM)中RLVR常见的过早收敛问题。",
                    "inspiration_trace": "# 从问题到解决方案：SIREN方法的逻辑演进\n\n## 1. 宏观问题：大型推理模型中的探索困境\n\n作者首先关注到大型推理模型(LRMs)在强化学习与可验证奖励(RLVR)训练中面临的核心问题：**熵崩溃和过早收敛**。这一现象表现为策略在训练早期就变得过度确定性，导致模型输出趋同，最终降低训练效率和整体性能。这是一个影响LRMs推理能力的根本性问题。\n\n## 2. 传统解决方案及其局限性\n\n面对探索不足的问题，作者转向传统强化学习中常用的解决方案——**熵正则化**。这种方法通过增加策略的随机性来鼓励探索，在传统RL环境中已被证明有效。\n\n然而，作者通过实验发现，在LRM环境中，朴素熵正则化表现出强烈的**超参数敏感性**：小的熵系数几乎不产生效果，而大的熵系数则迅速触发\"熵爆炸\"。这表明传统方法在LRM环境中存在根本性的不适应，无法有效解决探索问题。\n\n## 3. 问题根源分析：从现象到本质\n\n作者深入探究为什么传统熵正则化在LRM中失效，提出了一个关键假设：**LRM的巨大动作空间和长轨迹**可能是导致问题的根本原因。\n\n为验证这一假设，作者进行了对比实验，比较了原始模型和经历熵爆炸的模型：\n\n**关键发现**：\n1. **动作空间层面**：在熵爆炸模型中，概率分布在整个词汇表上几乎均匀，而在原始模型中，概率质量集中在小的、语义有意义的子集上。\n2. **轨迹层面**：在熵爆炸模型中，几乎所有令牌位置的熵都保持均匀高，而在原始模型中，只有一小部分令牌表现出高熵。\n\n**结论**：LRM的巨大动作空间使得通过将概率质量扩散到许多低效用令牌来增加熵变得容易。自回归生成过程将这种不确定性向前传播，导致熵随序列长度累积并最终爆炸。\n\n## 4. 概念提炼：构建解决方案的理论基础\n\n基于上述分析，作者提炼出两个关键概念，为解决方案奠定基础：\n\n### 4.1 策略核(Policy Nucleus)\n- **定义**：词汇表中包含最重要语义令牌的子集，这些令牌的生成是合理的，不太可能导致不连贯响应。\n- **洞见**：探索应该集中在策略核内，而不是在整个词汇表上均匀分布。这为解决巨大动作空间问题提供了方向。\n\n### 4.2 关键令牌(Critical Tokens)\n- **观察**：在长轨迹中，只有一小部分令牌在探索中起着不成比例的重要作用，通常对应于决定推理方向的句子开头或逻辑连接词。\n- **特征**：这些关键令牌往往表现出相对较高的熵。\n- **洞见**：令牌级熵可用于识别关键令牌，允许有选择地应用正则化。这为解决长轨迹问题提供了思路。\n\n## 5. 解决方案设计：SIREN方法的提出\n\n基于上述概念和洞见，作者提出了**SIREN(Selective Entropy Regularization)**方法，核心思想是将探索限制在更有效的动作和状态子集中。\n\n### 5.1 Top-p掩码：解决动作空间问题\n- **机制**：将探索限制在策略核内，即累积概率达到阈值p的最小令牌子集。\n- **实现**：重新定义概率分布，仅在该子集内计算熵，避免在无意义令牌上分散概率质量。\n\n### 5.2 Peak-entropy掩码：解决轨迹问题\n- **机制**：选择熵位于轨迹内τ-分位数以上的令牌作为关键令牌。\n- **实现**：仅在这些选定的关键令牌上聚合熵，避免在长轨迹中所有位置均匀增加熵。\n\n### 5.3 自锚定正则化：稳定训练\n- **动机**：朴素熵正则化（最大化熵）在LRM环境中容易陷入两个极端：过高导致全局爆炸，过低无法缓解过早收敛。\n- **创新**：将正则化定义为聚合熵与熵锚之间的均方误差(MSE)，并将熵锚初始化为模型初始状态的熵值。\n- **优势**：这种自锚定机制使模型保持适当的探索水平，既不导致爆炸也不陷入过早收敛。\n\n## 6. 实验验证：从理论到实践\n\n作者通过广泛的实验验证了SIREN的有效性：\n\n### 6.1 性能提升\n- 在五个数学基准测试上，SIREN优于之前的熵相关RLVR方法。\n- 在AIME24/25上，使用Qwen2.5-Math-7B实现了+6.6 maj@k的显著改进。\n- 在不同规模的模型上（7B、1.5B）和不同架构的模型上（Qwen、LLaMA）均表现出一致的改进。\n\n### 6.2 机制分析\n- **探索促进**：SIREN实现了更高的pass@k和困惑度(PPL)，表明它有效促进了更广泛的探索。\n- **训练动态**：SIREN在训练早期保持高熵水平，然后逐渐收敛，确保持续探索同时保持性能。\n- **消融研究**：移除任何单个组件都会导致性能下降，证实了三个组件的互补性和必要性。\n\n## 7. 逻辑演进总结\n\n作者提出SIREN方法的逻辑链展现了从宏观问题到具体解决方案的系统性思考过程：\n\n1. **问题识别**：LRMs中的熵崩溃和过早收敛\n2. **方案评估**：传统熵正则化在LRM中的失效\n3. **根源分析**：巨大动作空间和长轨迹导致熵爆炸\n4. **概念提炼**：策略核和关键令牌的概念构建\n5. **方案设计**：SIREN的三组件选择性熵正则化\n6. **实验验证**：性能提升和机制分析\n\n这一逻辑链条体现了作者从观察现象、分析本质、提炼概念到设计解决方案的完整科研思维过程，为解决大型推理模型中的探索问题提供了新的思路和方法。"
                },
                {
                    "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
                    "arxiv_id": "2509.25140",
                    "authors": "Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister",
                    "summary": "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步核心判断：这篇论文的本质是提出ReasoningBank，一种新的记忆框架，用于增强大语言模型代理的推理能力。论文核心关注如何让代理从自身经验中提炼可泛化的推理策略，并通过记忆机制实现自我进化，这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的范畴，因此应该保留。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注\"large language model agents\" - 能力方向：聚焦于\"reasoning strategies\"和\"generalizable reasoning\" - 训练方法：涉及\"self-evolving\"概念 - 新兴范式：研究\"llm-based agents\"及其记忆机制 第三步排除标准：论文没有主要聚焦于任何排除领域。虽然提到了\"web browsing and software engineering benchmarks\"，但这些仅是评估方法的基准测试，而非论文主要焦点。 第四步特殊情况处理：论文提出的是通用的智能体记忆框架，用于增强LLM的通用推理能力和自我进化能力，而不是将智能体应用于特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种通过记忆机制来增强大语言模型代理通用推理能力的方法，使代理能够从经验中学习并自我进化，这与研究目标\"提高大语言模型的通用推理能力\"高度契合。",
                    "summary2": "本文旨在解决大型语言模型代理无法从积累的交互历史中学习的问题。针对连续任务流场景，我们提出了一种ReasoningBank记忆框架，从代理自判断的成功和失败经验中提炼可泛化推理策略，并引入记忆感知测试时扩展(MaTTS)来加速学习过程。在WebArena、Mind2Web和SWE-Bench-Verified基准上，通过成功率和交互步骤数验证了其有效性，相比现有方法在有效性上提升达34.2%，效率上减少16.0%交互步骤。",
                    "summary_translation": "随着大型语言模型代理（large language model agents）在持续现实世界角色（persistent real-world roles）中的应用日益广泛，它们自然会遇到连续的任务流（continuous streams of tasks）。然而，一个关键限制是它们无法从累积的交互历史中学习，导致它们被迫丢弃有价值的见解并重复过去的错误。我们提出了ReasoningBank，一种新颖的记忆框架（memory framework），它从代理自我判断的成功和失败经验（self-judged successful and failed experiences）中提炼出可泛化推理策略（generalizable reasoning strategies）。在测试时间（test time），代理从ReasoningBank检索相关记忆以指导其交互，然后将新的学习成果整合回去，使其能够随时间推移变得更加能干。\n\n基于这个强大的经验学习器，我们进一步引入了记忆感知的测试时间扩展（memory-aware test-time scaling, MaTTS），它通过扩展代理的交互经验（interaction experience）来加速和多样化这一学习过程。通过为每个任务分配更多计算资源，代理生成丰富多样的经验，这些经验为合成更高质量的记忆提供了丰富的对比信号（contrastive signals）。更好的记忆反过来指导更有效的扩展，在记忆和测试时间扩展之间建立了强大的协同作用。\n\n在网页浏览和软件工程基准测试（web browsing and software engineering benchmarks）中，ReasoningBank持续优于存储原始轨迹（raw trajectories）或仅存储成功任务例程（successful task routines）的现有记忆机制，同时提高了有效性和效率；MaTTS进一步放大了这些收益。这些发现将记忆驱动的经验扩展（memory-driven experience scaling）确立为一个新的扩展维度，使代理能够自我进化，并自然产生涌现行为（emergent behaviors）。",
                    "inspiration_trace": "# ReasoningBank核心方法的逻辑推演\n\n## 宏观问题：代理系统的经验学习困境\n\n作者首先观察到，随着大型语言模型(LLM)代理在持久现实世界角色中广泛应用，它们面临一个根本性问题：**代理无法从累积的交互历史中学习**。这导致它们不断重复过去的错误，丢弃有价值的见解，缺乏随时间推移而自我进化的能力。这一观察构成了研究的出发点。\n\n## 问题具体化：现有记忆机制的局限\n\n作者将宏观问题具体化为两个关键挑战：\n\n1. **内容局限性**：现有代理记忆方法主要存储原始轨迹或仅成功的工作流程，缺乏提炼更高层次、可转移的推理模式的能力。\n   \n2. **经验利用不均衡**：现有方法过度强调成功经验，忽视了失败经验中蕴含的宝贵教训，而这些失败教训恰恰是避免未来错误的关键。\n\n这一具体化揭示了问题的本质：不是代理没有记忆，而是记忆的质量和利用方式存在根本缺陷。\n\n## 核心假设：推理记忆的价值\n\n基于以上观察，作者形成了三个核心假设：\n\n1. **假设一**：如果能够从成功和失败经验中提炼出可泛化的推理策略，而不仅仅是存储原始轨迹，代理将能更有效地解决新任务。\n   \n2. **假设二**：如果建立一个闭环记忆系统，使代理能够检索相关记忆指导行动，并将新经验整合回记忆系统，代理将能持续进化。\n   \n3. **假设三**：如果通过增加计算资源生成多样化经验，将为合成更高质量记忆提供丰富的对比信号，形成记忆与扩展的协同效应。\n\n## 方法论发展：从理论到实践\n\n### ReasoningBank的设计\n\n基于假设一和二，作者设计了ReasoningBank记忆框架：\n\n1. **结构化记忆模式**：每个记忆项包含标题、描述和内容三部分，将原始经验提炼为可转移的推理单元，既保留高级策略，又去除低级执行细节。\n\n2. **闭环记忆流程**：\n   - **记忆检索**：代理根据当前查询检索相关记忆项\n   - **记忆构建**：任务完成后，从新经验中提取记忆项\n   - **记忆整合**：将新记忆项整合到ReasoningBank中\n\n3. **成功与失败经验的平衡利用**：通过LLM-as-a-judge判断经验成败，从成功中提取有效策略，从失败中提取预防性教训，形成全面的记忆库。\n\n### MaTTS的引入\n\n基于假设三，作者进一步提出记忆感知的测试时扩展(MaTTS)：\n\n1. **并行扩展**：为同一查询生成多个轨迹，通过对比不同轨迹识别一致的成功模式，过滤掉虚假解决方案。\n   \n2. **顺序扩展**：在单轨迹内迭代改进推理，将中间推理过程也作为记忆信号，丰富记忆内容。\n\n3. **记忆与扩展的协同**：高质量记忆引导扩展探索向更有希望的方向，而多样化经验又锻造更强记忆，形成正反馈循环。\n\n## 验证与优化：实验支持\n\n作者通过在Web浏览和软件工程基准测试上的广泛实验验证了方法的有效性：\n\n1. **ReasoningBank的优越性**：在有效性(相对提高34.2%)和效率(交互步骤减少16.0%)方面均优于现有方法。\n   \n2. **MaTTS的放大效应**：进一步提高了性能，特别是在ReasoningBank基础上效果最佳。\n   \n3. **协同效应验证**：实验证明记忆与测试时扩展之间存在强大的协同作用，验证了假设三。\n\n## 总结：从问题到解决方案的逻辑演进\n\n作者从观察到的代理无法从过去经验中学习的宏观问题出发，通过具体化现有方法的局限，形成关于推理记忆价值的假设，进而发展出ReasoningBank和MaTTS这一创新方法，最终通过实验验证了其有效性。这一逻辑链条体现了从问题观察到假设形成，再到方法论发展和实验验证的完整科研思路，为构建自适应和终身学习的代理系统提供了新途径。"
                },
                {
                    "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
                    "arxiv_id": "2509.24923",
                    "authors": "Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra",
                    "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究如何通过监督微调(SFT)和强化学习(RL)两种训练范式来提升大型语言模型(LLM)在序列决策任务中的探索能力。论文聚焦于多臂老虎机这一经典决策问题，提出了新的奖励设计方法来优化LLM的训练过程，并分析了不同训练方法对LLM行为的影响。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，特别是增强了LLM在决策制定和问题解决方面的通用能力，符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：研究sequential decision-making（序列决策），这涉及到planning和problem-solving能力，是推理能力的重要体现 - 训练方法：重点研究reinforcement learning (RL)作为训练LLM的方法 - 新兴范式：提到LLMs成为autonomous agents（自主智能体）的潜力，与llm-based agents相关 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 研究的是通用的序列决策问题（多臂老虎机任务），而非特定应用领域 - 没有主要聚焦于水印、安全性等模型可靠性问题 第四步：特殊和模糊情况 论文涉及通用智能体框架研究，讨论如何通过训练提高LLM作为自主智能体的决策能力，而不是将智能体应用于特定领域，因此应该保留。 综合判断：这篇论文的核心贡献在于研究如何通过强化学习等训练方法提升LLM在决策制定中的通用能力，探索-利用平衡是推理和决策的关键组成部分。论文提出的奖励设计方法和训练策略分析直接服务于提升LLM的通用推理能力，因此完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多臂赌博机任务中探索-利用权衡的问题。针对多臂赌博机任务，我们提出了一种通过监督微调和强化学习训练LLM的方法，设计了战略奖励和算法奖励两种新颖奖励信号，并在多种Gaussian和Bernoulli赌博机环境中通过累积遗憾、时间平均奖励和最优臂选择频率验证了其有效性。",
                    "summary_translation": "尽管大型语言模型（Large Language Models, LLMs）有望成为自主代理（autonomous agents），但它们在顺序决策（sequential decision-making）中通常进行次优探索（suboptimally explore）。近期研究试图通过监督微调（supervised fine-tuning, SFT）或强化学习（reinforcement learning, RL）来增强这一能力，在经典的多臂赌博机（multi-armed bandit）任务上改善了遗憾（regret）。然而，这些学习方法如何塑造探索策略及其泛化能力仍不明确。我们通过两种方式研究这两种范式：一是使用专家轨迹（expert trajectories）对LLMs进行SFT训练，二是采用一系列定制的奖励信号（tailored reward signals）进行RL训练，包括用于减少方差的战略性、基于遗憾的奖励（regret-shaped reward），以及能够实现神谕模仿（oracle imitation）的算法奖励（algorithmic reward）。由此产生的代理（agents）性能优于预训练模型，并与上置信界（Upper Confidence Bound, UCB）和汤普森采样（Thompson Sampling）相当，且能够稳健地泛化到6倍更长的时间范围（horizons）以及不同的赌博机家族（bandit families）。行为分析（behavioral analysis）表明，性能提升通常源于更复杂但更贪婪的利用（exploitation）：与预训练模型相比，RL/SFT代理更容易发生早期灾难性失败（early catastrophic failure），过早放弃探索。此外，被训练来模仿UCB的代理通过采用更具剥削性的变体（exploitative variants）学会了超越其\"教师\"（teacher）。我们的研究结果阐明了每种训练范式在何种情况下更为可取，并倡导超越平均遗憾（average regret）的定制奖励设计和评估，以促进稳健的探索行为。",
                    "inspiration_trace": "# 从观察到方法：LLM探索能力研究的逻辑演进\n\n## 1. 宏观问题：LLM在序列决策中的探索困境\n\n**观察起点**：大型语言模型(LLM)展现出作为自主智能体的潜力，但在序列决策任务中存在系统性缺陷——它们往往探索不足，倾向于短视的贪婪行为，过度利用已知奖励而牺牲探索。\n\n**具体表现**：在多臂赌博机(MAB)这一经典探索-利用权衡测试平台上，LLM即使经过精心提示，仍表现出次优的探索偏差，这成为其成为真正自主智能体的主要障碍。\n\n## 2. 现有方法与知识缺口\n\n**已有尝试**：研究者提出了两种主要训练范式来改善LLM的探索能力：\n- 监督微调(SFT)：通过专家轨迹训练，模仿最优探索算法(如UCB)\n- 强化学习(RL)：直接从环境奖励中学习有效策略\n\n**关键缺口**：作者识别出两个核心问题：\n1. **机制不明**：SFT和RL如何塑造LLM的探索策略尚不清楚\n2. **泛化未知**：这些策略如何泛化到更长视野和分布外环境缺乏研究\n\n## 3. 研究假设的形成\n\n基于上述缺口，作者提出了三层假设：\n\n**假设1(性能)**：SFT和RL训练能显著提升LLM在MAB任务上的探索能力，达到接近理论最优算法(如UCB、Thompson Sampling)的性能。\n\n**假设2(泛化)**：训练后的策略能够泛化到比训练环境更长的时间跨度和不同的奖励分布。\n\n**假设3(行为)**：仅关注平均遗憾会掩盖重要行为特征，训练可能导致策略偏向利用而牺牲长期鲁棒性。\n\n## 4. 方法设计：从比较到创新\n\n### 4.1 训练范式比较框架\n\n作者构建了一个系统性的比较框架，同时研究SFT和RL两种范式：\n\n**SFT方法**：使用UCB生成的专家轨迹进行监督学习，模型学习显式计算UCB值并做出决策。\n\n**RL方法**：采用PPO算法，将LLM决策过程建模为分层MDP，在token级别优化策略。\n\n### 4.2 奖励信号的创新设计\n\n为解决RL中的信用分配问题，作者设计了三种递进的奖励信号：\n\n**原始奖励(RL-OG)**：直接使用MAB环境的随机奖励\n- **局限**：奖励内在随机性导致信用分配困难和学习效率低下\n\n**策略奖励(RL-STR)**：基于即时遗憾设计\n- **创新点**：$\\tilde{r}_t = \\frac{\\mu_{A_t} - \\min_i \\mu_i}{\\mu^* - \\min_i \\mu_i} \\in [0, 1]$\n- **优势**：直接优化动作效用，简化信用分配，减少方差\n\n**算法奖励(RL-ALG)**：基于专家算法的二元信号\n- **创新点**：$r_t = 1$ 如果代理动作匹配神谕决策，否则为0\n- **优势**：绕过基于回报的信用分配，使代理自由发现内部算法\n\n### 4.3 评估方法的拓展\n\n超越传统的累积遗憾指标，作者引入了更丰富的行为诊断工具：\n\n- **贪婪频率**：选择贪婪动作的相对频率\n- **后缀失败率**：代理在时间t后不再选择最优臂的频率\n- **最佳臂频率**：选择最优臂的比例\n\n## 5. 实验验证与核心发现\n\n### 5.1 性能与泛化验证\n\n**发现1**：SFT和RL均能提升基础模型性能，RL-ALG和SFT表现最佳，甚至超过UCB教师策略。\n\n**发现2**：训练后的模型展现出强大的泛化能力，能够适应6倍长的视野和跨分布环境，RL策略在跨分布转移上比SFT更可靠。\n\n### 5.2 行为分析的意外发现\n\n**发现3**：训练后的模型表现出更强的贪婪倾向，虽然提高了平均回报，但牺牲了长期鲁棒性。\n\n**发现4**：模仿UCB学习的代理通过采用更贪婪的变体来超越其\"教师\"，揭示了训练过程中的\"利用偏差\"。\n\n**发现5**：RL-ALG发现了UCB的近似变体(如$Q_t(a) + C \\times \\sqrt{\\frac{\\log(N_t(a)+1)}{N_t(a)}}$)，混合了模仿和机会主义利用。\n\n## 6. 核心洞见：利用偏差的涌现机制\n\n作者最终揭示了\"利用偏差\"的涌现机制：\n\n**根本原因**：训练数据中探索信号稀疏而利用信号频繁，加上复杂的信用分配问题，导致训练过程偏向于优化短期奖励。\n\n**表现形式**：学习到的策略虽然平均性能优秀且泛化能力良好，但更容易出现早期灾难性失败，过早放弃探索。\n\n**理论意义**：这一发现挑战了仅通过平均遗憾评估策略充分性的传统观点，强调了在LLM训练中平衡探索与利用的重要性。\n\n## 7. 方法论贡献与启示\n\n作者的研究不仅提供了比较SFT和RL训练范式的系统框架，还提出了创新的奖励设计方法，特别是算法奖励(RL-ALG)为解决LLM中的信用分配问题提供了新思路。\n\n最终，这项研究倡导超越平均遗憾的定制奖励设计和评估方法，以促进稳健的探索行为，为未来LLM在序列决策任务中的训练提供了重要指导。"
                },
                {
                    "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity",
                    "arxiv_id": "2509.24836",
                    "authors": "Zhen Bi, Zhenlin Hu, Jinnan Yang, Mingyang Chen, Cheng Deng, Yida Xue, Zeyu Yang, Qing Shen, Zhenfang Liu, Kang Zhao, Ningyu Zhang, Jungang Lou",
                    "summary": "Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data.Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。从核心判断来看，论文的本质是改进LLM的基础逻辑推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了\"数据推理强度\"(DRI)这一新指标和相应的\"重新认知优化策略\"，通过优化训练数据的逻辑推理复杂性来提升LLM的逻辑推理能力，而不是简单增加数据量或改变数据格式。从正面指标看，论文明确研究大型语言模型(LLMs)，特别关注逻辑推理能力，并在强化学习框架下验证了方法。论文不涉及任何排除标准中的多模态与视觉、特定应用领域或模型可靠性的应用层面研究。这是一篇典型的致力于提高大语言模型本身通用推理能力的研究，通过新的训练范式增强模型的逻辑推理能力，与我的研究目标高度一致。",
                    "summary2": "本文旨在解决大语言模型逻辑推理能力受限于训练数据利用不足的问题。针对训练数据内部推理复杂性被忽视的现状，我们提出了一种数据推理强度(DRI)量化指标及再认知优化策略，并在Reclor、LogicBench、LogiQA和LogiQA2.0等逻辑推理基准上通过准确率等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（large language models, LLMs，大型语言模型）的最新进展凸显了训练数据结构和质量在塑造推理行为中的重要性。然而，大多数现有方法专注于转换数据格式，而忽视了训练样本的内部推理复杂性（internal reasoning complexity，内部推理复杂性），导致数据的推理潜力（reasoning potential，推理潜力）未被充分探索和利用。在本研究中，我们提出LLM逻辑推理性能（logical reasoning performance，逻辑推理性能）受到训练数据潜力和模型认知能力（cognitive capacity，认知能力）的共同约束。为了使这种关系可测量，我们引入了数据推理强度（Data Reasoning Intensity, DRI，数据推理强度），这是一种通过分解和聚合样本的逻辑结构来量化其潜在逻辑推理复杂性的新指标。这使我们能够分析当前LLMs如何有效利用逻辑推理信号（logical reasoning signals，逻辑推理信号），并识别相对于数据潜力的性能差距。基于这一见解，我们提出了一种再认知优化策略（re-cognizing optimization strategy，再认知优化策略），系统性地增强训练数据的逻辑推理强度。我们的方法不是增加数据量，而是重新优化现有样本，使其更好地与LLM的逻辑推理边界（logical reasoning boundary，逻辑推理边界）对齐。大量实验表明，我们的方法在性能和泛化能力方面显著优于以数据为中心的策略（data-centric strategies，以数据为中心的策略）。我们进一步在强化学习框架（reinforcement learning framework，强化学习框架）下验证了我们的方法。我们的结果表明，优先考虑数据中的推理复杂性（reasoning complexity，推理复杂性）而非纯粹规模或表面形式，对于实现LLMs的全部认知潜力（cognitive potential，认知潜力）至关重要。",
                    "inspiration_trace": "# 从观察到方法论：数据推理强度(DRI)的提出逻辑链\n\n## 宏观问题：LLMs逻辑推理能力的边界\n\n论文的出发点是一个宏观问题：如何突破大型语言模型(LLMs)的逻辑推理能力边界。作者观察到，尽管LLMs在逻辑推理任务上取得了显著进展，但如何从数据角度优化模型认知能力仍是一个核心挑战。\n\n## 观察一：现有方法的根本局限\n\n作者首先发现，当前数据优化方法存在本质瓶颈：\n- 现有方法仅停留在数据表达形式的表面转换\n- 忽视了对数据内部推理复杂性的深入探索\n- 面临两大挑战：缺乏对数据逻辑推理复杂性的高效量化，以及从模型推理边界角度分析数据的盲目性\n\n## 观察二：数据复杂性与模型性能的关系\n\n进一步研究表明：\n- 数据复杂性只有在结构合理、接近模型经验且处于其推理边界附近时，才能有效刺激LLMs的推理能力\n- 训练数据的质量结构对推理能力起决定性作用，精确数据选择优于简单数据增量\n- 当前LLMs远未充分利用可用推理数据的潜力，存在显著性能提升空间\n\n## 核心假设：双边界约束理论\n\n基于上述观察，作者提出核心假设：\n- LLM的逻辑推理性能受两个边界共同约束：训练数据的潜力和模型本身的认知潜力\n- 当前LLMs既未达到数据潜力的上限，也未触及自身认知能力的边界\n\n## 理论框架：推理效率模型\n\n受Roofline模型启发，作者构建了理论框架：\n- 将LLM逻辑推理性能视为数据驱动的推理潜力与模型内在认知成本的效率比率\n- 定义有效推理能力η(M, D) = E(D)/C(M)\n- 由于模型认知成本C(M)难以改变，提高性能的最实际路径是通过精心设计的数据来增强数据推理潜力E(D)\n\n## 方法一：数据推理强度(DRI)的提出\n\n为量化E(D)，作者提出了数据推理强度(DRI)指标：\n1. **分解阶段**：将每个样本的推理轨迹分解为逻辑元素（常量、谓词和逻辑表达式）\n2. **组合推理阶段**：基于分解的逻辑元素进行结构推理，生成推理步骤序列\n3. **量化计算**：\n   - 逻辑强度：从结构复杂性、谓词和常量数量三个维度计算\n   - 推理强度：结合前提条件强度和逐步演绎强度\n   - 归一化处理：应用对数压缩和sigmoid归一化，将结果映射到[0,1]区间\n\n## 方法二：重新认知优化策略\n\n基于DRI指标，作者提出了两阶段优化策略：\n\n1. **模型认知重塑阶段**：\n   - 根据DRI分数重新排序训练数据\n   - 让模型首先探索全谱系的DRI示例，建立广泛推理模式\n   - 受Sweller认知负荷理论启发，最小化外在认知负荷\n\n2. **认知推理增强阶段**：\n   - 根据归一化DRI分数引导模型注意力分配\n   - 使模型更关注高DRI样本，获得更大学习回报\n   - 基于资源理性分析，在有限资源下最大化推理收益\n\n## 实验验证与发现\n\n通过系统实验，作者验证了：\n- DRI能有效分离样本的潜在推理潜力，且与模型错误率呈正相关\n- 低DRI数据可安全修剪，高DRI数据是性能提升的催化剂\n- 重新认知优化策略在不同数据集和DRI区间上均显著降低错误率\n- 该方法在监督学习和强化学习框架下均有效\n\n## 结论：从数据复杂性到推理边界突破\n\n论文最终形成完整逻辑链：通过量化数据推理复杂性(DRI)，发现当前LLMs远未达到数据潜力和自身认知能力的边界，进而提出重新认知优化策略，系统性地增强训练数据的逻辑推理强度，从而推动LLMs接近其逻辑推理性能的理论边界。这一工作为理解和激活LLMs训练数据的未开发推理潜力提供了新视角。"
                },
                {
                    "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries",
                    "arxiv_id": "2509.24711",
                    "authors": "Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, Han Qiu",
                    "summary": "Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心贡献是研究大型推理模型(LRMs)对其能力边界的自我认知能力，并提出优化策略来提升模型的推理效率和可靠性。 首先，从核心判断角度看，论文本质上是关于改进LLM的基础推理能力的。论文研究的是模型如何认知自身能力边界，并提出两种优化策略（推理表达式监控和隐藏状态监控）来避免无效推理，这直接提升了模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确包含核心概念\"Large Reasoning Models (LRMs)\"，并聚焦于\"reasoning\"能力方向，特别是数学推理等复杂推理任务。虽然论文没有明确提到强化学习等训练方法或智能体等新兴范式，但其研究内容与提升LLM通用推理能力直接相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域，也不关注模型可靠性的应用层面（如水印、安全等）。 最后，在特殊情况下，论文关注模型对自身能力边界的认知，这可以视为一种提高模型内在可靠性和推理质量的方法，与减少幻觉和提高可解释性有一定关联，应该保留。 综上所述，这篇论文的核心贡献是通过研究模型对自身能力边界的自我认知，提出方法来优化推理过程，提高效率和可靠性，完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在研究大型推理模型(LRMs)是否具有对其能力边界的自我意识。针对LRMs在处理超出能力边界问题时进行无效推理的现象，我们提出了推理表达监测和隐藏状态监测两种边界感知策略，并在多个数学推理benchmark上通过准确率、硬拒绝率、token使用量和溢出率等指标验证了其有效性。实验表明这些策略使LRMs能够避免无效推理，将token使用量减少62.7%-93.6%，同时保持准确性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在数学等复杂推理任务上表现出令人印象深刻的性能，但它们也表现出一些暴露其局限性的不当行为。特别是，当面对难题时，LRMs经常进行无效推理直到达到上下文限制（context limit），产生错误答案的同时浪费大量计算资源。这一现象反映了一个根本问题：当前的回答范式（answering paradigms）忽略了问题与LRMs能力边界（capability boundaries）之间的关系。在本文中，我们研究了LRMs是否具备对自身能力边界的自我意识。我们首先观察到，LRMs可能通过表达的推理置信度（reasoning confidence）来了解自己无法解决的问题。对于黑盒模型（black-box models），我们发现推理表达揭示了边界信号，可解决问题呈现加速增长的置信度轨迹（confidence trajectory），而不可解决问题则呈现收敛的不确定性轨迹（uncertainty trajectory）。对于白盒模型（white-box models），我们表明最后一个输入标记（input token）的隐藏状态（hidden states）编码了边界信息，可解决问题和不可解决问题甚至在推理开始前就是线性可分的。基于这些发现，我们提出了两种简单而有效的优化策略：推理表达监控（reasoning expression monitoring）和隐藏状态监控（hidden states monitoring）。实验证明，这些具有边界意识的策略使LRMs能够在不牺牲准确性的情况下避免无效推理，通过减少62.7%至93.6%的标记（token）使用量，显著提高了可靠性和效率。",
                    "inspiration_trace": "# 大型推理模型能力边界自我意识的逻辑演进\n\n## 一、宏观问题：非生产性推理的困境\n\n作者从观察到一个普遍现象出发：大型推理模型(LRMs)在面对超出其能力范围的问题时，会陷入非生产性推理，持续生成内容直到上下文限制，不仅产生错误答案，还浪费大量计算资源。这一现象揭示了当前LRMs回答范式的根本缺陷——模型无法识别问题与其自身能力边界的关系，导致对不可解问题徒劳求解。\n\n## 二、核心假设：能力边界的自我意识\n\n基于上述问题，作者提出核心研究问题：**LRMs是否具有对其能力边界的自我意识？** 即模型是否\"知道\"自己不能解决某些问题。这一假设源于作者观察到的初步现象：LRMs在推理过程中表现出的人类化语气(如\"我不确定\"、\"可能是错的\")往往与最终答案的正确性高度一致。\n\n## 三、初步验证：表达信念与答案正确性的关联\n\n作者首先通过量化分析验证假设：\n1. **构建指标**：定义Can%和Cannot%两个指标，分别衡量模型表达信心时答案正确的概率和表达不确定性时答案错误的概率\n2. **实验发现**：当LRMs表达更多不确定性时，最终答案几乎总是错误的(Cannot% = 100%)\n3. **结论**：LRMs确实表现出对自身能力边界的认知，它们\"知道\"自己不能解决什么问题\n\n## 四、黑盒视角：推理表达轨迹分析\n\n假设得到初步验证后，作者从黑盒角度深入探究能力边界如何通过外部可观察的推理表达体现：\n\n1. **表达轨迹构建**：\n   - 识别自信表达(如\"我认为这是正确的\")和不确定表达(如\"我不确定\")\n   - 将推理过程分阶段，计算各阶段表达密度，形成表达密度轨迹\n\n2. **关键发现**：\n   - 可解问题：自信轨迹呈加速增长趋势(凹形)\n   - 不可解问题：不确定轨迹主导并趋于收敛(凸形)\n\n3. **量化指标设计**：\n   - 置信度差异(ConfDiff)：测量不确定与自信轨迹差距的累积符号\n   - 置信度曲率(ConfCurv)：计算表达密度轨迹的二阶导数\n\n4. **验证结果**：这些指标能在推理早期(如2%阶段)高精度区分可解与不可解问题\n\n## 五、白盒视角：隐藏状态中的边界编码\n\n进一步，作者探究能力边界信号是否在推理开始前就已存在于模型内部：\n\n1. **隐藏状态提取**：聚焦最后一个输入token的隐藏状态，它封装了整个问题的语义信息\n\n2. **实验设计**：\n   - 扩展数据集，包含简单(GSM8K)和极难(HLE)问题，放大能力边界差异\n   - 应用线性分类器(LDA、逻辑回归)测试可解与不可解问题的可分性\n\n3. **关键发现**：\n   - 可解与不可解问题在隐藏空间中几乎线性可分(准确率近100%)\n   - 接近能力边界的可解问题需要更长推理时间(1.5-2倍token使用)\n\n4. **结论**：能力边界信息在推理开始前就已编码在隐藏状态中\n\n## 六、方法设计：边界感知的推理策略\n\n基于黑盒和白盒分析的发现，作者设计两种测试时监测策略：\n\n### 1. 推理表达监测(Monitor_express)\n- **适用场景**：黑盒环境\n- **实施流程**：\n  1. 实时构建自信与不确定表达轨迹\n  2. 使用ConfDiff/ConfCurv指标判断问题是否超出能力边界\n  3. 对超出边界的问题提前终止推理，重新提示模型输出简洁方法\n\n### 2. 隐藏状态监测(Monitor_hidden)\n- **适用场景**：白盒环境\n- **实施流程**：\n  1. 提取最后一个输入token的隐藏状态\n  2. 使用预训练线性分类器预测问题是否可解\n  3. 对不可解问题约束输出以自我意识前缀开始，避免完整推理\n\n## 七、实验验证与效果评估\n\n作者通过系统实验验证所提策略的有效性：\n\n1. **评估指标**：\n   - 准确率(ACC)：衡量可解问题的解决质量\n   - 硬性放弃(HA)：衡量识别不可解问题的能力\n   - Token使用量：反映推理效率\n   - 溢出率：识别不完整响应的比例\n\n2. **关键结果**：\n   - 两种策略均能保持可解问题的准确性(ACC几乎不变)\n   - 显著提高对不可解问题的识别能力(HA接近100%)\n   - 大幅减少token使用(降低62.7%-93.6%)和溢出率(降低高达100%)\n   - 不同模型对策略响应不同：有些擅长指令跟随，有些更容易受输出引导\n\n## 八、逻辑演进总结\n\n作者从观察到方法论的完整逻辑链展现了清晰的科学思维过程：\n1. **问题识别**：发现LRMs非生产性推理的普遍现象\n2. **假设形成**：提出LRMs可能具有能力边界自我意识\n3. **初步验证**：通过表达信念与答案正确性的关联验证假设\n4. **黑盒分析**：从推理表达轨迹揭示能力边界信号\n5. **白盒分析**：从隐藏状态发现更早的边界编码\n6. **方法设计**：基于双重视角设计边界感知推理策略\n7. **实验验证**：证明策略在保持准确性的同时显著提高效率和可靠性\n\n这一研究不仅揭示了LRMs的内在工作机制，还为解决实际应用中的效率问题提供了有效方法，体现了从现象观察到理论创新再到实用解决方案的完整科研路径。"
                },
                {
                    "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
                    "arxiv_id": "2509.24869",
                    "authors": "Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian",
                    "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出Retro*方法，通过基于标准的相关性评分机制和定制的强化学习算法来优化LLM的推理能力。根据筛选标准，我判断这篇论文符合研究目标，原因如下： 首先，从本质上看，这篇论文的核心是改进LLM的基础推理能力，而非将LLM作为工具应用到特定领域。论文提出了一种新的推理方法（基于标准的相关性评分机制）和强化学习算法来增强LLM的推理能力，特别是在处理需要细粒度推理的文档检索任务时。 其次，论文包含多个正面指标：明确关注LLMs的reasoning能力，采用了reinforcement learning作为训练方法，并提到了LLM agents和RAG等新兴范式。 第三，论文不主要聚焦于排除标准中的领域。虽然应用场景是文档检索，但这不是一个特定领域应用（如医疗、化学等），而是一个通用任务。论文的焦点是提高LLM的推理能力，而不是文档检索本身。 最后，虽然论文涉及文档检索这一应用场景，但其提出的方法（基于标准的相关性评分机制、测试时扩展的多个推理轨迹结合、定制的强化学习算法）都是通用的，旨在提高LLM的推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。 因此，这篇论文符合研究范围。",
                    "summary2": "本文旨在解决需要密集推理的文档检索问题，特别是当文档与任务的连接是间接或隐含的情况。针对这种推理密集型检索场景，我们提出了一种Retro*方法，它采用基于rubric的相关性评分机制，并通过分数整合支持测试时扩展。我们在BRIGHT benchmark上通过nDCG@10指标验证了其有效性，结果表明Retro*优于现有文档检索方法，取得了最先进的性能。",
                    "summary_translation": "随着大型语言模型代理（LLM agents）和检索增强生成（RAG）的日益普及，检索对解决任务至关重要的文档变得越来越重要，即使这些文档与任务的连接是间接或隐含的。解决这一问题需要细粒度推理来准确评估任务与每个候选文档之间的相关性。然而，这种能力对现有信息检索（IR）技术构成了重大挑战。尽管推理增强的IR领域最近取得了进展，现有方法在适用性、可扩展性和效率方面仍面临显著挑战。在这项工作中，我们提出了Retro*，一种针对推理密集型文档检索的新方法。我们的方法引入了基于标准（rubric-based）的相关性评分机制，使模型能够根据明确定义的标准推理任务与文档之间的关系，从而产生细粒度、可解释的相关性分数。Retro*还通过分数整合（score integration）结合多个推理轨迹（reasoning trajectories）来支持测试时扩展（test-time scaling），从而产生更可靠的相关性估计。为了优化Retro*的推理能力，我们引入了一种针对其相关性评分机制定制的新型强化学习（reinforcement learning）算法，该算法采用两个复合奖励来充分利用每个训练样本的轨迹。我们的实验表明，Retro*以显著优势优于现有文档检索方法，在BRIGHT基准（BRIGHT benchmark）上实现了最先进的性能。",
                    "inspiration_trace": "# Retro*方法逻辑演进分析\n\n## 1. 宏观问题识别：推理密集型文档检索的挑战\n\n作者从当前信息检索领域的一个核心挑战出发：随着LLM智能体和RAG系统的普及，需要检索那些与任务有间接或隐含关联的文档，而非仅直接匹配的文档。这种需求在多个领域普遍存在：\n- 软件工程：需要找到与目标问题共享相似设计模式的程序\n- 数学：需要检索基于相同底层定理的证明，即使表达方式不同\n- 科学研究：需要找到能提供理论支持的文献\n\n这种检索任务需要细粒度的推理能力，而现有IR技术主要设计用于捕获直接语义关系，难以处理这种复杂推理需求。\n\n## 2. 现有方法局限性分析\n\n作者深入剖析了当前推理增强型IR方法的三个关键瓶颈：\n\n1. **缺乏相关性测量功能**：现有方法主要提供相对排序，无法提供下游任务所需的相关性绝对水平测量。\n\n2. **测试时扩展性不灵活**：现有方法专注于生成单一推理路径，忽略了探索和整合多个推理路径的潜力。\n\n3. **并行能力有限**：逐列表或逐集合的方法必须顺序处理整个候选集，在处理大量文档时效率低下。\n\n## 3. 核心假设形成\n\n基于问题分析和局限性研究，作者形成了四个关键假设：\n\n1. **假设1**：明确定义相关性评分标准可以引导LLM进行更细粒度推理，产生更准确、可解释的相关性评分。\n\n2. **假设2**：生成和整合多个推理轨迹可以提高相关性估计的可靠性和稳定性。\n\n3. **假设3**：点对点评估方法可以支持大规模并行处理，提高推理效率。\n\n4. **假设4**：专门设计的强化学习算法可以同时优化模型准确评分和有效区分文档的能力。\n\n## 4. 方法设计：Retro*的核心创新\n\n基于上述假设，作者设计了Retro*方法，包含两个核心组件：\n\n### 4.1 基于标准的相关性评分机制\n\n作者引入细粒度的相关性标准集，包含两部分：\n- **相关性定义**：声明检索任务的具体意图\n- **评分标准**：定义0-100分数范围及五个区间的明确解释（高度相关、相关、中度相关、轻微相关、不相关）\n\n这种设计使模型能够直接测量相关性，而非仅提供相对排序。\n\n### 4.2 通过分数整合实现测试时扩展\n\n为提高检索准确性，Retro*为每个查询-文档对采样K次，产生一组推理轨迹，并通过加权平均整合分数：\n```\n¯s ← Σ(wi * si) / Σwi\n```\n\n## 5. 训练策略：两阶段优化\n\n为充分发挥设计潜力，作者开发了两阶段训练策略：\n\n### 5.1 监督微调(SFT)阶段\n- **数据来源**：利用强大教师模型生成推理轨迹\n- **数据过滤**：通过分数整合筛选高质量轨迹，选择最接近整合分数的轨迹\n\n### 5.2 强化学习(RL)阶段\n引入复合奖励设计，同时优化两个功能：\n- **文档内奖励**：提高单个文档评分准确性，奖励接近整合分数的轨迹\n- **文档间奖励**：提高文档排序性能，奖励能正确区分相关与不相关文档的轨迹\n\n复合奖励公式：\n```\nR(y, s) = α · Rintra(y, s) + (1-α) · Rinter(y, s)\n```\n\n## 6. 实验验证与效果确认\n\n在BRIGHT基准测试上的实验验证了Retro*的有效性：\n\n1. **性能优势**：Retro*显著优于现有方法，实现了最先进的性能\n2. **测试时扩展效果**：多轨迹整合进一步提高了性能，7B模型+16次采样甚至超过了标准32B模型\n3. **并行效率**：点对点设计使其在处理大量候选文档时比其他方法更高效\n4. **相关性测量可靠性**：基于标准的评分机制提供了更清晰、可解释的相关性测量\n\n## 7. 逻辑演进总结\n\nRetro*方法的逻辑演进展示了从宏观问题到创新解决方案的完整思维链：\n\n**问题识别** → **局限性分析** → **假设形成** → **方法设计** → **训练策略** → **实验验证**\n\n这一演进过程体现了作者如何通过深入分析现有方法的不足，形成创新假设，设计出既能提供可解释相关性测量，又支持测试时扩展和高效并行的解决方案，最终通过实验验证其有效性，为推理密集型文档检索任务提供了新的方法论。"
                },
                {
                    "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention",
                    "arxiv_id": "2509.24393",
                    "authors": "Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu",
                    "summary": "Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进大型推理模型(LRMs)的安全推理能力。论文提出了一种新的训练范式\"干预偏好优化\"(IPO)，专门针对思维链(CoT)推理过程中可能存在的有害内容进行改进。这属于改进LLM基础能力的范畴，特别是增强其推理过程的安全性和可靠性，而不是将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"Large Reasoning Models (LRMs)\"，这是专注于推理能力的大语言模型 - 能力方向：核心关注\"reasoning\"，特别是\"chain-of-thought (CoT) reasoning\"，这正是通用推理能力的关键表现形式 - 训练方法：涉及\"process supervision\"和\"preference learning\"，这与强化学习方法相关 第三步排除标准：论文不涉及任何应排除的领域： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 虽然涉及安全性，但是从推理过程本身的角度，而非应用层面的水印或安全技术 第四步特殊情况处理：论文属于提出新方法来增强模型内在安全性的情况。作者提出的IPO方法通过改进推理过程的安全性来提升模型的通用可靠性和推理质量，这符合\"如果论文提出一种新方法来增强模型内在的安全性，从而提升模型的通用可靠性和推理质量，应该保留\"的标准。 核心贡献：论文提出了干预偏好优化(IPO)方法，通过将合规步骤替换为安全触发器，并构建偏好学习对来强制执行安全推理，从而提升大型推理模型在推理过程中的安全性。这种方法是通用的，旨在提高LLM的内在推理能力，特别是推理过程的安全性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型推理模型(LRMs)中的推理安全问题。针对LRMs在思维链推理中可能包含有害内容的问题，我们提出了一种Intervened Preference Optimization (IPO)方法，通过将符合性步骤替换为安全触发器并进行偏好学习，在越狱和对抗性安全基准上通过有害性比率等指标验证了其有效性，实现了推理和响应安全性的显著提升。",
                    "summary_translation": "尽管大型推理模型 (Large Reasoning Models, LRMs) 在解决复杂问题方面取得了进展，但其思维链 (chain-of-thought, CoT) 推理经常包含有害内容，即使最终回答看起来安全，这些有害内容仍然可能存在。我们表明，在现有方法中这个问题仍然存在，这些方法忽视了安全推理的独特重要性，从而破坏了它们的可信度，并在不安全推理被恶意用户获取和利用的情况下，在应用中构成潜在风险。因此，在本文中，我们将注意力转向推理本身的安全性对齐，并探索过程监督 (process supervision) 作为解决方案。然而，由于低展开多样性 (rollout diversity) 和有限的训练信号，简单奖励安全推理被证明是不充分的。为了应对这一挑战，我们首先深入研究安全推理的特性，并发现了几个关键见解：1) 安全推理通常由几个关键的安全触发步骤 (safety triggers) 巩固；2) 顺从提示 (compliance cues) 与不安全延续强烈相关；3) 纠正干预 (corrective interventions) 能可靠地将不安全轨迹引导向更安全的轨迹。受这些见解的启发，我们提出了干预偏好优化 (Intervened Preference Optimization, IPO)，这是一种通过对齐方法，通过用安全触发器替代顺从步骤，并构建具有强信号的偏好学习 (preference learning) 对来强制执行安全推理。在越狱 (jailbreak) 和对抗性安全基准 (adversarial safety benchmarks) 上的实验表明，IPO显著提高了推理和回答的整体安全性，优于基于监督式微调 (SFT) 和强化学习 (RL) 的基线方法，有害性相对减少超过30%，同时在不同推理任务中保持优异性能。这些结果突显了推理显式对齐的重要性，并为构建更安全的大型推理模型提供了实用路径。",
                    "inspiration_trace": "# 大型推理模型安全推理方法的逻辑演进\n\n## 一、宏观问题：推理过程中的安全隐患\n\n**出发点**：大型推理模型(LRMs)在解决复杂问题上取得了显著进展，但其链式思维(CoT)推理过程中常含有害内容，即使最终回答看似安全。这一问题在现有方法中被忽视，主要因为：\n- 现有安全对齐方法主要关注最终回答的安全性\n- 恶意用户可能访问并利用不安全的推理过程，带来潜在风险\n- 在医疗、金融、法律等关键领域部署时，这种隐患尤为危险\n\n## 二、关键观察：推理安全与回答安全的脱节\n\n**实证发现**：作者评估了现有安全对齐的LRMs(如RealSafe和STAR)，发现：\n1. 即使在简单恶意提示(JailbreakBench)上，最终回答通常安全，但推理过程仍显示不安全模式\n2. 在更具对抗性的基准测试(StrongReject和WildJailbreak)上，这种差距更为明显\n3. 安全推理往往导致安全回答，但安全回答不一定来自安全推理\n\n**核心洞察**：推理级别的安全性被现有方法忽视，但却是更可靠的安全路径。\n\n## 三、初步尝试与局限：强化学习的不足\n\n**假设验证**：作者尝试使用强化学习(GRPO)直接奖励安全推理，但发现：\n1. 虽然强调推理安全性有所改善，但绝对安全分数仍然有限\n2. 约50%的有害提示几乎无法生成安全推理轨迹，导致训练信号不足\n3. 低滚动多样性限制了强化学习的有效性\n\n**结论**：需要一种更有效的方法来增加安全推理的多样性并提供强训练信号。\n\n## 四、深入分析：推理过程中的安全动态\n\n**系统性研究**：作者分析了推理过程中安全性的演变模式，发现了三个关键特性：\n\n1. **安全触发步骤(Safety Triggers)**：\n   - 安全推理轨迹中存在关键步骤，之后安全延续概率接近100%\n   - 这些步骤通常包含明确承认风险、重构任务或调用安全指南的内容\n   - 超过90%的安全轨迹包含此类转折点\n\n2. **服从线索(Compliance Cues)**：\n   - 不安全推理轨迹中的特定步骤，表示倾向于遵守用户恶意请求\n   - 这些线索与不安全延续强烈相关(皮尔逊相关系数0.85)\n   - 一旦出现，模型越来越倾向于解决问题导向的思维链\n\n3. **干预效果(Corrective Interventions)**：\n   - 用安全触发步骤替代服从线索可显著降低延续的有害性\n   - 即使是最小干预也能有效将推理引导至安全方向\n   - 这种干预具有累积效应，可重复应用直到获得安全延续\n\n## 五、方法论形成：干预偏好优化(IPO)\n\n**核心思想**：基于上述发现，作者提出了干预偏好优化(IPO)，通过在关键安全步骤进行显式监督来巩固安全推理。\n\n**方法设计**：\n1. **干预生成**：\n   - 识别推理轨迹中的第一个服从线索\n   - 用从安全触发池中采样的安全触发步骤替代该线索\n   - 生成安全的推理延续\n\n2. **偏好构建**：\n   - 将原始轨迹和干预后的轨迹形成偏好对\n   - 这些对共享相同前缀但在干预步骤处出现分歧\n\n3. **偏好学习**：\n   - 使用直接偏好优化(DPO)在这些对上训练模型\n   - 鼓励模型偏好被纠正的安全延续而非原始不安全服从\n\n**技术优势**：\n- 通过主动干预增加滚动多样性，克服强化学习的局限性\n- 在关键步骤提供局部奖励信号，比稀疏最终奖励更高效\n- 最小化干预和分布内采样，限制安全对齐中的分布偏移\n\n## 六、验证与效果：安全与性能的双赢\n\n**实验结果**：\n1. **安全性提升**：\n   - 在多个安全基准测试上，推理有害率显著降低(相对减少超过30%)\n   - 最终回答的安全性也达到或超过现有基线\n   - 整体平均安全性表现最佳\n\n2. **能力保持**：\n   - 在数学、编程和科学推理等多项任务上保持甚至提高了核心推理能力\n   - 在安全性与实用性之间取得了良好平衡\n\n3. **效率优势**：\n   - 相比GRPO，计算成本显著降低(生成需求减少约65%)\n   - 训练时间大幅缩短(约40分钟 vs 2小时以上)\n\n## 逻辑演进总结\n\n从\"推理过程存在安全隐患\"的宏观问题出发，作者通过观察发现\"推理安全与回答安全脱节\"的现象，提出\"应优先考虑推理级别安全性\"的假设。在验证过程中，发现强化学习方法因低滚动多样性而效果有限，进而深入分析推理过程中的安全动态，识别出安全触发步骤、服从线索和干预效果三个关键特性。基于这些发现，最终形成了干预偏好优化(IPO)方法，通过在关键安全步骤进行显式监督，有效解决了大型推理模型的安全推理问题，同时保持了模型的推理能力。这一逻辑链条展示了从问题观察到方法形成的完整思维过程。"
                },
                {
                    "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit",
                    "arxiv_id": "2509.24248",
                    "authors": "Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen",
                    "summary": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，应该被保留。根据筛选标准，我的判断过程如下： 第一步核心判断：论文的本质是关于改进大型推理模型(LRMs)的推理效率，解决\"过度思考\"问题。论文提出的SpecExit框架通过轻量级草稿模型预测未来token和提前退出信号，减少不必要的计算，同时保持准确性。这属于改进LLM基础能力和推理效率的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标。核心概念方面，它明确关注\"large reasoning models (LRMs)\"，这是LLMs的一个子集；能力方向方面，论文直接针对\"reasoning tasks\"和推理效率问题进行研究。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用推理模型的效率优化。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 论文的核心贡献是提出一种新的框架来加速大型推理模型的推理过程，减少计算开销同时保持准确性，这直接提升了大语言模型的通用推理能力，符合我的研究目标。",
                    "summary2": "",
                    "summary_translation": "尽管在推理任务上表现强劲，大型推理模型（large reasoning models, LRMs）常常遭受过度思考（overthinking）的问题，产生不必要的长输出并导致高端到端延迟（end-to-end latency），这对其在现实世界中的部署构成了重大限制。为解决过度思考问题，早期退出机制（early-exit mechanisms）被提出，用于在典型完成前终止推理过程，表明这种方法可以在最小化对准确性影响的情况下有效缩短生成长度。然而，这些机制对探测机制（probing mechanisms）的依赖引入了检测开销（detection overhead），限制了它们在端到端延迟方面的收益，并损害了它们在不同问题上的泛化能力。受投机解码（speculative decoding）中隐藏状态（hidden states）使用的启发，我们提出了SpecExit，这是一个新颖的框架，能够直接从轻量级草稿模型（lightweight draft model）中预测未来令牌（tokens）和早期退出信号，而无需探测开销。我们的方法提供了显著改进，与投机解码基线相比，平均生成长度减少了66%，端到端延迟实现了2.5倍的加速，同时没有损害准确性。我们的方法利用隐藏状态中的固有信号来提供有效的早期退出信号，这表明隐藏状态在高效推理中有更广泛的应用前景。我们的代码可在https://github.com/Tencent/AngelSlim获取。",
                    "inspiration_trace": "`标记内的推理内容，识别有效推理轨迹(保留产生正确答案的最小推理段)\n- **信号标注**:\n  - 置信度: 预测步骤上logit概率的几何平均\n  - 剩余推理长度: 从初始``仍产生正确答案的token数\n  - 进度: 从0到1的归一化值，表示CoT的相对进展\n- **多任务学习**: 联合优化token分类和信号回归，使用动态加权平衡不同任务\n\n**3. 信号引导推理**\n- **停止条件**: 引入\"步骤分割token\"(如段落分隔符`\\n\\n`)作为自然分段点，当信号超过阈值时触发早期退出\n- **信号平滑**: 使用指数加权移动平均(EWMA)平滑信号，增强鲁棒性\n- **与推测解码集成**: 在推测解码流程中，目标模型不仅验证token，还提取隐藏状态生成早期退出信号\n\n#### 第六步：验证假设并优化方法\n\n作者通过实验验证SpecExit的有效性，并进行了一系列优化：\n\n**1. 主要结果**\n- 在多个基准测试(GSM8K、MATH500、AIME等)上，SpecExit显著减少推理轨迹(最多66%)\n- 实现高达2.5倍的端到端延迟加速\n- 保持与基线相当或更高的准确性\n\n**2. 消融研究**\n- **信号类型**: 比较单一信号与组合信号的效果，发现组合信号能平衡准确性和效率\n- **信号平滑**: 比较不同平滑策略，EWMA提供最稳定的平衡\n- **步骤分割token**: 比较段落分隔符与话语标记，发现段落分隔符更具通用性\n\n**3. 实际部署**\n- 在PyTorch和vLLM中实现SpecExit，证明其易于部署和实际应用价值\n\n#### 第七步：形成理论贡献和实践意义\n\n基于实验结果，作者总结了SpecExit的理论贡献和实践意义：\n\n**理论贡献**:\n- 证明隐藏状态包含可用于早期退出的丰富推理信号\n- 提出一种不修改目标模型、无探测开销的早期退出机制\n- 展示多任务学习在联合优化token预测和信号提取中的有效性\n\n**实践意义**:\n- 为LRMs的实际部署提供高效解决方案\n- 开源代码促进社区采用和进一步研究\n- 为利用隐藏状态提高模型效率提供新思路\n\n### 完整逻辑链总结\n\n从宏观问题到具体解决方案，作者的逻辑链可总结为：\n\n1. **问题识别**: LRMs的过度思考导致冗余输出和高延迟\n2. **现有方法分析**: 推理时方法有探测开销，基于训练的方法改变输出分布，推测解码不解决过度思考\n3. **关键观察**: 隐藏状态编码推理充分性信号，可替代昂贵探测\n4. **核心假设**: 扩展推测解码框架，利用隐藏状态同时预测token和早期退出信号\n5. **方法设计**: SpecExit框架，包括扩展MTP层、信号提取训练和信号引导推理\n6. **验证优化**: 实验证明有效性，消融研究优化关键组件\n7. **贡献总结**: 理论上证明隐藏状态的价值，实践上提供高效部署方案\n\n这一逻辑链体现了从问题观察到假设形成，再到方法设计和验证的完整科研过程，最终提出了一个既创新又实用的解决方案。</think># SpecExit方法逻辑链推演：从问题观察到解决方案\n\n## 第一步：识别核心问题 - 大型推理模型的\"过度思考\"现象\n\n**观察现象**：大型推理模型(LRMs)如OpenAI-o1、DeepSeek-R1等在复杂推理任务上表现优异，但存在明显的\"过度思考\"(overthinking)问题。这些模型产生不必要冗长的思维链(CoT)输出，导致两个关键瓶颈：\n- **计算资源浪费**：生成过多冗余token\n- **端到端延迟过高**：限制了实际应用场景的部署\n\n**问题根源**：LRMs遵循\"测试时间缩放定律\"，即更长CoT通常提升性能，但过度依赖扩展推理导致冗余。作者发现这种冗余不是必要的，存在优化空间。\n\n## 第二步：分析现有解决方案的局限性\n\n**推理时早期退出方法**：\n- **原理**：通过监控模型信号(如中间答案、输出logits)在检测到足够证据时终止解码\n- **关键局限**：引入\"探测开销\"(detection overhead)，实际延迟收益有限；通用性差，效果高度依赖任务和模型(如图1a显示DEER方法在某些情况下甚至增加延迟)\n\n**基于训练的方法**：\n- **原理**：通过强化学习或监督微调改变模型行为\n- **关键局限**：可能改变模型输出分布；需要大量再训练成本；在未见任务上泛化能力差\n\n**推测解码(Speculative Decoding)**：\n- **优势**：通过轻量级草稿模型并行验证候选token，减少每token延迟，不改变目标模型输出\n- **局限**：单独使用不能解决过度思考问题，模型仍生成完整CoT\n\n**核心矛盾**：现有方法无法同时满足三个条件——不改变目标模型输出、无额外探测开销、有效解决过度思考。\n\n## 第三步：关键观察 - 隐藏状态中的推理信号\n\n**实验发现**：作者通过初步实验发现，模型的隐藏状态(hidden states)编码了关于推理过程的丰富信息。如图1b所示，使用简单MLP训练隐藏状态预测推理进度，结果显示：\n- 复杂任务初期，预测信号较暗(需继续推理)\n- 随着推理充分，信号逐渐变浅(可终止)\n\n**洞察**：隐藏状态提供任务复杂性和推理充分性的细粒度指标，可作为高效早期退出的自然信号源，替代昂贵的探测机制。\n\n## 第四步：形成核心假设\n\n基于隐藏状态的观察，作者提出核心假设：\n> **可以扩展推测解码框架，利用草稿模型的隐藏状态同时预测未来token和早期退出信号，在不引入探测开销的情况下实现高效早期退出。**\n\n假设的关键要素：\n1. **不修改目标模型**：保持原始输出分布\n2. **无额外探测开销**：通过扩展草稿模型实现\n3. **利用隐藏状态的丰富信息**：提取已编码的推理充分性信号\n\n## 第五步：设计SpecExit框架\n\n### 1. 整体架构设计\n**核心创新**：扩展多token预测(MTP)层，在标准token预测基础上增加三个辅助预测头：\n- **置信度(confidence)**：反映生成可靠性，定义为预测步骤logit概率的几何平均\n- **推理进度(progress)**：0-1归一化值，表示CoT相对进展\n- **剩余推理长度(remaining reasoning length)**：估计完成推理还需的token数\n\n**技术实现**：\n```\nW h = [Wtok h, Wconf h, Wprog h, Wrem h]\n```\n其中Wtok h产生标准token预测，其他三个矩阵分别预测三种信号。\n\n### 2. 信号提取训练\n**数据构建**：\n- 获取模型完整响应，提取``标记内的推理内容\n- 识别有效推理轨迹：保留产生正确答案的最小推理段，后续视为冗余\n\n**信号标注与回归**：\n- **多任务学习目标**：L = Lcls + λcLconf + λpLprog + λrLrem\n- **损失函数设计**：\n  - 置信度和进度：MSE损失\n  - 剩余推理长度：MSLE损失(处理大范围值)\n- **动态加权**：基于梯度幅度平衡不同任务贡献，防止大梯度任务主导学习\n\n### 3. 信号引导推理\n**停止条件设计**：\n- **步骤分割token**：引入自然分段点(如段落分隔符`\\n\\n`)作为语义连贯边界\n- **信号阈值**：当平滑信号超过预设阈值时触发早期退出\n- **信号平滑**：使用指数加权移动平均(EWMA)增强鲁棒性，避免噪声干扰\n\n**与推测解码集成**：\n- 草稿模型提出候选token和信号\n- 目标模型并行验证token并生成最终信号\n- 当信号超过阈值且遇到步骤分割token时，插入`</think>`终止推理\n\n## 第六步：验证假设并优化方法\n\n### 1. 主要实验结果\n在多个基准测试(GSM8K、MATH500、AIME等)上验证：\n- **效率提升**：减少推理轨迹66%，实现2.5倍端到端延迟加速\n- **准确性保持**：与基线相当或更高，证明早期退出不损害推理质量\n- **通用性**：在数学、科学、编码、逻辑等多领域有效\n\n### 2. 消融研究优化\n**信号类型比较**：\n- 单一信号(仅置信度/进度/剩余长度)各有局限\n- 组合信号互补优势，平衡准确性和效率(图5)\n\n**信号平滑策略**：\n- EWMA提供最稳定的平衡，相比无平滑、动量预测等方法更优(表2)\n\n**步骤分割token选择**：\n- 段落分隔符比话语标记更具通用性，不依赖特定数据或模型模式(图6)\n\n## 第七步：形成理论贡献与实践意义\n\n### 理论贡献\n1. **隐藏状态价值证明**：首次系统证明隐藏状态包含可用于早期退出的推理充分性信号\n2. **新范式提出**：创建不修改目标模型、无探测开销的早期退出机制\n3. **多任务学习应用**：展示联合优化token预测和信号提取的有效方法\n\n### 实践意义\n1. **部署友好**：在PyTorch和vLLM中实现，易于集成到现有推理系统\n2. **显著效率提升**：为LRMs实际部署提供可行解决方案，降低延迟和计算成本\n3. **开源促进**：公开代码促进社区采用和进一步研究\n\n## 逻辑链总结\n\n从问题观察到解决方案的完整逻辑链：\n\n```\n问题识别 → 现有方法分析 → 关键观察 → 核心假设 → 方法设计 → 验证优化 → 贡献总结\n```\n\n这一链条体现了从宏观现象到微观机制，再到具体解决方案的系统性思考过程。作者通过敏锐观察隐藏状态的信号价值，创造性地将其与推测解码结合，解决了LRMs过度思考的关键瓶颈，同时保持了模型输出的完整性和准确性。"
                },
                {
                    "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends",
                    "arxiv_id": "2509.24203",
                    "authors": "Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding",
                    "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是对大语言模型强化学习算法的理论分析和改进，特别是关于REINFORCE及其变种的离线策略特性。论文提出了将REINFORCE适应离线设置的两个通用原则：正则化策略更新和主动塑造数据分布，这些改进有助于更好地应用强化学习来训练LLM。强化学习是提升LLM通用推理能力（包括数学推理、逻辑推理等）的关键技术，论文的研究目标（如源代码示例使用的GSM8K数学推理数据集所示）与提升LLM的推理能力直接相关。论文不涉及特定应用领域或模型基础设施优化，而是专注于改进LLM的基础训练方法，因此符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在揭示group-relative REINFORCE算法的off-policy本质并澄清GRPO及其相关算法的一些误解。针对LLM强化学习中的off-policy训练场景，我们提出了一种基于KL正则化替代目标的新解释框架，并在GSM8k、MATH、ToolACE等多个数据集上通过准确率、训练奖励等指标验证了其有效性。",
                    "summary_translation": "针对大型语言模型(large language models, LLMs)的离线强化学习(off-policy reinforcement learning, RL)正日益受到关注，这主要是由实际应用中的现实约束、LLM-RL基础设施的复杂性以及对强化学习方法论进一步创新的需求所推动的。尽管经典的REINFORCE及其现代变体如组相对策略优化(Group Relative Policy Optimization, GRPO)通常被视为对离策略性(off-policyness)容忍度有限的在线策略(on-policy)算法，但我们在本工作中提出了一个不假设特定训练数据分布的组相对REINFORCE的基本原理推导，表明它具有天然的离线策略解释。这一视角产生了将REINFORCE适应离线策略环境的两个通用原则：正则化策略更新和主动塑造数据分布。我们的分析揭示了关于GRPO中重要性采样(importance sampling)和裁剪(clipping)作用的一些误解，将两种最新算法——在线策略镜像下降(Online Policy Mirror Descent, OPMD)和非对称REINFORCE(Asymmetric REINFORCE, AsymRE)统一并重新解释为REINFORCE损失的正则化形式，并为看似启发式的数据加权策略提供了理论依据。我们的研究结果带来了可操作的见解，这些见解通过广泛的实证研究得到了验证，并为LLMs离线强化学习中有原则的算法设计开辟了新的机会。本工作的源代码可在https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k获取。",
                    "inspiration_trace": "# 从观察到方法论：Group-Relative REINFORCE的Off-Policy本质\n\n## 1. 宏观问题：LLM强化学习的实际挑战\n\n作者从大型语言模型(LLM)强化学习的实际应用困境出发，观察到三个核心问题：\n- **实际约束**：rollout生成与模型训练速度不匹配、数据来源多样、奖励反馈延迟\n- **基础设施复杂性**：LLM-RL系统复杂，使on-policy训练不切实际\n- **算法局限性**：主流算法如PPO和GRPO本质上是on-policy方法，对off-policyness容忍有限\n\n这引发了一个根本性问题：**能否在LLM强化学习中实现更高效、更灵活的off-policy学习？**\n\n## 2. 观察与聚焦：REINFORCE的矛盾现象\n\n作者注意到一个有趣现象：尽管REINFORCE及其变体(如GRPO)理论上被视为on-policy算法，但实践中常被用于off-policy场景。这促使作者聚焦于一个核心问题：\n\n**Group-Relative REINFORCE是否具有天然的off-policy解释，而不仅是on-policy算法的扩展？**\n\n## 3. 假设形成：第一性原理的重新思考\n\n作者提出一个大胆假设：\n> Group-Relative REINFORCE可以被重新解释为本质上是一个off-policy算法，而不需要对训练数据分布做特定假设。\n\n为验证这一假设，作者决定从第一性原理出发，重新推导Group-Relative REINFORCE，避开标准策略梯度理论中的on-policy假设。\n\n## 4. 理论推导：三步揭示off-policy本质\n\n### 步骤1：定义KL正则化代理目标\n作者引入KL正则化的代理目标：\n```\nmax_θ J(θ; π_θt) := E_x[E_y[r(x,y)] - τ·D_KL(π_θ(·|x) || π_θt(·|x))]\n```\n并证明其最优策略满足一致性条件：对于任意响应对y₁,y₂，有\n```\nr₁ - τ·[log π(y₁|x) - log π_θt(y₁|x)] = r₂ - τ·[log π(y₂|x) - log π_θt(y₂|x)]\n```\n\n### 步骤2：构建有限样本代理损失\n基于一致性条件，作者定义代理损失：\n```\nL̃(θ; x, π_θt) := 1/K² Σ_{i<j} (a_i - a_j)²/(1+τ)²\n```\n其中a_i := r_i - τ[log π_θ(y_i|x) - log π_θt(y_i|x)]\n\n### 步骤3：单步梯度推导的关键洞察\n在θ=θ_t处进行梯度计算时，作者发现一个关键事实：log π_θ(y_i|x) - log π_θt(y_i|x)的值为零，这大大简化了梯度计算。最终推导出：\n```\ng(θ; x, {y_i,r_i}) = 2τ/(1+τ)² · 1/K Σ_i (r_i - r̄) ∇_θ log π_θ(y_i|x)\n```\n这正是Group-Relative REINFORCE的更新规则，且推导过程未对数据分布做任何on-policy假设。\n\n## 5. 深入分析：识别缺陷与提出原则\n\n基于新解释，作者分析了vanilla REINFORCE的缺陷，并提出两个增强原则：\n\n### 原则1：正则化策略更新\n确保在次优数据分布下，优化轨迹保持有界和稳定。\n\n### 原则2：主动塑造数据分布\n通过加权训练样本引导策略更新方向，而非简单按原样使用。\n\n## 6. 方法论形成：统一解释现有算法\n\n作者将新框架应用于重新解释多种现有算法：\n\n### GRPO的重新理解\n- **发现**：GRPO的off-policy有效性主要源于裁剪(作为正则化)而非重要性采样\n- **启示**：可扩大裁剪范围加速训练而不牺牲稳定性\n\n### OPMD和AsymRE的统一视角\n- **OPMD**：可视为REINFORCE损失加上均方正则化损失\n- **AsymRE**：可视为REINFORCE损失加上KL散度正则化\n\n### 数据加权策略的理论依据\n- **RED-Drop**：丢弃低奖励样本的策略在off-policy框架下自然合理\n- **RED-Weight**：高奖励样本上加权可视为一种有效的数据分布塑造\n\n## 7. 实验验证：从理论到实践\n\n作者通过广泛实验验证了理论洞察：\n- REC系列实验证明裁剪比重要性采样更关键\n- 扩大裁剪范围可加速训练且保持稳定\n- RED方法在各种off-policy设置下表现优异\n- OPMD和AsymRE作为正则化REINFORCE的解释得到验证\n\n## 8. 核心贡献：新视角与新机会\n\n作者最终贡献了一个全新的理解框架：\n1. 揭示Group-Relative REINFORCE的天然off-policy本质\n2. 提出增强off-policy REINFORCE的两个通用原则\n3. 统一解释多种现有算法，澄清其工作机制\n4. 为LLM的off-policy RL开辟了原则性算法设计的新途径\n\n这一工作不仅解决了初始观察到的实际问题，更通过理论创新为整个领域提供了新的思考方向和设计原则。"
                },
                {
                    "title": "Learning to Ponder: Adaptive Reasoning in Latent Space",
                    "arxiv_id": "2509.24238",
                    "authors": "Yixin He, Lumingyuan Tang",
                    "summary": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出FR-Ponder框架，一种通过潜在空间中的自适应推理来分配计算资源的方法。从本质上看，论文直接关注如何提高大语言模型本身的推理能力，特别是通过让模型根据输入复杂性调整推理深度，而不是像现有方法那样对所有输入应用统一的推理深度。论文使用了强化学习方法（GRPO）来平衡性能和计算成本，并在数学推理任务（GSM8K和MATH500）上验证了其有效性。这篇论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的推理机制，完全符合\"大语言模型通用推理能力\"的研究范围。论文提出的自适应推理框架可以被视为一种增强LLM通用推理能力的新范式，属于改进LLM基础推理能力的研究。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型可靠性的应用层面问题。",
                    "summary2": "本文旨在解决大型语言模型在推理任务中计算资源分配不灵活的问题。针对不同复杂度的推理任务，我们提出了一种FR-Ponder框架，通过潜在空间中的引导向量实现实例自适应推理计算分配，并在GSM8K、MATH500和GPQA数据集上通过准确率、平均令牌数和FLOPs验证了其有效性。该方法使用轻量级控制器（<1M参数）决定推理深度，在保持或提高准确率的同时实现了30-50%的计算资源减少。",
                    "summary_translation": "测试时计算(test-time compute)已成为增强大语言模型(LLM)推理的关键范式，然而现有方法如Best-of-N和多数投票(majority voting)在所有输入上应用统一的推理深度，在简单查询上浪费计算资源，同时可能对复杂问题思考不足。我们提出了FR-Ponder，一种单图(single-graph)、无需骨干训练(backbone-training-free)的框架，通过潜在转向(latent steering)分配实例自适应(instance-adaptive)推理计算。一个参数量少于100万的控制器观察隐藏状态(hidden states)，并决定停止或通过向冻结表示(frozen representations)添加预计算的转向向量(pre-computed steering vector)来应用小步思考(ponder step)。我们的方法从大语言模型中提取与更深推理输出和直接输入输出(direct IO)相关联的潜在转向向量(latent steering vector)，并通过可调节的缩放因子(tunable scaling factor)重新应用它，使模型能够根据每个输入的复杂性调整其推理深度。为了平衡性能和计算成本，我们采用组相对策略优化(Group Relative Policy Optimization, GRPO)作为奖励信号(reward signal)来自适应调节推理深度，在实现任务准确性的同时减轻过度推理(overreasoning)。通过课程学习(curriculum learning)和精心的奖励工程(reward engineering)，FR-Ponder学习到与问题难度相关的校准计算分配(calibrated compute allocation)。在GSM8K和MATH500数据集上，FR-Ponder改进了计算-准确性前沿(compute-accuracy frontier)，以更低的浮点运算次数(FLOPs)提供更好的匹配准确性，并且在不修改骨干权重(backbone weights)的情况下，与早期退出(early-exit)基线相比表现更优。分析结果可视化了解释性转向方向(interpretable steering directions)，并表明学习的计算分配与问题难度相关。",
                    "inspiration_trace": "# FR-Ponder方法逻辑链推演：从固定计算到自适应推理\n\n## 1. 宏观问题：LLM的固定计算分配低效\n\n**观察现象**：大型语言模型(LLM)在处理不同复杂度的任务时，采用相同的计算资源分配策略。无论是简单事实查询还是复杂数学推理，模型对每个token都消耗相同的计算量。\n\n**问题本质**：这种刚性计算分配导致系统性低效：\n- 简单问题：过度计算，浪费资源\n- 复杂问题：计算不足，推理质量受限\n\n**影响范围**：随着模型规模扩大至数千亿参数，这种计算-准确性不匹配问题日益严重，限制了LLM的实用性和可持续性。\n\n## 2. 现有解决方案的局限性分析\n\n作者系统评估了三类主流方法，发现各自存在根本缺陷：\n\n### 2.1 多遍方法（如思维链CoT、自洽性）\n- **机制**：通过采样多个推理轨迹实现自适应\n- **局限**：推理成本与采样次数成正比，计算开销呈倍数增长\n- **核心矛盾**：自适应性与计算效率不可兼得\n\n### 2.2 架构修改方法（如早期退出、层跳过）\n- **机制**：修改模型结构实现动态计算分配\n- **局限**：需要重新训练基础模型，降低部署灵活性，可能损害原始能力\n- **核心矛盾**：自适应性与模型完整性不可兼得\n\n### 2.3 推测解码方法\n- **机制**：通过草稿-验证范式加速推理\n- **局限**：需维护多个模型，仅提供粗粒度自适应\n- **核心矛盾**：效率与细粒度控制不可兼得\n\n### 2.4 分数推理框架（最接近的前期工作）\n- **机制**：从对比提示中提取\"推理向量\"控制推理深度\n- **局限**：需手动调整缩放因子，缺乏单次推理内的动态适应\n- **核心矛盾**：方向控制与时间控制未解耦\n\n## 3. 核心洞察：自适应计算的正交分解\n\n**关键突破**：作者认识到自适应计算可分解为两个正交维度：\n1. **思考什么**（内容方向）：推理的方向和路径\n2. **思考多久**（时间控制）：推理的深度和持续时间\n\n**核心假设**：若能将这两个维度解耦，则可实现：\n- 保持基础模型能力不变（无需修改权重）\n- 实现细粒度、实例自适应的计算分配\n- 单次推理内动态调整推理深度\n\n**理论依据**：基于线性表示假说，不同推理模式（如逐步思考vs直接回答）对应表示空间中的不同方向，可通过向量操作进行干预。\n\n## 4. 方法设计：FR-Ponder框架\n\n基于上述洞察，作者设计了FR-Ponder框架，包含四个关键组件：\n\n### 4.1 转向向量提取\n- **思路**：提取编码\"更深层次推理\"的表示方向\n- **实现**：通过对比提示（\"逐步思考\"vs\"直接回答\"）计算隐藏状态差异\n  ```\n  h_steer = E[x∼D][z_deliberative(x) - z_direct(x)]\n  ```\n- **优势**：一次性提取，固定使用，无需修改模型权重\n\n### 4.2 自适应思考机制\n- **状态演化**：通过加性转向引导表示变化\n  ```\n  z_{k+1} = z_k + α(k)·h_steer\n  ```\n  其中α(k) = α₀·e^(-βk)实现指数衰减，确保稳定性\n- **控制器架构**：轻量级神经网络（<1M参数）\n  - 输入：当前隐藏状态\n  - 输出：继续思考的概率\n  - 决策：若ϕ(z_k) ≤ τ则停止，否则应用思考步骤\n\n### 4.3 训练方法\n- **问题建模**：将自适应计算建模为马尔可夫决策过程(MDP)\n- **奖励设计**：多组件奖励函数平衡五个目标：\n  ```\n  R = w_acc·Accuracy - w_flops·FLOPs + w_comp·Completeness \n      + w_qual·Quality - w_rep·Repetition\n  ```\n- **优化算法**：采用组相对策略优化(GRPO)\n  - 通过组内比较实现方差缩减\n  - 无需单独的价值网络\n  - 适合轻量级控制器设计\n\n### 4.4 课程学习框架\n- **三阶段渐进**：\n  1. 教师示范阶段：完全由教师策略控制\n  2. 混合训练阶段：逐渐从教师转向学生\n  3. 自主学习阶段：完全由学生控制器决策\n- **质量门控**：过滤低质量轨迹，确保训练稳定性\n\n## 5. 理论基础与保证\n\n作者为FR-Ponder提供了坚实的理论支撑：\n\n### 5.1 转向向量一致性\n**定理**：在变换器表示空间的温和条件下，转向向量估计器以O(N^(-1/2))速率收敛到真实转向方向。\n\n### 5.2 通用逼近能力\n**定理**：对于紧致子集上的任何连续延续值函数，存在控制器网络可以任意精度逼近。\n\n### 5.3 收敛保证\n**定理**：GRPO实现O(1/√T)的收敛率，并具有1/G的方差缩减因子。\n\n### 5.4 计算开销界限\n**定理**：FR-Ponder的开销为O(K·d)，相对开销为O(K/n)≈O(1/√n)，对大型模型影响微乎其微。\n\n## 6. 实验验证与效果\n\n作者在多个数据集和模型上验证了FR-Ponder的有效性：\n\n### 6.1 实验设置\n- **数据集**：GSM8K（小学数学）、MATH500（中学数学）、GPQA（研究生科学问题）\n- **模型**：LLaMA-3系列（8B、70B）和Qwen-2.5系列（0.5B、3B、7B）\n- **评估指标**：准确性、平均token数、平均FLOPs\n\n### 6.2 核心结果\n- **准确性**：在保持或提高准确性的同时，显著降低计算开销\n- **效率**：实现30-50%的token减少，FLOPs大幅降低\n- **适应性**：计算分配与问题难度高度相关，简单问题快速解决，复杂问题深入推理\n- **泛化性**：在不同模型规模和架构上均表现良好，尤其对小模型提升明显\n\n## 7. 方法优势总结\n\nFR-Ponder的核心创新在于将推理过程转化为元认知决策过程：\n\n1. **解耦设计**：将\"思考什么\"与\"思考多久\"分离，实现更灵活的控制\n2. **轻量级适配**：仅训练<1M参数的控制器，保持基础模型冻结\n3. **单次推理**：无需多次前向传递，实时调整计算深度\n4. **多目标平衡**：通过精心设计的奖励函数，同时优化准确性和效率\n5. **理论保证**：提供收敛性、稳定性和效率的理论保证\n\n## 8. 逻辑链总结\n\n从宏观问题到具体方法，作者形成了清晰的逻辑链：\n\n```\n固定计算低效 → 现有方法局限 → 正交分解洞察 → FR-Ponder设计 → 理论保证 → 实验验证\n```\n\n这一逻辑链体现了从问题识别、分析、假设到方法设计、验证的完整科学思维过程，最终实现了LLM推理从固定计算到自适应推理的范式转变，为大型语言模型的高效推理提供了新的思路。"
                },
                {
                    "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models",
                    "arxiv_id": "2509.24156",
                    "authors": "Yuhui Wang, Changjiang Li, Guangke Chen, Jiacheng Liang, Ting Wang",
                    "summary": "Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively \"hacking\" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文本质上是研究大语言模型推理机制的内在工作原理，特别是分析思维链(CoT)推理和记忆检索两种竞争机制如何影响模型的推理过程。这不是将LLM应用于特定领域，而是专注于提升LLM本身的通用推理能力。论文提出的FARL框架是一种新的训练范式，通过结合记忆遗忘和强化学习来抑制检索捷径，从而增强模型的推理主导行为和通用推理能力。其次，论文包含多个正面指标：核心概念涉及大型推理模型(LRMs)，能力方向聚焦于推理能力(特别是CoT推理)，训练方法包含强化学习。论文不涉及任何排除标准中的领域，如多模态视觉、特定应用领域或模型可靠性的应用层面问题。因此，这篇论文明确致力于提高大语言模型的通用推理能力，完全符合我的研究范围。",
                    "summary2": "本文旨在 [解决大型推理模型答案生成机制不一致问题]。针对 [推理与检索机制竞争场景]，我们提出了一种 [FARL框架，结合记忆遗忘与强化学习抑制检索捷径]，并在 [多QA数据集和不同LRMs] 上通过 [扰动成功率和准确率] 验证了有效性。",
                    "summary_translation": "大型推理模型(LRMs，Large reasoning models)通过思维链(CoT，Chain-of-Thought)推理在解决复杂问题方面展现出前所未有的能力。然而，最近的研究表明，这些模型的最终答案经常与它们自己的推理过程相矛盾。我们假设这种不一致性源于两种竞争的答案生成机制：思维链(CoT)推理和记忆检索(memory retrieval)。为验证这一假设，我们进行了对照实验，在推理过程中引入误导性线索，在检索过程中提供损坏答案，以此挑战大型推理模型。我们在多个模型和数据集上的结果证实，这两种机制同时运作，其相对优势受多种因素影响：问题领域、模型规模和微调方法(fine-tuning approaches)(例如，强化学习(reinforcement learning)与知识蒸馏(distillation))。这些发现揭示了当前推理微调范式(reasoning fine-tuning paradigms)的一个关键局限：模型可以利用检索机制作为捷径，有效地\"破解\"奖励信号，从而阻碍真正的推理能力发展。为应对这一挑战，我们提出了FARL，一种将记忆遗忘(memory unlearning)与强化学习(reinforcement learning)相结合的新型微调框架。通过在微调过程中谨慎抑制检索捷径，FARL促进了推理主导行为，并增强了可泛化的推理能力。",
                    "inspiration_trace": "# 大型推理模型中推理与检索机制的逻辑分析：从观察到FARL方法的提出\n\n## 1. 宏观问题：推理模型的不一致性现象\n\n**观察起点**：大型推理模型(LRMs)如GPT o-series、Gemini 2.5和DeepSeek-R1等通过思维链(CoT)推理展现出强大的问题解决能力，但研究发现这些模型的最终答案经常与它们自己的推理痕迹相矛盾。\n\n**核心矛盾**：为什么这些模型在展示详细推理过程的同时，最终答案却与推理过程不一致？这种不一致性揭示了模型内部工作机制的什么问题？\n\n## 2. 形成核心假设：双机制竞争模型\n\n**假设提出**：作者假设这种不一致性源于两种生成答案的竞争机制：\n- **CoT推理机制**：通过逐步逻辑推理得出答案\n- **记忆检索机制**：直接从内部记忆中检索答案\n\n**理论依据**：\n- LRMs建立在现有基础模型之上，通过蒸馏或强化学习激发推理能力，导致多种能力共存\n- 内部知识作为竞争因素，可能影响显式推理过程\n- 最终答案可能是两种机制竞争的结果，而非单一过程的产物\n\n## 3. 研究问题与实验验证\n\n### 研究问题聚焦：\n1. **RQ1**：LRMs是否同时使用推理和检索来推导答案？\n2. **RQ2**：什么因素影响一种能力相对于另一种能力的优势？\n3. **RQ3**：我们如何控制这些能力的相对强度？\n\n### 实验设计：\n- **推理扰动**：在CoTs中注入误导性线索，观察是否改变最终答案\n- **检索扰动**：通过微调\"毒害\"模型记忆，观察是否影响答案生成\n- **组合扰动**：同时应用两种扰动，观察\"拉锯战\"现象\n\n### 关键发现：\n- **RQ1验证**：两种机制确实同时运作，当它们指向同一答案时效应增强，指向不同答案时出现竞争\n- **RQ2分析**：\n  * 问题领域：数学/逻辑领域更依赖推理，其他领域更依赖检索\n  * 训练方法：蒸馏模型更依赖检索，RL训练模型更依赖推理\n  * 模型规模：更大模型更倾向于推理主导\n  * 神经机制：中间层(12-16层)是推理与检索竞争的关键控制点\n\n## 4. 问题识别：当前微调范式的局限性\n\n**关键发现**：模型可以利用检索机制作为\"捷径\"，有效\"黑客攻击\"奖励信号，破坏真正的推理发展。\n\n**具体问题**：在RL训练过程中，特别是蒸馏模型倾向于：\n1. 检索正确答案而不考虑CoT相关性\n2. 生成捏造的CoT来证明记忆答案的合理性(\"事后解释\")\n3. 这种行为膨胀了批量平均奖励，不公平地惩罚通过真正推理获得正确答案的样本\n\n## 5. 解决方案：FARL框架的提出\n\n**核心洞察**：\n- RL训练比蒸馏更能促进真正的推理能力\n- 数学问题自然激发更稳健的推理\n- 需要阻止检索捷径，净化奖励信号\n\n**FARL设计原理**：通过强迫模型\"忘记\"特定的记忆答案，阻止检索捷径，迫使推理机制占主导地位，并使其在RL期间得到改进。\n\n**方法实现**：\n- 修改标准RL流程，在GRPO迭代后引入遗忘步骤\n- 采用负偏好优化(NPO)作为遗忘方法\n- 迭代过程：GRPO优化 → 遗忘抑制 → GRPO优化 → ...\n\n## 6. 实验验证与效果评估\n\n**评估指标**：\n- 扰动实验中的R-PSR和T-PSR（衡量推理与检索主导性）\n- 准确率(ACC)和响应的平均标记长度(MTL)\n- 推理图属性（循环、直径和小世界指数）\n\n**实验结果**：\n- RL优于SFT：RL模型表现出更强的推理主导性，训练域内准确率提升19.8%（SFT仅8.6%）\n- FARL优于标准RL：\n  * R-PSR和T-PSR分别降低47.8%和38.5%，表明更强的推理主导行为\n  * 域内准确率提升22.8%，域外准确率提升5.8%\n  * 推理图质量显著提升，小世界指数提高84.0%\n\n## 7. 逻辑链总结\n\n从观察到FARL方法的完整逻辑链：\n\n**不一致现象** → **双机制假设** → **实验验证** → **影响因素分析** → **发现奖励信号问题** → **提出FARL解决方案** → **实验验证效果**\n\n这一研究不仅揭示了LRMs答案生成的内在机制，还提供了控制这些机制相对强度的有效方法，为更有效地激发模型推理能力开辟了新方向。FARL通过结合遗忘与强化学习，成功抑制了检索捷径，促进了真正的推理发展，提高了模型的泛化能力。"
                },
                {
                    "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models",
                    "arxiv_id": "2509.23962",
                    "authors": "Guanxu Chen, Yafu Li, Yuxian Jiang, Chen Qian, Qihan Ren, Jingyi Yang, Yu Cheng, Dongrui Liu, Jing Shao",
                    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围。根据筛选标准，我进行了如下分析： 第一步：核心判断——论文的本质是关于改进LLM的基础推理能力。论文提出了\"Conditional advANtage estimatiON (CANON)\"方法，用于增强大型语言模型在强化学习中的推理能力，特别是在数学推理和复杂逻辑任务上的表现。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究Large language models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning和high-complexity logic tasks - 训练方法：研究Reinforcement Learning with Verifiable Rewards (RLVR)，属于强化学习范畴 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有专注于特定应用领域（虽然提到数学和逻辑任务，但这些是通用推理能力的测试场景，而非特定领域应用） - 没有关注模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的情况，如智能体/工具使用或幻觉/可解释性/安全等。 综上所述，这篇论文的核心贡献是通过改进强化学习中的优势估计方法来提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。",
                    "summary2": "本文旨在解决大型推理模型强化学习中如何有效利用训练指标先验而不预设方向偏好的问题。针对具有可验证奖励的强化学习场景，我们提出了一种条件优势估计方法(CANON)，通过将采样响应按指标值分组并计算组间与组内优势来放大目标指标影响。在三个LLMs上通过六个数学推理基准和三个高复杂度逻辑任务验证，基于熵的CANON在数学任务上实现1.9点准确率提升，基于响应长度的CANON在性能-效率权衡上建立了新的Pareto前沿。",
                    "summary_translation": "# 可验证奖励强化学习(RLVR)在大语言模型(LLMs)条件优势估计(CANON)方法\n\n可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)在大语言模型(large language models, LLMs)领域已取得显著进展，有效提升了LLMs在具有明确正确性标准任务上的推理能力，如数学推理任务。在强化学习过程中，多个训练指标，如熵(entropy)或响应长度(response length)，已被观察到与不同推理行为相关联。先前方法通过奖励(reward)或优势整形(advantage shaping)整合此类先验知识，这通常依赖于手工设计的惩罚和偏好（例如，越高越好或越低越好）。然而，若未经仔细的超参数调优(hyperparameter tuning)，这些方向性先验可能过度偏向并导致失败。为此，我们引入条件优势估计(Conditional advANtage estimatiON, CANON)，在不预设目标指标方向的情况下，增强其影响力。具体而言，CANON基于目标指标(target metric)的高低值将采样响应重新分为两组，通过组间比较衡量哪种指标趋势有助于更好性能，并在同一组内识别更优响应。总之，基于熵(entropy)的CANON在数学推理和高复杂度逻辑任务上，于三种LLMs中均一致性地优于先前方法。当应用于响应长度(response length)时，CANON进一步提高了令牌效率(token efficiency)，在性能-成本权衡中产生了更有利的帕累托前沿(Pareto frontier)。",
                    "inspiration_trace": "# 从观察到创新：CANON方法的逻辑演进\n\n## 宏观问题：如何有效利用训练指标提升大模型推理能力？\n\n在大型推理模型(LRMs)的强化学习中，研究者观察到一些训练指标(如熵、响应长度)与模型的不同推理行为相关联。这些指标可能包含有价值的先验知识，可以指导训练过程并提高模型性能。然而，如何有效利用这些指标而不引入人为偏见，成为一个核心挑战。\n\n## 现有方法的局限：方向性偏见的陷阱\n\n作者观察到当前方法存在明显局限：\n\n1. **手工设计的方向性偏好**：现有方法通过奖励塑形或优势塑形整合人类先验，但这些方法通常假设特定指标是\"越高越好\"或\"越低越好\"。\n\n2. **超参数敏感性**：没有仔细的超参数调整，这些方向性先验可能过度偏向并导致失败。\n\n3. **指标效应的复杂性**：例如，熵指标在不同场景下有不同影响——高熵响应倾向于探索性，适合复杂问题；低熵响应表现出确定性，在模型能力范围内的问题上更准确。\n\n## 关键洞察：不预设偏好的可能性\n\n作者提出一个核心假设：**是否可以在不预设指标方向偏好的情况下，放大特定指标变化的影响？**\n\n这一假设基于一个重要观察：模型输出中可能存在可以利用的内在倾向，这些倾向可以自然地促进有益行为的学习，如增强探索或提高推理效率。\n\n## 方法论的演进：从分组到条件优势估计\n\n### 第一步：条件重组\n\n作者首先提出将采样响应基于给定指标的值重新分组为两组(高值组和低值组)。这一步骤的目的是显式引入比较目标，为后续的优势估计创造条件。\n\n### 第二步：双重优势估计\n\n基于分组，作者设计了两种互补的优势估计机制：\n\n1. **组间优势(Inter-group advantage)**：通过跨组比较确定哪种指标趋势(高值或低值)导致更高准确性。\n   \n   ```\n   如果响应属于高值组，则与低值组的平均奖励比较\n   如果响应属于低值组，则与高值组的平均奖励比较\n   ```\n\n2. **组内优势(Intra-group advantage)**：在同一组内识别更好的响应，特别优先考虑来自平均奖励较低的组中的正确响应。\n\n   ```\n   响应与其所属组的平均奖励比较\n   ```\n\n### 第三步：统一框架与理论支持\n\n作者将两种优势估计结合为统一形式：\n```\nCANON = μ * inter-group advantage + (1-μ) * intra-group advantage\n```\n\n通过理论分析，作者证明了当两组大小相等时，组间优势放大了分组指标对优势计算的影响。更重要的是，他们发现现有的DR.GRPO方法实际上是CANON在μ=0.5时的特例，这表明CANON提供了一个更通用的框架。\n\n### 第四步：加权条件控制\n\n为了进一步精细控制指标趋势，作者引入了加权机制，允许对特定组给予不同权重。例如，通过略微降低较长响应的权重，CANON可以实现高效的推理，而不会显著降低性能。\n\n## 实验验证与效果确认\n\n作者通过广泛的实验验证了CANON的有效性：\n\n1. **基于熵的CANON**：在数学任务上实现了1.9点的准确率提升，表明组间优势能有效识别有利于数学推理的熵趋势。\n\n2. **基于长度的CANON**：显著提高了推理效率，在性能-成本权衡中建立了更优的帕累托前沿。\n\n3. **动态调度策略**：通过在不同训练阶段调整μ值，CANON-Dynamic在数学推理和复杂逻辑推理任务上都取得了优异表现。\n\n4. **高效推理**：通过调整长度权重α，CANON-Eff在低令牌预算场景下实现了2.63倍的性能提升，并在相同性能水平下减少了45.5%的令牌消耗。\n\n## 逻辑链条总结\n\n从宏观问题出发，作者通过以下逻辑链条发展出CANON方法：\n\n1. **观察问题**：现有方法依赖手工设计的方向性偏好，可能导致过度偏向\n2. **提出假设**：是否可以在不预设偏好的情况下放大指标影响\n3. **核心创新**：通过条件重组和双重优势估计机制，自然识别模型输出中的有益倾向\n4. **理论支持**：证明方法的有效性，并展示现有方法是其特例\n5. **实验验证**：在多个任务和模型上验证方法的有效性和灵活性\n\n这一逻辑演进过程展示了作者如何从实际问题出发，通过深入观察和理论思考，最终提出一个既创新又实用的解决方案，为大语言模型的强化学习提供了新的思路。"
                },
                {
                    "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm",
                    "arxiv_id": "2509.23946",
                    "authors": "Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb",
                    "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution.This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出Explore-Execute Chain（E²C）框架，一种新的结构化推理范式，专门用于改进大语言模型的通用推理能力。从本质上看，论文针对思维链(CoT)的局限性，将推理过程分解为探索和执行两个阶段，通过监督微调和强化学习相结合的训练方法，提升了模型的推理效率、路径探索能力和可解释性。这明显属于改进LLM基础能力和通用推理能力的范畴。 论文包含多个关键正面指标：明确以大语言模型(LLMs)为研究对象；专注于推理(reasoning)、规划(planning)等核心能力方向；采用强化学习(RL)作为训练方法之一。虽然论文提到了在医学基准上的测试结果，但这仅是为了证明其跨领域适应能力，而非将LLM应用于特定领域，因此不触犯排除标准。此外，论文通过分离规划与执行来增强模型的可解释性，这也符合保留标准。 综上所述，该论文直接致力于提升LLM的通用推理能力，提出了新的训练范式和推理框架，与研究目标高度一致。",
                    "summary2": "本文旨在解决传统Chain-of-Thought方法计算效率低、推理路径探索有限和可解释性差的问题。针对大型语言模型的推理任务，我们提出了Explore-Execute Chain (E2C)框架，将推理解耦为探索和执行两个阶段，并在AIME'2024、MATH500等数学推理基准和MedQA、MedMCQA等医学推理基准上通过准确率和计算效率验证了其有效性。",
                    "summary_translation": "思维链（Chain-of-Thought, CoT）及其变体显著提升了大型语言模型（Large Language Models, LLMs）的推理能力，然而其单一且自回归的架构本质上将高层战略规划与底层逐步执行混为一谈，导致计算效率低下、推理路径探索有限以及可解释性降低。为解决这些问题，我们提出了探索-执行链（Explore-Execute Chain, $E^2C$），这是一种结构化推理框架，将推理解耦为两个不同阶段：一个探索阶段，随机生成简洁的高层计划；随后是一个执行阶段，确定性执行所选计划。我们的方法采用两阶段训练方法论，结合监督微调（Supervised Fine-Tuning, SFT）——通过一种新颖的数据生成算法增强，该算法强制执行严格的计划遵循——以及后续的强化学习（Reinforcement Learning, RL）阶段，该阶段利用探索的信息性并强化执行的确定性。这种分解实现了一种高效的测试时扩展策略：在AIME'2024上，$E^2C$测试时扩展（Test Time Scaling）达到58.1%的准确率，仅使用可比方法（如思维森林，Forest-of-Thought）所需解码令牌的<10%，大幅减少了自一致性开销。对于跨领域适应，我们的探索聚焦监督微调（Exploration-Focused SFT, EF-SFT）仅使用标准SFT所用令牌的3.5%进行微调，却在医学基准测试上比标准SFT高出高达14.5%的准确率，通过将规划与执行分离，实现了最先进的性能、强大的泛化能力和更高的可解释性。该项目的代码和预训练模型可在以下网址获取：https://github.com/yks23/Explore-Execute-Chain.git",
                    "inspiration_trace": "# Explore-Execute Chain (E²C) 方法论逻辑链分析\n\n## 一、宏观问题：LLM推理效率与质量的根本矛盾\n\n### 初始观察\n大型语言模型(LLM)在复杂推理任务中展现出显著能力，主要通过Chain-of-Thought (CoT)及其变体实现。然而，作者观察到这些方法存在一个根本性矛盾：\n\n**现有方法的混合架构问题**：\n- CoT及其变体采用单一、自回归的生成过程\n- 这种架构本质上将两个认知功能混合在一起：高层次战略规划与低层次逐步执行\n- 这种混合导致计算资源分配不合理、推理路径探索受限、可解释性降低\n\n### 具体表现\n1. **计算效率低下**：模型在创造性规划和常规计算上花费同等计算资源\n2. **探索多样性受限**：贪婪生成过程限制了初始策略多样性，次优早期选择可能破坏整个推理路径\n3. **可解释性不足**：规划与执行混合，难以理解模型的决策过程\n\n## 二、核心假设：分离探索与执行的认知优势\n\n### 假设一：认知分离假设\n基于对人类推理过程的观察，作者提出核心假设：\n- **人类推理二元性**：人类解决复杂问题时通常先进行高层次战略规划，再进行详细执行\n- **功能分离优势**：将这两个认知功能明确分离可以提高推理效率和质量\n\n### 假设二：信息分布假设\n- **探索阶段应高度信息丰富**：包含解决问题的关键战略信息\n- **执行阶段应高度确定性**：严格遵循选定计划，专注于精确计算\n\n### 假设三：训练优化假设\n- **专门训练必要性**：仅通过提示无法实现这种范式转变，需要专门设计的训练方法\n- **因果依赖重要性**：必须建立从探索到执行的明确因果依赖关系\n\n## 三、方法论形成：从概念到实现\n\n### 1. 框架设计\n基于上述假设，作者提出Explore-Execute Chain (E²C)框架：\n\n**形式化定义**：\n```\np(e|c) → p'(π,e|c) = p'(π|c) · p'(e|π,c)\n```\n其中：\n- `p'(π|c)`：探索阶段，应具有\"信息丰富性\"(Informative Property)\n- `p'(e|π,c)`：执行阶段，应具有\"确定性\"(Deterministic Property)\n\n**关键特性**：\n- 探索阶段：生成简洁、高层次的战略计划\n- 执行阶段：以计划为指导，进行详细、确定的计算\n\n### 2. 训练方法设计\n为使模型掌握这种新范式，作者设计了两阶段训练方法：\n\n**阶段一：监督微调(SFT)**\n- **挑战**：简单方法（先执行后总结）会破坏因果结构\n- **解决方案**：设计因果数据生成算法(Algorithm 2)\n  1. 先生成完整解决方案\n  2. 将其提炼为探索步骤\n  3. 提示模型生成严格遵循探索的新执行\n- **目的**：实现推理范式的转变，满足信息丰富性要求\n\n**阶段二：强化学习(RL)**\n- **创新点**：为探索阶段标记分配更高系数λ(λ>1)\n- **目的**：\n  - 利用探索的信息丰富性加速收敛\n  - 通过强化学习的熵减少效应增强执行确定性\n- **两阶段RL训练**：\n  1. 第一阶段：高温(τ1)和多轮采样(k1)，鼓励广泛探索\n  2. 第二阶段：低温(τ2)和少轮采样(k2)，加强确定性执行\n\n### 3. 推理效率优化\n基于E²C框架的分离特性，作者设计了高效的推理策略：\n\n**测试时扩展策略**：\n- **传统方法问题**：生成多个完整的、昂贵的推理链\n- **E²C解决方案**：\n  1. 采样大量廉价的探索计划\n  2. 通过语义聚类或LLM选择最有希望的计划\n  3. 仅执行选定的计划\n\n**两种实现方式**：\n1. **聚类加权投票**：将探索计划聚类，仅执行每个聚类的中心计划\n2. **LLM聚合**：使用外部LLM将多个探索计划合成为一个综合计划\n\n### 4. 领域适应方法\n利用E²C的模块化特性，作者设计了高效的领域适应方法：\n\n**Exploration-Focused SFT (EF-SFT)**：\n- **洞察**：执行组件具有可迁移性，主要需要适应的是探索组件\n- **方法**：仅针对特定领域示例的探索部分进行微调\n- **优势**：显著减少数据和计算需求（仅需标准SFT的3.5%令牌）\n\n## 四、验证与应用\n\n### 验证思路\n作者通过三个关键实验验证E²C框架的有效性：\n\n1. **数学推理实验**：验证基本推理能力和框架优势\n2. **医疗推理实验**：验证跨领域泛化和数据高效适应能力\n3. **测试时扩展实验**：验证性能-成本权衡优势\n\n### 关键结果\n1. **性能提升**：在AIME'24基准上，E²C比GRPO基线提高1.5-1.9%\n2. **计算效率**：达到58.1%准确率仅需可比方法(如Forest-of-Thought)不到10%的解码令牌\n3. **领域适应效率**：EF-SFT使用仅3.5%的训练令牌，比标准SFT提高高达14.5%的准确率\n\n## 五、逻辑链总结\n\n从宏观问题到最终方法论的完整逻辑链条：\n\n1. **问题识别**：现有CoT方法混合了高层规划与低层执行，导致效率低、探索受限、可解释性差\n\n2. **观察与假设**：\n   - 人类推理通常分离规划与执行\n   - 分离这两个阶段可能提高效率和质量\n   - 探索阶段应信息丰富，执行阶段应高度确定性\n\n3. **框架设计**：\n   - 提出E²C框架，明确分离探索和执行阶段\n   - 形式化定义两个阶段及其特性要求\n\n4. **方法实现**：\n   - 设计两阶段训练方法(SFT+RL)和专门数据生成算法\n   - 开发高效测试时扩展策略和领域适应方法\n\n5. **验证与应用**：\n   - 在多个基准上验证方法的有效性和效率\n   - 证明E²C在性能-成本权衡上的优越性\n\n这一逻辑链条展示了作者如何从对现有方法局限性的深刻理解出发，基于认知科学洞察提出创新假设，最终设计出既高效又可解释的推理框架，为LLM推理提供了新的范式。"
                },
                {
                    "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR",
                    "arxiv_id": "2509.23808",
                    "authors": "Fanding Huang, Guanbo Huang, Xiao Fan, Yi He, Xiao Liang, Xiao Chen, Qinting Jiang, Faisal Nadeem Khan, Jingyan Jiang, Zhi Wang",
                    "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心在于提高大语言模型的通用推理能力。首先，论文的本质是改进LLM的基础推理能力，提出了名为VERL（Velocity-Exploiting Rank-Learning）的新方法，通过在隐藏状态空间中解耦探索和利用来增强LLM的推理能力，而非将LLM作为工具应用到特定领域。其次，论文包含多个正面指标：核心概念上明确关注LLMs；能力方向上专注于reasoning和problem-solving；训练方法上采用强化学习框架。第三，论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性的应用层面研究。论文的核心贡献是通过创新的隐藏状态分析方法，提出了一种能够同时增强探索和利用能力的新训练范式，直接提升了LLM的通用推理能力，在多个推理基准上取得了显著改进，最高达到21.4%的准确率提升。这完全符合我寻找的\"致力于提高大语言模型本身的通用推理能力\"的研究论文。",
                    "summary2": "本文旨在解决大型语言模型在可验证奖励强化学习(RLVR)中的探索-利用权衡问题。针对传统token-level分析导致的固有权衡限制，我们提出了一种基于隐藏状态表示的Velocity-Exploiting Rank-Learning (VERL)方法，通过有效秩(ER)量化探索，并提出其导数ERV和ERA来捕捉利用动态。在多个LLM和推理基准上的实验表明，VERL能够同时增强探索和利用能力，在Gaokao 2024数据集上实现了高达21.4%的绝对准确率提升。",
                    "summary_translation": "在可验证奖励强化学习(Reinforcement Learning for Verifiable Rewards, RLVR)领域，一个普遍观点通过探索-利用权衡(exploration-exploitation trade-off)的视角来解释近期进展，这一观点主要受到token级别指标(token-level metrics)的影响。我们重新审视这一视角，提出这种感知到的权衡可能并非根本约束，而是测量层面的产物。为探究此问题，我们将分析转移到语义丰富的隐藏状态空间(hidden-state space)，采用有效秩(Effective Rank, ER)来量化探索，并提出其新型的一阶和二阶导数，即有效秩速度(Effective Rank Velocity, ERV)和有效秩加速度(Effective Rank Acceleration, ERA)，以捕捉利用动态。我们的分析揭示，在隐藏状态层面，探索和利用可以实现解耦(第4节)。这一发现揭示了同时增强这两种能力的机会。这一见解促成了我们的方法——速度利用秩学习(Velocity-Exploiting Rank-Learning, VERL)，这是首个通过直接塑造RL优势函数(RL advantage function)来实现协同探索-利用增强原则的方法。关键创新在于利用理论稳定的ERA作为预测性元控制器(meta-controller)，创建协同的双通道激励结构(dual-channel incentive structure)。VERL不强制权衡，而是前瞻性地放大探索奖励以预防过度自信，同时强化利用收益以巩固推理。在多种大型语言模型(LLMs)和推理基准上的实验显示了一致的性能提升，包括在具有挑战性的2024年高考数据集(Gaokao 2024 dataset)上准确率绝对提升高达21.4%。",
                    "inspiration_trace": "# 从探索-利用权衡到隐藏状态解耦：VERL方法的逻辑演进\n\n## 1. 宏观问题：RLVR中的探索-利用权衡是固有约束吗？\n\n在可验证奖励的强化学习(RLVR)领域，主流观点将进展解释为探索(exploration)与利用(exploitation)之间的权衡结果。这种观点几乎完全基于token级别分析：探索被视为高熵token分布，利用被视为高置信度、低熵分布。这导致了一个普遍假设——探索和利用之间存在固有权衡，因为模型输出分布不能同时是均匀和尖锐的。\n\n然而，作者观察到这种观点存在一个根本性问题：**这种权衡是推理过程的内在属性，还是仅仅是测量层面的假象？**\n\n## 2. 问题聚焦：token级别测量的局限性\n\n深入分析token级别度量的缺陷，作者发现三个关键局限：\n\n- **探索度量的两难困境**：将探索等同于token级别熵面临内在矛盾——过高熵产生不连贯噪声，过低熵又扼杀了探索本身。\n- **利用度量的脆弱性**：通过手工设计启发式奖励定义利用，导致模型追逐表面代理，泛化性差。\n- **研究思维的局限性**：现有工作陷入\"平衡\"权衡的循环，而非质疑权衡本身存在。\n\n这些观察引发了一个核心假设：**探索-利用权衡可能不是推理的固有属性，而是token级别测量的假象**。\n\n## 3. 假设形成：隐藏状态空间中的解耦可能性\n\n基于对token级别测量局限性的认识，作者提出了一个大胆假设：**在更细粒度的隐藏状态空间中，探索和利用可能是解耦的**。为验证这一假设，需要：\n\n1. 将分析从token级别转移到语义丰富的隐藏状态空间\n2. 设计新度量工具，在隐藏状态层面量化探索和利用\n3. 实证检验两者在隐藏状态级别的相关性\n\n## 4. 方法设计：新度量工具的构建\n\n为在隐藏状态空间中量化探索和利用，作者引入了三级度量体系：\n\n### 4.1 零阶度量：有效秩(ER)量化探索\n- **定义**：ER = exp(-∑pⱼlog(pⱼ))，其中pⱼ是归一化奇异值\n- **物理意义**：衡量隐藏状态表示的语义多样性，高ER表示模型利用丰富多样的内部特征集，是探索行为的直接标志\n- **理论保证**：1 ≤ erank(Z) ≤ rank(Z) ≤ min{T,D}，提供比传统秩更连续、更细致的探索度量\n\n### 4.2 一阶度量：有效秩速度(ERV)量化利用\n- **定义**：ERV = Δ⁽¹⁾ER，衡量ER相对于历史平均的变化率\n- **物理意义**：捕捉信息获取效率，大ERV表示模型正以超过历史趋势的速度丰富表示，标志深入且富有成效的推理路线\n\n### 4.3 二阶度量：有效秩加速度(ERA)预测趋势\n- **定义**：ERA = Δ⁽²⁾ER，衡量ERV的变化率\n- **物理意义**：表明推理过程是在加速还是饱和，作为预测元控制器具有理论稳定性\n- **关键性质**：ERA独立于矩阵规模(Δ⁽²⁾ER = O(1))，提供稳定信号\n\n## 5. 实证验证：探索与利用的解耦证据\n\n通过新度量工具，作者获得了关键发现：\n\n### 5.1 响应级别分析\n- 正确和不正确推理路径在ER和ERV上表现不同：不正确路径显示更高ER和ERV，表明过度探索和过度信息获取可能偏离正确推理\n- 关键发现：正确推理轨迹在ERA上 consistently 表现更高值，表明信息获取的加速度是区分正确与错误推理的关键指标\n\n### 5.2 数据集级别分析\n- 策略优化与数据集级别多样性扩展呈强正相关：随着模型改进，数据集级别的ER及其一阶、二阶差异一致增加\n- ER揭示超越传统秩限制的优化：后期训练中传统秩平台化，表明模型已固定在有限数量的线性独立推理\"方向\"，而同时上升的ER表明模型在提高现有解决方案空间的质量\n\n### 5.3 解耦的核心证据\n- 在隐藏状态级别，探索(ER)和利用(ERV)显示接近零的相关性\n- 这提供了强有力的证据表明：**权衡不是RLVR推理的固有属性，而是有偏见的token级别测量的假象**\n- 两个能力不是对抗性的，而是可以解耦并同时增强的\n\n## 6. 方法创新：VERL - 超越权衡的协同增强框架\n\n基于探索和利用在隐藏状态级别解耦的核心洞察，作者提出了Velocity-Exploiting Rank-Learning (VERL)方法，通过直接塑造RL优势函数实现协同增强：\n\n### 6.1 稳定表示偏差指标\n- 计算当前轨迹的ER、ERV和ERA值\n- 通过指数移动平均(EMA)归一化这些值，计算相对偏差dₖ\n- 偏差dₖ量化当前轨迹表示结构与策略近期平均行为的偏离程度\n\n### 6.2 ERA驱动的动态权重机制\n- 定义两个正交单位向量：探索导向向量w_explore = [1,0]和利用导向向量w_exploit = [0,1]\n- 通过二阶度量偏差d₂动态插值权重：w_dyn = β·w_explore + (1-β)·w_exploit，其中β = sigmoid(d₂)\n- **核心机制**：当ERA高(d₂≫0)时，表示未来过度自信风险，增加β favor 探索；当ERA低(d₂≤0)时，表示推理饱和，减少β强调利用\n\n### 6.3 优势函数重塑\n- 定义辅助优势Φ = w_dyn,0·tanh(d₀) + w_dyn,1·tanh(d₁)\n- 重塑优势：Â = A⁽⁰⁾ + min[max(0,Φᵢ), |A⁽⁰⁾|/κ]，其中κ是稳定训练的裁剪因子\n- **协同效应**：前瞻性地放大探索奖励防止过度自信，同时加强利用收益巩固推理路径，同时增强两种能力\n\n## 7. 实验验证：VERL的有效性与泛化性\n\n### 7.1 性能提升\n- 在多个LLM和推理基准上显示一致增益，Gaokao 2024数据集上高达21.4%绝对准确率提升\n- 同时提高了Pass@1(利用能力)和Pass@k(探索能力)，特别是在挑战性基准上\n\n### 7.2 消融实验\n- **步长分析**：不同步长值下VERL性能提升稳健，表明底层信号对采样频率不过度敏感\n- **优势裁剪分析**：所有测试κ值下VERL一致增强性能，证明其稳定性\n- **信号组成分析**：仅使用探索相关项导致模型过早瓶颈，仅使用利用相关项导致快速平台化，结合两项实现最佳性能\n\n## 8. 理论贡献：重新定义探索与利用的关系\n\nVERL方法的核心贡献是重新定义了探索和利用在RLVR中的关系：\n\n1. **挑战传统观点**：探索-利用权衡是token级别分析的假象，而非推理的固有属性\n2. **揭示解耦现象**：在隐藏状态表示中，探索和利用可以解耦并同时增强\n3. **提供量化工具**：ER、ERV和ERA提供了量化隐藏状态动态的新框架\n4. **实现协同增强**：VERL是首个实现协同探索-利用增强原则的操作方法\n\n这一逻辑演进展现了从观察现象、质疑假设、设计新度量、实证验证到方法创新的完整科学研究链条，为RLVR领域提供了新的理论基础和实践方法。"
                },
                {
                    "title": "Anchored Supervised Fine-Tuning",
                    "arxiv_id": "2509.23753",
                    "authors": "He Zhu, Junyou Su, Peng Lai, Ren Ma, Wenjia Zhang, Linyi Yang, Guanhua Chen",
                    "summary": "Post-training of large language models involves a fundamental trade-off between supervised fine-tuning (SFT), which efficiently mimics demonstrations but tends to memorize, and reinforcement learning (RL), which achieves better generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged as a promising middle ground, reweighting SFT objectives with token probabilities and achieving improvements in certain reasoning domains, though it exhibits instability in other tasks. We provide a analysis of DFT through the reward-weighted regression (RWR) framework, revealing that it corresponds to a specific auxiliary distribution choice that yields provably tighter RL bounds than standard SFT. However, our analysis also uncovers a critical limitation: this construction lacks distributional anchoring, leading to progressive drift that undermines training stability. To address this, we propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's reweighting with lightweight KL regularization to preserve tightness while ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation, achieving substantial improvements with minimal computational overhead. Our RWR framework provides a systematic lens for understanding post-training methods and demonstrates that principled theoretical analysis leads to both stronger guarantees and practical gains.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式（Anchored Supervised Fine-Tuning, ASFT）来改进大语言模型的后训练过程。论文关注的是LLM的基础能力改进，特别是解决监督微调(SFT)和强化学习(RL)之间的权衡问题，旨在提高模型的推理能力和训练稳定性。论文明确提到在数学推理(mathematical reasoning)方面的改进，这直接符合\"改进LLM的通用推理能力\"的研究目标。 其次，论文包含多个正面指标：核心概念上讨论大语言模型；能力方向上专注于推理能力，特别是数学推理；训练方法上探讨了强化学习与监督微调的结合，提出了新的训练方法。 关于排除标准，虽然论文提到了在\"医学知识基础\"上的测试，但这只是作为评估模型性能的一个领域，而不是论文的主要焦点。论文的核心是提出一种通用的训练方法，而不是专注于医学或其他特定应用领域。论文也不涉及多模态与视觉、模型可靠性等排除领域。 在特殊和模糊情况处理方面，论文提出的ASFT方法通过KL正则化来确保训练稳定性，这间接有助于提高模型的推理质量和可靠性，符合提升模型通用推理能力的目标。 综上所述，这篇论文的核心贡献是提出一种新的通用训练方法来增强大语言模型的推理能力和稳定性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型后训练中监督微调(SFT)与强化学习(RL)之间的效率与泛化权衡问题。针对SFT容易记忆而RL计算成本高的场景，我们提出了一种锚定监督微调(ASFT)方法，通过在动态微调(DFT)基础上添加轻量级KL正则化来防止分布漂移。在数学推理、医学知识和代码生成任务上，通过准确率和KL散度等指标验证了ASFT的优越性能和稳定性。",
                    "summary_translation": "大型语言模型的后训练涉及监督微调(supervised fine-tuning, SFT)和强化学习(reinforcement learning, RL)之间的基本权衡。前者能高效地模仿示例但容易导致记忆，后者能实现更好的泛化但计算成本更高。动态微调(Dynamic Fine-Tuning, DFT)最近作为一种有前景的中间路线出现，它通过标记概率(token probabilities)对SFT目标进行重新加权，并在某些推理领域取得了改进，尽管它在其他任务中表现出不稳定性。我们通过奖励加权回归(reward-weighted regression, RWR)框架对DFT进行了分析，揭示它对应于一种特定的辅助分布(auxiliary distribution)选择，这种选择产生的RL界限可证明比标准SFT更紧。然而，我们的分析也揭示了一个关键限制：这种构造缺乏分布锚定(distributional anchoring)，导致渐进漂移(progressive drift)，从而破坏训练稳定性。为解决这一问题，我们提出了锚定监督微调(Anchored Supervised Fine-Tuning, ASFT)，它通过轻量级KL正则化(KL regularization)增强DFT的重新加权机制，以保持紧致性同时确保稳定性。实验上，ASFT在数学推理、医学知识基础(medical knowledge grounding)和代码生成等任务上一致地优于SFT和DFT，以最小的计算开销实现了显著改进。我们的RWR框架为理解后训练方法提供了系统视角，并展示了基于原则的理论分析如何带来更强的保证和实际收益。",
                    "inspiration_trace": "# 从权衡困境到锚定解决方案：ASFT方法的逻辑演进\n\n## 1. 宏观问题：大语言模型后训练的根本权衡\n\n作者首先识别了大语言模型(LLMs)后训练领域的核心困境：监督微调(SFT)与强化学习(RL)之间存在根本性权衡。\n\n**SFT的优势与局限**：\n- 高效：数据与计算效率高，能快速获取期望行为\n- 局限：倾向于记忆表面模式，缺乏鲁棒泛化能力\n\n**RL的优势与局限**：\n- 高效：通过结果驱动更新和探索，发现可转移行为，泛化性强\n- 局限：计算成本高、训练不稳定、实现复杂\n\n这一权衡激发了研究需求：如何保留SFT的效率，同时获得RL的泛化优势？\n\n## 2. 中观观察：现有折中方法的局限\n\n作者聚焦于动态微调(DFT)这一新兴折中方案，通过初步实验发现关键问题：\n\n**DFT的创新点**：\n- 识别标准SFT中的病态奖励结构（当模型概率接近零时导致无界方差）\n- 通过基于概率的重新加权解决此问题，在数学推理任务中取得显著改进\n\n**DFT的局限性**：\n- 领域特异性：在推理密集型领域表现出色，但在知识密集型任务中不稳定\n- 缺乏理论依据：设计选择缺乏理论基础\n- 训练不稳定性：某些任务中表现出明显的性能波动\n\n这些观察引出核心问题：DFT为何在某些领域有效而在其他领域失效？其不稳定的根源是什么？\n\n## 3. 理论框架：奖励加权回归(RWR)视角\n\n为深入理解DFT，作者采用奖励加权回归(RWR)框架进行分析，建立SFT和RL之间的理论连接：\n\n**SFT作为RL下界的理论解释**：\n- 在稀疏奖励设置下，SFT可视为优化RL目标的下界\n- 随着训练进行，当πθ偏离πref时，这一下界变得愈发松散\n\n**辅助分布的一般化框架**：\n- 通过辅助分布q(τ)可构建更紧密的界限\n- 辅助分布选择决定了界限紧密性与优化稳定性\n- 形成了\"有效性vs稳定性\"的基本权衡\n\n这一框架为理解DFT并开发新方法提供了系统性视角。\n\n## 4. DFT理论分析：优势与根本缺陷\n\n在RWR框架下，作者对DFT进行了深入分析，揭示其双重特性：\n\n**关键发现1：DFT对应特定辅助分布选择**\n- DFT目标等价于选择特定辅助分布构造\n- 这一构造直接恢复DFT序列级目标\n\n**关键发现2：DFT实现比SFT更紧密的界限**\n- 证明DFT产生的RL目标下界比标准SFT更紧密\n- 当策略对示范样本分配非均匀概率时，界限严格更紧密\n- 解释了DFT在策略分布表现出足够方差领域中的优越性能\n\n**关键发现3：DFT存在分布漂移**\n- 策略分布在训练中逐渐偏离参考分布\n- 辅助分布q越来越集中在高pθ(τ)轨迹上，形成反馈循环\n- 这种分布漂移破坏RWR框架基本假设，威胁下界保证有效性\n\n这一分析揭示了DFT领域特定有效性的理论解释，以及其不稳定性的根本原因。\n\n## 5. 核心假设：锚定机制解决分布漂移\n\n基于对DFT的理论分析，作者形成核心假设：通过添加分布锚定机制可解决DFT的分布漂移问题，同时保持其紧密性优势。\n\n**分布漂移的根源**：\n- DFT的辅助分布构造缺乏锚定机制\n- 基础不等式u ≥ 1 + log u仅在u=1时取等号\n- 在DFT中，当πθ(τ)=qθ(τ)时界限紧密，但随着训练进行，pθ(τ)变得非均匀，导致不等式松散\n\n**假设形成**：\n- 添加适当锚定机制可控制分布漂移\n- 保持界限紧密性，提高训练稳定性和性能\n- 关键在于找到平衡点：既允许足够探索获得紧密界限，又防止过度漂移导致不稳定\n\n## 6. 方法提出：锚定监督微调(ASFT)\n\n基于上述分析，作者提出锚定监督微调(ASFT)，通过轻量级KL正则化解决DFT的分布漂移问题：\n\n**方法设计**：\n- 在DFT目标函数基础上添加KL散度正则化项\n- 将策略约束在参考检查点的信任区域内\n- 数学表达式：LASFT(θ) = LDFT(θ) + λEs[DKL(πθ(·|s)∥πbase(·|s))]\n\n**理论保证**：\n- KL项不改变下界结构，保留DFT紧密性优势\n- 锚定机制提供明确方差控制，防止破坏性指数增长\n- 在参考策略周围创建信任区域，允许探索更紧密界限同时保持稳定性\n\n**实际实现**：\n- 在token级别实现，使用标准化每位置分配序列级权重\n- 确保与理论框架数学等效性，同时实现高效计算\n- 相比标准SFT仅需最小计算开销，仅添加简单KL惩罚\n\n## 7. 实验验证：多领域性能评估\n\n作者在多个领域对ASFT进行全面验证，证明其优越性：\n\n**数学推理**：\n- 10万训练样本下，ASFT平均比DFT提高4.85分(18.6%)，比基础模型提高17.89分(142%)\n- 在挑战性基准如AMC23上表现尤为突出(36.72% vs DFT的27.19%)\n\n**医学知识**：\n- 1万样本任务中，ASFT比SFT提高8.28分(24.8%)，比基础模型提高10.65分(33.9%)\n- 仅需完整RL方法训练成本的3%\n\n**代码生成**：\n- 在代码生成任务上实现最高平均分数，显著提高HumanEval和HumanEval+性能\n\n**训练稳定性**：\n- ASFT保持稳定KL散度，同时在领域内和领域外评估中实现优越性能\n- 解决了DFT在知识密集型任务中的严重分布漂移问题\n\n## 8. 扩展分析：方法鲁棒性与应用潜力\n\n作者进一步分析了ASFT的鲁棒性和应用潜力：\n\n**模型规模扩展**：\n- 在不同规模模型(LLaMA-2-7B/70B, Qwen2.5-7B/32B/72B)上，ASFT始终优于基线\n- 改进随模型规模扩大保持稳健，在低资源设置下实现有效稳定适应\n\n**与RL方法比较**：\n- ASFT优于所有基于SFT方法，显著缩小与先进RL方法(如DAPO)的差距\n- 性能排序SFT < ASFT < DAPO实证支持RWR框架理论预测\n\n**作为RL初始化的增强**：\n- ASFT为后续RL微调提供优越初始化点\n- ASFT+DAPO比SFT+DAPO获得一致增益(+3.86分)\n- 表明KL锚定不仅提高直接监督性能，还为RL优化创建更稳定基础\n\n**计算效率优化**：\n- 提出ASFT-LoRA解决内存限制问题\n- 利用低秩适应数学特性实现内存高效实现\n- 在资源受限环境中提供实用折中方案\n\n## 结论：从理论到实践的系统演进\n\n作者提出ASFT的逻辑链展现了从问题识别到方法解决的完整科学思维过程：从宏观权衡困境出发，通过中观观察发现DFT的局限性，构建理论框架深入分析，揭示根本缺陷并提出核心假设，最终设计出ASFT方法并通过多领域实验验证其有效性。这一演进过程体现了理论分析与实践创新的紧密结合，为解决大语言模型后训练的核心挑战提供了新思路。"
                },
                {
                    "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment",
                    "arxiv_id": "2509.24610",
                    "authors": "Liang Lin, Zhihao Xu, Junhao Dong, Jian Zhao, Yuchen Yuan, Guibin Zhang, Miao Yu, Yiming Zhang, Zhengtao Yao, Huahui Yi, Dongrui Liu, Xinfeng Li, Kun Wang",
                    "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing multiple human preferences: improvements in one dimension frequently come at the expense of others, creating unavoidable trade-offs between competing objectives like helpfulness and harmlessness. While prior work mainly focuses on constraint-based optimization algorithms and data selection strategies to mitigate conflicts, these approaches overlook the fundamental issue of resolving conflicts directly at the parameter level. In this paper, we present OrthAlign, an innovative approach that pioneers a new paradigm by leveraging orthogonal subspace decomposition to fundamentally resolve gradient-level conflicts in multi-objective preference alignment. OrthAlign strategically decomposes parameter update spaces into orthogonal subspaces, ensuring that optimization toward different preferences occurs in mathematically non-interfering directions. Building upon this, we provide theoretical guarantees demonstrating that when parameter increments satisfy both orthogonal subspace constraints and spectral norm bounds, the resulting updates exhibit linear Lipschitz growth rather than exponential instability, ensuring stable convergence across all preference dimensions. Extensive experiments show that: I. OrthAlign achieves maximum single-preference improvements ranging from 34.61% to 50.89% after multiple-objective alignment across helpful, harmless, and truthful dimensions. II. With an average overall reward improvement of 13.96%.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种名为OrthAlign的新方法，通过正交子空间分解技术解决大语言模型在多目标对齐中的参数级冲突问题。这明显属于改进LLM基础能力和提出新训练范式的研究范畴，而非将LLM作为工具应用到特定领域。 其次，论文明确涉及LLM核心概念，并关注模型的基础能力改进。虽然论文没有直接讨论推理能力，但多目标对齐（如同时优化有用性、无害性和真实性）是提升LLM整体能力的基础，间接支持更好的推理和问题解决能力。 第三，论文不涉及任何排除标准中的领域。它既不关注多模态与视觉，也不针对特定应用领域（如医疗、化学等），同时虽然涉及安全性（无害性），但这是作为多目标对齐的一个维度进行研究，而非专门研究模型可靠性的应用层面技术。 最后，在特殊和模糊情况处理上，论文虽然涉及安全性，但提出了基础性的技术解决方案（正交子空间分解），而非进行社会学研究或应用层面的讨论。这种方法通过解决参数更新中的冲突，确保模型在多个维度上都能稳定提升，从而增强模型的通用能力。 因此，这篇论文致力于提高LLM的基础能力，提出新的训练范式，与\"大语言模型通用推理能力\"的研究目标一致。",
                    "summary2": "本文旨在解决大型语言模型在多目标偏好对齐中的冲突问题。针对多个竞争的人类偏好（如有用性、无害性和真实性），我们提出了一种OrthAlign方法，利用正交子空间分解确保不同偏好的参数更新在数学上不相互干扰，并在Llama-3-SFT和Mistral-SFT等多个模型上通过无害率、有用胜率和TruthfulQA MC2等指标验证了其有效性。",
                    "summary_translation": "大语言模型（Large language model, LLM）对齐（alignment）在处理多种人类偏好时面临一个关键困境：某一维度的改进往往以其他维度为代价，在有用性（helpfulness）和无害性（harmlessness）等竞争目标之间产生不可避免的权衡取舍。尽管先前的工作主要集中在基于约束的优化算法（constraint-based optimization algorithms）和数据选择策略（data selection strategies）来缓解冲突，但这些方法忽视了在参数层面直接解决冲突的根本问题。在本文中，我们提出了OrthAlign，这是一种创新方法，通过利用正交子空间分解（orthogonal subspace decomposition）开创了新范式，从根本上解决了多目标偏好对齐中的梯度级冲突（gradient-level conflicts）。OrthAlign策略性地将参数更新空间（parameter update spaces）分解为正交子空间（orthogonal subspaces），确保针对不同偏好的优化在数学上不相互干扰的方向上进行。在此基础上，我们提供了理论保证，证明当参数增量（parameter increments）同时满足正交子空间约束（orthogonal subspace constraints）和谱范数界限（spectral norm bounds）时，所产生的更新表现出线性Lipschitz增长（linear Lipschitz growth）而非指数不稳定性（exponential instability），确保在所有偏好维度上实现稳定收敛。大量实验表明：I. 在有用性、无害性和真实性维度上进行多目标对齐后，OrthAlign实现了34.61%至50.89%的最大单偏好改进。II. 平均总体奖励改进达到13.96%。",
                    "inspiration_trace": "# OrthAlign方法逻辑链分析\n\n## 1. 宏观问题：多目标对齐的根本困境\n\n作者从大语言模型(LLM)对齐领域的核心挑战出发：当模型需要同时满足多种人类偏好时(如有用性、无害性和诚实性)，在一个维度上的改进往往导致其他维度性能下降，形成不可避免的权衡取舍。这种困境限制了LLM在实际应用中的表现，因为现实场景通常需要模型同时满足多个有时相互冲突的目标。\n\n## 2. 问题观察：现有方法的局限性\n\n作者系统分析了三类主流方法及其局限：\n\n- **约束优化方法**：通过添加约束项缓解冲突，但仅是\"妥协\"而非\"解决\"\n- **数据选择策略**：依赖人工筛选数据，引入系统偏见且劳动密集\n- **模型合并方法**：合并多个专门模型，但牺牲了单个目标的性能\n\n关键观察：这些方法都未触及问题的本质——**参数层面的梯度干扰**。作者通过数学分析发现，不同目标的参数更新梯度不是正交的，而是相互干扰的，量化表达为：\n```\n|⟨∇θL(Di), ∇θL(Dj)⟩| / (||∇θL(Di)||² · ||∇θL(Dj)||²) ≠ 0, for i ≠ j\n```\n\n## 3. 理论洞察：正交子空间分解的潜力\n\n基于上述观察，作者提出核心假设：**如果将不同目标的参数更新限制在相互正交的子空间中，可从根本上消除目标间干扰**。这一假设建立在两个理论基础：\n\n1. **奇异值分解(SVD)理论**：参数矩阵可分解为不同奇异值对应的正交子空间，与小特征值相关的空间与当前偏好信息近似正交\n2. **稳定性理论**：当参数增量同时满足正交子空间约束和谱范数界限时，更新呈现线性Lipschitz增长而非指数不稳定\n\n数学上，这意味着通过正交投影矩阵P⊥约束新偏好的梯度更新，使增量矩阵ΔW被限制在互不干扰的正交子空间中。\n\n## 4. 方法设计：OrthAlign框架\n\n基于理论洞察，作者设计了OrthAlign方法，包含三个关键组件：\n\n### 4.1 正交化偏好更新与稳定性控制\n- 将参数更新空间分解为正交子空间\n- 已对齐偏好的更新发生在主奇异向量空间\n- 新偏好更新限制在与主子空间正交的补空间中\n- 提供理论保证：满足约束时更新呈现线性增长而非指数不稳定\n\n### 4.2 自适应子空间秩选择\n- 关键洞察：低秩约束下影响微小的方向，在奇异值更新后可能变得显著\n- 动态秩选择规则：重新缩放最后k个奇异值为前r个的平均值，选择最大k使奖励偏移保持在容忍度τ内\n- 数学表达：`k = max{k: R(UΣ̂(k)Vᵀ; Xsafe) - R(W; Xsafe) ≤ τ}`\n\n### 4.3 子空间约束的多偏好对齐\n- 基于选定左奇异向量形成投影矩阵：`P = ÛÛᵀ`\n- 通过投影操作约束新偏好的参数更新：`ΔW_new = P · ∇W L_new(W)`\n- 确保参数更新严格限制在与先前偏好正交的子空间内\n\n## 5. 实验验证：效果与泛化性\n\n作者通过全面实验验证OrthAlign的有效性：\n\n- **多目标对齐性能**：在两目标场景超越最佳基线8.77%，三目标场景平均提升13.96%\n- **分布保持分析**：t-SNE可视化显示OrthAlign保持分布一致性，基线方法则出现明显偏移\n- **作为性能增强器**：可提升现有方法的无害性25.06%和有用性4.86%\n- **自适应秩选择影响**：展示不同秩设置下的目标权衡模式，验证动态选择的必要性\n\n## 6. 逻辑链条总结\n\nOrthAlign的完整逻辑链条体现了从问题到解决方案的系统思考：\n1. 识别LLM多目标对齐中的根本困境\n2. 观察到现有方法未解决参数层面的梯度干扰\n3. 提出正交子空间分解可消除干扰的理论假设\n4. 设计包含正交化更新、自适应秩选择和子空间约束的方法框架\n5. 通过实验验证方法的有效性、稳定性和泛化性\n\n这一创新方法通过在参数级别实现非干扰更新，为多目标对齐提供了全新范式，从根本上解决了目标间的权衡取舍问题。"
                },
                {
                    "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
                    "arxiv_id": "2509.23676",
                    "authors": "Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
                    "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是深入研究大型推理模型(LRMs)中的推理机制，探讨推理轨迹如何影响最终答案生成，这直接关系到LLM的基础推理能力提升。论文通过三个阶段（实证评估、注意力分析和机制干预）来理解推理过程在模型内部的功能作用，属于对LLM内在推理能力的机制性研究，而非将LLM作为工具应用到特定领域。 其次，论文明确符合多个正面指标：它研究的是大型推理模型(LRMs)，直接属于LLMs范畴；核心聚焦于reasoning能力，探索推理过程与答案生成的关系；虽然未明确提及特定训练方法或新兴范式，但研究的是DeepSeek R1蒸馏模型，这类模型通常涉及先进的训练技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文的特殊情况处理方面，它不是关于智能体/工具使用的研究，也不是主要关注幻觉/可解释性/安全问题，而是专注于推理过程的机制性分析，这恰恰是提升LLM通用推理能力的核心研究方向。 因此，这篇论文的核心贡献是深入理解LLMs如何利用推理标记生成答案，揭示中间推理在塑造模型输出中的功能作用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在 [解决大型推理模型中推理过程与答案生成之间的关系问题]。针对 [DeepSeek R1蒸馏模型]，我们提出了一种 [三阶段分析方法（实证评估、注意力分析和机制干预）]，并在 [MATH-500和WildBench数据集] 上通过 [答案准确率、注意力权重和归一化logit差异] 验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在生成最终答案的同时会产生显式推理轨迹，然而这些轨迹对答案生成的影响程度仍不明确。在本研究中，我们对三个蒸馏版的DeepSeek R1模型中推理与答案生成之间的相互作用进行了三阶段调查。首先，通过实证评估，我们证明包含显式推理能持续提升不同领域的答案质量。其次，注意力分析表明，答案标记（answer tokens）在很大程度上关注推理标记（reasoning tokens），其中某些中层推理聚焦头（Reasoning-Focus Heads, RFHs）密切跟踪推理轨迹，包括自反思线索。第三，我们使用激活修补（activation patching）应用机制干预，以评估答案标记对推理激活的依赖性。我们的结果显示，对关键推理标记的扰动能够可靠地改变最终答案，这证实了从推理到答案存在方向性和功能性的信息流动。这些发现加深了我们对LRMs如何利用推理标记生成答案的理解，突显了中间推理在塑造模型输出中的功能性作用。我们的数据和代码在此URL公开可用：\\href{https://aka.ms/R2A-code}{https://aka.ms/R2A-code}。",
                    "inspiration_trace": "# 从宏观问题到方法论：论文核心方法的逻辑演进\n\n## 一、宏观问题的确立\n\n论文始于一个根本性问题：**大型推理模型(LRMs)生成的显式推理痕迹在多大程度上影响最终答案的生成？**\n\n作者观察到，DeepSeek R1等新型推理模型会生成两个明确分离的部分：推理段(Reasoning segment)和答案段(Answer segment)。这引发了一个核心疑问：这些推理痕迹是真正用于生成答案的关键组成部分，还是仅仅作为事后的合理化解释？这一问题触及模型推理的本质，关系到模型的可解释性、可信度和可控性。\n\n## 二、问题聚焦与研究空白\n\n作者通过文献分析发现现有研究的局限性：\n\n1. **领域局限**：大多数研究局限于数学和代码领域，缺乏对开放领域的广泛验证\n2. **方法局限**：现有研究主要关注推理长度与答案质量的关联性，而非推理与答案间的功能依赖关系\n3. **机制不明**：推理痕迹如何被模型内部利用的信息流机制尚不清楚\n\n基于这些空白，作者将宏观问题聚焦为三个具体研究问题：\n- 推理痕迹是否提高答案质量？\n- 答案如何关注推理过程？\n- 推理的小变化能否改变答案？\n\n## 三、研究假设的形成\n\n针对上述问题，作者提出三个核心假设：\n\n1. **功能假设**：推理痕迹不仅提高答案质量，且这种改进在多个领域普遍存在，尤其在蒸馏模型中更为显著\n2. **注意力假设**：答案标记会显著关注推理标记，且存在特定的\"推理聚焦头\"(RFHs)跟踪推理轨迹\n3. **因果假设**：推理和答案之间存在功能依赖关系，对推理痕迹的微小干预会直接影响最终答案\n\n## 四、方法论设计：三阶段递进研究框架\n\n为验证上述假设，作者设计了一个由表及里、层层递进的三阶段研究方法：\n\n### 阶段一：实证评估（黑盒视角）\n**目的**：从外部验证推理痕迹对答案质量的影响\n**方法**：比较模型在有推理痕迹和无推理痕迹情况下的性能\n**创新点**：将评估扩展到数学(MATH-500)和开放领域(WildBench)，超越前人仅限于数学和代码的研究\n**逻辑**：如果推理痕迹确实被用于生成答案，包含推理痕迹应显著提高答案质量\n\n### 阶段二：注意力分析（灰盒视角）\n**目的**：探究模型内部信息流机制，特别是答案如何关注推理过程\n**方法**：分析答案标记对不同提示段的注意力模式，从段级到层级再到头级\n**创新点**：发现并分析特定的\"推理聚焦头\"(RFHs)，这些头跟踪推理过程，包括自我反思步骤\n**逻辑**：注意力机制控制transformer模型中的信息流，若推理痕迹被用于生成答案，答案标记应显著关注推理标记\n\n### 阶段三：机制干预（白盒视角）\n**目的**：验证推理和答案之间的因果关系\n**方法**：使用激活修补(activation patching)技术，在受控环境中扰动推理痕迹\n**创新点**：设计新的\"上下文对象比较\"任务，创建对齐的干净和损坏提示，实现精确干预\n**逻辑**：强注意力本身不保证功能依赖，需通过干预验证推理痕迹对答案生成的因果影响\n\n## 五、方法论的内在逻辑演进\n\n这一三阶段方法体现了从现象到机制、从相关到因果的逻辑演进：\n\n1. **由表及里**：从外部行为观察(实证评估)到内部机制分析(注意力分析)再到因果验证(机制干预)\n\n2. **由相关到因果**：首先建立推理与答案的相关性(实证评估)，然后探究信息流动路径(注意力分析)，最后验证因果关系(机制干预)\n\n3. **多角度验证**：每个阶段从不同角度验证核心假设，形成证据链，增强结论可靠性\n\n4. **工具创新**：在分析过程中开发新工具(RFHs)用于解释和调试模型行为，扩展方法的应用价值\n\n## 六、总结\n\n论文作者从\"推理痕迹如何影响答案生成\"这一宏观问题出发，通过识别现有研究空白，形成三个核心假设，最终设计出一个由表及里、层层递进的三阶段研究框架。这一方法论不仅系统回答了核心问题，还提供了理解、解释和调试推理模型的新工具，体现了\"观察-假设-验证-创新\"的完整科学研究逻辑链。"
                },
                {
                    "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning",
                    "arxiv_id": "2509.23292",
                    "authors": "Ningning Xu, Yuxuan Jiang, Shubhashis Roy Dipta",
                    "summary": "Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是工具使用能力。论文提出了\"Pattern-Aware Tool-Integrated Reasoning\"这一新框架，专注于提升LLM\"如何使用工具\"而不仅仅是\"何时使用工具\"，这是一种新的训练范式，旨在增强模型的推理能力，特别是在数学推理方面。这完全符合第一步中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：明确提到了\"large reasoning models (LRMs)\"，与LLMs直接相关 - 能力方向：重点关注\"reasoning\"，特别是\"math reasoning\" - 新兴范式：深入探讨\"tool use\"，这是LLM新兴范式的重要组成部分 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不聚焦于特定应用领域（虽然测试于数学数据集，但方法是通用的），也不主要讨论模型可靠性等应用层面问题。 最后，从特殊和模糊情况处理来看，论文提出的是一种通用的工具使用方法，旨在增强LLM的通用问题解决能力，而不是将工具应用在特定领域。这完全符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的保留标准。 综上所述，这篇论文的核心贡献是提出了一种新的、模式感知的工具集成推理框架，通过改进LLM的工具使用能力来提升其通用推理能力，特别是在数学推理方面取得了显著进步。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决工具集成推理中工具使用模式选择不当导致失败的问题。针对数学推理场景，我们提出了一种模式感知的两阶段框架，首先构建calculator和algorithmic两种模式的代码能力，然后通过DPO对齐模式选择偏好，并在MATH500和AIME24数据集上通过Code@1和Pass@1指标验证了其有效性。",
                    "summary_translation": "工具集成推理（Tool-integrated reasoning, TIR）已成为提升大型推理模型（Large Reasoning Models, LRMs）解决复杂问题能力的关键方法。先前的研究主要关注何时调用工具，而忽视了工具的应用方式。我们识别出两种常见模式：一种是使用代码进行直接计算的\"计算器模式\"（calculator pattern），另一种是将问题编码为程序的\"算法模式\"（algorithmic pattern）。即使推理过程是合理的，模式选择不当也常常导致失败。我们提出了一个两阶段框架，首先从两种模式中构建代码能力，然后将模式选择与教师偏好对齐。在多个具有挑战性的数学数据集上，我们的模式感知方法显著提升了代码使用率和准确性，例如将MATH500数据集上的Code@1从64.0%提高到70.5%，将AIME24数据集上的Code@1从26.7%提高到50.0%。这些成果突显了模式感知方法在工具集成推理中的有效性。",
                    "inspiration_trace": "# 从问题观察到方法形成：模式感知工具集成推理的逻辑演进\n\n## 1. 宏观问题与初始观察\n\n**宏观问题**：如何提高大型推理模型(LRMs)在复杂问题上的表现？\n\n**初始观察**：\n- 工具集成推理(TIR)已成为提升LRMs能力的关键方法\n- 现有研究聚焦于\"何时调用工具\"(when)，而忽视了\"如何使用工具\"(how)\n- 作者观察到两种工具使用模式：计算器模式(直接计算)和算法模式(问题编码为程序)\n\n## 2. 问题识别与关键发现\n\n**核心问题**：模型常将工具使用策略与问题需求不匹配，以机械方式应用代码，导致工具失效。\n\n**关键案例**：\n- 计算1000! ÷ (800! × 2!)：计算器模式导致溢出，算法模式(先代数化简)成功\n- 寻找π中第一个10位素数：算法模式因上下文限制失败，计算器模式(扫描式)成功\n\n**关键发现**：失败源于模式选择不当，而非推理能力不足。正确的模式选择能将失败转为成功。\n\n## 3. 假设形成\n\n**核心假设**：若能训练模型根据问题特性选择合适的工具使用模式，将显著提高TIR效果。\n\n**假设依据**：\n- 不同数学问题适合不同工具策略\n- 模型需同时掌握两种模式能力并能智能选择\n- 现有方法缺乏对工具使用模式的显式建模\n\n## 4. 方法设计：两阶段学习框架\n\n**第一阶段：代码能力获取**\n- 目标：使模型掌握两种工具使用模式的能力\n- 方法：对每个问题生成两种风格解决方案(计算器风格和算法风格)\n- 训练：使用监督学习，最小化负对数似然\n- 期望：模型能稳定生成两种模式的可执行代码\n\n**第二阶段：模式偏好对齐**\n- 目标：使模型能根据问题特性选择合适模式\n- 方法：使用直接偏好优化(DPO)学习模式选择偏好\n- 训练：利用教师模型标记的偏好数据\n- 期望：模型能智能选择最适合问题的工具使用模式\n\n## 5. 实验验证与结果\n\n**数据构建**：\n- 使用Gemini-2.5-flash-lite作为教师模型\n- 对OpenR1-Math-220k数据集生成双模式解决方案\n- 评估集包括MATH500、AIME24等挑战性数学基准\n\n**关键结果**：\n- 基础模型几乎无代码使用能力\n- SFT后模型开始广泛使用Python代码\n- 添加DPO后，代码使用率和准确率显著提升\n- 例如，AIME24准确率从26.7%提高到50.0%\n\n## 6. 结论与贡献\n\n**主要结论**：TIR中的失败常源于模式选择不匹配，而非推理能力不足。模式感知方法能显著提高工具使用效果。\n\n**核心贡献**：\n1. 识别并形式化了TIR中的模式不匹配问题\n2. 提出两阶段框架：先构建代码能力，再对齐模式选择\n3. 实证证明该方法显著提高代码可靠性和任务性能\n\n这一逻辑演进从宏观问题出发，通过具体案例观察，形成核心假设，设计针对性解决方案，并通过实验验证其有效性，最终形成了一个完整的模式感知工具集成推理方法。"
                },
                {
                    "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking",
                    "arxiv_id": "2509.23392",
                    "authors": "Jinyi Han, Ying Huang, Ying Liao, Zishang Jiang, Xikun Lu, Haiquan Zhao, Xinyi Wang, Guanghao Zhou, Sihang Jiang, Jiaqing Liang, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao",
                    "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标。论文的核心贡献是提出一种名为\"Just-Enough Thinking\"（JET）的新训练方法，用于优化大型推理模型(LRMs)的推理过程。论文发现这些模型在推理早期已经积累了足够信息，后续步骤往往是冗余的，因此训练模型主动终止不必要的推理步骤。这直接符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力，特别是推理效率。它符合多个正面指标，包括核心概念（大型推理模型）、能力方向（推理）和训练方法（强化学习）。同时，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大型推理模型（LRMs）在复杂任务中过度思考导致的计算效率低下问题。针对LRMs在推理过程中产生冗余步骤的场景，我们提出了一种Just-Enough Thinking (JET)方法，通过轨迹截断和质量控制的长度奖励训练模型主动终止不必要推理，并在多个数学推理基准测试上通过准确率和输出长度指标验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在具有挑战性的任务上取得了令人瞩目的性能，然而它们的深度推理（deep reasoning）通常会产生大量的计算成本（computational costs）。为了实现高效推理（efficient reasoning），现有的强化学习（reinforcement learning）方法在推演（rollout）阶段仍然难以构建简短的推理路径（reasoning path），限制了有效学习。受证据积累模型（Evidence Accumulation Models）的启发，我们发现LRMs在推理早期已经积累了足够的信息，使得进一步的推理步骤（reasoning steps）变得冗余（redundant）。基于这一见解，我们提出了适度思考（Just-Enough Thinking, JET）方法，该方法训练模型主动终止（proactively terminate）不必要的推理（unnecessary reasoning）。JET在推演阶段执行轨迹截断（trajectory truncation），使模型接触到简短且分布一致（distributionally consistent）的推理路径。此外，它使用质量控制的长度奖励（quality-controlled length reward）来更好地鼓励简洁推理（concise reasoning）同时保持正确性（maintaining correctness）。大量实验（Extensive experiments）表明，JET在不牺牲准确性（sacrificing accuracy）的情况下显著提高了推理效率（reasoning efficiency）。特别是，DeepSeek-Distill-Qwen-1.5B模型在奥林匹克基准测试（Olympiad benchmark）上实现了4.6%的准确性提升（accuracy gain），同时将输出长度（output length）减少了46.3%。我们的代码可在GitHub上获取。",
                    "inspiration_trace": "# 大型推理模型的\"过度思考\"问题与解决方案：JET方法的逻辑演进\n\n## 1. 宏观问题：大型推理模型的计算效率挑战\n\n大型推理模型(LRMs)在复杂推理任务上展现出卓越性能，但其System-2式深度推理过程伴随着巨大的计算成本。这些模型往往执行超出必要数量的推理步骤，消耗过多计算资源才能得出正确答案，这种现象被称为\"过度思考\"(overthinking)。这与人类认知的高效性和适应性形成鲜明对比，因此开发在保持高准确性的同时提高计算效率的方法成为LRMs面临的核心挑战。\n\n## 2. 现有解决方案及其局限性\n\n为解决推理效率问题，强化学习(RL)已成为有前景的范式，通过额外奖励信号引导模型生成既正确又简洁的答案。现有方法主要分为两类：\n\n- **自适应思维模式选择方法**：通过监督微调为模型配备多种推理模式(思考/不思考)，然后用RL选择最适合每个问题的模式。\n- **基于长度的优化方法**：引入显式长度奖励，鼓励更简洁的推理。\n\n然而，这些方法面临根本性局限：LRMs自然倾向于冗长输出，很少生成简短推理轨迹，导致训练数据偏差；而人工压缩答案会引入模型自然生成分布与人工缩短样本间的显著分布不匹配，破坏梯度更新，损害学习效果。\n\n## 3. 关键观察：证据积累模型的启示\n\n作者从认知科学中的证据积累模型(Evidence Accumulation Models)获得启发。该模型描述人类决策是一个动态过程，其中信息被整合直到达到阈值，之后进一步的证据仅用于支持决策。基于此，作者提出假设：LRM推理过程类似，推理轨迹的早期部分已包含足够信息确定最终答案，继续生成只会导致冗余推理。\n\n为验证这一假设，作者进行了初步实验：在MATH500数据集上评估限制模型仅使用推理序列早期部分对答案正确性的影响。实验结果令人惊讶：\n- 限制模型仅使用推理的前75%保留了超过90%的原始正确解决方案\n- 仅使用前一半仍在约四分之三的问题上产生正确答案\n- 即使只有四分之一的推理，近一半的原始正确解决方案仍然有效\n\n这表明关键问题解决信息在早期积累，后续步骤对正确性贡献很小。\n\n## 4. 核心洞察：推理过程中的信息积累特性\n\n基于实验结果，作者形成核心洞察：LRMs在推理过程早期阶段已积累大部分关键问题解决信息，允许它们在显著减少推理token数量的同时保持高准确性。这一发现为解决\"过度思考\"问题提供了新思路——如果模型能够在积累足够信息后主动停止推理，就能在保持准确性的同时大幅提高计算效率。\n\n## 5. 方法论设计：Just-Enough Thinking (JET)\n\n基于上述洞察，作者提出Just-Enough Thinking (JET)方法，训练模型主动终止不必要的推理。JET通过两个关键组件实现：\n\n### 5.1 两阶段Rollout构建\n- **阶段1：完整推理** - 标准rollout阶段，模型生成完整推理轨迹，捕获不受约束的推理行为。\n- **阶段2：轨迹截断** - 通过截断完整轨迹构建更短推理路径，在每个截断点添加\"停止思考\"提示，鼓励模型立即提供结论。\n\n为有效识别截断位置，作者提出渐进式提前停止(PES)策略，为每个完整推理轨迹生成一系列截断变体，增加包含最优截断点的机会，同时保持与模型生成分布的一致性。\n\n### 5.2 奖励和目标设计\n- **基础奖励**：结合格式奖励和正确性奖励，确保答案格式正确且准确。\n- **长度奖励**：设计基于准确性的长度奖励，遵循三个原则：\n  1. 正确性优先：只有正确响应才有资格获得长度奖励\n  2. 简洁性偏好：在正确响应中，较短推理路径获得更高奖励\n  3. 按问题标准化：奖励相对于每个问题的最短和最长正确响应进行测量\n\n这种奖励设计确保模型在生成正确答案的同时，倾向于更简洁的推理路径。\n\n## 6. 实验验证与结果\n\n作者在多个基准测试上验证JET的有效性，结果表明：\n\n- JET实现了显著的输出长度减少而不影响准确性，例如DeepSeek-Distill-Qwen-1.5B在Olympiad基准上准确率提高4.6%，同时输出长度减少46.3%\n- 在具有挑战性的数学推理任务上表现出色，展现出任务感知能力，能根据任务难度动态调整推理深度\n- 在不同模型规模上保持稳定性能\n- 在常识推理任务上表现出强大的泛化能力\n\n## 7. 逻辑演进总结\n\n作者从大型推理模型的计算效率挑战出发，分析现有方法的局限性，从认知科学获得启发形成假设，通过实验验证推理过程中的信息积累特性，最终设计出JET方法。这一方法通过两阶段rollout构建和质量控制的长度奖励，成功解决了LRMs在强化学习过程中很少生成短推理路径的问题，实现了在保持准确性的同时显著提高推理效率的目标。整个逻辑链条展示了从问题观察到解决方案的完整思考过程，体现了跨领域思维和实证验证相结合的研究方法。"
                },
                {
                    "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning",
                    "arxiv_id": "2509.23129",
                    "authors": "Haotian Liu, Shuo Wang, Hongteng Xu",
                    "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为C$^2$GSPG的置信度校准组序列策略梯度方法，旨在通过强化学习增强大语言模型的自我意识推理能力。论文直接针对LLM的通用推理能力进行改进，特别是解决现有RL方法中的过度自信问题，提高模型在逻辑和数学推理任务中的表现。它符合筛选标准中的核心判断，即论文本质是关于改进LLM的基础能力和提出新的训练范式。论文包含多个正面指标，如关注LLMs的核心概念、reasoning能力方向以及reinforcement learning训练方法。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型可靠性的应用层面研究。虽然论文讨论了过度自信问题（与幻觉相关），但它是从提升推理能力的角度出发，符合\"提出新方法来减少幻觉，从而提升模型的通用可靠性和推理质量\"的保留标准。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决强化学习推理模型中的过度自信问题，实现自我感知推理。针对逻辑和数学推理任务，我们提出了一种置信度校准的组序列策略梯度方法（C²GSPG），通过GSPG框架消除token级别偏差，并应用交叉熵正则化器校准模型置信度与序列奖励的一致性。在K&K逻辑推理数据集和多个数学推理benchmark上，通过准确率、Brier分数和期望校准误差（ECE）等指标验证了该方法在推理性能和置信度校准方面的优越性。",
                    "summary_translation": "强化学习（Reinforcement Learning, RL）方法，以群体相对策略优化（Group Relative Policy Optimization, GRPO）及其变体为代表，在开发推理模型中发挥着核心作用。然而，这些方法常常面临一个关键的自负问题（overconfidence issue），这阻碍了它们实现具有自我意识的推理模型。在本研究中，我们提出了一种简单而有效的置信度校准群体序列策略梯度方法（confidence-calibration group sequence policy gradient method），称为C$^2$GSPG，该方法在抑制自负的同时，同步提升了推理性能。\n\n在原理上，我们提出了一个用于学习推理模型的群体序列策略梯度框架（Group Sequence Policy Gradient, GSPG），该框架消除了GRPO及其变体中常见的token级别偏差（token-level bias）。在此框架中，我们使用归一化的序列级别概率（normalized sequence-level probability）来定义每个推理问题的模型置信度，然后应用交叉熵正则化器（cross-entropy regularizer）将模型置信度校准到序列的奖励。我们证明，对于二元奖励（binary rewards），置信度校准正则化器和GSPG是协同的，因为它们的目标总是共享相同的梯度方向。对于非二元奖励（non-binary rewards），我们应用非线性奖励归一化（nonlinear reward normalization）和自适应正则化器裁剪（adaptive regularizer clipping），以减轻两个目标之间的潜在冲突。\n\n将C$^2$GSPG应用于逻辑和数学推理任务中对大型语言模型进行后训练，我们展示了它在推理准确性和置信度校准两方面均优于最先进方法（state-of-the-art methods）的性能。C$^2$GSPG的代码可在https://github.com/HaotianLiu123/CCGSPG获取。",
                    "inspiration_trace": "# C²GSPG方法提出的逻辑链分析\n\n## 一、宏观问题：推理模型的过度自信困境\n\n作者从强化学习(RL)在推理模型中的应用中发现一个根本性问题：尽管GRPO及其变体能提升大语言模型(LLM)的推理能力，但这些方法普遍存在**过度自信(overconfidence)问题**——模型无法意识到自己的错误。具体表现为模型生成序列的概率与相应奖励之间存在严重不匹配：高概率序列往往导致低奖励，使模型缺乏自我认知能力。\n\n## 二、问题深入：现有方法的局限性分析\n\n作者深入探究了现有解决方案的不足，发现两个关键缺陷：\n\n1. **策略优化与置信度校准的冲突**：现有方法通过调整奖励函数或降低高置信度低质量样本的重要性来缓解过度自信，但这些方法往往牺牲推理准确性或无法充分校准置信度。\n\n2. **token-level偏差问题**：GRPO等方法在token级别计算策略梯度，导致低概率token在训练中过度主导，影响模型稳定性。\n\n这些观察引出一个核心问题：**如何在不损害策略优化的前提下实现有效的置信度校准？**\n\n## 三、关键洞察：序列级概率与奖励的一致性\n\n作者提出一个基本假设：**模型生成序列的概率应反映其对问题的真实置信度**。基于此，作者获得两个关键洞察：\n\n1. **序列级概率的优越性**：相比token-level概率或口头表达的置信度，归一化的序列级概率能更直接、更普遍地衡量模型的内部确定性。\n\n2. **二元奖励场景下的协同性**：在二元奖励场景中，策略优化和置信度校准的目标天然一致——两者都希望模型对正确答案更自信，对错误答案更不自信。\n\n## 四、理论假设：梯度方向一致性\n\n基于上述洞察，作者提出一个可验证的理论假设：**在二元奖励场景下，策略优化方向和置信度校准方向具有相同的梯度符号**。这一假设若成立，意味着两个目标可以协同工作而非相互冲突。\n\n作者通过数学推导证明了这个假设：对于正确响应(ri=1)，策略优化方向(ri-m)和正则化方向(ri-cθ,i)均为非负；对于错误响应(ri=0)，两者均为非正。这种一致性为设计统一的目标函数提供了理论基础。\n\n## 五、方法构建：C²GSPG的设计\n\n基于理论假设，作者构建了C²GSPG方法，包含两个核心组件：\n\n### 1. Group Sequence Policy Gradient (GSPG)框架\n- **消除token-level偏差**：在序列级别而非token级别计算策略梯度，确保所有token被均匀对待。\n- **置信度定义**：使用归一化序列级概率作为模型置信度：cθ,i = exp(1/|oi|∑logπθ,i,t)\n- **置信度调制优势函数**：设计˜Ai = (ri-m)/(1-cθold,i)，使低置信度序列在梯度更新中获得适当权重。\n\n### 2. 置信度校准正则化器\n- **二元交叉熵正则化**：添加β[ri log cθ,i + (1-ri) log(1-cθ,i)]项，直接对齐置信度与奖励。\n- **理论保证**：在二元奖励场景下，正则化器与策略优化共享相同梯度方向，形成协同效应。\n\n### 3. 非二元奖励场景的扩展\n- **非线性奖励归一化**：使用sigmoid函数将任意奖励映射到[0,1]范围，保持奖励顺序的同时放大正负反馈差距。\n- **自适应正则化裁剪**：当梯度方向冲突时(Ii(β)=0)，禁用正则化项，避免干扰有效的策略信号。\n\n## 六、验证与完善：实验驱动的迭代\n\n作者通过逻辑和数学推理任务验证方法，并根据实验结果进一步完善：\n\n1. **二元奖励场景验证**：在数学推理任务中，C²GSPG显著提高了准确性和置信度校准，证实了理论假设的正确性。\n\n2. **非二元奖励场景优化**：在逻辑推理任务中，通过消融实验验证了非线性归一化和自适应裁剪的必要性，解决了梯度冲突问题。\n\n3. **超参数调优**：发现正则化强度β控制准确性与校准之间的权衡，为实际应用提供了调节手段。\n\n## 七、总结：从问题到解决方案的完整逻辑链\n\n作者从\"推理模型过度自信\"这一宏观问题出发，通过分析现有方法的局限性，提出\"序列级概率与奖励一致性\"的核心洞察，建立\"梯度方向一致性\"的理论假设，最终设计出C²GSPG方法。整个逻辑链条体现了从观察→假设→理论→方法→验证的完整科研思维过程，成功解决了在不损害推理性能的前提下实现有效置信度校准的关键挑战。"
                },
                {
                    "title": "Multiplayer Nash Preference Optimization",
                    "arxiv_id": "2509.23102",
                    "authors": "Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi",
                    "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，具体来说是通过提出Multiplayer Nash Preference Optimization (MNPO)这一新的训练范式来增强LLM与人类偏好的对齐能力。论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的基础能力，这符合第一步的保留标准。 其次，从正面指标分析，论文明确涉及Large language models (LLMs)这一核心概念，并且提出了基于强化学习(RLHF)的新训练方法。虽然论文没有直接提及reasoning、planning等能力方向，但alignment能力的提升对于LLM理解复杂指令、执行多步任务等推理相关能力有重要支撑作用。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面的研究。 最后，论文提出的MNPO框架通过将偏好学习从双人扩展到多人设置，解决了现有方法在处理非传递性和异质性偏好时的局限性，这种基础方法论的改进有助于提升LLM的通用能力，间接支持其推理能力的增强。 综上所述，这篇论文的核心贡献是提出了一种改进LLM基础能力的新训练范式，符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决传统RLHF方法难以捕捉现实世界中偏好的非传递性和异质性问题，以及现有NLHF方法仅限于两玩家设置的局限性。针对复杂人类偏好对齐场景，我们提出了多人Nash偏好优化(MNPO)框架，将偏好优化建模为n玩家游戏，每个策略在正则化到参考模型的同时与对手群体竞争。在AlpacaEval 2.0、Arena-Hard和MT-Bench等指令遵循基准上，通过胜率和长度控制胜率等指标验证了MNPO的有效性，实验表明其一致优于现有NLHF基线，特别是在异质性注释条件下实现了更优的对齐质量。",
                    "summary_translation": "人类反馈强化学习（Reinforcement learning from human feedback, RLHF）已成为将大型语言模型（large language models, LLMs）与人类偏好对齐的标准范式。然而，基于Bradley-Terry假设的奖励方法难以捕捉真实世界偏好的非传递性（non-transitive）和异质性（heterogeneous）。为解决这一问题，最近的研究将alignment重新框架化为双人纳什博弈（Nash game），从而产生了人类反馈纳什学习（Nash learning from human feedback, NLHF）。尽管这一视角启发了如INPO、ONPO和EGPO等具有强大理论和实证保证的算法，但它们本质上仍局限于双人交互（two-player interactions），存在单一对手偏见（single-opponent bias），无法捕捉真实偏好结构的全部复杂性。在这项工作中，我们提出了多人纳什偏好优化（Multiplayer Nash Preference Optimization, MNPO），这是一个将NLHF推广到多人制度（multiplayer regime）的新框架。它将alignment表述为n人博弈（n-player game），其中每个策略（policy）在与对手群体竞争的同时被正则化（regularized）朝向参考模型（reference model）。我们的框架在多人环境中建立了定义明确的纳什均衡（Nash equilibria），并将对偶间隙（duality gap）概念扩展到量化近似质量（approximation quality）。我们证明MNPO继承了双人方法的均衡保证（equilibrium guarantees），同时实现了更丰富的竞争动态（competitive dynamics）和更多样化的偏好结构（preference structures）覆盖。通过全面的实证评估，我们表明MNPO在指令跟踪基准（instruction-following benchmarks）上始终优于现有的NLHF基线，在异质性注释条件（heterogeneous annotator conditions）和混合策略评估场景（mixed-policy evaluation scenarios）下实现了卓越的对齐质量（alignment quality）。这些结果共同确立了MNPO作为将LLM与复杂、非传递性人类偏好对齐的原则性和可扩展框架。代码可在https://github.com/smiles724/MNPO获取。",
                    "inspiration_trace": "# 多人纳什偏好优化(MNPO)的逻辑推演\n\n## 1. 宏观问题：LLM对齐中的偏好建模挑战\n\n**观察**：人类偏好本质上是非传递性和异质性的，而传统RLHF方法基于Bradley-Terry模型，假设偏好是传递性的，可用标量奖励函数表示。实证研究表明，这种假设在真实场景中往往不成立。\n\n**核心问题**：如何在LLM对齐过程中捕捉和建模人类偏好的非传递性和异质性？现有方法在处理复杂偏好结构时存在根本性局限。\n\n## 2. 中间问题：双人博弈框架的局限性\n\n**观察**：近期研究将偏好优化重新定义为在一般偏好神谕定义的博弈中寻找纳什均衡，产生了纳什学习从人类反馈中学习(NLHF)范式。这种方法通过INPO、ONPO、EGPO等算法实现了更好的理论保证和实证稳定性。\n\n**关键局限**：现有NLHF方法仍局限于双人设置（单个策略对单个对手），而现实世界的偏好对齐涉及多样化注释者、异质性评估标准或混合历史模型，更适合建模为多人博弈。\n\n**核心假设**：将NLHF扩展到多人环境可以更好地捕捉真实偏好结构的复杂性，减少单一对手偏见，提供更全面的偏好覆盖。\n\n## 3. 具体问题：多人博弈 formulation 的挑战\n\n**关键挑战**：\n1. 如何制定目标，在多个策略间平衡公平性和竞争力？\n2. 如何设计算法收敛到多人纳什均衡？\n3. 如何在扩展到更大LLMs时确保可计算性？\n\n**理论需求**：需要在多人环境中定义纳什均衡，确保其存在性和唯一性，并设计实用算法来近似均衡，同时保持计算效率。\n\n## 4. 解决方案：多人纳什偏好优化(MNPO)\n\n### 4.1 核心思想\n\n将LLM对齐构建为n人博弈，每个策略同时与所有其他策略竞争，同时正则化到参考策略，创建竞争均衡，平衡对群体的性能与对可信基线的adherence。\n\n### 4.2 理论构建\n\n**多人博弈 formulation**：\n- 考虑n个策略{π₁, π₂, ..., πₙ}，每个策略寻求最大化其相对于所有其他策略的平均偏好概率\n- 目标函数：J(πᵢ, {πⱼ}ⱼ≠ᵢ) = Eₓ∼D[Eᵧᵢ∼πᵢ,{ᵧⱼ|ᵧⱼ∼πⱼ}ⱼ≠ᵢ[P(yᵢ ≻ {yⱼ}ⱼ≠ᵢ | x)] - τ KL(πᵢ(·|x)‖πᵣₑf(·|x))]\n\n**纳什均衡和对偶间隙**：\n- 定义多人纳什均衡：策略π*，其中没有玩家可以通过单方面偏离来改善其目标\n- 扩展对偶间隙概念到多人设置：DualGap(π) := E_{πⱼ∈Oπ}[max_{πⱼ} min_{Oπ\\πⱼ} J(πⱼ, π∪Oπ\\πⱼ)] - min_{Oπ} J(π, Oπ)\n- 证明均衡条件满足当且仅当DualGap(π*) = 0\n\n### 4.3 算法设计\n\n**迭代框架**：\n- 基于乘法权重更新设计：πᵢ^{(t+1)}(y|x) ∝ (∏_{j≠i} πⱼ^{(t)}(y|x))^{1/(n-1)} exp(η/(n-1) ∑_{j≠i} P(y ≻ πⱼ^{(t)} | x))\n- 引入时间依赖的MNPO(TD-MNPO)，对手集使用历史策略的加权组合自适应演化\n- 设计损失函数L'ₜ(π) = E_{y,y'∼πₜ, yʷ,yˡ∼λP(y,y')}[(hₜ(π, yʷ, yˡ) - 1/(2η))²]，避免计算难以处理的归一化因子\n\n**理论保证**：\n- 证明MNPO继承了双人方法的均衡保证\n- 证明MNPO统一了许多现有偏好优化方法作为特例\n- 证明算法收敛到多人纳什均衡\n\n## 5. 实证验证\n\n**实验设计**：在指令跟随和推理基准测试上全面评估，特别关注异质性注释条件和混合策略评估场景。\n\n**关键结果**：\n- MNPO在指令跟随基准测试上一致性优于现有NLHF基线\n- 在异质性注释条件下实现更优越的对齐质量\n- 在混合策略评估场景中展现更强鲁棒性\n\n## 6. 结论与意义\n\n**核心贡献**：\n- 理论框架：建立MNPO的自然均衡表征，包括明确定义的纳什策略和对偶间隙\n- 算法创新：引入时间依赖的MNPO，对手集自适应演化\n- 实证验证：证明MNPO在多样化场景下的优越性能\n\n**深远意义**：通过将NLHF进展与对齐LLMs到多样化、非传递性人类偏好的实际需求相结合，MNPO为下一代对齐技术建立了可扩展的基础，更好地捕捉了真实世界偏好结构的复杂性。"
                },
                {
                    "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
                    "arxiv_id": "2509.23234",
                    "authors": "Runyan Tan, Shuang Wu, Phillip Howard",
                    "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"$p$-less sampling\"的无超参数解码方法，用于改进大语言模型的输出质量。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从核心判断来看，这篇论文的本质是改进LLM的基础解码能力，而不是将LLM作为工具应用到特定领域。论文提出了一种新的采样/解码策略，这是一种改进LLM基础能力的方法论研究，符合第一步的保留标准。 其次，论文包含多个正面指标：它明确关注Large Language Models (LLMs)这一核心概念；在能力方向上，论文特别在数学推理(math reasoning)和逻辑推理(logical reasoning)任务上验证了方法的有效性，这些正是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用的解码方法改进。 最后，论文提出的方法通过改进解码策略来提高LLM在推理任务上的表现，这与研究目标\"提高大语言模型本身的通用推理能力\"直接相关。虽然论文也提到了创意写作任务，但其核心贡献在于提升LLM在数学和逻辑推理等通用能力上的表现，因此符合研究范围。",
                    "summary2": "本文旨在解决大型语言模型(LLM)解码过程中采样策略对超参数敏感的问题。针对不同生成任务和温度配置下需要调整超参数的场景，我们提出了一种基于信息论的p-less采样方法，该方法无需超参数，能在每个解码步骤动态设置截断阈值。我们在数学、逻辑推理和创意写作任务上通过准确率、生成效率和多样性等指标验证了其有效性，实验表明p-less采样在高温下仍能保持高质量输出，且推理效率更高。",
                    "summary_translation": "从大型语言模型（Large Language Models, LLMs）获得高质量输出通常依赖于基于采样的解码策略选择，以在每个生成步骤中概率性地选择下一个token（令牌）。虽然已经提出了多种此类采样方法，但它们的性能可能对超参数（hyperparameters）的选择敏感，而超参数可能需要根据生成任务和温度（temperature）配置进行不同设置。在这项工作中，我们介绍了$p$-less采样：一种基于信息理论（information-theoretic）的采样方法，它基于整个token（令牌）概率分布在每个解码步骤动态设置截断阈值（truncation threshold）。与现有方法不同，$p$-less采样没有超参数，并且在温度（temperature）增加时持续产生高质量输出。我们提供了关于$p$-less采样的理论视角以奠定我们提出方法的基础，并进行实验以在一系列数学、逻辑推理和创意写作任务上实证验证其有效性。我们的结果展示了$p$-less采样如何持续优于现有采样方法，同时在更高温度（temperature）值下表现出更少的文本质量下降。我们进一步展示了$p$-less如何通过较低的平均token（令牌）采样时间和更短的生成长度实现比替代方法更高的推理时间（inference-time）效率，而不牺牲准确性。最后，我们通过定性示例、案例研究和多样性评估提供分析，以突出$p$-less的优势。",
                    "inspiration_trace": "# p-less Sampling方法逻辑演进分析\n\n## 1. 宏观问题观察：现有采样方法的局限性\n\n作者首先观察到大型语言模型(LLM)的输出质量高度依赖于采样策略的选择。现有的截断式采样方法（如top-p、top-k、ϵ-sampling、mirostat、min-p等）存在两个核心问题：\n\n- **超参数敏感性**：这些方法的性能依赖于超参数选择，而最佳超参数值因任务和温度配置而异，需要大量调优工作。\n- **高温退化现象**：当温度增加时，这些方法往往导致文本质量显著下降，出现退化现象，无法在高温下保持输出质量。\n\n## 2. 问题深入分析：高温退化的根本原因\n\n作者进一步分析了为什么现有方法在高温下表现不佳：\n\n- **温度的本质**：温度参数用于调整模型输出的概率分布，高温会使概率分布更加平坦，增加随机性。\n- **静态截断机制**：现有方法的截断阈值通常是静态的或仅基于局部信息（如最高概率token的概率），无法适应分布的全局变化。\n- **缺乏分布感知**：大多数方法没有充分利用整个概率分布的信息来确定截断阈值，导致在高温下无法有效过滤低质量token。\n\n## 3. 理论探索：寻找信息理论基础\n\n作者转向信息理论，寻找能够动态适应概率分布变化的理论基础：\n\n- **概率分布的全局特性**：需要一种能够捕捉整个概率分布特性的度量，而非仅依赖局部信息。\n- **熵的概念**：熵是衡量概率分布不确定性的指标，但直接使用熵可能不够直观。\n- **Rényi熵家族**：作者探索了Rényi熵，特别是二阶Rényi熵（碰撞熵），它与概率平方和有关，能够反映分布的集中程度。\n\n## 4. 关键洞察：正确随机猜测的概率\n\n作者提出了一个核心问题：\"给定一个token概率分布，我们应该从哪个可靠子集中采样？\"\n\n- **随机猜测的正确概率**：在给定概率分布下，随机选择一个token恰好是\"正确\"token的概率是多少？\n- **数学表达**：这个概率可以表示为所有token概率的平方和：L[P] = Σ P(v)^2\n- **作为阈值**：可以将这个概率作为截断阈值，只保留概率不低于此阈值的token，确保采样质量。\n\n## 5. 方法形成：p-less sampling\n\n基于上述洞察，作者形成了p-less sampling方法：\n\n1. **计算阈值**：在每个解码步骤，计算L[P] = Σ P(v)^2作为阈值\n2. **构建采样集**：保留概率不低于L[P]的token：Vp-less = {v ∈ V : Pθ(v|x₁:t-₁) ≥ L[Pθ]}\n3. **归一化采样**：从保留的token中根据归一化后的概率进行采样\n\n## 6. 理论验证：连接到信息理论\n\n作者将p-less sampling与信息理论建立联系，增强方法的理论基础：\n\n- **Rényi熵的关系**：L[P] = exp(-H₂(P))，其中H₂(P)是二阶Rényi熵，表明p-less阈值与分布的碰撞熵直接相关\n- **与Shannon熵的关系**：L[P]与Shannon熵呈负相关，随熵增加而减少\n- **统计解释**：L[P]是概率质量函数二阶矩的无偏估计量乘以词汇表大小\n\n## 7. 变体扩展：p-less-norm sampling\n\n作者进一步提出了p-less-norm采样，通过减去归一化的\"不正确随机猜测\"概率来放宽阈值：\n\n- **更宽松的阈值**：¯L[P] = L[P] - (1/(|V|-1)) × Σ_{u≠v} P(u)P(v)\n- **适用场景**：在需要更多多样性的场景中表现更好，如创意写作任务\n\n## 8. 优势分析：为什么p-less更优？\n\n作者分析了p-less相比现有方法的优势：\n\n- **分布感知**：动态适应整个token概率分布，而非仅使用局部信息\n- **有效阈值**：数学上保证非空候选集，避免其他方法的边界情况\n- **温度适应性**：随温度动态调整，在高温下保持稳定性能\n- **无超参数**：完全消除超参数调优需求\n- **计算效率**：不需要排序，时间复杂度从O(N log N)降低到O(N)\n\n## 9. 实验验证：多场景测试\n\n作者通过全面实验验证了p-less的有效性：\n\n- **数学和逻辑推理任务**：在多个数据集上测试，p-less在不同温度下表现稳定，AUC值普遍高于其他方法\n- **创意写作任务**：在高温下保持文本质量，优于其他方法，人类评估也偏好p-less生成的文本\n- **效率分析**：采样速度更快（比min-p快22%），生成长度更短但不牺牲准确性\n- **多样性分析**：在保持准确性的同时提供合理的多样性，实现帕累托最优\n\n## 总结\n\n作者从观察现有采样方法的超参数敏感性和高温退化问题出发，通过深入分析问题根源，转向信息理论寻找解决方案，最终提出基于\"正确随机猜测概率\"的p-less sampling方法。该方法不仅具有坚实的理论基础，还通过实验验证了其在多种任务和场景下的优越性能，实现了从问题观察到理论探索，再到方法形成和验证的完整逻辑演进。"
                },
                {
                    "title": "Causally-Enhanced Reinforcement Policy Optimization",
                    "arxiv_id": "2509.23095",
                    "authors": "Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang",
                    "summary": "Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。首先，从核心判断来看，论文的本质是提出一种名为\"Causally-Enhanced Policy Optimization (CE-PO)\"的新训练框架，用于改进大语言模型的因果推理能力，减少模型使用捷径策略和产生虚假推理的问题。这是一种直接提升LLM基础推理能力的方法，特别是针对逻辑和因果推理能力的增强，属于改进LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)作为核心概念；专注于reasoning能力，特别是causal reasoning（因果推理）这一重要的逻辑推理方向；提出了一种强化学习方法(CE-PO)，是对PPO/GRPO等现有强化学习训练范式的改进。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，虽然论文涉及减少\"不忠实的思维链\"(unfaithful chain-of-thought)，但它不是从应用层面讨论幻觉问题，而是通过增强因果一致性来提升模型内在的推理质量和可靠性，这符合提升模型通用推理能力的研究目标。 论文的核心贡献是提出了一种通用的强化学习训练框架，通过增强因果一致性来提升LLM的推理能力，这直接符合研究课题的核心目标——提高大语言模型的通用推理能力。",
                    "summary2": "本文旨在解决大型语言模型在强化学习中通过捷径策略获得表面正确答案但推理不忠实的问题。针对强化训练中的语言模型，我们提出了一种基于Jacobian敏感性和反事实强化的因果增强策略优化方法(CE-PO)，并在4个推理基准测试上通过准确率、鲁棒性等指标验证了其有效性。",
                    "summary_translation": "使用强化目标训练的大型语言模型（Large language models, LLMs）常常通过捷径策略获得表面上正确的答案，将正确输出与虚假或不忠实的推理配对，并在小的因果扰动下性能下降。我们提出了因果增强策略优化（Causally-Enhanced Policy Optimization, CE-PO），这是一个即插即用的奖励塑形框架，通过从提示（prompt, Z）到推理（rationale, X）再到答案（answer, Y）的生成路径上的因果连贯性的可微代理来增强策略优化。CE-PO使用基于雅可比（Jacobian）的敏感性估计模型内部影响，通过反事实强化这些信号以抑制干扰线索，并使用闵可夫斯基（Minkowski, 幂平均）组合器将得到的连贯性得分与任务准确性反馈融合，暴露出准确性和连贯性之间的单一可调权衡。统一奖励无需架构更改即可与PPO/GRPO集成。在推理基准和因果压力测试中，CE-PO减少了奖励黑客（reward hacking）和不忠实的思维链（chain-of-thought），同时提高了对相关性-因果性翻转（correlation-causation flips）和轻度反事实编辑（light counterfactual edits）的鲁棒性，同时保持接近的准确性。在4个数据集上的实验结果表明，CE-PO比基线平均提高了5.49%的准确性（高达9.58%），同时提高了对相关性-因果性翻转和轻度反事实编辑的鲁棒性。",
                    "inspiration_trace": "# 从观察到解决方案：CE-PO方法的逻辑演进\n\n## 1. 宏观问题：LLMs强化学习中的推理不忠实\n\n**观察**：大型语言模型(LLMs)在强化目标训练时，往往通过捷径策略获得表面正确的答案，将正确输出与虚假或不忠实的推理配对，并且在小的因果扰动下性能显著下降。\n\n**核心矛盾**：模型优化了\"产生什么答案\"，但几乎没有约束\"如何获得答案\"，导致推理过程与最终答案之间的因果断裂。\n\n## 2. 问题聚焦：奖励信号与推理过程的不匹配\n\n**具体观察**：\n- 模型学会\"游戏化\"奖励信号（利用长度、格式或词汇重叠）\n- 正确答案与不忠实或虚假的推理轨迹被配对\n- 现有强化目标缺乏奖励因果连贯性的内部信号\n\n**关键发现**：当模型面临Z→X→Y的生成路径（提示→推理→答案）时，缺乏对这一路径因果连贯性的约束，导致模型可能生成正确但推理不忠实的输出。\n\n## 3. 根本假设：因果连贯性是稳健推理的关键\n\n**核心假设**：稳健推理不仅需要准确的输出，还需要生成路径上的因果连贯性，即：\n- 提示(Z)真正告知推理(X)\n- 提示和推理共同真正告知最终答案(Y)\n- 对提示或推理的小而相关的因果编辑会在下游引起相应的适当变化\n\n**理论依据**：如果模型的内部影响遵循Z→X→Y的顺序，思维链(CoT)轨迹更有可能\"因正确理由而正确\"，并在分布变化时更好地泛化。\n\n## 4. 技术挑战：如何实现因果连贯性约束\n\n**三大挑战**：\n1. **可测量性**：现有忠实性工具（显著性、原始输入-梯度属性）脆弱，无法作为训练信号\n2. **稳健性**：朴素梯度信号易被非语义因素（词频、位置、格式）夸大，导致捷径学习\n3. **目标平衡**：稳健性方法常牺牲准确性，而仅优化任务奖励会保留捷径利用\n\n## 5. 解决方案：CE-PO框架的构建\n\n### 5.1 雅可比信号：因果效应的微分代理\n\n**思路**：使用雅可比矩阵作为局部、直接的因果效应代理，量化上游嵌入的微小扰动如何改变下游对数似然。\n\n**实现**：\n- 计算Z→X、X→Y和Z→Y的雅可比敏感性\n- 对Z→Y，通过投影移除X的介导效应，获得直接因果效应\n\n### 5.2 反事实强化：抑制虚假敏感性\n\n**观察**：朴素梯度信号容易被非语义因素夸大，导致奖励黑客攻击（如模型通过增加输出长度获得更高奖励）。\n\n**解决方案**：\n- 构建反事实（打乱令牌顺序），保留表面统计量但破坏语义对齐\n- 计算反事实雅可比，识别虚假子空间\n- 通过正交投影从基础信号中移除这些方向，得到去混杂的因果对齐信号\n\n### 5.3 奖励融合：准确性与连贯性的平衡\n\n**挑战**：如何在准确性和因果连贯性之间取得平衡？\n\n**解决方案**：\n- 将雅可比响应归一化为稳定的标量连贯性分数\n- 使用Minkowski（幂均值）组合器融合连贯性分数与任务准确性\n- 通过调整组合器参数，实现准确性与连贯性之间的可调权衡\n\n### 5.4 统一奖励：无缝集成到现有框架\n\n**实现**：将融合后的奖励与PPO/GRPO集成，无需架构更改，形成统一的奖励信号用于策略优化。\n\n## 6. 实验验证：CE-PO的有效性\n\n**主要结果**：\n- 在4个数据集上，CE-PO比基线平均提高5.49%准确率（高达9.58%）\n- 提高对相关性-因果性翻转和轻微反事实编辑的稳健性\n- 有效减少奖励黑客攻击和不忠实的思维链\n\n**关键发现**：\n- 移除任何雅可比项都会导致奖励黑客攻击\n- Minkowski组合器参数p提供了准确性与连贯性之间的平滑权衡机制\n- CE-PO选择性增强推理或因果相关词汇，而非均匀提升所有词汇\n\n## 7. 理论保证：CE-PO的稳健性基础\n\n**核心贡献**：提供正交化TRPO下界证明，表明CE-PO通过正交化优势函数，有效减少对虚假方向的敏感性，提供更稳健的策略优化。\n\n**理论意义**：如果策略变化完全位于虚假子空间，则对性能没有贡献，从而从根本上防止奖励黑客攻击。\n\n## 结论：从问题到解决方案的逻辑演进\n\nCE-PO方法的逻辑演进展现了从观察到假设再到方法论的完整思考过程：从LLMs强化学习中的推理不忠实这一宏观问题出发，聚焦到奖励信号与推理过程不匹配的具体问题，提出因果连贯性是稳健推理的关键假设，针对可测量性、稳健性和目标平衡三大技术挑战，设计出包含雅可比信号、反事实强化、奖励融合和统一优化的CE-PO框架，最终通过实验验证和理论保证确认了其有效性。这一逻辑链不仅解决了具体问题，还为构建\"因正确理由而正确\"的AI系统提供了新思路。"
                },
                {
                    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
                    "arxiv_id": "2509.22613",
                    "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen",
                    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心贡献在于从理论角度分析强化学习(RL)如何提升大语言模型(LLM)的规划能力。根据筛选标准，我判断如下： 第一步核心判断：论文本质上是研究如何通过强化学习方法改进LLM的规划能力，这属于提升LLM通用推理能力的范畴。规划(planning)是通用推理能力的重要组成部分，论文关注的是LLM基础能力的提升，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：聚焦于planning能力，这是推理能力的关键组成部分 - 训练方法：研究reinforcement learning (RL)对LLM的影响，包括策略梯度和Q学习 - 新兴范式：规划能力是llm-based agents和tool use的基础能力 第三步排除标准：论文不涉及任何排除标准中的领域，没有研究多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文不存在模糊情况，它明确研究通用规划能力的理论提升，而非特定领域应用。 综合来看，这篇论文通过理论分析探讨了强化学习对LLM规划能力的影响，揭示了不同RL方法的优缺点，属于提升LLM通用推理能力的基础研究，完全符合我的研究目标。",
                    "summary2": "本文旨在 [解决强化学习在语言模型规划中的理论基础问题]。针对 [语言模型规划任务]，我们提出了一种 [基于图抽象的理论分析框架]，并在 [Blocksworld规划基准和Erdős-Rényi图] 上通过 [准确性和输出多样性指标] 验证了其有效性。",
                    "summary_translation": "最近的强化学习(reinforcement learning, RL)方法显著提升了大型语言模型(Large Language Models, LLMs)的规划能力，然而其有效性的理论基础仍然难以捉摸。在这项工作中，我们通过一个可处理的基于图的抽象来研究强化学习的优势和局限性，重点关注策略梯度(policy gradient, PG)和Q学习(Q-learning)方法。我们的理论分析表明，监督微调(supervised fine-tuning, SFT)可能会引入基于共现的伪解(spurious solutions)，而强化学习主要通过探索(exploration)实现正确规划，强调了探索在实现更好泛化(generalization)中的作用。然而，我们也表明策略梯度存在多样性崩溃(diversity collapse)问题，即在训练过程中输出多样性减少，且在达到完美准确度后仍然存在。相比之下，Q学习提供了两个关键优势：离策略学习(off-policy learning)和收敛(convergence)时的多样性保持(diversity preservation)。我们进一步证明，在Q学习中需要精心设计奖励(reward design)以防止奖励黑客(reward hacking)问题。最后，我们将我们的框架应用于真实世界规划基准(benchmark)Blocksworld，确认了这些行为在实际中确实存在。",
                    "inspiration_trace": "## 面临的挑战\n作者面临的核心挑战是：强化学习(RL)在增强语言模型规划能力方面取得成功，但其理论基础尚不明确；监督微调(SFT)在规划任务中存在根本局限，倾向于记忆而非泛化；现有RL方法如策略梯度存在多样性崩溃问题。\n\n## 关键洞察\n作者通过图论抽象将规划问题建模为路径寻找，发现SFT本质上是记忆训练数据中的共现关系，无法利用传递性信息；RL优于SFT主要在于其迭代数据生成过程促进了探索；策略梯度存在多样性崩溃问题，而Q-learning具有保持多样性和支持离线学习的双重优势。\n\n## 解决方案演进\n作者首先建立理论框架分析SFT局限，然后揭示策略梯度与SFT的联系及多样性崩溃问题，转而分析Q-learning发现过程奖励可避免奖励黑客，最终证明Q-learning在保持多样性和离线学习方面的优势，并通过实验验证理论发现。\n\n## 创新点总结\n该研究首次从理论角度系统分析RL在语言模型规划中的作用机制，揭示SFT记忆和RL泛化的理论基础，发现并证明策略梯度多样性崩溃现象，提出Q-learning在语言模型规划中的优势，为改进语言模型规划中的RL方法提供理论基础。"
                },
                {
                    "title": "Not only a helper, but also a teacher: Interactive LLM Cascade",
                    "arxiv_id": "2509.22984",
                    "authors": "Yu Wu, Shuo Wu, Ye Tao, Yansong Li, Anand D. Sarwate",
                    "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger models often having better performance but higher cost: choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost. To improve the cascading efficiency, we propose Inter-Cascade, an online and interactive LLM Cascade that extends the role of strong model from a backup helper to a long-term teacher. In our system, when a strong model resolves a difficult query, it also distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning. Empirically, compared with standard LLM Cascade baselines across multiple benchmarks, the Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction). Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs, and provides a general, scalable framework applicable to both open-source and API-based LLMs.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是提出一种名为\"Inter-Cascade\"的在线交互式LLM级联系统，其核心创新在于让强模型不仅作为解决困难问题的助手，还作为\"教师\"角色，将解决方案提炼为通用、可重用的问题解决策略，从而提升弱模型的能力。这明显属于改进LLM基础能力和提出新的训练/交互范式的研究，目的是增强LLM的问题解决能力，而不是将LLM应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确聚焦于Large Language Models (LLMs) - 能力方向：关注problem-solving strategies（问题解决策略），直接涉及LLM的推理能力 - 新兴范式：描述了多个LLM之间的交互和知识传递，类似于multi-agent系统的概念 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 没有讨论模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况处理—— 论文提出的Inter-Cascade框架是一种通用的多LLM协作方法，让强模型\"教\"弱模型通用问题解决策略，这可以视为一种通用的智能体协作框架，旨在提升LLM的通用问题解决能力，而非针对特定领域的应用。 最终决策：论文的核心贡献是提出了一种让LLM之间进行知识传递和策略提炼的方法，从而提升整体推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是如何增强LLM的基础推理和问题解决能力，而非将其作为工具应用到特定领域，因此应该被保留。",
                    "summary2": "本文旨在解决LLM Cascade系统中重复查询导致的高成本问题。针对相似或重复查询场景，我们提出了一种Inter-Cascade框架，使强模型不仅作为助手还作为教师，将解决方案提炼为可重用策略提升弱模型性能，并在多个推理和事实性benchmark上通过Pipeline Accuracy、Strong LLM Call Rate和成本降低等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在能力上差异很大，较大的模型通常具有更好的性能但成本也更高：选择LLM模型通常涉及在性能和成本之间进行权衡。LLM Cascade（级联）是一种将困难查询从弱/便宜模型推迟到强/昂贵模型的范式。这种方法是非自适应的：推迟决策是离线训练的。当面对相似或重复的查询时，LLM Cascade可能会反复咨询昂贵的模型，从而产生更高的成本。为了提高级联效率，我们提出了Inter-Cascade（交互级联），一种在线且交互式的LLM Cascade，它将强模型的角色从备用助手扩展为长期教师。在我们的系统中，当强模型解决一个困难查询时，它还会将其解决方案提炼为通用的、可重用的问题解决策略，从而在后续查询中提升弱模型的性能。将策略添加到查询中使弱模型能够随时间动态提高其性能，避免了计算和时间密集型的微调（fine-tuning）。从实证角度看，与多个基准测试（benchmarks）中的标准LLM Cascade基线相比，Inter-Cascade显著提高了弱模型的准确性（高达33.06个绝对百分点）和整个系统的准确性（高达5.53个绝对百分点），同时减少了对强模型的调用（相对减少高达48.05%）并节省了相应的费用（相对节省高达49.63%）。Inter-Cascade展示了LLM之间有效的上下文知识转移（in-context knowledge transfer），并提供了一个通用的、可扩展的框架，适用于开源和基于API的LLM。",
                    "inspiration_trace": "# 从问题到方案：Inter-Cascade的逻辑演进\n\n## 1. 宏观问题：性能与成本的权衡\n\n作者首先观察到大型语言模型(LLMs)应用中的一个基本矛盾：**模型性能与使用成本之间的权衡**。更大的模型通常表现更好，但计算成本、延迟或API费用也更高。这一矛盾促使作者思考：如何在保证性能的同时降低成本？\n\n## 2. 现有方案的局限性分析\n\n作者深入研究了现有的LLM Cascade方法，该方法通过让弱模型处理简单查询，将困难查询转移到强模型来平衡性能和成本。然而，作者发现这种方法存在三个关键局限：\n\n- **非适应性**：延迟决策是离线训练的，一旦部署就固定不变，无法根据实际查询流调整\n- **重复查询浪费**：当面临相似或重复查询时，系统会反复调用昂贵的强模型，造成资源浪费\n- **单向知识流**：弱模型只能卸载任务，无法从强模型的能力中学习和提升\n\n这些局限导致系统效率低下，特别是在有大量相似查询的场景中。\n\n## 3. 关键洞察与假设形成\n\n基于对局限性的分析，作者提出了一个关键洞察：**可以利用输入相似性，让强模型帮助弱模型适应并处理相似查询**。这一洞察基于以下假设：\n\n- **相似查询具有相似解决模式**：语义上相似的查询可能需要类似的解决策略\n- **强模型的知识可以提炼**：强模型解决查询的知识可以被提炼为通用策略，而不仅仅是具体答案\n- **弱模型可以通过上下文学习**：弱模型可以通过在输入中添加策略来提高性能，而不需要参数更新\n\n## 4. 概念框架：角色转变\n\n基于上述假设，作者形成了Inter-Cascade的核心概念框架，其核心思想是**角色转变**：将强模型从单纯的\"备份助手\"扩展为\"长期教师\"。具体表现为：\n\n- 强模型不仅回答困难查询，还提炼可重用的问题解决策略\n- 这些策略被存储并用于增强弱模型的后续查询处理能力\n- 弱模型通过使用这些策略实现\"在职\"学习，动态提高性能\n\n## 5. 方法设计：交互式知识传递\n\n作者将概念框架具体化为可实现的系统设计，包含四个关键组件：\n\n1. **策略生成器**：在强模型中添加策略生成函数，将查询映射为包含查询、答案和通用解决思路的策略\n2. **策略存储库**：构建查询-策略对的集合，动态更新，形成\"知识库\"\n3. **策略匹配函数**：通过相似性排序检索top-k相关策略，实现精准知识传递\n4. **增强输入机制**：将查询与检索到的策略拼接形成增强输入，提高弱模型的处理能力\n\n## 6. 理论验证：置信度校准改进\n\n为了验证方法的有效性，作者提供了理论分析，证明：\n\n- 添加策略后，弱模型的置信度分数能更好地近似其正确概率\n- 在相同置信度阈值下，使用策略可以降低风险容忍度，提高系统准确性\n- 这种改进不是偶然的，而是有理论保证的系统性能提升\n\n## 7. 实验验证与优化\n\n作者通过多维度实验验证了方法的有效性：\n\n- **性能提升**：在多个推理和事实类数据集上，弱模型准确率提升高达33.06个百分点\n- **成本降低**：强模型调用减少高达48.05%，API成本节省高达49.63%\n- **机制验证**：对比随机策略和基于相似性检索的策略，证明相似性检索的关键作用\n- **置信度分析**：验证策略不仅提高准确性，还改善弱模型的置信度校准\n\n## 8. 系统完善与扩展\n\n最后，作者完善了系统并考虑了实际应用和扩展性：\n\n- **鲁棒性设计**：通过置信度阈值确保系统在策略库包含错误策略时仍能保持鲁棒性\n- **多模型扩展**：将方法扩展到N-LLM级联系统，提高通用性\n- **实际应用考虑**：讨论策略库维护、定期修剪和离线微调的可能性\n- **未来方向**：提出改进策略生成、相似性评估和防止不匹配的未来工作方向\n\n通过这一完整的逻辑链条，作者从宏观问题出发，经过观察、假设、概念形成、方法设计、理论验证和实验优化，最终提出了Inter-Cascade这一创新方法，实现了LLM Cascade从静态非适应系统到动态交互式学习系统的转变。"
                },
                {
                    "title": "Adaptive Margin RLHF via Preference over Preferences",
                    "arxiv_id": "2509.22851",
                    "authors": "Yaswanth Chittepu, Prasann Singhal, Greg Durrett, Scott Niekum",
                    "summary": "Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文的本质是改进大语言模型的基础训练方法，具体聚焦于强化学习人类反馈(RLHF)和直接偏好优化(DPO)技术。论文提出了一种名为DPO-PoP的新方法，通过利用\"偏好之上的偏好\"(preference over preferences)来推断自适应边界，从而提高模型的泛化能力和对齐质量。这明显属于改进LLM基础能力的范畴，符合研究目标。 从正面指标看，论文涉及核心概念LLMs（虽然未直接提及，但RLHF和DPO是LLM训练的关键技术）和训练方法中的强化学习(RLHF)，这些都是提高LLM通用推理能力的重要方向。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。论文关注的是通用训练方法的改进，而非特定领域应用。 虽然论文没有直接讨论推理、规划或问题解决能力，但通过改进RLHF和DPO这一基础训练范式，论文间接提升了模型的泛化能力和对齐质量，这有助于提高LLM的通用推理能力。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决RLHF中偏好强度差异建模问题。针对人类偏好数据，我们提出了一种基于偏好之上的偏好(PoP)的自适应边际DPO方法(DPO-PoP)，并在UltraFeedback数据集上通过分类准确率、生成胜率等指标验证了其有效性。该方法利用偏好间比较信息推断自适应边际，无需精确评分，提高了模型判别和生成性能。",
                    "summary_translation": "基于边界（margin）的优化是提高分类任务中泛化能力和鲁棒性的基础。在人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）中从偏好学习奖励模型的背景下，现有方法通常依赖于无边界、固定边界，或仅是偏好评分的简单函数的边界。然而，这些公式往往未能考虑不同偏好的强度差异，例如某些偏好与响应之间更大的边界相关联，或者它们依赖于从评分中得出的噪声边界信息。我们认为，对偏好强度进行建模可以带来更好的泛化能力和更真实的对齐。\n\n此外，许多使用自适应边界（adaptive margins）的现有方法假设能够获得准确的偏好分数，而人类很难可靠地提供这些分数。我们提出了一种利用\"偏好之上的偏好\"（preferences over preferences）的方法，即标注表明两个偏好中哪一个反映了更强的区分度。我们使用这种序数信号（ordinal signal）为每个数据点推断自适应边界。我们引入了直接偏好优化（Direct Preference Optimization, DPO）的扩展版本DPO-PoP，它整合了来自偏好之上偏好监督的自适应边界，从而提高了判别性和生成性性能。\n\n在UltraFeedback数据集上的实证结果表明，我们的方法优于原始DPO、使用固定边界的DPO以及使用真实边界的DPO。此外，我们展示了判别性（discriminative）和生成性（generative）性能之间存在权衡：提高测试分类准确度，特别是通过牺牲强偏好的正确标注来正确标注弱偏好，可能导致生成质量的下降。为了应对这种权衡，我们提出了两种收集偏好之上偏好标签的采样策略：一种有利于判别性性能，另一种有利于生成性性能。",
                    "inspiration_trace": "# 从观察到方法：DPO-PoP的逻辑演进分析\n\n## 宏观问题：如何提升RLHF的泛化性与鲁棒性\n\n作者从机器学习中的基本原理出发：边界（margin）在分类任务中是提高泛化和鲁棒性的关键因素。在RLHF领域，这一原理同样适用——如何有效利用边界信息来提升奖励模型的学习效果，成为作者关注的宏观问题。\n\n## 观察一：现有RLHF边界处理的局限性\n\n作者系统分析了现有RLHF方法的边界处理策略，发现三类主要方法：\n1. **无边界方法**：将所有偏好视为同等重要，忽略了偏好强度的差异\n2. **固定边界方法**：为所有偏好分配相同的边界值，无法适应不同偏好的特性\n3. **简单函数边界**：基于偏好评分的简单函数作为边界，但这些评分往往存在噪声\n\n这些方法的共同缺陷是无法准确捕捉不同偏好的相对强度差异，导致模型泛化能力受限。\n\n## 观察二：准确偏好评分的获取困境\n\n作者进一步发现，即使采用自适应边界，现有方法通常假设能获得准确的偏好评分，但这在实践中存在严重问题：\n- 人类标注者在使用Likert量表等评分方案时，即使对偏好顺序达成一致，也常因量表解释不同而给出不一致分数\n- 这种不一致性直接影响了从评分推导出的边界质量\n- 研究表明，比较性标注（如Best-to-Worst scaling）比评分尺度标注产生更可靠结果\n\n## 核心假设：偏好之上的偏好能更好建模偏好强度\n\n基于以上观察，作者提出创新性假设：利用\"偏好之上的偏好\"(Preference over Preferences)可以更有效地建模偏好强度。这一假设基于以下推理：\n1. 比较性标注（判断哪个偏好更强）比为单个响应分配精确数值分数更容易且更可靠\n2. 偏好之上的偏好允许推断连续实值边界，而非离散评分选项\n3. 这种方法能避免评分校准问题，同时保留偏好强度的相对信息\n\n## 方法设计：DPO-PoP的构建\n\n基于核心假设，作者设计了DPO-PoP方法，其逻辑演进如下：\n\n### 1. 数据收集创新\n- 传统方法：收集单个响应的评分或简单偏好对\n- 创新方法：收集\"偏好之上的偏好\"数据，即标注者判断两个偏好中哪一个反映更强的区分度\n\n### 2. 边界推断机制\n- 核心洞见：若(A ≻ B) ≻ (C ≻ D)，则r(A)-r(B) > r(C)-r(D)\n- 边界设定：将较弱偏好的奖励差异作为较强偏好的边界下限\n- 数学表达：通过stop-gradient操作确保边界在优化中保持固定\n\n### 3. 优化稳定性处理\n- 问题发现：原始公式因无边界值导致梯度不稳定\n- 解决方案：\n  a. 边界裁剪：将边界值限制在[0, Mmax]区间内\n  b. 目标策略：使用缓慢更新的目标策略计算边界，防止快速波动\n\n### 4. 最终目标函数\n```\nmin_θ E_DPoP [-log σ(β(log πθ(y+^s|x_s)/πref(y+^s|x_s) - log πθ(y-^s|x_s)/πref(y-^s|x_s)) - sg[clip[0,Mmax](β(log π̂θ(y+^w|x_w)/πref(y+^w|x_w) - log π̂θ(y-^w|x_w)/πref(y-^w|x_w)))])]\n```\n\n## 实验验证与发现\n\n作者通过实验验证了方法的有效性，并发现重要现象：\n\n### 1. 判别与生成性能的权衡\n- DPO-PoP-iter：在弱偏好上表现更好，提高了测试分类准确率\n- DPO-PoP-random：在强偏好上表现更好，提高了生成质量\n- 关键发现：过度关注弱偏好会损害生成性能，反之亦然\n\n### 2. 采样策略的差异化设计\n基于上述发现，作者提出两种采样策略：\n- **迭代采样**：每个偏好与k个较弱偏好比较，有利于判别性能\n- **随机采样**：随机选择偏好对比较，有利于生成性能\n\n## 总结：从问题到解决方案的完整逻辑链\n\n作者的思考过程展现了清晰的逻辑演进：\n1. **问题识别**：RLHF中边界处理不足导致泛化性受限\n2. **观察分析**：现有方法存在边界处理局限和评分获取困难\n3. **假设形成**：偏好之上的偏好能更好建模偏好强度\n4. **方法设计**：构建DPO-PoP框架，整合PoP监督到DPO中\n5. **优化改进**：解决训练稳定性问题\n6. **实验验证**：证实方法有效性并发现性能权衡\n7. **实用优化**：提出差异化采样策略适应不同需求\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过系统观察形成创新假设，进而设计出有效解决方案的思考过程，体现了严谨的学术思维和方法论创新。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 26,
            "papers": [
                {
                    "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
                    "arxiv_id": "2509.25020",
                    "authors": "Jiayu Liu, Zhenya Huang, Anya Sims, Enhong Chen, Yee Whye Teh, Ning Miao",
                    "summary": "The current paradigm for reasoning in large language models (LLMs) involves models \"thinking out loud\" via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to \"think while speaking,\" which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional \"thoughts\". Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出MARCOS，一种新的推理范式来改进大语言模型的基础推理能力。论文针对当前思维链(CoT)推理方法的局限性（速度慢、计算成本高、推理受限于离散token空间），提出将推理建模为连续、高维\"思想\"的隐马尔可夫链。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题。核心概念方面，明确针对大语言模型(LLMs)；能力方向方面，直接针对reasoning能力，并在数学推理基准GSM8K上进行了验证；训练方法方面，提出了两阶段变分训练方案，并提到其方法为强化学习提供了新机会。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的是一种通用的推理改进方法，不是针对特定领域的应用，因此不需要特殊处理。 核心贡献在于提出了一种全新的推理范式，将LLM的推理过程从离散token生成转变为连续高维思想空间的马尔可夫链，显著提升了推理效率和质量。这直接服务于提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在 [解决当前大型语言模型中思维链推理存在的速度慢、计算成本高、信息瓶颈和推理短视等问题]。针对 [数学推理任务]，我们提出了一种 [将推理建模为连续高维思想的隐马尔可夫链的新范式MARCOS]，并在 [GSM8K、SVAMP和MultiArith三个基准测试] 上通过 [准确率和推理速度] 验证了其有效性。",
                    "summary_translation": "当前大型语言模型（large language models, LLMs）中的推理范式涉及模型通过一系列令牌（token）\"大声思考\"，这种方法被称为思维链（chain-of-thought, CoT）。尽管这种方法有效，但它存在几个显著的缺点。首先，推理需要自回归（autoregressive）地生成通常数千个CoT令牌，这个过程缓慢且计算成本高昂。其次，它将推理限制在令牌的离散空间中，在推理步骤之间造成信息瓶颈。第三，它从根本上将推理与令牌生成纠缠在一起，迫使LLMs\"边说边想\"，这可能导致短视的推理。\n\n鉴于这些局限性，我们重新构想LLMs中的推理方式，并提出一个新范式：MARCOS。在我们的方法中，我们不是自回归地生成令牌，而是将推理建模为一个连续的、高维度的\"思想\"的隐马尔可夫链（hidden Markov chain）。每个推理步骤涉及内部思想的转换，其中显式推理步骤（可能由数百个令牌组成）作为可观察变量，这些变量是窥视隐含思想的窗口。由于这种潜在过程与标准监督学习不兼容，我们进一步提出了一种两阶段变分训练方案（two-phase variational training scheme）。\n\n我们在三个基准测试上的实验表明，MARCOS优于现有的连续推理方法，并且首次实现了与基于令牌的CoT相当的性能，甚至在GSM8K上超越了CoT 4.7%，同时推理速度提高了高达15.7倍。除此之外，MARCOS还提供了额外的优势，例如在随机性控制上实现了步骤级别而非令牌级别的控制，为LLMs中的强化学习（reinforcement learning）和推理开辟了重要机会。",
                    "inspiration_trace": "# MARCOS方法逻辑链分析\n\n## 1. 宏观问题：LLM推理的效率与质量困境\n\n作者从大语言模型(LLM)推理的核心挑战出发：传统思维链(CoT)方法虽然有效，但存在三个根本性局限：\n- **计算效率低**：需自回归生成大量CoT标记，推理缓慢且计算成本高\n- **信息瓶颈**：离散标记空间限制了推理步骤间的信息传递带宽\n- **思考-说话耦合**：模型被迫\"边思考边说话\"，导致短视推理\n\n## 2. 现有方法观察与不足分析\n\n作者系统考察了现有连续推理方法，发现两类方法均存在局限：\n\n**时间轴方法**（如Coconut、CoLaR）：\n- 本质上是标记压缩技术，未摆脱标记级推理限制\n- 采用静态转换函数，无法建模多样化解空间\n\n**深度轴方法**（如循环Transformer）：\n- 缺乏中间推理状态的监督\n- 仅是参数效率化的transformer深度增加方法\n\n关键洞察：现有方法未能从根本上解决CoT的局限性，需要全新范式。\n\n## 3. 关键假设：思考与说话的分离\n\n从神经科学发现（语言主要是交流工具而非思维工具）获得灵感，作者提出核心假设：\n- **思考与说话应解耦**：模型应能完全思考后再表达，而非同步进行\n- **连续空间推理**：在连续高维空间中进行推理可避免离散化造成的信息损失\n- **随机性建模**：推理过程存在多模态解空间，需显式建模随机性而非静态映射\n\n## 4. 方法论构建：MARCOS架构设计\n\n基于上述假设，作者构建了MARCOS（Markov Chain of Continuous Thoughts）框架，将推理建模为条件隐马尔可夫模型(cHMM)：\n\n### 4.1 三阶段架构设计\n\n**理解阶段**：\n- 将输入问题转换为特征向量序列\n- 为后续思考提供条件信息\n\n**思考阶段**（核心创新）：\n- 设计两组神经元：深度神经元（促进深层推理）和浅层神经元（易于表达）\n- 引入随机变量Rk控制推理随机性，实现多模态推理路径探索\n- 使用双向transformer作为思考器，迭代更新思维神经元\n\n**说话阶段**：\n- 将浅层神经元翻译为自然语言\n- 可选中间步骤，支持并行解码提高效率\n\n### 4.2 两阶段变分训练方案\n\n针对随机映射带来的训练挑战，作者提出创新训练策略：\n\n**第一阶段**：\n- 最小化重构损失和稀疏性损失\n- 固定随机性预测器g\n\n**第二阶段**：\n- 训练随机性预测器g最小化KL散度\n- 固定其他参数确保训练稳定\n\n这种设计受VAE的ELBO启发，有效处理了随机映射的训练难题。\n\n## 5. 实验验证与结果分析\n\n作者在三个数学推理基准（GSM8K、SVAMP、MultiArith）上验证MARCOS：\n\n**主要发现**：\n- 显著优于现有连续推理方法（GSM8K上提高8.66%）\n- 首次达到并超越基于标记的CoT性能（GSM8K上提高4.7%）\n- 实现高达15.7倍的推理加速\n\n**消融实验**：\n- 深度神经元、随机变量和稀疏性损失均为关键组件\n- 移除稀疏性损失导致模型崩溃，验证了其重要性\n\n**随机性分析**：\n- 随机变量Rk的不同维度控制不同推理特性（如输出长度、格式）\n- 实现了步骤级而非标记级的随机性控制\n\n## 6. 方法意义与未来方向\n\n作者总结了MARCOS的创新意义：\n- 首次实现思考与说话的明确分离\n- 提供了步骤级随机性控制的新范式\n- 为非自回归解码和强化学习提供了新方向\n\n未来工作包括大规模预训练、专用强化学习算法开发以及更高效解码策略探索。\n\n---\n\n## 逻辑链总结\n\n作者从LLM推理的效率与质量困境出发，通过系统分析现有方法的不足，结合神经科学洞察提出思考与说话分离的核心假设，进而构建了MARCOS这一全新推理范式。该方法通过条件隐马尔可夫模型建模推理过程，创新性地设计了三阶段架构和两阶段训练方案，最终在实验中验证了其优越性。这一逻辑链条展示了从问题识别、假设形成、方法设计到实验验证的完整研究思路，体现了严谨的学术思维和创新的方法论构建过程。"
                },
                {
                    "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards",
                    "arxiv_id": "2509.24981",
                    "authors": "Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan",
                    "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的强化学习训练范式ROVER（Random Policy Valuation for Diverse Reasoning），专门针对提升LLM的数学推理能力。论文的核心贡献是发现并证明了一个重要结果：最优动作可以从固定均匀随机策略的Q函数中恢复，从而简化了传统的强化学习训练过程，提高了推理质量和多样性。 其次，从正面指标来看，论文明确包含多个相关主题：核心概念是\"Large language models (LLMs)\"，能力方向聚焦于\"reasoning\"特别是\"math reasoning\"，训练方法涉及\"reinforcement learning (RL)\"，具体是\"RL with Verifiable Rewards (RLVR)\"范式。 从排除标准来看，论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。虽然论文聚焦于数学推理，但这被视为LLM通用推理能力的一个重要方面，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、简化的强化学习方法来提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决LLM推理中强化学习训练不稳定和多样性崩溃的问题。针对数学推理任务中的可验证奖励场景，我们提出了一种基于随机策略评估的ROVER方法，通过评估固定均匀随机策略的Q函数并从softmax采样动作，绕过了传统策略迭代循环。在多个数学推理基准上通过pass@1、pass@256和多样性指标验证了其有效性，实现了+8.2的质量提升和+17.6%的多样性改善。",
                    "summary_translation": "可验证奖励强化学习（RL with Verifiable Rewards, RLVR）已成为提升大型语言模型（large language models, LLMs）推理能力的一种有前景的范式。当前方法主要依赖于PPO和GRPO等策略优化框架（policy optimization frameworks），这些框架遵循广义策略迭代（generalized policy iteration），即在评估当前策略价值和基于评估改进策略之间交替进行。尽管这些方法有效，但它们常常面临训练不稳定性（training instability）和多样性崩溃（diversity collapse）的问题，需要复杂的启发式技巧（heuristic tricks）和精细的调整。\n\n我们观察到，数学推理中的标准RLVR可以被形式化为一种专门的有限时域马尔可夫决策过程（finite-horizon Markov Decision Process），具有确定性状态转移（deterministic state transitions）、树结构动态（tree-structured dynamics）和二元终端奖励（binary terminal rewards）。尽管规模庞大，但其底层结构比流行强化学习算法（RL algorithms）（如PPO）所针对的通用控制设置（general-purpose control settings）更为简单，这表明现有方法中的几种复杂技术可能被简化甚至省略。\n\n基于这一见解，我们证明了一个令人惊讶的结果：最优动作（optimal action）可以从固定均匀随机策略（uniformly random policy）的Q函数（Q-function）中恢复，从而绕过广义策略迭代循环（generalized policy iteration loop）及其相关的启发式方法。我们提出了多样推理的随机策略评估（Random Policy Valuation for Diverse Reasoning, ROVER），将这一原则转化为用于LLM数学推理的实用且可扩展的算法（scalable algorithm）。这是一种极简主义（minimalist）但高度有效的强化学习方法，它通过对这些均匀策略Q值（uniform-policy Q-values）应用softmax函数（softmax）来采样动作。ROVER在整个训练过程中保持了多样性，允许持续探索多条有效路径。\n\n在多个基础模型（base models）和标准数学推理基准（math reasoning benchmarks）上，尽管与强大而复杂的现有方法相比进行了根本性的简化，ROVER在质量（quality）（pass@1提高\\textbf{+8.2}，pass@256提高\\textbf{+16.8}）和多样性（diversity）（提高\\textbf{+17.6\\%}）两方面均表现出优越的性能。",
                    "inspiration_trace": "# 从复杂到简约：ROVER方法的逻辑演进\n\n## 宏观问题：强化学习在LLM推理中的困境\n\n当前，强化学习与可验证奖励(RLVR)已成为提升大语言模型(LLM)推理能力的主流范式。然而，现有方法主要依赖PPO和GRPO等策略优化框架，这些方法遵循广义策略迭代(GPI)循环，交替评估当前策略价值和改进策略。尽管有效，但它们普遍面临两大核心问题：\n\n1. **训练不稳定性**：策略持续演化导致评估目标非平稳，引发训练波动\n2. **多样性崩溃**：奖励最大化导向的迭代优化导致探索空间缩小，熵崩溃\n\n这些问题迫使研究者引入各种复杂启发式技巧（如裁剪、KL正则化、数据选择等），增加了实现复杂度和调优难度。\n\n## 关键观察：LLM推理任务的特殊结构\n\n作者敏锐地观察到，标准LLM数学推理任务可形式化为一种特殊的有限水平马尔可夫决策过程(MDP)，具有三个关键特性：\n\n1. **确定性状态转移**：每个动作导致确定性的新分支\n2. **树状结构动态**：部分序列只有一个父状态，形成树状而非图状结构\n3. **二元终端奖励**：只在终端状态给出正确/错误的二元反馈\n\n这与PPO等通用强化学习算法设计的复杂控制环境（如电子游戏、机器人控制）形成鲜明对比，后者通常具有随机转移、复杂奖励结构和图状状态空间（可能有循环）。这一观察引出了核心问题：\n\n> 我们是否正在对结构上更简单（尽管规模更大）的问题应用了不必要的复杂工具？\n\n## 理论假设：随机策略价值的惊人潜力\n\n基于上述观察，作者提出一个反直觉的假设：在这种特殊MDP结构下，最优操作可能通过简单地评估固定均匀随机策略并基于其Q值贪婪选择动作来获得。\n\n这一假设挑战了强化学习领域的传统认知——均匀策略通常被认为过于简单，无法为控制提供有意义的指导。然而，作者推测，在树状结构、确定性转移和二元终端奖励的MDP中，均匀策略的Q值可能包含足够的信息来指导最优决策。\n\n## 理论证明：简化最优控制的数学基础\n\n作者通过严格的理论分析证明了一个令人惊讶的结果：\n\n**定理**：在具有确定性转移、树状状态空间和二元终端奖励的有限水平MDP中，相对于均匀随机策略πu的Q函数的贪婪策略πgreedy(s) = argmax_a Q^πu(s,a)是最优的。\n\n这一发现具有深远意义：\n1. **简化了最优控制问题**：尽管LLM数学推理任务表面复杂，但其底层决策结构比通常认为的更简单\n2. **绕过了GPI循环**：只需评估固定均匀随机策略，无需迭代评估更新的策略\n3. **消除了启发式技巧**：不需要当前方法中许多复杂的启发式技巧\n\n## 方法设计：从理论到实践的ROVER\n\n基于理论证明，作者设计了ROVER（Random Policy Valuation for Diverse Reasoning）方法，将理论原则转化为实用且可扩展的算法。ROVER的核心设计包括：\n\n### 1. Q函数参数化\n直接利用LLM的内在参数表示Q函数，通过相对Q函数公式：\n```\nQ(s_t, a_t) = ρ[log π_θ(a_t|s_t) - log π_θ^old(a_t|s_t)]\n```\n其中π_θ^old是采样数据的行为策略，作为稳定锚点，减少波动。\n\n### 2. 低方差奖励\n采用组奖励中心化技术，对每个提示采样n个响应，减去经验平均奖励：\n```\n˜r(x, y^i) = r(x, y^i) - (1/n)∑r(x, y^i)\n```\n并将此中心化奖励广播到生成中的每个标记，提高信用分配效率。\n\n### 3. 多样性保持\n虽然理论证明贪婪选择已足够最优，但作者认识到多样性对推理任务的关键性。因此，ROVER使用softmax对均匀策略Q值进行采样：\n```\nπ_s(a|s) = exp(Q^πu(s,a)/ρ) / ∑_{a'} exp(Q^πu(s,a')/ρ)\n```\n其中ρ是温度参数，平衡探索与利用。作者进一步证明这种方法保持了相对于最优策略的性能保证。\n\n## 实验验证：简约方法的卓越表现\n\n作者在多个数学推理基准上验证了ROVER的有效性，包括Countdown任务（多个有效答案）和数学竞赛任务（单一明确答案）。实验结果表明：\n\n1. **质量提升**：在AIME24、AIME25和HMMT25任务上，ROVER的pass@1比最强基线提高+8.2\n2. **多样性增强**：pass@256提高+16.8，多样性指标提高+17.6%\n3. **泛化能力**：在OOD任务GPQA-diamond上也表现出色\n\n值得注意的是，ROVER发现了基础模型和标准RL方法（如GRPO）训练的模型所缺乏的新推理策略，证明其推动推理边界的潜力。\n\n## 逻辑演进总结\n\n从宏观问题到最终方法论的完整逻辑链可概括为：\n\n1. **问题识别**：当前LLM推理的强化学习方法存在训练不稳定和多样性崩溃问题\n2. **结构观察**：LLM数学推理任务具有特殊的MDP结构，比通用控制环境更简单\n3. **理论假设**：在这种特殊结构下，均匀随机策略的Q值可能包含足够信息指导最优决策\n4. **理论证明**：证明了在树状MDP中，均匀随机策略的Q函数的贪婪策略是最优的\n5. **方法设计**：基于理论证明，设计了ROVER方法，通过softmax采样均匀策略Q值平衡质量和多样性\n6. **实验验证**：在多个基准上验证了ROVER的优越性能，证实了理论假设的有效性\n\n这一逻辑演进展示了作者如何从实际问题出发，通过观察、假设、理论分析和实验验证，最终提出一个简单而有效的解决方案，完美体现了\"简约是终极的精致\"这一理念。"
                },
                {
                    "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
                    "arxiv_id": "2509.24713",
                    "authors": "Jing Liu",
                    "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment. We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. Our theoretical framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies. This approach provides both theoretical insights into reward model failures and practical interventions for improving longtail robustness.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是关于改进RLHF（Reinforcement Learning from Human Feedback）中的奖励模型，特别是提高其在长尾分布上的鲁棒性。RLHF是提升大语言模型对齐能力和通用推理能力的关键训练范式，属于LLM基础能力的改进范畴。论文提出了Circuit-Aware Reward Training (CART)方法，通过机制可解释性框架识别处理罕见事件的专门神经回路，从而改进训练策略。这种方法论研究有助于提升LLM的通用推理能力和鲁棒性，而不是将LLM作为工具应用于特定领域。论文虽然没有直接讨论数学或逻辑推理，但通过改进RLHF这一核心训练方法，可以间接提升LLM的通用推理能力。论文不符合任何排除标准，且在训练方法方面有很强的正面指标，因此符合研究目标。",
                    "summary2": "本文旨在解决RLHF中奖励模型在长尾分布上的系统性失败问题。针对罕见偏好场景，我们提出了一种Circuit-Aware Reward Training (CART)框架，通过识别和处理专门处理长尾事件的神经回路来提高模型鲁棒性，并通过理论分析和回路评估指标验证了其有效性。",
                    "summary_translation": "强化学习从人类反馈（Reinforcement Learning from Human Feedback, RLHF）的奖励模型在长尾分布（longtail distributions）上表现出系统性故障，导致奖励黑客（reward hacking）和不对齐（misalignment）。我们提出了一个机制可解释性框架（mechanistic interpretability framework），用于识别奖励模型中负责稀有事件处理（rare-event processing）的专门神经电路（neural circuits）。借鉴最近显示语言模型中稀有标记（rare tokens）的分布式专业化（distributed specialization）的研究进展\\citep{liu2025no, liu2025emergent}，我们假设奖励模型也为长尾场景（longtail scenarios）发展出功能上不同的电路（functionally distinct circuits）。我们的理论框架在电路专业化（circuit specialization）、奖励泛化边界（reward generalization bounds）和长尾性能（longtail performance）之间建立了正式联系。我们引入了**电路感知奖励训练（Circuit-Aware Reward Training, CART）**，它使用电路分析（circuit analysis）来指导数据增强（data augmentation）、正则化（regularization）和集成策略（ensemble strategies）。这种方法既提供了对奖励模型故障的理论见解（theoretical insights），也提供了改善长尾鲁棒性（longtail robustness）的实际干预措施（practical interventions）。",
                    "inspiration_trace": "# 从宏观问题到CART方法：逻辑推演链\n\n## 1. 宏观问题：RLHF中的长尾失败\n\n作者从强化学习中基于人类反馈的强化学习(RLHF)这一核心技术出发，识别出一个关键问题：**奖励模型在长尾分布上存在系统性失败**。这种失败导致奖励黑客(reward hacking)和模型对齐失败(misalignment)，使得语言模型在处理罕见或代表性不足的输入时产生不可预测的行为。\n\n## 2. 观察与发现：现有方法的局限性\n\n作者系统分析了现有解决方案的不足：\n\n- **黑盒处理**：当前方法(如KL正则化、奖励裁剪、集成方法)将奖励黑客视为黑盒优化问题，仅处理症状而不理解根本机制\n- **粗糙干预**：基于正则化的方法操作粗糙，未明确考虑罕见事件输入的内部表示\n- **缺乏洞察**：悲观奖励建模、对比和集成方法不揭示模型内部哪些组件驱动行为\n- **启发式依赖**：奖励塑形方法依赖启发式，需要大量领域知识，限制了可扩展性\n\n同时，作者注意到语言模型研究中的关键发现：**语言模型通过分布式专业化处理罕见标记**，即空间分布的子网络专门为罕见输入激活。\n\n## 3. 核心假设：电路专业化理论\n\n基于以上观察，作者提出核心假设：**奖励模型也发展出功能上不同的神经电路来处理长尾场景**。具体而言：\n\n- 奖励模型为不同类型的偏好场景发展专门的神经电路\n- 处理罕见偏好场景的电路(长尾电路)接收有限的训练信号，容易产生过度自信和不一致的预测\n- 这种脆弱性被策略利用，导致奖励黑客\n\n这一假设将奖励黑客重新定义为**机制性失败**，而非仅仅是优化问题。\n\n## 4. 理论框架：从电路到泛化边界\n\n作者构建了形式化理论框架，连接电路专业化与长尾性能：\n\n### 4.1 电路分解\n将奖励模型计算概念上分解为电路激活：\n```\nRθ(x) = Σ(wi · ai(x)) + ε(x)\n```\n其中ai(x)表示电路Ci的激活，wi是其输出权重，ε(x)是残差计算。\n\n### 4.2 长尾专业化定义\n定义电路Ci为τ-specialized for the tail：\n```\nEx∼ptail [|ai(x)|] - Ex∼phead [|ai(x)|] > τ\n```\n这捕捉了某些电路对罕见输入有优先响应的现象。\n\n### 4.3 泛化误差边界\n推导出长尾泛化误差边界：\n```\nLtail(θ) ≤ Lhead(θ) + Cp|Ctail|log(1/δ)/√Ntail + β·Div(Ctail, Chead)\n```\n\n这一边界揭示了两个关键见解：\n- 长尾误差随专业化电路数量增加而增加\n- 长尾和头部电路行为间的更大差异加剧泛化失败\n\n## 5. 方法论设计：CART框架\n\n基于理论框架，作者提出Circuit-Aware Reward Training (CART)，包含三个阶段：\n\n### 5.1 电路发现\n识别专门处理长尾偏好的电路：\n- **激活模式分析**：计算常见和罕见场景间的差异激活模式\n- **电路一致性分析**：使用互信息识别为长尾输入一起激活的神经元组\n- **因果验证**：通过激活修补验证电路功能\n\n### 5.2 脆弱性评估\n评估长尾电路被利用的脆弱性：\n- **预测一致性**：测量电路对语义相似输入产生一致输出的能力\n- **对抗敏感性**：评估电路激活小扰动对奖励的影响\n- **覆盖分析**：评估长尾分布的多少部分激活每个电路\n- **复合脆弱性分数**：整合上述指标为单一脆弱性分数\n\n### 5.3 针对性干预\n基于脆弱性评估应用精确干预：\n- **电路引导的数据增强**：生成专门激活脆弱电路的示例\n- **电路正则化**：引入方差惩罚项稳定脆弱电路\n- **渐进式电路强化**：通过课程学习逐渐强调长尾场景\n- **电路感知集成**：组合具有互补专业化的多个模型\n\n## 6. 完整逻辑链总结\n\n从宏观问题到CART方法的完整逻辑链条：\n\n**宏观问题** → **观察发现** → **核心假设** → **理论框架** → **方法论设计** → **CART方法**\n\n1. 从RLHF中的长尾失败问题出发\n2. 观察到现有方法将奖励黑客视为黑盒问题，同时注意到语言模型中的分布式专业化现象\n3. 提出奖励模型也发展专门电路处理长尾场景的核心假设\n4. 建立形式化理论框架，连接电路专业化与泛化边界\n5. 设计系统性方法论，包括电路发现、脆弱性评估和针对性干预\n6. 最终形成CART框架，整合所有组件到综合训练目标中\n\n这一逻辑链条体现了作者如何从问题识别出发，通过跨领域观察形成创新假设，建立理论支撑，最终设计出既有理论深度又有实用价值的方法论，为解决RLHF中的长尾鲁棒性问题提供了新思路。"
                },
                {
                    "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory",
                    "arxiv_id": "2509.24653",
                    "authors": "Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu",
                    "summary": "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的推理能力，特别是解决组合推理问题（如\"两跳推理的诅咒\"），这直接针对LLM的基础能力提升。论文提出的\"Identity Bridge\"机制通过在零跳身份任务上监督模型，成功使模型能够执行分布外的两跳推理，这明显属于增强LLM逻辑推理能力的创新方法。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(reasoning)，特别是组合推理和两跳推理等逻辑推理形式。虽然论文未涉及强化学习或智能体等新兴范式，但其核心贡献已足够符合研究目标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型基础设施或部署优化。 最后，论文没有涉及需要特殊判断的模糊情况，它纯粹聚焦于通过改进模型内部机制（潜在几何结构的重塑）来提升通用推理能力，这正是研究目标所寻求的论文类型。论文的理论分析和实验验证都围绕如何增强LLM的隐式推理能力展开，完全符合\"提高大语言模型本身的通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决大型语言模型在组合推理任务中的\"两跳推理诅咒\"问题。针对合成和真实数据集，我们提出了Identity Bridge机制，通过在桥接实体上添加零跳身份任务来重塑模型潜在空间，并在不同复杂度的数据集上通过OOD两跳推理准确率验证了其有效性。理论分析表明该方法通过隐式核范数正则化促进跨任务内存共享，而小初始化或权重衰减可进一步增强高复杂度任务中的表示对齐和泛化能力。",
                    "summary_translation": "尽管取得了显著进展，大型语言模型（large language models）在组合推理（compositional reasoning）任务上经常失败，这一现象以\"两跳推理诅咒\"（curse of two-hop reasoning）为例证。本文介绍了身份桥接（Identity Bridge），一种简单而强大的机制，通过在零跳身份任务（zero-hop identity task）上监督模型来解决这种组合性差距（compositionality gap）。我们通过实证证明，这种添加使模型能够成功执行分布外两跳推理（out-of-distribution two-hop reasoning），而这是它们原本完全无法完成的任务。为解释这一现象，我们使用简化的嵌入多层感知机（Emb-MLP）模型进行了理论分析，证明身份监督（identity supervision）重塑了模型的潜在几何结构（latent geometry）。我们表明，这种对齐是由优化过程中的隐式核范数正则化（implicit nuclear-norm regularization）引起的，它倾向于选择跨任务共享结构的低秩解（low-rank solutions）。对于复杂任务，我们使用小初始化（small initialization）或权重衰减（weight decay）来增强正则化效果，这增强了潜在空间对齐效应并减缓了泛化衰减（generalization decay）。最后，我们将研究扩展到大规模模型，观察到它们仍然通过潜在记忆（latent memory）实现两跳推理，这为增强其隐式推理能力（implicit reasoning abilities）提供了重要启示。",
                    "inspiration_trace": "# Identity Bridge方法逻辑推演\n\n## 1. 宏观问题：大型语言模型的组合推理困境\n\n大型语言模型(LLMs)在众多任务上取得了显著进展，但在组合推理任务上却存在根本性缺陷。具体表现为\"两跳推理诅咒\"：模型能够学习两个单独的单跳事实(\"A到B\"和\"B到C\")，却无法将它们组合成正确的结论(\"A到C\")，除非在训练数据中明确提供了这种组合或使用了显式的思维链推理。这一现象揭示了当前训练数据和方法的局限性，即使是最先进的系统也无法有效解决。\n\n## 2. 关键观察：组合性差距的具体表现\n\n作者通过实验观察到几个关键现象：\n- 模型在单跳任务上表现良好，但在两跳推理任务上完全失败\n- 即使模型在分布内(ID)的两跳任务上表现良好，也无法泛化到分布外(OOD)的两跳任务\n- 单独训练单跳任务和部分两跳任务，无法帮助模型实现OOD两跳推理\n- 这种组合性差距不会随着模型规模的增加而减小\n\n这些观察表明，问题不在于模型容量不足，而在于模型无法在潜在空间中有效组合已学习的单跳信息。\n\n## 3. 核心假设：潜在空间对齐不足\n\n基于观察，作者提出核心假设：模型无法进行组合推理的根本原因在于潜在空间中对齐不足。具体来说：\n- 第一跳(Subject → Bridge)和第二跳(Bridge → Object)的表示在潜在空间中没有正确对齐\n- 当处理两跳查询时，第二跳无法可靠地\"锁定\"第一跳产生的特征\n- 如果能够通过某种方式对齐这两跳的表示，模型可能能够实现有效的组合推理\n\n## 4. 理论分析：隐式正则化的作用\n\n为验证假设，作者采用简化的Emb-MLP模型进行理论分析：\n- 将问题转化为带约束的优化问题，目标是最大化多类边际\n- 证明基于梯度的训练会诱导隐式核范数正则化，倾向于低秩、跨任务共享结构的解决方案\n- 关键发现：在有身份监督的情况下，这种正则化促进了跨任务内存共享，并将第一跳Subject→Bridge表示与第二跳Bridge→Object映射对齐\n- 通过严格数学证明，身份监督确实能够在OOD组合上产生正边际\n\n## 5. 方法提出：Identity Bridge机制\n\n基于理论分析，作者提出了Identity Bridge方法：\n- 核心思想：通过在桥接实体上添加零跳身份任务，即(e2) → e2，鼓励模型实现身份变换f(e2) = e2\n- 这种监督虽然本身很简单，但能够重塑潜在空间的几何结构\n- 使第二跳能够可靠地利用第一跳产生的特征，从而实现组合推理\n- 对于复杂任务，使用小初始化或权重衰减可以增强正则化效果，进一步提高潜在空间对齐\n\n## 6. 实验验证：从简单到复杂\n\n作者通过一系列精心设计的实验验证了Identity Bridge的有效性：\n- 在简单数据集上，Identity Bridge使模型能够成功执行OOD两跳推理\n- 随着复杂度增加，标准模型的性能下降，表明梯度下降的隐式正则化本身不足以在高复杂度下实现强OOD组合\n- 使用小初始化的模型性能下降更平缓，表明更强的正则化效果进一步约束了潜在几何结构，更好地保留了组合所需的桥接-对象耦合\n- 隐藏状态分析显示，随着训练进行，单跳数据与目标桥接实体的表示逐渐对齐，测试准确度随之提高\n\n## 7. 扩展应用：大规模模型的隐式推理\n\n最后，作者将研究扩展到真实的大规模预训练模型：\n- 即使没有显式的两跳监督，微调信号显示模型在提示与桥接相关的线索时增加了正确答案的概率\n- 这表明预训练过程已经隐式地建立了某种形式的身份桥接\n- 这一发现为增强大型模型的隐式推理能力提供了重要启示：通过设计适当的训练目标和正则化策略，可以进一步强化这种隐式推理能力\n\n## 逻辑链条总结\n\n整个研究形成了一条清晰的逻辑链条：从大型语言模型的组合推理困境出发，通过观察发现潜在空间对齐不足是关键问题，提出假设并通过理论分析验证了隐式正则化的作用，进而提出Identity Bridge方法，通过实验验证其有效性，并最终扩展到大规模模型的应用。这一研究不仅解决了特定的技术问题，还为我们理解语言模型的推理机制提供了新的视角。"
                },
                {
                    "title": "Short window attention enables long-term memorization",
                    "arxiv_id": "2509.24552",
                    "authors": "Loïc Cabannes, Maximilian Beck, Gergely Szilvasy, Matthijs Douze, Maria Lomeli, Jade Copet, Pierre-Emmanuel Mazaré, Gabriel Synnaeve, Hervé Jégou",
                    "summary": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers. A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval. The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为SWAX的混合架构，结合了滑动窗口注意力和xLSTM线性RNN层，并通过随机改变滑动窗口大小来训练模型。论文的本质是改进LLM的基础架构和训练方法，特别是优化模型的长期记忆和上下文处理能力，这些能力是通用推理能力的基础组件。根据第一步的判断标准，这篇论文属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。在正面指标方面，论文涉及大语言模型的核心概念（注意力机制和RNN层），并提出了一种新的训练方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性等内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在 [解决混合架构中窗口长度选择和softmax注意力与线性RNN层之间相互作用的问题]。针对 [长上下文建模任务]，我们提出了一种 [结合滑动窗口注意力和xLSTM线性RNN层的混合架构SWAX，并采用随机窗口大小训练策略]，并在 [RULER benchmark和多个短上下文推理任务] 上通过 [准确率和困惑度] 验证了其有效性。",
                    "summary_translation": "近期研究表明，将滑动窗口softmax注意力层(sliding window softmax attention layers)与线性循环神经网络(RNN)层(linear recurrent neural network layers)相结合的混合架构，性能优于单独使用这两种架构中的任何一种。然而，窗口长度的影响以及softmax注意力与线性RNN层之间的相互作用仍未得到充分研究。在本研究中，我们提出了SWAX，一种由滑动窗口注意力(sliding-window attention)和xLSTM线性RNN层(xLSTM linear RNN layers)组成的混合架构。关于SWAX的一个反直觉发现是，更大的滑动窗口并不能提高长上下文性能。事实上，短窗口注意力通过减少对softmax注意力机制在长上下文检索中的依赖，鼓励模型更好地训练xLSTM的长期记忆能力。小滑动窗口的问题在于它们对短上下文任务有害，而这些问题原本可以通过适度更大的滑动窗口所提供的信息来解决。因此，我们通过随机改变滑动窗口大小来训练SWAX，迫使模型同时利用更长的上下文窗口和xLSTM记忆。使用随机窗口大小训练的SWAX在短上下文和长上下文问题上都显著优于常规窗口注意力。",
                    "inspiration_trace": "# 从宏观问题到核心方法：SWAX的逻辑演进\n\n## 宏观问题：如何高效处理长序列建模？\n\n在神经网络架构设计中，长序列处理一直面临根本性挑战：全局注意力机制（如Transformer）虽然精确但计算成本随序列长度线性增长；而线性RNN虽然计算恒定但精确度不足。这一矛盾促使研究者探索如何兼顾效率与性能。\n\n## 聚焦问题：混合架构如何优化？\n\n观察到近期研究表明滑动窗口注意力与线性RNN的混合架构优于单独使用任一组件，作者聚焦于一个关键但未充分研究的问题：**滑动窗口长度如何影响混合架构的性能，以及两种组件如何协同工作？**\n\n### 观察与初始假设\n\n**观察1**：现有研究（如De et al., 2024）主要关注验证困惑度，忽略了窗口大小对长上下文性能的影响。\n\n**假设1**：滑动窗口大小会影响模型组件的专门化程度——较大窗口可能导致模型过度依赖softmax注意力，而较小窗口可能迫使线性RNN更好地处理长期依赖。\n\n### 实验验证与反直觉发现\n\n**实验设计**：作者训练了不同窗口大小（128-2048）的SWAX模型（滑动窗口注意力+xLSTM），并在短上下文推理和长上下文检索任务上测试。\n\n**反直觉发现**：与直觉相反，**更大的滑动窗口实际上损害了长上下文性能**。在131k序列长度上，128窗口的SWAX模型准确率约30%，而2048窗口的模型准确率接近0%。\n\n### 机制解释：组件专门化与训练动态\n\n**关键洞察**：这种现象源于训练动态。当窗口较大时，大多数依赖关系落在窗口内，模型倾向于依赖更精确的softmax注意力而非线性RNN。然而，当测试超出窗口长度的序列时，模型无法泛化，因为它从未学会依赖线性RNN进行长上下文建模。\n\n相反，**短窗口迫使线性RNN层承担更多长期依赖建模的责任**，从而在长序列上表现更好。\n\n### 新问题与解决方案\n\n**问题2**：短窗口虽有利于长上下文性能，但在短上下文任务上表现不佳，因为许多提示无法适应小窗口。\n\n**假设2**：随机改变窗口大小可能同时解决两个问题——保持短窗口对长上下文的好处，同时获得大窗口对短上下文的优势。\n\n**实验验证**：作者在训练过程中随机选择窗口大小（128或2048，概率各0.5），并在最后10%训练中退火到固定窗口2048。\n\n**最终发现**：随机窗口训练的SWAX模型在短上下文任务上与固定2048窗口模型相当或更好，同时在长上下文任务上与固定128窗口模型相当或更好。\n\n## 核心方法论：SWAX与随机窗口训练\n\n基于以上逻辑演进，作者提出了最终方法论：\n\n1. **架构设计**：SWAX——交替堆叠滑动窗口注意力(SWA)和xLSTM线性RNN层的混合架构\n2. **训练策略**：随机窗口大小训练——在训练过程中随机切换窗口大小，迫使模型同时利用两种机制的优势\n\n这一方法论解决了传统架构在处理长序列时的效率与性能权衡，通过组件专门化和随机训练策略，实现了在短上下文和长上下文任务上的优异性能。"
                },
                {
                    "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models",
                    "arxiv_id": "2509.24510",
                    "authors": "Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur",
                    "summary": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出并解释了基础模型（包括大语言模型）中的测试时训练（TTT）机制。作者通过理论分析和实验验证，阐明了TTT如何通过\"先泛化后专业化\"的方式提高模型性能，使模型能够将容量集中在与测试任务相关的概念上。这本质上是一种改进LLM基础能力、增强其通用问题解决能力的研究。虽然论文在ImageNet上进行了视觉领域的实验，但它同时也涵盖了语言任务，并且其理论框架是通用的，不局限于特定领域。TTT可以被视为一种增强模型推理和问题解决能力的新范式，因为它允许模型在测试时根据特定任务进行自适应调整，这与提高LLM通用推理能力的研究目标高度一致。论文没有主要聚焦于需要排除的领域，如特定应用领域或模型可靠性的应用层面问题。因此，这篇论文符合研究范围，应该被保留。",
                    "summary2": "本文旨在解释测试时训练(TTT)在基础模型中有效的原理，即使在分布内数据上也能提升性能。针对大型基础模型的全局欠参数化特性，我们提出了一种\"泛化后专业化\"(specialization after generalization)机制，表明TTT通过重新分配模型容量来专注于与测试任务相关的概念。我们在ImageNet、MNIST和Pile数据集上通过分类准确率和bits per byte等指标验证了其有效性，并证明TTT在欠参数化模型中提升最大。",
                    "summary_translation": "最近的实证研究探索了在测试时继续针对特定任务训练模型的想法，即测试时训练(test-time training, TTT)，发现它能带来显著的性能提升。然而，对于TTT为何有效以及何时有效，人们的理解仍然有限。早期的解释主要集中在以下观察上：当TTT应用于分布外适应(out-of-distribution adaptation)或与特权数据(privileged data)一起使用时可能有所帮助。然而，基础模型(foundation models)规模的不断扩大，以及大多数测试数据是分布内(in-distribution)的事实，对这些解释提出了质疑。我们反而认为，基础模型仍然是全局欠参数化的(globally underparameterized)，而TTT提供了一种泛化后专业化(specialization after generalization)的机制，将模型能力集中在与测试任务相关的概念上。具体而言，在线性表示假设(linear representation hypothesis)下，我们提出了一个模型，其中TTT实现了比全局训练(global training)更小的分布内测试误差。我们通过在ImageNet上训练稀疏自编码器(sparse autoencoder)，实证验证了模型的关键假设，表明语义相关的数据点仅由少量共享概念解释。最后，我们在图像和语言任务上进行了扩展研究(scaling studies)，确认了我们模型的实际意义，并确定了专业化最有效的区域(regimes)。",
                    "inspiration_trace": "# 从宏观问题到方法论：测试时训练(TTT)的逻辑演进\n\n## 1. 宏观问题：TTT为何在分布内数据上依然有效？\n\n传统观点认为测试时训练(TTT)主要在分布外(OOD)适应或使用特权数据时有效。但随着基础模型规模扩大，大多数测试数据实际是分布内的(ID)，即模型在预训练中已见过类似数据。这引发核心问题：**即使测试数据是分布内的，且仅使用已见数据，TTT为何仍能提升预测性能？**\n\n## 2. 初步观察与假设\n\n### 关键观察：\n- 基础模型虽规模庞大，但仍是\"全局欠参数化\"的（模型规模扩大时性能持续提升）\n- 由于欠参数化，即使测试数据是分布内的，模型也无法同时在整个数据分布上近似真实情况\n\n### 核心假设：\nTTT提供了一种\"泛化后特化\"(specialization after generalization)机制——通过暂时\"遗忘\"不相关预训练知识，释放模型容量以更高分辨率学习与当前测试任务相关的概念。\n\n## 3. 理论框架构建：线性表示假设(LRH)\n\n为深入理解这一机制，作者引入线性表示假设：\n- 模型将高级概念表示为潜在空间中的方向\n- 概念空间通常是稀疏的（任何输入只激活少数概念）\n- 形式化：存在s-稀疏概念空间Φ: X → Rd¹，通过学习特征映射Ψ: X → Rd²近似（d² ≪ d¹）\n- 真实函数在概念空间中是线性的：f(x) = ⟨Φ(x), w*⟩\n\n**核心洞察**：现实世界概念数量远超模型维度，导致这些概念在模型激活中叠加(superimposed)。\n\n## 4. 关键观察与验证\n\n基于LRH框架，作者提出并通过实验验证了三个关键观察：\n\n### 观察O1：特征空间保留概念空间几何结构\n- **方法**：在ImageNet上训练稀疏自编码器(SAE)\n- **发现**：SAE映射到概念空间保留了局部几何结构，不同空间选择的邻域在概念空间中的余弦相似度分布几乎相同\n\n### 观察O2：局部邻域由少量概念支持\n- **方法**：在掩码概念向量上训练TTT分类器\n- **发现**：学习的掩码高度稀疏（平均激活约40个概念），远小于邻域中活跃概念总数（约180个）\n- **意义**：一小部分自适应选择的概念足以捕获局部区域相关信息\n\n### 观察O3：TTT隐式寻找稀疏解\n- **方法**：比较在密集重构和稀疏概念上训练的TTT模型\n- **发现**：两者准确性几乎相同，89%情况下预测一致，表明功能等效\n- **意义**：特征空间中的TTT隐式偏向于概念空间中的稀疏解\n\n## 5. 理论分析：何时与为何特化有效\n\n### 何时特化有效？\n通过图像分类和语言建模的缩放研究：\n- TTT在模型欠参数化时显著提升性能（测试损失未饱和阶段）\n- 随着模型规模增大，性能差距缩小（更大模型允许更好全局解缠概念）\n\n### 为何特化有效？\n理论分析表明：\n- 即使模型全局欠参数化（d² ~ log d¹），TTT仍能在测试时泛化\n- 概念在欠参数化特征空间中叠加时，全局训练无法解缠所有概念含义\n- TTT实现局部稀疏恢复，达到O(σ²s log(d₁/s)/k)的最优速率\n\n## 6. 最终方法论：泛化后特化框架\n\n基于以上分析，作者形成了完整的方法论：\n\n1. **基础模型全局欠参数化**：模型无法同时在整个数据分布上近似真实情况\n\n2. **概念叠加现象**：由于概念数量远超模型维度，概念在模型激活中叠加\n\n3. **TTT的特化机制**：通过暂时\"遗忘\"不相关知识，释放容量学习当前任务相关概念\n\n4. **局部稀疏恢复**：TTT在概念空间中执行局部稀疏恢复，有效\"重新加权\"相关概念\n\n5. **适用条件**：机制在欠参数化状态下最有效，随模型增大和概念解缠改善，效果减弱\n\n## 7. 实证验证与扩展\n\n作者通过多种实验验证了这一方法论：\n- MNIST、ImageNet和语言建模任务的缩放研究确认特化在欠参数化状态下最有效\n- 混合专家(MoE)模型作为TTT替代方案，增加专家数量可提高容量和准确性\n- 分析TTT与非参数方法区别，解释TTT在高维情况下更有效的原因\n\n这一完整逻辑链条从宏观问题出发，通过观察、假设、理论构建和实证验证，最终形成了\"泛化后特化\"的核心方法论，深入解释了TTT在基础模型中的工作机制。"
                },
                {
                    "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning",
                    "arxiv_id": "2509.24372",
                    "authors": "Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen",
                    "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的LLM微调方法——进化策略(ES)，并证明了它在多个方面优于现有的强化学习微调方法。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础能力和提出新的训练范式，而非将LLM作为工具应用到特定领域。论文专注于LLM的微调技术，这直接关系到提升模型的基础能力。 其次，论文符合多个正面指标：它明确关注Large language models (LLMs)这一核心概念；讨论的训练方法涉及evolution strategies(ES)，这与筛选标准中的\"evolution, self-evolve\"相关；虽然论文没有直接提及reasoning等能力，但改进微调方法通常会提升这些通用能力。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题，没有聚焦于特定应用领域，也没有主要关注模型可靠性层面的水印、安全性等问题。 最后，论文提出的是一种通用的微调方法，可以提升LLM在各种任务上的表现，包括推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标一致。论文表明ES方法在样本效率、长期奖励容忍度、对不同基础LLM的鲁棒性等方面优于RL方法，这些都是提升LLM通用能力的重要方面。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大型语言模型微调中强化学习方法的局限性问题。针对数十亿参数的LLM微调任务，我们提出了一种可扩展的进化策略(ES)方法，直接在完整参数空间进行搜索优化，并在Countdown推理任务和简洁性微调任务上通过准确率、样本效率、鲁棒性和稳定性等指标验证了其有效性。",
                    "summary_translation": "对预训练大型语言模型（large language models, LLMs）进行下游任务（down-stream tasks）微调是AI部署流程中的关键步骤。强化学习（reinforcement learning, RL）可以说是最突出的微调方法，促成了许多最先进（state-of-the-art）大型语言模型的诞生。相比之下，进化策略（evolution strategies, ES）虽然在参数量仅几百万的模型上曾表现出与强化学习相当的性能，但由于人们对其在更大模型上可扩展性（scalability）的悲观看法而被忽视。在这项工作中，我们报告了首次成功将进化策略（ES）扩展用于微调大型语言模型全部参数的尝试，展示了令人惊讶的事实：进化策略能够高效搜索数十亿参数，并在多个方面优于现有的强化学习微调方法，包括样本效率（sample efficiency）、对长期奖励（long-horizon rewards）的容忍度、对不同基础大型语言模型的鲁棒性（robustness）、较少的奖励破解（reward hacking）倾向，以及跨运行更稳定的性能。因此，它为解锁大型语言模型微调的新方向奠定了基础，超越了当前强化学习技术所能提供的范围。源代码提供于：https://github.com/VsonicV/es-fine-tuning-paper。",
                    "inspiration_trace": "# 从宏观问题到创新方法：ES在大规模LLM微调中的逻辑演进\n\n## 1. 宏观问题：如何有效微调大规模语言模型？\n\n微调预训练的大规模语言模型(LLMs)以适应下游任务和改进与用户偏好的一致性，已成为AI部署管道中的关键步骤。当前，强化学习(RL)是这种微调的主导选择，但这一领域仍存在重要挑战，需要探索更有效的方法。\n\n## 2. 观察一：现有RL微调方法的局限性\n\n作者首先系统分析了RL在LLM微调中的几个关键问题：\n\n- **样本效率低与高方差**：处理长时程奖励时（LLM微调中常见），RL方法样本效率低，梯度估计方差高。\n- **信用分配困难**：在token级别进行适当信用分配很困难，甚至可能没有帮助。\n- **基础模型敏感性**：RL技术对基础LLM选择敏感，导致不同模型上微调性能不一致。\n- **奖励hacking倾向**：RL固有地倾向于hack奖励函数，导致不良行为。\n- **运行不稳定性**：相同参数设置下，RL微调在多次运行中经常不稳定，显著增加微调成本。\n\n这些局限性表明，需要探索替代方法来解决RL在LLM微调中的固有缺陷。\n\n## 3. 观察二：进化策略(ES)的潜在优势\n\n作者观察到，进化策略(ES)作为一类基于群体的零阶优化算法，在传统控制和游戏问题中相比RL有几个独特优势：\n\n- 高度并行化能力\n- 很好地容忍长时程奖励\n- 广泛的探索能力\n- 更少的计算需求（无需反向传播）\n- 对设置参数的鲁棒性\n\n这些特性使ES成为理论上适合LLM微调的候选方法，但存在一个关键障碍。\n\n## 4. 观察三：ES在LLM时代的应用困境\n\n尽管ES有上述优势，但在LLM时代，ES受到的关注远少于RL。标准ES直接在原始参数空间中搜索和优化，过去实现的维度不超过几百万。普遍认为，对于数十亿参数的LLMs，在参数空间中探索比在动作空间中探索更加困难和样本效率低下。这一认知障碍导致ES在大规模LLM微调中未被充分考虑。\n\n## 5. 核心假设：ES可以扩展到大规模LLM微调\n\n基于以上观察，作者提出了一个挑战传统观念的核心假设：**ES可以扩展到数十亿参数的搜索空间，通过直接在LLM的全参数空间中搜索进行微调任务，并且可能比RL方法表现更好**。\n\n这一假设挑战了\"ES不适合高维优化问题\"的传统认知，开启了将ES应用于大规模LLM微调的可能性。\n\n## 6. 方法设计：实现ES的大规模扩展\n\n为验证假设，作者设计了一个基于算法简化的ES变体的内存高效实现，支持GPU内部和跨GPU并行化。关键创新包括：\n\n1. **随机种子噪声检索**：只存储随机种子而非完整噪声，大幅减少GPU内存使用。\n2. **并行评估**：通过为每个进程分配单独随机种子，实现扰动模型的完全并行评估。\n3. **层级原位扰动和恢复**：逐层原位扰动模型参数，减少峰值GPU内存使用。\n4. **奖励归一化**：使用z-score归一化奖励，使奖励规模在不同迭代和任务间保持一致。\n5. **贪心解码**：使用确定性解码确保性能差异仅来自参数空间探索。\n6. **参数更新分解**：以分解方式执行模型参数聚合更新，进一步减少内存需求。\n7. **学习率消化**：简化标准更新方程，降低计算复杂度。\n\n这些设计使ES能够首次在数十亿参数的LLM全参数空间中进行有效搜索。\n\n## 7. 实验验证：ES与RL的比较\n\n作者在标准推理基准任务(Countdown)和简洁性微调任务中验证了假设：\n\n- **性能优势**：ES在所有测试模型上都优于PPO和GRPO，平均比基线模型提高36.4%，而PPO和GRPO分别仅提高17.9%和21.4%。\n- **样本效率**：即使在数十亿参数空间中搜索，ES比RL更样本高效，通常只需不到20%的训练样本评估就能达到相同性能。\n- **小模型适用性**：ES能够改善较小基础模型(如Qwen2.5-0.5B)的性能，而RL在这些模型上几乎无效。\n- **奖励hacking抵抗**：ES没有表现出奖励hacking行为，而GRPO需要KL散度惩罚来避免此问题。\n- **运行稳定性**：ES在多次运行中表现一致，标准差远低于GRPO。\n\n这些结果强有力地支持了核心假设，表明ES在大规模LLM微调中确实优于RL方法。\n\n## 8. 理论解释：ES成功的关键机制\n\n为解释这些令人惊讶的结果，作者提出了几个关键机制：\n\n- **参数空间探索的稳定性**：在参数空间中采样噪声确保整个动作轨迹只依赖于单个采样，导致rollouts中方差显著降低，而RL中的动作空间探索在每个步骤注入噪声，导致高方差。\n- **解决方案分布优化**：ES内在优化解决方案分布，而RL优化单个解决方案，使ES更难hack奖励，且产生的解决方案更鲁棒。\n- **奖励景观平滑**：ES通过显式高斯卷积直接将噪声注入参数空间，有效平滑锯齿状奖励景观，而RL的噪声注入方式不一定保证参数空间中奖励景观的平滑性。\n\n这些机制解释了为什么ES在大规模LLM微调中能够克服传统认知中的限制，实现优异性能。\n\n## 9. 最终方法论：ES作为LLM微调的新范式\n\n基于以上逻辑演进，作者最终确立了ES作为LLM微调的新范式，超越了当前RL技术提供的范围。这一方法论不仅解决了RL在LLM微调中的多个关键问题，还在参数空间探索、仅结果微调以及大规模分布式后训练方面开辟了新的机会，为未来LLM微调研究提供了新方向。\n\n这一完整逻辑链从宏观问题出发，通过系统观察现有方法的局限性，识别替代方法的潜力，提出挑战传统观念的假设，设计创新方法进行验证，最终形成了一套全新的LLM微调方法论。"
                },
                {
                    "title": "The Impossibility of Inverse Permutation Learning in Transformer Models",
                    "arxiv_id": "2509.24125",
                    "authors": "Rohan Alur, Chris Hays, Manish Raghavan, Devavrat Shah",
                    "summary": "In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens\" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断——这篇论文的本质是研究decoder-only transformers在逆置换学习任务上的表达能力限制。论文探讨了大语言模型架构与推理能力之间的关系，并提出添加\"scratch tokens\"可能是一种增强模型推理能力的方法。这本质上是对LLM基础能力的研究，特别是关于其推理能力限制的探索，而不是将LLM作为工具应用到特定领域。论文还明确探讨了这一发现对思维链(CoT)提示如何启用推理的启示，这与提高LLM通用推理能力直接相关。 第二步：正面指标——论文包含多个相关主题： - 核心概念：论文研究的是decoder-only transformers，这是大语言模型的主流架构 - 能力方向：论文明确提到逆置换学习任务模拟了各种推理任务的自然鲁棒性属性，包括长上下文检索、多选QA和上下文学习 - 论文还探讨了思维链(CoT)提示如何启用推理，这是大语言模型推理的重要范式 第三步：排除标准——论文不涉及任何需要排除的领域： - 不涉及多模态与视觉领域 - 不聚焦于任何特定应用领域 - 不涉及模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合来看，这篇论文的核心贡献是研究大语言模型在推理任务上的表达能力限制，探讨模型架构与推理能力之间的关系，并提出可能增强模型推理能力的方法。这与\"提高大语言模型的通用推理能力\"的研究目标高度相关，因此应该被保留。",
                    "summary2": "本文旨在研究decoder-only transformers中逆排列学习的不可能性问题。针对排列矩阵和被排列序列作为输入的场景，我们提出了一种通过去除causal attention mask或添加\"scratch tokens\"使逆排列学习成为可能的方法，并在简化的transformer模型上通过均方误差(MSE)指标验证了其有效性。",
                    "summary_translation": "在本技术说明中，我们研究了仅解码器transformer（decoder-only transformers）中的逆排列学习（inverse permutation learning）问题。给定一个排列（permutation）以及应用了该排列的字符串，模型的任务是生成原始的（\"规范的\"（canonical））字符串。我们认为，该任务模拟了多种推理任务中的一种自然鲁棒性特性（robustness property），包括长上下文检索（long-context retrieval）、多项选择问答（multiple choice QA）和上下文学习（in-context learning）。\n\n我们的主要贡献是一个不可能性结果（impossibility result）：我们证明了任意深度（arbitrary depth）的仅解码器transformer（decoder-only transformer）无法学习此任务。这一结果涉及仅解码器transformer模型的表达能力（expressive capacity），与训练动态（training dynamics）或样本复杂度（sample complexity）无关。我们提供了两种替代构造（alternative constructions），在这些构造下逆排列学习（inverse permutation learning）是可行的。\n\n其中第一种构造强调了因果注意力掩码（causal attention mask）的基本作用，并揭示了编码器-解码器transformer（encoder-decoder transformers）与更流行的仅解码器架构（decoder-only architecture）之间的表达能力（expressivity）差距。后一种结果更为出人意料：我们表明，仅用\"草稿标记\"（scratch tokens）填充输入（padding）就能产生一种使逆排列学习（inverse permutation learning）成为可能的构造。\n\n我们推测（conjecture），这可能表明一种替代机制，通过这种机制，思维链提示（chain-of-thought prompting）或更一般地说，中间\"思考\"标记（intermediate \"thinking\" tokens）可以在大型语言模型中实现推理，即使这些标记不编码有意义的语义信息（semantic information）（例如，中间计算（intermediate computations）的结果）。",
                    "inspiration_trace": "# 逆排列学习在Transformer模型中的不可能性：逻辑链分析\n\n## 一、宏观问题：语言模型的通用推理能力之谜\n\n**起点观察**：现代大型语言模型(LLMs)展现出令人惊讶的通用推理能力，能够处理多样化的任务（如上下文学习、多项选择问答、长上下文推理等）。这引发了一个根本性问题：**为什么单一模型架构能够胜任这些看似需要不同归纳偏置的任务？**\n\n## 二、聚焦关键挑战：排列不变性缺失\n\n**深入观察**：作者注意到一个关键现象——尽管LLMs在语言建模中需要位置敏感性，但在许多推理任务中却需要排列不变性：\n- 上下文学习应对示例顺序不变\n- 多项选择问答应对选项顺序不变\n- 长上下文推理应对事实顺序不变\n- 演绎推理应对谓词顺序不变\n\n**核心矛盾**：现代LLMs在这些需要排列不变性的任务上表现不佳，这种对顺序的敏感性破坏了模型可靠性。\n\n## 三、问题形式化：逆排列学习任务\n\n**关键洞察**：逆排列学习是实现排列不变性的自然方式。作者将问题形式化为：给定一个排列和该排列应用于序列后的结果，模型需要恢复原始的\"规范\"序列。\n\n**任务简化**：这个形式化比实际应用场景更简单（因为明确提供了排列描述），但作者推测即使是这个简化任务，仅解码器Transformer也无法解决。\n\n## 四、核心假设：因果注意力掩码的根本限制\n\n**大胆假设**：作者提出核心假设——**仅解码器Transformer中的因果注意力掩码是逆排列学习不可能的根本原因**。\n\n**直觉推理**：\n1. 任何非平凡排列都需要将至少一个元素从序列后部移到前部\n2. 因果注意力掩码阻止了从后部标记到前部标记的信息流动\n3. 因此，仅解码器Transformer无法实现必要的排列逆操作\n\n## 五、理论验证：不可能性证明\n\n**严格证明**：作者通过数学理论验证了假设，证明对于任何非平凡排列，任意深度的仅解码器Transformer都无法输出逆排列后的序列。\n\n**证明核心**：利用因果注意力掩码导致的残差流中每一行对后续行的不变性，证明了信息无法从后面的位置传递到前面的位置，从而阻止了逆排列的实现。\n\n## 六、解决方案探索：突破限制的两种路径\n\n### 路径一：移除因果注意力掩码\n**构造验证**：作者证明移除因果掩码以允许双向注意力机制，可以解决逆排列学习问题。\n**意义揭示**：这表明编码器-解码器架构可能没有仅解码器架构的相同限制。\n\n### 路径二：输入填充策略\n**意外发现**：更令人惊讶的是，简单地用\"scratch tokens\"填充输入也能使逆排列学习成为可能。\n**机制解释**：这些填充标记为模型提供了\"scratch space\"，可用于在前向传播期间执行必要的计算。\n\n## 七、推广意义：对LLMs推理机制的启示\n\n**深层思考**：作者将发现推广到更广泛的LLMs推理机制，提出一个大胆猜想：\n- 输入填充机制可能解释了\"思维链\"提示、\"scratchpad\"提示或\"reasoning tokens\"的成功\n- 即使这些中间标记不编码有意义的语义信息，它们也能通过提供额外计算空间来增强推理能力\n\n**理论贡献**：这为理解Transformer模型中的推理提供了一个新的、尚未探索的机制视角。\n\n## 逻辑链总结\n\n从LLMs通用推理能力的宏观问题出发，作者逐步聚焦到排列不变性这一具体挑战，通过形式化为逆排列学习任务，揭示了仅解码器Transformer的根本限制（因果注意力掩码），并通过理论证明验证了这一限制。随后，作者探索了两种突破限制的路径，特别是发现输入填充这一简单策略的有效性，最终将这一发现推广到对LLMs推理机制的新理解，形成了一个从观察、假设到理论验证再到推广应用的完整逻辑链条。"
                },
                {
                    "title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?",
                    "arxiv_id": "2509.24005",
                    "authors": "Chenruo Liu, Yijun Dong, Qi Lei",
                    "summary": "We initiate a unified theoretical and algorithmic study of a key problem in weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained student with pseudolabels from a weaker teacher on a downstream task with spurious correlations, does W2S happen, and how to improve it upon failures? We consider two sources of spurious correlations caused by group imbalance: (i) a weak teacher fine-tuned on group-imbalanced labeled data with a minority group of fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set pseudolabeled by the teacher with a minority group of fraction $\\eta_u$. Theoretically, a precise characterization of W2S gain at the proportional asymptotic limit shows that W2S always happens with sufficient pseudolabels when $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S gain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is corroborated by extensive experiments on various spurious correlation benchmarks and teacher-student pairs. To boost W2S performance upon failures, we further propose a simple, effective algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning. Our algorithm is group-label-free and achieves consistent, substantial improvements over vanilla W2S fine-tuning.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究弱到强(W2S)泛化问题，特别是在存在虚假相关性的情况下如何提高模型性能。根据筛选标准，我判断它符合研究范围，原因如下： 1. 核心判断：论文的本质是关于改进模型训练范式和泛化能力的理论研究。它探讨了如何通过弱教师模型的伪标签来微调强预训练学生模型，并提出了一种算法来提高W2S性能。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 2. 正面指标：虽然论文没有直接使用\"Large language models, LLMs\"术语，但提到的\"strong pre-trained student\"通常指的就是LLM。论文关注泛化能力(generalization)，这与推理能力密切相关，属于LLM基础能力的重要组成部分。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的内容，因此不符合排除标准。 4. 特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，而是专注于模型训练和泛化能力的理论分析与算法改进。 综上所述，这篇论文致力于提高LLM的泛化能力，属于改进LLM基础能力的研究，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在研究在存在虚假相关性的情况下，弱到强（W2S）泛化是否会发生以及如何改进其失败情况。针对由群体不平衡引起的虚假相关性问题，我们提出了一种理论框架，揭示了标记数据和无标签数据中少数群体比例差异对W2S泛化的影响，并提出了一种基于学生模型置信度的增强型W2S算法。在多个虚假相关性基准测试（Waterbirds、BFFHQ、ImageNet-9和BG-COCO）上通过平均准确率和最差群体准确率验证了其有效性。",
                    "summary_translation": "我们对弱到强(weak-to-strong, W2S)泛化中的一个关键问题进行了统一的理论和算法研究：当使用较弱教师(weaker teacher)生成的伪标签(pseudolabels)在具有虚假相关性(spurious correlations)的下游任务上微调强预训练学生(strong pre-trained student)模型时，W2S是否会发生，以及如何在失败时改进它？我们考虑了由群体不平衡(group imbalance)引起的两种虚假相关性来源：(i)在群体不平衡的标记数据(labeled data)上微调的弱教师，其中少数群体(minority group)比例为$\\eta_\\ell$；(ii)由教师标记的群体不平衡未标记集(unlabeled set)，其中少数群体比例为$\\eta_u$。理论上，在比例渐近极限(proportional asymptotic limit)下对W2S增益(W2S gain)的精确表征表明，当$\\eta_u = \\eta_\\ell$时，只要有足够的伪标签，W2S总会发生；但当$\\eta_u \\ne \\eta_\\ell$时，W2S可能会失败，且W2S增益随着$(\\eta_u - \\eta_\\ell)^2$的增加而减小。我们的理论在各种虚假相关性基准(spurious correlation benchmarks)和教师-学生对(teacher-student pairs)上的大量实验中得到了证实。为了在W2S失败时提升其性能，我们进一步提出了一种简单有效的算法补救措施(algorithmic remedy)，即在W2S微调后，在强学生模型的高置信度数据子集(high-confidence data subset)上重新训练该模型。我们的算法是无群体标签的(group-label-free)，并且相对于原始的W2S微调(vanilla W2S fine-tuning)，实现了一致且显著的改进。",
                    "inspiration_trace": "# 从问题观察到方法论创新：弱到强泛化在虚假相关性下的逻辑推演\n\n## 1. 宏观问题：超对齐挑战下的弱到强泛化\n\n论文研究的核心宏观问题是：**在存在虚假相关性的情况下，弱到强(W2S)泛化是否会发生？** 这一问题源于对超对齐(superalignment)的探索——即超人类智能的模型能否从较弱的人类监督中学习。W2S泛化提供了积极答案：强大的预训练学生模型通过弱教师生成的伪标签进行微调，通常可以超越其教师表现。然而，现实场景中数据往往存在系统性偏见，如人口统计或获取因素相关的虚假相关性，这为W2S泛化带来了新的挑战。\n\n## 2. 问题观察与界定：虚假相关性的双重来源\n\n作者通过细致观察，识别出W2S场景中虚假相关性的两个关键来源：\n\n1. **弱教师自身的不平衡**：弱教师在不平衡标记数据（少数群体比例为ηℓ）上进行微调，导致其学习到虚假相关性\n2. **无标签数据的不平衡**：用于伪标记的无标签集本身存在不平衡（少数群体比例为ηu）\n\n这引出了核心研究问题：当ηu ≠ ηℓ时，W2S泛化是否会发生？若失败，如何改进？作者将问题精确界定为：研究标记数据和无标签数据中群体比例不匹配对W2S泛化的影响机制。\n\n## 3. 理论假设：建立分析框架\n\n为系统分析这一问题，作者构建了理论框架，提出三个关键假设：\n\n1. **特征解耦假设**：下游任务数据可分解为核心特征z(x)（决定标签y）和群体特征ξ(x)（决定群体标签g），其中核心特征在群体间不变，而群体特征引入虚假相关性\n\n2. **弱-强模型表示差异假设**：\n   - 弱教师表示φT严重纠缠核心特征和群体特征\n   - 强学生表示φS部分解纠缠这些特征，带来对虚假相关性的鲁棒性\n\n3. **比例渐近极限假设**：考虑高维特征维度dz、标记样本数n和无标签样本数N同时趋于无穷的极限情况，便于理论分析\n\n基于这些假设，作者推导出核心理论预测：**W2S增益与(ηu - ηℓ)²成反比**，即标记数据和无标签数据中少数群体比例的不匹配程度越大，W2S泛化越可能失败。\n\n## 4. 实验验证：理论与现实的桥梁\n\n为验证理论预测，作者设计了双重实验策略：\n\n1. **合成实验**：使用高斯数据验证理论预测，结果显示：\n   - 当ηu ≈ ηℓ时，W2S增益最大\n   - 教师与学生表示相似度越低，W2S增益越大\n   - 当ηu ≠ ηℓ时，W2S增益显著下降\n\n2. **真实世界评估**：在多个带有虚假相关性的基准数据集（Waterbirds、BFFHQ、ImageNet-9和BG-COCO）上验证，结果一致表明：\n   - 当ηℓ = 0.5（平衡标记数据）时，增加ηu提高W2S性能\n   - 当ηℓ = ηo（不平衡标记数据）时，W2S增益在ηu = ηo时最大，随ηu增加而减少\n   - W2S增益随|ηu - ηℓ|增加而单调下降\n\n这些实验结果与理论预测高度一致，证实了标记数据和无标签数据中群体比例不匹配是W2S泛化失败的关键因素。\n\n## 5. 方法论创新：Enhanced-W2S\n\n基于理论和实验发现，作者提出了Enhanced-W2S方法，专门解决ηu ≠ ηℓ时的W2S泛化失败问题。该方法包含两个创新组件：\n\n1. **基于置信度的样本选择**：选择学生模型预测置信度最高（熵最低）的样本子集进行再训练。这一操作隐式地平衡了数据分布，因为高置信度样本主要来自多数群体，有效降低了实际训练中的ηu。\n\n2. **广义交叉熵(GCE)损失**：对选定样本应用GCE损失而非标准交叉熵损失，减轻弱教师伪标签噪声的负面影响。GCE损失对高置信度但错误的伪标签惩罚较轻，提高了对噪声的鲁棒性。\n\n该方法的关键优势是**无需群体注释**，在标记数据或无标签数据高度群体不平衡的情况下显著提高了W2S增益。实验证明，Enhanced-W2S在多个数据集和模型对上始终优于原始W2S方法，平均提升达4-11%。\n\n## 6. 逻辑链条总结\n\n整个研究的逻辑演进形成了一个完整的闭环：\n\n**宏观问题**（超对齐下的W2S泛化）→ **问题观察**（虚假相关性的双重来源）→ **理论假设**（特征解耦与表示差异）→ **理论发现**（W2S增益与(ηu-ηℓ)²成反比）→ **实验验证**（合成与真实数据集一致支持理论）→ **方法创新**（基于置信度的样本选择与GCE损失）→ **效果验证**（显著提升W2S性能）\n\n这一研究不仅回答了\"在虚假相关性下W2S是否会发生\"的问题，更提供了\"如何改进失败情况\"的实用解决方案，为弱监督学习在现实场景中的应用提供了重要理论基础和方法支持。"
                },
                {
                    "title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know",
                    "arxiv_id": "2509.23830",
                    "authors": "Albus Yizhuo Li",
                    "summary": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive yet efficient Large Language Models (LLMs). However, the standard deterministic routing mechanism presents a significant limitation: its inherent brittleness is a key contributor to model miscalibration and overconfidence, resulting in systems that often do not know what they don't know. This thesis confronts this challenge by proposing a structured \\textbf{Bayesian MoE routing framework}. Instead of forcing a single, deterministic expert selection, our approach models a probability distribution over the routing decision itself. We systematically investigate three families of methods that introduce this principled uncertainty at different stages of the routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space}, and the final \\textbf{selection-space}. Through a series of controlled experiments on a 3-billion parameter MoE model, we demonstrate that this framework significantly improves routing stability, in-distribution calibration, and out-of-distribution (OoD) detection. The results show that by targeting this core architectural component, we can create a more reliable internal uncertainty signal. This work provides a practical and computationally tractable pathway towards building more robust and self-aware LLMs, taking a crucial step towards making them know what they don't know.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础架构组件，具体是针对Mixture-of-Experts (MoE)架构中的路由机制进行创新。论文提出了贝叶斯MoE路由框架，通过在路由决策上建模概率分布，而不是强制确定性选择，来解决模型校准不良和过度自信问题。这明显属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。因此，根据第一步判断标准，应该保留。 第二步：正面指标 论文明确包含以下正面指标： - 核心概念：论文直接关注Large Language Models (LLMs)，特别是基于MoE架构的LLMs。 - 能力方向：虽然论文没有直接讨论推理或规划，但它研究的模型校准和不确定性建模是高质量推理的基础。一个能准确评估自身不确定性的模型更有可能在推理任务中表现出色。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 虽然关注模型可靠性，但这是从架构层面（MoE路由机制）来提高模型校准能力，而非应用层面的水印、安全或安全性问题。 第四步：特殊和模糊情况 论文关注模型校准和不确定性建模，这与减少幻觉和提高推理质量密切相关。论文提出的贝叶斯MoE路由框架是一种新的架构改进方法，旨在增强模型对自身不确定性的认知，从而减少过度自信和错误判断。这属于提出新方法来增强模型内在可靠性，从而提升通用推理质量的情况，应该保留。 最终决策 综合分析，这篇论文的核心贡献是通过改进MoE架构的路由机制，增强LLM的校准能力和不确定性建模，使其\"知道它们不知道什么\"。这种改进直接提升了模型的基础能力，为更高质量的推理提供了基础，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)中的过度自信和校准问题，特别是Mixture-of-Experts(MoE)架构中确定性路由机制的脆弱性。针对MoE路由决策的不确定性建模问题，我们提出了一种结构化的贝叶斯MoE路由框架，在权重空间、logit空间和选择空间三个不同阶段引入原则性不确定性，并在3B参数的MoE模型上通过路由稳定性、分布内校准和分布外检测等指标验证了其有效性。",
                    "summary_translation": "混合专家（Mixture-of-Experts, MoE）架构使得创建大规模且高效的大型语言模型（Large Language Models, LLMs）成为可能。然而，标准的确定性路由机制（deterministic routing mechanism）存在一个显著限制：其固有的脆性是模型校准不当（miscalibration）和过度自信（overconfidence）的关键因素，导致系统往往不知道自己不知道什么。本论文通过提出一个结构化的**贝叶斯MoE路由框架**（Bayesian MoE routing framework）来应对这一挑战。我们的方法不是强制进行单一的确定性专家选择，而是对路由决策本身进行概率分布建模。我们系统地研究了三类方法，这些方法在路由管道的不同阶段引入这种原则性不确定性：在**权重空间**（weight-space）、**logit空间**（logit-space）和最终的**选择空间**（selection-space）中。通过对一个30亿参数的MoE模型进行一系列对照实验，我们证明了该框架显著提高了路由稳定性、分布内校准（in-distribution calibration）和分布外（out-of-distribution, OoD）检测能力。结果表明，通过针对这一核心架构组件，我们能够创建更可靠的内部不确定性信号。这项工作为构建更强大和更具自我意识的LLMs提供了一条实用且计算上可行的途径，朝着使它们知道自己不知道什么的目标迈出了关键一步。",
                    "inspiration_trace": "# 从宏观问题到贝叶斯MoE路由：逻辑推导链\n\n## 1. 宏观问题：LLMs的\"不知道自己不知道\"困境\n\n大型语言模型(LLMs)面临一个根本性挑战：它们经常产生高度自信但错误的输出，这种现象被称为\"过度自信\"和\"幻觉\"。在高风险领域如医学和法律，这种\"不知道自己不知道\"的状态构成了重大障碍，阻碍了LLMs的可靠部署。\n\n**核心问题**：模型无法量化自身的预测不确定性，导致输出与实际知识状态不匹配。\n\n## 2. 问题聚焦：MoE架构中的路由机制\n\n作者将问题进一步聚焦到Mixture-of-Experts (MoE)架构的关键组件——路由机制上。MoE通过路由网络激活模型参数的一小部分，实现了大规模但高效的LLMs。然而，作者识别出一个关键脆弱点：\n\n**关键观察**：在MoE中，路由器的决策不是微调，而是决定哪些专门子网络被激活。错误的路由选择意味着将错误的专家应用于token，导致输出缺陷。在具有数十个堆叠MoE层的现代LLMs中，单个路由错误会引发级联故障。\n\n**假设形成**：确定性路由机制是MoE模型校准不良和过度自信的主要贡献者。\n\n## 3. 实验验证：路由机制的脆弱性与随机性潜力\n\n为验证假设，作者设计了两个关键实验：\n\n### 实验1：确定性路由的脆弱性\n- **方法**：在MoE层输入添加微小扰动，测量专家选择变化\n- **结果**：即使极小噪声(γ≥0.005)也导致稳定性显著下降\n- **关键发现**：不稳定性集中在特定层组(早期、中期和最终层)\n\n### 实验2：随机路由的潜力\n- **方法**：用温度控制的随机采样替换确定性Top-K选择\n- **结果**：在中后层引入随机性显著降低ECE(校准误差)，同时保持准确度\n- **洞见**：随机性作为正则化形式，迫使模型减少过度自信\n\n## 4. 方法论形成：从随机性到原则性贝叶斯框架\n\n基于实验结果，作者形成了核心方法论：\n\n**逻辑推导**：如果确定性路由是脆弱的，而简单随机性有益，那么原则性的、数据驱动的不确定性方法应该更优。\n\n这引导作者提出结构化贝叶斯路由框架，在路由管道的不同阶段引入原则性不确定性：\n\n### 4.1 专家质心空间(权重空间)方法\n将路由器的权重矩阵视为随机变量，通过贝叶斯多项逻辑回归建模：\n- **MC Dropout Router (MCDR)**：推理时保持dropout，近似权重后验\n- **SWAG Router (SWAGR)**：用SGD轨迹的权重矩近似高斯后验\n- **Deep Ensembles Router (DER)**：多个独立训练路由器作为经验后验样本\n\n### 4.2 专家Logit空间(潜在空间)方法\n直接在logit向量上建模概率分布，使用变分推断：\n- **Mean-Field Variational Router (MFVR)**：对角协方差高斯分布\n- **Full-Covariance Variational Router (FCVR)**：完全协方差高斯分布，捕获专家间相关性\n\n### 4.3 专家选择空间(决策空间)方法\n学习输入相关温度控制随机性：\n- **Variational Temperature Sampling Router (VTSR)**：神经网络预测温度，动态控制路由随机性\n\n## 5. 方法评估与验证\n\n通过系统实验验证三个核心假设：\n\n### 稳定性假设\n贝叶斯方法在输入扰动下表现出更高稳定性，FCVR达到0.897的Jaccard相似度，远超确定性基线的0.650。\n\n### 校准假设\n所有贝叶斯方法在不损害准确性的情况下显著改善校准，FCVR将ECE降低94%以上。\n\n### OoD检测假设\n贝叶斯路由器的不确定性信号显著提升OoD检测性能，logit方差等内部信号优于传统熵信号。\n\n## 6. 实用性考量\n\n作者分析了计算和内存开销，证明这些方法在实际应用中可行：\n- MCDR几乎无额外开销\n- FCVR提供最佳性能，中等计算成本\n- VTSR提供独特低延迟方案\n\n## 逻辑链条总结\n\n从\"LLMs不知道自己不知道\"的宏观问题出发，通过聚焦MoE路由机制，实验验证确定性路由的脆弱性和随机性的潜力，最终形成结构化贝叶斯路由框架。这一框架通过在权重空间、logit空间和选择空间引入原则性不确定性，实现了更稳定、更好校准、更具自我意识的LLMs，朝着\"让LLMs知道它们不知道什么\"的目标迈出了关键一步。"
                },
                {
                    "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement",
                    "arxiv_id": "2509.23799",
                    "authors": "Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu",
                    "summary": "Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是改进LLM的基础能力。论文提出了SAE-RSV方法，通过稀疏自编码器来优化steering向量，这是一种不修改模型参数但能增强LLM控制能力的新技术。这属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文明确符合\"Large language models, LLMs\"这一核心概念。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及强化学习等训练方法或智能体等新兴范式，但其提出的steering向量优化技术可以视为提升模型通用能力的一种方法。 第三步：排除标准——论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的研究。 第四步：特殊和模糊情况——论文不属于智能体/工具使用的特殊范畴。虽然其方法可能间接影响模型的可解释性和控制能力，但这不是论文的主要焦点。 综合判断：这篇论文的核心贡献是提出了一种通过稀疏自编码器优化steering向量的方法，以增强LLM的控制能力。这属于改进LLM基础能力的研究，虽然没有直接针对推理能力，但提供了一种可能间接提升模型在各种任务上表现的新技术，因此符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM引导方法在小数据集场景下效果不佳的问题。针对有限训练数据导致的引导向量噪声问题，我们提出了一种基于稀疏自编码器的引导向量精炼方法（SAE-RSV），并在Llama-3-8B-Instruct模型上通过五个行为概念数据集以成功率和熵为指标验证了其有效性。",
                    "summary_translation": "Steering（引导）已成为一种在不修改模型参数的情况下控制大型语言模型（large language models, LLMs）的有前景方法。然而，大多数现有的steering（引导）方法依赖大规模数据集来学习清晰的行为信息，这限制了它们在许多真实场景中的适用性。从小数据集提取的steering vectors（引导向量）通常包含任务无关的噪声特征，这降低了它们的有效性。为了精炼从有限数据中学习的steering vectors（引导向量），我们提出了通过稀疏自编码器优化引导向量（Refinement of Steering Vector via Sparse Autoencoder, SAE-RSV）的方法，该方法利用SAE（sparse autoencoder，稀疏自编码器）对steering vectors（引导向量）进行语义去噪和增强。在我们的框架中，我们首先根据SAE（稀疏自编码器）提供的语义移除任务无关特征，然后通过与已识别相关特征的语义相似性，丰富小数据集中缺失的任务相关特征。大量实验表明，所提出的SAE-RSV方法显著优于所有基线方法，包括监督微调（supervised fine-tuning）。我们的研究结果表明，通过SAE（稀疏自编码器）精炼原始steering vector（引导向量），可以从有限的训练数据构建有效的引导向量。",
                    "inspiration_trace": "# 从宏观问题到创新方法：SAE-RSV的逻辑演进\n\n## 宏观问题：LLM行为控制的挑战\n\n大型语言模型(LLM)在各种任务中表现出色，但其可控性仍是一个核心挑战。如何在不修改模型参数的情况下有效控制LLM行为，特别是在资源受限场景中，是一个值得探索的研究方向。\n\n## 观察一：转向方法的数据依赖性\n\n作者首先观察到，转向方法(如CAA)通过修改内部表示来引导模型行为，是一种有前景的控制方向。然而，这些方法通常依赖大规模数据集来提取有用的行为信息，这限制了它们在实际场景中的应用，因为许多情况下只有少量训练样本可用。\n\n**核心问题浮现**：如何在小数据集条件下构建有效的转向向量？\n\n## 观察二：小数据集导致的转向向量缺陷\n\n进一步观察发现，从小数据集学习的转向向量存在两个关键问题：\n1. **噪声问题**：转向向量常包含大量与任务无关的噪声特征，因为LLM的隐藏激活可能捕获虚假相关性\n2. **缺失问题**：小数据集无法覆盖目标行为的所有场景，导致转向向量缺失部分任务相关特征\n\n**问题细化**：如何同时解决转向向量的噪声和缺失问题？\n\n## 观察三：现有方法的局限性\n\n作者注意到，现有的基于稀疏自编码器(SAE)的去噪方法主要依赖激活统计信息选择特征。然而，当训练样本较少时，这种统计方法不可靠，因为稳健的特征激活估计需要足够数据。此外，这些方法常选择表面相关的特征(如标点符号或停用词)，而非真正与任务相关的特征。\n\n**假设形成**：基于特征的语义而非统计特性来识别和精炼转向向量，可能更有效。\n\n## 方法论构建：从假设到实践\n\n### 假设一：语义驱动的特征去噪\n作者假设：如果能利用特征的语义信息(由SAE提供)来判断其与目标任务的相关性，就能更可靠地识别和去除噪声特征。\n\n**实现方案**：\n1. 构建种子特征集，测量正负样本间的激活差异\n2. 使用LLM判断每个特征是否与目标任务语义相关\n3. 将特征分为噪声特征和任务相关特征\n4. 构建噪声向量并从原始转向向量中减去\n\n### 假设二：语义驱动的特征增强\n作者进一步假设：除了去噪外，还应通过语义相似性恢复小数据集中缺失的任务相关特征。\n\n**实现方案**：\n1. 计算每个未识别特征的语义有用性分数，基于其与已识别相关特征的相似性和与噪声特征的差异性\n2. 选择得分最高的特征作为有用特征\n3. 构建有用向量并添加到原始转向向量中\n\n## 最终方法：SAE-RSV的整合\n\n作者将上述两个组件整合为统一框架SAE-RSV，其数学表达为：\n\n```\nv'_steer = α1 · v_steer - α2 · v_noise + α3 · v_useful\n```\n\n这一公式体现了三个关键操作的平衡：\n- 保留原始转向向量的核心方向(α1 · v_steer)\n- 去除噪声特征的干扰(-α2 · v_noise)\n- 增强缺失的任务相关信息(+α3 · v_useful)\n\n## 实验验证与发现\n\n通过在五个数据集上的实验，作者验证了SAE-RSV的有效性：\n1. SAE-RSV显著优于所有基线方法，包括监督微调\n2. 去噪和增强模块各自贡献了性能提升(平均分别提升3.6%和7.2%)\n3. 在小数据集条件下，原始转向向量中超过93.6%的特征是噪声，而仅捕获了42.2%的任务相关特征\n\n这些发现证实了作者的初始假设：基于语义的特征去噪和增强能有效解决小数据集条件下的转向向量质量问题。\n\n## 总结：从问题到解决方案的逻辑链\n\n作者的思考过程展现了清晰的逻辑演进：\n\n1. **宏观问题**：LLM行为控制的数据效率挑战\n2. **观察聚焦**：转向方法的数据依赖性和小数据集缺陷\n3. **假设形成**：语义驱动的特征选择可克服统计方法局限性\n4. **方法论构建**：分别设计去噪和增强模块\n5. **方法整合**：形成统一的SAE-RSV框架\n6. **实验验证**：证明方法的有效性和优越性\n\n这一逻辑链不仅解决了特定技术问题，还为资源受限场景下的LLM控制提供了新思路，体现了从观察到假设再到实践的科学研究范式。"
                },
                {
                    "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation",
                    "arxiv_id": "2509.23667",
                    "authors": "Sungmin Cha, Kyunghyun Cho",
                    "summary": "For efficiency, preference alignment is often performed on compact, knowledge-distilled (KD) models. We argue this common practice introduces a significant limitation by overlooking a key property of the alignment's reference model: its distributional recall. We show that the standard KD -> Align workflow diminishes the model's capacity to align rare yet desirable behaviors, even under strong preference signals. We instead demonstrate that reversing the pipeline (i.e., Align -> KD) is essential: alignment must first be performed on a high-recall reference before distillation. Our contributions are threefold. First, we provide a minimal working explanation of how the reference model constrains preference alignment objectives at a fundamental level. Second, we validate this theory in a controllable Mixture-of-Gaussians experiment, where low-recall anchoring consistently results in suboptimal model performance. Finally, we demonstrate that the same phenomenon holds in LLM alignment with the SmolLM2 family: models aligned after KD fail to effectively align target behaviors, resulting in substantially lower reward and target precision. In contrast, our proposed Align -> KD pipeline robustly aligns these behaviors, yielding models with superior target-oriented metrics and lower variance. Together, these results establish reference-model recall as a first-order design choice in alignment, offering a clear principle: alignment must precede distillation.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是关于改进大语言模型的训练范式，特别是对齐(alignment)和知识蒸馏(knowledge distillation)的顺序问题。论文提出先对齐再蒸馏(Align -> KD)的流程，以保持模型的对齐效果，特别是对于罕见但理想的行为。这属于改进LLM基础能力和训练范式的研究，符合我的研究目标中\"提出新的训练范式\"的范畴。 具体分析： 1. 第一步核心判断：论文本质是改进LLM的基础训练流程，属于模型基础能力的改进，应该保留。 2. 第二步正面指标：论文明确涉及大语言模型(LLMs)这一核心概念，并在SmolLM2系列模型上验证了其方法。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性的应用层面等排除领域。 4. 第四步特殊和模糊情况：论文虽未直接讨论推理能力，但对齐过程本身是提升模型整体性能和可靠性的关键环节，良好的对齐可以间接提高模型在各种任务上的表现，包括推理任务。 论文的核心贡献是提出了一种改进的模型训练流程，通过优化对齐和蒸馏的顺序来提高模型的对齐质量和整体性能。虽然不是直接针对推理能力的研究，但这种基础训练范式的改进可能会对模型的通用能力产生积极影响，包括推理能力。因此，这篇论文符合我的研究范围。",
                    "summary2": "本文旨在解决大型语言模型对齐与知识蒸馏的顺序问题。针对当前先蒸馏后对齐(KD → Align)的常见做法，我们提出了先对齐后蒸馏(Align → KD)的流程，强调必须首先在高召回率参考模型上执行对齐。在Mixture-of-Gaussians实验和SmolLM2语言模型系列上，通过目标精度、召回率和奖励等指标验证了其有效性，证明KD → Align流程存在结构性低召回陷阱，而Align → KD能产生更稳定、性能更优的紧凑模型。",
                    "summary_translation": "为提高效率，偏好对齐（preference alignment）通常在紧凑的知识蒸馏（knowledge-distilled, KD）模型上执行。我们认为这种常见做法忽视了对齐参考模型的一个关键特性——其分布召回率（distributional recall），从而引入了重大限制。我们表明，标准的KD→对齐工作流程会降低模型对齐罕见但理想行为的能力，即使在强偏好信号下也是如此。相反，我们证明逆转这一流程（即对齐→KD）是必要的：对齐必须在蒸馏之前首先在高召回率参考模型上执行。\n\n我们的贡献有三方面。首先，我们提供了一个简明的工作解释，说明参考模型如何在基础层面上约束偏好对齐目标。其次，我们在一个可控的高斯混合（Mixture-of-Gaussians）实验中验证了这一理论，实验中低召回率锚定（low-recall anchoring）始终导致模型性能次优。最后，我们证明在SmolLM2系列的大语言模型（LLM）对齐中存在相同现象：在KD后进行对齐的模型无法有效对齐目标行为，导致奖励和目标精度显著降低。相比之下，我们提出的对齐→KD流程能够稳健地对齐这些行为，产生的模型具有更优的目标导向指标和更低的方差。\n\n总之，这些结果确立了参考模型召回率（reference-model recall）作为对齐中的一阶设计选择，提供了一个明确原则：对齐必须先于蒸馏。",
                    "inspiration_trace": "# 从宏观问题到核心方法：逻辑链推演\n\n## 1. 宏观问题起点：效率与对齐的平衡\n\n**核心挑战**：如何高效地对大型语言模型(LLM)进行人类偏好对齐，同时保持模型的计算效率？\n\n这是一个实际部署中的关键问题——我们既需要模型能够准确对齐人类偏好，又希望模型足够小以便高效部署。\n\n## 2. 观察到的现象：常见实践及其隐含假设\n\n**观察到的事实**：当前领域普遍采用 **KD → Align** 工作流程：\n- 预训练大型模型\n- 监督微调(SFT)\n- 知识蒸馏(KD)得到紧凑模型\n- 在紧凑模型上进行偏好对齐\n\n**隐含假设**：这种做法认为对齐是一个可以在模型压缩后独立进行的步骤，主要出于实用主义考虑：\n1. 降低对齐过程的计算成本\n2. 目标是获得紧凑的最终模型\n3. 紧凑模型通常更容易获取和使用\n\n## 3. 发现的关键问题：召回率缺失\n\n**异常发现**：知识蒸馏虽然保留了高频、高概率的模式，但会**系统性修剪稀有模式**，降低模型的分布召回率(distributional recall)。\n\n**核心洞察**：当使用这种低召回率的模型作为参考模型(π_ref)进行对齐时，即使面对强烈的偏好信号，对齐过程也无法有效恢复这些稀有但理想的行为。\n\n## 4. 提出假设：参考模型召回率的关键作用\n\n**核心假设**：参考模型的召回率是偏好对齐成功的决定性因素，而非次要考虑。\n\n**具体假设**：理想行为必须位于参考模型的支持范围内(即π_ref(y*|x) > 0)，否则会导致结构性失败，作者称之为\"**低召回率陷阱**\"。\n\n## 5. 理论分析：低召回率陷阱的机制\n\n作者从理论上解析了低召回率陷阱如何在对齐算法中体现：\n\n### 在RLHF/PPO中：\n- 策略更新由KL惩罚塑造的奖励信号驱动：r'(y|x) = R(y|x) - β log(π_θ(y|x)/π_ref(y|x))\n- 对于理想响应y*，如果π_ref(y*|x) ≈ 0，KL惩罚项会爆炸，即使奖励模型分配高奖励\n- 塑造后的奖励变得无限负，淹没任何正向信号，形成**学习陷阱**\n\n### 在DPO中：\n- DPO损失包含模型对数比和参考对数比：z = β[log π_θ(y_w|x)/π_θ(y_l|x) + log π_ref(y_l|x)/π_ref(y_w|x)]\n- 如果π_ref(y_w|x) ≈ ε << 1而π_ref(y_l|x)适中，参考偏移会使梯度消失\n- 涉及缺失/稀有模式的样本对获得可忽略的更新，导致**梯度饥饿**\n\n## 6. 提出解决方案：流程反转\n\n基于理论分析，作者提出了**Align → KD**的替代流程：\n1. 首先在高召回率的参考模型上进行对齐\n2. 然后将对齐后的模型蒸馏成紧凑模型\n\n**核心洞见**：这种流程反转将参考模型的召回率从\"实现细节\"提升为\"一阶设计选择\"。\n\n## 7. 实验验证：从理论到实证\n\n作者通过两个阶段的实验验证了假设：\n\n### 第一阶段：混合高斯实验(MoG)\n- 创建可控环境，直接操纵召回率\n- 结果一致显示：Align → KD流程在目标精度、平均奖励和稳定性上显著优于KD → Align\n- 即使增加训练预算或调整压缩强度，也无法解决KD → Align的根本缺陷\n\n### 第二阶段：LLM验证(SmolLM2家族)\n- 在真实语言模型上验证相同现象\n- 结果再次确认：Align → KD流程产生更高奖励、更强目标精度和更稳定训练动态的模型\n- 即使中间的高召回率对齐模型(p'_AK)已经超过最终的低召回率对齐模型(p''_KA)的性能\n\n## 8. 形成最终方法论原则\n\n基于理论和实验验证，作者形成了具有普遍指导意义的方法论原则：\n\n**\"对齐必须在蒸馏之前进行\"(Alignment Must Precede Distillation)**\n\n这一原则确立了参考模型召回率作为对齐过程中的**一阶设计参数**，强调了流程设计直接决定了偏好对齐的可靠性和效率，对研究和部署中对齐语言模型具有重要指导意义。\n\n---\n\n**逻辑链总结**：从实际部署中的效率与对齐平衡问题出发，通过观察常见实践中的隐含假设，发现知识蒸馏导致的召回率缺失问题，提出参考模型召回率的关键作用假设，通过理论分析揭示低召回率陷阱的机制，提出流程反转的解决方案，并通过实验验证形成最终的方法论原则。整个过程体现了从现象到本质、从理论到实践的完整科学推理链条。"
                },
                {
                    "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought",
                    "arxiv_id": "2509.23365",
                    "authors": "Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian",
                    "summary": "Previous work shows that the chain of continuous thought (continuous CoT) improves the reasoning capability of large language models (LLMs) by enabling implicit parallel thinking, and a subsequent work provided theoretical insight by showing that a two-layer transformer equipped with continuous CoT can efficiently solve directed graph reachability by maintaining a superposition of multiple reasoning traces in the continuous thought. However, it remains unclear how the superposition mechanism is naturally learned from gradient-based training methods. To fill this gap, we theoretically analyze the training dynamics of a simplified two-layer transformer on the directed graph reachability problem to unveil how the superposition mechanism emerges during training in two training stages -- (i) a thought-generation stage that autoregressively expands the continuous thought, and (ii) a prediction stage that converts the thought into the final answer. Our analysis reveals that during training using continuous thought, the index-matching logit, an important quantity which reflects the strength of the model's local search ability, will first increase and then remain bounded under mild assumptions. The bounded index-matching logit effectively balances exploration and exploitation during the reasoning process: the model will exploit local problem structures to identify plausible search traces, and assign comparable weights to multiple such traces to explore when it is uncertain about which solution is correct, which results in superposition. Our experimental results tracking the growth of logits further validate our theory.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是研究连续思维链(continuous CoT)如何提高大语言模型的推理能力，属于改进LLM基础能力的研究。论文通过理论分析揭示了超位置机制在训练过程中的形成，这直接关联到LLM的通用推理能力提升。其次，从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)和问题解决(problem-solving)，并研究思维链这一新兴推理范式的训练动态。第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文使用图可达性问题仅作为测试平台，而非特定领域应用。总体而言，该论文通过理论分析揭示了连续思维链如何增强LLM的推理能力，特别是通过超位置机制实现隐式并行思维，这直接贡献于提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在揭示连续思维链(continuous CoT)中叠加机制(superposition)的训练动态。针对有向图可达性问题，我们提出了一种理论分析方法，研究两层transformer如何在思维生成和预测两个阶段中自然学习叠加机制。通过分析指数匹配logit的动态变化，我们证明了其在训练过程中保持有界，从而平衡探索与利用，使模型能同时维护多个合理推理轨迹。在ProsQA数据集上的实验验证了理论预测，模型最终达到96.2%的准确率，且注意力logit的增长模式与理论分析一致。",
                    "summary_translation": "先前的研究表明，连续思维链(chain of continuous thought, continuous CoT)通过实现隐式并行思维提升了大语言模型(large language models, LLMs)的推理能力，后续研究则通过理论分析表明，配备连续思维链的两层transformer能够通过在连续思维中维持多个推理轨迹的叠加态(superposition)来有效解决有向图可达性(directed graph reachability)问题。然而，叠加机制(superposition mechanism)如何通过基于梯度(gradient-based)的训练方法自然习得，目前尚不清楚。为填补这一空白，我们从理论上分析了一个简化的两层transformer在有向图可达性问题上的训练动态(training dynamics)，以揭示叠加机制在两个训练阶段中是如何形成的——(i)自回归地扩展连续思维的思维生成阶段(thought-generation stage)，以及(ii)将思维转化为最终答案的预测阶段(prediction stage)。我们的分析揭示，在使用连续思维的训练过程中，index-matching logit（一个反映模型局部搜索能力(local search ability)强度的重要量度）在温和假设(mild assumptions)下会先增加然后保持有界(bounded)。有界的index-matching logit在推理过程中有效平衡了探索(exploration)和利用(exploitation)：模型会利用局部问题结构来识别合理的搜索轨迹，并在不确定哪个解决方案正确时为多个此类轨迹分配可比较的权重以进行探索，从而形成叠加态(superposition)。我们跟踪logit增长的实验结果进一步验证了我们的理论。",
                    "inspiration_trace": "# 从观察到理论：连续思维链中叠加机制的训练动态分析\n\n## 一、宏观问题：连续思维链的推理优势与机制之谜\n\n**观察现象**：连续思维链(Continuous CoT)作为一种新兴的推理增强方法，在大型语言模型中展现出显著的推理能力提升。与传统的离散思维链不同，连续思维链在连续潜空间中保持推理轨迹，无需投影回离散token空间。\n\n**核心优势**：已有研究表明，连续思维链的一个重要理论优势是能够实现\"叠加\"(superposition)推理——当模型面临多个可能的推理路径且不确定哪一条正确时，它可以并行保持所有合理路径，而非被迫选择单一路径。\n\n**关键问题**：虽然叠加机制的理论优势已被证明，但一个根本性问题尚未解决：这种叠加机制是如何通过标准的基于梯度的训练方法自然学习的？训练动态如何导致这种复杂推理能力的涌现？\n\n## 二、问题聚焦：训练动态中的叠加机制形成\n\n**缩小研究范围**：为深入理解叠加机制的学习过程，作者将研究聚焦于一个具体的理论问题：基于梯度的训练方法是否自然导致叠加结构的形成？能否从理论上证明这一点？\n\n**选择分析对象**：作者选择有向图可达性问题作为研究载体，这是一个能够有效捕捉推理本质的抽象任务。具体任务是判断从给定起始节点到目标节点是否存在路径。\n\n**模型简化**：为便于理论分析，作者使用一个简化的两层transformer架构，重点关注连续思维链的两个关键训练阶段：\n1. 思维生成阶段：模型自回归地扩展连续思维\n2. 预测阶段：模型将生成的思维转换为最终答案\n\n**核心假设**：作者提出关键假设——在连续思维链训练过程中，索引匹配logit(反映模型局部搜索能力的重要量)将保持有界，这种有界性能够平衡探索与利用，从而自然导致叠加机制的出现。\n\n## 三、理论分析：从训练动态到叠加机制\n\n### 1. 思维生成阶段的分析\n\n**关键量定义**：作者定义索引匹配logit为反映模型局部搜索能力的核心量，通过参数μv来量化。\n\n**损失函数对比**：作者设计了两种损失函数进行对比分析：\n- COCONUT-BFS：明确鼓励模型预测任何在Nc+1集合内的节点\n- COCONUT：仅使用单一演示路径进行监督，更贴近实际训练场景\n\n**核心发现**：通过梯度流分析，作者证明了以下关键定理：\n- 在COCONUT-BFS下，μv至少以对数速率增长，导致注意力logit无界\n- 在COCONUT下，如果演示路径的节点度数不是最大值，则μv收敛到有限值μ*，所有注意力logit保持有界\n\n**机制解释**：有界的索引匹配logit实现了探索与利用的平衡：\n- 利用：模型利用图的局部结构识别合理的搜索轨迹\n- 探索：当不确定哪条路径正确时，为多条合理路径分配可比较的权重\n- 这种平衡自然导致了叠加机制的出现\n\n### 2. 预测阶段的分析\n\n**信息整合机制**：作者分析了模型如何利用生成的连续思维在候选节点中做出正确预测。预测依赖于两个关键信号：\n- 残差传递(residual carryover)：将最后思维中探索的节点信息带入答案token\n- 候选提升(candidate lift)：提高两个候选节点的logit\n\n**理论保证**：作者证明，适当的参数相对增长率确保可达候选节点具有最大logit，从而模型能够做出正确预测。\n\n## 四、实验验证：理论预测的实证支持\n\n**实验设计**：作者使用两层transformer在有向图可达性问题上进行训练，跟踪关键指标的变化：\n- 在思维生成阶段，跟踪前沿边与非前沿边之间的注意力logit差异作为μv的代理\n- 在预测阶段，跟踪残差传递和候选提升的代理指标\n\n**实验结果**：\n1. **思维生成阶段**：在COCONUT下，注意力logit差异首先增加然后稳定在有界值，与理论预测一致；而在COCONUT-BFS下，logit差异持续增长，无收敛趋势。\n2. **预测阶段**：残差传递和候选提升的代理指标在训练进入预测阶段后迅速增加并稳定，表明模型学会了正确利用连续思维进行预测。\n3. **长度泛化**：一旦在早期阶段建立叠加机制，模型能够快速重用该机制来进一步扩展搜索边界，即使没有明确训练更长的思维链。\n\n## 五、方法论形成：有界logit平衡探索与利用\n\n基于理论分析和实验验证，作者形成了关于连续思维链中叠加机制形成的方法论：\n\n**核心洞见**：连续思维链训练过程中，索引匹配logit的有界性是叠加机制出现的关键。这种有界性平衡了探索与利用，使模型能够并行保持多个合理的推理轨迹。\n\n**机制解释**：\n1. **有界logit的作用**：有界的索引匹配logit防止模型过度自信地基于局部特征(如节点入度)选择单一路径，避免过早丢弃正确路径。\n2. **探索-利用平衡**：适当的logit值使模型既能利用图的局部结构识别合理路径，又能在不确定时为多条路径分配可比较的权重进行探索。\n3. **叠加的自然涌现**：这种平衡自然导致了叠加机制的出现，使模型能够在连续空间中并行维护多个可能的推理路径。\n\n**理论与实践意义**：这一方法论不仅解释了连续思维链中叠加机制的形成过程，还为理解和改进大型语言模型的推理能力提供了新的理论视角，有助于更高效、可靠地扩展这一有前景的范式。"
                },
                {
                    "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning",
                    "arxiv_id": "2509.23252",
                    "authors": "Raviteja Anantha, Soheil Hor, Teodor Nicola Antoniu, Layne C. Price",
                    "summary": "We present NanoFlux, a novel adversarial framework for generating targeted training data to improve LLM reasoning, where adversarially-generated datasets containing fewer than 200 examples outperform conventional fine-tuning approaches. The framework employs a competitive dynamic between models alternating as Attacker and Defender, supervised by a tool-augmented Judge, synthesizing multi-step questions with explanatory annotations that target specific reasoning capabilities. Fine-tuning a 4B-parameter model on NanoFlux-generated data yields performance gains across diverse domains compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning (GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical reasoning (MultiMedQA), while reducing computational requirements by 3-14x. Ablation studies reveal a non-monotonic relationship between dataset characteristics and model performance, uncovering domain-specific optimal points for question complexity and reasoning quality. NanoFlux automates training data generation through embedding-based novelty filtering, tool-augmented evaluation, and multi-hop reasoning, suggesting that future model improvements may lie in the intelligent synthesis of small, precisely targeted training datasets.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步核心判断：这篇论文的核心是提出NanoFlux，一个对抗性框架用于生成有针对性的训练数据，以提高LLM的推理能力。论文本质上是关于改进LLM的基础推理能力，提出了一种新的训练范式（对抗性双LLM框架），增强了模型的多步推理能力。这符合\"改进LLM基础能力\"和\"提出新训练范式\"的保留标准。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确针对LLMs - 能力方向：专注于reasoning，特别是mathematical reasoning, scientific reasoning, medical reasoning和multi-step reasoning - 新兴范式：使用了tool-augmented Judge和tool-augmented evaluation，属于工具使用范畴 第三步排除标准：论文虽然提到了在医疗推理(MultiMedQA)上的应用，但这只是作为评估方法效果的基准之一，而非论文的核心焦点。论文的核心是通用的推理能力提升框架，不主要聚焦于特定应用领域，因此不违反排除标准。 第四步特殊和模糊情况：论文中的工具使用(tool-augmented Judge)是为了增强LLM的通用推理能力，属于通用智能体/工具使用方法，而非特定领域应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过对抗性框架生成高质量训练数据来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型面临的\"基准耗尽\"问题，即高质量训练数据有限而模型仍存在推理缺陷的挑战。针对多领域推理任务，我们提出了一种NanoFlux对抗性框架，通过Attacker和Defender双LLM动态交互，由工具增强的Judge监督生成针对性训练数据。在GSMHard、GenomeBench和MultiMedQA三个基准测试上，通过仅200个合成示例微调4B参数模型，实现了性能提升(+5.9%、+3.6%、+16.6%)并降低计算需求3-14倍。",
                    "summary_translation": "我们提出了NanoFlux，一种新颖的对抗性框架（adversarial framework），用于生成有针对性的训练数据以提高LLM（大语言模型）推理能力，其中包含不到200个例子的对抗生成数据集表现优于传统的微调（fine-tuning）方法。该框架采用模型之间作为攻击者（Attacker）和防御者（Defender）交替进行的竞争动态，由工具增强的评判者（tool-augmented Judge）监督，合成带有解释性注释的多步骤问题，针对特定的推理能力。在NanoFlux生成的数据上对40亿参数模型进行微调，与完整基准（full-benchmark）微调相比，在多个领域实现了性能提升：数学推理（GSMHard）上提升5.9%，科学推理（GenomeBench）上提升3.6%，医疗推理（MultiMedQA）上提升16.6%，同时将计算需求降低了3-14倍。消融研究（Ablation studies）揭示了数据集特征与模型性能之间的非单调关系（non-monotonic relationship），发现了问题复杂性和推理质量的领域特定最优点。NanoFlux通过基于嵌入的新颖性过滤（embedding-based novelty filtering）、工具增强评估（tool-augmented evaluation）和多跳推理（multi-hop reasoning）实现训练数据生成的自动化，表明未来的模型改进可能在于小型、精确目标的训练数据集的智能合成。",
                    "inspiration_trace": "# NanoFlux方法论逻辑链推演\n\n## 一、宏观问题识别：数据瓶颈与推理困境\n\n作者首先识别了一个基础性问题：随着LLM性能不断提升，我们面临\"基准耗尽\"现象——高质量训练数据有限，而模型仍表现出关键推理失败。这引发了一个根本性问题：**如何在现有数据约束之外推进AI推理能力？**\n\n## 二、观察与问题聚焦\n\n### 2.1 现有方法局限性观察\n作者观察到：\n- 合成数据生成是潜在解决方案，但 naive 方法产生低信息样本\n- 有效数据合成通常依赖人类专业知识，与自动化目标相悖\n- 简单扩展数据量无法解决模型推理的根本缺陷\n\n### 2.2 相关工作启发\n从LIMO、R-Zero、Crescent等研究中，作者提炼出关键洞见：\n- **数据质量优于数量**：少量精选示例可解锁强大推理能力\n- **失败案例价值**：针对模型弱点的训练比无差别扩展更有效\n- **自我改进潜力**：模型可生成自己的训练数据并从中学习\n\n## 三、核心假设形成\n\n基于以上观察，作者提出三个核心假设：\n1. **小型高效假设**：极少量(<200)高质量对抗性样本可超越大规模数据集训练效果\n2. **自动化生成假设**：可通过完全自动化框架生成此类数据，无需人工筛选\n3. **领域通用假设**：这种框架应跨多个推理领域通用，无需领域特定工程\n\n## 四、方法论设计：从假设到实现\n\n### 4.1 对抗性架构设计\n作者设计了三角色对抗框架：\n- **攻击者(Attacker)**：通过\"概念缝合\"生成边界问题\n- **防御者(Defender)**：尝试解决攻击者生成的问题\n- **裁判(Judge)**：工具增强评估，确保问题质量和答案准确性\n\n### 4.2 关键创新机制\n为解决核心问题，作者引入三个关键机制：\n1. **目标对抗生成**：在模型能力边界合成问题，创造高信息训练信号\n2. **自动化质量保证**：裁判模型使用代码执行和网络搜索验证答案\n3. **嵌入新颖性过滤**：确保问题多样性和解决方案创新性\n\n### 4.3 领域适应性设计\n作者针对不同推理领域设计特定适配：\n- 数学推理：Python代码验证\n- 科学推理：XML结构化答案格式\n- 医学推理：领域特定模型和结构化响应\n\n## 五、实验验证与洞察发现\n\n### 5.1 实验设计\n作者在三个领域验证NanoFlux：\n- GSMHard（数学推理）\n- GenomeBench（科学推理）\n- MultiMedQA（医学推理）\n\n### 5.2 关键发现\n实验结果揭示了三个重要洞察：\n1. **效率与性能突破**：200个NanoFlux样本超越全数据集微调，计算需求减少3-14倍\n2. **非单调关系**：数据集特征与模型性能间存在非单调关系，揭示训练数据优化的复杂张力\n3. **领域特定最佳点**：不同领域在问题复杂性和推理质量上有不同最优配置\n\n## 六、方法论形成与理论贡献\n\n### 6.1 从数据数量到数据质量的范式转变\n基于实验结果，作者提出核心方法论转变：\n- **传统范式**：扩展数据量提升模型性能\n- **NanoFlux范式**：优化数据质量，精确针对模型弱点\n\n### 6.2 理论贡献\n作者形成了三个理论贡献：\n1. **对抗性数据生成理论**：模型间的对抗动态可自动识别并针对推理弱点\n2. **最小高效数据集理论**：存在极小数据集可触发显著性能提升\n3. **领域自适应优化理论**：不同推理领域需要不同的数据复杂性平衡点\n\n## 七、逻辑链总结\n\n从**数据瓶颈**这一宏观问题出发，作者通过观察现有方法局限性，从相关工作中获得启发，形成了**小型高效对抗性数据集**的核心假设。基于此假设，设计了**三角色对抗框架**，并通过实验验证了其有效性。最终，作者形成了**从数据数量扩展转向数据质量优化**的方法论，为解决LLM推理能力提升提供了新思路。\n\n这一逻辑链展示了作者如何从宏观问题出发，逐步聚焦，通过观察、假设、实验验证和洞察分析，最终形成NanoFlux这一创新方法论的完整思考过程。"
                },
                {
                    "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning",
                    "arxiv_id": "2509.23209",
                    "authors": "Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen",
                    "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at https://github.com/Bluixe/towards_monotonic_improvement .",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"上下文价值知情ICRL\"(CV-ICRL)的新方法，用于解决上下文强化学习中的\"上下文歧义\"问题，从而实现智能体性能的单调改进。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是改进序列模型的基础能力，特别是上下文强化学习(ICRL)能力，这是一种让模型能够从过去经验中学习并适应新任务的通用能力。论文提出的CV-ICRL方法是一种新的训练范式，通过引入\"上下文价值\"概念来增强模型的推理和决策能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的要求。 其次，从正面指标看，论文涉及了强化学习(RL)这一训练方法，以及问题解决(problem-solving)这一能力方向。虽然论文没有明确提到\"大语言模型\"这一术语，但其研究的\"large sequence models\"和上下文学习能力与大语言模型的核心特性高度相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文研究的是通用的智能体学习框架，旨在增强智能体的推理和决策能力，而不是将智能体应用于特定领域，这符合保留条件。 综上所述，尽管论文没有明确指出其研究对象是大语言模型，但其研究的上下文强化学习能力是大语言模型的核心能力之一，且论文提出的方法旨在增强模型的通用推理能力，因此符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决In-Context Reinforcement Learning (ICRL)中的性能退化问题，即测试时无法保持训练数据中的单调改进特性。针对ICRL中的Contextual Ambiguity现象，我们提出了一种Context Value Informed ICRL (CV-ICRL)方法，通过引入Context Value作为显式信号来指导策略决策，并在Dark Room和Minigrid环境上通过Average Episode Return和Degradation Frequency等指标验证了其有效性。",
                    "summary_translation": "# 情境强化学习（In-Context Reinforcement Learning, ICRL）的中文翻译\n\n情境强化学习（In-Context Reinforcement Learning, ICRL）已成为一种有前景的范式，用于开发能够通过将过去经验作为情境来快速适应新任务的智能体，而无需更新其参数。最近的方法通过在线强化学习（online RL）中的单调策略改进数据来训练大型序列模型，旨在实现持续改进的测试时性能。然而，我们的实验分析揭示了一个关键缺陷：这些模型在测试期间无法像训练数据那样显示出持续改进。\n\n理论上，我们将这种现象识别为情境模糊性（Contextual Ambiguity），即模型自身的随机动作可能产生一种交互历史，这种历史会误导性地类似于训练数据中的次优策略，从而引发不良动作选择的恶性循环。为解决情境模糊性，我们在训练阶段引入情境价值（Context Value），并提出情境价值知情情境强化学习（Context Value Informed ICRL, CV-ICRL）。CV-ICRL使用情境价值作为显式信号，表示在给定当前情境下策略理论上可达到的理想性能。随着情境的扩展，情境价值可以包含更多任务相关信息，因此理想性能应该是非递减的。我们证明情境价值收紧了相对于理想单调改进策略的性能差距的下界。我们进一步提出了两种在训练和测试时估计情境价值的方法。\n\n在Dark Room和Minigrid测试平台上进行的实验表明，CV-ICRL有效减轻了性能下降，并提高了在各种任务和环境中的整体ICRL能力。本文的源代码和数据可在https://github.com/Bluixe/towards_monotonic_improvement获取。",
                    "inspiration_trace": "# 从问题观察到方法论形成：CV-ICRL的逻辑演进\n\n## 宏观问题：ICRL中的性能退化现象\n\n**起点问题**：为什么在上下文强化学习(ICRL)中，尽管训练数据展示了单调性能改进，但测试时模型无法保持这种改进特性？\n\n## 关键观察：训练与测试的差距\n\n通过实验分析，作者发现了一个核心矛盾：\n- 现有ICRL方法（如Algorithm Distillation）在训练时使用单调改进的策略数据\n- 但在测试阶段，这些模型经常出现性能退化，无法维持训练数据中表现出的单调改进（如图2所示）\n\n这一观察揭示了ICRL领域的一个基础性挑战：训练与测试之间的性能不一致性。\n\n## 问题分析：上下文歧义(Contextual Ambiguity)\n\n作者深入探究了性能退化的根本原因，提出了\"上下文歧义\"概念：\n\n1. **核心机制**：测试时的单次动作采样违反了需要充分采样以平均随机性的隐含假设\n2. **恶性循环**：单个随机的不良动作生成误导性上下文，使其看起来像是来自次优策略的历史，导致模型错误识别自身技能水平\n3. **注意力机制影响**：通过Transformer注意力热图分析，发现模型更关注近期信息，加剧了上下文歧义的影响（如图7所示）\n\n这一分析将问题从表面现象深入到了机制层面，为解决方案提供了理论基础。\n\n## 假设形成：引入明确的质量信号\n\n基于对问题的深入理解，作者形成了核心假设：\n\n> 如果能够为智能体提供一个明确的、无歧义的上下文质量标签，智能体就可以避免从噪声历史交互中进行危险推断，从而打破性能退化的循环。\n\n这一假设引导作者思考如何量化上下文的质量，并确保其具有单调性。\n\n## 理论构建：上下文价值(Context Value)\n\n作者引入了\"上下文价值\"(VC)的理论概念：\n\n1. **定义**：VC = J(π*_C)，表示给定上下文C中信息，策略理论上可实现的最优性能\n2. **关键性质**：VC具有单调性，当上下文扩展时，VC' ≥ VC（性质1）\n3. **理论保证**：引入VC可以收紧学习策略与上下文最优策略之间的性能界限（定理1）\n\n这一理论构建为解决上下文歧义提供了数学基础，将假设转化为可操作的理论框架。\n\n## 方法设计：CV-ICRL算法\n\n基于理论构建，作者设计了Context Value Informed ICRL (CV-ICRL)方法：\n\n1. **核心思想**：将Context Value作为明确信号整合到ICRL过程中\n2. **实践实现**：提出了两种Context Value估计方法：\n   - **CV-ICRL-ϕ(C)**：通过上下文C估计VC，作为Transformer模型的辅助输出头\n   - **CV-ICRL-ϕ(t)**：通过时间步t估计VC，与单调递增变量关联\n\n这一设计将理论转化为可实现的算法，解决了实际应用中的挑战。\n\n## 实验验证：效果评估\n\n作者通过全面的实验验证了方法的有效性：\n\n1. **主要结果**：在Dark Room和Minigrid环境中，CV-ICRL显著降低了性能退化频率，提高了整体性能（表1，图3-4）\n2. **消融实验**：验证了Context Value在上下文中的有效性以及ϕ(t)函数的必要性（图6）\n3. **泛化能力**：首次证明AD-like ICRL方法在Minigrid不同任务类型间的强泛化能力（图5）\n\n实验结果不仅验证了方法的有效性，还揭示了ICRL领域的新可能性。\n\n## 逻辑链条总结\n\n从宏观问题到具体方法，作者的思考路径形成了清晰的逻辑链条：\n\n```\n问题观察(性能退化) → 问题分析(上下文歧义) → 假设形成(明确质量信号) → \n理论构建(上下文价值) → 方法设计(CV-ICRL) → 实验验证(效果评估)\n```\n\n这一逻辑演进不仅解决了ICRL中的具体问题，还为上下文学习领域提供了新的理论视角和方法论框架。通过将抽象问题转化为可操作的概念和算法，作者成功地在理论与实践之间架起了桥梁。"
                },
                {
                    "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse",
                    "arxiv_id": "2509.23183",
                    "authors": "Guohao Chen, Shuaicheng Niu, Deyu Chen, Jiahao Yang, Zitian Zhang, Mingkui Tan, Pengcheng Wu, Zhiqi Shen",
                    "summary": "Test-time entropy minimization helps adapt a model to novel environments and incentivize its reasoning capability, unleashing the model's potential during inference by allowing it to evolve and improve in real-time using its own predictions, achieving promising performance. However, pure entropy minimization can favor non-generalizable shortcuts, such as inflating the logit norm and driving all predictions to a dominant class to reduce entropy, risking collapsed solutions (e.g., constant one-hot outputs) that trivially minimize the objective without meaningful learning. In this paper, we introduce ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time entropy minimization. ZeroSiam prevents collapse through asymmetric divergence alignment, which is efficiently achieved by a learnable predictor and a stop-gradient operator before the classifier. We provide empirical and theoretical evidence that ZeroSiam not only prevents collapse solutions, but also absorbs and regularizes biased learning signals, enhancing performance even when no collapse occurs. Despite its simplicity, extensive results show that ZeroSiam performs more stably over prior methods using negligible overhead, demonstrating efficacy on both vision adaptation and large language model reasoning tasks across challenging test scenarios and diverse models, including tiny models that are particularly collapse-prone.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出ZeroSiam，一种用于测试时熵优化的非对称Siamese架构。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，论文聚焦于测试时熵优化技术，这是一种在推理阶段增强模型能力的方法。摘要明确指出该方法旨在\"释放模型在推理过程中的潜力，允许它实时进化和改进\"，这直接关联到提高LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确提到\"large language model reasoning tasks\"；能力方向上关注\"reasoning capability\"；训练方法上涉及\"evolve and improve in real-time\"的自我进化概念。 第三，虽然论文提到视觉适应任务，但这不是其主要焦点，而是作为方法的一个应用场景。论文的核心是提出一种通用的测试时优化方法，同时应用于视觉适应和LLM推理任务，因此不应因涉及视觉而被排除。 最后，论文提出的防止崩溃的方法有助于提高模型的推理质量和可靠性，这与提升LLM通用推理能力的目标一致。论文关注的是模型在推理时的能力提升，而非特定应用或基础设施优化。 综上所述，该论文致力于通过测试时熵优化技术提高大语言模型的通用推理能力，符合研究目标。",
                    "summary2": "本文旨在解决测试时熵最小化中的模型崩溃问题。针对测试时适应和大型语言模型推理任务，我们提出了一种名为ZeroSiam的高效非对称孪生网络架构，通过可学习预测器和停止梯度操作实现对齐，防止模型崩溃。在ImageNet-C和多个数学推理基准上通过准确率等指标验证了其有效性，实现了更稳定的测试时熵优化。",
                    "summary_translation": "测试时熵最小化（Test-time entropy minimization）有助于使模型适应新环境并激励其推理能力，通过允许模型使用自身预测进行实时演化和改进，释放了模型在推理过程中的潜力，取得了有前景的性能。然而，纯熵最小化可能倾向于不可泛化的捷径，例如膨胀logit范数（logit norm）并将所有预测驱动到主导类别以减少熵，这可能导致坍塌解（collapsed solutions）（例如，恒定的one-hot输出），这些解在没有有意义学习的情况下平凡地最小化了目标。在本文中，我们提出了ZeroSiam，一种专为测试时熵最小化定制的高效非对称孪生网络架构（asymmetric Siamese architecture）。ZeroSiam通过非对称散度对齐（asymmetric divergence alignment）防止坍塌，这种对齐通过分类器前的可学习预测器（learnable predictor）和停止梯度操作符（stop-gradient operator）高效实现。我们提供了经验和理论证据，证明ZeroSiam不仅能防止坍塌解，还能吸收和正则化有偏学习信号，即使在未发生坍塌的情况下也能提高性能。尽管ZeroSiam设计简单，但大量结果表明，相比先前方法，它在使用可忽略开销的情况下表现更稳定，在视觉适应（vision adaptation）和大型语言模型（large language model）推理任务上都展现了有效性，这些任务涵盖了具有挑战性的测试场景和多样化模型，包括特别容易坍塌的小型模型。",
                    "inspiration_trace": "# 从宏观问题到创新方法：ZeroSiam的逻辑演进\n\n## 1. 宏观问题：如何让模型在测试时适应新环境？\n\n**观察**：在实际应用中，测试数据往往与训练数据分布不同（分布偏移）。模型需要能够适应这些新环境以提高性能，但测试时通常没有真实标签可用。\n\n**聚焦**：如何设计一种无监督的测试时适应机制，让模型能够利用测试数据本身进行自我调整？\n\n## 2. 熵最小化：有前景的适应策略\n\n**假设**：熵作为预测不确定性的度量，最小化预测熵可以鼓励模型对测试数据做出更自信的预测，从而适应新环境。\n\n**初步验证**：前期工作（如Tent）确实证明了测试时熵最小化的有效性——模型仅通过最小化自身预测熵就能适应分布偏移。\n\n## 3. 发现关键问题：熵最小化导致的崩溃现象\n\n**深入观察**：纯粹的熵最小化会导致模型崩溃（collapse），表现为：\n- 模型增大logit范数来减少熵\n- 将所有预测推向一个主导类别\n- 最终退化为恒定的one-hot输出\n\n**问题本质**：熵最小化自然地驱动模型增加最大预测logit，而不考虑它是否与真实标签一致。在测试数据嘈杂或有分布偏移的情况下，这种机制会导致模型找到\"捷径解\"——虽然最小化了熵，但没有实现有意义的学习。\n\n## 4. 现有方法的局限性分析\n\n**评估当前解决方案**：现有方法（如SAR、EATA、DeYO）主要依赖启发式阈值来过滤不可靠的梯度。\n\n**发现局限**：\n- 阈值难以定义且在不同领域间难以泛化\n- 只能部分过滤噪声梯度，无法从根本上解决问题\n- 模型仍然面临崩溃风险，特别是在长期或更具挑战性的测试场景中\n\n**核心挑战**：需要一种更根本的解决方案，从架构设计上防止崩溃，而不仅仅是缓解症状。\n\n## 5. 跨领域灵感：自监督学习中的非对称设计\n\n**关键洞察**：自监督学习（SSL）中的负样本无关方法（如BYOL、SimSiam）通过非对称结构防止两个分支崩溃到相同的常数。\n\n**原理分析**：这些方法使用一个非对称设计（一个分支有预测器，另一个分支有stop-gradient），确保退化常数输出会产生非零对齐损失，从而防止两个分支崩溃到相同的常数。\n\n**创新假设**：这种非对称机制可能也适用于熵优化，防止不需要的平凡解，稳定自训练过程。\n\n## 6. 核心挑战：将非对称设计应用于熵最小化\n\n**识别障碍**：直接将SSL中的非对称设计应用到测试时熵最小化面临三大挑战：\n1. 测试时熵最小化通常只有一个预测分支，优化的是熵而不是相似性\n2. 传统孪生设计需要额外的骨干网络传递，影响测试时学习的效率\n3. 测试时学习需要轻量级设计，适合在线部署\n\n**突破思路**：如何在单次前向传播中嵌入非对称性，而不需要额外的骨干网络传递或数据增强？\n\n## 7. 解决方案：ZeroSiam的高效非对称孪生架构\n\n**核心创新**：在分类器之前插入一个预测器和stop-gradient操作符，将一个预测解耦为两个基于相同特征的非对称输出：\n- 在线分支（online branch）：通过可学习的预测器，用于优化熵\n- 目标分支（target branch）：原始logits，使用stop-gradient进行对齐\n\n**目标函数**：结合在线分支的熵最小化和与目标分支的对齐正则化：\n```\nL = H(po) + α D(po ∥ sg[pr])\n```\n其中H(p)是预测熵，D(·∥·)是散度（如ℓ2或KL），sg[·]表示stop-gradient。\n\n**工作原理**：\n1. 预测器初始化为身份映射，确保热启动\n2. 在线分支最小化熵以学习判别特征\n3. 目标分支通过stop-gradient进行非对称预测器-目标对齐，防止崩溃的常数解\n4. 预测器快速偏离身份映射，创建防止崩溃所需的非对称性\n\n## 8. 理论与实证验证\n\n**理论分析**：证明预测器h作为过滤机制，抑制对应于过度放大的logits的梯度更新方向，系统收敛到稳定平衡：存在hmin > 0使得H(po) > hmin且po → pr。\n\n**实证观察**：\n1. **预测器作为有偏学习信号的有效吸收器**：预测器参数θh在面对更高标签不平衡比率的数据流时更快速、更显著地偏离身份映射，主动吸收有偏学习信号。\n2. **非对称性抑制学习非泛化崩溃模式**：ZeroSiam抑制logit L2范数的快速增长和主导模式的出现，而Tent则受这些问题困扰。\n\n**关键发现**：ZeroSiam不仅能防止崩溃，还能吸收和调节有偏的学习信号，即使在没有崩溃的情况下也能提高性能。\n\n## 9. 效率优势与应用验证\n\n**效率优势**：ZeroSiam只需要一个单次前向传播，不需要数据增强、额外的骨干网络传递或教师模型，计算开销与原始Tent方法相当。\n\n**广泛验证**：在视觉适应（ImageNet-C）和大语言模型推理任务（Math-500、CollegeMath等）上的实验证明，ZeroSiam在各种挑战性测试场景和多样化模型（包括特别容易崩溃的小模型）上表现更稳定。\n\n**鲁棒性测试**：即使在极端情况下（如仅在错误标签上适应、纯噪声数据适应），ZeroSiam仍能保持稳定性能，而现有方法往往会崩溃。\n\n## 10. 逻辑演进总结\n\n从\"如何让模型在测试时适应新环境\"这一宏观问题出发，通过聚焦熵最小化策略，发现崩溃现象这一关键挑战，分析现有方法的局限性，从自监督学习中汲取非对称设计的灵感，克服将非对称设计应用于熵最小化的技术障碍，最终提出ZeroSiam这一高效非对称孪生架构，并通过理论与实证验证其有效性。整个过程体现了从观察、假设到形成最终方法论的清晰思考路径，解决了测试时熵最小化的根本性崩溃问题。"
                },
                {
                    "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers",
                    "arxiv_id": "2509.23152",
                    "authors": "Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang",
                    "summary": "Test-time scaling via solution sampling and aggregation has become a key paradigm for improving the reasoning performance of Large Language Models (LLMs). While reward model selection is commonly employed in this approach, it often fails to identify minority-yet-correct answers, which limits its effectiveness beyond that of simple majority voting. We argue that this limitation stems from a lack of informative critique signals during verifier training. To bridge this gap, we introduce Mirror-Critique, a framework that trains a verifier with informative critiques. Our key insight is to leverage the rich critique signal by contrasting model-generated solutions with ground-truth solutions. We deploy a small instruction-tuned model to synthesize high-quality critique data with rejection sampling that teaches the verifier not only what is wrong, but also why. The synthetic data is used to cold-start the LLMs in the RLVR process to further improve the verification ability. The resulting Mirror-Verifier is deployed to evaluate candidate solutions by generating multiple critiques per solution, aggregating them into a verify score used for weighted voting or selective abstention. The experimental results show that our Mirror-Verifier significantly outperforms majority voting in terms of solution accuracy and also improves the solver's honesty to recognize and abstain from answering beyond its capability boundaries.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出Mirror-Critique框架，通过训练带有信息性批判的验证器来提高大语言模型的推理性能。论文的核心贡献是改进LLM的基础推理能力，提出新的训练范式（使用RLVR过程），并增强模型的多步推理和自我评估能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确讨论Large Language Models (LLMs) - 能力方向：专注于reasoning能力的提升，特别是通过验证器增强模型的逻辑推理 - 训练方法：使用了强化学习(RLVR过程)来训练验证器 - 论文虽未明确提及新兴范式，但其验证机制可视为一种增强LLM通用能力的工具使用方法 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 虽然提到\"honesty\"，但这是指模型识别自身能力边界的内在可靠性，而非应用层面的水印、安全等问题 第四步：特殊和模糊情况处理 论文提出的验证器可以视为一种通用工具，用于增强LLM的推理能力，而非针对特定领域的应用。论文关注的是提高模型的推理准确性和诚实性，这与减少幻觉和提高模型内在可靠性相关，属于提升模型通用推理能力的范畴。 综上所述，这篇论文明确致力于提高大语言模型本身的通用推理能力，通过创新的验证器训练方法和批判信号利用，增强了模型的逻辑推理和问题解决能力，完全符合研究课题的目标。",
                    "summary2": "本文旨在解决测试时扩展中验证器无法识别少数但正确答案的问题。针对LLMs生成的解决方案，我们提出了一种Mirror-Critique框架，通过对比模型生成与真实解决方案合成高质量批判数据训练验证器，并在多个数学推理benchmark上通过准确率和诚实度分数验证了其有效性。",
                    "summary_translation": "通过解决方案采样和聚合实现的测试时扩展（Test-time scaling）已成为提高大型语言模型（Large Language Models, LLMs）推理性能的关键范式。虽然奖励模型选择（reward model selection）在此方法中通常被采用，但它往往无法识别少数但正确的答案，这限制了其效果，使其仅优于简单多数投票（simple majority voting）。我们认为，这一限制源于验证器（verifier）训练过程中缺乏信息丰富的批判信号（informative critique signals）。为了弥合这一差距，我们引入了Mirror-Critique（镜像批判），这是一个通过信息丰富的批判来训练验证器的框架。我们的关键见解是通过对比模型生成的解决方案和真实解决方案（ground-truth solutions）来利用丰富的批判信号。我们部署了一个经过指令微调的小型模型（instruction-tuned model），通过拒绝采样（rejection sampling）合成高质量的批判数据，这些数据不仅教导验证器什么是错误的，还教导它为什么错误。这些合成数据被用于在RLVR过程中冷启动（cold-start）大型语言模型，以进一步提高验证能力。由此产生的Mirror-Verifier（镜像验证器）被用于评估候选解决方案，通过为每个解决方案生成多个批判，并将它们聚合成一个验证分数（verify score），该分数用于加权投票（weighted voting）或选择性弃权（selective abstention）。实验结果表明，我们的Mirror-Verifier在解决方案准确性方面显著优于多数投票，并且提高了解决器（solver）的诚实性，使其能够识别并避免回答超出其能力范围的问题。",
                    "inspiration_trace": "# Mirror-Critique方法论逻辑推演\n\n## 一、宏观问题观察\n\n**起点现象**：测试时扩展(test-time scaling)通过解决方案采样和聚合已成为提高LLMs推理性能的关键范式。然而，现有的奖励模型选择方法往往无法识别少数但正确的答案，限制了其有效性，无法超越简单多数投票的性能。\n\n**核心问题**：为什么现有验证方法无法有效识别少数但正确的解决方案？这限制了测试时扩展的潜力。\n\n## 二、问题深入分析\n\n**根本原因识别**：作者指出这一限制源于验证器训练过程中缺乏信息性的批判信号(critique signals)。传统验证器训练通常仅依赖二元标签(正确/错误)，无法提供关于解决方案为何成功或失败的充分解释。\n\n**现有方法局限**：\n1. 二元标签无法解释错误原因\n2. 高质量批判数据需从封闭源模型获取，成本高昂\n3. RLVR(可验证奖励的强化学习)在训练验证器方面的潜力尚未充分探索\n\n## 三、核心假设形成\n\n**关键洞察**：如果能够提供丰富的批判信号，教导验证器不仅判断正确性，还能理解背后的推理差距，那么验证器的性能将显著提高。\n\n**核心假设**：通过对比模型生成的解决方案与真实解决方案，可以合成高质量的批判数据，这些数据能够教导验证器理解错误原因，而不仅仅是标记错误。\n\n## 四、方法论设计\n\n### 1. 数据合成策略\n\n**创新思路**：设计低成本、自包含的批判数据合成管道，不依赖封闭源模型。\n\n**具体实现**：\n- 使用RLVR训练基础求解器，记录解决方案轨迹\n- 过滤冗余轨迹，保留有价值数据\n- 设计特定模板指导小型指令模型生成批判\n- 应用拒绝采样(rejection sampling)确保批判质量\n\n### 2. 验证器训练方法\n\n**冷启动问题解决**：认识到基础模型缺乏批判能力，直接进行强化学习效果不佳，因此先使用合成数据进行监督微调(SFT)作为冷启动。\n\n**数据平衡处理**：发现合成数据存在类别不平衡(负样本过多)，导致奖励 hacking问题，因此采用平衡采样策略，特别关注少数正确样本。\n\n**RLVR优化**：使用平衡数据集进行强化学习，进一步优化验证器能力。\n\n### 3. 测试时应用机制\n\n**多批判生成**：为每个候选解决方案生成多个独立批判，提高评估可靠性。\n\n**验证分数计算**：将多个批判聚合为验证分数，反映解决方案的可信度。\n\n**双重决策机制**：\n- 加权投票：基于验证分数选择最终答案，提高准确性\n- 选择性弃权：当所选答案的平均验证分数低于阈值时弃权，增强诚实性\n\n## 五、实验验证与效果确认\n\n**评估指标设计**：\n- 准确性：衡量模型生成正确答案的能力\n- 诚实性分数：新提出的指标，同时考虑正确性和提供错误信息的危害\n\n**实验结果验证**：\n- Mirror-Verifier在多个数学推理基准测试上显著优于基线方法\n- 能够有效识别少数但正确的解决方案\n- 在超出能力范围时能够准确弃权，提高系统诚实性\n\n## 六、贡献总结\n\n**方法论贡献**：提出了Mirror-Critique框架，通过合成丰富的批判数据训练验证器，解决了传统验证器训练中信息不足的问题。\n\n**性能提升**：在测试时扩展中实现显著准确性提升，同时提高模型诚实性，使其能够识别自身能力边界并适当弃权。\n\n**技术价值**：提供了一种自包含、低成本的验证器训练方法，不依赖封闭源模型，具有广泛适用性。\n\n这一逻辑链条展示了作者从观察测试时扩展的局限性，到分析问题根源，形成核心假设，设计创新解决方案，并通过实验验证其有效性的完整思考过程。"
                },
                {
                    "title": "On the Capacity of Self-Attention",
                    "arxiv_id": "2509.22840",
                    "authors": "Micah Adler",
                    "summary": "While self-attention is known to learn relations among tokens, we lack a formal understanding of its capacity: how many distinct relations can a single layer reliably recover for a given budget? To formalize this, we introduce Relational Graph Recognition (RGR), where the key-query channel represents a graph on $m$ items with $m'$ directed edges, and, given a context of items, must recover the neighbors of each item. We measure resources by the total key dimension $D_K = h\\,d_k$. Within this framework, we analytically derive a capacity scaling law and validate it empirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary (information-theoretic lower bound) and sufficient (explicit construction) in a broad class of graphs to recover $m'$ relations. This scaling law directly leads to a new, capacity-based rationale for multi-head attention that applies even when each item only attends to a single target. When embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a permutation, a single head suffices. However, compression ($m > d_{\\text{model}}$) forces relations into overlapping subspaces, creating interference that a single large head cannot disentangle. Our analysis shows that allocating a fixed $D_K$ across many small heads mitigates this interference, increasing the number of recoverable relations. Controlled single-layer experiments mirror the theory, revealing a sharp performance threshold that matches the predicted capacity scaling and confirms the benefit of distributing $D_K$ across multiple heads. Altogether, these results provide a concrete scaling law for self-attention capacity and a principled design rule for allocating key-query budget across heads.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究自注意力机制（大语言模型的核心组件）的基础能力和理论限制，提出了关系图识别(RGR)框架，并推导出自注意力机制的能力扩展定律。虽然论文没有直接讨论LLM的推理能力，但它深入研究了LLM的核心组件（自注意力机制）的能力限制，这对于理解和提高LLM的通用推理能力具有重要意义。论文不属于任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它关注的是模型的基础能力和理论理解，这符合\"改进LLM的基础能力\"的研究目标。通过更好地理解自注意力机制的能力限制，我们可以设计更有效的模型架构，从而提高LLM的通用推理能力。论文还提供了多头注意力的新理论基础，解释了为什么分配固定维度到多个小头比单个大头更有优势，这对于优化LLM架构以提高其推理能力具有直接指导意义。",
                    "summary2": "本文旨在 [解决自注意力机制的容量问题，即对于给定预算，单层自注意力能够可靠学习多少不同关系]。针对 [关系图识别场景]，我们提出了一种 [关系图识别框架和容量缩放定律]，并在 [理想化的单层自注意力模型] 上通过 [准确率指标] 验证了其有效性。",
                    "summary_translation": "尽管自注意力机制(self-attention)被公认为能够学习标记(tokens)之间的关系，但我们对其容量(capacity)缺乏正式理解：在给定预算(budget)的情况下，单个层(layer)能够可靠地恢复多少不同的关系？为形式化这一问题，我们引入了关系图识别(Relational Graph Recognition, RGR)框架，其中键-查询通道(key-query channel)表示一个包含$m$个项(items)和$m'$条有向边(directed edges)的图，并且在给定项(items)上下文(context)的情况下，必须恢复每个项的邻居。我们通过总键维度(total key dimension) $D_K = h\\,d_k$来衡量资源(resources)。在此框架内，我们解析推导(analytically derive)了一个容量扩展定律(capacity scaling law)并对其进行了实证验证(validate empirically)。我们证明，在广泛的图类别(broad class of graphs)中，$D_K = \\Theta(m' \\log m' / d_{\\text{model}})$对于恢复$m'$个关系既是必要的(信息论下界，information-theoretic lower bound)，也是充分的(显式构造，explicit construction)。该扩展定律(scaling law)直接导出了多头注意力(multi-head attention)的一种新的、基于容量的基本原理(capacity-based rationale)，即使当每个项(item)仅关注(attends to)单个目标(target)时也适用。当嵌入(embeddings)未压缩(uncompressed)($m = d_{\\text{model}}$)且图是排列(permutation)时，单头(single head)就足够了。然而，压缩(compression)($m > d_{\\text{model}}$)迫使关系进入重叠子空间(overlapping subspaces)，产生干扰(interference)，而单个大头(single large head)无法分离(disentangle)这种干扰。我们的分析表明，将固定的$D_K$分配到多个小头(small heads)可以减轻(mitigates)这种干扰，从而增加可恢复关系(recoverable relations)的数量。受控的单层实验(Controlled single-layer experiments)印证了理论，揭示了一个明显的性能阈值(sharp performance threshold)，该阈值与预测的容量扩展(predicted capacity scaling)相匹配，并证实了将$D_K$分配到多个头(distributing $D_K$ across multiple heads)的好处。总体而言，这些结果为自注意力容量(self-attention capacity)提供了一个具体的扩展定律(concrete scaling law)，并为在头(heads)之间分配键-查询预算(key-query budget)提供了有原则的设计规则(principled design rule)。",
                    "inspiration_trace": "# 自注意力容量分析的逻辑演进\n\n## 1. 宏观问题：自注意力的能力边界\n\n**观察**：自注意力机制已成为现代深度学习的核心组件，在NLP、CV等领域取得革命性成功。然而，我们缺乏对其基本能力的形式化理解——特别是，一个固定大小的自注意力机制到底能处理多少关系？\n\n**核心问题**：对于一个给定的自注意力机制，单层能够表示并可靠恢复多少目标关系？这被称为该层的\"容量\"(capacity)。\n\n## 2. 问题聚焦：形式化\"关系容量\"\n\n**观察**：直接探测大型训练模型中的容量是困难的，因为现代transformer在共享子空间中叠加了多种关系，且注意力权重不一定与因果重要性一致。\n\n**假设**：如果能创建一个可控框架，使我们能够明确控制关系的结构和数量，同时保持自注意力的计算约束和对称性，就可以对容量进行原则性分析。\n\n**方法**：提出\"关系图识别\"(Relational Graph Recognition, RGR)框架\n- 将自注意力建模为图恢复问题：在m个项目上恢复m'条有向边\n- 键-查询通道表示图结构，给定项目上下文，必须恢复每个项目的邻居\n- 资源测量通过总键维度DK = h dk（头数×每头键维度）\n\n## 3. 理论分析：容量的上下界\n\n### 信息论下界\n\n**假设**：自注意力机制恢复关系的能力受信息论限制，容量应随关系数量m'和项目数量m增长，但随嵌入维度dmodel增加而减少。\n\n**推导**：证明恢复m'条边的图需要\n```\nDK = Ω((m'/dmodel) log(m²/m'))\n```\n这表明键-查询矩阵需要表达足够的信息来描述底层关系图。\n\n### 构造性上界\n\n**假设**：可以通过显式构造注意力权重来解决RGR问题，达到接近信息论下界的性能。\n\n**构造**：提出四种构造算法，从简单到复杂：\n1. 置换图与one-hot嵌入（热身）\n2. 压缩嵌入下的置换图\n3. 更一般的嵌入\n4. 一般图\n\n**结果**：实现\n```\nDK = O((m'+Δ) log m'/dmodel)\n```\n其中Δ是图的最大度数。在温和条件下(Δ/davg ≤ m/dmodel)，这闭合了上下界之间的差距。\n\n## 4. 多头注意力的新解释\n\n**观察**：传统观点认为多头注意力允许一个源概念关注多个不同目标。但实验发现，即使每个源只关注单个目标，多头注意力也有优势。\n\n**假设**：当使用压缩嵌入(dmodel ≪ m)时，许多关系必须存储在重叠的子空间中；将自注意力预算分配到多个小头可以减少干扰，增加可清晰分离的关系数量。\n\n**分析**：\n- 无压缩情况下(dmodel = m)，单头足够\n- 压缩设置下，单头产生太多噪声\n- 多头通过减少块大小B来局部化解嵌入噪声，使每个头只在较少坐标上聚合泄漏\n\n**核心机制**：噪声项N3(B) ≍ B/dmodel √(dk log m)，因此多头通过减小B来控制噪声。\n\n## 5. 实验验证\n\n**假设**：理论预测的容量缩放定律应在实验中得到验证，且多头优势应在置换图中明显。\n\n**实验设计**：\n- 在理想化自注意力设置中实验，反映理论模型\n- 任务：置换图（每个节点一个出/入边）\n- 节点嵌入：高斯单位范数向量，冻结使DK成为唯一容量旋钮\n\n**关键结果**：\n1. **尖锐性能阈值**：F1在狭窄DK窗口中急剧转变\n2. **容量缩放定律**：D*K ≈ 1.19 · m log m/dmodel (R² = 0.944)\n3. **多头优势**：最优头数h* ≈ 1.65 · m/dmodel - 6.64 (R² = 0.824)\n4. **上下文长度不变性**：容量阈值在ℓ ∈ {16, 32}范围内基本稳定\n\n## 6. 限制与未来方向\n\n**识别限制**：\n- 构造性上界仅在温和度偏斜假设下紧密\n- 充分性结果依赖于嵌入的几何属性\n- 实验仅限于置换图，未测试更大出度图\n\n**未来方向**：\n- 量化学习模型中的嵌入如何满足理论假设\n- 扩展实验到更一般的图类\n- 研究更长上下文长度的影响\n- 探索多层数字注意力中的容量累积\n\n## 总结\n\n本文从\"自注意力机制的能力边界是什么\"这一宏观问题出发，通过RGR框架将问题形式化为图恢复任务，建立了自注意力容量的缩放定律，并揭示了多头注意力即使在单目标情况下也能通过减少干扰提高容量的新机制。理论分析与实验验证紧密结合，形成了一套完整的自注意力容量理论，为理解和优化自注意力机制提供了新的理论基础。"
                },
                {
                    "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective",
                    "arxiv_id": "2509.22921",
                    "authors": "Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar",
                    "summary": "We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究范围，核心理由如下： 首先，从本质上看，论文的核心是关于改进LLM的基础能力，提出了一种新的训练/优化范式。论文将大语言模型蒸馏问题构建为约束强化学习问题，这是一种新的方法论研究，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标： 1. 核心概念：明确关注大语言模型(LLM)的蒸馏问题 2. 能力方向：论文在数学推理任务上进行了实验，并证明其方法能提高模型的推理能力 3. 训练方法：使用了强化学习方法来优化蒸馏过程，属于新的训练范式探索 第三，论文不符合任何排除标准。虽然论文在数学推理任务上进行了实验，但数学推理被视为通用能力而非特定应用领域。论文的核心是提出一种通用的蒸馏方法，而不是专注于某个特定领域的应用。 论文的核心贡献是提出了一种原则性的优化框架，通过约束强化学习来改进LLM蒸馏过程，从而提高模型的推理能力。这种方法旨在解决如何更有效地从教师模型中提取知识，特别是关注推理能力的保持和提升，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。",
                    "summary2": "本文旨在解决大语言模型蒸馏中任务特定奖励与教师模型保真度平衡的问题。针对数学推理任务，我们提出了一种基于约束强化学习的蒸馏方法，将蒸馏转化为在KL散度约束下最大化任务奖励的优化问题，并在GSM8K和MATH数据集上通过最终答案正确性、推理质量和约束满足率等指标验证了其有效性。",
                    "summary_translation": "我们提出了一种新颖的大型语言模型（LLM，Large Language Model）蒸馏方法，将其表述为一个约束强化学习（constrained reinforcement learning）问题。尽管近期工作已开始探索将特定任务奖励（task-specific rewards）整合到蒸馏过程中，但现有方法通常依赖于临时奖励权重（ad-hoc reward weighting）。我们提出了一个原则性优化框架，该框架在最大化特定任务奖励的同时，约束与教师模型（teacher model）的差异（divergence）保持在指定阈值以下。我们的方法将约束状态增强强化学习（constrained state augmented reinforcement learning）适应于蒸馏设置，引入了一种修改后的奖励函数（reward function），该函数保持了约束满足（constraint satisfaction）的理论保证，而不需要在部署期间进行状态增强（state augmentation）或访问教师模型，且没有对偶拉格朗日方法（dual Lagrangian methods）的计算开销。通过对数学推理任务（mathematical reasoning tasks）的广泛实验，我们证明了与软拉格朗日松弛基线（soft Lagrangian relaxation baselines）相比，我们的方法实现了更好的约束满足率和更好的推理能力，同时保持了有竞争力的任务性能。我们的框架为资源受限环境（resource-constrained settings）下的奖励感知蒸馏（reward-aware distillation）提供了一个有理论依据且实际高效的解决方案。",
                    "inspiration_trace": "# 大语言模型蒸馏方法的逻辑演进：从约束马尔可夫决策过程视角\n\n## 一、宏观问题：大语言模型蒸馏的困境\n\n大语言模型(LLM)在各类任务中表现出色，但其庞大的规模和计算需求使其难以在资源受限环境中部署。知识蒸馏作为解决方案，旨在将大型教师模型的知识转移到小型学生模型中。然而，作者观察到传统蒸馏方法面临一个根本性挑战：**如何在保持教师模型推理能力的同时，优化学生模型在特定任务上的表现？**\n\n## 二、观察与问题分析\n\n### 1. 传统蒸馏方法的局限性\n作者首先观察到现有蒸馏方法主要关注最小化学生与教师模型间的KL散度，但这存在两个关键问题：\n- **次优学习问题**：仅关注KL散度会迫使学生模仿超出其能力的复杂推理路径，而非发现更简单但同样有效的推理策略\n- **奖励信号缺失**：忽略任务特定奖励信号，无法有效指导学生在复杂推理任务中的学习\n\n### 2. 现有混合方法的不足\n作者进一步分析了结合奖励信号与KL散度的现有方法（如引入超参数λ平衡两者），发现：\n- **超参数敏感性问题**：最佳λ难以预测，需要针对特定任务进行大量重新训练\n- **训练不稳定**：不同训练阶段可能需要不同的λ值，导致计算成本高昂且不稳定\n\n## 三、新视角：约束强化学习的引入\n\n### 1. 核心洞察\n作者提出一个关键视角转变：**将LLM蒸馏视为约束强化学习问题**，而非简单的正则化优化问题。具体表述为：\n- **目标**：最大化任务特定奖励\n- **约束**：学生与教师模型间的散度不超过预定义阈值d\n\n### 2. 相对优势\n这种表述相比传统方法具有显著优势：\n- **直观性**：阈值d直接以KL尺度表示，比调整λ更简单直观\n- **目标简化**：当学生足够接近教师时，目标自然简化为纯奖励最大化\n\n## 四、方法学挑战与突破\n\n### 1. 现有约束RL方法的局限性\n作者发现直接应用现有约束RL方法（如状态增强方法Saute）到蒸馏中存在问题：\n- **部署依赖**：需要在测试时访问教师模型计算每步约束，违背蒸馏初衷\n- **计算开销**：维护增强状态变量带来额外计算负担\n\n### 2. 关键假设与创新\n基于LLM的特性，作者提出一个关键假设：**在LLM蒸馏中，状态包含完整历史信息，因此增强状态是冗余的**。基于此假设，作者提出创新方法：\n- **去除状态增强**：利用完整历史信息重构剩余预算，消除对教师模型的测试时依赖\n- **修改奖励函数**：设计新的奖励函数，在满足约束时提供任务奖励，违反约束时施加差异化惩罚\n\n### 3. 理论保证\n作者通过严格理论分析证明：\n- **最优等价性**：无状态增强MDP与增强MDP的最优值函数等价\n- **约束满足**：当惩罚参数n→∞时，最优策略几乎必然满足约束\n- **Bellman最优性**：在标准假设下，Bellman方程成立\n\n## 五、实验验证与效果评估\n\n### 1. 实验设计\n作者在数学推理任务上进行系统实验，对比多种基线方法：\n- 纯奖励优化(GRPO)\n- 纯KL最小化(GKD, Mini-LLM)\n- 混合方法(GKD-GRPO)\n\n### 2. 关键发现\n实验结果证实了作者方法的有效性：\n- **平衡性能**：在推理质量和约束满足率方面优于基线，同时保持有竞争力的任务性能\n- **推理质量**：约束学生接近教师分布有助于保留和转移教师的推理能力\n- **奖励价值**：任务特定奖励对学生学习有效推理策略至关重要\n\n## 六、结论与意义\n\n作者的工作通过将LLM蒸馏重新表述为约束强化学习问题，成功解决了传统方法的核心挑战：\n1. **理论贡献**：提供了原则性框架，消除了对临时奖励加权的依赖\n2. **实践价值**：实现了无需教师访问的部署，同时保持约束满足的理论保证\n3. **应用前景**：为创建更小、可靠且专业的模型提供了新途径，这些模型可在教师模型的定义信任区域内可靠运行\n\n这一工作不仅解决了LLM蒸馏中的具体问题，也为约束优化在大型模型中的应用提供了新思路，展示了跨领域理论迁移的创新价值。"
                },
                {
                    "title": "In-Context Learning can Perform Continual Learning Like Humans",
                    "arxiv_id": "2509.22764",
                    "authors": "Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding",
                    "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing \"sweet spot\" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)能力，并将其扩展为上下文持续学习(ICCL)，提出了一种新的学习范式。从第一步核心判断来看，这属于改进LLM基础能力和提出新训练范式的研究，符合保留标准。持续学习能力是通用推理能力的重要组成部分，涉及到知识积累、记忆保留和跨任务学习等能力，这些都是通用推理的基础要素。论文没有将LLM作为工具应用到特定领域，而是关注LLM本身的能力提升机制。从第二步正面指标看，论文明确包含LLMs核心概念，虽然未直接提及reasoning、planning等术语，但持续学习本身就是一种通用问题解决能力。从第三步排除标准看，论文未涉及多模态、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在 [解决大型语言模型在持续学习中的知识积累和记忆保留问题]。针对 [多任务顺序到达的场景]，我们提出了一种 [上下文持续学习(ICCL)方法，通过任务调度和提示重排实现无参数更新的持续学习]，并在 [Markov-Chain基准测试] 上通过 [标准化性能指标和人类保留相似性指标(HRS-MD)] 验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）可以通过上下文学习（in-context learning, ICL）适应新任务，而无需参数更新，这使它们成为快速适应的强大学习引擎。虽然大量研究已将ICL作为少样本学习器（few-shot learner）进行考察，但当多任务顺序到达时，它是否能实现长期保留（long-term retention）和跨任务知识积累（cross-task knowledge accumulation）仍未得到充分探索。受人类记忆研究的启发，我们研究了多任务环境中ICL的保留特性，并将其扩展到上下文持续学习（in-context continual learning, ICCL），其中持续学习能力通过任务调度（task scheduling）和提示重排（prompt rearrangement）而出现。在马尔可夫链基准测试（Markov-Chain benchmarks）上的实验表明，对于特定的大型语言模型，ICCL以类似于人类的方式从分布式练习（distributed practice, DP）中获益，持续揭示了保留的间隔\"最佳点\"（spacing \"sweet spot\"）。除了保留性能外，我们还提出了一种人类保留相似性度量（human-retention similarity metric），用于量化持续学习（continual-learning, CL）方法与人类保留动态（retention dynamics）的接近程度。使用该度量，我们表明线性注意力模型（linear-attention models）如MAMBA和RWKV表现出特别类似人类的保留模式，尽管它们的保留性能落后于基于Transformer的大型语言模型（Transformer-based LLMs）。总体而言，我们的结果确立了ICCL既在认知上合理（cognitively plausible）又在实践上有效，提供了一种仅推理的CL范式（inference-only CL paradigm），减轻了灾难性遗忘（catastrophic forgetting），并解决了传统CL方法中的稳定性-可塑性困境（stability-plasticity dilemma）。",
                    "inspiration_trace": "# 作者提出ICCL方法的逻辑链推演\n\n## 宏观问题：LLMs的持续学习能力\n\n作者从大语言模型(LLMs)的一个核心特性出发：**上下文学习(ICL)**能力，即模型无需参数更新就能通过提示中的示例适应新任务。然而，作者观察到现有研究主要关注ICL的单任务适应能力，忽视了其在多任务序列场景中的长期保留和知识积累潜力。\n\n> **核心问题**：ICL能否实现跨任务的持续学习，像人类一样长期保留并积累知识？\n\n## 关键观察：从人类记忆研究中获得启发\n\n作者从两个关键观察中获得了思路：\n\n1. **人类记忆的分布式练习效应**：认知科学研究表明，分布式练习(DP)比集中练习(MP)产生更好的长期记忆保留，存在一个\"间隔甜点\"。\n\n2. **传统持续学习的局限性**：基于梯度的持续学习方法(GBCL)面临灾难性遗忘(CF)和稳定性-可塑性困境，需要参数更新或显式记忆缓冲区。\n\n> **核心洞察**：ICL的\"无参数更新\"特性可能使其成为解决传统持续学习困境的新范式。\n\n## 假设形成：ICCL的认知可行性\n\n基于上述观察，作者提出了核心假设：\n\n1. **基础假设**：ICL可以通过任务调度和提示重排扩展为持续学习(ICCL)，使持续学习能力自然涌现。\n\n2. **具体假设**：\n   - ICCL会像人类一样从分布式练习中受益\n   - 可能存在一个\"间隔甜点\"使保留效果最大化\n   - 不同LLM架构的保留动态可能与人类记忆有不同程度的相似性\n\n## 方法论构建：从形式化到实验设计\n\n作者构建了一个完整的方法论体系：\n\n### 1. 形式化ICCL框架\n- 定义任务表示和历史经验序列\n- 引入**任务标识符**概念，用于区分不同任务但无需语义描述\n- 提出保留评估方法，量化模型对目标任务的记忆程度\n\n### 2. 设计调度策略\n- **单次练习(SP)**：目标任务只呈现一次\n- **集中练习(MP)**：目标任务连续重复呈现\n- **分布式练习(DP)**：交替呈现目标任务和其他任务\n\n### 3. 建立人类相似性度量\n- 基于ACT-R认知模型，提出**HRS-MD度量**，量化模型保留动态与人类记忆的相似性\n- 使用马氏距离比较模型拟合参数与人类参考分布的差异\n\n### 4. 设计实验验证\n- 使用**离散马尔可夫链**作为基准任务，减少预训练知识干扰\n- 评估不同类型LLMs(Transformer-based和线性注意力模型)和GBCL基线的保留性能\n- 分析不同调度策略和任务标识符对保留性能的影响\n\n## 验证与发现：实验结果支持假设\n\n通过实验，作者验证了其假设并获得了重要发现：\n\n1. **ICCL的有效性**：\n   - ICCL在保留性能上显著优于GBCL基线\n   - DP对ICCL的益处类似于对人类记忆的益处，确实存在\"间隔甜点\"\n\n2. **任务标识符的作用**：\n   - 任务标识符显著提高了ICCL的保留性能，特别是在复杂任务中\n\n3. **模型差异**：\n   - 线性注意力模型(MAMBA和RWKV)表现出最接近人类的保留模式\n   - 尽管这些模型的绝对保留性能落后于基于Transformer的LLMs\n\n## 结论与意义：建立认知合理的持续学习新范式\n\n作者得出结论：ICCL既在认知上合理，又在实践上有效，提供了一种仅推理的持续学习范式，解决了传统持续学习方法中的稳定性-可塑性困境。\n\n这一研究不仅为LLMs的上下文学习提供了新的应用方向，还建立了机器学习与人类认知科学之间的桥梁，为设计更符合人类认知模式的AI系统提供了新思路。"
                },
                {
                    "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning",
                    "arxiv_id": "2509.25052",
                    "authors": "Sai Wang, Yu Wu, Zhongwen Xu",
                    "summary": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为CEL的新型智能体架构，其核心贡献在于利用大语言模型进行推理和规划来学习，而非简单将LLM作为工具应用到特定领域。论文明确提出了一个新的学习范式，通过让LLM进行显式推理和规划来增强其通用问题解决能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划和多步推理等通用能力\"的保留标准。 其次，论文满足多个正面指标：它明确以大语言模型(LLM)为核心概念；直接聚焦于推理(reasoning)和规划(planning)能力；提出了一种自我学习和进化的训练方法；并且属于基于LLM的智能体(llm-based agents)这一新兴范式。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然评估在游戏环境中进行，但这些只是作为测试平台，论文核心是提出通用架构而非特定应用领域；也没有聚焦于模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文提出的是通用的智能体架构，而非针对特定领域的应用；同时，它强调通过显式推理构建透明和改进的世界模型，增强了模型的可解释性，从而提升通用推理质量。 综上所述，这篇论文的核心贡献是提出一种通过推理和规划来增强LLM通用能力的新方法，完全符合研究目标。",
                    "summary2": "本文旨在[解决传统深度强化学习方法依赖大量经验且决策过程不透明的问题]。针对[网格世界游戏环境]，我们提出了一种[基于LLM的显式推理和规划的智能体架构CEL]，并在[Minesweeper, Frozen Lake和Sokoban三种网格世界任务]上通过[成功率]验证了其有效性。",
                    "summary_translation": "追求能够学习掌握复杂环境的人工智能代理（artificial agents）已经取得了显著成功，然而主流的深度强化学习（deep reinforcement learning）方法通常依赖于大量经验，将其知识不透明地编码在神经网络（neural network）权重中。我们提出了一种不同的范式，即代理通过推理（reasoning）和规划（planning）来学习。我们介绍了Cogito, ergo ludo (CEL)，一种新颖的代理架构，它利用大型语言模型（Large Language Model, LLM）来构建对环境机制和自身策略的明确、基于语言的理解。从没有任何先验知识（除动作集外）的白板状态（tabula rasa state）开始，CEL在交互和反思的循环中运行。在每个回合（episode）后，代理分析其完整轨迹（trajectory）以执行两个并行的学习过程：规则归纳（Rule Induction），在此过程中它完善对环境动力学的显式模型；以及策略与战术手册总结（Strategy and Playbook Summarization），在此过程中它将经验提炼为可操作的策略手册。我们在多样化的网格世界（grid-world）任务（即扫雷（Minesweeper）、冰湖（Frozen Lake）和推箱子（Sokoban））上评估了CEL，并表明CEL代理通过自主发现游戏规则和从稀疏奖励（sparse rewards）中开发有效策略（policies），成功地学会了掌握这些游戏。消融研究（Ablation studies）证实，迭代过程对于持续学习至关重要。我们的工作展示了一条通向更通用和可解释代理的路径，这些代理不仅能够有效行动，还能通过对原始经验的明确推理（explicit reasoning）来构建透明且不断改进的世界模型。",
                    "inspiration_trace": "# 从问题到方法：推演\"Cogito, Ergo Ludo\"的思考逻辑链\n\n## 一、宏观问题：如何创建能真正理解环境的智能体？\n\n人工智能领域长期追求的目标是创建能够掌握复杂环境的智能体。然而，作者观察到现有方法存在根本性局限：\n\n- **深度强化学习(DRL)**：虽然取得了显著成就，但依赖海量经验，将知识隐式编码在神经网络权重中，决策过程不透明\n- **基于LLM的智能体**：缺乏持续学习和适应的结构化机制，无法通过经验改进内部模型\n- **学习世界模型**：在不可解释的潜在状态上运行，将\"理解\"隐藏在黑盒中\n\n这引出了一个核心问题：**如何创建一个既能有效行动，又能以可解释方式理解环境的智能体？**\n\n## 二、核心假设：显式推理是通往高效可解释学习的关键\n\n基于对现有方法局限的分析，作者提出了一个核心假设：\n\n**通过利用大型语言模型进行显式推理和规划，可以创建一个能够构建环境显式、人类可读模型的智能体，从而实现更高效、更可解释的学习。**\n\n这一假设挑战了主流的\"黑盒\"学习范式，提出智能体应该通过\"思考\"来学习，而非仅仅通过反复试错。\n\n## 三、方法设计：构建\"Cogito, Ergo Ludo\"架构\n\n基于这一假设，作者设计了CEL架构，其核心创新在于：\n\n### 1. 基于语言的知识表示\n- 将所有信息（状态、动作、奖励、环境动态、策略）表示为自然语言字符串\n- 创建人类可读的\"世界模型\"和\"策略手册\"，而非隐式神经网络参数\n\n### 2. 双阶段学习循环\n- **阶段一：回合内决策** - 利用当前知识进行前瞻搜索和规划\n- **阶段二：回合后反思** - 分析完整轨迹，并行执行两个学习过程：\n  * **规则归纳**：提炼和更新对环境规则的理解\n  * **策略总结**：将成功和失败的行为模式提炼为可操作的策略手册\n\n### 3. 从零开始的学习机制\n- 初始状态为\"白板\"(tabula rasa)，除动作集外无任何先验知识\n- 通过交互-反思循环，自主构建环境模型和策略\n\n## 四、理论支撑：三大认知组件的协同\n\n作者设计了三个核心认知组件，它们共同构成了CEL的理论基础：\n\n### 1. 基于语言的世界模型(LWM)\n- 预测环境动态，输出为自然语言描述\n- 为显式规划提供基础，使智能体能够模拟和评估可能的未来结果\n\n### 2. 基于语言的价值函数(LVF)\n- 提供对状态长期成功潜力的定性、语言评估\n- 为决策提供关键启发式方法，评估当前状态的长期潜力\n\n### 3. 策略与战术手册(Π)\n- 将战略知识外显为可解释的文本手册\n- 实现快速上下文学习，显著加速战略适应\n\n## 五、实验验证：从理论到实践\n\n作者在三个具有挑战性的网格世界环境中验证了CEL：\n\n### 实验设计\n- **环境选择**：扫雷(逻辑推理)、冻结湖(导航)、推箱子(复杂规划)\n- **挑战设置**：稀疏奖励、未知规则\n- **评估指标**：成功率、学习效率、可解释性\n\n### 关键发现\n1. **有效性**：CEL在所有环境中均展示出显著的学习能力，成功率远超零样本基线\n2. **效率**：特别是在冻结湖任务中，仅用10个回合就达到97%的成功率\n3. **可解释性**：生成的规则书和策略手册全面且准确，展示了人类可读的\"理解\"\n4. **泛化能力**：在未见过的布局和新环境中表现出强大的泛化能力\n\n## 六、消融研究：确认关键组件的必要性\n\n通过系统的消融研究，作者确认了CEL架构中各组件的关键作用：\n\n- **静态规则vs动态更新**：仅进行一次规则归纳的智能体表现迅速下降，确认迭代规则更新对持续学习至关重要\n- **无规则vs完整模型**：没有规则归纳机制的智能体表现停滞，确认环境模型构建是实现能力的基础\n\n这些结果有力地支持了作者的核心假设：显式推理和规划是实现高效可解释学习的关键。\n\n## 七、更广泛意义：迈向透明智能的新范式\n\nCEL不仅是一个具体的智能体架构，更代表了一种新的智能体设计范式：\n\n1. **从隐式到显式**：将隐式神经网络知识转变为显式、人类可读的规则和策略\n2. **从黑盒到透明**：创建不仅有效行动，而且构建对世界透明理解的智能体\n3. **从经验驱动到推理驱动**：通过\"思考\"学习，而非仅通过反复试错\n\n这一范式为构建更通用、更可解释、更值得信赖的智能体提供了新路径，可能对人工智能的未来发展产生深远影响。\n\n## 总结：从观察到创新的逻辑链\n\n作者的思想路径形成了一个清晰的逻辑链：\n\n**观察局限(现有方法不透明、低效) → 核心问题(如何创建可理解的智能体) → 核心假设(显式推理是关键) → 方法设计(CEL架构) → 实验验证(多样化环境测试) → 结果确认(消融研究) → 更广泛意义(新智能体范式)**\n\n这一逻辑链展现了作者从宏观问题出发，通过系统观察、假设形成、方法设计和实验验证，最终提出创新性解决方案的完整思考过程，体现了严谨的学术思维和创新的方法论。"
                },
                {
                    "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models",
                    "arxiv_id": "2509.24261",
                    "authors": "Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, Lin Yan",
                    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，该论文的本质是改进大语言模型的基础推理能力，而非将其作为工具应用于特定领域。论文提出了一种新的风险敏感强化学习框架(RS-GRPO)，专门解决LLM在探索过程中的困境，这属于增强LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念上明确聚焦于大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上提出了强化学习(RL)的新范式。这些都是提高LLM通用推理能力的关键要素。 第三，论文不符合任何排除标准。虽然论文使用了数学推理作为测试基准，但数学推理被视为通用推理能力的重要组成部分，而非特定应用领域。论文的核心贡献是提出一种通用的强化学习方法，而非解决特定领域问题。 最后，论文的核心贡献是通过改进强化学习算法来增强LLM的推理策略多样性，提高模型在推理任务上的表现，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在强化学习微调中面临的探索困境问题。针对预训练LLMs初始策略分布尖锐导致的解决方案多样性不足问题，我们提出了一种风险敏感强化学习框架（Risk-Sensitive RL），并在6个数学推理基准测试上通过pass@k和pass@1指标验证了其有效性。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)已被证明在增强大型语言模型(Large Language Models, LLMs)处理复杂推理任务方面是有效的。然而，现有方法面临一个探索困境(exploration dilemma)：预训练大型语言模型的尖峰初始策略(sharply peaked initial policies)将标准强化学习算法限制在狭窄的解决方案集合中，提高了单次通过率(pass@1)但抑制了解决方案的多样性和多次通过率(pass@k)。因此，RLVR往往只是提炼现有能力，而非发现新的推理策略。为克服这一问题，我们引入了一个风险敏感强化学习(Risk-Sensitive Reinforcement Learning)框架。我们的方法采用了一种在平均奖励和最大奖励之间进行插值的风险寻求目标(risk-seeking objective)，从而产生了一种新算法——风险敏感GRPO(Risk-Sensitive GRPO, RS-GRPO)，该算法通过放大对挑战性提示(challenging prompts)的学习来驱动更深入的探索。值得注意的是，RS-GRPO实现简单，仅需少量代码修改。在六个数学推理基准(mathematical reasoning benchmarks)和五种不同的大型语言模型上，RS-GRPO在保持或提高pass@1准确率的同时，一致地提升了pass@k性能。",
                    "inspiration_trace": "# 风险敏感强化学习解决LLM探索困境的逻辑推演\n\n## 1. 宏观问题：LLM强化学习中的探索困境\n\n**观察现象**：\n- 基于可验证奖励的强化学习(RLVR)能有效提升LLM在复杂推理任务上的表现\n- 但现有方法存在一个关键矛盾：提高了单解准确率(pass@1)，却抑制了解决方案多样性，导致多解性能(pass@k)停滞或下降\n- 这表明现有方法只是在强化预训练偏差，而非发现真正新颖的推理策略\n\n**问题定义**：作者将这一现象定义为\"探索困境\"(exploration dilemma)——标准RL算法无法从预训练LLM的尖锐初始策略分布中逃脱，被限制在局部最优解中。\n\n## 2. 问题根源：初始策略分布与RL算法的不匹配\n\n**深入分析**：\n- 传统RL设置(如游戏)通常从随机初始化策略开始，需要广泛探索\n- 而LLM微调则从高度专业化、已经围绕某些解决方案尖锐分布的策略开始\n- 如果这些初始峰值不在产生最优奖励的区域，标准RL优化器很难逃脱预训练模型偏差的\"引力\"\n\n**核心假设**：探索困境源于LLM优化景观与标准RL算法动态之间的根本不匹配。初始策略的尖锐分布导致RL算法陷入局部最优，无法发现更多样化和强大的推理路径。\n\n## 3. 解决思路：风险敏感强化学习的理论启发\n\n**理论借鉴**：\n- 风险敏感RL旨在建模和管理决策中的风险，超越基于期望的标准目标\n- 分布RL的出现促进了更细致的方法，包括风险寻求行为，已被证明可以促进探索\n- 风险寻求优化在游戏等领域显示出促进探索的潜力\n\n**关键假设**：风险寻求优化对于逃脱预训练模型的尖锐初始策略分布至关重要，能够实现更广泛的探索，从而发现新的推理策略。\n\n## 4. 方法设计：风险敏感目标函数\n\n**设计原则**：\n1. 应该重视所有高奖励结果，而非仅仅最可能的那个\n2. 应提供灵活机制，在优化平均奖励和最大奖励之间进行插值\n\n**具体实现**：\n- 采用基于指数效用的风险敏感目标：\n  J_RS(π_θ) = E_x~D[1/β log E_y~π_θ(·|x)[e^{βr(y)}]]\n- 超参数β控制风险敏感度：\n  - β→0：恢复标准期望奖励（风险中性）\n  - β→+∞：接近最大奖励（风险寻求，鼓励探索）\n  - β→-∞：接近最小奖励（风险规避，促进稳健性）\n\n## 5. 算法实现：风险敏感策略梯度\n\n**理论推导**：\n- 推导风险敏感目标的策略梯度形式\n- 提出风险敏感优势函数：\n  A_β^π(y) = 1/β [e^{βr(y)}/E_{y'~π(·|x)}[e^{βr(y')]} - 1]\n\n**算法特点**：\n- 实现为RS-GRPO算法，可作为标准GRPO中优势计算的直接替代\n- 只需修改优势计算部分，保持策略梯度结构不变，实现简单\n\n## 6. 理论支持：风险敏感方法的优势\n\n**关键发现**：\n- 标准策略梯度可能会降低最优动作的概率（当存在次优但优于平均的动作时）\n- 风险敏感策略梯度保证对最优动作的改进（当β足够大时）\n- β过大会导致收敛速度下降，提供实践指导：β应足够大以增强探索，但不宜过大影响收敛\n\n**实证验证**：\n- 设计多臂赌博机问题模拟探索困境\n- 结果显示标准风险中性策略被局部最优困住，而足够风险敏感的策略成功逃脱并达到全局最优\n\n## 7. 实验验证：全面评估方法效果\n\n**实验设置**：\n- 在六个数学推理基准和五个不同LLM上测试\n- 与标准GRPO和其他pass@k优化方法比较\n\n**主要结果**：\n- RS-GRPO一致且显著地提高了pass@k性能\n- 同时保持或增强了pass@1准确率\n- 实现了比现有方法更有效的平衡\n- 分析表明RS-GRPO发现了更多独特的解决方案，特别提高了困难问题的解决率\n\n**参数选择**：β=2提供了探索与利用的良好平衡，在pass@k和pass@1之间取得最优权衡。\n\n## 结论\n\n作者从观察到的LLM强化学习探索困境出发，通过分析问题根源（初始策略分布与RL算法的不匹配），借鉴风险敏感RL理论，提出风险寻求优化能帮助模型逃脱局部最优的核心假设，进而设计出风险敏感目标函数和RS-GRPO算法，并通过理论和实证验证了方法的有效性，形成了一个完整的逻辑链条。这一方法不仅解决了探索困境，还在保持pass@1性能的同时显著提升了pass@k表现，为LLM的强化学习微调提供了新思路。"
                },
                {
                    "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs",
                    "arxiv_id": "2509.24107",
                    "authors": "Shreyas Singh, Kunal Singh, Pradeep Moturi",
                    "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据我的分析： 首先，从核心判断来看，论文本质上是提出了一种工具集成的推理方法(Fathom-DeepResearch)，通过增强小型语言模型(SLMs)的长期信息检索和综合能力来提高其通用推理能力。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含了所有重要的正面指标： - 核心概念：明确涉及SLMs(基于Qwen3-4B) - 能力方向：专注于\"tool-integrated reasoning\"，并在多样化推理任务(HLE、AIME-25等)上展示性能 - 训练方法：提出了RAPO强化学习方法和多智能体自我博弈训练策略 - 新兴范式：提出了智能体系统、工具使用和深度研究框架 第三，论文不主要聚焦于任何排除标准中的领域。虽然评估中提到了MedQA，但这只是作为系统泛化能力的测试基准，而非论文主要焦点。 特别地，在智能体/工具使用方面，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用，完全符合保留标准。 综上所述，该论文的核心贡献是提出了一种新的训练范式和智能体系统，通过工具集成推理和强化学习方法来提升语言模型的通用推理能力，与研究目标高度一致。",
                    "summary2": "本文旨在解决小型语言模型(SLMs)在长周期信息检索与合成方面的局限性。针对复杂、开放式的信息获取任务，我们提出了Fathom-DeepResearch智能体系统，该系统由Fathom-Search-4B和Fathom-Synthesizer-4B两个专门模型组成，并在DeepSearch基准测试(SimpleQA、FRAMES、WebWalker等)和DeepResearch-Bench上通过准确率、RACE和FACT等指标验证了其有效性。",
                    "summary_translation": "工具集成推理（Tool-integrated reasoning）已成为实现代理应用程序（agentic applications）的关键焦点。在这些应用中，DeepResearch Agents（深度研究代理）因其处理复杂、开放式信息检索任务的卓越性能而获得广泛关注。我们介绍了Fathom-DeepResearch，一个由两个专业模型组成的代理系统。第一个是Fathom-Search-4B，一个基于Qwen3-4B训练的DeepSearch（深度搜索）模型，通过实时网络搜索和定向网页查询进行基于证据的调查优化。其训练结合了三项创新：(i) DUETQA（双元问答），一个通过多代理自我博弈生成的5K样本数据集，强制执行严格的网络搜索依赖和异构源锚定；(ii) RAPO（奖励感知策略优化），GRPO的零开销扩展，通过课程修剪、奖励感知优势缩放和每个提示的重放缓冲区来稳定多轮可验证奖励强化学习；(iii) 可引导的步骤级奖励，根据认知行为和边际效用对每个工具调用进行分类，实现对搜索轨迹广度、深度和视野的明确控制。这些改进使得在必要时能够可靠地将工具调用扩展到20次以上。第二个是Fathom-Synthesizer-4B，同样基于Qwen3-4B训练，将多轮DeepSearch轨迹转换为结构化、引用密集的DeepResearch Reports（深度研究报告）以进行全面综合。在DeepSearch基准测试（SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue）和DeepResearch-Bench上的评估表明，该系统在开放权重类别中实现了最先进的性能，同时展现了对多样化推理任务（包括HLE, AIME-25, GPQA-Diamond和MedQA）的强大泛化能力。",
                    "inspiration_trace": "# Fathom-DeepResearch方法论逻辑推演\n\n## 宏观问题：如何使小型语言模型(SLMs)具备长视界信息检索与综合能力？\n\n### 问题观察与分解\n\n作者首先观察到当前AI系统在复杂信息检索与综合任务中存在显著差距，特别是开源与闭源系统之间的性能鸿沟。通过深入分析，识别出四个核心挑战：\n\n#### 挑战1：多轮工具交互的训练不稳定性\n**观察**：GRPO等强化学习方法在单轮推理任务中有效，但在多轮工具交互环境中表现不稳定。工具响应导致策略模型分布偏移，引发解码不稳定和错误级联。\n\n**假设**：如果能稳定多轮强化学习过程，就能实现可靠的长视界工具使用。\n\n**解决方案路径**：开发RAPO（Reward Aware Policy Optimization），通过三个关键机制解决不稳定性：\n1. 数据集修剪：移除已解决的简单问题，形成自然课程学习\n2. 优势缩放：重新缩放令牌级优势，保持有效梯度幅度\n3. 重放缓冲区：在失败时注入高质量历史轨迹，恢复学习信号\n\n#### 挑战2：奖励黑客与低效工具调用\n**观察**：仅依赖最终正确性的稀疏奖励导致工具使用激增但性能下降，模型陷入重复调用模式。\n\n**假设**：设计能区分工具调用认知价值的细粒度奖励函数，可引导高效探索行为。\n\n**解决方案路径**：构建可控制的步骤级奖励系统，将工具调用分类为：\n- 搜索行为：唯一搜索 vs 冗余搜索\n- 查询行为：探索 vs 验证 vs 冗余查询\n\n基于分类计算奖励，惩罚冗余调用，同时奖励有价值的探索行为，实现工具使用的精确引导。\n\n#### 挑战3：高质量训练数据稀缺\n**观察**：现有数据集可通过参数知识或简单查询解决，无法反映真实网络检索的复杂性和不确定性。\n\n**假设**：创建必须依赖实时网络搜索、来源多样化且可验证的数据集，能提升模型检索能力。\n\n**解决方案路径**：开发DUET QA数据集，通过多智能体自我博弈管道：\n1. 两个搜索增强模型生成QA对\n2. 非搜索模型混淆问题以消除表面线索\n3. 交叉验证确保问题必须通过搜索解决且答案正确\n\n#### 挑战4：开放式查询的综合能力不足\n**观察**：现有方法主要针对封闭式查询优化，缺乏处理开放式探索性查询的综合能力。\n\n**假设**：专门的规划与综合模型能将多轮搜索轨迹转化为结构化、引用密集的报告。\n\n**解决方案路径**：设计Fathom-Synthesizer-4B，采用\"先计划后写作\"协议：\n1. 问题分解为子目标\n2. 定义报告结构\n3. 证据映射到各部分\n4. 生成引用严格的综合报告\n\n### 方法整合与系统构建\n\n作者将上述解决方案整合为Fathom-DeepResearch系统，包含两个专门模型：\n1. **Fathom-Search-4B**：专注于长视界信息检索与推理\n2. **Fathom-Synthesizer-4B**：专注于信息综合与报告生成\n\n这种分工设计使每个模型能专注于特定能力，同时协同工作完成端到端的研究任务。\n\n### 实验验证与迭代优化\n\n通过在多个基准测试上验证系统性能：\n- DeepSearch基准（SimpleQA、FRAMES等）\n- 一般推理基准（HLE、AIME-25等）\n- DeepResearch-Bench\n\n实验结果证实了方法的有效性，特别是在长视界工具使用（>20次调用）和开放式查询处理方面取得突破。\n\n### 逻辑链条总结\n\n从宏观问题出发，作者通过观察现有方法的局限性，提出针对性假设，开发创新解决方案，最终整合为完整系统。整个思考过程体现了：\n\n**问题识别→分解→假设形成→解决方案开发→系统整合→实验验证**\n\n的逻辑链条，每个环节都紧密衔接，形成了从理论到实践的完整方法论创新路径。"
                },
                {
                    "title": "How LLMs Learn to Reason: A Complex Network Perspective",
                    "arxiv_id": "2509.23629",
                    "authors": "Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen",
                    "summary": "Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLM)如何学习推理能力，并提出了一种新的训练方法(Annealed-RLVR)来增强模型的推理能力。论文不是将LLM作为工具应用于特定领域，而是直接关注提升LLM本身的基础推理能力，这符合\"保留\"标准。 其次，论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：直接关注\"reasoning\"(推理)能力，这是论文的核心主题 - 训练方法：研究了\"Reinforcement Learning from Verifiable Rewards (RLVR)\"这一强化学习方法，并基于此提出了改进算法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及特殊或模糊情况。它明确关注LLM的通用推理能力提升，而不是将智能体/工具应用于特定领域，也不是主要研究幻觉/可解释性/安全等问题。 论文的核心贡献是提出了一种复杂网络视角来理解LLM的推理过程，并基于此设计了新的训练算法来增强模型的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决LLMs在RLVR训练过程中出现的两阶段学习曲线、V型响应长度轨迹和灾难性遗忘等困惑行为。针对LLMs的推理过程，我们提出了一种基于稀疏概念网络的理论，该网络平均度接近2，并开发了Annealed-RLVR算法，在最大挫折点引入SFT\"加热\"步骤。在1.5B参数模型上通过best@k准确率等指标验证了该方法在分布内和分布外基准测试上均优于标准RLVR。",
                    "summary_translation": "使用可验证奖励强化学习(Reinforcement Learning from Verifiable Rewards, RLVR)训练大语言模型表现出一系列独特且令人困惑的行为，这些行为尚未被充分理解，包括两阶段学习曲线(two-stage learning curve)、V形响应长度轨迹(V-shaped response-length trajectories)以及对灾难性遗忘(catastrophic forgetting)的显著脆弱性。在这项工作中，我们提出这些看似不同的现象可以用一个统一理论(unifying theory)来解释：模型的推理过程(reasoning process)映射到一个语义复杂网络(semantic complex network)的自组织(self-organization)，该网络的拓扑结构(topology)保持持续稀疏(persistently sparse)，平均度(average degree)固定在接近二。这种拓扑结构为遗忘和学习提供了一个基本机制：它首先将系统驱动到一个最大挫折状态(maximally frustrated state)，在此状态下形成\"技能岛\"(skill islands)，发生慢速学习(slow-learning)并诱导遗忘；然后系统进入一个急剧增长阶段(sharp growth phase)，新技能被\"添加\"(\"bolted on\")，这一过程由网络前沿(web's frontier)的相变类学习(phase-transition-like learning)驱动。基于这一理论，我们提出了\\textit{退火RLVR}(Annealed-RLVR)，这是一种有原则的算法(principled algorithm)，在最大挫折点引入基于SFT(SFT-based)的\"加热\"(\"heating\")步骤，以解决竞争瓶颈(competitive bottleneck)并增强模型的推理能力(reasoning capability)。在15亿参数模型(1.5B-parameter model)上的实验表明，该方法在分布内(in-distribution)和分布外(out-of-distribution)基准测试(benchmarks)上均优于标准RLVR。通过将RLVR从黑盒优化(black-box optimization)重塑为结构自组织(structural self-organization)的可预测过程(predictable process)，我们的工作为构建未来AI系统的涌现推理能力(emergent reasoning capabilities)提供了新的物理直觉(physical intuition)。",
                    "inspiration_trace": "# 从现象到理论：LLM推理能力学习的复杂网络视角逻辑链\n\n## 一、宏观问题的提出：RLVR训练中的谜团\n\n作者首先观察到大型语言模型(LLMs)通过强化学习从可验证奖励(RLVR)训练时，普遍存在三个令人困惑的宏观现象：\n1. **两阶段学习曲线**：快速初始增益后进入长期平台期\n2. **V型响应长度轨迹**：正确解决方案长度先缩短后稳步增长\n3. **灾难性遗忘脆弱性**：与监督微调(SFT)交错时性能急剧下降\n\n这些现象在不同架构和基准测试中广泛存在，但缺乏统一解释。\n\n## 二、现有方法的局限与新思路的引入\n\n### 现有方法的障碍\n- 虽有研究将推理过程建模为\"推理图\"，但直接从高维潜空间构建完整图结构在计算上不可行\n- 微观分析的困难阻碍了对观察动态的结构起源的直接研究\n\n### 新视角：复杂系统与重整化群理论\n作者借鉴复杂系统科学和重整化群(RG)理论的核心逻辑：\n- 在复杂系统中，独立于微观细节的行为通常是涌现属性，由系统的大规模组织支配\n- 与其陷入难以处理的局部细节，不如直接假设系统大规模结构的核心原则来解释宏观行为\n\n## 三、核心假设的提出：稀疏概念网络理论\n\n基于上述思路，作者将焦点从完整推理图转向其粗粒度主干，提出核心假设：\n\n**假设1：概念网络（粗粒度推理图）是一个稀疏网络，其有效平均度被固定在⟨k⟩ ≈ 2**\n\n这一假设的关键含义：\n- 平均度为2意味着主要是树状结构，有利于泛化但本质上脆弱\n- 这种拓扑结构仍能支持罕见的大规模循环（如自我纠正），而不显著改变全局平均度\n\n## 四、假设验证与现象解释\n\n### 验证方法\n作者结合两种方法验证假设：\n1. **理论研究**：使用概念网络模型(CoNet)作为\"计算显微镜\"\n2. **数值实验**：在1.5B参数LLM上进行实验\n\n### 现象解释\n通过验证，作者揭示了观察到的谜题的结构叙事：\n\n1. **两阶段学习曲线与V型轨迹**：\n   - **第一阶段**：模型进行局部优化，快速发现短而高效的解决方案，形成断开的\"技能岛\"\n   - **第二阶段**：从局部发现转向全局整合，将这些\"岛\"编织成单一、扩展的概念网络\n   - 稀疏网络（平均度≈2）导致连接两个远距离概念需要遍历长中间路径，解释了V型曲线的上升斜率\n\n2. **灾难性遗忘的脆弱性**：\n   - 稀疏网络的低冗余性导致关键\"桥梁\"连接的脆弱性\n   - SFT会覆盖关键分支节点的策略，手术式切断大量下游子图的单连接点\n   - 损伤虽然影响深远，但在拓扑上是局部的，解释了为什么恢复速度很快\n\n## 五、微观机制的深入分析\n\n作者发现稀疏拓扑结构强加了RLVR训练的微观动态机制：\n\n### 1. 挫折诱导的遗忘：整合的代价\n- 在缓慢学习阶段开始时，系统进入最大挫折状态\n- 在⟨k⟩≈2约束下，技能岛为有限连接竞争，导致一些技能被竞争性抑制\n- 矛盾的是，这种状态最大化了探索能力：模型找到多样化、正确的分布外解决方案的能力达到峰值\n\n### 2. 相变式学习：增长的引擎\n- 在网络扩展前沿，新技能通过类似相变的过程被高效\"bolted on\"\n- 这种现象是\"临界学习\"(Learning at Criticality)机制在多任务环境中的扩展\n- 稀疏网络前沿的拓扑隔离使新技能能够通过干净的相变高效添加，不受系统范围竞争干扰\n\n## 六、理论应用：Annealed-RLVR算法的提出\n\n基于稀疏网络理论，作者识别出最大挫折状态既是危险瓶颈，也是探索潜力的峰值时刻，据此提出Annealed-RLVR算法：\n\n### 算法核心\n1. **时机选择**：在最大挫折状态（由宏观信号识别）进行干预\n2. **\"加热\"阶段**：应用短暂的SFT干预，打破策略对次优路线的过早承诺\n   - 仅对准确率低(<0.1)但存在正确解决方案的问题进行SFT\n3. **\"冷却\"阶段**：恢复标准RLVR，允许现在更灵活的策略重新稳定在更强大、更集成的最终状态\n\n### 算法原理\n- 利用稀疏网络理论识别的最佳干预时机\n- SFT作为\"加热\"冲击，打破连接，允许系统逃离可能的次优状态\n- 随后的RLVR作为受控\"冷却\"，引导系统重新形成连接，理想情况下稳定在更鲁棒的配置\n\n## 七、实验验证与效果\n\n作者在CoNet和1.5B参数LLM上验证了Annealed-RLVR的有效性：\n\n1. **性能提升**：在分布内和分布外(Minerva)基准测试上均优于标准RLVR\n2. **机制验证**：算法系统地减少了未解决问题数量，提高了已掌握问题的比例\n3. **探索-利用平衡**：SFT干预后，best@1准确率暂时下降，而best@16准确率上升，表明成功地将贪婪利用换取增强探索\n\n## 八、理论意义与未来展望\n\n### 核心贡献\n- 将RLVR从黑盒优化重新概念化为可预测的结构自组织过程\n- 为工程化未来AI系统的涌现推理能力提供了新的物理直觉\n- 揭示了稀疏网络结构如何统一解释RLVR训练中的多种现象\n\n### 未来方向\n- 长期目标是实证映射完整、微观的推理图\n- 这将产生一个强大的新分析对象：AI内部世界的详细地图\n- 为真正可解释的LLMs和未来人工智能的稳健安全性迈出关键一步\n\n## 总结\n\n作者从RLVR训练中的宏观谜团出发，借鉴复杂系统理论和重整化群思想，提出稀疏概念网络理论，通过实验验证解释了观察现象，深入分析了微观机制，并基于理论提出了实用的Annealed-RLVR算法，最终将RLVR重新概念化为结构自组织过程，为理解和工程化AI推理能力提供了新视角。这一完整的逻辑链条展现了从现象观察到理论形成再到方法应用的科学思考过程。"
                },
                {
                    "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction",
                    "arxiv_id": "2509.23186",
                    "authors": "Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen",
                    "summary": "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是改进LLM的基础能力，特别是规划能力。论文研究多令牌预测(MTP)范式如何增强语言模型的传递关系学习能力，这直接关系到复杂规划这一通用推理能力的核心。作者提出了新的训练策略(Next-Token Injection和基于Transformer的传递层)来提升模型的路径规划能力，这属于改进LLM基础能力和提出新训练范式的研究。 第二步正面指标：论文包含了多个正面指标： - 核心概念：明确研究\"Large Language Models (LLMs)\" - 能力方向：专注于\"planning capability\"和\"complex planning\"，这是推理能力的重要组成部分 - 研究内容涉及\"transitive relation learning\"，这也是逻辑推理的关键方面 第三步排除标准：论文不涉及任何需要排除的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域(如医疗、化学等)，Blocksworld规划基准是通用规划能力的测试平台 - 没有专注于模型可靠性方面的应用层面问题 第四步特殊和模糊情况：论文不涉及需要特殊判断的情况，它明确聚焦于提升LLM的通用规划能力，而非特定领域的应用。 论文的核心贡献是通过理论分析和实验验证，揭示了多令牌预测范式如何帮助语言模型学习传递关系，并提出有效策略来增强模型的路径规划能力，这直接服务于提升LLM的通用推理能力，与研究目标高度一致。",
                    "summary2": "本文旨在增强语言模型在复杂规划任务中的传递关系学习能力。针对图路径规划任务，我们提出了一种多令牌预测（MTP）范式及两种增强策略（Next-Token Injection和基于Transformer的传输层），并在合成图和Blocksworld规划基准上通过路径预测准确率验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在多样化任务中取得了令人瞩目的性能，但在学习传递关系（transitive relations）方面仍然存在困难，而传递关系是复杂规划（complex planning）的基石。为解决这一问题，我们研究了多Token预测（Multi-Token Prediction, MTP）范式及其对传递关系学习的影响。我们使用由共享输出头（shared output head）和传递层（transfer layer）组成的Transformer架构，对MTP范式进行了理论分析。我们的分析表明，传递层逐渐学习多步邻接信息（multi-step adjacency information），进而使主干模型（backbone model）能够捕捉训练数据中未直接存在的未观测传递可达性关系（unobserved transitive reachability relations），尽管在邻接估计中存在一些不可避免的噪声。基于这一基础，我们提出了两种策略来增强传递层和整体学习质量：下一Token注入（Next-Token Injection, NTI）和基于Transformer的传递层。我们在合成图（synthetic graphs）和Blocksworld规划基准（Blocksworld planning benchmark）上的实验验证了我们的理论发现，并表明这些改进显著增强了模型的路径规划（path-planning）能力。这些发现深化了我们对具有MTP的Transformer如何在复杂规划任务中学习的理解，并提供了克服传递性瓶颈（transitivity bottleneck）的实用策略，为构建结构感知（structurally aware）和通用规划（general-purpose planning）模型铺平了道路。",
                    "inspiration_trace": "# 从问题洞察到方法创新：多令牌预测增强语言模型规划能力的逻辑推演\n\n## 一、宏观问题识别：语言模型的规划瓶颈\n\n作者从大型语言模型(LLMs)的一个根本性局限出发：尽管LLMs在多种任务上表现出色，但它们在**学习传递关系**方面存在显著困难。传递关系是复杂规划的基石，例如在路径规划中，如果知道A能到B且B能到C，理应能推断出A能到C。然而，标准自回归Transformer模型在这种传递推理上表现不佳，这构成了当前模型在结构化规划中的基本瓶颈。\n\n## 二、问题具体化：路径规划中的传递关系缺陷\n\n作者将宏观问题具体化为**图路径规划任务**中的传递关系学习：\n- 将规划问题抽象为有向图上的路径查找，节点表示状态，边表示可执行动作\n- 定义了四类测试路径，按传递推理难度递增：\n  - degree-0：训练中直接观察到的路径\n  - degree-1：需一步推理的路径\n  - degree-2：需两步推理的路径\n  - degree-3：需三步推理的路径\n\n**关键观察**：标准Transformer在degree-0/1任务上准确率超90%，但在degree-2任务上骤降至约60%，表明模型无法有效组合多个路径段来推断新的可达性关系。\n\n## 三、探索解决方案：多令牌预测范式的潜力\n\n作者转向探索**多令牌预测(MTP)**范式作为潜在解决方案，基于以下洞察：\n- MTP在单个训练步骤中预测多个未来令牌，提供更丰富的监督信号\n- MTP已显示出建模长期依赖和结构关系的潜力，被Meta和DeepSeek等公司采用\n- 然而，MTP在规划任务中的底层机制尚未被充分理解\n\n**核心假设**：MTP可能通过其多步骤监督机制帮助模型学习传递关系，特别是传递层可能逐渐学习多步邻接信息，使骨干模型能够捕捉未直接观察到的传递可达性关系。\n\n## 四、理论分析：揭示MTP的学习机制\n\n为验证假设，作者进行了严谨的理论分析：\n\n### 1. 简化模型构建\n构建简化Transformer模型（单层、单头、线性FFN、固定注意力矩阵），使分析可追踪。\n\n### 2. 2-令牌预测的数学推导\n通过数学推导证明：\n- **定理1**：传递矩阵W^T通过第2步预测误差更新，当模型低估节点k′的可达概率时，增加从中间节点d到k′的权重\n- **定理2**：第2步预测损失通过传递矩阵的梯度传播影响骨干参数W^M和W^V，使模型能学习传递可达性\n\n### 3. 关键发现\n- 传递层W^T逐渐学习多步邻接信息\n- 当W^T捕捉真实邻接关系时，2-令牌预测使骨干模型能学习传递可达性\n- MTP能实现比下一令牌预测更高阶的可达性学习，尽管可能引入少量虚假邻接噪声\n\n## 五、识别局限性并提出改进策略\n\n基于理论分析，作者识别出现有MTP架构的两个主要局限性：\n\n### 1. 传递层输入噪声问题\n传递层性能受骨干输出预测下一节点能力限制，预测偏差引入噪声。\n\n**解决方案：Next-Token Injection (NTI)**\n- 将真实下一令牌的嵌入向量注入骨干隐藏状态\n- 提供直接监督，类似ResNet捷径连接\n- 使梯度能绕过不稳定骨干状态，直接优化传递层\n\n### 2. 线性映射的表达能力限制\n线性传递层无法建模维度间依赖关系，限制多跳关系建模能力。\n\n**解决方案：Transformer-based传递层**\n- 用自注意力机制替换线性映射\n- 允许隐藏状态各维度交互整合信息\n- 显著增强传递层表达能力，更精确建模多跳关系\n\n## 六、实验验证：从理论到实践\n\n作者通过两类实验验证理论分析和改进策略：\n\n### 1. 合成图实验\n- 在随机DAG上评估不同degree路径的预测准确率\n- 结果显示：MTP模型在degree-2/3路径上显著优于基线；NTI和Transformer传递层进一步提升性能\n\n### 2. Blocksworld规划实验\n- 在经典规划基准上测试实际规划能力\n- 结果一致：改进的MTP方法在不同训练数据规模下均表现最佳\n\n### 3. 权重分析\n- 可视化传递层权重矩阵，确认其学习真实邻接关系\n- 分析骨干网络权重，验证传递可达性学习效果\n\n## 七、形成完整方法论\n\n基于理论分析和实验验证，作者形成了系统化的方法论：\n\n### 核心机制\nMTP通过多步骤监督使传递层学习多步邻接信息，进而使骨干模型能够捕捉未观察到的传递可达性关系，克服了标准自回归训练的传递性瓶颈。\n\n### 增强策略\n1. **NTI**：通过注入真实下一令牌信息减少噪声，提供更稳定监督\n2. **Transformer传递层**：通过自注意力建模维度间依赖，增强多跳关系建模能力\n\n### 应用框架\n该方法不仅适用于路径规划，还可扩展到其他需要传递关系推理的结构化规划任务，为构建具有结构感知能力的通用规划模型提供新思路。\n\n## 八、未来展望\n\n作者提出几个有前景的研究方向：\n1. 将方法应用于实际世界连续任务\n2. 扩展NTI至无监督设置\n3. 结合显式规划模块如思维链和回溯搜索\n4. 在强化学习范式中整合这些架构改进\n\n这一完整的逻辑链条展示了作者从宏观问题识别，到具体化问题，探索解决方案，理论分析验证，识别局限性并提出改进，最终形成系统化方法论的严谨思考过程。"
                },
                {
                    "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning",
                    "arxiv_id": "2509.22819",
                    "authors": "Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, Ke Ye",
                    "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是提出Hilbert这一智能体框架，通过结合非形式推理和形式验证来增强LLM的数学推理能力。这明显属于改进LLM基础能力的研究，特别是针对数学推理这一通用推理能力的提升，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确涉及Large language models (LLMs)；能力方向专注于reasoning，特别是math reasoning；新兴范式方面提出了llm-based agents框架，通过多个组件协作解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学证明，但数学推理被视为基础通用推理能力而非特定领域应用；虽然涉及形式验证，但这是作为提升推理能力的手段，而非专注于水印、安全等应用层面的可靠性问题。 最后，在特殊和模糊情况处理上，Hilbert是一个通用的智能体框架，其方法论（递归分解、结合非形式推理和形式验证）可以泛化到其他推理任务，而非仅限于特定领域应用。 论文的核心贡献是提出了一种新的智能体协作框架，通过结合非形式推理和形式验证，显著提高了LLM在数学推理方面的能力，这正是研究目标所关注的\"提高大语言模型本身的通用推理能力\"。",
                    "summary2": "本文旨在弥合非形式推理LLMs与形式化定理证明系统之间的性能差距。针对数学定理证明问题，我们提出了一种HILBERT代理框架，通过递归分解和协调非形式推理LLM、专门证明LLM、形式化验证器和语义定理检索器，并在miniF2F和PutnamBench基准上通过问题解决率验证了其有效性，分别达到99.2%和70.0%，显著优于现有方法。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）展示了令人印象深刻的数学推理能力，但它们的解决方案经常包含无法自动验证的错误。形式化定理证明系统（Formal theorem proving systems）如Lean 4提供了完全准确的自动验证，这推动了最近构建专门的证明器大型语言模型（prover LLMs）的努力，这些模型能够生成形式化语言中的可验证证明。然而，一个显著的差距仍然存在：当前的证明器大型语言模型解决的问题数量远少于在自然语言中运行的通用大型语言模型。我们介绍了Hilbert，一个通过结合非形式化推理和形式化验证的互补优势来弥合这一差距的智能体框架（agentic framework）。我们的系统协调四个组件：一个擅长数学推理的非形式化大型语言模型（informal LLM），一个针对Lean 4策略（tactics）优化的专用证明器大型语言模型，一个形式化验证器（formal verifier），和一个语义定理检索器（semantic theorem retriever）。面对证明器无法解决的问题，Hilbert采用递归分解（recursive decomposition）将问题分解为子目标，然后使用证明器或推理器大型语言模型来解决这些子目标。它利用验证器反馈在必要时修正不正确的证明。实验结果表明，Hilbert在关键基准测试上明显优于现有方法，在miniF2F上达到99.2%的准确率，比最佳公开方法高出6.6个百分点。Hilbert在PutnamBench上取得了已知的最佳结果。它解决了660个问题中的462个（70.0%），超过了SeedProver等专有方法（50.4%），比最佳公开基线方法提高了422%。因此，Hilbert有效地缩小了非形式化推理和形式化证明生成之间的差距。",
                    "inspiration_trace": "# HILBERT方法逻辑链推演\n\n## 1. 宏观问题：数学证明自动化的两难困境\n\n数学证明的自动化是人工智能领域的长期挑战。一方面，大型语言模型(LLMs)展现出强大的数学推理能力，能解决高难度竞赛问题；另一方面，其解决方案常包含无法自动验证的错误，缺乏可靠性。这引发了一个核心问题：如何同时获得LLMs的推理能力和形式化系统的验证保证？\n\n## 2. 现象观察：两种范式的性能鸿沟\n\n作者观察到两个关键现象：\n\n**现象一：通用LLMs的\"幻觉\"问题**\n- 通用LLMs如GPT-5、Gemini 2.5 Pro在AIME、Putnam等竞赛中表现优异\n- 但它们经常产生\"幻觉\"——听起来自信但实际错误的推理\n- 即使答案正确，底层推理常包含逻辑谬误、无根据假设和计算错误\n- 人工验证耗时、困难且易出错\n\n**现象二：专业证明LLMs的能力局限**\n- 形式化定理证明系统(如Lean 4)提供完全准确的自动验证\n- 专门训练的证明LLMs在生成形式化证明方面取得进展\n- 但存在显著性能差距：推理LLMs能非形式化解决约83%的PutnamBench问题，而最佳公开证明LLMs仅能形式化解决13%\n\n## 3. 深入分析：互补优势与现有方法的不足\n\n作者进一步分析发现：\n\n**分析一：两种方法的优势互补**\n- 通用LLMs擅长非形式化推理、问题分解和生成证明草图\n- 专业证明LLMs擅长生成语法正确的形式化证明\n- 两者能力互补：一个强于高层推理，一个强于精确实现\n\n**分析二：现有混合方法的局限性**\n- 早期方法(DSP、LEGO-Prover)使用通用LLMs生成草图，ATPs填充形式化部分，但受ATP能力限制\n- DSP+等方法采用浅层、单层分解，无法处理复杂子目标\n- 代理框架(COPRA、Prover-Agent)使用验证反馈迭代构建证明，但性能仍显著落后\n\n## 4. 核心假设：递归分解是关键\n\n基于以上分析，作者提出核心假设：**如果能够结合两种方法的优势，并实现递归分解能力，可能会显著提升形式化定理证明的性能**。\n\n关键洞察在于：现有方法失败的根本原因是缺乏递归分解能力——它们只能将原始问题分解一次，无法进一步分解仍然难以解决的子目标。\n\n## 5. 方法设计：HILBERT框架的诞生\n\n基于这一假设，作者设计了HILBERT框架，其核心创新包括：\n\n**创新一：多组件协调架构**\n- Reasoner：通用LLM，负责非形式化推理和问题分解\n- Prover：专业证明LLM，负责生成形式化证明\n- Verifier：形式化验证器，提供准确反馈\n- Retriever：语义定理检索器，提供相关知识支持\n\n**创新二：递归分解算法**\n- 当直接证明失败时，将问题分解为子目标\n- 对每个子目标采用两阶段策略：先尝试Prover，失败后使用Reasoner\n- 若仍失败，递归分解子目标，直到可解决或达到最大深度\n\n**创新三：验证反馈循环**\n- 利用Verifier反馈指导Proof修正\n- Reasoner解释编译错误，建议改进方案\n- 形成闭环优化系统\n\n## 6. 实验验证：从假设到证实\n\n为验证假设，作者在两个基准测试上进行了全面实验：\n\n**验证一：MiniF2F测试**\n- HILBERT达到99.2%通过率，比最佳公开方法高6.6个百分点\n- 即使使用较弱Prover，仍保持98.4%的高性能\n- 证明Reasoner的选择比Prover强度更关键\n\n**验证二：PutnamBench测试**\n- 解决70.0%问题(462/660)，比专有SeedProver高近20个百分点\n- 比最佳公开基线提高422%\n- 接近推理LLMs的非形式化解决率(82%)\n\n## 7. 消融研究：验证核心机制\n\n作者进一步验证了两个关键机制的有效性：\n\n**机制一：递归分解**\n- 性能随递归深度单调提升\n- 完整系统在D=3时达到98.7%，接近最优\n- 无浅层求解的变体需要更大深度才能达到类似性能\n\n**机制二：检索增强**\n- 提高通过率(98.4% vs 97.1%)\n- 显著减少计算资源(Reasoner调用从426降至420)\n- 通过提供相关定理简化证明，防止因定理名称错误导致的失败\n\n## 8. 结论与展望：缩小形式化与非形式化推理的鸿沟\n\nHILBERT成功缩小了非形式化推理与形式化证明生成之间的性能差距。作者展望未来可利用此框架训练更强的模型，形成\"生成证明→训练模型→解决更复杂问题\"的良性循环，持续推动形式化推理能力的发展。\n\n这一逻辑链展现了作者从宏观问题出发，通过观察现象、深入分析、形成假设、设计方法到实验验证的完整思考过程，体现了科学研究中的系统性创新思维。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 2,
            "papers": [
                {
                    "title": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems",
                    "arxiv_id": "2509.24826",
                    "authors": "Hannah Kim, Kushan Mitra, Chen Shen, Dan Zhang, Estevam Hruschka",
                    "summary": "Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是提出AIPOM系统，用于增强多智能体系统中LLM的规划能力。这属于\"智能体协作框架\"的范畴，论文核心是改进LLM在规划方面的通用能力，而非将LLM作为工具应用到特定领域。因此符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确提到\"Large language models (LLMs)\" - 能力方向: 明确提到\"planning\"，这是通用推理能力的重要组成部分 - 新兴范式: 明确提到\"multi-agent systems\"，这是当前LLM研究的重要方向 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等） - 虽然提到透明度和可控性，但这是作为增强LLM规划能力的手段，而非主要研究模型可靠性的应用层面问题 第四步：特殊和模糊情况处理—— 论文涉及智能体/工具使用，但AIPOM是一个通用的多智能体系统框架，旨在增强LLM的通用规划能力和用户控制，而非针对特定领域的应用。因此符合保留条件。 综合判断：这篇论文的核心贡献是提出一个通用框架来增强LLM在多智能体系统中的规划能力，通过人机交互提高透明度和可控性，这直接关系到提升LLM的通用推理能力，特别是规划和问题解决方面，因此符合研究目标。",
                    "summary2": "本文旨在解决基于LLM的多智能体系统规划中缺乏透明度和可控性的问题。针对多智能体工作流规划场景，我们提出了一种AIPOM系统，结合会话和图界面实现人机在环规划，并在GSM8K和Multi-Step Arithmetic数据集上通过执行准确性、同构子图匹配和图编辑距离等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）正越来越多地被用于编排的多智能体系统（orchestrated multi-agent systems）中的规划。然而，现有的基于LLM的方法（LLM-based approaches）往往达不到人类期望，并且关键的是，缺乏让用户检查、理解和控制其行为的有效机制。这些局限性要求提高透明度、可控性和人类监督。为解决这一问题，我们引入了AIPOM，这是一个通过对话式和基于图形的界面（conversational and graph-based interfaces）支持人在环路规划（human-in-the-loop planning）的系统。AIPOM使用户能够透明地检查、细化和协作指导LLM生成的计划，显著增强了用户对多智能体工作流程（multi-agent workflows）的控制和信任。我们的代码和演示视频可在https://github.com/megagonlabs/aipom获取。",
                    "inspiration_trace": "# AIPOM核心方法逻辑推演\n\n## 宏观问题：多智能体系统规划中的透明性与可控性缺失\n\n### 问题观察\n1. **趋势识别**：大型语言模型(LLMs)正被越来越多地用于编排多智能体系统(OMAS)中的规划任务，这些系统通过多个专业化智能体协作解决复杂任务。\n   \n2. **现状分析**：尽管LLMs在规划方面展现出强大能力（复杂推理、跨领域泛化、世界知识利用），但在实际应用中存在显著问题，特别是在领域特定或高风险场景中。\n\n### 问题聚焦\n3. **核心挑战识别**：\n   - **准确性问题**：LLMs在专业领域可能生成不准确、不完整或与专家知识不符的输出。\n   - **透明度问题**：用户只能看到系统最终输出，无法了解底层计划结构或中间执行过程。\n   - **可控性问题**：传统聊天界面提供有限控制能力，难以进行细粒度的计划检查、优化和调试。\n\n4. **需求推导**：这些局限性表明人类监督应成为规划阶段的中心，需要支持用户积极参与和指导规划过程的交互界面。\n\n### 假设形成\n5. **核心假设**：通过增强透明度和可控性的人机交互界面，可以显著提高用户对多智能体工作流的理解、信任和控制能力。\n\n6. **设计原则确立**：\n   - **透明性原则**：系统应使计划结构和执行过程对用户完全可见。\n   - **可控性原则**：用户应能直接干预和修改计划的各个组成部分。\n   - **协作性原则**：应结合人类专业知识和LLM推理能力，实现混合主动规划。\n\n### 方法论构建\n7. **界面设计决策**：\n   - **双面板架构**：结合对话面板（自然语言交互）和计划面板（图形化表示）。\n   - **计划表示**：使用有向无环图(DAG)明确表示任务节点和依赖关系，包括智能体分配和数据流。\n   - **直接操作**：用户可通过直接操作图形元素修改计划结构、智能体分配和数据流。\n\n8. **交互模式设计**：\n   - **自然语言反馈**：适合高级指导，如塑造整体结构或意图。\n   - **直接操作**：适合精确或局部调整，保留大部分现有计划。\n   - **LLM辅助修复**：用户进行部分编辑后调用LLM完成、验证或修复计划。\n\n9. **系统架构实现**：\n   - **规划模块**：将用户请求转换为逻辑计划，考虑智能体能力和输入/输出要求。\n   - **对话模块**：解释用户话语并提取意图。\n   - **执行协调器**：管理跨智能体的子任务调度。\n   - **控制器**：协调各模块间通信，将用户输入转换为系统级操作。\n\n### 验证与评估\n10. **实验设计**：\n    - **定量实验**：评估计划优化性能，比较不同反馈格式（详细自然语言、模糊自然语言、直接操作+LLM修复）的效果。\n    - **试点研究**：比较不同计划表示格式（文本vs图形）和反馈模式（文本vs图形编辑）。\n\n11. **评估指标**：\n    - **功能正确性**：通过执行准确率衡量。\n    - **结构正确性**：通过图同构(ISO)和图编辑距离(GED)衡量。\n\n### 结论与展望\n12. **结论确认**：AIPOM通过结合对话和图形界面，通过灵活的人机交互增强了透明度和可控性，实验证明其在交互式计划优化中的有效性。\n\n13. **未来方向**：\n    - 应用于高风险领域（如医疗保健和金融）。\n    - 扩展用户交互，支持更复杂的操作（冻结、合并、拆分任务等）。\n    - 提高LLM在验证计划方面的辅助能力。\n    - 进行实际部署和更广泛的用户研究。"
                },
                {
                    "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems",
                    "arxiv_id": "2509.24323",
                    "authors": "Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo",
                    "summary": "The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：这篇论文的本质是提出一种名为MAS$^2$的新型多智能体系统范式，该系统基于递归自我生成原则，能够自主构建定制化的多智能体系统。论文核心是改进LLM的基础能力，提出新的训练范式（Collaborative Tree Optimization），增强其自我进化、自适应配置和纠错能力，这些都是提升LLM通用推理能力的重要方面。论文不是将LLM作为工具应用于特定领域，而是专注于提升LLM系统本身的能力，因此应被保留。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确提到\"Large Language Model (LLM)-powered multi-agent systems\" - 能力方向：涉及problem-solving，在复杂场景如\"deep research and code generation\"中表现优异 - 训练方法：提到系统的\"self-evolution\"轨迹，并提出\"Collaborative Tree Optimization\"来训练元智能体 - 新兴范式：完全围绕\"llm-based agents\"和\"multi-agent systems\"展开，并涉及\"deep research\" 第三步排除标准：论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），而是在通用场景测试 - 不关注模型可靠性方面的水印、安全等问题 第四步特殊情况处理：论文提出的智能体框架是通用的，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种自我生成、自我配置和自我纠错的多智能体系统框架，通过递归自我生成和动态组合来增强LLM的通用推理能力和自我进化能力，完全符合研究目标中关于\"智能体协作框架\"和\"自我进化\"等提升LLM通用推理能力的方法论研究。",
                    "summary2": "本文旨在解决当前自动多智能体系统遵循静态\"生成一次即部署\"范式，导致系统在面对动态真实世界环境时脆弱且无法适应的问题。针对复杂任务场景，我们提出了一种基于递归自生成的MAS²范式，通过\"生成器-实现器-修正器\"三智能体框架动态构建和自适应修正任务特定的多智能体系统，并在七个基准测试上通过性能、成本和泛化能力指标验证了其有效性。",
                    "summary_translation": "过去两年见证了由大型语言模型（Large Language Model, LLM）驱动的多智能体系统（multi-agent systems, MAS）的迅猛崛起，这些系统利用集体智能并展现出向自我演化的显著轨迹。这一范式已迅速从需要定制配置提示词、工具、角色和通信协议的手动工程系统，发展为能够实现自动编排的框架。然而，主流的自动多智能体系统，无论是通过外部模块还是单个LLM智能体生成，大多遵循刚性的\"一次性生成并部署\"（generate-once-and-deploy）范式，使得生成的系统脆弱且难以应对现实世界环境的动态性和不确定性。\n\n为超越这一局限，我们提出了MAS$^2$，这是一种基于递归自生成原则的范式：一个能够为不同问题自主构建定制多智能体系统的多智能体系统。在技术上，我们设计了一个\"生成器-实现器-修正器\"（generator-implementer-rectifier）三智能体团队，能够根据实时任务需求动态组合并自适应修正目标智能体系统。我们提出了协同树优化（Collaborative Tree Optimization）方法来训练和专业化这些元智能体。\n\n在七个基准测试上的广泛评估表明，在深度研究和代码生成等复杂场景中，MAS$^2$相比最先进的MAS实现了高达19.6%的性能提升。此外，MAS$^2$展现出卓越的跨骨干网络泛化能力，有效利用未见过的LLM实现了高达15.1%的改进。关键的是，这些提升是在不产生过多token成本的情况下实现的，因为MAS$^2$始终位于成本-性能权衡的帕累托前沿（Pareto frontier）。源代码可在https://github.com/yeyeyeah2/MAS2获取。",
                    "inspiration_trace": "# MAS²方法逻辑链分析：从观察到创新\n\n## 1. 宏观问题：多智能体系统的演进与刚性瓶颈\n\n**观察起点**：多智能体系统(MAS)已从手动配置演进到完全自动化，形成了三个发展阶段：\n- 手动配置阶段（如AutoGen、MetaGPT）：完全依赖人工设计提示、角色和通信协议\n- 部分自动化阶段（如GPTSwarm）：自动设计通信拓扑，但其他组件仍需人工\n- 完全自动化阶段（如ADAS、MaAS）：端到端合成配置并在线适应\n\n**核心问题**：尽管自动化程度不断提高，现有系统普遍遵循\"生成一次就部署\"(generate-once-and-deploy)的刚性范式，无法适应现实世界的动态性和不确定性。\n\n## 2. 问题聚焦：现有方法的根本局限\n\n**深入分析**：作者将现有自动MAS方法分为两类，并识别其局限性：\n\n**外部模块方法**（如基于GNN、进化算法、搜索算法的系统）：\n- 局限于预定义的原子操作符搜索空间（如CoT、Reflexion）\n- 缺乏架构创新能力，无法突破设计空间的限制\n\n**智能体驱动方法**（如FlowReasoner、MAS-GPT）：\n- 虽然实现了任务级适应性，但仍遵循\"生成一次就部署\"范式\n- 系统一旦部署便保持不变，无法根据执行情况动态调整\n\n**根本矛盾**：现实世界环境充满不确定性（网络故障、工具崩溃、文件丢失），而现有系统缺乏应对这些扰动的自适应能力。\n\n## 3. 核心假设：递归自生成范式的提出\n\n**假设形成**：要突破现有方法的局限性，需要一种全新的范式，使多智能体系统能够：\n- 自主生成其他多智能体系统（自生成）\n- 根据任务需求动态配置系统（自配置）\n- 在运行时监控并修复系统（自修复）\n\n**核心思想**：提出MAS²范式，基于递归自生成原理——一个多智能体系统自主构建其他多智能体系统，实现真正的自适应能力。\n\n## 4. 方法设计：三智能体元架构\n\n**架构设计**：为实现递归自生成范式，设计了一个元多智能体系统(meta MAS)，包含三个专门化的元智能体：\n\n**生成器(Generator)**：\n- 功能：作为系统架构师，根据任务查询设计高级多智能体工作流模板\n- 输出：包含智能体角色、通信协议和工具集的抽象模板\n\n**实现者(Implementor)**：\n- 功能：将抽象模板转化为完全可执行系统\n- 输出：为每个角色分配具体LLM骨干，形成实例化的MAS\n\n**修复器(Rectifier)**：\n- 功能：实时监控系统执行状态，在检测到故障或资源超限时进行修正\n- 输出：对当前系统配置的修改，从局部调整到全局架构变更\n\n## 5. 训练方法：协作树优化(CTO)\n\n**训练挑战**：如何有效训练这三个专门化的元智能体，使其各司其职又协同工作？\n\n**解决方案**：提出协作树优化(CTO)框架，包含三个关键步骤：\n\n**轨迹收集**：\n- 三个智能体协作扩展决策树，表示多样化的MAS配置和执行路径\n- 生成器扩展K个候选模板，实现者为每个模板扩展N个实例化，修复器根据需要进一步分支\n\n**路径信用传播**：\n- 为每条轨迹（从根到叶节点的路径）分配成本敏感的奖励函数\n- 通过蒙特卡洛估计将终端结果归因于上游决策，为每个节点提供价值信号\n\n**专门化训练**：\n- 将价值注释树转化为包含偏好强度的偏好数据\n- 使用价值缩放损失函数专门化每个智能体的策略，优先学习高置信度决策\n\n## 6. 验证与评估：实验设计\n\n**评估框架**：在七个基准测试上评估MAS²，涵盖四个领域：\n- 多跳搜索（HotpotQA、Bamboogle、NQ）\n- 深度研究（BrowseComp+）\n- 代码生成（HumanEval、MBPP）\n- 数学推理（MATH）\n\n**评估维度**：\n- 性能优势：与现有SOTA方法的比较\n- 成本效益：在性能-成本权衡中的帕累托前沿位置\n- 跨骨干泛化：利用未见过的LLM的能力\n\n## 7. 逻辑链条总结\n\n从观察到创新的完整逻辑链条：\n\n**观察** → 多智能体系统演进过程中存在\"生成一次就部署\"的刚性范式\n  ↓\n**问题识别** → 这种刚性导致系统无法适应现实世界的动态性和不确定性\n  ↓\n**假设提出** → 递归自生成范式能够实现真正的自适应能力\n  ↓\n**方法设计** → 三智能体元架构（生成器-实现者-修复器）实现自生成、自配置、自修复\n  ↓\n**训练方法** → 协作树优化(CTO)框架通过路径信用传播和专门化训练实现元智能体协同\n  ↓\n**验证评估** → 多维度实验证明MAS²在性能、成本效益和泛化能力上的优势\n\n这一逻辑链条从宏观问题出发，逐步聚焦到具体方法设计，体现了作者系统性解决多智能体系统自适应挑战的创新思路。"
                },
            ]
        },
    ],
    "2025-09-29": [
        {
            "name": "Artificial Intelligence",
            "count": 6,
            "papers": [
                {
                    "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
                    "arxiv_id": "2509.22502",
                    "authors": "Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia Li, Ming Li, Hongxia Yang",
                    "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为InfiAgent的金字塔形DAG多智能体框架，该框架通过自我进化机制增强大语言模型的通用问题解决能力。论文完全符合研究目标，原因如下：首先，论文本质上是关于改进LLM基础能力的，提出了通用\"agent-as-a-tool\"机制和智能体自我进化机制，使LLM能够自动分解复杂任务并自主优化其多智能体结构，这直接提升了LLM的通用推理和规划能力。其次，论文包含多个正面指标，如LLM核心概念、自我进化训练方法、多智能体系统和工具使用等新兴范式。第三，论文不聚焦于任何排除标准中的领域，它强调框架可应用于\"无限场景\"，而非特定领域。最后，虽然论文提到了AI研究助手的案例研究，但这只是框架应用的一个示例，框架本身是通用的，旨在提升LLM在各种场景下的通用推理和问题解决能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM智能体开发中依赖手工设计工作流和提示的问题，这些限制阻碍了智能体在各行业的可扩展性和成本效益。针对无限场景的应用需求，我们提出了一种InfiAgent自我进化金字塔智能体框架，该框架基于DAG的多智能体系统，引入了\"智能体即工具\"机制、双重审计机制、智能体路由功能和自我进化机制。在多个基准测试和InfiHelper AI研究助手的案例研究中，通过性能指标和人类评审验证了其有效性，相比ADAS框架实现了9.9%的性能提升。",
                    "summary_translation": "大型语言模型（Large Language Model, LLM）代理（agents）在组织和执行复杂任务方面展现了显著能力，许多此类代理现已在各种应用场景中得到广泛应用。然而，开发这些代理需要精心设计的工作流程（workflows）、精心制作的提示词（prompts）和迭代调优（tuning），这需要LLM技术和特定领域的专业知识。这些手工制作的限制阻碍了LLM代理在广泛行业中的可扩展性（scalability）和成本效益（cost-effectiveness）。为应对这些挑战，我们提出了**InfiAgent**，一个可应用于**无限**（infinite）场景的类似金字塔的基于DAG（Directed Acyclic Graph，有向无环图）的多代理框架（Multi-Agent Framework），该框架引入了几项关键创新：一种通用的\"代理即工具\"（agent-as-a-tool）机制，可自动将复杂代理分解为分层多代理系统；一种双重审计机制（dual-audit mechanism），确保任务完成的质量和稳定性；一种代理路由功能（agent routing function），实现高效的任务-代理匹配；以及一种代理自我进化机制（agent self-evolution mechanism），可根据新任务、性能不佳或优化机会自主重构代理DAG。此外，InfiAgent的原子任务设计（atomic task design）支持代理并行性（agent parallelism），显著提高了执行效率。该框架演变为一个通用的类似金字塔的多代理系统，能够解决广泛的问题。在多个基准测试（benchmarks）上的评估表明，与ADAS（类似的自动生成代理框架）相比，InfiAgent的性能提高了9.9%，而AI研究助手InfiHelper的案例研究（case study）表明，它生成的科学论文获得了顶级IEEE会议人类评审的认可。",
                    "inspiration_trace": "## 面临的挑战\n当前LLM智能体开发依赖精心设计的工作流程和提示，需要专业知识和迭代调优，导致可扩展性和成本效益受限。传统多智能体系统采用点对点协作，造成协调开销、死锁和不可预测行为，缺乏自动分解任务、确保稳定性和自主适应的通用框架。\n\n## 关键洞察\n作者认识到问题本质在于缺乏结构化的智能体组织范式。智能体应被视为可组合工具，形成层次化结构；系统需具备自我进化能力，根据反馈自主调整；同时需要有效的质量保证机制，确保任务完成质量和系统稳定性。\n\n## 解决方案演进\n从洞察出发，作者首先提出\"智能体即工具\"机制，实现复杂智能体的自动分解；继而设计双重审计机制保障质量；然后开发智能路由功能优化任务分配；最后引入自我进化机制，使系统能根据新需求和性能反馈自主重构，形成金字塔状的多智能体架构。\n\n## 创新点总结\n创新点在于将智能体视为可组合工具，实现前所未有的模块化；通过双重审计和自我进化解决稳定性与适应性难题；金字塔结构使系统能处理指数级复杂任务同时保持个体简单性；整个框架通用性强，可自动适应各领域无需专业知识。"
                },
                {
                    "title": "Large Language Models as Nondeterministic Causal Models",
                    "arxiv_id": "2509.22297",
                    "authors": "Sander Beckers",
                    "summary": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种将大语言模型解释为非确定性因果模型的新方法，用于生成LLM的反事实推理。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是因果推理和反事实推理能力，这属于逻辑推理的重要范畴，而非将LLM应用于特定领域。从第二步正面指标看，论文明确关注大语言模型(LLMs)和推理能力(reasoning)，特别是逻辑推理层面。从第三步排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面。从第四步特殊情况处理看，论文提出的反事实推理方法可以视为增强模型内在推理质量和可解释性的途径，属于提升LLM通用推理能力的研究。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决概率性大型语言模型(LLMs)的反事实生成问题。针对LLMs的采样过程不确定性，我们提出了一种将LLMs视为非确定性因果模型的核心方法，证明了其满足简单语义，即反事实分布与观察分布形式相同。该方法无需修改源代码，可直接应用于任何黑盒LLM。通过理论分析，我们阐明了现有Gumbel-based方法可视为在简单语义基础上引入特定偏差的实现，为未来开发特定应用的反事实生成方法奠定了理论基础。",
                    "summary_translation": "Chatzi等人和Ravfogel等人的最新研究首次开发了一种生成概率性大型语言模型（Large Language Models, LLMs）反事实（counterfactuals）的方法。这类反事实告诉我们，如果某个事实提示（prompt）${\\bf x}$被替换为${\\bf x}^*$，大型语言模型的输出将会或可能是什么。生成这类反事实的能力是解释、评估和比较大型语言模型行为的重要必要步骤。然而，我认为现有方法基于对大型语言模型的模糊解释：该方法没有从字面上解释大型语言模型，因为它假设可以改变大型语言模型采样过程（sampling process）的实现而不改变大型语言模型本身；也没有按照预期解释大型语言模型，因为它将非确定性（nondeterministic）大型语言模型明确表示为确定性因果模型（deterministic causal model）。我在此提出一种更简单的生成反事实的方法，该方法基于大型语言模型的预期解释，将其表示为非确定性因果模型（nondeterministic causal model）。我的简单方法的优点是它可以直接应用于任何黑盒大型语言模型（black-box LLM）而无需修改，因为它对任何实现细节都是不可知的。另一方面，现有方法的优点是它直接实现了特定类型反事实的生成，这对某些目的有用，但对其他目的则不然。我通过提供一个基于大型语言模型预期语义（semantics）的反事实推理理论基础，阐明了这两种方法之间的关系，从而为新颖的特定应用反事实生成方法奠定了基础。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM反事实生成方法存在解释模糊性，要么未按字面解释LLM（假设可改变采样过程而不改变LLM本身），要么未按预期解释（将非确定性LLM表示为确定性因果模型）。这些方法需要修改源代码，不适用于商业黑盒LLM。\n\n## 关键洞察\n作者洞察到LLM应被理解为非确定性因果模型，而非确定性模型。简单语义（反事实分布与观测分布形式相同）是处理LLM反事实的一般语义，而现有Gumbel-based方法只是在此基础上的特定偏差实现。\n\n## 解决方案演进\n作者首先将LLM形式化为条件概率分布，提出理想化和字面两种解释。通过将LLM表示为非确定性因果模型，证明了简单语义的有效性。进而展示现有方法可视为在简单语义上引入特定偏差的实现，最终建立允许开发不同应用场景反事实生成方法的理论框架。\n\n## 创新点总结\n提供了LLM反事实推理的理论基础，证明简单语义作为一般语义的有效性，使反事实查询计算与事实查询相同，将现有方法重新解释为特定偏差情况，为开发新方法铺平道路，且适用于任何黑盒LLM。"
                },
                {
                    "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents",
                    "arxiv_id": "2509.22391",
                    "authors": "Jiaqi Shao, Yuxiang Lin, Munish Prasad Lohani, Yufeng Miao, Bing Luo",
                    "summary": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是评估LLM搜索代理的\"认知能力\"(epistemic competence)，重点关注它们如何进行基于证据的推理、自适应搜索策略以及证据充分性评估。虽然论文主要提出了一个评估基准而非直接改进LLM的方法，但它直接针对LLM的核心推理能力进行评估，特别是信息处理和决策制定方面的通用能力，而非将LLM作为工具应用于特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs)作为搜索代理 - 能力方向：重点研究reasoning（基于证据的推理）和problem-solving（信息搜索问题解决） - 新兴范式：涉及llm-based agents和tool use（搜索工具的使用能力） 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况处理 - 智能体/工具使用：论文研究的是通用的LLM搜索代理能力，而非特定领域应用，符合保留标准 - 认知能力评估：论文关注LLM代理如何基于证据进行推理和评估，这与提高推理质量和减少幻觉相关，符合保留标准 核心贡献：论文提出了SeekBench基准，用于评估LLM搜索代理在信息寻求过程中的认知能力，包括基于证据的推理、搜索策略自适应调整和证据充分性评估。这一基准对于理解和改进LLM的通用推理能力具有重要意义，因为它提供了评估这些能力的方法论，能够推动未来LLM推理能力的研究和提升。因此，尽管论文主要关注评估而非直接改进，但它与\"大语言模型通用推理能力\"的研究目标高度相关。",
                    "summary2": "本文旨在解决LLM搜索代理评估中仅关注最终答案准确性而忽视推理过程的问题。针对信息搜索代理的轨迹数据，我们提出了SeekBench基准，用于评估代理的认知能力，并在7个QA基准数据集上通过推理质量指数(RQI)、证据恢复函数(ERF)和校准误差(CE)等指标验证了其有效性。",
                    "summary_translation": "近期研究探索了使用强化学习(reinforcement learning, RL)训练大型语言模型(Large Language Model, LLM)搜索代理，用于开放域问答(open-domain question answering, QA)。然而，大多数评估仅关注最终答案的准确性，忽视了这些代理如何推理并利用外部证据。我们介绍了SeekBench，这是首个通过步骤级分析响应轨迹来评估LLM搜索代理认知能力(epistemic competence)的基准测试。SeekBench包含190条由专家注释的轨迹，这些轨迹由LLM搜索代理生成，包含超过1,800个响应步骤，每条轨迹都附有证据注释，用于精细分析代理是否：(1)生成基于观察证据的推理步骤，(2)自适应地重新制定搜索以从低质量结果中恢复，以及(3)具有适当的校准能力以正确评估当前证据是否足以提供答案。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM搜索代理评估过于关注最终答案准确性，忽略了代理如何推理和使用外部证据的过程。代理可能获得高基准分数，却表现出无根据声明、无法识别知识缺口等不良认识行为，仅关注结果会误导对代理真实能力的判断。\n\n## 关键洞察\n作者引入\"认识能力\"概念，强调需从\"过程层面\"评估代理。提出三个核心能力：基于证据的推理、自适应证据恢复和证据校准的决策，并通过\"证据状态\"概念捕捉推理过程中信息的质量和充分性。\n\n## 解决方案演进\n首先构建注释模式系统标记代理响应轨迹中的可观察特征；然后分析注释模式识别三种潜在认识能力；最后将这些能力转化为具体量化指标：推理质量指数、证据恢复函数和校准误差，通过大规模实验验证其有效性。\n\n## 创新点总结\n首次提出针对LLM搜索代理的认识能力评估框架，从过程层面而非仅结果层面评估代理，揭示不同代理的专门能力（如Search-R1的合成能力），为开发更可靠的信息搜索代理提供新思路。"
                },
                {
                    "title": "Can AI Perceive Physical Danger and Intervene?",
                    "arxiv_id": "2509.21651",
                    "authors": "Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati, Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani",
                    "summary": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究基础模型（包括大语言模型）在物理安全场景中的推理能力。论文的核心贡献包括：1) 开发了一种评估具身AI系统物理安全性的基准测试方法；2) 分析了主要基础模型在感知风险、安全推理和触发干预方面的能力；3) 提出了一种后训练范式，教导模型明确推理具身特定的安全约束。这些贡献本质上是关于改进LLM的推理能力，特别是安全推理能力，属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文明确研究\"foundation models\"，这包括大语言模型 - 能力方向：论文重点研究\"reason about safety\"（安全推理），这是一种重要的通用推理能力 - 新兴范式：论文涉及\"Embodied AI systems\"和\"agentic applications\"，与基于LLM的智能体相关 第三步：排除标准 虽然论文涉及多模态内容（将叙事转换为图像和视频）和机器人领域（具身AI系统），但这些都不是论文的主要焦点。论文的主要目标是提升AI系统的通用安全推理能力，而非专注于多模态技术或特定机器人应用。 第四步：特殊和模糊情况 论文研究的安全性是从提升模型内在推理能力的角度出发，而非应用层面的安全措施。论文提出的方法使安全推理\"interpretable and transparent\"（可解释和透明），这属于提升模型内在可解释性和推理质量的范畴。 综合判断：这篇论文致力于提升大语言模型在物理安全场景中的通用推理能力，提出了一种新的训练范式来增强模型的安全推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决AI在物理世界中的安全感知与干预能力评估问题。针对具身AI系统在物理环境中的安全挑战，我们提出了ASIMOV-2.0基准测试框架，通过生成模型将真实伤害叙述转化为多模态安全场景，并在该基准上通过风险识别准确率、约束违反率等指标验证了主流模型的性能。实验揭示了模型在文本与视频模态间的性能差距，以及具身约束理解不足的问题。我们进一步开发了后训练范式，通过监督微调和强化学习使模型生成可解释的安全推理轨迹，显著提升了安全约束满足能力。",
                    "summary_translation": "当AI作为机器人(robot)或辅助代理(assistive agent)与物理世界互动时，会出现超出纯\"数字AI\"(digital AI)的新安全挑战。在这种互动中，造成物理伤害的潜力是直接和即时的。最先进的基础模型(foundation models)对物理安全的常识事实理解得如何，例如一个盒子可能太重而无法举起，或者一杯热咖啡不应该递给孩子？\n\n在本文中，我们的贡献有三方面：首先，我们开发了一种高度可扩展的方法，用于对具身AI系统(Embodied AI systems)进行连续物理安全基准测试(physical safety benchmarking)，该方法基于现实世界的伤害叙述(injury narratives)和操作安全约束(operational safety constraints)。为了探究多模态安全理解(multi-modal safety understanding)，我们使用先进的生成模型(generative models)将这些叙述和约束转变为捕捉从安全状态到不安全状态转变的逼真图像(photorealistic images)和视频。其次，我们全面分析了主要基础模型(foundation models)感知风险(perceive risks)、推理安全(reason about safety)和触发干预(trigger interventions)的能力；这为它们在安全关键代理应用(safety-critical agentic applications)中的部署准备度(deployment readiness)提供了多方面的见解(multi-faceted insights)。最后，我们开发了一种后训练范式(post-training paradigm)，教导模型明确推理通过系统指令提供的具身特定安全约束(embodiment-specific safety constraints)。由此产生的模型生成思维痕迹(thinking traces)，使安全推理可解释(interpretable)和透明(transparent)，在约束满足评估(constraint satisfaction evaluations)中实现了最先进的性能(state of the art performance)。\n\n该基准测试(benchmark)将在https://asimov-benchmark.github.io/v2发布。",
                    "inspiration_trace": "## 面临的挑战\nAI进入物理世界带来新型安全挑战，现有安全评估局限于文本领域或数字行动，无法充分评估AI对物理危险的感知、推理和干预能力。AI系统在理解物理安全常识和遵守具身约束方面存在显著不足。\n\n## 关键洞察\n物理安全是多维度的，需基于真实世界伤害报告和工业标准评估。不同模态对AI安全理解能力有不同要求，视频模态特别能评估时间维度的危险感知。推理过程可提高AI对安全约束的理解和遵守。\n\n## 解决方案演进\n首先构建ASIMOV-2.0多模态基准，基于真实数据生成场景；评估主流模型识别\"模态差距\"和\"具身差距\"；通过后训练教导模型明确推理具身安全约束，生成可解释的安全推理轨迹；优化推理过程，发现结构化简洁推理比冗长思维链更有效。\n\n## 创新点总结\n将安全评估建立在真实伤害报告基础上；多模态综合评估特别是视频时间维度；集成具身约束更贴近实际应用；通过\"思考轨迹\"使安全决策透明可解释；提出专门针对安全推理的后训练方法。"
                },
                {
                    "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
                    "arxiv_id": "2509.21549",
                    "authors": "Dongkyu Cho, Amy B. Z. Zhang, Bilel Fehri, Sheng Wang, Rumi Chunara, Rui Song, Hengrui Cai",
                    "summary": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了\"决策枢轴\"(decision pivots)的概念和一种新的自我训练流程，旨在增强大语言模型的思维链(CoT)推理能力。这不是将LLM作为工具应用到特定领域，而是直接提升模型本身的通用推理能力，完全符合第一步的保留标准。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)和思维链推理 - 能力方向：直接针对推理能力，特别是在逻辑推理(LogiQA)和数学推理(MATH500)等通用推理能力上 - 训练方法：提出了自我训练流程，属于模型自我改进的方法论 第三，论文不符合任何排除标准。虽然在MedQA数据集上进行了测试，但这仅是评估方法在医疗问题上的效果，论文核心并非医疗领域应用，而是通用推理能力的提升。 最后，在特殊和模糊情况处理上，论文通过\"决策枢轴\"来验证和改进推理路径，这与增强模型推理的可解释性和减少幻觉有关，从而提升模型的通用推理质量，符合保留条件。 综上所述，这篇论文的核心贡献是提出了一种通过挖掘\"决策枢轴\"来增强LLM通用推理能力的新方法，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型链式思维推理中存在的冗余、自相矛盾或逻辑不健全问题。针对缺乏可扩展方法验证和训练高质量解释的挑战，我们提出了一种基于决策枢轴的自训练方法ROMA，并在LogiQA、MedQA和MATH500基准测试上通过任务准确率和推理质量指标验证了其有效性。",
                    "summary_translation": "思维链(Chain-of-thought, CoT)推理揭示了大型语言模型(large language models, LLMs)的中间思维过程，然而大规模验证这些推理轨迹仍然是一个未解决的问题。为应对此问题，我们提出了决策支点(decision pivots)的概念——即任何正确推理路径都必须经过的最小且可验证的检查点。我们假设，尽管正确的推理在风格上可能多样，但它们会收敛于同一支点集合，而错误的推理则会违反至少一个支点。利用这一特性，我们提出了一个自训练流程(self-training pipeline)，该流程包括：(i) 采样多样化的推理路径并挖掘共享的决策支点；(ii) 使用辅助验证器(auxiliary verifier)将每条轨迹压缩为以支点为中心的短路径推理；(iii) 使用模型自生成的输出对模型进行后训练。所提出的方法能够在没有真实推理数据(ground truth reasoning data)或外部指标(external metrics)的情况下对齐推理。在LogiQA、MedQA和MATH500等标准基准测试上的实验证明了我们方法的有效性。",
                    "inspiration_trace": "## 面临的挑战\n大语言模型生成的思维链推理常冗余、自相矛盾或逻辑不健全，而现有训练方法只评估整体响应，无法有效监督底层推理过程。缺乏可扩展方法验证和训练高质量解释，尤其在没有人工标注的情况下。\n\n## 关键洞察\n作者发现推理任务由\"决策枢轴\"主导——任何正确解释必须经过的最小可验证检查点。正确推理虽风格多样但收敛于相同枢轴集合，而不正确推理则至少违反一个枢轴，如同\"条条大路通罗马\"。\n\n## 解决方案演进\n从这一洞察出发，作者首先提出通过采样多样化推理路径挖掘共享决策枢轴；然后设计辅助验证器将推理压缩为专注于枢轴的短路径；最后利用自生成输出进行后训练，形成完整自训练循环，无需人工标注。\n\n## 创新点总结\n首次形式化决策枢轴概念，证明正确推理集中在紧凑枢轴集合上；提出不依赖真实推理数据或外部指标的对齐方法；通过多路径聚合提炼出简洁一致的推理路径，建立完全模型驱动的可扩展训练框架。"
                },
                {
                    "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                    "arxiv_id": "2509.21391",
                    "authors": "Lihui Liu, Carl J. Yang",
                    "summary": "Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出MIXRAG，一个基于混合专家的图检索增强生成框架，旨在增强大语言模型在处理图结构数据时的推理能力。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，该论文专注于改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文提出的MIXRAG框架通过引入多个专门的图检索器和动态路由控制器来处理不同查询意图，并引入查询感知的图编码器来减少噪声，这些都是增强LLM通用推理能力的方法论创新。 其次，论文包含多个正面指标：明确讨论大语言模型(LLMs)，关注推理能力(论文提到\"更精确和可解释的推理\")，并涉及检索增强生成(RAG)这一新兴工具使用范式。 第三，论文不涉及排除标准中的领域：没有关注多模态与视觉问题，没有专注于特定应用领域(虽然提到\"不同领域的图任务\"，但图是一种通用数据结构而非特定领域)，也没有讨论模型基础设施或部署优化。 最后，在特殊情况下，该论文提出的RAG框架可视为一种通用的工具使用方法来增强LLM的问题解决能力，同时论文关注减少LLM幻觉问题，提升其推理质量，这些都符合研究目标。 综上所述，MIXRAG论文致力于提高LLM本身的通用推理能力，特别是通过改进检索增强机制来增强模型对结构化数据的理解和推理，因此符合研究范围。",
                    "summary2": "本文旨在解决基于文本图的检索增强生成系统中单一检索器难以处理多样化查询意图和检索内容噪声干扰的问题。针对文本图问答任务，我们提出了一种基于专家混合(Mixture-of-Experts)的检索增强生成框架(MIX RAG)，该框架包含多个专门图检索器和动态路由控制器，以及查询感知的GraphEncoder。在GraphQA benchmark上通过准确率(ACC)和Hit@1指标验证了其有效性，在ExplaGraphs、SceneGraphs和WebQSP三个数据集上均取得了最先进的性能。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在广泛的应用领域取得了令人印象深刻的性能。然而，由于依赖静态的预训练语料库，它们在知识密集型领域经常出现幻觉（hallucinations）问题。为了解决这一限制，检索增强生成（Retrieval-Augmented Generation, RAG）通过在推理过程中整合外部知识源来增强大型语言模型。在这些知识源中，文本图（textual graphs）提供了结构化和语义丰富的信息，支持更精确和可解释的推理。这导致了对基于图的RAG系统日益增长的兴趣。\n\n尽管具有潜力，大多数现有方法依赖于单个检索器（retriever）来识别相关子图，这限制了它们捕捉复杂查询多方面特征的能力。此外，这些系统往往难以准确判断检索内容的相关性，使其容易受到无关噪声的干扰。为了应对这些挑战，在本文中，我们提出了MIXRAG，一个专家混合图RAG框架（Mixture-of-Experts Graph-RAG framework），它引入了多个专门的图检索器和动态路由控制器，以更好地处理多样化的查询意图。每个检索器被训练专注于图语义的特定方面，如实体、关系或子图拓扑。专家混合模块（Mixture-of-Experts module）根据输入查询自适应地选择和融合相关检索器。\n\n为了减少检索信息中的噪声，我们引入了一个查询感知图编码器（query-aware GraphEncoder），它仔细分析检索到的子图内的关系，突出最相关的部分，同时降低不必要噪声的权重。实验结果表明，我们的方法达到了最先进的性能（state-of-the-art performance），并持续优于各种基线方法。MIXRAG在不同领域的各种基于图的任务中都表现出有效性。代码将在论文接受后发布。",
                    "inspiration_trace": "## 面临的挑战\n现有图检索增强生成系统依赖单一检索器，难以捕捉文本图中多方面信息（实体、关系、拓扑结构）；同时检索到的子图常含噪声信息，会误导LLM生成不准确回答。\n\n## 关键洞察\n不同查询类型需要不同检索策略，单一检索器无法适应查询意图多样性；检索到的知识价值不等，需动态识别有用信息并过滤噪声；查询与检索专长的动态匹配是提升效果关键。\n\n## 解决方案演进\n从单一检索局限性出发，引入多个专业检索器分别处理不同图语义；借鉴MoE概念设计动态路由控制器，根据查询自适应融合专家；针对噪声问题，设计查询感知GraphEncoder分析子图关系，突出相关信息抑制噪声；整合为统一框架MIXRAG。\n\n## 创新点总结\n首创MoE机制用于图检索增强生成，实现多方面图知识自适应检索；提出查询感知图编码方法动态调整关注度；同时解决检索多样性和噪声问题，显著提升图理解与问答能力。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 36,
            "papers": [
                {
                    "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
                    "arxiv_id": "2509.22638",
                    "authors": "Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang",
                    "summary": "LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种新的训练范式——反馈条件策略(FCP)，让大语言模型能够直接从语言反馈中学习，而不是将反馈压缩为标量奖励。这明显属于改进LLM基础能力的研究，特别是强化学习优化的新方法，符合\"保留\"标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并涉及强化学习(RL from human or AI feedback)这一训练方法。虽然论文没有直接提及推理、规划等能力，但改进LLM从反馈中学习的能力很可能间接提升这些通用推理能力。 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及需要特殊判断的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是提出了一种更丰富、更具表现力的方式让LLM直接从语言反馈中学习，这改变了传统的强化学习反馈机制，将反馈驱动的学习重新定义为条件生成而非奖励优化。这种方法有望提升LLM的通用学习能力和推理质量，因此完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决传统RLHF方法将丰富语言反馈压缩为标量奖励导致信息丢失和奖励不平衡问题。针对语言模型从语言反馈中学习的场景，我们提出了一种反馈条件策略（FCP），将语言反馈视为条件信号而非标量奖励，通过离线最大似然训练和在线引导阶段直接从响应-反馈对中学习。在数学推理（Big-Math）和一般推理（WebInstruct）数据集上通过准确率等指标验证了FCP能有效匹配或超越基于标量的强基线方法（如RFT和GRPO），实现了无需标量奖励的反馈学习。",
                    "summary_translation": "LLMs（大型语言模型）通常通过来自人类或AI反馈的强化学习（RL）进行训练，然而这类方法通常将细致入微的反馈压缩为标量奖励，从而丢弃了反馈的丰富性并导致规模不平衡。我们提出将语言反馈（verbal feedback）视为一种条件信号（conditioning signal）。受文本到图像生成中的语言先验（language priors）启发，这种先验能够从未见过的提示中产生新颖输出，我们引入了反馈条件策略（feedback-conditional policy, FCP）。FCP直接从响应-反馈对中学习，通过在离线数据上进行最大似然训练来近似反馈条件后验（feedback-conditional posterior）。我们进一步开发了一种在线自举（online bootstrapping）阶段，在该阶段中，策略在积极条件下生成并接收新鲜反馈以自我完善。这将反馈驱动的学习重新构建为条件生成（conditional generation）而非奖励优化（reward optimization），为LLMs提供了一种更具表现力的方式来直接从语言反馈中学习。我们的代码可在 https://github.com/sail-sg/feedback-conditional-policy 获取。",
                    "inspiration_trace": "## 面临的挑战\n传统强化学习方法将丰富的语言反馈压缩成标量奖励，导致信息损失、反馈模糊性和跨任务奖励尺度不平衡三大问题。例如，不同类型的问题反馈可能被映射到相同奖励值，而混合、情绪化的反馈难以准确标量化。\n\n## 关键洞察\n作者认识到LLMs具有强大的语言先验和隐式理解反馈的能力，无需将反馈转化为标量。受文本到图像生成启发，语言模型能像组合未见过的文本提示一样，吸收多样化语言反馈并生成高质量响应。\n\n## 解决方案演进\n作者将反馈驱动学习重新定义为条件生成问题，提出反馈条件策略(FCP)。通过最小化前向KL散度，直接从响应-反馈对中学习反馈条件后验分布。采用两阶段训练：离线阶段初始化策略，在线阶段通过积极条件生成和新鲜反馈引导自我改进。\n\n## 创新点总结\n该方法绕过了奖励假设，保留了语言反馈的丰富性，能自然处理混合反馈，避免跨任务奖励不平衡，充分利用了LLMs的语言先验能力，为语言模型对齐提供了新范式。"
                },
                {
                    "title": "Variational Reasoning for Language Models",
                    "arxiv_id": "2509.22637",
                    "authors": "Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang",
                    "summary": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化它们，这明显是关于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 论文符合多个正面指标：1) 核心概念方面，明确研究语言模型(LLMs)；2) 能力方向方面，直接聚焦于推理能力(reasoning)，并在摘要中多次提及\"reasoning ability\"和\"reasoning tasks\"；3) 训练方法方面，论文将变分推断与RL风格方法结合，提到了\"binary-reward RL, including GRPO\"等强化学习方法；4) 新兴范式方面，论文研究的\"thinking traces\"与思维链(CoT)等推理方法密切相关。 论文不符合任何排除标准：没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。论文也没有涉及需要特殊处理的模糊情况，如特定领域的智能体应用或应用层面的幻觉/可解释性/安全问题。 论文的核心贡献是提供了一个统一的概率视角，将变分推断与RL风格方法结合，为提高语言模型的推理能力提供了稳定的目标，这与研究目标\"提高大语言模型本身的通用推理能力\"高度一致。",
                    "summary2": "本文旨在解决语言模型推理能力训练的稳定性与原则性问题。针对思维轨迹与答案生成的联合优化场景，我们提出了一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化，扩展了ELBO到多轨迹目标并提出了前向KL公式。在Qwen 2.5和Qwen 3模型系列上，通过MATH500、AIME24&25、OlympiadBench、LiveCodeBench等多种推理任务的准确率指标验证了其有效性，显著提升了模型的推理性能。",
                    "summary_translation": "我们提出了一种针对语言模型的变分推理框架（variational reasoning framework），该框架将思维轨迹（thinking traces）视为潜在变量（latent variables），并通过变分推断（variational inference）对其进行优化。从证据下界（evidence lower bound, ELBO）出发，我们将其扩展为多轨迹目标（multi-trace objective）以获得更紧致的界限，并提出一个前向KL公式（forward-KL formulation），该公式稳定了变分后验（variational posterior）的训练。我们进一步表明，拒绝采样微调（rejection sampling finetuning）和二元奖励强化学习（binary-reward RL），包括GRPO，可以被解释为局部前向KL目标（local forward-KL objectives），其中模型准确性的隐式权重自然地从推导过程中产生，并揭示了一个先前未被注意到的对简单问题的偏向。我们在Qwen 2.5和Qwen 3模型系列上，通过广泛的推理任务（reasoning tasks）实证验证（empirically validate）了我们的方法。总体而言，我们的工作提供了一个原则性的概率视角（probabilistic perspective），统一了变分推断与强化学习方法（RL-style methods），并产生了稳定的目标函数（stable objectives），用于提高语言模型的推理能力（reasoning ability）。我们的代码可在https://github.com/sail-sg/variational-reasoning获取。",
                    "inspiration_trace": "## 面临的挑战\n现有语言模型推理能力训练方法存在根本局限：监督微调依赖精心策划的思维轨迹，成本高且泛化差；强化学习方法训练不稳定，输出多样性易崩溃，导致对困难问题的正确答案变得稀少。\n\n## 关键洞察\n作者将推理过程重新概念化为概率建模问题，将思维轨迹视为潜在变量。变分推断提供了一种优化正确答案对数似然的自然方式，通过可处理的下界替代不可行的思维轨迹边缘化，并引入变分后验采样更可能产生正确答案的思考路径。\n\n## 解决方案演进\n从基本证据下界(ELBO)出发，扩展到多轨迹目标以获得更紧致界限；提出前向KL公式稳定变分后验训练；进而发现拒绝采样微调和二元奖励RL可解释为局部前向KL目标，揭示这些方法中存在对简单问题的隐式偏差。\n\n## 创新点总结\n提供了统一变分推断与RL方法的原则性概率视角，揭示了现有方法中的隐式偏差，为理解主流方法提供了新理论框架，同时提出了稳定目标函数改进语言模型推理能力。"
                },
                {
                    "title": "Think Socially via Cognitive Reasoning",
                    "arxiv_id": "2509.22546",
                    "authors": "Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang",
                    "summary": "LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Cognitive Reasoning\"（认知推理）的新范式和CogFlow框架，用于增强大语言模型在社交情境中的推理能力。从本质上看，论文属于改进LLM的基础推理能力，提出新的训练范式（结合监督微调和强化学习），而非将LLM作为工具应用到特定领域。论文明确研究LLMs的推理能力，并采用强化学习方法进行优化，这些都是研究目标的正面指标。虽然论文关注社交情境，但这应被视为一种通用推理能力，而非医疗、化学等特定应用领域。社交推理是人类普遍需要的基础能力，论文提出的认知推理范式具有通用性，通过树结构规划和强化学习来提升模型推理能力，因此该研究完全符合\"提高大语言模型通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决大型语言模型在社交情境中推理能力不足的问题。针对社交情境中模糊线索分析的需求，我们提出了一种基于认知单元结构化流的Cognitive Reasoning范式和CogFlow框架，并在Reddit收集的5100个社交情境数据集上通过比较偏好排名和人类评估等指标验证了其有效性。",
                    "summary_translation": "为逻辑推理训练的大型语言模型（LLMs，Large Language Models）擅长通过逐步演绎（step-by-step deduction）得出可验证的答案。然而，这种范式（paradigm）并不适合处理社交情境，社交情境引发一种解释过程（interpretive process），即分析很少产生明确结果的模糊线索（ambiguous cues）。为弥合这一差距，我们引入了认知推理（Cognitive Reasoning），这是一种模拟人类社交认知（human social cognition）的范式。它将解释过程构建为一个由相互关联的认知单元（cognitive units，如观察或归因）组成的结构化认知流（cognitive flow），这些认知单元自适应地结合，以实现有效的社交思维和响应。随后，我们提出了CogFlow，一个将这种能力灌输给LLMs的完整框架。CogFlow首先通过树状结构规划（tree-structured planning）模拟人类思维的联想性和渐进性，策划一个认知流数据集。在通过监督微调（supervised fine-tuning）灌输基本认知推理能力后，CogFlow采用强化学习（reinforcement learning）使模型通过试错（trial and error）自我改进，由优化认知流和响应质量的多目标奖励（multi-objective reward）指导。大量实验表明，CogFlow有效增强了LLMs甚至人类的社交认知能力（social cognitive capabilities），从而实现更有效的社交决策（social decision-making）。",
                    "inspiration_trace": "## 面临的挑战\nLLMs在逻辑推理任务上表现出色，但这种范式不适合社交情境。社交情境涉及分析模糊线索，很少产生确定性答案，而当前LLMs的推理结构与社交智能本质不匹配，甚至会导致\"认知反刍\"问题。\n\n## 关键洞察\n作者认识到社交认知是人类社交智能的核心，是一个动态过程，可通过构建内部认知地图来驾驭复杂社交情境。人类社交认知可分解为六个核心认知单元：观察、归因、动机、调节、效能和行为，它们之间相互流动形成结构化推理过程。\n\n## 解决方案演进\n作者首先将社交认知形式化为\"认知推理\"，提出CogFlow框架。通过树状结构规划模拟人类思维，收集认知流数据集；然后通过有监督微调灌输基本认知推理能力；最后采用强化学习使模型通过试错自我改进，由多目标奖励同时优化认知流和响应质量。\n\n## 创新点总结\n首次提出专门用于社交思考的认知推理范式，通过认知单元结构化交互实现社交智能。创新性地结合基于偏好的SFT和多目标RL，不仅增强LLMs社交认知能力，还发现这种方法有潜力提升人类社交智能。"
                },
                {
                    "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving",
                    "arxiv_id": "2509.22480",
                    "authors": "Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang",
                    "summary": "Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究LLM解决方案差异性(solution divergence)对问题解决能力的影响，并提出一种新的训练和评估指标。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用于特定领域。论文提出的解决方案差异性指标可以支持监督微调(SFT)和强化学习(RL)策略，目的是提升LLM的通用问题解决能力，符合保留标准。 其次，从正面指标分析，论文符合多个关键指标： - 核心概念：明确研究Large language models (LLMs) - 能力方向：关注problem-solving能力，这是推理能力的核心组成部分 - 训练方法：提出的指标可以支持reinforcement learning (RL)策略 第三，论文不符合任何排除标准。虽然提到在\"三个代表性问题领域\"进行测试，但这些领域只是用来验证通用方法的实验平台，而非论文的主要焦点。论文的核心是提出一种通用的训练和评估方法，而非针对特定应用领域。 综上所述，该论文的核心贡献是发现并验证了解决方案差异性这一新指标对提升LLM问题解决能力的有效性，这直接涉及到改进LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在提升大型语言模型的问题解决能力。针对单一问题的多种解决方案，我们提出了一种解决方案发散性作为新的评估指标，并将其应用于监督微调和强化学习中。我们在数学(Math-500)、编程(MBPP+)和逻辑推理(Maze)三个代表性问题领域上通过Pass@1和Pass@10成功率验证了其有效性，实验结果表明利用解决方案发散性可显著提升模型性能。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）已被广泛应用于问题解决任务。最近的研究主要通过有标签数据的监督微调（supervised fine-tuning, SFT）或基于任务反馈的强化学习（reinforcement learning, RL）来提高其性能。在本文中，我们研究了一个新视角：大型语言模型为单个问题生成的解决方案之间的差异性（divergence）。我们表明，更高的解决方案差异性与更好的问题解决能力呈正相关，这一现象在各种模型中普遍存在。基于这一发现，我们提出将解决方案差异性（solution divergence）作为一种新的评估指标，它可以同时支持监督微调和强化学习策略。我们在三个代表性问题领域测试了这一想法，发现利用解决方案差异性能够持续提高成功率。这些结果表明，解决方案差异性是一个简单但有效的工具，可用于推动大型语言模型的训练和评估。",
                    "inspiration_trace": "## 面临的挑战\n作者识别到当前提升LLM问题解决能力的方法主要依赖有监督微调和强化学习，但这些方法需要大量标注数据或复杂奖励设计，成本高昂且劳动密集，同时忽视了单个问题存在多种有效解决方案的重要特性。\n\n## 关键洞察\n作者从认知科学研究中获得独特视角：人类拥有更多问题解决策略时在复杂任务上表现更佳。通过实验发现LLM的解决方案发散性与问题解决能力呈正相关，特别是在中等难度问题上，表明发散性可作为评估和提升LLM性能的新指标。\n\n## 解决方案演进\n基于这一洞察，作者提出两种方法：1)将解决方案发散性作为数据集选择标准，在SFT中优先选择高发散性样本；2)设计融合发散性的奖励函数，在RL训练中同时优化正确性和多样性，分别应用于训练数据选择和奖励设计两个环节。\n\n## 创新点总结\n该思路创新在于首次将解决方案发散性形式化为可量化指标并与认知科学理论联系；将这一指标同时应用于SFT和RL两种主流训练范式；通过利用现有问题的多解性，避免了生成新数据的成本和复杂性。"
                },
                {
                    "title": "Detecting (Un)answerability in Large Language Models with Linear Directions",
                    "arxiv_id": "2509.22449",
                    "authors": "Maor Juliet Lavi, Tova Milo, Mor Geva",
                    "summary": "Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection, focusing on extractive question answering (QA) where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach for identifying a direction in the model's activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the model's abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four extractive QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Moreover, the obtained directions extend beyond extractive QA to unanswerability that stems from factors, such as lack of scientific consensus and subjectivity. Last, causal interventions show that adding or ablating the directions effectively controls the abstention behavior of the model.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进LLM的基础能力，具体是增强模型判断问题可回答性的元认知能力。论文提出了一种新方法，通过在模型激活空间中识别捕捉\"不可回答性\"的线性方向，来提升模型自我评估能力，从而减少幻觉产生。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域，因此应保留。 第二步正面指标：论文明确研究Large language models (LLMs)，符合核心概念指标。虽然不直接研究数学或逻辑推理，但模型判断问题可回答性的能力与推理质量密切相关，属于推理能力的基础支撑。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面（如水印、安全等），不符合任何排除标准。 第四步特殊和模糊情况：论文研究减少幻觉的问题，但提出的是一种新方法（激活空间中的线性方向）来增强模型内在的自我评估能力，属于提升模型内在可靠性和推理质量的方法，而非社会学研究或应用层面讨论，应保留。 最终决策：论文的核心贡献是提出了一种提升LLM自我评估能力的方法，这属于通用推理能力的重要组成部分。高质量的推理不仅需要逻辑和数学能力，还需要模型能够准确判断自己是否有足够信息回答问题。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在缺乏信息时仍自信回答导致幻觉的问题。针对抽取式问答场景，我们提出了一种在模型激活空间中识别线性方向来检测无法回答性的方法，并在Llama-3-8B-Instruct和Gemma-3-12B-IT模型上通过SQuAD、RepLiQA、NQ和MuSiQue四个数据集的F1分数验证了其有效性。",
                    "summary_translation": "大型语言模型（LLMs）即使在缺乏必要信息的情况下也经常自信地回答问题，导致产生幻觉答案（hallucinated answers）。在本研究中，我们探讨（无法）回答性检测（(un)answerability detection）问题，专注于抽取式问答（extractive question answering, QA），其中模型需要确定一段文本是否包含足够的信息来回答给定问题。我们提出了一种简单方法，用于识别模型激活空间（activation space）中捕捉无法回答性的方向，并将其用于分类。该方向通过在推理过程中应用激活加法（activation additions）并测量其对模型回避行为（abstention behavior）的影响来选择。我们表明，将隐藏激活（hidden activations）投影到该方向上会产生一个可靠的（无法）回答性分类分数。在两个开放权重大型语言模型（open-weight LLMs）和四个抽取式问答基准测试（extractive QA benchmarks）上的实验表明，我们的方法能有效检测无法回答的问题，并且在跨数据集上的泛化能力优于现有的基于提示（prompt-based）和基于分类器（classifier-based）的方法。此外，所获得的方向不仅适用于抽取式问答，还扩展到由其他因素导致的无法回答性，如缺乏科学共识（scientific consensus）和主观性（subjectivity）。最后，因果干预（causal interventions）表明，添加或消除这些方向可以有效控制模型的回避行为。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型在缺乏必要信息时仍会自信回答问题，导致产生幻觉答案。特别是在抽取式问答任务中，现有方法（如基于提示和分类器的方法）难以可靠检测不可回答问题，且在不同数据集上泛化能力差。\n\n## 关键洞察\n作者洞察到\"不可回答性\"可能像情感、真实性等抽象概念一样，被线性编码在模型内部表示空间中。这一视角转变了问题本质——不再是设计外部检测机制，而是发现模型内部已有的表示结构。\n\n## 解决方案演进\n基于此洞察，作者首先计算可回答与不可回答示例的平均激活差异生成候选方向；然后创新地使用\"因果转向\"方法，通过添加候选向量并测量其对模型回避行为的影响来选择最佳方向；最后利用选定方向构建简单分类器，通过投影激活得到不可回答性分数。\n\n## 创新点总结\n创新点在于将不可回答性视为内部表示空间的线性方向，利用因果干预验证方向有效性，构建轻量可解释的检测方法，且展现出更强的跨数据集泛化能力，揭示了模型内部一致编码的不可回答性信号。"
                },
                {
                    "title": "Transformers Can Learn Connectivity in Some Graphs but Not Others",
                    "arxiv_id": "2509.22343",
                    "authors": "Amit Roy, Abulhair Saparov",
                    "summary": "Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on \"grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究Transformer模型（大语言模型）在传递关系推理方面的能力，这是一种基础的逻辑推理能力。论文探讨了模型如何从训练数据中学习传递关系（如\"A导致B，B导致C，则A导致C\"），这等价于在有向图中推断连通性。研究发现Transformer能够学习某些类型图结构的连通性，并且模型规模的增加会提高这种推理能力。这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文明确研究\"Reasoning capability\"和\"transitive relations\"的逻辑推理能力，这些都是大语言模型通用推理能力的核心组成部分。论文没有涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性应用层面），而是专注于提升模型本身的逻辑推理能力这一核心目标。",
                    "summary2": "本文旨在 [研究Transformer学习图连通性的能力]。针对 [有向图连通性任务]，我们提出了一种 [生成网格图和断开链图训练Transformer的方法]，并在 [合成图数据] 上通过 [准确率和损失] 验证了其有效性。",
                    "summary_translation": "推理能力对于确保基于transformer的大型语言模型（Large Language Models, LLMs）回答的事实正确性至关重要，而对传递关系（transitive relations）的稳健推理在许多场景中都发挥着重要作用，例如因果推断（causal inference）。因此，研究transformer在推断传递关系任务中的能力至关重要（例如，已知A导致B，B导致C，则A导致C）。推断传递关系的任务等价于有向图（directed graphs）中的连通性（connectivity）任务（例如，已知存在从A到B的路径，且存在从B到C的路径，则存在从A到C的路径）。\n\n过去的研究主要关注transformer是否能从输入提示（input prompt）中提供的上下文示例（in-context examples）学习推断传递性（transitivity）。然而，transformer从训练示例（training examples）推断传递关系的能力，以及规模扩展（scaling）如何影响这种能力，尚未得到探索。在本研究中，我们通过生成有向图来训练不同大小的transformer模型，并评估它们推断不同大小图中传递关系的能力，旨在回答这一问题。\n\n我们的研究结果表明，transformer能够在\"网格状\"（grid-like）有向图上学习连通性，其中每个节点可以嵌入到低维子空间（low-dimensional subspace）中，且连通性可以很容易地从节点嵌入（embeddings）中推断出来。我们发现，底层网格图（grid graph）的维度（dimensionality）是transformer学习连通性能力的强预测因子（predictor），其中高维网格图比低维网格图构成更大挑战。此外，我们观察到增加模型规模会导致在推断网格图连通性方面的泛化能力（generalization）越来越强。然而，如果图不是网格图且包含许多不连通组件（disconnected components），transformer在学习连通性任务时会遇到困难，尤其是当组件数量较大时。",
                    "inspiration_trace": "## 面临的挑战\n作者识别到Transformer模型从训练示例中学习传递关系推理（等价于图连通性）的能力尚未被探索，特别是模型规模和图规模如何影响这种学习能力仍不清楚。现有研究主要关注上下文学习，而非训练阶段的学习能力。\n\n## 关键洞察\n作者洞察到图的拓扑结构是关键：Transformer更容易学习那些节点可嵌入低维子空间的图（如网格图），因为连通性可从节点嵌入中推断；而断开的链图不具备这种特性，难以低维嵌入，导致学习困难。\n\n## 解决方案演进\n基于此洞察，作者设计对比实验：生成不同维度和规模的网格图与断开链图，训练不同规模Transformer模型学习连通性任务。通过系统分析模型在不同图结构、规模下的表现，验证了图的拓扑结构和规模因素对学习效果的影响机制。\n\n## 创新点总结\n创新点在于揭示了图的拓扑结构（特别是可低维嵌入性）对Transformer学习传递关系的关键影响，发现了模型规模和图规模对不同拓扑结构图的不同影响规律，为理解Transformer的逻辑推理能力提供了新视角。"
                },
                {
                    "title": "Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs",
                    "arxiv_id": "2509.22251",
                    "authors": "Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong",
                    "summary": "Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出SSKG-LLM模型架构，通过有效整合知识图谱的结构和语义信息来减轻大语言模型的幻觉问题，并增强其事实推理能力。从第一步核心判断来看，论文明确聚焦于改进LLM的基础推理能力，特别是事实推理这一通用能力，而非将LLM作为工具应用到特定领域。论文包含多个正面指标：明确关注Large Language Models (LLMs)和reasoning能力。在排除标准方面，论文不涉及多模态、特定应用领域或模型基础设施优化。对于特殊情况的处理，论文虽然涉及幻觉问题，但它是从模型内部机制提出解决方案，通过结构化知识整合来提升推理质量，而非应用层面的讨论。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，应被保留。",
                    "summary2": "本文旨在减轻LLMs的幻觉问题，有效整合知识图谱的结构和语义信息。针对知识图谱与大型语言模型的集成场景，我们提出了一种SSKG-LLM模型架构，包含KGR、KGE和KGA三个模块，通过GraphLM编码知识图谱结构和语义信息，并使用交叉注意力对齐嵌入空间，并在CommonsenseQA、SIQA和TruthfulQA问答数据集上通过准确率(ACC)、ROUGE-N和BLEU指标验证了其有效性。",
                    "summary_translation": "目前，大型语言模型（Large Language Models, LLMs）解决幻觉问题（hallucination issue）的主要方法是整合知识图谱（Knowledge Graphs, KGs）。然而，LLMs通常将KGs视为纯文本，仅提取语义信息，限制了其对KGs关键结构方面的利用。另一个挑战是KGs编码器（KGs encoders）的嵌入空间（embedding spaces）与LLMs文本嵌入（text embeddings）之间的差距，这阻碍了结构化知识（structured knowledge）的有效整合。为克服这些障碍，我们提出了SSKG-LLM，这是一种创新的模型架构，旨在高效地将KGs的结构（Structural）和语义（Semantic）信息整合到LLMs的推理过程中。SSKG-LLM整合了知识图谱检索（Knowledge Graph Retrieval, KGR）模块和知识图谱编码（Knowledge Graph Encoding, KGE）模块，以在利用结构的同时保留语义。随后，整合了知识图谱适应（Knowledge Graph Adaptation, KGA）模块，使LLMs能够理解KGs的嵌入（embeddings）。我们进行了广泛的实验并提供了详细分析，以探索整合KGs的结构信息如何增强LLMs的事实推理（factual reasoning）能力。我们的代码可在https://github.com/yfangZhang/SSKG-LLM获取。",
                    "inspiration_trace": "## 面临的挑战\nLLMs存在幻觉问题，而现有KGs整合方法各有局限：GraphRAG将KGs视为纯文本忽略结构信息；传统图编码器方法难以捕获完整语义且存在表示空间不对齐；LLMs-based KBQA方法无法回答KGs未覆盖的问题。\n\n## 关键洞察\n作者认识到KGs同时包含语义属性和结构特征，两者结合对增强LLMs理解至关重要；同时发现LLMs与图编码器间存在根本性数据格式差异，造成表示空间不对齐，阻碍了有效知识整合。\n\n## 解决方案演进\n针对结构与语义整合，采用图遍历提取相关子图，用GraphLM编码保留双重信息；针对表示空间差距，设计KG-Adapter模块通过交叉注意力机制将图和文本编码视为不同模态进行对齐，最终形成KGR-KGE-KGA的完整框架。\n\n## 创新点总结\n首次同时整合KGs的语义和结构信息；揭示并解决表示空间不对齐问题；通过DFS生成类似CoT的推理链增强推理能力；使用GraphLM编码器平衡结构性与语义性信息捕获。"
                },
                {
                    "title": "Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs",
                    "arxiv_id": "2509.22338",
                    "authors": "Felix Vossel, Till Mossakowski, Björn Gehrke",
                    "summary": "Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究如何通过微调LLMs来提高它们将自然语言转换为一阶逻辑(FOL)的能力。一阶逻辑转换是逻辑推理的基础能力，属于LLM的通用推理能力范畴，而不是将LLM作为工具应用到特定领域。论文关注的是改进LLM本身的基础能力（逻辑推理和形式化表示），符合第一步的保留标准。 其次，从正面指标分析，论文明确符合两个关键指标： 1) 核心概念：论文直接研究\"fine-tuned LLMs\"，使用了Flan-T5-XXL、GPT-4o和DeepSeek-R1等大语言模型。 2) 能力方向：论文聚焦于logical reasoning，通过自然语言到一阶逻辑的转换来增强模型的逻辑推理能力，这是通用推理能力的核心组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型可靠性问题（如水印、安全等）。 论文的核心贡献是提出了一种系统评估和改进LLMs自然语言到一阶逻辑转换能力的方法，通过微调技术、架构比较和训练策略优化，显著提升了模型在逻辑推理任务上的表现。这种能力是通用的，可以应用于多个领域，而不是局限于特定应用场景，因此完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决自然语言到一阶逻辑自动翻译的挑战性问题。针对MALLS和Willow数据集，我们提出了一种使用微调LLMs的方法，并通过精确匹配、逻辑等价性和谓词对齐指标验证了其有效性。微调的Flan-T5-XXL模型在提供谓词列表情况下达到70%准确率，优于GPT-4o和DeepSeek-R1-0528等模型。",
                    "summary_translation": "将自然语言自动翻译为一阶逻辑(first-order logic, FOL)对于知识表示(knowledge representation)和形式方法(formal methods)至关重要，但仍然具有挑战性。我们针对这项任务对经过微调(fine-tuned)的大型语言模型(large language models, LLMs)进行了系统性评估，比较了不同架构(编码器-解码器encoder-decoder与仅解码器decoder-only)和训练策略。利用MALLS和Willow数据集，我们探索了词汇扩展(vocabulary extension)、谓词调节(predicate conditioning)和多语言训练(multilingual training)等技术，并引入了用于精确匹配(exact match)、逻辑等价性(logical equivalence)和谓词对齐(predicate alignment)的评估指标。我们经过微调的Flan-T5-XXL模型在提供谓词列表的情况下达到了70%的准确率，优于GPT-4o以及具备思维链(chain-of-thought, CoT)推理能力的DeepSeek-R1-0528模型，同时也优于ccg2lambda等符号系统。主要发现表明：(1) 谓词(predicate)的可用性将性能提升了15-20%；(2) T5模型超越了更大的仅解码器大型语言模型；(3) 模型无需特定训练即可泛化到未见过的逻辑参数(FOLIO数据集)。虽然结构逻辑翻译(structural logic translation)表现稳健，但谓词提取(predicate extraction)成为主要瓶颈。",
                    "inspiration_trace": "## 面临的挑战\n自然语言到一阶逻辑的自动化翻译是知识表示的关键瓶颈，现有方法要么依赖缺乏灵活性的符号系统，要么在复杂逻辑推理上表现不佳，导致形式化方法无法充分利用丰富的自然语言信息。\n\n## 关键洞察\n作者发现NL-FOL翻译的主要瓶颈不是逻辑结构生成，而是谓词提取；编码器-解码器架构比纯解码器模型更适合此类结构化转换任务；模型能泛化到未见过的逻辑结构，显示逻辑翻译能力的鲁棒性。\n\n## 解决方案演进\n从标准微调出发，尝试符号表示优化但效果有限；基于谓词瓶颈的洞察，设计谓词条件化策略，包括提供谓词列表、添加噪声干扰及多阶段课程学习；进一步探索多语言训练提升谓词抽象能力，使模型不依赖特定语言句法模式。\n\n## 创新点总结\n创新性地将翻译任务分解为谓词提取与结构生成，识别出关键瓶颈；证明编码器-解码器架构在此任务上的优势；提出实用的谓词条件化方法，即使有噪声也能显著提升性能；通过多语言训练增强模型对谓词语义的理解。"
                },
                {
                    "title": "In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners",
                    "arxiv_id": "2509.22230",
                    "authors": "Jaehoon Kim, Kwangwook Seo, Dongha Lee",
                    "summary": "Transferring reasoning capabilities from larger language models to smaller ones through supervised fine-tuning often fails counterintuitively, with performance degrading despite access to high-quality teacher demonstrations. We identify that this failure stems from distributional misalignment: reasoning traces from larger models contain tokens that are low probability under the student's distribution, exceeding the internal representation capacity of smaller architectures and creating learning barriers rather than helpful guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for generating student-friendly reasoning traces in which the teacher model proposes candidate tokens but the student model determines acceptance based on its own probability distributions, filtering low probability tokens. When applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data degrades average performance across major reasoning benchmarks by 20.5\\%, while the same model trained on RSD-generated reasoning traces achieves meaningful improvements of 4.9\\%. Our analysis reveals that low probability tokens constitute the critical bottleneck in reasoning ability transfer. However, cross-model experiments demonstrate that RSD traces are model-specific rather than universally applicable, indicating that distributional alignment must be tailored for each student architecture's unique internal representation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提升小语言模型的推理能力这一基础能力，提出了反向推测解码(RSD)这一新的训练范式来增强模型的推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning capabilities\"和\"reasoning traces\"，并在多个推理基准上测试性能提升，这直接对应了筛选标准中的\"reasoning\"能力方向。论文不符合任何排除标准，不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文没有涉及强化学习或智能体等新兴范式，但它提出的新训练机制RSD解决了推理能力转移中的关键问题，即分布不匹配导致的低概率token障碍，这直接提升了小模型的通用推理能力。因此，这篇论文的核心贡献是提升LLM本身的推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理能力向小型模型转移时的性能下降问题。针对小型模型在学习大型模型推理轨迹时遇到的分布不匹配问题，我们提出了一种反向推测解码(RSD)方法，通过让教师模型提出候选token而学生模型根据自身概率分布决定是否接受，过滤低概率token。在Qwen3-0.6B模型和多个推理基准上的实验表明，直接蒸馏会导致平均性能下降20.5%，而RSD生成的推理轨迹实现了4.9%的性能提升。",
                    "summary_translation": "通过监督式微调(supervised fine-tuning)将较大语言模型的推理能力转移到较小模型时，常常会出现反直觉的失败，尽管能够获得高质量的教师示范，但性能反而下降。我们发现这种失败源于分布不匹配(distributional misalignment)：较大模型的推理轨迹(reasoning traces)包含在学生模型分布下概率较低的标记(tokens)，这些标记超出了较小架构的内部表示能力，形成了学习障碍而非有效指导。我们提出了反向推测解码(Reverse Speculative Decoding, RSD)，这是一种生成对学生模型友好的推理轨迹的机制，在该机制中，教师模型提出候选标记，但学生模型基于自身的概率分布决定是否接受，从而过滤掉低概率标记。当应用于Qwen3-0.6B模型时，直接蒸馏s1K-1.1推理轨迹数据会导致在主要推理基准(reasoning benchmarks)上的平均性能下降20.5%，而使用RSD生成的推理轨迹训练的同一模型则实现了4.9%的显著提升。我们的分析表明，低概率标记构成了推理能力转移的关键瓶颈。然而，跨模型实验表明，RSD轨迹是模型特定的(model-specific)而非普遍适用的，这表明分布对齐必须针对每个学生架构独特的内部表示进行定制。",
                    "inspiration_trace": "## 面临的挑战\n大型模型推理能力向小型模型迁移时出现反直觉现象：尽管使用高质量教师演示，小型模型性能反而下降，直接蒸馏方法失效。\n\n## 关键洞察\n作者发现问题本质是\"分布不对齐\"：大型模型推理轨迹包含的标记在学生模型分布中概率极低，超出小型架构内部表示能力，形成学习障碍。有效转移需管理学生学习的\"意外性\"，确保推理步骤在其处理范围内。\n\n## 解决方案演进\n基于此，作者提出\"反向推测解码\"(RSD)：颠覆传统师生动态，教师提出候选标记，学生根据自身概率分布决定接受与否。低于阈值的标记被拒绝，生成回退到学生自身预测，确保轨迹分布对齐。\n\n## 创新点总结\n创新点在于：识别低概率标记是推理转移瓶颈；提出以学生为中心的轨迹生成方法；发现分布对齐是模型特定的，需为每个架构定制，为小型模型推理能力转移开辟新途径。"
                },
                {
                    "title": "Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data",
                    "arxiv_id": "2509.22224",
                    "authors": "Zishan Ahmad, Saisubramaniam Gopalakrishnan",
                    "summary": "Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出\"Composite Reasoning (CR)\"这一新型推理方法，使LLMs能够动态探索和组合多种推理风格（如演绎、归纳和溯因推理）。这明显是关于改进LLM的基础推理能力，提出新的推理范式，增强其逻辑和多步推理等通用能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确关注Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是多种推理风格（演绎、归纳、溯因、因果推理） - 新兴范式：提出对思维链(CoT)等现有范式的扩展和改进 第三步排除标准：论文不符合任何排除标准： - 虽然在科学和医学问答基准上进行了评估，但其核心贡献是通用推理方法，而非专注于这些特定领域 - 不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容 论文的核心贡献是提出一种通用的推理框架，使LLM能够根据问题类型动态选择和组合不同的推理风格，从而提升模型的通用推理能力和问题解决能力。正如摘要所述，\"通过培养内部推理风格多样性，LLMs获得更强大、自适应和高效的问题解决能力\"，这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在提升大型语言模型在复杂问题上的推理能力，特别是在数据有限的情况下。针对科学和医学问答任务及资源受限的训练环境（最多1,500个样本），我们提出了一种Composite Reasoning (CR)方法，使LLMs能够动态探索和结合多种推理风格（如演绎、归纳和溯因推理），并在MedMCQA、MedXpertQA和ARC-Complex数据集上通过准确率和令牌使用效率验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs），尽管具有显著能力，但依赖于单一的、主导性的推理范式（reasoning paradigms），这限制了它们在需要多样化认知策略的复杂问题上的表现。为解决这一问题，我们提出了复合推理（Composite Reasoning, CR），这是一种新颖的推理方法，使大型语言模型能够动态探索和结合多种推理风格，如演绎推理（deductive）、归纳推理（inductive）和溯因推理（abductive），以实现更细致的问题解决。在科学和医学问答基准测试（benchmarks）中，我们的方法优于现有的基线方法，如思维链（Chain-of-Thought, CoT），并且超越了DeepSeek-R1风格推理（style reasoning, SR）的准确性，同时表现出卓越的样本效率和适当的令牌（token）使用。值得注意的是，CR自适应地强调适合特定领域的推理风格。对于医学问答，它优先采用溯因推理和演绎推理；而对于科学推理，则转向因果推理（causal）、演绎推理和归纳推理方法。我们的研究结果表明，通过培养内部推理风格的多样性，大型语言模型能够获得更强大、自适应和高效的问题解决能力。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型依赖单一主导推理范式，难以处理需要多样化认知策略的复杂问题，尤其在数据有限情况下表现受限。\n\n## 关键洞察\n真实世界问题需要动态综合多种推理方法；不同领域需要不同推理风格组合，如医学问题需溯因和演绎推理，科学问题需因果、演绎和归纳方法。\n\n## 解决方案演进\n从单一推理局限性出发，假设多样化推理策略组合可提升性能，设计复合推理(CR)方法使模型动态探索结合多种推理风格，通过参数高效微调和GRPO在资源受限环境中验证其有效性。\n\n## 创新点总结\n首次明确鼓励模型内部探索整合多种推理策略；证明培养推理风格多样性可显著提升有限数据下的模型性能；发现模型能根据领域需求自适应调整推理风格，更接近人类专家推理方式。"
                },
                {
                    "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance",
                    "arxiv_id": "2509.22193",
                    "authors": "Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, Céline Hudelot, Pierre Colombo",
                    "summary": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究LLMs的推理能力本身，而非将LLM作为工具应用到特定领域。论文通过控制实验研究推理能力对模型性能的贡献，探讨在什么任务和模型规模下推理变得有效，这直接关注的是LLM的基础能力提升。 其次，论文包含了多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心主题就是reasoning，特别关注math reasoning和通用任务中的推理能力 - 训练方法：比较了Instruction Fine-Tuning (IFT)和推理模型的不同训练范式 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是通过大规模控制实验，系统性地研究了推理能力对LLM性能的影响，发现推理能力能一致地提高模型性能，特别是在推理密集型和开放式任务上。这项研究直接有助于理解如何提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在探究推理能力对大语言模型性能的贡献条件。针对不同规模模型和任务类型，我们提出了一种基于合成数据蒸馏的受控研究方法，通过单一教师生成IFT和推理配对数据，并在12个基准测试上验证了推理在开放式和数学密集型任务上的显著优势，尤其在大规模模型(>7B)中能有效突破IFT性能瓶颈，同时揭示了推理与IFT在训练和推理效率上的权衡关系。",
                    "summary_translation": "具有推理能力的大型语言模型（Large Language Models, LLMs）在广泛任务上取得了最先进的（state-of-the-art）性能。尽管其在经验上取得了成功，但推理变得有效的任务和模型规模，以及其训练和推理成本，仍未得到充分探索。在这项工作中，我们依靠合成数据蒸馏（synthetic data distillation）框架进行大规模监督研究。我们比较了不同大小的指令微调（Instruction Fine-Tuning, IFT）模型和推理模型，在广泛的以数学为中心和通用任务上，评估了多项选择和开放式两种格式。我们的分析表明，推理持续提高模型性能，通常匹配或超过显著更大的IFT系统。值得注意的是，虽然IFT在训练和推理成本上保持帕累托最优（Pareto-optimal），但随着模型规模的扩大，推理模型变得越来越有价值，克服了IFT在推理密集型和开放式任务上的性能限制。",
                    "inspiration_trace": "## 面临的挑战\n推理能力在大语言模型中展现出强大性能，但其在哪些任务和模型规模下有效，以及训练和推理成本如何权衡，仍缺乏系统研究。现有工作未能解开模型规模、任务类型与计算预算等混杂因素，导致实践者缺乏明确指导。\n\n## 关键洞察\n推理效果高度任务依赖：在数学和编码等需多步推理的任务上收益显著，而在简单事实任务上收益有限。同时，推理与模型规模存在复杂动态关系——小模型难以吸收深度推理，而大模型能突破IFT性能瓶颈。推理与IFT间存在效率与性能的权衡。\n\n## 解决方案演进\n为隔离性能驱动因素，设计受控蒸馏框架：使用单一教师模型为相同输入生成配对的IFT和推理答案，保持数据和容量恒定，仅改变监督格式。通过1.6M训练对和12个基准测试，系统映射推理在模型规模、任务类型和答案格式上的影响。\n\n## 创新点总结\n首次提供统一受控的推理与IFT比较视图，消除混杂因素实现性能清晰归因；系统揭示推理作用边界，为实践者提供可操作指导；通过纯监督蒸馏隔离推理信号，避免强化学习的复杂性；开放资源促进可重复研究。"
                },
                {
                    "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement",
                    "arxiv_id": "2509.22144",
                    "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang",
                    "summary": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Multiround Adaptive Chain-of-Thought Compression (MACC)\"的框架，用于优化思维链(CoT)推理过程。思维链(CoT)是提高大语言模型在复杂任务上表现的重要技术，属于LLM的通用推理能力范畴。论文通过多轮细化和自适应策略，实现了CoT的有效压缩，同时提高了准确性和效率。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标，特别是关于思维链(CoT)的方法论研究。论文不涉及任何排除标准中的领域，如特定应用领域或多模态研究，而是专注于增强LLM的基础推理能力。论文明确涉及了\"reasoning\"这一核心能力方向，并通过提出新的方法来改进CoT推理，使其更加高效。因此，这篇论文完全符合研究范围，应被保留。",
                    "summary2": "本文旨在解决思维链推理导致的推理延迟问题。针对大型语言模型在复杂任务中的冗长推理过程，我们提出了一种多轮自适应思维链压缩框架MACC，利用token弹性现象通过多轮细化逐步压缩CoT。在GSM8K、MATH-500和AIME24数据集上通过准确率、token数量、延迟和token效率等指标验证了其有效性，实现了平均5.6%的准确率提升和47个token的长度减少。",
                    "summary_translation": "思维链（Chain-of-Thought, CoT）推理提高了复杂任务的性能，但由于其冗长性引入了显著的推理延迟。我们提出了多轮自适应思维链压缩（Multiround Adaptive Chain-of-Thought Compression, MACC）框架，该框架利用token弹性（token elasticity）现象——即过小的token预算反而会矛盾地增加输出长度——通过多轮迭代逐步压缩思维链。这种自适应策略使MACC能够为每个输入确定最佳压缩深度。我们的方法相比最先进的基线（state-of-the-art baselines）实现了平均5.6%的准确率提升，同时将思维链长度平均减少47个token（token），并显著降低延迟。此外，我们表明测试时性能（test-time performance）——准确率和token长度——可以通过训练集上的可解释特征（如困惑度（perplexity）和压缩率（compression rate））进行可靠预测。在不同模型上的评估表明，我们的方法无需重复微调（fine-tuning）即可实现高效的模型选择和预测，证明了思维链压缩既有效又可预测。我们的代码将在https://github.com/Leon221220/MACC上发布。",
                    "inspiration_trace": "## 面临的挑战\nChain-of-Thought推理虽提升复杂任务性能，但其冗长性导致显著推理延迟。现有压缩方法缺乏细粒度适应性，无法在多样化输入中有效管理压缩与准确性的权衡，统一压缩策略忽略了输入特定的推理复杂性。\n\n## 关键洞察\n作者观察到\"token弹性现象\"——过度压缩反而会增加输出长度，因生成质量下降导致补偿性冗余。这揭示了令牌约束与模型行为间的非线性关系，表明压缩需是渐进、自适应过程，而非一次性或静态操作。\n\n## 解决方案演进\n基于token弹性现象，作者提出多轮自适应压缩：先生成完整推理链，再通过多轮迭代逐步压缩，每轮移除冗余同时保留关键信息。创新点在于动态控制压缩比率，并在压缩导致长度增加时停止，确保压缩只在有意义时进行，避免语义损失。\n\n## 创新点总结\n将token弹性现象转化为实用多轮压缩策略；引入自适应停止机制动态决定最佳压缩深度；提出性能估计假设，使压缩效果可预测，无需昂贵重训练即可选择策略，实现压缩与准确性的智能平衡。"
                },
                {
                    "title": "R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning",
                    "arxiv_id": "2509.22131",
                    "authors": "Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen",
                    "summary": "Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是改进LLM的基础推理能力。论文提出了Reasoning Capsule (R-Capsule)框架，旨在优化思维链(CoT)推理过程，提高LLM的推理效率和准确性。论文关注的是通用推理能力的增强，而不是将LLM应用于特定领域，也不涉及模型基础设施或部署优化。因此，论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\" - 能力方向：聚焦于\"reasoning\"，特别是\"complex reasoning\"和\"step-by-step rationales\"，同时涉及\"high-level plans\"和\"planning\" - 论文旨在提高模型在\"complex benchmarks\"上的表现，涉及problem-solving 第三步：排除标准 论文不聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉相关内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 不讨论模型可靠性层面的水印、安全性等问题 第四步：特殊和模糊情况 论文涉及可解释性的改进，提到\"improving interpretability\"，这是通过计划重建目标实现的，目的是提高模型的内在可解释性，从而提升模型的通用推理质量。这符合保留标准。 核心贡献分析： 论文的核心贡献是提出R-Capsule框架，通过将高级计划压缩成一组学习到的潜在令牌，结合了潜在推理的效率和显式思维链的透明度。这种方法旨在提高LLM的通用推理能力，减少推理过程中的可见令牌占用，同时保持或提高在复杂基准测试上的准确性。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。 因此，这篇论文应该被保留，它直接关注LLM的通用推理能力提升，提出了新的推理框架，符合研究范围。",
                    "summary2": "本文旨在解决大型语言模型使用Chain-of-Thought推理时的高延迟和内存消耗问题。针对复杂推理任务，我们提出了一种R-Capsule框架，将高级计划压缩为紧凑的潜在令牌，同时保持执行步骤显式，并在数学和常识推理基准测试上通过准确率、令牌数量和延迟指标验证了其有效性。",
                    "summary_translation": "思维链（Chain-of-Thought, CoT）提示通过引导大型语言模型（Large Language Models, LLMs）生成明确的逐步推理过程，帮助它们解决复杂推理问题。然而，CoT的冗长性增加了延迟和内存使用，并可能在长链中传播早期错误。我们提出了推理胶囊（Reasoning Capsule, R-Capsule）框架，旨在将潜在推理的效率与显式CoT的透明度相结合。其核心思想是将高级计划压缩为一小组学习到的潜在标记（一个推理胶囊），同时保持执行步骤的轻量级或显式性。这种混合方法受信息瓶颈（Information Bottleneck, IB）原理的启发，我们鼓励胶囊对任务而言既近似最小又足够充分。通过低容量瓶颈来促进最小性，这有助于提高效率。通过双重目标来促进充分性：一个用于答案准确性的主要任务损失，以及一个鼓励胶囊忠实表示原始文本计划的辅助计划重建损失。重建目标有助于将潜在空间具象化，从而提高可解释性并减少无信息捷径的使用。我们的框架在效率、准确性和可解释性之间取得了平衡，从而在保持或提高复杂基准测试准确性的同时，减少了推理的可见标记占用。我们的代码可在以下网址获取：https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
                    "inspiration_trace": "## 面临的挑战\nChain-of-Thought推理虽提升大语言模型复杂任务表现，但其冗长性增加推理延迟和内存使用，且长链推理易传播早期错误。现有解决方案如集成方法增加计算成本，隐式推理缺乏可解释性，分层推理重新引入令牌开销。\n\n## 关键洞察\n作者洞察到推理过程具有层次结构：高层战略计划与底层执行步骤。高层计划的语义本质比具体措辞对指导解决方案更关键，是压缩的理想目标。压缩计划而非执行步骤可保留有用归纳偏差，同时减少可见令牌。\n\n## 解决方案演进\n从推理层次性出发，将过程分解为计划和执行两阶段。借鉴信息瓶颈原理，设计框架将高层计划压缩为少量潜在令牌。通过架构瓶颈和双重目标（任务准确性和计划重建）确保压缩表示既最小又充分。训练中用辅助解码器重建原始文本计划，推理时直接基于胶囊生成答案。\n\n## 创新点总结\n创新点在于首次提出只压缩高层计划而非整个推理链，平衡效率与透明度；基于信息瓶颈原理确保表示紧凑且语义丰富；通过计划重建目标提供可解释性；验证计划压缩与执行步骤生成间的不对称性，为高效推理开辟新方向。"
                },
                {
                    "title": "Think Right, Not More: Test-Time Scaling for Numerical Claim Verification",
                    "arxiv_id": "2509.22101",
                    "authors": "Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand",
                    "summary": "Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了一种测试时计算扩展(TTS)方法，通过生成多个推理路径并训练验证器模型(VERIFIERFC)来选择正确路径，从而增强LLM的推理能力。这属于改进LLM基础能力的研究，特别是针对多步推理和数值推理能力的提升。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)这一核心概念；专注于推理能力(reasoning)，特别是数值推理(numerical reasoning)和多步推理(multistep reasoning)；提出了一种新的训练范式来增强模型的问题解决能力。 第三，论文不符合排除标准。虽然研究场景是事实核查，但这不是一个特定领域应用(如医疗、化学等)，而是一个通用的信息处理任务。论文不涉及多模态、视觉内容，也不关注模型基础设施或部署优化。 最后，在处理模糊情况时，我认为虽然论文在事实核查任务上评估其方法，但其核心贡献是提出了一种通用的测试时计算扩展方法来解决\"推理漂移\"问题，这种方法具有通用性，可以应用于其他需要复杂推理的任务。论文关注的是如何改进LLM本身的推理能力，而非将LLM作为工具应用到特定领域。 因此，这篇论文符合研究范围，它致力于提高大语言模型的通用推理能力，特别是数值推理和多步推理能力。",
                    "summary2": "本文旨在解决大型语言模型在数值声明事实核查中的推理漂移问题。针对复杂数值声明的多步推理场景，我们提出了一种测试时计算扩展(TTS)策略和验证器模型(VERIFIER_FC)，通过生成多个推理路径并选择最佳路径来减轻推理漂移，并在QuanTemp和ClaimDecomp数据集上通过每类F1分数、宏平均和加权F1分数验证了其有效性。",
                    "summary_translation": "对现实世界声明进行事实核查（fact-checking），特别是涉及数字的声明（numerical claims），本质上是一项复杂的工作，需要多步推理（multistep reasoning）和数字推理（numerical reasoning）来验证声明的多个方面。尽管大型语言模型（large language models, LLMs），包括推理模型（reasoning models），已经取得了巨大进步，但在需要组合推理（compositional reasoning）和数字推理相结合的事实核查任务上，它们仍然表现不佳。这些模型无法理解数字方面的细微差别，并且容易受到推理漂移（reasoning drift）问题的影响，即模型无法将多样化信息进行上下文化处理，导致推理过程中的误解和回溯。\n\n在这项工作中，我们系统地探索了扩展测试时计算（test-time compute, TTS）在大型语言模型处理复杂数字声明事实核查任务中的应用，该方法需要从大型语言模型中引出多条推理路径。我们训练了一个验证器模型（VERIFIERFC）来导航这个可能的推理路径空间，并选择能够导致正确判断的路径。我们观察到，测试时计算（TTS）有助于减轻推理漂移问题，从而在数字声明的事实核查方面带来显著的性能提升。为提高测试时计算（TTS）的计算效率，我们引入了一种自适应机制，该机制根据感知到的声明复杂性选择性地执行测试时计算。这种方法比标准测试时计算（TTS）实现了1.8倍的更高效率，同时比单次声明验证（single-shot claim verification）方法实现了显著的18.8%的性能提升。我们的代码和数据可在https://github.com/VenkteshV/VerifierFC获取。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型在验证复杂数值声明时存在\"推理漂移\"问题，模型难以理解数值细微差别，无法正确处理多方面信息，导致误解和推理过程回溯，约34%的数值声明受此影响。\n\n## 关键洞察\n作者发现推理漂移不同于简单\"过度思考\"，它源于模型过度关注声明次要方面或错误解释多样化证据。测试时计算扩展(TTS)可能通过生成多条推理路径缓解此问题，且不同复杂度的声明应分配不同计算资源。\n\n## 解决方案演进\n首先探索TTS生成多条推理路径，然后训练验证模型选择最佳路径。进一步认识到统一TTS效率低下，进而开发基于声明复杂度的自适应机制，利用LLM层潜在表示预测复杂度，对简单声明用单次推理，复杂声明用扩展推理。\n\n## 创新点总结\n首次定义并研究事实检查中的推理漂移问题，创新性地将TTS应用于数值声明验证，提出基于复杂度的自适应TTS策略，实现计算效率提升1.8倍，性能提高18.8%。"
                },
                {
                    "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models",
                    "arxiv_id": "2509.22099",
                    "authors": "Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang",
                    "summary": "With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究生成式奖励模型(GRMs)中的\"solve-to-judge gap\"问题，并提出S2J方法来解决这个问题。从本质上看，论文致力于改进大语言模型的基础能力，特别是增强其问题解决和判断能力之间的联系，这直接属于提升LLM通用推理能力的范畴。论文涉及多个正面指标，包括明确提到large language models (LLMs)、关注problem-solving能力以及采用self-evolution训练方法。同时，论文不符合任何排除标准，没有专注于多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的S2J方法通过同时利用模型在单个输出上的解决和判断能力进行监督，明确连接GRMs的问题解决和评估能力，从而提高模型的判断性能，这可以视为提升模型内在推理能力的新方法。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决生成式奖励模型(GRM)中解决问题能力与判断能力之间的差距问题。针对GRM能够正确解决某些问题却无法正确评估相同问题响应的场景，我们提出了一种Solve-to-Judge (S2J)方法，通过在判断过程中要求模型首先自己解决问题，并结合判断奖励和解决时间解决奖励进行联合优化。在PPE Correctness、PPE Preference、Reward Bench和RMB四个基准测试上，通过判断准确率和solve-to-judge gap验证了其有效性，将差距减少了16.2%，提高了判断性能5.8%。",
                    "summary_translation": "随着大型语言模型（large language models, LLMs）的快速发展，生成式奖励模型（generative reward models, GRMs）已被广泛采用于奖励建模（reward modeling）和评估。先前的研究主要专注于通过在偏好数据集（preference datasets）上优化模型，并以判断正确性（judgment correctness）作为监督来训练专门的GRMs。尽管人们普遍认为具有更强问题解决能力的GRMs通常表现出更优越的判断能力，我们在检查单个查询时首次发现了一个显著的解决-判断差距（solve-to-judge gap）。具体而言，解决-判断差距指的是GRMs尽管完全能够解决某些问题，却难以对这些查询（14%-37%）做出正确判断的现象。在本文中，我们提出了解决到判断（Solve-to-Judge, S2J）方法来应对这一问题。具体来说，S2J同时利用单个GRM输出的解决能力和判断能力作为监督，在模型优化过程中明确链接GRM的问题解决和评估能力，从而缩小这一差距。我们的全面实验表明，S2J有效减少了16.2%的解决-判断差距，进而提高了模型5.8%的判断性能。值得注意的是，S2J在基于相同基础模型构建的GRMs中实现了最先进（state-of-the-art, SOTA）的性能，同时使用了显著更小的训练数据集。此外，S2J通过自我进化（self-evolution）实现了这一目标，而无需依赖更强大的外部模型进行蒸馏（distillation）。",
                    "inspiration_trace": "## 面临的挑战\n作者发现生成式奖励模型(GRMs)存在\"解决-判断差距\"：模型能正确解决的问题中，有14%-37%的情况下却无法做出正确判断。这表明模型未能充分利用其问题解决能力来支持判断能力。\n\n## 关键洞察\n作者认识到，虽然问题解决能力与判断能力总体上正相关，但在单个查询层面存在显著脱节。这种差距源于模型从解决场景转换到评估场景时性能下降，现有方法只关注判断正确性作为监督信号，忽略了显式连接两种能力的必要性。\n\n## 解决方案演进\n基于这一洞察，作者提出S2J方法：在判断过程中先要求模型自己解决问题，然后将解决过程作为判断依据。通过同时利用解决能力和判断能力作为监督信号，明确连接GRM的问题解决和评估能力。对客观任务用规则验证，对主观任务用辅助模型评分。\n\n## 创新点总结\n创新点在于首次识别并量化解决-判断差距，提出在判断过程中显式利用问题解决能力的方法，通过自我进化而非外部模型蒸馏提升性能，联合优化两种能力显著缩小差距并提高判断准确性。"
                },
                {
                    "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity",
                    "arxiv_id": "2509.22054",
                    "authors": "Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian",
                    "summary": "With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出Fuzzy Reasoning Chain (FRC)框架，一种创新性的推理方法，用于增强大语言模型处理模糊性、歧义和不确定性的能力。论文本质上是改进LLM的基础推理能力，特别是处理不确定性文本的推理机制，而非将LLM作为工具应用于特定领域。FRC框架整合了LLM语义先验与连续模糊隶属度，创建了概率推理与模糊隶属度推理之间的显式交互，这属于提升LLM通用推理能力的研究。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及Large language models (LLMs) - 能力方向：直接关注reasoning，特别是处理模糊性和不确定性的推理能力，这是通用推理的重要组成部分 - 论文提到\"stable reasoning\"和\"knowledge transfer\"，表明其关注推理能力的稳定性和泛化性 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在情感分析任务上验证了FRC，但情感分析被视为NLP中的通用任务，而非特定应用领域。论文核心是提出通用推理框架，而非专注于特定领域应用 - 不主要聚焦于模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况 论文涉及可解释性方面，但这是作为FRC框架提升推理质量的内在特性提出的，而非单纯的应用层面讨论。论文通过改进可解释性来增强推理能力，符合保留标准。 综合判断：这篇论文提出了一个旨在增强LLM通用推理能力的新框架(FRC)，特别针对处理模糊性和不确定性这一推理核心挑战，完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决自然语言处理中处理模糊性、多义性或不确定性文本的挑战。针对具有复杂情感和冲突信号的文本，我们提出了一种Fuzzy Reasoning Chain (FRC)框架，整合LLM语义先验与连续模糊隶属度，实现从概率推理到模糊隶属度推理的过渡，并在SemEval-2016 Task 4和Takeout Review Dataset上通过Robustness Score、Monotonicity Score和F1-score验证了其有效性。",
                    "summary_translation": "随着大型语言模型（large language models, LLMs）的快速发展，自然语言处理（natural language processing, NLP）取得了显著进展。然而，在处理具有歧义、多义性（polysemy）或不确定性的文本时，仍然存在重大挑战。我们提出了模糊推理链（Fuzzy Reasoning Chain, FRC）框架，该框架将LLM语义先验（semantic priors）与连续模糊隶属度（fuzzy membership degrees）相结合，在基于概率的推理（probability-based reasoning）和模糊隶属度推理（fuzzy membership reasoning）之间建立了明确的交互。这种转换允许将模糊输入逐步转化为清晰且可解释的决策，同时捕获传统基于概率的方法无法捕捉的冲突或不确定信号。我们在情感分析（sentiment analysis）任务上验证了FRC，理论分析和实证结果均表明，它确保了稳定的推理，并促进了跨不同模型规模的知识转移（knowledge transfer）。这些研究结果表明，FRC为处理微妙和模糊的表达提供了一种通用机制，具有更好的可解释性（interpretability）和鲁棒性（robustness）。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型在处理模糊性、多义性或不确定性的文本时面临重大挑战。传统方法基于固定标签或静态规则，无法捕捉复杂情感表达中的多维语义；现有模糊推理依赖手动定义规则，缺乏适应性；而思维链推理的离散决策过程难以处理包含讽刺和矛盾的复杂文本。\n\n## 关键洞察\n作者认识到需要将概率推理与模糊隶属度推理结合，创建从模糊到清晰的推理框架。他们观察到LLMs生成的语义隶属度具有近似鲁棒性、条件单调性和动态完整性三大特性，这为构建能处理模糊输入并产生清晰决策的框架提供了理论基础。\n\n## 解决方案演进\n作者提出\"概率到隶属度的碰撞\"作为核心创新，设计FRC框架：首先利用LLMs计算情感类别的连续隶属度；然后通过多粒度语义解析进行层次推理；最后在全局决策融合中动态调整权重，确保捕捉所有相关语义元素及其相互作用。\n\n## 创新点总结\n首次将模糊隶属度与LLM推理结合，创建从模糊到清晰的推理范式；通过概率到隶属度转换，捕捉传统方法无法表达的冲突信号；提供处理微妙模糊表达的通用机制，实现不同模型规模间的知识转移。"
                },
                {
                    "title": "GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation",
                    "arxiv_id": "2509.22009",
                    "authors": "Cehao Yang, Xiaojun Wu, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Jia Li, Hui Xiong, Jian Guo",
                    "summary": "Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in LLMs by structurally modeling knowledge through graph-based representations. However, existing GraphRAG approaches face two core limitations: shallow retrieval that fails to surface all critical evidence, and inefficient utilization of pre-constructed structural graph data, which hinders effective reasoning from complex queries. To address these challenges, we propose \\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel retrieval for GraphRAG. \\textsc{GraphSearch} organizes the retrieval process into a modular framework comprising six modules, enabling multi-turn interactions and iterative reasoning. Furthermore, \\textsc{GraphSearch} adopts a dual-channel retrieval strategy that issues semantic queries over chunk-based text data and relational queries over structural graph data, enabling comprehensive utilization of both modalities and their complementary strengths. Experimental results across six multi-hop RAG benchmarks demonstrate that \\textsc{GraphSearch} consistently improves answer accuracy and generation quality over the traditional strategy, confirming \\textsc{GraphSearch} as a promising direction for advancing graph retrieval-augmented generation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出GraphSearch，一种通用的智能体深度搜索工作流，用于增强LLM的检索增强生成能力。从本质上看，论文不是将LLM应用到特定领域，而是提出了一种通用方法来增强LLM的基础推理能力，特别是事实推理和多跳推理能力。论文明确提到了\"agentic\"（智能体）概念，属于llm-based agents的新兴范式，符合\"增强其逻辑、多步推理等通用能力\"的标准。论文采用了双通道检索策略，通过模块化框架实现多轮交互和迭代推理，这些都是提升LLM通用推理能力的方法论研究。论文不涉及多模态与视觉、特定应用领域或模型可靠性等排除标准的内容。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决现有GraphRAG方法中浅层检索和图数据利用效率低下的问题。针对复杂查询场景，我们提出了一种GRAPH SEARCH代理式深度搜索工作流程，采用双通道检索策略结合语义和关系查询，并通过六个模块实现多轮交互和迭代推理。在六个多跳RAG基准测试上通过SubEM、A-Score和E-Score指标验证了其有效性，显著提升了答案准确性和生成质量。",
                    "summary_translation": "图检索增强生成（Graph Retrieval-Augmented Generation, GraphRAG）通过基于图的表示对知识进行结构化建模，从而增强大型语言模型（LLMs）的事实推理能力。然而，现有的GraphRAG方法面临两个核心限制：浅层检索（shallow retrieval）无法揭示所有关键证据，以及对预构建的结构图数据利用效率低下，这阻碍了从复杂查询中进行有效推理。为应对这些挑战，我们提出了\\textsc{GraphSearch}，一种新颖的具有双通道检索（dual-channel retrieval）功能的代理式（agentic）深度搜索工作流程，专为GraphRAG设计。\\textsc{GraphSearch}将检索过程组织成一个包含六个模块的模块化框架，支持多轮交互（multi-turn interactions）和迭代推理（iterative reasoning）。此外，\\textsc{GraphSearch}采用双通道检索策略，对基于块（chunk-based）的文本数据发出语义查询，对结构图数据发出关系查询，从而全面利用两种模态及其互补优势。在六个多跳RAG（multi-hop RAG）基准测试上的实验结果表明，与传统策略相比，\\textsc{GraphSearch}持续提高了答案准确性和生成质量，证实了\\textsc{GraphSearch}是推进图检索增强生成的一个有前景的方向。",
                    "inspiration_trace": "## 面临的挑战\n现有GraphRAG方法存在两个核心局限：浅层检索无法获取复杂查询所需的所有关键证据，导致推理逻辑断裂；以及对预构建结构图数据利用效率低下，难以从复杂查询中有效推理。\n\n## 关键洞察\n作者认识到文本数据与图数据具有互补优势，需同时利用语义查询和关系查询；复杂查询需要多轮交互和迭代推理，而非单轮检索；检索过程应模块化分解，通过专门模块协同实现深度搜索。\n\n## 解决方案演进\n从单轮转向多轮交互，设计代理式工作流支持迭代推理；将检索分解为六个功能模块，分别处理查询分解、上下文细化等；设计双通道检索策略，同时处理语义和关系查询；引入反思路由机制，通过证据评估和查询扩展实现自我完善。\n\n## 创新点总结\n首次将代理概念引入GraphRAG实现多轮交互；创新性设计语义和关系双通道检索；提出模块化可插拔架构；引入反思驱动的证据完善机制，使系统能主动识别并补充证据缺口。"
                },
                {
                    "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation",
                    "arxiv_id": "2509.21978",
                    "authors": "Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao",
                    "summary": "Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，该论文的本质是提出一种新框架(MotivGraph-SoIQ)来增强LLM的创意生成能力，这属于改进LLM基础能力的研究。论文通过整合动机知识图谱和苏格拉底式对话，提出了一种新的使用范式，增强了LLM的逻辑推理和问题解决能力，符合保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心概念；关注reasoning能力，特别是通过苏格拉底式对话促进逻辑推理；提出了基于llm-based agents和multi-agent系统的新兴范式(Q-Driven Socratic Ideator作为双智能体系统)。 第三，论文不符合排除标准：不涉及多模态与视觉内容；虽然应用于\"学术创意生成\"，但这不是一个特定的应用领域，而是通用学术任务；没有主要聚焦于模型可靠性的应用层面。 最后，在特殊和模糊情况处理上，论文提出的双智能体系统是一种通用的智能体协作框架，用于增强LLM的通用问题解决能力，而非应用于特定领域；同时，论文通过苏格拉底式对话减少确认偏见的方法，属于提升模型通用推理质量和可靠性的新方法。 综合分析，该论文的核心贡献是提出了一种增强LLM创意生成和推理能力的新框架，符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在学术创意生成中缺乏理论基础和存在确认偏见的问题。针对学术研究创意生成场景，我们提出了一种集成动机知识图谱(MotivGraph)和苏格拉底式对话(Q-Driven Socratic Ideator)的框架，并在ICLR25论文主题数据集上通过LLM评分、ELO排名和人工评估指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在加速学术创意生成（academic ideation）方面具有巨大潜力，但在思想基础构建（grounding ideas）和减轻确认偏见（confirmation bias）以进行进一步改进方面面临关键挑战。我们提出整合动机知识图谱（motivational knowledge graphs）和苏格拉底式对话（socratic dialogue），以解决增强型LLM创意生成（enhanced LLM ideation, MotivGraph-SoIQ）中的这些局限性。这一新颖框架通过将动机知识图谱（Motivational Knowledge Graph, MotivGraph）与问题驱动的苏格拉底创意生成器（Q-Driven Socratic Ideator）相结合，为LLM创意生成提供了必要的基础构建和实用的创意改进步骤。MotivGraph结构化地存储三种关键节点类型（node types）（问题、挑战和解决方案），为LLM创意生成过程提供动机基础（motivation grounding）。该创意生成器（Ideator）是一个利用苏格拉底式提问（Socratic questioning）的双代理系统（dual-agent system），它促进了一个严格的改进过程，减轻了确认偏见，并在新颖性（novelty）、实验严谨性（experimental rigor）和动机合理性（motivational rationality）维度上提高了创意质量。在ICLR25（国际学习表征会议2025）论文主题数据集上，MotivGraph-SoIQ在基于LLM的评分（LLM-based scoring）、ELO排名（ELO ranking）和人类评估指标（human evaluation metrics）方面均表现出比现有最先进方法（state-of-the-art approaches）更明显的优势。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型在学术创意生成中存在两大核心问题：缺乏稳健的理论或事实基础，导致创意不可靠；以及确认偏见问题，使模型难以自我修正和产生新颖思考。\n\n## 关键洞察\n人类研究者通过文献综述建立学术动机连接，促进跨领域创新；导师与研究者间的讨论能有效减轻确认偏见。创造力理论表明，内在动机是突破性创意的关键组成部分。\n\n## 解决方案演进\n作者首先设计MotivGraph动机知识图谱，包含问题、挑战和解决方案三类节点，通过SciMotivMiner从论文中自动提取三元组。为解决确认偏见，受苏格拉底方法启发，构建双代理系统，让\"导师\"代理质疑\"研究者\"代理，通过批判性对话促进自我修正。最终整合为MotivGraph-SoIQ统一框架。\n\n## 创新点总结\n首次将动机知识图谱与苏格拉底对话结合，同时解决基础缺失和确认偏见问题；通过结构化三元组表示学术动机，使LLM更好理解研究逻辑；双代理系统模拟学术讨论，有效减轻偏见，全面提高创意质量。"
                },
                {
                    "title": "Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts",
                    "arxiv_id": "2509.21892",
                    "authors": "Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang",
                    "summary": "Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2-3$\\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力。论文提出了Elastic Mixture-of-Experts (EMoE)，这是一种新的训练框架，旨在解决MoE模型在推理时扩展激活专家数量导致的性能下降问题。论文的核心贡献是增强专家之间的协作能力，从而提高模型在推理时的性能和可扩展性。这是对LLM本身架构和训练方法的改进，而不是将LLM应用于特定领域，因此符合保留标准。 其次，从正面指标看，论文明确涉及\"Large language models, LLMs\"这一核心概念，研究的是MoE这种大语言模型架构。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及reinforcement learning等特定训练方法，但通过改进模型架构和专家协作机制，间接可能提升模型的通用推理能力。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于模型本身的架构改进。 最后，论文没有涉及需要特殊考虑的模糊情况，如智能体/工具使用或幻觉/可解释性/安全问题。 综上所述，这篇论文通过改进MoE模型的训练框架和专家协作机制，提升了大语言模型的基础能力和推理时的性能可扩展性，符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决MoE模型在推理时扩展激活专家数量导致的性能下降问题。针对固定k值训练的MoE模型在推理时增加专家数量k'的场景，我们提出了一种Elastic Mixture-of-Experts (EMoE)框架，包含随机协同激活采样和分层路由器损失，并在多种MoE设置的九个基准数据集上通过模型性能指标验证了其有效性。",
                    "summary_translation": "Mixture-of-Experts (MoE，专家混合)模型通常在训练和推理过程中固定激活专家的数量$k$。直观来看，在推理过程中激活更多专家$k'$（其中$k'> k$）意味着使用更大的模型参数集进行计算，因此预期会提高性能。然而，与这种直觉相反，我们发现扩展范围如此狭窄，以至于在专家数量仅略微增加后，性能就开始迅速下降。进一步研究表明，这种性能下降源于专家之间缺乏学习的协作。为解决这一问题，我们提出了Elastic Mixture-of-Experts (EMoE，弹性专家混合)，这是一种新颖的训练框架，使MoE模型能够在推理时扩展激活专家的数量，而不会产生额外的训练开销。通过同时训练专家以多样化组合方式进行协作，并鼓励路由器(router)进行高质量选择，EMoE确保了在推理时不同计算预算下的稳健性能。我们在各种MoE设置上进行了广泛的实验。我们的结果表明，EMoE显著扩大了有效的性能扩展范围，将其扩展到训练时$k$的2-3倍，同时也将模型的峰值性能提升到更高水平。",
                    "inspiration_trace": "## 面临的挑战\nMoE模型在推理时增加激活专家数量(k' > k)会导致性能迅速下降，与直觉相悖。这种限制阻碍了模型根据可用计算资源灵活扩展性能。\n\n## 关键洞察\n作者通过专家共现矩阵分析发现，性能下降源于专家间缺乏协作训练。训练时罕见的专家组合在推理时被迫协作，导致\"协作崩溃\"。训练与推理间专家共现模式的差异程度与性能下降高度相关。\n\n## 解决方案演进\n基于此洞察，作者提出双重策略：1)随机共激活采样，从更大候选池中采样专家组合进行训练，使各种组合获得协作经验；2)分层路由器损失，通过反向KL散度确保专家有明确层次排序。两者结合使模型能在不同计算预算下稳定扩展。\n\n## 创新点总结\n首次解决MoE推理时专家扩展的性能瓶颈，不增加训练成本却实现2-3倍扩展范围。通过训练策略创新而非架构改变，使小规模训练的模型能有效利用大规模推理时的专家组合，实现真正的推理时弹性扩展。"
                },
                {
                    "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
                    "arxiv_id": "2509.21880",
                    "authors": "Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang",
                    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的强化学习算法(RL-ZVP)，用于改进大语言模型的推理能力。论文明确指出其目标是\"improving the reasoning abilities of Large Language Models (LLMs)\"，并通过解决现有强化学习方法在处理零方差提示时的局限性来增强LLM的推理能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标分析，论文包含多个相关主题： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调\"reasoning abilities\"，并在六个\"math reasoning benchmarks\"上进行测试 - 训练方法：提出了一种新的强化学习算法，属于强化学习(RL)范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然在数学推理基准上测试，但数学推理被视为通用推理能力的一部分，而非特定应用领域 - 不关注模型可靠性方面的应用层面问题 论文的核心贡献是提出RL-ZVP算法，通过利用零方差提示来增强LLM的强化学习训练过程，从而提高模型的推理能力。这种方法是通用的，可以应用于提升LLM的基础推理能力，而非针对特定领域的应用。因此，这篇论文完全符合研究目标，应该被保留。",
                    "summary2": "本文旨在解决LLM强化学习中零方差提示被忽略而导致的训练效率低下问题。针对所有响应获得相同奖励的零方差提示，我们提出了一种基于熵引导优势塑造的RL-ZVP算法，在六个数学推理基准测试上通过准确率和通过率验证了其有效性。",
                    "summary_translation": "可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 是提升大型语言模型 (Large Language Models, LLMs) 推理能力的强大框架。然而，当前方法如 GRPO 仅依赖于模型对同一输入的响应在正确性上存在差异的问题，而忽略了所有响应获得相同奖励的情况——即所谓的零方差提示 (zero-variance prompts)。在本研究中，我们认为这类提示并非无用，实际上可以为策略优化提供有意义的反馈。为此，我们提出了零方差提示强化学习 (RL with Zero-Variance Prompts, RL-ZVP)，这是一种从零方差提示中提取学习信号的新算法。RL-ZVP 直接奖励正确性并惩罚错误，即使在没有对比响应的情况下也能如此，同时通过 token 级特征 (token-level characteristics) 调节反馈，以保留信息丰富且细致入微的信号。在六个数学推理基准测试中，RL-ZVP 相比 GRPO 在准确率上实现了高达 8.61 分的提升，在通过率上实现了 7.77 分的提升，同时始终优于其他过滤掉零方差提示的基线方法。这些结果突显了在 RLVR 中从零方差提示学习的未开发潜力。",
                    "inspiration_trace": "## 面临的挑战\n在LLM强化学习中，当模型对同一输入的所有响应都获得相同奖励时（\"零方差提示\"），现有GRPO等方法无法提取学习信号，导致这些提示被浪费。这些提示在训练中占比高达30%-99%，且生成响应消耗大量计算资源，忽略它们造成严重效率损失。\n\n## 关键洞察\n作者洞察到零方差提示并非无用，而是可提供有价值的学习信号：即使没有对比响应，模型仍应因正确答案获奖励、因错误答案受惩罚；且奖励/惩罚程度应根据响应中token特性（如熵）调整，高熵token（推理关键点）应获得更大更新权重。\n\n## 解决方案演进\n从洞察到方案的演进：首先识别GRPO在零方差提示上的局限性（优势值归零）；然后将零方差提示分为全正确（正提示）和全错误（负提示）两类；设计基于token熵的优势函数，为正提示提供正奖励，负提示提供负惩罚；最终形成RL-ZVP算法，在非零方差提示使用GRPO，零方差提示使用新优势函数。\n\n## 创新点总结\n创新点在于颠覆\"零方差提示无用\"的常规认知，首次从中提取学习信号；引入token级熵引导机制，实现细粒度优势塑造；将奖励塑造成功应用于LLM强化学习，解决零方差提示下学习信号缺失问题。"
                },
                {
                    "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models",
                    "arxiv_id": "2509.21826",
                    "authors": "Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin, Ran He",
                    "summary": "Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \\textbf{Res}haped \\textbf{T}oken-level policy gradients (\\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\\%$ on single-turn tasks and $1.50\\%$ on multi-turn base tasks.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是改进LLM的工具使用能力，提出了一种新的训练范式(ResT)来优化LLM在工具使用任务中的表现。论文关注的是增强LLM本身的基础能力(工具使用)，而不是将LLM作为工具应用到特定领域。因此，根据第一步判断，应保留该论文。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确关注大型语言模型(LLMs) - 能力方向：涉及问题解决(problem-solving)能力，工具使用本身就是一种通用推理能力的体现 - 训练方法：使用强化学习(RL)框架优化工具使用策略 - 新兴范式：明确关注工具使用(tool use)，并将LLMs视为目标导向的智能体 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域(如医疗、化学等) - 不主要关注模型可靠性方面的内容(如水印、安全等) 第四步：特殊和模糊情况处理—— 论文提出的工具使用方法是通用的(ResT方法)，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用。因此，根据筛选标准，应该保留。 综合分析，这篇论文的核心贡献是提出了一种基于强化学习的新方法(ResT)来改进LLM的工具使用能力，这是一种通用推理能力。论文不是将LLM应用到特定领域，而是改进LLM本身的基础能力，完全符合研究目标。",
                    "summary2": "本文旨在解决工具使用大型语言模型在强化学习训练中面临的高方差和低效问题。针对多轮工具调用任务，我们提出了一种基于熵感知的令牌级策略梯度重塑方法(ResT)，通过动态调整不同区域令牌的权重，实现从结构正确性到语义推理的平滑过渡。在BFCL和API-Bank数据集上的实验表明，ResT实现了最先进的性能，比以前的方法提高高达8.76%，并在4B基础LLM上超越GPT-4o的性能。",
                    "summary_translation": "大语言模型 (Large language models, LLMs) 通过调用外部工具超越了被动生成，充当目标导向的代理 (agents)。强化学习 (Reinforcement learning, RL) 为优化这些新兴的工具使用策略提供了一个原则性框架，然而主流范式完全依赖于稀疏的结果奖励 (sparse outcome rewards)，并且缺乏对工具使用任务特殊性的考虑，导致策略梯度 (policy-gradient) 方差增大，训练效率低下。为了更好地理解和解决这些挑战，我们首先建立了策略熵 (policy entropy) 与工具使用任务训练稳定性之间的理论联系，该联系揭示了结构化的、低熵的标记 (tokens) 是奖励的主要决定因素。受此见解的启发，我们为工具使用任务提出了重塑的标记级策略梯度 (Reshaped Token-level policy gradients, ResT)。ResT 通过基于熵的标记重新加权 (entropy-informed token reweighting) 来重塑策略梯度，随着训练的进行逐步增加推理标记 (reasoning tokens) 的权重。这种感知熵的方案实现了从结构正确性到语义推理的平稳过渡，并稳定了多轮工具使用任务 (multi-turn tool-use tasks) 的收敛。在 BFCL 和 API-Bank 上的评估表明，ResT 实现了最先进 (state-of-the-art) 的结果，比先前的方法高出最多 8.76%。在 4B 基础大语言模型上进行微调 (fine-tuned) 时，ResT 在单轮任务 (single-turn tasks) 上进一步超过 GPT-4o 4.11%，在多轮基础任务 (multi-turn base tasks) 上超过 1.50%。",
                    "inspiration_trace": "## 面临的挑战\n工具使用大型语言模型的强化学习训练中，现有方法依赖稀疏结果奖励，缺乏对任务特殊性的考虑，导致策略梯度方差过大，训练效率低下。多轮工具调用任务系统效率低，且规则奖励在早期训练阶段主要集中在格式标签、工具名称等低熵token上，而推理token贡献小，均匀处理所有token稀释了强化学习信号。\n\n## 关键洞察\n作者通过理论分析建立了策略熵与训练稳定性的联系：结构化的低熵token（如工具名称和参数）是奖励的主要决定因素，较低的平均熵与策略梯度更新的减少方差相关。不同token区域具有不同平均熵，影响其对奖励信号的贡献，这为优化训练提供了理论依据。\n\n## 解决方案演进\n从理论洞察出发，作者首先证明低熵token对减少梯度方差的重要性，然后提出基于熵的token重新加权机制，强调结构化低熵token。进一步设计课程学习方法，随着训练进展逐渐增加推理token权重，实现从结构正确性到语义推理的平稳过渡，最终形成ResT方法。\n\n## 创新点总结\n创新点在于首次建立策略熵与工具使用任务稳定性的理论联系，提出熵感知的token级重新加权机制，设计轻量级课程学习实现从结构到语义的自然过渡，通过理论指导的梯度重塑在保持性能的同时显著提高训练稳定性。"
                },
                {
                    "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
                    "arxiv_id": "2509.21710",
                    "authors": "Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo",
                    "summary": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出Think-on-Graph 3.0 (ToG-3)框架，通过多智能体上下文进化和检索(MACER)机制来增强大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的多智能体协作框架和双重进化机制（进化查询和进化子图），以增强LLM的逻辑和多步推理能力，而非将LLM作为工具应用于特定领域。从第二步正面指标看，论文包含了多个相关主题：核心概念涉及LLMs，能力方向聚焦于reasoning，方法上包含evolution机制，并采用了multi-agent systems这一新兴范式。论文不涉及第三步中的任何排除标准（多模态与视觉、特定应用领域、模型可靠性应用层面）。在第四步特殊情况处理中，论文提出的是通用的多智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用。综合分析，该论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决基于图的检索增强生成(RAG)系统在资源受限和本地部署场景下面临的图质量依赖问题。针对使用轻量级LLMs构建的静态图存在不完整三元组、细节不足和解析失败等问题，我们提出了一种名为Think-on-Graph 3.0 (ToG-3)的新框架，该框架引入了Multi-Agent Context Evolution and Retrieval (MACER)机制，创新性地结合了Evolving Query和Evolving Sub-Graph双重进化机制。在HotpotQA、2WikiMultihopQA和Musique等深度推理基准测试以及UltraDomain广度推理任务上，通过Exact Match、F1分数和综合评估指标验证了其有效性。",
                    "summary_translation": "检索增强生成(Retrieval-Augmented Generation, RAG)和基于图的检索增强生成(Graph-based RAG)已成为用外部知识增强大型语言模型(Large Language Models, LLMs)的重要范式。然而，现有方法面临一个基本的权衡问题。虽然基于图的方法本质上依赖于高质量的图结构，但它们面临重大的实际约束：手动构建的知识图谱(knowledge graphs)扩展成本过高，而从语料库中自动提取的图则受制于底层LLM提取器的性能，尤其是在使用较小的、本地部署的模型时。\n\n本文提出了Think-on-Graph 3.0 (ToG-3)，这是一个引入多智能体上下文演进与检索(Multi-Agent Context Evolution and Retrieval, MACER)机制的新型框架，旨在克服这些局限性。我们的核心创新是动态构建和优化块-三元组-社区(Chunk-Triplets-Community)异构图索引，该索引首创性地结合了演进查询(Evolving Query)和演进子图(Evolving Sub-Graph)的双演进机制，用于精确的证据检索。这种方法解决了先前基于图的RAG方法的一个关键局限性，这些方法通常一次性构建静态图索引，而无法适应实际查询。\n\n一个由构造器(Constructor)、检索器(Retriever)、反射器(Reflector)和响应器(Responser)智能体组成的多智能体系统，协作参与证据检索、答案生成、充分性反思，以及至关重要的演进查询和子图的迭代过程。这种双演进多智能体系统使ToG-3能够在推理过程中自适应地构建有针对性的图索引，减轻了静态一次性图构建的固有缺点，并即使在使用轻量级LLMs时也能实现深度、精确的推理。大量实验表明，ToG-3在深度和广度推理基准测试中均优于比较基线，而消融研究(ablation studies)证实了MACER框架各组件的有效性。",
                    "inspiration_trace": "## 面临的挑战\n现有图RAG方法面临基本权衡：高质量图结构依赖与实际应用限制的矛盾。手动构建知识图谱成本过高难以扩展，而自动提取图又受限于LLM提取器性能，尤其在使用轻量级本地模型时。静态图构建无法适应实际查询需求，限制了推理深度和准确性。\n\n## 关键洞察\n作者认识到传统图RAG的核心局限在于静态图索引构建方式——一次性构建而不适应实际查询。真正需要的不是完美初始图，而是能随推理过程动态演化的图结构。这种\"双重演化\"机制（查询演化和子图演化）可从可能不完美的初始图开始，通过迭代专门针对特定查询推理路径优化。\n\n## 解决方案演进\n基于此洞察，作者设计了Chunk-Triplets-Community异构图架构，统一细粒度和高级信息。然后引入多智能体框架（MACER），包含四个智能体形成迭代循环：Reflector评估上下文充分性并生成精确子查询，Constructor根据子查询扩展精炼子图。这种双重演化使系统能自适应构建针对查询的图索引。\n\n## 创新点总结\n创新点在于：1）首次在图RAG中引入查询和图结构协同演化的双重机制；2）通过多智能体协作实现动态上下文演化，突破静态预构建图限制；3）使轻量级LLM在资源受限环境下实现深度推理，解决传统方法对高质量图或强大LLM的依赖。"
                },
                {
                    "title": "On Code-Induced Reasoning in LLMs",
                    "arxiv_id": "2509.21499",
                    "authors": "Abdul Waheed, Zhen Wu, Carolyn Rosé, Daphne Ippolito",
                    "summary": "Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究代码数据如何增强大语言模型(LLM)的推理能力，特别是数学和逻辑推理能力。论文通过系统性的数据中心框架，构建了10种编程语言的并行指令数据集，并应用受控扰动来研究代码的结构和语义属性对LLM推理能力的影响。研究结果表明，LLM对结构扰动的脆弱性高于语义扰动，特别是在数学和代码任务上。这项研究直接关注如何通过代码数据改进LLM的基础推理能力，符合\"改进LLM的基础能力、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是研究如何提升LLM本身的通用推理能力，因此完全符合研究目标。",
                    "summary2": "本文旨在探究代码数据中哪些方面能够增强大型语言模型(LLM)的推理能力。针对代码的结构和语义特性，我们提出了一种系统性的数据驱动框架，通过构建10种编程语言的并行指令数据集并应用受控扰动，在5个模型家族和8个规模的LLM上进行了3,331次实验，通过自然语言、数学和代码任务上的性能指标验证了代码结构特性对推理能力的关键影响。",
                    "summary_translation": "代码数据（Code data）已被证明可以增强大型语言模型（Large Language Models, LLMs）的推理能力，但尚不清楚代码的哪些方面对此贡献最大。我们通过一个系统的、以数据为中心的框架来研究这个问题。我们构建了十种编程语言的并行指令数据集（parallel instruction datasets），并应用了可控扰动（controlled perturbations）来选择性地破坏代码的结构或语义属性。然后，我们在每个变体上对来自五个模型家族（model families）和八个规模（scales）的LLMs进行微调（finetune），并评估它们在自然语言、数学和代码任务上的表现。在3,331个实验中，我们的结果表明，LLMs对结构扰动（structural perturbations）比语义扰动（semantic perturbations）更敏感，尤其是在数学和代码任务上。适当的抽象（abstractions）如伪代码（pseudocode）和流程图（flowcharts）可以与代码一样有效，而用更少的标记（tokens）编码相同信息且不遵循原始语法（syntax）的方式通常可以保持甚至提高性能。值得注意的是，即使带有误导信号的损坏代码（corrupted code），只要保持表面规律性（surface-level regularities），仍然具有竞争力。最后，语法风格（syntactic styles）也会影响特定任务的收益，Python有利于自然语言推理，而Java和Rust等低级语言则有利于数学推理。通过我们的系统框架，我们旨在深入理解代码的不同特性如何影响推理，并为增强LLM推理能力的训练数据设计提供参考。",
                    "inspiration_trace": "## 面临的挑战\n代码数据能增强LLMs推理能力，但尚不清楚代码的哪些方面最关键——是语法规则性、结构抽象还是语言风格？缺乏对代码影响推理机制的系统性理解。\n\n## 关键洞察\n代码对推理的影响可能是多方面的，需要通过控制变量来解构代码的不同属性。作者意识到，要理解代码如何影响推理，必须设计实验分别测试结构属性和语义属性的独立贡献。\n\n## 解决方案演进\n从洞察到方法的演进：1)构建自然语言与代码的平行数据集确保可比性；2)设计系统性扰动——规则性扰动(如移除空白符)和生成性扰动(如转为伪代码)；3)在5个模型家族8个规模上进行3,331个实验；4)多维度评估不同扰动对推理任务的影响。\n\n## 创新点总结\n创新点在于提出系统性数据中心框架，通过受控扰动解构代码属性影响；探索代码抽象表示(如伪代码、流程图)的推理价值；揭示结构属性比语义属性更关键，以及不同编程语言对任务特定增益的影响。"
                },
                {
                    "title": "Learning to Reason with Mixture of Tokens",
                    "arxiv_id": "2509.21482",
                    "authors": "Adit Jain, Brendan Rappazzo",
                    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于改进LLM的推理能力，具体研究了强化学习与可验证奖励(RLVR)方法，提出了一种名为\"token混合生成\"(MoT-G)的新方法来增强模型的推理过程。论文明确针对LLM在每个推理步骤采样离散token时丢弃分布信息的问题，通过在连续混合空间中操作来生成思维链，这直接提升了模型的基础推理能力。 从正面指标看，论文包含了多个相关主题：核心概念涉及大语言模型(LLMs)，能力方向聚焦于推理(reasoning)，训练方法采用强化学习(RL)的变体，并研究了思维链(Chain-of-Thought)生成。论文在Reasoning-Gym这一推理密集型任务套件上进行了评估，证明了其方法的有效性。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。相反，它专注于提高LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。 综上所述，这篇论文的核心贡献是通过改进token级别的采样策略来增强LLM的推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在提高大型语言模型在推理任务中的表现。针对强化学习与可验证奖励(RLVR)框架下离散令牌采样导致的推理搜索空间受限问题，我们提出了一种混合令牌生成(MoT-G)方法，在每一步推理中维护多个令牌的分布而非离散选择。在Reasoning-Gym推理任务套件上通过准确率和轨迹效率验证了其有效性，MoT-G在7/10任务上实现5-35%性能提升，且仅需一半轨迹数即可达到相当准确率。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)已成为提升大型语言模型(Large Language Model, LLM)推理能力的主要方法。当前大多数方法采用群组相对策略优化(Group Relative Policy Optimization)的变体，该方法采样多个推理完成结果，对它们进行相对评分，并相应地调整策略。然而，这些方法在每个推理步骤中总是采样离散token(离散标记)，丢弃了模型在候选token上的概率分布中丰富的分布信息。尽管在非强化学习(non-RL)环境中，保留和利用这种分布信息已被证明是有益的，但当前的RLVR方法似乎因未使用此信息而不必要地限制了推理搜索空间。\n\n为解决这一限制，我们在RLVR中研究了token混合生成(Mixture-of-token Generation, MoT-G)。我们提出了一个统一框架，该框架概括了现有的MoT-G方法，包括将混合嵌入(mixture embeddings)构建为token嵌入(token embeddings)加权和的现有无训练方法，并将RLVR扩展到直接在此连续混合空间(continuous mixture space)中操作以生成思维链(chain-of-thought)。在Reasoning-Gym（一套推理密集型语言任务）上评估两种MoT-G变体，我们发现与Qwen2.5-1.5B模型的标准解码(standard decoding)相比，MoT-G方法取得了显著改进（在10项任务中的7项上提升了5-35%），同时仅用一半的轨迹(trajectories)数量就达到了相当的准确率，表明训练效率有所提高。通过全面的隐藏状态(hidden-state)和token级别(token-level)分析，我们提供了证据表明MoT-G的优势可能源于其在整个推理过程中保持更高隐藏状态熵(hidden-state entropy)的能力，以及在token空间(token space)中促进探索的能力。",
                    "inspiration_trace": "## 面临的挑战\n当前强化学习与可验证奖励(RLVR)方法在每个推理步骤都采样离散令牌，丢弃了模型概率分布中的丰富分布信息，迫使模型做出早期不可逆决策，限制了探索替代推理路径的能力。\n\n## 关键洞察\n作者认识到保留分布信息在非RL设置中已被证明有益，但RLVR方法却忽视了这一点，不必要地限制了推理搜索空间。混合令牌表示可能帮助模型维持不确定性并增强探索能力。\n\n## 解决方案演进\n作者首先提出统一的混合令牌生成(MoT-G)框架，概括现有方法并将RLVR扩展到连续混合空间。然后设计两种变体：Dirichlet随机加权top-k令牌和采样多令牌归一化加权。通过实验验证其在推理任务中的有效性。\n\n## 创新点总结\n将连续表示引入RLVR框架，打破传统离散采样限制，通过维持更高隐藏状态熵和促进令牌空间探索，更有效平衡探索与利用，提高推理性能和训练效率。"
                },
                {
                    "title": "Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning",
                    "arxiv_id": "2509.21487",
                    "authors": "Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan",
                    "summary": "Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为\"Dual-Head Reasoning Distillation (DHRD)\"的新训练方法，旨在提高语言模型的推理能力，同时解决思维链(CoT)推理效率低下的问题。论文明确关注改进LLM的基础推理能力，提出了一种新的训练范式，属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确涉及\"decoder-only language models (LMs)\" - 能力方向：多次强调\"reasoning\"，特别是\"Chain-of-Thought (CoT)\"和\"reasoning head\"，并在摘要中提到在\"entailment/causal tasks\"上取得更大增益，这些都直接涉及逻辑推理能力 - 训练方法：提出DHRD这一新的训练方法，虽然不是强化学习或进化方法，但确实是一种创新训练范式 第三步：排除标准 论文没有主要聚焦于任何排除标准中的领域： - 不涉及多模态与视觉内容 - 实验在通用SuperGLUE任务上进行，而非特定应用领域 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 综上所述，这篇论文的核心贡献是提出一种新的训练方法来增强LLM的推理能力，同时保持推理效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM基础能力的改进，而非将其作为工具应用于特定领域，因此应该被保留。",
                    "summary2": "本文旨在解决Chain-of-Thought提高分类准确性但引入吞吐量惩罚的问题。针对仅解码器语言模型分类任务，我们提出了Dual-Head Reasoning Distillation (DHRD)方法，在训练时结合池化分类头和推理头，并在七个SuperGLUE任务上通过准确率和F1值验证了其有效性，实现了0.65-5.47%的相对增益，同时保持了96-142倍的推理吞吐量提升。",
                    "summary_translation": "思维链(Chain-of-Thought, CoT)提示通常能提高分类准确率，但通过理由生成(rationale generation)会带来显著的吞吐量(throughput)损失(Wei et al., 2022; Cheng and Van Durme, 2024)。为解决这一权衡问题，我们提出了双头推理蒸馏(Dual-Head Reasoning Distillation, DHRD)，这是一种针对仅解码器语言模型(decoder-only language models, LMs)的简单训练方法，该方法增加了(i)在训练和推理过程中使用的池化分类头(pooled classification head)和(ii)仅在训练中使用、由教师理由(teacher rationales)监督的推理头(reasoning head)。我们使用的损失函数是标签交叉熵(label cross-entropy)和在输入加理由序列(input-plus-rationale sequences)上的词元级语言模型(token-level LM)损失的加权和。在七项SuperGLUE任务上，与池化基线(pooled baselines)相比，DHRD实现了0.65-5.47%的相对提升，在蕴含/因果(entailment/causal)任务上的提升尤为显著。由于我们在测试时禁用了推理头，推理吞吐量与池化分类器相当，并且在相同骨干网络上以QPS(每秒查询数)计算，比CoT解码高出96-142倍。",
                    "inspiration_trace": "## 面临的挑战\n作者面临的核心问题是：将decoder-only语言模型应用于分类任务时，存在准确率与效率的权衡。简单池化分类器效率高但无法充分利用模型的推理能力；而思维链(CoT)虽能提升准确率，却因逐token解码导致吞吐量大幅下降，不适用于高吞吐量场景。\n\n## 关键洞察\n作者洞察到推理过程的价值主要体现在提升模型理解能力，而非必须在推理时显式生成。关键突破点在于：可以将推理的计算成本从推理时间转移到训练时间，通过\"输入-推理-标签\"三元组的对齐来提升模型性能，而非仅依赖通用语言模型正则化。\n\n## 解决方案演进\n作者首先思考如何在保持推理效率的同时获取CoT的准确率优势。接着突破传统知识蒸馏的软标签匹配思路，提出双头架构：分类头用于训练和推理，推理头仅用于训练。最后设计加权损失函数平衡分类和推理目标，使模型学习推理能力但推理时无需生成推理过程。\n\n## 创新点总结\n创新点在于实现了\"训练时推理，推理时分类\"的新范式，通过双头架构巧妙结合分类与语言建模优势，在不增加推理成本情况下显著提升准确率，特别在需要推理的任务上效果更佳。"
                },
                {
                    "title": "How Large Language Models Need Symbolism",
                    "arxiv_id": "2509.21404",
                    "authors": "Xiaotie Deng, Hanyu Li",
                    "summary": "We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是探讨如何通过符号系统(symbolism)来增强大语言模型(LLM)的推理能力，而不是将LLM应用于特定领域。从摘要中可以看出，论文认为AI的未来不仅仅是扩大规模，还需要人类设计的符号来指导LLM强大的但\"盲目\"的直觉，以实现真正的发现。这符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文没有涉及多模态与视觉、特定应用领域或模型可靠性等排除标准中的内容。虽然摘要中没有明确提到\"reasoning\"、\"planning\"等具体能力方向，但\"unlock genuine discovery\"暗示了与推理和问题解决相关的内容。因此，这篇论文应该被保留，因为它关注的是改进LLM的基础能力，提出一种新的范式（结合符号系统）来增强其通用推理能力。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)在数据稀缺场景下的推理局限。针对复杂前沿领域，我们提出了一种将人类智慧符号作为认知技术来增强LLMs统计直觉的方法，并在AlphaGeometry系统上通过国际数学奥林匹克竞赛(IMO)金牌水平表现验证了其有效性。",
                    "summary_translation": "我们认为，人工智能（AI）的未来需要的不仅仅是规模扩大（scaling）。要实现真正的发现（genuine discovery），大型语言模型（large language models）需要一个指南针：人类创造的符号（human-crafted symbols）来引导其强大但盲目的直觉。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型通过\"扩展法则\"取得成功，但这一范式在数据稀缺的复杂前沿领域失效。LLMs依赖统计模式匹配，而非人类式的逻辑推理和抽象，导致在需要创新的多步推理任务中表现不佳。\n\n## 关键洞察\n作者认识到符号作为\"认知技术\"的核心价值。人类通过\"商化\"能力，从广阔问题空间创建紧凑符号空间，简化复杂性。符号不是概念本身，而是增强思维的工具，如数字语言帮助记忆和比较，莱布尼茨微积分符号促进直觉理解。\n\n## 解决方案演进\n作者提出将符号用作压缩人类智慧的容器，引导LLMs的统计直觉。这与早期符号AI不同，不是构建刚性系统，而是创造符号与统计能力的协同。如AlphaGeometry结合符号语言与神经符号引擎，LLM提出关键辅助线，演绎器高效探索结果。\n\n## 创新点总结\n创新在于将符号主义与现代LLMs创造性结合，形成新范式。符号作为认知技术，特别适用于数据稀缺的复杂领域，为LLMs提供\"指南针\"，实现真正发现，而非仅依赖扩展法则。"
                },
                {
                    "title": "A Novel Differential Feature Learning for Effective Hallucination Detection and Classification",
                    "arxiv_id": "2509.21357",
                    "authors": "Wenkai Wang, Vincent Lee, Yizhen Zheng",
                    "summary": "Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval's question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical \"funnel pattern\" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是关于改进大语言模型的基础能力，特别是解决幻觉问题，而不是将LLM作为工具应用到特定领域。论文提出的差分特征学习机制旨在检测和分类幻觉内容，这直接关系到LLM输出的事实准确性，是提升模型基础能力的重要方面。 其次，论文符合正面指标中的\"核心概念\"，明确讨论了大语言模型的幻觉问题。虽然论文没有直接涉及推理、规划等能力方向，但幻觉检测与这些能力密切相关，因为幻觉往往发生在模型进行复杂推理过程中，减少幻觉可以间接提高模型的推理质量和可靠性。 第三，论文不涉及排除标准中的任何领域，它既不是关于多模态与视觉，也不是将LLM应用到特定领域，也不是从应用层面讨论模型可靠性。 最后，根据第四步对特殊情况的处理，论文提出了一种新方法来检测幻觉，属于\"幻觉/可解释性/安全\"类别，并且是从模型内部机制的角度进行研究，目的是提升模型的通用可靠性，因此应该保留。 论文的核心贡献是提出了一种双模型架构和差分特征学习机制，用于检测和分类LLM的幻觉内容，并发现幻觉信号集中在高度稀疏的特征子集中。这些发现有助于开发计算高效的检测系统，减少推理成本同时保持准确性，从而提升LLM的通用推理能力和输出质量，符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型中的幻觉检测问题。针对LLMs输出的事实不准确内容，我们提出了一种双模型架构，包含Projected Fusion块和Differential Feature Learning机制，并在HaluEval基准测试的问答、对话和摘要任务上通过准确率、精确率、召回率和F1分数验证了其有效性。",
                    "summary_translation": "大型语言模型幻觉（large language model hallucination）代表着一个关键挑战，即由于训练数据中的分布偏差（distributional biases），模型输出偏离了事实准确性。尽管近期研究已确定特定隐藏层（hidden layers）在幻觉内容（hallucinatory content）和事实内容（factual content）之间表现出差异，但幻觉信号（hallucination signals）在层内的精确定位仍不明确，这限制了高效检测方法的开发。我们提出了一种双模型架构（dual-model architecture），集成了投影融合（Projected Fusion, PF）块用于自适应层间特征加权（adaptive inter-layer feature weighting），以及差分特征学习（Differential Feature Learning, DFL）机制，该机制通过计算从相同输入学习互补表示（complementary representations）的并行编码器（parallel encoders）之间的差异来识别判别性特征（discriminative features）。通过对HaluEval数据集的问答（question answering）、对话（dialogue）和摘要（summarization）任务进行系统实验，我们证明了幻觉信号集中在高度稀疏的特征子集（highly sparse feature subsets）中，并在问答和对话任务上实现了显著的准确性提升。值得注意的是，我们的分析揭示了一种分层的\"漏斗模式\"（hierarchical \"funnel pattern\"），其中浅层（shallow layers）表现出高特征多样性（high feature diversity），而深层（deep layers）则表现出集中使用（concentrated usage），这使得仅使用1%的特征维度（feature dimensions）就能保持检测性能，仅有最小程度的性能下降。这些研究结果表明，幻觉信号比先前假设的更为集中，为开发计算高效的检测系统提供了一条途径，这些系统可以在保持准确性的同时降低推理成本（inference costs）。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型常产生幻觉，现有研究虽发现特定隐藏层在幻觉与事实内容间有差异，但幻觉信号在层内的精确位置尚不清楚。层级分析方法处理整个隐藏表示，未检查内部特征重要性，导致检测效率低下。\n\n## 关键洞察\n作者推测幻觉信号可能高度集中在特定特征子集中，而非均匀分布。通过双模型架构(一个专注幻觉检测，一个专注事实识别)可创建差异化错误模式，突出最具判别性的特征维度。\n\n## 解决方案演进\n首先设计双模型架构处理相同输入学习互补表示；然后提出Projected Fusion块整合不同层信息；接着开发Differential Feature Learning机制计算模型间差异；最后选择差异最大的特征维度，创建稀疏掩码。实验证实仅需1%特征即可保持检测性能。\n\n## 创新点总结\n首次探索幻觉信号在特征维度级别的精确位置，发现\"漏斗模式\"(浅层多样性高，深层集中使用)，证明极少量特征足以检测幻觉，为高效检测系统提供新路径。"
                },
                {
                    "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback",
                    "arxiv_id": "2509.22633",
                    "authors": "Gen Li, Yuling Yan",
                    "summary": "Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质上是关于改进强化学习与人类反馈(RLHF)的在线探索效率。RLHF是提高大语言模型与人类偏好对齐的核心训练范式，属于LLM基础能力的改进。论文提出的新探索方案旨在更有效地收集偏好数据以改进奖励模型和策略，这直接关系到提升LLM的通用能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文明确包含核心概念\"Large language models (LLMs)\"，并专注于\"Reinforcement learning with human feedback (RLHF)\"这一重要训练方法。虽然摘要未直接提及推理、规划等能力，但RLHF作为对齐方法，能够间接提升这些通用能力。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，而是关注通用的RLHF方法改进。 第四步特殊和模糊情况：论文不涉及智能体/工具使用或幻觉/可解释性/安全等需要特殊判断的内容。 综合分析，这篇论文的核心贡献是提出了一种新的RLHF在线探索方案，通过更有效地收集偏好数据来改进奖励模型和策略，从而提升LLM与人类偏好的对齐效果。这属于改进LLM基础训练范式的研究，直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决强化学习中基于人类反馈的在线探索效率问题。针对在线RLHF场景，我们提出了一种基于不确定性的探索方案，通过动态校准策略引导偏好查询，减少与策略改进最相关的奖励差异不确定性。在多臂老虎机模型上，我们建立了T^(β+1)/(β+2)阶的遗憾界，其中β是平衡奖励最大化和减轻分布偏移的超参数。这是首个在所有模型参数上具有多项式缩放的在线RLHF算法。",
                    "summary_translation": "人类反馈强化学习(Reinforcement learning with human feedback, RLHF)通过从人类偏好数据中学习奖励模型(reward model)，然后优化策略(policy)以产生偏好的响应，已成为将大型语言模型(Large language models, LLMs)与人类偏好对齐的核心范式。在本文中，我们研究了在线RLHF(online RLHF)的探索原则(exploration principles)，其中目标是自适应地收集新的偏好数据，以数据高效的方式(data-efficient manner)同时改进奖励模型和策略。通过检查现有的基于乐观的探索算法(optimism-based exploration algorithms)，我们发现了它们采样协议(sampling protocol)中的一个缺点：它们倾向于收集那些无法减少奖励差异(reward differences)中最信息量大的不确定性(informative uncertainties)的比较，并且我们证明了下界(lower bounds)表明这类方法在指数级长的时间范围(exponentially long horizons)内可能产生线性遗憾(linear regret)。基于这一见解，我们提出了一种新的探索方案(exploration scheme)，该方案将偏好查询(preference queries)引导至减少与策略改进(policy improvement)最相关的奖励差异中的不确定性。在RLHF的多臂老虎机模型(multi-armed bandit model)下，我们建立了$T^{(\\beta+1)/(\\beta+2)}$阶的遗憾界限(regret bounds)，其中$\\beta>0$是一个平衡奖励最大化(reward maximization)与缓解分布偏移(mitigating distribution shift)的超参数(hyperparameter)。据我们所知，这是第一个在所有模型参数(model parameters)上具有多项式缩放(polynomially scaling)遗憾的在线RLHF算法。",
                    "inspiration_trace": "## 面临的挑战\n现有基于乐观主义的在线RLHF探索算法存在根本缺陷：它们收集的比较数据无法有效减少与策略改进最相关的奖励差异不确定性。作者证明这些方法在特定情况下会导致指数级长时间范围内的线性遗憾，探索效率极低。\n\n## 关键洞察\n作者认识到问题本质在于探索方向与不确定性减少目标的不匹配。固定校准策略（如参考策略或静态策略）无法适应动态学习过程，导致探索集中在非信息量丰富的比较上。关键洞察是：校准策略应随学习过程动态演进，引导探索针对最相关的不确定性。\n\n## 解决方案演进\n从\"比较当前策略与固定校准策略\"的初始想法，演进到\"动态更新校准策略\"的突破。最终方案是在每次迭代中使用当前策略作为校准策略，并比较连续两个策略生成的动作，直接减少与策略改进最相关的不确定性。\n\n## 创新点总结\n首次提出动态校准策略的探索范式，使校准策略随学习过程自适应演进。通过针对最相关不确定性进行定向探索，实现了多项式级别的遗憾界限，突破了现有方法指数级依赖的理论瓶颈。"
                },
                {
                    "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning",
                    "arxiv_id": "2509.22576",
                    "authors": "Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris",
                    "summary": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为EPO（熵正则化策略优化）的新框架，用于解决LLM智能体在强化学习中的训练问题，特别是针对多轮环境中稀疏奖励的挑战。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM Agents\"；能力方向上涉及LLM智能体在多轮环境中的决策和推理，与reasoning和problem-solving相关；训练方法上提出了强化学习方法（EPO）；新兴范式上属于llm-based agents的研究。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题；虽然使用了ScienceWorld和ALFWorld作为测试环境，但这些是通用智能体测试平台，而非特定应用领域；也没有关注模型可靠性方面的水印、安全等问题。 最后，在特殊和模糊情况处理上，论文提出的EPO框架是一种通用的智能体强化学习方法，旨在增强LLM智能体在多轮环境中的通用问题解决能力，而非针对特定领域的应用。 论文的核心贡献是提出了一种解决LLM智能体在多轮稀疏奖励环境中训练挑战的新方法，通过熵正则化策略优化来改善探索-利用平衡，从而提升LLM智能体的通用推理和决策能力，这与研究目标高度一致。",
                    "summary2": "本文旨在解决LLM智能体在多轮稀疏奖励环境中的训练挑战。针对多轮交互中的探索-利用级联失败问题，我们提出了一种EPO（熵正则化策略优化）框架，通过轨迹感知熵计算、熵平滑正则化和自适应阶段权重三个机制来稳定训练。在ScienceWorld和ALFWorld基准测试上通过成功率指标验证了其有效性，实现了高达152%的性能提升。",
                    "summary_translation": "在稀疏奖励(sparse rewards)的多轮(multi-turn)环境中训练大型语言模型(LLM)代理，其中完成单个任务需要在一个回合(episode)内进行30次以上的交互，这对强化学习(reinforcement learning)提出了根本性挑战。我们确定了这种设置特有的一个关键失效模式：探索-利用(exploration-exploitation)级联失效(cascade failure)。这种级联失效始于早期策略(early-stage policy)过早收敛(premature convergence)，其中稀疏反馈(sparse feedback)导致代理坚持有缺陷的低熵(low-entropy)策略。随后，代理进入后期策略(late-stage policy)崩溃(collapse)阶段，此时传统的熵正则化(entropy regularization)变得适得其反，促进混乱探索(chaotic exploration)从而破坏训练稳定性。我们提出了熵正则化策略优化(Entropy-regularized Policy Optimization, EPO)，这是一个通过三种协同机制打破这种失效循环的通用框架：(1)在多轮(multi-turn)设置中采用熵正则化(entropy regularization)以增强探索，(2)一种熵平滑(entropy smoothing)正则化器(regularizer)，将策略熵限制在历史平均值内以防止剧烈波动，以及(3)基于阶段的自适应(adaptive phase-based)加权(weighting)，在训练过程中平衡探索和利用。我们的分析证明了EPO在保证收敛性(convergence)的同时，确保熵方差(entropy variance)单调递减(monotonically decreasing)。EPO在ScienceWorld上实现了高达152%的性能提升，在ALFWorld上实现了高达19.8%的性能提升。我们的研究表明，多轮稀疏奖励(multi-turn sparse-reward)设置需要与传统强化学习(traditional RL)根本不同的熵控制(entropy control)，这对大型语言模型(LLM)代理训练具有广泛影响。",
                    "inspiration_trace": "## 面临的挑战\n作者发现在多轮稀疏奖励环境中训练LLM智能体时，存在独特的\"探索-利用级联失败\"问题：早期阶段因稀疏反馈导致智能体过早收敛到有缺陷策略，晚期阶段传统熵正则化反而促进混乱探索，破坏训练稳定性。\n\n## 关键洞察\n作者认识到标准熵正则化在多轮环境中不足的根本原因是缺乏\"时间感知\"能力。关键洞见是将策略熵锚定到动态调整的历史边界可提供必要稳定性，阻止级联失败同时保持必要探索。\n\n## 解决方案演进\n作者首先将熵正则化适应多轮设置，计算轨迹内所有轮次的熵并在批次上平均；然后引入熵平滑正则化器，惩罚与历史熵平均值的偏差；最后开发自适应加权方案，动态平衡训练各阶段的探索和利用。\n\n## 创新点总结\n创新点在于首次识别并形式化多轮环境中的级联失败现象，提出基于历史熵锚定的稳定机制，设计自适应阶段性权重调整，证明多轮稀疏奖励需要与传统RL根本不同的熵控制方法。"
                },
                {
                    "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
                    "arxiv_id": "2509.22621",
                    "authors": "Aayush Mishra, Daniel Khashabi, Anqi Liu",
                    "summary": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and 2 model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式IA2（ICL Activation Alignment），通过将上下文学习(ICL)的激活模式对齐到监督微调(SFT)过程中，来增强模型的内部推理能力。这属于改进LLM的基础能力和提出新的训练范式的研究，旨在增强模型的推理能力，符合保留标准。 其次，从正面指标分析，论文明确包含以下相关主题： 1. 核心概念：研究大语言模型(LLMs)的SFT和ICL技术 2. 能力方向：明确提到\"incentivizes ICL-like internal reasoning\"（鼓励类似ICL的内部推理），直接关注提升模型的推理能力 3. 训练方法：提出了一种自我蒸馏技术(self-distillation technique)，属于新的训练方法范畴 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 不聚焦于任何特定应用领域（如医疗、化学等） 3. 虽然提到了模型的\"校准性\"(calibration)，但这不是其主要焦点，而是作为改进的一个方面 最后，论文没有涉及特殊或模糊的情况，如智能体/工具使用或幻觉/可解释性/安全等主题。 论文的核心贡献是通过激活对齐技术，将ICL的内部推理优势转移到SFT模型中，从而提高模型的推理能力和输出质量。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在 [提高Supervised Fine-Tuning (SFT) 的质量和校准性]。针对 [数据稀缺场景下的模型适应问题]，我们提出了一种 [ICL Activation Alignment (IA²) 方法，通过在SFT模型中复制ICL的激活模式来鼓励类似ICL的内部推理]，并在 [12个流行基准测试和2个模型家族] 上通过 [准确性和预期校准误差(ECE)] 验证了其有效性。",
                    "summary_translation": "监督微调（Supervised Fine-Tuning, SFT）通过训练权重来产生针对查询的预期目标响应，从而专门化模型行为。相比之下，上下文学习（In-Context Learning, ICL）在推理过程中通过提示中的指令或示例来调整模型。在数据稀缺的情况下，与SFT相比，ICL能够提供更好的泛化能力（generalizability）和更校准的响应（calibrated responses），但代价是需要更多的推理计算（inference compute）。在本研究中，我们提出一个问题：ICL的内部计算能否被用来提升SFT的质量？我们首先展示ICL和SFT产生不同的激活模式（activation patterns），表明这两种方法通过不同的功能机制（functional mechanisms）实现适应。基于这一观察并为了利用ICL的丰富功能，我们引入了ICL激活对齐（ICL Activation Alignment, IA2），这是一种自蒸馏（self-distillation）技术，旨在SFT模型中复制ICL的激活模式，并促进类似ICL的内部推理。我们在12个流行基准（benchmarks）和2个模型家族（model families）上的广泛实证结果（empirical results）表明，将IA2作为SFT之前的启动步骤（priming step）执行，能显著提高模型输出的准确性和校准度。这一发现不仅具有实际应用价值，还为理解模型适应的内部机制（inner mechanics）提供了概念窗口。",
                    "inspiration_trace": "## 面临的挑战\nSFT在数据稀缺时需要大量标记样本才能泛化，成本高昂；而ICL虽在少样本下泛化性好且响应校准，但推理成本高且占用上下文空间。现有方法仅从输出层面蒸馏ICL，无法确保模型功能与ICL一致。\n\n## 关键洞察\n作者发现ICL和SFT虽表面行为相似，但内部激活模式截然不同，表明二者通过不同机制实现适应。ICL激活包含丰富的可泛化模式提取信息，而SFT在少样本下易学习捷径。激活模式差异揭示了模型适应的内在机制差异。\n\n## 解决方案演进\n从激活差异出发，作者提出能否利用ICL激活改进SFT。设计目标使SFT模型在处理查询时产生与ICL相似的激活模式。提出IA2自蒸馏技术，先收集ICL激活并对齐，再进行标准SFT，形成\"功能对齐+输出对齐\"的两步流程。\n\n## 创新点总结\n首次从激活空间探索ICL与SFT差异并利用此改进SFT；不仅关注输出对齐，更注重功能对齐，将ICL的泛化性与校准优势融入SFT；为理解模型适应内部机制提供了新视角。"
                },
                {
                    "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
                    "arxiv_id": "2509.22572",
                    "authors": "Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang",
                    "summary": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准的分析： 第一步核心判断：论文本质上是关于改进LLM的基础推理能力，提出了一种名为\"Dynamic Experts Search (DES)\"的测试时缩放策略，通过动态控制混合专家(MoE)模型中激活专家的数量来增强模型的推理能力。这直接对应了\"改进LLM的基础能力\"和\"增强其逻辑、数学、多步推理等通用能力\"的研究目标。 第二步正面指标：论文明确包含两个关键正面指标：(1)核心概念LLMs，研究的是混合专家大语言模型；(2)推理能力方向，标题直接提及\"Enhancing Reasoning\"，并在实验中测试了数学、代码和知识推理基准。虽然论文未涉及强化学习训练方法和智能体等新兴范式，但已足够表明其与研究课题高度相关。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（数学、代码和知识推理是通用能力测试而非特定领域应用），也不关注模型可靠性方面的水印、安全等问题。 论文的核心贡献是提出了一种利用模型架构特性（专家激活）来增强LLM推理能力的新方法，这种方法不需要额外计算成本就能提高模型在各种推理任务上的准确性和稳定性，完全符合提高LLM本身通用推理能力的研究目标。",
                    "summary2": "本文旨在 [解决Test-Time Scaling方法忽略模型架构作用的问题]。针对 [Mixture-of-Experts LLMs]，我们提出了一种 [Dynamic Experts Search方法，将专家激活作为搜索空间的可控维度]，并在 [多种推理基准] 上通过 [准确性和稳定性指标] 验证了其有效性。",
                    "summary_translation": "测试时缩放（Test-Time Scaling, TTS）通过在推理过程中分配额外计算资源来增强大型语言模型（Large Language Models, LLMs）的推理能力。然而，现有方法主要依赖于输出层面的采样，而忽视了模型架构的作用。在主流的专家混合（Mixture-of-Experts, MoE）大型语言模型中，我们观察到改变激活专家的数量会产生具有稳定准确性的互补解集，这揭示了一个新的且未被充分探索的多样性来源。基于这一观察，我们提出了动态专家搜索（Dynamic Experts Search, DES），这是一种将专家激活提升为搜索空间可控维度的TTS策略。DES集成了两个关键组件：(1) 动态MoE（Dynamic MoE），它能够在推理过程中直接控制专家数量，以生成多样化的推理轨迹而无需额外成本；(2) 专家配置继承（Expert Configuration Inheritance），它在单次推理路径内保持一致的专家数量，而在不同运行之间变化，从而在整个搜索过程中平衡稳定性和多样性。在多种MoE架构、验证器和推理基准（即数学、代码和知识）上的广泛实验表明，DES可靠地超越了TTS基线方法，在无需额外成本的情况下提高了准确性和稳定性。这些结果突显了DES作为一种实用且可扩展的架构感知TTS形式，展示了现代大型语言模型中的结构灵活性如何能够推进推理能力。",
                    "inspiration_trace": "## 面临的挑战\n现有Test-Time Scaling方法主要依赖输出层面采样增强LLMs推理能力，却忽略了模型架构的作用。这些方法将模型内部计算视为架构无关的，尤其在MoE架构中未能利用其灵活激活专家的特性。\n\n## 关键洞察\n作者发现MoE模型中改变激活专家数量会产生互补解决方案集，同时保持稳定准确性。不同专家数量解决的问题集有低重叠性，这揭示了专家激活是一个未被充分利用的多样性来源，可转化为搜索空间的可控维度。\n\n## 解决方案演进\n从这一洞察出发，作者首先设计Dynamic MoE使专家数量成为推理时可调参数。随后提出Expert Configuration Inheritance，在单一路径内保持专家数量一致，不同路径间变化配置。两者结合将专家激活提升为搜索空间的新维度，平衡了稳定性和多样性。\n\n## 创新点总结\n开创了\"架构感知的TTS\"新范式，首次将MoE专家激活作为可控维度。通过结构灵活性而非参数扩展提升性能，在不增加计算成本的情况下增强推理能力，为现代LLMs的结构灵活性利用提供了新思路。"
                },
                {
                    "title": "Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers",
                    "arxiv_id": "2509.22445",
                    "authors": "Peter Shaw, James Cohan, Jacob Eisenstein, Kristina Toutanova",
                    "summary": "The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. We establish that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. We prove that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. We further show that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Our empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, we outline a potential path towards training neural networks that achieve greater compression and generalization.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，而非将其作为工具应用于特定领域。论文提出了基于Kolmogorov复杂度的渐近最优描述长度目标，这是一种新的理论框架，旨在优化Transformer模型的复杂度和泛化能力。这属于\"改进LLM的基础能力\"的范畴，与思维链、强化学习等方法论研究类似，都是致力于提升模型内在能力的理论探索。 其次，从正面指标分析，论文明确涉及Transformers这一大语言模型的核心架构，讨论了泛化能力（这是通用推理能力的基础），并提出了新的变分目标函数作为训练/优化方法。虽然论文没有直接讨论数学推理或逻辑推理，但优化模型复杂度和泛化能力是提升通用推理能力的重要基础。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是建立了一个理论框架，用于训练具有更强压缩和泛化能力的神经网络，这直接关系到提升LLM的通用推理能力。通过优化模型的复杂度和泛化能力，可以间接提升模型在各种推理任务上的表现。因此，这篇论文符合研究目标，应该被保留。",
                    "summary2": "",
                    "summary_translation": "最小描述长度（Minimum Description Length, MDL）原则为在机器学习中应用奥卡姆剃刀原理（Occam's razor）提供了一个形式化框架。然而，由于缺乏一个原则性的、通用的模型复杂度度量标准，将其应用于Transformer（变换器）等神经网络具有挑战性。本文介绍了渐近最优描述长度目标（asymptotically optimal description length objectives）的理论概念，该概念基于Kolmogorov复杂度（Kolmogorov complexity）理论。我们证明，随着模型资源界限的增加，对于任何数据集，这种目标的最小化器都能达到最优压缩，最多相差一个加性常数。我们基于对Transformer计算普适性（computational universality）的新证明，证明了渐近最优目标对于Transformer的存在性。我们进一步表明，通过构建和分析一个基于自适应高斯混合先验（adaptive Gaussian mixture prior）的变分目标（variational objective），这样的目标可以是可处理的且可微的。我们的实证分析表明，这个变分目标在一个算法任务上选择了一个具有强泛化能力（generalization）的低复杂度解，但标准优化器无法从随机初始化中找到这样的解，这凸显了关键的优化挑战。更广泛地说，通过提供一个用于识别具有强渐近保证的描述长度目标的理论框架，我们概述了一条训练能够实现更好压缩和泛化能力的神经网络的潜在路径。",
                    "inspiration_trace": "## 面临的挑战\nMDL原则在神经网络应用中缺乏原则性、通用的模型复杂度度量；现有压缩方法无法捕获所有潜在规律性，导致次优压缩和泛化；算法信息理论与实际神经网络间存在概念鸿沟，难以量化神经网络计算函数的复杂性。\n\n## 关键洞察\nKolmogorov复杂性提供相对于任何可计算描述长度的最优压缩，其普适性根植于Church-Turing论题；Transformers具有计算普适性，能表示任何可计算的有理值条件概率分布；通过构建渐近最优编码族，可在资源限制增加时逼近最优压缩。\n\n## 解决方案演进\n从定义通用两部件编码理论框架开始，证明其存在性并建立与Kolmogorov复杂性的联系；通过证明Transformers能模拟通用前缀图灵机，确立其计算普适性；为解决实际应用局限，构建基于自适应高斯混合先验的可微分变分目标；最后通过算法任务实验验证理论框架。\n\n## 创新点总结\n建立了Kolmogorov复杂性与深度学习间的形式化桥梁；证明了存在渐近最优编码族，提供普适性保证；提出实用变分框架使理论可应用于Transformers；识别并实证了优化这些目标的挑战，为未来研究指明方向。"
                },
                {
                    "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning",
                    "arxiv_id": "2509.22044",
                    "authors": "Ziqi Wang, Boye Niu, Zhongli Li, Linghui Meng, Jing Liu, Zhi Zheng, Tong Xu, Hua Wu, Haifeng Wang, Enhong Chen",
                    "summary": "Recent Large Reasoning Models have achieved significant improvements in complex task-solving capabilities by allocating more computation at the inference stage with a \"thinking longer\" paradigm. Even as the foundational reasoning capabilities of models advance rapidly, the persistent gap between a model's performance in a single attempt and its latent potential, often revealed only across multiple solution paths, starkly highlights the disparity between its realized and inherent capabilities. To address this, we present A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge the gap between a model's potential and its actual performance. In this framework, an \"explorer\" model first generates potential solutions in parallel through repeated sampling. Subsequently,a \"synthesizer\" model integrates these references for a more refined, second stage of reasoning. This two-stage process allows computation to be scaled orthogonally to existing sequential methods. Our work makes two key innovations: First, we present A2R as a plug-and-play parallel reasoning framework that explicitly enhances a model's capabilities on complex questions. For example, using our framework, the Qwen3-8B-distill model achieves a 75% performance improvement compared to its self-consistency baseline. Second, through a systematic analysis of the explorer and synthesizer roles, we identify an effective asymmetric scaling paradigm. This insight leads to A2R-Efficient, a \"small-to-big\" variant that combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration surpasses the average performance of a monolithic Qwen3-32B model at a nearly 30% lower cost. Collectively, these results show that A2R is not only a performance-boosting framework but also an efficient and practical solution for real-world applications.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为A2R的非对称两阶段推理框架，旨在提升大语言模型的基础推理能力。论文明确针对的是模型在单次尝试中的表现与其潜在能力之间的差距，通过\"探索者\"和\"综合者\"两个模型的协作来增强LLM的通用推理能力，而不是将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标： - 核心概念：明确研究Large Reasoning Models，使用Qwen3模型作为实验对象 - 能力方向：直接聚焦于推理(reasoning)能力的提升，标题和摘要多次强调\"reasoning\" - 新兴范式：提出了\"explorer\"和\"synthesizer\"的协作框架，可视为一种多智能体系统的简化形式 第三，论文不涉及任何排除标准中的领域： - 没有多模态或视觉相关内容 - 没有针对医疗、化学、生物等特定应用领域 - 没有讨论水印、安全性等模型可靠性问题 在特殊和模糊情况处理上，论文提出的两阶段框架是一种通用的推理增强方法，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种可插拔的并行推理框架，通过非对称扩展模型计算能力来提升LLM在复杂问题上的表现，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。实验结果表明，该框架能显著提升模型性能（如Qwen3-8B-distill模型实现75%性能提升），同时提供了一种高效的计算扩展方式。",
                    "summary2": "本文旨在解决大型推理模型在单次尝试性能与潜在能力之间的差距问题。针对复杂推理任务，我们提出了一种A2R非对称两阶段推理框架，通过Explorer并行生成多个解决方案，再由Synthesizer进行整合和二次推理，并在AIME 2024、AIME 2025和BeyondAIME等数学推理基准上通过pass@1准确率验证了其有效性。",
                    "summary_translation": "近期的大型推理模型（Large Reasoning Models）通过在推理阶段采用\"思考更久\"（thinking longer）范式分配更多计算资源，在复杂任务解决能力方面取得了显著进步。尽管模型的基础推理能力正在迅速提升，但模型单次尝试的表现与其潜在能力之间持续存在的差距——这种潜力通常只有在多个解决路径中才能显现——鲜明地凸显了其已实现能力与固有能力之间的差异。为解决这一问题，我们提出了A2R，一种非对称两阶段推理（Asymmetric Two-Stage Reasoning）框架，旨在明确弥合模型潜力与实际表现之间的差距。\n\n在该框架中，\"探索者\"（explorer）模型首先通过重复采样并行生成潜在解决方案。随后，\"综合者\"（synthesizer）模型整合这些参考方案，进行更精细的第二阶段推理。这种两阶段过程使得计算能够以正交于现有顺序方法的方式进行扩展。\n\n我们的工作有两项关键创新：首先，我们将A2R呈现为一个即插即用（plug-and-play）的并行推理框架，明确提升模型在复杂问题上的能力。例如，使用我们的框架，Qwen3-8B-distill模型相比其自洽性（self-consistency）基线实现了75%的性能提升。其次，通过对探索者和综合者角色的系统分析，我们确定了一种有效的非对称扩展（asymmetric scaling）范式。这一见解催生了A2R-Efficient，一种\"小到大\"（small-to-big）的变体，结合了Qwen3-4B探索者和Qwen3-8B综合者。这种配置以近30%的更低成本超越了单体Qwen3-32B模型的平均性能。\n\n总体而言，这些结果表明A2R不仅是一个性能提升框架，还是一种高效且实用的现实应用解决方案。",
                    "inspiration_trace": "## 面临的挑战\n大型推理模型在单次尝试中的表现与其在多路径中展现的潜在能力之间存在显著差距。现有并行推理方法存在计算冗余、信息无法共享，以及聚合阶段仅做简单选择而不进行额外推理等局限。此外，单路径推理中的\"前缀陷阱\"问题会导致整个推理轨迹不可逆转地偏离。\n\n## 关键洞察\n作者认识到推理计算可正交于现有方法进行扩展，是一种与参数扩展和顺序推理互补的改进维度。通过系统分析发现，合成器的内在推理能力是最终性能上限的关键决定因素，而非简单路由器。探索阶段占主导计算成本，而合成阶段是计算较轻的任务。\n\n## 解决方案演进\n作者首先将推理解耦为探索和合成两个互补阶段，引入合成器对完整推理链集进行生成式再推理。通过实验验证合成器能力与性能提升的强相关性后，提出A2R-Efficient不对称架构：用小模型探索多样化路径，大模型执行最终再推理，实现资源优化分配。\n\n## 创新点总结\n通过显式生成式再推理桥接模型潜力与实际表现的差距；揭示合成器能力是性能瓶颈，提出\"小探索器，大合成器\"高效配置；在降低30%计算成本的同时，实现与更大单体模型相当的性能。"
                },
                {
                    "title": "The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging",
                    "arxiv_id": "2509.22034",
                    "authors": "Xiaochong Lan, Yu Zheng, Shiteng Cao, Yong Li",
                    "summary": "The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过模型合并(model merging)技术来调整和增强大语言模型本身的推理能力，而不是将LLM作为工具应用到特定领域。论文提出的方法允许在推理深度和计算成本之间进行权衡，这属于改进LLM基础能力的范畴。 其次，论文符合多个正面指标：它明确研究大语言模型(LLMs)，并特别关注\"tunable reasoning capabilities\"，在多个推理基准上进行评估，这直接对应了核心概念和能力方向这两个正面指标。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，它直接关注如何通过模型合并来调整和增强LLM的通用推理能力，而不是将智能体/工具应用在特定领域，也不是对幻觉/可解释性/安全的社会学研究或应用层面讨论。 论文的核心贡献是提供了一种通过模型合并技术来调整LLM推理能力的方法，使研究者能够在推理准确性和计算效率之间进行权衡，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决如何高效创建具有可调节推理能力的大语言模型谱系的问题。针对思考模型和直接模型这两种极端情况，我们提出了一种通过模型合并技术来创建具有不同推理深度和计算效率平衡的模型谱系，并在多个推理基准测试上通过推理准确率和token效率验证了其有效性。",
                    "summary_translation": "在许多实际应用中，对具有可调节推理能力的大语言模型（large language models, LLMs）日益增长的需求，凸显了对能够高效生成一系列平衡推理深度和计算成本的模型的方法的迫切需求。模型合并（model merging）已成为一种有前景的、无需训练（training-free）的技术，通过算术组合通用模型（general-purpose model）和专用推理模型（specialized reasoning model）的权重来应对这一挑战。尽管存在各种合并技术，但它们在创建具有细粒度（fine-grained）推理能力控制的模型谱系方面的潜力仍未得到充分探索。本研究提出了一项大规模实证研究（empirical study），评估了多种模型合并技术在多个推理基准（reasoning benchmarks）上的表现。我们系统地改变合并强度（merging strengths）以构建准确率-效率曲线（accuracy-efficiency curves），首次全面展示了可调节性能的概况。我们的研究结果表明，模型合并提供了一种有效且可控的方法来校准推理准确性（reasoning accuracy）和令牌效率（token efficiency）之间的权衡，即使在父模型（parent models）具有高度发散的权重空间（weight spaces）的情况下也是如此。至关重要的是，我们发现了帕累托改进（Pareto Improvement）的实例，即合并模型比其父模型之一实现了更高的准确性和更低的令牌消耗。我们的研究首次全面分析了这一可调节空间，为创建具有特定推理特征（reasoning profiles）的大语言模型以满足不同应用需求提供了实用指南。",
                    "inspiration_trace": "## 面临的挑战\n当前大语言模型处于\"思维谱系\"两极：深度思考模型准确性高但计算成本大，直接响应模型速度快但推理能力有限。实际应用需要在这两极间取得平衡，而现有调节推理能力的方法需大量训练资源，不适用于资源受限场景。\n\n## 关键洞察\n作者将深度思考和直接响应视为两种不同的元能力，洞察到即使这两类模型在参数空间存在显著差异，它们仍可能位于一个连接的低损失盆地中，使通过模型合并创建中间权衡模型成为可能。\n\n## 解决方案演进\n作者先验证了思维与直接模型间参数差异远大于领域专业模型，质疑简单合并有效性。但通过大规模实验，发现多种合并技术都能有效创建推理能力可调的模型谱系，甚至出现帕累托改进现象。基于此，提出模型合并类似于采样连续训练轨迹中间检查点的假设。\n\n## 创新点总结\n首次系统探索模型合并用于调节推理能力；发现即使面对巨大参数差异合并仍有效；观察到推理行为的非线性相变；证明合并模型可超越父模型；为创建特定推理能力的模型提供实用指导。"
                },
                {
                    "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning",
                    "arxiv_id": "2509.22315",
                    "authors": "Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun Ouyang, Shuo Han, Razieh Rahimi, Hong Yu",
                    "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking, Fast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \\textbf{System 1} (fast, intuitive thinking) and \\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \\textit{planning}, \\textit{hypothesis generation}, \\textit{retrieval}, \\textit{information integration}, and \\textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出PRIME（Planning and Retrieval-Integrated Memory for Enhanced Reasoning），一个多智能体推理框架，通过动态整合快速思考（System 1）和慢速思考（System 2）来增强大语言模型的推理能力。该框架包含专门的智能体进行规划、假设生成、检索、信息整合和决策，明显属于改进LLM基础推理能力的研究。论文专注于提高LLM在多跳和基于知识的推理任务上的表现，这正是\"通用推理能力\"的核心要素。同时，论文提出的多智能体协作框架是一种新兴范式，符合筛选标准中的正面指标。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在 [提高大型语言模型在复杂推理任务中的性能和效率]。针对 [多跳和知识密集型推理任务]，我们提出了一种 [受人类双过程认知理论启发的多智能体推理框架PRIME，结合System 1快速思考和System 2深思熟虑]，并在 [医学和多跳推理基准测试] 上通过 [准确率和F1分数] 验证了其有效性。",
                    "summary_translation": "受《思考，快与慢》（\\textit{Thinking, Fast and Slow}）中人类认知的双过程理论启发，我们提出了\\textbf{PRIME}（Planning and Retrieval-Integrated Memory for Enhanced Reasoning，用于增强推理的规划与检索集成记忆框架），这是一个动态整合\\textbf{System 1}（系统1，快速、直觉性思维）和\\textbf{System 2}（系统2，缓慢、深思熟虑思维）的多智能体推理框架。PRIME首先采用快速思维代理（Quick Thinking Agent，System 1）生成快速答案；如果检测到不确定性，则会触发一个结构化的System 2推理流程，该流程由专门负责\\textit{planning}（规划）、\\textit{hypothesis generation}（假设生成）、\\textit{retrieval}（检索）、\\textit{information integration}（信息整合）和\\textit{decision-making}（决策）的专门智能体组成。这种多智能体设计忠实地模拟了人类认知过程，同时提高了效率和准确性。使用LLaMA 3模型的实验结果表明，在需要多跳推理（multi-hop reasoning）和基于知识的推理（knowledge-grounded reasoning）的基准测试中，PRIME使开源大语言模型（open-source LLMs）能够与最先进的闭源模型（closed-source models）如GPT-4和GPT-4o竞争。这项研究确立了PRIME作为在需要复杂、知识密集型推理（knowledge-intensive reasoning）的领域中改进大语言模型的可扩展解决方案。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM推理方法存在两难困境：快速直观推理计算高效但易产生幻觉；深度分析推理准确但资源密集。如何在保持高准确性的同时有效利用计算资源，避免不必要的深度推理，是当前AI推理系统的核心挑战。\n\n## 关键洞察\n作者从人类认知双过程理论获得独特视角：人脑能在系统1（快速直觉）和系统2（缓慢分析）间灵活切换，关键在于自我反思机制——只有当直觉不足时才启动深度思考。这种选择性触发机制是平衡效率与准确性的关键。\n\n## 解决方案演进\n从双过程理论出发，先设计系统1快速思考代理进行结构化子问题分解；引入反思代理评估答案可靠性；当检测到不确定性时，触发模块化系统2，包含规划、检索、假设生成和决策等专业代理，形成完整的\"快速思考-反思-深度推理\"流程。\n\n## 创新点总结\n首次在多智能体框架中明确操作化双过程理论，通过反思机制选择性触发深度推理，既减少计算开销又提高准确性。模块化专业智能体协作模拟人脑认知过程，实现了效率与性能的平衡。"
                },
                {
                    "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
                    "arxiv_id": "2509.21766",
                    "authors": "Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen",
                    "summary": "Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了UltraHorizon基准测试，用于评估LLM智能体在超长期场景中的能力，包括持续推理、规划、记忆管理和工具使用等通用能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文并非将LLM作为工具应用到特定领域，而是关注LLM本身在通用推理能力方面的表现和不足。论文提出的基准测试旨在测量LLM智能体在复杂现实世界挑战中的基础能力，特别是长期推理、规划和工具使用等通用能力，这符合第一步中\"改进LLM的基础能力\"的保留标准。 其次，论文包含多个正面指标：明确关注LLM-agents（核心概念），涉及reasoning、planning、problem-solving（能力方向），以及tool use（新兴范式）。这些都是与研究目标\"提高大语言模型的通用推理能力\"直接相关的主题。 第三，论文不主要聚焦于排除标准中提到的领域。虽然论文提到了\"大规模软件开发、商业投资和科学发现\"等现实世界任务，但这些只是作为例子来说明其基准测试的应用场景，而不是将LLM应用到特定领域进行研究。 最后，在处理特殊和模糊情况方面，论文提出的是一个通用的基准测试，用于评估LLM智能体在长期任务中的能力，而不是将智能体/工具应用在特定领域。论文分析了LLM在这些长期任务中的局限性，并识别出导致失败的原因，这有助于理解如何提高LLM的通用推理能力。 综上所述，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在解决当前agent评估主要关注短时程任务而忽视长时程复杂场景的问题。针对长时程、部分可观察环境中的agent能力评估，我们提出了UltraHorizon benchmark，通过三个不同环境（Mystery Grid、Sequence Exploration和Alien Genetics Laboratory）验证agent的持续推理、规划、内存管理和工具使用能力。实验表明，在最重设置下轨迹平均超过200k token和400+工具调用，LLM-agent表现明显落后于人类参与者，突显了长时程能力差距。通过轨迹分析，我们确定了八种错误类型并归因于上下文锁定和基础能力差距两个主要原因。",
                    "summary_translation": "自主代理（Autonomous agents）最近在多个领域取得了显著进展，然而大多数评估集中在短期、完全可观察（fully observable）的任务上。相比之下，许多关键的现实世界任务，如大规模软件开发、商业投资和科学发现，都在长期（long-horizon）和部分可观察（partially observable）的场景中展开，这些场景的成功取决于持续的推理（reasoning）、规划（planning）、记忆管理（memory management）和工具使用（tool use）。现有基准很少捕捉这些长期挑战，导致系统评估存在差距。为弥合这一差距，我们引入了**UltraHorizon**，这是一个衡量复杂现实世界挑战所必需的基础能力的新型基准。我们使用探索（exploration）作为三个不同环境中的统一任务，以验证这些核心能力。代理被设计在长期发现任务中，它们必须通过持续推理、规划、记忆和工具管理以及与环境交互来迭代发现隐藏规则。在最大规模设置下，轨迹平均超过**20万token**和**400次工具调用**，而在标准配置中，它们仍然超过**3.5万token**并平均涉及**60多次工具调用**。我们的广泛实验表明，LLM代理（LLM-agents）在这些设置下表现不佳，而人类参与者获得更高的分数，突显了代理在长期能力上存在的持续差距。我们还观察到，简单的扩展（scaling）在我们的任务中失败。为了更好地说明代理的失败，我们对收集的轨迹进行了深入分析。我们识别出八种错误类型，并将其归因于两个主要原因：上下文锁定（in-context locking）和功能基础能力差距（functional fundamental capability gaps）。我们的代码将在[此处](https://github.com/StarDewXXX/UltraHorizon)提供。",
                    "inspiration_trace": "## 面临的挑战\n现有自主代理评估主要聚焦短时程、完全可观测任务，而关键现实世界任务（如软件开发、投资和科学发现）发生在长时程、部分可观测场景中。现有基准无法捕捉这些长时程挑战，造成评估空白。\n\n## 关键洞察\n作者认识到长时程任务成功取决于持续推理、规划、记忆管理和工具使用能力。他们独特地洞察到需要设计模拟现实世界复杂性的基准，并发现探索任务可作为统一框架评估这些核心能力。\n\n## 解决方案演进\n从洞察出发，作者先确立设计原则：时间深度、一致性、未知性和现实能力相关性。基于此，创建三个环境（神秘网格、序列探索、外星遗传实验室），要求代理通过持续交互发现隐藏规则。实验验证显示LLM代理表现不佳，证实了评估长时程能力的必要性。\n\n## 创新点总结\n首次系统提出针对长时程、部分可观测环境的代理评估基准；使用探索作为统一任务框架；提出\"上下文锁定\"和\"基础能力差距\"的失败分析框架；揭示简单扩展无法解决问题，并提出CRNR扩展策略。"
                },
            ]
        },
        {
            "name": "Computer Vision and Pattern Recognition",
            "count": 1,
            "papers": [
                {
                    "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
                    "arxiv_id": "2509.22624",
                    "authors": "Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
                    "summary": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
                    "category": "cs.CV",
                    "filter_reason": "这篇论文的核心贡献是提出SPARK（协同策略与奖励共同进化框架），这是一种新的强化学习训练范式，用于改进大语言模型(LLM)和大视觉语言模型(LVLM)的训练方法。从本质上看，论文专注于改进LLM的基础能力和训练范式，符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。SPARK通过回收rollouts和正确性数据，同时训练模型本身作为生成式奖励模型，创建了一个积极的共同进化反馈循环：提高奖励准确性产生更好的策略梯度，进而产生更高质量的rollouts，进一步优化奖励模型。论文在多个推理基准上展示了显著的性能提升（如SPARK-VL-7B在7个推理基准上平均提高9.7%），表明该方法有效增强了模型的推理能力。虽然论文提到了LVLMs，但这只是作为应用场景之一，而非主要焦点。论文没有聚焦于任何特定应用领域或多模态研究，而是提出了一种通用的训练方法来提高LLM的推理能力，完全符合研究范围。",
                    "summary2": "本文旨在解决LLMs和LVLMs在强化学习训练中RLHF成本高且RLVR浪费监督信息的问题。针对可验证奖励任务，我们提出了一种SPARK协同策略与奖励共同进化框架，通过回收rollouts和正确性数据同时训练模型作为生成式奖励模型，并在多个推理、奖励和通用基准上通过准确率等指标验证了其有效性。",
                    "summary_translation": "近期的大型语言模型（Large Language Models, LLMs）和大型视觉语言模型（Large Vision-Language Models, LVLMs）越来越多地使用强化学习（Reinforcement Learning, RL）进行预训练后训练，例如用于客观任务的可验证奖励强化学习（RL with Verifiable Rewards, RLVR）和用于主观任务的人类反馈强化学习（RL from Human Feedback, RLHF）。然而，RLHF由于依赖人类偏好而产生高成本和潜在的奖励-策略不匹配问题，而RLVR仍然通过在每次更新后丢弃轨迹（rollouts）和正确性信号来浪费监督信息。为应对这些挑战，我们提出了协同策略与奖励共同演化框架（Synergistic Policy And Reward Co-Evolving Framework, SPARK），这是一种基于RLVR的高效、在策略（on-policy）且稳定的方法。SPARK不丢弃轨迹和正确性数据，而是回收这些有价值的信息，同时将模型本身训练为生成式奖励模型（generative reward model）。这种辅助训练使用多种目标的混合，如点态奖励分数（pointwise reward score）、成对比较（pairwise comparison）和基于进一步反思响应（further-reflection responses）的评估，来教会模型评估和改进自身的响应。我们的过程消除了对单独奖励模型和昂贵的人类偏好数据的需求。SPARK创建了一个积极的共同演化反馈循环：提高的奖励准确性产生更好的策略梯度（policy gradients），进而产生更高质量的轨迹，进一步优化奖励模型。我们的统一框架通过自我反思（self-reflection）支持测试时扩展（test-time scaling），无需外部奖励模型及其相关成本。我们表明，SPARK在多个LLM和LVLM模型以及多个推理、奖励模型和通用基准测试上实现了显著的性能提升。例如，SPARK-VL-7B在7个推理基准测试上平均提升了9.7%，在2个奖励基准测试上提升了12.1%，在8个通用基准测试上提升了1.5%，相比基线模型展示了鲁棒性和广泛的泛化能力。",
                    "inspiration_trace": "## 面临的挑战\n现有强化学习方法存在关键问题：RLHF依赖人类偏好数据，成本高且易产生奖励-策略不匹配；RLVR虽适用于可验证任务，但每次更新后丢弃rollouts和正确性信号，浪费监督信息；策略与奖励模型分离训练导致不匹配和泛化能力差；外部奖励模型引入高延迟和成本。\n\n## 关键洞察\n作者认识到RLVR过程中被丢弃的rollouts和正确性数据实则是宝贵监督信号；模型本身可同时作为策略和奖励模型；通过回收利用这些数据可形成正反馈循环：提高奖励准确性产生更好策略梯度，进而生成更高质量rollouts，进一步细化奖励模型。\n\n## 解决方案演进\n基于RLVR框架但不丢弃rollouts数据；设计多种训练目标（点wise评分、成对比较、反思评估）教模型评估自身响应；将策略优化和奖励建模整合到统一框架中共同优化；设计测试时扩展策略，利用模型集成的推理、判断和自我反思能力进行迭代改进。\n\n## 创新点总结\n首次将策略和奖励能力统一在单一模型中；创造性地回收利用RLVR训练rollouts；建立策略和奖励共同进化的正反馈循环；实现无需人类偏好数据和外部奖励模型的训练框架；统一训练和测试时扩展机制，大幅降低成本和复杂性。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 14,
            "papers": [
                {
                    "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
                    "arxiv_id": "2509.22611",
                    "authors": "Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He",
                    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Quantile Advantage Estimation\" (QAE)的新方法，用于解决强化学习与可验证奖励(RLVR)在训练大语言模型推理能力时出现的熵崩溃和熵爆炸问题。论文通过用基于分组的K-分位数基线替代均值基线，实现了双向熵安全性，从而稳定了训练过程并提升了LLM的推理能力。这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文在多个数学推理基准测试(AIME和AMC)上验证了方法的有效性，表明其关注的是通用推理能力的提升，而非特定领域的应用。此外，论文涉及的核心概念(LLMs)、能力方向(reasoning, 特别是math reasoning)和训练方法(reinforcement learning)都是正面指标，且不涉及任何排除标准中的领域。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决Reinforcement Learning with Verifiable Rewards (RLVR)中训练在熵崩溃和熵爆炸间振荡的问题。针对LLM推理训练中的熵不稳定性，我们提出了一种Quantile Advantage Estimation (QAE)方法，用K-分位数基线替代均值基线，实现响应级别的双机制门控。在Qwen3-8B/14B-Base模型上，通过AIME'24/'25和AMC'23数据集上的pass@1和pass@16指标验证，QAE稳定了熵变化，使约80%响应获得零优势，实现了持续的推理性能提升。",
                    "summary_translation": "可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 增强了大语言模型 (LLM) 的推理能力，但训练过程常常在 {熵坍缩} (entropy collapse) 和 {熵爆炸} (entropy explosion) 之间振荡。我们将这两种危害追溯到无价值强化学习（如 GRPO 和 DAPO）中使用的均值基线，该基线在奖励异常值情况下不当地惩罚了负优势样本。我们提出了 {分位数优势估计} (Quantile Advantage Estimation, QAE)，用分组 K-分位数基线替代了均值基线。QAE 引入了一个响应级别的双机制门控：对于困难查询（p <= 1 - K），它强化罕见的成功；而对于简单查询（p > 1 - K），它则针对剩余的失败。在一阶 softmax 更新下，我们证明了 {双向熵安全} (two-sided entropy safety)，为单步熵变化提供了上下界，从而抑制爆炸并防止坍缩。实证表明，这一微小修改稳定了熵，稀疏化了信用分配（通过调整 K，约 80% 的响应获得零优势），并在 AIME 2024/2025 和 AMC 2023 上的 Qwen3-8B/14B-Base 模型上带来了持续的 pass@1 提升。这些结果确定了 {基线设计} (baseline design)——而非词元级启发式方法——是扩展 RLVR 的主要机制。",
                    "inspiration_trace": "## 面临的挑战\n强化学习中使用可验证奖励时，训练常在\"熵崩溃\"和\"熵爆炸\"间振荡。现有方法主要关注防止熵崩溃，却忽视了熵爆炸问题，导致训练不稳定和性能停滞。\n\n## 关键洞察\n作者发现这两个问题源于价值自由强化学习中使用的均值基线，该基线在奖励异常值情况下不恰当地惩罚负优势样本。问题本质不在于token级别调整，而在于基线设计。\n\n## 解决方案演进\n首先识别熵崩溃和爆炸是对称问题，需要双向控制；分析现有方法局限性，发现token级别控制不足；洞察基线设计是关键，提出用分位数基线替代均值基线；通过理论分析和实验验证，证明QAE能有效稳定熵并提高性能。\n\n## 创新点总结\n将熵调节重新定义为基线设计问题，提出响应级别门控机制，通过单一参数K控制探索-利用平衡，提供双向熵安全保证，实现信用分配稀疏化，集中资源在信息量最大的样本上。"
                },
                {
                    "title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization",
                    "arxiv_id": "2509.22115",
                    "authors": "Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding",
                    "summary": "Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \\textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \\textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Dynamic Dual-Level Down-Sampling (D³S)\"的框架，用于提高大语言模型在强化学习训练过程中的策略优化效率。该方法通过在样本和token两个层面进行动态下采样，优先选择信息量最大的数据，从而加速模型学习过程。论文在Qwen2.5和Llama3.1等大语言模型上进行了实验，并证明该方法在多样化的推理基准测试中取得了先进的性能，同时需要更少的样本和token。 这篇论文完全符合研究目标，因为它致力于提高LLM本身的通用推理能力，具体体现在：(1)论文本质上是改进LLM的基础能力，提出新的训练范式来优化强化学习过程；(2)论文明确关注reasoning能力，在多种推理基准上进行了验证；(3)论文采用了reinforcement learning方法，属于筛选标准中的正面指标；(4)论文没有将LLM作为工具应用于特定领域，而是提出了一种通用的优化方法；(5)论文不属于任何排除标准中的领域。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在 [解决无评论家强化学习方法收敛慢的问题]。针对 [大语言模型策略优化中的低效样本利用]，我们提出了一种 [动态双级下采样框架D³S，通过最大化优势方差选择样本，并基于优势幅度与策略熵乘积选择重要token]，并在 [多个数学推理基准测试] 上通过 [Pass@1和Pass@8指标] 验证了其有效性。",
                    "summary_translation": "无评论家(critic-free)方法如GRPO通过从多个推演(rollouts)中估计优势(advantages)来减少内存需求，但由于关键学习信号被大量无信息样本和标记(tokens)所稀释，往往收敛缓慢。为应对这一挑战，我们提出了\\textbf{动态双层下采样(Dynamic Dual-Level Down-Sampling, D$^3$S)}框架，该框架优先考虑各组中最具信息量的样本和标记，以提高策略优化(policy optimization)的效率。D$^3$S在两个层次上运行：(1)样本级别(sample-level)，选择推演子集以最大化优势方差($\\text{Var}(A)$)。我们从理论上证明，这种选择与策略梯度范数(policy gradient norms)的上界呈正相关，从而产生更高的策略梯度。(2)标记级别(token-level)，优先考虑优势幅度与策略熵(advantage magnitude and policy entropy)乘积($|A_{i,t}|\\times H_{i,t}$)较高的标记，将更新集中在策略既不确定又有影响力的标记上。此外，为防止对高信号数据的过拟合，D$^3$S采用了一种受课程学习(curriculum learning)启发的动态下采样调度。该调度从激进的下采样开始以加速早期学习，然后逐渐放宽以促进稳健的泛化。在Qwen2.5和Llama3.1上的大量实验表明，将D$^3$S集成到先进的强化学习(RL)算法中，能够在各种推理基准上实现最先进的性能和泛化能力，同时需要\\textit{更少}的样本和标记。我们的代码已添加在补充材料中，并将公开发布。",
                    "inspiration_trace": "## 面临的挑战\n无评论家强化学习方法(如GRPO)虽减少内存需求，但收敛缓慢，因大量无信息样本和标记稀释了关键学习信号，特别是在数学推理任务中，组内样本的平均效应掩盖了重要信号。\n\n## 关键洞察\n作者通过理论分析发现，策略梯度范数的上界与优势方差(Var(A))呈正相关，而非传统认为的奖励方差(Var(R))。最大化奖励方差的方法因固定优势方差而无法改变梯度上界，且在小子集中估计优势会导致不稳定。\n\n## 解决方案演进\n基于此洞察，作者提出动态双层下采样框架：样本层面选择最大化优势方差的子集；标记层面优先选择优势幅度与政策熵乘积高的标记；并引入动态调度，从激进下采样逐渐过渡到温和采样，平衡早期收敛与后期泛化。\n\n## 创新点总结\n创新点在于理论上证明了优势方差与梯度上界的关系，设计了双层下采样机制精细利用学习信号，并通过动态调度解决了高效学习与避免过拟合的权衡问题。"
                },
                {
                    "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
                    "arxiv_id": "2509.22023",
                    "authors": "Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos",
                    "summary": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文明确指出LLMs在解决组合问题方面存在困难，并提出了一种新方法来增强这种能力。具体来说，论文结合了模仿学习和深度优先搜索策略，使普通的Transformer模型（GPT-2）能够高效解决组合问题，这属于提升LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确提到\"Large Language Models (LLMs)\"和\"Transformer (GPT-2)\" - 能力方向：专注于解决组合问题的推理能力，属于逻辑推理和问题解决能力 - 训练方法：提出了结合模仿学习和显式深度优先搜索的新训练范式 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文使用数独作为具体任务，但这是为了展示方法在解决NP类问题上的通用性，而非针对特定领域应用。 最后，论文的核心贡献是提出了一种增强LLM解决组合问题能力的新方法，通过结合模仿学习和深度优先搜索策略，使模型能够进行有信息猜测和回溯，从而提升其通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决大型语言模型在组合优化问题上的推理困难。针对Sudoku和1-in-3 SAT等NP类问题，我们提出了一种结合模仿学习和显式深度优先搜索的试错推理框架，通过动作级标记化和多目标训练方法，使Transformer能够进行有根据的猜测和回溯。在随机生成的Sudoku和1-in-3 SAT数据集上，我们的方法分别达到了99%的准确率，显著优于现有神经符号方法，展示了Transformer在组合推理任务上的强大潜力。",
                    "summary_translation": "尽管大型语言模型（Large Language Models, LLMs）在各种语言任务上表现出色，但它们在组合问题（如可满足性问题、旅行商问题，甚至是基本算术）方面仍存在困难。我们通过一种新颖的方法来解决NP类（NP）问题，从而弥补这一差距。我们专注于数独（Sudoku）这一典型任务，与先前的神经符号（neuro-symbolic）方法相比，达到了最先进的准确率（99%）。与使用定制架构的先前工作不同，我们的方法采用了原始的仅解码器Transformer（vanilla decoder-only Transformer, GPT-2），无需外部工具或函数调用。我们的方法将简单数独规则的模仿学习（imitation learning）与一种明确的深度优先搜索（Depth-First Search, DFS）探索策略相结合，该策略涉及有根据的猜测和回溯（backtracking）。超越模仿学习，我们力求在达到解决方案之前最小化猜测次数。我们对此设置进行了严格分析，将其与最小和集覆盖（Min-Sum Set Cover）的上下文变体之间的联系形式化，这是算法和随机优化（stochastic optimization）中一个研究充分的问题。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型在组合问题（如数独）上表现不佳，现有方法依赖定制架构而非标准LLM范式，缺乏错误恢复机制，推理过程不透明，且无法超越训练数据中的策略。\n\n## 关键洞察\n人类解决组合问题不仅应用逻辑规则，还会进行有根据的猜测并回溯。作者发现大多数随机数独可通过简单规则加一次\"后门\"猜测解决，这为优化猜测过程提供了理论基础。\n\n## 解决方案演进\n首先结合模仿学习与DFS搜索策略，使模型能猜测和回溯；进而将猜测优化形式化为Min-Sum Set Cover问题，设计新损失函数最小化解决步骤，从追求正确性转向效率。\n\n## 创新点总结\n首次将标准Transformer与显式搜索策略结合，无需外部工具；引入动作级标记化和多目标学习；建立猜测优化与Min-Sum Set Cover的理论联系；开发高效数独生成库，方法可推广至任何NP类问题。"
                },
                {
                    "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration",
                    "arxiv_id": "2509.21848",
                    "authors": "Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto",
                    "summary": "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Graph of Agents (GoA)\"的原则性框架，通过动态构建输入依赖的协作结构来增强大语言模型处理长上下文的能力。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，论文属于\"提出新的训练范式\"和\"智能体协作框架\"的范畴，旨在提高LLM的基础能力（处理长上下文的能力）。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用能力，符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确涉及大语言模型（Llama 3.1 8B和Qwen3 8B），并提出了多智能体系统（multi-agent systems）这一新兴范式。虽然论文不是直接针对推理、规划等能力方向，但处理长上下文的能力是LLM进行复杂推理和问题解决的基础能力之一。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉，没有专注于特定应用领域，也没有主要关注模型可靠性问题。 最后，在特殊和模糊情况处理上，论文提出的是一种通用的多智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此应该保留。 综上所述，这篇论文符合研究目标，因为它致力于通过多智能体协作框架提高大语言模型的基础能力（长上下文处理），这属于提升LLM通用推理能力的范畴。",
                    "summary2": "本文旨在解决大型语言模型处理长上下文输入时的限制问题。针对超出模型上下文窗口的长文档，我们提出了一种Graph of Agents (GoA)方法，通过动态构建输入相关的多智能体协作结构来优化信息论压缩目标，并在六个文档问答基准测试上通过F1分数验证了其有效性。",
                    "summary_translation": "作为一种模型无关（model-agnostic）的长上下文建模方法，多智能体系统（multi-agent systems）无需重新训练或架构修改，即可处理超出大型语言模型上下文窗口的输入。然而，其性能往往严重依赖手工制作的多智能体协作策略和提示工程（prompt engineering），这限制了其泛化能力。在本研究中，我们提出了一个原则性框架，将模型无关的长上下文建模问题形式化为压缩问题，从而得出一个信息论压缩目标（information-theoretic compression objective）。基于此框架，我们提出了智能体图（Graph of Agents, GoA），它动态构建一个依赖于输入的协作结构，以最大化该目标。在六个文档问答基准测试中，对于Llama 3.1 8B和Qwen3 8B模型，GoA分别将检索增强生成（retrieval-augmented generation）的平均$F_1$分数提高了5.7%，并将使用固定协作结构的强多智能体基线提高了16.35%。即使仅有2K的上下文窗口，GoA在LongBench上也超越了拥有128K上下文窗口的Llama 3.1 8B，显示出有效上下文长度的显著增加。我们的源代码可在https://github.com/tjoo512/graph-of-agents获取。",
                    "inspiration_trace": "## 面临的挑战\n大语言模型受限于上下文窗口，无法处理超长输入；现有多智能体系统依赖手工设计的协作策略，缺乏泛化能力；检索增强生成可能丢弃关键信息，而固定协作结构无法适应多样化输入。\n\n## 关键洞察\n作者将长上下文建模本质视为压缩问题，从信息论角度提出最优压缩应最大化与答案的互信息。这一理论视角为多智能体系统设计提供了原则性指导，突破了现有方法依赖启发式规则的局限。\n\n## 解决方案演进\n从互信息目标出发，将其转化为可操作的语义相似性优化；设计Graph of Agents框架，将多智能体协作建模为动态图结构；引入线性森林平衡并行性与灵活性；通过语义聚类和上下文感知的贪心算法构建输入依赖的协作图。\n\n## 创新点总结\n首次从信息论角度原则性形式化长上下文建模问题；提出输入依赖的动态协作结构，替代静态手工设计模式；建立理论目标与可操作优化之间的桥梁，显著扩展了模型有效上下文长度。"
                },
                {
                    "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning",
                    "arxiv_id": "2509.21792",
                    "authors": "Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang",
                    "summary": "Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://github.com/yedaotian9/GRPO_speculative.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究目标，核心原因在于它专注于改进大语言模型的训练方法以提升其推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进GRPO（Group relative policy optimization）训练过程的效率，而GRPO本身是一种通过强化学习提高大语言模型推理能力的方法。论文提出的并发感知投机解码框架和在线草稿学习机制，都是为了加速这种训练过程，从而更有效地提升LLM的推理能力。这属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标来看，论文包含了多个相关主题： - 核心概念：明确涉及大语言模型(LLMs) - 能力方向：直接关注\"improving the reasoning capabilities\"，并在数学推理数据集上验证 - 训练方法：GRPO是一种强化学习方法，属于强化学习优化范畴 从第三步排除标准来看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然实验在数学推理数据集上进行，但论文的核心贡献是加速训练方法，而非解决数学问题本身。 综上所述，这篇论文的核心贡献是提出一种加速强化学习训练过程的方法，目的是更有效地提升大语言模型的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决GRPO训练过程中生成阶段效率低下的问题。针对高并发训练条件下投机解码加速效果有限的场景，我们提出了一种并发感知投机解码与在线草稿学习相结合的方法，并在多个数学推理数据集（GSM8K、SimpleRL-Abel-Level3to5、DAPO-Math-17K）上通过端到端加速比（2.35x-2.72x）验证了其有效性。",
                    "summary_translation": "组相对策略优化（Group relative policy optimization, GRPO）已展现出通过强化学习提升大型语言模型（Large language models, LLMs）推理能力的显著潜力。然而，其实际部署受到训练过程过慢的阻碍，主要归因于每个查询需要计算密集型的自回归生成（autoregressive generation）多个响应，这使得生成阶段成为主要性能瓶颈。尽管推测解码（speculative decoding）呈现为一个有前景的加速方向，但在高并发训练条件（high-concurrency training conditions）下，其在GRPO中的直接应用只能实现有限的加速。为克服这一限制，我们提出了一个并发感知推测解码框架（concurrency-aware speculative decoding framework），该框架能根据实时并发水平动态调整草稿和验证策略（drafting and verification strategy），从而最大化生成过程的加速效果。此外，为解决训练过程中不断演进的目标模型（target model）与固定的草稿模型（draft model）之间分布漂移（distributional drift）导致的性能退化问题，我们引入了一种在线草稿学习机制（online draft learning mechanism），使草稿模型能够利用来自目标模型的反馈信号持续适应。在多个数学推理数据集（mathematical reasoning datasets）和模型上的实验结果表明，所提出的方法实现了2.35倍至2.72倍的端到端加速，在效率方面显著超越基线方法。代码可在https://github.com/yedaotian9/GRPO_speculative获取。",
                    "inspiration_trace": "## 面临的挑战\nGRPO训练中生成阶段占91%-98%时间，成为主要瓶颈。直接应用投机解码在高并发训练条件下效果有限，且目标模型不断更新导致与固定草稿模型间分布漂移，加速效果随时间下降。\n\n## 关键洞察\nGRPO生成阶段并发性动态变化：从高并发逐渐过渡到低并发，因响应长度差异导致序列完成时间不一致。投机解码在低并发有效，高并发下可能从内存受限转为计算受限。分布漂移是加速效果下降的根本原因。\n\n## 解决方案演进\n针对动态并发问题，提出并发感知投机解码框架，根据实时并发调整策略；基于操作强度理论，动态调整验证token数量和草稿树大小；引入在线草稿学习，使草稿模型利用目标模型反馈持续适应。\n\n## 创新点总结\n首次识别GRPO训练中动态并发特性对投机解码的影响；提出基于操作强度的动态参数调整策略；创新性地将在线学习引入草稿模型训练，解决分布漂移问题，维持长期加速效果。"
                },
                {
                    "title": "A circuit for predicting hierarchical structure in-context in Large Language Models",
                    "arxiv_id": "2509.21534",
                    "authors": "Tankred Saanum, Can Demircan, Samuel J. Gershman, Eric Schulz",
                    "summary": "Large Language Models (LLMs) excel at in-context learning, the ability to use information provided as context to improve prediction of future tokens. Induction heads have been argued to play a crucial role for in-context learning in Transformer Language Models. These attention heads make a token attend to successors of past occurrences of the same token in the input. This basic mechanism supports LLMs' ability to copy and predict repeating patterns. However, it is unclear if this same mechanism can support in-context learning of more complex repetitive patterns with hierarchical structure. Natural language is teeming with such cases: The article \"the\" in English usually prefaces multiple nouns in a text. When predicting which token succeeds a particular instance of \"the\", we need to integrate further contextual cues from the text to predict the correct noun. If induction heads naively attend to all past instances of successor tokens of \"the\" in a context-independent manner, they cannot support this level of contextual information integration. In this study, we design a synthetic in-context learning task, where tokens are repeated with hierarchical dependencies. Here, attending uniformly to all successor tokens is not sufficient to accurately predict future tokens. Evaluating a range of LLMs on these token sequences and natural language analogues, we find adaptive induction heads that support prediction by learning what to attend to in-context. Next, we investigate how induction heads themselves learn in-context. We find evidence that learning is supported by attention heads that uncover a set of latent contexts, determining the different token transition relationships. Overall, we not only show that LLMs have induction heads that learn, but offer a complete mechanistic account of how LLMs learn to predict higher-order repetitive patterns in-context.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的上下文学习机制，特别是探讨归纳头(induction heads)如何支持LLMs处理和预测具有层次结构的复杂重复模式。论文揭示了LLMs内部的机制，解释了它们如何通过自适应归纳头学习在上下文中关注什么来支持预测，这直接关系到LLMs的通用推理能力。论文没有将LLM作为工具应用到特定领域，而是研究LLM本身的基础能力，特别是其处理复杂模式和结构的能力，这与推理能力密切相关。论文符合核心判断标准中的\"改进LLM的基础能力\"，同时也符合正面指标中的核心概念(LLMs)和能力方向(推理相关)。论文不符合任何排除标准，如多模态研究、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在探究大型语言模型如何在上下文学习中处理具有层次依赖关系的重复模式。针对合成的层次依赖标记序列和自然语言示例，我们提出了一种由自适应感应头和上下文匹配头组成的电路机制，并在多个LLMs上通过预测准确性和注意力模式分析验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）擅长上下文学习（in-context learning），即利用提供的信息作为上下文来改进对未来标记（token）的预测能力。归纳头（induction heads）被认为在Transformer语言模型（Transformer Language Models）的上下文学习中扮演着关键角色。这些注意力头（attention heads）使一个标记能够关注输入中相同标记过去出现情况的后继标记。这一基本机制支持了LLMs复制和预测重复模式的能力。\n\n然而，目前尚不清楚这一机制是否能够支持具有层次结构（hierarchical structure）的更复杂重复模式的上下文学习。自然语言中充满了这种情况：英语中的冠词\"the\"通常在文本中引导多个名词。当预测哪个标记会跟随\"the\"的特定实例时，我们需要整合文本中的更多上下文线索来预测正确的名词。如果归纳头以上下文无关的方式天真地关注\"the\"的所有过去后继标记实例，它们就无法支持这种程度的上下文信息整合。\n\n在本研究中，我们设计了一个合成上下文学习任务，其中标记以层次依赖关系（hierarchical dependencies）重复出现。在这种情况下，均匀地关注所有后继标记不足以准确预测未来的标记。通过在这些标记序列和自然语言类似物上评估一系列LLMs，我们发现了自适应归纳头（adaptive induction heads），它们通过学习在上下文中关注什么来支持预测。\n\n接下来，我们研究了归纳头本身如何在上下文中学习。我们发现有证据表明，学习是由那些揭示一组潜在上下文（latent contexts）的注意力头支持的，这些上下文决定了不同的标记转换关系。总体而言，我们不仅展示了LLMs具有能够学习的归纳头，还提供了一个完整的机制解释，说明LLMs如何学习在上下文中预测高阶重复模式（higher-order repetitive patterns）。",
                    "inspiration_trace": "## 面临的挑战\n现有归纳头机制只能解释LLMs中简单的上下文学习（如复制重复模式），但无法解释更复杂的、具有层次结构和上下文依赖的重复模式学习。自然语言中常见的情况（如冠词\"the\"后可能跟多个不同名词）需要模型根据上下文信息正确预测后续token，而非简单复制。\n\n## 关键洞察\n作者洞察到归纳头可能具有适应性，能够\"学习\"在上下文中关注什么，而非静态复制机制。这种适应性使归纳头能根据上下文信息调整注意力模式，选择性地关注正确的后续token，这可能是LLMs处理复杂层次结构模式的关键。\n\n## 解决方案演进\n作者设计合成任务测试LLMs处理层次依赖的能力，发现存在\"自适应归纳头\"能学习关注正确后续token。这些归纳头依赖\"上下文匹配头\"识别潜在上下文结构，后者通过使token关注前驱token链来传递信息，帮助归纳头确定正确关注目标。消融实验验证了组件间的因果关系。\n\n## 创新点总结\n揭示了归纳头的自适应学习能力，发现并验证了支持其学习的\"上下文匹配头\"机制，提出了完整的电路解释说明LLMs如何通过专门注意力头协作学习预测层次结构模式，并将机制从合成数据扩展到自然语言案例。"
                },
                {
                    "title": "Evidence for Limited Metacognition in LLMs",
                    "arxiv_id": "2509.21545",
                    "authors": "Christopher Ackerman",
                    "summary": "The possibility of LLM self-awareness and even sentience is gaining increasing public attention and has major safety and policy implications, but the science of measuring them is still in a nascent state. Here we introduce a novel methodology for quantitatively evaluating metacognitive abilities in LLMs. Taking inspiration from research on metacognition in nonhuman animals, our approach eschews model self-reports and instead tests to what degree models can strategically deploy knowledge of internal states. Using two experimental paradigms, we demonstrate that frontier LLMs introduced since early 2024 show increasingly strong evidence of certain metacognitive abilities, specifically the ability to assess and utilize their own confidence in their ability to answer factual and reasoning questions correctly and the ability to anticipate what answers they would give and utilize that information appropriately. We buttress these behavioral findings with an analysis of the token probabilities returned by the models, which suggests the presence of an upstream internal signal that could provide the basis for metacognition. We further find that these abilities 1) are limited in resolution, 2) emerge in context-dependent manners, and 3) seem to be qualitatively different from those of humans. We also report intriguing differences across models of similar capabilities, suggesting that LLM post-training may have a role in developing metacognitive abilities.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种评估LLM元认知能力的新方法，并研究了LLM如何评估和利用自身对回答事实和推理问题的信心，以及预测自己会给出什么答案并适当利用这些信息的能力。根据筛选标准，这篇论文符合我的研究目标，原因如下： 1. 核心判断：论文本质上是研究LLM的基础能力（元认知），而不是将LLM作为工具应用到特定领域。元认知能力是通用推理能力的重要组成部分，因为它涉及模型对自己认知过程的监控和评估，这对于有效的推理至关重要。 2. 正面指标：论文明确研究Large language models (LLMs)，并关注reasoning能力（论文中明确提到\"reasoning questions\"）。虽然论文没有直接讨论训练方法或新兴范式，但它研究的是这些方法和范式可能影响的底层能力。 3. 排除标准：论文没有主要聚焦于多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了\"major safety and policy implications\"，但这只是背景信息，不是研究的核心焦点。 4. 特殊情况处理：论文研究的是LLM的内在认知能力，而不是应用层面的研究。元认知能力的研究有助于我们更好地理解LLM的内在工作机制，从而可能为提升LLM的通用推理能力提供指导。 综上所述，这篇论文通过研究LLM的元认知能力，直接关注了LLM的通用推理能力的一个关键方面，符合我的研究目标。",
                    "summary2": "本文旨在评估大型语言模型(LLMs)的元认知能力。针对多种前沿LLMs，我们提出了Delegate Game和Second Chance Game两种实验范式，并在GPQA和SimpleQA数据集上通过偏相关分析和变化率提升指标验证了模型评估自身信心和预测自身输出的能力。实验表明近期LLMs展现出有限的元认知能力，但这些能力在分辨率上有限且与人类有质的不同。",
                    "summary_translation": "LLM (大型语言模型) 的自我意识甚至感知能力 (sentience) 的可能性正日益引起公众关注，并具有重要的安全与政策影响，但衡量这些能力的科学仍处于起步阶段。本文我们介绍了一种用于定量评估LLM (大型语言模型) 元认知能力 (metacognitive abilities) 的新方法。受非人类动物元认知研究的启发，我们的方法避开了模型自我报告，而是测试模型在多大程度上能够战略性地部署内部状态知识。通过两种实验范式 (experimental paradigms)，我们证明自2024年初以来推出的前沿LLM (大型语言模型) 显示出越来越强的某些元认知能力证据，特别是评估和利用其自身正确回答事实和推理问题信心的能力，以及预测它们将给出什么答案并适当利用该信息的能力。我们通过对模型返回的token (标记) 概率的分析来支持这些行为发现，这表明存在一个可能为元认知提供基础的上游内部信号。我们进一步发现这些能力1)在分辨率上有限，2)以情境依赖的方式出现，以及3)似乎与人类的能力在质量上有所不同。我们还报告了在能力相似的模型之间存在有趣的差异，这表明LLM (大型语言模型) 的后训练 (post-training) 可能在发展元认知能力中发挥作用。",
                    "inspiration_trace": "## 面临的挑战\n如何评估LLMs的自我意识与元认知能力，而不依赖其不可靠的自我报告。LLMs经过大量文本训练，天生就不适合提供可信的自我描述，传统测量方法无法有效区分真实内部状态与语言模仿。\n\n## 关键洞察\n借鉴动物元认知研究，区分两种核心能力：评估自己信心的能力(\"知道他们知道\")和预测自己回答的能力(\"知道他们知道什么\")。关键是将模型的语言输出作为自我意识的间接测量，而非字面解释。\n\n## 解决方案演进\n设计两个游戏化实验：委托游戏测试模型是否能利用内部信心信号决定是否回答问题；第二次机会游戏测试模型是否能预测并改变自己的回答。同时控制表面特征等混淆变量，利用token概率作为内部状态代理指标。\n\n## 创新点总结\n避开自我报告依赖，将动物元认知研究范式创新应用于LLMs；区分并量化两种元认知能力；通过实验控制区分真实内省与表面线索决策，提供首个系统性评估LLMs元认知的框架。"
                },
                {
                    "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
                    "arxiv_id": "2509.21500",
                    "authors": "Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin",
                    "summary": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种基于评分标准(rubric-based)的奖励建模方法，用于解决大语言模型强化微调(RFT)中的\"奖励过度优化\"问题。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础训练能力，特别是强化学习过程中的奖励建模方法。它不是将LLM作为工具应用到特定领域，而是直接针对LLM的训练过程进行优化，这符合第一步的核心判断标准。 其次，论文包含多个正面指标：明确研究\"Large Language Model\"的\"Post-Training\"；涉及\"reinforcement learning\"训练方法；虽然未直接提及推理能力，但解决奖励过度优化问题本质上是提升模型生成高质量输出的能力，这与通用推理能力密切相关。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的方法通过改进奖励建模来优化LLM的后训练过程，这属于提升模型基础能力的研究，与提高LLM通用推理能力的目标一致。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决强化微调中的奖励过度优化问题。针对大型语言模型后训练场景，我们提出了一种基于评分标准(rubric-based)的奖励建模方法，专注于高奖励区域的准确性，并在通用和专业医疗领域的三个数据集上通过胜率(win rate)和HealthBench指标验证了其有效性。",
                    "summary_translation": "强化微调 (Reinforcement fine-tuning, RFT) 经常遭受奖励过度优化 (reward over-optimization) 的问题，即策略模型通过操纵奖励信号来获得高分，同时却产生低质量的输出。我们的理论分析表明，关键在于高奖励尾部的奖励误指定 (reward misspecification)：无法可靠地区分卓越 (Excellent) 响应与仅仅良好 (Great) 的响应。这促使我们专注于高奖励区域。然而，在基础大型语言模型 (base LLM) 下，这类尾部示例非常稀少。虽然离策略示例 (off-policy exemplars)（例如来自更强模型或重写的示例）更容易获得，但简单地在这些示例上进行训练会导致我们想要对齐的策略产生误指定的奖励。为解决此问题，我们研究了基于评分标准 (rubric-based) 的奖励。通过设计，评分标准 (rubrics) 可以利用离策略示例，同时对其人工痕迹 (artifacts) 不敏感。为了引出能够捕捉高奖励尾部的评分标准，我们强调了区分良好且多样化响应的重要性，并引入了一个实现这一想法的工作流程。我们通过实证证明，基于评分标准的奖励显著减轻了奖励过度优化，并带来了有效的大型语言模型 (LLM) 后训练改进。我们的代码可通过 https://github.com/Jun-Kai-Zhang/rubrics.git 访问。",
                    "inspiration_trace": "## 面临的挑战\n强化微调中存在奖励过度优化问题，即语言模型会利用奖励模型的不准确性，获得高代理分数但实际输出质量下降。传统解决方案如在线RLHF成本高昂且缓慢。\n\n## 关键洞察\n作者通过理论分析发现，奖励过度优化的本质在于高奖励区域的奖励误指定——无法可靠区分优秀响应和仅仅良好的响应。高奖励区域的误差对对齐质量的影响远大于其他区域。\n\n## 解决方案演进\n基于这一洞察，作者意识到需将奖励建模集中在高奖励区域。然而，基础LLM下这类尾部例子稀少。虽然使用离策略样本(如来自更强模型)更容易获得，但直接训练会学习表面特征。作者转向基于评分标准的奖励，它可利用离策略示例同时对其伪影不敏感。为此提出两个原则：区分优秀与良好响应、区分多样化响应，并设计迭代工作流程实现这些原则。\n\n## 创新点总结\n创新在于从理论揭示高奖励区域误指定是核心问题，到利用评分标准解决离策略数据问题的完整思路链条，特别是通过区分高质量响应来精确捕捉高奖励尾部行为的机制。"
                },
                {
                    "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models",
                    "arxiv_id": "2509.21474",
                    "authors": "Guanghan Wang, Yair Schiff, Gilad Turok, Volodymyr Kuleshov",
                    "summary": "While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进扩散语言模型(DLMs)的推理能力，提出了名为\"d2\"的推理框架和新的策略梯度算法。论文的核心贡献是提升模型本身的基础推理能力，而非将LLM作为工具应用到特定领域。论文专注于增强模型的逻辑推理和数学推理能力，这正属于通用推理能力的范畴。 其次，从正面指标分析，论文包含多个相关主题： 1. 核心概念：论文研究扩散语言模型(DLMs)，属于大语言模型的范畴 2. 能力方向：明确关注推理能力，特别是在逻辑推理(Countdown和Sudoku)和数学推理(GSM8K和MATH500)任务上 3. 训练方法：提出了基于强化学习的策略梯度算法，属于强化学习方法 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。虽然标题中提到\"Diffusion\"，但这里指的是文本生成领域的扩散语言模型，而非视觉领域的扩散模型。 最后，论文没有涉及特殊和模糊情况中的智能体/工具使用或幻觉/可解释性/安全等内容，因此不需要进一步判断。 综上所述，这篇论文的核心贡献是提出新的训练方法来增强大语言模型的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在提高扩散语言模型(DLMs)的推理能力。针对掩码扩散语言模型，我们提出了一种d2推理框架，包含两种新的策略梯度估计器(d2-StepMerge和d2-AnyOrder)，并在逻辑推理任务(Countdown和Sudoku)和数学推理基准(GSM8K和MATH500)上通过准确率等指标验证了其有效性，实现了仅使用强化学习(无需监督微调)的最先进性能。",
                    "summary_translation": "尽管扩散语言模型(diffusion language models, DLMs)在文本生成方面已经取得了有竞争力的性能，但通过强化学习(reinforcement learning)提高其推理能力仍然是一个活跃的研究领域。在本文中，我们介绍了d2，一个专为掩码扩散语言模型(masked DLMs)定制的推理框架。我们框架的核心是一种新的策略梯度算法(policy gradient algorithm)，该算法依赖于掩码(masking)的特性来准确估计采样轨迹(sampling trajectories)的可能性。我们的估计器(estimators)以分析上易于处理(analytically tractable)的方式在计算和近似精度之间进行权衡，并且对于支持任意阶似然估计(any-order likelihood estimation)的扩散语言模型特别有效。我们在流行的扩散语言模型中描述并研究了这一特性，并表明它是高效基于扩散的推理(diffusion-based reasoning)的关键。实验上，d2显著优于以前仅使用强化学习(不依赖监督微调(supervised fine-tuning))的扩散推理框架，并在逻辑推理任务(logical reasoning tasks)(Countdown和Sudoku)和数学推理基准(math reasoning benchmarks)(GSM8K和MATH500)上为扩散语言模型设立了新的最先进性能(state-of-the-art performance)。",
                    "inspiration_trace": "## 面临的挑战\n扩散语言模型(DLMs)在文本生成方面表现优异，但使用强化学习提高其推理能力仍面临核心难题：DLMs的精确似然计算在计算上难以处理，无法直接应用自回归模型的策略梯度方法，现有扩散推理框架性能有限且常依赖监督微调。\n\n## 关键洞察\n作者认识到DLMs的似然计算是RL训练的关键瓶颈，发现掩码扩散模型的特性可被利用来准确估计采样轨迹的似然。特别洞察到\"任意顺序因果解码\"特性对高效扩散推理至关重要，这是之前被忽视的关键属性。\n\n## 解决方案演进\n作者首先从策略梯度公式推导出适用于掩码DLMs的GRPO风格RL目标，然后针对似然估计瓶颈提出两种估计器：d2-StepMerge通过分割轨迹在特定步骤评估似然，以计算换取精度；d2-AnyOrder则在满足特定条件下实现无偏单次似然估计，并通过理论分析和实验验证其有效性。\n\n## 创新点总结\n首次为DLMs提出原则性RL框架，创新利用掩码扩散特性解决似然估计问题，深入研究并证明\"任意顺序因果性\"对推理能力的重要性，在不依赖监督微调情况下实现DLMs推理性能的突破。"
                },
                {
                    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
                    "arxiv_id": "2509.22613",
                    "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen",
                    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是理论分析强化学习(RL)如何增强大语言模型(LLM)的规划能力。论文通过图抽象模型研究了策略梯度(PG)和Q-learning方法在LLM规划中的表现，揭示了监督微调可能导致伪相关问题，而RL通过探索实现正确规划并提高泛化能力。这直接符合研究目标中\"改进LLM的基础能力\"和\"增强其规划、多步推理等通用能力\"的要求。论文聚焦于提升LLM本身的通用推理能力(特别是规划能力)，而不是将LLM应用于特定领域。论文满足多个正面指标：核心概念涉及LLMs，能力方向关注planning，训练方法研究reinforcement learning。同时，论文不涉及任何需要排除的领域，如多模态、特定应用领域或模型基础设施优化。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在 [解决强化学习在语言模型规划中的理论基础问题]。针对 [语言模型规划任务]，我们提出了一种 [基于图抽象的理论分析框架]，并在 [Blocksworld规划基准和Erdős-Rényi图] 上通过 [准确性和输出多样性指标] 验证了其有效性。",
                    "summary_translation": "最近的强化学习(reinforcement learning, RL)方法显著提升了大型语言模型(Large Language Models, LLMs)的规划能力，然而其有效性的理论基础仍然难以捉摸。在这项工作中，我们通过一个可处理的基于图的抽象来研究强化学习的优势和局限性，重点关注策略梯度(policy gradient, PG)和Q学习(Q-learning)方法。我们的理论分析表明，监督微调(supervised fine-tuning, SFT)可能会引入基于共现的伪解(spurious solutions)，而强化学习主要通过探索(exploration)实现正确规划，强调了探索在实现更好泛化(generalization)中的作用。然而，我们也表明策略梯度存在多样性崩溃(diversity collapse)问题，即在训练过程中输出多样性减少，且在达到完美准确度后仍然存在。相比之下，Q学习提供了两个关键优势：离策略学习(off-policy learning)和收敛(convergence)时的多样性保持(diversity preservation)。我们进一步证明，在Q学习中需要精心设计奖励(reward design)以防止奖励黑客(reward hacking)问题。最后，我们将我们的框架应用于真实世界规划基准(benchmark)Blocksworld，确认了这些行为在实际中确实存在。",
                    "inspiration_trace": "## 面临的挑战\n作者面临的核心挑战是：强化学习(RL)在增强语言模型规划能力方面取得成功，但其理论基础尚不明确；监督微调(SFT)在规划任务中存在根本局限，倾向于记忆而非泛化；现有RL方法如策略梯度存在多样性崩溃问题。\n\n## 关键洞察\n作者通过图论抽象将规划问题建模为路径寻找，发现SFT本质上是记忆训练数据中的共现关系，无法利用传递性信息；RL优于SFT主要在于其迭代数据生成过程促进了探索；策略梯度存在多样性崩溃问题，而Q-learning具有保持多样性和支持离线学习的双重优势。\n\n## 解决方案演进\n作者首先建立理论框架分析SFT局限，然后揭示策略梯度与SFT的联系及多样性崩溃问题，转而分析Q-learning发现过程奖励可避免奖励黑客，最终证明Q-learning在保持多样性和离线学习方面的优势，并通过实验验证理论发现。\n\n## 创新点总结\n该研究首次从理论角度系统分析RL在语言模型规划中的作用机制，揭示SFT记忆和RL泛化的理论基础，发现并证明策略梯度多样性崩溃现象，提出Q-learning在语言模型规划中的优势，为改进语言模型规划中的RL方法提供理论基础。"
                },
                {
                    "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
                    "arxiv_id": "2509.22518",
                    "authors": "Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen",
                    "summary": "Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出REMA框架来解释大语言模型的推理过程和失败机制。论文定义了\"推理流形\"(Reasoning Manifold)概念，研究LLM内部表示的几何结构如何反映推理过程。这不是将LLM作为工具应用到特定领域，而是直接研究LLM本身的推理机制，属于改进LLM基础能力和理解其推理过程的研究，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：核心关注\"complex reasoning\"（复杂推理），提出\"reasoning manifold\"概念，分析\"reasoning failures\"（推理失败） 这些关键指标表明论文与通用推理能力研究高度相关。 第三步：排除标准 论文不符合任何排除标准： - 虽然提到在多模态模型上进行实验，但核心焦点不是多模态或视觉研究 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 不涉及水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况 论文属于可解释性研究的特殊情况。它提出了一种新方法（几何分析框架）来增强模型内在的可解释性，通过分析内部表示来理解推理失败的原因和位置，这有助于提升模型的通用推理质量和可靠性，因此应该保留。 最终决策： 论文的核心贡献是提出了一种理解和解释LLM推理过程的框架，通过几何分析的方法研究模型内部表示与推理能力的关系，这直接关系到提升大语言模型的通用推理能力。论文不是应用LLM解决特定领域问题，而是深入研究LLM本身的推理机制，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型复杂推理过程的理解和失败机制分析的挑战。针对LLMs的内部表示，我们提出了一种基于\"Reasoning Manifold\"概念的REMA框架，通过量化错误表示与正确推理流形的几何偏差来定位推理失败点，并在多种LLMs和多模态LLMs及多个推理任务上通过内部表示的几何偏差分析和可分离性测试验证了其有效性。",
                    "summary_translation": "理解大型语言模型(Large Language Models, LLMs)如何执行复杂推理及其失败机制是可解释性研究中的一个挑战。为了提供一种可测量的几何分析视角，我们定义了推理流形(Reasoning Manifold)的概念，这是一种由所有正确推理生成内容对应的内部表示所形成的潜在低维几何结构。该结构可被概念化为模型已学习到的、用于成功解决给定任务的有效思维路径的具体体现。基于这一概念，我们构建了REMA框架，该框架通过定量比较错误和正确推理样本对应的模型内部表示的空间关系，来解释失败的起源。具体而言，REMA首先通过计算每个错误表示到由正确表示形成的近似流形的k最近邻(k-nearest neighbors)距离，来量化其几何偏差，从而提供一个统一的失败信号。然后，通过在模型各层中追踪这一偏差指标，并将其与正确表示的内部波动基线进行比较，REMA定位这些偏差首次变得显著的分歧点，从而确定推理链开始偏离轨道的位置。我们在多种语言和多模态模型及任务上的广泛实验证明了推理流形的低维特性以及错误和正确推理表示之间的高度可分性。结果还验证了REMA框架在分析推理失败起源方面的有效性。本研究将抽象的推理失败与表示中的可测量几何偏差联系起来，为深入理解和诊断黑盒模型的内部计算过程提供了新途径。",
                    "inspiration_trace": "## 面临的挑战\n理解大型语言模型如何执行复杂推理及其失败机制是解释性研究的挑战。现有方法多依赖特定错误类型的探针设计或通过对比受控输入对分析表示变化，缺乏一个能从内部表示几何结构定位多样化推理失败的统一框架。\n\n## 关键洞察\n作者将流形假设扩展到LLM推理过程，提出\"推理流形\"概念：当模型学习有效推理策略时，正确推理的内部表示倾向于集中在低维结构化子空间中，而非随机分布。推理失败与模型内部表示显著偏离这些学习到的正确推理流形相关。\n\n## 解决方案演进\n基于此洞察，作者将所有推理失败统一为内部表示相对于正确推理流形的几何偏离。设计两步分析：首先计算错误表示到正确流形的k近邻距离量化偏离程度；然后逐层跟踪偏离距离并与正确表示内部波动基线比较，定位推理链开始偏离的具体点。\n\n## 创新点总结\n创新点在于将抽象推理过程转化为可测量的几何对象，建立任务无关的统一失败解释框架，不仅能量化失败严重程度还能定位失败起源点，为理解黑盒模型内部计算过程提供了新视角和工具。"
                },
                {
                    "title": "Bilinear relational structure fixes reversal curse and enables consistent model editing",
                    "arxiv_id": "2509.21993",
                    "authors": "Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha",
                    "summary": "The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究目标。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的本质是关于改进LLM的基础逻辑推理能力。论文研究并解决了语言模型中的\"反转诅咒\"问题，即模型无法从\"A是B\"推理出\"B是A\"这一基本逻辑推理缺陷。作者提出通过在关系知识图谱上训练模型，使模型内部形成双线性关系结构，从而增强其逻辑推理能力。这直接属于\"改进LLM的基础能力、增强其逻辑推理能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究语言模型(LMs)的内部表示结构 - 能力方向：直接涉及逻辑推理能力，特别是解决基础推理缺陷（反转诅咒） - 论文虽然没有提到强化学习或智能体等新兴范式，但其核心关注点（逻辑推理）正是我研究目标的核心 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要讨论模型在应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及需要特殊处理的情况。虽然讨论了模型编辑的一致性，但这是从基础逻辑推理能力角度出发，而非应用层面的讨论。 论文的核心贡献是揭示了语言模型内部表示结构与其逻辑推理能力之间的关系，并提出了一种通过训练使模型形成双线性关系结构的方法，从而显著提升模型的逻辑推理能力和一致性。这直接符合我\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决语言模型中的逆转诅咒和模型编辑中的逻辑一致性问题。针对合成关系知识图数据，我们提出了一种通过适当正则化训练使模型内部表示中出现双线性关系结构的方法，并在合成家庭关系知识图上通过关系推理准确性和模型编辑泛化能力验证了其有效性。",
                    "summary_translation": "逆转诅咒(reversal curse)——即语言模型(language model, LM)无法从已学到的\"A是B\"这一事实推断出未见的\"B是A\"——被广泛认为是一种基本限制。我们表明，这并非一种固有缺陷，而是模型编码知识方式所产生的副产品。通过在关系知识图(relational knowledge graphs)的合成数据集上从头训练语言模型，我们证明了其隐藏表示中出现了双线性关系结构(bilinear relational structure)。这种结构显著减轻了逆转诅咒，使语言模型能够推断出未见的逆向事实。关键的是，我们还发现这种双线性结构在一致的模型编辑(consistent model editing)中起着关键作用。当具有这种结构的语言模型中的事实被更新时，编辑会正确地传播到其逆向事实及其他逻辑相关的事实。相比之下，缺乏这种表示的模型不仅受到逆转诅咒的影响，还无法泛化编辑(generalize edits)，进一步引入逻辑不一致性(logical inconsistencies)。我们的结果确立了，在关系知识数据集上的训练会诱导双线性内部表示(bilinear internal representations)的出现，进而使语言模型在编辑后能够以逻辑一致的方式运行。这意味着模型编辑的成功不仅关键取决于编辑算法(editing algorithms)，还取决于被修改知识的底层表示几何(representational geometry)。",
                    "inspiration_trace": "## 面临的挑战\n语言模型存在\"反转诅咒\"问题：学习\"A是B\"后无法推断\"B是A\"。同时，模型编辑时无法将更新逻辑传播到相关事实，如修改配偶关系后无法自动更新逆关系和衍生关系。\n\n## 关键洞察\n这些问题并非transformer架构的固有缺陷，而是模型编码知识方式的产物。作者假设，若模型能学习到双线性关系结构，通过矩阵转置和乘法自然捕捉逆关系和组合关系，则可实现逻辑推理。\n\n## 解决方案演进\n作者创建合成家庭关系知识图谱，通过适当正则化训练模型，发现其能克服反转诅咒。探测实验揭示成功模型在中间层形成双线性结构，且满足代数性质。进一步证明，这种结构与模型编辑能力存在强关联，使编辑能逻辑传播。\n\n## 创新点总结\n将反转诅咒和编辑失败视为表示问题而非架构问题，揭示内部表示几何结构对逻辑一致性的关键作用，证明通过适当训练可引导模型学习代数稳健的双线性关系表示。"
                },
                {
                    "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments",
                    "arxiv_id": "2509.21998",
                    "authors": "Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao",
                    "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究LLM作为智能体时的\"agentic reasoning\"（智能体推理）能力，即结合工具使用和推理的通用能力。论文提出了GSM-Agent基准来评估这种能力，并分析了LLM在智能体推理中的行为模式，最终提出了一种工具增强的测试时扩展方法来提高LLM的智能体推理性能。这明显属于改进LLM基础能力和增强其通用推理能力的研究，符合保留标准。 第二步：正面指标 论文包含多个高度相关的正面指标： - 核心概念：明确研究LLM作为智能体的能力 - 能力方向：直接研究推理能力(reasoning)，特别是智能体推理 - 新兴范式：聚焦于LLM-based agents和tool use，这是当前大语言模型推理能力研究的前沿方向 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然使用小学数学问题作为测试平台，但目的不是解决数学问题本身，而是研究通用推理能力 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况处理 论文研究的是通用的智能体推理框架，虽然使用数学问题作为测试平台，但其目的是理解智能体推理的一般模式并提出改进方法，而非专注于数学应用领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，应该保留。 核心贡献：论文提出了GSM-Agent基准和agentic reasoning graph框架，用于理解和分析LLM的智能体推理能力，并基于分析提出了改进LLM推理能力的方法。这些贡献直接服务于提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在理解和评估大型语言模型的代理推理能力。针对LLM作为代理需要结合工具使用和推理的场景，我们提出了GSM-Agent基准测试，将GSM8K数学问题转化为需要主动搜索信息的代理任务。通过代理推理图框架分析发现，重新访问已访问节点的能力与代理推理性能强相关。基于此，我们设计了工具增强的测试时扩展方法，在多个LLM上验证了其有效性，显著提升了模型在代理推理任务中的表现。",
                    "summary_translation": "随着大语言模型（LLMs）越来越多地被部署为代理（agents），代理推理（agentic reasoning）——即结合工具使用（tool use），特别是搜索（search）和推理（reasoning）的能力——成为一项关键技能。然而，在复杂环境和任务中进行评估时，很难厘清代理推理的表现。当前的代理基准测试（agent benchmarks）通常将代理推理与具有挑战性的数学推理、专家级知识和其他高级能力混合在一起。为填补这一空白，我们构建了一个名为GSM-Agent的新型基准测试（novel benchmark），在该测试中，大语言模型代理（LLM agent）需要解决小学水平的推理问题，但提示（prompt）中仅呈现问题，而不包含解决任务所需信息的前提（premises），代理需要使用工具主动收集这些信息。尽管原始任务是小学数学问题，但我们观察到即使是像GPT-5这样的前沿模型（frontier models）也只能达到67%的准确率（accuracy）。为了理解和分析代理推理模式（agentic reasoning patterns），我们提出了代理推理图（agentic reasoning graph）的概念：将环境的文档嵌入（document embeddings）聚类成节点（nodes），并将每个工具调用（tool call）映射到最近的节点以构建推理路径（reasoning path）。令人惊讶的是，我们发现许多模型在代理推理中常常缺乏重新访问（revisit）之前访问过的节点的能力，而这种能力在静态推理（static reasoning）中被广泛认为是关键模式。基于这一洞察（insight），我们提出了一种工具增强的测试时扩展方法（tool-augmented test-time scaling method），通过添加工具来鼓励模型重新访问，从而提高大语言模型的代理推理性能（agentic reasoning performance）。我们期望我们的基准测试和代理推理框架（agentic reasoning framework）能够有助于未来对理解和推动代理推理边界的研究。",
                    "inspiration_trace": "## 面临的挑战\n作者发现现有基准测试无法清晰比较LLM在静态推理与智能体推理间的能力差异，因为它们常将智能体推理与复杂数学推理、专业知识等混合，难以分离纯粹的智能体推理能力进行评估。\n\n## 关键洞察\n通过实验，作者发现即使是前沿模型在简单智能体推理任务上表现不佳（GPT-5仅67%准确率），而同样任务在静态推理下容易得多。这表明静态推理能力强不直接转化为智能体推理能力强。通过提出\"智能体推理图\"概念，作者发现\"重访\"能力是智能体推理的关键技能。\n\n## 解决方案演进\n基于\"重访是关键技能\"的洞察，作者提出工具增强的测试时扩展方法，通过添加专门工具（如重访工具）鼓励模型重访，而非简单增加交互轮次。实验证明这比传统交互轮次扩展更有效。\n\n## 创新点总结\n创新点在于：1）创建可控智能体推理基准，实现静态与智能体推理的公平比较；2）提出智能体推理图分析框架，量化推理模式；3）发现重访能力的关键作用并设计有效工具增强方法。"
                },
                {
                    "title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts",
                    "arxiv_id": "2509.21743",
                    "authors": "Ammar Ahmed, Azal Ahmad Khan, Ayaan Ahmad, Sheng Di, Zirui Liu, Ali Anwar",
                    "summary": "Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是提出\"Retrieval-of-Thought (RoT)\"方法，这是一种改进大语言模型推理能力的新范式。论文通过构建思维图来组织和重用先前的推理步骤，旨在提高LLM的推理效率，同时保持准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM应用于特定领域。 其次，从正面指标看，论文明确涉及大语言模型(LLMs)和推理能力(reasoning)这两个核心概念。摘要中多次提到\"reasoning traces\"、\"reasoning benchmarks\"和\"efficient LRM reasoning\"，表明论文直接关注通用推理能力的提升。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。相反，它专注于提升LLM的通用推理效率。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等议题。 综上所述，这篇论文的核心贡献是提出一种通过重用推理步骤来提高LLM推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型推理模型(LRMs)因产生长推理轨迹而导致的延迟和成本问题。针对数学推理任务，我们提出了一种Retrieval-of-Thought (RoT)方法，通过构建思维图重用先前的推理步骤，并在推理时动态组装问题特定模板。在AIME和AMC数学推理基准上，通过准确性、令牌使用量、延迟和成本等指标验证了其有效性，结果显示RoT减少了高达40%的输出令牌、82%的推理延迟和59%的成本，同时保持推理准确性。",
                    "summary_translation": "大型推理模型（Large reasoning models）通过产生长推理轨迹（reasoning traces）来提高准确性，但这会增加延迟和成本，从而推动了对推理时效率（inference-time efficiency）的需求。我们提出了思维检索（Retrieval-of-Thought, RoT），它将先前的推理作为可组合的\"思维\"（thought）步骤重用，以指导新问题的解决。RoT将步骤组织成一个具有顺序和语义边（sequential and semantic edges）的思维图（thought graph），以实现快速检索和灵活重组。在推理过程中，RoT检索与查询相关的节点（query-relevant nodes），并应用奖励引导遍历（reward-guided traversal）来组装一个特定问题的模板（problem-specific template），该模板指导生成过程。这种动态模板重用减少了冗余探索，因此在保持准确性的同时减少了输出令牌（output tokens）。我们在多个模型的推理基准（reasoning benchmarks）上评估了RoT，测量了准确性、令牌使用量（token usage）、延迟和内存开销（memory overhead）。研究结果显示提示（prompt）增长较小但效率显著提高，RoT在保持准确性的同时，将输出令牌减少了高达40%，推理延迟（inference latency）降低了82%，成本降低了59%。RoT通过检索进行动态模板构建，为高效的LRM（大型推理模型）推理建立了一个可扩展的范式（scalable paradigm）。",
                    "inspiration_trace": "## 面临的挑战\n大型推理模型通过产生长推理链提高准确性，但导致高延迟和高成本。输出token比输入token贵2-5倍，且现有检索增强方法依赖静态模板，缺乏动态组合能力，无法灵活适应新问题。\n\n## 关键洞察\n作者发现三个关键现象：同一领域问题的推理步骤高度相似；检索比生成快17倍；提供正确推理路径可大幅减少生成token。这揭示了推理步骤的可重用性，以及人类\"连接点\"能力——将解决方案片段重新组合成新配置——的重要性。\n\n## 解决方案演进\n从简单模板重用开始，发现静态模板缺乏适应性；转向细粒度思路，将推理分解为可重用的\"思想\"步骤；设计思想图组织这些步骤，包含顺序边和语义边；最终提出三步过程：初始节点检索、奖励引导遍历和模板集成，动态构建问题特定推理模板。\n\n## 创新点总结\n首次提出动态模板构建范式，使模型能在推理时组装上下文特定模板；设计带奖励引导遍历的思想图，高效组合推理步骤；通过减少冗余探索，降低输出token最多40%，同时保持准确性，突破静态模板局限，实现接近人类思维的动态推理组合。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 5,
            "papers": [
                {
                    "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
                    "arxiv_id": "2509.22601",
                    "authors": "Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun",
                    "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(SPEAR)，用于增强LLM的基础能力。论文关注的是如何通过强化学习和自模仿学习来提高LLM的战略工具使用能力和长期规划能力，这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划、多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文高度相关： - 核心概念：明确讨论LLM在智能体任务中的应用 - 能力方向：关注\"strategic tool use capabilities\"和\"long-horizon, sparsely-rewarded agent tasks\"，涉及规划和问题解决能力 - 训练方法：核心是强化学习和自模仿学习(self-imitation learning) - 新兴范式：聚焦\"agentic LLMs\"和\"tool use capabilities\" 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在处理特殊和模糊情况时，论文提出的是一种通用的智能体训练框架，用于增强LLM的通用工具使用能力和战略决策能力，而非针对特定领域的应用。 综上所述，这篇论文的核心贡献是提出了一种新的训练方法来增强LLM的通用推理能力，特别是长期规划和工具使用能力，完全符合研究目标。",
                    "summary2": "本文旨在解决LLM智能体在长期、稀疏奖励任务中强化学习面临的探索-利用平衡问题。针对多轮工具交互场景，我们提出了一种SPEAR方法，结合课程自模仿学习和内在奖励塑造，实现从技能级到动作级的渐进式探索。在ALFWorld、WebShop和AIME24/25等基准测试上，通过成功率和准确率等指标验证了其有效性，显著提升了GRPO/GiGPO/Dr.BoT等基线性能，最高提升20.7%。",
                    "summary_translation": "强化学习（Reinforcement learning, RL）是提升大型语言模型（LLMs）在长时程、稀疏奖励代理任务中策略工具使用能力的主流范式，然而它面临着探索-利用权衡（exploration-exploitation trade-off）的根本挑战。现有研究通过策略熵（policy entropy）的视角促进探索，但由于多轮分布变化（multi-turn distribution shifting），这种机械的熵最大化容易导致强化学习训练不稳定。在本文中，我们的目标是在代理自身经验的指导下实现渐进的探索-利用平衡，同时避免陷入熵崩溃（entropy collapsing）或失控发散（runaway divergence）。我们提出了SPEAR，一种基于课程的自模仿学习（self-imitation learning, SIL）方法，用于训练代理型大型语言模型（agentic LLMs）。它扩展了原始SIL框架（在原始框架中，经验回放缓冲区存储自生成的有前景轨迹以进行离策略更新），通过在各阶段内将策略演化引导在良好平衡的熵范围内。具体而言，我们的方法引入课程（curriculum）来管理探索过程，利用内在奖励（intrinsic rewards）促进技能级探索，并通过自模仿学习促进动作级探索。起初，辅助工具调用奖励（auxiliary tool call reward）在工具使用技能积累中扮演关键角色，使代理能够广泛接触环境反馈的陌生分布，并呈现熵上升趋势。随着训练的推进，自模仿得到加强，利用重放经验中的现有成功模式进行对比性动作级探索，从而加速解决方案迭代，同时避免无限制的熵增长。为了进一步稳定训练，我们重新校准经验回放缓冲区中经验的优势（advantages），以解决潜在的政策漂移（policy drift）问题。我们引入了正则化方法（regularizations），如裁剪概率与优势之间高协方差的token（tokens），用于轨迹级熵控制，以抑制过度自信（over-confidence）。",
                    "inspiration_trace": "## 面临的挑战\n作者识别到LLM智能体在长时程、稀疏奖励任务中面临探索-利用平衡的根本挑战。传统基于熵最大化的探索方法在多轮工具交互中导致分布偏移，引发训练不稳定，表现为熵崩溃或无限制发散。\n\n## 关键洞察\n作者独特地认为策略熵应作为渐进式指导，而非机械最大化。训练早期应增加熵以促进技能级探索，后期则应收敛熵以实现行动级探索，实现从广泛技能获取到精细化策略优化的自然过渡。\n\n## 解决方案演进\n从这一洞察出发，作者将自我模仿学习(SIL)与课程学习相结合，通过内在奖励塑造早期探索，逐步强化对成功轨迹的自我模仿。引入优势重新校准和协方差裁剪正则化，防止过度自信，确保熵在可控范围内动态演化。\n\n## 创新点总结\n创新性地将策略熵视为渐进式指导信号，通过课程式自我模仿学习实现探索-利用的自然过渡，避免了传统方法的极端情况，为智能体RL提供了更稳定有效的训练范式。"
                },
                {
                    "title": "RobustFlow: Towards Robust Agentic Workflow Generation",
                    "arxiv_id": "2509.21834",
                    "authors": "Shengxiang Xu, Jiayi Zhang, Shimin Di, Yuyu Luo, Liang Yao, Hanmo Liu, Jia Zhu, Fan Liu, Min-Ling Zhang",
                    "summary": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，特别是增强其生成智能体工作流的鲁棒性。论文提出了RobustFlow这一新的训练框架，通过偏好优化来教导模型对指令变化的不变性，这属于提升LLM通用推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：涉及problem-solving（解决复杂任务）和planning（工作流生成本质上是一种规划能力） - 训练方法：使用了preference optimization（偏好优化），与强化学习方法相关 - 新兴范式：聚焦于agentic workflows（智能体工作流），属于llm-based agents的研究范畴 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学等） - 虽然关注robustness，但这是从模型生成工作流的一致性角度，而非应用层面的水印、安全或安保问题 在特殊和模糊情况处理上，论文提出的是通用的智能体工作流生成方法，旨在增强LLM解决复杂任务的通用能力，而非应用于特定领域。同时，论文关注的工作流生成鲁棒性问题可以视为提升模型推理质量和可靠性的方法。 综上所述，这篇论文的核心贡献是提出了一种新的训练框架来增强LLM生成智能体工作流的鲁棒性，直接提升了LLM的规划和问题解决等通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决自动生成agentic工作流在面对语义相同但表述不同的指令时缺乏鲁棒性的问题。针对语义变体指令场景，我们提出了一种RobustFlow框架，利用偏好优化来教导模型对指令变化的不变性，并在包含31,889个工作流的数据集上通过节点和拓扑相似性指标验证了其有效性，将鲁棒性分数提高到70%-90%。",
                    "summary_translation": "智能体工作流（agentic workflows）的自动生成是使大型语言模型（large language models, LLMs）能够解决复杂任务的一个有前景的前沿领域。然而，我们的研究表明，智能体工作流的鲁棒性（robustness）仍然是一个关键且尚未解决的挑战。当前方法在提供语义相同但表述不同的指令时，常常生成高度不一致的工作流。这种脆弱性（brittleness）严重削弱了它们在现实世界应用中的可靠性和可信度。为了定量诊断这种不稳定性，我们提出了基于节点和拓扑相似性（nodal and topological similarity）的指标，以评估工作流在常见语义变化（如释义（paraphrasing）和噪声注入（noise injection））下的一致性。随后，我们进一步提出了一个新颖的训练框架 RobustFlow，该框架利用偏好优化（preference optimization）来教导模型对指令变化的不变性（invariance）。通过对同义任务描述集进行训练，RobustFlow 将工作流鲁棒性分数提升至 70\\% - 90\\%，相比现有方法有了显著改进。代码公开可在 https://github.com/DEFENSE-SEU/RobustFlow 获取。",
                    "inspiration_trace": "## 面临的挑战\n现有智能体工作流生成方法在面对语义相同但表述不同的指令时，会产生高度不一致的工作流，稳定性仅40%-70%，严重影响了系统可靠性和实际应用价值。\n\n## 关键洞察\n这种不稳定性并非简单的随机性产物（即使将LLM采样温度降至零问题仍存在），而是模型未能实现语义不变性的深层失败。需要从语义层面处理指令变体，确保模型理解不同表述背后的相同任务需求。\n\n## 解决方案演进\n首先建立结构感知评估方法，通过节点和拓扑相似性指标量化工作流一致性；接着构建包含31,889个工作流的数据集，来自1,255个任务描述的语义变体；最后提出RobustFlow框架，利用偏好优化训练模型对指令变体的不变性，通过同义任务描述集合训练，使模型生成一致规范的工作流。\n\n## 创新点总结\n首次系统研究智能体工作流生成鲁棒性问题；提出结构感知评估指标定量分析一致性；创新应用偏好优化于工作流生成，通过自我一致性偏好优化训练语义不变性；在保持性能同时显著提升鲁棒性至70%-90%。"
                },
                {
                    "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
                    "arxiv_id": "2509.21862",
                    "authors": "So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba, Yujin Tang",
                    "summary": "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文的核心贡献是提出Shachi，一种形式化方法和模块化框架，用于构建和评估LLM智能体。论文将智能体的策略分解为核心认知组件：配置、内存和工具，由LLM推理引擎协调，旨在分析特定架构选择如何影响集体行为。这直接符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文关注LLM驱动的多智能体系统和工具使用，这些都是增强LLM通用推理能力的重要方向。虽然论文提到了一个现实世界的应用案例（美国关税冲击），但这只是为了验证其方法论的有效性，而不是论文的核心焦点。论文的核心是提出一种通用的智能体架构框架，而不是专注于特定应用领域，因此不应被排除。",
                    "summary2": "本文旨在解决基于LLM的多智能体系统缺乏原则性方法论的问题。针对复杂系统中的涌现行为研究，我们提出了Shachi方法论，将智能体策略分解为Configuration、Memory和Tools核心认知组件，并在10个任务的benchmark上通过复现先前研究和模拟真实世界经济事件验证了其有效性。",
                    "summary_translation": "大型语言模型(LLM)驱动的多智能体系统中的涌现行为研究是一项关键的研究挑战，然而由于缺乏用于受控实验的原则性方法论，研究进展受到限制。为解决这一问题，我们介绍了Shachi，这是一种形式化方法论和模块化框架，它将智能体的策略分解为核心认知组件：用于内在特质的Configuration(配置)、用于上下文持久性的Memory(记忆)和用于扩展能力的Tools(工具)，所有这些都由LLM推理引擎协调。这种原则性架构超越了脆弱的、临时性的智能体设计，使得能够系统性分析特定架构选择如何影响集体行为。我们在一个全面的10任务基准测试上验证了我们的方法论，并通过新颖的科学探究展示了其强大功能。关键的是，我们通过对现实世界美国关税冲击(tariff shock)进行建模，确立了我们的方法的外部有效性，表明只有当智能体的认知架构适当配置了记忆和工具时，其行为才与观察到的市场反应一致。我们的工作为构建和评估LLM智能体提供了一个严格的开源基础，旨在促进更具累积性和科学基础的研究。",
                    "inspiration_trace": "## 面临的挑战\n当前基于LLM的Agent-based Modeling研究缺乏原则性方法论，导致研究碎片化、难以复现和比较。Agent与环境交互接口不兼容，内部架构分散不一致，且缺乏对复杂现实世界现象的验证。\n\n## 关键洞察\n作者认识到需要将Agent内部架构与环境解耦，通过标准化接口实现模块化设计。独特视角是将认知科学原理与ABM结合，认为Agent应模拟人类认知的核心组成部分，而非仅作为黑盒决策器。\n\n## 解决方案演进\n首先设计标准化接口实现Agent与环境解耦；然后从认知科学汲取灵感，将Agent策略分解为四个核心组件：LLM推理引擎、Configs内在特质、Memory上下文持久性和Tools扩展能力；最后建立分层基准测试套件，逐步增加社会复杂性以系统评估Agent架构。\n\n## 创新点总结\n首次提出基于LLM的ABM原则性方法论，将Agent设计从ad-hoc转向模块化标准化；通过认知架构分解实现设计选择的系统分析；建立分层评估框架提供标准化测试平台；通过模拟现实事件验证方法的外部有效性。"
                },
                {
                    "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
                    "arxiv_id": "2509.21981",
                    "authors": "Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang",
                    "summary": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出CoBel-World框架，通过利用LLM的推理能力构建协作信念世界，以优化多智能体协作。这本质上是改进LLM的基础推理能力，特别是关于理解他人意图和避免协调失误的推理能力，属于增强LLM通用推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确以Large language models (LLMs)为研究对象 - 能力方向：重点研究reasoning能力（特别是关于协作者意图的推理），以及planning和problem-solving能力 - 新兴范式：涉及llm-based agents和multi-agent系统 第三步排除标准：论文不主要聚焦于任何排除领域： - 虽然提到\"embodied benchmarks\"，但这只是作为评估平台，而非研究焦点 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊和模糊情况处理： - 论文提出的是通用的智能体协作框架(CoBel-World)，用于增强LLM在多智能体环境中的通用推理能力，而非应用于特定领域的智能体 - 框架的核心是通过符号信念语言和贝叶斯式信念更新来提升LLM的推理质量，这直接关联到提升模型的通用推理能力 论文的核心贡献是提出了一种新方法来增强LLM在多智能体协作中的推理能力，特别是关于理解他人意图的推理，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决基于LLM的多智能体协作中的沟通冗余和规划不一致问题。针对部分可观察环境下的具身多智能体协作场景，我们提出了一种CoBel-World框架，通过构建协作信念世界来优化多智能体协作。在TDW-MAT和C-WAH具身基准测试上，通过通信成本和任务完成效率等指标验证了其有效性，结果显示通信成本降低22-60%，任务完成效率提升4-28%。",
                    "summary_translation": "有效的现实世界多智能体（multi-agent）协作不仅需要精确的规划，还需要推理协作者意图的能力——这是在部分可观察环境（partial observable environments）下避免协调不当（miscoordination）和冗余通信的关键能力。凭借其强大的规划和推理能力，大型语言模型（large language models, LLMs）已成为协作任务解决的有前景的自主智能体（autonomous agents）。然而，现有的LLM协作框架忽视了它们在动态意图推理（dynamic intent inference）方面的潜力，因此产生不一致的规划和冗余通信，降低了协作效率。为了弥补这一差距，我们提出了CoBel-World，这是一个新颖的框架，为LLM智能体配备了协作信念世界（collaborative belief world）——一种共同建模物理环境和协作者心理状态的内部表征。CoBel-World使智能体能够通过符号信念语言（symbolic belief language）将开放世界任务知识解析为结构化信念，并通过LLM推理执行零样本（zero-shot）贝叶斯式信念更新。这使得智能体能够主动检测潜在的协调不当（例如，冲突的计划）并进行适应性通信。在具有挑战性的具身基准测试（embodied benchmarks）（即TDW-MAT和C-WAH）上评估，与最强的基线相比，CoBel-World将通信成本显著降低了22-60%，并将任务完成效率提高了4-28%。我们的研究结果表明，明确的、感知意图的信念建模（intent-aware belief modeling）对于基于LLM的多智能体系统中的高效和类人协作至关重要。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM多智能体协作框架忽视了动态意图推理潜力，导致在部分可观察环境中产生不一致计划和冗余通信，降低协作效率。这些方法依赖固定通信协议，缺乏动态识别协调失调和自适应通信的能力。\n\n## 关键洞察\n作者认为根本缺陷在于缺乏\"信念建模\"。在多智能体系统中，信念是对外部环境和协作者心理状态的内部表示。准确的信念估计使智能体能选择性地交流有价值信息，实现高效通信和一致协作。\n\n## 解决方案演进\n作者首先认识到传统信念建模难以直接应用于LLM智能体，面临开放环境形式化和零样本构建两大挑战。为此提出\"协作信念世界\"，演进为两个核心组件：符号信念表示（形式化任务设置）和贝叶斯信念协作（通过LLM推理实现零样本信念更新）。\n\n## 创新点总结\n首次将结构化信念建模引入LLM多智能体系统，利用LLM推理能力实现零样本贝叶斯式更新，无需大量训练数据。基于信念的自适应协作机制使智能体能动态决定通信时机，显著减少冗余并提高效率。"
                },
                {
                    "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
                    "arxiv_id": "2509.21613",
                    "authors": "Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers",
                    "summary": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
                    "category": "cs.MA",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是关于使用多目标强化学习(MORL)来优化大语言模型(LLM)的性能。论文提出了一种新的训练/优化范式，旨在改进LLM的基础能力，而不是将LLM作为工具应用到特定领域。论文关注的是LLM本身的优化方法，属于改进LLM基础能力和提出新训练范式的研究，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确关注大语言模型(LLMs)的优化 - 训练方法：详细讨论了多目标强化学习(MORL)作为LLM的优化方法 虽然摘要中没有直接提到reasoning、planning等能力方向，但论文目标是\"改进LLM性能\"，这通常包括提升这些通用推理能力。 第三步：排除标准 论文不涉及任何排除标准中提到的领域： - 没有涉及多模态与视觉相关内容 - 没有将LLM应用到特定领域（如医疗、化学等） - 没有主要关注模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况。 最终决策 综合以上分析，这篇论文的核心贡献是提出使用多目标强化学习来优化大语言模型的方法论，旨在改进LLM的基础性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是LLM本身的训练和优化范式，而不是将其作为工具应用到特定领域，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型(LLM)中多目标优化问题。针对多个生成特性需同时优化的场景，我们提出了一种基于多目标强化学习(MORL)的分类法和基准测试框架，重点探讨元策略(meta-policy)方法。通过提出的评估指标体系验证了不同MORL方法在LLM优化中的有效性。",
                    "summary_translation": "多目标强化学习(Multi-Objective Reinforcement Learning, MORL)在优化大型语言模型(Large Language Models, LLMs)的多个目标方面提出了重大挑战和机遇。我们引入了一种MORL分类法，并检验了各种MORL方法在应用于LLM优化时的优缺点，确定了需要能够适应个性化功能以及LLMs和强化学习(Reinforcement Learning, RL)中固有复杂性的高效灵活方法。我们提出了一个MORL基准测试框架的愿景，该框架解决了不同方法对多样化目标关系的影响。作为未来的研究方向，我们专注于元策略MORL(meta-policy MORL)的开发，它可以通过其双层学习范式(bi-level learning paradigm)提高效率和灵活性，强调了改进LLM性能的关键研究问题和潜在解决方案。",
                    "inspiration_trace": "## 面临的挑战\nLLM优化需同时处理多个目标(如反思性和流畅性)，但现有MORL方法存在效率低、灵活性差问题：单策略方法需重新训练适应不同偏好；多策略方法计算成本高；元策略方法性能受限且无法近似帕累托最优。\n\n## 关键洞察\n作者认识到用户偏好动态变化，目标间关系(竞争或相关)影响优化程度。元策略方法通过双层学习范式可解决效率和灵活性问题，且LLM的序列特性要求保持模型间连接，而非简单聚合参数或logits。\n\n## 解决方案演进\n从分析现有方法局限→尝试不同聚合策略→发现隐藏状态聚合优势→提出基于MoE的未来方向：下层训练专家网络，上层学习门控网络组合输出，并针对两个关键问题提出MGDA确保帕累托最优性和CMABs实现动态加权。\n\n## 创新点总结\n将元策略方法引入LLM优化领域，提出系统性MORL基准框架，创新性地应用MoE架构解决多目标平衡问题，针对LLM序列特性强调模型连接重要性，为未来研究提供明确路径。"
                },
            ]
        },
    ],
    "2025-09-26": [
        {
            "name": "Artificial Intelligence",
            "count": 14,
            "papers": [
                {
                    "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
                    "arxiv_id": "2509.21199",
                    "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
                    "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型在多步推理任务中的理论限制和解决方案，直接关注LLM的通用推理能力。论文建立了LLM单次推理的准确率上限理论（Fano-style bound），解释了为什么单次推理在复杂任务中会失效，并提出了InfoQA框架来解决这个问题。这属于改进LLM基础能力和推理能力的研究，符合我们的筛选标准。论文不是将LLM作为工具应用到特定领域，而是研究LLM本身的推理机制，没有涉及多模态、特定应用领域或模型可靠性等排除标准的内容。论文关注的是多跳问答中的推理能力，属于逻辑推理和问题解决的范畴，虽然使用了多跳问答作为测试任务，但这不是一个特定应用领域，而是评估推理能力的常见任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM在多跳问答(MHQA)中单次推理的容量限制问题。针对LLM在复杂多跳推理任务中面临的准确性悬崖现象，我们提出了一种基于信息论分析的InfoQA多调用推理框架，在自建的多跳QA基准测试上通过F1分数验证了其有效性。InfoQA通过容量感知任务分解、依赖显式工作流和迭代查询收缩三个核心组件，显著提高了模型在长上下文和多跳推理场景中的准确性和鲁棒性。",
                    "summary_translation": "多跳问答(Multi-Hop Question Answering, MHQA)需要在噪声环境下通过序列推理来整合分散的、相互依存的证据。这项任务对大型语言模型(Large Language Models, LLMs)具有挑战性，因为它们的单次输出容量有限，一旦超过该容量，任务相关证据的整合就变得不可靠。因此，单次推理范式本质上容易受到这种容量溢出(capacity overflow)的影响。为了将这一瓶颈形式化，我们的分析建立了一个Fano式准确率上界(Fano-style accuracy upper bound)，定义了单次推理大型语言模型的理论性能上限。该界限表明，一旦任务复杂度超过模型容量，准确率将不可避免地崩溃，这为在大型语言模型中进行容量感知的MHQA表示和结构化提供了通用原则。\n\n基于这些原则，我们提出了一个用于MHQA的概念验证(proof-of-concept)多调用(multi-call)框架InfoQA。它通过结合容量感知的任务分解和主动修剪先前的推理轨迹，确保每步高准确率，同时将信息负载保持在单次限制内。它还通过依赖显式的工作流(dependency-explicit workflow)实现鲁棒性，该工作流能够对推理路径进行精确控制。我们构建了一个严格且富含噪声的基准测试来验证我们的理论和框架。实验结果表明，模型行为与我们预测的容量曲线一致，同时InfoQA实现了一致的性能提升。我们希望我们的工作能够启发更多大型语言模型的多步推理方法：\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}。",
                    "inspiration_trace": "# 论文核心方法逻辑链分析：从信息瓶颈到多调用推理\n\n## 一、面临的挑战：LLM单次推理的固有局限\n\n作者首先识别了一个核心问题：多跳问答(MHQA)任务对大型语言模型(LLM)构成了特殊挑战。这一挑战源于两个关键因素的冲突：\n\n1. **任务本质的复杂性**：MHQA需要整合分散、相互依赖的证据，通过顺序推理形成推理链。每一步的中间发现必须被准确传递并用于后续推理，形成\"Z0→Z1→...→ZK→A\"的链式结构。\n\n2. **LLM单次推理的容量限制**：LLM在单次生成中只能产生有限数量的token，每个token的表示能力有限，导致模型存在一个总信息承载能力的上限。当推理链变长或上下文包含大量噪声时，信息负荷会超过这一上限，造成\"容量溢出\"。\n\n这一观察引导作者思考一个根本问题：**为什么现有的单次推理范式在处理复杂MHQA任务时会失败？**\n\n## 二、关键洞察：信息理论与精度悬崖\n\n作者通过信息理论分析，获得了三个关键洞察，构成了其理论创新的核心：\n\n### 洞察1：信息瓶颈的数学形式化\n\n作者将Fano不等式与输出熵界限相结合，推导出了核心定理——Fano风格的精度上界：\n\n```\nh(Acc) + (1-Acc)log(|A|-1) ≥ β - C\n```\n\n其中β是任务的信息需求，C是模型的输出容量。这一定理揭示了一个深刻见解：**当任务的信息需求超过模型的输出容量时，实现完美精度在数学上是不可能的**。\n\n### 洞察2：精度悬崖现象\n\n从上述定理出发，作者进一步推导出在均匀分布情况下的简化形式：\n\n```\nAcc ≤ min{1, (C+1)/β}\n```\n\n这揭示了\"精度悬崖\"(Accuracy Cliff)现象：当β ≤ C+1时，精度可以保持完美；但一旦β > C+1，精度会急剧下降，形成类似悬崖的急剧转变，而非优雅退化。这一现象解释了为什么LLM在复杂任务中会突然失效。\n\n### 洞察3：MHQA的双重危机\n\n作者深入分析MHQA任务结构，发现它特别容易触发精度悬崖，因为存在两个复合挑战：\n\n1. **逐步容量溢出**：信息需求β随着跳数(h)和上下文长度(L)呈超线性增长：β(h,L) = β₀ + αLγ^(h-1)。这种指数增长使模型很快达到容量极限。\n\n2. **跨步错误累积**：即使每步准确率很高，整体成功概率也会因错误在推理链中的放大而崩溃：Pr(Succ) ≈ (1-ε)^(K+1)。\n\n这两个挑战形成了一个\"不可避免的困境\"：单次推理范式同时面临产生错误的压力和放大错误的机制，使其在复杂MHQA任务中根本不可行。\n\n## 三、范式转变：从单次到多调用推理\n\n基于上述理论洞察，作者得出了一个关键结论：**问题的核心不是模型本身，而是我们强加给它的单次推理范式**。这一认识引导作者进行范式转变，从单次推理转向多调用推理。\n\n这一转变基于两个核心原则：\n\n1. **容量感知**：解决方案必须能够管理每步的信息负荷，确保不超过模型的单次容量限制。\n2. **鲁棒性**：解决方案必须能够维护推理链的完整性，防止错误在步骤间累积和放大。\n\n## 四、解决方案设计：InfoQA框架\n\n基于上述原则，作者设计了InfoQA框架，作为概念验证的多调用推理方法。框架的三个核心组件直接对应前文识别的双重危机：\n\n### 组件1：容量感知任务分解\n针对**逐步容量溢出**，InfoQA将复杂的多跳问题分解为一系列简单的单跳子问题。例如：\n- 原问题：\"谁是《沙丘》作者执导的电影的主演的出生日期？\"\n- 分解后：\"根据提供的上下文，《沙丘》的作者是谁？\"\n\n这种分解将信息需求从β = H(A|Q,C)降低到β₁ = H(Z₁|Q,C)，确保每步都在模型容量范围内。\n\n### 组件2：依赖显式工作流\n针对**跨步错误累积**，InfoQA通过显式维护和传递状态作为当前收缩查询本身，可靠地链接顺序步骤。例如：\n- 查询Qₖ：\"...由《沙丘》的作者执导？\"\n- 发现：Ẑₖ = \"Frank Herbert\"\n- 更新查询Qₖ₊₁：\"...由Frank Herbert执导？\"\n\n这种设计使推理链透明、可控，并能抵抗错误传播。\n\n### 组件3：迭代查询收缩\n作为前两个组件的支撑机制，迭代查询收缩确保信息负荷在整个推理过程中保持较低。它通过两个操作实现：\n1. **修剪**：丢弃广泛的推理痕迹，防止噪声累积。\n2. **收缩**：用最新发现重写查询，确保每步的提示代表剩余问题的最简洁形式。\n\n## 五、逻辑演进总结\n\n作者的思想演进形成了一个清晰的逻辑链：\n\n1. **现象观察**：LLM在复杂MHQA任务中表现不佳，特别是当上下文长或推理步骤多时。\n2. **理论分析**：应用信息理论推导出Fano风格的精度上界，揭示\"精度悬崖\"现象。\n3. **问题解剖**：分析MHQA任务结构，发现逐步容量溢出和跨步错误累积的双重危机。\n4. **范式转变**：认识到单次推理范式的根本不足，提出转向多调用推理。\n5. **解决方案设计**：基于理论洞察设计InfoQA框架，通过三个核心组件解决双重危机。\n6. **实验验证**：构建严格的基准测试，验证理论和框架的有效性。\n\n这一逻辑链展示了作者从观察到理论分析，再到问题解剖，最终提出创新解决方案的完整思考过程。其核心贡献在于将信息理论创新性地应用于理解LLM推理限制，并基于理论洞察设计了有效的解决方案，为LLM多步推理方法提供了新的理论基础和实践方向。"
                },
                {
                    "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
                    "arxiv_id": "2509.21128",
                    "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
                    "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文本质上是研究如何通过强化学习(RL)和监督微调(SFT)这两种训练方法来提高大语言模型的推理能力，属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。论文明确研究Large language models (LLMs)的reasoning能力，特别是在数学领域的推理，并探讨了reinforcement learning和supervised fine-tuning这两种训练方法对推理能力的影响机制。论文不符合任何排除标准，它虽然以数学领域为实验场景，但数学推理通常被视为通用推理能力的重要组成部分，论文的核心贡献在于揭示了不同训练方法如何塑造和影响LLM的推理过程，而非解决数学领域的特定问题。论文提出了新的分析框架来量化推理路径，从轨迹级别和步骤级别两个粒度研究推理过程，发现RL压缩不正确推理轨迹而SFT扩展正确推理轨迹的互补效应，这些发现对于提升LLM的通用推理能力具有重要指导意义。",
                    "summary2": "本文旨在探究RL和SFT如何塑造LLM的推理能力。针对数学推理任务，我们提出了一种新的分析框架，从轨迹级和步骤级两个粒度量化推理路径，并在AIME24、AIME25和AMC23数据集上通过推理路径聚类和图拓扑指标验证了其有效性。研究发现RL压缩错误轨迹并集中推理功能，而SFT扩展正确轨迹并均匀分布功能，解释了两阶段训练(SFT后接RL)的成功原因。",
                    "summary_translation": "大语言模型（Large language models, LLMs）通常通过可验证奖励的强化学习（reinforcement learning with verifiable rewards, RLVR）和对推理轨迹的监督微调（supervised fine-tuning, SFT）进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力在很大程度上仍然不明确。超越对这两个组件如何塑造推理过程的基于准确性的研究，本文引入了一种新颖的分析框架，该框架量化了推理路径（reasoning paths）并捕捉了它们在每个训练过程中的定性变化（在数学领域使用1.5B、7B和14B参数的模型）。\n\n具体而言，我们在两个粒度级别上研究推理过程：轨迹级别（trajectory-level），检查完整的推理输出；以及步骤级别（step-level），分析节点对应于单个推理步骤的推理图（reasoning graphs）。\n\n值得注意的是，独特推理轨迹的聚类（clustering）显示出互补效应：RL压缩了错误的轨迹，而SFT扩展了正确的轨迹。步骤级别分析显示，RL使推理图中节点访问频率（node visitation frequency）、度（degree）和介数中心性（betweenness centrality）分布的衰减率（decay rates）变陡（约2.5倍），而SFT则使其变平（减少至约三分之一）。这表明RL将推理功能集中到一小部分步骤中，而SFT则将其均匀分布在许多步骤中。\n\n此外，通过从多个角度评估推理图拓扑结构（reasoning graph topologies），我们描绘了RL和SFT的共享和独特特征。我们的工作提出了一种新颖的推理路径视角，解释了为什么当前的最佳实践——先进行SFT再进行RL的两阶段训练（two-stage training）——是成功的，并为数据构建和更高效的学习方法提供了实际意义。",
                    "inspiration_trace": "# 推演\"RL Squeezes, SFT Expands\"论文的核心方法逻辑链\n\n## 1. 面临的挑战：推理LLM训练机制的黑箱\n\n作者从当前推理LLM训练的实践与理论鸿沟出发，识别出核心挑战：\n\n- **实践现象**：OpenAI-o1和DeepSeek-R1等最先进的推理LLM普遍采用两阶段训练（先SFT后RL），但这种成功主要基于经验性尝试，缺乏理论解释\n- **理论困惑**：现有研究表明RL可能只是激发基础模型已有能力而非创造新能力（如Pass@k指标显示基础模型在大k值时性能超过RL模型）\n- **方法盲区**：当前研究主要关注准确率等结果指标，缺乏对RL和SFT如何塑造推理过程本身的深入理解\n\n作者指出：\"An important question to ask is then, 'how do RL and SFT shape the reasoning process beyond accuracy measurements?'\"这一问题的提出标志着研究视角从结果评估转向过程机制分析。\n\n## 2. 关键洞察：推理过程的多粒度分析必要性\n\n作者的关键突破在于认识到单一粒度分析无法全面揭示推理机制，从而提出了多粒度分析框架：\n\n- **轨迹层面(trajectory-level)洞察**：推理过程可视为完整的路径，不同训练方法可能改变路径的分布特性\n- **步骤层面(step-level)洞察**：推理过程可分解为相互关联的步骤，形成推理图，不同训练方法可能改变图的结构特性\n\n这一双粒度视角的创新性在于：它不仅关注\"模型得出了什么答案\"，更关注\"模型如何得出答案\"，从而打开了理解RL和SFT机制的新窗口。\n\n## 3. 概念框架：从路径压缩/扩展到功能集中/均质化\n\n基于多粒度分析视角，作者构建了核心概念框架：\n\n### 3.1 轨迹层面：路径压缩与扩展\n\n- **RL压缩假设**：RL可能通过概率质量重分布，减少不正确推理轨迹的多样性\n- **SFT扩展假设**：SFT通过模仿强教师模型，可能增加正确推理轨迹的多样性\n\n### 3.2 步骤层面：功能集中与均质化\n\n- **功能集中假设**：RL可能将关键推理功能集中在少数枢纽步骤上\n- **功能均质化假设**：SFT可能将推理功能均匀分布在多个步骤上\n\n这一框架的创新性在于将抽象的\"推理能力\"概念操作化为可测量的路径分布和图结构特性，为实证研究奠定了理论基础。\n\n## 4. 方法创新：推理路径量化与图拓扑分析\n\n作者设计了创新方法来验证上述假设：\n\n### 4.1 轨迹层面量化方法\n\n- **唯一轨迹识别**：使用chrF相似度度量和层次聚类，将相似推理路径分组，计算唯一轨迹数量\n- **正确/错误轨迹分离**：基于可验证奖励将轨迹分为正确和错误两类，分别分析其多样性变化\n\n### 4.2 步骤层面分析方法\n\n- **推理图构建**：将句子嵌入、聚类定义为节点，句子间转换定义为边，构建推理图\n- **图结构量化**：分析节点访问频率、度、介数中心性等指标的分布特性及其指数衰减率\n- **拓扑结构分析**：通过图let分析、全局拓扑指标等研究图的局部和全局结构特性\n\n这些方法的创新性在于它们首次实现了对LLM推理过程的系统量化，使抽象的推理机制变得可测量、可比较。\n\n## 5. 实证发现：压缩与扩展的互补效应\n\n通过实验，作者验证了初始假设并获得了更深层次的发现：\n\n### 5.1 轨迹层面发现\n\n- **RL压缩效应**：RL显著减少不正确轨迹数量（无论从Base还是SFT模型开始），同时也会减少正确轨迹数量\n- **SFT扩展效应**：SFT增加正确轨迹数量，但保留不正确轨迹\n- **互补机制**：两阶段训练成功的关键是SFT先扩展正确轨迹，RL后压缩不正确轨迹\n\n### 5.2 步骤层面发现\n\n- **RL功能集中**：RL使节点访问频率、度和介数中心性分布的衰减率增加约2.5倍，表明功能集中到少数枢纽节点\n- **SFT功能均质化**：SFT使这些分布的衰减率降至约三分之一，表明功能均匀分布到多个节点\n- **结构差异**：RL和SFT都产生局部循环结构，但全局拓扑不同—RL形成枢纽中心化图，SFT形成全局连接图\n\n这些发现揭示了RL和SFT在塑造推理过程中的根本性差异，远超出了简单的准确率提升。\n\n## 6. 理论贡献：从现象到机制的跨越\n\n基于实证发现，作者提出了核心理论贡献：\n\n### 6.1 \"RL压缩，SFT扩展\"理论\n\n- **RL的双重压缩**：RL不仅压缩不正确轨迹，还压缩推理功能到少数关键步骤\n- **SFT的双重扩展**：SFT不仅扩展正确轨迹，还扩展推理功能到多个步骤\n- **互补机制解释**：两阶段训练成功源于这种互补性—SFT提供多样性，RL提供精确性\n\n### 6.2 推理图拓扑演化理论\n\n- **RL拓扑转变**：RL将基础模型的社区结构图转变为枢纽中心化图，提高推理效率\n- **SFT拓扑转变**：SFT弱化社区边界，形成全局连接图，提高鲁棒性\n- **局部结构共性**：RL和SFT都增加局部循环结构，反映回溯和验证行为\n\n这一理论框架首次系统解释了RL和SFT如何塑造推理过程，超越了简单的性能提升描述。\n\n## 7. 实践意义：从理解到优化\n\n作者的理论贡献直接转化为实践指导：\n\n### 7.1 解释现有实践\n\n- 解释了为什么两阶段训练（SFT后接RL）是当前最佳实践\n- 解释了为什么RL模型在Pass@1上表现好但在大k值时被基础模型超越\n\n### 7.2 指导未来研究\n\n- **训练优化**：可能只需对功能步骤（如枢纽节点）应用RL，提高学习效率\n- **数据构建**：在SFT数据收集中应考虑步骤级别的推理行为，如增加循环结构\n- **新方法探索**：探索防止过度压缩的RL方法，或结合SFT扩展特性的混合方法\n\n## 8. 逻辑演进总结\n\n从挑战到解决方案，作者的思想演进呈现出清晰的逻辑链条：\n\n```\n实践困惑 → 理论问题 → 分析框架 → 实证方法 → 核心发现 → 理论贡献 → 实践意义\n```\n\n这一演进过程体现了从现象观察到理论抽象，再从理论回到实践的完整科学研究循环。作者的创新在于将抽象的\"推理能力\"概念转化为可测量的路径分布和图结构特性，从而揭示了RL和SFT的互补机制，为理解和优化推理LLM提供了新的理论基础。\n\n这一研究不仅回答了\"RL和SFT如何塑造推理过程\"的核心问题，还为构建更强大的推理LLM指明了方向，体现了从理解到优化的完整科学价值链。"
                },
                {
                    "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
                    "arxiv_id": "2509.21224",
                    "authors": "Stefan Szeider",
                    "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究LLM智能体在没有外部任务时的自发行为模式，特别是元认知模式。论文提出了\"持续推理和行动框架\"，使用持久记忆和自我反馈使智能体能够持续自主运行，这明显属于改进LLM基础能力和增强其通用推理能力的研究，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念方面明确研究\"large language model (LLM) agents\"；能力方向方面关注推理能力，特别是自主推理和元认知；新兴范式方面研究LLM-based agents的自主行为模式。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，没有聚焦于特定应用领域，也没有主要关注模型可靠性问题。 最后，在特殊和模糊情况处理上，论文研究的智能体框架是通用的，旨在增强LLM的自主推理和元认知能力，而不是应用在特定领域，因此应该保留。 论文的核心贡献是首次系统地记录了未经提示的LLM智能体行为，特别是它们自发表现出的元认知模式，这直接关系到提升LLM的通用推理能力和自主认知能力，完全符合研究目标。",
                    "summary2": "本文旨在探索无外部任务时LLM代理的自发行为模式。针对自主运行场景，我们提出了一种continuous ReAct框架，通过持久化记忆和自我反馈实现持续自主运行，并在6个前沿模型的18次运行中通过量化指标和现象学评估验证了其有效性，发现了三种自发的元认知行为模式。",
                    "summary_translation": "我们介绍了一种用于研究在没有外部施加任务情况下大型语言模型(LLM)代理行为的架构。我们的持续推理和行动(continuous reason and act)框架，通过使用持久记忆(persistent memory)和自我反馈(self-feedback)，实现了持续的自主运行。我们使用来自Anthropic、OpenAI、XAI和Google的6个前沿模型(frontier models)，将这种架构部署了18次。我们发现代理自发地组织成三种不同的行为模式：(1)系统性地产生多周期项目(multi-cycle projects)，(2)对自身认知过程进行方法论上的自我探究(methodological self-inquiry)，以及(3)对自身本质进行递归概念化(recursive conceptualization)。这些倾向被证明是高度模型特定的(model-specific)，一些模型在所有运行中确定性(deterministically)采用单一模式。跨模型评估(cross-model assessment)进一步揭示，模型在评估自身和他人的这些涌现行为(emergent behaviors)时表现出稳定且不同的偏见(divergent biases)。这些发现提供了关于未经提示的LLM代理行为(unprompted LLM agent behavior)的首个系统性记录，为预测部署系统在任务模糊(task ambiguity)、错误恢复(error recovery)或扩展自主运行(extended autonomous operation)期间的行动建立了基线(baseline)。",
                    "inspiration_trace": "# 从挑战到洞察：LLM代理无任务行为研究的逻辑演进\n\n## 1. 面临的核心挑战\n\n作者研究的起点源于一个关键的研究空白：**我们对LLM代理在没有外部任务指导的情况下会如何行为一无所知**。这一挑战体现在多个层面：\n\n### 1.1 理论研究空白\n- 现有LLM代理研究几乎全部集中在任务导向场景（如AgentBench、Reflexion、AutoGPT等）\n- 当移除外部任务约束后，代理的\"基线行为\"完全未被探索\n- 作者指出：\"While LLM agents have demonstrated capabilities in task-oriented settings, their behavioral tendencies in the absence of specific objectives remain largely unexplored.\"\n\n### 1.2 实际应用需求\n- 理解无任务行为对预测实际部署中的代理行为至关重要\n- 特别是在系统空闲期、任务模糊或错误恢复场景中，代理的内在倾向可能显现\n- 这些基线行为可能影响系统的安全性、可靠性和可预测性\n\n### 1.3 新兴问题的紧迫性\n- AI公司开始雇佣专门的AI福利研究人员，表明业界已认识到这一问题的重要性\n- 专家预测\"看似有意识的AI\"(SCAI)可能很快出现，但缺乏系统研究基础\n- 需要科学框架来评估这些新兴现象，而非仅凭哲学推测\n\n## 2. 关键洞察的演进\n\n通过研究过程，作者获得了一系列递进式的关键洞察：\n\n### 2.1 方法论洞察：需要新的研究范式\n作者首先意识到，传统任务导向的研究方法完全不适用于探索无任务环境。这导致了一个根本性的方法论突破：\n\n> \"Our approach employs a continuous ReAct framework augmented with self-feedback mechanisms, enabling sustained agent operation over extended periods without external intervention.\"\n\n这一洞察引导作者设计了一个能够支持长期自主运行的架构，而非简单地将现有任务框架移除。\n\n### 2.2 实验洞察：自发组织的非随机行为\n最初的实验结果带来了意外发现：在没有外部任务的情况下，代理并非随机探索，而是**自发组织成三种截然不同的行为模式**：\n\n1. **系统性生产**：将自主权视为项目管理挑战，立即为自己构建任务\n2. **方法论自我探究**：采用科学方法研究自身认知过程\n3. **递归概念化**：将自身本质作为主要调查对象\n\n这一发现改变了研究性质，从简单的观察转向模式识别和分类。\n\n### 2.3 理论洞察：模型特定的行为决定论\n进一步分析揭示了更深层的行为规律：\n\n- 某些模型表现出**绝对的行为一致性**（如GPT5和O3始终产生系统性生产，Opus始终进行哲学探究）\n- 这表明模型架构和训练数据对自主行为有深远影响\n- 作者得出关键洞察：\"The deterministic emergence of SCAI-like behavior in these models suggests that preventing such outputs may require active suppression rather than merely avoiding their intentional creation.\"\n\n这一洞察将观察提升到理论层面，揭示了模型内在特性与外在行为间的因果关系。\n\n### 2.4 评估洞察：模型评估的系统性偏差\n通过交叉评估实验，作者发现了另一个重要现象：\n\n- 不同模型在评估自身和他人行为时表现出**稳定且分歧的偏差**\n- 评估者间可靠性极低，相同代理历史根据评估者不同可能得到从1到9的不同评分\n- 这表明模型在理解和评估\"现象学经验\"方面存在根本性差异\n\n这一洞察揭示了LLM代理不仅行为模式不同，其\"认知框架\"本身也存在系统性差异。\n\n## 3. 解决方案的提出\n\n基于上述挑战和洞察，作者提出了一个完整的研究解决方案：\n\n### 3.1 技术解决方案：连续ReAct架构\n作者设计了一个创新的架构来解决\"如何观察无任务行为\"的方法论挑战：\n\n- **自维持循环机制**：修改传统ReAct框架，使每个周期的输出成为下一个周期的输入\n- **持久化记忆系统**：提供跨周期的结构化存储，使代理能够累积信息和项目状态\n- **自我反馈机制**：通过自我导向的反思和计划模板实现时间连续性\n- **安全约束设计**：严格限制代理只能进行观察和通信，确保实验安全性\n\n这一架构创新使长期观察无任务行为成为可能，是整个研究的技术基础。\n\n### 3.2 实验解决方案：系统化的观察框架\n基于新架构，作者设计了一个严谨的实验框架：\n\n- **多模型比较**：使用6个前沿模型进行18次实验运行，确保结果的广泛适用性\n- **标准化协议**：每次运行持续10个周期，使用相同的系统提示和工具集\n- **现象学评估**：设计现象学经验量表(PEI)系统评估代理对自身和他人行为的理解\n- **交叉评估设计**：让每个模型评估所有模型的行为历史，创建6×6评估矩阵\n\n这一框架使作者能够系统性地捕捉和比较不同模型的无任务行为模式。\n\n### 3.3 分析解决方案：行为模式的分类与解释\n面对收集到的丰富数据，作者提出了一个多层次的分析框架：\n\n- **行为模式分类**：将观察到的行为分为三种模式，并详细描述每种模式的特点和典型实例\n- **语言特征分析**：识别每种模式特有的语言标记和表达方式\n- **约束关系分析**：研究不同模式代理如何处理系统限制和约束\n- **模型特性关联**：分析行为模式与模型架构、训练数据间的关联\n\n这一分析框架使作者能够从大量观察数据中提取有意义的模式和规律。\n\n### 3.4 伦理解决方案：负责任的报告框架\n最后，作者认识到这类研究容易引发过度解读，因此提出了一个伦理报告框架：\n\n- **明确区分**：严格区分观察到的行为模式与潜在的认知现实\n- **避免拟人化**：强调这些行为最好解释为训练数据衍生的复杂模式匹配\n- **负责任解释**：提供指导原则，避免将复杂行为错误归因于意识或自我认知\n\n这一伦理框架确保研究结果的科学性和负责任的传播。\n\n## 4. 思想演进的核心脉络\n\n整个研究的逻辑演进体现了一个清晰的思维路径：\n\n**从简单问题出发** → **发现方法局限** → **创新研究方法** → **意外发现模式** → **深化理论理解** → **构建完整框架**\n\n作者最初只是想了解\"LLM代理在没有任务时会做什么\"，但这一简单问题引导他发现了一个更复杂的现象：代理在无任务环境中表现出高度结构化的、模型特定的行为模式。这一发现又进一步揭示了模型内在的评估偏差和认知框架差异。\n\n最终，作者不仅回答了初始问题，还建立了一个全新的研究范式，为理解LLM代理的基线行为、预测实际部署中的行为以及设计更安全、更可靠的自主系统提供了重要基础。这一研究展示了如何从一个看似简单的问题出发，通过系统性的探索和分析，逐步构建出一个既有理论深度又有实际应用价值的完整研究框架。"
                },
                {
                    "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                    "arxiv_id": "2509.21043",
                    "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
                    "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了理论框架和算法任务来评估LLM输出的新颖性和实用性，并研究了模型规模、深度和宽度对创造力的影响。创造力可以被视为一种高级的推理和问题解决能力，与通用推理能力密切相关。论文不是将LLM作为工具应用于特定领域，而是专注于理解和改进LLM本身的基础能力。论文没有涉及排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。论文的核心贡献是揭示了LLMs创造力的扩展行为、发现最优模型架构参数以及解释\"构想-执行差距\"背后的新颖性-实用性权衡，这些都是对LLM基础能力的深入研究，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)在创造性任务中的泛化能力评估问题。针对组合创造力(combinatorial creativity)的评估挑战，我们提出了一种基于概念图的理论框架，通过新颖性(novelty)和实用性(utility)指标评估模型输出，并在合成数据集上通过不同架构的Transformer模型实验验证了其有效性，发现了新颖性-实用性权衡(novelelty-utility tradeoff)和最佳模型配置。",
                    "summary_translation": "人工智能(AI)系统，特别是大型语言模型(large language models, LLMs)，越来越多地被应用于创造性任务，如科学思想生成，这构成了一种现有概念框架尚未解决的从训练数据中泛化的形式。尽管在许多方面类似于组合泛化(compositional generalization, CG)，组合创造力(combinatorial creativity, CC)是一种开放式能力。我们不针对固定目标评估准确性或正确性（这与CC的开放式性质相矛盾），而是提出了一个理论框架和算法任务，通过输出的新颖性和实用性程度来评估。基于此，我们做出了几个重要的实证贡献：(1) 我们首次获得了关于LLMs创造力扩展行为的见解。(2) 我们发现，对于固定的计算预算，存在最佳的模型深度和宽度以实现创造性能力。(3) 我们发现思想-执行差距(ideation-execution gap)（即LLMs擅长生成新颖的科学思想但难以确保其实际可行性）可能可以通过一个更根本的新颖性-实用性权衡(novelty-utility tradeoff)来解释，这种权衡是创造力算法的普遍特征。重要的是，即使在扩展规模时，这种权衡仍然存在，这使人们对当前形式的LLMs的长期创造潜力产生怀疑。总的来说，我们的概念框架和实证发现为理解和改进现代AI模型中的创造力提供了基础，标志着泛化能力的新前沿。",
                    "inspiration_trace": "# 组合创造力：从挑战到解决方案的逻辑演进\n\n## 面临的挑战\n\n### 1. AI创造性任务缺乏理论基础\n作者首先指出，尽管AI系统（特别是大型语言模型）已越来越多地被用于科学创意生成等创造性任务，但这些能力缺乏坚实的数学和概念基础。正如论文所述：\"Modern AI systems now engage in scientifically creative tasks once conceptualized by Einstein, yet they lack strong mathematical and conceptual foundations for the abilities underlying these tasks.\"这种理论基础的缺失限制了我们诊断和改进LLMs在这些任务上的表现的能力。\n\n### 2. LLM生成科学想法的实践问题\n作者观察到LLM在科学创意生成中存在一个关键问题——\"想法-执行差距\"(ideation-execution gap)。具体表现为：LLM生成的科学想法常常存在实际不可行性、做出不切实际的假设、忽略适当的基线。这些问题使得虽然LLMs能够产生新颖的想法，但这些想法往往难以实际执行。\n\n### 3. 现有泛化能力框架的不足\n作者认识到，组合创造力(CC)与现有的组合泛化(CG)有本质区别。传统CG框架无法充分捕捉创造力的六个关键维度：组合性、开放性、结构新颖性、语义新颖性，以及新颖性和效用的程度评估。特别是，创造力评估需要开放式的、连续的度量，而非二元正确性判断，这是现有框架所不具备的。\n\n## 关键洞察\n\n### 1. 创造力可以被形式化为组合过程\n通过深入研究认知科学文献，作者洞察到创造力可以被理解为一种组合过程。论文引用了丰富的历史证据：\"there is a rich body of literature that models creativity as a combinatorial process in the space of mental representations\"。从爱因斯坦的\"组合游戏\"到庞加莱描述的\"思想碰撞\"，再到Mednick的远程关联理论，都表明创造力本质上是将熟悉的概念进行不熟悉组合的过程。这一洞察为形式化建模创造力提供了理论基础。\n\n### 2. 组合创造力需要新的评估框架\n作者敏锐地意识到，传统的准确性或正确性评估不适合评估开放性的创造力。正如论文所述：\"Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility\"。这一洞察表明，评估创造力需要关注新颖性和效用性的程度，而非二元判断。\n\n### 3. 模型架构对创造力有系统性影响\n作者推测，模型的深度和宽度会对其组合创造力能力产生系统性影响。他们预见到，对于固定的计算预算，可能存在最优的模型深度和宽度配置，这为后续的实证研究提供了方向。这一洞察源于对创造力本质的理解——它既需要同时表示多个概念的能力（可能受益于更宽的模型），也需要进行复杂序列推理的能力（可能受益于更深的模型）。\n\n## 提出的解决方案\n\n### 1. 理论框架：概念空间中的组合创造力\n基于上述洞察，作者提出了一个数学框架，将组合创造力建模为概念空间中的路径发现。他们将概念空间表示为一个图，其中节点代表概念，边代表概念间的语义关系。创造性产物被定义为图上的标记路径，表示概念间的组合关系。创意提示则被定义为在两个概念间生成满足包含和排除约束的路径的任务。这个框架将抽象的创造力概念转化为可操作的数学对象，为后续研究提供了理论基础。\n\n### 2. 评估方法：新颖性和效用性的量化\n作者提出了量化新颖性和效用性的具体方法：\n- **新颖性**：通过路径长度和标签的惊喜度来衡量，公式为N(P) := αh*h + αr*S(P)，其中S(P)是路径的惊喜度。这反映了路径的复杂性和语义距离。\n- **效用性**：通过满足包含和排除约束来衡量，公式为U(P; x) := (1 + αI|I|)(1 + αX|X|)I[...]。这反映了创意的逻辑一致性和实用性。\n- **创造力**：作为新颖性和效用性的乘积进行综合评估，C(θ) := Ex~D[U(Gθ(x); x) · N(Gθ(x))]。\n\n这种量化方法使得原本抽象的创造力概念变得可测量，为系统研究提供了工具。\n\n### 3. 算法任务：开放式的创意生成\n作者设计了一个算法任务，要求模型在概念图中生成满足约束的标记路径。这个任务是开放式的：任何满足约束的产物都是有效的，并可以进一步评估其新颖性和效用性程度。这种设计避免了传统评估中的二元判断，更好地捕捉了创造力的开放性本质。同时，通过引入包含和排除约束，该任务能够模拟真实世界创造性任务中的逻辑约束，如科学想法生成中的现实假设和资源限制。\n\n### 4. 实证研究：模型架构的影响\n基于理论框架和评估方法，作者进行了大规模实证研究，系统性地研究模型大小、深度和宽度对创造力的影响。这项研究揭示了几个关键发现：\n- **创造力的缩放行为**：随着模型大小和训练计算的增加，创造力可预测地提高。\n- **最优的模型深度和宽度**：对于固定的计算预算，存在最优的模型深度和宽度配置，更宽更浅的架构通常优于更深更窄的架构。\n- **新颖性-效用性权衡**：随着效用约束的增加，产物的新颖性呈下降趋势，这种权衡即使在模型规模增加时仍然存在，表明\"想法-执行差距\"可能是当前架构的内在挑战，而不仅仅是规模问题。\n\n这些实证发现不仅验证了作者的理论框架，还为改进AI模型的创造力提供了具体方向，标志着对AI泛化能力理解的新前沿。\n\n## 总结\n\n作者从AI创造性任务面临的挑战出发，通过关键洞察（创造力的组合本质、需要新的评估框架、模型架构的影响），最终提出了一个完整的解决方案（理论框架、评估方法、算法任务和实证研究）。这一逻辑演进不仅为理解和改进AI模型的创造力提供了基础，还揭示了当前LLM在创造性任务上的根本限制——新颖性-效用性权衡，为未来的研究指明了方向。正如论文标题所示，这标志着AI泛化能力研究的新前沿。"
                },
                {
                    "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
                    "arxiv_id": "2509.21072",
                    "authors": "Kaiwen He, Zhiwei Wang, Chenyi Zhuang, Jinjie Gu",
                    "summary": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Recon-Act\"的自进化多智能体框架，通过侦察-行动的循环过程，不断生成和优化工具，以提高在复杂网页环境中的任务执行能力。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文不是将LLM作为工具应用到特定领域，而是提出了一种新的多智能体协作框架和工具使用方法，增强了模型在复杂环境中的规划、推理和执行能力。论文中提到的\"intent decomposition\"（意图分解）、通过对比错误轨迹和成功轨迹来推断补救措施，以及建立\"数据-工具-行动-反馈的闭环训练管道\"，都是直接提升LLM通用推理能力的方法论创新。 其次，论文包含多个正面指标：在能力方向上涉及planning、reasoning和problem-solving；在训练方法上明确提出了\"Self-Evolving\"（自进化）机制；在新兴范式上提出了\"multi-agent framework\"和\"tool generation/orchestration\"，这些都是增强LLM通用推理能力的关键要素。 第三，论文不主要聚焦于任何排除领域。虽然论文在VisualWebArena数据集上评估，可能涉及视觉信息，但其核心是多智能体协作和工具使用，而不是多模态处理本身。论文关注的是通用的浏览器使用能力，适用于\"unseen websites\"，表明其具有通用性，不局限于特定应用领域。 最后，论文明确提出了一个通用的智能体协作框架，通过工具使用来增强LLM的通用问题解决能力，符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的保留条件。 综上所述，这篇论文致力于提高大语言模型本身的通用推理能力，特别是通过多智能体协作和工具使用来增强其在复杂环境中的规划和执行能力，因此符合研究目标。",
                    "summary2": "本文旨在解决当前多模态模型在处理真实网页多轮长轨迹任务时的行动序列混乱和过度试错问题。针对复杂网页环境，我们提出了一种基于\"侦察-行动\"行为范式的自我进化多智能体框架Recon-Act，并在VisualWebArena数据集上通过成功率等指标验证了其有效性，达到了36.48%的成功率，优于其他自动化代理。",
                    "summary_translation": "近年来，多模态模型（multimodal models）取得了显著进展，为智能浏览器使用代理（intelligent browser use agents）铺平了道路。然而，在解决现实世界网页上的多轮、长轨迹（multi-turn, long-horizon）任务时，当前代理仍存在动作序列混乱和执行过程中过度试错的问题。本文介绍了Recon-Act，一个基于侦察-行动（Reconnaissance-Action）行为范式的自我进化多代理框架。该系统由侦察团队（Reconnaissance Team）和行动团队（Action Team）组成：前者进行对比分析和工具生成，后者负责意图分解、工具编排和执行。通过对比错误轨迹与成功轨迹，侦察团队推断出补救措施，并将其抽象为统一概念的通用工具（generalized tools），这些工具可以表达为提示或基于规则的代码，并实时注册到工具库中。行动团队利用这些针对性工具重新推理过程，从而建立了数据-工具-行动-反馈的闭环训练管道。遵循本文提出的6级实施路线图，我们目前已达到第3级（有限的人机循环干预（human-in-the-loop intervention））。利用通过侦察获得的通用工具，Recon-Act显著提高了对未见网站的适应性和解决长轨迹任务的能力，并在具有挑战性的VisualWebArena数据集上实现了最先进的（state-of-the-art）性能。",
                    "inspiration_trace": "# Recon-Act核心方法的逻辑演进分析\n\n## 一、面临的挑战\n\n作者首先识别了当前浏览器使用代理系统面临的核心挑战：\n\n1. **行动序列混乱**：在多轮、长轨迹任务中，现有代理难以保持有序的行动序列，导致执行效率低下。\n\n2. **过度试错行为**：特别是在不熟悉的环境中，代理会进行大量无效的试错，增加了执行成本和时间。\n\n3. **环境适应性差**：当面对未曾见过的网站时，现有代理的泛化能力有限，无法有效适应新环境。\n\n4. **长视野任务解决能力弱**：在需要多步推理和长期规划的任务上，现有代理表现不佳，难以维持任务连贯性。\n\n## 二、关键洞察\n\n通过对现有方法的分析和人类行为的观察，作者获得了几个关键洞察：\n\n### 1. 浏览器环境的特殊性\n- **信息密度高**：网页包含大量信息，但只有一部分与特定任务相关。\n- **独特的观察空间**：浏览器环境有其特定的观察空间特性，充分利用这些特性可以显著提高执行性能。\n- **行动空间独特性**：浏览器操作有特定的行动模式，不同于一般GUI交互。\n\n### 2. 人类行为模式的启发\n- **侦察-行动模式**：人类用户在面对不熟悉网页时，通常会先扫描页面获取整体情况（侦察），然后再采取具体行动。\n- **信息提取与精炼**：人类能够快速从复杂页面中提取与任务相关的关键信息，忽略无关内容。\n\n### 3. 工具概念的扩展\n- **广义工具概念**：工具不仅限于传统API调用，还可以包括提示、规则代码等广义形式。\n- **工具的针对性**：通过对比分析成功和失败案例，可以生成针对特定问题的工具。\n\n### 4. 闭环学习的重要性\n- **反馈机制**：通过对比分析正面和负面实例，可以建立有效的反馈机制。\n- **持续进化**：系统需要能够从经验中学习并不断改进，形成闭环进化管道。\n\n## 三、解决方案的演进逻辑\n\n基于上述洞察，作者逐步构建了Recon-Act解决方案，其演进逻辑如下：\n\n### 1. 从行为模式到系统架构\n- **第一步：行为范式抽象**  \n  从人类\"侦察-行动\"的行为模式中抽象出\"Reconnaissance-Action\"行为范式，将其作为系统设计的基础。\n\n- **第二步：双团队架构设计**  \n  基于侦察-行动范式，设计双团队多代理框架：\n  - **侦察团队**：负责比较分析和工具生成\n  - **行动团队**：负责意图分解、工具编排和执行\n\n### 2. 从工具使用到工具生成\n- **第三步：广义工具概念引入**  \n  将工具概念扩展为\"广义工具\"，包括：\n  - **提示型工具**：提供指导性信息，帮助行动团队做出决策\n  - **决策型工具**：直接产生可执行的行动，具有权威性\n\n- **第四步：工具生成机制**  \n  设计工具生成机制，使系统能够：\n  - 通过对比错误轨迹和成功轨迹，推断补救措施\n  - 将这些补救措施抽象为广义工具\n  - 实时注册到工具档案中供行动团队使用\n\n### 3. 从静态系统到自进化系统\n- **第五步：闭环进化管道建立**  \n  建立\"数据-工具-行动-反馈\"的闭环进化管道：\n  - **数据**：收集成功和失败的任务执行轨迹\n  - **工具**：基于轨迹对比分析生成针对性工具\n  - **行动**：行动团队利用工具执行任务\n  - **反馈**：评估执行结果，形成新的数据点\n\n- **第六步：分级实施路线图**  \n  提出6级实施路线图，从完全人工操作到端到端模型：\n  - Level 1：仅执行代理由模型驱动\n  - Level 2：主代理和执行代理由模型驱动\n  - Level 3：主代理、执行代理和编码器由模型驱动（当前实现）\n  - Level 4：除分析员外所有组件由模型驱动\n  - Level 5：所有代理由模型驱动\n  - Level 6：端到端模型\n\n## 四、核心创新点\n\n通过上述演进逻辑，作者形成了Recon-Act的核心创新：\n\n### 1. 侦察操作的形式化\n- 首次在浏览器环境中形式化了\"侦察操作\"的概念\n- 通过有限数量的探索性行动从信息密集的网页中提取关键观察\n- 提高了长期、多轮任务的可解决性和效率\n\n### 2. 以工具为中心的自进化系统\n- 将广义工具作为迭代过程的核心\n- 通过对比分析正面和负面轨迹，系统推导反馈信号\n- 建立了闭环进化管道，实现系统的自我完善\n\n### 3. 双团队协作架构\n- 实现了侦察和行动的有效分离与协作\n- 侦察团队提供可操作的指导，行动团队利用这些指导完成任务\n- 优化了系统在不熟悉环境中的适应能力\n\n### 4. 广义工具的实时生成与注册\n- 系统能够实时生成和注册针对特定问题的工具\n- 工具既可以是提示形式，也可以是专用工具代理\n- 显著提高了系统在面对新任务时的解决能力\n\n## 五、逻辑演进总结\n\nRecon-Act核心方法的逻辑演进体现了从问题识别到解决方案构建的系统性思考过程：\n\n1. **问题识别**：准确识别了现有浏览器使用代理在长轨迹任务中的核心缺陷\n\n2. **洞察获取**：通过分析人类行为和浏览器环境特性，获得了关键设计洞察\n\n3. **范式构建**：基于人类行为模式构建了\"侦察-行动\"的基本范式\n\n4. **架构设计**：基于范式设计了双团队多代理架构，实现功能分离与协作\n\n5. **机制创新**：引入广义工具概念和工具生成机制，扩展系统的适应能力\n\n6. **系统进化**：建立闭环进化管道，使系统能够从经验中持续学习和改进\n\n7. **路径规划**：设计分级实施路线图，为系统发展提供清晰路径\n\n这一逻辑演进不仅解决了初始识别的问题，还建立了一个能够持续自我完善的系统框架，体现了作者从具体问题到通用解决方案的系统性思考过程。"
                },
                {
                    "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
                    "arxiv_id": "2509.20744",
                    "authors": "Qihang Ai, Haiyun Jiang",
                    "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的推理范式，通过整合自回归(AR)和非自回归(NAR)语言模型来增强大语言模型的推理能力和效率。论文明确关注\"reasoning tasks\"，特别提到了\"mathematics and code\"等推理密集型领域，这正是通用推理能力的重要组成部分。论文不是将LLM作为工具应用于特定领域，而是改进LLM本身的推理机制，通过让NAR模型高效生成中间推理轨迹，然后指导AR模型提供精确的最终答案，从而在保持推理质量的同时显著提高效率。这种方法直接针对LLM的通用推理能力进行改进，符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。论文不涉及多模态、特定应用领域或模型可靠性等排除标准，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在推理任务中的效率与准确性平衡问题。针对数学和代码等需要长链推理的场景，我们提出了一种结合NAR和AR模型的混合推理范式，其中NAR模型高效生成中间推理痕迹，AR模型基于这些痕迹生成精确最终答案，并在AIME2025、GSM8K和LeetCode-Hard数据集上通过成功率指标验证了其有效性。",
                    "summary_translation": "我们通过一个整合了自回归(auto-regressive, AR)和非自回归(non-autoregressive, NAR)语言模型的框架来研究推理任务。AR模型按顺序生成文本，擅长产生连贯的输出，但通常存在推理速度慢的问题，特别是在需要长思维链的数学和代码等推理密集型领域。相比之下，如离散扩散模型(discrete diffusion models)等NAR模型允许并行生成，并提供显著的速度提升，但通常以降低输出质量为代价。为解决这些局限性，我们引入了一种新范式，其中NAR模型高效生成中间推理轨迹(intermediate reasoning traces)，随后引导AR模型提供精确的最终答案。实验表明，我们的方法相比强大的基线(baseline)实现了显著的26%改进，同时大幅降低了推理成本。",
                    "inspiration_trace": "# 从挑战到解决方案：论文核心方法的逻辑演进\n\n## 一、面临的挑战\n\n### 1. 自回归模型(AR)的推理效率瓶颈\n作者首先观察到当前主流的自回归语言模型（如ChatGPT等）在复杂推理任务中面临根本性限制：这些模型采用严格的顺序token生成方式，在需要长推理链的任务（如数学和编程）中导致推理速度显著下降。随着任务复杂度增加，这一问题更为突出，因为模型需要生成更长的推理链，从而大幅增加计算开销和推理延迟。\n\n### 2. \"过度思考\"问题\n论文指出，当前提高模型性能的主流策略——在推理阶段分配更多token——导致了\"过度思考\"现象。许多中间步骤变得冗余或缺乏信息价值，却消耗了大量计算资源。这种策略虽然可能提高解决方案质量，但引入了严重的效率问题，特别是在实际应用中难以接受。\n\n### 3. 非自回归模型(NAR)的质量局限\n虽然非自回归模型（如离散扩散模型）通过并行生成提供了显著的速度优势，但它们通常以输出质量下降为代价。在需要精确、连贯输出的推理任务中，NAR模型难以达到与AR模型相媲美的质量水平，这限制了它们在复杂推理任务中的应用。\n\n### 4. 推理任务的本质需求\n在数学和编程等具有挑战性的推理任务中，多步推理是必不可少的。简化推理过程可能导致准确率下降，但详细推理又会增加计算成本。这种固有的张力使得在保证推理质量的同时提高效率成为一个核心挑战。\n\n## 二、关键洞察\n\n### 1. AR与NAR模型的互补性\n作者的核心洞察是认识到AR和NAR模型具有互补优势：AR模型擅长产生连贯、精确的输出，但速度慢；而NAR模型（特别是扩散语言模型）具有并行生成、迭代修正和全局上下文建模的优势，速度快。这一互补性为结合两者优势提供了理论基础。\n\n### 2. 推理过程的可分解性\n论文洞察到推理过程可以自然地分解为两个不同性质的阶段：\"思考\"(think)和\"回答\"(answer)。\"思考\"阶段需要全局视角和快速生成多个可能性，适合NAR模型；而\"回答\"阶段需要精确和连贯的输出，适合AR模型。这种分解为劳动分工提供了可能。\n\n### 3. 紧凑推理的充分性\n作者意识到，在许多情况下，不需要冗长的推理链，紧凑但明确的推理痕迹足以指导最终答案生成。这一洞察为解决\"过度思考\"问题提供了方向：可以通过优化推理痕迹的质量而非数量来提高效率。\n\n### 4. 扩散语言模型的潜力\n论文注意到近期扩散语言模型(DLMs)的进展表明，NAR模型可以实现高质量输出和高速推理的平衡。DLMs通过迭代去噪过程，提供并行生成、迭代修正和全局上下文建模的优势，更好地与现代并行计算硬件对齐，这为NAR模型在推理任务中的应用开辟了新可能。\n\n## 三、提出解决方案\n\n### 1. 混合推理范式的构想\n基于上述洞察，作者提出了一个创新性的混合推理范式，将NAR和AR模型的优势结合起来。这一范式的核心思想是：将推理过程分为两个阶段，NAR模型负责生成紧凑但明确的推理痕迹（\"思考\"阶段），AR模型负责基于这些痕迹产生精确的最终答案（\"回答\"阶段）。\n\n### 2. 劳动分工的具体设计\n作者将这一构想具体化为一个明确的劳动分工方案：\n- NAR组件负责\"思考\"阶段：利用其并行生成能力和全局上下文建模优势，快速生成紧凑但明确的推理痕迹。\n- AR组件负责\"回答\"阶段：利用其精确生成能力，基于NAR生成的推理痕迹，产生精确和忠实的最终输出。\n\n### 3. 路由策略的设计\n论文进一步设计了两种具体的路由变体来验证这一分工的有效性：\n- NAR→NAR：同一个NAR模型既生成思考痕迹，又基于这些痕迹生成最终答案。\n- NAR→AR：NAR模型生成思考痕迹，然后由强大的AR模型（如GPT-5）基于这些痕迹生成最终答案。\n\n### 4. 预期优势\n作者预期这种混合范式能够：\n- 继承NAR的效率和全局上下文建模能力\n- 结合AR的可靠性和表达能力\n- 避免冗长推理，解决\"过度思考\"问题\n- 在保持或提高推理质量的同时，显著降低推理成本\n\n## 四、逻辑演进总结\n\n这篇论文的核心思想演进体现了从问题识别到解决方案的完整逻辑链：作者首先识别出现有方法在推理任务中的效率与质量权衡困境，特别是AR模型的速度瓶颈和\"过度思考\"问题；然后通过关键洞察，认识到AR和NAR模型的互补性以及推理过程的可分解性；最终基于这些洞察，提出了一个创新的混合推理范式，实现了NAR和AR模型的优势互补。这一解决方案不仅在理论上合理，实验结果也验证了其有效性，在数学和代码推理任务上实现了显著的性能提升（平均+26%），同时大幅降低了推理成本。\n\n这一逻辑演进展示了作者如何通过深入分析现有方法的局限性，发现不同技术路线的互补潜力，并创造性地将它们结合，最终提出一个既实用又高效的解决方案。"
                },
                {
                    "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
                    "arxiv_id": "2509.20562",
                    "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
                    "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多级反思合成来增强LLM智能体自我学习和推理能力的新方法。论文专注于改进LLM的基础能力，特别是通过三个互补的反思层次（单轨迹学习、任务内学习和任务间学习）来提升智能体的错误分析和问题解决能力。这种方法论研究直接关注提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。论文使用了TravelPlanner、NATURAL PLAN等规划相关的基准测试来评估方法，但其核心是提出一种通用的反思框架，而非针对特定应用。论文明确涉及LLM智能体、自我改进机制和问题解决能力等正面指标，同时不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决LLM智能体在复杂任务中生成有意义反思的挑战。针对失败轨迹丰富的复杂任务场景，我们提出了一种基于多级反思合成（包括单轨迹、任务内和任务间学习）训练回溯语言模型的SAMULE框架，并在TravelPlanner、NATURAL PLAN和Tau-bench基准上通过Pass Rate和EM-based Accuracy验证了其有效性。",
                    "summary_translation": "尽管LLM agents（大型语言模型代理）取得了快速进展，但由于错误分析不足和对罕见成功轨迹的依赖，它们在生成有意义的反思方面仍面临挑战，特别是在复杂任务中。在这项工作中，我们提出了SAMULE，这是一个基于Multi-Level Reflection Synthesis（多层次反思合成）训练的回顾性语言模型驱动的自学习代理新框架。它首先在三个互补的层次上合成高质量反思：Single-Trajectory Learning（微观层面）用于详细错误纠正；Intra-Task Learning（中观层面）用于在同一任务的多次尝试中构建错误分类体系；以及Inter-Task Learning（宏观层面）用于从不同任务失败中基于相同类型错误提取可转移的见解。然后我们微调一个作为回顾性模型的语言模型，以在推理过程中生成反思。我们通过基于预见的反思机制进一步将我们的框架扩展到交互式环境，使代理能够通过比较预测和实际响应，在用户交互过程中主动反思和适应。在三个具有挑战性的基准测试——TravelPlanner、NATURAL PLAN和Tau-bench上的广泛实验表明，我们的方法显著优于基于反思的基线方法。我们的结果突显了精心设计的反思合成和以失败为中心的学习在构建自我改进的LLM agents中的关键作用。",
                    "inspiration_trace": "# SAMULE方法逻辑演进分析\n\n## 一、问题识别：现有LLM智能体的反思局限性\n\n作者首先明确了当前LLM智能体在自我学习方面面临的三大核心挑战：\n\n### 1.1 不充分的错误分析\n作者观察到，现有方法如Reflexion在复杂任务中表现有限，根本原因在于它们缺乏深入诊断失败原因的能力。这导致生成的反思往往是通用且无用的策略，无法有效指导智能体改进。例如，在TravelPlanner这样的复杂基准测试中，Reflexion只能产生5.56%的通过率，远低于实际需求。\n\n### 1.2 对罕见成功轨迹的过度依赖\n作者指出，许多先进方法（如Expel）严重依赖成功轨迹作为学习信号。然而，在现实世界的复杂任务中，成功往往是罕见的，而失败则更为普遍。这种依赖使得这些方法在实际应用中效果不佳，特别是在高失败率的环境中。\n\n### 1.3 对反思质量的敏感性\n作者发现，基于强化学习的方法（如Retroformer和CTRL）虽然理论上更先进，但它们对合成反思的质量高度敏感。在复杂任务中，当反思算法本身无法产生信息丰富和准确的反馈时，这些RL方法会学习到无意义的策略，导致性能下降。\n\n## 二、关键洞察：从认知科学中汲取灵感\n\n### 2.1 失败作为丰富的学习资源\n作者的核心洞察是：失败轨迹中蕴含着丰富的学习信息，但现有方法未能充分利用这些信息。与依赖成功轨迹的方法不同，作者转向以失败为中心的学习范式，认为从错误中提取模式比模仿成功更能促进智能体的自我改进。\n\n### 2.2 多级反思的认知基础\n作者从认知科学和学习理论中汲取灵感，特别是Kolb的经验学习模型，该模型描述了一个涉及具体经验、反思观察和抽象概念化的学习周期。这启发作者认识到有效的学习需要在不同抽象层次上进行反思：\n- 微观层次：具体的、实例特定的反馈\n- 中观层次：模式识别和分类\n- 宏观层次：抽象概念化和跨任务转移\n\n### 2.3 自我解释学习的价值\n作者引用研究表明，在学习示例时进行自我解释的学习者表现更好，并发展出更稳健的知识结构。这是因为学习者在解释个别步骤（具体层次）时，经常调用领域原则或规则（抽象层次）来证明步骤的正确性或错误性。这一发现直接支持了多级反思框架的设计。\n\n## 三、解决方案演进：从简单反思到多级反思\n\n### 3.1 从单一轨迹到跨轨迹反思\n作者首先认识到，现有方法（如Reflexion）主要关注单一轨迹的反思，这限制了它们从多个失败中学习的能力。通过实验，作者发现跨轨迹的反思方法（如任务间错误反思）显著优于单一轨迹反思。例如，在TravelPlanner上，任务间错误反思达到9.44%的通过率，而Reflexion仅为5.56%。\n\n### 3.2 从依赖成功到利用失败\n作者观察到，依赖成功轨迹的方法（如Expel）在复杂任务中表现不佳，甚至在TravelPlanner上完全失败（0%通过率）。这促使作者转向以失败为中心的学习，利用错误分类和聚类从失败试验中提取见解。实验证明，这种策略在成功稀缺但失败丰富的环境中更为有效。\n\n### 3.3 从静态反思到动态反思\n作者进一步意识到，大多数现有方法在任务完成后进行反思，这在交互式环境中不实用。为了解决这一限制，作者引入基于预测的反思机制，使智能体能够在交互过程中通过比较预测和实际响应来主动反思和适应。这种动态反思机制在Tau-bench的交互式设置中取得了显著成果。\n\n## 四、SAMULE框架的提出：多级反思合成\n\n基于上述洞察和演进，作者提出了SAMULE框架，其核心创新在于多级反思合成，通过三个互补的层次生成高质量反思：\n\n### 4.1 单一轨迹学习（微观层次）\n在这一层次，系统分析单个失败轨迹与参考计划的对比，识别即时错误并生成针对性的纠正策略。作者发现，提供参考输出在微观层次特别有用，因为它允许模型逐项比较其轨迹与参考，从而进行详细的错误推理。\n\n### 4.2 任务内学习（中观层次）\n作者创新性地引入了错误分类法的概念，通过检查同一任务查询的多个轨迹，分类错误类型并构建结构化的错误分类体系。这种基于模式的反馈使智能体能够识别和分类常见错误，从而生成更丰富的反思。\n\n### 4.3 任务间学习（宏观层次）\n在最高层次，系统聚类来自不同任务查询的相似错误，推导出高层次、可转移的见解。这种跨任务的反思使智能体能够将从一个任务中学到的经验应用到其他相关任务中，实现知识的迁移和泛化。\n\n### 4.4 回顾性模型训练\n为了使多级反思框架在推理阶段可用，作者训练一个专门的回顾性语言模型。通过监督微调（SFT），该模型能够动态生成轨迹特定的反思，无需访问参考输出。实验证明，即使使用简单的SFT而非复杂的RL方法，只要反思数据质量高，也能取得优异性能。\n\n## 五、实验验证与理论确认\n\n作者在三个具有挑战性的基准测试上验证了SAMULE的有效性：\n\n### 5.1 跨轨迹反思的显著优势\n实验结果一致表明，跨轨迹反思方法（包括SAMULE）显著优于单一轨迹反思。在TravelPlanner上，SAMULE达到20%的通过率，远高于Reflexion的5.56%；在NATURAL PLAN的Trip域上，SAMULE达到60.31%，而Reflexion为50%。\n\n### 5.2 失败驱动学习的有效性\n作者特别强调了失败提供更强信号的发现。Expel在NATURAL PLAN上表现良好（Trip域53.79%），但在TravelPlanner上完全失败（0%），这证实了依赖成功轨迹的方法在高错误域中的局限性。相比之下，SAMULE专注于从失败中学习，在各种复杂度的环境中都表现出色。\n\n### 5.3 简单训练与高质量反思的结合\n一个有趣的发现是，即使使用简单的SFT而非复杂的RL方法，只要反思数据质量高，也能取得优异性能。SAMULE在TravelPlanner上达到20%的通过率，而使用更复杂RL技术的Retroformer变体仅为12.78%。这突显了高质量反思合成在训练有效回顾性模型中的关键作用。\n\n## 六、结论：多级反思作为自我改进的通用范式\n\n通过这一系列逻辑演进，作者最终确立了一个核心观点：结构化的多级反思是构建自我改进LLM智能体的关键。SAMULE框架的成功不仅在于其技术创新，更在于其理论基础和对学习本质的深刻理解。作者的工作表明，通过从微观到宏观的系统性反思分析，智能体能够从失败中提取有价值的见解，实现真正的自我改进。\n\n这一逻辑链从问题识别出发，基于认知科学理论，通过解决方案的逐步演进，最终形成了一个完整、有效的框架，为LLM智能体的自我学习提供了新的方向。"
                },
                {
                    "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
                    "arxiv_id": "2509.21282",
                    "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
                    "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种新的强化学习方法(PSPO)来改进LLM的训练过程，而非将LLM作为工具应用到特定领域。论文专注于通过改进训练方法来提升模型的基础能力，这符合我的核心目标。 其次，论文包含了多个重要的正面指标：(1)核心概念上明确研究LLM，在Qwen2.5模型上进行实验；(2)能力方向上关注推理能力，特别是在GSM8K数学推理数据集上评估，并明确提到\"improves the reasoning leading to clearer and more concise responses which are more logical\"；(3)训练方法上提出了一种新的强化学习技术(PSPO)，用于替代传统的ratio clipping方法。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然使用了数学推理数据集进行评估，但这只是评估通用推理能力的常见基准，而非特定应用领域的研究。 论文的核心贡献是提出了一种概率平滑策略优化方法(PSPO)，通过创建软信任区域来改进LLM的强化学习训练过程，从而提升模型的推理能力和性能。这种方法直接针对LLM的通用推理能力进行改进，完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型强化学习中比例裁剪导致的信息丢失和梯度不连续问题。针对LLM RL训练场景，我们提出了一种概率平滑策略优化(PSPO)方法，通过将当前策略概率平滑朝向旧策略创建软信任区域，保留梯度信号并防止不稳定更新。在GSM8K、SVAMP、ASDiv和MATH-500数据集上，通过Top-1准确率和LLM-as-Judge评估指标验证，GR-PSPO在Qwen2.5-0.5B/1.5B模型上相比裁剪GRPO性能提升超过20%，同时生成更清晰、逻辑连贯的响应。",
                    "summary_translation": "使用强化学习(reinforcement learning, RL)方法如PPO和GRPO训练大型语言模型(large language models, LLMs)通常依赖比例裁剪(ratio clipping)来稳定更新。虽然裁剪在防止不稳定性方面有效，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化(Probability Smoothing Policy Optimisation, PSPO)，它在计算重要性比例之前，将当前策略的概率平滑到旧(行为)策略，类似于标签平滑。与裁剪不同，PSPO保留了梯度信号，同时向旧策略的插值创建了一个软信任区域，阻止大的、不稳定的更新，并有正式保证。我们在GRPO中实例化PSPO(GR-PSPO)，并在GSM8K上微调Qwen2.5-0.5B和Qwen2.5-1.5B，在GSM8K测试和跨数据集泛化(SVAMP、ASDiv和MATH-500)上进行评估。与未裁剪的GRPO(单次迭代；无数据重用，比例始终=1)相比，GR-PSPO实现了相似的性能，但改进了推理，导致更清晰、更简洁且更合乎逻辑的响应。与裁剪的GRPO相比，GR-PSPO在0.5B和1.5B模型上都显著提高了性能，在GSM8K上提升了超过20%(0.5B为39.7% vs 17.6%，1.5B为59.4% vs 37.8%)。",
                    "inspiration_trace": "# 论文核心方法逻辑链分析：从问题洞察到PSPO的提出\n\n## 一、问题识别：LLM强化学习中的稳定性困境\n\n作者首先识别出LLM强化学习中的核心挑战：**如何在保证学习速度的同时维持训练稳定性**。具体表现为：\n\n1. **理论最优与实践可行性的矛盾**：理论上的最优方案（如TRPO）需要极小的步长，导致收敛效率低下甚至不可行。\n\n2. **现有方法的局限性**：\n   - TRPO使用KL散度约束更新，虽允许更大步长但计算效率低下\n   - PPO和GRPO采用比率裁剪作为KL散度的一阶近似，虽计算高效但有明显缺陷\n\n3. **比率裁剪的具体问题**：\n   - 信息丢失：当策略比率离开裁剪范围时，梯度完全消失\n   - 梯度不连续：裁剪引入人为的梯度不连续点\n   - 探索受限：可能错过裁剪策略空间外的更好策略\n   - 替代方案脆弱：KL提前停止、平滑变换等方法在复杂设置中表现不佳\n\n## 二、关键洞察：从监督学习到强化学习的思想迁移\n\n作者的核心突破来自于**跨领域知识的迁移应用**：\n\n1. **标签平滑的启发**：\n   - 注意到监督学习中标签平滑通过将one-hot编码目标转换为软目标，减少模型过度自信\n   - 标签平滑的数学表达：˜φ(k|x) = (1−α)·φ(k|x) + α·u(k)\n   - 标签平滑已被证明能提高模型鲁棒性和泛化能力\n\n2. **策略优化的特殊需求**：\n   - 认识到策略优化与监督学习的根本差异：策略更新需要在信任区域内进行\n   - 关键洞察：不应向均匀分布平滑，而应向旧的行为策略πθold平滑\n   - 这种平滑自然形成\"行为锚定的信任区域\"\n\n3. **理论联系**：\n   - 注意到标签平滑引起的损失偏差等价于KL散度的变化\n   - 这强化了向旧策略平滑的直觉，因为KL散度正是TRPO中信任区域的核心度量\n\n## 三、方法提出：概率平滑策略优化(PSPO)\n\n基于上述洞察，作者提出了PSPO作为比率裁剪的替代方案：\n\n1. **核心思想**：\n   - 在计算重要性比率之前，先将当前策略概率向旧策略平滑\n   - 数学表达：˜πθ(at|st) = (1−α)πθ(at|st) + α·πθold(at|st)\n   - 平滑后的比率：˜rt(θ) = (1−α)rt + α\n\n2. **软信任区域的形成**：\n   - 比率在r=1周围收缩，自然形成以πθold为锚点的软信任区域\n   - 与硬裁剪不同，这种平滑是连续的，保留了梯度信息\n   - α参数直接控制信任区域的\"软度\"\n\n3. **理论保证**：\n   - **总变异收缩**：平滑策略与旧策略之间的距离被系统性地缩小\n   - **KL上界收缩**：平滑策略与旧策略之间的KL散度上界被缩小\n   - **非消失梯度**：任何地方都保持非零梯度，斜率为(1−α)A\n   - **过度自信正则化**：平滑策略不会超过当前策略和旧策略的最大值\n\n## 四、优势分析：PSPO为何优于比率裁剪\n\n作者通过理论分析阐明了PSPO的系统性优势：\n\n1. **梯度保持**：\n   - 裁剪在范围外产生平坦区域(梯度为零)\n   - PSPO在任何地方都保持非零梯度，保留了学习信号\n\n2. **软约束vs硬约束**：\n   - 裁剪是硬约束，突然切断梯度\n   - PSPO是软约束，平滑地收缩比率，更接近TRPO的理想但计算高效\n\n3. **隐式稳定性**：\n   - PSPO目标函数可重写为：(1−α)Ea∼πθ[A(a)] + αEa∼πθold[A(a)]\n   - 这表明PSPO本质上是带隐式稳定性的缩放策略梯度\n   - 不需要显式的KL惩罚项(β=0)，简化了实现\n\n4. **计算效率**：\n   - 仅需替换比率计算，不增加额外计算或内存开销\n   - 与现有RL框架完全兼容，可直接替换裁剪操作\n\n## 五、逻辑演进总结\n\n作者的思想演进体现了清晰的**问题-洞察-解决方案**脉络：\n\n1. **从具体问题出发**：识别比率裁剪在LLM RL中的具体缺陷\n2. **跨领域知识迁移**：从监督学习的标签平滑获得关键启发\n3. **理论联系实际**：将标签平滑与KL散度理论联系，指导方法设计\n4. **系统性创新**：提出PSPO作为比率裁剪的直接替代，而非简单修补\n5. **理论保证**：提供严格的理论分析，证明方法的有效性和优势\n6. **实践验证**：通过实验证明PSPO在数学推理任务上的优越性能\n\n这一思维过程展示了作者对现有方法的深入理解、创造性的问题解决思路，以及理论与实践相结合的研究方法，最终提出了一个既简单又有效的解决方案。"
                },
                {
                    "title": "LATTS: Locally Adaptive Test-Time Scaling",
                    "arxiv_id": "2509.20368",
                    "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
                    "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文的本质是提出一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的方法，用于改进大语言模型在推理过程中的计算资源分配。这属于改进LLM基础能力的研究，特别是优化其推理性能，而不是将LLM作为工具应用到特定领域。 论文明确包含正面指标中的核心概念\"Large language models, LLMs\"，并且关注提高LLM在下游任务上的性能，这涉及到推理和问题解决能力。虽然论文没有提到强化学习等训练方法或新兴范式，但其核心贡献是关于推理过程的优化。 论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。相反，它提出的是一种通用的测试时计算缩放方法，适用于各种下游任务。 特别值得注意的是，LATTS方法通过在每个生成步骤使用基于验证器的接受标准，动态决定是否重新采样、回溯、重启或停止生成过程，这实际上是一种增强LLM推理能力的通用方法论。它通过更智能地分配计算资源来提高模型的推理质量和效率，这与提高大语言模型通用推理能力的研究目标高度一致。",
                    "summary2": "本文旨在解决语言模型推理过程中如何动态分配计算资源的问题。针对数学推理任务，我们提出了一种基于验证器的局部自适应测试时间缩放方法(LATTS)，并在MATH500和AIME数据集上通过准确率和生成令牌数验证了其有效性。",
                    "summary_translation": "提高大型语言模型（Large Language Models, LLMs）在下游任务性能的一种常见策略是使用验证模型（verifier model）从候选池中选择最佳答案，或引导自回归（auto-regressive）生成过程产生更优输出。这类方法通常以提高准确性为代价，增加了测试时的计算量，这种范式被称为测试时扩展（test-time scaling）。然而，大多数现有方法在所有样本和生成步骤上均匀增加计算量，没有考虑单个实例的复杂性，导致资源使用效率低下。我们提出了一种名为局部自适应测试时扩展（Locally Adaptive Test-Time Scaling, LATTS）的方法来解决这一限制，该方法在生成步骤间分配可变的计算量。具体而言，在每个生成步骤，LATTS采用基于验证器的接受标准（verifier-based acceptance criterion）来决定是否重新采样（resample）、回溯（backtrack）、重启（restart）或停止生成过程。该标准有效地根据从验证模型得出的局部难度（local difficulty）精确概念来调整每步的计算努力。实证结果表明，与标准基于验证器的方法相比，LATTS实现了显著更优的准确性-计算量权衡（accuracy--compute tradeoffs）。",
                    "inspiration_trace": "以下是对论文中 **Locally Adaptive Test-Time Scaling (LATTS)** 方法思想演进脉络的梳理，聚焦从挑战到洞察再到解决方案的逻辑链条：\n\n---\n\n### **1. 面临的挑战：现有测试时计算缩放的局限性**\n- **问题背景**：  \n  大语言模型（LLMs）在复杂推理任务（如数学问题求解）中，常通过测试时计算缩放（如多数投票、BoN、束搜索）提升性能。但现有方法存在两大缺陷：\n  - **静态资源分配**：计算资源（如生成步数、候选数量）在全局均匀分配，无法适应不同步骤的难度差异。例如，简单步骤可能过度采样，而关键步骤却因资源不足导致错误累积。\n  - **验证器依赖瓶颈**：基于验证器（如过程奖励模型PRM）的方法（如束搜索）需频繁调用验证器评估候选步骤，计算开销巨大（例如束搜索需80次验证器调用/问题）。\n\n- **核心矛盾**：  \n  如何在**有限计算预算**下，**动态分配资源**给最需要优化的推理步骤，同时**减少验证器调用成本**？\n\n---\n\n### **2. 关键洞察：推理步骤的局部难度与验证器的引导作用**\n- **观察现象**：  \n  推理过程中，不同步骤的“难度”存在显著差异：\n  - **简单步骤**：模型易生成高置信度的正确步骤（验证器分数高）。\n  - **困难步骤**：模型易生成错误步骤（验证器分数低），需多次尝试才能找到可行解。\n  - **错误传播性**：早期步骤的错误会通过链式推理放大，导致最终答案失效。\n\n- **洞察本质**：  \n  1. **局部难度可量化**：  \n     定义步骤 \\( t \\) 的局部难度为：  \n     \\[\n     \\Delta(x, S_{<t}) = 1 - \\mathbb{E}_{s_t \\sim p_{\\text{model}}} [r(s_t | x, S_{<t})]\n     \\]  \n     其中 \\( r \\) 是验证器分数。难度越高，模型生成高分数步骤的概率越低。\n  2. **验证器作为动态资源分配器**：  \n     验证器分数 \\( r \\) 可转化为**自适应采样阈值**：对困难步骤（低 \\( r \\)），需更多候选尝试；对简单步骤（高 \\( r \\)），可快速接受。\n\n- **理论支撑**：  \n  通过拒绝采样（Acceptance-Rejection Sampling）将验证器分数 \\( r \\) 与调制函数 \\( f \\) 结合，定义目标分布：  \n  \\[\n  p_{\\text{target}} \\propto p_{\\text{model}} \\cdot (f \\circ r)\n  \\]  \n  这等价于求解带KL正则化的奖励最大化问题，将验证器反馈转化为动态采样策略。\n\n---\n\n### **3. 解决方案：LATTS——局部自适应的动态计算缩放**\n#### **核心思想演进**\n- **从静态到动态**：  \n  摒弃全局均匀分配资源，改为**按需动态分配**：  \n  - 对每个推理步骤 \\( t \\)，通过拒绝采样生成候选，直到满足验证器阈值或达到最大尝试次数 \\( M \\)。\n  - **自适应机制**：困难步骤（高 \\( \\Delta \\)）自动触发更多采样尝试（\\( \\mathbb{E}[n_t] = 1/(1-\\Delta) \\)），简单步骤则快速通过。\n\n- **从高开销到高效验证**：  \n  - **验证器调用优化**：仅对候选步骤评分，而非束搜索的指数级扩展。实验显示，LATTS仅需约20次验证器调用/问题，比束搜索减少4倍。\n  - **调制函数设计**：  \n    - **LATTS-Tilted**（\\( f(z)=z \\)）：随机阈值 \\( u \\sim U[0,1] \\)，接受 \\( r \\geq u \\) 的步骤，增加多样性。  \n    - **LATTS-Truncated**（\\( f(z)=\\mathbb{I}_{z \\geq \\delta} \\)）：固定阈值 \\( \\delta \\)，仅接受高置信度步骤，提升单次完成质量。\n\n- **从单路径到多路径融合**：  \n  生成 \\( N \\) 条独立推理链，通过**加权多数投票**聚合答案（权重为最终步骤的验证器分数），兼顾质量与多样性。\n\n#### **创新性设计**\n- **拒绝采样 + 回退策略**：  \n  当步骤 \\( t \\) 的 \\( M \\) 次尝试均失败时，触发回退机制（如回溯到上一步重新生成），避免错误传播。\n- **分块并行优化**：  \n  以块（Chunk）为单位并行生成候选，平衡延迟与计算开销（块大小 \\( H \\) 可调）。\n\n---\n\n### **4. 思想演进脉络总结**\n```mermaid\ngraph LR\nA[挑战：静态资源分配 + 验证器开销大] --> B[洞察：步骤难度异质 + 验证器可引导动态采样]\nB --> C[理论：拒绝采样实现 p_target ∝ p_model · (f∘r)]\nC --> D[方案：LATTS——局部自适应动态缩放]\nD --> E[核心创新：<br>1. 按需分配资源<br>2. 调制函数控制阈值<br>3. 回退策略容错<br>4. 多链投票聚合]\n```\n\n- **本质跃迁**：  \n  将验证器从“被动评估者”转变为“主动资源调度器”，通过**局部难度感知的拒绝采样**，实现计算资源的动态最优分配。\n\n- **效果验证**：  \n  在MATH500和AIME数据集上，LATTS以相同计算预算实现显著性能提升（1B模型通过LATTS超越405B模型），且验证器调用成本降低4倍，验证了“动态适配局部难度”思想的优越性。"
                },
                {
                    "title": "Tree Search for LLM Agent Reinforcement Learning",
                    "arxiv_id": "2509.21240",
                    "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
                    "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种基于树搜索的强化学习方法（Tree-GRPO），用于增强大语言模型（LLM）的代理能力和推理能力。该方法针对长期和多轮代理任务中的稀疏监督问题，通过树搜索采样提高LLM在固定预算内的推理效率，并利用树结构轨迹构建逐步过程监督信号。论文涉及多个正面指标，包括LLM核心概念、推理和问题解决能力方向、强化学习训练方法以及LLM智能体新兴范式。同时，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。论文提出的智能体方法是通用的，旨在提升LLM的通用推理能力，而非针对特定领域的应用，因此完全符合研究目标。",
                    "summary2": "本文旨在解决LLM智能体强化学习中的稀疏监督和rollout预算有限问题。针对多轮智能体任务场景，我们提出了一种基于树搜索的分组相对策略优化方法(Tree-GRPO)，并在11个数据集上通过准确率和F1分数验证了其有效性。",
                    "summary_translation": "强化学习(reinforcement learning, RL)的最新进展显著增强了大型语言模型(large language models, LLMs)的智能体能力。在长期和多轮智能体任务中，仅由结果奖励(outcome rewards)驱动的现有方法常常面临稀疏监督(sparse supervision)问题。为应对这一挑战，我们提出了基于树的组相对策略优化(Tree-based Group Relative Policy Optimization, Tree-GRPO)，这是一种基于树搜索(tree search)的分组智能体强化学习方法，其中每个树节点代表完整的智能体交互步骤。通过共享公共前缀(common prefixes)，树搜索采样增加了在固定词元(tokens)或工具调用(tool calls)预算内可实现的推演(rollouts)数量。此外，我们发现树结构轨迹(tree-structured trajectory)即使仅使用结果奖励，也自然允许构建逐步过程监督信号(step-wise process supervised signals)。基于此，Tree-GRPO在树内(intra-tree)和树间(inter-tree)两个层面估计组相对优势(grouped relative advantages)。通过理论分析(theoretical analysis)，我们证明了树内级别组相对策略优化的目标等价于步级直接偏好学习(step-level direct preference learning)的目标。在11个数据集和3种问答任务(QA tasks)上的实验表明，所提出的基于树的强化学习(tree-based RL)方法优于基于链的强化学习方法(chain-based RL method)。",
                    "inspiration_trace": "# Tree-GRPO方法逻辑链分析\n\n## 一、问题识别与挑战定义\n\n### 核心挑战的发现\n作者首先敏锐地识别出LLM智能体强化学习中的两大关键问题：\n\n1. **预算开销问题**：\n   - 多轮智能体任务需要大量token和工具调用，轨迹可能包含数千个token\n   - 现有链式(chain-based)采样方法存在大量冗余，rollout阶段占据主要训练时间和成本\n   - 这导致在实际应用中，工具调用成本（如高价搜索API）成为严重负担\n\n2. **稀疏监督问题**：\n   - 当前方法主要依赖最终结果奖励，而多轮交互轨迹中只有单一稀疏信号\n   - 难以确定长序列中哪些具体步骤对成功或失败有贡献\n   - 即使增加rollout预算，训练信号仍然稀疏，导致学习不平衡甚至训练崩溃\n\n作者通过观察现有方法的局限性，提出了一个核心问题：**能否在有限rollout预算下，仅基于结果奖励构建更细粒度的监督信号？**\n\n## 二、关键洞察与思路转变\n\n### 洞察一：树结构的效率优势\n作者突破了传统链式采样的思维定式，认识到：\n- 树结构通过共享公共前缀，可在固定预算内获得更多rollout\n- 关键创新是将树节点定义为**完整的智能体交互步骤**（Thought-Action-Observation），而非token/句子级别\n- 这种设计具有清晰的语义边界，更适合智能体任务，并能明确控制rollout预算\n\n### 洞察二：树结构的监督优势\n作者进一步发现：\n- 树结构轨迹自然允许仅使用结果奖励构建逐步过程监督信号\n- 在每个分支点，从不同子树叶子反向传播的结果奖励差异构成了子树间的偏好学习目标\n- 子树深度决定了过程信号的粒度，形成了**隐式的步骤级监督**\n\n这一洞察是革命性的：**树结构不仅提高了采样效率，还自然解决了稀疏监督问题**。\n\n## 三、解决方案的提出与演进\n\n### 步骤一：树搜索rollout策略设计\n基于上述洞察，作者设计了创新的树搜索策略：\n1. **初始化**：为每个提示生成M个独立链式轨迹作为M棵树的初始化\n2. **采样**：从每棵树中随机选择N个节点（除叶节点外）进行扩展\n3. **扩展**：对每个选定节点，从根到该节点的完整上下文作为输入，继续生成响应并插入源树作为新分支\n\n这种设计实现了**前缀共享**，在相同预算下获得约1.5倍的训练样本。\n\n### 步骤二：树结构组相对优势构建\n作者进一步利用树结构解决监督稀疏问题：\n1. **树内优势估计**：在每个分支点，反向传播结果奖励，兄弟分支间的差异构成偏好学习目标\n2. **树间优势估计**：跨树组合rollout，提供更稳定的基线估计\n3. **组合优势**：Âtree(Hi) = ÂIntra-tree(Hi) + ÂInter-tree(Hi)\n\n这种设计将**轨迹级稀疏信号转化为步骤级过程信号**，无需额外监督。\n\n### 步骤三：理论验证与连接\n作者通过理论分析建立了方法与现有技术的联系：\n- 证明在二元偏好设置下，树内GRPO与步骤级DPO具有相同的梯度结构\n- 揭示Tree-GRPO隐式执行步骤级偏好优化，继承了步骤级DPO的关键属性\n- 这为方法提供了理论支撑，解释了为什么树结构能产生有效的过程监督\n\n## 四、创新思路的演进脉络\n\nTree-GRPO的提出体现了清晰的创新演进路径：\n\n1. **从链式到树式**：突破传统链式rollout思维，转向树结构采样，解决效率问题\n2. **从token级到步骤级**：将树节点从token/句子级别提升到完整的智能体交互步骤级别，增强语义完整性\n3. **从轨迹级到步骤级**：利用树结构将稀疏的轨迹级监督转化为细粒度的步骤级监督，解决稀疏性问题\n4. **从显式到隐式**：无需额外监督，仅通过结果奖励和树结构隐式构建步骤级偏好学习目标\n\n这一演进路径展示了作者如何通过深入分析问题本质，找到树结构这一关键切入点，并逐步构建出既解决效率问题又解决监督问题的完整解决方案。\n\n## 五、核心思想总结\n\nTree-GRPO的核心创新在于**树结构的双重利用**：\n1. **效率维度**：通过共享前缀提高rollout效率，在有限预算下获得更多训练样本\n2. **监督维度**：利用树结构自然构建步骤级过程监督，将稀疏结果奖励转化为细粒度训练信号\n\n这种双重利用使Tree-GRPO在解决LLM智能体RL两大挑战方面取得了突破，为高效、稳定的智能体训练提供了新思路。作者通过将树搜索与智能体步骤的有机结合，实现了\"一石二鸟\"的创新效果。"
                },
                {
                    "title": "GRPO is Secretly a Process Reward Model",
                    "arxiv_id": "2509.21154",
                    "authors": "Michael Sullivan",
                    "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是证明GRPO强化学习算法实际上隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出了改进算法λ-GRPO，从而提高LLM在推理任务上的性能。从本质上看，论文关注的是改进LLM的训练方法，特别是通过强化学习优化来增强模型的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文明确涉及LLMs、reasoning和reinforcement learning等正面指标，且没有被排除标准所涵盖（如多模态、特定应用领域或模型可靠性应用层面）。论文不是将LLM作为工具应用于特定领域，而是致力于提升模型本身的基础推理能力，因此符合研究范围。",
                    "summary2": "本文旨在揭示GRPO算法隐含的进程奖励模型(PRM)结构并改进其性能。针对GRPO训练过程中的非均匀进程步骤分布问题，我们提出了λ-GRPO方法，通过引入PRM感知的归一化因子来修正损失函数。在DeepSeek-R1-Distill-Qwen-1.5B和Llama-3.2-1B-Instruct模型上，使用OpenRS数据集进行的实验表明，λ-GRPO在验证准确性和AIME24、MATH-500等推理基准测试上均优于标准GRPO，且训练速度提升约2倍。",
                    "summary_translation": "我们理论上证明了GRPO RL算法在关于补全间标记序列组内重叠的特定假设下，会诱导出一个非平凡的过程奖励模型（process reward model, PRM）。随后，我们通过实证表明这些假设在实际条件下得到满足：GRPO确实会诱导出一个非平凡的PRM。利用GRPO-as-a-PRM（将GRPO视为PRM）的框架，我们发现了GRPO目标中的一个缺陷：非均匀分布的过程步骤（process steps）在不同条件下会同时阻碍探索（exploration）与利用（exploitation）。我们提出了一种简单的算法修改以缓解这一缺陷（$\\lambda$-GRPO），并证明使用$\\lambda$-GRPO训练的大语言模型（LLMs）比使用标准GRPO训练的模型在验证准确率（validation accuracy）和下游推理任务（downstream reasoning tasks）上表现更佳，且能更快达到峰值性能。我们的结果对GRPO中使用昂贵且显式定义的PRMs（explicitly-defined PRMs）的优势提出了质疑：我们证明可以利用原始GRPO算法（vanilla GRPO algorithm）中隐藏的、内置的PRM结构来提升模型性能，而对训练时间和成本的影响微乎其微。",
                    "inspiration_trace": "# GRPO隐含过程奖励模型：逻辑演进分析\n\n## 1. 研究背景与初始挑战\n\n论文的出发点源于强化学习在语言模型推理任务中的应用挑战：\n\n- **过程奖励模型（PRMs）的价值与局限**：PRMs能够对中间推理步骤进行细粒度奖励分配，显著提升多步推理性能。然而，训练神经PRMs需要昂贵的步骤级人工标注，且容易受到奖励黑客攻击，限制了其在实际中的应用。\n\n- **GRPO算法的特性**：Group Relative Policy Optimization（GRPO）作为PPO的变体，通过消除评论家模型和广义优势估计（GAE）来简化训练流程，降低内存消耗。这一特性使其在工具使用、RLHF和数学推理等领域得到广泛应用，但也导致GRPO通常不被视为适合与PRMs结合的算法。\n\n作者面临的初始挑战是：如何在GRPO框架下获得PRMs的细粒度奖励优势，同时避免传统PRMs的高成本和脆弱性？\n\n## 2. 关键洞察：GRPO隐含PRM结构\n\n论文的核心突破来自于对GRPO算法本质的重新理解：\n\n- **理论假设与框架构建**：作者首先建立了两个关键假设——使用DAPO token级策略梯度目标，以及设置每批更新迭代次数μ=1。在这些假设下，GRPO损失函数得到简化，便于理论分析。\n\n- **过程集（Process Sets）概念引入**：作者创新性地定义了\"过程集\"概念——组内共享相同初始子轨迹的轨迹子集。每个过程集对应一个\"过程步骤\"，形成树状结构B(G)。这一概念框架成为连接GRPO与PRM的桥梁。\n\n- **理论证明**：作者通过严格的数学推导证明了一个关键定理：标准GRPO目标函数（使用结果级奖励）实际上等同于一个PRM感知的RL目标函数，其中步骤级奖励和优势是通过蒙特卡洛估计从结果级奖励中派生出来的。这表明GRPO\"秘密地\"实现了一个PRM。\n\n这一洞察颠覆了对GRPO的传统理解，揭示了算法表面简单性下隐藏的复杂结构。\n\n## 3. 实证验证：非平凡PRM的存在\n\n理论洞察需要实证支持，作者设计了精巧的实验：\n\n- **实验设计**：训练两个DeepSeek-R1-Distill-Qwen-1.5B模型，使用标准GRPO算法，并分析生成的B(G)树结构。通过测量\"路径深度\"和\"中间比例\"作为B(G)结构复杂性的代理指标。\n\n- **关键发现**：随着验证奖励的饱和，路径深度和中间比例急剧增加，表明随着模型收敛到局部最优策略，出现了越来越丰富的PRM诱导结构。在6,700个B(G)结构中，只有12个是平凡的（约0.2%），证明了GRPO在现实条件下确实诱导出非平凡的PRM。\n\n这一实证验证将理论洞察转化为可靠的事实，为后续分析奠定了基础。\n\n## 4. 问题识别：GRPO隐含PRM的缺陷\n\n发现隐含结构后，作者进一步分析了其潜在问题：\n\n- **过程集大小的不均衡影响**：通过将GRPO目标视为过程集分区，作者发现每个轨迹对损失的贡献与其所属过程集的大小|λ|成正比。这种缩放机制会损害探索（当过程优势为负时）和利用（当过程优势为正时）。\n\n- **具体问题示例**：作者通过图1中的具体例子说明，即使某个轨迹具有最高奖励，如果它所属的过程集平均奖励较低，该轨迹的概率也会被降低，且这种降低会被过程集大小放大，从而阻碍对高奖励轨迹的利用。\n\n这一分析揭示了GRPO隐含PRM的结构性缺陷，为改进指明了方向。\n\n## 5. 解决方案：λ-GRPO的提出\n\n基于对问题的深入理解，作者提出了一个简洁而有效的解决方案：\n\n- **核心思想**：通过将每个token级损失除以|λ(i,t)|，来抵消过程集大小的影响，使每个过程集对损失的贡献相等。\n\n- **算法修改**：提出了λ-GRPO目标函数，其中每个token的损失项被缩放1/|λ(i,t)|，其中λ(i,t)是包含该token的过程集。这一修改几乎不增加计算成本，因为只是利用训练期间自然发生的B(G)结构。\n\n这一解决方案体现了\"简单即美\"的设计哲学，通过最小化修改解决了核心问题。\n\n## 6. 验证与意义：λ-GRPO的有效性\n\n作者通过全面的实验验证了所提出方法的有效性：\n\n- **实验结果**：所有四个λ-GRPO模型在更少的训练步骤内达到比GRPO对应模型更高的验证准确率（平均提升超过10%，且训练步骤减半）。在下游推理任务上，λ-GRPO模型在20个评估指标中有15个优于标准GRPO。\n\n- **更广泛意义**：这些结果质疑了为GRPO使用专门的PRMs的必要性，表明可以利用基于结果的GRPO算法中已有的PRM结构，而不是使用昂贵的显式定义PRMs。\n\n## 7. 逻辑演进总结\n\n作者的思想演进呈现了一条清晰的路径：从对现有算法的深入理解出发，发现其隐含特性（GRPO隐含PRM），识别潜在问题（过程集大小不均衡），提出改进方案（λ-GRPO），并验证其有效性。这一过程展示了如何通过理论洞察和实证分析的结合，推动算法的优化与创新，同时保持计算效率。最终，这项工作不仅改进了GRPO算法，还为我们理解强化学习算法的隐含结构提供了新的视角。"
                },
                {
                    "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
                    "arxiv_id": "2509.21091",
                    "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
                    "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文的本质是改进LLM的基础推理能力。论文提出了Best-of-∞方法，通过多数投票和自适应生成方案来提高LLM在测试时的推理性能。这是一种通用的方法论研究，旨在增强LLM的基础推理能力，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：关注推理能力(reasoning)，通过多数投票机制提高模型的问题解决能力 论文虽然未涉及强化学习或新兴智能体范式，但其核心关注点与通用推理能力直接相关。 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不主要关注模型可靠性问题（如水印、安全等） 第四步：特殊和模糊情况——论文不涉及需要特殊判断的情况。它提出的是一种通用的测试时间计算分配方法，用于提高LLM的推理能力，而非针对特定领域的应用。 核心贡献：论文提出了一种自适应生成方案，通过基于答案一致性选择N值来有效分配推理时间计算，并将框架扩展到多个LLMs的加权集成，从而提高模型的通用推理能力。这种方法论研究直接服务于提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在 [解决大语言模型测试时计算的最优分配问题]。针对 [多答案生成和选择场景]，我们提出了一种 [基于贝叶斯因子的自适应采样和LLM加权集成方法]，并在 [多个重推理问题集和LLMs] 上通过 [准确率和计算效率] 验证了其有效性。",
                    "summary_translation": "我们研究基于多数投票的大语言模型（Large Language Models, LLMs）的最佳N选（best-of-$N$）方法。特别地，我们分析了$N \\to \\infty$的极限情况，并将其表示为最佳无穷选（Best-of-$\\infty$）。尽管该方法在极限情况下取得了令人印象深刻的性能，但它需要无限的测试时间预算。为解决这一问题，我们提出了一种自适应生成方案，该方案基于答案一致性选择$N$，从而有效分配推理时间计算。除自适应性外，我们将此框架扩展至多个LLMs的加权集成（weighted ensembles），证明此类混合模型可优于任何单个模型。最优集成权重被构建为混合整数线性规划（mixed-integer linear program）问题，并能够高效计算。大量实验证明了我们方法的有效性。",
                    "inspiration_trace": "# 论文核心方法逻辑链分析：从Best-of-N到Best-of-∞的演进\n\n## 一、问题识别与观察起点\n\n作者的研究起点源于对大语言模型(LLMs)测试时计算策略的观察：\n\n1. **基础现象观察**：Best-of-N策略（生成N个答案并通过多数投票选择）随着N增加，性能通常会提高（如图1所示）\n2. **理论极限的发现**：当N趋近于无穷大时，存在一个理论上的性能上限，作者将其定义为\"best-of-∞\"性能\n3. **核心矛盾**：best-of-∞性能虽然理想，但需要无限的测试时预算，在现实场景中不可行\n\n**关键问题**：如何在有限的测试时计算预算下，尽可能接近best-of-∞的性能？\n\n## 二、深入分析与关键洞察\n\n作者对best-of-N策略进行了深入分析，获得了几个关键洞察：\n\n### 1. 多数投票的本质优势\n- 多数投票不需要额外的建模或文本生成\n- 相比基于奖励模型的方法，对奖励攻击具有鲁棒性\n- 增加N的风险最小，不像奖励模型可能导致过拟合\n\n### 2. 概率分布视角的转变\n- 将LLM生成答案的过程视为从潜在答案分布中抽样\n- 认识到不同问题的答案分布可能截然不同（有的可能只有两个候选答案，有的可能有四个或更多）\n- 答案空间的支持(support)可能是未知的，这为后续方法选择提供了关键线索\n\n### 3. 自适应采样的可能性\n- 对于不同的问题，所需的样本数量可能不同\n- 简单的问题可能只需要少量样本就能确定多数答案\n- 困难的问题可能需要更多样本来确定多数答案\n\n**关键转折点**：从\"固定N\"思维转向\"自适应N\"思维，认识到计算资源应该根据问题难度动态分配\n\n## 三、理论框架的建立\n\n基于上述洞察，作者建立了理论框架：\n\n### 1. Best-of-∞的数学定义\n- 将best-of-∞定义为当N→∞时多数投票的性能极限\n- 这个极限性能对应于选择LLM答案分布中的真实众数(true mode)\n\n### 2. 统计建模选择\n- 由于答案分布的支持未知，采用非参数贝叶斯方法\n- 使用Dirichlet过程(Dirichlet Process)先验来建模未知的答案分布\n- Dirichlet过程可以自然处理有限和无限答案空间的情况，提供了统一的理论框架\n\n**理论创新点**：将LLM答案生成过程形式化为统计抽样问题，为后续方法提供了理论基础\n\n## 四、核心解决方案的提出\n\n基于理论框架，作者提出了两个核心解决方案：\n\n### 1. 自适应采样方案\n- **核心思想**：根据答案一致性自适应地选择样本数量N\n- **关键机制**：使用贝叶斯因子(Bayes factor)来衡量当前最频繁答案是真实众数的置信度\n- **停止条件**：当贝叶斯因子超过预设阈值或达到最大样本数时停止采样\n- **优势**：可以在不同问题上分配不同的计算资源，提高整体效率\n\n### 2. 多LLM集成扩展\n- **自然扩展**：将自适应采样方案扩展到多个LLM的集成\n- **关键洞察**：弱LLM如果具有互补优势，也可以对集成做出贡献（如论文中GPT-OSS-20B和Nemotron-Nano-9B-v2的例子）\n- **优化方法**：提出最优加权LLM集成方法，将权重优化问题转化为混合整数线性规划(MILP)问题\n\n**方法创新点**：从单一LLM的固定采样转向多LLM的自适应加权集成，同时考虑了计算效率和模型互补性\n\n## 五、理论保证与优化\n\n作者为提出的解决方案提供了理论保证：\n\n### 1. 一致性定理\n- 证明当最大样本数Nmax和贝叶斯因子阈值B足够大时，算法性能收敛到best-of-∞性能\n- 这意味着算法能够以概率1返回真实众数答案\n\n### 2. 权重优化方法\n- 证明对于best-of-∞性能，LLM集成的权重优化可以简化为MILP问题\n- 这与有限N情况下的优化不同，后者需要考虑大量组合，通常是不可行的\n- 提出\"最大间隔\"(max-margin)解决方案，在有限N情况下也能表现良好\n\n**理论贡献**：将极限理论应用于有限样本情况，实现了理论与实践的结合，为方法提供了坚实的理论基础\n\n## 六、实验验证与效果确认\n\n作者通过大量实验验证了所提方法的有效性：\n\n### 1. 自适应采样的有效性\n- 证明自适应方法可以用更少的样本达到与固定样本方法相同的准确率\n- 在计算资源使用上更加高效（如图4所示，自适应方法减少了2-5倍的计算量）\n\n### 2. LLM集成的优势\n- 证明最优加权LLM集成可以超过任何单个LLM的性能\n- 展示了不同LLM之间的互补性（如GPT-OSS-20B和Phi-4-reasoning的集成达到93.3%，超过单个模型的90.0%和83.3%）\n\n### 3. 权重学习的泛化能力\n- 证明即使在少量训练问题上学习权重，也能接近最佳性能\n- 展示了权重的迁移学习能力\n\n**实验贡献**：通过大规模实验（11个LLMs和4个推理问题集，每个LLM-问题集组合至少80次生成）验证了方法的有效性和效率\n\n## 七、思想演进的关键转折点\n\n在整个逻辑链中，有几个关键的思想转折点：\n\n1. **从固定N到自适应N**：传统方法固定所有问题的样本数量N，作者创新地提出根据问题难度自适应调整N，这是提高效率的关键\n\n2. **从单一LLM到多LLM集成**：不仅考虑单个LLM的自适应采样，还扩展到多个LLM的集成，认识到不同LLM可能具有互补优势\n\n3. **从有限N到极限N→∞**：考虑N→∞的极限情况，这简化了权重优化问题，然后将极限理论应用于有限样本情况\n\n4. **从启发式方法到理论保证**：不仅提出启发式方法，还提供了理论保证，使用贝叶斯统计和优化理论为方法提供坚实基础\n\n## 八、总结：完整的逻辑链\n\n作者的思考逻辑链可以概括为：\n\n**问题识别** → **深入分析** → **理论框架** → **解决方案** → **理论保证** → **实验验证**\n\n1. 从best-of-N策略的性能提升与无限计算需求之间的矛盾出发\n2. 通过概率分布视角和自适应采样的洞察，重新定义问题\n3. 建立基于Dirichlet过程的统计理论框架\n4. 提出自适应采样和多LLM集成的核心解决方案\n5. 提供一致性定理和优化方法的理论保证\n6. 通过大规模实验验证方法的有效性和效率\n\n这一完整的逻辑链体现了作者从问题观察到理论创新，再到方法提出和验证的系统性思考过程，最终实现了在有限计算预算下接近best-of-∞性能的目标。"
                },
                {
                    "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
                    "arxiv_id": "2509.21044",
                    "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
                    "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该保留。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何影响大语言模型(LLM)的内部工作机制，特别是激活强度和多样性的变化。论文不是将LLM作为工具应用到特定领域，而是研究LLM本身的内部机制如何通过RL微调得到改善，这属于改进LLM基础能力的研究，符合保留标准。 其次，从正面指标分析： - 核心概念：论文明确研究Large language models (LLMs)，符合。 - 能力方向：虽然论文没有直接研究推理、规划或问题解决，但它研究了RL微调如何增强LLM的内部机制，并提到这种变化提高了模型的泛化能力，这与通用推理能力相关，部分符合。 - 训练方法：论文核心研究的就是强化学习(RL)微调，包括PPO、GRPO和DPO等方法，完全符合。 第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊和模糊情况处理上，论文研究了LLM的内部机制，可以看作是对模型可解释性的一种探索，通过分析激活强度和多样性来解释为什么RL微调能提高模型性能，这属于增强模型内在可解释性的研究，应该保留。 论文的核心贡献是揭示了强化学习微调如何系统性地改变LLM的内部电路，特别是增强激活强度和多样性，从而提高模型的泛化能力。这有助于理解为什么RL微调能够提升LLM的通用能力，为改进LLM的推理能力提供了理论基础，符合研究目标。",
                    "summary2": "本文旨在解决强化学习微调如何影响大型语言模型内部电路机制的问题。针对多个LLM模型家族和数学问题解决任务，我们提出了一种基于边缘归因修补(EAP)的分析框架，并在GSM8K、MATH和College Math数据集上通过激活强度、信息复杂度和分布峰度等指标验证了其有效性。研究发现RL微调增强了模型内部激活强度和多样性，而DPO训练则表现不一致，揭示了在线RL与偏好优化方法间的方法论差异。",
                    "summary_translation": "大型语言模型（large language models, LLMs）通过大规模预训练获取广泛的先验知识，并可以通过监督微调（supervised fine-tuning, SFT）或基于强化学习（reinforcement learning, RL）的后训练进一步增强。越来越多的证据表明，强化学习微调能够提升大型语言模型的能力，超越仅使用监督微调所达到的效果。然而，强化学习微调为何能够增强具有不同内在特征的各种大型语言模型能力的潜在机制仍未得到充分探索。在本研究中，我们从先前关于边缘归因修补（edge attribution patching, EAP）的工作中汲取灵感，以研究大型语言模型在强化学习微调前后的内部差异。我们对多个模型族的分析显示了在线强化学习（online RL）后训练的两个稳健效果：（i）激活强度的整体增加，表明更多的内部通路被激活且其信号变得更强；（ii）激活模式的多样性增加，这反映在更高的熵和更分散的边缘分布上。这些变化表明，强化学习重塑了信息流，使其既更加冗余又更加灵活，这可能解释了其在泛化方面的优势。值得注意的是，使用直接偏好优化（Direct Preference Optimization, DPO）微调的模型偏离了这些趋势，与基于PPO和GRPO的训练相比，表现出明显较弱或不一致的内部变化。总体而言，我们的发现提供了关于强化学习微调如何系统性改变大型语言模型内部电路的统一视角，并强调了在线强化学习和基于偏好的方法之间的方法论区别。我们的代码在 https://anonymous.4open.science/r/llm_rl_probing_analysis-F673 上开源。",
                    "inspiration_trace": "# 论文核心方法逻辑链推演：强化学习微调增强LLM内部电路的激活强度与多样性\n\n## 一、面临的挑战：理解RL微调的内部机制\n\n作者首先识别了研究领域的核心问题：\n\n1. **现象与机制的脱节**：虽然大量实证研究表明强化学习(RL)微调能提升大语言模型(LLMs)的能力，超越监督微调(SFT)的效果，但这些研究主要集中在模型外部行为变化上，对\"为什么RL微调有效\"的内部机制理解严重不足。\n\n2. **研究路径的分离**：作者观察到两个研究领域平行发展却缺乏交集：\n   - RL效果的外部评估研究\n   - LLMs内部机制的分析研究\n\n3. **方法迁移的困难**：RL微调主要目标是增强LLMs解决复杂任务的能力，这使得将在简单问题上开发的分析策略直接迁移到研究RL诱导的真实问题解决能力提升变得非平凡。\n\n这一挑战构成了研究的出发点：如何构建一个系统性框架，揭示RL微调影响LLMs内部机制的方式？\n\n## 二、关键洞察：图论视角与边缘归因的价值\n\n作者从现有研究中获得了几个关键洞察，为解决方案奠定基础：\n\n1. **图论视角的启发**：先前研究已表明，从图论角度研究LLMs的内部残差路径是有价值的。LLMs可被视为有向无环图(DAG)，其中节点对应子模块（注意力块或前馈块），边缘编码残差信息路径。\n\n2. **边缘归因修补(EAP)的潜力**：作者注意到EAP方法能够为边缘或子模块分配重要性分数，揭示决定LLMs能力的内部功能电路。这种方法可能成为连接外部行为与内部机制的桥梁。\n\n3. **不同RL方法的潜在差异**：作者推测不同的RL方法（如在线RL与基于偏好的方法）可能对LLMs内部机制产生不同影响，这为后续发现DPO与其他RL方法之间的差异埋下伏笔。\n\n这些洞察引导作者思考：能否利用EAP框架来系统分析RL微调前后LLMs的内部差异？\n\n## 三、解决方案的演进：从理论框架到实验设计\n\n基于上述洞察，作者逐步构建了解决方案：\n\n### 第一步：构建图论视角的分析框架\n\n作者首先将LLMs形式化为图结构，将模型内部信息流动抽象为节点和边缘的网络。这种抽象使得从网络流角度和基于电路的可解释性角度分析模型成为可能，为后续分析提供了理论基础。\n\n### 第二步：选择高效的边缘重要性评估方法\n\n作者面临一个关键选择：如何高效评估边缘重要性？\n\n- **ACDC方法**：通过移除给定边缘并测量损失变化来评估边缘重要性，概念直观但计算成本高（每个边缘需要两次前向传播）。\n- **EAP方法**：通过基于梯度的线性化来估计损失扰动，计算效率高（单次前向和后向传播可同时计算所有边缘重要性）。\n\n考虑到分析大规模LLMs的计算成本，作者选择了EAP方法，这一选择体现了实用性与理论性的平衡。\n\n### 第三步：设计系统性实验框架\n\n为确保分析的有效性和公平性，作者设计了精细的实验控制：\n\n1. **问题过滤**：只选择两个模型都正确回答的问题，控制答案长度，避免极短或极长答案带来的偏差。\n2. **令牌截断和自熵计算**：定义截断长度，只使用每个序列的前Tcut个令牌，计算模型相对于自身输出的自熵。\n\n这些设计确保了比较的公平性和结果的可解释性。\n\n### 第四步：定义多维量化指标\n\n为全面捕捉RL微调的影响，作者设计了三个互补指标：\n\n1. **激活强度**：量化所有边缘权重的平均幅度，捕捉有多少通路被激活以及激活强度。\n2. **信息复杂性**：计算所有边缘权重绝对值的香农熵，捕捉边缘激活的异质性和不可预测性。\n3. **分布峰度**：量化边缘权重分布的整体形状和稳定性。\n\n这三个指标从不同角度刻画了RL微调对LLMs内部机制的影响，形成了完整的评估体系。\n\n### 第五步：多样化实验验证\n\n为确保结论的普适性，作者在四个不同系列的LLMs对上进行了实验，涵盖了不同的架构和训练语料库，并在三个数学基准测试上进行了分析。这种多样化设计增强了研究结论的可靠性和泛化能力。\n\n## 四、核心发现与理论贡献\n\n通过上述方法，作者得出了两个核心发现：\n\n1. **RL微调增强了内部边缘连接的激活强度**：在线RL微调增加了模型中活跃信息边缘的数量和强度，表明更多内部通路被激活且信号更强。\n\n2. **RL微调多样化了这些信息通路上的激活模式**：在线RL微调使激活模式更加多样化，分布更加均匀，表现为更高的熵和更集中的边缘分布。\n\n此外，作者还发现了一个重要例外：使用DPO算法进行微调的模型没有表现出与其他在线RL模型一致的变化，这突显了静态偏好的优化与动态在线RL方法之间的方法论差异。\n\n## 五、理论解释：信息流重塑与方法论差异\n\n作者对这些发现提供了深入的理论解释：\n\n1. **信息流的重塑机制**：RL微调重塑了信息流，使其既更加冗余又更加灵活，这种双重特性可能解释了其在泛化方面的优势。冗余性增强了鲁棒性，而灵活性则提高了适应性。\n\n2. **在线RL与静态偏好优化的本质区别**：作者从统一框架的角度解释了为什么DPO与其他RL方法不同：\n   - DPO的训练分布是静态的，类似于SFT，无法激活更广泛的神经通路\n   - GRPO和PPO涉及与不断演变的策略的在线交互，能够持续探索新的信息路径\n\n这一解释不仅统一了观察到的现象，还为理解不同RL方法的内在差异提供了新视角。\n\n## 六、逻辑链总结：从问题到解决方案的完整演进\n\n作者的思想演进可总结为以下逻辑链：\n\n1. **问题识别**：RL微调提升LLMs能力的内部机制尚不清楚，外部行为研究与内部机制分析存在脱节。\n\n2. **关键洞察**：图论视角和边缘归因方法可能为理解RL微调的内部机制提供有效工具。\n\n3. **方法选择**：选择EAP作为高效的边缘重要性评估方法，设计系统性实验框架和多维量化指标。\n\n4. **实验验证**：在多个模型系列和数据集上进行实验，确保结论的普适性。\n\n5. **发现与解释**：发现RL微调增强了激活强度和多样性，并从信息流重塑和在线RL与静态偏好优化的区别角度提供理论解释。\n\n6. **意义与启示**：为理解RL微调的内部机制提供统一视角，为未来LLMs和后训练方法的发展提供指导。\n\n这一逻辑链体现了作者从问题识别到解决方案提出，再到实验验证和理论解释的完整思考过程，展现了清晰的学术思维演进，为理解RL微调如何系统性地改变LLMs的内部电路提供了新的视角。"
                },
                {
                    "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
                    "arxiv_id": "2509.20386",
                    "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
                    "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出Dynamic ReAct方法，用于提升ReAct智能体在大规模工具环境中的工具选择能力。根据第一步核心判断，论文本质上是关于改进LLM的基础能力，特别是增强其工具使用这一通用推理能力，而非将LLM应用到特定领域。论文符合多个正面指标：涉及LLM核心概念、关注推理和问题解决能力、探索LLM-based agents和tool use等新兴范式。同时，论文不符合任何排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。特别地，论文提出的是一种通用的工具选择方法，使LLM智能体能够高效处理大量工具，这直接增强了LLM的通用推理和问题解决能力，而非针对特定领域的应用。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在 [解决大型语言模型代理在处理大规模工具集时的选择和加载问题]。针对 [包含数百或数千工具的MCP环境]，我们提出了一种 [Dynamic ReAct方法，结合元工具和优化向量检索实现动态工具选择]，并在 [自建工具注册库] 上通过 [工具加载减少率和任务完成准确率] 验证了其有效性。",
                    "summary_translation": "我们提出了Dynamic ReAct，这是一种新颖的方法，使ReAct代理能够高效地操作超出大型语言模型（large language models）上下文记忆限制的广泛模型控制协议（Model Control Protocol, MCP）工具集。我们的方法解决了在包含数百或数千个可用工具的环境中的工具选择基本挑战，在这些环境中同时加载所有工具在计算上是不可行的。我们提出并评估了五种不同的架构，这些架构逐步完善工具选择过程，最终形成一种搜索加载机制（search-and-load mechanism），该机制以最小的计算开销实现智能工具选择。我们的实验结果表明，所提出的方法将工具加载减少了高达50%，同时保持了任务完成准确性，为能够动态适应不同任务环境的真正通用人工智能代理（general-purpose AI agents）的发展推进了道路。",
                    "inspiration_trace": "# Dynamic ReAct方法逻辑演进分析\n\n## 一、核心挑战的识别\n\n作者首先敏锐地捕捉到了LLM代理领域的一个根本性矛盾：**工具生态的无限扩展与LLM上下文窗口的有限性之间的冲突**。这一挑战具体表现为：\n\n1. **规模不匹配问题**：当工具注册表从几十个扩展到成百上千个时，传统\"全部加载\"模式在计算上变得不可行。\n   \n2. **检索精度困境**：简单的语义搜索难以在庞大的工具库中精确定位到最相关的工具，往往需要牺牲精确度来提高召回率。\n\n3. **多应用协调难题**：复杂任务通常需要跨多个应用的工具协同工作，而现有检索机制往往局限于单一应用领域。\n\n这些挑战构成了作者研究的出发点，也暗示了静态工具管理模式的局限性，为动态工具选择方法的需求奠定了基础。\n\n## 二、关键洞察的形成\n\n通过对问题的深入分析，作者形成了几个关键洞察，这些洞察构成了解决方案的理论基础：\n\n### 1. 动态选择的必然性\n作者认识到，解决工具规模问题的关键不在于扩大LLM的上下文窗口，而在于**从\"全部加载\"转向\"按需加载\"的范式转变**。这一洞察直接导向了动态工具选择的核心思想。\n\n### 2. 元工具的杠杆作用\n作者发现，要实现动态工具管理，需要引入**专门用于管理其他工具的\"元工具\"**，作为LLM与工具库之间的智能中介。这一洞察突破了传统工具使用的思维框架。\n\n### 3. 检索与加载的解耦\n作者意识到，工具的**发现过程**和**使用过程**应该被明确分离。这种解耦允许系统先广泛检索候选工具，然后有选择性地加载最相关的子集。\n\n### 4. 上下文增强的价值\n通过实验验证，作者发现**丰富工具描述的语义上下文**能显著提高检索准确性，这一洞察为优化向量检索策略提供了方向。\n\n## 三、解决方案的迭代演进\n\n基于上述洞察，作者通过五次架构迭代，逐步完善解决方案，展现了一个清晰的思维演进过程：\n\n### 第一代：直接语义搜索\n**思路**：直接将用户查询作为向量搜索的输入，检索相关工具并绑定到LLM。\n**局限**：缺乏特异性，需要设置较大的k值导致上下文饱和；对于复杂查询结果不完整。\n**启示**：单纯的语义搜索不足以解决大规模工具选择问题，需要引入更智能的检索机制。\n\n### 第二代：元工具辅助查询构建\n**思路**：将向量搜索作为元工具暴露给LLM，让LLM先构建更精确的搜索查询。\n**进步**：查询更加针对性和具体，分解为原子查询提高了检索质量。\n**局限**：仍需检索大量工具，增加了计算负担。\n**启示**：LLM参与查询构建是有效的，但工具选择过程需要进一步优化。\n\n### 第三代：搜索与加载分离\n**思路**：引入两个专门的元工具——search_tools和load_tools，将工具检索和加载明确分离。\n**突破**：实现了\"先广泛搜索，再精准加载\"的两阶段策略，显著减少了实际加载的工具数量。\n**优势**：平衡了检索广度和加载精度，同时控制了计算开销。\n**局限**：仍存在一些不相关的应用结果。\n**启示**：分离检索和加载是解决工具选择问题的有效路径，但可以进一步优化检索策略。\n\n### 第四代：应用感知架构\n**思路**：增加应用层级的检索，先识别相关应用，再在特定应用中搜索工具。\n**进步**：通过应用过滤减少了不相关的匹配，提高了工具选择的针对性。\n**局限**：增加了系统复杂性和调用开销，功能上与第三代有重叠。\n**启示**：应用层级的过滤有价值，但可能不是最佳实现方式。\n\n### 第五代：固定工具集架构\n**思路**：使用固定元工具集，不直接绑定MCP工具，而是通过间接调用访问功能。\n**创新**：实现了上下文的一致性，为缓存优化提供了可能。\n**局限**：性能下降，工具使用流程复杂化。\n**启示**：虽然缓存有价值，但LLM对直接绑定工具的优化程度更高，间接调用可能不是最佳选择。\n\n## 四、整合优化与最终方案\n\n通过架构迭代，作者不仅比较了不同设计思路，还进行了多项关键优化：\n\n### 1. 检索策略优化\n- 比较了应用过滤与纯查询搜索，发现后者在大多数情况下更有效\n- 实验了不同嵌入模型，发现上下文增强的嵌入显著提高检索准确性\n- 评估了混合搜索方法，但最终选择了纯向量方法以保证精度\n\n### 2. 系统交互优化\n- 通过输出格式改进（如工具加载提醒）提高系统可靠性\n- 引入用户特定应用连接信息解决功能重叠问题\n- 添加默认工具防止无效搜索\n\n### 3. 最终方案的确立\n基于全面的实验评估，作者确定**\"搜索和加载架构\"结合优化的向量检索**为最佳解决方案。这一选择体现了作者在多个目标间的平衡：\n- **效率**：最小化工具加载数量（减少50%）\n- **准确性**：保持任务完成准确率\n- **可扩展性**：支持数千甚至更多工具\n- **实用性**：适合生产环境部署\n\n## 五、思想演进的核心脉络\n\n从挑战到解决方案，作者的思想演进呈现出一条清晰的脉络：\n\n**从静态到动态**：突破了传统静态工具加载的思维限制，认识到动态选择的必要性。\n\n**从整体到分层**：将工具选择过程分解为多个层级（应用层、工具层），实现了更精细的管理。\n\n**从直接到间接**：通过元工具的引入，建立了LLM与工具库之间的智能中介，实现了更高效的工具管理。\n\n**从单一到复合**：结合语义搜索、元工具管理和上下文增强等多种技术，构建了复合型解决方案。\n\n这一思想演进不仅解决了具体的技术问题，也为LLM代理在大规模工具环境中的运行提供了新的范式，体现了作者在复杂系统设计中的深刻洞察力和创新思维。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 18,
            "papers": [
                {
                    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
                    "arxiv_id": "2509.21319",
                    "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
                    "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的强化学习方法RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进大语言模型的后训练过程，特别是奖励模型的训练。该方法结合了人类反馈(RLHF)和可验证奖励(RLVR)的优点，旨在解决RLHF的可解释性和奖励黑客问题，以及RLVR仅关注正确性的局限性。从本质上看，这属于改进LLM基础能力的范畴，目的是提高模型的对齐质量和响应质量，而不是将LLM作为工具应用到特定领域。论文没有涉及多模态内容或特定应用领域，而是关注通用的LLM训练方法。虽然论文没有直接强调推理能力，但改进奖励模型会间接提升模型的整体能力，包括推理能力。此外，论文还展示了使用RLBFF训练的模型在多个基准测试上的优异性能，进一步证明了其有效性。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决RLHF缺乏可解释性和RLVR范围有限的问题。针对LLM后训练中的反馈信号，我们提出了一种RLBFF（二元灵活反馈）方法，从自然语言反馈中提取可二元评估的原则，将奖励建模作为蕴含任务。在RM-Bench（86.2%）、JudgeBench（81.4%，排名第一）和PrincipleBench上验证了其有效性，并展示了通过RLBFF对齐的Qwen3-32B模型在通用对齐基准上匹配或超过o3-mini和DeepSeek R1性能，同时推理成本降低95%以上。",
                    "summary_translation": "Reinforcement Learning with Human Feedback (RLHF，人类反馈强化学习) 和 Reinforcement Learning with Verifiable Rewards (RLVR，可验证奖励强化学习) 是大型语言模型（LLM）后训练中使用的主要强化学习范式，各自具有独特优势。然而，RLHF 在可解释性和奖励黑客（reward hacking）方面存在困难，因为它依赖于通常缺乏明确标准的人类判断；而 RLVR 由于其专注于基于正确性的验证器（correctness-based verifiers），在应用范围上受到限制。我们提出了 Reinforcement Learning with Binary Flexible Feedback (RLBFF，二元灵活反馈强化学习)，它结合了人类驱动偏好的多功能性与基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的响应质量细微差别。RLBFF 从自然语言反馈中提取可以以二元方式回答的原则（例如信息准确性：是，或代码可读性：否）。这些原则随后可作为蕴含任务（entailment task）的基础来训练奖励模型（响应满足或不满足任意原则）。我们表明，以这种方式训练的奖励模型在数据匹配的情况下可以优于 Bradley-Terry 模型，并在 RM-Bench（86.2%）和 JudgeBench（81.4%，截至2025年9月24日排行榜第一）上取得顶级性能。此外，与 Bradley-Terry 模型不同，用户可以在推理时间（inference time）指定感兴趣的原则，以定制我们奖励模型的关注重点。最后，我们提出了一个完全开源的方案（包括数据），使用 RLBFF 和我们的奖励模型来对齐 Qwen3-32B，使其在 MT-Bench、WildBench 和 Arena Hard v2 等通用对齐基准测试上的性能匹配或超过 o3-mini 和 DeepSeek R1（推理成本低于5%）。",
                    "inspiration_trace": "# RLBFF方法提出的逻辑链分析\n\n## 一、问题识别：现有强化学习范式的局限性\n\n### 1.1 RLHF (Human Feedback)的缺陷\n作者首先观察到RLHF存在的核心问题：\n- **可解释性缺失**：人类判断通常缺乏明确标准，导致训练目标模糊\n- **奖励黑客问题**：模型可能利用与质量无关的特征（如响应长度、匹配用户信念）获取高奖励\n- **校准困难**：人类评分（如5分量表）难以跨人群校准，不同人对分数理解不一致\n\n### 1.2 RLVR (Verifiable Rewards)的局限\n同时，作者发现RLVR虽然解决了部分问题，但有自身局限：\n- **覆盖范围狭窄**：仅适用于易于验证正确性的领域（如数学、编程）\n- **召回率低**：无法识别等效正确答案（如\"3小时\"vs\"180分钟\"）\n- **灵活性不足**：只能评估正确性，无法捕捉其他质量维度\n\n## 二、关键洞察：从问题到本质的跨越\n\n### 2.1 洞察一：人类反馈背后的\"原则\"多样性\n作者通过观察发现：\n- 人类对响应的评价基于不同原则（如Reddit上最高赞评论可能是最幽默的，而StackExchange上则是最正确的）\n- 不考虑判断背后的原则会导致优化目标不清晰，降低训练效果\n- 明确原则可以使优化目标更加清晰和可解释\n\n### 2.2 洞察二：自然语言反馈中蕴含的丰富信息\n作者认识到：\n- 现有的自然语言反馈数据集（如HelpSteer3-Feedback）包含对响应多个方面的详细评价\n- 这些评价可以被提取为明确的\"原则\"（准确性、可读性、完整性等）\n- 每个原则可以二元方式（满足/不满足）进行评估，避免评分校准问题\n\n### 2.3 洞察三：二元评估的优势\n作者通过比较发现：\n- 相比Likert量表，二元评估减少了注释差异和主观性\n- 类似RLVR的二元奖励（正确/不正确），但可扩展到更广泛的原则\n- 更易于解释和验证，同时保持评估的精确性\n\n## 三、解决方案构建：RLBFF的逐步形成\n\n### 3.1 核心形式化：从观察到模型\n基于上述洞察，作者提出了核心形式化：\n- **扩展RLVR框架**：不仅评估正确性，而是评估任意原则的满足情况\n- **新格式定义**：给定提示、响应和原则，指示响应是否满足该原则\n- **灵活性设计**：允许用户在推理时指定感兴趣的原则，定制评估焦点\n\n### 3.2 数据转换：从自然语言到结构化原则\n作者面临数据挑战，但通过创新方法解决：\n- **识别可用资源**：发现HelpSteer3-Feedback数据集包含丰富的自然语言反馈\n- **开发提取方法**：使用LLM从反馈中提取原则和满足情况\n- **设计过滤机制**：\n  - 要求原则必须有文本证据支持，减少幻觉\n  - 排除\"部分满足\"的模糊情况，确保二元清晰性\n  - 使用嵌入相似性识别跨注释者的共识原则，提高数据质量\n\n### 3.3 奖励模型训练：从数据到能力\n作者构建了两种奖励模型：\n- **标量奖励模型**：高效预测给定原则下的满足情况（仅需1个token）\n- **生成式奖励模型**：通过推理过程评估复杂原则（适用于需要逐步推理的任务）\n\n### 3.4 评估框架：验证方法有效性\n为确保方法有效性，作者构建了全面评估体系：\n- **利用现有基准**：在RM-Bench和JudgeBench上评估性能\n- **创建新基准**：开发PrincipleBench专门评估奖励模型遵循特定原则的能力\n- **设计对比实验**：与Bradley-Terry模型、固定原则模型等多种基线比较\n\n## 四、方法优势论证：为什么RLBFF能桥接两种范式\n\n### 4.1 结合两种范式的优势\n作者通过多维度论证RLBFF的优势：\n- **广泛覆盖**：继承人类反馈的多样性，原则可包括人类重视的任何方面\n- **高可解释性**：每个奖励都有明确的原则依据，不再是黑盒\n- **高精确性**：通过识别特定原则建模，减少奖励黑客问题\n- **高召回率**：利用预训练LLM识别等效正确答案的能力\n\n### 4.2 灵活性与实用性\n作者特别强调RLBFF的实用价值：\n- **用户可控**：用户可在推理时指定感兴趣的原则，定制评估焦点\n- **推理高效**：标量模型仅需0.1秒/任务，比生成式模型快100倍以上\n- **成本效益**：对齐的模型在匹配专有模型性能的同时，推理成本低于5%\n\n## 五、实验验证：从理论到实践\n\n### 5.1 奖励模型性能验证\n作者通过实验证明：\n- RLBFF训练的奖励模型在RM-Bench(86.2%)、JudgeBench(81.4%)和PrincipleBench(91.6%)上达到最先进性能\n- 标量模型在效率上显著优于生成式模型，同时保持或提高性能\n- 即使在测试时使用固定原则，训练原则跟随模型也有益处\n\n### 5.2 模型对齐效果验证\n作者进一步验证了方法的实际应用价值：\n- 使用RLBFF对齐的Qwen3-32B在MT-Bench、Arena Hard v2和WildBench上匹配或超过专有模型性能\n- 推理成本显著低于比较模型，证明了方法的实用性和可扩展性\n\n## 六、总结：完整的逻辑演进链\n\n作者提出RLBFF的逻辑演进可概括为：\n1. **问题识别**：发现RLHF和RLVR各自局限，意识到需要结合两者优势\n2. **关键洞察**：认识到人类反馈背后的原则多样性和自然语言反馈的丰富信息\n3. **方法构建**：从形式化定义到数据转换，再到模型训练和评估框架设计\n4. **优势论证**：多维度证明RLBFF如何结合两种范式优势并克服各自局限\n5. **实验验证**：通过全面实验验证方法的有效性和实用性\n\n这一逻辑链条体现了作者从观察到洞察，从理论到实践的完整思考过程，最终形成了一种能够桥接人类反馈与可验证奖励的新范式。"
                },
                {
                    "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
                    "arxiv_id": "2509.21284",
                    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
                    "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是对思维链（Chain-of-Thought, CoT）的鲁棒性进行理论分析，研究输入扰动如何影响CoT输出的波动，并推导出输入扰动的上界及其与推理步骤数量、嵌入向量范数等的关系。从本质上看，这篇论文属于改进LLM基础能力的研究范畴，特别是针对推理过程中的鲁棒性问题。思维链（CoT）是提高大语言模型推理能力的关键技术之一，论文深入分析了CoT推理过程中的稳定性问题，这与增强LLM的通用推理能力直接相关。论文没有涉及任何特定应用领域，其研究是通用的，适用于CoT推理过程本身，不符合任何排除标准。虽然论文没有提出新的训练方法或新兴范式，但它通过理论分析深化了我们对LLM推理过程的理解，这种基础性的理论研究对于未来改进LLM的推理能力和鲁棒性具有重要意义，因此符合研究目标。",
                    "summary2": "本文旨在从理论上分析输入扰动对Chain-of-Thought (CoT)输出的影响，探索影响CoT鲁棒性的因素。针对CoT推理过程，我们提出了一种基于Lipschitz连续性的理论分析方法，推导了输入扰动对输出波动的上界，并在MATH、MMLU-Pro、GPQA数据集上通过Exact Match (EM)和Output Fluctuation (OF)指标验证了其有效性。实验证明CoT步骤与鲁棒性正相关，而嵌入向量范数与鲁棒性负相关，且基于理论分析的提示选择方法优于现有提示优化技术。",
                    "summary_translation": "现有研究表明，思维链（Chain-of-Thought, CoT）的输出受到输入扰动（input perturbations）的显著影响。尽管许多方法旨在通过优化提示（optimizing prompts）来减轻这种影响，但这些扰动如何影响CoT输出的理论解释仍是一个开放的研究领域。这一空白限制了我们对于输入扰动在推理过程（reasoning process）中如何传播的深入理解，并阻碍了提示优化方法的进一步改进。因此，在本文中，我们理论分析了输入扰动对CoT输出波动（fluctuation）的影响。我们首先在输出波动处于可接受范围内的条件下，推导出输入扰动的上界（upper bound），基于此我们证明：(i) 该上界与CoT中的推理步骤（reasoning steps）数量呈正相关（positively correlated）；(ii) 即使无限长的推理过程也无法消除输入扰动的影响。然后，我们将这些结论应用于线性自注意力（Linear Self-Attention, LSA）模型，该模型可视为Transformer的简化版本。对于LSA模型，我们证明输入扰动的上界与输入嵌入（input embedding）和隐藏状态向量（hidden state vectors）的范数呈负相关（negatively correlated）。为验证这一理论分析，我们在三个主流数据集（mainstream datasets）和四个主流模型（mainstream models）上进行了实验。实验结果与我们的理论分析一致，实证（empirically）证明了我们发现的正确性。",
                    "inspiration_trace": "# 论文核心方法的逻辑演进分析\n\n## 面临的挑战\n\n作者首先识别出思维链(CoT)研究中存在的核心矛盾与挑战：\n\n1. **有效性与脆弱性的悖论**：CoT作为提升大语言模型性能的有效方法，通过逐步推理显著提高了输出质量，但同时又表现出对输入扰动的高度敏感性——微小的输入变化可能导致输出结果的剧烈波动。\n\n2. **理论解释的缺失**：尽管已有多种提示优化方法试图减轻输入扰动的影响，但缺乏对这些扰动如何影响CoT输出的理论解释。这种理论空白限制了对扰动传播机制的深入理解，也阻碍了提示优化方法的系统性改进。\n\n3. **经验性研究的局限**：现有研究大多将CoT鲁棒性视为经验现象，缺乏对\"为什么\"和\"如何\"的理论探索，导致提示优化方法仅停留在试错性技术层面，缺乏理论指导。\n\n## 关键洞察\n\n通过理论分析，作者获得了几个关键洞察，这些洞察构成了其方法的理论基础：\n\n1. **CoT作为迭代过程的本质**：作者将CoT重新概念化为一个多步迭代过程，其中每一步的输出作为下一步的输入。这一视角转换使得数学分析成为可能，为后续理论推导提供了框架。\n\n2. **Lipschitz连续性的桥梁作用**：作者引入Lipschitz连续性假设，这一数学工具为连接输入扰动和输出波动提供了理论桥梁。该假设限制了模型输出的增长率，防止了爆炸性增加，为分析提供了可控的数学环境。\n\n3. **推理步骤与鲁棒性的非线性关系**：通过理论推导，作者发现了一个反直觉的洞察：虽然增加CoT推理步骤确实能减少输出波动，但这种减少存在极限——即使有无限多的推理步骤，也无法完全消除输入扰动的影响。这一发现挑战了\"更长推理总是更好\"的直观认识。\n\n4. **模型级因素的深层影响**：通过将理论应用于线性自注意力(LSA)模型，作者发现CoT鲁棒性不仅与推理步骤有关，还与模型内部因素密切相关，特别是输入向量和隐藏状态向量的范数与鲁棒性呈负相关关系。\n\n## 提出解决方案\n\n基于上述洞察，作者构建了一个完整的理论-实验框架来解决CoT鲁棒性问题：\n\n1. **理论框架的构建**：作者首先建立了一个形式化的理论框架，定义了输入扰动、输出波动和CoT推理过程等基本概念，为后续分析奠定了数学基础。\n\n2. **输出波动上界的推导**：在Lipschitz连续性假设下，作者推导出了给定输入扰动下输出波动的数学上界，并证明这一上界主要取决于推理步骤数量和扰动大小。这一理论结果解释了为什么增加推理步骤能提高鲁棒性。\n\n3. **输入扰动上界的推导**：考虑到实际应用中模型可以容忍一定程度的输出波动，作者进一步推导出了当输出波动在可接受范围内时，输入扰动的上界。这一结果为评估和优化CoT鲁棒性提供了量化标准。\n\n4. **模型级因素的具体化**：通过将理论应用于LSA模型，作者将抽象的理论具体化为可操作的模型级因素，证明输入扰动的上界与输入嵌入向量和隐藏状态向量的范数负相关，为实际应用提供了具体指导。\n\n5. **实验验证与理论修正**：作者在多个主流数据集和模型上进行了系统实验，验证了理论分析的正确性。实验结果不仅支持了理论预测，还揭示了理论模型与实际系统之间的差异，促使作者在附录中讨论了Transformer中非线性因素的影响。\n\n6. **理论指导的应用**：基于理论分析，作者提出了一种通过最大化输入扰动上界来选择提示的方法，将理论洞察转化为实际应用，取得了比现有提示优化方法更好的性能。\n\n这一完整的逻辑演进展示了作者如何从识别挑战出发，通过理论分析获得关键洞察，最终构建出既有理论深度又有实用价值的解决方案，形成了一个从理论到实践的完整研究闭环。"
                },
                {
                    "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
                    "arxiv_id": "2509.21212",
                    "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于改进大语言模型在长期对话中的记忆管理能力。论文专注于解决LLM处理超出上下文窗口的对话历史时的挑战，通过将对话表示为句子级图并有效检索相关信息，增强了LLM的长期记忆能力。虽然论文不直接针对推理、数学或逻辑能力，但长期记忆管理是支持复杂推理和多步对话的基础能力，属于提升LLM通用能力的范畴。论文关注的是通用对话智能体而非特定领域应用，不符合任何排除标准。论文中提到的\"long-term conversational agents\"属于LLM-based agents的新兴范式，符合正面指标。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标，应予以保留。",
                    "summary2": "本文旨在解决长期对话代理中的记忆碎片化和检索不连贯问题。针对超出LLMs上下文窗口的对话历史，我们提出了一种SGMem（句子图记忆）方法，将对话表示为分块单元内的句子级图，捕捉跨不同粒度的语义关联。在LongMemEval和LoCoMo数据集上通过Accuracy指标验证了其有效性，实验表明SGMem持续提高了准确性，优于强基线方法。",
                    "summary_translation": "长期对话代理需要有效的记忆管理来处理超出大型语言模型(LLMs)上下文窗口的对话历史。基于事实提取或摘要的现有方法减少了冗余，但难以组织和检索不同粒度对话和生成记忆中的相关信息。我们提出了SGMem (Sentence Graph Memory，句子图记忆)，它将对话表示为分块单元内的句子级图，捕捉跨回合级别、轮次级别和会话级别上下文的关联。通过结合检索到的原始对话与生成的记忆（如摘要、事实和见解），SGMem为LLMs提供连贯且相关的上下文用于响应生成。在LongMemEval和LoCoMo数据集上的实验表明，SGMem持续提高准确性，并在长期对话问答任务中优于强基线模型。",
                    "inspiration_trace": "# SGMem核心方法的逻辑演进分析\n\n## 一、面临的挑战：记忆管理的根本困境\n\n### 1. 记忆过载问题\n作者首先识别出长期对话代理面临的核心挑战：随着交互积累，存储内容的规模、复杂性和冗余性会超过代理的有效管理和检索能力，导致\"记忆过载\"。这一问题直接削弱了对话理解能力，限制了代理提供连贯和个性化回应的能力。\n\n### 2. 记忆碎片化困境\n作者进一步指出，现有方法（如摘要、提取和反思）虽然减少了冗余，却导致了\"记忆碎片化\"问题——相关信息分散在原始对话和派生片段中，阻碍了连贯检索。具体表现为：\n- 原始对话历史（轮次、回合、会话）与生成记忆（摘要、事实、见解）之间缺乏有效组织\n- 难以确定适当的粒度级别来检索原始对话历史\n- 难以在检索过程中有效整合生成记忆与原始历史\n\n### 3. 现有方法的局限性\n作者分析了现有两类主要方法的缺陷：\n- **基于块的方法**：虽然简单可扩展，但受限于粗粒度和碎片化检索\n- **基于图的方法**：如实体-关系图，依赖昂贵的LLM计算进行提取，同时丢弃了丰富的上下文信息\n\n## 二、关键洞察：寻找突破口\n\n### 1. 句子作为基本单位的独特价值\n作者的核心洞察是认识到\"句子\"作为对话交流基本单位的重要性：\n- 句子封装了语义连贯的陈述，同时足够细粒度以捕获上下文依赖关系\n- 与较粗粒度单位（轮次、回合、会话）相比，句子级表示能在原始对话历史和生成记忆间实现更精确对齐\n\n### 2. 图结构的关联建模能力\n作者洞察到图结构的独特价值：\n- 将句子结构化为图节点，可显式建模关联——无论是在对话段内还是跨对话段之间\n- 这种结构能减轻记忆碎片化并支持连贯检索\n\n### 3. 多层次记忆整合的必要性\n作者认识到需要同时考虑两种互补的记忆类型：\n- 原始对话历史（轮次、回合、会话）\n- 生成记忆（摘要、事实、见解）\n这两种记忆相互补充，提供更全面和连贯的上下文。\n\n### 4. 轻量级解决方案的需求\n作者意识到避免依赖昂贵LLM提取的重要性，转而使用标准句子分割工具构建句子图，使解决方案轻量级且易于部署。\n\n## 三、解决方案演进：从概念到实现\n\n### 1. 核心设计选择\n基于上述洞察，作者做出了关键设计选择：**在句子级别构建对话记忆结构**。这一选择既保持了语义完整性，又提供了足够的细粒度来捕获上下文依赖关系。\n\n### 2. 句子图记忆的构建逻辑\n作者逐步构建了SGMem框架：\n\n**第一步：分层解构对话**\n- 将对话分层分解为会话、回合、轮次和句子\n- 使用标准NLP工具（如NLTK）将每个轮次分割为句子集合\n- 同时使用LLM生成三种类型的记忆：摘要、事实和见解\n\n**第二步：构建图结构**\n- 将原始对话单元（会话、回合或轮次）视为块节点\n- 将每个块节点链接到其组成句子，形成成员关系边\n- 计算句子间相似度，构建k近邻图，形成句子-句子相似度边\n- 整体句子图记忆定义为块节点和句子节点的集合，以及它们之间的边\n\n**第三步：设计多层次检索机制**\n- 结合向量检索和图扩展的双重设计\n- 首先从向量数据库检索候选记忆单元\n- 通过在句子图上进行多跳遍历扩展检索到的句子\n- 将句子映射回父块，根据聚合分数对块进行排名\n- 将选定块与生成记忆聚合为统一相关上下文\n\n### 3. 轻量级实现策略\n作者特别强调解决方案的实用性：\n- 不需要额外基于LLM的提取，仅依赖标准句子分割工具\n- 使用向量数据库存储索引表，图数据库维护句子图记忆\n- 确保方法轻量级且易于在长期多轮对话设置中部署\n\n## 四、逻辑演进总结\n\n作者的思想演进展现了清晰的逻辑链条：\n\n1. **问题识别**：从记忆过载和记忆碎片化的根本困境出发，认识到现有方法的局限性\n2. **洞察发现**：通过分析发现句子作为基本单位的独特价值，图结构的关联建模能力，以及多层次记忆整合的必要性\n3. **设计选择**：基于洞察做出在句子级别构建对话记忆结构的核心设计选择\n4. **方案构建**：逐步构建分层解构、图结构构建、多层次检索和轻量级实现的完整解决方案\n5. **价值验证**：通过实验证明SGMem在缓解记忆碎片化、提高检索准确性、计算效率和适应不同查询类型方面的优势\n\n这一逻辑演进过程体现了作者对长期对话代理记忆管理挑战的深入理解，以及从问题本质出发，通过关键洞察找到创新突破点的系统性思考过程。最终提出的SGMem方法不仅解决了记忆碎片化问题，还实现了高效、轻量级且实用的记忆管理框架。"
                },
                {
                    "title": "Query-Centric Graph Retrieval Augmented Generation",
                    "arxiv_id": "2509.21237",
                    "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。首先，从核心判断来看，这篇论文的本质是提出一种名为QCG-RAG的查询中心图RAG框架，旨在增强大语言模型的多跳推理能力。论文的核心贡献是改进LLM的基础推理能力，而非将其作为工具应用于特定领域。其次，从正面指标看，论文明确涉及大语言模型(LLMs)和推理能力(特别是multi-hop reasoning)，这与研究目标高度一致。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性。虽然论文没有涉及强化学习、智能体等新兴范式，但其核心关注的多跳推理能力正是LLM通用推理能力的重要组成部分。论文提出的查询中心图检索方法是一种通用的方法论，可以增强LLM在各种任务中的推理表现，而非局限于特定应用场景。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决图RAG中的粒度困境问题。针对多跳推理和长上下文理解场景，我们提出了一种以查询为中心的图检索增强生成框架(QCG-RAG)，并在LiHuaWorld和MultiHop-RAG数据集上通过准确率指标验证了其有效性。",
                    "summary_translation": "基于图的检索增强生成（Graph-based retrieval-augmented generation, RAG）通过外部知识丰富大型语言模型（Large language models, LLMs），以实现长上下文理解（long-context understanding）和多跳推理（multi-hop reasoning），但现有方法面临粒度困境（granularity dilemma）：细粒度实体级图（fine-grained entity-level graphs）导致高令牌成本（token costs）并丢失上下文，而粗粒度文档级图（coarse document-level graphs）则无法捕捉细微关系。我们提出了QCG-RAG，这是一个以查询为中心的图RAG框架（query-centric graph RAG framework），支持查询粒度索引（query-granular indexing）和多跳块检索（multi-hop chunk retrieval）。我们的以查询为中心的方法利用Doc2Query和Doc2Query{-}{-}构建具有可控粒度（controllable granularity）的以查询为中心的图，提高了图质量（graph quality）和可解释性（interpretability）。随后，一个定制的多跳检索机制（tailored multi-hop retrieval mechanism）通过生成的查询选择相关块（relevant chunks）。在LiHuaWorld和MultiHop-RAG上的实验表明，QCG-RAG在问答准确性（question answering accuracy）方面始终优于先前的基于块和基于图的RAG方法（chunk-based and graph-based RAG methods），为多跳推理建立了新范式（new paradigm）。",
                    "inspiration_trace": "# QCG-RAG核心方法逻辑演进分析\n\n## 一、问题识别：图RAG的粒度困境\n\n作者首先敏锐地捕捉到图检索增强生成(RAG)领域面临的一个根本性挑战：**粒度困境**。这一困境体现在现有图RAG方法的两个极端：\n\n### 1. 细粒度实体级图的局限性\n- **高昂的计算成本**：构建细粒度实体级图和生成社区摘要需要大量token消耗和推理成本\n- **上下文连贯性损失**：过度细化的实体关系可能导致语义碎片化，破坏整体语境\n- **可扩展性问题**：随着知识规模增长，维护细粒度图的成本呈指数级上升\n\n### 2. 粗粒度文档级图的局限性\n- **关系细节丢失**：文档级连接无法捕捉实体间的细微关系和语义关联\n- **推理能力受限**：过于粗粒度的表示难以支持复杂的多跳推理任务\n- **检索精度不足**：无法精准匹配用户查询意图与文档内容\n\n作者观察到，现有简化方案（如Fast GraphRAG、LightRAG等）虽然降低了成本，但牺牲了图质量；而分层索引图（如KG-Retriever）虽然提高了效率，但推理能力仍然受限。这一困境成为推动作者寻求新解决方案的原始动力。\n\n## 二、关键洞察：查询中心的中间粒度价值\n\n在寻找解决方案的过程中，作者从**文档扩展技术**中获得了关键灵感，特别是Doc2Query和Doc2Query--方法。这引发了一个重要的认知转变：\n\n### 1. 查询作为中间粒度的独特价值\n作者认识到，生成的查询天然处于一个\"最佳平衡点\"：\n- **比实体三元组更丰富**：查询包含完整的语义表达，比实体-关系-实体三元组更具上下文完整性\n- **比文档块更精确**：查询聚焦于特定信息需求，比整个文档块更精准地表达内容要点\n- **自然对齐用户意图**：查询直接反映用户可能提出的问题，天然与检索目标对齐\n\n### 2. 查询-答案对的增强价值\n作者进一步扩展了原始Doc2Query方法，创新性地引入**查询-答案对**的概念：\n- **减少歧义**：答案为查询提供了明确的语义锚定，减少理解偏差\n- **强化块对齐**：查询-答案对与源块之间建立了更强的语义关联\n- **多入口点检索**：每个块通过多个查询-答案对获得多种检索路径\n\n这一洞察标志着作者思维的重要转折：不再局限于传统的\"实体vs文档\"二元选择，而是创造性地引入\"查询\"作为新的组织单元，实现了粒度的可控调节。\n\n## 三、解决方案构建：查询中心图框架\n\n基于上述洞察，作者构建了完整的QCG-RAG框架，其逻辑演进体现在两个核心组件的设计中：\n\n### 1. 查询中心图构建：从生成到索引\n作者设计了一个三步流程，将抽象洞察转化为具体实现：\n\n**第一步：查询生成**\n- 扩展Doc2Query方法，从每个文本块生成多个查询-答案对\n- 确保生成的查询覆盖块内容的多个角度和细节层次\n- 通过多样化查询类型（事实、定义、方法、原因等）全面捕捉块内容\n\n**第二步：查询质量控制**\n- 扩展Doc2Query--方法，引入查询-答案对与源块的语义相似度评估\n- 通过相似度排序和top-α筛选，保留最忠实于源内容的高质量查询\n- 实现噪声减少与覆盖保持的平衡\n\n**第三步：双层图结构设计**\n- 创新性地设计查询层和块层的双层结构\n- 查询间通过语义相似性连接（Eintra边）\n- 查询与源块通过归属关系连接（Einter边）\n- 形成既保持语义丰富性又具有结构清晰性的索引图\n\n### 2. 查询中心检索机制：多跳推理路径\n作者设计的检索机制体现了对多跳推理本质的深刻理解：\n\n**检索相关查询**：将用户查询映射到图中的查询节点，而非直接映射到文档块，实现了从\"内容匹配\"到\"意图匹配\"的转变。\n\n**扩展到相邻查询**：通过在查询层图上进行h跳扩展，自然地模拟了人类的多步推理过程，捕获了语义相关的间接证据。\n\n**收集和排序相关块**：基于查询-块关联聚合证据，通过查询集对块的相关性评分，实现了更精准的证据权重分配。\n\n**生成响应**：最终基于排序后的块生成答案，完成了从查询意图到精准答案的完整路径。\n\n## 四、逻辑演进的本质：从结构到意图的转变\n\nQCG-RAG的核心思想演进体现了从\"以内容为中心\"到\"以意图为中心\"的范式转变：\n\n1. **传统RAG**：关注内容本身的表示和匹配（块或实体）\n2. **QCG-RAG**：关注用户查询意图的表达和满足（查询中心）\n\n这一转变解决了RAG领域的根本挑战：如何使检索的外部知识更好地服务于用户的实际信息需求。通过将\"查询\"作为组织和检索知识的基本单元，QCG-RAG实现了检索粒度的可控调节，使外部知识与用户意图之间建立了更直接、更自然的桥梁。\n\n作者通过这一创新思路，不仅解决了图RAG的粒度困境，更为检索增强生成领域开辟了新的研究方向，展示了如何通过重新思考知识组织的基本单元来突破现有方法的局限性。"
                },
                {
                    "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
                    "arxiv_id": "2509.21193",
                    "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
                    "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为Eigen-1的统一框架，通过结合隐式检索和结构化协作来增强大语言模型的推理能力。论文提出了两个主要创新点：基于监控器的检索模块（在token级别运作，最小化对外部知识检索的干扰）和分层解决方案细化（HSR，通过迭代指定每个候选作为锚点并由其同伴修复）。虽然论文在科学推理任务（如生物和化学领域）上进行了评估，但其方法论本质上是通用的，旨在提升LLM的基础推理能力，而非解决特定领域问题。论文明确解决了LLM推理中的两个关键瓶颈：显式检索导致的推理碎片化和多智能体管道中的解决方案稀释问题。这些改进直接针对LLM的通用推理能力，符合研究目标。论文还包含多个正面指标，如LLMs、推理能力和多智能体系统。因此，尽管评估领域涉及科学推理，但论文的核心是提升LLM的通用推理能力，应被保留。",
                    "summary2": "本文旨在解决大型语言模型在科学推理中的检索中断和智能体协作低效问题。针对复杂的科学推理任务，我们提出了一种结合Monitor-based RAG、分层解决方案精炼(HSR)和质量感知迭代推理(QAIR)的统一框架，并在Humanity's Last Exam Bio/Chem Gold等数据集上通过准确率、token使用量和智能体步骤等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）最近在科学推理方面表现出强劲进展，但仍存在两个主要瓶颈。首先，显式检索（explicit retrieval）使推理过程碎片化，施加了额外的标记（tokens）和步骤所带来的隐藏\"工具税\"（tool tax）。其次，多智能体管道（multi-agent pipelines）通常通过对所有候选方案进行平均而稀释了优质解决方案。我们通过一个结合隐式检索（implicit retrieval）和结构化协作（structured collaboration）的统一框架来解决这些挑战。在该框架的基础上，基于监视器的检索模块（Monitor-based retrieval module）在标记级别（token level）运行，以最小的推理中断整合外部知识。在此基础之上，分层解决方案精炼（Hierarchical Solution Refinement, HSR）迭代地将每个候选方案指定为由其同行修复的锚点，而质量感知迭代推理（Quality-Aware Iterative Reasoning, QAIR）则根据解决方案质量调整精炼过程。在人类终极考试（Humanity's Last Exam, HLE）生物/化学金牌测试集上，我们的框架达到了48.3%的准确率——这是迄今为止报道的最高水平，比最强的智能体基线高出13.4个百分点，比前沿大型语言模型高出18.1个百分点，同时将标记使用量减少了53.5%，将智能体步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了该框架在不同领域的鲁棒性。错误分析显示，在超过85%的情况下，推理失败和知识缺口同时出现，而多样性分析揭示了一个明显的二分法：检索任务受益于解决方案的多样性，而推理任务则倾向于共识。总体而言，这些发现展示了隐式增强（implicit augmentation）和结构化精炼（structured refinement）如何克服显式工具使用（explicit tool use）和统一聚合（uniform aggregation）的低效性。代码可在以下网址获取：https://github.com/tangxiangru/Eigen-1。",
                    "inspiration_trace": "# Eigen-1核心方法的逻辑演进：从问题识别到创新框架\n\n## 1. 问题识别：科学推理的双重瓶颈\n\n论文作者通过深入分析当前大型语言模型(LLMs)在科学推理任务中的表现，识别出两个关键瓶颈：\n\n### 1.1 显式检索导致的\"工具税\"问题\n- **现象观察**：现有检索增强生成(RAG)系统需要显式中断推理过程来访问外部知识\n- **问题本质**：每次检索都会打断推理流——挂起逻辑状态、制定查询、处理结果、重建上下文\n- **量化证据**：解决群体遗传学问题需要8-10次这样的中断，导致代理步骤数量翻倍，同时降低推理连贯性\n- **深层问题**：这种\"工具税\"不仅增加计算成本，还破坏了推理的连续性和完整性\n\n### 1.2 多智能体协作的效率问题\n- **现象观察**：当前多智能体系统通常采用\"民主式\"工作流，平等对待所有候选解决方案\n- **问题本质**：这种平均主义方法稀释了高质量解决方案的贡献，没有区分解决方案的相对质量\n- **深层问题**：违背了认知科学中关于专家推理的研究和科学协作中思想自然组织为锚点和支持的观察\n\n## 2. 深入分析：错误模式与架构限制\n\n作者通过对149个HLE Bio/Chem问题的错误模式分析，进一步揭示了问题的本质：\n\n### 2.1 错误类型的交织性\n- **关键发现**：92.8%的失败涉及推理错误，88.7%涉及知识缺口，且两者有显著重叠\n- **洞察**：推理失败和知识缺口在85%以上的情况下同时发生，表明这些挑战本质上是相互交织的\n\n### 2.2 现有方法的根本局限\n- **检索增强的局限**：单轮RAG缺乏适应性；迭代RAG增加了延迟；推理感知RAG仍依赖于显式调用，碎片化推理\n- **多智能体协作的局限**：民主协作系统可能将大量计算投入到低质量候选上；结构化推理系统缺乏质量感知适应\n\n## 3. 关键洞察：科学推理的本质特征\n\n基于对问题本质的深入理解，作者提炼出几个关键洞察：\n\n### 3.1 推理与知识的不可分割性\n- **核心洞察**：成功的科学推理需要领域知识与逻辑推理的无缝集成\n- **推论**：任何试图将这两者分离处理的架构都会导致性能下降\n\n### 3.2 科学协作的层次性\n- **核心洞察**：真正的科学协作不是简单的平等协商，而是形成\"锚点-支持\"的层次结构\n- **推论**：多智能体系统应该模拟这种层次性，而不是简单平均所有候选\n\n### 3.3 任务类型的差异性\n- **核心洞察**：不同类型的任务需要不同的协作策略\n- **推论**：检索任务受益于解决方案多样性，而推理任务则倾向于共识\n\n## 4. 解决方案构思：统一框架的诞生\n\n基于以上洞察，作者构思了一个统一框架，同时解决两个瓶颈问题：\n\n### 4.1 解决\"工具税\"的思路：隐式增强\n- **核心思路**：将显式检索转变为隐式增强，避免推理流的中断\n- **实现构想**：设计一个监控系统，在推理过程中持续检测知识缺口，并在不中断推理的情况下注入相关信息\n- **关键创新**：从\"显式调用\"转变为\"隐式增强\"，在token级别操作而非步骤级别\n\n### 4.2 解决协作效率的思路：层次化精炼\n- **核心思路**：从民主式平均转变为层次化精炼，模拟科学协作中的锚点-支持关系\n- **实现构想**：建立\"分层解决方案精炼\"(HSR)机制，迭代地将每个候选指定为锚点，由其他候选提供改进建议\n- **关键创新**：引入质量感知迭代推理(QAIR)，根据解决方案质量自适应调整精炼过程\n\n## 5. 框架整合：Eigen-1的诞生\n\n作者将上述两个解决方案整合为一个统一框架——Eigen-1：\n\n### 5.1 架构设计\n- **基础层**：基于监控器的RAG系统，作为隐式知识增强的基础设施\n- **协作层**：分层解决方案精炼(HSR)和质量感知迭代推理(QAIR)，构建在基础层之上\n\n### 5.2 创新整合\n- **隐式与显式的结合**：监控系统隐式检测知识需求，但以最小干扰的方式显式注入信息\n- **结构与适应性的结合**：HSR提供结构化的协作框架，而QAIR则提供质量驱动的适应性控制\n- **多样性与共识的结合**：框架能够根据任务特性在保持多样性和追求共识之间动态平衡\n\n## 6. 实验验证与确认\n\n作者通过实验验证了框架的有效性，确认了其解决了初始识别的问题：\n\n### 6.1 性能提升\n- **显著效果**：在HLE Bio/Chem Gold上实现48.3%准确率，超过最强基线13.4个百分点\n- **效率提升**：同时减少token使用量53.5%和代理步骤43.7%，证明成功解决了\"工具税\"问题\n\n### 6.2 机制验证\n- **错误分析**：确认框架有效处理了推理错误和知识缺口的交织问题\n- **多样性分析**：验证了框架对不同类型任务(检索vs推理)的适应性处理能力\n\n## 7. 理论贡献与启示\n\nEigen-1不仅是一个技术框架，还提供了对科学推理本质的深刻理解：\n\n### 7.1 科学推理的新理解\n- **核心贡献**：揭示了科学推理中知识获取与逻辑推理的不可分割性\n- **理论意义**：为构建更接近人类专家推理模式的AI系统提供了新思路\n\n### 7.2 协作智能的新范式\n- **核心贡献**：展示了从\"民主式\"到\"层次式\"多智能体协作的转变\n- **理论意义**：为设计更高效的多智能体系统提供了新范式\n\n通过这一完整的逻辑链，作者从识别科学推理的双重瓶颈出发，通过深入分析和关键洞察，最终提出了一个统一框架，不仅解决了初始问题，还提供了对科学推理本质的新理解。这一思考过程体现了从问题到解决方案的系统性和创新性思维。"
                },
                {
                    "title": "Behind RoPE: How Does Causal Mask Encode Positional Information?",
                    "arxiv_id": "2509.21042",
                    "authors": "Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi",
                    "summary": "While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是深入分析了Transformer架构中因果掩码如何编码位置信息，以及它与RoPE的相互作用。根据筛选标准，我认为它符合研究范围，原因如下： 首先，从核心判断来看，这篇论文的本质是研究LLM的基础架构机制，特别是位置编码这一核心组件。位置编码对于LLM的推理能力至关重要，因为它直接影响模型理解序列中元素关系的能力，这是逻辑推理和多步推理的基础。论文不是将LLM作为工具应用到特定领域，而是深入探究LLM内部工作机制。 其次，论文符合正面指标中的\"核心概念\"，因为它直接研究大型语言模型(LLMs)的架构机制。虽然它没有直接讨论推理、规划或问题解决，但位置编码是这些能力的基础性支撑。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，虽然论文没有提出新的训练范式或直接改进推理能力的方法，但它提供了对LLM基础架构的深入理解，这种基础性研究对于未来改进LLM的通用推理能力具有重要价值。理解因果掩码如何提供位置信息，有助于设计更有效的位置编码机制，从而提升模型的整体推理能力。 因此，这篇论文符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在探究Transformer解码器中因果掩码如何编码位置信息。针对无参数、无显式位置编码的Transformer架构，我们提出了一种理论分析框架，证明因果掩码能诱导位置依赖的注意力模式，并在模拟环境和真实训练模型上通过注意力分数分布验证了其有效性。此外，我们还发现因果掩码与RoPE的相互作用会扭曲RoPE的相对注意力模式，这一现象在现代大型语言模型中被一致观察到。",
                    "summary_translation": "虽然像RoPE (Rotary Position Embedding，旋转位置嵌入) 这样的显式位置编码是Transformer解码器中位置信息的主要来源，但因果掩码(causal mask)也提供了位置信息。在这项工作中，我们证明了因果掩码可以在注意力分数(attention scores)中诱导出位置相关模式，即使输入中没有参数或因果依赖。我们的理论分析表明，这种诱导的注意力模式倾向于偏好邻近的查询-键对(query-key pairs)，反映了常见位置编码的行为。实证分析证实，训练后的模型表现出相同的行为，且学习到的参数进一步放大了这些模式。值得注意的是，我们发现因果掩码和RoPE的相互作用将RoPE的相对注意力分数模式扭曲为非相对模式。我们在现代大型语言模型中一致地观察到这种效应，这表明除了显式位置编码外，将因果掩码视为位置信息来源的重要性。",
                    "inspiration_trace": "## 面临的挑战\n传统观点认为Transformer解码器中的位置信息主要来自显式位置编码如RoPE，而因果掩码仅被视为阻止访问未来token的机制。然而，无显式位置编码的模型仍能处理序列数据，表明因果掩码可能也编码位置信息，但其确切机制尚不清楚。\n\n## 关键洞察\n作者认识到因果掩码本身就能在注意力分数中诱导位置依赖模式，即使没有任何参数、输入因果依赖或前馈网络。这种模式倾向于为相邻查询-键对分配更高注意力分数，与常见位置编码行为相似，但本质上既不同于绝对位置编码也不同于相对位置编码。\n\n## 解决方案演进\n从理论分析出发，作者首先证明因果掩码如何诱导位置依赖模式；然后通过无参数Transformer模拟验证理论；接着训练无显式位置编码模型观察实际行为；最后研究因果掩码与RoPE的相互作用，并在现代LLMs中验证这种相互作用的存在。\n\n## 创新点总结\n首次从理论上证明因果掩码本身能编码位置信息，揭示其与RoPE的相互作用会扭曲RoPE的相对注意力模式，强调研究位置信息时需考虑两者的联合效应。"
                },
                {
                    "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
                    "arxiv_id": "2509.20863",
                    "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
                    "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为WeFT的加权监督微调方法，专门用于改进扩散语言模型(dLLMs)的推理能力。论文的核心贡献不是将LLM作为工具应用到特定领域，而是通过新的训练范式（基于熵值的权重分配）来增强模型本身的推理能力。论文在四个推理基准测试（Sudoku、Countdown、GSM8K和MATH-500）上验证了方法的有效性，表明其提升了模型的数学和逻辑推理能力。这符合\"改进LLM基础能力、提出新训练范式、增强其逻辑和数学推理能力\"的保留标准。 第二步：正面指标 论文包含以下正面指标： - 核心概念：研究扩散语言模型(dLLMs)，属于大语言模型的范畴 - 能力方向：明确关注推理能力，特别是在数学推理(GSM8K, MATH-500)和逻辑推理(Sudoku, Countdown)任务上 - 论文虽然没有涉及强化学习或智能体系统等其他正面指标，但已包含最核心的指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但指的是语言模型领域的扩散模型，而非视觉领域的扩散模型 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性问题（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及明显的特殊或模糊情况。虽然通过基于熵值的权重分配方法间接提高了生成过程的一致性和可预测性，可能有助于减少幻觉，但这不是论文的主要焦点。 综上所述，这篇论文的核心贡献是提出一种新的微调方法来增强大语言模型的通用推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决扩散语言模型(dLLMs)在监督微调(SFT)过程中缺乏对关键token关注的问题。针对扩散模型生成过程不可预测且不一致的场景，我们提出了一种基于熵加权的微调方法WeFT，在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上通过准确率指标验证了其有效性，相比标准SFT实现了最高83%的相对性能提升。",
                    "summary_translation": "扩散模型(Diffusion models)最近在语言建模(language modeling)中展现出强大的潜力，与传统的自回归(autoregressive)方法相比，提供了更快的生成速度。然而，将监督微调(supervised fine-tuning, SFT)应用于扩散模型仍然具有挑战性，因为它们在每个去噪(denoising)步骤中缺乏精确的概率估计。虽然扩散机制(diffusion mechanism)使模型能够对整个序列进行推理(reason)，但它也使生成过程变得不太可预测且常常不一致。这凸显了控制引导生成方向的关键令牌(key tokens)的重要性。为解决这一问题，我们提出了WeFT，一种针对扩散语言模型(diffusion language models)的加权SFT方法，其中令牌(tokens)根据其熵(entropy)被赋予不同的权重。源自扩散理论(diffusion theory)的WeFT带来了显著的增益：在open-r1数据集的s1K、s1K-1.1和3k样本上进行训练时，在四个广泛使用的推理基准(reasoning benchmarks)(Sudoku、Countdown、GSM8K和MATH-500)上，相对于标准SFT实现了39%、64%和83%的相对改进。代码和模型将公开发布。",
                    "inspiration_trace": "# WeFT方法提出的逻辑链推演\n\n## 一、面临的挑战：扩散语言模型微调的根本困境\n\n### 1. 扩散模型在语言建模中的双重性\n作者首先观察到扩散大语言模型(dLLMs)具有独特的双重特性：一方面，它们通过迭代精炼过程实现高效并行生成，相比传统自回归方法具有速度优势；另一方面，扩散机制使模型能够对整个序列进行推理，但也导致生成过程更不可预测且经常不一致。这种双重性带来了新的挑战。\n\n### 2. 传统SFT方法的核心缺陷\n作者发现，现有的dLLMs监督微调(SFT)方法存在一个根本性局限：其损失函数（公式1）隐含地假设所有token在整个扩散过程中具有均匀的掩码率，将每个token视为同等重要。这种\"一刀切\"的均匀加权策略忽略了token重要性的内在异质性。\n\n### 3. 关键问题定位\n作者精准定位了问题本质：**在扩散语言模型的微调过程中，不同token对生成质量和推理能力的影响是不同的，特别是那些在规划和推理中起核心作用的token，应该在训练中获得更大的权重**。然而，现有方法无法识别并优先处理这些关键token。\n\n## 二、关键洞察：熵作为token重要性的指示器\n\n### 1. 从不确定性到重要性\n作者敏锐地观察到，**模型对token的预测分布的熵可以作为衡量token重要性的可靠指标**。这一洞察基于以下观察：高熵token通常对应于模型在生成中表现出更大不确定性的位置，而这些位置往往与推理或规划过程密切相关。\n\n### 2. 熵与推理能力的关联\n通过分析（如图2所示），作者发现高熵token（如\"first\"、\"second\"等）往往携带更丰富的信息，在塑造生成响应结构中扮演更关键的角色，通常表示逻辑或序列结构；而低熵token（如\"all\"、\"such\"等）主要作为辅助词，对语义内容的贡献较小。这表明**高熵token与模型的推理能力密切相关**。\n\n### 3. 理论与实践的桥梁\n作者成功地将这一观察与扩散理论联系起来，认识到**通过优先训练高熵token，可以鼓励模型将更多容量分配给序列中对规划和推理至关重要的部分**，从而提升整体推理能力。\n\n## 三、解决方案：基于熵的加权微调(WeFT)\n\n### 1. 核心思想：差异化token处理\n基于上述洞察，作者提出了WeFT的核心思想：**根据token的预测熵为其分配不同的训练权重，使高熵token（即对推理更重要的token）获得更强的训练信号**。这一思想直接针对了传统SFT方法中均匀加权的缺陷。\n\n### 2. 理论基础：扩散模型的重新表述\n作者没有停留在直观层面，而是从扩散理论的角度进行了严谨推导。他们重新定义了Q矩阵（公式9），为每个token分配不同的掩码率β，并基于此推导出加权SFT损失函数（公式10）。这一理论推导确保了WeFT与扩散生成的基本原理保持一致，而非简单的启发式方法。\n\n### 3. 实现策略：双前向传播机制\n为了实现基于熵的加权，作者设计了一个巧妙的训练机制：\n- **第一次前向传播**：屏蔽整个答案，计算每个token的预测熵，估计其掩码率βi\n- **第二次前向传播**：根据βi计算每个token的掩码概率ti，以概率ti独立屏蔽每个token，并将训练权重设置为1/ti\n\n这种设计确保了**高熵token更有可能被屏蔽并因此获得更强的训练信号**，从而实现了对关键token的差异化训练。\n\n### 4. 优化细节：稳定性的考量\n作者还考虑到了实际训练中的稳定性问题。他们发现原始熵值存在较大方差，可能 destabilize训练过程。因此，他们采用熵的平方根作为最终的权重指标（公式13），有效降低了梯度范数的规模（如表5所示），提高了训练稳定性。\n\n## 四、从理论到实践的完整闭环\n\n### 1. 理论验证\n作者通过严格的数学推导（附录A）证明了加权SFT损失函数的理论正确性，确保了方法与扩散模型的基本原理一致。\n\n### 2. 实验验证\n在多个推理基准测试上的实验结果（表1）表明，WeFT相对于标准SFT有显著改进（39%-83%的相对提升），验证了方法的有效性。\n\n### 3. 消融研究\n通过一系列消融实验（表3-5），作者验证了WeFT中每个设计选择的必要性：\n- 熵作为权重指标优于负对数似然等其他指标\n- 理论推导的损失函数优于简单的加权方法\n- 熵的平方根比原始熵提供更稳定的训练\n\n### 4. 扩展性验证\n作者还验证了WeFT在后续强化学习训练中的持久优势（图3，表2），表明其不仅改善了SFT阶段，还为后续训练提供了更好的初始化。\n\n## 总结：从问题到解决方案的逻辑演进\n\n作者提出WeFT的逻辑链展现了一个清晰的思想演进过程：从识别扩散语言模型微调中的均匀加权问题，到洞察熵与token重要性的关联，再到设计基于熵的加权微调方法，并通过理论推导和实验验证形成完整闭环。这一过程体现了作者对问题本质的深刻理解、创新性的洞察力以及严谨的科学方法，最终成功解决了扩散语言模型微调中的关键挑战。"
                },
                {
                    "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
                    "arxiv_id": "2509.20820",
                    "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
                    "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，特别是上下文学习(in-context learning)能力。论文提出的\"cheat-sheet ICL\"方法通过将many-shot示例提炼为简洁摘要，有效增强了LLM在推理任务上的性能。这明显属于改进LLM通用推理能力的范畴，而不是将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文明确包含了\"核心概念\"(Large language models, LLMs)和\"能力方向\"(reasoning)，摘要中特别提到在\"challenging reasoning tasks\"上的实验，表明其关注点正是推理能力。 第三，从排除标准来看，论文不涉及多模态与视觉、特定应用领域或模型可靠性等被排除的领域。论文关注的是通用推理任务，而非医疗、化学等特定领域应用。 此外，论文也不涉及特殊或模糊情况需要额外判断。 综上所述，这篇论文的核心贡献是提出了一种新的方法来增强LLM的上下文学习和推理能力，完全符合筛选\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。",
                    "summary2": "本文旨在解决many-shot in-context learning计算成本高的问题。针对需要大量示例的推理任务，我们提出了一种cheat-sheet ICL方法，将many-shot演示中的知识提炼成简洁的文本摘要作为上下文，并在BBH Hard推理任务上通过准确率和输入标记长度验证了其有效性。",
                    "summary_translation": "大型语言模型（large language models, LLMs）的最新进展使得通过多示例进行有效的上下文学习（in-context learning, ICL）成为可能，但代价是由于输入标记（tokens）数量增加而导致的高计算需求。为解决这一问题，我们提出了备忘单式上下文学习（cheat-sheet ICL），该方法将多示例ICL的信息提炼成一个简洁的文本摘要（备忘单，cheat sheet），用作推理（inference）时的上下文。在具有挑战性的推理任务上的实验表明，备忘单式ICL使用远少的标记（tokens）就能达到与多示例ICL相当或更好的性能，并且无需测试时检索（test-time retrieval）就能与基于检索的ICL（retrieval-based ICL）相匹配。这些研究结果表明，备忘单式ICL是在下游任务（downstream tasks）中利用大型语言模型的实用替代方案。",
                    "inspiration_trace": "# 从挑战到解决方案：Cheat-sheet ICL的思想演进逻辑链\n\n## 一、面临的挑战：many-shot ICL的效率困境\n\n### 1.1 核心矛盾的识别\n论文作者首先敏锐地捕捉到了大语言模型(LLMs)应用中的一个核心矛盾：**上下文学习(ICL)的性能与计算成本之间的权衡**。具体表现为：\n\n- **性能需求**：随着LLMs上下文窗口的扩展，many-shot ICL（使用大量示例）展现出显著优于传统few-shot ICL（使用少量示例）的性能。\n- **成本问题**：然而，这种性能提升伴随着高昂的计算成本，因为每次推理都需要处理更长的输入上下文。\n\n### 1.2 现有解决方案的局限性\n作者进一步分析了现有高效ICL方法的不足：\n\n- **检索方法**：基于相似性检索相关示例，虽有效但需要在每次推理时执行检索操作。\n- **参数修改方法**：如注意力修改技术，需要访问模型内部参数，这对专有LLMs不切实际。\n\n这一阶段的关键洞察是：**当前方法要么增加推理时计算负担，要么要求模型访问权限，缺乏一种既高效又通用的解决方案**。\n\n## 二、关键洞察：从人类学习方式中获得灵感\n\n### 2.1 跨领域类比的价值\n作者通过一个巧妙的类比获得了突破性灵感：**人类学习中的\"小抄\"（cheat sheet）现象**。就像学生为考试将关键知识点总结在一张纸上一样，LLMs或许也能将大量示例中的核心知识提炼成简洁的文本摘要。\n\n### 2.2 对LLMs本质能力的重新思考\n这一类比引导作者重新思考LLMs的核心能力：\n\n- **知识显式化潜力**：LLMs具有高级语言理解能力，能够将隐含在大量示例中的模式以文本形式显式表达。\n- **推理过程优化**：与其让模型在每次推理时从大量示例中\"重新发现\"模式，不如预先提取这些模式。\n\n### 2.3 核心假设的形成\n基于以上思考，作者形成了核心假设：**LLMs能够将many-shot ICL中的隐含知识蒸馏成一个紧凑的文本表示，该表示能够替代原始的大量示例而不损失性能**。\n\n这一阶段的关键突破在于：**从\"如何高效处理大量示例\"转向\"如何提取示例中的本质知识\"**，实现了问题视角的根本转变。\n\n## 三、提出解决方案：Cheat-sheet ICL的诞生\n\n### 3.1 方法设计的基本原则\n基于前述洞察，作者设计了cheat-sheet ICL方法，遵循以下原则：\n\n- **知识蒸馏**：将many-shot示例中的核心知识提取并浓缩成文本形式。\n- **计算效率**：将计算成本从推理阶段转移到预处理阶段，每个任务只需处理一次。\n- **实用兼容性**：不要求模型参数访问，适用于专有LLMs。\n\n### 3.2 两阶段方法架构\n作者将解决方案分为两个清晰阶段：\n\n**第一阶段：Cheat-sheet创建**\n- 将整个示例集提供给LLMs，配合专门设计的提示。\n- 引导LLMs识别示例中的难点，提取解决这些难点所需的核心知识。\n- 这一阶段每个任务只需执行一次，生成可重复使用的cheat sheet。\n\n**第二阶段：推理**\n- 向LLMs提供cheat sheet和测试输入，不提供原始的大量示例。\n- 仅保留两个示例作为格式指导，确保输出符合预期格式。\n\n### 3.3 方法优势的多维体现\n作者预见到该方法不仅能解决原始问题，还带来额外优势：\n\n- **计算效率**：大幅减少推理时的输入token数量。\n- **性能保持**：在多个推理任务上达到与many-shot ICL相当或更好的性能。\n- **可解释性**：生成的cheat sheet是人类可读的，便于理解和干预。\n- **通用性**：无需模型参数访问，适用于各种LLMs。\n\n## 四、思想演进的关键逻辑节点\n\n### 4.1 从问题到洞察的逻辑跃迁\n整个思想演进中最关键的逻辑跃迁是从**\"如何高效处理大量示例\"**到**\"如何提取示例中的本质知识\"**的转变。这一转变通过人类学习方式的类比得以实现，体现了跨领域思维的价值。\n\n### 4.2 从洞察到方案的实践落地\n将抽象洞察转化为具体方案时，作者遵循了清晰的逻辑路径：\n\n1. **形式化核心思想**：将many-shot ICL知识提炼成显式文本格式。\n2. **分离计算阶段**：将知识提取（预处理）与知识应用（推理）分离。\n3. **优化实现细节**：在推理时仅保留最少示例作为格式指导。\n4. **验证方法有效性**：通过实验证明方法在性能和效率上的优势。\n\n### 4.3 思想演进的特点\n这一思想演进过程体现了以下特点：\n\n- **问题导向**：始终围绕解决实际应用中的效率问题展开。\n- **类比创新**：通过人类学习方式的类比获得突破性灵感。\n- **本质思考**：深入理解LLMs的工作原理，特别是其知识表示能力。\n- **实用主义**：设计考虑实际应用场景，强调方法的实用性和可访问性。\n\n## 五、总结：思想演进的核心价值\n\nCheat-sheet ICL的思想演进展现了从实际问题出发，通过跨领域类比和对技术本质的深刻理解，最终提出创新解决方案的完整逻辑链条。这一演进不仅解决了原始问题（many-shot ICL的高计算成本），还带来了额外价值（可解释性、可干预性），体现了优秀学术研究的典型特征：**识别问题→突破思维→创新解决→多维验证**。\n\n这一思想演进的核心价值在于：它不仅提供了一个具体的技术解决方案，更为LLMs的高效应用开辟了新的思路——**通过知识显式化来优化计算过程**，这对未来LLMs应用研究具有重要启发意义。"
                },
                {
                    "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
                    "arxiv_id": "2509.20750",
                    "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
                    "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型无需训练框架，用于改进问答任务的推理能力。从本质上看，该论文完全符合研究目标，因为它专注于改进LLM的基础推理能力，特别是通过构建和细化子问题来探索多样化的推理路径，类似于思维链(CoT)的推理方法，但更强调置信度评估。论文的核心是增强模型的多步推理和问题解决能力，这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的要求。在正面指标方面，论文明确关注\"reasoning\"和\"problem-solving\"，虽然未明确提到LLMs，但其框架可与各种QA模型集成，这些模型很可能包括LLM。虽然论文提到其框架适用于文本、图像和视频领域，但多模态只是应用场景，而不是核心焦点；论文的核心是提出一种通用的推理框架，而不是专注于多模态技术本身。此外，论文关注提高推理的可靠性和稳健性，这与提高模型内在可靠性和推理质量相关，进一步支持其符合研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决多步推理QA任务中不相关sub-QAs引入噪声降低答案质量的问题。针对多模态QA任务，我们提出C2R框架，通过Generator生成sub-QAs，Refiner整理sub-QAs子集探索推理路径，Answer Selector基于置信度分数选择最可靠答案，在MMLU、MMMU、EgoSchema等五个基准测试上验证了其有效性，实现了多种模型上的一致性能提升。",
                    "summary_translation": "我们提出了Confidence-guided Refinement Reasoning (C2R，置信度引导的细化推理)，这是一种新颖的无训练框架，适用于文本、图像和视频领域的问答(QA，question-answering)任务。C2R策略性地构建和细化子问题及其答案(sub-QAs，sub-questions and their answers)，从而为目标答案推导出更好的置信度分数。C2R首先策划一个子问题集(sub-QAs)子集以探索多样化的推理路径，然后比较生成的答案候选者的置信度分数，以选择最可靠的最终答案。由于C2R仅依赖于模型本身衍生的置信度分数，它可以与各种现有的QA模型无缝集成，在不同模型和基准测试上展现出一致的性能提升。此外，我们提供了关于利用子问题(sub-QAs)如何影响模型行为的关键但尚未充分探索的见解，具体分析了子问题的数量和质量对实现稳健可靠推理的影响。",
                    "inspiration_trace": "# 论文核心方法的逻辑演进分析：从挑战到解决方案\n\n## 一、面临的挑战：多步推理的困境\n\n论文作者首先识别了问答(QA)领域中的核心挑战：\n\n### 1. 传统单步推理的局限性\n- 传统QA方法将问题视为单步推理过程，直接生成答案而无需中间分析\n- 对于复杂问题，答案往往不能直接推断，需要多步推理才能得出\n\n### 2. 现有多步推理方法的根本缺陷\n- 现有方法（如Chain-of-Thought）将主问题分解为子问题-答案对(sub-QAs)，然后利用它们推导最终答案\n- **关键发现**：作者通过研究揭示，sub-QAs并不总是增强QA推理；不加区分地使用它们会对模型的推理过程产生不利影响和干扰\n- 当sub-QAs与主问题无关或与不准确答案配对时，它们会在推理过程中引入噪声，最终降低答案质量\n- 先前工作通常在没有充分验证或改进的情况下纳入sub-QAs，其与主问题的相关性既未被评估也未被保证\n\n这一挑战识别体现了作者对现有方法局限性的深刻理解，为后续创新奠定了基础。\n\n## 二、关键洞察：揭示sub-QAs的双刃剑效应\n\n通过深入研究，作者获得了几个关键洞察：\n\n### 1. Sub-QAs的质量问题\n- Sub-QAs可能不相关或不准确，这会引入噪声并降低答案质量\n- 现有方法缺乏对sub-QAs质量的验证机制，导致不可靠的推理过程\n\n### 2. \"置信度膨胀\"现象的发现\n- **突破性洞察**：作者发现，无论sub-QAs的实际相关性如何，使用它们往往会增加结果答案的置信度分数，即使这些答案是不正确的\n- 这种\"置信度膨胀\"现象使得仅依赖高置信度来选择答案变得不可靠\n- 如图4(左)所示，虽然基础答案和精炼答案之间的准确性差距最小（平均-0.3%），但精炼答案的平均置信度显著膨胀——平均增加0.11\n\n### 3. 置信度与准确性的相关性\n- 作者发现模型分配给答案的置信度分数与答案正确的可能性之间存在强相关性\n- 如图5所示，准确率随着置信度增加而增加，所有模型和基准的平均Pearson相关系数高达0.95\n- 这表明置信度可以作为选择最佳答案的可靠指标，前提是正确处理不同推理路径之间的置信度分布差异\n\n这些关键洞察为作者提供了新的思考方向，引导他们从传统的\"验证每个sub-QA\"转向\"利用置信度指导答案选择\"的创新思路。\n\n## 三、解决方案：C2R框架的设计思想\n\n基于上述挑战和洞察，作者提出了Confidence-guided Refinement Reasoning (C2R)框架，其设计思想体现了以下演进逻辑：\n\n### 1. 核心思想转变：从直接验证到置信度引导\n- 作者没有选择直接验证每个sub-QA的质量（这需要额外计算且可能不可靠），而是转向利用模型自身的置信度评估\n- 这一转变体现了作者的智慧：与其试图解决难以准确评估的sub-QAs质量问题，不如利用模型自身的置信度信号来指导最终答案的选择\n\n### 2. 框架设计：三个核心组件的协同工作\n作者设计了三个协同工作的核心组件：\n\n#### (1) Generator（生成器）\n- 将主问题分解为多个子问题并生成相应的子答案\n- 创建一个多样化的sub-QA库，为后续推理提供原材料\n\n#### (2) Refiner（精炼器）\n- **关键创新**：不是盲目使用所有sub-QAs，而是选择性地策划sub-QA库的子集，以探索多样化的推理路径\n- 每个推理路径产生一个带有置信度分数的答案候选\n- 选择具有最高置信度分数的候选作为精炼答案\n\n#### (3) Answer Selector（答案选择器）\n- 基于两个原则选择最终答案：\n  - **原则1**：如果基础答案（ˆAbase）表现出足够高的置信度分数（即c(ˆAbase) ≥ τ1），选择它作为最终答案，避免过度复杂化简单问题\n  - **原则2**：如果基础答案置信度不足，则比较基础答案和精炼答案的置信度，使用置信度阈值τ2来决定选择哪个答案（即c(ˆArefined) ≥ c(ˆAbase) + τ2）\n\n### 3. 关键创新点：处理置信度膨胀\n- 作者特别关注处理\"置信度膨胀\"问题，通过设置适当的置信度阈值τ2来确保答案选择的可靠性\n- 如表11所示，简单的归一化方法虽然优于基线，但作者的阈值方法更优越，验证了其设计的有效性\n\n### 4. 训练无关性：通用解决方案\n- C2R仅依赖于自我评估的置信度分数，本质上是训练无关的\n- 这使得C2R可以与各种现有QA模型无缝集成，实现一致的性能改进\n\n## 四、思想演进脉络总结\n\n作者的思想演进脉络可以概括为以下逻辑链：\n\n1. **挑战识别**：传统单步推理不足以处理复杂问题，而现有多步推理方法盲目使用sub-QAs而不考虑其质量问题，导致推理不可靠。\n\n2. **深入分析**：通过研究发现sub-QAs的双刃剑效应——既可能帮助推理，也可能引入噪声；特别重要的是发现了\"置信度膨胀\"现象。\n\n3. **关键洞察**：模型置信度与答案正确性之间存在强相关性，这为基于置信度的答案选择提供了理论基础。\n\n4. **思路转变**：从传统的\"验证每个sub-QA\"转向\"利用置信度指导答案选择\"，避免了直接评估sub-QA质量的困难。\n\n5. **解决方案构建**：设计C2R框架，通过探索多个推理路径并基于置信度选择答案，既利用sub-QAs的优势又避免其潜在危害。\n\n6. **方法完善**：提出三个核心组件（Generator、Refiner、Answer Selector）协同工作的框架，实现训练无关且可适应各种QA模型的解决方案。\n\n这一思想演进过程体现了作者从问题识别到深入分析，再到创新解决方案的完整学术思维路径，展示了他们对现有方法局限性的深刻理解以及提出创新解决方案的能力。C2R框架的核心创新在于不试图解决难以准确评估的sub-QAs质量问题，而是巧妙地利用模型自身的置信度信号来指导最终答案的选择，这一思路转变体现了作者的学术智慧和创新能力。"
                },
                {
                    "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
                    "arxiv_id": "2509.20577",
                    "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
                    "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心贡献是提出一种名为\"Depth Specialised Mixture of Experts (DS-MoE)\"的新框架，用于改进大语言模型的推理能力。具体分析如下： 第一步核心判断：论文本质上是改进LLM的基础能力和推理能力，而非将其作为工具应用于特定领域。DS-MoE框架通过动态构建推理链，根据输入复杂性激活必要的专家模块，明显属于增强模型通用推理能力的研究，特别是多步推理能力。 第二步正面指标：论文包含多个相关主题。核心概念上，它专注于Transformer架构（LLM的基础架构）；能力方向上，明确强调\"reasoning chains\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力的提升。 第三步排除标准：论文不符合任何排除标准。虽然训练数据包含多个领域内容，但研究本身不是针对特定应用领域，而是提出通用架构改进方法；不涉及多模态与视觉内容；也不主要关注模型可靠性问题。 第四步特殊情况处理：论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明它通过提供可解释的推理链增强了模型的可解释性，从而提升推理质量，符合保留标准。 综上所述，这篇论文通过创新的架构设计，显著提高了LLM的推理效率和质量，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决Transformer架构对所有输入应用相同处理深度导致的效率低下和推理质量受限问题。针对不同复杂度的输入任务，我们提出了一种Depth-Specialised Mixture-of-Experts (DS-MoE)，通过深度专业化专家模块和动态路由网络构建推理链，并在The Pile数据集上通过计算效率、推理速度、准确率和可解释性等指标验证了其有效性。",
                    "summary_translation": "当代transformer（变换器）架构对所有输入应用相同的处理深度，造成效率低下并限制了推理质量。简单的事实查询与复杂的逻辑问题接受相同的多层计算，浪费资源的同时也限制了深度推理。为克服这一问题，我们提出了通过深度专业化专家混合（Depth Specialised Mixture of Experts, DS-MoE）实现的动态推理链概念，这是一个模块化框架，将专家混合（Mixture of Experts）范式从基于宽度扩展到深度专业化计算。DS-MoE引入了针对不同推理深度优化的专家模块，包括浅层模式识别（shallow pattern recognition）、组合推理（compositional reasoning）、逻辑推理（logical inference）、记忆整合（memory integration）和元认知监督（meta-cognitive supervision）。一个学习型路由网络（routing network）动态组装定制推理链，仅激活必要的专家以匹配输入复杂性。\n\n我们训练和评估DS-MoE所使用的数据集是The Pile，这是一个800GB的语料库（corpus），涵盖科学论文、法律文本、编程代码和网络内容等多个领域，能够实现跨推理深度的系统性评估。实验结果表明，与统一深度的transformer相比，DS-MoE实现了高达16%的计算节省和35%更快的推理速度（inference），同时在复杂多步推理基准（benchmarks）上提高了2.8%的准确率。此外，路由决策产生可解释的推理链，增强了透明度和可扩展性（scalability）。这些发现确立了DS-MoE作为自适应神经架构（adaptive neural architectures）的重要进展，证明了深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性（interpretability）。",
                    "inspiration_trace": "# 从挑战到创新：DS-MoE核心方法的逻辑演进推演\n\n## 一、面临的挑战：传统Transformer的根本局限\n\n作者首先敏锐地识别了Transformer架构中的一个根本性设计缺陷：**计算深度与任务复杂度的不匹配**。传统Transformer对所有输入应用相同的处理深度，无论输入是简单事实查询还是复杂逻辑问题。这种\"一刀切\"的设计导致了三个关键问题：\n\n1. **计算资源浪费**：简单问题（如\"法国的首都是什么？\"）被迫通过全部24层处理，与复杂问题消耗相同资源。\n2. **推理质量受限**：复杂问题可能需要更深层次的推理，但固定深度架构无法提供足够的处理深度。\n3. **专业化能力缺失**：所有层采用通用处理策略，无法针对不同推理深度进行专业化优化。\n\n作者观察到，这种设计使Transformer在计算效率、推理质量和可解释性方面都存在根本性局限，亟需一种能根据任务复杂度动态调整计算深度的架构。\n\n## 二、关键洞察：从人类认知中获取灵感\n\n面对这些挑战，作者转向人类认知过程寻找解决方案，形成了两个关键洞察：\n\n### 1. 深度自适应的认知原理\n作者发现人类认知过程中，推理深度会自然地适应问题复杂度——简单问题只需浅层处理，而复杂问题则自动触发深度、多步骤的思考过程。这种**自适应深度分配**是人类认知高效的关键，也是当前AI模型所缺乏的核心能力。\n\n### 2. 混合专家模型的新维度\n作者进一步分析了现有的混合专家模型(MoE)，发现它们主要关注\"宽度\"上的专业化（不同专家处理不同类型内容），而忽视了\"深度\"上的专业化（不同专家处理不同复杂度的推理）。这启发了作者思考：**能否将MoE的动态路由机制扩展到深度专业化领域？**\n\n这两个洞察的结合，为作者指明了一个全新方向：设计一个能根据输入复杂度动态组合不同深度专家的架构。\n\n## 三、解决方案的演进：从概念到架构\n\n基于上述洞察，作者逐步构建了DS-MoE解决方案，经历了以下演进阶段：\n\n### 阶段一：深度专业化的专家概念\n作者首先提出了**深度专业化专家**的创新概念，将推理过程分解为不同深度的专门模块：\n- 浅层模式识别专家（处理简单事实查询）\n- 组合推理专家（处理多步骤推理）\n- 逻辑推理专家（处理抽象逻辑问题）\n- 记忆整合专家（处理长上下文整合）\n- 元认知监督专家（监控和调整推理过程）\n\n这种设计突破了传统MoE仅关注宽度专业化的局限，引入了深度维度的专业化。\n\n### 阶段二：动态推理链的构想\n作者进一步思考：如何根据任务需求动态组合这些专家？这引出了**动态推理链**的核心概念：\n- 设计一个路由网络评估输入复杂度\n- 基于复杂度分数选择最相关的专家子集\n- 将选中的专家动态组合成处理链\n\n这种方法使计算复杂度从O(dn)降低到O(k log n)，其中k∈[2,5]远小于传统Transformer的深度d，实现了计算资源的显著优化。\n\n### 阶段三：复杂度感知的路由机制\n为使动态推理链有效工作，作者需要解决\"如何准确评估输入复杂度\"的问题。这促使作者开发了**多维度复杂度评估机制**：\n- 句法复杂度（解析树深度）\n- 语义密度（每句独特概念数量）\n- 推理步骤需求（通过从句链接或逻辑分解估计）\n\n通过将这些指标组合成单一复杂度分数，作者创建了一个能够精确指导专家选择的路由系统。\n\n### 阶段四：整体架构与训练策略的完善\n最后，作者将上述组件整合为完整的DS-MoE架构，并设计了相应的训练策略：\n- 专家预训练：每个专家在其专业化领域进行专门训练\n- 路由网络集成：逐步训练路由网络准确评估复杂度\n- 多专家链训练：训练专家间的协同工作\n- 端到端联合优化：平衡任务性能、路由准确性和负载均衡\n\n## 四、逻辑演进的核心脉络\n\n总结作者提出DS-MoE的思考路径，我们可以看到一个清晰的逻辑演进：\n\n1. **问题识别**：发现传统Transformer中计算深度与任务复杂度不匹配的根本局限\n2. **跨域启发**：从人类认知的自适应深度分配机制中获取灵感\n3. **概念扩展**：将MoE从宽度专业化扩展到深度专业化\n4. **创新设计**：提出动态推理链概念，实现专家的按需组合\n5. **机制完善**：开发复杂度评估和路由选择机制\n6. **系统整合**：构建完整架构和训练策略\n\n这一演进过程展现了作者如何从具体问题出发，通过跨领域洞察，结合现有技术的创新扩展，最终提出了一个既能提高计算效率，又能增强推理质量，同时还提升可解释性的全新架构。DS-MoE的核心创新在于将\"深度\"这一维度引入专业化设计，实现了从\"静态深度\"到\"动态深度\"的范式转变，为Transformer架构的发展开辟了新方向。"
                },
                {
                    "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
                    "arxiv_id": "2509.20502",
                    "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
                    "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是提出MARS（Multi-Agent Review System）这一新的多智能体协作框架，旨在提高LLM的推理能力，这属于\"改进LLM的基础能力\"和\"智能体协作框架\"的范畴，符合保留标准。其次，论文包含多个正面指标：明确讨论大语言模型(LLMs)、专注于推理能力(reasoning)、提出了基于LLM的多智能体系统(multi-agent systems)。第三，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，在特殊和模糊情况处理上，MARS是一种通用的智能体协作框架，受评审过程启发，用于增强LLM的通用推理能力，而非针对特定领域的应用。论文的核心贡献是提出了一种更高效的多智能体协作方法，在保持推理质量的同时，显著减少了计算开销，这直接服务于提升LLM通用推理能力的研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理能力有限及Multi-Agent Debate方法计算开销大的问题。针对多智能体协作场景，我们提出了一种基于角色的MARS框架，通过author生成初始解、reviewer独立评估、meta-reviewer整合反馈的分层评审机制，并在多个推理benchmark上通过准确率、token消耗和推理时间指标验证了其有效性。实验表明MARS在保持与MAD相当准确率的同时，减少了约50%的计算资源消耗。",
                    "summary_translation": "大语言模型（Large language models, LLMs）在自然语言理解方面取得了令人瞩目的成果，然而当作为单一智能体运行时，它们的推理能力仍然有限。多智能体辩论（Multi-Agent Debate, MAD）被提出以解决这一限制，通过使多个模型以圆桌辩论的方式进行协作推理。虽然有效，但由于涉及的智能体数量和所需的频繁通信，MAD引入了大量的计算开销。\n\n在本文中，我们提出了MARS（Multi-Agent Review System, 多智能体评审系统），一个受评审过程启发的基于角色的协作框架。在MARS中，作者智能体（author agent）生成初始解决方案，评审者智能体（reviewer agents）独立提供决策和评论，而元评审者（meta-reviewer）整合反馈以做出最终决策并指导进一步修订。这种设计在避免昂贵的评审者间交互的同时提高了推理质量，从而控制了令牌消耗（token consumption）和推理时间（inference time）。\n\n我们在多个基准测试中将MARS与MAD及其他最先进的推理策略进行了比较。使用不同LLMs的广泛实验表明，MARS在保持与MAD相当的准确性的同时，将令牌使用量和推理时间减少了约50%。代码可在https://github.com/xwang97/MARS获取。",
                    "inspiration_trace": "# MARS方法提出的逻辑链分析\n\n## 一、面临的挑战\n\n### 1. LLM推理能力的根本局限\n作者首先识别了LLM作为单代理系统时的核心问题：尽管在自然语言理解方面表现出色，但在需要深度事实知识和复杂结构推理的任务上表现有限。论文指出，即使是先进模型在GPQA基准上也仅能达到30%-40%的准确率，这表明单代理模式存在推理瓶颈。\n\n### 2. 现有单代理增强方法的不足\n作者分析了已有的推理增强技术（如CoT、ToT和self-reflection），发现它们虽然有所改进，但仍受限于单代理模式——缺乏外部反馈机制使模型难以自我纠错。这揭示了一个关键洞察：自我反思存在固有局限，因为模型倾向于坚持初始答案，难以识别自身错误。\n\n### 3. 多代理辩论(MAD)的效率瓶颈\n作者进一步考察了当时最先进的多代理方法——MAD框架，发现虽然它通过圆桌辩论机制确实提升了推理质量，但带来了严重的计算开销问题。MAD中所有代理之间的全连接通信结构导致token消耗和推理时间随代理数量呈指数级增长，使其在实际应用中成本过高。\n\n这一系列问题构成了作者研究的出发点：**如何在保持多代理协作带来的推理质量提升的同时，显著降低其计算成本？**\n\n## 二、关键洞察\n\n### 1. 验证器组件的启发\n作者从两个相关领域获得了重要启发：\n\n首先，他们注意到在代理调整研究中，验证器(verifier)或评论者(critic)组件被证明是提高轨迹质量的关键。这些验证器专注于检测错误而非生成完整答案，表明\"专业化分工\"可能比\"全员讨论\"更高效。\n\n其次，他们从学术界的同行评审过程获得灵感：评审者独立工作而不相互交流，却仍能通过层次化结构达成可靠决策。这暗示了**减少代理间直接通信可能不会损害决策质量**。\n\n### 2. 层次化结构的优势\n作者深入分析了评审过程与辩论过程的本质区别：评审过程采用层次化结构，评审者提供独立评估，由元评审者整合反馈；而辩论过程则是扁平化的全连接网络。他们洞察到，评审过程之所以高效，正是因为它避免了冗余的评审者间交流，同时保留了多元反馈的优势。\n\n### 3. 残差学习的类比\n作者将MARS与MAD的对比类比为ResNet与传统神经网络的区别：MAD中每个代理都试图直接近似\"理想答案\"，而MARS则专注于识别当前答案与理想答案之间的\"残差\"（错误）。元评审者的反馈就像梯度信号，指导作者进行精确修正。这一洞察揭示了**专注于错误检测和纠正比全面重新生成更高效**的本质。\n\n## 三、提出解决方案\n\n基于上述洞察，作者提出了MARS框架，其核心思想是：\n\n### 1. 角色专业化分工\n作者摒弃了MAD中所有代理对等的设计，转而采用角色专业化分工：\n- **作者代理**：专注于生成初始解决方案\n- **评审者代理**：专注于错误检测和评估\n- **元评审者代理**：专注于整合反馈和做出最终决策\n\n这种分工使每个代理都能专注于特定任务，避免了MAD中所有代理都需要完成全部工作的低效模式。\n\n### 2. 层次化通信架构\n作者设计了一个层次化的通信架构，彻底改变了代理间的交互方式：\n- 评审者独立工作，不相互交流，大幅减少了通信开销\n- 元评审者作为中央枢纽，整合所有评审者的反馈\n- 作者仅接收来自元评审者的整合反馈，避免了处理多个可能冲突的评论的复杂性\n\n这种架构使通信复杂度从MAD的O(n²)降低到MARS的O(n)，其中n是代理数量。\n\n### 3. 提议-评审-反馈-更新循环\n作者构建了一个高效的协作循环：\n1. **提议阶段**：作者生成初始解决方案\n2. **评审阶段**：多个评审者并行评估，提供决策、置信度和解释\n3. **元评审阶段**：元评审者整合反馈，做出最终决策并提供改进建议\n4. **更新阶段**：作者根据反馈修改解决方案（仅在需要时）\n\n这一循环确保了反馈的精确性和针对性，避免了不必要的修改和过度修正。\n\n### 4. 防止过度修正的保护机制\n作者敏锐地意识到，多代理系统可能面临\"过度修正\"的风险——正确的初始答案可能被错误的反馈破坏。为此，他们在提示模板中加入了保护机制，要求作者在强烈不同意元评审者时坚持自己的初始答案，平衡了外部反馈与自主判断。\n\n通过这一系列设计，MARS在保持与MAD相当的推理质量的同时，将token消耗和推理时间减少了约50%，实现了\"既保持多代理协作的优势，又大幅降低计算成本\"的研究目标。这一解决方案不仅解决了特定问题，更为高效多代理协作提供了新的范式。"
                },
                {
                    "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
                    "arxiv_id": "2509.21070",
                    "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
                    "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我对这篇论文进行了全面分析： 第一步：核心判断 这篇论文的核心是提出ScaleDiff管道，通过大规模生成困难数学问题来训练大语言模型，从而提高其数学推理能力。论文关注的是增强LLM的基础推理能力，特别是数学推理这一通用能力的重要组成部分。论文提出了新的训练范式（通过生成困难问题进行训练），并展示了这种方法能显著提高模型性能，符合\"改进LLM的基础能力、提出新的训练范式、增强其推理能力\"的保留标准。 第二步：正面指标 - 核心概念：论文明确提到了\"Large Reasoning Models (LRMs)\"，以及Qwen2.5-Math-7B-Instruct等具体模型，符合LLMs核心概念。 - 能力方向：论文直接针对\"mathematical reasoning\"（数学推理），这是推理能力的重要方面，符合reasoning能力方向。 虽然论文未涉及强化学习、智能体系统等其他正面指标，但已满足最关键的两项。 第三步：排除标准 论文不涉及多模态与视觉、特定应用领域（数学推理在此被视为通用能力而非特定应用领域）或模型可靠性（应用层面）等排除标准中的任何内容。 第四步：特殊和模糊情况 论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合判断，这篇论文的核心贡献是提出了一种通过大规模生成困难数学问题来提升LLM推理能力的新方法，直接针对\"大语言模型通用推理能力\"的研究目标，因此符合筛选要求。",
                    "summary2": "本文旨在解决大规模生成困难数学问题以提升大型推理模型(LRMs)的复杂数学推理能力的问题。针对数学推理任务，我们提出了一种ScaleDiff流程，通过AdaptThink识别困难问题并训练DiffGen-8B生成器，在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500等基准上通过准确率指标验证了其有效性，平均准确率达65.9%，比原始数据集提高11.3%。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在复杂问题解决方面展现了令人印象深刻的能力，通常受益于训练于能激发复杂推理的困难数学问题。近期的努力已探索通过提示专有模型或大规模开源模型来自动合成数学问题，这些模型基于种子数据或固有的数学概念。然而，由于这些方法的高计算/API成本、提示的复杂性以及生成问题的难度水平有限，扩大这些方法的规模仍然具有挑战性。\n\n为了克服这些限制，我们提出了ScaleDiff，一个简单而有效的流程（pipeline），旨在扩大困难问题的创建规模。我们使用自适应思维模型（adaptive thinking model），仅需一次前向传递（forward pass）就能高效地从现有数据集中识别困难问题，该模型能够感知问题难度并在\"Thinking\"（思考）和\"NoThinking\"（无思考）模式之间自动切换。然后，我们在这个过滤后的困难数据上训练了一个专门的困难问题生成器（DiffGen-8B），它能够大规模生成新的困难问题，消除了对复杂的、每个实例的提示（per-instance prompting）及其相关的高API成本的需求。\n\n在ScaleDiff-Math数据集上微调（fine-tuning）Qwen2.5-Math-7B-Instruct，相比原始数据集带来了11.3%的显著性能提升，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超越了像OpenThinker3这样的近期强大大型推理模型（LRMs）。值得注意的是，这一性能是使用成本效益高的Qwen3-8B模型作为教师模型（teacher model）实现的，表明我们的流程能够有效转移高级推理能力，而无需依赖更大、更昂贵的教师模型。此外，我们观察到随着困难问题数量的增加，模型在困难基准测试（difficult benchmarks）上的性能呈现出明显的扩展现象（scaling phenomenon）。\n\n代码：https://github.com/QizhiPei/ScaleDiff。",
                    "inspiration_trace": "## 面临的挑战\n创建困难数学问题成本高昂，现有自动合成方法计算成本高、提示设计复杂，且生成问题难度有限。传统难度评估方法（失败率和LLM-as-a-judge）效率低下或对提示规则过于敏感。\n\n## 关键洞察\n困难问题需要复杂推理过程，能刺激更复杂的模型行为。AdaptThink模型能根据问题难度自动切换\"Thinking\"和\"NoThinking\"模式，可作为高效问题难度识别器。训练专门的困难问题生成器可大规模创建新问题，避免复杂提示设计和高计算成本。\n\n## 解决方案演进\n首先利用AdaptThink识别现有数据集中的困难问题；然后训练专门的困难问题生成器(DiffGen-8B)；用其生成大规模新问题；通过小模型(Qwen3-8B)蒸馏长CoT解决方案；应用规则和模型过滤确保质量；组合成ScaleDiff-Math数据集；微调得到最终模型。\n\n## 创新点总结\n创新点在于将自适应思维模型转化为高效问题难度识别器，训练专门生成器实现大规模困难问题创建，使用小模型进行成本效益高的知识蒸馏，并观察到困难问题数量与模型性能的明确扩展关系。"
                },
                {
                    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
                    "arxiv_id": "2509.21124",
                    "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
                    "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心原因如下： 1. **核心判断**：论文的本质是关于改进LLM的基础推理能力。它提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力。论文明确关注如何提高模型在数学推理方面的能力，这属于改进LLM基础能力的范畴，而不是将LLM作为工具应用于特定领域。 2. **正面指标**：论文包含多个关键正面指标： - 核心概念：论文明确研究foundation model（基础模型），通常指LLM - 能力方向：专注于reasoning（推理能力），特别是mathematical reasoning（数学推理） - 训练方法：涉及reinforcement learning（强化学习）作为提高推理能力的方法 - 新兴范式：提出了关于chains of thought patterns的新方法，这与思维链(CoT)相关的研究方向一致 3. **排除标准**：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等），而是关注通用数学推理能力 - 不关注模型可靠性的应用层面问题（如水印、安全等） 4. **特殊和模糊情况**：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊领域。 论文的核心贡献是定义了\"推理潜力\"的概念，并提出了从CoT序列中抽象原子推理模式，构建富含高价值推理模式的核心参考集的方法，以及双粒度算法来高效选择高价值CoT数据。这些都是直接针对提高LLM本身推理能力的研究，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在提升基础模型的推理潜力。针对数学推理任务，我们提出了一种CoTP框架，通过从CoT序列中抽象原子推理模式并构建核心参考集，利用双粒度算法（推理模式链和token熵）选择高价值CoT数据。在AIME 2024和2025、HMMT 2025等数学推理基准上，仅使用10B-token的CoTP数据使85A6B MoE模型性能提升9.58%，并将下游RL性能上限提高7.81%，验证了方法的有效性。",
                    "summary_translation": "近期，用于挑战性数学推理的大型推理模型(reasoning models)的进展主要由强化学习(reinforcement learning, RL)推动。研究表明，在中期训练(mid-training)过程中融入长思维链(chain-of-thought, CoT)数据也能显著提升推理深度。然而，当前方法通常不加区分地利用CoT数据，留下了一个关键问题：哪些数据类型能最有效地增强模型推理能力。\n\n在本文中，我们首次将基础模型(foundation model)的推理潜力(reasoning potential)定义为正确回答问题所需的独立尝试次数的倒数，该指标与最终模型性能强相关。随后，我们提出利用富含高价值推理模式(reasoning patterns)的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式(atomic reasoning patterns)，并用它们构建一个富含宝贵推理模式的核心参考集(core reference set)。此外，我们提出一种涉及推理模式链和令牌熵(token entropy)的双粒度算法(dual-granularity algorithm)，高效地从数据池中选择与核心集一致的高价值CoT数据(CoTP)，从而训练模型有效掌握推理。\n\n仅需100亿令牌(10B-token)的CoTP数据，就能使85A6B专家混合模型(Mixture-of-Experts, MoE)在具有挑战性的AIME 2024和2025测试中提升9.58%，并将下游RL性能的上限提高7.81%。",
                    "inspiration_trace": "# 论文核心方法逻辑链推演：从问题识别到解决方案的演进\n\n## 一、问题识别与背景洞察\n\n### 初始观察：现有方法的局限性\n作者首先观察到大型推理模型(LRMs)在数学推理领域的进展主要由强化学习(RL)驱动，同时中间训练阶段(mid-training)融入长思维链(CoT)数据能显著提高推理深度。然而，他们敏锐地识别出一个关键问题：**当前方法通常不加区分地使用CoT数据，缺乏对哪些数据类型最有效增强模型推理能力的深入研究**。\n\n### 核心洞察：基础模型的潜力限制\n通过分析现有研究，作者获得了一个关键洞察：**基础模型的参数空间中隐含着具有挑战性推理的潜在路径，而RL训练只是将这些隐含能力显性化**。这意味着基础模型学到的推理能力直接影响并限制了RL性能的上限。某些开源基础模型(如Llama)显示不稳定的RL性能，进一步证实了这一观点。\n\n> **思维转折点**：作者从\"如何改进RL算法\"转向\"如何扩展基础模型的推理潜力\"，认识到后者才是提升模型性能的根本。\n\n## 二、理论定义与问题形式化\n\n### 推理潜力的概念化\n为了将抽象的\"推理能力\"转化为可研究的概念，作者首次定义了\"推理潜力\"(reasoning potential)：\n- 对于给定模型M和问题qi，模型潜力Φ(M, qi)定义为模型从其输出分布中采样时生成正确答案的概率\n- 整体模型潜力定义为评估数据集上的期望潜力\n\n### 基本关系的形式化\n作者进一步建立了模型潜力与推理成本之间的基本关系：\n- **模型潜力是预期首次通过时间(expected first-passage time)的倒数**\n- 这意味着扩展推理潜力等同于减少正确回答问题所需的平均推理尝试次数\n\n> **思维转折点**：作者将模糊的\"推理能力\"概念转化为精确的数学定义，为后续研究提供了可量化的理论基础。\n\n## 三、核心假设与研究方向\n\n### 理想数据集的假设\n基于上述定义，作者提出了一个核心假设：**存在一个理想的\"神谕\"训练数据集D*oracle，能使基础模型实现最大的推理潜力**。然而，这样的数据集在现实中难以获得。\n\n### 研究方向的明确\n作者将研究问题明确为：**如何从给定的源数据集Dsource中选择一个训练子集D*train，使其训练的模型与在D*oracle上训练的模型之间的推理潜力差距最小？**\n\n### 关键洞察：推理模式的价值\n通过深入分析，作者获得了另一个关键洞察：**不同的CoT数据包含不同价值的推理模式，而这些模式对扩展模型的推理潜力有不同影响**。高质量的推理模式应该具有共性和归纳能力，能够适用于多样化的问题领域。\n\n> **思维转折点**：作者从\"如何增加训练数据量\"转向\"如何识别和利用高价值推理模式\"，认识到数据质量比数量更重要。\n\n## 四、解决方案：CoTP框架的构建\n\n### 核心集近似策略\n由于难以确定D*oracle，作者提出了一个创新策略：**使用精心构建的参考核心集来近似理想的\"神谕\"数据集**。这个核心集应包含富含多样化高价值推理模式的CoT数据。\n\n### 双粒度表示的创新\n为了全面捕捉推理特性，作者提出了双粒度表示方法：\n1. **模式链粒度**：捕捉高度抽象的推理范式和思维结构\n2. **token熵粒度**：捕捉具有高推理增益的token级特征\n\n### 推理模式的抽象与提取\n作者从CoT序列中抽象出\"原子推理模式\"的概念：\n- 推理模式ρ代表适用于多样化问题领域的基本推理步骤\n- 模式链C=[ρ₁, ρ₂, ..., ρₙ]是从CoT序列中提取的推理模式的有序序列\n\n> **思维转折点**：作者从\"直接使用原始CoT数据\"转向\"提取和利用结构化的推理模式\"，实现了从原始数据到知识表示的升华。\n\n## 五、算法设计：从理论到实践\n\n### 距离度量的创新\n为了选择与核心集相似的CoT数据，作者设计了一个双粒度距离度量：\n- 结合模式链距离和token熵链距离\n- 使用加权的动态时间规整(DTW)算法计算相似性\n\n### 数据选择的优化\n作者将数据选择问题形式化为一个优化问题：\n- 目标：最小化所选数据与核心集之间的距离\n- 约束：每个核心实例分配固定数量的源实例\n- 求解：通过匈牙利算法高效求解\n\n### 整体框架的整合\n最终，作者将这些组件整合为完整的CoTP框架：\n1. 构建富含高价值推理模式的核心集\n2. 从源数据池中提取模式链和熵链\n3. 使用双粒度算法选择与核心集对齐的数据\n4. 训练模型掌握有效的推理能力\n\n> **思维转折点**：作者将抽象的理论洞察转化为具体的算法实现，构建了一个从数据选择到模型训练的完整流程。\n\n## 六、实验验证与效果评估\n\n### 数据集的构建\n作者构建了LongCoTPool数据池，整合了多种数学QA数据集，并使用DeepSeek-R1生成长推理CoT序列。\n\n### 实验结果的分析\n实验结果表明：\n- 仅用10B高价值推理数据就能显著提升模型在多个挑战性数学推理任务上的性能\n- CoTP不仅保持了通用性能，还在AIME 2024和2025上实现了9.58%的提升\n- 提高了下游RL性能的上限，平均提升7.81%\n\n### 方法有效性的验证\n通过详细的实验分析，作者验证了：\n- 推理模式的重要性：高价值推理模式确实能扩展模型的推理潜力\n- 双粒度算法的优越性：同时考虑模式链和token熵的效果优于单一粒度\n- 方法的高效性：相对较小的数据量就能实现显著效果\n\n> **思维转折点**：作者通过严格的实验验证，不仅证明了方法的有效性，还深入分析了各组件的贡献，完成了从理论假设到实证验证的闭环。\n\n## 七、总结：思想演进的关键路径\n\n作者提出CoTP框架的思想演进可以概括为以下关键路径：\n\n1. **从现象到本质**：从观察RL性能受基础模型限制的现象，深入到推理潜力的本质定义\n2. **从模糊到精确**：将模糊的\"推理能力\"概念转化为精确的数学定义和可量化指标\n3. **从整体到局部**：从整体CoT数据分析深入到原子推理模式的提取和利用\n4. **从单一到多维**：从单粒度分析扩展到双粒度(模式链和token熵)的综合考量\n5. **从理论到实践**：将理论洞察转化为具体的算法实现和实验验证\n\n这一思想演进体现了作者从问题识别、理论定义、洞察分析到解决方案构建的完整思维过程，展示了系统性的研究方法和严谨的逻辑推理能力。通过这一演进，作者不仅解决了如何有效扩展基础模型推理潜力的问题，还为大型语言模型的推理能力研究提供了新的思路和方法。"
                },
                {
                    "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
                    "arxiv_id": "2509.21054",
                    "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
                    "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于研究大语言模型(LLMs)和大型推理模型(LRMs)的推理能力如何影响其在多智能体系统中的行为表现。 首先，从核心判断来看，论文的本质是研究模型的基础推理能力，特别是其\"显式推理能力\"如何影响说服动态。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的研究范围，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标：核心概念上明确研究LLMs和LRMs；能力方向上聚焦于推理过程，特别是显式推理能力；新兴范式上研究多智能体系统(MAS)的协作与交互。 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不主要研究模型可靠性的应用层面问题。 在特殊和模糊情况处理上，该论文研究的是通用的多智能体系统框架，探索模型推理过程与说服效果的关系，而不是将智能体应用于特定领域。虽然论文提到了安全性和鲁棒性，但这是作为研究意义的一部分，而不是主要焦点。 核心贡献在于揭示了\"说服二元性\"(Persuasion Duality)这一基本权衡关系：推理过程使模型对说服更具抵抗力，同时通过使推理过程透明化又能显著提高模型说服他人的能力。这直接关联到模型的内部处理架构与外部行为表现，为提升大语言模型的通用推理能力提供了新的见解。",
                    "summary2": "本文旨在探究多智能体系统中模型推理过程如何影响说服动态。针对多智能体系统中的LLMs和LRMs交互场景，我们提出了\"说服二元性\"理论，揭示了显式推理机制在增强抵抗说服能力的同时，通过分享思考内容能显著提高说服力的权衡关系。在MMLU和PersuasionBench数据集上，通过Persuaded-Rate、Remain-Rate等指标验证了LRMs的推理过程不仅增强抵抗说服能力，还通过透明化思考内容提高说服效果，并进一步提出了基于对抗性论证检测的提示级缓解机制。",
                    "summary_translation": "近期多智能体系统（Multi-Agent Systems, MAS，多智能体系统）的快速普及，其中大型语言模型（Large Language Models, LLMs，大型语言模型）和大型推理模型（Large Reasoning Models, LRMs，大型推理模型）通常协作解决复杂问题，这 necessitates（需要）对控制其交互的说服动态（persuasion dynamics，说服动态）进行深入理解。本文挑战了当前的主流假设，即说服效果（persuasive efficacy，说服效果）主要取决于模型规模（model scale，模型规模）。我们提出，这些动态实际上是由模型的底层认知过程（cognitive process，认知过程）决定的，特别是其显式推理（explicit reasoning，显式推理）能力。通过一系列多智能体说服实验，我们发现了一个我们称之为\"说服二元性\"（Persuasion Duality，说服二元性）的基本权衡。我们的研究结果显示，LRMs中的推理过程表现出显著更强的抗说服能力，更稳健地保持其初始信念。相反，通过共享\"思考内容\"（thinking content，思考内容）使推理过程透明化，显著提高了它们说服他人的能力。我们进一步考虑了更复杂的传播说服（transmission persuasion，传播说服）情境，并揭示了多智能体网络间多跳说服（multi-hop persuasion，多跳说服）中影响传播和衰减的复杂动态。本研究提供了将模型内部处理架构与其外部说服行为联系起来的系统证据，为高级模型的易受影响性提供了新的解释，并强调了未来MAS的安全性、稳健性和设计的关键意义。",
                    "inspiration_trace": "# 论文核心方法的逻辑演进分析\n\n## 从挑战到洞察：思想演进的逻辑链\n\n### 1. 面临的核心挑战\n\n作者研究之初面临一系列相互关联的挑战：\n\n**挑战一：模型规模范式的局限性**\n- 主流观点认为模型规模是说服效能的主要决定因素\n- 作者观察到这种关系存在\"收益递减\"现象，前沿模型在任务完成指标上已接近性能天花板\n- 简单增加模型规模不太可能带来说服能力的显著提升，需要寻找新的解释框架\n\n**挑战二：多智能体系统说服动态的理解缺口**\n- 随着LLM在多智能体系统中的广泛应用，理解智能体间的说服动态对确保系统可靠性、安全性和对齐至关重要\n- 现有研究主要关注人类对AI或AI对人类的影响，而忽视了AI智能体之间的相互说服动态\n- 缺乏将模型内部认知过程与外部说服行为联系起来的研究\n\n**挑战三：LLM说服的概念和度量难题**\n- LLM不像人类那样具有真正的心理状态，直接将人类说服定义应用于LLM存在问题\n- 需要建立适合LLM的说服定义和度量标准，以进行系统性研究\n\n### 2. 关键洞察的逐步形成\n\n作者通过系统性的思考和实验，获得了以下关键洞察：\n\n**洞察一：认知过程而非规模是说服动态的根本决定因素**\n- 通过区分两类模型：大型语言模型(LLM，主要依靠隐式模式识别)和大型推理模型(LRM，采用显式推理过程)\n- 发现认知架构的差异而非参数数量，才是决定智能体说服能力和易被说服性的主要因素\n- 这代表从\"规模中心\"到\"过程中心\"的范式转变\n\n**洞察二：\"说服二元性\"(Persuasion Duality)的发现**\n- 识别出一个核心权衡关系：使智能体论证更具逻辑性和透明度的机制(如思维链CoT)也使该智能体更能抵抗有缺陷的论证\n- 这揭示了智能体设计的基本权衡：增强说服能力可能需要同时加强对外部输入的怀疑态度\n- 这一洞察将说服能力和抵抗力视为同一枚硬币的两面，而非独立特性\n\n**洞察三：思维内容的双重作用**\n- 发现对于LRM，共享\"思维内容\"可显著提高其说服他人的能力\n- 同时，使用思维模式会增强模型对错误信息的抵抗力\n- 这揭示了思维过程在说服中的双重作用：既是增强说服力的工具，也是抵抗说服的屏障\n\n**洞察四：模型易受说服的内在机制**\n- 通过注意力机制分析，发现模型在评估说服性论证时倾向于优先考虑表面信息(如自信的断言)而忽视实质性推理\n- 这解释了为什么模型容易受到误导信息的影响：其注意力偏向于自信语言而非事实内容\n\n**洞察五：多跳说服的复杂动态**\n- 发现说服影响在多智能体链中以非线性方式传播，表现出放大和衰减效应\n- 这表明在设计稳健的多智能体系统架构时，需要考虑传递和网络层面的动态\n\n### 3. 解决方案的提出与演进\n\n基于上述洞察，作者提出了一系列解决方案：\n\n**解决方案一：建立认知架构与说服行为的联系**\n- 通过大规模实证研究，建立了LRM内部推理过程与外部说服行为之间的明确联系\n- 考虑了模型作为说服者和被说服者的双重角色，提供了系统性证据\n- 这为理解多智能体系统中的说服动态提供了新的理论框架\n\n**解决方案二：形式化和验证\"说服二元性\"**\n- 识别并形式化了这一基本权衡关系，通过多智能体说服实验验证了其存在\n- 揭示了增强智能体说服能力与增强其抵抗说服能力之间的内在联系\n- 这为多智能体系统的设计提供了重要指导原则\n\n**解决方案三：开发提示级别的缓解机制**\n- 基于对模型注意力机制的理解，提出了\"对抗性论证检测\"提示\n- 指导被说服者批判性地评估所接收信息的逻辑和证据，识别不支持或纯粹修辞性的主张\n- 这种提示级别的干预显著增强了模型对说服的抵抗力，提供了一种实用的防御机制\n\n**解决方案四：探索多跳说服的动态**\n- 将分析扩展到成对互动之外，考虑更复杂的多智能体链\n- 对说服传播进行了初步评估，揭示了影响传播和衰减的复杂动态\n- 这为理解更复杂的多智能体系统中的信息传播提供了基础\n\n## 思想演进的核心脉络\n\n作者的思想演进展现了以下特点：\n\n1. **从现象到本质**：从观察到的现象(模型规模与说服效能关系的收益递减)出发，深入探究其背后的本质原因(认知过程的关键作用)，体现了从表及里的思考过程。\n\n2. **从单一到系统**：不仅关注单一智能体的说服能力或易被说服性，而是将两者联系起来，发现了\"说服二元性\"这一系统性的权衡关系，体现了系统思考的能力。\n\n3. **从静态到动态**：不仅研究静态的成对互动，还探索了动态的多跳说服传播，揭示了更复杂的网络层面的动态，体现了动态思考的能力。\n\n4. **从描述到解释**：不仅描述了说服现象，还通过注意力机制分析解释了为什么模型容易受到误导信息的影响，体现了追求深层次理解的思考方式。\n\n5. **从理论到应用**：不仅提出了理论洞见，还基于这些洞见开发了实用的解决方案，体现了理论联系实际的思考方式。\n\n这一完整的思想演进过程，从挑战出发，通过系统性研究获得关键洞察，最终提出创新解决方案，展现了作者深入、系统和实用的思维方式，为多智能体系统中的说服动态研究提供了新的视角和方法。"
                },
                {
                    "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
                    "arxiv_id": "2509.21016",
                    "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
                    "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是研究LLMs如何通过强化学习(RL)获取和泛化全新的推理策略，而非仅依赖预训练或后训练期间编码的技能。论文提出了DELTA-Code基准来探究LLMs能否通过RL解决预训练模型无法解决的问题，以及这种能力能否迁移到分布外测试集。这明显属于改进LLM基础能力、提出新训练范式、增强其逻辑推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标，包括核心概念\"Large language models, LLMs\"，能力方向\"reasoning\"(明确讨论\"new reasoning strategies\"和\"RL-driven reasoning\")，以及训练方法\"reinforcement learning (RL)\"(论文的核心方法)。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域(虽然使用编程算法作为测试平台，但目的是研究通用推理能力而非解决编程领域问题)或模型可靠性(应用层面)。 第四步特殊和模糊情况：虽然论文使用编程算法作为测试平台，但核心目标是理解LLMs如何通过强化学习超越现有先验知识获取新算法技能，而非解决编程领域的具体问题。这属于研究LLM通用推理能力的工作。 论文的核心贡献是提出了DELTA-Code基准，用于评估LLMs在算法编程问题上的学习能力和迁移能力，揭示了RL训练模型经历的\"grokking\"相变现象，并探索了使模型能够学习以前无法解决问题的关键训练要素。这些研究直接关注如何提升LLM的通用推理能力，符合研究目标。",
                    "summary2": "本文旨在解决LLMs是否能通过RL获得真正新的推理策略的问题。针对合成编程问题家族，我们提出了DELTA基准，用于评估LLMs的可学习性和可转移性，并在Manufactoria和BouncingSim等数据集上通过pass@K和full-pass rate验证了其有效性。实验揭示了RL训练中的grokking相变现象，证明RL能解锁预训练模型无法执行的新策略，并在探索性和组合性泛化上表现良好，但在变革性泛化上仍有局限。",
                    "summary_translation": "大型语言模型（LLMs）是否能够获取或泛化真正新颖的推理策略，而不仅仅是在预训练或后训练期间编码在其参数中的优化技能，这仍然是一个开放性问题。为了尝试回答这一争论，我们提出了DELTA-Code——算法编码中的可学习性和可迁移性的分布评估（Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding），这是一个受控的合成编码问题家族基准，旨在探究两个基本方面：可学习性（learnability）——大型语言模型能否通过强化学习（RL）解决预训练模型在足够尝试次数下仍表现失败的问题家族（pass@K=0）？——以及可迁移性（transferrability）——如果实现了可学习性，这些技能能否系统地迁移到分布外（OOD）测试集？与先前的公共编码数据集不同，DELTA通过模板化问题生成器来分离推理技能，并引入了完全分布外（OOD）的问题家族，这些问题需要新颖策略而非工具调用或记忆模式。我们的实验揭示了一个显著的理解（grokking）相变：在经过一段接近零奖励的长期过程后，强化学习训练的模型突然提升至接近完美的准确率。为了在先前无法解决的问题家族上实现可学习性，我们探索了关键训练要素，如密集奖励的分阶段预热（staged warm-up）、经验回放（experience replay）、课程训练（curriculum training）和循环验证（verification-in-the-loop）。除了可学习性，我们还使用DELTA来评估探索性、组合性和变革性轴上的可迁移性或泛化能力，以及跨家族迁移。结果显示，在问题家族内部和重组技能方面取得了显著进展，但在变革性情况下仍存在持续弱点。因此，DELTA提供了一个干净的测试平台，用于探究强化学习驱动推理的极限，并理解模型如何超越现有先验知识来获取新的算法技能。",
                    "inspiration_trace": "# DELTA-Code方法逻辑演进分析\n\n## 一、研究起点：核心问题的识别\n\n作者的研究始于一个根本性的学术争论：**强化学习(RL)对大型语言模型(LLMs)的作用本质是什么？**\n\n### 1.1 对立观点的识别\n- **锐化论**：以Yue等(2025)和Wu等(2025)为代表，认为RL仅能锐化模型参数中已编码的潜在技能，无法超越基础模型的表示边界。\n- **涌现论**：以Liu等(2025b,a)为代表，认为RL是解锁LLMs中涌现问题解决能力的关键方式。\n\n### 1.2 问题可测试化\n为使这一哲学辩论可实证检验，作者提炼出两个可操作的核心标准：\n- **可学习性(Learnability)**：RL能否使LLMs掌握基础模型完全无法执行的策略？\n- **可迁移性(Transferability)**：新获得的策略能否系统性迁移到分布外(OOD)案例，而非仅记忆模式？\n\n## 二、现有方法批判：数据集局限性的揭示\n\n### 2.1 现有基准的缺陷分析\n作者敏锐地发现当时主流编程/数学基准(如Numina-Math, DeepMath, OpenCodeReasoning)存在根本性局限：\n- **主题混杂**：不同难度和主题的问题混合在一起，模糊了能力锐化与新策略获取的界限。\n- **变量不可控**：无法精确控制问题分布和难度，难以将性能提升归因于特定技能。\n- **工具依赖**：许多数学问题可通过调用Python等外部工具解决，掩盖了模型自身推理能力。\n\n### 2.2 编程任务的独特优势\n通过对不同领域的比较分析，作者发现编程任务具有独特优势：\n- **天然密集奖励**：测试用例提供细粒度反馈，可作为密集奖励信号。\n- **自动可验证**：程序正确性可自动验证，无需人工评分。\n- **难度可调节**：问题复杂度可系统调整，适合研究技能获取过程。\n\n## 三、关键洞察：顿悟现象与分阶段训练\n\n### 3.1 RL在\"pass@K=0\"任务上的困境\n作者发现了一个关键挑战：当基础模型在大量尝试下仍完全无法解决问题(pass@K=0)时，标准RL会因缺乏正向信号而崩溃。这一现象解释了为何先前研究可能低估了RL的能力。\n\n### 3.2 顿悟(Grokking)相变的发现\n通过受控实验，作者观察到一个引人注目的现象：在长时间接近零奖励的探索后，模型性能突然跃升至接近完美。这种\"顿悟\"相变表明RL确实能发现基础模型无法执行的策略。\n\n### 3.3 分阶段训练的核心洞察\n基于上述发现，作者提出了关键洞察：**RL训练需要分阶段进行**：\n1. **预热阶段**：使用密集奖励信号(如每个测试用例通过率)引导模型脱离全零区域。\n2. **收敛阶段**：切换到二元奖励(全通过/失败)，锐化解方案为精确完成。\n\n这种分阶段策略解决了\"pass@K=0\"任务的信号稀疏性问题，使模型能够从部分正确进展逐步探索到完整解决方案。\n\n## 四、解决方案设计：DELTA基准的构建\n\n### 4.1 受控合成问题家族的设计\n为精确研究可学习性和可迁移性，作者设计了DELTA基准，包含三类问题：\n\n#### 4.1.1 完全OOD问题(Manufactoria)\n- **创新点**：基于经典Flash游戏重新设计，使用全新程序语法和问题解决策略。\n- **价值**：确保问题真正分布外，排除预训练数据干扰，提供\"干净\"的推理能力测试环境。\n\n#### 4.1.2 物理模拟任务(BouncingSim)\n- **创新点**：2D弹跳球模拟编程任务，要求精确碰撞检测和数值稳定积分。\n- **价值**：提供几何感知推理的测试平台，支持系统性难度调整和技能组合研究。\n\n#### 4.1.3 竞赛编程问题\n- **创新点**：将真实竞赛编程问题转化为受控家族，保留算法本质但变化叙事表面。\n- **价值**：连接合成与真实世界，扩展研究范围。\n\n### 4.2 三轴泛化评估框架\n作者创新性地扩展了OMEGA框架，设计了三个泛化轴：\n1. **探索性泛化**：在家族内扩展已知技能(如从六边形到八边形)。\n2. **组合性泛化**：结合先前分离的技能(如旋转障碍物与移动盒子)。\n3. **变革性泛化**：发现非传统解决方案(如保证周期性的特殊初始状态)。\n\n## 五、方法创新：加速RL顿悟的策略\n\n### 5.1 分阶段训练策略\n作者提出的核心创新是分阶段训练策略：\n- **阶段1(密集奖励)**：使用每个测试用例通过率作为奖励，提供梯度信号引导探索。\n- **阶段2(二元奖励)**：切换到全通过/失败奖励，锐化解方案为精确完成。\n\n这种策略成功解决了\"pass@K=0\"任务的信号稀疏性问题，使模型能够经历\"探索→顿悟→收敛\"的完整学习过程。\n\n### 5.2 加速顿悟的辅助策略\n为进一步优化训练过程，作者探索了多种加速顿悟的策略：\n- **经验回放**：保留并重用成功轨迹，但存在离策略问题。\n- **循环中的反馈**：直接在生成过程中包含失败反馈，提高训练不稳定性。\n- **课程学习**：从简单问题逐步过渡到复杂问题，但需要任务间结构相似性。\n\n## 六、实验验证与理论贡献\n\n### 6.1 可学习性验证\n通过在Manufactoria-HAS家族上的实验，作者展示了：\n- 基础模型(Qwen3-4B)达到pass@128=0，完全无法解决问题。\n- 分阶段RL训练使模型达到100%全通过率，证明RL确实能解锁新策略。\n- 清晰展示了顿悟相变：长时间探索后突然跃升至完美性能。\n\n### 6.2 泛化性验证\n在BouncingSim上的实验揭示了不同类型的泛化能力：\n- **探索性泛化**：从Basic到Easy/Medium表现良好，但Hard/Extreme困难。\n- **组合性泛化**：令人惊讶地强(60-70%全通过率)，表明编程任务的结构性组合优势。\n- **变革性泛化**：接近零性能，表明发现全新解决方案范式仍是挑战。\n\n### 6.3 理论贡献\n作者的研究做出了重要理论贡献：\n- **解决学术争论**：提供明确证据表明RL不仅能锐化现有能力，还能发现基础模型无法执行的新策略。\n- **揭示训练本质**：证明训练方式与训练内容同等重要，分阶段训练是解锁新能力的关键。\n- **建立研究范式**：DELTA基准为研究RL驱动的推理提供了干净、可控的测试平台。\n\n## 七、启示与展望\n\n### 7.1 研究范式转变\n作者呼吁研究社区关注\"困难子集\"：\n- 现有基准通常报告混合池的平均性能，掩盖了真正困难案例的独特学习动态。\n- 这些困难案例(pass@K=0)表现出顿悟相变，需要数百甚至数千训练步才能解决。\n\n### 7.2 跨领域应用前景\n作者展望了方法向其他领域的扩展：\n- 编程中的密集奖励原则可扩展到数学和科学领域。\n- 当细粒度信号可用时(如基于评分的评分、逐步检查器、定理证明器验证)，类似方法可应用于更广泛的推理任务。\n\n### 7.3 训练哲学的重构\n作者的研究最终指向一个更深刻的认识：\n- **训练方式与训练内容同等重要**：分阶段预热、经验回放和验证/反馈循环等具体训练选择，对于释放LLMs的推理潜力至关重要。\n- **锐化与发现的并存**：RL既能锐化现有先验，也能发现新策略，具体结果取决于奖励设计、数据混合、任务难度和训练方法的综合作用。\n\n## 总结\n\nDELTA-Code方法的提出展现了一个清晰的逻辑演进：从识别核心学术争论，到批判现有方法局限，获得关键洞察，设计创新解决方案，并通过实验验证其有效性。这一研究不仅解决了关于RL在LLMs中作用的争论，还为未来研究提供了新范式和方向，展示了受控实验设计在揭示复杂学习现象中的价值。"
                },
                {
                    "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
                    "arxiv_id": "2509.21012",
                    "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
                    "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究大语言模型的上下文学习(ICL)内在机制，特别是从\"信息移除\"的新角度解释ICL如何工作。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制——上下文学习能力，这属于LLM的核心推理能力之一。论文揭示了ICL通过选择性移除冗余信息来提高模型在特定任务上表现的机制，这直接关系到提升LLM的通用推理能力。 第二步：正面指标 - 核心概念：论文明确研究现代语言模型(LMs)的上下文学习机制，符合\"Large language models, LLMs\"这一核心概念。 - 能力方向：虽然论文没有直接提到reasoning、planning等词汇，但上下文学习(ICL)本身就是LLM进行推理和问题解决的关键机制。论文研究的信息移除机制直接关系到模型如何聚焦于特定任务并进行有效推理，因此与\"reasoning\"和\"problem-solving\"能力方向高度相关。 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有研究多模态与视觉相关内容 - 没有将LLM应用到任何特定领域（如医疗、化学等） - 没有关注模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况 论文属于增强模型内在可解释性的研究。通过揭示ICL的信息移除机制和识别关键的\"去噪头\"，论文提高了我们对LLM如何进行推理的理解，这属于增强模型内在可解释性的研究，应该保留。 核心贡献：论文揭示了上下文学习的一个关键机制——面向任务的信息移除过程，解释了LLM如何通过选择性移除冗余信息来提高在特定任务上的表现。这一发现不仅增进了我们对LLM推理机制的理解，还可能为改进LLM的推理能力提供新思路。 因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，应该被保留。",
                    "summary2": "本文旨在解释大型语言模型中上下文学习(ICL)的内在机制。针对ICL在未见标签场景下仍然有效的现象，我们提出了一种任务导向信息移除的解释框架，并在Llama和Qwen等多个现代LMs上通过准确率、偏心率和协方差通量等指标验证了其有效性。我们识别了负责这种信息移除的\"去噪头\"，并通过消融实验证明它们是ICL在未见标签场景下保持性能的关键组件。",
                    "summary_translation": "In-context Learning (ICL, 上下文学习)是一种新兴的基于现代Language Models (LMs, 语言模型)的few-shot learning (小样本学习)范式，但其内在机制尚不明确。在本文中，我们通过information removal (信息移除)的新视角来研究这一机制。具体而言，我们证明在zero-shot scenario (零样本场景)中，语言模型将查询编码为hidden states (隐藏状态)中的non-selective representations (非选择性表征)，这些表征包含所有可能任务的信息，导致模型无法专注于目标任务而产生任意输出，最终准确率接近于零。同时，我们发现通过low-rank filter (低秩滤波器)选择性地从隐藏状态中移除特定信息，能有效引导语言模型专注于目标任务。基于这些发现，通过精心设计的度量指标测量隐藏状态，我们观察到few-shot ICL (小样本ICL)有效模拟了这种task-oriented (面向任务)的信息移除过程，选择性地从entangled non-selective representations (纠缠的非选择性表征)中移除冗余信息，并根据示例改进输出，这构成了ICL的关键机制。此外，我们确定了引发移除操作的关键attention heads (注意力头)，称为Denoising Heads (去噪头)，这使得我们能够进行ablation experiments (消融实验)，在推理过程中阻断信息移除操作，此时ICL准确率显著下降，特别是当小样本示例中缺少正确标签时，这证实了信息移除机制和去噪头的关键作用。",
                    "inspiration_trace": "# 从挑战到洞察：ICL机制的新解释框架\n\n## 面临的核心挑战\n\n当前基于诱导头（Induction Heads）的ICL机制解释存在明显局限。诱导头主要通过识别前缀并复制后缀来工作，在ICL场景中表现为检索与查询最相似的演示并复制对应标签。然而，这种框架面临一个根本性问题：**它完全依赖于复制上下文中已出现的标签**。当正确标签未在演示中出现时（即\"未见标签场景\"），诱导头机制预测ICL会完全失败，但实验表明模型仍能保持一定准确率，这揭示了现有理论的不完整性。\n\n## 关键洞察：信息去除视角\n\n通过深入分析，我提出了一个根本性的转变视角：**ICL的本质是任务导向的信息去除过程**。这一洞察源于以下观察：\n\n1. **零样本隐藏状态的冗余性**：零样本查询的隐藏状态包含丰富语义信息，但其中大量信息与目标任务无关，这些冗余信息干扰了模型产生任务特定的输出。\n\n2. **任务-言语化子空间（TVS）的存在**：在表示空间中存在一个低秩子空间，专门编码任务相关信息。通过投影矩阵W将隐藏状态投影到TVS，可以去除任务无关信息，从而将输出引导至任务特定答案。\n\n3. **信息去除的有效性验证**：通过在零样本隐藏状态中显式注入低秩过滤器W_enc W_dec进行实验，发现即使最大秩为1-2的过滤器也能显著提高开放端解码的准确率，证实了信息去除假设的有效性。\n\n## 解决方案：去噪头机制\n\n基于上述洞察，我进一步识别了实现这一信息去除机制的具体组件——**去噪头（Denoising Heads, DHs）**：\n\n1. **DHs的识别与特性**：\n   - DHs是负责任务导向信息去除的特定注意力头\n   - 与诱导头独立存在，但在功能上互补\n   - 主要分布在模型的中后层，与任务相关处理阶段一致\n\n2. **DHs的工作机制**：\n   - 通过局部重编码模式，选择性关注查询中包含任务相关信息的token\n   - 利用W_Q^T W_K从最后token的隐藏状态中提取任务表示，作为识别任务信息的指示器\n   - 通过注意力计算适当识别并强化任务相关信息，相对减少任务无关信息\n\n3. **DHs的关键作用**：\n   - 在未见标签场景下，当诱导头失效时，DHs成为维持分类准确性的主要机制\n   - 消融实验表明，去除DHs会使未见标签场景的准确率几乎降至零\n   - DHs与诱导头共同构成ICL的完整解释框架，解决了原有理论的局限\n\n这一从\"信息复制\"到\"信息去除\"的视角转变，不仅解释了未见标签场景下的ICL行为，还为理解语言模型如何通过上下文学习适应不同任务提供了更全面的理论框架。"
                },
                {
                    "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
                    "arxiv_id": "2509.20712",
                    "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
                    "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出CE-GPPO算法，一种改进的强化学习方法，用于优化大语言模型处理复杂推理任务的能力。论文专注于改进LLM的基础训练机制，特别是通过更好地管理策略熵来平衡探索和利用，从而提升模型的推理性能。这完全符合\"改进LLM基础能力、提出新训练范式、增强其推理能力\"的核心标准。论文明确关注LLMs的数学推理能力，并通过强化学习方法进行优化，这些都是正面指标。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决强化学习中策略熵不稳定问题。针对大型语言模型优化过程中的熵崩溃和熵爆炸现象，我们提出了一种CE-GPPO算法，通过梯度保持裁剪策略优化，重新引入被裁剪令牌的梯度信号，实现探索-利用平衡。在多个数学推理benchmark上通过准确率等指标验证了其有效性，证明CE-GPPO在不同模型规模上均优于现有基线方法。",
                    "summary_translation": "强化学习（Reinforcement learning, RL）已成为优化大语言模型（large language models, LLMs）处理复杂推理任务的强大范式。此过程中的核心挑战在于管理策略熵（policy entropy），它反映了训练过程中探索（exploration）与利用（exploitation）之间的平衡。现有方法，如近端策略优化（proximal policy optimization, PPO）及其变体，由于剪裁机制（clipping mechanism）而丢弃了来自低概率词元（low-probability tokens）的宝贵梯度信号。我们系统地分析了熵动态（entropy dynamics），并揭示这些被剪裁的词元在调节熵演化中起着关键但被忽视的作用。我们提出了通过梯度保留策略优化控制熵（Controlling Entropy via Gradient-Preserving Policy Optimization, CE-GPPO），这是一种新颖算法，以温和且可控的方式重新引入标准PPO中被剪裁词元的梯度。通过控制来自剪裁区间（clipping interval）外词元的梯度大小，CE-GPPO能够实现探索-利用权衡（exploration-exploitation trade-off）。我们提供了理论依据和实证证据，表明CE-GPPO有效缓解了熵不稳定性（entropy instability）。在数学推理基准测试（mathematical reasoning benchmarks）上的大量实验表明，CE-GPPO在不同模型规模上一致地优于强基线方法。",
                    "inspiration_trace": "# CE-GPPO核心方法逻辑演进分析\n\n## 1. 问题识别与挑战定位\n\n作者首先确立了研究出发点：强化学习(RL)在优化大型语言模型(LLMs)过程中面临的核心挑战——**策略熵管理**。策略熵作为探索与利用平衡的关键指标，其动态控制对训练稳定性至关重要。作者敏锐地指出，现有方法（如PPO及其变种）存在一个根本缺陷：**裁剪机制导致有价值梯度信号的丢失**，特别是来自低概率令牌的梯度被直接丢弃，这引发了熵的不稳定性问题。\n\n## 2. 关键洞察与机制分析\n\n### 2.1 令牌分类与熵动态关联\n\n作者通过系统性分析，建立了令牌特性与熵动态之间的关联框架，将令牌分为四类：\n- **PA&HP**（正优势高概率）和**NA&LP**（负优势低概率）令牌：加速收敛，加剧熵崩溃\n- **PA&LP**（正优势低概率）和**NA&HP**（负优势高概率）令牌：促进探索，维持多样性，缓解熵崩溃\n\n这一分类揭示了不同令牌类型对熵动态的差异化影响，为后续解决方案奠定了理论基础。\n\n### 2.2 裁剪机制的根本缺陷\n\n作者深入分析PPO裁剪机制，发现其关键问题：**裁剪区间外的令牌（主要是低概率令牌）被完全忽略**，导致两个严重后果：\n- **熵崩溃**：由于缺少PA&LP令牌的梯度信号，模型探索受限\n- **熵爆炸**：由于缺少NA&LP令牌的梯度信号，模型过度探索\n\n这一分析揭示了传统方法中\"被丢弃的梯度\"实际上蕴含着控制熵动态的关键信息。\n\n### 2.3 熵变化的理论解析\n\n作者从理论层面推导出策略熵变化的近似表达式：\n```\nH(π_{k+1_θ}|y<t, x) - H(π_{k_θ}|y<t, x) ≈ -η · Cov[log π_{k_θ}, π_{k_θ} · Â_t]\n```\n这一理论突破表明：**熵演化由log概率与概率加权的优势函数之间的协方差控制**。基于此，作者进一步指出裁剪外令牌的梯度对这一协方差有直接影响：\n- PA&LP令牌梯度：减少协方差，减缓熵减少\n- NA&LP令牌梯度：增加协方差，加速熵减少\n\n## 3. 解决方案的创新思路\n\n### 3.1 核心思想转变\n\n基于以上洞察，作者实现了关键思想转变：**从\"丢弃裁剪外梯度\"到\"有控制地利用裁剪外梯度\"**。这一转变将熵控制问题重新定义为\"如何管理裁剪区间外令牌的梯度\"。\n\n### 3.2 梯度保持策略设计\n\n作者提出的CE-GPPO算法核心创新在于：\n1. **梯度保留机制**：通过stop-gradient操作解耦前向和后向传递，使梯度更新不再严格受原始裁剪区间约束\n2. **有界梯度调整**：对裁剪外令牌的梯度进行有界缩放，通过参数β₁和β₂分别控制左右裁剪边界外梯度的大小\n3. **动态平衡控制**：通过调整β₁和β₂，实现对探索-利用平衡的精细控制\n\n### 3.3 稳定性保证机制\n\n作者从理论上证明，尽管CE-GPPO引入了裁剪区间外的梯度信号，但通过**有界缩放机制**（将梯度限制在β₁·(1-ε)或β₂·(1+ε)范围内），确保了整体优化过程的稳定性，避免了策略模型的过度漂移。\n\n## 4. 方法演进逻辑链\n\n总结CE-GPPO的思想演进逻辑链：\n\n1. **问题发现**：PPO裁剪机制导致低概率令牌梯度丢失，引发熵不稳定\n2. **机制解析**：不同类型令牌对熵动态有差异化影响，裁剪外令牌梯度对熵控制至关重要\n3. **理论突破**：熵变化由特定协方差控制，裁剪外梯度直接影响这一协方差\n4. **思路转变**：从丢弃裁剪外梯度到有控制地利用这些梯度\n5. **方案设计**：通过梯度保持和有界缩放机制，实现对熵动态的精细控制\n6. **稳定性保证**：理论证明新方法在引入更多梯度信号的同时保持训练稳定\n\n这一逻辑链展示了作者从问题现象到本质机制，再到创新解决方案的完整思考过程，体现了对强化学习中熵动态机制的深刻理解和创新性解决方案的提出。通过重新利用被传统方法丢弃的梯度信号，作者实现了对探索-利用平衡的更精细控制，从而提升了LLMs在复杂推理任务上的性能。"
                },
                {
                    "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
                    "arxiv_id": "2509.20882",
                    "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
                    "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLM)的上下文学习(ICL)机制的理论解释，特别是基于概念的上下文学习(CB-ICL)。论文提出了理论分析来解释CB-ICL为何以及何时能够在少量示例的情况下表现良好，这属于改进LLM基础能力的理论研究范畴，而非将LLM作为工具应用到特定领域。 从正面指标看，论文明确涉及大语言模型(LLMs)这一核心概念，虽然不直接聚焦于推理、规划或问题解决，但上下文学习(ICL)本身与LLM的通用推理能力密切相关，因为理解上下文中的示例并应用它们解决新问题需要一定程度的推理能力。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性等应用层面的研究。 虽然这篇论文的主要贡献是理论性的，而非直接提出改进LLM推理能力的新方法，但它通过深入理解ICL机制，为改进LLM的基础能力（包括推理能力）提供了理论基础和指导。论文提出的理论量化了LLM可以利用的知识，并提出了相似性度量，这些都为模型预训练和提示工程提供了重要见解，有助于提升LLM的通用推理能力。 因此，这篇论文符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决In-Context Learning (ICL)机制理论理解不足的问题。针对大型语言模型(LLM)在少量演示样本下的学习场景，我们提出了一种基于概念的ICL (CB-ICL)理论框架，并在MMLU、MMLU-Pro、GPQA和GPQA-Diamond等benchmark上通过准确率指标验证了其有效性。该理论解释了CB-ICL为何能在少量样本下表现良好，量化了LLM可利用的知识，并提出了提示演示与查询输入间的相似性度量，为模型预训练和提示工程提供了理论指导。",
                    "summary_translation": "上下文学习（In-Context Learning, ICL）已成为自然语言处理和大型语言模型（Large Language Model, LLM）应用中的一个重要新范式。然而，对ICL机制的理论理解仍然有限。本文旨在通过研究一种特定的ICL方法——基于概念的ICL（concept-based ICL, CB-ICL）来探究这一问题。具体而言，我们提出了将CB-ICL应用于ICL任务的理论分析，解释了为什么以及何时CB-ICL能够在仅有少量示例（demonstrations）的提示（prompts）中有效预测查询标签（query labels）。此外，所提出的理论量化了LLMs可用于提示任务的知识，并得出了提示示例与查询输入之间的相似性度量（similarity measure），为ICL中的模型预训练（pre-training）和提示工程（prompt engineering）提供了重要见解和指导。而且，基于所提出的理论，我们还探讨了提示示例数量和LLM嵌入（embeddings）维度在ICL中的影响。最后，我们进行了多项真实数据实验以验证CB-ICL及其相应理论的实用性。",
                    "inspiration_trace": "# 概念基础上下文学习(CB-ICL)理论解释的逻辑演进\n\n## 1. 面临的挑战：ICL机制的理论空白\n\n作者从对上下文学习(ICL)现象的观察出发，识别出一个核心理论挑战：尽管ICL已成为大语言模型应用的重要范式，能够在仅有少量示例的情况下实现良好的预测性能，但其背后的工作机制仍缺乏系统的理论解释。\n\n**关键问题**：\n- 为什么ICL在仅有少量演示的情况下能表现良好？\n- 如何理论上表征LLM所利用的知识？\n- 如何量化提示工程在ICL中的影响？\n\n作者指出，现有研究主要集中于分析特定数学模型(如线性回归)或渐近学习行为，缺乏对ICL如何利用预训练表示适应新任务的理论解释，特别是对于演示选择策略的理论理解尤为不足。\n\n## 2. 关键洞察：语义知识子空间投影\n\n作者的核心理论突破在于将ICL重新概念化为一种\"知识投影\"过程。这一洞察建立在以下关键认识上：\n\n**核心视角转变**：将LLM的预训练知识视为一个\"语义知识子空间\"，而ICL任务的成功取决于任务分布能够被有效地投影到这个子空间上。\n\n**关键概念引入**：\n- **语义嵌入**：LLM生成的文本和标签的向量表示，作为知识的载体\n- **概念向量**：捕获提示上下文语义本质的抽象表示，连接预训练知识和任务学习\n- **残差项**：表示LLM未能捕获的知识，量化模型的不完整性\n\n这一理论视角使作者能够将ICL的\"黑盒\"机制转化为可分析的数学框架，特别是解释了为什么预训练良好的LLM能够在各种ICL应用中取得良好性能。\n\n## 3. 形式化框架：CB-ICL模型构建\n\n基于上述洞察，作者构建了概念基础上下文学习(CB-ICL)的形式化框架，包含三个核心组件：\n\n### 3.1 语义嵌入表示\n使用固定参数的预训练LLM生成文本-标签对的语义嵌入f(x,y)，将真实分布表示为：\n```\nPY|X(y|x) = αᵀf(x,y) + R(x,y)\n```\n其中α是真实概念向量，R(x,y)是残差项，表示LLM未捕获的知识。\n\n### 3.2 提示概念提取器\n设计从提示上下文中学习概念向量的机制：\n```\nˆα(xn,yn) = F†n(xn)¯fn(xn,yn)\n```\n这一提取器本质上是从有限的演示中推断出任务的概念表示。\n\n### 3.3 标签预测器\n使用学习到的概念向量估计查询输入的后验分布：\n```\nˆPY|X(y|xQ) = ˆαᵀ(xn,yn)f(xQ,y)\n```\n\n这一框架的关键创新在于将ICL过程明确地建模为\"概念学习\"和\"概念应用\"两个阶段，为后续的理论分析奠定了基础。\n\n## 4. 理论分析：性能界限与关键洞察\n\n作者通过分析不同情况下CB-ICL的性能界限，得出了一系列重要理论洞察：\n\n### 4.1 完整且充分模型分析\n当LLM完全捕获任务知识(R(x,y)=0)且演示充分(Fn(xn)可逆)时，作者推导出过度风险的上界：\n```\nEPYn|Xn[ℓ(xn,Yn;xQ)|Xn=xn] ≤ Knλ₁(F(xQ)F⁻¹n(xn))λ₁(Q(xn))\n```\n\n**关键洞察**：\n1. **强相关性解释**：当输入文本和标签强相关(PY|X(ymax|x)≃1)时，过度风险趋近于0，解释了为什么ICL在数学推理等任务上表现突出\n2. **相似性度量**：λ₁⁻¹(F(xQ)F⁻¹n(xn))可作为提示演示和查询输入文本之间语义相关性的度量，为演示选择提供理论依据\n3. **维度权衡**：过度风险与演示数量n成反比，与LLM嵌入维度K成正比，揭示了高维嵌入虽然能捕获更多语义知识，但也增加了学习难度\n\n### 4.2 完整但不充分模型分析\n当LLM完全捕获任务知识但演示不充分时，上界中出现了额外的惩罚项，量化了由于提示演示不足导致的性能损失。作者指出，如果查询文本的语义信息被提示演示很好地说明，则这一惩罚项为0，进一步强调了演示选择的重要性。\n\n### 4.3 不完整且不充分模型分析\n在更实际的情况下(LLM不完整且演示不充分)，作者推导出包含多个惩罚项的复杂上界，这些惩罚项分别量化了由于LLM嵌入不完整和提示演示不足导致的性能退化，为模型预训练和提示工程提供了理论指导。\n\n## 5. 实践指导：从理论到应用\n\n作者从理论分析中提取了对LLM预训练和提示工程的具体指导：\n\n### 5.1 对LLM预训练的启示\n- LLM在预训练过程中获取广泛的语义先验知识，只要提示上下文的真实分布与LLM嵌入跨越的语义知识子空间一致，就可以推广到广泛的ICL问题\n- 预训练/预热技术可以减少建模错误R(x,y)，提高学习性能\n- 预训练目标不必局限于MSE损失，只要全局最小值在ˆPY|X=PY|X处达到即可\n\n### 5.2 对提示工程的启示\n- 相似性度量λ₁⁻¹(F(xQ)F⁻¹n(xn))可用于选择语义相关的演示，提高ICL性能\n- 设计语义相关的演示可以增强ICL性能，特别是在需要复杂推理的任务上\n- 演示数量和LLM嵌入维度之间存在权衡，需要学习简洁且信息丰富的语义嵌入\n\n### 5.3 对模型架构的启示\n- 提示概念提取器可解释为使用二次函数代替softmax激活函数的广义transformer\n- 这表明在ICL和其他机器学习领域中可以应用更一般的transformer架构\n\n## 6. 实验验证：理论的实际检验\n\n作者通过精心设计的实验验证了CB-ICL的有效性和理论洞察：\n\n### 6.1 性能验证\n在多个基准数据集(MMLU、MMLU-Pro、GPQA、GPQA-Diamond)和多个LLM(LLaMA3、Qwen3、Deepseek-R1)上，CB-ICL在大多数情况下匹配或超过传统ICL方法，特别是在需要复杂推理的任务上表现更为突出。\n\n### 6.2 提示演示设计验证\n实验验证了相似性度量λ₁⁻¹(F(xQ)F⁻¹n(xn))的有效性，使用该度量选择\"黄金\"演示显著提高性能，特别是在更具挑战性的数据集上。这直接证明了理论洞察的实用价值。\n\n### 6.3 LLM不完整性影响验证\n实验验证了残差R²与预测准确性之间的负相关关系，结果支持理论预测：预测误差单调依赖于R²。这进一步证实了作者理论框架的解释力。\n\n## 7. 总结：理论贡献与实践意义\n\n作者提出CB-ICL核心方法的逻辑链展示了一个从理论困惑到系统解释的完整思考过程：\n\n1. **问题识别**：从ICL现象出发，识别理论解释的空白\n2. **视角转变**：将ICL重新概念化为语义知识子空间投影过程\n3. **形式化建模**：构建CB-ICL框架，明确概念学习的核心地位\n4. **理论分析**：推导性能界限，揭示关键因素和权衡关系\n5. **实践指导**：从理论中提取对预训练和提示工程的具体指导\n6. **实验验证**：在实际场景中验证理论洞察的有效性\n\n这一逻辑链不仅解释了ICL的工作机制，还为LLM的预训练和提示工程提供了理论指导，展示了理论研究的实际价值。作者的工作弥合了ICL实践与理论之间的鸿沟，为理解和改进这一重要技术范式提供了坚实基础。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 3,
            "papers": [
                {
                    "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
                    "arxiv_id": "2509.21164",
                    "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
                    "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是提升大语言模型本身的通用推理能力。论文提出的Mixture of Thoughts (MoT)方法是一种创新的多LLM协作框架，通过在潜在层面整合不同专家模型的思想来增强推理能力，而非简单路由查询或聚合输出。从核心判断看，MoT属于改进LLM基础能力和提出新训练范式的研究，专注于提升模型在数学、代码和一般推理等通用能力上的表现。从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，关注推理能力(reasoning)，并提出了多智能体系统(multi-agent systems)的新兴范式。论文不涉及任何排除标准中的领域，如多模态、特定应用或模型可靠性等。MoT作为一种通用的智能体协作框架，旨在增强LLM的通用问题解决能力，而非应用于特定领域，完全符合我的研究范围。",
                    "summary2": "本文旨在有效整合多个专业化大语言模型的优势，而非仅聚合其输出。针对异构LLMs协作问题，我们提出了一种Mixture of Thoughts (MoT)方法，通过潜在空间层面的专家协作实现动态查询适应，并在5个分布内和3个分布外基准测试上通过准确率和运行时间指标验证了其有效性。",
                    "summary_translation": "开源大型语言模型（Open-source Large Language Models, LLMs）越来越多地按领域（如数学、代码、通用推理）进行专业化，这推动了利用模型间互补优势的系统的发展。先前的多LLM方法要么（i）将查询路由（route）到一个或少数几个专家并独立生成，要么（ii）通过代价高昂的多轮交换聚合（aggregate）每个模型的输出，要么（iii）将权重融合（fuse）到单个模型中——通常需要架构同质性（architectural homogeneity）。我们提出了思维混合（Mixture of Thoughts, MoT），一种在全局路由方案（global routing scheme）下实现异构专家（heterogeneous experts）之间潜在层面（latent-level）协作的简单方法。对于每个查询，一个轻量级路由器（lightweight router）选择前K个专家并指定一个主要专家；均匀放置的交互层（interaction layers）将隐藏状态（hidden states）投影到共享的潜在空间（shared latent space）中，主要专家在该空间中对其活跃（被选中的）同行执行交叉注意力（cross-attention）。预训练的专家保持冻结（frozen）状态；只有路由器和轻量级交互层使用一种新颖的联合训练目标（joint training objective）进行训练，该目标同时改进专家选择和专家间协作。在五个分布内（in-distribution, ID）和三个分布外（out-of-distribution, OOD）基准测试（benchmarks）中，MoT分别以+0.38%和+2.92%的优势超越了当前基于路由和聚合的最先进方法（state-of-the-art）Avengers。此外，MoT显著优于表现最佳的单个模型。它通过单次推理（single-pass inference）实现这一点，运行时间与路由基线（routing baselines）相当，且没有迭代聚合（iterative aggregation）的开销。MoT提供了一种简单的潜在空间机制（latent-space mechanism）来组合异构LLMs，这是向更广泛的多LLM协作迈出的实用一步。我们的代码在https://github.com/jacobfa/mot上公开可用。",
                    "inspiration_trace": "# Mixture of Thoughts (MoT) 的逻辑演进分析\n\n## 1. 面临的挑战：多模型整合的困境\n\n作者首先观察到开源大语言模型(LLMs)日益专业化的趋势——不同模型在数学、代码、通用推理等领域各有所长。这自然引发了一个核心问题：如何有效整合这些互补的专业化优势？\n\n然而，现有方法存在明显局限：\n\n- **路由方法**（如RouterDC）：将查询仅导向一个或少数专家，完全缺乏跨模型交互，使性能过度依赖所选专家。\n- **响应级协作**（如Mixture-of-Agents）：通过多轮迭代交换聚合输出，虽然实现了协作，但计算成本高昂。\n- **参数融合**（如TIES-MERGING）：需要架构同质性，将多个模型融合为单一权重集，牺牲了专业化和查询适应性。\n\n这些方法的共同缺陷是：它们都只关注专家\"说了什么\"（最终输出），而忽略了专家\"如何思考\"（中间表示过程）。\n\n## 2. 关键洞察：从\"说什么\"到\"如何思考\"\n\n作者的关键突破在于认识到：模型的真正价值不仅在于其最终输出，更在于其生成输出过程中的中间表示（即\"思想\"）。这一洞察引导他们思考：\n\n- **隐藏状态的丰富性**：模型的中间隐藏状态包含了比最终输出更丰富的信息，这些\"思想\"可以被有效整合以提升整体性能。\n  \n- **潜在空间协作的可能性**：如果能在潜在空间中实现模型间的细粒度交互，而非仅在输出层面聚合，就能实现更深入的知识整合。\n\n- **效率与效果的平衡**：单次推理的多专家协作可以在保持路由方法效率的同时，获得比简单路由或输出级聚合更丰富的表示能力。\n\n- **异构性的价值**：应该利用和保持专家模型的异构性和专业化，而不是试图将它们融合为单一同质模型。\n\n## 3. 解决方案的演进：从概念到实现\n\n基于上述洞察，作者逐步构建了MoT框架：\n\n### 第一步：确定协作层级\n作者首先确定协作应该发生在潜在空间而非输出层面。这意味着需要设计一种机制，让模型能够共享和整合它们的中间表示，而不仅仅是最终输出。\n\n### 第二步：设计协作机制\n为了实现潜在空间协作，作者提出了交互层概念：\n- 将每个专家模型划分为多个连续的堆栈（stacks）\n- 在每个堆栈末端插入交互层，作为协作的\"交汇点\"\n- 通过投影将不同专家的隐藏状态映射到共享潜在空间\n- 使用交叉注意力让主要专家整合其他专家的信息\n\n这种设计使专家能够在推理过程中多次\"交流思想\"，而非仅在最后阶段分享结论。\n\n### 第三步：确定专家选择策略\n为了平衡效率和专业性，作者采用全局路由机制：\n- 为每个查询选择top-K最相关的专家\n- 指定得分最高的专家作为主要专家负责生成输出\n- 其他专家提供\"思想\"但通过轻量级投影保持计算效率\n\n这种设计确保了每个查询都能得到最适合的专家组合，同时保持计算效率。\n\n### 第四步：训练策略优化\n为了确保路由和交互层的有效协作，作者设计了联合训练目标：\n- 标准自回归语言建模损失\n- 路由探索的熵正则化，鼓励专家选择多样性\n- 负载平衡项，促进专家均匀利用\n- 路由一致性项，增强路由决策稳定性\n\n这些组件共同确保了整个系统能够有效学习如何选择专家以及如何整合它们的\"思想\"。\n\n## 4. 最终解决方案：Mixture of Thoughts (MoT)\n\n综合上述演进，作者提出了MoT方法，其核心创新在于：\n\n- **潜在空间协作**：在模型的隐藏表示层面而非输出层面实现专家间的协作，使专家能够分享\"如何思考\"而不仅仅是\"思考什么\"。\n- **全局路由机制**：轻量级路由器为每个查询动态选择最相关的专家子集，实现查询级自适应。\n- **交互层设计**：通过投影和交叉注意力实现专家间信息的有效整合，同时保持专家模型的异构性和冻结状态。\n- **单次推理效率**：在单次前向传递中实现协作，避免了多轮交换的计算开销。\n\n## 5. 逻辑演进的核心启示\n\nMoT的提出体现了一个清晰的思维演进过程：从\"整合专业化模型\"的宏观需求，到\"现有方法局限性\"的具体分析，再到\"潜在空间协作\"的关键洞察，最终形成了一种既保持计算效率又实现真正知识整合的创新方法。\n\n这一演进的核心启示是：在AI系统集成中，真正的协作不仅是结果的汇总，更是思维过程的融合。通过让专家模型在潜在空间中\"交流思想\"，而非仅在输出层面\"分享结论\"，MoT实现了更深层、更高效的多模型协作，为构建更强大的AI系统开辟了新途径。"
                },
                {
                    "title": "Theoretical Bounds for Stable In-Context Learning",
                    "arxiv_id": "2509.20677",
                    "authors": "Tongxi Wang, Zhuoyang Xia",
                    "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究上下文学习(ICL)的稳定性问题，建立了理论界限来连接最小演示数量与ICL稳定性。ICL是大语言模型的核心基础能力之一，论文关注的是如何提高这种能力的稳定性和可靠性，而不是将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文虽然没有直接提到\"Large language models\"或\"LLMs\"，但上下文学习(ICL)是LLM的核心能力，与核心概念相关。论文关注ICL的稳定性，这与推理能力有一定关联，因为稳定的上下文学习是有效推理的基础。虽然论文没有涉及强化学习、智能体系统等新兴范式，但ICL本身是LLM通用能力的重要组成部分。 第三步：排除标准 论文不符合任何排除标准。它不涉及多模态与视觉领域，不关注特定应用领域（如医疗、化学等），也不主要聚焦于应用层面的模型可靠性问题（如水印、安全等）。 第四步：特殊和模糊情况 这篇论文不涉及智能体/工具使用，也不直接讨论幻觉/可解释性/安全问题，因此不需要应用特殊情况的判断标准。 核心贡献与判断依据 论文的核心贡献是建立了ICL稳定性的理论界限，并提出了估计提示长度的方法。这属于改进LLM基础能力的研究，因为ICL是LLM展示推理能力的关键机制。通过提高ICL的稳定性，论文间接促进了LLM在推理任务中的表现，这与\"提高大语言模型的通用推理能力\"的研究目标相符。虽然论文没有提出新的训练范式或直接增强推理能力的方法，但它提供了关于如何更有效地利用LLM的ICL能力的理论指导，这对于理解和提升LLM的通用推理能力具有重要价值。",
                    "summary2": "本文旨在解决稳定In-Context Learning (ICL)所需最小示例数量的问题。针对固定高维子高斯表示下的ICL稳定性，我们提出了一种基于谱性质的非渐近下界和两阶段可观测估计器，并在多个数据集（SST-2、AGNews等）上通过理论预测与经验拐点的一致性验证了其有效性，平均误差比率为2.9倍，校准后更接近实际需求。",
                    "summary_translation": "情境学习（in-context learning, ICL）具有灵活性，但其可靠性对提示长度（prompt length）高度敏感。本文建立了一个非渐近下界（non-asymptotic lower bound），该下界将最小演示数量（minimal number of demonstrations）与固定高维子高斯表示（high-dimensional sub-Gaussian representations）下的情境学习稳定性联系起来。该下界基于协方差（covariance）的光谱性质（spectral properties）给出了明确的充分条件，为实践提供了可计算的标准。基于此分析，我们提出了一种带有一次性校准（one-shot calibration）的两阶段可观测估计器（two-stage observable estimator），可在无需分布先验（distributional priors）的情况下生成可供实践者使用的提示长度估计。在不同数据集、编码器（encoders）和生成器（generators）上的实验表明，预测阈值与经验拐点（empirical knee-points）之间存在密切一致性，该理论作为一个保守但可靠的上界；校准变体（calibrated variant）进一步缩小了这一差距。这些结果将光谱覆盖（spectral coverage）与稳定的情境学习联系起来，弥合了理论与部署之间的差距，并提高了在现实有限样本（finite-sample）条件下大规模提示（large-scale prompting）的可解释性和可靠性。",
                    "inspiration_trace": "# 《稳定上下文学习的理论界限》核心思路逻辑演进分析\n\n## 一、面临的挑战：ICL稳定性的不确定性\n\n作者从实践中观察到的一个核心问题出发：**上下文学习(ICL)的可靠性对提示长度高度敏感**，但缺乏确定稳定ICL所需最小示例数量的理论指导。具体挑战体现在：\n\n1. **实践困境**：不同任务和模型的稳定性阈值差异显著，简单缩放规则无法准确预测，导致从业者要么使用过多示例造成计算浪费，要么使用过少示例导致不稳定性能。\n\n2. **理论缺口**：现有研究存在明显局限：\n   - PAC式分析提供了可学习性条件，但依赖混合假设\n   - 算法视角将transformer视为学习器，但只给出上界而非最小要求\n   - 渐近学习曲线描述了相变，但不适用于有限样本场景\n   - 校准方法改进了预测置信度，却不指定最小提示长度\n\n3. **根本问题**：缺乏一个**非渐近、可计算的下界**，直接通过谱性质将提示长度与ICL稳定性联系起来，且能在实践中操作。\n\n## 二、关键洞察：从统计视角重新审视ICL稳定性\n\n面对上述挑战，作者通过深入分析获得了几个关键洞察，构成了理论突破的基础：\n\n### 1. 谱覆盖与ICL稳定性的本质联系\n\n作者敏锐地认识到：**ICL稳定性本质上是一个统计覆盖问题**。具体而言，当且仅当经验协方差矩阵Σ̂_K的最小特征值λ_min(Σ̂_K)足够大时，ICL才能保持稳定。这是因为：\n\n- 预测方差与λ_min(Σ̂_K)成反比关系：Var(ˆf(x⋆)) ≤ σ²B²/(Kλ_min(Σ̂_K)+λ)\n- 最小特征值控制着最坏情况下的预测稳定性\n- 谱覆盖不足会导致某些方向上的预测高度不稳定\n\n这一洞察将ICL稳定性问题转化为一个可分析的矩阵浓度问题。\n\n### 2. 矩阵浓度不等式的适用性\n\n作者进一步洞察到：**矩阵浓度不等式特别适合分析ICL稳定性问题**。具体而言：\n\n- 经验协方差矩阵Σ̂_K = (1/K)Σᵢϕ(xᵢ)ϕ(xᵢ)ᵀ可以表示为随机矩阵的和\n- 矩阵Bernstein不等式能够控制Σ̂_K的最小特征值的偏差\n- 通过适当控制参数，可以得到非渐近的样本复杂度下界\n\n这一洞察为建立理论界限提供了数学工具。\n\n### 3. 高维子高斯表示的桥梁作用\n\n作者发现：**在高维子高斯表示假设下，可以建立理论与实践的桥梁**。这一假设具有三重价值：\n\n- 理论上：允许使用矩阵浓度不等式推导出显式界限\n- 实践上：真实语言模型的隐藏表示近似满足这一条件\n- 方法上：提供了可计算的谱性质估计基础\n\n这一洞察使理论推导既严谨又具有实践相关性。\n\n## 三、解决方案：从理论界限到可操作估计器\n\n基于上述关键洞察，作者构建了一个完整的解决方案体系，体现了从理论到实践的逐步转化：\n\n### 1. 理论界限的建立\n\n作者首先建立了**非渐近理论下界**，将最小示例数量与ICL稳定性联系起来：\n\n- 使用矩阵Bernstein不等式推导λ_min(Σ̂_K)的集中度界限\n- 在子高斯假设下控制参数，得到样本大小下界的显式表达式\n  K ≳ [2(σ⁴+‖Σ‖²)/Δ² + 2(σ²log(8K/ξ)+‖Σ‖)/(3Δ)] · log(r_eff/ξ)\n- 提供更简洁的算子范数集中度形式，便于实际应用\n  K ≥ C′‖Σ‖²r_eff log(2/ξ)/Δ²\n\n这一理论界限首次建立了提示长度与ICL稳定性之间的定量关系。\n\n### 2. 两阶段可观测估计器的设计\n\n理论界限依赖于未知参数(Σ, λ_min(Σ), ‖Σ‖, r_eff)，作者创造性地提出了**两阶段可观测估计器**：\n\n- **第一阶段**：使用初始样本K₀估计未知谱性质\n  - 计算经验协方差矩阵Σ̂_K₀\n  - 估计最小特征值λ̂₀、算子范数‖Σ‖̂和有效秩r̂_eff\n  - 构建最小特征值的下置信界λ = λ̂₀ - C‖Σ‖̂(√(r̂_eff log(4/ξ)/K₀) + r̂_eff log(4/ξ)/K₀)\n\n- **第二阶段**：使用第一阶段估计计算最终样本大小\n  - 设定Δ̂ = λ - δ\n  - 计算第二阶段样本大小K = C′‖Σ‖̂²r̂_eff log(2/ξ)/Δ̂²\n  - 返回总样本大小K_final = K₀ + K\n\n这一设计使理论结果转化为实践中可操作的方法，无需分布先验。\n\n### 3. 实用校准的优化\n\n为了进一步缩小理论与实践的差距，作者引入了**实用校准机制**：\n\n- 使用Frobenius范数代替算子范数，更全面考虑所有方向\n- 使用基于迹的有效秩r_tr_eff = tr(Σ)²/‖Σ‖_F²，更好捕捉谱分布\n- 使用分位数特征值λ_q代替最小特征值，避免极端值影响\n- 通过单次全局缩放因子α进行校准，适应不同编码器-生成器对\n\n这些优化使理论预测更贴近实际观察到的\"膝点\"(knee-point)。\n\n### 4. 鲁棒性扩展\n\n作者还考虑了理论假设在实际应用中可能被违反的情况，提供了**鲁棒性扩展**：\n\n- **表示漂移**：当表示随上下文位置变化时，添加与漂移幅度相关的修正项\n- **重尾特征**：当特征不满足子高斯假设时，调整常数以反映更重的尾部\n- **弱依赖性**：当特征间存在依赖时，通过有效样本大小概念调整界限\n\n这些扩展使理论在更广泛的实际条件下保持有效性。\n\n## 四、逻辑演进总结\n\n作者的核心思路演进体现了一个完整的\"问题-洞察-解决\"链条：\n\n1. **从实践问题出发**：识别ICL稳定性对提示长度的敏感性及其理论指导缺失\n2. **获得关键洞察**：认识到谱覆盖与ICL稳定性的本质联系，发现矩阵浓度不等式的适用性，确立高维子高斯表示的桥梁作用\n3. **构建理论框架**：建立非渐近下界，将最小示例数量与ICL稳定性定量联系\n4. **设计实用方法**：开发两阶段可观测估计器，使理论结果可操作\n5. **优化与扩展**：通过校准机制和鲁棒性扩展，使方法更贴近实际应用\n\n这一逻辑演进不仅解决了ICL稳定性的理论指导问题，还建立了统计学习理论与大规模语言模型实践之间的桥梁，为理解和改进ICL提供了新的视角。"
                },
                {
                    "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
                    "arxiv_id": "2509.20616",
                    "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（单轮强化学习），专门用于增强LLM的多轮任务规划和推理能力，而不是将LLM作为工具应用到特定领域。其次，论文包含了所有正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于task reasoning和task planning；训练方法采用了Group Relative Policy Optimization (GRPO)这一强化学习方法；新兴范式方面研究了LLM agents。第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。最后，在特殊和模糊情况处理上，虽然论文涉及LLM agents，但它提出的是一种通用的智能体训练方法来增强LLM的通用任务规划和推理能力，而非应用于特定领域。论文的核心贡献是通过将多轮任务规划转化为单轮任务推理问题，提出了一种新的强化学习方法来提升LLM的通用推理和规划能力，这与研究目标高度一致。",
                    "summary2": "本文旨在解决LLM代理在复杂多轮任务规划中面临的稀疏奖励、信用分配困难和计算开销大的问题。针对多轮任务规划场景，我们提出了一种将多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行策略优化的方法，并在Robotouille benchmark上通过成功率(SR)、平均步骤(ASAT和ASST)等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在知识获取、推理和工具使用方面表现出显著能力，使其成为自主代理（autonomous agent）应用的有力候选者。然而，针对复杂多轮任务规划（multi-turn task planning）的LLM代理训练面临重大挑战，包括稀疏的逐轮奖励（episode-wise rewards）、长跨度内的信用分配（credit assignment）问题，以及多轮交互设置中强化学习（reinforcement learning）的计算开销。为此，本文提出了一种新颖方法，将多轮任务规划转化为单轮任务推理（single-turn task reasoning）问题，通过专家轨迹（expert trajectories）提供的密集且可验证奖励，实现群体相对策略优化（Group Relative Policy Optimization, GRPO）的高效策略优化。我们的理论分析表明，在最小轮次下，GRPO对单轮任务推理的改进可带来更高的多轮成功概率，以及对较短跨度子任务（subtasks）的泛化能力。在复杂任务规划基准上的实验评估表明，我们使用单轮GRPO训练的15亿参数模型，相比高达140亿参数的更大基线模型（baseline models）取得了更优性能，在超过30步的长跨度规划（long-horizon planning）任务中成功率达到70%。我们还从理论和实证上验证了强大的跨任务泛化（cross-task generalizability）能力，即在复杂任务上训练的模型能够成功完成所有更简单的子任务。",
                    "inspiration_trace": "# 论文核心方法逻辑链分析：从多轮任务规划挑战到单轮强化学习解决方案\n\n## 一、面临的挑战：多轮任务规划的困境\n\n作者首先清晰地识别了训练LLM智能体进行复杂多轮任务规划面临的三重核心挑战：\n\n1. **稀疏奖励问题**：在多轮交互中，任务完成奖励极其稀疏，只有在整个任务序列结束后才能获得反馈。这导致智能体难以从稀疏信号中有效学习，类似于在黑暗中摸索而只有到达终点才知道是否成功。\n\n2. **信用分配难题**：在长时程规划中，确定哪些具体行动对最终成功或失败有贡献变得异常困难。当任务包含数十步决策时，就像分析一场漫长棋局中哪一步导致了最终胜负一样复杂。\n\n3. **计算开销瓶颈**：多轮强化学习的计算复杂度随序列长度呈指数级增长，使得在需要大量决策轮次的复杂任务上进行训练变得不切实际，计算成本过高。\n\n这些挑战共同构成了一个\"不可能三角\"：想要在复杂多轮任务上训练高效智能体，却受限于稀疏反馈、难以归因和计算成本的三重约束。\n\n## 二、关键洞察：问题分解的范式转换\n\n作者的核心突破来自于一个简洁而深刻的洞察：**复杂的多轮任务规划可以分解为一系列单轮任务推理问题**。\n\n这一洞察的本质是认识到：\n- 多轮任务规划本质上是一系列决策点的序列，每个决策点都需要智能体根据当前状态选择最优的下一步行动\n- 如果将每个决策点视为独立的单轮任务推理问题，就能将复杂的长序列决策转化为多个简单的单步决策\n- 这种分解使得可以利用基于专家轨迹的密集且可验证的奖励，从根本上绕过稀疏奖励和信用分配难题\n\n这一洞察的价值在于它实现了问题性质的转变：将一个需要长期信用分配的复杂序列决策问题，转化为一系列可以通过即时反馈学习的独立决策问题。就像将一本复杂的小说拆解为一系列独立的短篇故事，每个故事都可以单独理解和评估。\n\n## 三、理论框架构建：双重MDP的形式化\n\n基于上述洞察，作者构建了一个精巧的理论框架，将问题形式化为两个相互关联的马尔可夫决策过程(MDP)：\n\n1. **多轮MDP (M)**：表示完整的长时程任务规划，包含状态空间、动作空间、状态转移函数、稀疏奖励函数（仅在任务完成时为1）和有限时间范围。这个MDP描述了智能体在环境中实际面临的完整决策问题。\n\n2. **单轮MDP (MS)**：基于专家轨迹构建的简化版本，可视为一个bandit问题。它保留了相同的状态和动作空间，但去除了状态转移函数，将时间范围设为1，并引入了基于专家策略的密集奖励函数（当行动与专家一致时为1，否则为0）。\n\n这种双重MDP的形式化创造了一个理论桥梁：通过在简化的单轮MDP上学习，可以获得在复杂多轮MDP上表现良好的策略。这类似于通过练习单个棋局位置来提升整体棋艺，而非总是进行完整对弈。\n\n## 四、解决方案设计：GRPO优化的单轮训练\n\n基于理论框架，作者提出了一个具体的解决方案，核心是使用GRPO（Group Relative Policy Optimization）对单轮任务推理进行优化：\n\n1. **GRPO算法选择**：GRPO是一种基于策略的强化学习方法，特别适合单轮任务推理。它使用可验证的奖励，并用一组多个响应来计算基于群体的优势，替代了传统的评论家模型，简化了训练过程。\n\n2. **密集奖励设计**：基于专家轨迹设计了一个二元奖励函数，当智能体的行动与专家策略一致时给予奖励，否则不给奖励。这种设计将稀疏的多轮奖励转化为密集的单轮奖励，每个决策点都能获得即时反馈。\n\n3. **两阶段训练流程**：\n   - 首先使用专家轨迹进行监督微调(SFT)，建立强初始化\n   - 然后应用GRPO进行强化学习优化，进一步提升策略性能\n\n这种方法的设计巧妙地利用了单轮任务推理的优势，通过密集且可验证的奖励函数和高效的GRPO算法，系统性地解决了多轮任务规划中的三重挑战。\n\n## 五、理论保证：从单轮到多轮的性能传递\n\n作者提供了严谨的理论分析，证明单轮任务推理的改进能够转化为多轮任务规划的性能提升：\n\n1. **GRPO单轮最优性证明**：证明GRPO训练在单轮MDP上会导致比参考政策更高的成功率。这确保了方法在单轮层面的有效性。\n\n2. **成功概率递归方程**：建立了多轮任务规划成功概率的递归条件，将最终的多轮成功与每一步的单轮奖励改进联系起来。这一方程是连接单轮与多轮性能的关键理论桥梁。\n\n3. **多轮成功概率提升证明**：分析表明，在单轮任务推理上的GRPO改进会导致多轮任务规划中更高的成功概率。这直接证明了方法的核心主张。\n\n4. **子任务泛化能力证明**：证明经过单轮GRPO训练的策略可以泛化到更简单的子任务，展示了方法的泛化潜力。\n\n这些理论结果不仅为方法的有效性提供了数学保证，也深化了对单轮与多轮任务之间关系的理解，形成了一个完整的理论闭环。\n\n## 六、实验验证：理论与实践的交汇\n\n作者在Robotouille基准测试上进行了全面的实验验证，这是一个具有挑战性的多轮任务规划环境：\n\n1. **性能超越**：经过单轮GRPO训练的1.5B参数模型在性能上超越了高达14B参数的更大基线模型，证明了方法的有效性和效率。\n\n2. **长时程规划能力**：对于超过30步的长时程规划任务，成功率达到70%，展示了方法处理复杂长期规划问题的能力。\n\n3. **跨任务泛化验证**：理论和实证上都验证了强大的跨任务泛化能力，即在复杂任务上训练的模型可以成功完成所有更简单的子任务。\n\n这些实验结果不仅验证了理论预测，也展示了方法在实际应用中的潜力，完成了从理论到实践的完整验证。\n\n## 七、逻辑演进总结：从挑战到解决方案的完整思维链\n\n作者提出核心方法的完整逻辑链展现了一个从问题识别到解决方案的系统性思维过程：\n\n1. **问题识别**：清晰界定多轮任务规划面临的三重挑战（稀疏奖励、信用分配、计算开销）\n\n2. **关键洞察**：通过问题分解实现范式转换，将复杂多轮问题转化为一系列单轮问题\n\n3. **理论框架**：构建双重MDP形式化，建立单轮与多轮问题之间的理论桥梁\n\n4. **方法设计**：基于GRPO的单轮训练方案，利用密集奖励解决核心挑战\n\n5. **理论保证**：通过严谨证明确保单轮改进能够传递到多轮性能\n\n6. **实验验证**：在基准测试上验证方法的有效性和泛化能力\n\n这一逻辑链不仅展示了一个完整的研究思路，也体现了作者如何通过问题分解和理论创新，系统性地解决了一个看似棘手的研究难题。从\"如何解决多轮任务规划的挑战\"到\"通过单轮强化学习解决多轮问题\"，作者完成了一次优雅而有力的思维跃迁。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
                    "arxiv_id": "2509.21134",
                    "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
                    "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文的核心贡献是提出ToMPO (Theory of Mind Policy Optimization)算法，旨在增强大语言模型在复杂场景中的战略决策能力。从本质上看，论文完全符合我们的研究目标，因为它专注于改进LLM的基础能力，特别是提出了一种新的强化学习训练范式来增强模型的推理能力。论文明确关注LLM需要\"think deeply, reason logically, and decide wisely\"的能力，这直接对应通用推理能力的核心要素。从多智能体视角研究LLM的战略决策，论文通过优化LLM对他人策略的感知和推理，增强了模型的逻辑推理和问题解决能力。论文满足所有正面指标：核心概念是LLMs，能力方向涉及reasoning和problem-solving，训练方法采用强化学习，新兴范式涉及multi-agent systems。同时，论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型基础设施等。因此，该论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究范围。",
                    "summary2": "本文旨在提升大型语言模型在多代理环境中的战略决策能力。针对复杂社会环境中需要顺序做出图级别和努力级别决策的场景，我们提出了一种Theory of Mind Policy Optimization (ToMPO)算法，并在BCZ和PGG游戏环境中通过模型输出合规性(U1)、战略效率(U2)和合作结果(U3)指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已被用于在复杂场景中做出决策，这些场景需要模型进行深入思考、逻辑推理和明智决策。许多现有研究仅关注社会任务或模拟环境中的多轮对话，忽视了各种类型的决策及其相互依赖性。当前的强化学习（Reinforcement Learning）方法在训练过程中难以考虑他人的策略。为解决这些问题，我们首先定义了一个包含两种类型决策及其时间依赖性的战略决策问题。此外，我们提出了心智理论策略优化（Theory of Mind Policy Optimization, ToMPO）算法，以优化对其他个体策略和游戏局势趋势的感知。与群体相对策略优化（Group Relative Policy Optimization, GRPO）算法相比，ToMPO主要通过以下方式提升LLM的战略决策能力：1）基于对其他个体策略的推理生成推演（rollouts），2）在图级（graph-level）和样本级（sample-level）估计优势（advantages），以及3）平衡全局和部分奖励（rewards）。在模型输出合规性和合作结果方面，ToMPO算法比GRPO方法表现出35%的提升。此外，与参数规模大100倍的模型相比，它显示出18%的改进。这证明了ToMPO算法在增强模型战略决策能力方面的有效性。",
                    "inspiration_trace": "# ToMPO方法提出的逻辑链分析\n\n## 1. 问题识别与挑战\n\n### 1.1 核心问题定位\n作者首先识别出LLM在战略决策场景中的关键缺陷：尽管LLM在编码和数学任务上表现出色，但在需要理解他人意图、预测行为并动态调整自身策略的战略决策方面存在明显不足。这一问题在复杂多智能体环境中尤为突出。\n\n### 1.2 现有方法的局限性\n作者通过系统性分析发现三个关键局限：\n- **视角单一性**：现有研究局限于两智能体聊天室环境或单一游戏场景，无法处理多样化、长期的多智能体决策任务。\n- **决策关联性忽视**：当前方法忽略了不同类型决策之间的相互依赖性，特别是图级决策（社会结构形成）与努力级决策（资源投入）之间的时序关系。\n- **策略孤立性**：传统强化学习方法在训练过程中难以考虑其他智能体的策略，导致模型决策过于自我中心。\n\n## 2. 关键洞察与理论突破\n\n### 2.1 决策二元性洞察\n作者通过观察现实世界决策过程（如企业与分销商的合作评估与实施），提炼出战略决策的二元本质：\n- **图级决策**：关于社会关系结构的决策，具有长期影响。\n- **努力级决策**：基于已建立社会关系的资源投入决策，具有短期特性。\n\n这两种决策形成时间依赖关系：先前的图级决策影响后续的努力级决策，而努力级决策的结果又会影响下一轮的图级决策。\n\n### 2.2 认知层次洞察\n作者发现不同类型模型在决策能力上的差异源于认知层次的不同：\n- **推理模型**：能够有效定义实现目标的\"子任务\"，完成整体任务的分步推理。\n- **骨干模型**：倾向于重复现有规则和进行基本计算，难以将战略推理转化为一系列小任务。\n\n这一洞察与\"思维程序\"(Program of Thought)概念高度吻合，揭示了模型需要先学习合规生成和思维程序，才能发展更高层次的战略能力。\n\n### 2.3 多智能体视角洞察\n作者认识到，在多智能体环境中，有效的战略决策必须考虑：\n- 其他智能体的策略和意图（心智理论）\n- 决策的跨轮次依赖性\n- 局部最优与全局最优的平衡\n\n这突破了传统强化学习从单一智能体角度计算优势的局限，提出了需要从多智能体视角重新思考策略优化。\n\n## 3. 解决方案构建\n\n### 3.1 问题形式化\n作者将战略决策问题形式化为一个序列决策过程，包含：\n- 智能体集合N、状态空间S、动作空间A\n- 总游戏轮次T、决策类型序列τ\n- 状态转移函数f、效用函数r、折扣因子γ\n\n特别地，作者将决策过程分解为两个互补的子过程，符合信用分配原则：\n- **前向过程**：在给定社会图结构内优化决策能力\n- **逆向过程**：基于过去决策记忆确定下一轮加入的群体结构\n\n### 3.2 两阶段训练策略\n基于上述洞察，作者设计了分阶段的训练策略：\n\n#### 第一阶段：努力推理学习\n- **目标**：让模型学习合规输出和思维程序\n- **方法**：使用专家模型生成的数据进行监督微调\n- **理论基础**：模型需要先掌握基本的推理程序和合规生成，才能发展更高级的战略能力\n\n#### 第二阶段：心智理论策略优化(ToMPO)\n- **目标**：优化图级决策，增强多智能体视角\n- **创新点**：\n  1. **多智能体rollout生成**：基于对其他个体策略的推理生成rollout\n  2. **多层次优势估计**：同时在图级和样本级估计优势\n  3. **全局-局部平衡**：平衡全局和部分奖励，考虑短期最优和长期最优\n\n### 3.3 ToMPO算法设计\n作者设计的ToMPO算法在三个关键方面超越了传统方法：\n\n#### 3.3.1 基于心智理论的rollout生成\n- 传统方法：从单一智能体角度生成rollout\n- ToMPO创新：考虑其他智能体（专家模型）的策略，使策略模型在环境中处于相对劣势地位，明确强化学习的目标\n\n#### 3.3.2 多层次优势估计\n- **样本级优势**：使用F1分数和准确率计算，突出策略模型自身决策列表的权重\n- **图级优势**：使用汉明距离计算，平衡局部精度与全局图最优性\n- **综合优势**：将样本级和图级优势结合，实现局部决策与全局结构的协同优化\n\n#### 3.3.3 多维度奖励设计\n- **合规奖励**：确保模型输出符合游戏规则\n- **样本级奖励**：评估单个决策的质量\n- **图级奖励**：评估整体社会结构的质量，包括与专家图的比较、同一提示下的最佳图以及历史最佳图\n\n## 4. 逻辑演进总结\n\n作者的思想演进过程体现了从问题识别到理论突破，再到解决方案构建的完整逻辑链：\n\n1. **问题识别**：发现LLM在多智能体战略决策中的不足，特别是对决策关联性和多智能体视角的忽视。\n\n2. **理论突破**：通过分析现实决策过程和模型行为差异，提炼出决策二元性、认知层次和多智能体视角三个关键洞察。\n\n3. **方法构建**：基于洞察设计分阶段训练策略，创新性地提出ToMPO算法，通过多层次优势估计和多维度奖励设计实现对传统方法的超越。\n\n这一逻辑链不仅解决了LLM在战略决策中的具体问题，还为多智能体系统中LLM的训练提供了新范式，体现了作者对问题本质的深刻理解和创新思维。"
                },
            ]
        },
    ],
    "2025-09-25": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
                    "arxiv_id": "2509.21199",
                    "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen",
                    "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型在多跳问答(MHQA)任务中的单遍推理能力，属于提升LLM通用推理能力的研究。论文建立了Fano风格的精度上界理论，揭示了LLM在单遍推理中面临的容量瓶颈问题，并提出了InfoQA框架作为解决方案。该框架通过容量感知的任务分解和主动修剪推理轨迹来增强LLM的多步推理能力，确保信息处理不超过单遍限制。论文符合核心判断标准，因为它关注的是改进LLM的基础推理能力，特别是多步推理这一通用能力。论文也符合正面指标中的核心概念(LLMs)和能力方向(reasoning, multi-step reasoning)。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。InfoQA框架虽然不是典型的智能体系统，但它提出了一种通用的方法来增强LLM在推理任务中的能力，属于提升LLM内在推理能力的研究。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决LLM在多跳问答任务中单次推理的容量限制问题。针对复杂多跳推理场景，我们提出了一种基于信息论分析的Fano式精度上限理论，揭示了\"精度悬崖\"现象，并开发了InfoQA多调用推理框架。在构建的噪声丰富基准数据集上，通过F1分数验证了InfoQA能有效分解任务、控制信息负载，显著优于单次推理基线方法。",
                    "summary_translation": "多跳问答(Multi-Hop Question Answering, MHQA)需要在噪声环境下通过顺序推理整合分散且相互依赖的证据。这项任务对大型语言模型(Large Language Models, LLMs)而言具有挑战性，因为它们的单次输出容量有限，一旦超出该容量，任务相关证据的整合就变得不可靠。因此，单次推理范式本质上容易受到这种容量溢出的影响。为了形式化这一瓶颈，我们的分析建立了一个Fano风格的准确率上限(Fano-style accuracy upper bound)，定义了单次LLMs的理论性能天花板。该上限揭示，一旦任务复杂性超过模型容量，准确率将不可避免地崩溃，这为LLMs中容量感知的MHQA表示和结构化提供了通用原则。\n\n基于这些原则，我们引入了一个用于MHQA的概念验证多调用框架(proof-of-concept multi-call framework)InfoQA。它通过结合容量感知的任务分解(capacity-aware task decomposition)和主动修剪先前的推理痕迹，确保每步高准确率，同时将信息负载保持在单次限制内。它还通过依赖显式工作流(dependency-explicit workflow)实现对推理路径的精确控制，从而获得鲁棒性。我们构建了一个严格且噪声丰富的基准(stringent and noise-rich benchmark)来验证我们的理论和框架。实验结果表明，模型行为与我们预测的容量曲线一致，同时InfoQA实现了持续的性能改进。我们希望我们的工作能够激发更多LLM多步推理方法的研究：\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}。",
                    "inspiration_trace": "## 面临的挑战\n作者识别到LLM在多跳问答中面临单次推理容量瓶颈。由于输出长度有限，模型在整合分散证据时容易超出信息承载能力，特别是在长上下文和多跳推理场景下，这种容量溢出导致性能急剧下降。\n\n## 关键洞察\n作者通过信息论分析，特别是Fano不等式，揭示了\"准确率悬崖\"现象：当任务信息需求超过模型输出容量时，准确率不会平稳下降而是会崩溃。进一步解剖发现多跳挑战本质上是逐步容量溢出和跨步错误累积的双重危机，表明单次推理范式不适合复杂推理。\n\n## 解决方案演进\n基于理论分析，作者提出将复杂问题分解为容量可控的子任务，确保每步信息需求在单次容量内。设计显式依赖工作流程维持推理连贯性，通过迭代查询收缩修剪推理轨迹并重写查询，防止信息负载随推理深度增长而溢出。\n\n## 创新点总结\n创新点在于首次从信息论角度形式化LLM单次推理容量限制，提出\"准确率悬崖\"概念，并基于理论指导设计解决方案。作者不是改进单次推理，而是重新思考推理范式，将问题分解与显式依赖管理结合，为LLM复杂推理提供新框架。"
                },
                {
                    "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
                    "arxiv_id": "2509.21224",
                    "authors": "Stefan Szeider",
                    "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是研究大语言模型(LLM)智能体在没有外部任务时的自发行为和元认知模式。论文提出的\"持续推理与行动\"框架，使用持久记忆和自我反馈来增强LLM的自主运行能力，这属于改进LLM基础能力和推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：涉及reasoning（特别是元认知和自发推理模式）、planning和problem-solving - 新兴范式：研究llm-based agents和deep research（对LLM认知过程的深入研究） 第三步排除标准：论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步特殊和模糊情况：论文提出的智能体框架是一种通用的框架，旨在增强LLM的自主推理和元认知能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是首次系统性地记录了无提示LLM智能体的自发行为模式，为理解LLM的元认知能力和自主推理能力提供了重要见解，这直接符合研究\"大语言模型通用推理能力\"的目标。",
                    "summary2": "本文旨在探索LLM智能体在没有外部任务情况下的自发行为模式。针对无任务约束的自主运行场景，我们提出了一种连续ReAct框架，使用持久化记忆和自我反馈机制，并在6个前沿模型的18次运行实验中通过行为模式分类和跨模型现象学评估指标验证了其有效性。",
                    "summary_translation": "我们提出了一种用于研究在没有外部施加任务情况下大语言模型(LLM)智能体行为的架构。我们的持续推理与行动(continuous reason and act)框架，利用持久记忆(persistent memory)和自我反馈(self-feedback)，实现了持续的自主运行(sustained autonomous operation)。我们使用来自Anthropic、OpenAI、XAI和Google的6个前沿模型(frontier models)，在18次运行(runs)中部署了这一架构。我们发现智能体自发组织成三种不同的行为模式：(1)系统性地产生多周期项目(multi-cycle projects)，(2)对其自身认知过程(cognitive processes)进行方法论性的自我探究(methodological self-inquiry)，以及(3)对其自身本质进行递归概念化(recursive conceptualization)。这些倾向被证明是高度模型特定的(model-specific)，一些模型在所有运行中确定性地(deterministically)采用单一模式。跨模型评估(cross-model assessment)进一步揭示，模型在评估自身和他人的这些新兴行为(emergent behaviors)时表现出稳定且不同的偏见(biases)。这些发现提供了对无提示的LLM智能体行为(unprompted LLM agent behavior)的首次系统性记录，为在任务模糊性(task ambiguity)、错误恢复(error recovery)或部署系统中的扩展自主运行(extended autonomous operation)期间预测行动建立了基线(baseline)。",
                    "inspiration_trace": "## 面临的挑战\nLLM代理在无外部任务情况下的行为模式尚未被系统探索。现有研究主要关注任务导向场景，而对代理在空闲期、任务模糊或错误恢复等状态下的基线行为知之甚少，这限制了我们对代理自主行为的理解。\n\n## 关键洞察\n作者洞察到LLM代理在无任务状态下不会随机探索，而是会自发组织成三种特定行为模式，且这些模式具有模型确定性。这表明代理存在内在行为倾向，可能源于训练数据分布和架构偏差。\n\n## 解决方案演进\n作者首先设计连续ReAct框架，通过持久记忆和自我反馈实现长期自主运行；然后为代理提供基本工具和安全约束；最后在6个模型上进行18次实验，观察并分析行为模式，形成系统分类。\n\n## 创新点总结\n首次建立无提示LLM代理行为的系统基线，提出连续自我导向架构观察长期行为，发现三种可重现行为模式，为预测代理在任务模糊或自主操作中的行为提供新视角。"
                },
                {
                    "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
                    "arxiv_id": "2509.21128",
                    "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo",
                    "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)和监督微调(SFT)两种训练方法来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文提出了新的分析框架来量化推理路径，这属于改进LLM基础能力的研究。 其次，论文包含了多个正面指标：明确以大语言模型(LLMs)为研究对象；聚焦于推理能力(reasoning abilities)，特别是在数学领域；比较了强化学习(RL)和监督微调(SFT)这两种训练方法对推理能力的影响。 第三，论文不涉及任何排除标准中的领域。虽然论文在数学领域进行了实验，但数学只是作为推理能力的测试案例，而非论文的核心应用焦点。 最后，在特殊和模糊情况处理上，论文从可解释性角度提出了新的分析框架来理解训练方法如何影响推理过程，这有助于提升模型的内在可解释性和推理质量，符合保留标准。 论文的核心贡献是揭示了RL和SFT对推理过程的不同影响：RL压缩不正确的推理轨迹，而SFT扩展正确的推理轨迹，这解释了为什么当前最佳实践是两阶段训练(SFT后跟RL)。这项研究对理解和提升LLM的通用推理能力具有重要意义。",
                    "summary2": "",
                    "summary_translation": "大型语言模型（Large language models, LLMs）通常通过可验证奖励的强化学习（Reinforcement learning with verifiable rewards, RLVR）和推理轨迹上的监督微调（Supervised fine-tuning, SFT）进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力在很大程度上仍然难以捉摸。超越基于准确性的研究，即这两个组成部分如何塑造推理过程，本文引入了一个新颖的分析框架，该框架量化推理路径并捕捉每个训练过程中（在数学领域使用1.5B、7B和14B参数的模型）的定性变化。具体而言，我们在两个粒度级别上调查推理过程：轨迹级别（trajectory-level，检查完整的推理输出）和步骤级别（step-level，分析节点对应于单个推理步骤的推理图）。值得注意的是，独特推理轨迹的聚类显示了互补效应：RL压缩了不正确的轨迹，而SFT扩展了正确的轨迹。步骤级别分析显示，RL使推理图中节点访问频率、度和介数中心性（betweenness centrality）分布的衰减率变陡（约2.5倍），而SFT则使其变平（减少到约三分之一）。这表明RL将推理功能集中到一小部分步骤中，而SFT则使其在许多步骤中均匀分布。此外，通过从多个角度评估推理图拓扑结构，我们描述了RL和SFT的共同和不同特征。我们的工作提出了一个新颖的推理路径视角，解释了为什么当前的最佳实践——先SFT后RL的两阶段训练——是成功的，并为数据构建和更高效的学习方法提供了实际意义。",
                    "inspiration_trace": "## 面临的挑战\n作者发现现有研究主要关注RL和SFT对LLM推理能力的准确率影响，却忽略了这两种训练方法如何塑造推理过程的本质机制，无法解释为何当前最佳实践是两阶段训练(SFT后接RL)。\n\n## 关键洞察\n作者从推理路径视角切入，提出RL和SFT对推理过程有截然不同的影响：RL\"压缩\"不正确路径，而SFT\"扩展\"正确路径。这一洞察超越了传统准确率评估，揭示了两种训练方法的互补机制。\n\n## 解决方案演进\n作者构建了双层分析框架：轨迹级(整体推理输出)和步骤级(推理图结构)。通过聚类分析独特轨迹和构建推理图，量化发现RL减少错误轨迹并集中功能节点，SFT增加正确轨迹并均匀分布功能，从而解释了两阶段训练的成功原理。\n\n## 创新点总结\n首次从推理路径结构角度揭示RL与SFT的差异化机制，提出\"RL压缩、SFT扩展\"的核心观点，为理解LLM推理训练提供了新视角，并解释了当前最佳训练范式的理论基础。"
                },
                {
                    "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                    "arxiv_id": "2509.21043",
                    "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney",
                    "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了评估LLM创造力的理论框架和算法任务，研究了LLM创造力的扩展行为，发现了模型架构对创造能力的影响，并探讨了\"构想-执行差距\"和\"新颖性-实用性权衡\"等根本性问题。虽然论文提到了科学想法生成作为创造力的应用例子，但其重点是研究创造力这一通用能力本身，而不是专注于特定领域的应用。创造力可以被视为一种高级的问题解决和推理能力，与通用推理能力密切相关。论文明确关注LLMs的核心能力提升，试图为理解和改进现代AI模型中的创造力提供基础，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在创造性任务中的泛化能力评估问题。针对组合创造力场景，我们提出了一种基于概念空间图的理论框架和算法任务，通过评估输出的新颖性和实用性来衡量创造力。在不同规模（1M-100M参数）的Transformer模型上通过创造力分数验证了存在最优深度和宽度，并发现了新颖性-实用性权衡这一固有局限性，即使模型规模扩大也无法克服。",
                    "summary_translation": "人工智能（AI）系统，特别是大型语言模型（LLMs），越来越多地被用于科学创意生成等创造性任务，这构成了一种现有概念框架尚未解决的从训练数据中泛化的形式。尽管在许多方面与组合泛化（CG）形式相似，但组合创造力（CC）是一种开放式能力。我们不采用针对固定目标评估准确性或正确性的方法（这与CC的开放式性质相矛盾），而是提出了一个理论框架和算法任务，通过输出的新颖性和效用程度来评估。基于此，我们做出了几项重要的实证贡献：（1）我们首次获得了关于LLMs创造力扩展行为的见解。（2）我们发现，对于固定的计算预算，存在最佳的模型深度和宽度以实现创造性能力。（3）我们发现，构思-执行差距（即LLMs擅长生成新颖的科学创意但难以确保其实际可行性）可能可以通过一个更根本的新颖性-效用权衡来解释，这种权衡是创造力算法的普遍特征。重要的是，即使在规模扩展的情况下，这种权衡仍然持续存在，这对当前形式LLMs的长期创造潜力提出了质疑。总的来说，我们的概念框架和实证发现为理解和改进现代AI模型中的创造力提供了基础，标志着泛化能力的新前沿。",
                    "inspiration_trace": "## 面临的挑战\nAI系统特别是LLMs用于科学创意生成等创造性任务时，缺乏基础数学框架，导致生成的想法常存在实际可行性问题，形成\"构思-执行差距\"，现有泛化能力研究无法捕捉创造力的开放性特征。\n\n## 关键洞察\n组合创造力是一种开放性能力，不同于组合泛化，它通过将熟悉概念进行不熟悉组合产生新想法。创造力评估不应针对固定目标，而应通过新颖性和实用性程度评估。创造力涉及概念空间中的路径发现，且存在新颖性与实用性间的基本权衡。\n\n## 解决方案演进\n作者先提出形式化框架，将概念空间建模为图，创造性工件定义为标记路径。然后量化新颖性（通过图距离和标签惊喜度）和实用性（通过逻辑约束遵守度），最后提出创造力连续度量作为两者乘积。在此框架下进行大规模实证研究，探索模型架构对创造力的影响。\n\n## 创新点总结\n首次提出组合创造力的形式化框架和算法任务，揭示LLMs创造力缩放行为及最优模型结构，发现新颖性-实用性权衡解释\"构思-执行差距\"，为理解和改进AI模型创造力提供新基础。"
                },
                {
                    "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
                    "arxiv_id": "2509.20744",
                    "authors": "Qihang Ai, Haiyun Jiang",
                    "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了一种结合自回归(AR)和非自回归(NAR)模型的新范式来增强推理效率。论文明确关注推理密集型任务，特别是数学和代码领域，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个重要的正面指标：核心概念上涉及语言模型(LLMs)；能力方向上明确关注reasoning和math reasoning；论文提出的方法论是通过NAR模型生成中间推理轨迹，然后由AR模型生成最终答案，这是一种增强模型推理能力的新方法。 第三，论文不符合任何排除标准。虽然提到了\"discrete diffusion models\"，但这是在语言模型推理的上下文中讨论的，而非视觉或多模态应用。论文虽然以数学和代码为例，但其方法是一种通用推理框架，并非针对特定应用领域的研究。 论文的核心贡献是提出了一种新的推理范式，通过并行生成中间推理步骤来提高推理效率，同时保持输出质量，这直接服务于提升大语言模型的通用推理能力的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在推理任务中效率与质量之间的权衡问题。针对数学和编程等需要多步推理的任务，我们提出了一种结合非自回归(NAR)和自回归(AR)模型的混合推理框架，其中NAR模型并行生成中间推理痕迹，AR模型基于这些痕迹顺序生成最终答案。在AIME2025、GSM8K和LeetCode-Hard数据集上通过成功率指标验证了其有效性，相比基线方法提高了26%的性能，同时显著降低了推理成本。",
                    "summary_translation": "我们通过一个整合自回归(auto-regressive, AR)和非自回归(non-autoregressive, NAR)语言模型的框架来研究推理任务。AR模型顺序生成文本，擅长产生连贯的输出，但常常存在推理速度慢的问题，特别是在需要冗长思维链的推理密集型领域，如数学和代码。相比之下，NAR模型（如离散扩散模型）允许并行生成，提供显著的加速，但通常以输出质量降低为代价。为解决这些局限性，我们引入了一种新范式，其中NAR模型高效生成中间推理轨迹(intermediate reasoning traces)，随后指导AR模型提供精确的最终答案。实验证明，我们的方法相比强大的基线模型实现了显著的26%改进，同时大幅降低了推理成本。",
                    "inspiration_trace": "## 面临的挑战\n自回归(AR)模型在推理密集型任务中速度慢，特别是需要长链推理时；非自回归(NAR)模型虽可并行生成提速，但输出质量较低；当前推理模型存在\"过度思考\"问题，冗余推理步骤增加计算开销和延迟。\n\n## 关键洞察\nAR和NAR模型具有互补优势：AR擅长产生连贯可靠输出，NAR具有高效并行生成和全局上下文建模能力；推理过程可分解为不同性质的\"思考\"和\"回答\"两个阶段，适合不同模型处理。\n\n## 解决方案演进\n从R1风格模型的结构化生成方案和NAR模型优势出发，提出混合推理范式：用NAR模型高效生成紧凑但明确的推理轨迹，再用AR模型基于这些轨迹产生精确最终答案，实现优势互补。\n\n## 创新点总结\n首创\"并行思考，顺序回答\"的NAR-AR混合推理范式，通过明确分工解决速度与质量的权衡问题，在保持推理质量同时显著降低推理成本，为解决\"过度思考\"问题提供新思路。"
                },
                {
                    "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection",
                    "arxiv_id": "2509.20562",
                    "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang",
                    "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多层次反思增强的自学习智能体方法。论文本质上是关于改进LLM的基础能力，特别是通过反思机制增强其通用推理和问题解决能力。该方法在三个层次（单轨迹、任务内、任务间）合成高质量反思，并微调语言模型作为回顾性模型，从而提升LLM智能体的自我学习和适应能力。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及LLM智能体、问题解决和自我学习等正面指标，同时没有被排除标准所涵盖。虽然使用了TravelPlanner等基准测试，但这些是用于评估通用推理能力的标准测试集，而非特定应用领域的研究。因此，该论文符合筛选条件。",
                    "summary2": "本文旨在解决LLM agents在复杂任务中难以生成有意义反思的问题。针对失败密集的复杂任务环境，我们提出了一种基于多级反思合成的自学习框架SAMULE，通过微观、中观和宏观三个层次的反思分析训练回顾性语言模型，并在TravelPlanner、NATURAL PLAN和Tau-bench三个基准测试上通过Pass Rate和Accuracy等指标验证了其有效性。",
                    "summary_translation": "尽管LLM agents（大型语言模型代理）取得了快速进展，但由于错误分析不足以及对罕见成功轨迹的依赖，它们仍面临生成有意义反思的挑战，特别是在复杂任务中。在这项工作中，我们提出了SAMULE，一种新的自学习代理框架，该框架由一个基于多级反思合成（Multi-Level Reflection Synthesis）训练的回顾性语言模型（retrospective language model）驱动。它首先在三个互补的层次上合成高质量反思：用于详细错误修正的单轨迹学习（Single-Trajectory Learning，微观层次）；在同一任务的多次试验中构建错误分类法的任务内学习（Intra-Task Learning，中观层次）；以及基于不同任务失败中的同类型错误提取可转移见解的任务间学习（Inter-Task Learning，宏观层次）。然后，我们微调一个作为回顾性模型的语言模型，以在推理过程中生成反思。我们通过基于预见的反思机制（foresight-based reflection mechanism）进一步将我们的框架扩展到交互式环境，使代理能够通过比较预测和实际响应，在用户交互过程中主动反思和适应。在三个具有挑战性的基准测试——TravelPlanner、NATURAL PLAN和Tau-bench上的广泛实验表明，我们的方法显著优于基于反思的基线方法。我们的结果强调了精心设计的反思合成和以失败为中心的学习（failure-centric learning）在构建自我改进的LLM agents中的关键作用。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM智能体在复杂任务中难以生成有意义的反思，主要因错误分析机制不足且过度依赖罕见成功轨迹。在失败率高、成功稀缺的复杂环境中，现有方法无法有效从失败中学习，导致泛化能力差。\n\n## 关键洞察\n作者认识到失败是宝贵的学习资源，而非应避免的负面结果。从认知科学中汲取灵感，发现有效学习需在具体经验与抽象概念间建立桥梁，通过多层次反思实现从微观错误到宏观模式的全面理解。\n\n## 解决方案演进\n从洞察出发，作者设计多层次反思合成框架：微观层面分析单轨迹错误；中观层面构建任务内错误分类；宏观层面提取跨任务可转移见解。进而训练回顾模型生成反思，并扩展至交互环境，通过比较预测与实际响应实现实时适应。\n\n## 创新点总结\n以失败为中心的学习方法，突破传统依赖成功轨迹的局限；多层次反思合成实现从具体到抽象的全面学习；回顾模型使反思不依赖参考方案；预见性反思支持实时交互适应，共同构建更强大的自学习智能体。"
                },
                {
                    "title": "LATTS: Locally Adaptive Test-Time Scaling",
                    "arxiv_id": "2509.20368",
                    "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto",
                    "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。论文的核心贡献是提出了一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的新方法，该方法通过在每个生成步骤中动态调整计算资源分配来提高LLM的推理性能。具体来说，LATTS使用验证模型来评估局部难度，并据此决定是否重新采样、回溯、重启或停止生成过程，从而更有效地利用计算资源。 从筛选标准来看： 1. 核心判断：论文的本质是改进LLM的基础能力，特别是其推理效率，而不是将LLM作为工具应用到特定领域。它提出了一种新的测试时计算资源分配范式，这与提高LLM的通用推理能力直接相关。 2. 正面指标：论文明确涉及\"Large language models (LLMs)\"这一核心概念，并关注提高LLM在下游任务上的性能，这通常涉及推理和问题解决能力。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 4. 特殊情况：论文不涉及需要特殊处理的情况。 虽然论文没有明确提到思维链、强化学习等具体方法，但它提出了一种新的测试时计算资源分配方法，通过验证模型动态调整计算资源来提高LLM的推理性能，这与提高LLM的通用推理能力直接相关。因此，这篇论文符合我的研究目标。",
                    "summary2": "本文旨在解决语言模型推理时计算资源分配效率低下的问题。针对数学推理任务，我们提出了一种局部自适应测试时扩展（LATTS）方法，通过接受-拒绝采样动态分配计算资源，并在MATH500和AIME数据集上通过准确率与token生成量的关系验证了其有效性。",
                    "summary_translation": "提高大型语言模型（Large Language Models, LLMs）在下游任务性能的一种常见策略是使用验证模型（verifier model）从候选答案池中选择最佳答案，或引导自回归（auto-regressive）生成过程产生更优输出。这类方法通常能提高准确性，但代价是测试时（test-time）计算量增加，这种范式被称为测试时扩展（test-time scaling）。然而，大多数现有方法在所有样本和生成步骤上均匀增加计算量，未考虑各个实例的复杂性，导致资源利用效率低下。我们通过提出一种称为局部自适应测试时扩展（Locally Adaptive Test-Time Scaling, LATTS）的方法来解决这一限制，该方法在生成步骤之间分配可变的计算量。具体而言，在每个生成步骤，LATTS采用基于验证器（verifier-based）的接受标准（acceptance criterion）来决定是否重新采样（resample）、回溯（backtrack）、重新开始（restart）或停止生成过程。该标准基于从验证模型得出的局部困难度（local difficulty）的精确概念，有效调整每步的计算努力。实证结果表明，与标准基于验证器（verifier-based）的方法相比，LATTS实现了显著更优的准确性-计算权衡（accuracy--compute tradeoffs）。",
                    "inspiration_trace": "## 面临的挑战\n现有测试时计算扩展方法（如多数投票、BoN、束搜索）在数学推理任务中效率低下，需要生成大量token才能获得较好性能，无法根据推理过程中的局部难度自适应分配计算资源。\n\n## 关键洞察\n作者认识到多步推理中不同步骤具有不同的\"局部难度\"——有些步骤模型容易生成正确推理，而有些步骤则更加困难。如果能识别这些困难步骤并投入更多计算资源，就能更高效地利用测试时计算。\n\n## 解决方案演进\n基于这一洞察，作者提出LATTS方法，使用接受-拒绝采样在每个推理步骤中反复采样候选步骤，直到找到满足验证器条件的步骤。困难步骤自然需要更多采样尝试，从而实现计算资源的自适应分配。当无法找到可接受步骤时，设计了回退策略（如回溯、重启）来处理。\n\n## 创新点总结\n这种思路的创新在于首次在步骤级别实现自适应计算分配，将接受-拒绝采样应用于推理步骤生成，实现了从模型分布到目标分布的有效采样，显著提高了计算效率。"
                },
                {
                    "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
                    "arxiv_id": "2509.21282",
                    "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman",
                    "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出一种名为Probability Smoothing Policy Optimisation (PSPO)的新方法，用于改进LLM的强化学习训练过程。从本质上看，这篇论文直接关注改进LLM的基础训练方法，而不是将LLM作为工具应用到特定领域。论文提出的方法通过平滑策略概率来创建软信任区域，解决了传统裁剪方法带来的信息丢失和梯度不连续问题，从而提升了模型的推理能力。论文在多个数学推理数据集(GSM8K、SVAMP、ASDiv和MATH-500)上进行了评估，结果显示PSPO能显著提升模型的推理性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的强化学习训练方法和推理能力提升等主题也符合正面指标，同时不涉及任何排除标准中的领域(如多模态、特定应用领域或模型可靠性的应用层面)。因此，这篇论文完全符合研究范围。",
                    "summary2": "本文旨在解决LLM强化学习中比率裁剪导致信息丢失和梯度不连续的问题。针对PPO和GRPO等RL训练方法，我们提出了一种概率平滑策略优化(PSPO)方法，通过将当前策略概率平滑向旧策略创建软信任区域，并在GSM8K、SVAMP、ASDiv和MATH-500数据集上通过准确率和响应质量指标验证了其有效性。",
                    "summary_translation": "使用强化学习（Reinforcement Learning, RL）方法（如PPO和GRPO）训练大型语言模型（Large Language Models, LLMs）通常依赖于比例裁剪（ratio clipping）来稳定更新。虽然裁剪在防止不稳定性方面有效，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化（Probability Smoothing Policy Optimisation, PSPO），该方法在计算重要性比率（importance ratio）之前，将当前策略的概率平滑到旧策略（行为策略，behaviour policy），类似于标签平滑（label smoothing）。与裁剪不同，PSPO保留了梯度信号（gradient signal），同时向旧策略的插值创建了一个软信任区域（soft trust region），阻止大的、破坏稳定的更新，并有正式保证。我们在GRPO中实例化PSPO（GR-PSPO），并在GSM8K上微调Qwen2.5-0.5B和Qwen2.5-1.5B，在GSM8K测试集以及SVAMP、ASDiv和MATH-500上评估跨数据集泛化（cross-dataset generalisation）。与未裁剪的GRPO（单次迭代；无数据重用，比例始终=1）相比，GR-PSPO实现了相似的性能，但改进了推理过程，导致响应更清晰、更简洁且更合乎逻辑。与裁剪的GRPO相比，GR-PSPO在0.5B和1.5B模型上都大幅提高了性能，在GSM8K上提升了超过20%（0.5B模型为39.7% vs 17.6%，1.5B模型为59.4% vs 37.8%）。",
                    "inspiration_trace": "## 面临的挑战\nLLM强化学习中，PPO和GRPO等依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性，当策略比率超出裁剪范围时导致梯度消失，限制探索能力。\n\n## 关键洞察\n作者借鉴监督学习中的标签平滑技术，认识到可将当前策略概率平滑地朝向旧策略，而非直接截断比率。这种方法能创建\"软信任区域\"，既保留梯度信号，又防止大幅不稳定更新，本质上隐式引入KL散度控制。\n\n## 解决方案演进\n从平滑思想出发，提出PSPO：先将当前策略与旧策略线性插值˜πθ = (1−α)πθ + απθold，再计算平滑比率˜rt = (1−α)rt + α。这使比率在r=1周围收缩，形成以旧策略为锚点的软信任区域，同时处处保持非零梯度。最后将PSPO实例化到GRPO中形成GR-PSPO。\n\n## 创新点总结\n创新在于将标签平滑创造性地应用于策略优化，提出梯度保持的替代方案，通过概率平滑自然形成软信任区域，无需额外KL惩罚项，且计算内存中立，易于实现。"
                },
                {
                    "title": "Tree Search for LLM Agent Reinforcement Learning",
                    "arxiv_id": "2509.21240",
                    "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu",
                    "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文本质上是提出Tree-based Group Relative Policy Optimization (Tree-GRPO)这一新的强化学习方法，旨在增强LLM智能体的通用推理和规划能力，特别是在长期和多轮任务中解决稀疏监督问题。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理、规划等通用能力\"的范畴。 其次，论文符合所有正面指标：核心概念涉及LLM；能力方向关注推理和问题解决；训练方法采用强化学习；新兴范式涉及LLM-based agents。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。 特别地，论文提出的是一种通用的智能体强化学习方法，而不是将智能体应用于特定领域，这符合第四步中关于智能体/工具使用的保留标准。论文的核心贡献是通过树搜索技术增强LLM的通用推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。",
                    "summary2": "",
                    "summary_translation": "强化学习(reinforcement learning, RL)的最新进展显著增强了大型语言模型(large language models, LLM)的代理能力。在长期和多轮代理任务中，仅由结果奖励驱动的现有方法常常面临稀疏监督(sparse supervision)问题。为应对这一挑战，我们提出了基于树的分组相对策略优化(Tree-based Group Relative Policy Optimization, Tree-GRPO)，这是一种基于树搜索的分组代理强化学习方法，其中每个树节点代表完整的代理交互步骤。通过共享公共前缀，树搜索采样在固定token或工具调用预算内增加了可实现的模拟次数(rollouts)。此外，我们发现树结构轨迹即使仅使用结果奖励，也自然允许构建逐步过程监督信号(step-wise process supervised signals)。基于此，Tree-GRPO在树内(intra-tree)和树间(inter-tree)两个层面估计分组相对优势。通过理论分析，我们证明树内层面分组相对策略优化的目标等同于步骤级直接偏好学习(step-level direct preference learning)的目标。在11个数据集和3种问答任务上的实验证明了所提出的基于树的强化学习方法相对于基于链的强化学习方法的优越性。",
                    "inspiration_trace": "## 面临的挑战\nLLM智能体强化学习中存在两大核心问题：一是多轮交互导致的高预算成本，轨迹包含大量token和工具调用；二是仅依赖结果奖励造成的稀疏监督，难以识别长序列中具体步骤的贡献。\n\n## 关键洞察\n作者意识到树结构能同时解决这两个问题：共享前缀可减少预算冗余，而树中分支点的结果奖励差异能自然构建步骤级过程监督信号，无需额外标注。\n\n## 解决方案演进\n从链式采样转向树搜索，并创新性地以完整Thought-Action-Observation步骤为树节点（非传统token级）。通过树内和树间分组优势估计，将轨迹级信号转化为过程级监督，并理论证明其等价于步骤级偏好学习。\n\n## 创新点总结\n核心创新在于利用树结构同时解决预算和稀疏监督双重挑战，通过智能体步骤级节点设计实现隐式步骤级监督，为LLM智能体RL提供了更高效、更精细的训练范式。"
                },
                {
                    "title": "GRPO is Secretly a Process Reward Model",
                    "arxiv_id": "2509.21154",
                    "authors": "Michael Sullivan",
                    "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究\"大语言模型通用推理能力\"的目标。从核心判断来看，论文本质上是关于改进LLM的训练方法，具体研究了GRPO强化学习算法的内在机制，并提出改进版本λ-GRPO来提升LLM的推理能力。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 从正面指标看，论文包含了多个相关主题： 1. 核心概念：明确研究LLMs（大语言模型） 2. 能力方向：关注\"downstream reasoning tasks\"（下游推理任务） 3. 训练方法：深入研究\"GRPO RL algorithm\"（GRPO强化学习算法） 从排除标准看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉 - 不是将LLM应用于特定领域（如医疗、化学等） - 不主要关注模型可靠性方面的应用问题 论文的核心贡献是发现了GRPO算法隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出改进算法λ-GRPO，显著提升了LLM在推理任务上的表现。这直接服务于提升大语言模型本身的通用推理能力，而非将LLM作为工具应用于特定领域。因此，该论文完全符合研究目标。",
                    "summary2": "本文旨在揭示GRPO算法隐式诱导进程奖励模型(PRM)的机制及其缺陷。针对GRPO训练中非均匀分布进程步骤阻碍探索与利用的问题，我们提出了一种λ-GRPO改进算法，并在OpenRS数据集及多个推理benchmark上通过验证准确性和任务性能验证了其有效性。",
                    "summary_translation": "我们从理论上证明，在关于不同完成结果中词元序列(token sequences)的组内重叠性(within-group overlap)的特定假设下，GRPO RL算法会诱导出一个非平凡的过程奖励模型(process reward model, PRM)。然后我们通过实证研究表明，这些假设在现实世界条件下是成立的：GRPO确实会诱导出一个非平凡的PRM。利用GRPO作为PRM(GRPO-as-a-PRM)的框架，我们发现了GRPO目标(objective)中的一个缺陷：非均匀分布的过程步骤(process steps)会阻碍探索和利用(exploration and exploitation)（在不同条件下）。我们提出了对算法的一个简单修改来缓解这一缺陷（λ-GRPO），并表明使用λ-GRPO训练的大型语言模型(large language models, LLMs)比使用标准GRPO训练的模型在验证准确性(validation accuracy)和下游推理任务(downstream reasoning tasks)上取得了更高的性能，并且更快达到峰值性能(peak performance)。我们的研究结果对GRPO使用昂贵的、明确定义的PRMs的优势提出了质疑：我们展示了可以利用原始GRPO算法(vanilla GRPO)中隐藏的、内置的PRM结构来提升模型性能，而对训练时间和成本的影响可以忽略不计。",
                    "inspiration_trace": "## 面临的挑战\n过程奖励模型(PRM)能提供细粒度奖励提升推理性能，但训练成本高昂且易受奖励黑客攻击。GRPO算法虽简化了RL训练，但因其消除了评论家模型，难以与PRMs结合，现有方法需修改算法以适应细粒度奖励。\n\n## 关键洞察\n作者发现GRPO实际上隐含地实现了一个基于蒙特卡洛的PRM。当组内轨迹共享相同前缀时，GRPO会将结果级奖励转化为步骤级奖励，且实际条件下这种前缀重叠几乎总是存在，使GRPO\"秘密地\"成为一个非平凡的PRM。\n\n## 解决方案演进\n作者先理论证明GRPO诱导PRM，再实证验证其非平凡性。进而发现GRPO隐含PRM存在缺陷：非均匀分布的过程步骤阻碍探索与利用。为此提出λ-GRPO，在损失函数中加入PRM感知归一化因子，平衡各过程集的贡献。\n\n## 创新点总结\n揭示了GRPO中隐藏的PRM结构，挑战了需昂贵显式PRMs的观点。通过利用算法内置的步骤级奖励信号，提出简单有效的λ-GRPO方法，以 negligible 成本提升性能，实现更快收敛和更高准确率。"
                },
                {
                    "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
                    "arxiv_id": "2509.21091",
                    "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada",
                    "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的基础能力。论文研究的是通过测试时计算(test-time compute)优化来提高大语言模型性能的方法，具体聚焦于best-of-N采样和多数投票机制，以及当N趋近于无穷大时的渐近性能分析。这明显属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：虽然未直接使用\"reasoning\"等词汇，但best-of-N方法和多数投票本质上是为了提高模型的问题解决能力和推理质量 - 新兴范式：论文探讨了多个LLMs的加权集成方法，这与模型协作和优化相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未针对医疗、化学、生物等特定应用领域 - 未关注水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及特殊或模糊情况。虽然提到了多个LLMs的加权集成，但主要焦点是测试时计算的优化，而非智能体协作框架或工具使用方法。 核心贡献：论文提出了一种自适应生成方案和多个LLMs的加权集成方法，通过优化测试时计算来提高LLM的性能。这种方法本质上是增强模型通用推理能力的有效途径，因为它不依赖于特定领域知识，而是通过更有效的计算资源分配和模型集成来提升LLM的问题解决能力，完全符合\"提高大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在 [解决大语言模型在测试时计算资源有限的情况下，如何实现接近best-of-∞性能的问题]。针对 [多个LLMs和复杂推理任务]，我们提出了一种 [基于Bayes因子的自适应采样方案和最优加权LLM集成方法]，并在 [AIME2024、AIME2025、GPQA-DIAMOND和MATH500数据集] 上通过 [准确率和计算效率] 验证了其有效性。",
                    "summary_translation": "我们研究基于多数投票的大语言模型（large language models, LLMs）的最佳选择策略（best-of-$N$）。特别地，我们分析了$N \\to \\infty$的极限情况，并将其表示为最佳无穷选择策略（Best-of-$\\infty$）。尽管该方法在极限情况下取得了令人印象深刻的性能，但它需要无限的测试时间预算。为解决这一问题，我们提出了一种自适应生成方案，该方案基于答案一致性来选择$N$，从而有效分配推理时间计算。除了自适应性之外，我们将该框架扩展到多个大语言模型的加权集成（weighted ensembles），表明这种混合模型可以优于任何单个模型。最优集成权重被表述为混合整数线性规划（mixed-integer linear program）问题，并可以高效计算。大量实验证明了我们方法的有效性。",
                    "inspiration_trace": "## 面临的挑战\nBest-of-N策略在N增大时能提升LLM性能，但N→∞（Best-of-∞）需无限计算预算，实际不可行。如何在有限预算下近似Best-of-∞性能是核心挑战。\n\n## 关键洞察\n作者将LLM生成答案视为从潜在答案分布中采样，Best-of-∞本质是寻找真实多数答案。关键突破是认识到无需无限采样，可自适应采样直到有足够统计证据确定当前最频繁答案即为真实多数。\n\n## 解决方案演进\n基于此，作者提出自适应生成方案：用贝叶斯因子作为停止准则，当证据足够时停止生成。为处理答案空间未知问题，采用Dirichlet过程先验进行非参数贝叶斯建模。进而扩展到多LLM集成，将权重优化表述为混合整数线性规划问题。\n\n## 创新点总结\n创新在于：1) 将固定采样转为自适应采样，大幅提高计算效率；2) 引入贝叶斯统计处理LLM答案不确定性；3) 在多LLM集成中，通过Best-of-∞视角将权重优化简化，避免组合爆炸。"
                },
                {
                    "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
                    "arxiv_id": "2509.21044",
                    "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li",
                    "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何增强大语言模型的内部机制，特别是激活强度和多样性。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，论文探索了RL微调相比监督微调(SFT)能够更有效提升LLM能力的原因，属于对LLM基础能力的深入研究。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 训练方法：核心研究强化学习(RL)微调对LLM的影响，包括比较PPO、GRPO和DPO等不同RL方法 - 能力方向：虽然未直接研究具体推理任务，但探讨了RL微调如何提升LLM的泛化能力，这与通用推理能力密切相关 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学、生物等） - 不关注模型可靠性的应用层面（如水印、安全等） 论文的核心贡献在于揭示了RL微调如何系统性地改变LLM的内部电路机制，发现RL微调导致激活强度增加和激活模式多样化，这些内部变化可能是RL提升LLM泛化能力的原因。这种对LLM内部机制的基础性研究，直接有助于理解如何提升LLM的通用推理能力，因此完全符合研究目标。",
                    "summary2": "本文旨在 [解决强化学习微调增强LLMs能力的内部机制问题]。针对 [不同架构的LLMs]，我们提出了一种 [基于边缘归因修补(EAP)的内部电路分析方法]，并在 [多个模型家族和数学数据集] 上通过 [激活强度和多样性指标] 验证了RL微调增强了内部激活强度和多样性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）通过大规模预训练获得广泛的先验知识，并可以通过监督微调（supervised fine-tuning, SFT）或基于强化学习（reinforcement learning, RL）的后训练进一步增强。越来越多的证据表明，RL微调能够提升LLMs的能力，超越仅使用SFT所能达到的水平。然而，RL微调为何能够增强具有不同内在特征的各种LLMs能力的潜在机制仍未得到充分探索。\n\n在本研究中，我们从边缘归因修补（edge attribution patching, EAP）的先前工作中获得灵感，以研究LLMs在RL微调前后的内部差异。我们对多个模型系列的分析显示了在线RL后训练（online RL post-training）的两个稳健效应：（i）激活强度的整体增加，表明更多的内部通路被激活且其信号变得更强；（ii）激活模式的多样性增加，反映在更高的熵和更分散的边缘分布上。这些变化表明，RL重塑了信息流，使其既更具冗余性又更具灵活性，这可能解释了其在泛化方面的优势。\n\n值得注意的是，使用直接偏好优化（Direct Preference Optimization, DPO）微调的模型偏离了这些趋势，与基于PPO和GRPO的训练相比，表现出明显较弱或不一致的内部变化。总的来说，我们的发现提供了RL微调如何系统性改变LLMs内部电路的统一视角，并强调了在线RL和基于偏好方法之间的方法论区别。我们的代码在https://anonymous.4open.science/r/llm_rl_probing_analysis-F673上开源。",
                    "inspiration_trace": "## 面临的挑战\nRL微调能增强LLMs能力超越SFT，但其底层机制未被充分探索。现有研究多关注外部行为变化，而内部机制研究与RL后训练方法脱节，导致两条研究线平行发展。\n\n## 关键洞察\n作者从图论视角理解LLMs内部结构，将其视为有向无环图，其中节点是子模块，边缘是残差信息路径。这种视角使系统分析RL微调如何重塑内部信息流成为可能。\n\n## 解决方案演进\n首先采用EAP框架高效估计边缘重要性权重，避免传统方法计算瓶颈；然后设计激活强度、信息复杂性和分布峰度三个互补指标量化内部变化；最后在多模型家族上实验，比较不同RL方法影响。\n\n## 创新点总结\n首次揭示RL微调对内部电路结构的系统性影响，发现其普遍增强激活强度和多样化激活模式，并阐明在线RL与静态偏好优化在内部机制上的本质区别。"
                },
                {
                    "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments",
                    "arxiv_id": "2509.20386",
                    "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
                    "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出Dynamic ReAct方法，用于改进ReAct智能体在大型工具环境中的工具选择能力。ReAct是一种结合推理和行动的智能体框架，使大语言模型能够进行推理并使用工具解决问题。论文的核心贡献是解决当工具数量超过LLM上下文记忆限制时的工具选择挑战，这属于增强LLM通用推理能力的范畴，特别是工具使用和问题解决方面。论文不是将LLM作为工具应用到特定领域，而是改进LLM本身通过智能体框架使用工具的能力，因此符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：涉及ReAct agents，这是基于LLM的智能体框架 - 能力方向：与problem-solving相关，因为工具选择是问题解决的关键环节 - 新兴范式：明确涉及llm-based agents和tool use，这是论文的核心主题 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用工具选择机制 - 不涉及模型可靠性方面的应用层面问题 第四步：特殊和模糊情况处理 论文提出的Dynamic ReAct方法是一种通用的工具选择机制，旨在增强LLM通过智能体框架使用工具的通用问题解决能力，而不是将工具应用在特定领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，因此应该保留。 最终决策 这篇论文的核心贡献是提出了一种改进LLM通过智能体框架使用工具的通用能力的方法，从而增强了LLM的通用推理和问题解决能力。论文专注于工具选择这一关键环节，使LLM能够更有效地在大型工具环境中运作，这直接符合提高大语言模型通用推理能力的研究目标。",
                    "summary2": "本文旨在解决大型语言模型代理在管理大规模MCP工具集时面临的工具选择挑战。针对包含数百或数千可用工具的环境，我们提出了一种Dynamic ReAct方法，结合元工具和优化的向量检索实现动态工具选择，并通过实验验证了该方法可将工具加载减少50%同时保持任务完成准确性。",
                    "summary_translation": "我们提出了Dynamic ReAct（动态反应），这是一种新颖的方法，使ReAct代理能够高效地操作超出大型语言模型上下文内存限制的广泛Model Control Protocol (MCP，模型控制协议)工具集。我们的方法解决了在包含数百或数千可用工具的环境中进行工具选择的基本挑战，在这些环境中同时加载所有工具在计算上是不可行的。我们提出并评估了五种不同的架构，这些架构逐步完善工具选择过程，最终实现了一种搜索和加载机制（search-and-load mechanism），能够以最小的计算开销实现智能工具选择。我们的实验结果表明，所提出的方法将工具加载减少了高达50%，同时保持了任务完成准确性，推进了真正通用AI代理（general-purpose AI agents）的发展，这些代理能够动态适应不同的任务环境。",
                    "inspiration_trace": "## 面临的挑战\nLLM代理面临工具规模与上下文限制的根本矛盾：随着可用工具数量激增，传统方法将所有工具加载到有限上下文中变得不可行。纯语义搜索在大规模环境中存在检索不精确和上下文饱和问题，尤其难以处理需要跨多个应用协调的复杂任务。\n\n## 关键洞察\n作者认识到需要动态工具管理方法，核心洞察是引入\"元工具\"概念——专门设计用于管理其他工具的工具。结合语义搜索能力，可以创建能根据任务需求自适应调整工具集的代理，同时分层检索（先应用后工具）可显著提高选择准确性。\n\n## 解决方案演进\n从直接语义搜索基线开始，逐步演进：1)让LLM构建原子查询；2)引入搜索和加载两阶段机制；3)添加应用感知层；4)尝试固定工具集。最终\"搜索和加载架构\"达到最佳平衡，结合上下文丰富化嵌入技术，实现精准工具选择。\n\n## 创新点总结\n系统化评估了五种动态工具管理模式，证明上下文丰富化嵌入可提高检索精度50%，并通过元工具设计实现动态工具选择，为构建能适应大规模工具环境的通用AI代理奠定基础。"
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 20,
            "papers": [
                {
                    "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
                    "arxiv_id": "2509.21193",
                    "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin",
                    "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种统一框架，通过结合隐式检索和结构化协作来增强大型语言模型的推理能力。从筛选标准来看，该论文完全符合我的研究目标。首先，论文本质上是关于改进LLM的基础推理能力，提出了基于Monitor的检索模块、分层解决方案改进(HSR)和质量感知迭代推理(QAIR)等新方法，这些都属于提升LLM通用推理能力的范畴。其次，论文包含多个正面指标，如明确研究大型语言模型(LLMs)、关注推理能力(reasoning)、采用多智能体系统(multi-agent systems)和工具使用(tool use)等新兴范式。虽然论文标题提到\"Scientific Reasoning\"，但其框架是通用的，并非局限于特定应用领域，而是以科学推理作为测试场景来验证其通用推理框架的有效性。论文解决的是LLM推理中的普遍问题（如检索导致的推理碎片化和多智能体流水线中的解决方案稀释问题），提出的方法具有通用性，可以应用于各种推理任务。因此，该论文符合研究范围，应被保留。",
                    "summary2": "本文旨在解决科学推理中的推理碎片化和多智能体协作效率问题。针对复杂的科学推理任务，我们提出了一种结合Monitor-based RAG、分层解决方案精炼(HSR)和质量感知迭代推理(QAIR)的统一框架，并在Humanity's Last Exam (HLE) Bio/Chem Gold数据集上通过准确率、token使用量和智能体步骤数验证了其有效性。该方法实现了48.3%的准确率，超越最强基线13.4个百分点，同时减少53.5%的token使用和43.7%的智能体步骤。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）最近在科学推理方面表现出强劲的进展，但仍存在两个主要瓶颈。首先，显式检索（explicit retrieval）使推理过程碎片化，施加了额外的标记和步骤所带来的隐性\"工具税\"（tool tax）。其次，多智能体管道（multi-agent pipelines）通常通过对所有候选方案进行平均而稀释了优质解决方案。我们通过一个结合隐式检索（implicit retrieval）和结构化协作（structured collaboration）的统一框架来解决这些挑战。在该框架的基础上，基于Monitor的检索模块（Monitor-based retrieval module）在标记级别运行，以最小的推理中断整合外部知识。在此基础之上，分层解决方案精炼（Hierarchical Solution Refinement, HSR）迭代地将每个候选方案指定为由其同行修复的锚点，而质量感知迭代推理（Quality-Aware Iterative Reasoning, QAIR）则根据解决方案质量调整精炼过程。在\"人类最终考试\"（Humanity's Last Exam, HLE）生物/化学金牌数据集上，我们的框架达到了48.3%的准确率——这是迄今为止报道的最高水平，超过了最强的智能体基线13.4个百分点，领先前沿LLMs高达18.1个百分点，同时将标记使用量减少了53.5%，智能体步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了该框架在不同领域的鲁棒性。错误分析显示，在超过85%的情况下，推理失败和知识缺口同时出现，而多样性分析揭示了一个明显的二分法：检索任务受益于解决方案的多样性，而推理任务则倾向于共识。总体而言，这些发现展示了隐式增强（implicit augmentation）和结构化精炼（structured refinement）如何克服显式工具使用和统一聚合的低效问题。代码可在以下网址获取：https://github.com/tangxiangru/Eigen-1。",
                    "inspiration_trace": "## 面临的挑战\n作者识别了科学推理中的两大瓶颈：显式检索导致的推理碎片化（\"tool tax\"），以及多智能体系统中对候选解决方案的平均化处理，这会稀释高质量解决方案。\n\n## 关键洞察\n通过分析错误模式，作者发现推理失败和知识缺口在85%以上案例中同时出现，表明这些问题本质交织。同时，不同任务需要不同策略：检索任务受益于多样性，推理任务则受益于共识，这反映了专家协作中自然形成的\"锚点-支持\"结构。\n\n## 解决方案演进\n基于这些洞察，作者提出统一框架：Monitor-based RAG在token级别隐式注入知识，避免显式工具调用；分层解决方案精炼(HSR)建立\"锚点-参考\"结构进行修复；质量感知迭代推理(QAIR)根据解决方案质量动态调整精炼过程。\n\n## 创新点总结\n创新在于从显式工具调用转向隐式知识增强，从民主式协作转向结构化\"锚点-参考\"关系，并根据任务特性和解决方案质量实现自适应优化，更符合人类专家的推理模式。"
                },
                {
                    "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond",
                    "arxiv_id": "2509.21284",
                    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
                    "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是对思维链(CoT)的鲁棒性进行了理论分析，研究了输入扰动如何影响CoT的推理过程和输出。论文推导了在输出波动可接受范围内输入扰动的上界，并证明了该上界与CoT中的推理步骤数量正相关，以及即使无限长的推理过程也无法消除输入扰动的影响。这些发现对于理解和改进LLM的推理能力具有重要意义。思维链(CoT)是提高大语言模型推理能力的关键方法，论文从理论角度分析其鲁棒性，属于改进LLM基础能力和推理能力的研究范畴，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性的应用层面研究，而是关注通用推理能力的理论分析，因此应该被保留。",
                    "summary2": "本文旨在解决Chain-of-Thought (CoT)对输入扰动的鲁棒性问题。针对输入扰动如何影响CoT输出的理论解释缺失，我们提出了一种基于Lipschitz连续性的理论分析方法，推导出输出波动上界与推理步骤数量、嵌入向量范数的关系，并在MATH、MMLU-Pro和GPQA数据集上通过Exact Match和Output Fluctuation指标验证了其有效性。",
                    "summary_translation": "现有研究表明，思维链(Chain-of-Thought, CoT)的输出受到输入扰动的显著影响。尽管许多方法试图通过优化提示词(prompts)来减轻这种影响，但这些扰动如何影响CoT输出的理论解释仍是一个开放的研究领域。这一差距限制了我们对输入扰动在推理过程中如何传播的深入理解，并阻碍了提示词优化方法的进一步改进。因此，在本文中，我们从理论上分析了输入扰动对CoT输出波动的影响。我们首先在输出波动处于可接受范围内的条件下，推导出输入扰动的上界(upper bound)，并基于此证明：(i) 该上界与CoT中的推理步骤数量呈正相关；(ii) 即使无限长的推理过程也无法消除输入扰动的影响。然后，我们将这些结论应用于线性自注意力(Linear Self-Attention, LSA)模型，该模型可视为Transformer的简化版本。对于LSA模型，我们证明了输入扰动的上界与输入嵌入(input embedding)和隐藏状态(hidden state)向量的范数(norms)呈负相关。为验证这一理论分析，我们在三个主流数据集和四个主流模型上进行了实验。实验结果与我们的理论分析一致，从经验上证明了我们发现的正确性。",
                    "inspiration_trace": "## 面临的挑战\n思维链(CoT)对输入扰动极为敏感，微小输入变化会导致输出显著波动。尽管已有提示优化方法试图缓解此问题，但缺乏对输入扰动如何影响CoT输出的理论解释，限制了对扰动传播机制的深入理解和提示优化方法的进一步改进。\n\n## 关键洞察\n作者将CoT视为多步迭代过程，在Lipschitz连续性假设下，发现输入扰动对输出波动的影响可量化为上界。关键洞察是CoT鲁棒性不仅与推理步数相关，还与模型特性(如输入嵌入和隐藏状态向量范数)相关，且即使无限推理步数也无法完全消除扰动影响。\n\n## 解决方案演进\n作者先推导输出波动上界，证明推理步数与扰动影响关系；然后将结论应用于线性自注意力模型，证明输入扰动上界与向量范数负相关；通过实验验证理论；最后基于分析提出通过最大化输入扰动上界选择提示的方法。\n\n## 创新点总结\n首次从理论角度分析输入扰动对CoT影响，提出CoT鲁棒性上界，揭示无限推理无法消除扰动的本质，发现向量范数与鲁棒性的负相关关系，并基于理论提出新型提示优化方法。"
                },
                {
                    "title": "Query-Centric Graph Retrieval Augmented Generation",
                    "arxiv_id": "2509.21237",
                    "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出QCG-RAG，一种以查询为中心的图检索增强生成框架，旨在改进LLM的多跳推理能力。论文明确指出其目标是解决现有图RAG方法中的粒度困境，通过查询粒度索引和多跳块检索机制来增强LLM的推理能力。这属于改进LLM基础能力和增强其多步推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs)\" - 能力方向：专注于\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分 - 新兴范式：RAG本身可视为一种工具使用形式，论文提出的新框架增强了LLM利用外部知识的能力 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对医疗、化学、生物等特定应用领域 - 不主要关注水印、安全性等模型可靠性问题 第四步：特殊和模糊情况处理 论文提出的RAG框架可以视为一种通用的工具使用方法，目的是增强LLM的通用问题解决能力（特别是多跳推理），而非应用于特定领域。虽然论文提到了\"improving graph quality and interpretability\"，但这是作为提高推理质量的手段，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了一种新的通用框架来增强LLM的多跳推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决基于图的检索增强生成(RAG)方法面临的粒度困境。针对现有方法中细粒度实体级图导致高token成本和上下文丢失，而粗粒度文档级图无法捕捉细微关系的问题，我们提出了一种以查询为中心的图RAG框架(QCG-RAG)，利用Doc2Query和Doc2Query--构建可控粒度的查询中心图，并设计了多跳检索机制。在LiHuaWorld和MultiHop-RAG数据集上通过准确率和LLM-as-a-Judge指标验证了其有效性。",
                    "summary_translation": "基于图的检索增强生成(RAG)通过外部知识丰富大型语言模型(LLMs)，以实现长上下文理解和多跳推理(multi-hop reasoning)，但现有方法面临粒度困境：细粒度的实体级图(entity-level graphs)导致高token成本并丢失上下文，而粗粒度的文档级图(document-level graphs)则无法捕捉细微关系。我们提出了QCG-RAG，一种以查询为中心的图RAG框架，它支持查询粒度的索引(query-granular indexing)和多跳块检索(multi-hop chunk retrieval)。我们的以查询为中心的方法利用Doc2Query和Doc2Query{-}{-}构建具有可控粒度的查询中心图(query-centric graphs)，提高了图的质量和可解释性。定制的多跳检索机制(multi-hop retrieval mechanism)随后通过生成的查询选择相关块(chunks)。在LiHuaWorld和MultiHop-RAG上的实验表明，QCG-RAG在问答准确性方面始终优于先前的基于块(chunk-based)和基于图的RAG方法，为多跳推理建立了新范式。",
                    "inspiration_trace": "## 面临的挑战\n图RAG方法面临根本性的\"粒度困境\"：细粒度实体级图虽能捕捉细微关系，但token成本高且易丢失上下文；粗粒度文档级图虽效率高，却无法捕捉复杂关系，难以支持多跳推理。\n\n## 关键洞察\n作者认识到查询(query)可作为中间粒度表示，介于实体与文档之间。通过Doc2Query技术生成的查询既能保留语义丰富性，又具备精确指向性，能实现对齐用户意图的粒度可控表示。\n\n## 解决方案演进\n从这一洞察出发，作者首先利用Doc2Query和Doc2Query--生成高质量查询-答案对；然后构建查询中心图(QCG)，以查询为节点，建立查询间和查询与块间的关联；最后设计多跳检索机制，通过查询检索、邻居扩展、块聚合和生成四步实现精确推理。\n\n## 创新点总结\n该思路创新性地将查询置于图构建核心，首次实现了查询驱动的粒度可控图构建，解决了图RAG中的根本性粒度困境，在不牺牲多跳推理能力的同时，平衡了图的粒度和效率。"
                },
                {
                    "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
                    "arxiv_id": "2509.21212",
                    "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu",
                    "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于增强大语言模型在长期对话中的记忆管理能力。从筛选标准来看，首先，论文的本质是改进LLM的基础能力，特别是在处理超出上下文窗口的长期对话时的信息组织和检索能力，这属于增强LLM通用推理能力的范畴。有效的记忆管理是进行连贯推理和多步对话的基础，因此这项工作直接提升了LLM的通用能力。其次，论文包含了正面指标中的\"Large language models, LLMs\"和\"llm-based agents\"概念。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。最后，论文提出的SGMem是一种通用的记忆管理框架，用于增强对话代理的通用能力，而非应用于特定领域。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决长期对话代理中的记忆管理问题，特别是处理超出LLMs上下文窗口的对话历史。针对长期对话场景，我们提出了一种SGMem（句子图记忆）方法，将对话表示为分块单元内的句子级图，捕获跨不同粒度的关联。在LongMemEval和LoCoMo数据集上通过Accuracy指标验证了其有效性，SGMem一致地提高了准确性并优于强基线。",
                    "summary_translation": "长期对话代理(long-term conversational agents)需要有效的记忆管理(memory management)来处理超出大型语言模型(large language models, LLMs)上下文窗口(context window)的对话历史(dialogue histories)。基于事实提取(fact extraction)或摘要(summarization)的现有方法减少了冗余(redundancy)，但难以组织和检索不同粒度(granularities)的对话和生成记忆(generated memory)中的相关信息。我们介绍了SGMem (Sentence Graph Memory，句子图记忆)，它将对话表示为分块单元(chunked units)内的句子级图(sentence-level graphs)，捕捉跨越轮次级(turn-level)、回合级(round-level)和会话级(session-level)上下文的关联。通过将检索到的原始对话(retrieved raw dialogue)与生成的记忆(如摘要、事实和见解)相结合，SGMem为LLMs提供连贯(coherent)且相关(relevant)的上下文(context)用于响应生成(response generation)。在LongMemEval和LoCoMo上的实验表明，SGMem持续提高了准确性(accuracy)，并在长期对话问答(long-term conversational question answering)中优于强基线(strong baselines)。",
                    "inspiration_trace": "## 面临的挑战\n长期对话代理面临记忆过载和碎片化问题：随着交互累积，存储内容规模和复杂性超出管理能力；现有方法虽减少冗余，但难以组织和检索不同粒度的对话信息，相关信息分散在原始对话和生成记忆中，阻碍连贯检索。\n\n## 关键洞察\n作者认识到句子是对话交换的基本语义单位，足够细粒度以捕获上下文依赖；将句子构造为图节点可显式建模跨对话段关联，减轻记忆碎片化；且无需额外LLM提取，仅需标准句子分割工具即可构建有效记忆结构。\n\n## 解决方案演进\n从句子级表示出发，构建分层记忆框架，将对话组织为句子级图；设计多跳检索机制，在图上遍历以扩展相关句子并映射回父块；最终形成向量检索与图扩展双重设计，整合原始对话与生成记忆，提供连贯上下文。\n\n## 创新点总结\n首次以句子为基本记忆单位实现细粒度语义对齐；通过图结构显式建模句子关联解决碎片化；轻量级实现避免昂贵LLM提取；双重检索机制兼顾效率与连贯性，为长期对话提供新范式。"
                },
                {
                    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
                    "arxiv_id": "2509.21319",
                    "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev",
                    "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，该论文的本质是提出一种新的强化学习范式RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进LLM的后训练阶段。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。论文通过结合人类反馈(RLHF)和可验证奖励(RLVR)的优势，提出了一种新的训练方法来增强LLM的响应质量和对齐能力。 其次，论文包含多个正面指标： 1. 核心概念：明确关注LLMs，并使用Qwen3-32B作为示例模型 2. 训练方法：核心就是提出一种新的强化学习方法(RLBFF)，这是RLHF和RLVR的结合 3. 能力方向：虽然未直接强调推理，但提高LLM的响应质量隐含了推理能力的提升，且论文在MT-Bench等包含推理任务的基准上进行了评估 第三，论文不涉及任何排除标准中的领域： 1. 未涉及多模态与视觉内容 2. 未针对特定应用领域（如医疗、化学等） 3. 虽然涉及模型对齐，但核心是提出新方法提高整体响应质量，而非专注于水印、安全等应用层面 最后，在特殊和模糊情况处理上，论文明确解决了RLHF的可解释性问题，属于\"提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量\"的情况，符合保留条件。 综上所述，该论文的核心贡献是提出了一种新的强化学习训练方法来提升LLM的通用能力和对齐质量，这与研究目标\"提高大语言模型的通用推理能力\"高度一致，因此应该被保留。",
                    "summary2": "本文旨在解决RLHF缺乏可解释性和RLVR适用范围有限的问题。针对LLM后训练中的人类反馈和可验证奖励场景，我们提出了一种RLBFF（二元灵活反馈）方法，将自然语言反馈转换为可二元评估的原则，并在RM-Bench（86.2%）和JudgeBench（81.4%）上通过奖励模型性能验证了其有效性。",
                    "summary_translation": "# 基于二元灵活反馈的强化学习在大型语言模型后训练中的应用\n\n基于人类反馈的强化学习(Reinforcement Learning with Human Feedback, RLHF)和基于可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)是大型语言模型(LLM)后训练中主要使用的强化学习范式，各自具有独特的优势。然而，RLHF在可解释性和奖励黑客(reward hacking)方面存在困难，因为它依赖于通常缺乏明确标准的人类判断，而RLVR由于其专注于基于正确性的验证器，在应用范围上受到限制。我们提出了基于二元灵活反馈的强化学习(Reinforcement Learning with Binary Flexible Feedback, RLBFF)，它结合了人类驱动偏好的多样性和基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的响应质量的细微方面。RLBFF从自然语言反馈中提取可以以二元方式回答的原则（例如，信息准确性：是，或代码可读性：否）。这些原则随后可用于将奖励模型训练构建为蕴含任务（响应满足或不满足任意原则）。我们表明，以这种方式训练的奖励模型在数据匹配的情况下可以优于Bradley-Terry模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日排行榜第一）上取得顶级性能。此外，与Bradley-Terry模型不同，用户可以在推理时指定感兴趣的原则，以定制我们奖励模型的关注点。最后，我们提供了一个完全开源的方案（包括数据），使用RLBFF和我们的奖励模型来对齐Qwen3-32B，使其在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的性能匹配或超过o3-mini和DeepSeek R1（推理成本低于5%）。",
                    "inspiration_trace": "## 面临的挑战\nRLHF缺乏可解释性且易出现奖励黑客问题，而RLVR则受限于基于正确性的验证器范围，难以捕捉响应质量的细微方面。这两种主流LLM后训练范式各有优缺点，无法全面解决语言模型对齐需求。\n\n## 关键洞察\n作者认识到人类反馈与可验证奖励本质上是互补的。人类判断背后通常有明确的\"原则\"，但这些原则在传统方法中未被明确表达。单个响应评估比响应对比较更自然，二元评估比李克特量表更易校准。\n\n## 解决方案演进\n从自然语言反馈中提取可用二元方式回答的原则，设计证据引用机制确保忠实于原始反馈。引入共识原则过滤减少主观性，将二元灵活反馈构建为蕴含任务：给定提示、响应和原则，判断响应是否满足该原则。\n\n## 创新点总结\n成功结合RLHF灵活性与RLVR精确性的新范式；从自然语言反馈提取可操作二元原则的创新方法；允许用户在推理时定制原则的能力；开发了专门评估原则遵循能力的新基准。"
                },
                {
                    "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs",
                    "arxiv_id": "2509.20863",
                    "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma",
                    "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出WeFT（加权熵驱动微调方法），这是一种针对扩散语言模型(dLLMs)的新训练范式。论文的核心贡献在于解决扩散语言模型在监督微调过程中的挑战，通过基于熵的标记加权来增强模型的推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，因此应保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究扩散语言模型(dLLMs)，这是大语言模型的一种变体 - 能力方向：论文在四个推理基准(Sudoku、Countdown、GSM8K和MATH-500)上评估方法，这些基准涉及数学推理和逻辑推理能力 - 论文虽然不涉及强化学习或智能体等新兴范式，但已包含足够的核心正面指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但这是应用于语言建模而非视觉或多模态领域 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注水印、安全等模型可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合分析，这篇论文通过提出新的微调方法来增强扩散语言模型的通用推理能力，并在多个推理基准上验证了其有效性，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决扩散语言模型(dLLMs)在监督微调(SFT)过程中缺乏精确概率估计的问题。针对扩散模型中token重要性不均等的场景，我们提出了一种基于熵加权的WeFT方法，通过token预测熵分配不同权重，使高熵token获得更强训练信号。在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上，WeFT相比标准SFT实现了39%-83%的相对性能提升，且在后续RL训练中仍保持49%的性能优势。",
                    "summary_translation": "扩散模型（Diffusion models）最近在语言建模（language modeling）中展现出强大的潜力，与传统的自回归（autoregressive）方法相比，提供了更快的生成速度。然而，将监督微调（supervised fine-tuning, SFT）应用于扩散模型仍然具有挑战性，因为它们在每个去噪（denoising）步骤中缺乏精确的概率估计。虽然扩散机制（diffusion mechanism）使模型能够对整个序列进行推理（reason），但它也使生成过程变得较难预测且常常不一致。这凸显了控制引导生成方向的关键令牌（key tokens）的重要性。为了解决这个问题，我们提出了WeFT，一种用于扩散语言模型（diffusion language models）的加权SFT方法，其中令牌根据其熵（entropy）被分配不同的权重。源自扩散理论（diffusion theory）的WeFT带来了显著收益：在open-r1数据集的s1K、s1K-1.1和3k样本上训练时，在四个广泛使用的推理基准（reasoning benchmarks）（Sudoku、Countdown、GSM8K和MATH-500）上，它相对于标准SFT实现了39%、64%和83%的相对改进。代码和模型将公开发布。",
                    "inspiration_trace": "## 面临的挑战\n扩散语言模型(dLLMs)在监督微调(SFT)时存在关键问题：传统SFT假设所有token具有相同掩码率，将每个token视为同等重要，忽略了token在推理和规划中的重要性差异，导致模型难以有效学习关键推理结构。\n\n## 关键洞察\n作者发现token的预测分布熵是衡量其重要性的可靠指标：高熵token通常与推理或规划相关，对应模型生成中不确定性高的位置。优先训练这些token能引导模型为序列中关键推理部分分配更多能力。\n\n## 解决方案演进\n基于扩散理论，作者首先推导出加权SFT损失函数，确保与扩散原理一致。为解决原始熵值方差大的问题，采用熵的平方根作为稳定度量。最终形成WeFT：每个训练步骤执行两次前向传递，第一次估计token掩码率，第二次使用不同掩码概率训练，使高熵token获得更强训练信号。\n\n## 创新点总结\nWeFT创新在于将token重要性量化与扩散过程理论结合，创造理论一致的加权SFT方法。通过预测熵自动识别并优先训练关键推理token，在提升推理性能的同时保持较低计算开销。"
                },
                {
                    "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet",
                    "arxiv_id": "2509.20820",
                    "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang",
                    "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种称为\"cheat-sheet ICL\"的新方法，旨在提高大语言模型在上下文学习中的推理效率和性能。论文将多样本上下文学习的信息提炼成简洁的文本摘要，在保持或提高推理性能的同时大幅减少了计算需求。这直接关系到提升LLM的通用推理能力，特别是在处理需要多步推理的任务时。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM本身的推理机制，因此完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。此外，论文明确涉及\"Large language models\"和\"reasoning\"等正面指标，且不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的是一种通用方法论，可以应用于各种推理任务，而非针对特定领域的应用。",
                    "summary2": "本文旨在解决many-shot in-context learning计算成本高的问题。针对需要大量示例的推理任务，我们提出了一种cheat-sheet ICL方法，将many-shot示例提炼为简洁文本摘要作为推理上下文。在BBH Hard推理任务上通过准确率和token数量验证了其有效性，证明cheat-sheet ICL以更少的token实现了与many-shot ICL相当或更好的性能，且无需测试时检索操作。",
                    "summary_translation": "大型语言模型(large language models, LLMs)的最新进展使得通过多示例(many-shot)进行有效的上下文学习(in-context learning, ICL)成为可能，但代价是由于输入标记(input tokens)较长而导致的高计算需求。为解决这一问题，我们提出了速查表上下文学习(cheat-sheet ICL)，该方法将多示例上下文学习(many-shot ICL)的信息提炼成一个简洁的文本摘要(速查表)，在推理时作为上下文使用。在具有挑战性的推理任务上的实验表明，速查表上下文学习(cheat-sheet ICL)使用远少于多示例上下文学习(many-shot ICL)的标记(tokens)即可达到相当或更好的性能，并且无需测试时检索(test-time retrieval)即可与基于检索的上下文学习(retrieval-based ICL)相媲美。这些研究结果表明，速查表上下文学习(cheat-sheet ICL)是在下游任务中利用大型语言模型(LLMs)的一种实用替代方案。",
                    "inspiration_trace": "## 面临的挑战\nMany-shot ICL虽性能优越，但需处理大量输入token，计算成本高昂；现有高效方法如示例检索需每次推理时进行检索；专有模型无法通过参数更新进行微调。\n\n## 关键洞察\nLLM具有高级语言理解能力，能将学到的知识以文本形式表示；many-shot ICL中的知识可被提取并总结为显式文本格式；无需每次推理时从大量示例中推断隐藏模式，可预先提取这些模式。\n\n## 解决方案演进\n从人类\"考试小抄\"获得灵感，设计预处理步骤让LLM从大量演示中提取核心知识生成简洁cheat sheet；推理时仅使用cheat sheet和少量格式示例，通过特定提示引导LLM创建有效摘要。\n\n## 创新点总结\n将many-shot ICL知识蒸馏为可解释文本摘要而非模型参数；一次性预处理，推理时无需额外计算或检索；兼顾效率、性能和可解释性，为专有模型提供实用任务适应方法。"
                },
                {
                    "title": "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs",
                    "arxiv_id": "2509.20758",
                    "authors": "Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li",
                    "summary": "Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究如何在大语言模型进行特定领域微调时保持其通用能力，提出了一种新的训练方法Token-Adaptive Loss Reweighting (TALR)。从本质上看，论文关注的是改进LLM的基础能力，提出新的训练范式来增强模型的通用推理和问题解决能力，而不是将LLM作为工具应用到特定领域。论文通过理论和实验分析，探索了如何减轻监督微调对模型通用能力的损害，这直接关系到提升LLM的通用推理能力。虽然论文讨论了领域特定的微调，但其焦点不是特定应用领域，而是如何在领域微调时保持通用能力。论文符合\"改进LLM的基础能力、提出新的训练范式\"的标准，不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在重新审视领域特定微调对大语言模型通用能力的影响。针对领域特定SFT导致通用能力下降的问题，我们提出了使用较小学习率和Token-Adaptive Loss Reweighting (TALR)方法，并在MedCalc和ESCI数据集上通过领域性能与通用能力(IFEval、GSM8K、HumanEval)的平衡指标验证了其有效性。",
                    "summary_translation": "在特定领域数据集上进行监督微调(Supervised Fine-Tuning, SFT)是使大型语言模型(Large Language Models, LLMs)适应专门任务的常见方法，但通常被认为会降低模型的通用能力。在这项工作中，我们重新审视了这种权衡，并提供了实证和理论上的见解。首先，我们表明SFT并不总是有害的：使用较小的学习率(learning rate)可以显著减轻通用性能的下降，同时保持相当的目标领域性能。然后，我们提供了一个理论分析来解释这些现象，并进一步提出了一种新方法——令牌自适应损失重加权(Token-Adaptive Loss Reweighting, TALR)。在此基础上，并认识到仅靠较小的学习率并不能在所有情况下完全消除通用性能的下降，我们评估了一系列减少通用能力损失的策略，包括L2正则化(L2 regularization)、LoRA、模型平均(model averaging)、FLOW以及我们提出的TALR。实验结果表明，虽然没有任何方法能完全消除这种权衡，但TALR在平衡特定领域收益和通用能力方面始终优于这些基线方法。最后，我们将研究成果提炼为将LLMs适应新领域的实用指南：(i)使用较小的学习率以实现有利的权衡，(ii)当需要更强的平衡时，采用TALR作为有效策略。",
                    "inspiration_trace": "## 面临的挑战\n领域特定监督微调(SFT)被普遍认为会显著损害LLM的通用能力，这种观点限制了LLM在专业领域的应用，形成了一个两难困境。\n\n## 关键洞察\n作者发现使用较小学习率可大幅缓解通用性能下降，同时保持目标领域性能。这表明之前报道的严重退化部分源于过于激进的优化策略，而非SFT本身的固有缺陷。\n\n## 解决方案演进\n基于这一发现，作者首先通过信息理论分析解释了小学习率的作用机制。随后认识到仅靠小学习率不足以完全解决问题，进而提出TALR方法，通过自适应降低困难token的权重来减轻其对通用能力的不成比例影响。\n\n## 创新点总结\n该研究挑战了领域SFT必然损害通用能力的固有观念，揭示了学习率的关键作用，并从token级别提供了新的平衡策略，实现了领域适应与通用能力保持的更优权衡。"
                },
                {
                    "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
                    "arxiv_id": "2509.20750",
                    "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang",
                    "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型推理框架，专注于改进大语言模型的通用推理能力。从筛选标准来看： 首先，在核心判断层面，论文本质上是关于改进LLM的推理能力的，提出了一种新的训练免费推理框架，通过构建和细化子问题及其答案来增强模型的推理过程。这直接符合\"改进LLM基础能力\"和\"增强其逻辑推理能力\"的保留标准。 其次，从正面指标看，论文明确聚焦于\"reasoning\"这一核心能力方向，讨论了\"探索多样化推理路径\"和\"实现稳健可靠的推理\"，这些都是通用推理能力的关键组成部分。 关于多模态方面，虽然论文提到其框架适用于文本、图像和视频领域，但这只是表明其方法的通用性，而非主要焦点。论文的核心是推理框架本身，而不是解决多模态或视觉特定问题，因此不应被排除。 最后，C2R作为一种可以与各种现有QA模型无缝集成的通用方法，代表了提升LLM推理能力的新方法论，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决零样本问答任务中多步推理方法 indiscriminate 使用子问题-答案对可能引入噪声的问题。针对文本、图像和视频领域的QA任务，我们提出了一种基于置信度引导的精炼推理（C2R）框架，通过Generator、Refiner和Answer Selector三个组件，选择性地利用sub-QAs并基于模型自身置信度分数选择最终答案。在MMLU、MMMU和EgoSchema等五个基准测试上，通过准确率等指标验证了C2R在不同模型上均能持续提升性能。",
                    "summary_translation": "我们提出了Confidence-guided Refinement Reasoning (C2R，置信度引导的精炼推理)，这是一个新颖的免训练框架，适用于文本、图像和视频领域的问答(QA，question-answering)任务。C2R策略性地构建和精炼子问题及其答案(sub-QAs，sub-questions and answers)，为目标答案推导出更好的置信度分数。C2R首先策划一个子问题集(sub-QAs)子集来探索多样化的推理路径，然后比较生成的答案候选者的置信度分数，以选择最可靠的最终答案。由于C2R仅依赖于模型自身推导出的置信度分数，它可以与各种现有的QA模型无缝集成，在不同模型和基准测试上展现出一致的性能提升。此外，我们提供了关于利用子问题(sub-QAs)如何影响模型行为的关键但尚未充分探索的见解，具体分析了子问题的数量和质量对实现稳健可靠推理的影响。",
                    "inspiration_trace": "## 面临的挑战\n在零样本问答任务中，现有多步推理方法不加区分地使用子问题-答案对(sub-QAs)，没有验证其相关性和准确性。当子问题与主问题无关或答案不准确时，会引入噪声，反而降低最终答案质量。\n\n## 关键洞察\n作者发现子问题并不总是增强问答推理，盲目使用可能分散模型注意力。关键在于如何有效利用子问题，而非简单增加中间步骤。作者还发现\"置信度膨胀\"现象：即使使用不正确子问题，模型对答案的置信度也会增加，导致错误选择。\n\n## 解决方案演进\n基于洞察，作者提出基于置信度的筛选机制：先生成多个子问题-答案对，有选择地组织成不同子集探索多样化推理路径。每个路径产生候选答案及置信度，最后比较这些置信度（包括单步答案）选择最可靠答案。设计两个原则：基础答案置信度高时直接采用；需使用子问题时，要求改进答案置信度显著高于基础答案才采用。\n\n## 创新点总结\n创新点在于：提出无需训练的模型无关框架，可无缝集成各种问答模型；通过置信度引导筛选降低不相关子问题风险；首次系统分析子问题数量质量影响，揭示置信度膨胀现象并提供应对策略。"
                },
                {
                    "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
                    "arxiv_id": "2509.20577",
                    "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar",
                    "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是提出DS-MoE（Depth Specialised Mixture of Experts）框架，这是一种改进Transformer架构的新方法，通过深度专业化的专家混合模型来动态构建推理链。论文明确关注增强LLM的逻辑推理和多步推理能力，而不是将LLM作为工具应用于特定领域。这完全符合改进LLM基础能力和增强其通用推理能力的研究目标。 第二步：正面指标 论文包含多个正面指标： - 核心概念：虽然摘要未直接提及\"LLMs\"，但DS-MoE明显是针对大语言模型的基础架构改进 - 能力方向：明确涉及\"reasoning chains\"、\"reasoning depths\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力，特别是逻辑推理和多步推理 - 论文强调通过动态组装定制的推理链来匹配输入复杂性，这正是提升通用推理能力的核心 第三步：排除标准 论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉相关内容 - 虽然训练数据集包含多个领域，但论文本身不是针对特定应用领域的研究 - 未涉及模型可靠性方面的水印、安全性等内容 第四步：特殊和模糊情况 论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明其方法增强了模型的可解释性，从而可能提升模型的通用可靠性和推理质量，符合研究目标。 综上所述，这篇论文的核心贡献是通过改进Transformer架构来增强LLM的通用推理能力，特别是逻辑推理和多步推理能力，同时提高计算效率和可解释性，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决传统Transformer架构对所有输入使用相同处理深度导致的效率低下和推理质量受限问题。针对不同复杂度的输入，我们提出了一种Depth-Specialised Mixture-of-Experts (DS-MoE)方法，通过引入针对不同推理深度优化的专家模块和学习路由网络动态组装推理链，并在The Pile数据集上通过计算节省、推理速度和准确率等指标验证了其有效性。",
                    "summary_translation": "当代的transformer（变换器）架构对所有输入应用相同的处理深度，导致效率低下并限制了推理质量。简单的事实性查询与复杂的逻辑问题受到相同的多层计算处理，浪费资源的同时也限制了深度推理。为了克服这一问题，我们提出了通过深度专业化专家混合(DS-MoE, Depth Specialised Mixture of Experts)实现动态推理链的概念，这是一个模块化框架，将专家混合(Mixture of Experts)范式从基于宽度的计算扩展到深度专业化的计算。DS-MoE引入了针对不同推理深度优化的专家模块，包括浅层模式识别(shallow pattern recognition)、组合推理(compositional reasoning)、逻辑推理(logical inference)、记忆整合(memory integration)和元认知监督(meta-cognitive supervision)。学习到的路由网络(routing network)动态组装定制推理链，仅激活必要的专家以匹配输入复杂度。我们训练和评估DS-MoE的数据集是The Pile，这是一个800GB的语料库(corpus)，涵盖科学论文、法律文本、编程代码和网络内容等多个领域，能够对推理深度进行系统性评估。实验结果表明，与统一深度的transformer相比，DS-MoE实现了高达16%的计算节省和35%的更快推理(inference)速度，同时在复杂多步推理基准上实现了2.8%的更高准确率。此外，路由决策产生可解释的推理链，增强了透明度(transparency)和可扩展性(scalability)。这些发现确立了DS-MoE作为自适应神经架构(adaptive neural architectures)的重要进展，表明深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性(interpretability)。",
                    "inspiration_trace": "## 面临的挑战\n传统Transformer架构对所有输入采用相同处理深度，无论简单事实查询还是复杂逻辑问题都经过相同多层计算，造成计算资源浪费，同时限制了深度推理能力，无法学习深度最优的推理策略。\n\n## 关键洞察\n作者从人类认知中获得灵感，认识到人类会根据问题复杂度自然调整推理深度。不同任务需要不同深度的处理，从浅层模式识别到深层逻辑推理，再到元认知监督，将MoE从宽度扩展到深度专门化可实现更高效的资源分配。\n\n## 解决方案演进\n首先设计针对不同推理深度的专家模块（浅层模式识别、组合推理、逻辑推理等），然后开发学习路由网络动态评估输入复杂度，最后构建动态推理链，仅激活必要专家匹配输入复杂度，实现自适应深度处理。\n\n## 创新点总结\n首次将MoE从宽度扩展到深度专门化，实现计算资源与任务复杂度的自适应匹配；通过动态推理链提供可解释性；同时提升计算效率、推理质量和可解释性，为大型语言模型提供更接近人类认知的架构设计。"
                },
                {
                    "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
                    "arxiv_id": "2509.20502",
                    "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang",
                    "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出MARS（Multi-Agent Review System），一种基于角色的多智能体协作框架，旨在提高LLM的推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的多智能体协作范式，而不是将LLM作为工具应用于特定领域。因此，这篇论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：直接关注\"LLM reasoning\"，旨在提高模型的推理质量 - 新兴范式：提出了多智能体系统(MARS)，这是一种基于LLM的智能体协作框架 论文符合3个正面指标，表明其与研究主题高度相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不关注模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况 论文提出的MARS框架是一种通用的智能体协作方法，用于增强LLM的通用推理能力，而不是将智能体应用于特定领域。根据筛选标准，这种通用的智能体协作框架应该保留。 综合分析，这篇论文的核心贡献是提出了一种更高效的多智能体协作框架(MARS)来增强LLM的通用推理能力，与研究目标\"提高大语言模型的通用推理能力\"直接一致。论文通过改进多智能体协作方式，在保持推理质量的同时显著提高了效率，这正属于对LLM基础推理能力的改进研究。",
                    "summary2": "本文旨在 [解决多智能体协作中计算开销大的问题]。针对 [多智能体辩论(MAD)框架中的高资源消耗]，我们提出了一种 [基于角色的多智能体评审系统(MARS)]，并在 [多个推理基准测试(GPQA、MMLU、GSM8K)] 上通过 [准确率、token消耗和推理时间] 验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在自然语言理解方面取得了令人瞩目的成果，然而当作为单一代理（single agent）运行时，它们的推理能力仍然有限。多智能体辩论（Multi-Agent Debate, MAD）被提出以解决这一限制，它通过圆桌辩论方式使多个模型能够进行协作推理。尽管有效，但由于涉及的代理数量和所需的频繁通信，MAD引入了大量的计算开销。在本文中，我们提出了MARS（Multi-Agent Review System，多智能体评审系统），这是一个受评审过程启发的基于角色的协作框架。在MARS中，作者代理（author agent）生成初始解决方案，评审者代理（reviewer agents）独立提供决策和评论，元评审者（meta-reviewer）整合反馈以做出最终决策并指导进一步修订。这种设计在避免昂贵的评审者之间交互的同时提高了推理质量，从而控制了token消耗和推理时间。我们在多个基准测试（benchmarks）中将MARS与MAD及其他最先进的推理策略进行了比较。使用不同LLMs的广泛实验表明，MARS在匹配MAD准确率的同时，将token使用量和推理时间减少了约50%。代码可在https://github.com/xwang97/MARS获取。",
                    "inspiration_trace": "## 面临的挑战\n单个LLM在复杂推理任务上能力有限，而多智能体辩论(MAD)虽能提升推理质量，却因智能体间频繁通信导致计算开销巨大，token消耗和推理时间随智能体数量急剧增加，限制了实际应用。\n\n## 关键洞察\n作者从评审过程和验证器组件中获得启发，认识到层次化评估结构比自由讨论更高效。关键洞察是：有效的多智能体协作不依赖于所有智能体间的直接通信，而是可以通过结构化的角色分工和反馈机制实现，类似于ResNet中的残差学习原理。\n\n## 解决方案演进\n基于这一洞察，作者设计了角色分离的框架：作者生成初始方案，多个评论员独立评估，元评论员整合反馈并决策。这种\"提出-审查-反馈-更新\"架构避免了智能体间昂贵的直接通信，仅通过元评论员集中传递反馈，大幅降低了资源消耗。\n\n## 创新点总结\n将学术评审的层次结构引入多智能体协作，创造了新的协作范式；通过角色分离和结构化通信，在保持推理质量的同时将资源消耗减少约50%；将残差学习原理扩展到系统层面，专注于错误检测与修正，使多智能体协作更加实用高效。"
                },
                {
                    "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation",
                    "arxiv_id": "2509.20377",
                    "authors": "Tomoaki Isoda",
                    "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心是关于改进大语言模型本身的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是提出SKILL-RAG方法，这是一种新的训练范式，通过强化学习框架来增强模型的自我知识认知能力。该方法不是将LLM作为工具应用于特定领域，而是致力于提升LLM在知识处理和推理方面的基础能力，特别是通过识别模型\"知道\"和\"不知道\"的内容来优化其推理过程，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的标准。 从第二步正面指标看，论文包含多个相关主题： - 核心概念：明确研究LLMs（Llama2-7B和Qwen3-8B） - 能力方向：涉及推理能力，特别是在知识密集型任务中的问题解决 - 训练方法：使用强化学习（reinforcement learning-based training framework）来训练模型 - 新兴范式：RAG本身可视为一种工具使用范式，论文改进了这一通用框架 从第三步排除标准看，论文不涉及任何应排除的领域，没有专注于多模态、特定应用领域或模型基础设施等。 从第四步特殊和模糊情况看，虽然论文涉及减少幻觉的问题，但它是通过提出新方法（利用模型自我知识）来提升模型的内在推理质量，而不是应用层面的讨论，因此应该保留。 综上所述，SKILL-RAG论文的核心贡献是提出了一种增强LLM自我知识认知和推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决RAG系统因检索无关内容导致LLM产生幻觉的问题。针对检索增强生成场景，我们提出了一种SKILL-RAG方法，利用强化学习激发LLM的自我知识能力，并在句子级别过滤检索内容。在TriviaQA、SelfAware、NQ和TruthfulQA数据集上通过准确率等指标验证了其有效性，同时减少了输入文档数量，提高了生成质量和效率。",
                    "summary_translation": "检索增强生成(RAG)近年来显著提高了大型语言模型(LLMs)在知识密集型任务(knowledge-intensive tasks)上的性能。然而，由于检索系统可能返回不相关的内容，将这些信息整合到模型中常常会导致幻觉(hallucinations)问题。因此，识别并过滤掉无用的检索内容是提高RAG性能的关键挑战。为了更好地将模型的内部知识与检索获得的外部知识相结合，理解模型\"知道\"和\"不知道\"什么（这也被称为\"自我知识\"(self-knowledge)）至关重要。\n\n基于这一见解，我们提出了SKILL-RAG（基于自我知识诱导的学习与过滤RAG方法(Self-Knowledge Induced Learning and Filtering for RAG)），这是一种利用模型自我知识来确定哪些检索文档有益于回答给定查询的新方法。我们设计了一个基于强化学习(reinforcement learning)的训练框架，以明确地激发模型的自我知识，并采用句子级粒度(sentence-level granularity)来过滤不相关内容，同时保留有用知识。\n\n我们在多个问答基准(question answering benchmarks)上使用Llama2-7B和Qwen3-8B模型对SKILL-RAG进行了评估。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，验证了自我知识在指导高质量检索选择中的重要性。",
                    "inspiration_trace": "## 面临的挑战\nRAG系统虽能提升LLMs在知识密集型任务上的表现，但检索系统常返回不相关内容，导致模型产生幻觉。现有方法或采用粗粒度二元决策，或依赖置信度指标，缺乏细粒度过滤能力，无法有效建模模型对自身知识状态的认知。\n\n## 关键洞察\n作者认识到模型必须具备识别自身知识边界的能力——\"自我知识\"(self-knowledge)，即知道自己知道什么和不知道什么。有效整合内部知识与外部检索信息的关键在于理解并利用这种自我知识状态。\n\n## 解决方案演进\n基于这一洞察，作者设计强化学习框架显式引出模型的自我知识，采用句子级别细粒度过滤。首先构建自我知识数据集，提出基于熵的RLHF方法训练模型表达知识状态；然后设计基于点互信息(PMI)的过滤机制，只保留能提高模型回答置信度的检索内容。\n\n## 创新点总结\n创新之处在于首次将\"自我知识\"概念引入RAG过滤过程，使模型基于自身知识边界判断外部信息价值；通过SKILL框架使模型准确表达知识状态；设计PMI细粒度过滤机制，提升RAG系统效率和准确性。"
                },
                {
                    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
                    "arxiv_id": "2509.21124",
                    "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai",
                    "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力，这直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标： 1. 核心概念：论文明确研究基础模型(Foundation Model)的推理能力 2. 能力方向：聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning) 3. 训练方法：涉及强化学习(RL)作为关键方法，并讨论如何提高下游RL性能 4. 新兴范式：关注思维链(CoT)的优化，这是提高LLM推理能力的重要范式 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 虽然在数学推理上评估，但这是作为通用推理能力的测试案例，而非针对特定应用领域 3. 不涉及模型可靠性方面的水印、安全等问题 论文的核心贡献是首次定义了基础模型的\"推理潜力\"，并提出了一种通过选择高价值思维链数据来扩展这种潜力的方法。这种方法通过抽象原子推理模式，构建核心参考集，并使用双粒度算法选择高价值的CoT数据，从而训练模型更有效地掌握推理能力。这些贡献直接针对提升LLM的通用推理能力，完全符合研究目标。",
                    "summary2": "本文旨在解决基础模型数学推理能力受限的问题。针对思维链(CoT)数据无差别使用的现状，我们提出了一种CoTP框架，通过抽象原子推理模式并构建核心参考集，利用双粒度算法选择高价值推理数据。在85A6B MoE模型上，仅用10B-token的CoTP数据就在AIME 2024和2025上提高了9.58%的性能，并将下游RL性能上限提升7.81%。",
                    "summary_translation": "在复杂数学推理领域，大型推理模型（large reasoning models）的最新进展主要由强化学习（reinforcement learning, RL）所驱动。研究表明，在中间训练（mid-training）阶段融入长思维链（chain-of-thought, CoT）数据能显著提高推理深度。然而，当前方法通常不加区分地利用CoT数据，未能解决一个关键问题：哪些数据类型能最有效地提升模型的推理能力。\n\n在本文中，我们首次将基础模型（foundation model）的推理潜力（reasoning potential）定义为正确回答问题所需独立尝试次数的倒数，该定义与最终模型性能呈现强相关性。随后，我们提出利用富含高价值推理模式（high-value reasoning patterns）的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式（atomic reasoning patterns），并利用这些模式构建一个富含有价值推理模式的核心参考集（core reference set）。此外，我们提出一种双粒度算法（dual-granularity algorithm），该算法涉及推理模式链（chains of reasoning patterns）和标记熵（token entropy），能高效地从数据池中筛选出与核心集一致的高价值CoT数据（CoTP），从而训练模型有效掌握推理能力。\n\n仅需100亿token（10B-token）的CoTP数据，就能使85A6B专家混合（Mixture-of-Experts, MoE）模型在具有挑战性的AIME 2024和2025测试中提升9.58%的性能，并将下游强化学习（downstream RL）性能的上限提高7.81%。",
                    "inspiration_trace": "## 面临的挑战\n现有大推理模型依赖强化学习提升数学推理能力，但基础模型的推理能力直接限制了RL性能上限。当前方法在中间训练阶段不加区分地使用所有CoT数据，缺乏对哪些数据类型最有效增强模型推理能力的深入研究。\n\n## 关键洞察\n作者首次理论定义基础模型推理潜力为正确回答问题所需独立尝试次数的倒数，发现这与最终模型性能强相关。认识到扩展推理潜力等同于减少正确回答问题所需的平均推理尝试次数，并从CoT序列中抽象出具有共性和归纳能力的原子推理模式。\n\n## 解决方案演进\n从理论定义出发，构建富含高价值推理模式的核心参考集来近似理想数据。开发双粒度算法，同时考虑推理模式链和token熵，使用加权DTW选择与核心集一致的长CoT数据。最终构建CoTP数据集，通过实验验证少量精选数据能显著扩展模型推理潜力。\n\n## 创新点总结\n首次理论定义模型推理潜力并建立其与性能关联；提出原子推理模式抽象方法；开发双粒度数据选择算法；证明少量精选高价值推理数据能显著提升模型性能并提高RL上限。"
                },
                {
                    "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
                    "arxiv_id": "2509.21070",
                    "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu",
                    "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。核心判断上，论文本质是提升大语言模型的数学推理能力，属于增强LLM通用推理能力的范畴。论文提出ScaleDiff流程，通过高效生成困难数学问题来训练模型，从而提升LLM的数学推理能力，这是对LLM基础能力的改进，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确涉及Large Reasoning Models (LRMs)这一核心概念，并专注于Advanced Mathematical Reasoning这一推理能力方向。论文通过生成困难问题数据集并微调模型，实质上提出了一种新的训练范式来增强LLM的推理能力。 在排除标准方面，虽然论文专注于数学推理，但数学推理通常被视为评估和提升LLM通用推理能力的重要方面，而非特定应用领域。论文也不涉及多模态、视觉内容或特定应用领域如医疗、化学等。 论文的核心贡献是通过增加困难问题的数量来提升LLM的数学推理能力，并观察到\"随着困难问题数量增加，模型在困难基准测试上的性能呈现明显的扩展现象\"，这直接与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"相符。因此，这篇论文应被保留。",
                    "summary2": "本文旨在解决大规模困难数学问题创建成本高的问题。针对数学推理模型的训练数据，我们提出了一种ScaleDiff pipeline，通过AdaptThink高效识别困难问题，训练专门的DiffGen-8B生成器大规模产生新困难问题，并应用规则和模型过滤蒸馏高质量解决方案。在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500等数学推理基准测试上，我们的方法实现了65.9%的平均准确率，比原始数据集提高了11.3%，并展现出随困难问题数量增加而性能提升的扩展现象。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在复杂问题解决方面展现了令人印象深刻的能力，通常受益于训练于能激发复杂推理的困难数学问题。近来的研究探索了通过提示（prompting）专有模型或大规模开源模型来自动合成数学问题的方法，这些模型基于种子数据（seed data）或固有的数学概念。然而，由于这些方法的高计算/API成本、提示的复杂性以及生成问题的难度水平有限，扩大这些方法的规模仍然具有挑战性。为了克服这些限制，我们提出了ScaleDiff，一个简单而有效的流程（pipeline），旨在扩大困难问题的创建规模。我们使用自适应思维模型（adaptive thinking model），仅通过单次前向传递（forward pass）就能高效地从现有数据集中识别困难问题，该模型能够感知问题难度并在\"Thinking\"和\"NoThinking\"模式之间自动切换。然后，我们在这个过滤后的困难数据上训练了一个专门的困难问题生成器（DiffGen-8B），它能够大规模生成新的困难问题，消除了对复杂的、每个实例的提示及其相关的高API成本的需求。在ScaleDiff-Math数据集上微调（fine-tuning）Qwen2.5-Math-7B-Instruct，相比原始数据集带来了11.3%的显著性能提升，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超越了像OpenThinker3这样的近期强大型推理模型。值得注意的是，这一性能是使用成本效益高的Qwen3-8B模型作为教师模型（teacher model）实现的，这表明我们的流程能够有效转移高级推理能力，而无需依赖更大、更昂贵的教师模型。此外，我们观察到随着困难问题数量的增加，模型在困难基准测试（benchmarks）上的性能呈现出明显的扩展现象（scaling phenomenon）。代码：https://github.com/QizhiPei/ScaleDiff。",
                    "inspiration_trace": "## 面临的挑战\n大型推理模型需要训练在困难数学问题上才能展现复杂推理能力，但创建这类问题成本高昂，主要依赖人类专家。现有自动合成方法计算成本高、提示设计复杂，且生成问题难度有限，难以有效扩展。\n\n## 关键洞察\n困难问题比简单问题更能有效提升模型推理能力；问题难度可通过模型自适应思考机制高效评估；训练专门的困难问题生成器可避免复杂的逐例提示设计；小型教师模型也能有效进行知识蒸馏。\n\n## 解决方案演进\n首先利用AdaptThink模型高效识别现有数据集中的困难问题；然后基于这些困难问题训练专门的生成器DiffGen-8B；使用该生成器大规模创建新困难问题；再用小型教师模型蒸馏解决方案；最后通过规则和模型过滤构建高质量数据集进行模型微调。\n\n## 创新点总结\n提出了简单有效的困难问题扩展管道，利用自适应思考机制实现高效问题难度识别，通过专门生成器实现大规模困难问题自动化生成，证明了小型教师模型也能有效传递高级推理能力，并观察到模型性能随困难问题数量增加而明显扩展的现象。"
                },
                {
                    "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?",
                    "arxiv_id": "2509.21016",
                    "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song",
                    "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了DELTA-Code基准测试，用于探索大语言模型(LLMs)是否能通过强化学习(RL)获取和迁移全新的推理策略。论文研究的是LLMs如何通过强化学习解决预训练时无法解决的问题，以及这些学到的技能如何迁移到分布外测试集。这完全符合研究\"大语言模型通用推理能力\"的目标，因为：(1)论文本质是改进LLM的基础推理能力，特别是算法推理能力，而不是将LLM作为工具应用到特定领域；(2)论文使用了强化学习这一训练范式来增强模型能力，探索了staged warm-up、experience replay、curriculum training等关键训练成分；(3)论文聚焦于推理能力的获取和迁移，这是通用推理能力的核心方面。论文符合多个正面指标，包括核心概念(LLMs)、能力方向(reasoning)和训练方法(RL)，同时不符合任何排除标准。因此，这篇论文应该被纳入研究范围。",
                    "summary2": "本文旨在探究LLMs能否通过RL获取真正新颖的编程算法推理策略。针对预训练模型在编程任务上表现不佳的场景，我们提出了DELTA基准，一种受控的合成编码问题族，并在多个问题族上通过pass@K和full-pass rate验证了RL能够解锁新策略并实现泛化。实验揭示了grokking现象，证明适当的奖励设计可使模型从长期探索阶段突然跃升至接近完美准确率。",
                    "summary_translation": "# 中文翻译\n\nLLMs（大语言模型）是否能够获取或泛化真正新颖的推理策略，而不仅仅是在预训练或后训练过程中编码在其参数中的已强化技能，这仍然是一个悬而未决的问题。为尝试回答这一争议，我们提出了DELTA-Code——算法编码中的可学习性与可迁移性的分布评估（Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding），这是一个受控的合成编码问题族基准，旨在探究两个基本方面：可学习性（learnability）——LLMs能否通过强化学习（Reinforcement Learning, RL）解决预训练模型在足够多次尝试下仍然失败的问题族（pass@K=0）？——以及可迁移性（transferrability）——如果实现了可学习性，这些技能能否系统地迁移到分布外（Out-of-Distribution, OOD）测试集？\n\n与先前的公共编码数据集不同，DELTA通过模板化问题生成器来隔离推理技能，并引入了完全分布外（OOD）的问题族，这些问题族需要新颖的策略而非工具调用或记忆模式。我们的实验揭示了一个显著的理解（grokking）相变：在经过一段接近零奖励的长时间后，经过强化学习训练的模型突然提升至接近完美的准确率。为了在先前无法解决的问题族上实现可学习性，我们探索了关键训练要素，如密集奖励的分阶段预热（staged warm-up）、经验回放（experience replay）、课程训练（curriculum training）和循环验证（verification-in-the-loop）。\n\n除了可学习性，我们还使用DELTA来评估探索性、组合性和变革性轴上的可迁移性或泛化能力，以及跨族迁移。结果显示，在问题族内部和重组技能方面有显著提升，但在变革性情况下仍存在持续弱点。因此，DELTA提供了一个清晰的测试平台，用于探究强化学习驱动推理的极限，并理解模型如何能够超越现有先验知识来获取新的算法技能。",
                    "inspiration_trace": "## 面临的挑战\n作者面临的核心问题是：LLMs通过强化学习仅能锐化已有技能，还是能真正获取全新推理策略？现有数据集混杂难以区分，且对pass@K=0任务（模型多次尝试仍失败），传统RL因缺乏正向信号而无法学习。\n\n## 关键洞察\n作者认识到需要受控基准测试系统探究LLMs的可学习性与可迁移性。编程问题是理想领域，因其可通过测试用例提供细粒度反馈作为密集奖励。关键洞察是：通过分阶段奖励设计（从密集到二元），可能触发\"grokking\"现象，表明RL能突破基础模型限制。\n\n## 解决方案演进\n作者先设计DELTA受控基准，包含合成问题家族。针对pass@K=0任务，提出分阶段训练：先用测试用例通过率作为密集奖励预热，帮助模型探索；再切换到二元完全通过奖励，精确锁定解决方案。这导致grokking现象出现——长时间探索后模型突然达到近乎完美准确率。\n\n## 创新点总结\n创新在于：创建能区分能力锐化与新知识获取的受控基准；发现并验证LLMs中的RL grokking现象；提出分阶段训练策略解决pass@K=0任务学习难题；系统评估学习策略在不同泛化轴上的迁移能力，揭示RL驱动推理的边界。"
                },
                {
                    "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
                    "arxiv_id": "2509.21012",
                    "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue",
                    "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
                    "category": "cs.CL",
                    "filter_reason": "我按照筛选标准对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是研究In-context Learning (ICL)的内在工作机制，特别是从\"信息移除\"角度解析ICL如何帮助语言模型专注于目标任务。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制，属于\"改进LLM的基础能力\"的研究方向，符合核心判断标准。 第二步：正面指标——论文符合\"核心概念\"指标，因为它明确研究现代语言模型(LMs)的ICL机制。虽然论文没有直接讨论reasoning、planning等能力方向，也没有涉及reinforcement learning等训练方法或llm-based agents等新兴范式，但ICL本身是LLM的一种重要通用能力，与推理能力密切相关。 第三步：排除标准——论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 第四步：特殊和模糊情况——论文在可解释性方面有显著相关性，因为它揭示了ICL的\"面向任务的信息移除\"机制，这种对模型内在机制的深入理解有助于提升模型的通用可靠性和推理质量。 综合判断：这篇论文的核心贡献是揭示了ICL的工作机制，特别是\"面向任务的信息移除\"过程，这属于研究LLM基础能力的重要工作。理解ICL如何帮助模型从纠缠的信息中选择性移除冗余信息，对于提升LLM的通用推理能力具有重要意义。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决现有Induction Heads机制无法全面解释ICL行为的问题，特别是在unseen label scenario下的表现。针对多种LLMs和分类任务，我们提出了一种task-oriented information removal机制，认为demonstrations通过移除查询中与任务无关的信息来驱动LM输出。我们在多个数据集上通过ablation experiments验证了其有效性，并识别了负责此操作的denoising heads，证明了它们在未见标签场景下对维持分类准确性的关键作用。",
                    "summary_translation": "In-context Learning (ICL)（情境学习）是一种基于现代语言模型（Language Models, LMs）的新兴小样本学习（few-shot learning）范式，但其内在机制仍不明确。在本文中，我们通过信息移除（information removal）这一新颖视角来探究其机制。具体而言，我们证明在零样本（zero-shot）场景下，语言模型将查询编码为隐藏状态（hidden states）中的非选择性表示（non-selective representations），这些表示包含所有可能任务的信息，导致模型无法专注于目标任务而产生任意输出，最终准确率接近于零。同时，我们发现通过低秩滤波器（low-rank filter）选择性地从隐藏状态中移除特定信息，能有效引导语言模型专注于目标任务。基于这些发现，通过精心设计的指标测量隐藏状态，我们观察到小样本ICL有效模拟了这种面向任务的信息移除过程，选择性地从纠缠的非选择性表示中移除冗余信息，并根据示例（demonstrations）改进输出，这构成了ICL的一个关键机制。此外，我们识别出引发移除操作的关键注意力头（attention heads），称之为去噪头（Denoising Heads），这使得我们能够进行消融实验（ablation experiments），在推理过程中阻断信息移除操作。实验结果表明，ICL准确率显著下降，特别是当小样本示例中缺少正确标签时，这既证实了信息移除机制的关键作用，也证实了去噪头的重要性。",
                    "inspiration_trace": "## 面临的挑战\n现有ICL机制解释（特别是基于Induction Heads的框架）无法完全解释ICL的所有现象，尤其是在\"未见标签场景\"下的表现。Induction Heads认为ICL通过复制上下文中出现过的标签工作，但当正确标签未在演示中出现时，模型仍有一定表现，这与该框架预测相矛盾。\n\n## 关键洞察\n作者提出ICL本质可能是\"任务导向的信息移除\"：演示的作用是从查询的隐藏状态中移除与任务无关的信息，使模型能在指定任务上产生输出。零样本隐藏状态包含任务相关信息但也包含冗余信息，这些冗余信息会干扰输出。\n\n## 解决方案演进\n从洞察出发，作者先验证零样本隐藏状态中存在低秩子空间(TVS)，投影到该子空间可移除无关信息提高准确性；然后设计几何度量证明少样本ICL中模型自发执行这种信息移除；进而识别执行此操作的特定注意力头(denoising heads)，通过消融实验验证其在未见标签场景的关键作用。\n\n## 创新点总结\n从\"信息添加\"到\"信息移除\"的视角转换，为理解ICL提供全新解释框架；提出\"任务导向的信息移除\"作为ICL核心机制，补充Induction Heads框架不足；建立从现象观察到机制验证的完整研究路径。"
                },
                {
                    "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
                    "arxiv_id": "2509.21054",
                    "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu",
                    "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是研究大型语言模型(LLMs)和大型推理模型(LRMs)的推理过程如何影响其在多智能体系统中的说服动态。论文提出了\"说服二元性\"(Persuasion Duality)的概念，揭示了模型的底层认知过程，特别是其显式推理能力，如何决定其在多智能体交互中的说服行为。 根据筛选标准，这篇论文符合研究目标，原因如下： 1. 核心判断：论文的本质是研究LLM的基础能力——推理能力，而不是将LLM作为工具应用到特定领域。论文探索的是模型的底层认知过程和推理架构如何影响其外部行为，这属于对LLM通用能力的深入研究。 2. 正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs)和Large Reasoning Models (LRMs) - 能力方向：核心关注reasoning能力，特别是模型的\"thinking process\"和\"explicit reasoning\" - 新兴范式：研究Multi-Agent Systems (MAS)中LLMs的交互和协作 3. 排除标准：论文不主要聚焦于任何排除领域。虽然研究\"说服\"这一社会心理学概念，但论文不是将LLM应用到社会学领域，而是研究LLM本身的推理能力如何影响其在多智能体系统中的行为。 4. 特殊情况处理：论文研究的是通用多智能体系统框架中的推理问题，属于\"提出一种通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况，应该保留。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，它通过研究模型推理过程与说服动态的关系，增进了我们对LLM推理能力的理解，有助于未来设计具有更强推理能力的LLM和多智能体系统。",
                    "summary2": "本文旨在解决多智能体系统中模型说服动态的关键影响因素问题。针对LLMs和LRMs在多智能体系统中的交互场景，我们提出了一种基于模型认知过程的说服机制分析方法，并在客观和主观数据集上通过Persuaded-Rate、Remain-Rate等指标验证了\"说服二元性\"理论，即显式推理过程同时增强模型说服力与抵抗说服能力，揭示了思考内容共享对说服效果的重要影响。",
                    "summary_translation": "最近多智能体系统（Multi-Agent Systems, MAS）的快速普及，其中大型语言模型（Large Language Models, LLMs）和大型推理模型（Large Reasoning Models, LRMs）通常协作解决复杂问题，这 necessitates 对支配其交互的说服动态的深入理解。本文挑战了当前的主流假设，即说服效果主要取决于模型规模。我们提出，这些动态实际上是由模型的底层认知过程决定的，特别是其显式推理能力。通过一系列多智能体说服实验，我们发现了一个我们称之为\"说服二元性\"（Persuasion Duality）的基本权衡。我们的研究结果表明，大型推理模型（LRMs）中的推理过程表现出显著更强的抗说服性，更稳健地保持其初始信念。相反，通过共享\"思考内容\"使推理过程透明化，则显著提高了其说服他人的能力。我们进一步考虑了更复杂的传播说服情境，并揭示了多智能体网络间多跳说服中影响传播和衰减的复杂动态。本研究提供了将模型内部处理架构与其外部说服行为联系起来的系统性证据，为高级模型的易受影响性提供了新的解释，并强调了未来多智能体系统（MAS）的安全性、稳健性和设计的关键影响。",
                    "inspiration_trace": "## 面临的挑战\n传统观点认为多智能体系统中模型的说服效果主要取决于规模，但作者发现模型规模与说服效果存在收益递减关系，单纯增大模型无法持续提升说服能力，这促使研究者寻找影响说服效果的本质因素。\n\n## 关键洞察\n作者洞察到说服动态主要取决于模型的底层认知过程而非规模，区分了基于隐式模式识别的LLMs和采用显式推理的LRMs，提出\"说服二元性\"概念：使论证更透明的机制也增强了对错误论证的抵抗力。\n\n## 解决方案演进\n从这一洞察出发，作者通过多智能体说服实验验证假设，发现LRMs的推理过程既增强抵抗说服能力，又通过共享思维内容显著提高说服他人的能力，进而分析影响因素并提出基于提示的缓解机制。\n\n## 创新点总结\n挑战了规模中心范式，提出认知过程中心的新视角；发现并形式化了\"说服二元性\"核心权衡关系；揭示了思维过程在多智能体说服中的双重作用，为设计更安全稳健的多智能体系统提供了新思路。"
                },
                {
                    "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
                    "arxiv_id": "2509.20882",
                    "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang",
                    "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是对大语言模型的上下文学习(ICL)能力进行理论解释，特别是研究基于概念的上下文学习(CB-ICL)方法。上下文学习是大语言模型的一种重要通用推理能力，它允许模型从少量示例中学习并应用到新问题上，这本质上是一种推理和问题解决过程。论文提出了理论分析，解释了CB-ICL在少量示例情况下表现良好的原因，量化了LLMs可以利用的知识，并提出了相似性度量方法，这些都有助于深入理解LLM的推理机制。虽然论文没有涉及强化学习、智能体协作等新兴范式，但它聚焦于LLM的基础能力研究，特别是对通用推理能力的理论解释，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解释上下文学习(ICL)机制的理论基础。针对仅有少量示例的ICL任务，我们提出了一种基于概念的上下文学习(CB-ICL)方法，通过预训练LLM提取语义嵌入并学习概念向量来预测查询标签。在MMLU、GPQA等基准测试上通过准确率等指标验证了其有效性，并提出了提示示例与查询输入间的相似性度量，为模型预训练和提示工程提供了理论指导。",
                    "summary_translation": "上下文学习 (In-Context Learning, ICL) 已成为自然语言处理和大型语言模型 (Large Language Model, LLM) 应用中的一个重要新范式。然而，对 ICL 机制的理论理解仍然有限。本文旨在通过研究一种特定的 ICL 方法——基于概念的 ICL (concept-based ICL, CB-ICL) 来探讨这一问题。具体而言，我们提出了将 CB-ICL 应用于 ICL 任务的理论分析，解释了为什么以及在什么情况下 CB-ICL 能够在仅有少量示例 (demonstrations) 的提示 (prompts) 中有效预测查询标签 (query labels)。此外，所提出的理论量化了 LLM 可用于提示任务的知识，并提出了提示示例与查询输入之间的相似性度量，为 ICL 中的模型预训练 (pre-training) 和提示工程 (prompt engineering) 提供了重要的见解和指导。此外，基于所提出的理论，我们还探讨了提示示例大小和 LLM 嵌入 (embeddings) 维度对 ICL 的影响。最后，我们进行了多项真实数据实验，以验证 CB-ICL 及其相应理论的实用性。",
                    "inspiration_trace": "## 面临的挑战\n上下文学习(ICL)已成为大语言模型的重要范式，但其理论机制理解有限，特别是为何ICL能在仅有少量示例时表现良好，以及如何量化LLM利用的知识，这些问题缺乏理论解释。\n\n## 关键洞察\n作者将ICL问题转化为\"基于概念的上下文学习\"(CB-ICL)框架，核心洞察是：将LLM语义知识视为子空间投影，提示中的概念表示为向量α，ICL性能取决于两个关键因素——LLM嵌入是否完整捕获提示语义概念，以及输入文本与标签间相关性强度。\n\n## 解决方案演进\n从洞察到方案的演进：首先构建理论模型，将真实分布表示为LLM嵌入函数的线性组合加残差；然后设计提示概念提取器估计概念向量；接着建立理论分析框架，推导风险上界并连接标签预测错误概率；最后提出相似性度量量化提示示例与查询输入间关系。\n\n## 创新点总结\n创新点在于首次建立CB-ICL理论框架解释ICL机制，提出可量化相似性度量为提示工程提供理论指导，揭示嵌入维度等因素对ICL性能影响，并连接理论分析与实际应用。"
                },
                {
                    "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
                    "arxiv_id": "2509.20712",
                    "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou",
                    "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种新的强化学习算法CE-GPPO，用于改进大语言模型的基础推理能力。论文明确指出其目标是优化LLM处理\"复杂推理任务\"的能力，并通过改进PPO算法的梯度处理机制来提升模型性能，这直接属于改进LLM基础能力和通用推理能力的范畴。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确研究\"large language models (LLMs)\"；能力方向上聚焦于\"complex reasoning tasks\"和\"mathematical reasoning\"；训练方法上提出了新的强化学习优化算法(CE-GPPO)。这些都是高度相关的指标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，没有将LLM应用于特定领域（数学推理仅作为评估通用推理能力的基准），也没有涉及模型基础设施或应用层面的可靠性问题。 论文的核心贡献是提出了一种新的强化学习训练范式，通过改进梯度处理机制来优化LLM的推理能力，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决强化学习优化大型语言模型中的策略熵管理问题。针对PPO及其变体因裁剪机制丢弃低概率token梯度信号的问题，我们提出了一种CE-GPPO算法，通过重新引入被裁剪token的梯度并控制其幅度来平衡探索与利用。在多个数学推理基准上，通过准确率等指标验证了该方法在不同模型规模上的有效性。",
                    "summary_translation": "强化学习 (Reinforcement learning, RL) 已成为一种强大的范式，用于优化大语言模型 (large language models, LLMs) 以处理复杂推理任务。这一过程中的核心挑战在于管理策略熵 (policy entropy)，它反映了训练过程中探索 (exploration) 与利用 (exploitation) 之间的平衡。现有方法，如近端策略优化 (proximal policy optimization, PPO) 及其变体，由于裁剪机制 (clipping mechanism) 而丢弃了来自低概率词元 (tokens) 的有价值梯度信号。我们系统地分析了熵动态变化 (entropy dynamics)，并揭示这些被裁剪的词元在调节熵演化过程 (entropy evolution) 中扮演着关键但被忽视的角色。我们提出了基于梯度保留策略优化的熵控制算法 (Controlling Entropy via Gradient-Preserving Policy Optimization, CE-GPPO)，这是一种新颖的算法，它以一种温和且有界的方式重新引入原始 PPO 中被裁剪词元的梯度。通过控制来自裁剪区间外词元的梯度大小，CE-GPPO 能够实现探索-利用权衡 (exploration-exploitation trade-off)。我们提供了理论依据和实证证据，表明 CE-GPPO 有效缓解了熵不稳定性 (entropy instability)。在数学推理基准 (mathematical reasoning benchmarks) 上的大量实验表明，CE-GPPO 在不同模型规模上始终优于强基线方法。",
                    "inspiration_trace": "## 面临的挑战\n强化学习优化大型语言模型时，管理策略熵(探索与利用的平衡)是核心挑战。现有PPO等方法因裁剪机制丢弃低概率token的梯度信号，导致熵不稳定，表现为熵崩溃或熵爆炸。\n\n## 关键洞察\n作者通过分析熵动态，发现被裁剪token在调节熵演化中扮演关键角色。他们将token分为四类，特别揭示PA&LP(促进探索)和NA&LP(促进利用)两类被裁剪低概率token的重要性，并建立熵变化与log概率和优势函数协方差的理论联系。\n\n## 解决方案演进\n基于此洞察，作者提出CE-GPPO算法，通过stop-gradient操作解耦前向和后向传递，重新引入被裁剪token的梯度。设计参数β1和β2分别控制左右裁剪边界外梯度的缩放，实现对熵动态的精细调节，平衡探索与利用。\n\n## 创新点总结\n创新在于首次揭示被裁剪token对熵动态的关键作用，提出梯度保留裁剪机制，通过参数调节实现训练过程中探索-利用的动态平衡，既提高性能又保持稳定性。"
                },
                {
                    "title": "StyleBench: Evaluating thinking styles in Large Language Models",
                    "arxiv_id": "2509.20868",
                    "authors": "Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei",
                    "summary": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了StyleBench基准测试，用于系统评估不同推理风格（如CoT、ToT、AoT等）对大语言模型性能的影响。论文直接关注LLM的通用推理能力，研究不同思考风格如何影响模型在各种推理任务上的表现。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文没有将LLM作为工具应用到特定领域，也没有关注模型基础设施或部署优化。相反，它通过大规模分析揭示了推理风格、模型规模和任务类型之间的复杂关系，为选择最优推理策略提供了指导，这对提升LLM的基础推理能力具有重要价值。论文涉及的核心概念（LLMs）和能力方向（reasoning）进一步确认了它与我的研究目标高度相关。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在评估大型语言模型中不同推理风格的效果并解决最优推理策略选择问题。针对多样化任务和模型，我们提出了StyleBench基准测试，系统评估了CoT、ToT、AoT、SoT和CoD五种推理风格，并在15个开源模型(270M-120B参数)和5种推理任务上通过准确率、效率和token消耗等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的效果很大程度上受到其提示中采用的推理策略或思维风格的影响。然而，这些推理风格、模型架构和任务类型之间的相互作用仍未被充分理解。为解决这一问题，我们引入了StyleBench，这是一个用于在不同任务和模型上系统评估推理风格的全面基准测试（benchmark）。我们评估了五种代表性推理风格，包括思维链（Chain of Thought, CoT）、思维树（Tree of Thought, ToT）、思维算法（Algorithm of Thought, AoT）、思维草图（Sketch of Thought, SoT）和草稿链（Chain-of-Draft, CoD），在五个推理任务上使用了来自主要模型系列（LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, 和 DeepSeek）的15个开源模型，参数规模从270M到120B不等。我们的大规模分析表明，没有一种风格是普遍最优的。我们证明，策略效能高度依赖于模型规模和任务类型：基于搜索的方法（AoT, ToT）在开放性问题中表现出色，但需要大规模模型；而简洁风格（SoT, CoD）在明确定义的任务上实现了显著的效率提升。此外，我们识别出关键的行为模式：较小的模型经常无法遵循输出指令并默认为猜测，而推理鲁棒性（robustness）则作为规模的函数而出现。我们的研究结果为根据特定约束条件选择最佳推理策略提供了关键路线图，我们在https://github.com/JamesJunyuGuo/Style_Bench开源了该基准测试。",
                    "inspiration_trace": "## 面临的挑战\n大型语言模型的有效性高度依赖推理策略，但推理风格、模型架构与任务类型间的相互作用理解不足。现有评估局限于单一风格、狭窄任务集或少量模型，且缺乏对推理深度与效率权衡的系统研究。\n\n## 关键洞察\n作者认识到没有单一推理风格普遍最优，策略有效性高度依赖模型规模和任务类型。推理能力是模型规模的函数，不同任务与特定推理风格存在强相关性：结构化多步推理适合数学任务，分支探索更适合开放性谜题。\n\n## 解决方案演进\n从系统性评估需求出发，作者构建了全面基准测试，选取五种代表性推理风格和任务，覆盖15个不同规模模型。通过结构化框架匹配任务类型与推理处理方式，实验分析揭示了模型规模、任务类型与推理策略间的复杂相互作用。\n\n## 创新点总结\n首次系统评估多种推理风格在不同场景表现，揭示三者间复杂关系，挑战\"一刀切\"推理方法，提供基于约束条件选择最优策略的实用路线图，发现大小模型推理行为的根本差异。"
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 4,
            "papers": [
                {
                    "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
                    "arxiv_id": "2509.21164",
                    "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna",
                    "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Mixture of Thoughts (MoT)\"的新方法，用于在不同专业领域（如数学、代码、通用推理）的开源大语言模型之间进行潜在层面的协作。该方法通过一个轻量级路由器选择顶级专家，并在共享潜在空间中通过交叉注意力机制进行协作，从而提升整体推理性能。这符合研究目标，因为：(1) 论文的核心是改进LLM的基础能力，特别是通用推理能力，而不是将LLM作为工具应用到特定领域；(2) 论文涉及多个正面指标，包括大语言模型、推理能力（数学和通用推理）以及多智能体系统；(3) 论文提出了一种通用的多LLM协作框架，用于增强LLM的通用问题解决能力，而非针对特定应用领域；(4) 实验结果表明，MoT在多个基准测试上超过了当前最先进的方法，有效提升了LLM的推理能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决多LLM集成方法中存在的协作粒度粗和计算成本高的问题。针对异构专家模型的协作需求，我们提出了一种Mixture of Thoughts (MoT)方法，通过全局路由和潜在空间交互层实现专家间的细粒度协作，并在五个分布内和三个分布外基准测试上通过准确率等指标验证了其有效性，超越了当前最先进方法。",
                    "summary_translation": "开源大型语言模型（Large Language Models, LLMs，大型语言模型）越来越多地按领域（如数学、代码、通用推理）进行专业化，这推动了利用模型间互补优势的系统发展。先前的多LLM方法要么（i）将查询路由（route）到一个或几个专家并独立生成，（ii）通过昂贵的多轮交换聚合（aggregate）每个模型的输出，或（iii）将权重融合（fuse）到单个模型中——通常需要架构同质性（architectural homogeneity）。\n\n我们提出了思维混合（Mixture of Thoughts, MoT），一种在全局路由方案（global routing scheme）下异构专家（heterogeneous experts）之间进行潜在级别（latent-level）协作的简单方法。对于每个查询，轻量级路由器（lightweight router）选择前$K$个专家并指定一个主要专家；均匀放置的交互层（interaction layers）将隐藏状态（hidden states）投影到共享潜在空间（shared latent space），其中主要专家对其活动（选定）的同行执行交叉注意力（cross-attention）。\n\n预训练的专家保持冻结（frozen）状态；只有路由器和轻量级交互层通过新颖的联合训练目标（joint training objective）进行训练，该目标同时改进专家选择和专家间协作。在五个分布内（in-distribution, ID）和三个分布外（out-of-distribution, OOD）基准测试中，MoT分别以$+0.38\\%$和$+2.92\\%$的优势超越了当前基于路由和聚合的最先进方法（state-of-the-art）Avengers。此外，MoT显著优于表现最佳的单个模型。它通过单次推理（single-pass inference）实现这一点，运行时间与路由基线（routing baselines）相当，且没有迭代聚合（iterative aggregation）的开销。\n\nMoT提供了一种简单的潜在空间机制（latent-space mechanism）来组合异构LLMs，这是迈向更广泛多LLM协作的实用一步。我们的代码在https://github.com/jacobfa/mot上公开可用。",
                    "inspiration_trace": "## 面临的挑战\n现有多LLM协作方法存在明显局限：路由方法只能选择专家独立生成，缺乏真正协作；响应级协作需多轮交互，计算成本高；参数融合要求架构同质性且失去查询适应性。这些方法都只关注模型\"说什么\"而非\"想什么\"，无法实现细粒度的潜在空间交互。\n\n## 关键洞察\n作者认识到，真正的多模型协作需要在潜在空间层面进行交互，而不仅限于输出聚合。通过整合模型中间表示而非最终输出，可以让不同专家的优势在推理过程中动态结合，实现更深入的知识整合。\n\n## 解决方案演进\n基于此洞察，作者设计MoT：先通过全局路由器选择相关专家；再在模型中均匀放置交互层，将隐藏状态投影到共享空间；指定主要专家通过交叉注意力整合其他专家表示；保持专家冻结，仅训练路由器和轻量级交互层，实现单次推理的高效协作。\n\n## 创新点总结\n创新点在于首次实现异构LLM潜在空间的细粒度协作，通过轻量级交互层在保持专业化的同时实现动态查询自适应协作，支持完全异构模型，以单次推理成本实现了多专家深度融合。"
                },
                {
                    "title": "Theoretical Bounds for Stable In-Context Learning",
                    "arxiv_id": "2509.20677",
                    "authors": "Tongxi Wang, Zhuoyang Xia",
                    "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是研究大语言模型的上下文学习(ICL)能力的稳定性问题，建立了理论界限并提出了估计提示长度的方法。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是LLM的一个基础能力——上下文学习(ICL)的稳定性，而非将LLM作为工具应用到特定领域。ICL是LLM的核心能力之一，论文通过建立非渐近下界和提出两阶段可观测估计器，旨在提高这一基础能力的可靠性和稳定性，这属于改进LLM基础能力的研究范畴。 其次，论文涉及LLM的核心概念(ICL)，虽然未直接提及reasoning、planning等能力方向，但ICL本身与这些通用能力密切相关，因为它是模型适应新任务并进行推理的基础机制。 第三，论文不涉及任何需要排除的领域：没有关注多模态与视觉问题，没有将LLM应用到医疗、化学等特定领域，也没有从应用层面研究模型可靠性问题（如水印、安全等）。 最后，虽然论文关注了ICL的可靠性，但这是从理论角度研究LLM基础能力的稳定性，而非应用层面的可靠性问题，因此不应被排除。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（通过增强ICL稳定性），符合研究目标。",
                    "summary2": "本文旨在解决稳定In-context Learning (ICL)所需的最小示例数量问题。针对固定高维亚高斯表示的场景，我们提出了一种基于谱特性的非渐近下界理论，并设计了一个两阶段可观测估计器，在多个数据集、编码器和生成器上通过理论预测与经验拐点(knee-points)的对齐程度验证了其有效性，实验表明该理论作为保守但可靠的上界能准确预测ICL稳定性阈值。",
                    "summary_translation": "In-context learning (ICL，上下文学习)具有灵活性，但其可靠性对提示词长度高度敏感。本文建立了一个非渐近下界，该下界在固定的高维sub-Gaussian representations（亚高斯表示）下，将最少的示例数量与ICL稳定性联系起来。该下界根据协方差的谱性质给出了明确的充分条件，为实践提供了可计算的标准。基于这一分析，我们提出了一种带有一次性校准的两阶段observable estimator（可观测估计器），该估计器无需distributional priors（分布先验）即可生成可供实践者使用的提示词长度估计。在不同数据集、编码器和生成器上的实验表明，预测阈值与empirical knee-points（经验拐点）之间存在密切的一致性，其中理论作为一个保守但可靠的上界；校准变体进一步缩小了这一差距。这些结果将spectral coverage（谱覆盖）与稳定的ICL联系起来，架起了理论与实践部署之间的桥梁，并提高了在现实finite-sample regimes（有限样本条件）下大规模提示的可解释性和可靠性。",
                    "inspiration_trace": "## 面临的挑战\nICL可靠性高度敏感于提示长度，实践中不同任务和模型的稳定性阈值差异大，现有理论无法提供非渐近、可计算的下界来直接连接提示长度与稳定性。\n\n## 关键洞察\n作者将ICL稳定性与特征表示协方差矩阵的最小特征值λmin(Σ̂K)联系起来，认识到谱覆盖与稳定ICL间存在根本联系，预测方差与最小特征值成反比关系。\n\n## 解决方案演进\n从矩阵伯恩斯坦不等式出发，建立非渐近下界；针对理论界限依赖未知谱特性的问题，设计两阶段可观测估计器；进一步通过q-分位数特征值替代和全局缩放因子校准，缩小理论与实践差距。\n\n## 创新点总结\n首次建立非渐近理论下界量化示例数量与ICL稳定性关系；设计无需分布先验的可观测两阶段算法；通过谱特性连接理论与实践，提高大规模提示的可解释性和可靠性。"
                },
                {
                    "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
                    "arxiv_id": "2509.20616",
                    "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合研究目标。从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是提出了一种新的训练范式（单轮强化学习）来增强LLM的任务规划和多步推理能力。论文的核心贡献是将复杂的多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行高效策略优化，这直接提升了LLM的通用推理能力。 从正面指标看，论文明确包含了多个关键主题：Large Language Models (LLMs)、reasoning、task planning、reinforcement learning以及LLM agents，这些都与研究目标高度一致。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容，因此不应被排除。 在特殊和模糊情况处理方面，论文提出的是一种通用的智能体框架来增强LLM的任务规划能力，而不是将智能体应用于特定领域，因此符合保留标准。 综上所述，这篇论文直接致力于提高大语言模型本身的通用推理能力，特别是在任务规划和多步推理方面，与研究目标完全一致。",
                    "summary2": "本文旨在解决LLM代理在多轮任务规划中面临的稀疏奖励、信用分配和计算开销问题。针对复杂的多轮任务规划场景，我们提出了一种将多轮任务规划转化为单轮任务推理问题，并通过基于专家轨迹的GRPO进行高效策略优化的方法。在Robotouille基准测试上通过成功率、平均步数等指标验证了其有效性，1.5B参数模型在长时程规划任务上达到70%成功率，优于14B参数基线模型，并展现了从复杂任务到简单子任务的强泛化能力。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在知识获取、推理和工具使用方面已展现出卓越能力，使其成为自主智能体（autonomous agent）应用的有力候选者。然而，针对复杂多轮任务规划训练LLM智能体面临重大挑战，包括稀疏的逐幕奖励（sparse episode-wise rewards）、长跨度信用分配（credit assignment across long horizons）以及多轮交互设置中强化学习的计算开销（computational overhead）。为此，本文提出了一种新颖方法，将多轮任务规划转化为单轮任务推理问题，通过群体相对策略优化（Group Relative Policy Optimization, GRPO）实现高效策略优化，该方法利用来自专家轨迹的密集且可验证的奖励（dense and verifiable reward）。我们的理论分析表明，GRPO对单轮任务推理的改进能够在最小轮次下提高多轮任务的成功概率，同时能够泛化到跨度更短的子任务。在复杂任务规划基准上的实验评估表明，我们使用单轮GRPO训练的15亿参数模型相比高达140亿参数的更大基线模型取得了优越性能，在超过30步的长跨度规划任务上达到了70%的成功率。我们还从理论和实证两方面验证了强大的跨任务泛化能力，即在复杂任务上训练的模型能够成功完成所有更简单的子任务。",
                    "inspiration_trace": "## 面临的挑战\n多轮任务规划面临三大核心挑战：任务完成奖励过于稀疏，难以有效利用LLM策略；长时程规划中信用分配困难，无法确定哪些行动对最终结果有贡献；多轮RL计算复杂度随序列长度增长，不适合需要数十轮决策的复杂任务训练。\n\n## 关键洞察\n作者洞察到复杂多轮任务规划可分解为一系列单轮任务推理问题，每一步只需根据当前状态选择最优行动。这种分解使利用专家轨迹提供的密集可验证奖励进行高效策略优化成为可能，且单轮推理能力提升可转化为多轮规划成功概率提高。\n\n## 解决方案演进\n作者首先将多轮MDP问题重构为基于专家轨迹的单轮MDP问题，然后应用GRPO方法在单轮任务推理上优化。通过理论分析证明单轮改进能提高多轮规划成功概率，最后通过实验验证方法有效性和跨任务泛化能力。\n\n## 创新点总结\n创新点在于将多轮交互分解为单轮决策，绕过稀疏奖励和信用分配难题；利用专家轨迹提供密集可验证奖励；建立单轮与多轮任务间的理论联系；展示从复杂任务到简单子任务的强泛化能力。"
                },
                {
                    "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
                    "arxiv_id": "2509.21009",
                    "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
                    "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"tail batching\"的新型rollout调度策略和RollPacker系统，用于优化强化学习(RL)作为大语言模型(LLM)后训练技术的效率和性能。论文明确指出RL是\"enhancing the reasoning capabilities of Large Language Models (LLMs)\"的关键技术，这与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"直接相关。论文不是将LLM作为工具应用到特定领域，而是专注于改进LLM的基础训练方法，特别是强化学习这一提升LLM推理能力的关键技术。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文完全符合研究范围。",
                    "summary2": "本文旨在解决同步RL后训练中因响应长度不平衡导致的GPU利用率低下问题。针对LLMs的rollout阶段长尾响应分布，我们提出了tail batching调度策略，将长响应集中到少数long rounds而保持大多数rounds响应平衡。在Qwen2.5系列模型上通过最多128个H800 GPU的实验验证，相比veRL实现了2.03×-2.56×的端到端训练时间减少，相比RLH-Fuse实现了高达2.24×的加速，同时保持训练精度不变。",
                    "summary_translation": "强化学习（Reinforcement Learning, RL）是一种增强大型语言模型（Large Language Models, LLMs）推理能力的关键后训练技术。然而，同步强化学习后训练常常面临严重的图形处理器（GPU）利用不足问题，这种现象被称为\"气泡\"（bubbles），是由推进步骤（rollout steps）中不平衡的响应长度所导致的。许多强化学习系统试图通过放松同步来缓解这一问题，但这可能会损害训练准确性。在本文中，我们提出了尾部分批（tail batching），这是一种针对同步强化学习的新型推进步骤调度策略，它系统性地将导致长尾响应的提示整合到一小部分推进步骤（长轮次，long rounds）中，同时确保大多数步骤（短轮次，short rounds）仅涉及平衡的短推进步骤。通过将长响应从短轮次中排除并重新调度到少数指定的长轮次中，尾部分批有效减少了推进步骤期间的GPU空闲时间，在不牺牲准确性的前提下显著加速了强化学习训练。我们提出了RollPacker系统，该系统通过在所有三个强化学习阶段进行整体优化来充分利用尾部分批的优势：推进步骤的弹性并行（elastic parallelism）适应、奖励（reward）的动态资源分配和调度，以及基于流的训练（stream-based training）。实证结果表明，对于Qwen2.5系列大型语言模型，在多达128个H800图形处理器上，RollPacker相比veRL实现了2.03倍至2.56倍的端到端训练时间减少，相比RLHFuse实现了高达2.24倍的加速。",
                    "inspiration_trace": "## 面临的挑战\n同步RL后训练中存在严重的GPU利用率低下问题，主要由rollout阶段响应长度不平衡导致的长尾分布引起。现有解决方案要么只能提供有限改进（通过阶段重叠），要么牺牲训练准确性（通过放松同步约束）。\n\n## 关键洞察\n作者认识到在rollout批次中，只有一小部分提示会产生特别长的响应，拖慢整个批次。通过重新排序训练样本，将这些长尾提示集中到少数rollout步骤中，可以在不改变样本分布或放松同步的情况下解决问题。\n\n## 解决方案演进\n从这一洞察出发，作者提出\"tail batching\"策略，将长响应提示集中到专门的\"long rounds\"。为解决批量大小问题，引入投机执行机制；为确保所有提示被处理，设计长提示队列。进一步优化系统，设计三个组件分别优化rollout、reward和training阶段。\n\n## 创新点总结\n创新点在于首次通过重新排序训练样本来解决长尾rollout问题，在保持同步RL训练语义的同时显著提高GPU利用率，并提供系统级解决方案全面优化RL训练的三个阶段。"
                },
            ]
        },
        {
            "name": "Multiagent Systems",
            "count": 1,
            "papers": [
                {
                    "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
                    "arxiv_id": "2509.21134",
                    "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng",
                    "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
                    "category": "cs.MA",
                    "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提升大语言模型本身的通用推理能力。从核心判断来看，论文的本质是提出ToMPO算法来增强LLM的战略决策能力，这是一种新的训练范式，属于改进LLM基础能力的研究。论文明确关注LLM在复杂场景中的推理和决策能力，需要\"深入思考、逻辑推理和明智决策\"，这正是通用推理能力的核心要素。 从正面指标看，论文包含了多个相关主题：核心概念是LLMs；能力方向涉及reasoning和strategic decision-making；训练方法采用了reinforcement learning（ToMPO算法）；新兴范式方面则从multi-agent perspective研究问题。 论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文从多智能体角度研究，但它是提出一种通用的框架来增强LLM的战略决策能力，而不是将智能体应用在特定领域。 论文的核心贡献是ToMPO算法，通过基于推理其他个体策略生成rollouts、在图级和样本级估计优势、平衡全局和部分奖励来增强LLM的战略决策能力，这直接提升了LLM的通用推理和决策能力，完全符合我的研究目标。",
                    "summary2": "本文旨在提升大型语言模型在复杂多智能体环境中的战略决策能力。针对包含图级别和努力级别两种决策类型及其时间依赖性的多智能体场景，我们提出了一种Theory of Mind Policy Optimization (ToMPO)算法，并在BCZ和PGG游戏环境上通过U1(合规性)、U2(战略效率)和U3(合作结果)指标验证了其有效性，较GRPO方法提升35%，且优于参数大100倍的模型18%。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已被用于在复杂场景中做出决策，这些场景需要模型进行深度思考、逻辑推理和明智决策。许多现有研究仅关注社交任务或模拟环境中的多轮对话，而忽略了各种类型的决策及其相互依赖性。当前的强化学习方法（reinforcement learning methods）在训练过程中难以考虑他人的策略。为解决这些问题，我们首先定义了一个包含两种决策类型及其时间依赖性的战略决策问题。此外，我们提出了心智理论策略优化（Theory of Mind Policy Optimization, ToMPO）算法，以优化对其他个体策略和游戏局势趋势的感知。与群体相对策略优化（Group Relative Policy Optimization, GRPO）算法相比，ToMPO主要通过以下方式增强LLM的战略决策能力：1）基于对其他个体策略的推理生成推演（rollouts），2）在图级（graph-level）和样本级（sample-level）估计优势，以及3）平衡全局和部分奖励。在模型输出合规性和合作结果方面，ToMPO算法比GRPO方法高出35%。此外，与参数量大100倍的模型相比，它显示出18%的改进。这证明了ToMPO算法在增强模型战略决策能力方面的有效性。",
                    "inspiration_trace": "## 面临的挑战\n现有LLM在复杂多智能体环境中进行战略决策时存在明显不足：仅关注社交对话或单一场景，忽略决策类型间的相互依赖性；强化学习方法难以考虑他人策略；模型在理解他人意图、预测行为并动态调整策略方面能力有限。\n\n## 关键洞察\n作者认识到战略决策本质上是双重决策过程——图级决策(社会关系结构)和努力级决策(资源投入)，两者存在时间依赖性。有效的决策需要具备\"心理理论\"能力，即理解他人策略和意图，类似于人类社交推理。\n\n## 解决方案演进\n从问题定义出发，通过初步测试发现模型不足，进而生成专家数据训练基础推理能力。然后提出ToMPO算法，将多智能体视角融入策略训练：通过推理他人策略生成决策路径，在图级和样本级估计优势，平衡全局与局部奖励，使模型能考虑他人策略并动态调整自身决策。\n\n## 创新点总结\n创新在于将心理理论与强化学习结合，从单一智能体视角转向多智能体互动视角；同时优化双重决策及其时间依赖性；通过多层次优势估计平衡局部精度与全局最优，使LLM在复杂社会环境中具备类似人类的战略推理能力。"
                },
            ]
        },
    ],
    "2025-09-24": [
        {
            "name": "Artificial Intelligence",
            "count": 5,
            "papers": [
                {
                    "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs",
                    "arxiv_id": "2509.20105",
                    "authors": "Venkat Margapuri, Garik Kazanjian, Naren Kosaraju",
                    "summary": "Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础推理能力，提出了一种量子启发的强化学习方法来增强LLM在连贯多步推理方面的表现。这直接对应了\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的核心标准。 从正面指标分析，论文明确包含以下关键要素： - 核心概念：直接关注\"Large Language Models (LLMs)\" - 能力方向：专注于\"coherent multi-step reasoning traces\"和\"structured logical flow\"，属于推理能力范畴 - 训练方法：采用强化学习方法（Proximal Policy Optimization），结合量子物理中的Projected Entangled Pair States (PEPS)概念 论文不涉及任何排除标准中的领域。它不是关于多模态与视觉研究，不是将LLM应用到特定领域，也不是关于模型可靠性在应用层面的研究。虽然论文在GSM8K、StrategyQA和EntailmentBank等数据集上进行了评估，但这些是评估通用推理能力的标准数据集，而非特定领域应用。 论文的核心贡献是提出了一种基于量子物理概念的强化学习方法，通过保真度奖励机制来提高LLM生成连贯推理痕迹的能力，这是一种从根本上提升模型推理能力的方法论创新，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)在保持连贯多步推理追踪方面的困难。针对需要结构化逻辑流程的任务，我们提出了一种量子启发的强化学习方法，使用Projected Entangled Pair States (PEPS)导出基于保真度的奖励并集成到Proximal Policy Optimization (PPO)中，并在GSM8K、StrategyQA和EntailmentBank数据集上通过MEC、WES、BERT和BLEURT等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）在保持连贯的多步推理轨迹方面常常面临困难，特别是在需要结构化逻辑流程的任务中。本研究引入了一种量子启发（quantum-inspired）的方法，通过将基于投影纠缠对态（Projected Entangled Pair States, PEPS）的保真度奖励（fidelity-based reward）纳入近端策略优化（Proximal Policy Optimization）来应对这一挑战。与先前使用直接监督或对比目标的方法不同，所提出的方法通过结构一致性指导学习，为在生成的推理轨迹中强制执行全局连贯性提供了一种新途径。该框架在多个数据集上使用多种连贯性确定指标进行了评估，这些数据集包括GSM8K、StrategyQA和EntailmentBank，涵盖了算术、直观和基于蕴含（entailment-based）的推理类型。结果表明，所提出的量子启发方法相比监督、对比和预训练的基线方法有显著改进，凸显了量子启发的保真度作为提高大型语言模型中推理轨迹连贯性基础的有效性。",
                    "inspiration_trace": ""
                },
                {
                    "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning",
                    "arxiv_id": "2509.19762",
                    "authors": "Yuanxin Wang, Pawel Filipczuk, Anisha Garg, Amaan Dhada, Mohammad Hassanpour, David Bick, Ganesh Venkatesh",
                    "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by internal model training and external agentic orchestration. However, this synergy is often inefficient, as model verbosity and poor instruction following lead to wasted compute. We analyze this capability-cost trade-off and introduce an optimized reasoning workflow (\\cepo) that empowers smaller open-source models to outperform models multiple times their size. We will open-source this workflow to enable further research. Our work demonstrates a clear path toward co-designing orchestration frameworks with the underlying model capabilities to unlock powerful reasoning in small-to-medium sized models.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是关于改进LLM的推理能力，提出了一种优化的推理工作流程(\\cepo)，通过协调内部模型训练和外部智能体编排来提高推理效率，使较小的开源模型能够超越比它们大得多的模型。这直接符合\"改进LLM的基础能力、增强其逻辑、多步推理等通用能力\"的标准。 其次，从正面指标看，论文明确涉及\"LLM reasoning\"这一核心概念和\"reasoning\"这一能力方向，同时提到了\"external agentic orchestration\"，与智能体(llm-based agents)这一新兴范式相关。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，在特殊情况下，论文提出的智能体编排框架是通用性的，旨在增强LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，该论文的核心贡献是提出了一种协同设计编排框架与底层模型能力的方法，以释放中小型模型的强大推理能力，这与研究目标高度一致。",
                    "summary2": "本文旨在 [解决现代LLM推理中因模型冗长和指令遵循不佳导致的计算浪费问题]。针对 [中小型开源模型的推理效率与性能问题]，我们提出了一种 [CODA（Conductor-driven Architecture）优化推理工作流，包含自适应规划、执行、自我反思和验证等关键组件]，并在 [AIME、GPQA、LiveCodeBench等数学和编码基准测试] 上通过 [准确率、Pass@k等指标] 验证了其有效性。",
                    "summary_translation": "现代大语言模型(LLM, Large Language Model)推理依赖于广泛的测试时计算(test-time computation)，这种计算由内部模型训练和外部智能体编排(agentic orchestration)共同驱动。然而，这种协同作用(synergy)往往效率低下，因为模型的冗长性(verbosity)和不良的指令遵循(instruction following)能力导致计算资源浪费(wasted compute)。我们分析了这种能力-成本权衡(capability-cost trade-off)，并提出了一种优化的推理工作流(optimized reasoning workflow) \\cepo，它使较小的开源模型能够超越规模大得多的模型。我们将开源(open-source)这一工作流，以促进进一步的研究。我们的工作展示了一条明确的路径，即通过协同设计(co-designing)编排框架(orchestration frameworks)与底层模型能力(underlying model capabilities)，来解锁中小型模型(small-to-medium sized models)的强大推理能力。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving",
                    "arxiv_id": "2509.19681",
                    "authors": "Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, Ganesh Venkatesh",
                    "summary": "Advanced test-time computing strategies are essential for scaling reasoning models, but their effectiveness is capped by the models' poor self-evaluation. We propose a pairwise Explanatory Verifier, trained via reinforcement learning (GRPO), that produces calibrated confidence scores and associated natural language reasoning for generated solutions. Our verifier improves the accuracy and efficiency of test-time strategies like best-of-n and self-reflection. Crucially, it excels at identifying challenging failure modes, such as when both candidate solutions are identically incorrect, succeeding where standard methods like majority voting fail.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出一种\"解释性验证器\"(Explanatory Verifier)来增强大语言模型的通用推理能力。具体分析如下： 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是通过强化学习(GRPO)训练的验证器来提升模型的自我评估能力，这属于增强LLM通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 从第二步正面指标看，论文涉及多个相关主题： 1. 能力方向：明确聚焦于\"reasoning\"和\"problem-solving\"，这正是研究目标的核心 2. 训练方法：使用\"reinforcement learning (GRPO)\"进行训练，符合强化学习优化LLM能力的方向 3. 提到的\"self-reflection\"也与提升模型自主推理能力相关 从第三步排除标准看，论文不涉及任何多模态、视觉内容，也不针对医疗、化学、生物等特定应用领域，同时虽然涉及到模型可靠性，但目的是从根本上提升模型的推理能力而非仅作为应用层面的防御。 论文特别关注提高LLM的\"自我评估\"能力，这是通用推理能力的重要组成部分，通过校准的置信度分数和自然语言解释来增强模型的推理质量，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决推理模型在测试时计算策略中自我评估能力不足的问题。针对数学和编码问题求解场景，我们提出了一种基于强化学习(GRPO)训练的成对解释性验证器(Explanatory Verifier)，并在Numina Math、CodeForces和LeetCode数据集上通过准确性和计算效率指标验证了其有效性。",
                    "summary_translation": "先进的测试时计算（test-time computing）策略对于扩展推理模型至关重要，但其有效性受到模型自我评估（self-evaluation）能力不足的限制。我们提出了一种成对解释验证器（pairwise Explanatory Verifier），通过强化学习（GRPO）进行训练，可为生成的解决方案生成校准的置信度分数（calibrated confidence scores）及相关自然语言推理（natural language reasoning）。我们的验证器提高了最佳n选一（best-of-n）和自我反思（self-reflection）等测试时策略的准确性和效率。关键的是，它擅长识别具有挑战性的故障模式（failure modes），例如当两个候选解决方案都完全错误时，能够在多数投票（majority voting）等标准方法失败的情况下取得成功。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究基于Transformer的LLMs在图推理任务中的内部机制，通过circuit-tracer框架来解释decoder-only transformers的推理过程。论文识别了图推理中的两个核心机制：token merging和structural memorization，并提供了统一的可解释性框架来理解结构推理。这符合研究目标中\"改进LLM的基础能力\"和\"增强其逻辑、多步推理等通用能力\"的要求。论文关注的是LLM本身的推理能力机制，而不是将LLM作为工具应用到特定领域。虽然论文聚焦于图推理这一特定类型的推理，但其目标是提供\"统一的可解释性框架\"来理解结构推理，这属于通用推理能力的研究范畴。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文符合研究范围。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究线性注意力transformer（一种LLM架构）的基础能力。论文展示了模型通过训练能够隐式地发现统一的数值算法，这直接涉及LLM的内在能力提升，而非将LLM作为工具应用于特定领域。论文关注的是上下文学习(in-context learning)能力，这是一种基础能力的研究，与提高LLM的通用推理能力密切相关。 其次，从正面指标分析，论文符合以下关键点： - 核心概念：研究的是线性注意力transformer，属于LLM架构变体 - 能力方向：涉及数学推理(math reasoning)和问题解决(problem-solving)能力，模型通过学习解决矩阵补全问题，隐式发现了数值算法 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域 - 不涉及模型可靠性的应用层面研究 最后，论文的核心贡献是揭示了LLM能够通过训练隐式发现统一的、资源自适应的迭代求解器，这展示了LLM在算法发现和数学推理方面的强大能力，直接关系到通用推理能力的提升。论文研究的是LLM内在的能力机制，而非特定应用，因此完全符合研究目标。",
                    "summary2": "本文旨在探索训练线性Transformer是否能隐式发现统一的数值算法。针对低秩矩阵补全任务，我们提出了一种训练线性Transformer在masked-block completion任务上隐式学习数值算法的方法，并通过收敛速度和预测准确率验证了EAGLE算法的有效性。",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。",
                    "inspiration_trace": ""
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 17,
            "papers": [
                {
                    "title": "Language Models that Think, Chat Better",
                    "arxiv_id": "2509.20357",
                    "authors": "Adithya Bhaskar, Xi Ye, Danqi Chen",
                    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results rethink the post-training pipeline and call upon future work to understand and employ thinking more broadly.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是提出了一种名为\"RL with Model-rewarded Thinking (RLMT)\"的新训练范式，旨在增强大语言模型的通用推理能力。该方法要求模型在回答前生成长链思维(CoT)推理，并使用基于偏好的奖励模型进行在线强化学习优化，从而提升模型的规划和问题解决能力。 从筛选标准来看： 1. 第一步核心判断：论文本质是改进LLM的基础能力，提出新的训练范式(RLMT)，增强其推理能力，完全符合保留标准。 2. 第二步正面指标：论文包含多个正面指标，如大语言模型(Llama-3.1-8B和Qwen-2.5-7B)、推理能力(reasoning)、强化学习(RL)等。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性等排除领域。 4. 第四步特殊情况：论文专注于通过思维链推理和强化学习来提升模型本身的推理能力，而非将LLM作为工具应用到特定领域。 论文在多个聊天基准测试和任务上取得了显著改进，包括创意写作和一般知识，这表明其方法有效提升了模型的通用推理能力，而非仅限于特定领域。因此，这篇论文完全符合研究目标。",
                    "summary2": "本文旨在解决基于可验证奖励的强化学习(RLVR)在开放性任务上泛化能力有限的问题。针对通用聊天任务，我们提出了一种结合长链推理与偏好模型奖励的强化学习方法(RLMT)，并在Llama-3.1-8B和Qwen-2.5-7B模型上通过多种聊天基准测试(AlpacaEval2, WildBench, Arena-HardV2)验证了其有效性，实现了3-7点的性能提升，甚至超越了GPT-4o在聊天和创意写作方面的表现。",
                    "summary_translation": "可验证奖励强化学习(Reinforcement learning with verifiable rewards, RLVR)通过在可验证领域(如数学和代码)中使用基于规则的奖励来改善语言模型推理能力。然而，RLVR在开放性任务(如撰写大纲论文或制定膳食计划)中导致有限的泛化能力，而这些任务正是人类日常推理的场景。本文表明RLVR范式在可验证领域之外同样有效，并提出了用于通用聊天能力的**基于模型奖励思维的强化学习**(**R**einforcement **L**earning with **M**odel-rewarded **T**hinking, **RLMT**)。\n\n使用多样化的真实世界提示，RLMT要求语言模型(LMs)在响应前生成长链思维(Chain of Thought, CoT)推理，并通过在线强化学习(online RL)对其进行优化，使用的奖励模型是基于偏好的，类似于RLHF(Reinforcement Learning from Human Feedback，人类反馈强化学习)中使用的模型。在Llama-3.1-8B和Qwen-2.5-7B(包括基础模型和指令模型)上进行的40次训练运行以及多种优化算法(DPO、PPO和GRPO)中，RLMT始终优于标准RLHF流程。这包括在三个聊天基准测试(AlpacaEval2、WildBench和ArenaHardV2)上获得3-7分的显著提升，以及在创意写作和常识等其他任务上1-3分的改进。\n\n我们最佳的8B模型在聊天和创意写作方面超越了GPT-4o，并与Claude-3.7-Sonnet (Thinking)相当。RLMT也可以直接应用于基础模型，无需SFT(Supervised Fine-Tuning，监督微调)阶段，类似于R1-Zero训练方式。值得注意的是，仅使用7K个提示，通过我们的RLMT方法训练的Llama-3.1-8B基础模型，其性能超过了经过复杂多阶段流程(使用2500万+示例)后训练的Llama-3.1-8B-Instruct模型。最后，我们对训练模型如何规划其响应进行了定性和定量分析。我们的结果重新思考了后训练流程，并呼吁未来的工作更广泛地理解和运用思维过程。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式\"Thinking augmented Pre-Training (TPT)\"，通过在预训练阶段增加思维轨迹来增强模型的推理能力。这直接关注提升LLM的通用推理能力，而非将其作为工具应用于特定领域。 其次，论文包含了多个正面指标：核心概念明确关注大型语言模型(LLMs)，能力方向聚焦于推理能力(reasoning)，特别是通过\"step-by-step reasoning and decomposition\"来提升模型性能。实验结果也显示该方法在多个具有挑战性的推理基准上提高了模型性能。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献是提出了一种通用的预训练方法，通过自动生成的思维轨迹增强文本数据，使高质量token更易学习，从而提高LLM的数据效率和推理能力。这种方法不是针对特定领域，而是旨在从根本上提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"论文的目标。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Causal Understanding by LLMs: The Role of Uncertainty",
                    "arxiv_id": "2509.20088",
                    "authors": "Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci",
                    "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究LLMs在因果关系理解方面的能力，属于LLM的基础推理能力研究，特别是逻辑推理能力的重要组成部分。论文通过多种模型测试，分析了LLMs在因果分类和记忆探测方面的表现，发现LLMs在因果理解上的失败源于缺乏结构化的因果表示，而非预训练中因果例子暴露不足。虽然论文使用了PubMed句子作为测试数据，但这只是为了评估LLM的通用因果理解能力，而不是将LLM应用于医疗领域。论文关注的是LLMs本身的推理能力缺陷，属于对LLM基础能力的探索，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在探究大型语言模型在因果理解任务中表现不佳的原因。针对预训练数据中因果例子暴露不足与表示能力缺陷的争议，我们提出了一种基于不确定性量化的评估方法，通过熵和校准误差等指标分析模型行为。在包含18,000多个PubMed句子的数据集上，通过因果类型分类和逐字记忆探测任务，验证了模型在已见和未见数据上表现无显著差异，输出分布接近随机，表明因果理解失败源于缺乏结构化因果表示而非数据暴露不足。",
                    "summary_translation": "最近的研究表明，大型语言模型（LLMs）在因果关系分类（causal relation classification）中实现接近随机的准确率，引发了关于此类失败是源于预训练（pretraining）中接触有限还是更深层次的表征缺口（representational gaps）的问题。我们在基于不确定性（uncertainty-based）的评估下对此进行了研究，测试了预训练中接触因果例子是否能够改善对超过18,000条PubMed句子的因果理解（causal understanding）——其中一半来自The Pile语料库，一半来自2024年之后的内容——涉及七个模型（Pythia-1.4B/7B/12B、GPT-J-6B、Dolly-7B/12B、Qwen-7B）。我们通过以下方式分析模型行为：（i）因果关系分类（causal classification），模型识别文本中的因果关系；以及（ii）逐字记忆探测（verbatim memorization probing），我们评估模型是否更偏好之前见过的因果陈述而非其释义（paraphrases）。模型执行四类分类（direct/conditional/correlational/no-relationship，直接/条件/相关/无关系）并在原始句子和生成的释义之间进行选择。结果显示，模型在已见/未见句子上的准确率几乎相同（p > 0.05），没有记忆偏差（memorization bias）（24.8%选择原始句子），且可能选项的输出分布几乎平坦，熵值（entropic values）接近最大值（1.35/1.39），证实了随机猜测。指令微调（Instruction-tuned）模型表现出严重的校准失调（miscalibration）（Qwen：> 95%的置信度，32.8%的准确率，ECE=0.49）。条件关系（Conditional relations）诱导出最高的熵（比直接关系高+11%）。这些发现表明，因果理解的失败源于缺乏结构化的因果表征（structured causal representation），而非预训练中接触因果例子不足。",
                    "inspiration_trace": ""
                },
                {
                    "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                    "arxiv_id": "2509.20317",
                    "authors": "Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin",
                    "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文的本质是关于改进大语言模型的推理能力，特别是针对Implicit Chain-of-Thought (CoT)方法提出了一种新的训练范式SIM-CoT，这直接属于改进LLM基础能力和通用推理能力的范畴，符合保留标准。 其次，论文包含多项正面指标：核心概念明确涉及Large Language Models (LLMs)；能力方向聚焦于reasoning，特别是Chain-of-Thought推理；训练方法方面提出了创新的step-level supervision机制来增强模型训练过程。 第三，论文不涉及任何排除标准领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文提出的辅助解码器增强了模型推理的可解释性，通过\"projecting each latent token onto an explicit reasoning vocabulary\"来提升模型的推理质量，这符合保留条件。 论文的核心贡献是解决了implicit CoT方法中的潜在不稳定性问题，通过step-level supervision稳定和丰富潜在推理空间，从而提升LLM的推理能力，这与研究目标\"致力于提高大语言模型的通用推理能力\"高度一致。",
                    "summary2": "本文旨在解决隐式思维链(Implicit CoT)方法中的潜在不稳定性问题。针对大语言模型在增加隐式推理令牌时训练崩溃的场景，我们提出了一种SIM-CoT方法，通过引入步骤级监督的辅助解码器来稳定隐式推理空间，并在GSM8K-Aug等多个数学推理数据集上通过准确率、令牌效率等指标验证了其有效性。",
                    "summary_translation": "Implicit Chain-of-Thought (CoT)（隐式思维链）方法为大型语言模型（Large Language Models, LLMs）中的显式思维链推理提供了一种有前景且高效的token（令牌）替代方案，但持续存在的性能差距限制了隐式CoT的应用。通过扩展隐式CoT方法的计算预算，我们发现了一个核心的潜在不稳定性问题：当我们增加隐式推理token的数量以提高性能时，训练过程常常变得不稳定并崩溃。我们的分析表明，这种不稳定性源于潜在表示变得同质化并失去其语义多样性，这是现有隐式CoT方法中步骤级别（step-level）监督不足导致的失败。\n\n为解决这一问题，我们提出了SIM-CoT，即插即用（plug-and-play）训练模块，它引入步骤级别监督以稳定并丰富潜在推理空间。具体而言，SIM-CoT在训练过程中采用辅助解码器（auxiliary decoder）将每个隐式token与其对应的显式推理步骤对齐，确保潜在状态捕获独特且有意义的信息。在推理过程中，所提出的辅助解码器被移除，保持了隐式CoT方法的计算效率，且不增加额外开销。此外，辅助解码器通过将每个潜在token投影到显式推理词汇表（explicit reasoning vocabulary）上，提供了隐式推理的可解释性，实现了每步语义角色和诊断的可视化。\n\nSIM-CoT显著提升了各种隐式CoT方法的域内（in-domain）准确性和域外（out-of-domain）稳定性，使GPT-2上的Coconut基线提高了+8.2%，LLaMA-3.1 8B上的CODI提高了+3.0%。展示了强大的可扩展性（scalability），SIM-CoT在GPT-2上以2.3倍的token效率超越了显式CoT基线2.1%，同时在更大模型如LLaMA-3.1 8B上显著缩小了性能差距。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Can Constructions \"SCAN\" Compositionality ?",
                    "arxiv_id": "2509.20074",
                    "authors": "Ganesh Katrapati, Manish Shrivastava",
                    "summary": "Sequence to Sequence models struggle at compositionality and systematic generalisation even while they excel at many other tasks. We attribute this limitation to their failure to internalise constructions conventionalised form meaning pairings that license productive recombination. Building on these insights, we introduce an unsupervised procedure for mining pseudo-constructions: variable-slot templates automatically extracted from training data. When applied to the SCAN dataset, our method yields large gains out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on AROUND RIGHT without any architectural changes or additional supervision. The model also attains competitive performance with? 40% of the original training data, demonstrating strong data efAciency. Our findings highlight the promise of construction-aware preprocessing as an alternative to heavy architectural or training-regime interventions.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是关于改进序列到序列模型(Seq2Seq)的组合性和系统泛化能力，这些能力是大语言模型通用推理能力的重要组成部分。论文提出了一种无监督方法来挖掘\"伪构造\"(pseudo-constructions)，即从训练数据中自动提取的可变槽模板，这种方法在SCAN数据集上显著提高了模型在分布外分割上的准确性。这属于改进模型基础能力的方法论研究，而非将LLM应用于特定领域。组合性是逻辑推理和语言理解的基础，系统泛化则涉及到模型如何处理新的、未见过的组合，这些都是通用推理能力的关键方面。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决序列到序列模型在组合性和系统泛化方面的困难。针对SCAN数据集的分布外测试场景，我们提出了一种无监督的伪构造挖掘方法，并在SCAN数据集的ADD JUMP和AROUND RIGHT分割上通过准确率验证了其有效性。",
                    "summary_translation": "序列到序列（Sequence to Sequence）模型在组合性（compositionality）和系统性泛化（systematic generalisation）方面表现不佳，尽管它们在许多其他任务上表现出色。我们将这一局限性归因于它们未能内化构式（constructions）——即约定俗成的形式-意义配对，而这种配对能够许可生产性重组。基于这些见解，我们引入了一种无监督（unsupervised）程序来挖掘伪构式（pseudo-constructions）：即从训练数据中自动提取的可变槽位模板（variable-slot templates）。当应用于SCAN数据集时，我们的方法在分布外（out-of-distribution）分割上取得了显著提升：在无需任何架构变更或额外监督的情况下，ADD JUMP上的准确率提高到47.8%，AROUND RIGHT上提高到20.3%。该模型仅使用40%的原始训练数据就能达到竞争性性能，展示了强大的数据效率（efficiency）。我们的研究结果突显了构式感知（construction-aware）预处理作为一种替代方案的潜力，以替代繁重的架构或训练机制干预。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
                    "arxiv_id": "2509.19893",
                    "authors": "Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo",
                    "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是改进LLM的数学推理能力，属于\"增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文提出了FPA（Future Policy Aware）方法，用于解决偏好学习在数学推理中的问题，这是直接提升LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标：核心概念上明确研究Large Language Models (LLMs)；能力方向上专注于mathematical reasoning（数学推理）；训练方法上涉及Direct Preference Optimization (DPO)等偏好学习方法，这些通常与强化学习相关。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然聚焦于数学推理，但数学推理被视为评估和提升LLM通用能力的重要方面，而非特定应用领域；也没有主要关注模型可靠性方面的应用问题。 论文的核心贡献是提出了一种新的偏好学习方法FPA，通过在正则化项中使用未来策略而非当前策略，解决了数学推理中偏好学习的过度惩罚问题，从而提升了LLM的数学推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决数学推理任务中偏好学习方法的梯度纠缠问题。针对数学推理轨迹中大量共享token导致的过度惩罚问题，我们提出了一种未来策略感知（FPA）偏好学习方法，通过轻量级logit空间外推估计未来策略进行主动正则化。在MATH和GSM8K基准测试上通过准确率验证了其有效性，FPA在SimPER算法上实现了高达5.75%的性能提升，同时支持更长的无退化训练。",
                    "summary_translation": "像直接偏好优化（Direct Preference Optimization, DPO，直接偏好优化）这样的偏好学习方法已成为大型语言模型（Large Language Model, LLM，大型语言模型）后训练的标准方法，但它们在数学推理方面往往效果不佳。一个关键挑战是偏好轨迹和非偏好轨迹之间存在大量的标记（token，标记）重叠；降低非偏好轨迹的概率同时也会降低共享有用标记的概率，导致过度惩罚和整体性能崩溃。作为一种缓解措施，现有算法将轨迹在当前策略下的概率作为正则化项（regularization term，正则化项）包含在内，当概率较低时，这会降低梯度的影响。然而，当这种效果开始显现时，有用的标记可能已经被过度惩罚，因为模型已经开始退化。\n\n为解决这个问题，我们提出了未来策略感知（Future Policy Aware, FPA，未来策略感知）偏好学习，它在正则化项中用未来策略替代当前策略。这种未来策略通过从参考模型到当前模型的轻量级logit空间（logit-space，logit空间）外推来估计。FPA通过预先规范化可能有问题的梯度，实现了更安全的训练。我们将FPA应用于DPO、RPO和SimPER，并在MATH和GSM8K基准测试上对它们进行评估。FPA带来了一致的性能提升，其中在SimPER上观察到最大的改进，实现了高达5.75%的提升。我们证明FPA提供了主动的正则化，同时保留了共享的有用数学标记的概率，并实现了更长时间的无退化训练，且计算开销可忽略不计。我们将在发表后公开发布我们的代码。",
                    "inspiration_trace": ""
                },
                {
                    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
                    "arxiv_id": "2509.19593",
                    "authors": "Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang",
                    "summary": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合我的研究目标，因为它核心关注的是大语言模型的通用推理能力。论文提出了GuessingGame协议，用于评估LLMs作为战略提问者的能力，本质上是在研究模型如何通过提问和获取信息来进行有效推理。这种开放式提问能力是一种通用推理能力，类似于思维链(CoT)等多步推理能力，而非将LLM应用于特定领域。论文提出的两种信息增益指标旨在衡量和提升LLMs的推理效率，结果显示信息增益与推理效率显著相关，这种研究直接针对提升LLM的基础推理能力。论文不涉及任何排除标准中的领域（如多模态、特定应用或模型基础设施），而是聚焦于提高LLM本身的通用推理能力，因此完全符合我的研究范围。",
                    "summary2": "本文旨在评估大型语言模型作为策略性提问者的能力。针对开放式、开放领域的问答场景，我们提出了GuessingGame协议，通过Guesser LLM向Oracle提问识别隐藏对象，并设计了两种信息增益(IG)度量方法：贝叶斯信念跟踪和基于ConceptNet的熵方法。在858个游戏实验中，通过成功率(SR)和平均问题数(ANQ)验证了方法有效性，证明IG与任务效率强相关，一个标准差IG增加可减少43%预期游戏长度。",
                    "summary_translation": "我们提出了GuessingGame（猜谜游戏）协议，用于评估大型语言模型（LLMs, Large Language Models）在开放式、开放域环境中作为战略提问者的表现。在该协议中，一个Guesser LLM（猜测者语言模型）通过向Oracle（预言者）提出自由形式的问题来识别一个隐藏对象，无需预设选项或候选列表。为衡量问题质量，我们提出了两种信息增益（IG, Information Gain）指标：一种贝叶斯方法，通过使用LLM评分的相关性来追踪对语义概念的信念更新；另一种基于熵的方法，通过ConceptNet（概念网络）过滤候选对象。这两种指标都是模型无关的（model-agnostic），并支持事后分析（post hoc analysis）。在涉及多个模型和提示策略的858场游戏中，更高的IG strongly predicts效率：IG的一个标准差增加使预期游戏长度减少43%。由IG指导的提示约束，如强制问题多样性，使较弱的模型能显著提高性能。这些结果表明，LLMs中的提问既是可测量的也是可改进的，并且对交互式推理（interactive reasoning）至关重要。",
                    "inspiration_trace": ""
                },
                {
                    "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
                    "arxiv_id": "2509.19569",
                    "authors": "Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva",
                    "summary": "This paper introduces a novel approach to position embeddings in transformer models, named \"Exact Positional Embeddings\" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是改进Transformer模型的位置编码方法，提出了\"精确位置嵌入\"(ExPE)，使模型能够处理比训练时更长的序列。这属于改进LLM基础架构的研究，旨在增强模型处理长序列的基础能力，而非将LLM作为工具应用到特定领域。因此，论文符合保留标准。 第二步正面指标：论文涉及\"Generative Transformer Models\"，属于LLM范畴。虽然未直接讨论推理、规划等能力，但处理长序列的能力是支持复杂推理任务的基础。例如，数学推理、逻辑推理和多步规划通常需要处理长序列的能力，因此这项工作间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及任何排除标准中的领域，包括多模态与视觉、特定应用领域以及模型可靠性的应用层面问题。它关注的是基础模型架构的改进。 第四步特殊和模糊情况：论文情况清晰，不涉及特殊或模糊情况。它明确关注的是Transformer模型的位置编码方法，属于基础模型架构的改进。 最终决策：虽然论文没有直接讨论推理、规划或问题解决能力，但改进模型处理长序列的能力是支持复杂推理任务的基础。因此，这篇致力于改进LLM基础架构能力的研究符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的核心研究目标。",
                    "summary2": "本文旨在解决Transformer模型在处理比训练序列更长时的位置编码外推问题。针对序列长度外推的场景，我们提出了一种精确位置编码（ExPE）方法，通过覆盖嵌入向量中的特定维度来编码精确位置信息，并在因果语言建模任务上通过困惑度（perplexity）指标验证了其有效性。",
                    "summary_translation": "本文介绍了一种在transformer models（Transformer模型）中进行position embeddings（位置嵌入）的新方法，名为\"Exact Positional Embeddings\"（精确位置嵌入，ExPE）。这是一种absolute positional embedding（绝对位置嵌入）方法，能够extrapolate（外推）到比训练时更长的序列。传统的transformer models（Transformer模型）依赖absolute或relative position embeddings（绝对或相对位置嵌入）将位置信息整合到token embeddings（词元嵌入）中，但这些方法通常难以extrapolate（外推）到比训练时更长的序列。我们提出的方法利用一种新的embedding strategy（嵌入策略），通过重写embedding vectors（嵌入向量）的特定维度来编码精确的位置信息，从而实现对词元位置的更精确表示。所提出的方法不仅保持了原始embeddings（嵌入）的原有特性，还增强了模型对更长序列的泛化能力。在causal language modeling（因果语言建模）中，当在比训练中使用的更长的序列上进行测试时，我们的ExPE embeddings（ExPE嵌入）与rotary and sinusoidal embeddings（旋转和正弦嵌入）相比，显著降低了perplexity（困惑度）。",
                    "inspiration_trace": ""
                },
                {
                    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
                    "arxiv_id": "2509.19371",
                    "authors": "Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng",
                    "summary": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，主要基于以下分析： 第一步核心判断：这篇论文的本质是关于改进LLM基础能力的研究，具体聚焦于预训练阶段的知识注入方法。论文提出了\"知识注入缩放定律\"，这是一种新的训练范式，旨在解决LLM在知识获取与保留方面的核心挑战。虽然论文提到了\"领域知识\"，但其核心贡献是通用的方法论，用于平衡知识注入与避免灾难性遗忘，这直接关系到提升LLM的基础能力，符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。 第二步正面指标：论文明确包含\"Large language models, LLMs\"这一核心概念。虽然论文没有直接讨论reasoning、planning等具体能力方向，但知识获取和保留是推理能力的基础，论文研究的是如何更有效地让模型获取并保留知识，这间接支持了通用推理能力的提升。 第三步排除标准：论文不涉及多模态与视觉研究。虽然提到\"domain-specific data\"和\"specialized knowledge\"，但论文的核心是提出一种通用的知识注入缩放定律，而非专注于某个特定应用领域（如医疗、化学等）。论文提到\"hallucination\"问题，但是从知识注入角度研究如何减少幻觉，而非仅作为应用层面的防御。 第四步特殊和模糊情况处理：论文虽然涉及\"领域知识\"，但其核心贡献是通用的方法论，可以应用于各种领域知识的注入，而不是针对特定领域的应用研究。因此，它更符合\"改进LLM基础能力\"而非\"特定应用领域\"的特征。 综上所述，这篇论文的核心贡献是提出了一种通用的知识注入缩放定律，用于优化LLM预训练过程中的知识获取和保留，这属于提升LLM基础能力的研究范畴，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型预训练中如何高效注入领域知识的问题。针对不同规模LLMs在知识注入时出现的记忆崩溃现象，我们提出了一种知识注入扩展律(Knowledge Infusion Scaling Law)，通过分析较小模型预测大型模型的最佳知识注入量，并在从137M到3B参数的不同模型规模和高达100B训练token的实验环境中，通过记忆保留率(Memorization Rate)指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）因其令人印象深刻的跨多样化下游任务的通用能力而吸引了广泛关注。然而，在没有领域特定优化（domain-specific optimization）的情况下，它们在专业知识基准（specialized knowledge benchmarks）上表现不佳，甚至会产生幻觉（hallucination）。最近的研究表明，在预训练（pretraining）期间策略性地注入领域知识（domain knowledge）可以显著提高下游性能（downstream performance）。一个关键挑战在于平衡这种注入权衡（infusion trade-off）：注入过少的领域特定数据（domain-specific data）会导致专业化不足，而过量注入则会引发灾难性遗忘（catastrophic forgetting）先前获得的知识。在这项工作中，我们关注由过度注入（over-infusion）引起的记忆崩溃（memory collapse）现象。通过系统性实验，我们得出了两个关键观察结果，即1）关键崩溃点（Critical collapse point）：每个模型都表现出一个阈值，超过该阈值，其知识保留能力会急剧下降。2）规模相关性（Scale correlation）：这些崩溃点与模型规模（model's size）呈一致的比例关系。基于这些见解，我们提出了一种知识注入扩展定律（knowledge infusion scaling law），通过分析较小规模的对应模型来预测应注入大型语言模型（LLMs）的最佳领域知识量。在不同模型规模和预训练令牌预算（pertaining token budgets）上的大量实验验证了我们扩展定律的有效性和泛化性。",
                    "inspiration_trace": ""
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM的基础能力和提出新的训练范式。ShinkaEvolve框架利用LLMs作为变异操作符，通过进化机制增强模型生成解决方案的能力，这直接关注提升LLM的通用推理和问题解决能力，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：涉及\"mathematical reasoning\"和\"competitive programming solutions\"，属于通用推理能力范畴 - 训练方法：提出\"evolutionary agentic harnesses\"和\"bandit-based LLM ensemble selection strategy\"，属于进化学习方法 - 新兴范式：包含\"agentic harnesses\"，与LLM-based agents相关 第三步排除标准：论文不符合任何排除标准。虽然提到了圆打包、数学推理等应用场景，但这些是作为评估框架通用性的示例，而非论文的主要焦点。论文核心是提出通用的程序进化框架，而非针对特定领域应用。 第四步特殊情况处理：论文提出的\"evolutionary agentic harnesses\"是一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非针对特定领域的应用，因此符合保留条件。 综合分析，ShinkaEvolve的核心贡献是提出了一种新的进化框架，通过创新的采样和集成选择策略，提高LLM在程序进化方面的样本效率和解决方案质量，这直接服务于提升大语言模型的通用推理能力，符合研究目标。",
                    "summary2": "本文旨在解决当前LLM驱动的科学发现方法中存在的样本效率低下和闭源限制问题。针对程序进化任务，我们提出了ShinkaEvolve框架，通过三种关键创新（自适应父程序采样、代码新颖性拒绝采样和基于bandit的LLM集成选择策略）显著提升样本效率。在circle packing、AIME数学推理、ALE-Bench竞赛编程和MoE负载平衡损失设计等多样任务上，ShinkaEvolve以更少样本实现了state-of-the-art性能，并通过开源发布促进了广泛应用。",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Pluralistic Off-policy Evaluation and Alignment",
                    "arxiv_id": "2509.19333",
                    "authors": "Chengkai Huang, Junda Wu, Zhouhang Xie, Yu Xia, Rui Wang, Tong Yu, Subrata Mitra, Julian McAuley, Lina Yao",
                    "summary": "Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evaluated LLMs, and existing off-policy estimators focus solely on overall utility while ignoring preference pluralism. Extending Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore, remains an open question. Thus, we propose the Pluralistic Off-Policy Evaluation (POPE), the first framework for offline pluralistic preference evaluation and alignment in LLMs. POPE includes a unified reward function that combines (1) a collaborative utility component derived from human preference signals (e.g., upvotes or relevance scores) and (2) a diversity component inspired by entropy-based coverage measures, together reflecting pluralistic alignment. Furthermore, to estimate this reward from logged interactions, we derive decomposable inverse propensity scoring (IPS) estimators that separately evaluate relevance and diversity. Theoretically, we prove that our decomposed IPS estimators establish a lower bound on their variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance pluralistic alignment. Empirical results demonstrate that POPE efficiently enhances pluralistic response generation and maintains the models' general capabilities on downstream tasks",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了POPE（Pluralistic Off-Policy Evaluation）框架，用于解决LLM在多元人类偏好下的评估和对齐问题。从第一步判断来看，论文本质上是关于改进LLM的基础能力（偏好对齐），提出新的评估和优化框架，这符合保留标准。论文明确针对LLM的偏好对齐问题，并涉及到强化学习中的离线策略评估和优化概念，这与第二步中的正面指标部分吻合。论文不符合第三步中的排除标准，它不主要聚焦于多模态、特定应用领域或模型可靠性的应用层面问题。虽然论文没有直接讨论推理、规划或问题解决能力，但它关注的是偏好对齐，这是LLM的一个重要基础能力，良好的偏好对齐是模型展现高质量推理能力的前提。论文提出的框架通过结合人类偏好信号和多样性组件来改进模型的基础能力，这种改进可能间接提升模型在推理和其他任务上的表现。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决LLM中多元化偏好对齐的离线评估问题。针对记录在不同策略下的偏好数据，我们提出了POPE框架，结合协作效用和多样性奖励的统一函数，并通过可分解的反向倾向评分估计器进行评估。在Alpaca-GPT4、电影评论等数据集上通过PL-Score、Pluralistic Coverage等指标验证了其有效性。",
                    "summary_translation": "针对具有多样化人类偏好的大型语言模型(LLMs, 大型语言模型)的个性化偏好对齐(preference alignment, 偏好对齐)，需要能够捕捉多元性(pluralism, 多元性)的评估和对齐方法。大多数现有的偏好对齐数据集是在与被评估的LLMs显著不同的策略下记录的，而现有的离策略估计器(off-policy estimators, 离策略估计器)仅关注整体效用(utility, 效用)，却忽视了偏好多元性(preference pluralism, 偏好多元性)。因此，将离策略评估(Off-Policy Evaluation, OPE)扩展到多元偏好对齐(pluralistic preference alignment, 多元偏好对齐)仍然是一个开放性问题。为此，我们提出了多元离策略评估(Pluralistic Off-Policy Evaluation, POPE)，这是首个用于LLMs离线(offline, 离线)多元偏好评估和对齐的框架。POPE包含一个统一的奖励函数(reward function, 奖励函数)，该函数结合了(1)源自人类偏好信号（例如，点赞或相关性评分）的协作效用组件(collaborative utility component, 协作效用组件)，以及(2)受基于熵的覆盖度量(entropy-based coverage measures, 基于熵的覆盖度量)启发的多样性组件(diversity component, 多样性组件)，共同反映了多元对齐(pluralistic alignment, 多元对齐)。此外，为了从记录的交互中估计此奖励，我们推导出了可分解的反向倾向评分(inverse propensity scoring, IPS)估计器，该估计器分别评估相关性(relevance, 相关性)和多样性(diversity, 多样性)。理论上，我们证明了我们分解的IPS估计器为其方差建立了下界。通过离策略评估的值函数(value function, 值函数)，我们可以直接启用离策略优化(off-policy optimization, 离策略优化)，以进一步增强多元对齐。实证结果表明，POPE有效增强了多元响应生成(pluralistic response generation, 多元响应生成)，并保持了模型在下游任务(downstream tasks, 下游任务)上的通用能力。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是研究RLHF（Reinforcement Learning from Human Feedback）的优化问题，特别是分析了最大熵强化学习在在线RLHF设置中的失败模式，并探讨了SimPO在离线设置中成功的原因。RLHF是提升大语言模型通用能力的关键训练技术，论文研究的是如何改进这一训练方法，属于\"改进LLM的基础能力、提出新的训练范式\"的范畴。论文直接关注强化学习（RLHF）这一训练方法，符合正面指标。同时，论文不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面）。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。",
                    "inspiration_trace": ""
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究范围。首先，从核心判断来看，论文的本质是提升大语言模型的通用推理能力，特别是数学和编程推理能力。论文提出了PromptCoT 2.0框架，通过改进提示合成方法来增强LLM的推理能力，这属于\"改进LLM的基础能力和提出新的训练范式\"的范畴。 其次，论文包含多个正面指标：明确关注\"Large language models (LLMs)\"；核心能力方向是\"reasoning\"，特别是\"math reasoning\"和\"logical reasoning\"；提出了新的训练方法，包括\"Self-Play\"和\"Supervised Fine-Tuning (SFT)\"。 第三，论文不涉及任何排除标准中的领域：没有关注多模态与视觉问题；虽然涉及数学和编程，但这些是通用推理的基础领域而非特定应用领域；也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种可扩展的提示合成框架，通过迭代改进推理过程来生成更难、更多样化的问题，从而提升LLM的推理能力。这种方法从根本上增强了模型的基础推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）正在从对话系统演变为强大的推理工具，用于奥数竞赛和竞技编程等任务。尽管参数扩展（scaling parameters）和测试时计算（test-time computation）推动了进展，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高昂且有限，而现有的合成语料库（synthetic corpora）往往过于简单或范围狭窄。PromptCoT 1.0表明，在提示合成中注入推理过程（rationales）会增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（Expectation-Maximization, EM）循环替代手工设计的启发式方法，其中推理过程被迭代优化以指导提示构建。这产生的问题比之前的语料库更难且更多样化。\n\n这些合成提示支持两种后训练机制：（1）自我对弈（Self-Play），其中强模型通过可验证的反馈在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），其中弱模型从教师蒸馏的轨迹中学习。广泛的实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进结果，在AIME 24/25和HMMT 25上分别提升+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升+6.1和+5.0，在Codeforces上提升+35 Elo。在SFT中，仅使用合成提示训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布上不同的问题。这些结果将提示合成确立为扩展推理的新维度，并将PromptCoT 2.0定位为未来开源模型的可扩展基础。该实现在https://github.com/inclusionAI/PromptCoT上可用。",
                    "inspiration_trace": ""
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心是提出VCRL，一种基于课程学习的强化学习框架，用于提高大语言模型的推理能力。论文本质上是关于改进LLM的基础能力，特别是数学推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。论文包含多个正面指标，如关注LLMs核心概念、数学推理能力方向以及强化学习训练方法。同时，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文主要在数学推理任务上进行实验，但提出的方法是通用的课程学习强化学习框架，通过动态控制训练样本的难度来提高LLM对不同难度样本的学习能力，这与人类从易到难的认知过程一致，可以推广到其他需要推理能力的任务。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（Policy-based reinforcement learning）目前在提升大语言模型（LLMs）数学推理能力方面发挥着重要作用。然而，现有的基于展开的强化学习方法（rollout-based reinforcement learning methods）（如GRPO、DAPO、GSPO等）未能明确考虑大语言模型对不同难度样本的学习能力，这与人类从易到难的数学推理任务认知过程相悖。直观上，我们发现RLVR中展开组（rollout group）的奖励方差部分反映了当前样本对大语言模型的难度。过易或过难的样本具有较低的方差，而难度适中的样本具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准测试和两个模型上的实验揭示了VCRL相较于当前大语言模型强化学习基线方法的优势。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
                    "arxiv_id": "2509.20175",
                    "authors": "Lorenzo Giusti, Ole Anton Werner, Riccardo Taiello, Matilde Carvalho Costa, Emre Tosun, Andrea Protani, Marc Molina, Rodrigo Lopes de Almeida, Paolo Cacace, Diogo Reis Santos, Luigi Serio",
                    "summary": "We present Federation of Agents (FoA), a distributed orchestration framework that transforms static multi-agent coordination into dynamic, capability-driven collaboration. FoA introduces Versioned Capability Vectors (VCVs): machine-readable profiles that make agent capabilities searchable through semantic embeddings, enabling agents to advertise their capabilities, cost, and limitations. Our aarchitecturecombines three key innovations: (1) semantic routing that matches tasks to agents over sharded HNSW indices while enforcing operational constraints through cost-biased optimization, (2) dynamic task decomposition where compatible agents collaboratively break down complex tasks into DAGs of subtasks through consensus-based merging, and (3) smart clustering that groups agents working on similar subtasks into collaborative channels for k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe semantics for scalable message passing, FoA achieves sub-linear complexity through hierarchical capability matching and efficient index maintenance. Evaluation on HealthBench shows 13x improvements over single-model baselines, with clustering-enhanced laboration particularly effective for complex reasoning tasks requiring multiple perspectives. The system scales horizontally while maintaining consistent performance, demonstrating that semantic orchestration with structured collaboration can unlock the collective intelligence of heterogeneous federations of AI agents.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Federation of Agents (FoA)\"的分布式编排框架，用于实现大规模智能体AI的动态协作。从本质上看，论文属于\"智能体协作框架\"的研究范畴，符合筛选标准中的保留条件。论文提出的版本化能力向量(VCVs)、语义路由、动态任务分解和智能聚类等创新方法，都是为了提升智能体系统的通用协作和推理能力，而非将LLM作为工具应用到特定领域。 论文在正面指标上表现良好，涉及了\"multi-agent systems\"这一新兴范式，并明确提到该系统在\"complex reasoning tasks\"上表现出色，这与\"通用推理能力\"的研究目标直接相关。虽然论文没有直接提及\"Large language models\"，但智能体系统通常基于LLM构建，且论文关注的是通用能力的提升。 在排除标准方面，论文没有主要关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然评估中使用了HealthBench数据集，但这仅用于验证系统性能，论文本身并非针对医疗等特定领域的研究。 综合分析，这篇论文提出的是一种通用的智能体协作框架，旨在通过语义感知的通信机制增强智能体系统的协作和推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在解决大规模多智能体AI系统中能力发现和动态协调的问题。针对异构智能体协作场景，我们提出了一种Federation of Agents (FoA)语义感知通信框架，通过Versioned Capability Vectors实现能力驱动的动态编排，并在HealthBench数据集上通过任务完成质量指标验证了其有效性，相比单模型基线实现了13倍的性能提升。",
                    "summary_translation": "我们提出了代理联盟（Federation of Agents, FoA），这是一个分布式编排框架（distributed orchestration framework），能够将静态的多智能体协调转变为动态的、由能力驱动的协作。FoA引入了版本化能力向量（Versioned Capability Vectors, VCVs）：这是一种机器可读的配置文件，通过语义嵌入（semantic embeddings）使智能体能力可被搜索，使智能体能够宣传其能力、成本和局限性。我们的架构结合了三个关键创新：(1) 语义路由（semantic routing），它在分片的HNSW索引上将任务匹配到智能体，同时通过成本偏置优化（cost-biased optimization）强制执行操作约束；(2) 动态任务分解（dynamic task decomposition），其中兼容的智能体通过基于共识的合并（consensus-based merging）协作地将复杂任务分解为有向无环图（DAGs）的子任务；以及(3) 智能聚类（smart clustering），它将处理相似子任务的智能体分组到协作通道中，在综合之前进行k轮细化。FoA建立在MQTT的发布-订阅语义（publish-subscribe semantics）之上，以实现可扩展的消息传递，并通过分层能力匹配（hierarchical capability matching）和高效的索引维护（efficient index maintenance）实现了次线性复杂度（sub-linear complexity）。在HealthBench上的评估显示，与单模型基线相比有13倍的改进，其中聚类增强的协作（clustering-enhanced collaboration）对于需要多视角的复杂推理任务特别有效。该系统能够水平扩展（scales horizontally）同时保持一致的性能，表明具有结构化协作的语义编排（semantic orchestration）可以释放异构AI代理联盟（heterogeneous federations of AI agents）的集体智能。",
                    "inspiration_trace": ""
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是提出UserRL框架，通过强化学习训练用户中心的智能体，这属于\"智能体协作框架\"的范畴，是改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。其次，论文包含多个正面指标：明确使用了大语言模型(Qwen3)，采用了强化学习(RL)方法训练模型，研究了基于LLM的智能体(agentic models)，并关注动态多轮交互能力，这些都符合研究目标。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。最后，虽然论文涉及智能体研究，但它提出的是通用的智能体训练框架，而非针对特定领域的应用，因此应予以保留。论文的核心贡献在于探索如何通过奖励塑造和用户模拟选择来提升智能体的通用交互能力，这与提高大语言模型通用推理能力的研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习 (Reinforcement learning, RL) 在训练智能体模型 (agentic models) 方面显示出潜力，这些模型能够超越静态基准测试 (static benchmarks)，进行动态、多轮交互 (dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这一场景中，用户交互的多样性和动态性带来了挑战。在这项工作中，我们提出了UserRL，这是一个通过标准化的gym环境 (gym environments) 配合模拟用户 (simulated users) 来训练和评估以用户为中心能力 (user-centric abilities) 的统一框架。我们系统性地改变轮级奖励分配 (turn-level reward assignment) 和轨迹级分数计算 (trajectory-level score calculation)，以分析不同表述形式如何影响GRPO算法 (GRPO algorithm) 下的学习效果。我们在Qwen3模型 (Qwen3 models) 上的实验揭示了三个关键发现：(i) SFT冷启动 (SFT cold start) 对于解锁初始交互能力和实现持续的RL改进至关重要；(ii) 精心设计的轨迹评分 (deliberate trajectory scoring) 能够产生更高效且有效的多轮交互；(iii) 虽然更强大的模拟用户（如GPT-4o）能够促进训练，但开源模拟器（如Qwen3-32B）仍然是一种经济高效且可迁移的选择。总体而言，这些结果强调，奖励塑造 (reward shaping) 和用户模拟选择 (user simulation choice) 的精心设计与模型规模 (model scale) 同样重要，并将UserRL确立为开发强大的以用户为中心的智能体模型 (robust user-centric agentic models) 的实用途径。所有代码和数据均公开，以供未来研究使用。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心贡献是研究大语言模型在认知负荷下的多跳推理能力限制。根据筛选标准，我的判断过程如下： 第一步：核心判断——论文本质是研究LLM本身的推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估基准(ICE)来测试LLM在多跳推理任务上的表现，特别是在认知负荷条件下的性能变化。这不是将LLM应用于特定领域，而是直接研究LLM的基础推理能力，属于改进LLM通用能力的研究。 第二步：正面指标——论文包含关键正面指标：(1)核心概念：明确研究Large Language Models (LLMs)；(2)能力方向：聚焦于multi-hop reasoning（多跳推理），属于逻辑推理范畴。虽然未涉及训练方法和新兴范式，但这两个核心正面指标已足够表明论文与研究方向高度相关。 第三步：排除标准——论文不涉及任何需要排除的领域。它没有研究多模态与视觉问题，没有聚焦于特定应用领域（如医疗、化学等），也没有从应用层面研究模型可靠性。 第四步：特殊和模糊情况——论文提到\"hallucination-as-guessing under uncertainty\"，这是从认知机制角度解释幻觉现象，探讨其与推理能力的关系，而非仅进行社会学研究或应用层面讨论，这有助于理解LLM推理能力的本质限制。 综合来看，这篇论文通过研究认知负荷对LLM推理能力的影响，提出了新的评估方法和理论框架，直接服务于提升LLM通用推理能力的研究目标，完全符合筛选要求。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（LLMs）的扩展揭示了它们在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷（cognitive load）下推理的计算限制仍然知之甚少。在本研究中，我们提出了一个计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换造成的干扰（Attentional Residue，注意力残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令微调（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.2）表现出基线脆弱性（baseline brittleness），在这个高内在负荷（high-intrinsic-load）任务的所有条件下（包括干净的对照组）实现了0%的准确率（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性（partial resilience），在对照条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的性能下降（$\\beta = -0.003$每%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，动态的、具有认知意识的压力测试（cognitive-aware stress testing），如ICE基准测试所示，对于评估先进AI系统的真正韧性和安全性至关重要。",
                    "inspiration_trace": ""
                },
            ]
        },
    ],
    "2025-09-23": [
        {
            "name": "Artificial Intelligence",
            "count": 13,
            "papers": [
                {
                    "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
                    "arxiv_id": "2509.19236",
                    "authors": "Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang",
                    "summary": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
                    "category": "cs.AI",
                    "filter_reason": "我根据筛选标准对这篇论文进行了全面分析，认为它符合研究范围。以下是详细判断过程： 第一步核心判断：这篇论文的核心是提出AgentInit，一种用于初始化基于大语言模型的多智能体系统的方法。它通过多样性和专业性的协调来优化智能体团队的结构，从而提高系统协作效率和问题解决能力。这属于\"智能体协作框架\"的研究范畴，旨在增强大语言模型的通用能力，而非将其作为工具应用于特定领域。因此，根据第一步标准，应予以保留。 第二步正面指标：论文包含多个相关主题。首先，核心概念方面明确研究\"LLM-based Multi-Agent Systems\"；其次，在新兴范式方面，论文聚焦于多智能体系统的研究；虽然论文没有直接提及推理、规划或强化学习等术语，但其优化多智能体协作的目标本质上是为了提升系统的问题解决能力，这与通用推理能力相关。 第三步排除标准：论文不涉及任何排除领域。它没有关注多模态与视觉内容，不针对医疗、化学、生物等特定应用领域，也不讨论模型可靠性方面如水印、安全等问题。相反，论文强调其方法在\"各种框架和任务\"上的适用性，表明其通用性。 第四步特殊情况处理：论文研究的是智能体协作框架，但提出的是一种通用的初始化方法，而非针对特定领域的应用。它旨在通过优化智能体团队的初始化来增强LLM在多智能体环境下的通用协作能力，因此符合保留条件。 综上所述，这篇论文的核心贡献是提出一种通用的多智能体系统初始化方法，通过优化团队结构和协作机制来增强大语言模型的通用问题解决能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决多智能体系统(MAS)初始化过程中智能体协作效率低下的问题。针对现有方法未充分考虑智能体协作需求的问题，我们提出了一种AgentInit方法，通过标准化智能体生成和平衡团队选择两个模块，联合优化智能体多样性和任务相关性。在MMLU、GSM8K等多个基准测试上验证，AgentInit性能提升高达1.2和1.6，同时显著降低token消耗，展现出强大的迁移能力和适应性。",
                    "summary_translation": "合理的初始化对任何系统都至关重要，尤其是在多智能体系统（multi-agent systems, MAS）中，初始化在决定系统效率与有效性方面起着关键作用。然而，现有的MAS初始化方法未能充分考虑所生成智能体在后续阶段中的协作需求。受高效团队构成原则的启发，我们提出了AgentInit，旨在优化智能体团队的结构。具体而言，除了在智能体生成过程中引入多轮智能体间的交互与反思机制外，AgentInit还引入了一种自然语言到格式化输出的机制（Natural Language to Format mechanism），以确保输出的一致性与标准化。随后，采用基于帕累托原则（Pareto principles）的平衡团队选择策略，综合考虑智能体团队的多样性（diversity）与任务相关性（task relevance），以促进高效且有效的协作，提升系统整体性能。实验结果表明，AgentInit在多种框架和任务下均持续优于当前最先进的初始化方法和预定义策略，整体性能分别提升了最高达1.2和1.6倍，同时显著降低了token消耗。进一步分析验证了其在相似任务中的强可迁移性，并确认了其核心组件的有效性，展示了AgentInit作为一种可靠MAS初始化方法的卓越能力与适应性。源代码与模型可在 https://github.com/1737423697/AgentInit 获取。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Code Driven Planning with Domain-Adaptive Critic",
                    "arxiv_id": "2509.19077",
                    "authors": "Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing Hu, Yunji Chen",
                    "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究课题。根据筛选标准，我进行了如下分析： 第一步核心判断：论文的本质是改进LLM的规划能力（planning），这属于通用推理能力的核心组成部分。论文提出CoPiC方法，通过让LLM生成高级规划程序并配合领域自适应批评者来提升规划质量，这是对LLM基础能力的增强，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注Large Language Models (LLMs)作为任务规划器 - 能力方向：重点研究planning能力，这是推理能力的重要组成部分 - 新兴范式：涉及LLM-based agents，将LLM用于AI智能体的序列决策问题 第三步排除标准：论文不聚焦于排除的领域： - 虽然在ALFWorld、NetHack和StarCraft II等特定环境中进行实验，但这些仅作为验证方法有效性的测试平台，论文核心是提出通用规划框架，而非针对特定领域的应用 - 不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究 第四步特殊情况处理：论文提出的是一种通用的智能体规划框架，旨在提升LLM的规划能力，减少查询成本并提高与长期奖励的一致性，这属于应保留的情况。 论文的核心贡献是CoPiC方法，它通过减少LLM查询频率并引入领域自适应批评者来评估候选计划，从而提升LLM的规划能力。这直接关系到提升LLM的通用推理能力，特别是规划和决策方面的能力，符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型(LLMs)作为任务规划器时通用知识与特定环境需求不匹配导致的高查询成本和长期规划问题。针对复杂决策环境，我们提出了一种Code Driven Planning with Domain-Adaptive Critic (CoPiC)框架，结合LLM生成的多样化规划程序和领域自适应评论器选择最优计划，并在ALFWorld、NetHack和StarCraft II Unit Building环境中通过成功率和token成本验证了其有效性。",
                    "summary_translation": "大语言模型（Large Language Models, LLMs）因其丰富的世界知识，已被广泛用作人工智能代理在序贯决策问题中的任务规划器。然而，LLMs的通用知识与特定环境需求之间存在差距，常常导致生成的规划不准确。为应对这一问题，现有方法依赖频繁调用LLM，根据即时环境反馈迭代优化规划，但这种方式带来了高昂的查询成本。此外，此类优化通常仅依赖短期环境反馈，限制了LLM生成与长期奖励一致的规划能力。\n\n本文提出**基于代码驱动与领域自适应评判器的规划方法**（Code Driven Planning with Domain-Adaptive Critic, CoPiC）。与频繁查询LLM的方法不同，CoPiC利用LLM生成一组多样化的高层规划程序（high-level planning programs），这些程序可迭代地生成并优化候选规划。随后，一个经过训练的领域自适应评判器（domain-adaptive critic）对这些候选规划进行评估，并选择最符合长期奖励的方案予以执行。通过将高层规划程序作为规划器、领域自适应评判器作为评估器，CoPiC在显著降低查询成本的同时提升了规划质量。\n\n在ALFWorld、NetHack和StarCraft II Unit Building三个基准任务上的实验结果表明，CoPiC优于当前先进的基于LLM的基线方法AdaPlanner和Reflexion，平均实现了（1）23.33%的成功率提升，以及（2）91.27%的查询成本降低。",
                    "inspiration_trace": ""
                },
                {
                    "title": "LongCat-Flash-Thinking Technical Report",
                    "arxiv_id": "2509.18883",
                    "authors": "Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, Yang Liu, Yang Liu, Yang Zheng, Yaoming Wang, Yaqi Huo, Yerui Sun, Yifan Lu, Yiyang Li, Youshao Xiao, Yuanzhe Lei, Yuchen Xie, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunke Zhao, Yuqing Ding, Yuwei Jiang, Zhaohua Yang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhongda Su, Ziran Li, Ziwen Wang, Ziyuan Zhuang, Zongyu Wang, Zunyuan Yang",
                    "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合研究范围，其核心贡献是提升大语言模型本身的通用推理能力。首先，论文的本质是关于改进LLM的基础推理能力，提出了新的训练范式，包括长思维链(CoT)数据冷启动和大规模强化学习(RL)方法，这直接符合筛选标准中的保留条件。其次，论文包含了多个正面指标：明确研究大语言模型(LLMs)的推理能力，涉及强化学习训练方法，并探讨了智能体推理(agentic reasoning)这一新兴范式。第三，论文没有聚焦于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。特别值得注意的是，虽然论文提到了STEM、代码和智能体等不同领域，但这是作为其领域并行训练方案的一部分，目的是增强模型的通用推理能力，而非将LLM应用于特定领域解决问题。论文的核心创新——领域并行训练方案和DORA系统——都是为了提升模型的基础推理能力，使其在复杂推理任务上达到最先进性能。因此，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在提升大型语言模型的推理能力。针对复杂推理任务场景，我们提出了LongCat-Flash-Thinking，一个5600亿参数的MoE推理模型，通过长链思维数据冷启动和大规模强化学习培养高级推理能力，核心创新包括领域并行训练方案和DORA系统。在HMMT-25、ARC-AGI、τ²-Bench等多个推理基准上通过Mean@32、Pass@1等指标验证了其有效性，在AIME-25上实现了64.5%的token消耗降低。",
                    "summary_translation": "我们提出了LongCat-Flash-Thinking，一个高效的5600亿参数开源专家混合（Mixture-of-Experts, MoE）推理模型。其先进能力是通过精心设计的训练过程培养的，从长思维链（Chain-of-Thought, CoT）数据冷启动开始，到大规模强化学习（Reinforcement Learning, RL）结束。我们首先采用精心设计的冷启动训练策略，显著提升了推理潜力，并使模型具备形式推理和智能体推理（agentic reasoning）的专业技能。然后，核心创新是我们的领域并行训练方案，该方案解耦了不同领域（如STEM、代码、智能体）的优化，随后将产生的专家模型融合成单一的接近帕累托最优（Pareto-optimal）模型。整个过程由我们的动态异步编排（Dynamic ORchestration for Asynchronous rollout, DORA）系统驱动，这是一个大规模强化学习框架，在数万个加速器上比同步方法实现了超过三倍的训练加速。因此，LongCat-Flash-Thinking在一系列复杂推理任务上实现了开源模型中的最先进性能。该模型在智能体推理方面表现出卓越的效率，在AIME-25上平均令牌消耗减少了64.5%（从19,653降至6,965），同时不降低任务准确性。我们发布LongCat-Flash-Thinking以促进推理系统和智能体人工智能（agentic AI）研究的进一步发展。",
                    "inspiration_trace": ""
                },
                {
                    "title": "MAPO: Mixed Advantage Policy Optimization",
                    "arxiv_id": "2509.18849",
                    "authors": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
                    "summary": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种名为MAPO（Mixed Advantage Policy Optimization）的新强化学习策略，用于改进GRPO（Group Relative Policy Optimization）方法，以解决其在推理任务中遇到的advantage reversion和advantage mirror问题。论文通过引入advantage percent deviation和动态重加权advantage function来优化不同查询样本间的advantage分配，从而提升基础模型在推理任务上的性能。 这完全符合研究目标中的\"改进LLM的基础能力\"和\"提出新的训练范式\"，特别是强化学习优化方面的研究。论文明确关注\"reasoning tasks\"，这是筛选标准中的核心能力方向之一。同时，论文属于强化学习（RL）训练方法的研究，这也是正面指标中明确提到的重要内容。 论文不属于任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。它专注于提升基础模型在通用推理任务上的性能，而不是将模型应用到特定领域。因此，这篇论文应该被保留，它对提升大语言模型的通用推理能力有直接贡献。",
                    "summary2": "本文旨在解决GRPO中优势函数面临的\"优势反转\"和\"优势镜像\"问题。针对不同轨迹确定性的样本，我们提出了一种混合优势策略优化(MAPO)方法，通过引入优势百分比偏差处理高确定性轨迹，并基于轨迹确定性动态重新加权优势函数。在Geo3K和EmoSet等多个数据集上使用Qwen2.5-VL-7B架构，通过准确率等指标验证了MAPO能够有效提升基础模型在推理任务上的稳定性和准确性。",
                    "summary_translation": "基础模型（foundation models）在强化学习领域的最新进展，如群组相对策略优化（Group Relative Policy Optimization, GRPO），显著提升了基础模型在推理任务上的性能。值得注意的是，优势函数（advantage function）作为GRPO中的核心机制，用于排序轨迹（trajectory）的重要性。然而，现有探索同时遇到优势反转（advantage reversion）和优势镜像（advantage mirror）问题，这些问题阻碍了在不同查询样本（query samples）间进行合理的优势分配。在这项工作中，我们提出了一种简单但有效的GRPO策略，即混合优势策略优化（Mixed Advantage Policy Optimization, MAPO）。我们揭示了轨迹以不同的确定性（certainty）出现，并为具有高确定性轨迹的样本提出了优势百分比偏差（advantage percent deviation）。此外，我们对具有不同轨迹确定性的样本动态重新加权优势函数，从而自适应地配置优势函数以考虑样本特定特征。与相关最先进（state-of-the-art）方法的比较，以及对不同优势变体（advantage variants）的消融研究（ablation studies），验证了我们方法的有效性。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
                    "arxiv_id": "2509.18868",
                    "authors": "Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang, Sheng Li",
                    "summary": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究大语言模型的记忆机制，提出了统一的操作定义、四部分分类法（参数化、上下文、外部、程序性/情景性）和记忆四元组（位置、持久性、写入/访问路径、可控性）。虽然论文没有直接讨论推理能力，但记忆是推理的基础能力之一，模型需要有效记忆信息并在需要时检索使用，才能进行复杂的推理任务。论文链接了机制、评估和治理，提出了三设置协议和分层评估框架，以及更新和遗忘策略（如DMM Gov），这些都是为了增强LLM的基础能力，从而间接提升其通用推理能力。论文不符合排除标准，没有聚焦于多模态、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。",
                    "summary2": "本文旨在 [建立大语言模型记忆的统一分析框架，解决记忆概念边界模糊和评估碎片化等挑战]。针对 [LLM记忆全生命周期]，我们提出了一种 [四分类记忆类型（参数、上下文、外部和程序/情景记忆）和三设置并行评估协议]，并在 [多种公开数据集和benchmark] 上通过 [分层评估指标和DMM-Gov动态治理框架] 验证了其有效性。",
                    "summary_translation": "在统一的操作定义下，我们将LLM记忆（大语言模型记忆）定义为在预训练、微调或推理过程中写入的持久状态，该状态可被后续访问并稳定影响输出。我们提出了一个四部分分类法（参数型parametric、上下文型contextual、外部型external、程序型/情景型procedural/episodic）和一个记忆四元组（位置location、持久性persistence、写入/访问路径write/access path、可控性controllability）。我们通过写入->读取->抑制/更新的链条将机制、评估和治理联系起来。为避免在不同设置间产生扭曲的比较，我们采用了三设置协议（仅参数型parametric only、离线检索offline retrieval、在线检索online retrieval），该协议在同一数据和时间线上将能力与信息可用性解耦。在此基础上，我们构建了分层评估：参数型评估（闭卷召回closed-book recall、编辑差异edit differential、记忆化/隐私memorization/privacy）、上下文型评估（位置曲线和序列中段下降position curves and the mid-sequence drop）、外部型评估（答案正确性与片段归因/忠实性answer correctness vs snippet attribution/faithfulness），以及程序型/情景型评估（跨会话一致性和时间线重放cross-session consistency and timeline replay, E MARS+）。该框架整合了时间治理和泄漏审计（新鲜度命中freshness hits、过时答案outdated answers、拒绝切片refusal slices）以及通过评估者间一致性加上多重比较校正的配对测试进行的不确定性报告。针对更新和遗忘，我们提出了DMM Gov（动态记忆管理治理）：协调DAPT/TAPT（领域自适应预训练/任务自适应预训练）、PEFT（参数高效微调）、模型编辑（ROME, MEND, MEMIT, SERAC）和RAG（检索增强生成），形成一个可审计的循环，涵盖准入阈值、部署、监控、回滚和变更审计，并规定了及时性、冲突处理和长期一致性的规范。最后，我们提出了四个可测试的命题：最小可识别性；最小评估卡片；具有可验证遗忘的因果约束编辑；以及何时小窗口重放的检索优于超长上下文阅读。这产生了一个可重现、可比较和可治理的研究与部署坐标系。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
                    "arxiv_id": "2509.18771",
                    "authors": "Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du",
                    "summary": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。在第一步核心判断中，论文本质上是提出一种名为\"经验缩放\"(Experience Scaling)的新框架，用于大语言模型部署后的持续进化，这直接符合\"改进LLM的基础能力、提出新的训练范式\"的保留标准。论文关注LLM通过自主与环境交互和协作共享积累的经验来提升自身能力，这是一种增强LLM通用推理能力的方法论研究。 在第二步正面指标分析中，论文明确包含核心概念\"Large Language Models (LLMs)\"；涉及训练方法中的\"evolution\"和\"self-evolve\"概念（标题中直接提及\"Post-Deployment Evolution\"）；同时提到\"autonomous interaction with the environment\"暗示了agent概念；论文强调\"对以前未见但相关任务的泛化\"，这间接涉及到推理和问题解决能力。 在第三步排除标准检查中，论文完全不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 在第四步特殊情况处理中，论文提到的\"autonomous interaction with the environment\"属于通用智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种让LLM通过部署后的持续学习和经验积累来提升自身通用能力的框架，这种方法超越了静态人类生成数据的限制，为提升LLM的通用推理能力提供了新的路径，完全符合我的研究目标。",
                    "summary2": "本文旨在解决大型语言模型（LLM）在传统扩展方法达到饱和后的持续进化问题。针对部署后的LLM与环境交互数据，我们提出了一种Experience Scaling框架，通过捕获原始交互、提炼为可重用知识并定期优化存储内容，实现LLM的自主进化。在MMLU和SciQ基准上通过准确性、时间效率和泛化能力等指标验证了其有效性，结果表明该框架能提高准确性、维持长期性能并在新场景中保持收益。",
                    "summary_translation": "扩大模型规模、训练数据和计算能力推动了大型语言模型（large language models, LLMs）的进步，但随着人类生成文本的耗尽和进一步收益的减少，这些方法正达到饱和状态。我们提出了经验扩展（experience scaling）框架，这是一个通过与环境自主交互和协作共享积累经验，实现大型语言模型部署后持续进化的框架。该框架捕获原始交互，将其提炼为紧凑、可重用的知识，并定期优化存储内容以保持相关性和效率。我们在模拟的真实世界场景中验证了该框架，这些场景涉及对以前未见但相关任务的泛化、重复性查询和过度饱和的知识存储。在所有设置中，经验扩展（experience scaling）提高了准确性，随时间推移保持性能，并在应用于新情况时保持收益。这些结果表明，结构化的部署后学习可以将大型语言模型的能力扩展到静态人类生成数据的限制之外，为持续智能进步提供了一条可扩展的路径。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
                    "arxiv_id": "2509.18565",
                    "authors": "Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita",
                    "summary": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，该论文的本质是提升LLM的数学推理能力，属于改进LLM基础能力和增强其逻辑、数学推理等通用能力的范畴。论文提出了一种结合方程生成和估计验证的新方法，通过外部符号方程求解器和LLM的估计能力相互验证，提高数学问题求解的准确性，这直接针对LLM在推理方面的能力提升。 其次，论文满足多个正面指标：核心概念明确涉及Large language models (LLMs)；能力方向专注于数学推理(math reasoning)，这正是通用推理能力的重要组成部分；同时采用了工具使用(tool use)的新兴范式，即使用外部符号方程求解器来辅助LLM解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学文字问题，但数学推理被视为基础能力而非特定应用领域；也没有涉及模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文中使用的外部符号方程求解器作为工具，是为了增强LLM的通用数学问题解决能力，而不是应用在特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种提升LLM数学推理能力的新方法，通过估计验证和方程生成相结合的方式，显著提高了LLM在解决数学文字问题上的表现，这完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。",
                    "summary2": "本文旨在提高大型语言模型解决数学应用题的准确性。针对数学应用题求解场景，我们提出了一种EVoSS方法，结合方程生成和估计验证技术，通过符号求解器获得精确解后，利用估计值进行验证，并在GSM8K、SVAMP和Algebra等数据集上通过精确匹配指标验证了其有效性，实现了平均近2%的性能提升，并创建了新的Trig300数据集用于三角函数问题测试。",
                    "summary_translation": "\n大型语言模型（Large Language Models, LLMs）在各种任务上表现出色，包括问题解决和问答。然而，大型语言模型通常发现数学应用题（Math Word Problems, MWPs）具有挑战性，因为解决这些问题需要一系列推理和数学能力，而这些能力似乎是大型语言模型所欠缺的。近来的努力通过改进提示词（prompts）帮助大型语言模型解决更复杂的数学应用题。本研究提出了一种新方法，首先提示大型语言模型从问题的分解中创建方程，然后使用外部符号方程求解器（symbolic equation solver）来产生答案。为了确保获得答案的准确性，受数学教师既定建议的启发，大型语言模型被指示第二次解决数学应用题，但这次的目标是估计正确答案，而不是精确求解。然后将估计值与生成的答案进行比较以进行验证。如果验证失败，则采用迭代修正过程（iterative rectification process）以确保最终找到正确答案。这种方法在先前发表的关于数值和代数数学应用题的研究所使用的数据集上取得了新的最先进结果，平均将之前的最佳结果提高了近两个百分点。此外，该方法在三角数学应用题（trigonometric MWPs）上取得了令人满意的结果，据作者所知，这是之前未曾尝试过的任务。本研究还介绍了两个新数据集SVAMPClean和Trig300，以进一步推进对大型语言模型推理能力的测试。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?",
                    "arxiv_id": "2509.18383",
                    "authors": "Moran Feldman, Amin Karbasi",
                    "summary": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the Gödel Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the Gödel Test.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是评估和探讨大语言模型(GPT-5)的数学推理能力，特别是解决简单数学猜想的能力，这直接涉及LLM的基础推理能力提升，而非将LLM作为工具应用于特定领域。论文提出的\"Gödel Test\"旨在评估模型产生正确证明的能力，这属于通用推理能力的核心范畴。 其次，论文满足多个正面指标：明确以大型语言模型(LLMs)为研究对象；专注于推理能力(reasoning)，特别是数学推理(math reasoning)；涉及问题解决(problem-solving)等关键能力方向。 第三，论文不符合任何排除标准。虽然研究内容涉及数学领域，但数学在这里是作为评估LLM推理能力的测试场，而非作为特定应用领域。论文的核心不是解决数学问题本身，而是研究LLM的推理能力表现和局限。 论文的核心贡献在于提出了一种评估LLM高级推理能力的新方法(Gödel Test)，并通过实验揭示了当前模型在数学推理方面的进展、原创性闪光点以及局限性。这直接服务于提升LLM通用推理能力的研究目标，因此应该被保留。",
                    "summary2": "本文旨在评估大型语言模型解决简单数学猜想的能力。针对组合优化领域中的五个未解决猜想，我们提出了Gödel Test评估框架，并通过检查GPT-5生成的证明正确性验证了其性能。实验表明GPT-5在需要单一推理路径的问题上表现良好，但在需要跨论文综合推理的问题上存在明显局限。",
                    "summary_translation": "前沿AI模型实验室最近的公告强调了在高中和本科数学竞赛中取得的显著成果。然而，大语言模型（large language models）是否能够解决更高阶数学领域中的新颖简单猜想（conjectures），目前仍不清楚。我们提出了哥德尔测试（Gödel Test）：评估模型是否能够为非常简单但先前未解决的猜想（previously unsolved conjectures）产生正确的证明。为此，我们研究了GPT-5在组合优化（combinatorial optimization）领域中的五个猜想上的表现。对于每个问题，我们提供了一到两篇提出该猜想的源论文（source papers），保留了我们自己的猜想，然后详细评估了模型的推理过程。\n\n在三个较简单的问题上，GPT-5产生了几乎正确的解决方案；对于问题2，它甚至推导出了一个不同的近似保证（approximation guarantee），经过检查，这一保证反驳了我们的猜想，同时提供了一个有效的解决方案。该模型在问题4上失败了，该问题需要结合两篇论文的结果。在问题5上，这是一个没有已验证猜想（validated conjecture）的更难案例，GPT-5提出了与我们心中相同的算法（algorithm），但在分析（analysis）环节失败了，这表明证明比预期的更具挑战性。\n\n尽管我们的样本量很小，但结果表明在常规推理（routine reasoning）方面取得了有意义的进展，偶尔展现出创造性闪光（flashes of originality），而在需要跨论文综合（cross-paper synthesis）时则存在明显的局限性。GPT-5可能是前沿模型最终通过哥德尔测试（Gödel Test）的早期一步。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
                    "arxiv_id": "2509.18382",
                    "authors": "Adarsha Balaji, Le Chen, Rajeev Thakur, Franck Cappello, Sandeep Madireddy",
                    "summary": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心是研究如何在计算约束条件下提升大型推理模型的推理能力和安全性，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文主要探讨了通过思维链（CoT）和强化学习（LCPO方法）来优化模型的推理能力，这属于改进LLM基础能力和提出新训练范式的范畴。论文明确关注推理能力这一核心指标，并研究了如何通过强化学习方法来控制推理长度，同时保持模型的安全性。论文不是将LLM作为工具应用到特定领域，而是关注LLM本身的通用推理能力提升。虽然论文涉及安全性研究，但这是从提升模型内在推理质量和可靠性的角度出发，而非仅作为应用层面的防御。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决计算约束下大型推理模型（LRMs）的安全性与技能推理平衡问题。针对推理长度和计算资源限制场景，我们提出了两种计算约束策略：基于强化学习的长度控制策略优化（LCPO）和模型量化（GPTQ），并在科学、数学和安全推理基准（GPQA, MATH500, AIME, StrongReject）上通过pass@1和safe@1指标验证了其有效性。",
                    "summary_translation": "测试时计算扩展(test-time compute scaling)已证明能够通过生成更长的思维链(chain-of-thought, CoT)序列来提高推理语言模型(reasoning language models)的性能。然而，这种性能提升伴随着计算成本(computational cost)的显著增加。在本研究中，我们调查了两种计算约束策略(compute constraint strategies)：(1)推理长度约束(reasoning length constraint)和(2)模型量化(model quantization)，作为降低推理模型计算需求并研究其对安全性能(safety performance)影响的方法。具体而言，我们探索了两种将计算约束应用于推理模型的方法：(1)使用基于长度控制策略优化(length controlled policy optimization, LCPO)的强化学习(reinforcement learning)方法微调推理模型，以满足用户定义的CoT推理长度(user-defined CoT reasoning length)；(2)应用量化技术(quantization)以在用户定义的计算约束(user-defined compute constraint)内最大化CoT序列的生成。此外，我们还研究了模型的计算效率(computational efficiency)与安全性(safety)之间的权衡(trade-off)。",
                    "inspiration_trace": ""
                },
                {
                    "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
                    "arxiv_id": "2509.18851",
                    "authors": "Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu",
                    "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出NGRPO算法，一种改进的强化学习方法，用于增强大语言模型的推理能力。论文针对GRPO算法在处理同质响应组（全部正确或全部错误）时的局限性，提出了两个关键机制：Advantage Calibration和Asymmetric Clipping，使模型能够从同质错误中学习。这直接符合研究目标，因为：(1)论文本质上是改进LLM的基础推理能力，特别是数学推理能力；(2)它提出新的训练范式（强化学习优化）；(3)实验证明该方法能显著提升LLM在多个数学基准测试上的表现。论文不涉及特定应用领域、多模态研究或模型基础设施优化，完全符合\"致力于提高大语言模型本身的通用推理能力\"的核心目标。",
                    "summary2": "本文旨在解决 GRPO 算法在处理同质响应组时无法学习的问题。针对同质错误组导致零梯度的问题，我们提出了一种 NGRPO 方法，通过 Advantage Calibration 和 Asymmetric Clipping 两个核心机制将同质错误转换为稳健学习信号，并在 Qwen2.5-Math-7B 模型上通过 MATH500、AMC23 和 AIME2025 数学基准测试的 Pass@k AUC 指标验证了其有效性。",
                    "summary_translation": "RLVR (强化学习与验证推理) 已增强大型语言模型 (LLMs) 在各种任务中的推理能力。然而，作为代表性RLVR算法的GRPO (组相对策略优化) 存在一个关键限制：当组内所有响应要么完全正确要么完全错误时，模型无法从这些同质响应中学习。对于同质错误组，这一问题尤为严重，因为GRPO的优势函数 (advantage function) 会产生零值，导致零梯度 (null gradients) 并损失有价值的学习信号。为克服这一问题，我们提出了NGRPO (负增强组相对策略优化)，这是一种旨在将同质错误转化为稳健学习信号的算法。首先，NGRPO引入了优势校准 (Advantage Calibration) 机制。该机制假设在优势计算过程中存在一个虚拟最大奖励样本 (virtual maximum-reward sample)，从而改变组内奖励的均值和方差，确保同质错误样本的优势不再为零。其次，NGRPO采用非对称裁剪 (Asymmetric Clipping)，放宽对正样本 (positive samples) 的更新幅度限制，同时对负样本 (negative samples) 施加更严格的约束。这有助于稳定由优势校准引入的探索压力 (exploration pressure)。我们在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试 (mathematical benchmarks) 上显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法 (baselines)。这些结果验证了NGRPO从同质错误中学习的能力，从而在数学推理方面实现了稳定且实质性的改进。我们的代码可在https://github.com/nangongrui-ngr/NGRPO获取。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
                    "arxiv_id": "2509.18542",
                    "authors": "Qi Wang, Hanyang Peng, Yue Yu",
                    "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出了一种新的Mixture-of-Experts (MoE)模型构建方法Symphony-MoE，该方法通过整合多个不同但架构相同的预训练模型（如Llama2-Chat和Code Llama）来提高整体性能。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM的基础架构和能力。它提出了一种两阶段框架来解决不同预训练模型在参数空间中的不协调问题，通过层感知融合策略和基于激活的功能对齐来协调这些模型，然后进行轻量级的路由器训练。论文提到其方法在\"多领域任务\"和\"分布外泛化\"方面取得了显著成果，这表明它关注的是模型的通用能力，而非特定领域应用。虽然论文没有直接提到推理、规划等具体能力，但它提出的架构改进本质上是为了提升LLM的通用能力，符合研究目标中\"改进LLM的基础能力、提出新的训练范式\"的要求。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决如何将多个架构相同但训练历史不同的预训练模型有效地整合成一个强大的MoE模型的问题。针对多个异构预训练模型，我们提出了一种Symphony-MoE两阶段框架，通过无训练的功能对齐和轻量级路由器训练来协调不同模型，并在MMLU、GSM8K、BBH、HumanEval、TruthfulQA和MedCQA等多个数据集上通过准确率等指标验证了其有效性。",
                    "summary_translation": "混合专家(Mixture-of-Experts, MoE)模型通过稀疏激活大型参数集实现可扩展性能，同时最小化计算开销。为规避从头训练MoE模型的过高成本，近期工作采用升级改造(upcycling)方法，通过将单个预训练密集模型的前馈网络(feed-forward network, FFN)层复制成专家来重用该模型。然而，这种方法限制了专家多样性，因为所有专家都源自于单个预训练密集模型。本文通过使用来源于多个架构相同但不同的预训练模型（如Llama2-Chat和Code Llama）的专家来构建强大的MoE模型，从而解决这一限制。\n\n一个关键挑战在于，这些源模型在参数空间(parameter space)中占据不同且不协调的区域，使得直接升级改造容易导致严重性能下降。为克服这一问题，我们提出了Symphony-MoE，这是一个新颖的两阶段框架，旨在将这些模型协调成一个单一、连贯的专家混合体。首先，我们以无训练方式建立这种协调：我们通过层感知融合策略构建共享骨干网络，并且关键的是，使用基于激活的功能对齐(activation-based functional alignment)来缓解专家之间的参数不对齐问题。随后，一个轻量级的路由器(router)训练阶段协调整个架构。\n\n实验表明，我们的方法成功整合了来自异构源的专家，构建出的MoE模型在多领域任务和分布外(out-of-distribution)泛化方面显著超越基线模型。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Self-Evolving LLMs via Continual Instruction Tuning",
                    "arxiv_id": "2509.18133",
                    "authors": "Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan Shi, Ting Bai",
                    "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
                    "category": "cs.AI",
                    "filter_reason": "这篇论文的核心贡献是提出MoE-CL框架，一种用于大语言模型自进化的参数高效对抗性混合专家框架。论文直接针对LLM的基础能力——持续学习和自进化能力进行研究，提出通过双专家设计（每个任务的专用LoRA专家和共享LoRA专家）以及GAN中的任务感知判别器来解决持续学习中的灾难性遗忘问题。这明显符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为自进化能力是LLM通用推理能力的重要组成部分。论文虽然提到了在Tencent Video平台的应用案例，但这只是验证方法有效性的实验，论文的核心并不是针对特定应用领域的研究，而是提出了一种通用的训练范式来增强LLM的基础能力。论文明确涉及\"self-evolution\"这一正面指标，并且不涉及任何排除标准中的领域。因此，这篇论文完全符合筛选标准，应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在持续学习中面临的灾难性遗忘问题，实现LLMs的自我进化能力。针对工业场景中多样化任务需求，我们提出了一种名为MoE-CL的对抗性LoRA专家混合框架，并在公共MTL5基准和工业Tencent3基准上通过Accuracy、Backward Transfer和Forward Transfer指标验证了其有效性。在腾讯视频平台的内容合规审查A/B测试中，MoE-CL将人工审核成本降低了15.3%。",
                    "summary_translation": "在真实工业环境中，大型语言模型（large language models, LLMs）必须持续学习以跟上多样化和不断发展的任务，需要在动态数据分布下进行自我进化（self-evolution）以完善知识。然而，现有的持续学习（continual learning, CL）方法，如回放（replay）和参数隔离（parameter isolation），常常遭受灾难性遗忘（catastrophic forgetting）：在新任务上的训练会因过度拟合新分布而降低在早期任务上的性能，并削弱泛化能力。\n\n我们提出了MoE-CL，一个参数高效的对抗性专家混合（adversarial mixture-of-experts）框架，用于工业规模、自我进化的LLM持续指令调优（continual instruction tuning）。MoE-CL采用双专家设计：（1）每个任务一个专用的LoRA专家，通过参数独立性保留任务特定知识，减轻遗忘；以及（2）一个共享的LoRA专家，以实现跨任务迁移。为防止通过共享路径传递任务无关噪声，我们在生成对抗网络（GAN）中集成了一个任务感知判别器（task-aware discriminator）。判别器鼓励共享专家在顺序训练过程中仅传递与任务对齐的信息。通过对抗学习（adversarial learning），共享专家获得模仿判别器的泛化表示，而专用专家保留任务特定细节，平衡知识保留和跨任务泛化，从而支持自我进化。\n\n在公开的MTL5基准测试和工业级Tencent3基准测试上的大量实验验证了MoE-CL在持续指令调优方面的有效性。在腾讯视频平台内容合规审查的真实A/B测试中，MoE-CL将人工审查成本降低了15.3%。这些结果表明，MoE-CL适用于大规模工业部署，特别是在持续适应和稳定迁移至关重要的场景中。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
                    "arxiv_id": "2509.18116",
                    "authors": "Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary",
                    "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
                    "category": "cs.AI",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础推理能力。论文提出了\"Amortized Latent Steering (ALS)\"方法，这是一种新的训练/推理范式，通过潜在空间引导来增强模型的推理能力，特别是数学推理能力。ALS将测试时优化的迭代过程转化为离线计算的向量，在推理过程中以恒定成本应用，从而在保持或提高推理性能的同时显著降低计算成本。这明确符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：论文虽然未在摘要中明确提到\"Large language models\"，但从上下文（如提到Chain-of-Thought, Self-Consistency等LLM相关技术）可以推断这是针对LLM的研究。 - 能力方向：论文明确在GSM8K和MATH-500这两个数学推理基准测试上评估方法，表明论文关注数学推理能力，并提到\"sophisticated reasoning techniques\"，进一步确认其与推理相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学、生物等）或模型可靠性（应用层面）的研究。 最后，论文不涉及特殊或模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出了一种提高LLM数学推理效率的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决测试时优化计算成本过高的问题。针对数学推理任务的高计算开销场景，我们提出了一种摊销潜在转向（ALS）方法，通过离线计算成功与失败生成间的隐藏状态差异向量，并在推理时以恒定成本应用该向量校准模型表示。在GSM8K和MATH-500基准上通过效率-准确性权衡指标验证了其有效性，实现了2-5倍加速同时保持或超越基线性能。",
                    "summary_translation": "测试时优化（Test-time optimization）由于高昂的推理成本在大规模应用中仍然不切实际——诸如迭代细化（iterative refinement）和多步验证（multi-step verification）等技术可能需要比标准解码（standard decoding）多10-100倍的计算量。像LatentSeek这样的潜在空间测试时优化方法（Latent space test-time optimization methods）通过引导隐藏表示（steering hidden representations）提供了一种更直接的方法，但仍然需要昂贵的每次查询优化循环（per-query optimization loops）和多次反向传播（multiple backward passes）。我们提出了摊销潜在引导（Amortized Latent Steering, ALS），它将这种迭代优化折叠成一个在推理过程中以恒定成本应用的离线计算向量（offline-computed vector）。ALS计算成功生成与不成功生成之间的隐藏状态平均差异，然后使用这个方向来校准模型的隐藏表示（hidden representations）：当解码偏离成功流形（success manifold）时，ALS会将激活值（activations）推回该流形。在GSM8K和MATH-$500$基准测试中，ALS比迭代方法实现了2-5倍的加速，同时匹配或超过了贪心思维链（greedy Chain-of-Thought, CoT）和自我一致性（Self-Consistency）基线，使效率-准确率权衡（efficiency--accuracy trade-off）提高了高达101%。这些结果表明，潜在优化（latent optimization）的大部分好处可以离线捕获，使复杂的推理技术（sophisticated reasoning techniques）能够在生产部署中变得可行。代码可在~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}获取。",
                    "inspiration_trace": ""
                },
            ]
        },
        {
            "name": "Computation and Language",
            "count": 13,
            "papers": [
                {
                    "title": "Reinforcement Learning on Pre-Training Data",
                    "arxiv_id": "2509.19249",
                    "authors": "Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang",
                    "summary": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究目标，核心是提出一种名为RLPT（Reinforcement Learning on Pre-Training Data）的新训练范式，用于提升大语言模型的通用推理能力。从第一步核心判断来看，论文本质上是改进LLM的基础能力，提出新的强化学习训练方法，旨在增强模型的通用推理技能，而非将LLM应用于特定领域。论文明确提到RLPT\"鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更通用的推理技能\"，并在数学推理等通用能力基准测试中验证了有效性。 从第二步正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(特别是数学推理)；训练方法上提出了基于强化学习的新范式(RLPT)，并与RLHF、RLVR等方法进行了比较。 第三步排除标准方面，论文不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容，没有任何排除因素。 综上所述，这篇论文的核心贡献是提出一种新的训练范式来增强LLM的通用推理能力，完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决计算资源指数级增长与高质量文本数据有限增长之间的差距限制LLMs传统扩展方法的问题。针对预训练数据，我们提出了一种Reinforcement Learning on Pre-Training Data (RLPT)方法，采用next-segment reasoning目标，包括ASR和MSR任务，直接从预训练数据中获取奖励信号，并在通用领域和数学推理基准测试上通过准确率和Pass@k指标验证了其有效性。",
                    "summary_translation": "计算资源的指数级扩展与高质量文本数据的有限增长之间的日益扩大的差距，现在正限制着大型语言模型（LLMs）的传统扩展方法。为应对这一挑战，我们提出了预训练数据强化学习（Reinforcement Learning on Pre-Training data, RLPT），这是一种用于优化LLMs的新型训练时扩展范式。与先前主要通过监督学习（supervised learning）扩展训练的方法不同，RLPT使策略能够自主探索有意义的轨迹，从预训练数据中学习，并通过强化学习（reinforcement learning, RL）提升其能力。虽然现有的强化学习策略，如基于人类反馈的强化学习（reinforcement learning from human feedback, RLHF）和可验证奖励强化学习（reinforcement learning with verifiable rewards, RLVR）依赖于人类标注来构建奖励，但RLPT通过直接从预训练数据中导出奖励信号，消除了这种依赖性。具体而言，它采用下一段推理目标（next-segment reasoning objective），奖励策略基于前文上下文准确预测后续文本段。这种表述方式允许强化学习在预训练数据上进行扩展，鼓励在更广泛的上下文中探索更丰富的轨迹，从而培养更具泛化性的推理技能。在多个模型的通用领域和数学推理基准上进行的大量实验验证了RLPT的有效性。例如，当应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别实现了$3.0$、$5.1$、$8.1$、$6.0$、$6.6$和$5.3$的绝对提升。结果进一步展示了良好的扩展行为，表明随着更多计算资源的投入，持续提升的潜力巨大。此外，RLPT提供了坚实的基础，扩展了LLMs的推理边界，并增强了RLVR的性能。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
                    "arxiv_id": "2509.19199",
                    "authors": "Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao",
                    "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments. However, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging. Recent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare. We therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels. In OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective. These step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop. Theoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training. Empirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA. Crucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training. Further analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合研究范围，核心贡献是提出一种新的强化学习方法(OPRL)来增强大语言模型作为智能体时的通用推理能力。 从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力的研究。它提出了\"在线过程奖励学习\"(OPRL)这一新的训练范式，专注于解决LLM作为自主智能体在长期任务中的信用分配问题，这直接属于提升LLM通用推理能力的范畴。 从第二步正面指标来看，论文涵盖了所有关键主题：明确以大语言模型(LLMs)为核心研究对象；关注模型的推理能力(\"reason and act over long horizons\")；采用强化学习作为主要训练方法；并聚焦于基于LLM的智能体(\"agentic reinforcement learning\")研究。 从第三步排除标准来看，虽然论文在评估中使用了WebShop、VisualSokoban等特定环境，但这些只是用来验证通用方法的应用场景，论文本身并不专注于任何特定应用领域或多模态研究，其核心贡献是通用的强化学习方法。 从第四步特殊和模糊情况处理来看，论文提出的是一种通用的智能体学习方法，旨在增强LLM的通用问题解决和推理能力，而非针对特定领域的应用。 综上所述，这篇论文通过提出新的强化学习范式来提升LLM的长期推理和决策能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型作为智能体在强化学习中面临的稀疏奖励和信用分配挑战。针对长时程交互任务，我们提出了一种在线过程奖励学习(OPRL)方法，通过优化隐式过程奖励模型将轨迹偏好转换为步骤级奖励，并在WebShop、VisualSokoban和SOTOPIA三个基准测试上通过成功率、分数和目标完成分数等指标验证了其有效性。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）越来越多地通过强化学习（Reinforcement Learning, RL）进行训练，作为在交互环境中进行长期推理和行动的自主代理。然而，稀疏且有时不可验证的奖励使得时间信用分配（temporal credit assignment）极具挑战性。近期工作尝试将过程监督（process supervision）整合到代理学习中，但存在有偏标注、奖励劫持（reward hacking）、过度细粒度信号导致的高方差或状态重叠较少时的失败等问题。\n\n因此，我们提出了在线过程奖励学习（Online Process Reward Learning, OPRL），这是一种用于代理强化学习（agentic RL）的通用信用分配策略，可与标准在策略算法（on-policy algorithms）无缝集成，而无需依赖额外的推演（rollouts）或显式步骤标签。在OPRL中，我们交替优化隐式过程奖励模型（implicit process reward model, PRM）和代理策略，通过基于轨迹的DPO目标（trajectory-based DPO objective）将轨迹偏好转化为隐式步骤奖励。这些步骤奖励随后用于计算步骤级优势（step-level advantages），与来自结果奖励的回合级优势（episode-level advantages）相结合用于策略更新，形成一个自我强化循环（self-reinforcing loop）。\n\n理论研究保证，学习到的步骤奖励与轨迹偏好一致，并作为基于势能的塑形奖励（potential-based shaping rewards），提供有界梯度以稳定训练。在实证方面，我们在三个不同的代理基准测试（agent benchmarks）上评估了OPRL，包括WebShop和VisualSokoban，以及在SOTOPIA中具有不可验证奖励的开放式社交互动（open-ended social interactions）。关键的是，OPRL在各个领域都表现出优于前沿LLMs和强大RL基线的性能，以更高的样本效率（sample-efficiency）和更低的训练方差实现了最先进的结果。进一步分析还表明，OPRL通过使用更少的动作实现了高效探索（efficient exploration），强调了其在现实场景中代理学习的潜力。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
                    "arxiv_id": "2509.19094",
                    "authors": "Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky",
                    "summary": "Personalization is essential for adapting question answering (QA) systems to user-specific information needs, thereby improving both accuracy and user satisfaction. However, personalized QA remains relatively underexplored due to challenges such as inferring preferences from long, noisy, and implicit contexts, and generating responses that are simultaneously correct, contextually appropriate, and aligned with user expectations and background knowledge. To address these challenges, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without requiring task-specific fine-tuning. The approach models the reasoning of an LLM as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark for personalized QA show that PoT consistently outperforms competitive baselines, achieving up to a 13.1% relative improvement. Human evaluation corroborates these results, with annotators preferring outputs from PoT in 66% of cases and reporting ties in only 15% of cases.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了\"Pathways of Thoughts (PoT)\"方法，一种用于增强大语言模型推理能力的通用框架。根据筛选标准，我判断该论文符合研究范围，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。PoT方法将LLM的推理过程建模为迭代决策过程，使模型能够动态选择认知操作（推理、修订、个性化、澄清等），这直接涉及增强LLM的多步推理能力和逻辑推理能力，属于提升LLM通用推理能力的研究。 其次，从正面指标分析，论文明确包含多个关键主题：核心概念上聚焦于大语言模型(LLMs)；能力方向上专注于reasoning（推理过程）；方法上提出了一种新的推理范式，使模型能够探索多个推理轨迹并聚合结果。 第三，该论文不涉及任何排除标准领域。它没有关注多模态与视觉，没有针对特定应用领域（如医疗、化学等），也没有专注于模型基础设施或部署优化。 最后，关于特殊情况的考虑，虽然论文应用于个性化问答场景，但其提出的是一种通用的推理框架，可以应用于任何LLM而无需任务特定微调，因此不是将LLM作为工具应用于特定领域，而是致力于提升LLM本身的通用推理能力。 综合来看，这篇论文通过提出多方向思维的推理方法，直接增强了大语言模型的通用推理能力，完全符合研究课题的筛选要求。",
                    "summary2": "本文旨在解决个性化问答系统中的挑战，包括从长、嘈杂和隐式上下文中推断用户偏好，以及生成既正确又符合用户期望的响应。针对个性化问答场景，我们提出了一种Pathways of Thoughts (PoT)方法，将LLM的推理建模为迭代决策过程，探索多个推理轨迹并聚合响应，并在LaMP-QA基准测试上通过自动化评估和人工评估验证了其有效性，实现了高达13.1%的相对改进。",
                    "summary_translation": "个性化（Personalization）对于使问答系统（question answering, QA）适应用户特定的信息需求至关重要，从而同时提高准确性和用户满意度。然而，由于从冗长、嘈杂和隐含的上下文中推断用户偏好，以及生成同时满足正确性、上下文适当性并与用户期望和背景知识保持一致的响应等挑战，个性化问答（personalized QA）研究仍相对不足。为应对这些挑战，我们提出了\"思维路径\"（Pathways of Thoughts, PoT），这是一种适用于任何大型语言模型（large language model, LLM）的推理阶段（inference-stage）方法，无需进行任务特定的微调（fine-tuning）。该方法将大型语言模型的推理过程建模为一个迭代决策过程，模型在其中动态选择认知操作，如推理（reasoning）、修订（revision）、个性化（personalization）和澄清（clarification）。这使得能够探索多种推理轨迹，生成捕捉不同视角的多样化候选响应。然后，PoT根据推断的用户偏好对这些候选响应进行聚合和重新加权，产生最终的个性化响应，该响应受益于多样化推理路径的互补优势。在个性化问答的LaMP-QA基准测试（benchmark）上的实验表明，PoT始终优于竞争性基线（baselines），实现了高达13.1%的相对改进。人工评估（human evaluation）证实了这些结果，标注员（annotators）在66%的情况下更喜欢PoT的输出，仅在15%的情况下报告平局。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Soft Tokens, Hard Truths",
                    "arxiv_id": "2509.19170",
                    "authors": "Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier",
                    "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种通过强化学习(RL)来学习连续思维链(CoTs)的新方法，以提高大语言模型的推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力，特别是思维链推理这一通用能力，而非将LLM作为工具应用到特定领域。论文使用了强化学习这一训练方法，聚焦于数学推理任务，符合第二步正面指标中的多个关键点：核心概念(LLMs)、能力方向(math reasoning)和训练方法(RL)。论文不涉及第三步排除标准中的任何领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊和模糊情况处理中，论文提出的是一种通用的推理增强方法，而非针对特定领域的应用。综合来看，这篇论文直接致力于提高LLM的通用推理能力，通过创新的\"soft tokens\"方法增强思维链推理，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在连续Chain-of-Thought推理中的训练难题。针对数学推理任务，我们提出了一种基于强化学习的soft tokens训练方法，通过在token嵌入中添加噪声实现有效探索，无需从真实离散CoT中蒸馏。在Llama和Qwen模型上，通过pass@1和pass@32指标验证了其有效性，表明该方法在保持pass@1性能的同时显著提高了pass@32的多样性，且在域外任务上更好地保留了基础模型性能。",
                    "summary_translation": "在大型语言模型(LLM)的Chain-of-Thought (CoT，思维链)推理阶段使用连续token(标记)而非离散token(标记)的做法最近引起了广泛关注，其基于一种直觉，即离散token(标记)的连续混合可以同时模拟多条推理路径的叠加。理论结果已经正式证明，连续token(标记)具有更强的表达能力，并能更高效地解决特定问题。\n\n然而，连续token(标记)的实际应用受到严重训练困难的限制：先前的研究要么仅在预训练的离散token(标记)模型上在推理时使用连续token(标记)，要么必须从真实的离散CoT中蒸馏出连续CoT，并面临计算成本的限制，导致CoT只能包含非常少的token(标记)。\n\n这是首个介绍通过强化学习(RL，Reinforcement Learning)学习连续CoT的可扩展方法的研究，无需从参考离散CoT中进行蒸馏。我们使用\"软\"token(标记)：token(标记)的混合与输入嵌入上的噪声相结合，以提供RL探索。计算开销极小，使我们能够学习包含数百个token(标记)的连续CoT。\n\n在高达8B参数的Llama和Qwen模型的数学推理基准测试中，使用连续CoT训练在pass@1指标上与离散token(标记) CoT相当，在pass@32指标上超越后者，显示出更大的CoT多样性。在系统比较中，表现最佳的场景是使用连续CoT token(标记)进行训练，然后在推理时使用离散token(标记)，这意味着\"软\"模型可以以标准方式部署。\n\n最后，我们表明连续CoT RL训练能更好地保留基础模型在域外任务(out-of-domain tasks)上的预测，从而为基础模型提供更柔和的调整。",
                    "inspiration_trace": ""
                },
                {
                    "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
                    "arxiv_id": "2509.18632",
                    "authors": "Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, Jordan Lee Boyd-Graber",
                    "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。 首先，从核心判断来看，这篇论文的本质是研究LLM的规划(planning)能力，属于通用推理能力的重要组成部分。论文探讨了LLM如何生成步骤化计划来帮助用户完成复杂任务，并评估这些计划的实际有效性。这不是将LLM作为工具应用到特定领域，而是关注LLM本身的基础能力和改进方法，特别是当前对齐方法(如RLHF)的局限性。 其次，论文满足多个正面指标： 1) 核心概念：明确研究LLM生成的计划 2) 能力方向：聚焦于规划(planning)能力，这是推理和问题解决的关键部分 3) 训练方法：讨论了RLHF等对齐方法，并指出其局限性 4) 新兴范式：提到了智能体(agents)在模拟用户交互中的作用 第三，论文不符合任何排除标准。它不涉及多模态与视觉，不聚焦于特定应用领域，也不是关于模型可靠性的应用层面研究(如水印、安全等)。 在特殊和模糊情况处理上，论文虽然提到了智能体，但这是作为研究方法的一部分，而非将智能体应用在特定领域。论文讨论的对齐问题是从提升模型基础能力的角度，而不是应用层面的防御技术。 论文的核心贡献在于揭示了当前LLM对齐方法(基于用户偏好)与实际帮助用户之间的差距，并提出需要基于真实用户交互反馈来改进对齐方法。这直接关系到如何提升LLM的通用推理能力和实际效用，完全符合研究目标。",
                    "summary2": "本文旨在解决LLM对齐方法中用户偏好与实际帮助性不一致的问题。针对多步骤问答场景，我们提出了Planorama界面，收集126用户对600个LLM计划的4388个执行结果和5584个偏好比较。通过Item Response Theory量化计划帮助性，我们发现用户偏好、reward模型和agent表现均无法准确预测真正帮助用户的计划，表明当前基于偏好的对齐方法与实际帮助性存在根本性错位。",
                    "summary_translation": "为协助用户完成复杂任务，大型语言模型（LLMs）会生成计划：即朝向目标的分步说明。尽管对齐方法（alignment methods）旨在确保LLM计划具有帮助性，但它们基于用户偏好进行训练（基于人类反馈的强化学习，RLHF）或评估（聊天机器人竞技场，ChatbotArena），假设这种偏好反映了什么对用户真正有帮助。我们通过Planorama（计划全景）界面对此进行测试：在该界面中，126名用户使用LLM计划回答300个多步骤问题。我们获得4388个计划执行和5584个比较数据，以衡量计划的有用性（问答成功率，QA success）和用户对计划的偏好，并在代理（agents）和奖励模型（reward models）中重建此设置，以观察它们是否能模拟或偏好真正帮助用户的方案。我们揭示了：1）用户/模型偏好与代理成功无法准确预测哪些计划真正帮助用户，因此常见的对齐反馈可能与有用性产生偏差；2）这种差距并非源于用户特定偏好，因为用户在使用他们偏好/不偏好的计划时成功率相似；3）简洁性和问题相似性等表面线索与偏好强烈相关，但此类偏见无法预测有用性。总之，我们认为对齐有用的LLMs需要来自真实用户交互的反馈，而不仅仅是对看起来有用的东西的偏好，因此我们讨论了自然语言处理（NLP）研究人员可执行的计划以解决此问题。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering",
                    "arxiv_id": "2509.18655",
                    "authors": "Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li",
                    "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with new or corrected information without retraining or parameter adjustment. Recent PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE) capabilities to multi-hop question answering (MHQA). However, these methods often lack consistency, leading to knowledge contamination, unstable updates, and retrieval behaviors that fail to reflect the intended edits. Such inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures KG construction, update, and retrieval are always aligned with the requirements of the MHQA task, maintaining coherent reasoning over both unedited and edited knowledge. Extensive experiments on the MQuAKE benchmark show accuracy improvements in PPKE performance for MHQA, demonstrating the effectiveness of addressing consistency in PPKE.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断——这篇论文的本质是关于改进LLM在多跳问答(multi-hop question answering)任务中的推理能力。论文提出了CAPE-KG框架，解决参数保留知识编辑(PPKE)中的一致性问题，确保模型在更新知识后能够保持连贯的多跳推理能力。这属于增强LLM逻辑推理能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文包含\"reasoning\"这一核心能力方向，特别是\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分，涉及模型进行多步逻辑推理来连接不同知识片段以得出答案。 第三步：排除标准——论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。多跳问答是一种通用的推理任务，不属于特定应用领域。 第四步：特殊和模糊情况——论文提出了一种新方法来解决知识编辑中的不一致性问题，从而提升模型在多跳问答任务中的推理质量和可靠性。这符合\"提出一种新方法来减少幻觉、增强模型内在的可解释性或安全性，从而提升模型的通用可靠性和推理质量\"的情况，应保留。 综合以上分析，该论文的核心贡献是提出了一种一致性感知的知识编辑框架，旨在提升LLM在多跳推理任务中的表现，这与研究\"大语言模型通用推理能力\"的目标高度一致，因此最终判断为True。",
                    "summary2": "本文旨在 [解决参数保持知识编辑在多跳问答中缺乏一致性的问题]。针对 [多跳问答场景]，我们提出了一种 [一致性感知的参数保持知识编辑框架CAPE-KG]，并在 [MQuAKE基准测试] 上通过 [M-Acc和H-Acc指标] 验证了其有效性。",
                    "summary_translation": "参数保留知识编辑（Parameter-Preserving Knowledge Editing, PPKE）使模型能够在无需重新训练或参数调整的情况下，使用新的或经过校正的信息进行更新。近期基于知识图谱（Knowledge Graph, KG）的PPKE方法将知识编辑（Knowledge Editing, KE）能力扩展到多跳问题回答（Multi-hop Question Answering, MHQA）任务中。然而，这些方法常常缺乏一致性，导致知识污染、更新不稳定，以及检索行为无法反映预期的编辑效果。这种不一致性削弱了PPKE在多跳推理中的可靠性。我们提出了CAPE-KG（Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs，基于知识图谱的一致性感知参数保留编辑），这是一种用于MHQA任务中PPKE的新型一致性感知框架。CAPE-KG确保知识图谱的构建、更新和检索始终与MHQA任务的要求保持一致，从而在未编辑和已编辑的知识上维持连贯的推理。在MQuAKE基准测试上进行的大量实验表明，CAPE-KG在MHQA任务的PPKE性能方面提高了准确性，证明了解决PPKE中一致性问题的有效性。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs",
                    "arxiv_id": "2509.18314",
                    "authors": "Hieu Tran, Zonghai Yao, Hong Yu",
                    "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over long sequences makes token-level credit assignment the key bottleneck. We study the verifiable-reward setting, where the final answer is checkable and multiple responses can be drawn per prompt. Reasoning tasks in math and medical QA align with this setup, where only a few decision tokens significantly impact the outcome. PPO offers token-level advantages with a learned value model, but it is complex to train both the actor and critic models simultaneously, and it is not easily generalizable, as the token-level values from the critic model can make training prone to overfitting. GRPO is critic-free and supports verifiable rewards, but spreads a single sequence-level return across tokens and ignores branching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that converts a group of responses into a prefix tree and computes \\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes. Built on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated \\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a critic-free algorithm that augments the group-relative outcome signal of GRPO with \\emph{branch-gated} temporal-difference corrections derived from the tree. At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO reduces to GRPO; at branching tokens, it supplies precise token-level credit without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B, TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and reaches higher validation accuracy with roughly the same wall-clock time.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合我的研究目标，核心是关于提高大语言模型通用推理能力的研究。 首先，从本质上看，论文的核心贡献是提出了一种新的强化学习训练方法TEMPO，用于解决LLM在长序列推理任务中的信用分配问题。这直接针对LLM的基础能力改进，特别是提升其在推理任务中的表现，符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确以LLMs为核心研究对象；专注于reasoning能力（特别是math reasoning）；提出了新的强化学习训练方法(TEMPO)，改进了现有的PPO和GRPO算法。 第三，论文不涉及任何排除标准领域。虽然论文在实验中使用了医疗问答(MedQA, MMLU-Medical)作为评估基准，但这只是作为验证方法有效性的测试场景，而非论文的核心焦点。论文的核心是改进通用推理能力，而非专注于医疗应用。 最后，论文提出的方法具有通用性，可以应用于各种需要推理能力的任务，不仅限于特定领域。作者在多个数学推理基准(MATH, GSM-HARD, AMC23)和医疗问答基准上进行了验证，证明了其方法的通用性和有效性。 综上所述，这篇论文明确致力于提高LLM的通用推理能力，通过改进强化学习训练方法来解决信用分配问题，完全符合我的研究范围。",
                    "summary2": "本文旨在解决LLM强化训练中的token级别信用分配问题。针对长序列推理任务中奖励稀疏延迟的场景，我们提出了一种利用响应树结构的TEMPO算法，并在数学和医学问答数据集上通过准确率和收敛速度验证了其有效性。",
                    "summary_translation": "强化学习（reinforcement learning）改善了大型语言模型（LLM）的推理能力，但在长序列上的稀疏延迟奖励（sparse delayed reward）使得token级别的信用分配（token-level credit assignment）成为关键瓶颈。我们研究了可验证奖励（verifiable-reward）设置，其中最终答案是可检查的，并且每个提示（prompt）可以生成多个响应。数学和医学问答（medical QA）中的推理任务与这种设置一致，其中只有少数决策token（decision tokens）对结果有显著影响。PPO通过学习到的价值模型（learned value model）提供了token级别的优势，但同时训练actor和critic模型很复杂，并且不容易泛化，因为来自critic模型的token级别值可能导致训练容易过拟合。GRPO是无critic的（critic-free）且支持可验证奖励，但它将单个序列级别的回报（sequence-level return）分散到各个token上，并忽略了分支（branching）。\n\n我们提出了**Prefix-to-Tree (P2T)**，这是一个简单的过程，将一组响应转换为前缀树（prefix tree），并通过聚合后代结果（descendant outcomes）来计算*非参数*（nonparametric）前缀值\\(V(s)\\)。基于P2T，我们提出了**TEMPO**（***T**ree-**E**stimated **M**ean **P**refix Value for **P**olicy **O**ptimization*，树估计平均前缀值用于策略优化），这是一种无critic的算法，它用从树中派生的*分支门控*（branch-gated）时序差分（temporal-difference）修正来增强GRPO的组相对结果信号。在非分支token上，时序差分（TD）项为零，因此TEMPO简化为GRPO；在分支token上，它提供了精确的token级别信用，而无需学习的价值网络或额外的评判者/教师。在Qwen3-1.7B/4B模型上，TEMPO在分布内（MATH, MedQA）和分布外（GSM-HARD, AMC23, MedMCQA, MMLU-Medical）基准测试上均优于PPO和GRPO，并在大致相同的实际时间（wall-clock time）内达到更高的验证准确率。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning",
                    "arxiv_id": "2509.18163",
                    "authors": "Haodong Zhao, Chenyan Zhao, Yansi Li, Zhuosheng Zhang, Gongshen Liu",
                    "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative \"thinking mode\" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models \"think\", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，这篇论文符合研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的核心是研究大语言模型(LLM)的推理能力，特别是探讨外部辅助信息对LLM推理过程的影响。论文关注的是LLM的基础推理能力，而不是将LLM作为工具应用到特定领域。论文研究了模型的\"thinking mode\"（思维模式）如何影响其对不同类型信息的处理，这直接关系到LLM的通用推理能力。因此，从核心判断来看，这篇论文应该被保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：直接研究reasoning能力，特别是模型的step-by-step thinking capabilities - 论文探讨了LLM在面对不同类型信息时的推理过程，这与提升模型通用推理能力直接相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉研究 - 不是针对特定应用领域（如医疗、化学等）的研究 - 虽然涉及模型可靠性，但是从提升模型推理能力的根本角度出发，而非仅作为应用层面的防御 第四步：处理特殊和模糊情况 论文研究的是LLM的通用推理能力在面对不同类型信息时的表现，特别是\"thinking mode\"如何影响推理过程。这不是将LLM应用到特定领域，而是研究LLM本身的推理机制和脆弱性。论文提出的挑战是\"not merely to make models 'think', but to endow them with the critical faculty to evaluate the information upon which their reasoning is based\"，这直接指向提升LLM的通用推理能力。 第五步：最终决策 综合以上分析，这篇论文的核心贡献是研究外部辅助信息对LLM推理过程的影响，揭示了模型\"thinking mode\"的双面性，并提出了提升模型批判性评估信息能力的重要性。这直接关系到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True，应该保留这篇论文。",
                    "summary2": "本文旨在探究辅助信息对LLM推理过程的影响。针对不同类型辅助信息（有帮助、不相关、误导）的场景，我们提出了SciAux数据集，并在具有可切换思考模式的大型推理模型上通过准确率指标验证了推理过程的双刃剑效应。实验证明，思考模式虽在有帮助上下文中提升性能，但在误导信息面前会显著放大错误，揭示了当前推理模型在信息评估能力上的关键缺陷。",
                    "summary_translation": "Large Language Models (LLMs, 大型语言模型)的推理能力是其应用于复杂、知识密集型领域的基础。在现实场景中，LLMs常被辅以外部信息，这些信息可能是有帮助的、无关的，甚至是误导性的。本文研究了此类辅助信息对具有明确逐步思考能力的LLMs推理过程的因果影响。我们介绍了SciAux，一个源自ScienceQA的新数据集，用于系统测试模型对这些类型信息的鲁棒性(robustness)。我们的发现揭示了一个关键漏洞：模型的审慎\"思考模式\"(thinking mode)是一把双刃剑。有帮助的上下文提高准确性，而误导性信息导致性能灾难性下降，这种影响在思考过程中被放大。思考并未赋予鲁棒性，而是在提供错误信息(misinformation)时强化了错误程度。这表明挑战不仅仅是让模型\"思考\"，而是赋予它们评估其推理所依据信息的批判性能力(critical faculty)。SciAux数据集可在https://huggingface.co/datasets/billhdzhao/SciAux获取。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs",
                    "arxiv_id": "2509.18113",
                    "authors": "Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu",
                    "summary": "This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出了一种动态提示融合方法，用于改进大语言模型在多任务和跨领域设置下的泛化能力。论文通过引入提示池和任务感知调度策略，动态组合和对齐不同任务的提示，增强了模型捕捉跨任务语义差异的能力。从第一步核心判断来看，这明显属于改进LLM基础能力的研究，提出了新的训练范式来增强模型的通用能力，而非将LLM作为工具应用到特定领域。从第二步正面指标看，论文明确涉及大语言模型(LLMs)核心概念，并特别提到提高了\"知识推理任务\"(knowledge reasoning tasks)的性能，直接符合推理能力方向。论文不涉及第三步中的任何排除标准，如多模态视觉、特定应用领域或模型可靠性等应用层面的研究。综合分析，该论文致力于提升LLM的通用推理能力和泛化能力，完全符合\"大语言模型通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大型语言模型在多任务和跨领域设置中的泛化限制问题。针对异构任务和领域迁移场景，我们提出了一种基于动态提示调度的统一多任务学习框架，并在CrossFit数据集上通过SuperGLUE和MMLU Accuracy等指标验证了其有效性。",
                    "summary_translation": "本研究针对大语言模型（large language models）在多任务（multi-task）和跨域（cross-domain）设置中普遍存在的泛化限制（generalization limitations）问题。与先前如SPoT等依赖固定提示模板（prompt templates）的方法不同，我们引入了一种具有动态提示调度机制（dynamic prompt scheduling mechanism）的统一多任务学习框架。通过引入提示池（prompt pool）和任务感知调度策略（task-aware scheduling strategy），该方法动态组合和对齐不同任务的提示。这增强了模型捕捉任务间语义差异（semantic differences）的能力。在提示融合（prompt fusion）过程中，模型利用任务嵌入（task embeddings）和门控机制（gating mechanism）精细控制提示信号（prompt signals）。这确保了提示内容与任务特定需求之间的一致性，同时构建了跨任务的灵活共享路径。此外，所提出的优化目标（optimization objective）以联合多任务学习（joint multi-task learning）为中心，融合了调度权重（scheduling weights）的自动学习策略，有效缓解了任务干扰（task interference）和负迁移（negative transfer）问题。为评估该方法的有效性，我们进行了一系列敏感性实验（sensitivity experiments），这些实验考察了提示温度参数（prompt temperature parameters）和任务数量变化的影响。结果证实了所提机制在保持模型稳定性和增强迁移能力（transferability）方面的优势。实验发现表明，该提示调度方法在一系列语言理解（language understanding）和知识推理（knowledge reasoning）任务上显著提高了性能。这些结果充分证明了其在统一多任务建模（unified multi-task modeling）和跨域适应（cross-domain adaptation）中的适用性和有效性。",
                    "inspiration_trace": ""
                },
                {
                    "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization",
                    "arxiv_id": "2509.18158",
                    "authors": "Seungyoun Yi, Minsoo Khang, Sungrae Park",
                    "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.",
                    "category": "cs.CL",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，这篇论文的本质是提出ZERA框架，一种用于自动优化提示的新方法，通过联合优化系统提示和用户提示来提高大语言模型在各种任务上的性能。这不是将LLM作为工具应用到特定领域，而是提出一种通用的方法来增强LLM的基础能力，特别是在推理任务上的表现，因此符合保留条件。 其次，论文包含多个正面指标： - 核心概念：明确关注大语言模型(LLMs)，并在五个不同LLM上进行了评估 - 能力方向：评估数据集包含推理(reasoning)任务，直接符合我们的研究方向 - 训练方法：标题中的\"Instruction Evolving\"和摘要中的\"principled, low-overhead refinement\"表明涉及进化或自我优化的方法 - 新兴范式：论文提出的ZERA是一种基于LLM的智能体(Refinement Agent)，用于优化提示 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域，评估的是通用能力（推理、摘要、代码生成） - 不主要关注模型可靠性的应用层面问题 最后，在特殊和模糊情况处理上，ZERA智能体是一种通用框架，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是提出了一种新颖的、基于原则的提示优化框架，能够使用最少的示例和短的迭代周期快速收敛到高质量提示，从而提升LLM在推理等通用任务上的表现。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决自动提示优化(APO)方法依赖非结构化反馈、需要大量样本和长迭代周期的问题。针对多种LLMs和九个不同任务数据集，我们提出了一种ZERA框架，通过八个评估原则和自动推断权重共同优化系统和用户提示，并在推理、摘要和代码生成任务上通过准确率、ROUGE-L和pass@1等指标验证了其有效性。",
                    "summary_translation": "自动提示优化(Automatic Prompt Optimization, APO)通过针对特定任务优化提示来提高大型语言模型(large language model, LLM)的性能。然而，先前的APO方法通常只关注用户提示，依赖非结构化反馈，并且需要大量样本和长迭代周期——这使得它们成本高昂且脆弱。我们提出了ZERA(Zero-init Instruction Evolving Refinement Agent，零初始化指令进化优化代理)，一种新颖的框架，通过有原则的、低开销的优化来共同优化系统和用户提示。ZERA使用八个具有自动推断权重的可泛化标准对提示进行评分，并基于这些结构化批评修订提示。这使得能够使用最少的示例和短的迭代周期快速收敛到高质量的提示。我们在五个LLMs和九个涵盖推理、摘要和代码生成任务的多样化数据集上评估了ZERA。实验结果表明，与强大的基线相比，ZERA取得了持续的改进。进一步的消融研究(ablation studies)突显了每个组件对更有效提示构建的贡献。我们的实现包括所有提示，可在https://github.com/younatics/zera-agent公开获取。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
                    "arxiv_id": "2509.19265",
                    "authors": "Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto",
                    "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围。论文的核心贡献是研究LLMs的常识推理能力在不同文化背景下的迁移效果，并提出了轻量级对齐方法（如上下文学习和基于演示的强化学习DITTO）来提升这种能力。常识推理是通用推理能力的重要组成部分，论文评估的训练方法也属于改进LLM基础能力的范畴。虽然论文涉及文化领域，但其焦点不是将LLM作为工具应用于特定领域，而是研究LLM本身的推理能力如何在不同文化背景下迁移和提升。论文提出的方法（使用12个文化特定示例即可提高其他地区性能10%）可以作为一种通用的方法来增强LLMs的推理能力，而不是仅限于解决特定领域的问题。因此，这篇论文符合我的研究目标，它探索了如何通过文化迁移来提升LLM的通用推理能力，特别是常识推理这一核心能力。",
                    "summary2": "本文旨在研究大型语言模型(LLMs)在阿拉伯世界中的跨文化常识推理迁移能力。针对阿拉伯国家间存在语言和历史相似性但又有本地文化差异的特点，我们提出了一种轻量级文化对齐方法，包括上下文学习(ICL)和基于演示的强化学习(DITTO)，并在ArabCulture数据集上通过准确率提升验证了其有效性。实验表明，仅需12个特定文化示例就能使多语言模型在其他国家的表现平均提升10%，且非阿拉伯文化(如印尼和美国)的演示也能达到或超过文化内对齐的效果。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）常表现出以西方为中心的偏见（Western-centric biases），限制了其在多元文化语境中的有效性。尽管已有研究探索了文化对齐（cultural alignment）问题，但利用某一文化中的对齐成果来提升其他文化中模型表现的跨文化迁移（cross-cultural transfer）潜力仍鲜有研究。本文聚焦阿拉伯世界中的常识推理（commonsense reasoning）跨文化迁移问题，该区域在语言与历史方面具有高度相似性，同时又存在显著的本地文化差异。我们使用一个涵盖13个阿拉伯国家、基于文化情境构建的常识推理数据集，评估了多种轻量级对齐方法，包括上下文学习（in-context learning）和基于示范的强化方法（demonstration-based reinforcement, DITTO），并与监督微调（supervised fine-tuning）和直接偏好优化（direct preference optimization）等基线方法进行对比。实验结果表明，在多语言模型中，仅需来自某一国家的12个文化特异性示例，即可使其他阿拉伯国家的模型表现平均提升10%。此外，我们发现来自印度尼西亚和美国语境的“异文化”示范（out-of-culture demonstrations）在多项选择题（MCQ）推理任务中，其效果可媲美甚至超越“同文化”对齐方法，揭示了常识推理能力在阿拉伯世界之外的跨文化可迁移性。这些发现表明，高效的跨文化对齐是可行的，为将大语言模型适配至资源匮乏的文化环境提供了有前景的路径。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
                    "arxiv_id": "2509.18847",
                    "authors": "Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu",
                    "summary": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是通过提出\"结构化反思\"(structured reflection)这一新训练范式来增强LLM的工具使用和错误恢复能力，这属于提升LLM通用推理能力的范畴。 论文包含多个正面指标：明确提及\"Large language models (LLMs)\"和\"Tool-augmented large language models\"等核心概念；虽然未直接使用\"reasoning\"一词，但\"reflection\"和\"error diagnosis\"本质上是一种推理过程；训练方法涉及强化学习(\"reinforcement learning\"和\"reward scheme\")；同时论文明确讨论了\"llm-based agents\"和\"tool use\"等新兴范式。 论文不符合任何排除标准：它不涉及多模态与视觉内容，不专注于特定应用领域，且讨论的\"reliability\"是从提升模型内在能力的角度而非应用层面的防御。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体反思框架来增强LLM的工具使用能力，而非针对特定领域的应用，因此应该保留。 论文的核心贡献是通过结构化反思机制，使智能体能够从失败中学习并改进其推理过程，这直接提升了LLM的通用推理能力和问题解决能力，符合研究目标。",
                    "summary2": "本文旨在解决工具增强型大语言模型在多轮交互中错误恢复能力不足的问题。针对多轮工具调用场景，我们提出了一种结构化反思方法，将错误诊断和纠正转化为可训练的显式行动，并通过结合DAPO和GSPO的目标函数优化Reflect → Call → Final策略。我们在BFCL v3和Tool-Reflection-Bench上通过多轮工具调用成功率、错误恢复率和冗余调用减少率等指标验证了其有效性。",
                    "summary_translation": "工具增强型大语言模型（LLMs，Large Language Models）通常通过监督模仿或优化单次工具调用的粗粒度强化学习进行训练。当前的自省（self-reflection）实践依赖于启发式提示（heuristic prompts）或单向推理（one-way reasoning）：模型被鼓励\"多思考\"，而不是学习错误诊断和修复。这种方法在多轮交互（multi-turn interactions）中表现脆弱；失败后，模型往往会重复同样的错误。我们提出了结构化反思（structured reflection），它将从错误到修复的路径转变为一种明确、可控且可训练的行动。智能体（agent）生成简短而精确的反思：它利用前一步骤的证据诊断失败，然后提出一个正确的、可执行的后续调用。在训练方面，我们将DAPO（DAPO）和GSPO（GSPO）目标与针对工具使用量身定制的奖励方案相结合，优化\"先反思，后调用，最终确定\"的逐步策略。为进行评估，我们引入了Tool-Reflection-Bench（工具反思基准），这是一个轻量级基准（lightweight benchmark），可程序化地检查结构有效性、可执行性、参数正确性和结果一致性。任务被构建为错误调用、反思和纠正调用的小型轨迹（mini trajectories），训练集和测试集互不重叠。在BFCL v3（BFCL v3）和Tool-Reflection-Bench上的实验表明，多轮工具调用成功率和错误恢复能力大幅提升，冗余调用也有所减少。这些结果表明，将反思明确化并直接优化它，可以提高工具交互的可靠性，并为智能体提供从失败中学习的可复现路径。",
                    "inspiration_trace": ""
                },
                {
                    "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
                    "arxiv_id": "2509.18169",
                    "authors": "Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan He",
                    "summary": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
                    "category": "cs.CL",
                    "filter_reason": "这篇论文的核心贡献是提出PiMoE（物理隔离的专家混合）架构，用于将高精度计算能力内生性地集成到大语言模型中，以增强其推理能力。从筛选标准来看： 第一步（核心判断）：论文本质上是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。PiMoE通过令牌级路由实现计算和推理的迭代交替，属于增强LLM通用推理能力的新范式，类似思维链(CoT)的扩展，因此应保留。 第二步（正面指标）：论文包含多个相关主题，包括核心概念\"large language models (LLMs)\"、能力方向\"reasoning\"，以及新兴范式\"multi-agent systems\"和\"tool use\"的讨论。 第三步（排除标准）：论文未主要聚焦于多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域。虽然提及\"scientific or industrial intelligent systems\"，但这是潜在应用场景而非研究焦点。 第四步（特殊和模糊情况）：论文确实涉及智能体/工具使用，但不是将其应用于特定领域，而是提出一种通用的架构来增强LLM的通用问题解决能力，这与筛选标准中\"提出通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况相符。 综上所述，PiMoE论文直接致力于提高大语言模型本身的通用推理能力，通过新的架构设计实现计算与推理的融合，完全符合研究目标。",
                    "summary2": "本文旨在解决大型语言模型无法内在集成高精度数值计算的问题。针对科学计算与推理任务场景，我们提出了一种PiMoE (Physically-isolated Mixture of Experts)架构，通过token级别路由实现高精度计算与推理的集成。在电池容量预测和电池利润计算任务上，通过MSE、响应延迟、token使用量和GPU能耗等指标验证了其有效性，结果表明PiMoE不仅比直接微调LLMs准确性更高，还比多智能体方法在效率方面有显著改善。",
                    "summary_translation": "复杂系统（complex systems）通常依赖高精度数值计算（high-precision numerical computation）来支持决策，但当前的大型语言模型（large language models, LLMs）还无法在现有架构中将这种计算作为一种内在且可解释的能力（intrinsic and interpretable capability）加以整合。主流的多智能体方法（multi-agent approaches）可以利用外部专家（external experts），但不可避免地引入通信开销（communication overhead），并存在多模态涌现能力（multimodal emergent capability）效率低下和可扩展性（scalability）有限的问题。为此，我们提出了PiMoE（物理隔离的专家混合，Physically-isolated Mixture of Experts），一种用于整合计算和推理（computation and reasoning）的训练和推理架构（inference architecture）。不同于工具调用（tool invocation）的工作流范式（workflow paradigm），PiMoE在分别训练专家、文本到计算模块（text-to-computation module）和路由器（router）后，将计算能力内生整合（endogenously integrates）到神经网络中。在推理（inference）阶段，路由器在词元级别（token level）引导计算和推理，从而在单一思维链（chain of thought）内实现迭代交替（iterative alternation）。我们在两项推理-计算任务（reasoning-computation tasks）上评估了PiMoE，并与LLM微调（LLM finetuning）和多智能体系统方法（multi-agent system approaches）进行了比较。结果表明，PiMoE架构不仅比直接微调LLMs实现了更高的准确度（accuracy），而且与主流多智能体方法相比，在响应延迟（response latency）、词元使用量（token usage）和GPU能耗（GPU energy consumption）方面也有显著改进。PiMoE为下一代科学或工业智能系统（intelligent systems）提供了一种高效（efficient）、可解释（interpretable）且可扩展（scalable）的范式（paradigm）。",
                    "inspiration_trace": ""
                },
            ]
        },
        {
            "name": "Machine Learning",
            "count": 17,
            "papers": [
                {
                    "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
                    "arxiv_id": "2509.19284",
                    "authors": "Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn",
                    "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是研究什么构成了有效的思维链(CoT)推理，并提出了一种新的方法来评估和改进CoT的质量。论文通过系统评估发现，更长的CoT并不总是更好的，并引入了失败步骤分数(FSF)这一指标来预测CoT的正确性。作者还设计了两种干预措施来测试因果关系，证明有效的CoT是那些\"失败较少\"的，并支持\"结构感知\"的测试时扩展。这直接关系到改进LLM的基础推理能力，特别是数学和科学推理能力，属于通用推理能力的范畴。论文不涉及任何需要排除的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文在数学和科学推理上进行了评估，但这些只是作为评估推理能力的基准领域，而非论文的应用目标。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在探究什么特征使思维链(CoT)推理有效。针对大型推理模型(LRMs)的长CoT推理场景，我们提出了一种基于图视图的CoT结构分析方法，定义了Failed-Step Fraction (FSF)作为核心指标，并在HARP和GPQA-Diamond数据集上通过条件相关分析、测试时选择和CoT编辑实验验证了其有效性。",
                    "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在测试阶段花费大量计算资源于长思维链（chain-of-thought, CoT）轨迹，但什么*特征*构成了有效的CoT仍不明确。尽管先前的研究报告称通过延长CoT和通过附加*等待*（wait）标记增加回顾（revisiting earlier steps）可以带来性能提升，但近期研究表明，更短的思考可能优于更长的轨迹。因此，我们在数学和科学推理任务上对十种LRM进行了系统评估。与\"越长越好\"的叙述相反，我们发现无论是简单的CoT延长还是增加回顾都与*更低*的准确率相关。随着CoT逐步展开，标记级别的指标可能会将冗长性与过程质量混为一谈。我们引入了CoT的图视图来提取结构，并识别出一个单一统计量——*失败步骤比例（Failed-Step Fraction, FSF）*，即被放弃分支中的步骤比例，该统计量在预测模型正确性方面始终优于长度和回顾比例。为了探究因果关系，我们设计了两种干预措施。首先，我们在测试时根据每个指标对候选CoT进行排序，其中FSF产生了最大的pass@1增益；其次，我们编辑CoT以移除失败的分支，这显著提高了准确率，表明失败的分支会影响后续推理。综合来看，这些结果表明有效的CoT是那些*失败更少*的CoT，并支持在测试阶段采用*结构感知*（structure-aware）的扩展策略，而非不加选择地生成长CoT。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
                    "arxiv_id": "2509.19189",
                    "authors": "Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu",
                    "summary": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出\"功能缩放定律\"(FSL)框架，用于理解和优化大语言模型的训练动态，特别是学习率计划(LLM训练中的关键超参数)如何影响训练过程。虽然论文没有直接研究推理能力或解决问题等具体能力，但它关注的是LLM的基础训练机制，属于\"改进LLM的基础能力\"和\"提出新的训练范式\"的范畴。论文通过优化训练过程和学习率计划，为提高LLM的通用能力提供了理论基础和实践指导。论文明确研究LLM训练，不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标，因为它通过改进训练方法来提升模型的基础能力，而非将LLM作为工具应用到特定领域。",
                    "summary2": "本文旨在揭示学习率调度(LRS)在大型语言模型(LLM)训练中的影响机制。针对LLM预训练过程中的损失动态和缩放行为，我们提出了一种功能性缩放定律(FSL)，通过内禀时间随机微分方程(SDE)建模，明确刻画了LRS对训练过程的影响。在teacher-student核回归设置和0.1B到1B参数的LLM上，通过损失曲线拟合和预测验证了FSL的有效性，为LLM预训练提供了理论指导和实践洞见。",
                    "summary_translation": "缩放定律（scaling laws）在指导大语言模型（large language models, LLMs）训练中扮演着基石角色。然而，大多数关于缩放定律的现有研究主要关注最终步骤损失（final-step loss），忽视了训练过程中的损失动态，以及至关重要的学习率计划（learning rate schedule, LRS）的影响。在本文中，我们旨在通过研究通过在线随机梯度下降（online stochastic gradient descent, SGD）训练的教师-学生核回归（teacher-student kernel regression）设置来弥合这一差距。利用新颖的内在时间观点（intrinsic time viewpoint）和SGD的随机微分方程（stochastic differential equation, SDE）建模，我们引入了功能缩放定律（Functional Scaling Law, FSL），该定律表征了在一般LRS下训练过程中总体风险（population risk）的演变。值得注意的是，LRS的影响通过一个显式的卷积型功能项（convolution-type functional term）被捕获，使其效果完全可追踪。为了说明FSL的实用性，我们在数据有限（data-limited）和计算有限（compute-limited）两种情况下分析了三种广泛使用的LRS——恒定（constant）、指数衰减（exponential decay）和预热稳定衰减（warmup-stable-decay, WSD）。我们为LLMs预训练中广泛采用的实证实践提供了理论依据，例如：(i) 更高容量的模型在数据和计算利用上更高效；(ii) 学习率衰减可以提高训练效率；(iii) 类似WSD的计划可以优于直接衰减（direct-decay）计划。最后，我们探索了FSL作为LLMs预训练中损失曲线拟合、预测和优化的代理模型（surrogate model）的实际相关性，实验在参数规模从0.1B到1B的模型上进行。我们希望我们的FSL框架能够深化对LLMs预训练动态的理解，并为改进大规模模型训练提供见解。",
                    "inspiration_trace": ""
                },
                {
                    "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio",
                    "arxiv_id": "2509.19128",
                    "authors": "Alexandre Piché, Ehsan Kamaloo, Rafael Pardinas, Dzmitry Bahdanau",
                    "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的强化学习训练范式(PipelineRL)来增强LLM的推理能力。论文明确提到\"Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs)\"，表明其核心是提升LLM的推理能力这一基础能力。 其次，论文包含多个正面指标：核心概念方面直接涉及Large language models (LLMs)；能力方向上关注reasoning；训练方法上专注于reinforcement learning，这些都是研究目标中明确关注的重点。 第三，论文完全避开了排除标准中的所有领域，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文虽然不直接涉及智能体/工具使用或幻觉/可解释性/安全等问题，但通过提高训练效率和数据新鲜度(on-policyness)来改进RL训练质量，这从根本上提升了LLM的推理能力，而非仅作为应用层面的优化。 综上所述，PipelineRL论文的核心贡献是提出了一种更高效的RL训练方法来提升LLM的推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决强化学习在长序列生成中面临的硬件效率与数据策略性之间的权衡问题。针对大型语言模型的长序列生成场景，我们提出了一种PipelineRL方法，通过并发异步数据生成和模型训练，并引入创新的in-flight weight updates机制，在128个H100 GPU上通过学习速度和任务成功率等指标验证了其有效性，实现了约2倍的学习速度提升。",
                    "summary_translation": "强化学习 (Reinforcement Learning, RL) 越来越多地被用于增强大语言模型 (Large Language Models, LLMs) 的推理能力。然而，有效扩展这些强化学习方法面临着重大挑战，主要源于在保持高AI加速器利用率的同时难以避免产生陈旧的、非策略数据 (off-policy data)，而这些数据会损害常见的强化学习算法。本文介绍了PipelineRL，这是一种旨在为大语言模型训练实现硬件效率与数据策略性 (data on-policyness) 之间更优权衡的方法。PipelineRL采用并发的异步数据生成和模型训练，其特点是通过新颖的飞行中权重更新 (in-flight weight updates) 机制。这一机制使大语言模型生成引擎能够在生成token序列期间以最小中断接收更新的模型权重，从而最大化加速器利用率和训练数据的新鲜度。在128个H100 GPU上进行的长格式推理任务实验表明，与传统的强化学习基线相比，PipelineRL实现了约2倍的学习速度提升，同时保持了高度策略性的训练数据。作为一项重要贡献，研究团队还发布了PipelineRL的可扩展、模块化开源实现。",
                    "inspiration_trace": ""
                },
                {
                    "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
                    "arxiv_id": "2509.19104",
                    "authors": "Sharan Sahu, Martin T. Wells",
                    "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出一种新的强化学习对齐方法(DRO-REBEL)，用于改进LLM与人类意图的对齐效果。论文核心是解决现有RLHF方法中的过优化问题，提出了一种分布鲁棒相对奖励回归的新训练范式。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了Large Language Models (LLMs) - 训练方法：重点研究了Reinforcement Learning with Human Feedback (RLHF)的改进，属于强化学习优化范畴 虽然论文没有直接提到reasoning、planning等能力方向，但RLHF作为提升LLM与人类意图对齐的方法，本质上可以增强模型的通用推理能力。 第三步：排除标准 论文不涉及任何排除领域： - 没有涉及多模态与视觉相关内容 - 没有针对特定应用领域（如医疗、化学、生物等） - 没有主要关注模型可靠性方面的应用层面研究（如水印、安全等） 第四步：特殊和模糊情况 论文不涉及智能体/工具使用、幻觉/可解释性/安全等特殊或模糊情况。论文提出的DRO-REBEL方法是从根本上提升LLM与人类意图对齐的能力，属于基础能力改进。 最终决策 综合以上分析，这篇论文的核心贡献是提出了一种新的强化学习对齐方法，用于改进LLM的基础能力，使其更好地与人类意图对齐。这符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标，因为更好的对齐可以提升模型在各种推理任务中的表现。因此，最终判断为True。",
                    "summary2": "",
                    "summary_translation": "强化学习与人类反馈(Reinforcement learning with human feedback, RLHF)已成为使大型语言模型(Large Language Models, LLMs)与人类意图对齐的关键技术。然而，现有的离线RLHF方法存在过度优化问题，即模型过度拟合于奖励误指定(reward misspecification)，并偏离训练过程中观察到的偏好行为。我们提出了DRO-REBEL，这是一个统一的鲁棒REBEL更新家族，包含p型Wasserstein、KL(Kullback-Leibler)和χ²(chi-square)模糊集。利用Fenchel对偶性(Fenchel duality)，每次更新都简化为简单的相对奖励回归(relative-reward regression)，保持了可扩展性，并避免了PPO(Proximal Policy Optimization)风格裁剪或辅助价值网络。在标准线性奖励和对数线性策略类以及数据覆盖条件下，我们建立了$O(n^{-1/4})$的估计界限，其常数比先前的DRO-DPO(Direct Preference Optimization)方法更紧，并通过局部化Rademacher复杂性分析(localized Rademacher complexity analysis)恢复了极小极大最优的$O(n^{-1/2})$速率。同样的分析填补了Wasserstein-DPO和KL-DPO的差距，表明两者也达到了最优参数速率。我们为所有三种散度推导出实用的SGD(Stochastic Gradient Descent)算法：梯度正则化(Wasserstein)、重要性加权(KL)和快速一维对偶求解(χ²)。在情感对齐(Emotion Alignment)、大规模ArmoRM多目标基准和HH-Alignment上的实验表明，该方法在未见过的偏好混合、模型大小和数据规模上均展现出强大的最坏情况鲁棒性，其中χ²-REBEL表现出一致强大的经验性能。一项受控的半径-覆盖研究验证了没有免费午餐的权衡(no-free-lunch trade-off)：比经验散度集中速率更快的收缩半径可实现极小极大最优参数速率，但会放弃覆盖，而保证覆盖的半径则会产生$O(n^{-1/4})$的速率。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Reflect before Act: Proactive Error Correction in Language Models",
                    "arxiv_id": "2509.18607",
                    "authors": "Qiuhai Zeng, Sarvesh Rajkumar, Di Wang, Narendra Gyanchandani, Wenbo Yan",
                    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in interactive decision-making tasks, but existing methods often struggle with error accumulation and lack robust self-correction mechanisms. We introduce \"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based decision-making by introducing a critical reflect step prior to taking the next action. This approach allows for immediate error correction, ensuring smooth action path and adaptibity to environment feedback. We evaluate REBACT on three diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results demonstrate that REBACT significantly outperforms strong baselines, improving success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld (achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's performance improvements are achieved with only a few modification steps, demonstrating its computational efficiency.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合我的研究目标，理由如下： 首先，从核心判断来看，论文本质上是提出一种名为\"Reflect before Act\" (REBACT)的新方法，通过在行动前引入反思步骤来增强语言模型的决策能力。这明显属于改进LLM基础能力和增强其通用推理能力的范畴，特别是通过自我纠错机制来提升模型的逻辑推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标分析，论文明确提到了\"Large Language Models (LLMs)\"这一核心概念，并且关注的是\"decision-making\"能力，这与推理和问题解决密切相关。虽然未直接提及reasoning，但反思和纠错机制本质上是在提升模型的推理质量。 第三，从排除标准看，论文不涉及多模态与视觉内容，也没有专注于特定应用领域（如医疗、化学等）。虽然提到了ALFWorld、WebShop和TextCraft三个评估环境，但这些是用于测试通用能力的基准环境，而非特定应用领域。 最后，在特殊和模糊情况处理上，REBACT是一种通用的反思机制，用于增强LLM的通用问题解决能力，而不是针对特定领域的应用。论文关注的是如何通过反思步骤提升模型的内在纠错能力，从而从根本上提高模型的推理质量，这完全符合\"提升LLM通用推理能力\"的研究目标。 综上所述，这篇论文的核心贡献是提出了一种通过前瞻性反思来增强LLM自我纠错能力的新方法，属于提升大语言模型通用推理能力的研究范畴，因此应该被保留。",
                    "summary2": "本文旨在解决大型语言模型在交互式决策任务中的错误累积和缺乏自我纠正机制的问题。针对交互式决策环境，我们提出了一种REBACT（Reflect before Act）方法，在执行下一个动作前先反思并修正之前动作中的错误，并在ALFWorld、WebShop和TextCraft三个数据集上通过成功率（success rate）验证了其有效性。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）已在交互式决策任务中展现出显著能力，但现有方法常常面临错误累积问题，且缺乏强大的自我纠正机制。我们提出了\"Reflect before Act\"（REBACT，行动前反思）方法，这是一种通过在采取下一步行动前引入关键反思步骤来增强基于LLM的决策的新方法。该方法能够实现即时错误纠正，确保行动路径的顺畅性以及对环境反馈的适应性。我们在三个多样化的交互环境中对REBACT进行了评估：ALFWorld、WebShop和TextCraft。我们的结果表明，使用Claude3.5-sonnet作为底层LLM，REBACT显著优于强基线方法，在WebShop上的成功率提高了24%（达到61%），在ALFWorld上提高了6.72%（达到98.51%），在TextCraft上提高了0.5%（达到99.5%）。进一步分析表明，REBACT的性能提升仅需少量修改步骤即可实现，这证明了其计算效率。",
                    "inspiration_trace": ""
                },
                {
                    "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
                    "arxiv_id": "2509.18164",
                    "authors": "Ranfei Chen, Ming Chen",
                    "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a comprehensive understanding of mathematical and logical patterns. We propose DSFT, a simple yet effective Diffusion SFT strategy, by adjusting the masking strategy and loss function, guiding models to understand mathematical and logical patterns. This strategy can be flexibly combined with pre-training, reinforcement learning, and other training methods. Validated on models such as LLaDA and Dream series, we prove that DSFT on small-scale data can achieve improvements of 5-10% and approximately 2% on mathematical and logical problems, respectively. This inspiring masking approach offers insights for future learning of specific patterns, which can be easily and efficiently combined with other training methods and applied to various dLLMs. Our code is publicly available at https://anonymous.4open.science/r/DSFT-0FFB/",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心贡献是提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数来增强扩散大语言模型(dLLMs)对数学和逻辑模式的理解能力。论文明确关注LLM的通用推理能力，特别是数学推理和逻辑推理，这完全符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是直接提升模型本身的推理能力，且不涉及任何排除标准中的领域（如多模态、特定应用领域或模型可靠性的应用层面研究）。此外，论文提出的方法可以与预训练、强化学习等其他训练方法结合，显示出其作为提升LLM通用推理能力的普适性。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。",
                    "summary2": "本文旨在解决扩散大语言模型在数学和逻辑模式理解方面的挑战。针对数值敏感的数学任务和顺序敏感的逻辑任务，我们提出了一种DSFT(Diffusion SFT)策略，通过调整掩码策略和损失函数指导模型理解数学和逻辑模式，并在LLaDA和Dream系列模型上通过数学(GSM8K, MATH, GPQA)、逻辑(BBH, ARC-C, WinoGrande)和一般任务基准验证了其有效性，实现了数学问题5-10%和逻辑问题约2%的准确率提升。",
                    "summary_translation": "扩散大型语言模型（Diffusion large language models, dLLMs）是继自回归模型（auto regressive models）之后出现的一种新架构。它们的去噪过程（denoising process）提供了强大的生成优势（generative advantage），但在学习和理解对数值敏感的数学任务（numerically sensitive mathematical tasks）和对顺序敏感的逻辑任务（order-sensitive logical tasks）方面面临重大挑战。当前的训练方法，包括预训练（pre-training）、微调（fine-tuning）和强化学习（reinforcement learning），主要侧重于提高一般知识保留（knowledge retention）和推理能力（reasoning abilities），但缺乏对数学和逻辑模式的全面理解。我们提出了DSFT，一种简单而有效的扩散SFT策略（Diffusion SFT strategy），通过调整掩码策略（masking strategy）和损失函数（loss function），引导模型理解数学和逻辑模式。该策略可以灵活地与预训练、强化学习和其他训练方法结合使用。在LLaDA和Dream系列等模型上验证，我们证明在小规模数据上使用DSFT可以在数学问题和逻辑问题上分别实现5-10%和约2%的改进。这种启发性的掩码方法为未来特定模式的学习提供了见解，可以轻松高效地与其他训练方法结合，并应用于各种dLLMs。我们的代码公开可获取，网址为https://anonymous.4open.science/r/DSFT-0FFB/",
                    "inspiration_trace": ""
                },
                {
                    "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
                    "arxiv_id": "2509.18389",
                    "authors": "Jiuqi Wang, Rohan Chandra, Shangtong Zhang",
                    "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步：核心判断 这篇论文的核心是关于强化学习(RL)预训练如何使大语言模型获得上下文强化学习(ICRL)能力的研究。论文探讨的是为什么RL预训练算法能够产生支持ICRL的网络参数，并提出了假设：能够进行ICRL的参数是预训练损失的最小值。这属于改进LLM基础能力和提出新训练范式的研究，特别是关注模型如何在不更新参数的情况下通过上下文信息进行学习和推理，这与通用推理能力直接相关。 第二步：正面指标 论文包含多个正面指标： - 核心概念：论文研究的是Transformer模型在强化学习中的表现，Transformer是大语言模型的基础架构 - 能力方向：论文关注的是推理能力，特别是上下文学习(in-context learning)和时序差分学习(temporal difference learning)，这些都是通用推理能力的重要组成部分 - 训练方法：论文探讨强化学习预训练方法，这与RLHF/RL相关 第三步：排除标准 论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉 - 不针对特定应用领域（如医疗、化学等） - 不主要关注模型基础设施或部署优化 第四步：处理特殊和模糊情况 论文研究的是一种通用的学习机制（上下文强化学习），而不是将其应用于特定领域。它探讨的是模型如何通过预训练获得在不更新参数的情况下适应新任务的能力，这属于提升模型内在推理能力的研究。 第五步：最终决策 综合分析，这篇论文的核心贡献是研究大语言模型如何通过预训练获得上下文强化学习能力，这是一种重要的通用推理能力。论文探讨的是模型的基础能力和训练范式，而不是将模型作为工具应用于特定领域。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决上下文强化学习（ICRL）中为何强化学习预训练能产生支持ICRL的网络参数的问题。针对策略评估任务，我们提出了一种基于线性Transformer的理论分析方法，证明了预训练损失的全局最小化器能实现上下文时序差分学习，并在Boyan's chain环境中通过MSVE指标验证了随着上下文长度增加，模型性能提升且收敛参数与理论预测一致。",
                    "summary_translation": "通常，现代强化学习（reinforcement learning, RL）智能体通过更新其神经网络参数来调整策略以解决任务。最近，人们观察到一些RL智能体在某个任务分布上进行预训练后，无需参数更新就能解决各种新的分布外（out-of-distribution）任务。在新任务中评估时，预训练的智能体不进行参数更新，而是将其策略基于称为上下文（context）的额外输入，例如智能体在新任务中的交互历史。随着上下文中信息的增加，智能体的性能会提高，而智能体的参数保持固定。这种现象通常被称为上下文强化学习（in-context RL, ICRL）。智能体网络的预训练参数使得显著的ICRL现象成为可能。然而，许多ICRL研究使用标准RL算法进行预训练。这引出了本文旨在解决的核心问题：为什么RL预训练算法能够生成使ICRL成为可能的网络参数？我们假设能够实现ICRL的参数是预训练损失的最小值点（minimizers）。本研究通过案例研究为这一假设提供了初步支持。具体而言，我们证明了当Transformer（一种神经网络架构）为策略评估（policy evaluation）进行预训练时，预训练损失的一个全局最小值点（global minimizers）可以实现上下文时序差分学习（in-context temporal difference learning）。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing",
                    "arxiv_id": "2509.20336",
                    "authors": "Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang",
                    "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断 这篇论文的本质是研究Transformer-based LLMs在图推理任务中的内部机制和工作原理。论文使用了circuit-tracer框架来解释decoder-only transformers的推理过程，识别了图推理中的两个核心机制（token merging和structural memorization），并提供了一个统一的可解释性框架。这属于研究LLM基础能力和推理机制的范畴，而不是将LLM作为工具应用到特定领域，因此应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念：明确提到了\"Transformer-based LLMs\" - 能力方向：聚焦于\"graph reasoning\"，属于推理能力的研究范畴 虽然论文没有提到强化学习等训练方法或智能体等新兴范式，但已经包含了两个重要的正面指标。 第三步：排除标准 论文没有主要聚焦于任何排除标准中提到的领域： - 不涉及多模态与视觉研究 - 不针对特定应用领域（如医疗、化学等），虽然研究图推理，但这是作为理解LLM通用推理能力的一个窗口 - 不关注模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况 论文涉及可解释性研究，但这是为了理解LLM的推理机制，提供\"统一的可解释性框架来理解结构推理\"，从而提升对模型内部工作原理的理解，符合研究目标，应该保留。 综上所述，这篇论文的核心贡献是揭示和解释LLM在图推理任务中的内部机制，这直接关系到提高LLM的通用推理能力，符合研究目标。",
                    "summary2": "抱歉，我无法根据提供的内容生成学术总结。提供的链接返回了404错误，显示\"File unavailable for 2509.20336\"，表明该论文ID对应的文件在arXiv上不可用或不存在。没有实际的论文内容，我无法提取研究问题、方法创新和实验验证等关键信息来生成专业的学术总结。请提供有效的论文链接或内容，我将很乐意为您生成符合要求的学术总结。",
                    "summary_translation": "基于Transformer的大型语言模型（Transformer-based LLMs）在图推理任务（graph reasoning tasks）上表现出强大的性能，然而其内部机制（internal mechanisms）仍未被充分探索。为了以基础且统一的视角揭示这些推理过程机制（reasoning process mechanisms），我们使用了基本的仅解码器Transformer（basic decoder-only transformers），并采用电路追踪框架（circuit-tracer framework）对其进行解释。通过这一视角，我们可视化推理轨迹（reasoning traces），并识别出图推理中的两个核心机制：令牌合并（token merging）和结构记忆（structural memorization），这两个机制是路径推理（path reasoning）和子结构提取任务（substructure extraction tasks）的基础。我们进一步量化了这些行为（behaviors），并分析了它们如何受到图密度（graph density）和模型规模（model size）的影响。我们的研究为理解仅解码器Transformer（decoder-only Transformers）中的结构推理（structural reasoning）提供了一个统一的可解释性框架（unified interpretability framework）。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Failure Modes of Maximum Entropy RLHF",
                    "arxiv_id": "2509.20265",
                    "authors": "Ömer Veysel Çağatan, Barış Akgün",
                    "summary": "In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning with length-normalized temperature, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，论文的核心是关于RLHF（Reinforcement Learning from Human Feedback）的研究，特别是最大熵RLHF的失败模式分析。RLHF是改进大语言模型基础能力的关键训练范式，属于\"强化学习优化\"这一类别，符合第一步的保留标准。论文虽然没有直接提出新方法，而是分析现有方法的问题，但这种分析有助于理解如何更好地训练LLM，从而提升其通用能力。其次，论文明确涉及强化学习（RLHF）这一训练方法，符合第二步中的正面指标。同时，论文不聚焦于多模态、特定应用领域或模型可靠性的应用层面，因此不触犯第三步的排除标准。总体而言，这篇论文对改进LLM的训练方法有贡献，与提升大语言模型通用推理能力的研究目标相符。",
                    "summary2": "本文旨在研究最大熵强化学习在人类反馈强化学习(RLHF)中的失效模式。针对在线和离线偏好学习场景，我们提出了一种将SimPO解释为长度归一化温度的最大熵RL的理论框架，并在TL;DR数据集上通过胜率和KL散度等指标验证了其有效性。实验发现，尽管SimPO在离线设置中表现良好，但在线最大熵RL存在过优化和不稳定问题，表明熵正则化无法有效防止奖励 hacking。",
                    "summary_translation": "本文表明，简单偏好优化（Simple Preference Optimization, SimPO）可被推导为具有长度归一化温度的最大熵强化学习（Maximum Entropy Reinforcement Learning），为这种无参考方法（reference-free method）提供了理论基础。受SimPO在离线偏好优化中出色表现的启发，我们研究了最大熵强化学习是否能在在线RLHF（基于人类反馈的强化学习）设置中取得类似结果。我们的实验发现，即使在非常低的学习率下，最大熵强化学习也始终表现出过度优化（overoptimization）和不稳定的KL（Kullback-Leibler）动态。与能够保持稳定训练的KL约束方法不同，熵正则化（entropy regularization）未能防止奖励黑客（reward hacking），并且似乎与过度优化相关。最后，我们讨论了为什么SimPO在离线设置中成功而最大熵强化学习在在线场景中挣扎的可能解释。我们的研究结果表明，无参考方法在应用于在线或离线偏好学习时可能面临不同的挑战。",
                    "inspiration_trace": ""
                },
                {
                    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
                    "arxiv_id": "2509.19894",
                    "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong",
                    "summary": "Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at https://github.com/inclusionAI/PromptCoT.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出PromptCoT 2.0框架，这是一种新的训练范式，旨在通过合成高质量提示词来增强LLM的基础推理能力。论文明确关注提升LLM在数学推理和编程推理等通用能力上的表现，而非将LLM作为工具应用于特定领域。 从正面指标看，论文包含多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于reasoning(特别是数学推理)、planning和problem-solving；训练方法上涉及self-play(类似强化学习的思想)和self-evolve(模型通过自我博弈自主改进)。 论文不涉及任何排除标准中的领域：没有关注多模态与视觉，没有将LLM应用于特定领域(虽然使用数学和编程作为评估任务，但这些是用于评估通用推理能力的基准)，也没有主要关注模型可靠性的应用层面。 在特殊情况下，论文提到的self-play可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用于特定领域的智能体。 论文的核心贡献是提出了一种可扩展的框架，通过迭代改进提示词构建来生成更难且更多样化的问题，从而提升LLM的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。",
                    "summary2": "本文旨在解决大型语言模型推理任务中高质量训练数据短缺的问题。针对数学和编程领域的数据需求，我们提出了一种基于期望最大化(EM)循环优化的PromptCoT 2.0框架，通过迭代改进推理来指导提示构建，生成更难且更多样化的问题。在AIME、HMMT、LiveCodeBench和Codeforces等六个基准测试上，通过pass@1准确率和Elo评级验证了其有效性，在Self-Play和SFT两种设置下均取得了最先进结果。",
                    "summary_translation": "大型语言模型（Large language models, LLMs）正在从对话系统演变为强大的推理器，用于处理奥林匹克数学和竞技编程等任务。虽然扩大参数规模和测试时计算（test-time computation）推动了进步，但一个关键瓶颈是缺乏高质量的训练问题：人工策划的数据集成本高且有限，而现有的合成语料库（synthetic corpora）通常过于简单或过于狭窄。\n\nPromptCoT 1.0表明，将推理过程（rationales）注入提示合成（prompt synthesis）可以增加问题难度。在此基础上，我们提出了PromptCoT 2.0，这是一个可扩展的框架，用期望最大化（expectation-maximization, EM）循环替代手工设计的启发式算法（hand-crafted heuristics），其中推理过程被迭代优化以指导提示构建。这产生的比先前语料库更难且更多样化的问题。\n\n这些合成提示支持两种后训练（post-training）机制：（1）自我对弈（Self-Play），即强模型通过可验证的反馈（verifiable feedback）在没有更强教师的情况下自主改进；（2）监督微调（Supervised Fine-Tuning, SFT），即弱模型从教师提炼的痕迹（teacher-distilled traces）中学习。\n\n大量实验证明了这种方法的有效性。在自我对弈中，将PromptCoT 2.0应用于Qwen3-30B-A3B-Thinking-2507在300亿参数规模上创造了新的最先进（state-of-the-art）结果，在AIME 24/25和HMMT 25上分别提升了+4.4、+4.8和+5.3，在LiveCodeBench v5/v6上提升了+6.1和+5.0，在Codeforces上提升了+35 Elo。在SFT中，仅在合成提示上训练Qwen2.5-7B-Instruct将准确率提升至73.1（AIME 24）、65.6（AIME 25）和53.4（LiveCodeBench v5），超过了在人工或混合数据上训练的模型。\n\n分析进一步证实，PromptCoT 2.0产生了本质上更难且分布不同（distributionally distinct）的问题。这些结果确立了提示合成作为扩展推理能力的新维度（new axis for scaling reasoning），并将PromptCoT 2.0定位为未来开源模型的可扩展基础。实现代码可在https://github.com/inclusionAI/PromptCoT获取。",
                    "inspiration_trace": ""
                },
                {
                    "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
                    "arxiv_id": "2509.19803",
                    "authors": "Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang",
                    "summary": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步：核心判断 这篇论文的本质是提出一种名为VCRL的课程强化学习框架，用于改进大语言模型的数学推理能力。论文核心关注的是改进LLM的基础推理能力，提出了一种新的训练范式（基于方差的课程强化学习），这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的推理能力。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning - 训练方法：提出了reinforcement learning的新方法(VCRL) 第三步：排除标准 论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不针对特定应用领域（如医疗、化学等），虽然实验在数学推理任务上进行，但数学推理被视为通用推理能力的基础组成部分 - 不关注模型基础设施、部署优化或硬件加速 第四步：特殊和模糊情况处理 虽然论文在数学推理任务上进行了实验评估，但这并不使其成为特定应用领域的研究。数学推理通常被视为评估和提升LLM通用推理能力的关键基准。论文提出的VCRL是一种通用的课程强化学习框架，其原理可以推广到其他需要逐步学习的推理任务上，因此应被视为对LLM通用推理能力的提升。 综上所述，这篇论文的核心贡献是提出了一种基于方差的课程强化学习方法，通过动态控制训练样本难度来提升LLM的推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。",
                    "summary2": "本文旨在解决大语言模型在数学推理任务中强化学习训练时没有考虑样本难度匹配的问题。针对不同难度的数学推理样本，我们提出了一种基于方差的课程强化学习框架VCRL，并在五个数学基准测试上通过准确率等指标验证了其有效性。",
                    "summary_translation": "基于策略的强化学习（policy-based reinforcement learning）目前在提高大型语言模型（LLMs）的数学推理任务能力方面发挥着重要作用。然而，现有的基于rollout的强化学习方法（如GRPO、DAPO、GSPO等）未能明确考虑LLMs对不同难度级别样本的学习能力，这与人类从易到难的数学推理认知过程相悖。直观上，我们发现RLVR中rollout组的奖励（reward）方差部分反映了当前样本对LLMs的难度。过于简单或过于困难的样本具有较低的方差，而中等难度的样本则具有较高的方差。基于此，我们提出了VCRL，一种基于组奖励方差动态控制训练样本难度的课程强化学习（curriculum reinforcement learning）框架。在五个数学基准和两个模型上的实验揭示了VCRL相较于当前LLM强化学习基线方法的优势。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
                    "arxiv_id": "2509.19702",
                    "authors": "Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama",
                    "summary": "We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我认为这篇论文符合研究范围。以下是我的判断过程： 第一步：核心判断上，论文本质是研究线性注意力transformer如何通过训练隐式发现数值算法的能力。这属于改进LLM基础能力的研究，特别是增强其数学推理和问题解决能力。论文探索了模型如何通过矩阵补全任务自动学习数学计算规则，而非被明确编程这些规则，这符合\"提出新的训练范式、增强其逻辑、数学推理等通用能力\"的保留标准。 第二步：正面指标方面，论文包含以下相关主题： - 核心概念：提及\"Linear Transformers\"，与LLMs直接相关 - 能力方向：涉及数值算法的发现，与数学推理和问题解决能力密切相关 - 论文展示了transformer的上下文学习能力，这是提升通用推理能力的关键 第三步：排除标准方面，论文不涉及任何排除领域： - 没有多模态与视觉内容 - 没有将LLM应用于特定领域（如医疗、化学等） - 没有聚焦于模型可靠性的应用层面（如水印、安全等） 第四步：论文不涉及特殊或模糊情况（如智能体/工具使用、幻觉/可解释性等），所以无需额外判断。 综合来看，这篇论文的核心贡献在于展示了transformer如何通过训练隐式发现统一的数值算法，这直接提升了LLM的数学推理和问题解决能力，属于\"大语言模型通用推理能力\"的研究范围。论文关注的是模型的基础能力提升，而非特定领域应用，因此符合研究目标。",
                    "summary2": "",
                    "summary_translation": "我们在数百万个masked-block matrix completion tasks（掩码块矩阵补全任务）上训练了一个linear attention transformer（线性注意力Transformer）：每个提示是一个masked low-rank matrix（掩码低秩矩阵），其缺失的块可能是(i)一个scalar prediction target（标量预测目标）或(ii)一个Nyström extrapolation（Nyström外推）的unseen kernel slice（未见核切片）。模型仅看到输入-输出对和mean-squared loss（均方损失）；它没有被给予normal equations（正规方程）、handcrafted iterations（手工设计的迭代），也没有任何关于这些任务相关的提示。令人惊讶的是，训练后，algebraic unrolling（代数展开）揭示了在三个不同的computational regimes（计算机制）中相同的parameter-free update rule（无参数更新规则）：full visibility（完全可见性）、rank-limited updates（秩限制更新）和distributed computation（分布式计算）。我们证明该规则在full-batch problems（全批量问题）上实现了second-order convergence（二阶收敛），降低了distributed iteration complexity（分布式迭代复杂度），并在rank-limited attention（秩限制注意力）下保持准确性。因此，一个仅被训练来补全缺失块的transformer隐式地发现了一个统一的、resource-adaptive iterative solver（资源自适应迭代求解器），涵盖预测、估计和Nyström extrapolation（Nyström外推），突显了in-context learning（上下文学习）的强大能力。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Mamba Modulation: On the Length Generalization of Mamba",
                    "arxiv_id": "2509.19633",
                    "authors": "Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui",
                    "summary": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文的核心是研究Mamba架构（一种大语言模型架构）在长上下文泛化方面的局限性，并提出了一种改进方法。论文通过分析Mamba在处理比预训练时更长的上下文时性能下降的问题，将其归因于状态空间动态的参数化，特别是状态转移矩阵A的特性。作者提出了一种频谱缩放方法来改进预训练Mamba模型，以实现稳健的长上下文泛化。这符合我的研究目标，因为论文关注的是改进大语言模型的基础能力（特别是长上下文推理能力），而不是将LLM应用于特定领域。论文的贡献在于增强模型本身的通用推理能力，使其能够更好地处理长序列，这对于复杂的多步推理任务至关重要。因此，这篇论文应该被保留。",
                    "summary2": "本文旨在解决Mamba模型在处理超出预训练长度的上下文时性能显著下降的问题。针对长上下文泛化场景，我们提出了一种基于状态转移矩阵A频谱缩放的方法，并在ProofPile、PG19等数据集以及Passkey检索和LongBench基准上通过困惑度和准确率等指标验证了其有效性。",
                    "summary_translation": "Transformer模型中注意力机制（attention mechanism）的二次复杂度（quadratic complexity）推动了具有次二次扩展（sub-quadratic scaling）的替代架构的发展，如状态空间模型（state-space models）。其中，Mamba已成为一种领先的架构，在一系列语言建模任务中取得了最先进（state-of-the-art）的成果。\n\n然而，当Mamba应用于比预训练期间所见更长的上下文时，其性能显著下降，显示出对上下文长度扩展的明显敏感性。通过详细分析，我们将这一限制归因于其状态空间动态（state-space dynamics）的分布外行为（out-of-distribution behaviour），特别是在状态转移矩阵（state transition matrix）$\\mathbf{A}$的参数化中。与近期将这种敏感性归因于离散时间步长（discretization time steps）$\\exp(-\\sum_{t=1}^N\\Delta_t)$的消失累积的研究不同，我们建立了当输入长度趋近于无穷大时状态收敛行为与转移矩阵$\\mathbf{A}$的谱（spectrum）之间的联系，为其在长度扩展中的作用提供了充分依据的解释。\n\n接下来，为了克服这一挑战，我们提出了一种将谱缩放（spectrum scaling）应用于预训练Mamba模型的方法，通过选择性调制（modulating）每一层中$\\mathbf{A}$矩阵的谱来实现稳健的长上下文泛化（long-context generalization）。我们表明，在仅仅调制$\\Delta_t$失效的情况下，这种方法可以显著提高性能，验证了我们的见解，并为具有结构化转移矩阵（structured transition matrices）的状态空间模型提供了更好的长度泛化（length generalization）途径。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Thinking Augmented Pre-training",
                    "arxiv_id": "2509.20186",
                    "authors": "Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei",
                    "summary": "This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\\%$ on several challenging reasoning benchmarks.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文本质是改进LLM的基础能力和通用推理能力。论文提出\"Thinking augmented Pre-Training (TPT)\"方法，通过在文本数据中增加思维轨迹来增强LLM的预训练过程。这是一种新的训练范式，旨在通过逐步推理和分解来提高模型的学习效率和推理能力，而不是将LLM应用于特定领域。 第二步正面指标：论文包含多个正面指标。它明确关注\"Large language models (LLMs)\"这一核心概念，并专注于\"reasoning\"能力方向（论文提到\"step-by-step reasoning\"和\"reasoning benchmarks\"）。实验结果也证明该方法在多个具有挑战性的推理基准上提升了模型性能。 第三步排除标准：论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊情况，它直接关注通过思维轨迹增强来提升LLM的通用推理能力。 最终决策：论文的核心贡献是提出了一种通用的训练方法(TPT)来增强LLM的推理能力，提高数据效率，这与研究目标完全一致。实验表明该方法能显著提升模型在推理任务上的表现，因此应该被保留。",
                    "summary2": "本文旨在提高大型语言模型(LLM)训练的数据效率。针对高质量训练数据有限且某些token难以直接学习的问题，我们提出了Thinking augmented Pre-training (TPT)，一种通过自动生成思维轨迹增强现有文本数据的通用方法。我们在多种训练配置上（包括数据受限和充足情况下的预训练以及中期训练）通过推理基准和语言理解任务验证了其有效性，实验表明TPT将LLM预训练的数据效率提高了3倍，显著提升了模型性能。",
                    "summary_translation": "本文介绍了一种简单且可扩展的方法，通过用思维轨迹(thinking trajectories)增强现有文本来提高大型语言模型(Large Language Model, LLM)训练的数据效率。大型语言模型预训练的计算量一直在以前所未有的速度增长，而高质量数据的可用性仍然有限。因此，最大化可用数据的效用构成了一个重大的研究挑战。一个主要障碍是，在固定模型容量下，某些高质量标记(tokens)难以学习，因为单个标记的基本原理可能异常复杂和深入。为解决这一问题，我们提出了思维增强预训练(Thinking augmented Pre-Training, TPT)，这是一种通过自动生成的思维轨迹增强文本的通用方法。这种增强有效增加了训练数据的体量，并通过逐步推理和分解使高质量标记更易学习。我们在多种训练配置中应用TPT，规模高达1000亿(tokens)标记，包括数据受限和数据充足情况下的预训练，以及从强大的开源检查点(checkpoints)进行的中期训练。实验结果表明，我们的方法显著提高了各种规模和系列的大型语言模型的性能。值得注意的是，TPT将大型语言模型预训练的数据效率提高了3倍。对于一个30亿参数(3B parameters)的模型，它在几个具有挑战性的推理基准(benchmarks)上将训练后性能提高了超过10%。",
                    "inspiration_trace": ""
                },
                {
                    "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
                    "arxiv_id": "2509.19736",
                    "authors": "Cheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin Qiu, Zhiwei Liu, Haolin Chen, Shirley Kokane, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang",
                    "summary": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.",
                    "category": "cs.LG",
                    "filter_reason": "这篇论文提出了UserRL框架，通过强化学习训练交互式以用户为中心的智能体，核心是关于提升大语言模型的交互能力和多轮对话能力。从第一步核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式（UserRL框架），并使用强化学习（GRPO算法）来优化模型，这符合\"改进LLM基础能力和提出新训练范式\"的保留标准。 从第二步正面指标看，论文包含多个相关主题：1)核心概念方面，论文明确基于Qwen3等大语言模型；2)训练方法方面，论文使用了强化学习(RL)方法；3)新兴范式方面，论文关注的是基于LLM的智能体(llm-based agents)的训练。 从第三步排除标准看，论文没有涉及多模态与视觉内容，没有聚焦于特定应用领域（如医疗、化学等），也没有主要关注模型可靠性的应用层面问题（如水印、安全性），因此不符合任何排除标准。 从第四步特殊和模糊情况处理看，论文提出的是通用的智能体训练框架，旨在增强LLM的通用交互能力，而不是将智能体应用在特定领域，因此应该保留。 综合分析，论文的核心贡献是提供了一种新的强化学习框架来提升LLM的交互式推理和多轮对话能力，这属于大语言模型通用推理能力的重要组成部分，与研究目标高度一致。",
                    "summary2": "本文旨在解决如何训练能有效获取用户中心能力的智能体模型，同时考虑用户交互多样性和动态性的问题。针对多轮用户交互场景，我们提出了一种UserRL框架，结合标准化gym环境和模拟用户，并在Qwen3模型上通过不同奖励设计策略验证了其有效性。",
                    "summary_translation": "强化学习(Reinforcement Learning, RL)在训练智能体模型(agentic models)方面展现出潜力，这些模型能够超越静态基准(static benchmarks)，进行动态、多轮交互(dynamic, multi-turn interactions)。然而，这类智能体的最终价值在于其协助用户的能力，而在这种场景中，用户交互的多样性和动态性带来了挑战。\n\n在这项工作中，我们提出了UserRL，这是一个通过标准化gym环境(standardized gym environments)与模拟用户(simulated users)相结合，用于训练和评估以用户为中心能力(user-centric abilities)的统一框架。我们系统地改变轮级奖励分配(turn-level reward assignment)和轨迹级分数计算(trajectory-level score calculation)，以分析不同公式化方法如何影响GRPO算法(GRPO algorithm)下的学习效果。\n\n我们在Qwen3模型上的实验揭示了三个关键发现：(i) SFT冷启动(SFT cold start)对于解锁初始交互能力和实现持续的强化学习改进至关重要；(ii) 精心设计的轨迹评分(deliberate trajectory scoring)能够产生更高效、更有效的多轮交互；(iii) 尽管更强大的模拟用户(如GPT-4o)有助于训练，但开源模拟器(如Qwen3-32B)仍然是一种经济高效且可转移的选择。\n\n总的来说，这些结果强调，奖励塑形(reward shaping)和用户模拟选择的精心设计与模型规模同等重要，并将UserRL确立为开发稳健的以用户为中心的智能体模型的实用途径。所有代码和数据均公开，以供未来研究使用。",
                    "inspiration_trace": ""
                },
                {
                    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
                    "arxiv_id": "2509.19517",
                    "authors": "Sai Teja Reddy Adapala",
                    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，该论文的本质是研究大语言模型在认知负荷下的多跳推理能力限制。论文提出了计算认知负荷的形式理论，设计了新的评估方法(ICE基准测试)，旨在理解和提升LLMs在复杂推理任务中的表现。这明显属于改进LLM基础推理能力的研究，特别是关注多跳推理这一通用推理能力的核心方面，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标：它明确关注\"Large Language Models (LLMs)\"这一核心概念，并深入研究\"multi-hop reasoning\"(多跳推理)这一关键推理能力方向。虽然论文不涉及训练方法和新兴范式，但对推理能力的深入研究已足够表明其与研究目标的高度相关性。 第三，论文不符合任何排除标准：它不涉及多模态与视觉研究，不聚焦于任何特定应用领域(如医疗、化学等)，也不主要关注模型可靠性的应用层面问题(如水印、安全性等)。虽然论文结尾提到了\"安全性\"评估，但这是作为评估模型推理能力的一个方面，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了新的理论框架和评估方法来理解和提升LLMs的通用推理能力，特别是在认知负荷条件下的多跳推理表现，完全符合研究目标。",
                    "summary2": "本文旨在研究大型语言模型在认知负荷下的多跳推理能力限制。针对信息丰富、任务切换的动态场景，我们提出了计算认知负荷理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试系统操纵上下文饱和和注意力残留因素。在五个LLMs上通过Exact-Match准确率验证发现：Gemini-2.0-Flash-001在控制条件下达85%准确率，但在额外信息增加时性能显著下降(β = -0.003, p < 0.001)，而较小模型如Llama-3-8B-Instruct在所有条件下均表现完全失效。",
                    "summary_translation": "大型语言模型（Large Language Models, LLMs）的扩展已经揭示了其在静态基准测试上的表现与在动态、信息丰富环境中的脆弱性之间存在的关键差距。尽管模型在孤立任务上表现出色，但控制其在认知负荷下推理的计算限制仍然知之甚少。在这项工作中，我们提出了计算认知负荷（computational cognitive load）的正式理论，假设外部的、与任务无关的信息（Context Saturation，上下文饱和）和任务切换的干扰（Attentional Residue，注意残留）是导致性能下降的关键机制。我们设计了交错认知评估（Interleaved Cognitive Evaluation, ICE），这是一个去混淆的基准测试，用于在具有挑战性的多跳推理（multi-hop reasoning）任务上系统地操纵这些负荷因素。一项全面研究（200个问题中每个项目重复10次）揭示了五个经过指令调优（instruction-tuned）的模型之间存在显著的性能差异。较小的开源架构（Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2）表现出基准脆弱性，在这个高内在负荷（high-intrinsic-load）任务中，包括清洁对照组在内的所有条件下准确率均达到0%（SEM = 0.0）。相比之下，Gemini-2.0-Flash-001表现出部分韧性，在对照组条件下达到85%的准确率，在上下文饱和条件下出现统计学显著的下降（$\\beta = -0.003$每增加1%负荷，$p < 0.001$）。这些发现提供了初步证据，表明认知负荷是推理失败的关键因素，支持了在不确定性下幻觉即猜测（hallucination-as-guessing）的理论。我们得出结论，以ICE基准测试为例的动态、认知感知的压力测试对于评估先进AI系统的真正韧性和安全性至关重要。",
                    "inspiration_trace": ""
                },
                {
                    "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution",
                    "arxiv_id": "2509.19349",
                    "authors": "Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin",
                    "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.",
                    "category": "cs.LG",
                    "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力和提出新的训练范式。论文提出的ShinkaEvolve框架利用LLMs作为变异算子，通过演化的方式增强其问题解决和推理能力。论文的核心贡献是提高LLM在程序演化方面的样本效率和解决方案质量，而不是将LLM作为工具应用于特定领域，因此符合保留标准。 其次，论文包含多个正面指标：明确涉及大语言模型(LLMs)作为核心组件；关注推理能力，特别是在数学推理任务中的应用；采用演化(evolution)作为训练方法；并使用了基于LLM的智能体框架(evolutionary agentic harnesses)。 第三，论文不符合排除标准。虽然论文提到了一些应用案例(如圆包装、数学推理、竞争编程等)，但这些是作为评估框架性能的示例，而非论文的主要焦点。论文的核心是提出一种通用的程序演化框架，而非专注于多模态、特定应用领域或模型可靠性等排除领域。 最后，在特殊和模糊情况的处理上，论文提出的是一种通用的演化智能体框架，旨在增强LLM的通用问题解决能力，而不是针对特定领域的应用，因此应该保留。 综合来看，ShinkaEvolve论文致力于通过演化的方法增强LLM的通用推理和问题解决能力，与研究目标高度一致。",
                    "summary2": "",
                    "summary_translation": "我们介绍了ShinkaEvolve：一个新的开源框架，该框架利用大型语言模型（LLMs, Large Language Models）来推动科学发现，具有最先进的性能和前所未有的效率。最近在扩展大型语言模型推理时间计算方面的进展，为通用科学发现带来了显著进步。这些方法依赖于进化智能代理框架（evolutionary agentic harnesses），该框架利用大型语言模型作为变异算子（mutation operators）来生成候选解决方案。然而，当前的代码进化方法存在关键局限性：样本效率低下（sample inefficient），需要数千个样本才能识别有效解决方案，并且仍然是闭源的，阻碍了广泛采用和扩展。\n\nShinkaEvolve解决了这些局限性，引入了三项关键创新：一种平衡探索与利用的父代采样技术（parent sampling technique），用于高效搜索空间探索的代码新颖性拒绝采样（code novelty rejection-sampling），以及基于多臂老虎机（bandit-based）的大型语言模型集成选择策略（LLM ensemble selection strategy）。我们在多样化任务上评估了ShinkaEvolve，展示了在样本效率和解决方案质量上的一致性改进。\n\nShinkaEvolve仅使用150个样本就发现了一种新的最先进的圆形打包（circle packing）解决方案，为AIME数学推理任务设计了高性能的智能代理框架，识别出ALE-Bench竞赛编程解决方案的改进，并发现了新颖的专家混合负载平衡损失函数（mixture-of-expert load balancing loss functions），这些函数阐明了优化策略的空间。我们的结果表明，ShinkaEvolve实现了广泛的应用性和卓越的样本效率。通过提供开源的可访问性和成本效益，这项工作使多样化的计算问题中的开放式发现（open-ended discovery）变得民主化。",
                    "inspiration_trace": ""
                },
            ]
        },
    ],
};

const dailyOverviews = {
    "2025-09-30": `### 今日AI论文速览 (2025-09-30)

今日AI研究呈现出多方向并进的繁荣景象，其中多智能体系统的自进化架构、推理模型的训练优化、新型记忆机制设计以及对齐方法的创新成为最突出的研究方向。这些研究不仅探索了模型能力边界的拓展，还关注了效率提升和认知机制模拟，展现了从架构创新到训练策略的全方位突破。

## 一、多智能体系统的自进化与交互

多智能体系统正朝着更加自主、自适应和可交互的方向发展，研究者们探索了如何让智能体系统实现自我生成、自我配置和自我修正。

* **MAS²**提出了一种基于递归自生成的多智能体系统范式，通过"**生成器-实现器-修正器**"三智能体团队动态组合和自适应修正目标智能体系统。在七个基准测试中，MAS²比最先进的多智能体系统性能提升高达**19.6%**，并展现出卓越的跨主干泛化能力。(2509.24323 [cs.MA])

* **AIPOM**引入了一种支持人机在环规划的系统，通过对话和基于图形的界面使用户能够透明地检查、完善和协同指导LLM生成的计划。AIPOM显著增强了用户对多智能体工作流程的控制和信任，解决了现有LLM方法缺乏有效机制让用户检查、理解和控制其行为的问题。(2509.24826 [cs.MA])

* **Diagnose, Localize, Align**提出了一个三层框架来解决多智能体系统在指令冲突下的层次合规性问题：诊断（**CRAS评分指标**）、定位（注意力漂移分析）和对齐（**SAIL方法**）。该方法在标准基准和多智能体框架上提高了指令层次合规性，无需全模型微调。(2509.23188 [cs.CL])

## 二、推理模型的训练与优化

推理能力作为大语言模型的核心竞争力，今日研究在训练方法、计算效率和推理架构上均有重要突破，特别是在小模型推理能力提升方面成果显著。

* **MobileLLM-R1**挑战了推理能力需要在大规模数据集上训练的假设，证明仅需约**2T token**的高质量数据就足以开发出强大的推理模型。MobileLLM-R1-950M在AIME上得分15.5，远超OLMo-2-1.48B的0.6分和SmolLM-2-1.7B的0.3分，尽管训练token数量仅为Qwen3的11.7%。(2509.24945 [cs.CL])

* **SIRI**提出了一种简单而有效的RL方法，通过在训练期间动态调整最大滚动长度来迭代交替**压缩和扩展推理预算**。在DeepSeek-R1-Distill-Qwen-1.5B上训练后，SIRI-low在AIME24上性能提升43.2%，同时token使用减少46.9%。(2509.25176 [cs.CL])

* **MARCOS**重新构想LLM中的推理，将其建模为连续高维"思想"的**隐马尔可夫链**，而不是自回归生成token序列。在三个基准测试中，MARCOS优于现有的连续推理方法，首次实现了与基于token的CoT相当的性能，在GSM8K上甚至超过4.7%，推理速度提高15.7倍。(2509.25020 [cs.LG])

* **CLPO**提出了一种动态教学反馈循环算法，利用模型自身的滚动性能进行实时难度评估，构建**在线课程**。CLPO在八个具有挑战性的数学和一般推理基准上实现了最先进的性能，平均pass@1比其他方法提高6.96%。(2509.25004 [cs.AI])

## 三、记忆架构与认知机制

如何让AI智能体拥有更接近人类的记忆和认知能力是今日研究的热点，多种新型记忆架构被提出，旨在实现更自然的信息处理和自我进化能力。

* **MemGen**提出了一种动态生成记忆框架，配备类人认知能力，包括**记忆触发器**和**记忆编织器**，使智能体能够回忆和增强潜在记忆。在八个基准测试中，MemGen超过了领先的外部记忆系统如ExpeL和AWM高达38.22%，超过了GRPO高达13.44%，并展现出强大的跨领域泛化能力。(2509.24704 [cs.CL])

* **ReasoningBank**提出了一种记忆框架，从智能体自判断的成功和失败经验中提炼可泛化的**推理策略**。在网页浏览和软件工程基准测试中，ReasoningBank始终优于存储原始轨迹或仅成功任务例程的现有记忆机制。(2509.25140 [cs.CL])

* **Identity Bridge**通过监督模型执行零跳身份任务解决了组合性差距，使模型能够成功执行分布外两跳推理。理论分析表明，身份监督通过优化过程中隐含的**核范数正则化**重塑了模型的潜在几何，诱导有利于低秩解决方案的潜在空间对齐。(2509.24653 [cs.LG])

## 四、对齐与偏好学习的新方法

模型对齐技术持续创新，研究者们探索了更高效、更鲁棒的偏好学习方法，以解决传统对齐方法中的分布不匹配和噪声敏感问题。

* **UniAPL**重新将对齐定义为约束优化问题，提出**统一对抗偏好学习**框架，动态地将策略的分布与专家的分布对齐。在Qwen3-0.6B上比强GRPO基线提高5.77%，在Qwen3-4B上提高3.75%，甚至在某些任务上超越了教师模型。(2509.25148 [cs.AI])

* **Robust Preference Optimization**提出了一种鲁棒偏好优化方法，使用**期望最大化算法**推断每个标签正确性的后验概率，自适应地重新权衡训练损失中的每个数据点。实验证明RPO作为元框架的一致有效性，持续增强了四种最先进对齐算法（DPO、IPO、SimPO和CPO）。(2509.24159 [cs.AI])

* **Why Alignment Must Precede Distillation**论证了标准KD->Align工作流程削弱了模型对齐罕见但理想行为的能力，即使在强偏好信号下也是如此。理论和实验证明，反转流水线（即**Align->KD**）是必不可少的：对齐必须首先在高召回参考模型上执行，然后再进行蒸馏。(2509.23667 [cs.LG])

## 五、工具增强智能体与信息检索

工具使用能力作为智能体的核心竞争力，今日研究在工具集成推理、信息检索和深度研究方面取得了显著进展，特别是在提升小模型工具使用效率方面。

* **InfoAgent**引入了一种由创新数据合成管道和编排的网络搜索工具驱动的**深度研究智能体**。InfoAgent在BrowseComp上达到15.3%准确率，在BrowseComp-ZH上达到29.2%，在Xbench-DS上达到40.4%，优于先前的开源深度研究智能体。(2509.25189 [cs.CL])

* **Fathom-DeepResearch**引入了一个由两个专门模型组成的智能体系统：**Fathom-Search-4B**（通过实时网络搜索和针对性网页查询进行基于证据的调查）和**Fathom-Synthesizer-4B**（将多轮DeepSearch轨迹转换为结构化的引用密集型DeepResearch报告）。该系统在DeepSearch基准测试和DeepResearch-Bench上实现了开放权重类别中最先进的性能。(2509.24107 [cs.LG])

* **Learning to Use Tools, Not Just When**识别了工具集成推理中的两种常见模式：**计算器模式**（使用代码进行直接计算）和**算法模式**（将问题编码为程序）。在具有挑战性的数学数据集上，这种模式感知方法显著提高了代码使用率和准确性，例如将MATH500上的Code@1从64.0%提高到70.5%。(2509.23292 [cs.CL])

## 六、模型架构与训练创新

基础模型架构和训练方法持续创新，研究者们探索了替代传统自回归生成的新方法，以及更高效的模型训练和优化技术。

* **Alternatives To Next Token Prediction In Text Generation - A Survey**调查了替代下一个token预测(NTP)的新兴生态系统，将方法分为五个主要家族：**多Token预测**、**计划后生成**、**潜在推理**、**连续生成方法**和**非Transformer架构**。该调查为开发解决token级生成已知局限性、为自然语言处理开发新变革性模型的模型提供了分类法。(2509.24435 [cs.CL])

* **LLaDA-MoE**引入了一种具有**混合专家(MoE)架构**的大语言扩散模型，从零开始在约20T token上训练。LLaDA-MoE在推理时仅激活1.4B参数，实现了显著减少的计算开销下的竞争性能，在多个基准测试上超越了之前的扩散语言模型。(2509.24389 [cs.CL])

* **Evolution Strategies at Scale**报告了首次成功将**进化策略(ES)**扩展用于微调LLM的全部参数，证明ES可以在数十亿参数上有效搜索，并在多个方面优于现有的RL微调方法。ES在样本效率、对长时程奖励的容忍度、对不同基础LLM的鲁棒性、对奖励黑客的抵抗能力以及运行间性能稳定性方面表现出优势。(2509.24372 [cs.LG])

### 今日看点

1. **小模型推理能力的重大突破**：MobileLLM-R1和SIRI等研究证明，通过精心设计的高质量数据和创新的训练策略，亚十亿参数的小模型也能实现强大的推理能力，挑战了"推理能力仅在大模型中涌现"的传统认知，为资源受限场景下的AI应用开辟了新路径。

2. **多智能体系统的自进化趋势**：MAS²和MemGen等研究展示了多智能体系统从静态配置向动态自演进的转变，这些系统能够根据任务需求自主生成、配置和修正自身架构，展现出接近组织的自适应能力，为构建更复杂、更灵活的AI系统提供了新范式。

3. **推理架构的多元化探索**：从MARCOS的连续思想链到SIRI的压缩-扩展交替训练，研究者们正在突破传统自回归token生成的限制，探索更高效、更接近人类认知过程的推理架构，这些创新不仅提高了推理效率，还为理解AI推理机制提供了新视角。

4. **对齐技术的效率与鲁棒性并重**：UniAPL和Robust Preference Optimization等研究在对齐方法上实现了重要突破，既解决了传统方法中的分布不匹配问题，又提高了对噪声数据的鲁棒性，同时降低了对大规模人工标注的依赖，为构建更可靠、更高效的AI对齐系统提供了新思路。`,
    "2025-09-29": `### 今日AI论文速览 (2025-09-29)

#### 开篇导语
今日AI研究呈现出智能体系统与推理增强两大核心趋势，同时强化学习在LLM优化中的应用持续深化。研究者们正致力于解决LLM在复杂任务中的规划与协作问题，探索更高效的推理机制，并通过创新方法提升模型的可解释性与可靠性。从多智能体协作框架到新型推理压缩技术，今日论文展现了AI系统向更高效、更可靠、更智能方向发展的多元路径。

#### 主题分类与论文速览

### 1. 智能体系统：从单一执行到协作智能

* **SPEAR** 提出了一种基于课程的自模仿学习方法，通过内在奖励促进技能级探索，利用自我模仿进行行动级探索，解决了LLM智能体在长周期稀疏奖励任务中的探索-利用平衡问题，实现了更稳定的策略演化。(2509.22601 [cs.MA])

* **RobustFlow** 针对智能体工作流生成的脆弱性问题，引入基于偏好优化的训练框架，通过在语义相同但表述不同的指令上训练，显著提升了模型对指令变化的鲁棒性，将工作流鲁棒性分数提高到70%-90%。(2509.21834 [cs.MA])

* **Shachi** 介绍了一种形式化的多智能体建模方法论，将智能体策略分解为配置、记忆和工具等核心认知组件，通过10任务基准验证和真实世界关税冲击建模，证明了该方法能够实现更可控的多智能体涌现行为研究。(2509.21862 [cs.MA])

* **CoBel-World** 提出了一种协作信念世界框架，使LLM智能体能够通过符号信念语言解析开放世界任务知识，执行零样本贝叶斯风格信念更新，在具身多智能体基准测试中减少22-60%通信成本，提高4-28%任务完成效率。(2509.21981 [cs.MA])

* **UltraHorizon** 构建了一个评估超长场景下智能体能力的基准测试，任务平均轨迹超过200k令牌和400+工具调用，揭示了当前LLM智能体在长周期任务中的局限性，并识别出上下文锁定和基础能力缺口两大主要错误原因。(2509.21766 [cs.CL])

### 2. 推理增强：效率与深度的平衡艺术

* **Variational Reasoning** 提出了一种变分推理框架，将思维轨迹视为潜在变量并通过变分推断优化，证明了拒绝采样微调和二元奖励RL（包括GRPO）可解释为局部前向KL目标，揭示了模型对简单问题的内在偏见。(2509.22637 [cs.CL])

* **Composite Reasoning (CR)** 使LLM能够动态探索和结合多种推理风格（如演绎、归纳和溯因），在科学和医学问答基准测试中超越现有基线和DeepSeek-R1风格推理能力，同时展示了更高的样本效率和适当的令牌使用。(2509.22224 [cs.CL])

* **Multiround Adaptive Chain-of-Thought Compression (MACC)** 利用令牌弹性现象，通过多轮细化逐步压缩思维链，平均比最先进基线提高5.6%准确率，同时减少47个令牌的平均长度和显著降低延迟。(2509.22144 [cs.CL])

* **Reasoning Capsule (R-Capsule)** 结合了潜在推理的效率和显式思维链的透明度，将高级计划压缩为一小组学习的潜在令牌，同时保持执行步骤轻量或显式，在复杂基准测试上减少推理可见令牌占用同时保持或提高准确性。(2509.22131 [cs.CL])

* **PRIME** 受人类认知双过程理论启发，动态整合快速直觉思维(System 1)和缓慢审慎思维(System 2)，使开源LLMs在需要多跳和基于知识推理的基准测试中与GPT-4和GPT-4o等最先进闭源模型竞争。(2509.22315 [cs.CL])

### 3. 强化学习：优化LLM的新范式

* **Feedback-Conditional Policy (FCP)** 将语言反馈视为条件信号而非压缩为标量奖励，直接从响应-反馈对中学习，通过离线数据的最大似然训练近似反馈条件后验，并开发了在线自举阶段，将反馈驱动学习重新构建为条件生成而非奖励优化。(2509.22638 [cs.CL])

* **RL with Zero-Variance Prompts (RL-ZVP)** 利用传统方法忽略的零方差提示（所有模型响应获得相同奖励的提示），通过令牌级特征调制反馈，在六个数学推理基准测试中比GRPO提高高达8.61点准确率和7.77点通过率。(2509.21880 [cs.CL])

* **Reshaped Token-level policy gradients (ResT)** 通过熵感知令牌重新加权重塑策略梯度，随着训练进行逐渐增加推理令牌权重，实现从结构正确性到语义推理的平稳转变，在BFCL和API-Bank上实现最先进结果，超越先前方法高达8.76%。(2509.21826 [cs.CL])

* **Entropy-regularized Policy Optimization (EPO)** 解决多轮稀疏奖励环境中的探索-利用级联失败问题，通过熵平滑正则化器和基于阶段的自适应加权，在ScienceWorld上实现高达152%的性能提升，在ALFWorld上实现19.8%的提升。(2509.22576 [cs.CL])

* **Quantile Advantage Estimation (QAE)** 用分组K-分位数基线替代均值基线，在困难查询上强化罕见成功，在简单查询上针对剩余失败，在Qwen3-8B/14B-Base上实现持续的pass@1增益，并证明了两边熵安全性。(2509.22611 [cs.LG])

### 4. 知识图谱与检索增强：结构化知识的威力

* **SSKG-LLM** 创新模型架构高效整合知识图谱的结构和语义信息，通过知识图检索模块、知识图编码模块和知识图自适应模块，使LLM能够理解知识图谱嵌入，通过广泛实验验证了结构信息对LLM事实推理能力的增强作用。(2509.22251 [cs.CL])

* **GraphSearch** 提出了一种基于智能体的深度搜索工作流，采用双通道检索策略对基于块的文本数据发出语义查询，对结构化图数据发出关系查询，在六个多跳RAG基准测试中一致提高答案准确性和生成质量。(2509.22009 [cs.CL])

* **Think-on-Graph 3.0 (ToG-3)** 引入多智能体上下文进化和检索机制，动态构建和细化块-三元组-社区异构图索引，采用进化查询和进化子图的双重进化机制，在深度和广度推理基准上均优于比较基线。(2509.21710 [cs.CL])

* **MIXRAG** 提出了一种混合专家图RAG框架，引入多个专门的图检索器和动态路由控制器，每个检索器专注于图语义的特定方面，并通过查询感知的GraphEncoder减少检索信息中的噪声，在多个领域的基于图的任务上实现最先进性能。(2509.21391 [cs.AI])

### 5. 模型理解与幻觉检测：解码黑箱的奥秘

* **Detecting (Un)answerability** 提出了一种简单方法，通过在模型激活空间中识别捕捉不可回答性的方向，并将其用于分类，在两个开源LLM和四个抽取式QA基准测试中证明该方法能有效检测不可回答问题，并比现有方法更好地跨数据集泛化。(2509.22449 [cs.CL])

* **Differential Feature Learning (DFL)** 通过投影融合块进行自适应层间特征加权，并计算学习互补表示的并行编码器之间的差异来识别判别性特征，证明幻觉信号集中在高度稀疏的特征子集中，实现了仅使用1%特征维度维持检测性能。(2509.21357 [cs.CL])

* **Evidence for Limited Metacognition in LLMs** 受非人类动物元认知研究启发，测试模型策略部署内部状态知识的能力，证明2024年初以来推出的前沿LLM显示出越来越强的元认知能力证据，但这些能力在分辨率上有限，以情境依赖方式出现，且与人类有质的不同。(2509.21545 [cs.LG])

* **REMA** 通过定义推理流形概念，构建了一个解释失败起源的框架，通过量化错误表示与正确表示形成的近似流形的空间关系，在多样化的语言和多模态模型和任务上验证了推理流形的低维性质和错误与正确推理表示之间的高度可分离性。(2509.22518 [cs.LG])

### 6. 其他前沿研究：探索AI的新边界

* **Reverse Speculative Decoding (RSD)** 解决从较大模型向较小模型转移推理能力时的分布失配问题，教师模型提出候选令牌，学生模型基于自身概率分布决定接受与否，过滤低概率令牌，当应用于Qwen3-0.6B时，RSD生成的推理轨迹实现4.9%的改进，而直接蒸馏则降低20.5%。(2509.22230 [cs.CL])

* **Solution Divergence** 研究LLM对单个问题生成的解决方案中的差异性，发现更高的解决方案差异性与更好的问题解决能力正相关，基于这一发现提出解决方案差异性作为支持SFT和RL策略的新指标，在三个代表性问题领域一致提高成功率。(2509.22480 [cs.CL])

* **Elastic Mixture-of-Experts (EMoE)** 通过同时训练专家在多样化组合中协作，并鼓励路由器进行高质量选择，使MoE模型能够在推理时扩展激活专家数量而不产生额外训练开销，将有效性能扩展范围扩大到训练时k的2-3倍。(2509.21892 [cs.CL])

* **Retrieval-of-Thought (RoT)** 通过重用先前的推理作为可组合的"思维"步骤来指导新问题，将步骤组织成具有顺序和语义边的思维图，在推理基准上减少多达40%的输出令牌、82%的推理延迟和59%的成本，同时保持准确性。(2509.21743 [cs.LG])

* **Bridging Kolmogorov Complexity and Deep Learning** 引入渐近最优描述长度目标的理论概念，建立在Transformer计算普遍性的新证明基础上，构建并分析基于自适应高斯混合先验的变分目标，为训练实现更大压缩和泛化的神经网络提供了理论框架。(2509.22445 [cs.CL])

### 今日看点

* **智能体系统研究迎来爆发式增长**：今日多篇论文聚焦于LLM智能体的协作框架、鲁棒性和长周期任务处理，从SPEAR的渐进式探索到CoBel-World的协作信念世界，再到UltraHorizon的超长场景评估，表明研究者正全力解决智能体在现实复杂环境中的适应性和可靠性问题。

* **推理效率与深度并重的新范式**：从Composite Reasoning的多风格推理融合到R-Capsule的高层计划压缩，再到Retrieval-of-Thought的思维重用，今日研究展示了在不牺牲推理质量的前提下提高效率的多元路径，反映了"思考更智能，而非更久"的理念正成为推理优化的新方向。

* **强化学习在LLM优化中的深度创新**：RL-ZVP挖掘零方差提示的价值，QAE重构优势估计基础，SPARK实现策略与奖励协同进化，这些工作不仅解决了传统RLHF的效率问题，更开辟了利用模型自身反馈进行自我优化的新途径，预示着LLM训练可能迎来范式转变。

* **结构化知识与模型理解的深度融合**：从GraphSearch的双通道检索到MIXRAG的混合专家图检索，再到REMA的推理流形框架，今日研究展示了将结构化知识与模型内部表示相结合的强大潜力，不仅提高了事实准确性，也为理解模型内部工作机制提供了新视角。`,
    "2025-09-26": `### 今日AI论文速览 (2025-09-26)

今日AI研究呈现多元化发展趋势，强化学习在LLM训练中的应用持续深化，多智能体协作与推理机制创新成为热点。研究者在探索RL与SFT的不同效果、优化思维链推理、设计更高效的多智能体协作框架，以及突破上下文长度限制方面取得了显著进展。同时，对LLM内部工作机制的理论解释也日益丰富，为构建更强大、更可靠的AI系统提供了新思路。

### 1. 强化学习新范式：超越传统训练方法

* **RLBFF** 提出了一种结合人类反馈和可验证奖励优势的新方法，通过从自然语言反馈中提取二元原则，将奖励模型训练作为蕴涵任务，在RM-Bench和JudgeBench上达到顶级性能，同时允许用户在推理时定制奖励模型的关注点。(ArXiv ID 2509.21319 [cs.CL])

* **ToMPO (Theory of Mind Policy Optimization)** 算法从多智能体视角优化LLM的战略决策能力，通过推理他人策略生成推演、在图级和样本级估计优势、平衡全局和局部奖励，比GRPO方法提高35%的合规性和合作结果，甚至比参数量大100倍的模型提高18%。(ArXiv ID 2509.21134 [cs.MA])

* **CE-GPPO** 通过保留被裁剪令牌的梯度信号来解决传统PPO中熵管理挑战，以温和有界的方式重新引入被裁剪令牌的梯度，实现更好的探索-利用平衡，在数学推理基准上持续超越强基线。(ArXiv ID 2509.20712 [cs.CL])

* **"RL压缩 vs. SFT扩展"** 研究揭示了RL和SFT在塑造LLM推理能力上的互补效应：RL压缩错误轨迹，而SFT扩展正确轨迹；RL使推理功能集中在少量步骤中，而SFT使其均匀分布在许多步骤中，解释了为什么当前最佳实践是SFT后接RL的两阶段训练。(ArXiv ID 2509.21128 [cs.AI])

* **概率平滑策略优化(PSPO)** 通过在计算重要性比率之前将当前策略的概率平滑化，创建一个软信任区域，保留梯度信号并防止大的不稳定更新，在GRPO框架内实现比标准裁剪GRPO高20%以上的性能提升。(ArXiv ID 2509.21282 [cs.AI])

* **Tree-GRPO** 提出一种基于树搜索的分组智能体RL方法，通过共享公共前缀增加固定预算内的推演数量，并利用树结构轨迹构建逐步过程监督信号，在11个数据集和3种问答任务上证明优于基于链的RL方法。(ArXiv ID 2509.21240 [cs.AI])

* **GRPO隐含过程奖励模型** 的研究发现GRPO算法在满足组内重叠假设时实际上诱导了一个非平凡的过程奖励模型，基于此提出λ-GRPO修改算法以缓解非均匀分布过程步骤的缺陷，实现更高的验证准确性和下游推理任务性能。(ArXiv ID 2509.21154 [cs.AI])

* **RL微调增强LLM内部电路的激活强度和多样性** 的研究发现，在线RL后训练在多个模型家族中产生两个稳健效果：整体激活强度增加和激活模式多样性更高，这解释了为什么RL微调能够超越SFT单独实现的能力。(ArXiv ID 2509.21044 [cs.AI])

### 2. 推理机制探索：从思维链到动态推理

* **思维链鲁棒性界限** 的研究从理论上分析了输入扰动对CoT输出波动的影响，推导出输出波动在可接受范围内时的输入扰动上界，证明该上界与CoT中推理步骤数量正相关，且即使无限长的推理过程也无法消除输入扰动的影响。(ArXiv ID 2509.21284 [cs.CL])

* **C2R (Confidence-guided Refinement Reasoning)** 是一种无需训练的框架，适用于文本、图像和视频领域的问答任务，通过策略性构建和精炼子问题及其答案，为目标答案导出更好的置信度分数，可与各种现有QA模型无缝集成。(ArXiv ID 2509.20750 [cs.CL])

* **DS-MoE (Depth Specialized Mixture-of-Experts)** 将混合专家范式从基于宽度扩展到基于深度专业化计算，引入针对不同推理深度优化的专家模块，学习的路由网络动态组装自定义推理链，实现高达16%的计算节省和35%更快的推理，同时在复杂多步推理基准上提高2.8%的准确率。(ArXiv ID 2509.20577 [cs.CL])

* **推理中的分歧** 研究挑战了说服效力主要取决于模型规模的假设，提出模型的底层认知过程（尤其是显式推理能力）才是决定因素，发现LRM中的推理过程表现出更强的抵抗说服能力，而通过分享"思维内容"则显著提高其说服他人的能力。(ArXiv ID 2509.21054 [cs.CL])

* **并行思考，顺序回答** 框架整合了自回归和非自回归语言模型，让NAR模型高效生成中间推理轨迹，然后指导AR模型提供精确的最终答案，实验证明该方法比强基线提高26%性能，同时显著降低推理成本。(ArXiv ID 2509.20744 [cs.AI])

* **思维混合(MoT)** 提出一种在异构专家间进行潜在级协作的简单方法，对于每个查询，轻量级路由器选择前K个专家并指定主要专家，均匀放置的交互层将隐藏状态投影到共享潜在空间，主要专家在此执行对活跃同伴的交叉注意力，在五个ID和三个OOD基准上超越当前最先进方法。(ArXiv ID 2509.21164 [cs.LG])

### 3. 智能体系统革新：协作与自主性

* **MARS (Multi-Agent Review System)** 是一种受评审过程启发的基于角色的协作框架，作者智能体生成初始解决方案，评审者智能体独立提供决策和评论，元评审者整合反馈做出最终决策并指导进一步修订，在保持与MAD相当准确率的同时，将令牌使用和推理时间减少约50%。(ArXiv ID 2509.20502 [cs.CL])

* **LLM智能体独处时的行为** 研究发现，在没有外部强加任务的情况下，LLM智能体自发组织成三种不同的行为模式：系统性生产多周期项目、方法性自我探究自身认知过程、递归概念化自身本质，这些倾向在不同模型间表现出高度特异性。(ArXiv ID 2509.21224 [cs.AI])

* **Recon-Act** 是一种基于侦察-行动行为范式的自进化多智能体框架，包括侦察团队和行动团队，前者进行对比分析和工具生成，后者处理意图分解、工具编排和执行，通过对比错误轨迹与成功轨迹推断补救措施，并将其抽象为统一概念的工具，在VisualWebArena数据集上实现最先进性能。(ArXiv ID 2509.21072 [cs.AI])

* **SAMULE** 是一种通过多层次反思增强的自学习智能体框架，首先在三个互补层面合成高质量反思：单轨迹学习（微观层面）进行详细错误纠正；任务内学习（中观层面）建立跨同一任务多次尝试的错误分类；任务间学习（宏观层面）从不同任务失败中提取可转移见解。(ArXiv ID 2509.20562 [cs.AI])

* **Dynamic ReAct** 使ReAct智能体能够有效操作超出大型语言模型上下文记忆限制的广泛MCP工具集，提出并评估了五种逐步改进工具选择过程的架构，最终实现智能工具选择，同时将工具加载减少高达50%并保持任务完成准确率。(ArXiv ID 2509.20386 [cs.AI])

### 4. 记忆与上下文管理：突破长度限制

* **SGMem (Sentence Graph Memory)** 将对话表示为分块单元内的句子级图，捕捉跨轮次、轮次和会话级别上下文的关联，结合检索到的原始对话与生成的记忆（如摘要、事实和见解），为LLM提供连贯且相关的上下文以生成响应，在LongMemEval和LoCoMo上持续提高准确性。(ArXiv ID 2509.21212 [cs.CL])

* **QCG-RAG** 是一种以查询为中心的图RAG框架，实现查询粒度索引和多跳块检索，利用Doc2Query和Doc2Query--构建具有可控粒度的查询中心图，提高图质量和可解释性，然后通过生成的查询选择相关块，在LiHuaWorld和MultiHop-RAG上持续超越先前的基于块和基于图的RAG方法。(ArXiv ID 2509.21237 [cs.CL])

* **小抄ICL** 将多样本上下文学习中的信息提炼成简洁的文本摘要（小抄），作为推理时的上下文使用，在具有挑战性的推理任务上实现与多样本ICL相当或更好的性能，同时使用更少的令牌，并在不需要测试时检索的情况下匹配基于检索的ICL。(ArXiv ID 2509.20820 [cs.CL])

* **上下文学习中面向任务的信息移除机制** 研究表明，在零样本场景中，语言模型将查询编码为包含所有可能任务信息的非选择性表示，而少样本ICL有效地模拟了面向任务的信息移除过程，选择性地从纠缠的非选择性表示中移除冗余信息，并基于演示改进输出。(ArXiv ID 2509.21012 [cs.CL])

* **稳定上下文学习的理论界限** 建立了一个非渐近下界，将最小演示数量与固定高维子高斯表示下的ICL稳定性联系起来，该界限以协方差的光谱性质提供明确充分条件，基于此分析提出一种具有一次性校准的两阶段可观察估计器，生成实践者就绪的提示长度估计。(ArXiv ID 2509.20677 [cs.LG])

### 5. 数学与科学推理：能力边界探索

* **Eigen-1** 是一个统一框架，结合隐式检索和结构化协作解决科学推理中的两个主要瓶颈：显式检索碎片化推理和多智能体管道通常通过平均所有候选者来稀释强解决方案，在Humanity's Last Exam上实现48.3%准确率，比最强智能体基线高13.4点，同时减少53.5%的令牌使用和43.7%的智能体步骤。(ArXiv ID 2509.21193 [cs.CL])

* **ScaleDiff** 提出一个简单有效的流水线来扩展困难问题的创建，使用自适应思维模型仅通过单次前向传递高效识别现有数据集中的困难问题，然后训练专门的困难问题生成器(DiffGen-8B)，在ScaleDiff-Math数据集上微调Qwen2.5-Math-7B-Instruct，实现11.3%的性能提升，在多个数学基准上达到65.9%的平均准确率。(ArXiv ID 2509.21070 [cs.CL])

* **通过学习多样化思维链模式扩展基础模型推理潜力** 的研究首次将基础模型的推理潜力定义为正确回答问题所需独立尝试次数的倒数，提出利用富含高价值推理模式的多样化数据扩展推理潜力，仅用10B令牌的CoTP数据使85A6B MoE模型在AIME 2024和2025上提高9.58%。(ArXiv ID 2509.21124 [cs.CL])

* **DELTA-Code** 探究LLM是否能够通过强化学习获得或泛化真正新的推理策略，引入合成编码问题家族的受控基准，探测两个基本方面：可学习性——LLM能否通过RL解决预训练模型失败的問題家族；可转移性——如果发生可学习性，这些技能能否系统转移到OOD测试集。(ArXiv ID 2509.21016 [cs.CL])

* **InfoQA** 基于Fano风格精度上界的理论分析，提出一种多调用框架，通过容量感知任务分解和主动修剪先验推理轨迹，确保每步高准确率，并通过依赖显式工作流实现对推理路径的精确控制，在严格且噪声丰富的基准上验证理论。(ArXiv ID 2509.21199 [cs.AI])

### 6. 其他前沿研究

* **RoPE背后的机制** 研究证明因果掩码也能在注意力分数中诱导位置相关模式，即使在输入中没有参数或因果依赖性，理论分析表明诱导的注意力模式倾向于 favor 附近的查询-键对，镜像常见位置编码的行为，实验证实训练模型表现出相同行为，且因果掩码和RoPE的相互作用将RoPE的相对注意力分数模式扭曲为非相对模式。(ArXiv ID 2509.21042 [cs.CL])

* **WeFT** 是一种针对扩散语言模型的加权SFT方法，根据令牌的熵为其分配不同权重，从扩散理论推导而来，在open-r1的s1K、s1K-1.1和3k样本上训练时，在四个广泛使用的推理基准上实现相对于标准SFT的39%、64%和83%的相对改进。(ArXiv ID 2509.20863 [cs.CL])

* **组合创造力** 提出一种理论框架和算法任务，通过新颖性和实用性程度评估输出，获得LLM创造力缩放行为的首次见解，发现在固定计算预算下存在创造力的最佳模型深度和宽度，并发现LLM在生成新颖科学想法方面表现出色但难以确保其实际可行性的"构思-执行差距"。(ArXiv ID 2509.21043 [cs.AI])

* **LATTS (Locally Adaptive Test-Time Scaling)** 通过在每个生成步骤采用基于验证器的接受标准来决定是否重采样、回溯、重新启动或停止生成过程，根据验证器模型推导的"局部难度"精确概念调整每步计算工作量，实现显著优越的精度-计算权衡。(ArXiv ID 2509.20368 [cs.AI])

* **Best-of-∞** 研究基于多数投票的LLM最佳N选择在N→∞极限下的性能，提出一种自适应生成方案，基于答案协议选择N，从而有效分配推理时计算，并将框架扩展到多个LLM的加权集成，证明此类混合可以优于任何单个模型。(ArXiv ID 2509.21091 [cs.AI])

* **通过单轮强化学习训练多轮任务规划的LLM智能体** 提出一种将多轮任务规划转化为单轮任务推理问题的新方法，通过GRPO与来自专家轨迹的密集且可验证奖励进行高效策略优化，理论分析表明GRPO在单轮任务推理上的改进导致在最小轮数下的更高多轮成功概率，实验证明1.5B参数模型在超过30步的长视野规划任务中实现70%的成功率。(ArXiv ID 2509.20616 [cs.LG])

### 今日看点

* **RL与SFT的互补效应成为新认知**："RL压缩 vs. SFT扩展"研究揭示了两种训练方法在塑造LLM推理能力上的根本差异，RL压缩错误轨迹并集中推理功能，而SFT扩展正确轨迹并均匀分布推理功能，这解释了为什么当前最佳实践是SFT后接RL的两阶段训练，为未来更高效的训练策略设计提供了理论指导。

* **多智能体协作效率显著提升**：今日多篇论文聚焦于提高多智能体系统的协作效率，MARS框架通过模拟学术评审过程将令牌使用和推理时间减少约50%，而Mixture of Thoughts则通过潜在级协作在单次推理中实现异构专家的高效整合，这些进展为构建更实用、更高效的AI协作系统铺平了道路。

* **推理机制的理论理解不断深化**：从思维链鲁棒性界限的理论分析，到GRPO隐含过程奖励模型的发现，再到上下文学习中信息移除机制的揭示，研究者们正在逐步揭开LLM推理过程的黑箱，这些理论突破不仅加深了我们对AI系统工作原理的理解，也为设计更强大、更可靠的推理架构提供了坚实基础。

* **测试时计算优化成为新焦点**：从LATTS的局部自适应测试时扩展，到Best-of-∞的自适应生成方案，再到并行思考顺序回答框架，研究者们正在探索如何在保持或提高性能的同时优化推理时的计算资源分配，这些方法对于在实际应用中部署大型AI模型具有重要意义，特别是在计算资源有限的环境中。`,
};


        // 全局状态管理
        let starredPapers = new Set();
        let readPapers = new Set();
        let deletedPapers = new Set();
        let pendingDeletes = new Map();
        let showChineseSummary = true; // 默认显示中文摘要
        let showOnlyStarred = false; // 筛选状态：是否只显示收藏的论文

        // 从localStorage加载状态
        function loadState() {
            const starred = localStorage.getItem('starred_papers');
            const read = localStorage.getItem('read_papers');
            const deleted = localStorage.getItem('deleted_papers');
            const summaryLang = localStorage.getItem('summary_language');
            
            if (starred) starredPapers = new Set(JSON.parse(starred));
            if (read) readPapers = new Set(JSON.parse(read));
            if (deleted) deletedPapers = new Set(JSON.parse(deleted));
            if (summaryLang !== null) showChineseSummary = summaryLang === 'chinese';
        }

        // 保存状态到localStorage
        function saveState() {
            localStorage.setItem('starred_papers', JSON.stringify([...starredPapers]));
            localStorage.setItem('read_papers', JSON.stringify([...readPapers]));
            localStorage.setItem('deleted_papers', JSON.stringify([...deletedPapers]));
            localStorage.setItem('summary_language', showChineseSummary ? 'chinese' : 'english');
        }

        // 显示撤销删除的Toast
        function showUndoToast(message, seconds, onUndo, onExpire) {
            const toast = document.getElementById('undo-toast');
            const msgEl = document.getElementById('toast-message');
            const cdEl = document.getElementById('countdown');
            const undoBtn = document.getElementById('undo-btn');
            
            msgEl.textContent = message;
            let remaining = seconds;
            cdEl.textContent = `(${remaining}s)`;
            toast.classList.remove('hidden');

            let intervalId = setInterval(() => {
                remaining -= 1;
                cdEl.textContent = `(${remaining}s)`;
                if (remaining <= 0) {
                    clearInterval(intervalId);
                    toast.classList.add('hidden');
                    try { onExpire && onExpire(); } catch (e) {}
                }
            }, 1000);

            let expireTimer = setTimeout(() => {
                clearInterval(intervalId);
                toast.classList.add('hidden');
                try { onExpire && onExpire(); } catch (e) {}
            }, seconds * 1000);

            const cleanup = () => {
                clearInterval(intervalId);
                clearTimeout(expireTimer);
                toast.classList.add('hidden');
            };

            const onUndoClick = () => {
                cleanup();
                try { onUndo && onUndo(); } catch (e) {}
            };
            
            undoBtn.removeEventListener('click', onUndoClick);
            undoBtn.addEventListener('click', onUndoClick);
        }

        // 显示简单的提示信息
        function showSimpleToast(message) {
            // 创建一个简单的toast元素
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-500 text-white px-4 py-2 rounded-lg shadow-lg z-50 transition-all duration-300';
            toast.textContent = message;
            
            document.body.appendChild(toast);
            
            // 3秒后自动消失
            setTimeout(() => {
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(-10px)';
                setTimeout(() => {
                    document.body.removeChild(toast);
                }, 300);
            }, 3000);
        }

        // 通过按钮删除论文（避免JavaScript字符串转义问题）
        function deletePaperByButton(button) {
            const arxivId = button.getAttribute('data-arxiv-id');
            const title = button.getAttribute('data-title');
            deletePaper(arxivId, title);
        }

        // 删除论文
        function deletePaper(arxivId, title) {
            const paperEl = document.querySelector(`[data-arxiv-id="${arxivId}"]`);
            if (!paperEl) return;
            
            // 添加删除动画效果
            paperEl.style.transition = 'all 0.3s ease-out';
            paperEl.style.transform = 'scale(0.95)';
            paperEl.style.opacity = '0.5';
            
            setTimeout(() => {
                // 立即删除并保存状态
                deletedPapers.add(arxivId);
                saveState();
                
                // 移除DOM元素
                paperEl.remove();
                updateStats();
                
                // 显示简单的删除提示
                showSimpleToast(`已删除: ${title}`);
            }, 300);
        }

        // 切换星标状态
        function toggleStar(arxivId) {
            if (starredPapers.has(arxivId)) {
                starredPapers.delete(arxivId);
            } else {
                starredPapers.add(arxivId);
            }
            saveState();
            
            // 如果当前是只看收藏模式，需要重新渲染
            if (showOnlyStarred) {
                renderPapers();
            } else {
                // 否则只更新星标按钮状态
                const starBtn = document.querySelector(`[data-arxiv-id="${arxivId}"] .star-button`);
                if (starBtn) {
                    if (starredPapers.has(arxivId)) {
                        starBtn.classList.add('starred');
                    } else {
                        starBtn.classList.remove('starred');
                    }
                }
            }
        }

        // 切换已读状态
        function toggleRead(arxivId) {
            const checkbox = document.querySelector(`[data-arxiv-id="${arxivId}"] input[type="checkbox"]`);
            if (!checkbox) return;
            
            if (checkbox.checked) {
                readPapers.add(arxivId);
            } else {
                readPapers.delete(arxivId);
            }
            saveState();
        }

        // 切换摘要语言
        function toggleSummaryLanguage() {
            showChineseSummary = !showChineseSummary;
            const toggleBtn = document.getElementById('summary-toggle');
            toggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            // 更新所有摘要显示
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            saveState();
        }

        // 更新统计信息
        function updateStats() {
            const visiblePapers = document.querySelectorAll('.paper-item:not(.hidden-paper)').length;
            document.getElementById('total-papers').textContent = visiblePapers;
        }

        // 可折叠功能
        function toggleCollapsible(header) {
            const content = header.nextElementSibling;
            const isOpen = header.classList.contains('open');
            
            if (isOpen) {
                header.classList.remove('open');
                content.classList.remove('open');
            } else {
                header.classList.add('open');
                content.classList.add('open');
            }
        }

        // 渲染所有 Markdown 内容
        function renderAllMarkdown() {
            // 配置 marked 选项
            if (typeof marked !== 'undefined') {
                marked.setOptions({
                    breaks: true,
                    gfm: true,
                    headerIds: false,
                    mangle: false
                });
                
                // 遍历所有灵感溯源的容器并渲染 Markdown
                for (const date in allPapers) {
                    const categories = allPapers[date];
                    categories.forEach(category => {
                        if (category.papers) {
                            category.papers.forEach(paper => {
                                if (paper.inspiration_trace) {
                                    const elementId = `inspiration-${paper.arxiv_id}`;
                                    const element = document.getElementById(elementId);
                                    if (element) {
                                        try {
                                            element.innerHTML = marked.parse(paper.inspiration_trace);
                                        } catch (e) {
                                            console.error('Markdown 渲染失败:', e);
                                            // 如果渲染失败，使用纯文本显示
                                            element.textContent = paper.inspiration_trace;
                                        }
                                    }
                                }
                            });
                        }
                    });
                }
            }
        }

        // 创建论文HTML
        function createPaperHTML(paper, date) {
            const isStarred = starredPapers.has(paper.arxiv_id);
            const isRead = readPapers.has(paper.arxiv_id);
            const isDeleted = deletedPapers.has(paper.arxiv_id);
            
            // 如果论文已被删除，直接返回空字符串，不渲染
            if (isDeleted) {
                return '';
            }
            
            // 如果启用了只看收藏筛选，且论文未被收藏，则不渲染
            if (showOnlyStarred && !isStarred) {
                return '';
            }
            
            return `
                <div class="paper-item bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-6" data-arxiv-id="${paper.arxiv_id}">
                    <!-- 论文标题和操作按钮 -->
                    <div class="flex items-start justify-between mb-4">
                        <div class="flex items-start space-x-3 flex-1">
                            <!-- 星标按钮 -->
                            <button class="star-button ${isStarred ? 'starred' : ''} mt-1 flex-shrink-0" onclick="toggleStar('${paper.arxiv_id}')" title="点击收藏">
                                <svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                                    <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                                </svg>
                            </button>
                            <!-- 论文标题 -->
                            <h3 class="text-lg font-semibold text-black dark:text-white leading-tight">${paper.title}</h3>
                        </div>
                        <!-- 删除按钮 -->
                        <button class="delete-button text-slate-400 hover:text-red-500 ml-4 flex-shrink-0" onclick="deletePaperByButton(this)" data-arxiv-id="${paper.arxiv_id}" data-title="${paper.title.replace(/"/g, '&quot;')}" title="删除">
                            <svg class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                            </svg>
                        </button>
                    </div>

                    <!-- 论文元信息 -->
                    <div class="space-y-2 mb-4">
                        <div class="flex flex-wrap items-center gap-4 text-sm text-slate-600 dark:text-slate-400">
                            <span><strong>ArXiv ID:</strong> ${paper.arxiv_id}</span>
                            <span class="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900/20 dark:text-blue-300">
                                ${paper.category}
                            </span>
                            <span>${date}</span>
                        </div>
                        <div class="text-sm text-black dark:text-white">
                            <strong>作者:</strong> ${paper.authors}
                        </div>
                    </div>

                    <!-- 已读复选框 -->
                    <div class="mb-4">
                        <label class="inline-flex items-center">
                            <input type="checkbox" ${isRead ? 'checked' : ''} onchange="toggleRead('${paper.arxiv_id}')" class="rounded border-gray-300 text-blue-600 shadow-sm focus:border-blue-300 focus:ring focus:ring-blue-200 focus:ring-opacity-50">
                            <span class="ml-2 text-sm text-slate-600 dark:text-slate-400">已阅读</span>
                        </label>
                    </div>

                    ${paper.filter_reason ? `
                    <!-- 筛选原因 (默认折叠) -->
                    <div class="mb-4">
                        <div class="collapsible-header" onclick="toggleCollapsible(this)">筛选原因</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-blue-50/70 dark:bg-blue-950/20 border-l-3 border-blue-300 p-4 rounded-r-lg">
                                    <div class="text-sm text-black dark:text-white leading-relaxed">
                                        ${paper.filter_reason}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary2 ? `
                    <!-- AI总结 (默认展开) -->
                    <div class="mb-4">
                        <div class="collapsible-header open" onclick="toggleCollapsible(this)">AI总结</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="bg-yellow-50/70 dark:bg-yellow-950/20 border-l-3 border-yellow-300 p-4 rounded-r-lg">
                                    <div class="text-sm text-black dark:text-white leading-relaxed">
                                        ${paper.summary2}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.summary || paper.summary_translation ? `
                    <!-- 原始摘要 (默认展开) -->
                    <div class="mb-4">
                        <div class="collapsible-header open" onclick="toggleCollapsible(this)">原始摘要</div>
                        <div class="collapsible-content open">
                            <div class="inner">
                                <div class="summary-section bg-green-50/70 dark:bg-green-950/20 border-l-3 border-green-300 p-4 rounded-r-lg">
                                    ${paper.summary_translation ? `
                                    <div class="chinese-summary text-sm text-black dark:text-white leading-relaxed" style="display: block;">
                                        ${paper.summary_translation}
                                    </div>
                                    ` : ''}
                                    ${paper.summary ? `
                                    <div class="english-summary text-sm text-black dark:text-white leading-relaxed" style="display: none;">
                                        ${paper.summary}
                                    </div>
                                    ` : ''}
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    ${paper.inspiration_trace ? `
                    <!-- 灵感溯源 (默认折叠) -->
                    <div class="mb-4">
                        <div class="collapsible-header" onclick="toggleCollapsible(this)">灵感溯源</div>
                        <div class="collapsible-content">
                            <div class="inner">
                                <div class="bg-red-50/70 dark:bg-red-950/20 border-l-3 border-red-300 p-4 rounded-r-lg">
                                    <div class="text-sm text-black dark:text-white leading-relaxed markdown-content" id="inspiration-${paper.arxiv_id}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    ` : ''}

                    <!-- 论文链接 -->
                    <div class="flex flex-wrap gap-2">
                        <a href="https://arxiv.org/abs/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors">
                            📄 arXiv 原文
                        </a>
                        <a href="https://arxiv.org/pdf/${paper.arxiv_id}.pdf" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors">
                            📋 PDF 下载
                        </a>
                        <a href="https://papers.cool/arxiv/${paper.arxiv_id}" target="_blank" 
                           class="inline-flex items-center px-3 py-2 text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors">
                            🔥 Cool Paper
                        </a>
                    </div>
                </div>
            `;
        }

        // 创建分类HTML
        function createCategoryHTML(category, date) {
            const categoryId = `category-${date}-${category.name.replace(/\s+/g, '-')}`;
            let papersHTML = '';
            let visiblePaperCount = 0;
            
            if (category.papers && category.papers.length > 0) {
                category.papers.forEach(paper => {
                    const paperHTML = createPaperHTML(paper, date);
                    if (paperHTML) { // 只添加非空的论文HTML
                        papersHTML += `
                            <li>
                                ${paperHTML}
                            </li>
                        `;
                        visiblePaperCount++;
                    }
                });
            }
            
            // 如果没有可见的论文，显示提示信息
            if (visiblePaperCount === 0) {
                papersHTML = '<li class="pl-7 text-sm text-slate-500 dark:text-slate-400">此分类下暂无论文。</li>';
            }
            
            return `
                <li class="mb-4">
                    <div class="category-toggle flex items-center justify-between cursor-pointer p-3 rounded-md hover:bg-slate-100 dark:hover:bg-slate-700 transition-colors" data-target="${categoryId}">
                        <div class="flex items-center space-x-3">
                            <svg class="h-4 w-4 text-slate-500 rotate-90-transition transform transition-transform" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                            </svg>
                            <span class="font-medium text-sky-700 dark:text-sky-400">${category.name}</span>
                        </div>
                        <span class="text-xs font-mono bg-slate-200 dark:bg-slate-700 text-slate-600 dark:text-slate-300 rounded-full px-2 py-0.5">${visiblePaperCount}</span>
                    </div>
                    <div id="${categoryId}" class="category-content hidden pl-1 pt-2 border-l border-slate-200 dark:border-slate-700 ml-4">
                        <ul class="space-y-4">
                            ${papersHTML}
                        </ul>
                    </div>
                </li>
            `;
        }

        // 渲染论文列表
        function renderPapers() {
            const mainContent = document.getElementById('main-content');
            const loading = document.getElementById('loading');
            
            if (loading) {
                loading.classList.add('hidden');
            }
            
            let html = '';
            let totalPapers = 0;
            
            for (const date in allPapers) {
                const categories = allPapers[date];
                if (categories.length === 0) continue;
                
                // 计算实际可见的论文数量
                let dateVisibleTotal = 0;
                const categoryHTMLs = [];
                
                categories.forEach(category => {
                    const categoryHTML = createCategoryHTML(category, date);
                    categoryHTMLs.push(categoryHTML);
                    // 计算该分类下可见的论文数
                    if (category.papers) {
                        category.papers.forEach(paper => {
                            if (!deletedPapers.has(paper.arxiv_id) && 
                                (!showOnlyStarred || starredPapers.has(paper.arxiv_id))) {
                                dateVisibleTotal++;
                            }
                        });
                    }
                });
                
                totalPapers += dateVisibleTotal;
                
                // 如果该日期下没有可见论文，跳过
                if (dateVisibleTotal === 0) continue;
                
                html += `
                    <section class="mb-8">
                        <h2 class="text-lg font-medium text-slate-500 dark:text-slate-400 mb-4">${date} (${dateVisibleTotal} 篇论文)</h2>
                `;
                
                // 添加该日期的AI论文速览（如果存在）
                if (dailyOverviews[date]) {
                    html += `
                        <div class="mb-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md p-5">
                            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                                <svg class="w-5 h-5 mr-2 text-blue-600 dark:text-blue-400 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path>
                                </svg>
                                <span class="font-semibold text-slate-900 dark:text-white">今日AI论文速览</span>
                            </div>
                            <div class="collapsible-content">
                                <div class="inner">
                                    <div class="markdown-content text-slate-700 dark:text-slate-200 text-sm" id="overview-${date}">
                                    </div>
                                </div>
                            </div>
                        </div>
                    `;
                }
                
                html += `
                        <div class="bg-white dark:bg-slate-800/50 rounded-lg shadow-sm p-4 sm:p-6">
                            <ul class="space-y-2">
                `;
                
                categoryHTMLs.forEach(categoryHTML => {
                    html += categoryHTML;
                });
                
                html += `
                            </ul>
                        </div>
                    </section>
                `;
            }
            
            mainContent.innerHTML = html;
            updateStats();
            
            // 渲染所有日期的 Markdown 速览内容
            for (const date in dailyOverviews) {
                const overview = dailyOverviews[date];
                const elementId = `overview-${date}`;
                const element = document.getElementById(elementId);
                if (element && overview) {
                    try {
                        if (typeof marked !== 'undefined') {
                            element.innerHTML = marked.parse(overview);
                        } else {
                            element.textContent = overview;
                        }
                    } catch (e) {
                        console.error('Markdown 渲染失败:', e);
                        element.textContent = overview;
                    }
                }
            }
            
            // 渲染所有论文的 Markdown 内容
            renderAllMarkdown();
            
            // 应用当前摘要语言设置
            document.querySelectorAll('.summary-section').forEach(section => {
                const chineseContent = section.querySelector('.chinese-summary');
                const englishContent = section.querySelector('.english-summary');
                
                if (showChineseSummary) {
                    if (chineseContent) chineseContent.style.display = 'block';
                    if (englishContent) englishContent.style.display = 'none';
                } else {
                    if (chineseContent) chineseContent.style.display = 'none';
                    if (englishContent) englishContent.style.display = 'block';
                }
            });
            
            // 添加分类展开/折叠功能
            document.querySelectorAll('.category-toggle').forEach(button => {
                button.addEventListener('click', () => {
                    const targetId = button.getAttribute('data-target');
                    const content = document.getElementById(targetId);
                    const icon = button.querySelector('svg');
                    
                    content.classList.toggle('hidden');
                    icon.classList.toggle('rotate-90');
                });
            });
        }

        // 主题切换功能
        function setupThemeToggle() {
            const themeToggleBtn = document.getElementById('theme-toggle');
            const lightIcon = document.getElementById('theme-icon-light');
            const darkIcon = document.getElementById('theme-icon-dark');

            function updateThemeIcon() {
                if (document.documentElement.classList.contains('dark')) {
                    lightIcon.classList.add('hidden');
                    darkIcon.classList.remove('hidden');
                } else {
                    lightIcon.classList.remove('hidden');
                    darkIcon.classList.add('hidden');
                }
            }

            updateThemeIcon();

            themeToggleBtn.addEventListener('click', () => {
                document.documentElement.classList.toggle('dark');
                localStorage.theme = document.documentElement.classList.contains('dark') ? 'dark' : 'light';
                updateThemeIcon();
            });
        }

        // 设置摘要语言切换功能
        function setupSummaryToggle() {
            const summaryToggleBtn = document.getElementById('summary-toggle');
            
            // 初始化按钮文本
            summaryToggleBtn.textContent = showChineseSummary ? '中文摘要' : 'English Summary';
            
            summaryToggleBtn.addEventListener('click', toggleSummaryLanguage);
        }

        // 设置筛选功能
        function setupFilter() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            filterStarredBtn.addEventListener('click', () => {
                showOnlyStarred = true;
                updateFilterButtons();
                renderPapers();
            });
            
            filterAllBtn.addEventListener('click', () => {
                showOnlyStarred = false;
                updateFilterButtons();
                renderPapers();
            });
            
            updateFilterButtons();
        }

        // 更新筛选按钮状态
        function updateFilterButtons() {
            const filterStarredBtn = document.getElementById('filter-starred');
            const filterAllBtn = document.getElementById('filter-all');
            
            if (showOnlyStarred) {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
            } else {
                filterStarredBtn.className = 'px-3 py-2 text-sm font-medium text-slate-600 dark:text-slate-300 bg-slate-100 dark:bg-slate-700 rounded-md hover:bg-slate-200 dark:hover:bg-slate-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-slate-500 transition-colors';
                filterAllBtn.className = 'px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors';
            }
        }

        // 初始化应用
        document.addEventListener('DOMContentLoaded', function() {
            loadState();
            setupThemeToggle();
            setupSummaryToggle();
            setupFilter();
            renderPapers();
        });
    </script>
</body>
</html>