{"date": "2025-12-19", "categories": [{"name": "Artificial Intelligence", "count": 1, "papers": [{"index": "#13", "title": "Reinforcement Learning for Self-Improving Agent with Skill Library", "link": "/arxiv/2512.17102", "arxiv_id": "2512.17102", "authors": "Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong", "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.", "subjects": "Artificial Intelligence", "date": "2025-12-18", "category": "cs.AI", "crawl_time": "2025-12-23T11:00:05.763422", "filter_reason": "这篇论文完全符合我的研究范围，属于“自我演化”方向的核心文献。具体判断依据如下： 1.  **核心判断（第一步）**：论文的核心贡献是提出了一种名为 SAGE 的新框架，旨在通过强化学习（RL）和技能库来增强 LLM 智能体的自我改进和自我演化能力。这直接对应了我的核心目标中关于“构建、改进或演化 LLM智能体”的要求。它不是单纯的应用，也不是基础设施优化，而是提出了新的智能体演化方法论。 2.  **正面指标匹配（第二步）**： *   **核心范式**：论文明确涉及 `Self-Evolving`（自我演化）和 `LLM-based Agents`。 *   **演化机制**：论文详细描述了 `Self-Improvement`（自我改进）和 `Iterative Improvement`（迭代改进）机制，特别是通过“Sequential Rollout”在任务链中积累技能，体现了智能体通过经验进行迭代完善的过程。 *   **智能体能力**：引入“Skill Library”作为智能体获取和重用能力的机制，这与 `Tool Use` 和 `Memory` 的概念紧密相关，增强了智能体在新环境中的适应性。 3.  **排除标准检查（第三步）**：论文不涉及安全对齐、多模态视觉或图神经网络等排除领域，纯粹聚焦于智能体的算法架构和演化逻辑。 4.  **特殊情况处理（第四步）**：虽然论文在 AppWorld 数据集上进行了实验，但其核心在于提出一种通用的自我演化机制（SAGE），而非仅仅解决 AppWorld 领域的特定问题。根据规则，这种提出新“自我演化”机制的应用研究应当保留。 综上所述，该论文在“自我演化”这一核心研究焦点上做出了实质性贡献，符合筛选标准。", "summary2": "本文旨在解决基于LLM的智能体在新环境中难以持续改进和适应的问题。针对工具使用智能体，我们提出了一种名为SAGE的RL框架，通过Sequential Rollout和Skill-integrated Reward增强技能生成与利用。在AppWorld数据集上，通过Scenario Goal Completion (SGC)和Task Goal Completion (TGC)等指标验证了其有效性，显著提升了性能与效率。", "inspiration_trace": "生成灵感溯源时发生错误", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-09 13:06:43", "summary_model": "z-ai/glm-4.7"}]}, {"name": "Computation and Language", "count": 1, "papers": [{"index": "#19", "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "link": "/arxiv/2512.17260", "arxiv_id": "2512.17260", "authors": "Jiangjie Chen, Wenxiang Chen, Jiacheng Du, Jinyi Hu, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Wenlei Shi, Zhihong Wang, Mingxuan Wang, Chenrui Wei, Shufa Wei, Huajian Xin, Fan Yang, Weihao Gao, Zheng Yuan, Tianyang Zhan, Zeyu Zheng, Tianxi Zhou, Thomas Hanwen Zhu", "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "subjects": "Computation and Language", "date": "2025-12-19", "category": "cs.CL", "crawl_time": "2025-12-23T11:00:05.163121", "filter_reason": "这篇论文完全符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合“自我演化”与“Agentic AI”方向**： 论文的核心贡献是提出了 **Seed-Prover 1.5**，这是一个通过 **\"agentic reinforcement learning\"（智能体强化学习）** 训练的模型。论文明确指出模型通过 **\"extensive interactions with Lean and other tools\"（与工具的广泛交互）** 来 **\"continuously accumulates experience\"（持续积累经验）**。这直接对应了筛选标准中的 **自我演化** 机制和 **Agentic AI** 的核心范式。 2.  **涉及智能体的关键能力**： 论文中提到的与 Lean（形式化证明工具）的交互属于 **Tool Use（工具使用）**；通过强化学习过程积累经验并提升能力，属于 **Self-Improvement（自我完善）** 和 **Iterative Improvement（迭代改进）**。这些都是筛选标准第二步中的核心正面指标。 3.  **符合“特殊和模糊情况”的处理规则**： 虽然论文的应用领域是数学定理证明（属于特定领域应用），但根据筛选标准第四步第2点：“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域……也应该保留”。本文的核心在于提出了一种基于智能体强化学习和测试时扩展（TTS）的**新框架和方法论**，而不仅仅是应用现有模型解决数学问题。因此，它不属于“非演化型应用”的排除范畴。 4.  **排除标准检查**： 论文不涉及安全对齐、多模态视觉或图技术等排除项。 综上所述，该论文在构建具有自我演化能力的LLM智能体方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题。", "summary2": "本文旨在解决形式化定理证明在本科及以上水平面临的计算成本高和能力不足的问题。针对Lean环境下的数学证明任务，我们提出了一种基于大规模智能体强化学习训练的Agentic Prover，并结合Rubric RL训练的Sketch模型及高效的测试时扩展工作流。我们在PutnamBench、FATE-H/X等基准数据集上，通过问题解决率验证了其有效性，显著优于现有SOTA方法。", "inspiration_trace": "生成灵感溯源时发生错误", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-09 13:06:41", "summary_model": "z-ai/glm-4.7"}]}, {"name": "Machine Learning", "count": 2, "papers": [{"index": "#59", "title": "Dynamic Tool Dependency Retrieval for Efficient Function Calling", "link": "/arxiv/2512.17052", "arxiv_id": "2512.17052", "authors": "Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermovol, Bence Major", "summary": "Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.", "subjects": "Machine Learning", "date": "2025-12-18", "category": "cs.LG", "crawl_time": "2025-12-23T11:00:05.743103", "filter_reason": "1.  **核心判断（符合）**：这篇论文的核心贡献是提出了一种名为“动态工具依赖检索（DTDR）”的方法，旨在改进LLM智能体在函数调用过程中的工具检索机制。这属于构建和改进LLM智能体的方法论，完全符合“单智能体”的研究范畴。 2.  **正面指标匹配**： *   **核心范式**：论文明确研究对象是“Function calling agents”（函数调用智能体），属于 `Agentic AI` 和 `LLM-based Agents`。 *   **智能体能力**：论文的核心焦点在于 `Tool Use / Tool Augmentation`（工具使用/工具增强）。它解决了智能体在多步任务中如何根据上下文动态选择相关工具的问题，这直接关联到智能体的 `Planning`（规划）能力，因为它涉及处理“多步工具依赖”和“随着计划的展开”进行自适应检索。 3.  **排除标准检查**： *   论文并非将智能体作为工具应用到特定垂直领域（如生物、医疗），而是改进智能体本身的基础组件（工具检索模块）。 *   论文不涉及安全、对齐、多模态核心研究或图技术。 *   虽然摘要中提到了“evolving task context”（演化的任务上下文），但这指的是任务执行过程中的状态变化，而非智能体模型本身的“自我演化”或“自我完善”，因此归类为单智能体的工具使用改进最为准确。 4.  **结论**：该论文通过改进工具检索机制，显著提升了LLM智能体在复杂任务中的执行效率和准确性，是对Agentic AI中工具使用能力的具体增强，完全符合筛选要求。", "summary2": "本文旨在解决设备端 Function Calling agents 中现有工具检索方法无法捕捉多步依赖和动态上下文的问题。针对用户查询和工具调用历史，我们提出了一种 Dynamic Tool Dependency Retrieval (DTDR) 方法，结合查询和执行历史动态检索相关工具。在 TinyAgent 和 TaskBench 等多个数据集上，通过 Function Selection Accuracy 和 Success Rate 等指标验证了其有效性，显著提升了检索精度和任务成功率。", "inspiration_trace": "生成灵感溯源时发生错误", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-09 13:06:42", "summary_model": "z-ai/glm-4.7"}, {"index": "#62", "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs", "link": "/arxiv/2512.17008", "arxiv_id": "2512.17008", "authors": "Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li", "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.", "subjects": "Machine Learning", "date": "2025-12-18", "category": "cs.LG", "crawl_time": "2025-12-23T11:00:05.743874", "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”方向。 1.  **核心判断（符合）**：论文的核心贡献是提出了一种名为 \"Turn-PPO\" 的新算法，旨在改进 Agentic LLMs（智能体大模型）在多轮交互中的强化学习（RL）训练效果。这属于构建和改进 LLM 智能体方法论的研究，而非简单的应用或基础设施优化。 2.  **符合研究焦点（单智能体）**： *   论文明确针对 \"Agentic LLMs\"，关注智能体在多轮任务中的表现。 *   它解决了智能体在长视界推理中的局限性，涉及智能体的规划和多步推理能力，这正是筛选标准中“单智能体”方向的核心关注点。 3.  **排除标准检查（通过）**： *   **非应用型**：虽然论文在 WebShop 和 Sokoban 数据集上进行了验证，但其核心在于提出一种通用的训练算法（Turn-PPO），而非解决特定领域的业务问题。 *   **非基础设施**：研究重点在于算法逻辑和策略优化，而非硬件或部署。 *   **非排除领域**：不涉及安全、对齐、多模态视觉或图技术。 4.  **特殊情况处理**： *   论文讨论的是智能体级别的交互和决策，将 MDP（马尔可夫决策过程）从 Token 级别提升到 Turn 级别，这符合“智能体如何进行规划或在复杂任务中进行多步推理”的保留规则，而非单纯的基础 Token 预测能力提升。 综上所述，该论文通过改进强化学习算法来增强 LLM 智能体的多轮交互和规划能力，是构建高质量 Agentic AI 的关键研究，应予以保留。", "summary2": "本文旨在解决多轮智能体 LLM 训练中 GRPO 算法不稳定及优势估计不准确的问题。针对多轮交互场景，我们提出了一种 Turn-PPO 方法，采用 turn-level MDP 公式化，将整轮交互作为状态-动作对，并利用可学习的 Critic 进行优势估计。我们在 WebShop 和 Sokoban 数据集上通过平均奖励验证了其有效性，实验表明该方法显著提升了训练稳定性和任务性能。", "inspiration_trace": "基于论文《Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs》的内容，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观背景与问题识别\n**观察：** 随着LLM智能体的发展，任务从单轮推理（如数学题）转向多轮交互（如网页浏览、工具使用）。强化学习（RL）是训练此类智能体的主流方法。\n**现状：** 目前最流行的算法是 **GRPO**（Group Relative Policy Optimization，源自DeepSeek-R1），它在单轮任务上表现优异。\n**冲突：** 当直接将GRPO应用于多轮任务时，训练经常出现崩溃或不稳定，尤其是在需要长视界推理的场景下。\n\n### 第二阶段：诊断现有方法的局限性\n**思考：** 为什么在单轮任务中有效的GRPO，在多轮任务中失效了？\n**分析：**\n1.  **采样方差过大：** 多轮任务涉及环境交互，不可控因素多，导致基于采样的优势估计方差极高，训练不稳定。\n2.  **信用分配不均：** GRPO将整个轨迹的最终奖励归一化后，均等地分配给轨迹中的每一个Token。然而，在多轮对话中，不同轮次对最终成功的贡献差异巨大（有的轮次是关键决策，有的只是废话）。这种“一刀切”的优势估计引入了严重的噪声。\n\n### 第三阶段：算法选型的初步探索\n**假设：** 为了解决采样方差大和信用分配不准的问题，我们需要一个更稳定、更精确的优势估计策略。\n**尝试：** 重新引入 **PPO**（Proximal Policy Optimization）。\n**理由：** 与GRPO依赖纯采样不同，PPO引入了一个可学习的**Critic（价值函数）**。Critic可以利用广义优势估计（GAE）来平滑奖励信号，理论上比基于样本的归一化更稳定。\n**验证：** 实验表明，PPO确实比GRPO更稳定，缓解了训练崩溃的问题。\n\n### 第四阶段：深挖“状态表示”的错位\n**思考：** 虽然PPO比GRPO好，但现有的PPO实现通常沿用了单轮RLHF中的**Token级MDP**（Token-level MDP） formulation。这在多轮场景下是最优的吗？\n**洞察：** 在多轮交互中，状态转移具有“异质性”。\n*   **Token级视角：** 模型生成一个Token，状态加一个Token（平滑变化）；突然插入一大段环境输出，状态剧烈跳变。\n*   **问题：** 这种不连续的状态转移导致Critic很难学习。Critic被迫在Token级别预测价值，但任务的结构性变化发生在Turn级别。这种**“状态表示错位”**导致Critic回归到一个平均值，无法准确捕捉不同轮次的价值，从而降低了优势估计的准确性。\n\n### 第五阶段：核心创新——重新定义MDP粒度\n**顿悟：** 既然智能体的决策是以“轮”为单位的，那么RL的优化粒度也应该是“轮”，而不是“Token”。\n**方法论构建：** 提出 **Turn-PPO**。\n1.  **重新定义MDP：**\n    *   **状态：** 当前轮次的完整历史 + 当前查询。\n    *   **动作：** 当前轮次的完整回复。\n2.  **Turn-level Critic：** Critic不再预测每个Token的价值，而是预测每个Turn的价值。\n3.  **优势估计：** 在Turn级别计算GAE优势，从而实现精确的轮次级信用分配。\n\n### 第六阶段：逻辑闭环与验证\n**预期效果：**\n*   **更准确的Critic：** 状态表示与任务结构对齐，Critic更容易收敛。\n*   **更好的信用分配：** 能够准确识别哪一轮对话是好的，哪一轮是坏的，而不是模糊地分配给所有Token。\n*   **更稳定的训练：** Turn-level的裁剪机制能更好地防止策略剧烈变动。\n**结论：** 通过将MDP formulation从Token级提升到Turn级，并结合PPO的Critic机制，Turn-PPO在多轮智能体任务中实现了比GRPO和传统Token-PPO更优的性能与稳定性。", "summary_translation": "强化学习 (RL) 再次成为一种在真实环境中训练交互式大语言模型 (LLM) 智能体的自然途径。然而，将广泛使用的群组相对策略优化 (GRPO) 算法直接应用于多轮任务时，暴露出了显著的局限性，特别是在需要长视界推理的场景中。为应对这些挑战，我们探究了更稳定且有效的优势估计策略，特别是针对多轮场景的策略。我们首先探索了近端策略优化 (PPO) 作为替代方案，发现其比 GRPO 具有更强的鲁棒性。为了进一步增强 PPO 在多轮场景中的表现，我们引入了 turn-PPO，这是一种基于轮次级马尔可夫决策过程 (MDP) 形式化构建的变体，区别于常用的词元级 MDP。我们在 WebShop 和 Sokoban 数据集上的实验结果验证了 turn-PPO 的有效性，无论任务中是否包含长推理组件。", "summary_generated_time": "2026-01-09 13:06:29", "summary_model": "z-ai/glm-4.7"}]}, {"name": "Multiagent Systems", "count": 2, "papers": [{"index": "#5", "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework", "link": "/arxiv/2512.16970", "arxiv_id": "2512.16970", "authors": "Kamer Ali Yuksel", "summary": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.", "subjects": "Artificial Intelligence, Computation and Language, Machine Learning, Multiagent Systems", "date": "2025-12-18", "category": "cs.MA", "crawl_time": "2025-12-23T11:00:04.854123", "filter_reason": "这篇论文完全符合筛选标准，属于核心关注的“单智能体”研究方向。 1.  **核心贡献判断 (第一步)**: 论文的核心贡献是提出了 **PAACE** 框架，这是一个用于优化 LLM 智能体在复杂工作流中“演化状态”的统一框架。它不是将智能体作为工具应用到特定垂直领域（如医疗、金融），而是致力于**改进智能体本身**的上下文工程机制，解决智能体在长视界任务中的上下文膨胀和注意力稀释问题。这符合“构建、改进 LLM 智能体”的核心目标。 2.  **符合核心关注点 (第二步)**: *   **Agentic AI**: 论文明确针对 LLM 智能体，涉及智能体的规划、工具使用和反思。 *   **Planning (规划)**: 论文强调 \"Plan-Aware\"（感知计划），通过分析计划结构和 next-k-task relevance modeling 来优化上下文，直接关联到智能体的规划能力。 *   **Memory (记忆)**: 论文处理的是智能体工作流中“快速扩展的上下文”，这本质上是智能体的短期记忆管理问题。 3.  **排除标准检查 (第三步)**: 论文不涉及安全对齐、多模态视觉核心研究或图神经网络，因此不在排除范围内。 4.  **特殊情况处理 (第四步)**: *   **推理/规划**: 论文关注的是智能体在多步骤工作流中的推理过程，通过压缩和优化上下文来辅助智能体更好地执行规划，属于 Agentic 的范畴，而非单纯的 LLM 基础推理能力提升（如数学题求解）。 综上所述，PAACE 提出了一种改进智能体架构（特别是上下文管理和规划感知）的新方法，能够提升智能体在复杂任务中的表现，符合“LLM智能体及其演化”的研究课题。", "summary2": "本文旨在解决LLM Agent在复杂多步骤工作流中上下文膨胀导致的注意力稀释与推理成本问题。针对长视界Agent工作流，我们提出了PAACE框架，通过next-k-task相关性建模、计划结构分析和指令协同优化实现上下文压缩。我们在AppWorld、OfficeBench和8-Objective QA上通过Accuracy、F1及Peak Context等指标验证了其有效性，结果显示PAACE在提升准确率的同时显著降低了上下文负载。", "inspiration_trace": "生成灵感溯源时发生错误", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-09 13:06:37", "summary_model": "z-ai/glm-4.7"}, {"index": "#3", "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues", "link": "/arxiv/2512.17060", "arxiv_id": "2512.17060", "authors": "Monika Zamojska, Jarosław A. Chudziak", "summary": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.", "subjects": "Multiagent Systems, Artificial Intelligence", "date": "2025-12-18", "category": "cs.MA", "crawl_time": "2025-12-23T11:00:04.853583", "filter_reason": "这篇论文符合筛选标准，应予以保留。具体判断过程如下： 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了一种受交互分析理论启发的**多智能体系统（MAS）架构**。它不仅仅是将现有的LLM作为工具应用，而是设计了一种新的智能体内部结构（将智能体划分为父母、成人、儿童三种自我状态）并结合了上下文检索机制。这属于“构建、改进LLM智能体”的范畴，符合保留条件。 2.  **正面指标匹配（第二步）**： *   **多智能体**: 论文明确提出了Multi-Agent System (MAS)，旨在模拟群体动力学和社会行为。 *   **智能体能力**: 论文涉及智能体的**记忆**（通过向量存储检索上下文信息）和特定的**推理风格**（基于不同自我状态的推理）。 *   **核心范式**: 符合 `LLM-based Agents` 和 `Multi-Agent Systems` 的定义。 3.  **排除标准检查（第三步）**： *   论文不涉及安全与对齐、多模态视觉技术或图神经网络，因此不在排除之列。 4.  **特殊情况处理（第四步）**： *   虽然论文涉及心理学领域，但其核心在于**构建智能体架构**以增强模拟的真实感，而非单纯的应用研究。它属于对智能体行为和认知结构的改进，符合研究目标。 综上所述，该论文通过引入心理学理论改进了多智能体的架构和行为模式，属于Agentic AI和多智能体系统的研究范畴。", "summary2": "本文旨在解决LLM智能体缺乏心理深度和行为一致性的问题。针对模拟对话场景，我们提出了一种基于Transactional Analysis (TA) 理论的Multi-Agent System (MAS) 架构，将智能体划分为Parent、Adult和Child三种ego states，并结合上下文信息检索机制。我们在模拟的职场对话环境中，通过消融测试验证了其有效性，结果显示该架构能显著提升智能体的心理真实感和行为多样性。", "inspiration_trace": "生成灵感溯源时发生错误", "summary_translation": "基于大语言模型（LLM）的智能体目前已广泛应用于从客户服务到教育等多个领域，学界和业界对其拟人化行为能力的兴趣日益浓厚。这涵盖了社会、政治及心理学等领域，这些领域的研究目标通常是对群体动力学和社会行为进行建模。然而，现有的LLM智能体往往缺乏捕捉人类思维真实模式所需的心理深度和一致性。它们通常提供直接或符合统计概率的答案，却忽略了驱动真实人类互动的深层目标、情感冲突及动机。本文提出了一种受交互分析理论启发的多智能体系统。在该系统中，每个智能体被划分为三种自我状态——父母、成人和儿童。这些自我状态被视为独立的知识结构，各自拥有独特的视角和推理风格。为了丰富其响应过程，系统引入了信息检索机制，使智能体能够从向量存储中检索相关的上下文信息。该架构在模拟对话场景中通过消融实验进行了评估，对比了具备与不具备信息检索功能的智能体表现。结果令人鼓舞，为探索基于心理学基础的结构如何丰富智能体行为开辟了新方向。本文的贡献在于提出了一种智能体架构，该架构将交互分析理论与上下文信息检索相结合，以增强基于LLM的多智能体模拟的真实感。", "summary_generated_time": "2026-01-09 13:05:33", "summary_model": "z-ai/glm-4.7"}]}], "overview": "# 今日AI论文速览 (2025-12-19)\n\n生成每日速览时发生错误: Connection error."}