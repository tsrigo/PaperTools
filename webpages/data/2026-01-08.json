{"date": "2026-01-08", "categories": [{"name": "Artificial Intelligence", "count": 9, "papers": [{"index": "#5", "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning", "link": "/arxiv/2601.05187", "arxiv_id": "2601.05187", "authors": "Yanchang Liang, Xiaowei Zhao", "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.843459", "filter_reason": "这篇论文符合筛选标准，主要基于以下判断： 1.  **核心贡献符合“自我演化”与“单智能体”方向**： 论文的核心不仅仅是将LLM应用于Simulink这一特定领域，而是提出了一种新的智能体架构和训练方法。具体而言，它提出了 **Reflection-GRPO (ReGRPO)**，这是一种结合了**自我反思**机制的强化学习算法。该算法通过自我反思痕迹提供中间反馈，解决长视距任务中的稀疏奖励问题，从而加速收敛并提高鲁棒性。这直接对应了筛选标准中的“自我演化”和“自我反思”机制。 2.  **具备明确的Agentic特征**： 论文描述了一个轻量级的 **“计划-执行”架构**，并明确提到该架构赋予智能体低级工具技能和高级设计推理能力。这符合筛选标准中关于“规划”和“工具使用”的正面指标。 3.  **符合“自我演化的应用”例外规则**： 虽然论文的应用场景是Simulink建模（特定工程领域），但根据筛选标准第四步第2点，如果论文的核心是提出一种新的“自我演化”机制（即ReGRPO和两阶段课程学习），即使它被应用在特定领域，也应该保留。本文的重点在于智能体如何通过反思和强化学习进行自我完善和迭代，而非单纯的应用。 综上所述，该论文在构建智能体架构、引入自我反思机制以及通过强化学习实现自我演化方面做出了实质性贡献，符合“LLM智能体及其演化”的研究课题要求。", "summary2": "本文旨在解决LLM在图形化Simulink建模中的应用难题。针对Simulink建模任务，我们提出了SimuAgent框架，采用轻量级Python字典表示模型，并引入Reflection-GRPO (ReGRPO)算法结合两阶段训练策略。我们在新发布的SimuBench数据集（5300个任务）上进行了实验，通过建模准确率验证了其有效性。结果显示，SimuAgent比标准RL收敛更快，且超越了GPT-4o。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "大语言模型彻底变革了基于文本的代码自动化，但它们在面向图形的工程工作流中的潜力仍未得到充分探索。我们介绍了SimuAgent，这是一个专为Simulink定制的、由LLMs（大语言模型）驱动的建模与仿真智能体。SimuAgent用简洁的字典风格Python表示法取代了冗长的XML（可扩展标记语言），大幅减少了token（词元）数量，提高了可解释性，并实现了快速的进程内仿真。一种轻量级的plan-execute（规划-执行）架构，分两个阶段进行训练，使智能体既具备低级工具技能，又具备高级设计推理能力。为了解决长视界任务中的稀疏奖励问题，我们提出了Reflection-GRPO (ReGRPO)，它通过自我反思轨迹增强了Group Relative Policy Optimization (GRPO)（群体相对策略优化），这些轨迹提供了丰富的中间反馈，从而加速收敛并提高鲁棒性。在我们新发布的包含5300个多领域建模任务的基准SimuBench上的实验表明，使用SimuAgent微调的Qwen2.5-7B模型比标准RL（强化学习）基线收敛更快，建模精度更高，并且在同一基准上使用few-shot prompting（少样本提示）进行评估时，甚至超越了GPT-4o。消融实验证实，两阶段课程和抽象-重构数据增强进一步提高了泛化能力。SimuAgent完全在本地使用适度的硬件进行训练和运行，为工业模型驱动工程提供了一个保护隐私且具有成本效益的解决方案。SimuAgent弥合了LLMs与图形建模环境之间的差距，为工业环境下的AI辅助工程设计提供了实用的解决方案。", "summary_generated_time": "2026-01-13 10:37:49", "summary_model": "z-ai/glm-4.7"}, {"index": "#10", "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction", "link": "/arxiv/2601.05107", "arxiv_id": "2601.05107", "authors": "Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng", "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.844939", "filter_reason": "这篇论文完全符合我的研究范围，属于“单智能体”方向下的核心研究。 1.  **核心贡献判断 (第一步)**: *   论文的核心贡献是提出了 **SteeM (Steerable Memory Agent)**，这是一个用于LLM智能体的新框架。 *   它旨在解决长期人机交互中智能体如何使用记忆的问题，即如何平衡“记忆锚定”与“创新”。这属于对LLM智能体核心组件（记忆机制）的构建和改进，而非简单的应用或基础设施研究。 2.  **符合核心关注点 (第二步)**: *   论文直接对应我的研究焦点中的 **“单智能体”** 类别。 *   具体而言，它涉及智能体的关键能力指标：**`Memory` (记忆)**。论文探讨了如何量化记忆依赖度以及如何动态调节记忆的使用，这是提升智能体在长期任务中表现的关键技术。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 *   虽然提到了“个性化人机协作”，但这只是应用场景，其核心在于提出一种可控的记忆机制框架，而非单纯解决特定领域的业务问题。 综上所述，该论文致力于改进LLM智能体的记忆机制，属于构建和演化LLM智能体的核心方法论研究，因此予以保留。", "summary2": "本文旨在解决长期人机交互中智能体过度依赖历史记忆导致的“Memory Anchoring”问题，实现用户对记忆依赖度的动态控制。针对Research和Tutoring场景，我们提出了一种名为SteeM的框架，通过偏好对齐的数据生成、SFT和GRPO训练，使智能体能根据用户偏好动态调整记忆使用程度。我们在合成的长期交互数据集上，通过alignment error和reward score验证了其有效性，结果显示SteeM优于传统提示方法和记忆掩码策略。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "随着 LLM-based agents (基于大语言模型的智能体) 在长期交互中的应用日益广泛，cumulative memory (累积记忆) 对于实现个性化以及保持风格一致性至关重要。然而，大多数现有系统在记忆使用上采取了一种 all-or-nothing approach (全有或全无的方法)：纳入所有相关的过往信息可能导致 Memory Anchoring (记忆锚定)，即智能体受困于过去的交互；而完全排除记忆则会导致 under-utilization (利用不足) 以及重要交互历史的丢失。我们表明，智能体的 reliance on memory (记忆依赖) 可以被建模为一个显式的且用户可控的维度。我们首先引入了一种 memory dependence (记忆依赖度) 的 behavioral metric (行为指标)，用于量化过往交互对当前输出的影响。随后，我们提出了 **Stee**rable **M**emory Agent (可操控记忆智能体)，即 \\texttt{SteeM}，这是一个允许用户动态调节 reliance on memory (记忆依赖) 的框架，其调节范围涵盖促进创新的 fresh-start mode (全新开始模式) 到紧密遵循交互历史的 high-fidelity mode (高保真模式)。在不同场景下的实验表明，我们的方法始终优于 conventional prompting (传统提示) 和 rigid memory masking strategies (僵化的记忆掩码策略)，为个性化的人机协作提供了更为 nuanced (细致的) 且有效的控制手段。", "summary_generated_time": "2026-01-13 10:37:49", "summary_model": "z-ai/glm-4.7"}, {"index": "#28", "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents", "link": "/arxiv/2601.04888", "arxiv_id": "2601.04888", "authors": "Tongyu Wen, Guanting Dong, Zhicheng Dou", "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.850247", "filter_reason": "这篇论文完全符合我的研究范围，理由如下： 1.  **核心判断**: *   论文的核心贡献是提出了 **SmartSearch**，这是一个专门用于改进 **LLM-based search agents**（基于LLM的搜索智能体）的新框架。 *   它不是简单地将智能体作为工具应用到特定垂直领域（如医疗或金融），而是专注于解决智能体本身在执行搜索任务时的核心缺陷（即中间搜索查询质量不高），属于对智能体能力的底层构建和改进。 2.  **符合研究焦点**: *   **单智能体**: 论文明确针对搜索智能体的架构进行优化，涉及智能体的 **Tool Use**（使用搜索工具）和 **Planning**（生成查询的推理过程）。 *   **自我演化**: 论文引入了“过程奖励”和“查询优化”机制，并设计了一个三阶段的课程学习框架（模仿 -> 对齐 -> 泛化）。这种机制旨在让智能体通过反馈信号，逐步内化并自我改进其生成查询的能力。这符合“自我演化”中通过环境反馈进行自我完善和迭代的标准。 3.  **排除标准检查**: *   论文不涉及安全、对齐、可解释性或水印等排除主题。 *   论文不涉及多模态或视觉技术。 *   论文不涉及知识图谱或图神经网络。 综上所述，该论文通过提出新的方法论来提升LLM智能体的工具使用效率和自我优化能力，高度契合“LLM智能体及其演化”的研究课题。", "summary2": "本文旨在解决基于LLM的搜索代理中中间搜索查询质量低下的问题。针对知识密集型任务，我们提出了一种结合过程奖励和查询优化的SmartSearch框架，并设计了三阶段课程学习策略。我们在2WikiMQA、HotpotQA等六个基准上，通过EM和F1分数验证了其有效性，显著提升了搜索效率和查询质量。", "inspiration_trace": "基于对论文《SmartSearch: Process Reward-Guided Query Refinement for Search Agents》的深入分析，以下是作者产出该核心方法的逻辑推演与思考过程还原：\n\n### 1. 宏观观察：搜索智能体的“阿喀琉斯之踵”\n**起点：** 作者首先关注到 LLM 搜索智能体在解决知识密集型任务时的巨大潜力。现有的研究范式（如 ReAct）已经证明了通过迭代调用搜索工具可以弥补模型的知识盲区。\n**发现盲区：** 尽管学术界在优化智能体的“推理范式”（如思维链设计、强化学习策略）上投入巨大，但作者敏锐地发现了一个被忽视的环节：**中间搜索查询的质量**。\n**直觉判断：** 智能体的推理能力再强，如果它向搜索引擎发出的指令是模糊、错误或冗余的，那么检索回来的信息就是无用的，后续的推理自然建立在沙堆之上。\n\n### 2. 问题聚焦：从“结果导向”到“过程诊断”\n**现象分析：** 作者通过案例分析（如文中提到的 Kevin McCarthy 演员与政治家混淆的例子）发现，一个微小的查询偏差（如漏掉“Actor”关键词）会导致检索结果完全跑偏，进而导致整个推理轨迹崩塌。\n**现有方法的局限：** 传统的强化学习训练通常只关注“结果奖励”，即最终答案是否正确。这种反馈是稀疏且滞后的。模型即使最终答对了，中间可能走了很多弯路；或者答错了，模型也不知道具体是哪一步的查询出了问题。\n**核心假设：** 如果能对每一个中间步骤的搜索动作进行细粒度的评估和反馈，就能从根本上提升智能体的信息获取能力。\n\n### 3. 机制设计一：构建“过程奖励”作为质检员\n**思考：** 如何定义一个“好的搜索查询”？作者意识到这不能仅靠单一指标。\n**逻辑拆解：**\n*   **新颖性：** 查询不应重复。如果这一步搜到的内容和上一步完全一样，那就是浪费算力。这可以通过规则（文档重叠度）低成本解决。\n*   **有用性：** 查询必须服务于最终答案。这需要语义理解。这一步的意图是否必要？检索结果是否包含预期信息？这需要模型来判断。\n**方案形成：** 结合两者，提出了“双层信用评估”。规则层负责查重（效率），模型层负责语义评估（质量），从而输出具体的分数和文本反馈。\n\n### 4. 机制设计二：引入“查询重写”作为修正员\n**思考：** 仅仅给低分查询打分是不够的，模型需要知道“什么是更好的”。\n**灵感来源：** 类似于人类写作时的修改过程。如果发现某一步查询质量低，为什么不直接修改它，然后基于修改后的查询重新跑一遍后续流程？\n**逻辑闭环：**\n1.  生成原始轨迹。\n2.  利用过程奖励找出“坏”的查询步骤。\n3.  基于反馈文本，利用模型重写该查询。\n4.  从重写点开始重新生成后续步骤，形成一条“修正后的轨迹”。\n**价值：** 这不仅生成了更好的数据，还天然构成了用于对比学习的成对数据。\n\n### 5. 训练策略：三阶段课程学习\n**思考：** 直接让模型学会上述所有能力太难，需要一个循序渐进的过程。\n**逻辑演进：**\n*   **阶段一（模仿）：** 先让模型看“好榜样”。利用过程奖励筛选出那些不仅答案正确、且中间查询也高质量的轨迹进行监督微调（SFT）。这确立了基准能力。\n*   **阶段二（对齐）：** 让模型学会“辨别好坏”。利用上一阶段的“查询重写”机制，构造“原始轨迹”和“修正轨迹”的对比对。通过 DPO（直接偏好优化），让模型偏好那些查询质量更高的路径。\n*   **阶段三（泛化）：** 让模型在实战中“探索”。在强化学习阶段，将查询重写机制融入 Rollout 过程。如果模型生成了坏查询，系统会自动修正并继续，让模型在探索中不断内化“如何写出好查询”的策略。\n\n### 6. 总结：逻辑链的终点\n作者最终构建的 SmartSearch 框架，其核心思想并非单纯堆砌算力或模型参数，而是通过**引入细粒度的过程监督**和**主动的轨迹修正机制**，解决了搜索智能体中“工具使用不当”这一核心瓶颈。从发现中间查询的重要性，到设计评估标准，再到利用修正数据驱动训练，形成了一个完整的逻辑闭环。", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "基于大语言模型 (LLM) 的搜索智能体通过结合信息检索能力，在解决知识密集型问题方面展现出了巨大的潜力。现有工作主要集中在优化搜索智能体的推理范式，然而推理过程中中间搜索查询的质量却往往被忽视。结果是，生成的查询往往不准确，导致检索结果不理想，最终限制了搜索智能体的整体效能。为了缓解这一问题，我们提出了 SmartSearch，这是一个基于两个关键机制的框架：(1) 过程奖励，它通过双层信用评估为每个中间搜索查询的质量提供细粒度的监督；(2) 查询精炼，通过选择性精炼低质量搜索查询并基于这些精炼结果重新生成后续搜索轮次，从而促进查询生成的优化。为了使搜索智能体能够在过程奖励的指导下逐步内化提高查询质量的能力，我们设计了一个三阶段的课程学习框架。该框架引导智能体经历从模仿，到对齐，最终到泛化的递进过程。实验结果表明，SmartSearch 始终优于现有基线，额外的定量分析进一步证实了其在搜索效率和查询质量方面的显著提升。代码可在 https://github.com/MYVAE/SmartSearch 获取。", "summary_generated_time": "2026-01-13 10:37:46", "summary_model": "z-ai/glm-4.7"}, {"index": "#33", "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models", "link": "/arxiv/2601.04861", "arxiv_id": "2601.04861", "authors": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li, Bing Qin, Ting Liu", "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.851663", "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（符合）**：论文的本质是构建和改进 LLM 智能体框架。作者提出了 OI-MAS（Orchestrating Intelligence），这是一个新颖的多智能体系统框架。它并非将智能体作为工具应用于特定垂直领域（如医疗或金融），而是针对多智能体系统本身的“计算效率低下”这一痛点，提出了新的架构解决方案（自适应模型选择策略和状态依赖路由机制）。这属于对智能体框架的改进和优化。 2.  **正面指标（强匹配）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体能力**：论文重点研究了智能体间的 `Collaboration`（协作），并引入了机制来动态选择智能体角色和模型尺度。 *   **优化机制**：虽然涉及效率，但其核心是通过智能体层面的“路由”和“置信度感知”来实现的，这是提升多智能体系统性能的关键技术，属于构建智能体方法论的一部分。 3.  **排除标准（无冲突）**：论文不涉及安全对齐、多模态视觉或图技术，也不是单纯的基础设施研究。 综上所述，该论文致力于解决多智能体协作中的效率与资源分配问题，是对 LLM 智能体架构的直接改进，符合“构建、改进或演化 LLM 智能体”的核心目标。", "summary2": "本文旨在解决多智能体系统计算效率低下和成本高昂的问题。针对复杂推理任务，我们提出了一种OI-MAS框架，该框架结合了状态依赖路由和置信度感知机制，能动态选择智能体角色和多尺度LLM。我们在GSM8K、MATH、MedQA、GPQA和MBPP数据集上通过准确率和推理成本验证了其有效性。", "inspiration_trace": "基于论文《Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models》，以下是作者产出该核心方法的逻辑链推演：\n\n### 1. 宏观问题：多智能体系统的“性能-成本”悖论\n**思考起点：**\n作者首先观察到多智能体系统（MAS）在复杂推理任务中表现卓越，超越了单智能体。然而，这种性能的提升伴随着巨大的计算开销和延迟。\n**核心矛盾：**\n现有的MAS框架通常采用“一刀切”的策略，即在整个推理流程中，无论任务难易或角色分工，都统一调用最大规模的LLM（如70B参数模型）。这就像为了拧一颗螺丝而动用了整个工厂的产能，造成了极大的资源浪费。\n\n### 2. 深入观察：现有路由机制的局限性\n**现象分析：**\n作者审视了现有的两类优化尝试：\n1.  **动态智能体路由：** 能够根据任务动态调整“谁来做”（Agent角色），但通常假设所有Agent共享同一个大模型，忽略了不同步骤对算力需求的差异。\n2.  **LLM模型路由：** 能够根据输入选择“用哪个模型”，但这主要应用于单智能体场景，且往往是静态的（在推理开始前决定），无法适应推理过程中不断变化的上下文状态。\n**关键缺口：**\n缺乏一种机制，能够**在推理的每一步**，同时动态决定“由哪个角色处理”以及“该角色需要多大算力的模型”。现有的方法要么是“静态团队+动态模型”，要么是“动态团队+静态模型”，未能实现两者的联合动态优化。\n\n### 3. 提出假设：解耦角色与算力，引入状态依赖\n**核心假设 1（功能与资源解耦）：**\n决定“做什么”（Agent Role，如生成、验证、分解）和决定“用多大力量做”（Model Scale，如3B vs 70B）应该是两个独立的决策过程。将它们解耦可以让系统先规划推理路径，再根据路径需求分配资源。\n**核心假设 2（状态依赖性）：**\n任务的复杂性是随着推理轨迹演进的。一个任务可能在初始阶段很简单（适合小模型），但在中间验证阶段变得极其复杂（必须用大模型）。因此，路由决策必须依赖于当前的“推理状态”，而不仅仅是初始的查询。\n\n### 4. 方法论构建：指挥家隐喻与置信度引导\n**设计理念（指挥家模式）：**\n作者将多智能体协作比作交响乐演奏。系统需要一个“指挥家”，它不直接演奏（不直接生成答案），而是负责在每一个时刻决定：\n1.  哪种乐器（角色）现在需要发声？\n2.  需要多大的音量（模型规模）？\n\n**机制创新（置信度作为复杂度代理）：**\n为了实现上述动态调度，作者面临一个核心难题：**系统如何“知道”当前步骤有多难？**\n作者引入了“置信度”作为关键信号：\n*   **逻辑：** 如果模型对当前状态的处理很有信心（高置信度），说明当前任务简单，应强制使用低成本的小模型以节省资源；如果模型表现出犹豫或低置信度，说明遇到了复杂情况，应允许甚至鼓励调用大模型。\n*   **实现：** 在强化学习的优化目标中，将置信度作为成本惩罚项的权重。置信度高时，成本惩罚极大（迫使选小模型）；置信度低时，成本惩罚降低（允许选大模型）。\n\n### 5. 逻辑闭环与验证\n**最终架构（OI-MAS）：**\n构建了一个分层路由系统：\n*   **第一层（角色路由器）：** 分析当前状态，决定激活哪些Agent角色（如Generator, Verifier）。\n*   **第二层（模型路由器）：** 结合当前状态和选定角色，从多尺度模型池中分配最匹配的模型。\n*   **优化目标：** 通过置信度加权的损失函数，训练系统学会“好钢用在刀刃上”。\n\n**预期结果：**\n这种设计预期会产生一种智能的分配模式：生成核心内容时调用大模型，进行简单的格式检查或聚合时调用小模型；任务简单时提前终止，任务困难时自动升级算力。\n\n---\n\n**总结：**\n作者的思考路径从**发现资源浪费**出发，通过**批判现有方法的静态性**，提出了**角色与模型联合动态路由**的构想，并巧妙地利用**模型置信度**作为调节资源分配的内生信号，最终构建了一个像指挥家一样高效调配算力的多智能体框架。", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "尽管 multi-agent systems (MAS，多智能体系统) 在复杂推理任务中展现出优于 single-agent approaches (单智能体方法) 的性能，但它们往往面临显著的计算效率低下问题。现有框架通常在所有 agent roles (智能体角色) 中统一部署 large language models (LLMs，大语言模型)，未能顾及不同 reasoning stages (推理阶段) 各异的 cognitive demands (认知需求)。我们通过提出 OI-MAS framework (OI-MAS 框架) 来解决这一效率问题，这是一种新颖的 multi-agent framework (多智能体框架)，在 multi-scale LLMs (多尺度大语言模型) 的 heterogeneous pool (异构池) 中实现了 adaptive model-selection policy (自适应模型选择策略)。具体而言，OI-MAS 引入了一种 state-dependent routing mechanism (状态依赖路由机制)，能够在推理过程中动态选择 agent roles (智能体角色) 和 model scales (模型规模)。此外，我们引入了一种 confidence-aware mechanism (置信度感知机制)，该机制基于 task complexity (任务复杂度) 选择合适的 model scales (模型规模)，从而减少对 large-scale models (大规模模型) 的不必要依赖。实验结果表明，OI-MAS 始终优于 baseline multi-agent systems (基线多智能体系统)，在将 cost (成本) 降低高达 79.78% 的同时，将 accuracy (准确率) 提高了高达 12.88%。", "summary_generated_time": "2026-01-13 10:37:52", "summary_model": "z-ai/glm-4.7"}, {"index": "#49", "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search", "link": "/arxiv/2601.04703", "arxiv_id": "2601.04703", "authors": "Yiqun Chen, Lingyong Yan, Zixuan Yang, Erhan Zhang, Jiashu Zhao, Shuaiqiang Wang, Dawei Yin, Jiaxin Mao", "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.856378", "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“多智能体”方向。 1.  **核心判断 (第一步)**: *   论文的核心贡献是提出了 **M-ASK (Multi-Agent Search and Knowledge)**，这是一个新的**多智能体框架**。 *   它旨在解决现有单体智能体在 Agentic Search 任务中的结构性瓶颈，而非仅仅将现有模型应用到一个特定领域。因此，它属于构建和改进 LLM 智能体的方法论研究，符合“保留”标准。 2.  **正面指标匹配 (第二步)**: *   **核心范式**: 论文明确涉及 `Multi-Agent Systems (MAS)` 和 `Agentic AI`。 *   **多智能体**: 论文将智能体解耦为两个互补的角色：`Search Behavior Agents`（负责规划和执行搜索动作）和 `Knowledge Management Agents`（负责聚合、过滤和维护内部上下文）。这体现了智能体间的协作与分工。 *   **智能体能力**: 涉及 `Planning`（规划搜索动作）、`Tool Use`（工具使用）以及 `Memory`（通过知识管理智能体维护内部上下文）。 3.  **排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **特殊与模糊情况处理 (第四步)**: *   虽然论文在 Multi-hop QA（多跳问答）基准测试上进行了实验，但这只是为了验证框架的有效性，其核心在于提出了一种新的多智能体协作架构来优化 Agentic Search，而非单纯的应用型研究。 综上所述，该论文通过构建多智能体系统来改进 LLM 智能体的搜索与推理能力，精准契合“多智能体”这一研究焦点。", "summary2": "本文旨在解决单体 Agentic Search 架构因无约束输出、稀疏奖励和搜索噪声导致的训练不稳定及信用分配难题。针对复杂的多跳问答场景，我们提出了一种 M-ASK 多智能体框架，通过解耦 Search Behavior Agents 和 Knowledge Management Agents，并利用 turn-level dense rewards 实现联合优化。在 HotpotQA、2Wiki、Musique 等多跳问答基准上，通过 F1 Score 验证了其有效性和稳定性。", "inspiration_trace": "基于论文《Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search》，以下是对作者核心方法产出逻辑链的系统性推演：\n\n### 第一阶段：宏观观察与问题定位\n**——从“单体智能”的繁荣中看到结构性隐患**\n\n1.  **观察现象**：\n    *   随着 Agentic Search（智能体搜索）的兴起，LLM 被赋予了使用工具（如搜索引擎）进行迭代推理的能力。\n    *   现有的 SOTA 方法（如 Search-r1）大多采用**单体架构**，即由一个单一的 LLM 承担所有任务：规划、搜索、信息整合和最终回答。\n\n2.  **发现问题**：\n    *   作者发现这种“全能型”单体智能体在训练中表现出极不稳定的特征，容易崩溃。\n    *   虽然端到端的强化学习（RL）能提升性能，但在复杂的多跳问答任务中，单体模型往往难以收敛。\n\n### 第二阶段：深度诊断与归因分析\n**——解构“长视界信用分配”的死结**\n\n作者并未止步于“不稳定”的现象，而是深入剖析了单体架构背后的三个互为因果的致命缺陷，构成了一个**“毒性三角”**：\n\n1.  **无约束的输出长度**：\n    *   单体模型倾向于生成冗长的推理链。这不仅增加了计算成本，更重要的是拉长了决策轨迹，使得“最终结果”与“早期决策”之间的距离过远。\n2.  **稀疏的奖励信号**：\n    *   通常只有在任务结束时（回答正确与否）才有反馈。在长达数十步的推理中，模型无法知道哪一步是对的，哪一步是错的。\n3.  **搜索噪声**：\n    *   外部工具（搜索引擎）会引入无关或错误的信息。在单体架构中，这些噪声会直接累积在上下文中，干扰后续推理。\n\n**核心洞察**：\n上述三者共同导致了**长视界信用分配难题**。当一条冗长、充满噪声的轨迹最终只得到一个简单的对错反馈时，优化算法根本无法将奖励归因到具体的某个 Token 或动作上，导致训练梯度发散，模型崩溃。\n\n### 第三阶段：战略假设与范式转移\n**——从“分身乏术”到“术业专攻”**\n\n为了打破上述死结，作者提出了一个核心假设：**如果将“搜索行为”与“知识管理”解耦，就能从根本上隔离噪声并压缩轨迹。**\n\n1.  **角色解耦**：\n    *   不再让一个大脑既负责“找信息”又负责“记信息”。\n    *   **搜索行为代理**：只负责决策（搜什么、何时停）。\n    *   **知识管理代理**：只负责记忆（过滤噪声、更新状态）。\n2.  **预期效果**：\n    *   通过 KMA 的过滤，进入上下文的信息是高密度的，解决了“噪声”问题。\n    *   通过 SBA 的专注，每次生成的动作更短更精准，解决了“输出长度”问题。\n\n### 第四阶段：机制设计与优化逻辑\n**——构建“协作-反馈”闭环**\n\n有了架构假设，作者进一步思考：如何让这两个独立的团队协同工作并稳定训练？\n\n1.  **通信机制设计**：\n    *   设计了一个**结构化知识状态**。这不仅是共享内存，更是两个团队交互的唯一接口。SBA 写入查询，KMA 更新状态，双方通过这个紧凑的状态进行异步协作。\n\n2.  **解决信用分配难题（关键创新）**：\n    *   既然全局奖励太稀疏，那就引入**Turn-level Dense Rewards（轮级密集奖励）**。\n    *   **状态奖励**：给负责输出的 Agent（如 Answer Agent）基于当前答案质量的绝对分数。\n    *   **边际奖励**：这是最精妙的一笔。给负责迭代的 Agent（Search, Summary, Update）分配“边际增益”（$F1_{current} - F1_{previous}$）。\n    *   **逻辑**：如果这一轮搜索和更新让答案变好了，大家都有奖；如果没变好或变差了，大家都要负责。这迫使搜索团队必须找对信息，知识团队必须滤对噪声。\n\n### 第五阶段：验证与理论闭环\n**——从“假设”到“定律”**\n\n最后，作者通过实验验证了这一逻辑链条的完整性：\n*   **消融实验**：移除知识管理模块（KMA）导致性能下降，证明了“过滤噪声”的必要性；移除轮级奖励导致多跳任务崩溃，证明了“密集反馈”在长链路中的关键作用。\n*   **稳定性分析**：对比单体架构 Search-r1 的高崩溃率，M-ASK 实现了 0% 崩溃，证实了“解耦”确实解决了长视界训练的不稳定性。\n\n---\n\n**总结：作者的思考路径**\n从**单体架构的不稳定性**出发 $\\rightarrow$ 诊断出**长视界信用分配**是核心病灶 $\\rightarrow$ 提出通过**角色解耦**来切断噪声与长度的累积 $\\rightarrow$ 利用**边际奖励机制**将全局反馈转化为局部指导 $\\rightarrow$ 最终构建出一个既分工明确又利益绑定的多智能体协作系统。", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-13 10:37:57", "summary_model": "z-ai/glm-4.7"}, {"index": "#59", "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering", "link": "/arxiv/2601.04620", "arxiv_id": "2601.04620", "authors": "Di Zhang", "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.859191", "filter_reason": "1.  **核心判断**: 论文的核心贡献是提出了 **AgentDevel**，这是一个将 LLM 智能体的自我改进过程重构为“发布工程”的新框架。它通过迭代运行智能体、生成质量信号、诊断故障并合成新版本，实现了智能体的自我演化。这完全符合“构建、改进或演化 LLM 智能体”的目标，不属于非演化型应用、非Agentic的基础推理或基础设施研究。 2.  **研究焦点匹配**: 论文直接对应研究焦点的第三点 **“自我演化”**。它提出了一种新的演化机制，即通过外部化的回归感知发布管道来迭代改进智能体，这与传统的内部自我反思或基于种群的搜索不同，强调了演化的稳定性和可审计性。 3.  **正面指标**: 论文明确涉及 `Self-Evolving`、`Self-Improvement`、`Iterative Improvement`、`Agentic AI` 以及 `LLM-based Agents` 等核心范式和能力。 4.  **排除标准**: 论文不涉及安全对齐、多模态技术或特定领域的垂直应用（如医疗、金融），也不属于基础设施优化。 5.  **结论**: 该论文为 LLM 智能体的演化提供了新的方法论视角，属于前沿研究，应予以保留。", "summary2": "本文旨在解决自进化 LLM Agent 改进过程不稳定且难以审计的问题。针对执行密集型任务，我们提出了一种名为 AgentDevel 的发布工程管道，该管道包含实现无关 Critic、可执行诊断及翻转中心 Gating 机制。我们在 SWE-bench、WebArena 和 StableToolBench 上通过 Resolved rate、Success rate 及回归率等指标验证了其有效性，实现了性能显著提升且回归更少的稳定改进。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-13 10:38:00", "summary_model": "z-ai/glm-4.7"}, {"index": "#69", "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration", "link": "/arxiv/2601.04544", "arxiv_id": "2601.04544", "authors": "Jiuzhou Zhao, Chunrong Chen, Chenqi Qiao, Lebin Zheng, Minqi Han, Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang", "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.862038", "filter_reason": "这篇论文完全符合筛选标准，属于“多智能体”方向的核心研究。 1.  **核心判断（第一步）**： *   论文的核心贡献是提出了 **TCAndon-Router (TCAR)**，这是一个用于多智能体协作的自适应推理路由器。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统（MAS）中的关键组件——路由器——进行了构建和改进。 *   论文设计了一个“协作执行流水线”，其中包括一个专门的“Refining Agent”来聚合和优化响应，这属于构建新的多智能体交互机制。 2.  **正面指标（第二步）**： *   **核心范式**：明确属于 `Multi-Agent Systems (MAS)`。 *   **多智能体特性**：涉及 `Collaboration`（协作执行流水线）、`Communication`（通过路由器进行任务分配）以及 `Agent Society`（支持动态智能体接入）。 *   **智能体能力**：利用 `Reasoning`（生成自然语言推理链）来辅助路由决策。 3.  **排除标准（第三步）**： *   论文不涉及安全、对齐、多模态视觉或图技术等排除项。 4.  **综合结论**： 该论文致力于解决多智能体系统中的任务分配与协作效率问题，提出了新的路由机制和协作框架，直接对应研究课题中的“多智能体”方向，因此应当保留。", "summary2": "本文旨在解决多智能体系统中现有路由策略难以动态接入新智能体及处理能力重叠导致的路由冲突问题。针对企业级应用中模糊或跨域的查询场景，我们提出了一种名为TCAndon-Router (TCAR) 的自适应推理路由器，通过生成自然语言推理链选择候选智能体，并利用Refining Agent聚合多智能体响应。在CLINC150、HWU64等公共数据集及腾讯云私有数据集上，通过Accuracy和F1等指标验证了其有效性。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "多智能体系统已成为构建高性能智能应用的有力范式。在这些系统中，负责确定哪些专家智能体应处理给定查询的路由器在整体性能中起着至关重要的作用。现有的路由策略通常分为两类：性能路由，它在不同规模的模型之间平衡延迟和成本；以及任务路由，它将查询分配给特定领域的专家以提高准确性。在现实世界的应用程序中，任务路由更为适用；然而，大多数现有方法依赖于静态单标签决策，这引入了两个主要局限性：随着业务领域的扩展，难以无缝集成新的智能体；以及由智能体能力重叠引起的路由冲突，最终降低了准确性和鲁棒性。\n\n为了解决这些挑战，我们提出了 TCAndon-Router (TCAR)：一种用于多智能体协作的自适应推理路由器。与传统路由器不同，TCAR 支持动态智能体接入，并在预测能够处理查询的候选智能体集合之前，首先生成一条自然语言推理链。此外，我们设计了一个协作执行管道，其中选定的智能体独立生成响应，随后由专门的精炼智能体将这些响应聚合并精炼为单一的高质量响应。\n\n在公共数据集和真实企业数据上的实验表明，TCAR 显著提高了路由准确性，减少了路由冲突，并在模糊场景中保持了鲁棒性。我们已在 https://huggingface.co/tencent/TCAndon-Router 发布了 TCAR，以支持未来关于可解释和协作多智能体路由的研究。", "summary_generated_time": "2026-01-13 10:38:02", "summary_model": "z-ai/glm-4.7"}, {"index": "#75", "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery", "link": "/arxiv/2601.04500", "arxiv_id": "2601.04500", "authors": "Yifei Gao, Jiang Wu, Xiaoyi Chen, Yifan Yang, Zhe Cui, Tianyi Ma, Jiaming Zhang, Jitao Sang", "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.", "subjects": "Artificial Intelligence", "date": "2026-01-08", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.863714", "filter_reason": "这篇论文符合筛选标准，应予以保留。判断依据如下： 1.  **核心贡献符合要求 (第一步 & 第二步)**: *   论文的核心贡献是提出了 **GUITester**，这是一个**多智能体框架**。它不仅仅是将现有的LLM智能体作为工具应用，而是构建了一个新的架构来解决特定问题。 *   该框架包含两个关键模块：**规划-执行模块 (PEM)** 和 **层次化反思模块 (HRM)**。这直接对应了研究焦点中的 **\"Agentic\" (规划)** 和 **\"自我演化\" (自我反思 Self-Reflection / 自我修正 Self-Correction)** 方向。 2.  **涉及多智能体与自我反思机制 (第二步 & 第四步)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   论文提出的 **HRM (Hierarchical Reflection Module)** 通过分析交互历史来解决归因歧义，这是一种典型的 **自我反思** 和 **自我修正** 机制。虽然它应用于GUI测试，但其核心在于改进智能体处理反馈和自我完善的能力，符合“自我演化”中通过反思进行迭代的定义。 3.  **特殊情况的正确处理 (第三步 & 第四步)**: *   **应用 vs. 方法论**: 虽然论文的应用场景是GUI测试（特定领域），但其核心在于提出了解决“目标导向掩蔽”和“执行偏差归因”的**新方法论**（即解耦导航与验证的框架），而非单纯的应用。 *   **多模态**: 论文虽然使用了多模态大模型 (MLLM) 来感知GUI界面，但视觉仅作为智能体感知环境的工具，研究的核心并非视觉模型的改进，而是智能体的决策与反思框架，因此符合“除非它们被用作智能体感知环境的工具”的例外条款。 综上所述，该论文在构建多智能体框架、引入规划机制以及实现自我反思方面做出了实质性贡献，完全符合“LLM智能体及其演化”的研究课题。", "summary2": "本文旨在解决现有GUI智能体在探索性测试中难以自主发现缺陷的问题。针对目标导向掩蔽和执行偏差归因两大挑战，我们提出了一种名为GUITester的多智能体框架，通过规划执行模块（PEM）主动探测缺陷，并利用分层反思模块（HRM）解决归因歧义。我们在首个交互式benchmark GUITestBench上通过F1-score（Pass@3）验证了其有效性，结果显示GUITester达到48.90%，显著优于现有基线。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "翻译失败", "summary_generated_time": "2026-01-13 10:38:03", "summary_model": "z-ai/glm-4.7"}, {"index": "#98", "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements", "link": "/arxiv/2601.04235", "arxiv_id": "2601.04235", "authors": "Hong Su", "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.", "subjects": "Artificial Intelligence", "date": "2026-01-04", "category": "cs.AI", "crawl_time": "2026-01-12T11:00:05.870053", "filter_reason": "这篇论文完全符合筛选标准，属于核心研究范围内的“单智能体”与“自我演化”交叉领域。 1.  **核心贡献符合要求 (第一步)**: 论文的核心贡献是提出了一种“主动获取反馈”的模型。这不仅仅是应用现有智能体解决具体问题，而是构建了一种新的智能体交互与评估机制，使智能体能够在没有预设测量的情况下，通过主动与环境交互来获取反馈。这属于构建和改进LLM智能体方法论的研究。 2.  **高度匹配核心关注点 (第二步)**: *   **单智能体**: 论文详细探讨了智能体如何进行“自主行动评估”、“与环境交互”以及“自主规划和调整行动”，这直接对应了Agentic AI中的规划、工具使用和环境交互能力。 *   **自我演化**: 论文引入了“自触发机制”，允许智能体根据内部目标（如准确性、效率）自主调整行动。这种基于反馈的自我调整和迭代优化，正是“自我演化”和“自我修正”的关键体现。 3.  **通过排除标准检查 (第三步)**: *   论文不涉及安全、对齐、多模态视觉核心或图技术等排除项。 *   它不是非演化型应用，而是提出了一种通用的智能体能力增强机制。 综上所述，该论文通过解决智能体如何在开放环境中自主获取反馈并自我完善的问题，为Agentic AI的演化和能力提升提供了新的方法论，因此应当保留。", "summary2": "本文旨在解决AI智能体在开放环境中依赖预定义测量评估行动的问题。针对无预设测量的动态环境，我们提出了一种Actively Feedback Getting model，利用action-induced environmental differences进行反馈检测与主动干预。在文本场景和模拟环境上，通过语义相似度和LLM查询数量验证了其有效性。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "从环境中获取可靠反馈是 Intelligent agents (智能体) 评估其行为正确性并积累可复用知识的基本能力。然而，大多数现有方法依赖于 predefined measurements (预定义度量) 或 fixed reward signals (固定奖励信号)，这限制了它们在开放式和动态环境中的适用性，因为在这些环境中，新的行为可能需要以前未知的反馈形式。为了解决这些局限性，本文提出了一种 Actively Feedback Getting model (主动反馈获取模型)，其中 AI agent 主动与环境交互以发现、筛选和验证反馈，而无需依赖 predefined measurements (预定义度量)。该方法不假设显式的反馈定义，而是利用 action-induced environmental differences (动作引起的环境差异) 来识别未预先指定的目标反馈，这是基于动作不可避免地在环境中产生可测量变化这一观察。此外，本文引入了一种由提高准确度、精确度和效率等 internal objectives (内部目标) 驱动的 self-triggering mechanism (自触发机制)，以自主规划和调整行为，从而在没有 external commands (外部指令) 的情况下实现更快、更聚焦的反馈获取。实验结果表明，所提出的主动方法显著提高了 factor identification (因子识别) 的效率和鲁棒性。", "summary_generated_time": "2026-01-13 10:38:05", "summary_model": "z-ai/glm-4.7"}]}, {"name": "Computation and Language", "count": 1, "papers": [{"index": "#62", "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation", "link": "/arxiv/2601.04516", "arxiv_id": "2601.04516", "authors": "Yuxiao Ye, Yiming Zhang, Yiran Ma, Huiyuan Xie, Huining Zhu, Zhiyuan Liu", "summary": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.", "subjects": "Computation and Language", "date": "2026-01-08", "category": "cs.CL", "crawl_time": "2026-01-12T11:00:05.113595", "filter_reason": "这篇论文完全符合我的研究范围，属于 **Multi-Agent (多智能体)** 方向的前沿研究。具体判断依据如下： 1.  **核心判断 (符合)**: *   论文的核心贡献是提出了一种名为 **LinguaGame** 的新范式，这是一种基于博弈论的多智能体对话生成框架。 *   它不仅仅是应用现有的智能体框架，而是针对多智能体系统的 **交互过程** 本身进行了改进，旨在通过建模意图和策略来提升智能体间的通信效率。这符合“构建、改进 LLM智能体”的核心目标。 2.  **研究焦点匹配 (多智能体)**: *   论文明确属于 **Multi-Agent Systems (MAS)** 范畴。 *   它的核心关注点是多智能体之间的 **Communication (通信)** 和 **Interaction (交互)**。 *   论文涉及智能体如何推断彼此的意图和策略，这属于多智能体协作与博弈中的高级认知能力。 3.  **排除标准检查 (通过)**: *   **非单纯应用**: 虽然论文在模拟法庭和辩论中进行了评估，但其核心贡献是通用的交互机制，而非解决特定领域的业务问题。 *   **非安全/对齐/多模态**: 论文不涉及安全对齐、视觉或多模态内容，纯粹关注语言层面的智能体交互逻辑。 4.  **正面指标**: *   包含核心范式关键词：`Multi-Agent Systems (MAS)`, `Game-Theoretic`。 *   包含多智能体能力关键词：`Communication`, `Negotiation` (隐含在辩论场景中), `Agent Society` (隐含在多智能体环境中)。 综上所述，该论文提出了一种改进多智能体通信机制的新方法，属于 Agentic AI 中多智能体交互的重要进展，因此予以保留。", "summary2": "本文旨在解决基于LLM的Multi-Agent Systems中沟通效率低下的问题。针对模拟法庭和辩论场景，我们提出了一种名为LinguaGame的基于语言学的博弈论范式。该方法将对话建模为关于交际意图和策略的Signalling Game，并利用无训练的均衡近似算法在推理时优化决策。在模拟法庭和辩论数据集上，通过人工专家评估（涵盖清晰度、简洁性、论证和策略等指标），验证了该方法能显著提升对话质量和沟通效率。", "inspiration_trace": "生成灵感溯源时发生错误", "research_insights": "研究洞察分析生成失败", "critical_evaluation": "批判性评估生成失败", "summary_translation": "大语言模型推动了多智能体系统的发展，在该系统中，智能体通过自然语言进行交互，以解决复杂任务或模拟多方对话。现有关于基于大语言模型的多智能体系统的研究主要集中在架构设计方面，例如角色分配和工作流编排。与之不同，本文聚焦于交互过程本身，旨在通过帮助智能体利用语言更有效地传达其预期含义，从而提升通信效率。为此，我们提出了 LinguaGame，这是一种基于语言学的博弈论范式，用于多智能体对话生成。我们的方法将对话建模为基于交际意图和策略的信号博弈，并采用一种无训练的均衡近似算法进行求解，以实现推理时的决策调整。与先前的博弈论多智能体系统不同，后者的博弈设计往往与特定任务目标紧密耦合，而我们的框架依赖于基于语言学的推理，仅与特定任务存在最小程度的耦合。具体而言，该框架将对话视为一种意图性和策略性的通信，要求智能体推断他人旨在实现的目标（意图）以及其追求这些目标的方式（策略）。我们在模拟法庭庭审和辩论场景中对该框架进行了评估，人类专家的评估结果显示通信效率得到了显著提升。", "summary_generated_time": "2026-01-13 10:37:49", "summary_model": "z-ai/glm-4.7"}]}], "overview": "# 今日AI论文速览 (2026-01-08)\n\n生成每日速览时发生错误: Connection error."}