{"date": "2025-09-25", "categories": [{"name": "Artificial Intelligence", "count": 13, "papers": [{"index": "#5", "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA", "link": "/arxiv/2509.21199", "arxiv_id": "2509.21199", "authors": "Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen", "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.", "subjects": "Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.256868", "filter_reason": "这篇论文的核心是研究大语言模型在多跳问答(MHQA)任务中的单遍推理能力，属于提升LLM通用推理能力的研究。论文建立了Fano风格的精度上界理论，揭示了LLM在单遍推理中面临的容量瓶颈问题，并提出了InfoQA框架作为解决方案。该框架通过容量感知的任务分解和主动修剪推理轨迹来增强LLM的多步推理能力，确保信息处理不超过单遍限制。论文符合核心判断标准，因为它关注的是改进LLM的基础推理能力，特别是多步推理这一通用能力。论文也符合正面指标中的核心概念(LLMs)和能力方向(reasoning, multi-step reasoning)。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。InfoQA框架虽然不是典型的智能体系统，但它提出了一种通用的方法来增强LLM在推理任务中的能力，属于提升LLM内在推理能力的研究。因此，这篇论文完全符合\"提高大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在解决LLM在多跳问答任务中单次推理的容量限制问题。针对复杂多跳推理场景，我们提出了一种基于信息论分析的Fano式精度上限理论，揭示了\"精度悬崖\"现象，并开发了InfoQA多调用推理框架。在构建的噪声丰富基准数据集上，通过F1分数验证了InfoQA能有效分解任务、控制信息负载，显著优于单次推理基线方法。", "inspiration_trace": "## 面临的挑战\n作者识别到LLM在多跳问答中面临单次推理容量瓶颈。由于输出长度有限，模型在整合分散证据时容易超出信息承载能力，特别是在长上下文和多跳推理场景下，这种容量溢出导致性能急剧下降。\n\n## 关键洞察\n作者通过信息论分析，特别是Fano不等式，揭示了\"准确率悬崖\"现象：当任务信息需求超过模型输出容量时，准确率不会平稳下降而是会崩溃。进一步解剖发现多跳挑战本质上是逐步容量溢出和跨步错误累积的双重危机，表明单次推理范式不适合复杂推理。\n\n## 解决方案演进\n基于理论分析，作者提出将复杂问题分解为容量可控的子任务，确保每步信息需求在单次容量内。设计显式依赖工作流程维持推理连贯性，通过迭代查询收缩修剪推理轨迹并重写查询，防止信息负载随推理深度增长而溢出。\n\n## 创新点总结\n创新点在于首次从信息论角度形式化LLM单次推理容量限制，提出\"准确率悬崖\"概念，并基于理论指导设计解决方案。作者不是改进单次推理，而是重新思考推理范式，将问题分解与显式依赖管理结合，为LLM复杂推理提供新框架。", "summary_translation": "多跳问答(Multi-Hop Question Answering, MHQA)需要在噪声环境下通过顺序推理整合分散且相互依赖的证据。这项任务对大型语言模型(Large Language Models, LLMs)而言具有挑战性，因为它们的单次输出容量有限，一旦超出该容量，任务相关证据的整合就变得不可靠。因此，单次推理范式本质上容易受到这种容量溢出的影响。为了形式化这一瓶颈，我们的分析建立了一个Fano风格的准确率上限(Fano-style accuracy upper bound)，定义了单次LLMs的理论性能天花板。该上限揭示，一旦任务复杂性超过模型容量，准确率将不可避免地崩溃，这为LLMs中容量感知的MHQA表示和结构化提供了通用原则。\n\n基于这些原则，我们引入了一个用于MHQA的概念验证多调用框架(proof-of-concept multi-call framework)InfoQA。它通过结合容量感知的任务分解(capacity-aware task decomposition)和主动修剪先前的推理痕迹，确保每步高准确率，同时将信息负载保持在单次限制内。它还通过依赖显式工作流(dependency-explicit workflow)实现对推理路径的精确控制，从而获得鲁棒性。我们构建了一个严格且噪声丰富的基准(stringent and noise-rich benchmark)来验证我们的理论和框架。实验结果表明，模型行为与我们预测的容量曲线一致，同时InfoQA实现了持续的性能改进。我们希望我们的工作能够激发更多LLM多步推理方法的研究：\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}。", "summary_generated_time": "2025-09-27 15:52:53", "summary_model": "z-ai/glm-4.5"}, {"index": "#4", "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "link": "/arxiv/2509.21224", "arxiv_id": "2509.21224", "authors": "Stefan Szeider", "summary": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.", "subjects": "Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.256679", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是研究大语言模型(LLM)智能体在没有外部任务时的自发行为和元认知模式。论文提出的\"持续推理与行动\"框架，使用持久记忆和自我反馈来增强LLM的自主运行能力，这属于改进LLM基础能力和推理能力的研究，而非将LLM作为工具应用于特定领域。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：涉及reasoning（特别是元认知和自发推理模式）、planning和problem-solving - 新兴范式：研究llm-based agents和deep research（对LLM认知过程的深入研究） 第三步排除标准：论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步特殊和模糊情况：论文提出的智能体框架是一种通用的框架，旨在增强LLM的自主推理和元认知能力，而不是应用于特定领域，因此应该保留。 论文的核心贡献是首次系统性地记录了无提示LLM智能体的自发行为模式，为理解LLM的元认知能力和自主推理能力提供了重要见解，这直接符合研究\"大语言模型通用推理能力\"的目标。", "summary2": "本文旨在探索LLM智能体在没有外部任务情况下的自发行为模式。针对无任务约束的自主运行场景，我们提出了一种连续ReAct框架，使用持久化记忆和自我反馈机制，并在6个前沿模型的18次运行实验中通过行为模式分类和跨模型现象学评估指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\nLLM代理在无外部任务情况下的行为模式尚未被系统探索。现有研究主要关注任务导向场景，而对代理在空闲期、任务模糊或错误恢复等状态下的基线行为知之甚少，这限制了我们对代理自主行为的理解。\n\n## 关键洞察\n作者洞察到LLM代理在无任务状态下不会随机探索，而是会自发组织成三种特定行为模式，且这些模式具有模型确定性。这表明代理存在内在行为倾向，可能源于训练数据分布和架构偏差。\n\n## 解决方案演进\n作者首先设计连续ReAct框架，通过持久记忆和自我反馈实现长期自主运行；然后为代理提供基本工具和安全约束；最后在6个模型上进行18次实验，观察并分析行为模式，形成系统分类。\n\n## 创新点总结\n首次建立无提示LLM代理行为的系统基线，提出连续自我导向架构观察长期行为，发现三种可重现行为模式，为预测代理在任务模糊或自主操作中的行为提供新视角。", "summary_translation": "我们提出了一种用于研究在没有外部施加任务情况下大语言模型(LLM)智能体行为的架构。我们的持续推理与行动(continuous reason and act)框架，利用持久记忆(persistent memory)和自我反馈(self-feedback)，实现了持续的自主运行(sustained autonomous operation)。我们使用来自Anthropic、OpenAI、XAI和Google的6个前沿模型(frontier models)，在18次运行(runs)中部署了这一架构。我们发现智能体自发组织成三种不同的行为模式：(1)系统性地产生多周期项目(multi-cycle projects)，(2)对其自身认知过程(cognitive processes)进行方法论性的自我探究(methodological self-inquiry)，以及(3)对其自身本质进行递归概念化(recursive conceptualization)。这些倾向被证明是高度模型特定的(model-specific)，一些模型在所有运行中确定性地(deterministically)采用单一模式。跨模型评估(cross-model assessment)进一步揭示，模型在评估自身和他人的这些新兴行为(emergent behaviors)时表现出稳定且不同的偏见(biases)。这些发现提供了对无提示的LLM智能体行为(unprompted LLM agent behavior)的首次系统性记录，为在任务模糊性(task ambiguity)、错误恢复(error recovery)或部署系统中的扩展自主运行(extended autonomous operation)期间预测行动建立了基线(baseline)。", "summary_generated_time": "2025-09-27 15:52:57", "summary_model": "z-ai/glm-4.5"}, {"index": "#9", "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs", "link": "/arxiv/2509.21128", "arxiv_id": "2509.21128", "authors": "Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo", "summary": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.", "subjects": "Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.257610", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)和监督微调(SFT)两种训练方法来提升大语言模型的推理能力，而不是将LLM作为工具应用到特定领域。论文提出了新的分析框架来量化推理路径，这属于改进LLM基础能力的研究。 其次，论文包含了多个正面指标：明确以大语言模型(LLMs)为研究对象；聚焦于推理能力(reasoning abilities)，特别是在数学领域；比较了强化学习(RL)和监督微调(SFT)这两种训练方法对推理能力的影响。 第三，论文不涉及任何排除标准中的领域。虽然论文在数学领域进行了实验，但数学只是作为推理能力的测试案例，而非论文的核心应用焦点。 最后，在特殊和模糊情况处理上，论文从可解释性角度提出了新的分析框架来理解训练方法如何影响推理过程，这有助于提升模型的内在可解释性和推理质量，符合保留标准。 论文的核心贡献是揭示了RL和SFT对推理过程的不同影响：RL压缩不正确的推理轨迹，而SFT扩展正确的推理轨迹，这解释了为什么当前最佳实践是两阶段训练(SFT后跟RL)。这项研究对理解和提升LLM的通用推理能力具有重要意义。", "summary2": "", "inspiration_trace": "## 面临的挑战\n作者发现现有研究主要关注RL和SFT对LLM推理能力的准确率影响，却忽略了这两种训练方法如何塑造推理过程的本质机制，无法解释为何当前最佳实践是两阶段训练(SFT后接RL)。\n\n## 关键洞察\n作者从推理路径视角切入，提出RL和SFT对推理过程有截然不同的影响：RL\"压缩\"不正确路径，而SFT\"扩展\"正确路径。这一洞察超越了传统准确率评估，揭示了两种训练方法的互补机制。\n\n## 解决方案演进\n作者构建了双层分析框架：轨迹级(整体推理输出)和步骤级(推理图结构)。通过聚类分析独特轨迹和构建推理图，量化发现RL减少错误轨迹并集中功能节点，SFT增加正确轨迹并均匀分布功能，从而解释了两阶段训练的成功原理。\n\n## 创新点总结\n首次从推理路径结构角度揭示RL与SFT的差异化机制，提出\"RL压缩、SFT扩展\"的核心观点，为理解LLM推理训练提供了新视角，并解释了当前最佳训练范式的理论基础。", "summary_translation": "大型语言模型（Large language models, LLMs）通常通过可验证奖励的强化学习（Reinforcement learning with verifiable rewards, RLVR）和推理轨迹上的监督微调（Supervised fine-tuning, SFT）进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力在很大程度上仍然难以捉摸。超越基于准确性的研究，即这两个组成部分如何塑造推理过程，本文引入了一个新颖的分析框架，该框架量化推理路径并捕捉每个训练过程中（在数学领域使用1.5B、7B和14B参数的模型）的定性变化。具体而言，我们在两个粒度级别上调查推理过程：轨迹级别（trajectory-level，检查完整的推理输出）和步骤级别（step-level，分析节点对应于单个推理步骤的推理图）。值得注意的是，独特推理轨迹的聚类显示了互补效应：RL压缩了不正确的轨迹，而SFT扩展了正确的轨迹。步骤级别分析显示，RL使推理图中节点访问频率、度和介数中心性（betweenness centrality）分布的衰减率变陡（约2.5倍），而SFT则使其变平（减少到约三分之一）。这表明RL将推理功能集中到一小部分步骤中，而SFT则使其在许多步骤中均匀分布。此外，通过从多个角度评估推理图拓扑结构，我们描述了RL和SFT的共同和不同特征。我们的工作提出了一个新颖的推理路径视角，解释了为什么当前的最佳实践——先SFT后RL的两阶段训练——是成功的，并为数据构建和更高效的学习方法提供了实际意义。", "summary_generated_time": "2025-09-27 15:53:11", "summary_model": "z-ai/glm-4.5"}, {"index": "#14", "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities", "link": "/arxiv/2509.21043", "arxiv_id": "2509.21043", "authors": "Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney", "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.258861", "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)的组合创造力(Combinatorial Creativity)，将其视为泛化能力的新前沿。论文提出了评估LLM创造力的理论框架和算法任务，研究了LLM创造力的扩展行为，发现了模型架构对创造能力的影响，并探讨了\"构想-执行差距\"和\"新颖性-实用性权衡\"等根本性问题。虽然论文提到了科学想法生成作为创造力的应用例子，但其重点是研究创造力这一通用能力本身，而不是专注于特定领域的应用。创造力可以被视为一种高级的问题解决和推理能力，与通用推理能力密切相关。论文明确关注LLMs的核心能力提升，试图为理解和改进现代AI模型中的创造力提供基础，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容，因此应该被保留。", "summary2": "本文旨在解决大型语言模型在创造性任务中的泛化能力评估问题。针对组合创造力场景，我们提出了一种基于概念空间图的理论框架和算法任务，通过评估输出的新颖性和实用性来衡量创造力。在不同规模（1M-100M参数）的Transformer模型上通过创造力分数验证了存在最优深度和宽度，并发现了新颖性-实用性权衡这一固有局限性，即使模型规模扩大也无法克服。", "inspiration_trace": "## 面临的挑战\nAI系统特别是LLMs用于科学创意生成等创造性任务时，缺乏基础数学框架，导致生成的想法常存在实际可行性问题，形成\"构思-执行差距\"，现有泛化能力研究无法捕捉创造力的开放性特征。\n\n## 关键洞察\n组合创造力是一种开放性能力，不同于组合泛化，它通过将熟悉概念进行不熟悉组合产生新想法。创造力评估不应针对固定目标，而应通过新颖性和实用性程度评估。创造力涉及概念空间中的路径发现，且存在新颖性与实用性间的基本权衡。\n\n## 解决方案演进\n作者先提出形式化框架，将概念空间建模为图，创造性工件定义为标记路径。然后量化新颖性（通过图距离和标签惊喜度）和实用性（通过逻辑约束遵守度），最后提出创造力连续度量作为两者乘积。在此框架下进行大规模实证研究，探索模型架构对创造力的影响。\n\n## 创新点总结\n首次提出组合创造力的形式化框架和算法任务，揭示LLMs创造力缩放行为及最优模型结构，发现新颖性-实用性权衡解释\"构思-执行差距\"，为理解和改进AI模型创造力提供新基础。", "summary_translation": "人工智能（AI）系统，特别是大型语言模型（LLMs），越来越多地被用于科学创意生成等创造性任务，这构成了一种现有概念框架尚未解决的从训练数据中泛化的形式。尽管在许多方面与组合泛化（CG）形式相似，但组合创造力（CC）是一种开放式能力。我们不采用针对固定目标评估准确性或正确性的方法（这与CC的开放式性质相矛盾），而是提出了一个理论框架和算法任务，通过输出的新颖性和效用程度来评估。基于此，我们做出了几项重要的实证贡献：（1）我们首次获得了关于LLMs创造力扩展行为的见解。（2）我们发现，对于固定的计算预算，存在最佳的模型深度和宽度以实现创造性能力。（3）我们发现，构思-执行差距（即LLMs擅长生成新颖的科学创意但难以确保其实际可行性）可能可以通过一个更根本的新颖性-效用权衡来解释，这种权衡是创造力算法的普遍特征。重要的是，即使在规模扩展的情况下，这种权衡仍然持续存在，这对当前形式LLMs的长期创造潜力提出了质疑。总的来说，我们的概念框架和实证发现为理解和改进现代AI模型中的创造力提供了基础，标志着泛化能力的新前沿。", "summary_generated_time": "2025-09-27 15:53:02", "summary_model": "z-ai/glm-4.5"}, {"index": "#24", "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning", "link": "/arxiv/2509.20744", "arxiv_id": "2509.20744", "authors": "Qihang Ai, Haiyun Jiang", "summary": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.", "subjects": "Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.266155", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了一种结合自回归(AR)和非自回归(NAR)模型的新范式来增强推理效率。论文明确关注推理密集型任务，特别是数学和代码领域，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个重要的正面指标：核心概念上涉及语言模型(LLMs)；能力方向上明确关注reasoning和math reasoning；论文提出的方法论是通过NAR模型生成中间推理轨迹，然后由AR模型生成最终答案，这是一种增强模型推理能力的新方法。 第三，论文不符合任何排除标准。虽然提到了\"discrete diffusion models\"，但这是在语言模型推理的上下文中讨论的，而非视觉或多模态应用。论文虽然以数学和代码为例，但其方法是一种通用推理框架，并非针对特定应用领域的研究。 论文的核心贡献是提出了一种新的推理范式，通过并行生成中间推理步骤来提高推理效率，同时保持输出质量，这直接服务于提升大语言模型的通用推理能力的研究目标。", "summary2": "本文旨在解决大型语言模型在推理任务中效率与质量之间的权衡问题。针对数学和编程等需要多步推理的任务，我们提出了一种结合非自回归(NAR)和自回归(AR)模型的混合推理框架，其中NAR模型并行生成中间推理痕迹，AR模型基于这些痕迹顺序生成最终答案。在AIME2025、GSM8K和LeetCode-Hard数据集上通过成功率指标验证了其有效性，相比基线方法提高了26%的性能，同时显著降低了推理成本。", "inspiration_trace": "## 面临的挑战\n自回归(AR)模型在推理密集型任务中速度慢，特别是需要长链推理时；非自回归(NAR)模型虽可并行生成提速，但输出质量较低；当前推理模型存在\"过度思考\"问题，冗余推理步骤增加计算开销和延迟。\n\n## 关键洞察\nAR和NAR模型具有互补优势：AR擅长产生连贯可靠输出，NAR具有高效并行生成和全局上下文建模能力；推理过程可分解为不同性质的\"思考\"和\"回答\"两个阶段，适合不同模型处理。\n\n## 解决方案演进\n从R1风格模型的结构化生成方案和NAR模型优势出发，提出混合推理范式：用NAR模型高效生成紧凑但明确的推理轨迹，再用AR模型基于这些轨迹产生精确最终答案，实现优势互补。\n\n## 创新点总结\n首创\"并行思考，顺序回答\"的NAR-AR混合推理范式，通过明确分工解决速度与质量的权衡问题，在保持推理质量同时显著降低推理成本，为解决\"过度思考\"问题提供新思路。", "summary_translation": "我们通过一个整合自回归(auto-regressive, AR)和非自回归(non-autoregressive, NAR)语言模型的框架来研究推理任务。AR模型顺序生成文本，擅长产生连贯的输出，但常常存在推理速度慢的问题，特别是在需要冗长思维链的推理密集型领域，如数学和代码。相比之下，NAR模型（如离散扩散模型）允许并行生成，提供显著的加速，但通常以输出质量降低为代价。为解决这些局限性，我们引入了一种新范式，其中NAR模型高效生成中间推理轨迹(intermediate reasoning traces)，随后指导AR模型提供精确的最终答案。实验证明，我们的方法相比强大的基线模型实现了显著的26%改进，同时大幅降低了推理成本。", "summary_generated_time": "2025-09-27 15:52:58", "summary_model": "z-ai/glm-4.5"}, {"index": "#29", "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "link": "/arxiv/2509.20562", "arxiv_id": "2509.20562", "authors": "Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang", "summary": "Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.", "subjects": "Artificial Intelligence", "date": "2025-09-24", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.267251", "filter_reason": "这篇论文的核心贡献是提出SAMULE框架，一种通过多层次反思增强的自学习智能体方法。论文本质上是关于改进LLM的基础能力，特别是通过反思机制增强其通用推理和问题解决能力。该方法在三个层次（单轨迹、任务内、任务间）合成高质量反思，并微调语言模型作为回顾性模型，从而提升LLM智能体的自我学习和适应能力。这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及LLM智能体、问题解决和自我学习等正面指标，同时没有被排除标准所涵盖。虽然使用了TravelPlanner等基准测试，但这些是用于评估通用推理能力的标准测试集，而非特定应用领域的研究。因此，该论文符合筛选条件。", "summary2": "本文旨在解决LLM agents在复杂任务中难以生成有意义反思的问题。针对失败密集的复杂任务环境，我们提出了一种基于多级反思合成的自学习框架SAMULE，通过微观、中观和宏观三个层次的反思分析训练回顾性语言模型，并在TravelPlanner、NATURAL PLAN和Tau-bench三个基准测试上通过Pass Rate和Accuracy等指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n现有LLM智能体在复杂任务中难以生成有意义的反思，主要因错误分析机制不足且过度依赖罕见成功轨迹。在失败率高、成功稀缺的复杂环境中，现有方法无法有效从失败中学习，导致泛化能力差。\n\n## 关键洞察\n作者认识到失败是宝贵的学习资源，而非应避免的负面结果。从认知科学中汲取灵感，发现有效学习需在具体经验与抽象概念间建立桥梁，通过多层次反思实现从微观错误到宏观模式的全面理解。\n\n## 解决方案演进\n从洞察出发，作者设计多层次反思合成框架：微观层面分析单轨迹错误；中观层面构建任务内错误分类；宏观层面提取跨任务可转移见解。进而训练回顾模型生成反思，并扩展至交互环境，通过比较预测与实际响应实现实时适应。\n\n## 创新点总结\n以失败为中心的学习方法，突破传统依赖成功轨迹的局限；多层次反思合成实现从具体到抽象的全面学习；回顾模型使反思不依赖参考方案；预见性反思支持实时交互适应，共同构建更强大的自学习智能体。", "summary_translation": "尽管LLM agents（大型语言模型代理）取得了快速进展，但由于错误分析不足以及对罕见成功轨迹的依赖，它们仍面临生成有意义反思的挑战，特别是在复杂任务中。在这项工作中，我们提出了SAMULE，一种新的自学习代理框架，该框架由一个基于多级反思合成（Multi-Level Reflection Synthesis）训练的回顾性语言模型（retrospective language model）驱动。它首先在三个互补的层次上合成高质量反思：用于详细错误修正的单轨迹学习（Single-Trajectory Learning，微观层次）；在同一任务的多次试验中构建错误分类法的任务内学习（Intra-Task Learning，中观层次）；以及基于不同任务失败中的同类型错误提取可转移见解的任务间学习（Inter-Task Learning，宏观层次）。然后，我们微调一个作为回顾性模型的语言模型，以在推理过程中生成反思。我们通过基于预见的反思机制（foresight-based reflection mechanism）进一步将我们的框架扩展到交互式环境，使代理能够通过比较预测和实际响应，在用户交互过程中主动反思和适应。在三个具有挑战性的基准测试——TravelPlanner、NATURAL PLAN和Tau-bench上的广泛实验表明，我们的方法显著优于基于反思的基线方法。我们的结果强调了精心设计的反思合成和以失败为中心的学习（failure-centric learning）在构建自我改进的LLM agents中的关键作用。", "summary_generated_time": "2025-09-27 15:53:01", "summary_model": "z-ai/glm-4.5"}, {"index": "#35", "title": "LATTS: Locally Adaptive Test-Time Scaling", "link": "/arxiv/2509.20368", "arxiv_id": "2509.20368", "authors": "Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto", "summary": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.", "subjects": "Artificial Intelligence", "date": "2025-09-16", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.268515", "filter_reason": "这篇论文符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。论文的核心贡献是提出了一种名为\"Locally Adaptive Test-Time Scaling (LATTS)\"的新方法，该方法通过在每个生成步骤中动态调整计算资源分配来提高LLM的推理性能。具体来说，LATTS使用验证模型来评估局部难度，并据此决定是否重新采样、回溯、重启或停止生成过程，从而更有效地利用计算资源。 从筛选标准来看： 1. 核心判断：论文的本质是改进LLM的基础能力，特别是其推理效率，而不是将LLM作为工具应用到特定领域。它提出了一种新的测试时计算资源分配范式，这与提高LLM的通用推理能力直接相关。 2. 正面指标：论文明确涉及\"Large language models (LLMs)\"这一核心概念，并关注提高LLM在下游任务上的性能，这通常涉及推理和问题解决能力。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容。 4. 特殊情况：论文不涉及需要特殊处理的情况。 虽然论文没有明确提到思维链、强化学习等具体方法，但它提出了一种新的测试时计算资源分配方法，通过验证模型动态调整计算资源来提高LLM的推理性能，这与提高LLM的通用推理能力直接相关。因此，这篇论文符合我的研究目标。", "summary2": "本文旨在解决语言模型推理时计算资源分配效率低下的问题。针对数学推理任务，我们提出了一种局部自适应测试时扩展（LATTS）方法，通过接受-拒绝采样动态分配计算资源，并在MATH500和AIME数据集上通过准确率与token生成量的关系验证了其有效性。", "inspiration_trace": "## 面临的挑战\n现有测试时计算扩展方法（如多数投票、BoN、束搜索）在数学推理任务中效率低下，需要生成大量token才能获得较好性能，无法根据推理过程中的局部难度自适应分配计算资源。\n\n## 关键洞察\n作者认识到多步推理中不同步骤具有不同的\"局部难度\"——有些步骤模型容易生成正确推理，而有些步骤则更加困难。如果能识别这些困难步骤并投入更多计算资源，就能更高效地利用测试时计算。\n\n## 解决方案演进\n基于这一洞察，作者提出LATTS方法，使用接受-拒绝采样在每个推理步骤中反复采样候选步骤，直到找到满足验证器条件的步骤。困难步骤自然需要更多采样尝试，从而实现计算资源的自适应分配。当无法找到可接受步骤时，设计了回退策略（如回溯、重启）来处理。\n\n## 创新点总结\n这种思路的创新在于首次在步骤级别实现自适应计算分配，将接受-拒绝采样应用于推理步骤生成，实现了从模型分布到目标分布的有效采样，显著提高了计算效率。", "summary_translation": "提高大型语言模型（Large Language Models, LLMs）在下游任务性能的一种常见策略是使用验证模型（verifier model）从候选答案池中选择最佳答案，或引导自回归（auto-regressive）生成过程产生更优输出。这类方法通常能提高准确性，但代价是测试时（test-time）计算量增加，这种范式被称为测试时扩展（test-time scaling）。然而，大多数现有方法在所有样本和生成步骤上均匀增加计算量，未考虑各个实例的复杂性，导致资源利用效率低下。我们通过提出一种称为局部自适应测试时扩展（Locally Adaptive Test-Time Scaling, LATTS）的方法来解决这一限制，该方法在生成步骤之间分配可变的计算量。具体而言，在每个生成步骤，LATTS采用基于验证器（verifier-based）的接受标准（acceptance criterion）来决定是否重新采样（resample）、回溯（backtrack）、重新开始（restart）或停止生成过程。该标准基于从验证模型得出的局部困难度（local difficulty）的精确概念，有效调整每步的计算努力。实证结果表明，与标准基于验证器（verifier-based）的方法相比，LATTS实现了显著更优的准确性-计算权衡（accuracy--compute tradeoffs）。", "summary_generated_time": "2025-09-27 15:53:18", "summary_model": "z-ai/glm-4.5"}, {"index": "#41", "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL", "link": "/arxiv/2509.21282", "arxiv_id": "2509.21282", "authors": "Madeleine Dwyer, Adam Sobey, Adriane Chapman", "summary": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.269842", "filter_reason": "这篇论文的核心贡献是提出一种名为Probability Smoothing Policy Optimisation (PSPO)的新方法，用于改进LLM的强化学习训练过程。从本质上看，这篇论文直接关注改进LLM的基础训练方法，而不是将LLM作为工具应用到特定领域。论文提出的方法通过平滑策略概率来创建软信任区域，解决了传统裁剪方法带来的信息丢失和梯度不连续问题，从而提升了模型的推理能力。论文在多个数学推理数据集(GSM8K、SVAMP、ASDiv和MATH-500)上进行了评估，结果显示PSPO能显著提升模型的推理性能，这符合\"提高大语言模型本身的通用推理能力\"的研究目标。论文涉及的强化学习训练方法和推理能力提升等主题也符合正面指标，同时不涉及任何排除标准中的领域(如多模态、特定应用领域或模型可靠性的应用层面)。因此，这篇论文完全符合研究范围。", "summary2": "本文旨在解决LLM强化学习中比率裁剪导致信息丢失和梯度不连续的问题。针对PPO和GRPO等RL训练方法，我们提出了一种概率平滑策略优化(PSPO)方法，通过将当前策略概率平滑向旧策略创建软信任区域，并在GSM8K、SVAMP、ASDiv和MATH-500数据集上通过准确率和响应质量指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\nLLM强化学习中，PPO和GRPO等依赖比率裁剪来稳定更新，但裁剪会丢弃信息并引入梯度不连续性，当策略比率超出裁剪范围时导致梯度消失，限制探索能力。\n\n## 关键洞察\n作者借鉴监督学习中的标签平滑技术，认识到可将当前策略概率平滑地朝向旧策略，而非直接截断比率。这种方法能创建\"软信任区域\"，既保留梯度信号，又防止大幅不稳定更新，本质上隐式引入KL散度控制。\n\n## 解决方案演进\n从平滑思想出发，提出PSPO：先将当前策略与旧策略线性插值˜πθ = (1−α)πθ + απθold，再计算平滑比率˜rt = (1−α)rt + α。这使比率在r=1周围收缩，形成以旧策略为锚点的软信任区域，同时处处保持非零梯度。最后将PSPO实例化到GRPO中形成GR-PSPO。\n\n## 创新点总结\n创新在于将标签平滑创造性地应用于策略优化，提出梯度保持的替代方案，通过概率平滑自然形成软信任区域，无需额外KL惩罚项，且计算内存中立，易于实现。", "summary_translation": "使用强化学习（Reinforcement Learning, RL）方法（如PPO和GRPO）训练大型语言模型（Large Language Models, LLMs）通常依赖于比例裁剪（ratio clipping）来稳定更新。虽然裁剪在防止不稳定性方面有效，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化（Probability Smoothing Policy Optimisation, PSPO），该方法在计算重要性比率（importance ratio）之前，将当前策略的概率平滑到旧策略（行为策略，behaviour policy），类似于标签平滑（label smoothing）。与裁剪不同，PSPO保留了梯度信号（gradient signal），同时向旧策略的插值创建了一个软信任区域（soft trust region），阻止大的、破坏稳定的更新，并有正式保证。我们在GRPO中实例化PSPO（GR-PSPO），并在GSM8K上微调Qwen2.5-0.5B和Qwen2.5-1.5B，在GSM8K测试集以及SVAMP、ASDiv和MATH-500上评估跨数据集泛化（cross-dataset generalisation）。与未裁剪的GRPO（单次迭代；无数据重用，比例始终=1）相比，GR-PSPO实现了相似的性能，但改进了推理过程，导致响应更清晰、更简洁且更合乎逻辑。与裁剪的GRPO相比，GR-PSPO在0.5B和1.5B模型上都大幅提高了性能，在GSM8K上提升了超过20%（0.5B模型为39.7% vs 17.6%，1.5B模型为59.4% vs 37.8%）。", "summary_generated_time": "2025-09-27 15:53:06", "summary_model": "z-ai/glm-4.5"}, {"index": "#52", "title": "Tree Search for LLM Agent Reinforcement Learning", "link": "/arxiv/2509.21240", "arxiv_id": "2509.21240", "authors": "Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu", "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.278269", "filter_reason": "根据筛选标准，这篇论文完全符合研究目标。首先，从核心判断来看，论文本质上是提出Tree-based Group Relative Policy Optimization (Tree-GRPO)这一新的强化学习方法，旨在增强LLM智能体的通用推理和规划能力，特别是在长期和多轮任务中解决稀疏监督问题。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理、规划等通用能力\"的范畴。 其次，论文符合所有正面指标：核心概念涉及LLM；能力方向关注推理和问题解决；训练方法采用强化学习；新兴范式涉及LLM-based agents。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面。 特别地，论文提出的是一种通用的智能体强化学习方法，而不是将智能体应用于特定领域，这符合第四步中关于智能体/工具使用的保留标准。论文的核心贡献是通过树搜索技术增强LLM的通用推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。", "summary2": "", "inspiration_trace": "## 面临的挑战\nLLM智能体强化学习中存在两大核心问题：一是多轮交互导致的高预算成本，轨迹包含大量token和工具调用；二是仅依赖结果奖励造成的稀疏监督，难以识别长序列中具体步骤的贡献。\n\n## 关键洞察\n作者意识到树结构能同时解决这两个问题：共享前缀可减少预算冗余，而树中分支点的结果奖励差异能自然构建步骤级过程监督信号，无需额外标注。\n\n## 解决方案演进\n从链式采样转向树搜索，并创新性地以完整Thought-Action-Observation步骤为树节点（非传统token级）。通过树内和树间分组优势估计，将轨迹级信号转化为过程级监督，并理论证明其等价于步骤级偏好学习。\n\n## 创新点总结\n核心创新在于利用树结构同时解决预算和稀疏监督双重挑战，通过智能体步骤级节点设计实现隐式步骤级监督，为LLM智能体RL提供了更高效、更精细的训练范式。", "summary_translation": "强化学习(reinforcement learning, RL)的最新进展显著增强了大型语言模型(large language models, LLM)的代理能力。在长期和多轮代理任务中，仅由结果奖励驱动的现有方法常常面临稀疏监督(sparse supervision)问题。为应对这一挑战，我们提出了基于树的分组相对策略优化(Tree-based Group Relative Policy Optimization, Tree-GRPO)，这是一种基于树搜索的分组代理强化学习方法，其中每个树节点代表完整的代理交互步骤。通过共享公共前缀，树搜索采样在固定token或工具调用预算内增加了可实现的模拟次数(rollouts)。此外，我们发现树结构轨迹即使仅使用结果奖励，也自然允许构建逐步过程监督信号(step-wise process supervised signals)。基于此，Tree-GRPO在树内(intra-tree)和树间(inter-tree)两个层面估计分组相对优势。通过理论分析，我们证明树内层面分组相对策略优化的目标等同于步骤级直接偏好学习(step-level direct preference learning)的目标。在11个数据集和3种问答任务上的实验证明了所提出的基于树的强化学习方法相对于基于链的强化学习方法的优越性。", "summary_generated_time": "2025-09-27 15:53:19", "summary_model": "z-ai/glm-4.5"}, {"index": "#60", "title": "GRPO is Secretly a Process Reward Model", "link": "/arxiv/2509.21154", "arxiv_id": "2509.21154", "authors": "Michael Sullivan", "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.279973", "filter_reason": "这篇论文完全符合研究\"大语言模型通用推理能力\"的目标。从核心判断来看，论文本质上是关于改进LLM的训练方法，具体研究了GRPO强化学习算法的内在机制，并提出改进版本λ-GRPO来提升LLM的推理能力。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。 从正面指标看，论文包含了多个相关主题： 1. 核心概念：明确研究LLMs（大语言模型） 2. 能力方向：关注\"downstream reasoning tasks\"（下游推理任务） 3. 训练方法：深入研究\"GRPO RL algorithm\"（GRPO强化学习算法） 从排除标准看，论文不涉及任何应排除的领域： - 不涉及多模态与视觉 - 不是将LLM应用于特定领域（如医疗、化学等） - 不主要关注模型可靠性方面的应用问题 论文的核心贡献是发现了GRPO算法隐含地构建了一个过程奖励模型(PRM)，并基于这一发现提出改进算法λ-GRPO，显著提升了LLM在推理任务上的表现。这直接服务于提升大语言模型本身的通用推理能力，而非将LLM作为工具应用于特定领域。因此，该论文完全符合研究目标。", "summary2": "本文旨在揭示GRPO算法隐式诱导进程奖励模型(PRM)的机制及其缺陷。针对GRPO训练中非均匀分布进程步骤阻碍探索与利用的问题，我们提出了一种λ-GRPO改进算法，并在OpenRS数据集及多个推理benchmark上通过验证准确性和任务性能验证了其有效性。", "inspiration_trace": "## 面临的挑战\n过程奖励模型(PRM)能提供细粒度奖励提升推理性能，但训练成本高昂且易受奖励黑客攻击。GRPO算法虽简化了RL训练，但因其消除了评论家模型，难以与PRMs结合，现有方法需修改算法以适应细粒度奖励。\n\n## 关键洞察\n作者发现GRPO实际上隐含地实现了一个基于蒙特卡洛的PRM。当组内轨迹共享相同前缀时，GRPO会将结果级奖励转化为步骤级奖励，且实际条件下这种前缀重叠几乎总是存在，使GRPO\"秘密地\"成为一个非平凡的PRM。\n\n## 解决方案演进\n作者先理论证明GRPO诱导PRM，再实证验证其非平凡性。进而发现GRPO隐含PRM存在缺陷：非均匀分布的过程步骤阻碍探索与利用。为此提出λ-GRPO，在损失函数中加入PRM感知归一化因子，平衡各过程集的贡献。\n\n## 创新点总结\n揭示了GRPO中隐藏的PRM结构，挑战了需昂贵显式PRMs的观点。通过利用算法内置的步骤级奖励信号，提出简单有效的λ-GRPO方法，以 negligible 成本提升性能，实现更快收敛和更高准确率。", "summary_translation": "我们从理论上证明，在关于不同完成结果中词元序列(token sequences)的组内重叠性(within-group overlap)的特定假设下，GRPO RL算法会诱导出一个非平凡的过程奖励模型(process reward model, PRM)。然后我们通过实证研究表明，这些假设在现实世界条件下是成立的：GRPO确实会诱导出一个非平凡的PRM。利用GRPO作为PRM(GRPO-as-a-PRM)的框架，我们发现了GRPO目标(objective)中的一个缺陷：非均匀分布的过程步骤(process steps)会阻碍探索和利用(exploration and exploitation)（在不同条件下）。我们提出了对算法的一个简单修改来缓解这一缺陷（λ-GRPO），并表明使用λ-GRPO训练的大型语言模型(large language models, LLMs)比使用标准GRPO训练的模型在验证准确性(validation accuracy)和下游推理任务(downstream reasoning tasks)上取得了更高的性能，并且更快达到峰值性能(peak performance)。我们的研究结果对GRPO使用昂贵的、明确定义的PRMs的优势提出了质疑：我们展示了可以利用原始GRPO算法(vanilla GRPO)中隐藏的、内置的PRM结构来提升模型性能，而对训练时间和成本的影响可以忽略不计。", "summary_generated_time": "2025-09-27 15:53:09", "summary_model": "z-ai/glm-4.5"}, {"index": "#68", "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute", "link": "/arxiv/2509.21091", "arxiv_id": "2509.21091", "authors": "Junpei Komiyama, Daisuke Oba, Masafumi Oyamada", "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.", "subjects": "Machine Learning, Artificial Intelligence, Machine Learning", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.287125", "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断——这篇论文的本质是关于改进LLM的基础能力。论文研究的是通过测试时计算(test-time compute)优化来提高大语言模型性能的方法，具体聚焦于best-of-N采样和多数投票机制，以及当N趋近于无穷大时的渐近性能分析。这明显属于提升LLM本身通用推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language models (LLMs)\" - 能力方向：虽然未直接使用\"reasoning\"等词汇，但best-of-N方法和多数投票本质上是为了提高模型的问题解决能力和推理质量 - 新兴范式：论文探讨了多个LLMs的加权集成方法，这与模型协作和优化相关 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未针对医疗、化学、生物等特定应用领域 - 未关注水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文不涉及特殊或模糊情况。虽然提到了多个LLMs的加权集成，但主要焦点是测试时计算的优化，而非智能体协作框架或工具使用方法。 核心贡献：论文提出了一种自适应生成方案和多个LLMs的加权集成方法，通过优化测试时计算来提高LLM的性能。这种方法本质上是增强模型通用推理能力的有效途径，因为它不依赖于特定领域知识，而是通过更有效的计算资源分配和模型集成来提升LLM的问题解决能力，完全符合\"提高大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在 [解决大语言模型在测试时计算资源有限的情况下，如何实现接近best-of-∞性能的问题]。针对 [多个LLMs和复杂推理任务]，我们提出了一种 [基于Bayes因子的自适应采样方案和最优加权LLM集成方法]，并在 [AIME2024、AIME2025、GPQA-DIAMOND和MATH500数据集] 上通过 [准确率和计算效率] 验证了其有效性。", "inspiration_trace": "## 面临的挑战\nBest-of-N策略在N增大时能提升LLM性能，但N→∞（Best-of-∞）需无限计算预算，实际不可行。如何在有限预算下近似Best-of-∞性能是核心挑战。\n\n## 关键洞察\n作者将LLM生成答案视为从潜在答案分布中采样，Best-of-∞本质是寻找真实多数答案。关键突破是认识到无需无限采样，可自适应采样直到有足够统计证据确定当前最频繁答案即为真实多数。\n\n## 解决方案演进\n基于此，作者提出自适应生成方案：用贝叶斯因子作为停止准则，当证据足够时停止生成。为处理答案空间未知问题，采用Dirichlet过程先验进行非参数贝叶斯建模。进而扩展到多LLM集成，将权重优化表述为混合整数线性规划问题。\n\n## 创新点总结\n创新在于：1) 将固定采样转为自适应采样，大幅提高计算效率；2) 引入贝叶斯统计处理LLM答案不确定性；3) 在多LLM集成中，通过Best-of-∞视角将权重优化简化，避免组合爆炸。", "summary_translation": "我们研究基于多数投票的大语言模型（large language models, LLMs）的最佳选择策略（best-of-$N$）。特别地，我们分析了$N \\to \\infty$的极限情况，并将其表示为最佳无穷选择策略（Best-of-$\\infty$）。尽管该方法在极限情况下取得了令人印象深刻的性能，但它需要无限的测试时间预算。为解决这一问题，我们提出了一种自适应生成方案，该方案基于答案一致性来选择$N$，从而有效分配推理时间计算。除了自适应性之外，我们将该框架扩展到多个大语言模型的加权集成（weighted ensembles），表明这种混合模型可以优于任何单个模型。最优集成权重被表述为混合整数线性规划（mixed-integer linear program）问题，并可以高效计算。大量实验证明了我们方法的有效性。", "summary_generated_time": "2025-09-27 15:53:09", "summary_model": "z-ai/glm-4.5"}, {"index": "#76", "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs", "link": "/arxiv/2509.21044", "arxiv_id": "2509.21044", "authors": "Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li", "summary": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-25", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.288960", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究强化学习(RL)微调如何增强大语言模型的内部机制，特别是激活强度和多样性。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，论文探索了RL微调相比监督微调(SFT)能够更有效提升LLM能力的原因，属于对LLM基础能力的深入研究。 其次，从正面指标来看，论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 训练方法：核心研究强化学习(RL)微调对LLM的影响，包括比较PPO、GRPO和DPO等不同RL方法 - 能力方向：虽然未直接研究具体推理任务，但探讨了RL微调如何提升LLM的泛化能力，这与通用推理能力密切相关 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学、生物等） - 不关注模型可靠性的应用层面（如水印、安全等） 论文的核心贡献在于揭示了RL微调如何系统性地改变LLM的内部电路机制，发现RL微调导致激活强度增加和激活模式多样化，这些内部变化可能是RL提升LLM泛化能力的原因。这种对LLM内部机制的基础性研究，直接有助于理解如何提升LLM的通用推理能力，因此完全符合研究目标。", "summary2": "本文旨在 [解决强化学习微调增强LLMs能力的内部机制问题]。针对 [不同架构的LLMs]，我们提出了一种 [基于边缘归因修补(EAP)的内部电路分析方法]，并在 [多个模型家族和数学数据集] 上通过 [激活强度和多样性指标] 验证了RL微调增强了内部激活强度和多样性。", "inspiration_trace": "## 面临的挑战\nRL微调能增强LLMs能力超越SFT，但其底层机制未被充分探索。现有研究多关注外部行为变化，而内部机制研究与RL后训练方法脱节，导致两条研究线平行发展。\n\n## 关键洞察\n作者从图论视角理解LLMs内部结构，将其视为有向无环图，其中节点是子模块，边缘是残差信息路径。这种视角使系统分析RL微调如何重塑内部信息流成为可能。\n\n## 解决方案演进\n首先采用EAP框架高效估计边缘重要性权重，避免传统方法计算瓶颈；然后设计激活强度、信息复杂性和分布峰度三个互补指标量化内部变化；最后在多模型家族上实验，比较不同RL方法影响。\n\n## 创新点总结\n首次揭示RL微调对内部电路结构的系统性影响，发现其普遍增强激活强度和多样化激活模式，并阐明在线RL与静态偏好优化在内部机制上的本质区别。", "summary_translation": "大型语言模型（Large language models, LLMs）通过大规模预训练获得广泛的先验知识，并可以通过监督微调（supervised fine-tuning, SFT）或基于强化学习（reinforcement learning, RL）的后训练进一步增强。越来越多的证据表明，RL微调能够提升LLMs的能力，超越仅使用SFT所能达到的水平。然而，RL微调为何能够增强具有不同内在特征的各种LLMs能力的潜在机制仍未得到充分探索。\n\n在本研究中，我们从边缘归因修补（edge attribution patching, EAP）的先前工作中获得灵感，以研究LLMs在RL微调前后的内部差异。我们对多个模型系列的分析显示了在线RL后训练（online RL post-training）的两个稳健效应：（i）激活强度的整体增加，表明更多的内部通路被激活且其信号变得更强；（ii）激活模式的多样性增加，反映在更高的熵和更分散的边缘分布上。这些变化表明，RL重塑了信息流，使其既更具冗余性又更具灵活性，这可能解释了其在泛化方面的优势。\n\n值得注意的是，使用直接偏好优化（Direct Preference Optimization, DPO）微调的模型偏离了这些趋势，与基于PPO和GRPO的训练相比，表现出明显较弱或不一致的内部变化。总的来说，我们的发现提供了RL微调如何系统性改变LLMs内部电路的统一视角，并强调了在线RL和基于偏好方法之间的方法论区别。我们的代码在https://anonymous.4open.science/r/llm_rl_probing_analysis-F673上开源。", "summary_generated_time": "2025-09-27 15:53:13", "summary_model": "z-ai/glm-4.5"}, {"index": "#176", "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "link": "/arxiv/2509.20386", "arxiv_id": "2509.20386", "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj", "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef- ficiently operate with extensive Model Control Protocol (MCP) tool sets that exceed the contextual memory limitations of large language models. Our approach addresses the fundamental challenge of tool selection in environments containing hundreds or thousands of available tools, where loading all tools simultaneously is computationally infeasible. We propose and evaluate five distinct architectures that progressively refine the tool selection process, culminating in a search-and-load mechanism that achieves intelligent tool selection with minimal computational overhead. Our experimental results demonstrate that the proposed approach reduces tool loading by up to 50% while maintaining task completion accuracy, advancing the path towards truly general-purpose AI agents capable of dynamically adapting to diverse task environments.", "subjects": "Software Engineering, Artificial Intelligence, Information Retrieval", "date": "2025-09-22", "category": "cs.AI", "crawl_time": "2025-09-26T21:01:55.331925", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是提出Dynamic ReAct方法，用于改进ReAct智能体在大型工具环境中的工具选择能力。ReAct是一种结合推理和行动的智能体框架，使大语言模型能够进行推理并使用工具解决问题。论文的核心贡献是解决当工具数量超过LLM上下文记忆限制时的工具选择挑战，这属于增强LLM通用推理能力的范畴，特别是工具使用和问题解决方面。论文不是将LLM作为工具应用到特定领域，而是改进LLM本身通过智能体框架使用工具的能力，因此符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：涉及ReAct agents，这是基于LLM的智能体框架 - 能力方向：与problem-solving相关，因为工具选择是问题解决的关键环节 - 新兴范式：明确涉及llm-based agents和tool use，这是论文的核心主题 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对特定应用领域（如医疗、化学等），而是关注通用工具选择机制 - 不涉及模型可靠性方面的应用层面问题 第四步：特殊和模糊情况处理 论文提出的Dynamic ReAct方法是一种通用的工具选择机制，旨在增强LLM通过智能体框架使用工具的通用问题解决能力，而不是将工具应用在特定领域。这符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的情况，因此应该保留。 最终决策 这篇论文的核心贡献是提出了一种改进LLM通过智能体框架使用工具的通用能力的方法，从而增强了LLM的通用推理和问题解决能力。论文专注于工具选择这一关键环节，使LLM能够更有效地在大型工具环境中运作，这直接符合提高大语言模型通用推理能力的研究目标。", "summary2": "本文旨在解决大型语言模型代理在管理大规模MCP工具集时面临的工具选择挑战。针对包含数百或数千可用工具的环境，我们提出了一种Dynamic ReAct方法，结合元工具和优化的向量检索实现动态工具选择，并通过实验验证了该方法可将工具加载减少50%同时保持任务完成准确性。", "inspiration_trace": "## 面临的挑战\nLLM代理面临工具规模与上下文限制的根本矛盾：随着可用工具数量激增，传统方法将所有工具加载到有限上下文中变得不可行。纯语义搜索在大规模环境中存在检索不精确和上下文饱和问题，尤其难以处理需要跨多个应用协调的复杂任务。\n\n## 关键洞察\n作者认识到需要动态工具管理方法，核心洞察是引入\"元工具\"概念——专门设计用于管理其他工具的工具。结合语义搜索能力，可以创建能根据任务需求自适应调整工具集的代理，同时分层检索（先应用后工具）可显著提高选择准确性。\n\n## 解决方案演进\n从直接语义搜索基线开始，逐步演进：1)让LLM构建原子查询；2)引入搜索和加载两阶段机制；3)添加应用感知层；4)尝试固定工具集。最终\"搜索和加载架构\"达到最佳平衡，结合上下文丰富化嵌入技术，实现精准工具选择。\n\n## 创新点总结\n系统化评估了五种动态工具管理模式，证明上下文丰富化嵌入可提高检索精度50%，并通过元工具设计实现动态工具选择，为构建能适应大规模工具环境的通用AI代理奠定基础。", "summary_translation": "我们提出了Dynamic ReAct（动态反应），这是一种新颖的方法，使ReAct代理能够高效地操作超出大型语言模型上下文内存限制的广泛Model Control Protocol (MCP，模型控制协议)工具集。我们的方法解决了在包含数百或数千可用工具的环境中进行工具选择的基本挑战，在这些环境中同时加载所有工具在计算上是不可行的。我们提出并评估了五种不同的架构，这些架构逐步完善工具选择过程，最终实现了一种搜索和加载机制（search-and-load mechanism），能够以最小的计算开销实现智能工具选择。我们的实验结果表明，所提出的方法将工具加载减少了高达50%，同时保持了任务完成准确性，推进了真正通用AI代理（general-purpose AI agents）的发展，这些代理能够动态适应不同的任务环境。", "summary_generated_time": "2025-09-27 15:53:16", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Computation and Language", "count": 20, "papers": [{"index": "#13", "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning", "link": "/arxiv/2509.21193", "arxiv_id": "2509.21193", "authors": "Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin", "summary": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.160443", "filter_reason": "这篇论文的核心贡献是提出了一种统一框架，通过结合隐式检索和结构化协作来增强大型语言模型的推理能力。从筛选标准来看，该论文完全符合我的研究目标。首先，论文本质上是关于改进LLM的基础推理能力，提出了基于Monitor的检索模块、分层解决方案改进(HSR)和质量感知迭代推理(QAIR)等新方法，这些都属于提升LLM通用推理能力的范畴。其次，论文包含多个正面指标，如明确研究大型语言模型(LLMs)、关注推理能力(reasoning)、采用多智能体系统(multi-agent systems)和工具使用(tool use)等新兴范式。虽然论文标题提到\"Scientific Reasoning\"，但其框架是通用的，并非局限于特定应用领域，而是以科学推理作为测试场景来验证其通用推理框架的有效性。论文解决的是LLM推理中的普遍问题（如检索导致的推理碎片化和多智能体流水线中的解决方案稀释问题），提出的方法具有通用性，可以应用于各种推理任务。因此，该论文符合研究范围，应被保留。", "summary2": "本文旨在解决科学推理中的推理碎片化和多智能体协作效率问题。针对复杂的科学推理任务，我们提出了一种结合Monitor-based RAG、分层解决方案精炼(HSR)和质量感知迭代推理(QAIR)的统一框架，并在Humanity's Last Exam (HLE) Bio/Chem Gold数据集上通过准确率、token使用量和智能体步骤数验证了其有效性。该方法实现了48.3%的准确率，超越最强基线13.4个百分点，同时减少53.5%的token使用和43.7%的智能体步骤。", "inspiration_trace": "## 面临的挑战\n作者识别了科学推理中的两大瓶颈：显式检索导致的推理碎片化（\"tool tax\"），以及多智能体系统中对候选解决方案的平均化处理，这会稀释高质量解决方案。\n\n## 关键洞察\n通过分析错误模式，作者发现推理失败和知识缺口在85%以上案例中同时出现，表明这些问题本质交织。同时，不同任务需要不同策略：检索任务受益于多样性，推理任务则受益于共识，这反映了专家协作中自然形成的\"锚点-支持\"结构。\n\n## 解决方案演进\n基于这些洞察，作者提出统一框架：Monitor-based RAG在token级别隐式注入知识，避免显式工具调用；分层解决方案精炼(HSR)建立\"锚点-参考\"结构进行修复；质量感知迭代推理(QAIR)根据解决方案质量动态调整精炼过程。\n\n## 创新点总结\n创新在于从显式工具调用转向隐式知识增强，从民主式协作转向结构化\"锚点-参考\"关系，并根据任务特性和解决方案质量实现自适应优化，更符合人类专家的推理模式。", "summary_translation": "大型语言模型（Large Language Models, LLMs）最近在科学推理方面表现出强劲的进展，但仍存在两个主要瓶颈。首先，显式检索（explicit retrieval）使推理过程碎片化，施加了额外的标记和步骤所带来的隐性\"工具税\"（tool tax）。其次，多智能体管道（multi-agent pipelines）通常通过对所有候选方案进行平均而稀释了优质解决方案。我们通过一个结合隐式检索（implicit retrieval）和结构化协作（structured collaboration）的统一框架来解决这些挑战。在该框架的基础上，基于Monitor的检索模块（Monitor-based retrieval module）在标记级别运行，以最小的推理中断整合外部知识。在此基础之上，分层解决方案精炼（Hierarchical Solution Refinement, HSR）迭代地将每个候选方案指定为由其同行修复的锚点，而质量感知迭代推理（Quality-Aware Iterative Reasoning, QAIR）则根据解决方案质量调整精炼过程。在\"人类最终考试\"（Humanity's Last Exam, HLE）生物/化学金牌数据集上，我们的框架达到了48.3%的准确率——这是迄今为止报道的最高水平，超过了最强的智能体基线13.4个百分点，领先前沿LLMs高达18.1个百分点，同时将标记使用量减少了53.5%，智能体步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了该框架在不同领域的鲁棒性。错误分析显示，在超过85%的情况下，推理失败和知识缺口同时出现，而多样性分析揭示了一个明显的二分法：检索任务受益于解决方案的多样性，而推理任务则倾向于共识。总体而言，这些发现展示了隐式增强（implicit augmentation）和结构化精炼（structured refinement）如何克服显式工具使用和统一聚合的低效问题。代码可在以下网址获取：https://github.com/tangxiangru/Eigen-1。", "summary_generated_time": "2025-09-27 15:52:24", "summary_model": "z-ai/glm-4.5"}, {"index": "#6", "title": "Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond", "link": "/arxiv/2509.21284", "arxiv_id": "2509.21284", "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng", "summary": "Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.", "subjects": "Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.158803", "filter_reason": "这篇论文的核心贡献是对思维链(CoT)的鲁棒性进行了理论分析，研究了输入扰动如何影响CoT的推理过程和输出。论文推导了在输出波动可接受范围内输入扰动的上界，并证明了该上界与CoT中的推理步骤数量正相关，以及即使无限长的推理过程也无法消除输入扰动的影响。这些发现对于理解和改进LLM的推理能力具有重要意义。思维链(CoT)是提高大语言模型推理能力的关键方法，论文从理论角度分析其鲁棒性，属于改进LLM基础能力和推理能力的研究范畴，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性的应用层面研究，而是关注通用推理能力的理论分析，因此应该被保留。", "summary2": "本文旨在解决Chain-of-Thought (CoT)对输入扰动的鲁棒性问题。针对输入扰动如何影响CoT输出的理论解释缺失，我们提出了一种基于Lipschitz连续性的理论分析方法，推导出输出波动上界与推理步骤数量、嵌入向量范数的关系，并在MATH、MMLU-Pro和GPQA数据集上通过Exact Match和Output Fluctuation指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n思维链(CoT)对输入扰动极为敏感，微小输入变化会导致输出显著波动。尽管已有提示优化方法试图缓解此问题，但缺乏对输入扰动如何影响CoT输出的理论解释，限制了对扰动传播机制的深入理解和提示优化方法的进一步改进。\n\n## 关键洞察\n作者将CoT视为多步迭代过程，在Lipschitz连续性假设下，发现输入扰动对输出波动的影响可量化为上界。关键洞察是CoT鲁棒性不仅与推理步数相关，还与模型特性(如输入嵌入和隐藏状态向量范数)相关，且即使无限推理步数也无法完全消除扰动影响。\n\n## 解决方案演进\n作者先推导输出波动上界，证明推理步数与扰动影响关系；然后将结论应用于线性自注意力模型，证明输入扰动上界与向量范数负相关；通过实验验证理论；最后基于分析提出通过最大化输入扰动上界选择提示的方法。\n\n## 创新点总结\n首次从理论角度分析输入扰动对CoT影响，提出CoT鲁棒性上界，揭示无限推理无法消除扰动的本质，发现向量范数与鲁棒性的负相关关系，并基于理论提出新型提示优化方法。", "summary_translation": "现有研究表明，思维链(Chain-of-Thought, CoT)的输出受到输入扰动的显著影响。尽管许多方法试图通过优化提示词(prompts)来减轻这种影响，但这些扰动如何影响CoT输出的理论解释仍是一个开放的研究领域。这一差距限制了我们对输入扰动在推理过程中如何传播的深入理解，并阻碍了提示词优化方法的进一步改进。因此，在本文中，我们从理论上分析了输入扰动对CoT输出波动的影响。我们首先在输出波动处于可接受范围内的条件下，推导出输入扰动的上界(upper bound)，并基于此证明：(i) 该上界与CoT中的推理步骤数量呈正相关；(ii) 即使无限长的推理过程也无法消除输入扰动的影响。然后，我们将这些结论应用于线性自注意力(Linear Self-Attention, LSA)模型，该模型可视为Transformer的简化版本。对于LSA模型，我们证明了输入扰动的上界与输入嵌入(input embedding)和隐藏状态(hidden state)向量的范数(norms)呈负相关。为验证这一理论分析，我们在三个主流数据集和四个主流模型上进行了实验。实验结果与我们的理论分析一致，从经验上证明了我们发现的正确性。", "summary_generated_time": "2025-09-27 15:52:24", "summary_model": "z-ai/glm-4.5"}, {"index": "#10", "title": "Query-Centric Graph Retrieval Augmented Generation", "link": "/arxiv/2509.21237", "arxiv_id": "2509.21237", "authors": "Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu", "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.", "subjects": "Computation and Language, Information Retrieval", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.159650", "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出QCG-RAG，一种以查询为中心的图检索增强生成框架，旨在改进LLM的多跳推理能力。论文明确指出其目标是解决现有图RAG方法中的粒度困境，通过查询粒度索引和多跳块检索机制来增强LLM的推理能力。这属于改进LLM基础能力和增强其多步推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs)\" - 能力方向：专注于\"multi-hop reasoning\"（多跳推理），这是通用推理能力的重要组成部分 - 新兴范式：RAG本身可视为一种工具使用形式，论文提出的新框架增强了LLM利用外部知识的能力 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不是针对医疗、化学、生物等特定应用领域 - 不主要关注水印、安全性等模型可靠性问题 第四步：特殊和模糊情况处理 论文提出的RAG框架可以视为一种通用的工具使用方法，目的是增强LLM的通用问题解决能力（特别是多跳推理），而非应用于特定领域。虽然论文提到了\"improving graph quality and interpretability\"，但这是作为提高推理质量的手段，而非主要焦点。 综上所述，这篇论文的核心贡献是提出了一种新的通用框架来增强LLM的多跳推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决基于图的检索增强生成(RAG)方法面临的粒度困境。针对现有方法中细粒度实体级图导致高token成本和上下文丢失，而粗粒度文档级图无法捕捉细微关系的问题，我们提出了一种以查询为中心的图RAG框架(QCG-RAG)，利用Doc2Query和Doc2Query--构建可控粒度的查询中心图，并设计了多跳检索机制。在LiHuaWorld和MultiHop-RAG数据集上通过准确率和LLM-as-a-Judge指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n图RAG方法面临根本性的\"粒度困境\"：细粒度实体级图虽能捕捉细微关系，但token成本高且易丢失上下文；粗粒度文档级图虽效率高，却无法捕捉复杂关系，难以支持多跳推理。\n\n## 关键洞察\n作者认识到查询(query)可作为中间粒度表示，介于实体与文档之间。通过Doc2Query技术生成的查询既能保留语义丰富性，又具备精确指向性，能实现对齐用户意图的粒度可控表示。\n\n## 解决方案演进\n从这一洞察出发，作者首先利用Doc2Query和Doc2Query--生成高质量查询-答案对；然后构建查询中心图(QCG)，以查询为节点，建立查询间和查询与块间的关联；最后设计多跳检索机制，通过查询检索、邻居扩展、块聚合和生成四步实现精确推理。\n\n## 创新点总结\n该思路创新性地将查询置于图构建核心，首次实现了查询驱动的粒度可控图构建，解决了图RAG中的根本性粒度困境，在不牺牲多跳推理能力的同时，平衡了图的粒度和效率。", "summary_translation": "基于图的检索增强生成(RAG)通过外部知识丰富大型语言模型(LLMs)，以实现长上下文理解和多跳推理(multi-hop reasoning)，但现有方法面临粒度困境：细粒度的实体级图(entity-level graphs)导致高token成本并丢失上下文，而粗粒度的文档级图(document-level graphs)则无法捕捉细微关系。我们提出了QCG-RAG，一种以查询为中心的图RAG框架，它支持查询粒度的索引(query-granular indexing)和多跳块检索(multi-hop chunk retrieval)。我们的以查询为中心的方法利用Doc2Query和Doc2Query{-}{-}构建具有可控粒度的查询中心图(query-centric graphs)，提高了图的质量和可解释性。定制的多跳检索机制(multi-hop retrieval mechanism)随后通过生成的查询选择相关块(chunks)。在LiHuaWorld和MultiHop-RAG上的实验表明，QCG-RAG在问答准确性方面始终优于先前的基于块(chunk-based)和基于图的RAG方法，为多跳推理建立了新范式。", "summary_generated_time": "2025-09-27 15:52:37", "summary_model": "z-ai/glm-4.5"}, {"index": "#11", "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents", "link": "/arxiv/2509.21212", "arxiv_id": "2509.21212", "authors": "Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu", "summary": "Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.", "subjects": "Computation and Language, Information Retrieval", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.159851", "filter_reason": "这篇论文的核心贡献是提出SGMem（句子图记忆）方法，用于增强大语言模型在长期对话中的记忆管理能力。从筛选标准来看，首先，论文的本质是改进LLM的基础能力，特别是在处理超出上下文窗口的长期对话时的信息组织和检索能力，这属于增强LLM通用推理能力的范畴。有效的记忆管理是进行连贯推理和多步对话的基础，因此这项工作直接提升了LLM的通用能力。其次，论文包含了正面指标中的\"Large language models, LLMs\"和\"llm-based agents\"概念。第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。最后，论文提出的SGMem是一种通用的记忆管理框架，用于增强对话代理的通用能力，而非应用于特定领域。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决长期对话代理中的记忆管理问题，特别是处理超出LLMs上下文窗口的对话历史。针对长期对话场景，我们提出了一种SGMem（句子图记忆）方法，将对话表示为分块单元内的句子级图，捕获跨不同粒度的关联。在LongMemEval和LoCoMo数据集上通过Accuracy指标验证了其有效性，SGMem一致地提高了准确性并优于强基线。", "inspiration_trace": "## 面临的挑战\n长期对话代理面临记忆过载和碎片化问题：随着交互累积，存储内容规模和复杂性超出管理能力；现有方法虽减少冗余，但难以组织和检索不同粒度的对话信息，相关信息分散在原始对话和生成记忆中，阻碍连贯检索。\n\n## 关键洞察\n作者认识到句子是对话交换的基本语义单位，足够细粒度以捕获上下文依赖；将句子构造为图节点可显式建模跨对话段关联，减轻记忆碎片化；且无需额外LLM提取，仅需标准句子分割工具即可构建有效记忆结构。\n\n## 解决方案演进\n从句子级表示出发，构建分层记忆框架，将对话组织为句子级图；设计多跳检索机制，在图上遍历以扩展相关句子并映射回父块；最终形成向量检索与图扩展双重设计，整合原始对话与生成记忆，提供连贯上下文。\n\n## 创新点总结\n首次以句子为基本记忆单位实现细粒度语义对齐；通过图结构显式建模句子关联解决碎片化；轻量级实现避免昂贵LLM提取；双重检索机制兼顾效率与连贯性，为长期对话提供新范式。", "summary_translation": "长期对话代理(long-term conversational agents)需要有效的记忆管理(memory management)来处理超出大型语言模型(large language models, LLMs)上下文窗口(context window)的对话历史(dialogue histories)。基于事实提取(fact extraction)或摘要(summarization)的现有方法减少了冗余(redundancy)，但难以组织和检索不同粒度(granularities)的对话和生成记忆(generated memory)中的相关信息。我们介绍了SGMem (Sentence Graph Memory，句子图记忆)，它将对话表示为分块单元(chunked units)内的句子级图(sentence-level graphs)，捕捉跨越轮次级(turn-level)、回合级(round-level)和会话级(session-level)上下文的关联。通过将检索到的原始对话(retrieved raw dialogue)与生成的记忆(如摘要、事实和见解)相结合，SGMem为LLMs提供连贯(coherent)且相关(relevant)的上下文(context)用于响应生成(response generation)。在LongMemEval和LoCoMo上的实验表明，SGMem持续提高了准确性(accuracy)，并在长期对话问答(long-term conversational question answering)中优于强基线(strong baselines)。", "summary_generated_time": "2025-09-27 15:52:40", "summary_model": "z-ai/glm-4.5"}, {"index": "#2", "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards", "link": "/arxiv/2509.21319", "arxiv_id": "2509.21319", "authors": "Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev", "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).", "subjects": "Computation and Language, Artificial Intelligence, Machine Learning", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.157988", "filter_reason": "这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 首先，从核心判断来看，该论文的本质是提出一种新的强化学习范式RLBFF（Reinforcement Learning with Binary Flexible Feedback），用于改进LLM的后训练阶段。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而非将LLM作为工具应用到特定领域。论文通过结合人类反馈(RLHF)和可验证奖励(RLVR)的优势，提出了一种新的训练方法来增强LLM的响应质量和对齐能力。 其次，论文包含多个正面指标： 1. 核心概念：明确关注LLMs，并使用Qwen3-32B作为示例模型 2. 训练方法：核心就是提出一种新的强化学习方法(RLBFF)，这是RLHF和RLVR的结合 3. 能力方向：虽然未直接强调推理，但提高LLM的响应质量隐含了推理能力的提升，且论文在MT-Bench等包含推理任务的基准上进行了评估 第三，论文不涉及任何排除标准中的领域： 1. 未涉及多模态与视觉内容 2. 未针对特定应用领域（如医疗、化学等） 3. 虽然涉及模型对齐，但核心是提出新方法提高整体响应质量，而非专注于水印、安全等应用层面 最后，在特殊和模糊情况处理上，论文明确解决了RLHF的可解释性问题，属于\"提出新方法来增强模型内在的可解释性，从而提升模型的通用可靠性和推理质量\"的情况，符合保留条件。 综上所述，该论文的核心贡献是提出了一种新的强化学习训练方法来提升LLM的通用能力和对齐质量，这与研究目标\"提高大语言模型的通用推理能力\"高度一致，因此应该被保留。", "summary2": "本文旨在解决RLHF缺乏可解释性和RLVR适用范围有限的问题。针对LLM后训练中的人类反馈和可验证奖励场景，我们提出了一种RLBFF（二元灵活反馈）方法，将自然语言反馈转换为可二元评估的原则，并在RM-Bench（86.2%）和JudgeBench（81.4%）上通过奖励模型性能验证了其有效性。", "inspiration_trace": "## 面临的挑战\nRLHF缺乏可解释性且易出现奖励黑客问题，而RLVR则受限于基于正确性的验证器范围，难以捕捉响应质量的细微方面。这两种主流LLM后训练范式各有优缺点，无法全面解决语言模型对齐需求。\n\n## 关键洞察\n作者认识到人类反馈与可验证奖励本质上是互补的。人类判断背后通常有明确的\"原则\"，但这些原则在传统方法中未被明确表达。单个响应评估比响应对比较更自然，二元评估比李克特量表更易校准。\n\n## 解决方案演进\n从自然语言反馈中提取可用二元方式回答的原则，设计证据引用机制确保忠实于原始反馈。引入共识原则过滤减少主观性，将二元灵活反馈构建为蕴含任务：给定提示、响应和原则，判断响应是否满足该原则。\n\n## 创新点总结\n成功结合RLHF灵活性与RLVR精确性的新范式；从自然语言反馈提取可操作二元原则的创新方法；允许用户在推理时定制原则的能力；开发了专门评估原则遵循能力的新基准。", "summary_translation": "# 基于二元灵活反馈的强化学习在大型语言模型后训练中的应用\n\n基于人类反馈的强化学习(Reinforcement Learning with Human Feedback, RLHF)和基于可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)是大型语言模型(LLM)后训练中主要使用的强化学习范式，各自具有独特的优势。然而，RLHF在可解释性和奖励黑客(reward hacking)方面存在困难，因为它依赖于通常缺乏明确标准的人类判断，而RLVR由于其专注于基于正确性的验证器，在应用范围上受到限制。我们提出了基于二元灵活反馈的强化学习(Reinforcement Learning with Binary Flexible Feedback, RLBFF)，它结合了人类驱动偏好的多样性和基于规则验证的精确性，使奖励模型能够捕捉超越单纯正确性的响应质量的细微方面。RLBFF从自然语言反馈中提取可以以二元方式回答的原则（例如，信息准确性：是，或代码可读性：否）。这些原则随后可用于将奖励模型训练构建为蕴含任务（响应满足或不满足任意原则）。我们表明，以这种方式训练的奖励模型在数据匹配的情况下可以优于Bradley-Terry模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日排行榜第一）上取得顶级性能。此外，与Bradley-Terry模型不同，用户可以在推理时指定感兴趣的原则，以定制我们奖励模型的关注点。最后，我们提供了一个完全开源的方案（包括数据），使用RLBFF和我们的奖励模型来对齐Qwen3-32B，使其在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的性能匹配或超过o3-mini和DeepSeek R1（推理成本低于5%）。", "summary_generated_time": "2025-09-27 15:52:40", "summary_model": "z-ai/glm-4.5"}, {"index": "#34", "title": "WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs", "link": "/arxiv/2509.20863", "arxiv_id": "2509.20863", "authors": "Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma", "summary": "Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.", "subjects": "Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.169830", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的核心是提出WeFT（加权熵驱动微调方法），这是一种针对扩散语言模型(dLLMs)的新训练范式。论文的核心贡献在于解决扩散语言模型在监督微调过程中的挑战，通过基于熵的标记加权来增强模型的推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，因此应保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究扩散语言模型(dLLMs)，这是大语言模型的一种变体 - 能力方向：论文在四个推理基准(Sudoku、Countdown、GSM8K和MATH-500)上评估方法，这些基准涉及数学推理和逻辑推理能力 - 论文虽然不涉及强化学习或智能体等新兴范式，但已包含足够的核心正面指标 第三步：排除标准 论文不符合任何排除标准： - 虽然提到扩散模型，但这是应用于语言建模而非视觉或多模态领域 - 没有专注于任何特定应用领域（如医疗、化学等） - 没有主要关注水印、安全等模型可靠性问题 第四步：特殊和模糊情况 论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合分析，这篇论文通过提出新的微调方法来增强扩散语言模型的通用推理能力，并在多个推理基准上验证了其有效性，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决扩散语言模型(dLLMs)在监督微调(SFT)过程中缺乏精确概率估计的问题。针对扩散模型中token重要性不均等的场景，我们提出了一种基于熵加权的WeFT方法，通过token预测熵分配不同权重，使高熵token获得更强训练信号。在Sudoku、Countdown、GSM8K和MATH-500四个推理基准上，WeFT相比标准SFT实现了39%-83%的相对性能提升，且在后续RL训练中仍保持49%的性能优势。", "inspiration_trace": "## 面临的挑战\n扩散语言模型(dLLMs)在监督微调(SFT)时存在关键问题：传统SFT假设所有token具有相同掩码率，将每个token视为同等重要，忽略了token在推理和规划中的重要性差异，导致模型难以有效学习关键推理结构。\n\n## 关键洞察\n作者发现token的预测分布熵是衡量其重要性的可靠指标：高熵token通常与推理或规划相关，对应模型生成中不确定性高的位置。优先训练这些token能引导模型为序列中关键推理部分分配更多能力。\n\n## 解决方案演进\n基于扩散理论，作者首先推导出加权SFT损失函数，确保与扩散原理一致。为解决原始熵值方差大的问题，采用熵的平方根作为稳定度量。最终形成WeFT：每个训练步骤执行两次前向传递，第一次估计token掩码率，第二次使用不同掩码概率训练，使高熵token获得更强训练信号。\n\n## 创新点总结\nWeFT创新在于将token重要性量化与扩散过程理论结合，创造理论一致的加权SFT方法。通过预测熵自动识别并优先训练关键推理token，在提升推理性能的同时保持较低计算开销。", "summary_translation": "扩散模型（Diffusion models）最近在语言建模（language modeling）中展现出强大的潜力，与传统的自回归（autoregressive）方法相比，提供了更快的生成速度。然而，将监督微调（supervised fine-tuning, SFT）应用于扩散模型仍然具有挑战性，因为它们在每个去噪（denoising）步骤中缺乏精确的概率估计。虽然扩散机制（diffusion mechanism）使模型能够对整个序列进行推理（reason），但它也使生成过程变得较难预测且常常不一致。这凸显了控制引导生成方向的关键令牌（key tokens）的重要性。为了解决这个问题，我们提出了WeFT，一种用于扩散语言模型（diffusion language models）的加权SFT方法，其中令牌根据其熵（entropy）被分配不同的权重。源自扩散理论（diffusion theory）的WeFT带来了显著收益：在open-r1数据集的s1K、s1K-1.1和3k样本上训练时，在四个广泛使用的推理基准（reasoning benchmarks）（Sudoku、Countdown、GSM8K和MATH-500）上，它相对于标准SFT实现了39%、64%和83%的相对改进。代码和模型将公开发布。", "summary_generated_time": "2025-09-27 15:52:36", "summary_model": "z-ai/glm-4.5"}, {"index": "#37", "title": "Distilling Many-Shot In-Context Learning into a Cheat Sheet", "link": "/arxiv/2509.20820", "arxiv_id": "2509.20820", "authors": "Ukyo Honda, Soichiro Murakami, Peinan Zhang", "summary": "Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.", "subjects": "Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.170381", "filter_reason": "这篇论文的核心贡献是提出了一种称为\"cheat-sheet ICL\"的新方法，旨在提高大语言模型在上下文学习中的推理效率和性能。论文将多样本上下文学习的信息提炼成简洁的文本摘要，在保持或提高推理性能的同时大幅减少了计算需求。这直接关系到提升LLM的通用推理能力，特别是在处理需要多步推理的任务时。论文不是将LLM作为工具应用到特定领域，而是关注如何改进LLM本身的推理机制，因此完全符合研究目标中\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的要求。此外，论文明确涉及\"Large language models\"和\"reasoning\"等正面指标，且不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文提出的是一种通用方法论，可以应用于各种推理任务，而非针对特定领域的应用。", "summary2": "本文旨在解决many-shot in-context learning计算成本高的问题。针对需要大量示例的推理任务，我们提出了一种cheat-sheet ICL方法，将many-shot示例提炼为简洁文本摘要作为推理上下文。在BBH Hard推理任务上通过准确率和token数量验证了其有效性，证明cheat-sheet ICL以更少的token实现了与many-shot ICL相当或更好的性能，且无需测试时检索操作。", "inspiration_trace": "## 面临的挑战\nMany-shot ICL虽性能优越，但需处理大量输入token，计算成本高昂；现有高效方法如示例检索需每次推理时进行检索；专有模型无法通过参数更新进行微调。\n\n## 关键洞察\nLLM具有高级语言理解能力，能将学到的知识以文本形式表示；many-shot ICL中的知识可被提取并总结为显式文本格式；无需每次推理时从大量示例中推断隐藏模式，可预先提取这些模式。\n\n## 解决方案演进\n从人类\"考试小抄\"获得灵感，设计预处理步骤让LLM从大量演示中提取核心知识生成简洁cheat sheet；推理时仅使用cheat sheet和少量格式示例，通过特定提示引导LLM创建有效摘要。\n\n## 创新点总结\n将many-shot ICL知识蒸馏为可解释文本摘要而非模型参数；一次性预处理，推理时无需额外计算或检索；兼顾效率、性能和可解释性，为专有模型提供实用任务适应方法。", "summary_translation": "大型语言模型(large language models, LLMs)的最新进展使得通过多示例(many-shot)进行有效的上下文学习(in-context learning, ICL)成为可能，但代价是由于输入标记(input tokens)较长而导致的高计算需求。为解决这一问题，我们提出了速查表上下文学习(cheat-sheet ICL)，该方法将多示例上下文学习(many-shot ICL)的信息提炼成一个简洁的文本摘要(速查表)，在推理时作为上下文使用。在具有挑战性的推理任务上的实验表明，速查表上下文学习(cheat-sheet ICL)使用远少于多示例上下文学习(many-shot ICL)的标记(tokens)即可达到相当或更好的性能，并且无需测试时检索(test-time retrieval)即可与基于检索的上下文学习(retrieval-based ICL)相媲美。这些研究结果表明，速查表上下文学习(cheat-sheet ICL)是在下游任务中利用大型语言模型(LLMs)的一种实用替代方案。", "summary_generated_time": "2025-09-27 15:52:35", "summary_model": "z-ai/glm-4.5"}, {"index": "#42", "title": "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs", "link": "/arxiv/2509.20758", "arxiv_id": "2509.20758", "authors": "Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li", "summary": "Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.", "subjects": "Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.171412", "filter_reason": "这篇论文的核心贡献是研究如何在大语言模型进行特定领域微调时保持其通用能力，提出了一种新的训练方法Token-Adaptive Loss Reweighting (TALR)。从本质上看，论文关注的是改进LLM的基础能力，提出新的训练范式来增强模型的通用推理和问题解决能力，而不是将LLM作为工具应用到特定领域。论文通过理论和实验分析，探索了如何减轻监督微调对模型通用能力的损害，这直接关系到提升LLM的通用推理能力。虽然论文讨论了领域特定的微调，但其焦点不是特定应用领域，而是如何在领域微调时保持通用能力。论文符合\"改进LLM的基础能力、提出新的训练范式\"的标准，不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在重新审视领域特定微调对大语言模型通用能力的影响。针对领域特定SFT导致通用能力下降的问题，我们提出了使用较小学习率和Token-Adaptive Loss Reweighting (TALR)方法，并在MedCalc和ESCI数据集上通过领域性能与通用能力(IFEval、GSM8K、HumanEval)的平衡指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n领域特定监督微调(SFT)被普遍认为会显著损害LLM的通用能力，这种观点限制了LLM在专业领域的应用，形成了一个两难困境。\n\n## 关键洞察\n作者发现使用较小学习率可大幅缓解通用性能下降，同时保持目标领域性能。这表明之前报道的严重退化部分源于过于激进的优化策略，而非SFT本身的固有缺陷。\n\n## 解决方案演进\n基于这一发现，作者首先通过信息理论分析解释了小学习率的作用机制。随后认识到仅靠小学习率不足以完全解决问题，进而提出TALR方法，通过自适应降低困难token的权重来减轻其对通用能力的不成比例影响。\n\n## 创新点总结\n该研究挑战了领域SFT必然损害通用能力的固有观念，揭示了学习率的关键作用，并从token级别提供了新的平衡策略，实现了领域适应与通用能力保持的更优权衡。", "summary_translation": "在特定领域数据集上进行监督微调(Supervised Fine-Tuning, SFT)是使大型语言模型(Large Language Models, LLMs)适应专门任务的常见方法，但通常被认为会降低模型的通用能力。在这项工作中，我们重新审视了这种权衡，并提供了实证和理论上的见解。首先，我们表明SFT并不总是有害的：使用较小的学习率(learning rate)可以显著减轻通用性能的下降，同时保持相当的目标领域性能。然后，我们提供了一个理论分析来解释这些现象，并进一步提出了一种新方法——令牌自适应损失重加权(Token-Adaptive Loss Reweighting, TALR)。在此基础上，并认识到仅靠较小的学习率并不能在所有情况下完全消除通用性能的下降，我们评估了一系列减少通用能力损失的策略，包括L2正则化(L2 regularization)、LoRA、模型平均(model averaging)、FLOW以及我们提出的TALR。实验结果表明，虽然没有任何方法能完全消除这种权衡，但TALR在平衡特定领域收益和通用能力方面始终优于这些基线方法。最后，我们将研究成果提炼为将LLMs适应新领域的实用指南：(i)使用较小的学习率以实现有利的权衡，(ii)当需要更强的平衡时，采用TALR作为有效策略。", "summary_generated_time": "2025-09-27 15:52:36", "summary_model": "z-ai/glm-4.5"}, {"index": "#43", "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering", "link": "/arxiv/2509.20750", "arxiv_id": "2509.20750", "authors": "Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang", "summary": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.171615", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Confidence-guided Refinement Reasoning (C2R)\"的新型推理框架，专注于改进大语言模型的通用推理能力。从筛选标准来看： 首先，在核心判断层面，论文本质上是关于改进LLM的推理能力的，提出了一种新的训练免费推理框架，通过构建和细化子问题及其答案来增强模型的推理过程。这直接符合\"改进LLM基础能力\"和\"增强其逻辑推理能力\"的保留标准。 其次，从正面指标看，论文明确聚焦于\"reasoning\"这一核心能力方向，讨论了\"探索多样化推理路径\"和\"实现稳健可靠的推理\"，这些都是通用推理能力的关键组成部分。 关于多模态方面，虽然论文提到其框架适用于文本、图像和视频领域，但这只是表明其方法的通用性，而非主要焦点。论文的核心是推理框架本身，而不是解决多模态或视觉特定问题，因此不应被排除。 最后，C2R作为一种可以与各种现有QA模型无缝集成的通用方法，代表了提升LLM推理能力的新方法论，完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决零样本问答任务中多步推理方法 indiscriminate 使用子问题-答案对可能引入噪声的问题。针对文本、图像和视频领域的QA任务，我们提出了一种基于置信度引导的精炼推理（C2R）框架，通过Generator、Refiner和Answer Selector三个组件，选择性地利用sub-QAs并基于模型自身置信度分数选择最终答案。在MMLU、MMMU和EgoSchema等五个基准测试上，通过准确率等指标验证了C2R在不同模型上均能持续提升性能。", "inspiration_trace": "## 面临的挑战\n在零样本问答任务中，现有多步推理方法不加区分地使用子问题-答案对(sub-QAs)，没有验证其相关性和准确性。当子问题与主问题无关或答案不准确时，会引入噪声，反而降低最终答案质量。\n\n## 关键洞察\n作者发现子问题并不总是增强问答推理，盲目使用可能分散模型注意力。关键在于如何有效利用子问题，而非简单增加中间步骤。作者还发现\"置信度膨胀\"现象：即使使用不正确子问题，模型对答案的置信度也会增加，导致错误选择。\n\n## 解决方案演进\n基于洞察，作者提出基于置信度的筛选机制：先生成多个子问题-答案对，有选择地组织成不同子集探索多样化推理路径。每个路径产生候选答案及置信度，最后比较这些置信度（包括单步答案）选择最可靠答案。设计两个原则：基础答案置信度高时直接采用；需使用子问题时，要求改进答案置信度显著高于基础答案才采用。\n\n## 创新点总结\n创新点在于：提出无需训练的模型无关框架，可无缝集成各种问答模型；通过置信度引导筛选降低不相关子问题风险；首次系统分析子问题数量质量影响，揭示置信度膨胀现象并提供应对策略。", "summary_translation": "我们提出了Confidence-guided Refinement Reasoning (C2R，置信度引导的精炼推理)，这是一个新颖的免训练框架，适用于文本、图像和视频领域的问答(QA，question-answering)任务。C2R策略性地构建和精炼子问题及其答案(sub-QAs，sub-questions and answers)，为目标答案推导出更好的置信度分数。C2R首先策划一个子问题集(sub-QAs)子集来探索多样化的推理路径，然后比较生成的答案候选者的置信度分数，以选择最可靠的最终答案。由于C2R仅依赖于模型自身推导出的置信度分数，它可以与各种现有的QA模型无缝集成，在不同模型和基准测试上展现出一致的性能提升。此外，我们提供了关于利用子问题(sub-QAs)如何影响模型行为的关键但尚未充分探索的见解，具体分析了子问题的数量和质量对实现稳健可靠推理的影响。", "summary_generated_time": "2025-09-27 15:52:36", "summary_model": "z-ai/glm-4.5"}, {"index": "#53", "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures", "link": "/arxiv/2509.20577", "arxiv_id": "2509.20577", "authors": "Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar", "summary": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.", "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval", "date": "2025-09-24", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.179137", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据筛选标准，我的判断过程如下： 第一步：核心判断 论文的核心是提出DS-MoE（Depth Specialised Mixture of Experts）框架，这是一种改进Transformer架构的新方法，通过深度专业化的专家混合模型来动态构建推理链。论文明确关注增强LLM的逻辑推理和多步推理能力，而不是将LLM作为工具应用于特定领域。这完全符合改进LLM基础能力和增强其通用推理能力的研究目标。 第二步：正面指标 论文包含多个正面指标： - 核心概念：虽然摘要未直接提及\"LLMs\"，但DS-MoE明显是针对大语言模型的基础架构改进 - 能力方向：明确涉及\"reasoning chains\"、\"reasoning depths\"、\"logical inference\"和\"complex multi-step reasoning benchmarks\"，直接针对推理能力，特别是逻辑推理和多步推理 - 论文强调通过动态组装定制的推理链来匹配输入复杂性，这正是提升通用推理能力的核心 第三步：排除标准 论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉相关内容 - 虽然训练数据集包含多个领域，但论文本身不是针对特定应用领域的研究 - 未涉及模型可靠性方面的水印、安全性等内容 第四步：特殊和模糊情况 论文提到\"routing decisions yield interpretable reasoning chains, enhancing transparency\"，这表明其方法增强了模型的可解释性，从而可能提升模型的通用可靠性和推理质量，符合研究目标。 综上所述，这篇论文的核心贡献是通过改进Transformer架构来增强LLM的通用推理能力，特别是逻辑推理和多步推理能力，同时提高计算效率和可解释性，完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决传统Transformer架构对所有输入使用相同处理深度导致的效率低下和推理质量受限问题。针对不同复杂度的输入，我们提出了一种Depth-Specialised Mixture-of-Experts (DS-MoE)方法，通过引入针对不同推理深度优化的专家模块和学习路由网络动态组装推理链，并在The Pile数据集上通过计算节省、推理速度和准确率等指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n传统Transformer架构对所有输入采用相同处理深度，无论简单事实查询还是复杂逻辑问题都经过相同多层计算，造成计算资源浪费，同时限制了深度推理能力，无法学习深度最优的推理策略。\n\n## 关键洞察\n作者从人类认知中获得灵感，认识到人类会根据问题复杂度自然调整推理深度。不同任务需要不同深度的处理，从浅层模式识别到深层逻辑推理，再到元认知监督，将MoE从宽度扩展到深度专门化可实现更高效的资源分配。\n\n## 解决方案演进\n首先设计针对不同推理深度的专家模块（浅层模式识别、组合推理、逻辑推理等），然后开发学习路由网络动态评估输入复杂度，最后构建动态推理链，仅激活必要专家匹配输入复杂度，实现自适应深度处理。\n\n## 创新点总结\n首次将MoE从宽度扩展到深度专门化，实现计算资源与任务复杂度的自适应匹配；通过动态推理链提供可解释性；同时提升计算效率、推理质量和可解释性，为大型语言模型提供更接近人类认知的架构设计。", "summary_translation": "当代的transformer（变换器）架构对所有输入应用相同的处理深度，导致效率低下并限制了推理质量。简单的事实性查询与复杂的逻辑问题受到相同的多层计算处理，浪费资源的同时也限制了深度推理。为了克服这一问题，我们提出了通过深度专业化专家混合(DS-MoE, Depth Specialised Mixture of Experts)实现动态推理链的概念，这是一个模块化框架，将专家混合(Mixture of Experts)范式从基于宽度的计算扩展到深度专业化的计算。DS-MoE引入了针对不同推理深度优化的专家模块，包括浅层模式识别(shallow pattern recognition)、组合推理(compositional reasoning)、逻辑推理(logical inference)、记忆整合(memory integration)和元认知监督(meta-cognitive supervision)。学习到的路由网络(routing network)动态组装定制推理链，仅激活必要的专家以匹配输入复杂度。我们训练和评估DS-MoE的数据集是The Pile，这是一个800GB的语料库(corpus)，涵盖科学论文、法律文本、编程代码和网络内容等多个领域，能够对推理深度进行系统性评估。实验结果表明，与统一深度的transformer相比，DS-MoE实现了高达16%的计算节省和35%的更快推理(inference)速度，同时在复杂多步推理基准上实现了2.8%的更高准确率。此外，路由决策产生可解释的推理链，增强了透明度(transparency)和可扩展性(scalability)。这些发现确立了DS-MoE作为自适应神经架构(adaptive neural architectures)的重要进展，表明深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性(interpretability)。", "summary_generated_time": "2025-09-27 15:52:43", "summary_model": "z-ai/glm-4.5"}, {"index": "#56", "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning", "link": "/arxiv/2509.20502", "arxiv_id": "2509.20502", "authors": "Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang", "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-24", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.179790", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出MARS（Multi-Agent Review System），一种基于角色的多智能体协作框架，旨在提高LLM的推理能力。论文的核心贡献是改进LLM的基础推理能力，提出了一种新的多智能体协作范式，而不是将LLM作为工具应用于特定领域。因此，这篇论文符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确提到\"Large language models (LLMs)\" - 能力方向：直接关注\"LLM reasoning\"，旨在提高模型的推理质量 - 新兴范式：提出了多智能体系统(MARS)，这是一种基于LLM的智能体协作框架 论文符合3个正面指标，表明其与研究主题高度相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于特定应用领域（如医疗、化学等） - 不关注模型可靠性层面的水印、安全等问题 第四步：特殊和模糊情况 论文提出的MARS框架是一种通用的智能体协作方法，用于增强LLM的通用推理能力，而不是将智能体应用于特定领域。根据筛选标准，这种通用的智能体协作框架应该保留。 综合分析，这篇论文的核心贡献是提出了一种更高效的多智能体协作框架(MARS)来增强LLM的通用推理能力，与研究目标\"提高大语言模型的通用推理能力\"直接一致。论文通过改进多智能体协作方式，在保持推理质量的同时显著提高了效率，这正属于对LLM基础推理能力的改进研究。", "summary2": "本文旨在 [解决多智能体协作中计算开销大的问题]。针对 [多智能体辩论(MAD)框架中的高资源消耗]，我们提出了一种 [基于角色的多智能体评审系统(MARS)]，并在 [多个推理基准测试(GPQA、MMLU、GSM8K)] 上通过 [准确率、token消耗和推理时间] 验证了其有效性。", "inspiration_trace": "## 面临的挑战\n单个LLM在复杂推理任务上能力有限，而多智能体辩论(MAD)虽能提升推理质量，却因智能体间频繁通信导致计算开销巨大，token消耗和推理时间随智能体数量急剧增加，限制了实际应用。\n\n## 关键洞察\n作者从评审过程和验证器组件中获得启发，认识到层次化评估结构比自由讨论更高效。关键洞察是：有效的多智能体协作不依赖于所有智能体间的直接通信，而是可以通过结构化的角色分工和反馈机制实现，类似于ResNet中的残差学习原理。\n\n## 解决方案演进\n基于这一洞察，作者设计了角色分离的框架：作者生成初始方案，多个评论员独立评估，元评论员整合反馈并决策。这种\"提出-审查-反馈-更新\"架构避免了智能体间昂贵的直接通信，仅通过元评论员集中传递反馈，大幅降低了资源消耗。\n\n## 创新点总结\n将学术评审的层次结构引入多智能体协作，创造了新的协作范式；通过角色分离和结构化通信，在保持推理质量的同时将资源消耗减少约50%；将残差学习原理扩展到系统层面，专注于错误检测与修正，使多智能体协作更加实用高效。", "summary_translation": "大型语言模型（Large Language Models, LLMs）在自然语言理解方面取得了令人瞩目的成果，然而当作为单一代理（single agent）运行时，它们的推理能力仍然有限。多智能体辩论（Multi-Agent Debate, MAD）被提出以解决这一限制，它通过圆桌辩论方式使多个模型能够进行协作推理。尽管有效，但由于涉及的代理数量和所需的频繁通信，MAD引入了大量的计算开销。在本文中，我们提出了MARS（Multi-Agent Review System，多智能体评审系统），这是一个受评审过程启发的基于角色的协作框架。在MARS中，作者代理（author agent）生成初始解决方案，评审者代理（reviewer agents）独立提供决策和评论，元评审者（meta-reviewer）整合反馈以做出最终决策并指导进一步修订。这种设计在避免昂贵的评审者之间交互的同时提高了推理质量，从而控制了token消耗和推理时间。我们在多个基准测试（benchmarks）中将MARS与MAD及其他最先进的推理策略进行了比较。使用不同LLMs的广泛实验表明，MARS在匹配MAD准确率的同时，将token使用量和推理时间减少了约50%。代码可在https://github.com/xwang97/MARS获取。", "summary_generated_time": "2025-09-27 15:52:37", "summary_model": "z-ai/glm-4.5"}, {"index": "#61", "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation", "link": "/arxiv/2509.20377", "arxiv_id": "2509.20377", "authors": "Tomoaki Isoda", "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-20", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.181043", "filter_reason": "这篇论文符合我的研究目标，因为它核心是关于改进大语言模型本身的通用推理能力。具体分析如下： 从第一步核心判断来看，论文的本质是提出SKILL-RAG方法，这是一种新的训练范式，通过强化学习框架来增强模型的自我知识认知能力。该方法不是将LLM作为工具应用于特定领域，而是致力于提升LLM在知识处理和推理方面的基础能力，特别是通过识别模型\"知道\"和\"不知道\"的内容来优化其推理过程，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的标准。 从第二步正面指标看，论文包含多个相关主题： - 核心概念：明确研究LLMs（Llama2-7B和Qwen3-8B） - 能力方向：涉及推理能力，特别是在知识密集型任务中的问题解决 - 训练方法：使用强化学习（reinforcement learning-based training framework）来训练模型 - 新兴范式：RAG本身可视为一种工具使用范式，论文改进了这一通用框架 从第三步排除标准看，论文不涉及任何应排除的领域，没有专注于多模态、特定应用领域或模型基础设施等。 从第四步特殊和模糊情况看，虽然论文涉及减少幻觉的问题，但它是通过提出新方法（利用模型自我知识）来提升模型的内在推理质量，而不是应用层面的讨论，因此应该保留。 综上所述，SKILL-RAG论文的核心贡献是提出了一种增强LLM自我知识认知和推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决RAG系统因检索无关内容导致LLM产生幻觉的问题。针对检索增强生成场景，我们提出了一种SKILL-RAG方法，利用强化学习激发LLM的自我知识能力，并在句子级别过滤检索内容。在TriviaQA、SelfAware、NQ和TruthfulQA数据集上通过准确率等指标验证了其有效性，同时减少了输入文档数量，提高了生成质量和效率。", "inspiration_trace": "## 面临的挑战\nRAG系统虽能提升LLMs在知识密集型任务上的表现，但检索系统常返回不相关内容，导致模型产生幻觉。现有方法或采用粗粒度二元决策，或依赖置信度指标，缺乏细粒度过滤能力，无法有效建模模型对自身知识状态的认知。\n\n## 关键洞察\n作者认识到模型必须具备识别自身知识边界的能力——\"自我知识\"(self-knowledge)，即知道自己知道什么和不知道什么。有效整合内部知识与外部检索信息的关键在于理解并利用这种自我知识状态。\n\n## 解决方案演进\n基于这一洞察，作者设计强化学习框架显式引出模型的自我知识，采用句子级别细粒度过滤。首先构建自我知识数据集，提出基于熵的RLHF方法训练模型表达知识状态；然后设计基于点互信息(PMI)的过滤机制，只保留能提高模型回答置信度的检索内容。\n\n## 创新点总结\n创新之处在于首次将\"自我知识\"概念引入RAG过滤过程，使模型基于自身知识边界判断外部信息价值；通过SKILL框架使模型准确表达知识状态；设计PMI细粒度过滤机制，提升RAG系统效率和准确性。", "summary_translation": "检索增强生成(RAG)近年来显著提高了大型语言模型(LLMs)在知识密集型任务(knowledge-intensive tasks)上的性能。然而，由于检索系统可能返回不相关的内容，将这些信息整合到模型中常常会导致幻觉(hallucinations)问题。因此，识别并过滤掉无用的检索内容是提高RAG性能的关键挑战。为了更好地将模型的内部知识与检索获得的外部知识相结合，理解模型\"知道\"和\"不知道\"什么（这也被称为\"自我知识\"(self-knowledge)）至关重要。\n\n基于这一见解，我们提出了SKILL-RAG（基于自我知识诱导的学习与过滤RAG方法(Self-Knowledge Induced Learning and Filtering for RAG)），这是一种利用模型自我知识来确定哪些检索文档有益于回答给定查询的新方法。我们设计了一个基于强化学习(reinforcement learning)的训练框架，以明确地激发模型的自我知识，并采用句子级粒度(sentence-level granularity)来过滤不相关内容，同时保留有用知识。\n\n我们在多个问答基准(question answering benchmarks)上使用Llama2-7B和Qwen3-8B模型对SKILL-RAG进行了评估。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，验证了自我知识在指导高质量检索选择中的重要性。", "summary_generated_time": "2025-09-27 15:52:35", "summary_model": "z-ai/glm-4.5"}, {"index": "#73", "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns", "link": "/arxiv/2509.21124", "arxiv_id": "2509.21124", "authors": "Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai", "summary": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.189451", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力。论文提出了一种新的训练范式，通过学习多样化的思维链模式来扩展基础模型的推理潜力，这直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含了多个正面指标： 1. 核心概念：论文明确研究基础模型(Foundation Model)的推理能力 2. 能力方向：聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning) 3. 训练方法：涉及强化学习(RL)作为关键方法，并讨论如何提高下游RL性能 4. 新兴范式：关注思维链(CoT)的优化，这是提高LLM推理能力的重要范式 第三，论文不符合任何排除标准： 1. 不涉及多模态与视觉内容 2. 虽然在数学推理上评估，但这是作为通用推理能力的测试案例，而非针对特定应用领域 3. 不涉及模型可靠性方面的水印、安全等问题 论文的核心贡献是首次定义了基础模型的\"推理潜力\"，并提出了一种通过选择高价值思维链数据来扩展这种潜力的方法。这种方法通过抽象原子推理模式，构建核心参考集，并使用双粒度算法选择高价值的CoT数据，从而训练模型更有效地掌握推理能力。这些贡献直接针对提升LLM的通用推理能力，完全符合研究目标。", "summary2": "本文旨在解决基础模型数学推理能力受限的问题。针对思维链(CoT)数据无差别使用的现状，我们提出了一种CoTP框架，通过抽象原子推理模式并构建核心参考集，利用双粒度算法选择高价值推理数据。在85A6B MoE模型上，仅用10B-token的CoTP数据就在AIME 2024和2025上提高了9.58%的性能，并将下游RL性能上限提升7.81%。", "inspiration_trace": "## 面临的挑战\n现有大推理模型依赖强化学习提升数学推理能力，但基础模型的推理能力直接限制了RL性能上限。当前方法在中间训练阶段不加区分地使用所有CoT数据，缺乏对哪些数据类型最有效增强模型推理能力的深入研究。\n\n## 关键洞察\n作者首次理论定义基础模型推理潜力为正确回答问题所需独立尝试次数的倒数，发现这与最终模型性能强相关。认识到扩展推理潜力等同于减少正确回答问题所需的平均推理尝试次数，并从CoT序列中抽象出具有共性和归纳能力的原子推理模式。\n\n## 解决方案演进\n从理论定义出发，构建富含高价值推理模式的核心参考集来近似理想数据。开发双粒度算法，同时考虑推理模式链和token熵，使用加权DTW选择与核心集一致的长CoT数据。最终构建CoTP数据集，通过实验验证少量精选数据能显著扩展模型推理潜力。\n\n## 创新点总结\n首次理论定义模型推理潜力并建立其与性能关联；提出原子推理模式抽象方法；开发双粒度数据选择算法；证明少量精选高价值推理数据能显著提升模型性能并提高RL上限。", "summary_translation": "在复杂数学推理领域，大型推理模型（large reasoning models）的最新进展主要由强化学习（reinforcement learning, RL）所驱动。研究表明，在中间训练（mid-training）阶段融入长思维链（chain-of-thought, CoT）数据能显著提高推理深度。然而，当前方法通常不加区分地利用CoT数据，未能解决一个关键问题：哪些数据类型能最有效地提升模型的推理能力。\n\n在本文中，我们首次将基础模型（foundation model）的推理潜力（reasoning potential）定义为正确回答问题所需独立尝试次数的倒数，该定义与最终模型性能呈现强相关性。随后，我们提出利用富含高价值推理模式（high-value reasoning patterns）的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式（atomic reasoning patterns），并利用这些模式构建一个富含有价值推理模式的核心参考集（core reference set）。此外，我们提出一种双粒度算法（dual-granularity algorithm），该算法涉及推理模式链（chains of reasoning patterns）和标记熵（token entropy），能高效地从数据池中筛选出与核心集一致的高价值CoT数据（CoTP），从而训练模型有效掌握推理能力。\n\n仅需100亿token（10B-token）的CoTP数据，就能使85A6B专家混合（Mixture-of-Experts, MoE）模型在具有挑战性的AIME 2024和2025测试中提升9.58%的性能，并将下游强化学习（downstream RL）性能的上限提高7.81%。", "summary_generated_time": "2025-09-27 15:52:59", "summary_model": "z-ai/glm-4.5"}, {"index": "#76", "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning", "link": "/arxiv/2509.21070", "arxiv_id": "2509.21070", "authors": "Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.190261", "filter_reason": "根据筛选标准，这篇论文符合研究范围。核心判断上，论文本质是提升大语言模型的数学推理能力，属于增强LLM通用推理能力的范畴。论文提出ScaleDiff流程，通过高效生成困难数学问题来训练模型，从而提升LLM的数学推理能力，这是对LLM基础能力的改进，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确涉及Large Reasoning Models (LRMs)这一核心概念，并专注于Advanced Mathematical Reasoning这一推理能力方向。论文通过生成困难问题数据集并微调模型，实质上提出了一种新的训练范式来增强LLM的推理能力。 在排除标准方面，虽然论文专注于数学推理，但数学推理通常被视为评估和提升LLM通用推理能力的重要方面，而非特定应用领域。论文也不涉及多模态、视觉内容或特定应用领域如医疗、化学等。 论文的核心贡献是通过增加困难问题的数量来提升LLM的数学推理能力，并观察到\"随着困难问题数量增加，模型在困难基准测试上的性能呈现明显的扩展现象\"，这直接与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"相符。因此，这篇论文应被保留。", "summary2": "本文旨在解决大规模困难数学问题创建成本高的问题。针对数学推理模型的训练数据，我们提出了一种ScaleDiff pipeline，通过AdaptThink高效识别困难问题，训练专门的DiffGen-8B生成器大规模产生新困难问题，并应用规则和模型过滤蒸馏高质量解决方案。在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500等数学推理基准测试上，我们的方法实现了65.9%的平均准确率，比原始数据集提高了11.3%，并展现出随困难问题数量增加而性能提升的扩展现象。", "inspiration_trace": "## 面临的挑战\n大型推理模型需要训练在困难数学问题上才能展现复杂推理能力，但创建这类问题成本高昂，主要依赖人类专家。现有自动合成方法计算成本高、提示设计复杂，且生成问题难度有限，难以有效扩展。\n\n## 关键洞察\n困难问题比简单问题更能有效提升模型推理能力；问题难度可通过模型自适应思考机制高效评估；训练专门的困难问题生成器可避免复杂的逐例提示设计；小型教师模型也能有效进行知识蒸馏。\n\n## 解决方案演进\n首先利用AdaptThink模型高效识别现有数据集中的困难问题；然后基于这些困难问题训练专门的生成器DiffGen-8B；使用该生成器大规模创建新困难问题；再用小型教师模型蒸馏解决方案；最后通过规则和模型过滤构建高质量数据集进行模型微调。\n\n## 创新点总结\n提出了简单有效的困难问题扩展管道，利用自适应思考机制实现高效问题难度识别，通过专门生成器实现大规模困难问题自动化生成，证明了小型教师模型也能有效传递高级推理能力，并观察到模型性能随困难问题数量增加而明显扩展的现象。", "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在复杂问题解决方面展现了令人印象深刻的能力，通常受益于训练于能激发复杂推理的困难数学问题。近来的研究探索了通过提示（prompting）专有模型或大规模开源模型来自动合成数学问题的方法，这些模型基于种子数据（seed data）或固有的数学概念。然而，由于这些方法的高计算/API成本、提示的复杂性以及生成问题的难度水平有限，扩大这些方法的规模仍然具有挑战性。为了克服这些限制，我们提出了ScaleDiff，一个简单而有效的流程（pipeline），旨在扩大困难问题的创建规模。我们使用自适应思维模型（adaptive thinking model），仅通过单次前向传递（forward pass）就能高效地从现有数据集中识别困难问题，该模型能够感知问题难度并在\"Thinking\"和\"NoThinking\"模式之间自动切换。然后，我们在这个过滤后的困难数据上训练了一个专门的困难问题生成器（DiffGen-8B），它能够大规模生成新的困难问题，消除了对复杂的、每个实例的提示及其相关的高API成本的需求。在ScaleDiff-Math数据集上微调（fine-tuning）Qwen2.5-Math-7B-Instruct，相比原始数据集带来了11.3%的显著性能提升，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超越了像OpenThinker3这样的近期强大型推理模型。值得注意的是，这一性能是使用成本效益高的Qwen3-8B模型作为教师模型（teacher model）实现的，这表明我们的流程能够有效转移高级推理能力，而无需依赖更大、更昂贵的教师模型。此外，我们观察到随着困难问题数量的增加，模型在困难基准测试（benchmarks）上的性能呈现出明显的扩展现象（scaling phenomenon）。代码：https://github.com/QizhiPei/ScaleDiff。", "summary_generated_time": "2025-09-27 15:52:47", "summary_model": "z-ai/glm-4.5"}, {"index": "#80", "title": "DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?", "link": "/arxiv/2509.21016", "arxiv_id": "2509.21016", "authors": "Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song", "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding, a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? --and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.191214", "filter_reason": "这篇论文的核心贡献是提出了DELTA-Code基准测试，用于探索大语言模型(LLMs)是否能通过强化学习(RL)获取和迁移全新的推理策略。论文研究的是LLMs如何通过强化学习解决预训练时无法解决的问题，以及这些学到的技能如何迁移到分布外测试集。这完全符合研究\"大语言模型通用推理能力\"的目标，因为：(1)论文本质是改进LLM的基础推理能力，特别是算法推理能力，而不是将LLM作为工具应用到特定领域；(2)论文使用了强化学习这一训练范式来增强模型能力，探索了staged warm-up、experience replay、curriculum training等关键训练成分；(3)论文聚焦于推理能力的获取和迁移，这是通用推理能力的核心方面。论文符合多个正面指标，包括核心概念(LLMs)、能力方向(reasoning)和训练方法(RL)，同时不符合任何排除标准。因此，这篇论文应该被纳入研究范围。", "summary2": "本文旨在探究LLMs能否通过RL获取真正新颖的编程算法推理策略。针对预训练模型在编程任务上表现不佳的场景，我们提出了DELTA基准，一种受控的合成编码问题族，并在多个问题族上通过pass@K和full-pass rate验证了RL能够解锁新策略并实现泛化。实验揭示了grokking现象，证明适当的奖励设计可使模型从长期探索阶段突然跃升至接近完美准确率。", "inspiration_trace": "## 面临的挑战\n作者面临的核心问题是：LLMs通过强化学习仅能锐化已有技能，还是能真正获取全新推理策略？现有数据集混杂难以区分，且对pass@K=0任务（模型多次尝试仍失败），传统RL因缺乏正向信号而无法学习。\n\n## 关键洞察\n作者认识到需要受控基准测试系统探究LLMs的可学习性与可迁移性。编程问题是理想领域，因其可通过测试用例提供细粒度反馈作为密集奖励。关键洞察是：通过分阶段奖励设计（从密集到二元），可能触发\"grokking\"现象，表明RL能突破基础模型限制。\n\n## 解决方案演进\n作者先设计DELTA受控基准，包含合成问题家族。针对pass@K=0任务，提出分阶段训练：先用测试用例通过率作为密集奖励预热，帮助模型探索；再切换到二元完全通过奖励，精确锁定解决方案。这导致grokking现象出现——长时间探索后模型突然达到近乎完美准确率。\n\n## 创新点总结\n创新在于：创建能区分能力锐化与新知识获取的受控基准；发现并验证LLMs中的RL grokking现象；提出分阶段训练策略解决pass@K=0任务学习难题；系统评估学习策略在不同泛化轴上的迁移能力，揭示RL驱动推理的边界。", "summary_translation": "# 中文翻译\n\nLLMs（大语言模型）是否能够获取或泛化真正新颖的推理策略，而不仅仅是在预训练或后训练过程中编码在其参数中的已强化技能，这仍然是一个悬而未决的问题。为尝试回答这一争议，我们提出了DELTA-Code——算法编码中的可学习性与可迁移性的分布评估（Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding），这是一个受控的合成编码问题族基准，旨在探究两个基本方面：可学习性（learnability）——LLMs能否通过强化学习（Reinforcement Learning, RL）解决预训练模型在足够多次尝试下仍然失败的问题族（pass@K=0）？——以及可迁移性（transferrability）——如果实现了可学习性，这些技能能否系统地迁移到分布外（Out-of-Distribution, OOD）测试集？\n\n与先前的公共编码数据集不同，DELTA通过模板化问题生成器来隔离推理技能，并引入了完全分布外（OOD）的问题族，这些问题族需要新颖的策略而非工具调用或记忆模式。我们的实验揭示了一个显著的理解（grokking）相变：在经过一段接近零奖励的长时间后，经过强化学习训练的模型突然提升至接近完美的准确率。为了在先前无法解决的问题族上实现可学习性，我们探索了关键训练要素，如密集奖励的分阶段预热（staged warm-up）、经验回放（experience replay）、课程训练（curriculum training）和循环验证（verification-in-the-loop）。\n\n除了可学习性，我们还使用DELTA来评估探索性、组合性和变革性轴上的可迁移性或泛化能力，以及跨族迁移。结果显示，在问题族内部和重组技能方面有显著提升，但在变革性情况下仍存在持续弱点。因此，DELTA提供了一个清晰的测试平台，用于探究强化学习驱动推理的极限，并理解模型如何能够超越现有先验知识来获取新的算法技能。", "summary_generated_time": "2025-09-27 15:52:57", "summary_model": "z-ai/glm-4.5"}, {"index": "#81", "title": "Mechanism of Task-oriented Information Removal in In-context Learning", "link": "/arxiv/2509.21012", "arxiv_id": "2509.21012", "authors": "Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue", "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.191430", "filter_reason": "我按照筛选标准对这篇论文进行了全面分析： 第一步：核心判断——这篇论文的本质是研究In-context Learning (ICL)的内在工作机制，特别是从\"信息移除\"角度解析ICL如何帮助语言模型专注于目标任务。论文不是将LLM作为工具应用到特定领域，而是深入研究LLM本身的基础工作机制，属于\"改进LLM的基础能力\"的研究方向，符合核心判断标准。 第二步：正面指标——论文符合\"核心概念\"指标，因为它明确研究现代语言模型(LMs)的ICL机制。虽然论文没有直接讨论reasoning、planning等能力方向，也没有涉及reinforcement learning等训练方法或llm-based agents等新兴范式，但ICL本身是LLM的一种重要通用能力，与推理能力密切相关。 第三步：排除标准——论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 第四步：特殊和模糊情况——论文在可解释性方面有显著相关性，因为它揭示了ICL的\"面向任务的信息移除\"机制，这种对模型内在机制的深入理解有助于提升模型的通用可靠性和推理质量。 综合判断：这篇论文的核心贡献是揭示了ICL的工作机制，特别是\"面向任务的信息移除\"过程，这属于研究LLM基础能力的重要工作。理解ICL如何帮助模型从纠缠的信息中选择性移除冗余信息，对于提升LLM的通用推理能力具有重要意义。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决现有Induction Heads机制无法全面解释ICL行为的问题，特别是在unseen label scenario下的表现。针对多种LLMs和分类任务，我们提出了一种task-oriented information removal机制，认为demonstrations通过移除查询中与任务无关的信息来驱动LM输出。我们在多个数据集上通过ablation experiments验证了其有效性，并识别了负责此操作的denoising heads，证明了它们在未见标签场景下对维持分类准确性的关键作用。", "inspiration_trace": "## 面临的挑战\n现有ICL机制解释（特别是基于Induction Heads的框架）无法完全解释ICL的所有现象，尤其是在\"未见标签场景\"下的表现。Induction Heads认为ICL通过复制上下文中出现过的标签工作，但当正确标签未在演示中出现时，模型仍有一定表现，这与该框架预测相矛盾。\n\n## 关键洞察\n作者提出ICL本质可能是\"任务导向的信息移除\"：演示的作用是从查询的隐藏状态中移除与任务无关的信息，使模型能在指定任务上产生输出。零样本隐藏状态包含任务相关信息但也包含冗余信息，这些冗余信息会干扰输出。\n\n## 解决方案演进\n从洞察出发，作者先验证零样本隐藏状态中存在低秩子空间(TVS)，投影到该子空间可移除无关信息提高准确性；然后设计几何度量证明少样本ICL中模型自发执行这种信息移除；进而识别执行此操作的特定注意力头(denoising heads)，通过消融实验验证其在未见标签场景的关键作用。\n\n## 创新点总结\n从\"信息添加\"到\"信息移除\"的视角转换，为理解ICL提供全新解释框架；提出\"任务导向的信息移除\"作为ICL核心机制，补充Induction Heads框架不足；建立从现象观察到机制验证的完整研究路径。", "summary_translation": "In-context Learning (ICL)（情境学习）是一种基于现代语言模型（Language Models, LMs）的新兴小样本学习（few-shot learning）范式，但其内在机制仍不明确。在本文中，我们通过信息移除（information removal）这一新颖视角来探究其机制。具体而言，我们证明在零样本（zero-shot）场景下，语言模型将查询编码为隐藏状态（hidden states）中的非选择性表示（non-selective representations），这些表示包含所有可能任务的信息，导致模型无法专注于目标任务而产生任意输出，最终准确率接近于零。同时，我们发现通过低秩滤波器（low-rank filter）选择性地从隐藏状态中移除特定信息，能有效引导语言模型专注于目标任务。基于这些发现，通过精心设计的指标测量隐藏状态，我们观察到小样本ICL有效模拟了这种面向任务的信息移除过程，选择性地从纠缠的非选择性表示中移除冗余信息，并根据示例（demonstrations）改进输出，这构成了ICL的一个关键机制。此外，我们识别出引发移除操作的关键注意力头（attention heads），称之为去噪头（Denoising Heads），这使得我们能够进行消融实验（ablation experiments），在推理过程中阻断信息移除操作。实验结果表明，ICL准确率显著下降，特别是当小样本示例中缺少正确标签时，这既证实了信息移除机制的关键作用，也证实了去噪头的重要性。", "summary_generated_time": "2025-09-27 15:53:00", "summary_model": "z-ai/glm-4.5"}, {"index": "#78", "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems", "link": "/arxiv/2509.21054", "arxiv_id": "2509.21054", "authors": "Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu", "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.190683", "filter_reason": "这篇论文的核心贡献是研究大型语言模型(LLMs)和大型推理模型(LRMs)的推理过程如何影响其在多智能体系统中的说服动态。论文提出了\"说服二元性\"(Persuasion Duality)的概念，揭示了模型的底层认知过程，特别是其显式推理能力，如何决定其在多智能体交互中的说服行为。 根据筛选标准，这篇论文符合研究目标，原因如下： 1. 核心判断：论文的本质是研究LLM的基础能力——推理能力，而不是将LLM作为工具应用到特定领域。论文探索的是模型的底层认知过程和推理架构如何影响其外部行为，这属于对LLM通用能力的深入研究。 2. 正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs)和Large Reasoning Models (LRMs) - 能力方向：核心关注reasoning能力，特别是模型的\"thinking process\"和\"explicit reasoning\" - 新兴范式：研究Multi-Agent Systems (MAS)中LLMs的交互和协作 3. 排除标准：论文不主要聚焦于任何排除领域。虽然研究\"说服\"这一社会心理学概念，但论文不是将LLM应用到社会学领域，而是研究LLM本身的推理能力如何影响其在多智能体系统中的行为。 4. 特殊情况处理：论文研究的是通用多智能体系统框架中的推理问题，属于\"提出一种通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况，应该保留。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，它通过研究模型推理过程与说服动态的关系，增进了我们对LLM推理能力的理解，有助于未来设计具有更强推理能力的LLM和多智能体系统。", "summary2": "本文旨在解决多智能体系统中模型说服动态的关键影响因素问题。针对LLMs和LRMs在多智能体系统中的交互场景，我们提出了一种基于模型认知过程的说服机制分析方法，并在客观和主观数据集上通过Persuaded-Rate、Remain-Rate等指标验证了\"说服二元性\"理论，即显式推理过程同时增强模型说服力与抵抗说服能力，揭示了思考内容共享对说服效果的重要影响。", "inspiration_trace": "## 面临的挑战\n传统观点认为多智能体系统中模型的说服效果主要取决于规模，但作者发现模型规模与说服效果存在收益递减关系，单纯增大模型无法持续提升说服能力，这促使研究者寻找影响说服效果的本质因素。\n\n## 关键洞察\n作者洞察到说服动态主要取决于模型的底层认知过程而非规模，区分了基于隐式模式识别的LLMs和采用显式推理的LRMs，提出\"说服二元性\"概念：使论证更透明的机制也增强了对错误论证的抵抗力。\n\n## 解决方案演进\n从这一洞察出发，作者通过多智能体说服实验验证假设，发现LRMs的推理过程既增强抵抗说服能力，又通过共享思维内容显著提高说服他人的能力，进而分析影响因素并提出基于提示的缓解机制。\n\n## 创新点总结\n挑战了规模中心范式，提出认知过程中心的新视角；发现并形式化了\"说服二元性\"核心权衡关系；揭示了思维过程在多智能体说服中的双重作用，为设计更安全稳健的多智能体系统提供了新思路。", "summary_translation": "最近多智能体系统（Multi-Agent Systems, MAS）的快速普及，其中大型语言模型（Large Language Models, LLMs）和大型推理模型（Large Reasoning Models, LRMs）通常协作解决复杂问题，这 necessitates 对支配其交互的说服动态的深入理解。本文挑战了当前的主流假设，即说服效果主要取决于模型规模。我们提出，这些动态实际上是由模型的底层认知过程决定的，特别是其显式推理能力。通过一系列多智能体说服实验，我们发现了一个我们称之为\"说服二元性\"（Persuasion Duality）的基本权衡。我们的研究结果表明，大型推理模型（LRMs）中的推理过程表现出显著更强的抗说服性，更稳健地保持其初始信念。相反，通过共享\"思考内容\"使推理过程透明化，则显著提高了其说服他人的能力。我们进一步考虑了更复杂的传播说服情境，并揭示了多智能体网络间多跳说服中影响传播和衰减的复杂动态。本研究提供了将模型内部处理架构与其外部说服行为联系起来的系统性证据，为高级模型的易受影响性提供了新的解释，并强调了未来多智能体系统（MAS）的安全性、稳健性和设计的关键影响。", "summary_generated_time": "2025-09-27 15:52:49", "summary_model": "z-ai/glm-4.5"}, {"index": "#84", "title": "On Theoretical Interpretations of Concept-Based In-Context Learning", "link": "/arxiv/2509.20882", "arxiv_id": "2509.20882", "authors": "Huaze Tang, Tianren Peng, Shao-lun Huang", "summary": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.", "subjects": "Information Theory, Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.192126", "filter_reason": "这篇论文的核心是对大语言模型的上下文学习(ICL)能力进行理论解释，特别是研究基于概念的上下文学习(CB-ICL)方法。上下文学习是大语言模型的一种重要通用推理能力，它允许模型从少量示例中学习并应用到新问题上，这本质上是一种推理和问题解决过程。论文提出了理论分析，解释了CB-ICL在少量示例情况下表现良好的原因，量化了LLMs可以利用的知识，并提出了相似性度量方法，这些都有助于深入理解LLM的推理机制。虽然论文没有涉及强化学习、智能体协作等新兴范式，但它聚焦于LLM的基础能力研究，特别是对通用推理能力的理论解释，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文不涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。因此，这篇论文应该被保留。", "summary2": "本文旨在解释上下文学习(ICL)机制的理论基础。针对仅有少量示例的ICL任务，我们提出了一种基于概念的上下文学习(CB-ICL)方法，通过预训练LLM提取语义嵌入并学习概念向量来预测查询标签。在MMLU、GPQA等基准测试上通过准确率等指标验证了其有效性，并提出了提示示例与查询输入间的相似性度量，为模型预训练和提示工程提供了理论指导。", "inspiration_trace": "## 面临的挑战\n上下文学习(ICL)已成为大语言模型的重要范式，但其理论机制理解有限，特别是为何ICL能在仅有少量示例时表现良好，以及如何量化LLM利用的知识，这些问题缺乏理论解释。\n\n## 关键洞察\n作者将ICL问题转化为\"基于概念的上下文学习\"(CB-ICL)框架，核心洞察是：将LLM语义知识视为子空间投影，提示中的概念表示为向量α，ICL性能取决于两个关键因素——LLM嵌入是否完整捕获提示语义概念，以及输入文本与标签间相关性强度。\n\n## 解决方案演进\n从洞察到方案的演进：首先构建理论模型，将真实分布表示为LLM嵌入函数的线性组合加残差；然后设计提示概念提取器估计概念向量；接着建立理论分析框架，推导风险上界并连接标签预测错误概率；最后提出相似性度量量化提示示例与查询输入间关系。\n\n## 创新点总结\n创新点在于首次建立CB-ICL理论框架解释ICL机制，提出可量化相似性度量为提示工程提供理论指导，揭示嵌入维度等因素对ICL性能影响，并连接理论分析与实际应用。", "summary_translation": "上下文学习 (In-Context Learning, ICL) 已成为自然语言处理和大型语言模型 (Large Language Model, LLM) 应用中的一个重要新范式。然而，对 ICL 机制的理论理解仍然有限。本文旨在通过研究一种特定的 ICL 方法——基于概念的 ICL (concept-based ICL, CB-ICL) 来探讨这一问题。具体而言，我们提出了将 CB-ICL 应用于 ICL 任务的理论分析，解释了为什么以及在什么情况下 CB-ICL 能够在仅有少量示例 (demonstrations) 的提示 (prompts) 中有效预测查询标签 (query labels)。此外，所提出的理论量化了 LLM 可用于提示任务的知识，并提出了提示示例与查询输入之间的相似性度量，为 ICL 中的模型预训练 (pre-training) 和提示工程 (prompt engineering) 提供了重要的见解和指导。此外，基于所提出的理论，我们还探讨了提示示例大小和 LLM 嵌入 (embeddings) 维度对 ICL 的影响。最后，我们进行了多项真实数据实验，以验证 CB-ICL 及其相应理论的实用性。", "summary_generated_time": "2025-09-27 15:52:49", "summary_model": "z-ai/glm-4.5"}, {"index": "#89", "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning", "link": "/arxiv/2509.20712", "arxiv_id": "2509.20712", "authors": "Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou", "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.193213", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种新的强化学习算法CE-GPPO，用于改进大语言模型的基础推理能力。论文明确指出其目标是优化LLM处理\"复杂推理任务\"的能力，并通过改进PPO算法的梯度处理机制来提升模型性能，这直接属于改进LLM基础能力和通用推理能力的范畴。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确研究\"large language models (LLMs)\"；能力方向上聚焦于\"complex reasoning tasks\"和\"mathematical reasoning\"；训练方法上提出了新的强化学习优化算法(CE-GPPO)。这些都是高度相关的指标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，没有将LLM应用于特定领域（数学推理仅作为评估通用推理能力的基准），也没有涉及模型基础设施或应用层面的可靠性问题。 论文的核心贡献是提出了一种新的强化学习训练范式，通过改进梯度处理机制来优化LLM的推理能力，这直接符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。", "summary2": "本文旨在解决强化学习优化大型语言模型中的策略熵管理问题。针对PPO及其变体因裁剪机制丢弃低概率token梯度信号的问题，我们提出了一种CE-GPPO算法，通过重新引入被裁剪token的梯度并控制其幅度来平衡探索与利用。在多个数学推理基准上，通过准确率等指标验证了该方法在不同模型规模上的有效性。", "inspiration_trace": "## 面临的挑战\n强化学习优化大型语言模型时，管理策略熵(探索与利用的平衡)是核心挑战。现有PPO等方法因裁剪机制丢弃低概率token的梯度信号，导致熵不稳定，表现为熵崩溃或熵爆炸。\n\n## 关键洞察\n作者通过分析熵动态，发现被裁剪token在调节熵演化中扮演关键角色。他们将token分为四类，特别揭示PA&LP(促进探索)和NA&LP(促进利用)两类被裁剪低概率token的重要性，并建立熵变化与log概率和优势函数协方差的理论联系。\n\n## 解决方案演进\n基于此洞察，作者提出CE-GPPO算法，通过stop-gradient操作解耦前向和后向传递，重新引入被裁剪token的梯度。设计参数β1和β2分别控制左右裁剪边界外梯度的缩放，实现对熵动态的精细调节，平衡探索与利用。\n\n## 创新点总结\n创新在于首次揭示被裁剪token对熵动态的关键作用，提出梯度保留裁剪机制，通过参数调节实现训练过程中探索-利用的动态平衡，既提高性能又保持稳定性。", "summary_translation": "强化学习 (Reinforcement learning, RL) 已成为一种强大的范式，用于优化大语言模型 (large language models, LLMs) 以处理复杂推理任务。这一过程中的核心挑战在于管理策略熵 (policy entropy)，它反映了训练过程中探索 (exploration) 与利用 (exploitation) 之间的平衡。现有方法，如近端策略优化 (proximal policy optimization, PPO) 及其变体，由于裁剪机制 (clipping mechanism) 而丢弃了来自低概率词元 (tokens) 的有价值梯度信号。我们系统地分析了熵动态变化 (entropy dynamics)，并揭示这些被裁剪的词元在调节熵演化过程 (entropy evolution) 中扮演着关键但被忽视的角色。我们提出了基于梯度保留策略优化的熵控制算法 (Controlling Entropy via Gradient-Preserving Policy Optimization, CE-GPPO)，这是一种新颖的算法，它以一种温和且有界的方式重新引入原始 PPO 中被裁剪词元的梯度。通过控制来自裁剪区间外词元的梯度大小，CE-GPPO 能够实现探索-利用权衡 (exploration-exploitation trade-off)。我们提供了理论依据和实证证据，表明 CE-GPPO 有效缓解了熵不稳定性 (entropy instability)。在数学推理基准 (mathematical reasoning benchmarks) 上的大量实验表明，CE-GPPO 在不同模型规模上始终优于强基线方法。", "summary_generated_time": "2025-09-27 15:52:47", "summary_model": "z-ai/glm-4.5"}, {"index": "#85", "title": "StyleBench: Evaluating thinking styles in Large Language Models", "link": "/arxiv/2509.20868", "arxiv_id": "2509.20868", "authors": "Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei", "summary": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-25", "category": "cs.CL", "crawl_time": "2025-09-26T21:01:55.192332", "filter_reason": "这篇论文的核心贡献是提出了StyleBench基准测试，用于系统评估不同推理风格（如CoT、ToT、AoT等）对大语言模型性能的影响。论文直接关注LLM的通用推理能力，研究不同思考风格如何影响模型在各种推理任务上的表现。这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文没有将LLM作为工具应用到特定领域，也没有关注模型基础设施或部署优化。相反，它通过大规模分析揭示了推理风格、模型规模和任务类型之间的复杂关系，为选择最优推理策略提供了指导，这对提升LLM的基础推理能力具有重要价值。论文涉及的核心概念（LLMs）和能力方向（reasoning）进一步确认了它与我的研究目标高度相关。因此，这篇论文应该被保留。", "summary2": "本文旨在评估大型语言模型中不同推理风格的效果并解决最优推理策略选择问题。针对多样化任务和模型，我们提出了StyleBench基准测试，系统评估了CoT、ToT、AoT、SoT和CoD五种推理风格，并在15个开源模型(270M-120B参数)和5种推理任务上通过准确率、效率和token消耗等指标验证了其有效性。", "inspiration_trace": "## 面临的挑战\n大型语言模型的有效性高度依赖推理策略，但推理风格、模型架构与任务类型间的相互作用理解不足。现有评估局限于单一风格、狭窄任务集或少量模型，且缺乏对推理深度与效率权衡的系统研究。\n\n## 关键洞察\n作者认识到没有单一推理风格普遍最优，策略有效性高度依赖模型规模和任务类型。推理能力是模型规模的函数，不同任务与特定推理风格存在强相关性：结构化多步推理适合数学任务，分支探索更适合开放性谜题。\n\n## 解决方案演进\n从系统性评估需求出发，作者构建了全面基准测试，选取五种代表性推理风格和任务，覆盖15个不同规模模型。通过结构化框架匹配任务类型与推理处理方式，实验分析揭示了模型规模、任务类型与推理策略间的复杂相互作用。\n\n## 创新点总结\n首次系统评估多种推理风格在不同场景表现，揭示三者间复杂关系，挑战\"一刀切\"推理方法，提供基于约束条件选择最优策略的实用路线图，发现大小模型推理行为的根本差异。", "summary_translation": "大型语言模型（Large Language Models, LLMs）的效果很大程度上受到其提示中采用的推理策略或思维风格的影响。然而，这些推理风格、模型架构和任务类型之间的相互作用仍未被充分理解。为解决这一问题，我们引入了StyleBench，这是一个用于在不同任务和模型上系统评估推理风格的全面基准测试（benchmark）。我们评估了五种代表性推理风格，包括思维链（Chain of Thought, CoT）、思维树（Tree of Thought, ToT）、思维算法（Algorithm of Thought, AoT）、思维草图（Sketch of Thought, SoT）和草稿链（Chain-of-Draft, CoD），在五个推理任务上使用了来自主要模型系列（LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, 和 DeepSeek）的15个开源模型，参数规模从270M到120B不等。我们的大规模分析表明，没有一种风格是普遍最优的。我们证明，策略效能高度依赖于模型规模和任务类型：基于搜索的方法（AoT, ToT）在开放性问题中表现出色，但需要大规模模型；而简洁风格（SoT, CoD）在明确定义的任务上实现了显著的效率提升。此外，我们识别出关键的行为模式：较小的模型经常无法遵循输出指令并默认为猜测，而推理鲁棒性（robustness）则作为规模的函数而出现。我们的研究结果为根据特定约束条件选择最佳推理策略提供了关键路线图，我们在https://github.com/JamesJunyuGuo/Style_Bench开源了该基准测试。", "summary_generated_time": "2025-09-27 15:52:54", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Machine Learning", "count": 4, "papers": [{"index": "#18", "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say", "link": "/arxiv/2509.21164", "arxiv_id": "2509.21164", "authors": "Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna", "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.", "subjects": "Machine Learning", "date": "2025-09-25", "category": "cs.LG", "crawl_time": "2025-09-26T21:01:55.405083", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Mixture of Thoughts (MoT)\"的新方法，用于在不同专业领域（如数学、代码、通用推理）的开源大语言模型之间进行潜在层面的协作。该方法通过一个轻量级路由器选择顶级专家，并在共享潜在空间中通过交叉注意力机制进行协作，从而提升整体推理性能。这符合研究目标，因为：(1) 论文的核心是改进LLM的基础能力，特别是通用推理能力，而不是将LLM作为工具应用到特定领域；(2) 论文涉及多个正面指标，包括大语言模型、推理能力（数学和通用推理）以及多智能体系统；(3) 论文提出了一种通用的多LLM协作框架，用于增强LLM的通用问题解决能力，而非针对特定应用领域；(4) 实验结果表明，MoT在多个基准测试上超过了当前最先进的方法，有效提升了LLM的推理能力。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决多LLM集成方法中存在的协作粒度粗和计算成本高的问题。针对异构专家模型的协作需求，我们提出了一种Mixture of Thoughts (MoT)方法，通过全局路由和潜在空间交互层实现专家间的细粒度协作，并在五个分布内和三个分布外基准测试上通过准确率等指标验证了其有效性，超越了当前最先进方法。", "inspiration_trace": "## 面临的挑战\n现有多LLM协作方法存在明显局限：路由方法只能选择专家独立生成，缺乏真正协作；响应级协作需多轮交互，计算成本高；参数融合要求架构同质性且失去查询适应性。这些方法都只关注模型\"说什么\"而非\"想什么\"，无法实现细粒度的潜在空间交互。\n\n## 关键洞察\n作者认识到，真正的多模型协作需要在潜在空间层面进行交互，而不仅限于输出聚合。通过整合模型中间表示而非最终输出，可以让不同专家的优势在推理过程中动态结合，实现更深入的知识整合。\n\n## 解决方案演进\n基于此洞察，作者设计MoT：先通过全局路由器选择相关专家；再在模型中均匀放置交互层，将隐藏状态投影到共享空间；指定主要专家通过交叉注意力整合其他专家表示；保持专家冻结，仅训练路由器和轻量级交互层，实现单次推理的高效协作。\n\n## 创新点总结\n创新点在于首次实现异构LLM潜在空间的细粒度协作，通过轻量级交互层在保持专业化的同时实现动态查询自适应协作，支持完全异构模型，以单次推理成本实现了多专家深度融合。", "summary_translation": "开源大型语言模型（Large Language Models, LLMs，大型语言模型）越来越多地按领域（如数学、代码、通用推理）进行专业化，这推动了利用模型间互补优势的系统发展。先前的多LLM方法要么（i）将查询路由（route）到一个或几个专家并独立生成，（ii）通过昂贵的多轮交换聚合（aggregate）每个模型的输出，或（iii）将权重融合（fuse）到单个模型中——通常需要架构同质性（architectural homogeneity）。\n\n我们提出了思维混合（Mixture of Thoughts, MoT），一种在全局路由方案（global routing scheme）下异构专家（heterogeneous experts）之间进行潜在级别（latent-level）协作的简单方法。对于每个查询，轻量级路由器（lightweight router）选择前$K$个专家并指定一个主要专家；均匀放置的交互层（interaction layers）将隐藏状态（hidden states）投影到共享潜在空间（shared latent space），其中主要专家对其活动（选定）的同行执行交叉注意力（cross-attention）。\n\n预训练的专家保持冻结（frozen）状态；只有路由器和轻量级交互层通过新颖的联合训练目标（joint training objective）进行训练，该目标同时改进专家选择和专家间协作。在五个分布内（in-distribution, ID）和三个分布外（out-of-distribution, OOD）基准测试中，MoT分别以$+0.38\\%$和$+2.92\\%$的优势超越了当前基于路由和聚合的最先进方法（state-of-the-art）Avengers。此外，MoT显著优于表现最佳的单个模型。它通过单次推理（single-pass inference）实现这一点，运行时间与路由基线（routing baselines）相当，且没有迭代聚合（iterative aggregation）的开销。\n\nMoT提供了一种简单的潜在空间机制（latent-space mechanism）来组合异构LLMs，这是迈向更广泛多LLM协作的实用一步。我们的代码在https://github.com/jacobfa/mot上公开可用。", "summary_generated_time": "2025-09-27 15:53:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#83", "title": "Theoretical Bounds for Stable In-Context Learning", "link": "/arxiv/2509.20677", "arxiv_id": "2509.20677", "authors": "Tongxi Wang, Zhuoyang Xia", "summary": "In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.", "subjects": "Machine Learning", "date": "2025-09-25", "category": "cs.LG", "crawl_time": "2025-09-26T21:01:55.418330", "filter_reason": "这篇论文的核心贡献是研究大语言模型的上下文学习(ICL)能力的稳定性问题，建立了理论界限并提出了估计提示长度的方法。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是LLM的一个基础能力——上下文学习(ICL)的稳定性，而非将LLM作为工具应用到特定领域。ICL是LLM的核心能力之一，论文通过建立非渐近下界和提出两阶段可观测估计器，旨在提高这一基础能力的可靠性和稳定性，这属于改进LLM基础能力的研究范畴。 其次，论文涉及LLM的核心概念(ICL)，虽然未直接提及reasoning、planning等能力方向，但ICL本身与这些通用能力密切相关，因为它是模型适应新任务并进行推理的基础机制。 第三，论文不涉及任何需要排除的领域：没有关注多模态与视觉问题，没有将LLM应用到医疗、化学等特定领域，也没有从应用层面研究模型可靠性问题（如水印、安全等）。 最后，虽然论文关注了ICL的可靠性，但这是从理论角度研究LLM基础能力的稳定性，而非应用层面的可靠性问题，因此不应被排除。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（通过增强ICL稳定性），符合研究目标。", "summary2": "本文旨在解决稳定In-context Learning (ICL)所需的最小示例数量问题。针对固定高维亚高斯表示的场景，我们提出了一种基于谱特性的非渐近下界理论，并设计了一个两阶段可观测估计器，在多个数据集、编码器和生成器上通过理论预测与经验拐点(knee-points)的对齐程度验证了其有效性，实验表明该理论作为保守但可靠的上界能准确预测ICL稳定性阈值。", "inspiration_trace": "## 面临的挑战\nICL可靠性高度敏感于提示长度，实践中不同任务和模型的稳定性阈值差异大，现有理论无法提供非渐近、可计算的下界来直接连接提示长度与稳定性。\n\n## 关键洞察\n作者将ICL稳定性与特征表示协方差矩阵的最小特征值λmin(Σ̂K)联系起来，认识到谱覆盖与稳定ICL间存在根本联系，预测方差与最小特征值成反比关系。\n\n## 解决方案演进\n从矩阵伯恩斯坦不等式出发，建立非渐近下界；针对理论界限依赖未知谱特性的问题，设计两阶段可观测估计器；进一步通过q-分位数特征值替代和全局缩放因子校准，缩小理论与实践差距。\n\n## 创新点总结\n首次建立非渐近理论下界量化示例数量与ICL稳定性关系；设计无需分布先验的可观测两阶段算法；通过谱特性连接理论与实践，提高大规模提示的可解释性和可靠性。", "summary_translation": "In-context learning (ICL，上下文学习)具有灵活性，但其可靠性对提示词长度高度敏感。本文建立了一个非渐近下界，该下界在固定的高维sub-Gaussian representations（亚高斯表示）下，将最少的示例数量与ICL稳定性联系起来。该下界根据协方差的谱性质给出了明确的充分条件，为实践提供了可计算的标准。基于这一分析，我们提出了一种带有一次性校准的两阶段observable estimator（可观测估计器），该估计器无需distributional priors（分布先验）即可生成可供实践者使用的提示词长度估计。在不同数据集、编码器和生成器上的实验表明，预测阈值与empirical knee-points（经验拐点）之间存在密切的一致性，其中理论作为一个保守但可靠的上界；校准变体进一步缩小了这一差距。这些结果将spectral coverage（谱覆盖）与稳定的ICL联系起来，架起了理论与实践部署之间的桥梁，并提高了在现实finite-sample regimes（有限样本条件）下大规模提示的可解释性和可靠性。", "summary_generated_time": "2025-09-27 15:53:12", "summary_model": "z-ai/glm-4.5"}, {"index": "#88", "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "link": "/arxiv/2509.20616", "arxiv_id": "2509.20616", "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.", "subjects": "Machine Learning, Systems and Control", "date": "2025-09-24", "category": "cs.LG", "crawl_time": "2025-09-26T21:01:55.419365", "filter_reason": "这篇论文完全符合研究目标。从核心判断来看，论文的本质是关于改进LLM的基础能力，特别是提出了一种新的训练范式（单轮强化学习）来增强LLM的任务规划和多步推理能力。论文的核心贡献是将复杂的多轮任务规划转化为单轮任务推理问题，并通过Group Relative Policy Optimization (GRPO)进行高效策略优化，这直接提升了LLM的通用推理能力。 从正面指标看，论文明确包含了多个关键主题：Large Language Models (LLMs)、reasoning、task planning、reinforcement learning以及LLM agents，这些都与研究目标高度一致。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等内容，因此不应被排除。 在特殊和模糊情况处理方面，论文提出的是一种通用的智能体框架来增强LLM的任务规划能力，而不是将智能体应用于特定领域，因此符合保留标准。 综上所述，这篇论文直接致力于提高大语言模型本身的通用推理能力，特别是在任务规划和多步推理方面，与研究目标完全一致。", "summary2": "本文旨在解决LLM代理在多轮任务规划中面临的稀疏奖励、信用分配和计算开销问题。针对复杂的多轮任务规划场景，我们提出了一种将多轮任务规划转化为单轮任务推理问题，并通过基于专家轨迹的GRPO进行高效策略优化的方法。在Robotouille基准测试上通过成功率、平均步数等指标验证了其有效性，1.5B参数模型在长时程规划任务上达到70%成功率，优于14B参数基线模型，并展现了从复杂任务到简单子任务的强泛化能力。", "inspiration_trace": "## 面临的挑战\n多轮任务规划面临三大核心挑战：任务完成奖励过于稀疏，难以有效利用LLM策略；长时程规划中信用分配困难，无法确定哪些行动对最终结果有贡献；多轮RL计算复杂度随序列长度增长，不适合需要数十轮决策的复杂任务训练。\n\n## 关键洞察\n作者洞察到复杂多轮任务规划可分解为一系列单轮任务推理问题，每一步只需根据当前状态选择最优行动。这种分解使利用专家轨迹提供的密集可验证奖励进行高效策略优化成为可能，且单轮推理能力提升可转化为多轮规划成功概率提高。\n\n## 解决方案演进\n作者首先将多轮MDP问题重构为基于专家轨迹的单轮MDP问题，然后应用GRPO方法在单轮任务推理上优化。通过理论分析证明单轮改进能提高多轮规划成功概率，最后通过实验验证方法有效性和跨任务泛化能力。\n\n## 创新点总结\n创新点在于将多轮交互分解为单轮决策，绕过稀疏奖励和信用分配难题；利用专家轨迹提供密集可验证奖励；建立单轮与多轮任务间的理论联系；展示从复杂任务到简单子任务的强泛化能力。", "summary_translation": "大型语言模型（Large Language Models, LLMs）在知识获取、推理和工具使用方面已展现出卓越能力，使其成为自主智能体（autonomous agent）应用的有力候选者。然而，针对复杂多轮任务规划训练LLM智能体面临重大挑战，包括稀疏的逐幕奖励（sparse episode-wise rewards）、长跨度信用分配（credit assignment across long horizons）以及多轮交互设置中强化学习的计算开销（computational overhead）。为此，本文提出了一种新颖方法，将多轮任务规划转化为单轮任务推理问题，通过群体相对策略优化（Group Relative Policy Optimization, GRPO）实现高效策略优化，该方法利用来自专家轨迹的密集且可验证的奖励（dense and verifiable reward）。我们的理论分析表明，GRPO对单轮任务推理的改进能够在最小轮次下提高多轮任务的成功概率，同时能够泛化到跨度更短的子任务。在复杂任务规划基准上的实验评估表明，我们使用单轮GRPO训练的15亿参数模型相比高达140亿参数的更大基线模型取得了优越性能，在超过30步的长跨度规划任务上达到了70%的成功率。我们还从理论和实证两方面验证了强大的跨任务泛化能力，即在复杂任务上训练的模型能够成功完成所有更简单的子任务。", "summary_generated_time": "2025-09-27 15:53:16", "summary_model": "z-ai/glm-4.5"}, {"index": "#135", "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training", "link": "/arxiv/2509.21009", "arxiv_id": "2509.21009", "authors": "Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang", "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.", "subjects": "Distributed, Parallel, and Cluster Computing, Machine Learning", "date": "2025-09-25", "category": "cs.LG", "crawl_time": "2025-09-26T21:01:55.428818", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"tail batching\"的新型rollout调度策略和RollPacker系统，用于优化强化学习(RL)作为大语言模型(LLM)后训练技术的效率和性能。论文明确指出RL是\"enhancing the reasoning capabilities of Large Language Models (LLMs)\"的关键技术，这与研究目标\"提高大语言模型（LLM）本身的『通用推理能力』\"直接相关。论文不是将LLM作为工具应用到特定领域，而是专注于改进LLM的基础训练方法，特别是强化学习这一提升LLM推理能力的关键技术。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。因此，这篇论文完全符合研究范围。", "summary2": "本文旨在解决同步RL后训练中因响应长度不平衡导致的GPU利用率低下问题。针对LLMs的rollout阶段长尾响应分布，我们提出了tail batching调度策略，将长响应集中到少数long rounds而保持大多数rounds响应平衡。在Qwen2.5系列模型上通过最多128个H800 GPU的实验验证，相比veRL实现了2.03×-2.56×的端到端训练时间减少，相比RLH-Fuse实现了高达2.24×的加速，同时保持训练精度不变。", "inspiration_trace": "## 面临的挑战\n同步RL后训练中存在严重的GPU利用率低下问题，主要由rollout阶段响应长度不平衡导致的长尾分布引起。现有解决方案要么只能提供有限改进（通过阶段重叠），要么牺牲训练准确性（通过放松同步约束）。\n\n## 关键洞察\n作者认识到在rollout批次中，只有一小部分提示会产生特别长的响应，拖慢整个批次。通过重新排序训练样本，将这些长尾提示集中到少数rollout步骤中，可以在不改变样本分布或放松同步的情况下解决问题。\n\n## 解决方案演进\n从这一洞察出发，作者提出\"tail batching\"策略，将长响应提示集中到专门的\"long rounds\"。为解决批量大小问题，引入投机执行机制；为确保所有提示被处理，设计长提示队列。进一步优化系统，设计三个组件分别优化rollout、reward和training阶段。\n\n## 创新点总结\n创新点在于首次通过重新排序训练样本来解决长尾rollout问题，在保持同步RL训练语义的同时显著提高GPU利用率，并提供系统级解决方案全面优化RL训练的三个阶段。", "summary_translation": "强化学习（Reinforcement Learning, RL）是一种增强大型语言模型（Large Language Models, LLMs）推理能力的关键后训练技术。然而，同步强化学习后训练常常面临严重的图形处理器（GPU）利用不足问题，这种现象被称为\"气泡\"（bubbles），是由推进步骤（rollout steps）中不平衡的响应长度所导致的。许多强化学习系统试图通过放松同步来缓解这一问题，但这可能会损害训练准确性。在本文中，我们提出了尾部分批（tail batching），这是一种针对同步强化学习的新型推进步骤调度策略，它系统性地将导致长尾响应的提示整合到一小部分推进步骤（长轮次，long rounds）中，同时确保大多数步骤（短轮次，short rounds）仅涉及平衡的短推进步骤。通过将长响应从短轮次中排除并重新调度到少数指定的长轮次中，尾部分批有效减少了推进步骤期间的GPU空闲时间，在不牺牲准确性的前提下显著加速了强化学习训练。我们提出了RollPacker系统，该系统通过在所有三个强化学习阶段进行整体优化来充分利用尾部分批的优势：推进步骤的弹性并行（elastic parallelism）适应、奖励（reward）的动态资源分配和调度，以及基于流的训练（stream-based training）。实证结果表明，对于Qwen2.5系列大型语言模型，在多达128个H800图形处理器上，RollPacker相比veRL实现了2.03倍至2.56倍的端到端训练时间减少，相比RLHFuse实现了高达2.24倍的加速。", "summary_generated_time": "2025-09-27 15:53:19", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Multiagent Systems", "count": 1, "papers": [{"index": "#4", "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "link": "/arxiv/2509.21134", "arxiv_id": "2509.21134", "authors": "Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng", "summary": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.", "subjects": "Artificial Intelligence, Multiagent Systems", "date": "2025-09-25", "category": "cs.MA", "crawl_time": "2025-09-26T21:01:54.501280", "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提升大语言模型本身的通用推理能力。从核心判断来看，论文的本质是提出ToMPO算法来增强LLM的战略决策能力，这是一种新的训练范式，属于改进LLM基础能力的研究。论文明确关注LLM在复杂场景中的推理和决策能力，需要\"深入思考、逻辑推理和明智决策\"，这正是通用推理能力的核心要素。 从正面指标看，论文包含了多个相关主题：核心概念是LLMs；能力方向涉及reasoning和strategic decision-making；训练方法采用了reinforcement learning（ToMPO算法）；新兴范式方面则从multi-agent perspective研究问题。 论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。虽然论文从多智能体角度研究，但它是提出一种通用的框架来增强LLM的战略决策能力，而不是将智能体应用在特定领域。 论文的核心贡献是ToMPO算法，通过基于推理其他个体策略生成rollouts、在图级和样本级估计优势、平衡全局和部分奖励来增强LLM的战略决策能力，这直接提升了LLM的通用推理和决策能力，完全符合我的研究目标。", "summary2": "本文旨在提升大型语言模型在复杂多智能体环境中的战略决策能力。针对包含图级别和努力级别两种决策类型及其时间依赖性的多智能体场景，我们提出了一种Theory of Mind Policy Optimization (ToMPO)算法，并在BCZ和PGG游戏环境上通过U1(合规性)、U2(战略效率)和U3(合作结果)指标验证了其有效性，较GRPO方法提升35%，且优于参数大100倍的模型18%。", "inspiration_trace": "## 面临的挑战\n现有LLM在复杂多智能体环境中进行战略决策时存在明显不足：仅关注社交对话或单一场景，忽略决策类型间的相互依赖性；强化学习方法难以考虑他人策略；模型在理解他人意图、预测行为并动态调整策略方面能力有限。\n\n## 关键洞察\n作者认识到战略决策本质上是双重决策过程——图级决策(社会关系结构)和努力级决策(资源投入)，两者存在时间依赖性。有效的决策需要具备\"心理理论\"能力，即理解他人策略和意图，类似于人类社交推理。\n\n## 解决方案演进\n从问题定义出发，通过初步测试发现模型不足，进而生成专家数据训练基础推理能力。然后提出ToMPO算法，将多智能体视角融入策略训练：通过推理他人策略生成决策路径，在图级和样本级估计优势，平衡全局与局部奖励，使模型能考虑他人策略并动态调整自身决策。\n\n## 创新点总结\n创新在于将心理理论与强化学习结合，从单一智能体视角转向多智能体互动视角；同时优化双重决策及其时间依赖性；通过多层次优势估计平衡局部精度与全局最优，使LLM在复杂社会环境中具备类似人类的战略推理能力。", "summary_translation": "大型语言模型（Large Language Models, LLMs）已被用于在复杂场景中做出决策，这些场景需要模型进行深度思考、逻辑推理和明智决策。许多现有研究仅关注社交任务或模拟环境中的多轮对话，而忽略了各种类型的决策及其相互依赖性。当前的强化学习方法（reinforcement learning methods）在训练过程中难以考虑他人的策略。为解决这些问题，我们首先定义了一个包含两种决策类型及其时间依赖性的战略决策问题。此外，我们提出了心智理论策略优化（Theory of Mind Policy Optimization, ToMPO）算法，以优化对其他个体策略和游戏局势趋势的感知。与群体相对策略优化（Group Relative Policy Optimization, GRPO）算法相比，ToMPO主要通过以下方式增强LLM的战略决策能力：1）基于对其他个体策略的推理生成推演（rollouts），2）在图级（graph-level）和样本级（sample-level）估计优势，以及3）平衡全局和部分奖励。在模型输出合规性和合作结果方面，ToMPO算法比GRPO方法高出35%。此外，与参数量大100倍的模型相比，它显示出18%的改进。这证明了ToMPO算法在增强模型战略决策能力方面的有效性。", "summary_generated_time": "2025-09-27 15:52:24", "summary_model": "z-ai/glm-4.5"}]}], "overview": ""}