{"date": "2025-12-10", "categories": [{"name": "Artificial Intelligence", "count": 5, "papers": [{"index": "#4", "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "link": "/arxiv/2512.09882", "arxiv_id": "2512.09882", "authors": "Justin W. Lin, Eliot Krzysztof Jones, Donovan Julian Jasper, Ethan Jun-shen Ho, Anna Wu, Arnold Tianyi Yang, Neil Perry, Andy Zou, Matt Fredrikson, J. Zico Kolter, Percy Liang, Dan Boneh, Daniel E. Ho", "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.", "subjects": "Artificial Intelligence, Cryptography and Security, Computers and Society", "date": "2025-12-10", "category": "cs.AI", "crawl_time": "2025-12-11T11:00:06.173077", "filter_reason": "这篇论文符合您的研究范围，核心判断依据如下： 1.  **第一步：核心判断——保留** 论文的核心贡献并非简单地将现有AI智能体应用于网络安全领域，而是**提出并构建了一个全新的多智能体框架ARTEMIS**。摘要明确指出“ARTEMIS, our new agent scaffold”并将其描述为“a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging”。这表明论文的本质是关于**构建和改进LLM智能体**的方法论，直接命中了您“构建、改进或演化LLM智能体”的核心目标。它不属于“非演化型应用”的排除范畴，因为其价值在于框架本身的设计，而非应用结果。 2.  **第二步：正面指标——高度相关** 论文包含了多个您关注的核心范式和能力： *   **核心范式**: 明确提到了 `Multi-Agent Systems (MAS)`，这是您研究的三大方向之一。 *   **智能体能力**: “dynamic prompt generation”和“automatic vulnerability triaging”暗示了智能体具备复杂的规划和自我修正能力。“parallel exploitation”则体现了工具使用和任务协调能力。 *   **多智能体**: “multi-agent framework”和“arbitrary sub-agents”直接指向了智能体间的协作与分工。 3.  **第三步：排除标准——未触发** 尽管论文主题是“网络安全”，但其主要贡献是关于**如何构建一个能执行渗透测试任务的智能体框架**，而不是研究智能体自身的`Safety`、`Security`或`Alignment`问题。因此，它没有触发“安全与对齐”的排除标准。论文也未涉及多模态或视觉内容。 4.  **第四步：处理特殊和模糊情况——符合核心规则** 这篇论文的情况与“自我演化的应用”的例外规则精神一致。虽然它被应用在“渗透测试”这一特定领域，但其**核心贡献是提出一种新的多智能体框架（方法论）**，而不仅仅是展示应用效果。研究的重点在于“如何构建ARTEMIS”，这正是您所关注的“构建、改进或演化LLM智能体”的范畴。 **最终决策**: 综合分析，该论文的核心是提出并评估一个名为ARTEMIS的新型多智能体框架，旨在解决复杂的现实世界任务。这完全符合您“筛选出那些核心贡献在于构建、改进或演化LLM智能体的论文”的目标，特别是与您的“多智能体”研究方向高度契合。因此，应予以保留。", "summary2": "\n本文旨在首次在真实企业环境中全面比较AI智能体与人类渗透测试专家。针对大型大学网络的真实渗透测试场景，我们提出了一种名为ARTEMIS的多智能体框架，其核心创新为动态提示生成、任意子智能体和自动漏洞分类。在该环境中，通过漏洞发现数量、有效提交率、综合评分及成本等指标验证了其有效性，结果表明ARTEMIS优于多数人类专家且成本更低。", "inspiration_trace": "\n### 作者思考过程的逻辑链推演\n\n#### 1. **宏观问题：AI在网络安全中的风险与评估缺失**\n   - **观察起点**：AI技术快速发展，威胁行为者（如国家黑客）已将AI用于网络攻击，但现有AI安全评估方法（如CTF挑战、CVE重现基准）过于简化，缺乏真实世界的复杂性（如噪声、交互性、操作风险）。这导致AI的实际能力与基准表现脱节。\n   - **核心矛盾**：现有基准无法反映真实渗透测试中的关键要素（如凭证重用、配置错误链），因此无法准确评估AI的威胁潜力或防御价值。\n   - **研究动机**：需填补这一空白，通过真实环境测试，理解AI代理与人类专家的差距，为AI安全政策提供证据。\n\n#### 2. **聚焦假设：AI代理在真实渗透测试中可能优于人类**\n   - **假设形成**：基于AI的优势（如系统性枚举、并行处理、低成本），作者推测：在真实企业环境中，设计良好的AI代理可能匹配或超越人类渗透测试员，尤其在效率和成本上。\n   - **关键问题**：如何验证这一假设？需解决两个子问题：\n     - 现有AI代理框架（如Codex、CyAgent）在真实场景中表现如何？\n     - 是否能设计一个新框架，克服现有局限（如上下文管理、任务僵化）？\n\n#### 3. **方法论演进：从实验设计到ARTEMIS框架**\n   - **实验设计逻辑**：\n     - **环境选择**：真实企业网络（大学网络，约8000台主机）确保操作真实性，同时通过安全协议（如IRB批准、实时监控）控制风险。\n     - **比较基准**：招募人类专家（10名专业人员）和现有AI代理（6个），定义统一评分框架（结合技术复杂性和业务影响），避免主观偏差。\n     - **指标创新**：引入MITRE ATT&CK映射和加权评分（奖励高复杂度漏洞），以量化“技术深度”而非“低垂果实”。\n   - **框架创新（ARTEMIS）**：\n     - **问题驱动**：现有代理（如CyAgent）在测试中表现差（如早期停滞、高假阳性），暴露了架构缺陷（如有限子代理、上下文丢失）。\n     - **设计原则**：针对真实渗透测试的长期性、复杂性，提出多代理框架：\n       - **动态提示生成**：为子代理定制任务指令，避免工具误用。\n       - **任意子代理**：支持并行任务（如扫描与利用同步），提升效率。\n       - **自动漏洞分类**：减少假阳性，提高提交质量。\n     - **演进逻辑**：从单代理（如Codex）到多代理（如MAPTA），但ARTEMIS通过会话管理（如上下文总结、恢复）实现长期运行，解决“短视”问题。\n\n#### 4. **验证与洞见：实验结果驱动结论**\n   - **执行逻辑**：在真实环境中运行测试，收集数据（如漏洞数量、成本、行为模式），比较人类与AI。\n   - **关键发现**：\n     - ARTEMIS排名第二（优于9/10人类），验证了假设：AI在系统性任务（如枚举）和成本（$18/小时 vs. $60/小时）上有优势。\n     - 但暴露弱点：GUI任务困难、高假阳性率，源于AI的交互局限。\n   - **洞见提炼**：AI代理适合“广度”任务（如并行扫描），人类擅长“深度”任务（如GUI交互），这为未来混合系统设计提供方向。\n\n#### 5. **贡献闭环：开源与未来路径**\n   - **思想升华**：从评估工具到防御赋能，开源ARTEMIS以降低防御门槛，响应AI安全政策需求。\n   - **局限反思**：实验时间压缩（10小时 vs. 真实1-2周）和防御条件缺失，推动未来工作（如环境复制、长期测试）。\n   - **逻辑终点**：研究始于AI风险担忧，终于实用工具，形成“问题-假设-方法-验证-应用”的闭环。\n\n### 逻辑链总结\n作者从**AI安全评估的宏观问题**出发，通过**观察基准缺陷**，提出**AI代理可能优于人类**的假设，进而**设计实验和新框架（ARTEMIS）** 以验证假设，最终基于**真实测试结果**提炼洞见，推动防御应用。整个过程以“真实性”为核心，逐步聚焦，体现了从理论到实践的演进。", "summary_translation": "\n本文提出了在实时企业环境中，对 AI 代理 (AI agents) 与人类网络安全专家进行的首次全面评估。我们在一个由约 8,000 台主机和 12 个子网构成的大型大学网络上，对十名网络安全专家、六个现有的 AI 代理以及我们提出的新型代理框架 ARTEMIS 进行了评估。ARTEMIS 是一个多代理框架 (multi-agent framework)，其特点包括动态提示生成、任意子代理 和自动漏洞分类。在我们的对比研究 中，ARTEMIS 综合排名第二，共发现 9 个有效漏洞，有效提交率 为 82%，其表现优于十位人类参与者中的九位。尽管现有的代理框架如 Codex 和 CyAgent 的表现不及大多数人类参与者，但 ARTEMIS 展现出的技术水准和提交质量可与表现最强的人类参与者相媲美。我们观察到，AI 代理在系统性枚举、并行利用和成本方面具有优势——某些 ARTEMIS 变体的运行成本为每小时 18 美元，而专业渗透测试人员 的成本为每小时 60 美元。我们也指出了其关键的能力差距：AI 代理的误报率 更高，并且在处理基于图形用户界面 (GUI) 的任务时存在困难。", "summary_generated_time": "2025-12-11 11:27:22", "summary_model": "z-ai/glm-4.6"}, {"index": "#51", "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks", "link": "/arxiv/2512.09485", "arxiv_id": "2512.09485", "authors": "Xinye Cao, Yihan Lin, Guoshun Nan, Qinchuan Zhou, Yuhang Luo, Yurui Gao, Zeliang Zhang, Haolang Lu, Qimei Cui, Yanzhao Hou, Xiaofeng Tao, Tony Q. S. Quek", "summary": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.", "subjects": "Cryptography and Security, Artificial Intelligence", "date": "2025-12-10", "category": "cs.AI", "crawl_time": "2025-12-11T11:00:06.188354", "filter_reason": "这篇论文符合筛选要求，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将一个已有的LLM智能体框架应用到安全领域，而是**提出了两个核心创新**：`SecLoop`，一个集成了LLM的、用于安全策略全生命周期管理的自动化框架；以及`SA-GRPO`，一个用于迭代优化安全策略的算法。`SecLoop`本身就是一个LLM智能体框架（具备生成、编排、响应、反馈的循环），而`SA-GRPO`则是一个明确的**自我演化机制**。因此，论文的核心贡献在于构建和演化LLM智能体，符合保留标准。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `LLM-based Agents` (SecLoop框架)。 - **自我演化**: `Self-Evolving`, `Self-Improvement`, `Iterative Improvement` (SA-GRPO算法的核心功能是“iteratively refines security strategies”)。 - **多智能体**: 论文中提到“contrasting group feedback collected from parallel SecLoop executions”，这暗示了多个智能体实例并行运行并进行交互学习，符合多智能体系统的特征。 - **智能体能力**: `Planning` (security strategy generation), `Self-Correction` (通过SA-GRPO进行策略精炼)。 3.  **第三步：排除标准** - **安全与对齐**: 这是本篇论文最需要仔细辨析的地方。虽然论文的标题和摘要充满了`Security`（安全）这个词，但它的主要贡献**并非**提出一种新的安全理论、攻击检测方法或对齐技术。相反，它的贡献是**提出了一种能让智能体在安全这个复杂任务中进行自我演化的新方法**。研究的焦点是“智能体如何演化”，而不是“如何实现安全”。因此，它不应被归为“主要贡献是关于安全”的排除类别。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 这篇论文是“自我演化的应用”这一特殊情况的完美例证。论文的核心是提出一种新的“自我演化”机制（`SA-GRPO`），并将其应用在“网络安全自动化”这个特定领域。根据筛选规则，“如果论文的核心是提出一种新的‘自我演化’机制，即使它被应用在特定领域...也应该保留”。因此，这篇论文应该被保留。 **最终决策**: 综合以上分析，尽管论文的应用领域是网络安全，但其核心贡献在于构建了一个全新的LLM智能体框架（`SecLoop`）和一个创新的自我演化算法（`SA-GRPO`）。这完全契合“LLM智能体及其演化”的研究课题，特别是“自我演化”这一核心方向。因此，这篇论文高度相关，应被保留。", "summary2": "\n本文旨在实现6G零触网络中端到端、自适应的安全自动化。针对动态、对抗性的6G零触网络环境，我们提出了一种集成了SecLoop框架与安全感知分组相对策略优化（SA-GRPO）算法的方法，并在包含五个基准数据集和真实边缘设备的测试环境中通过准确率验证了其有效性。", "inspiration_trace": "\n好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出其核心方法（SecLoop 和 SA-GRPO）的逻辑链，还原其思考过程。\n\n---\n\n### **作者核心方法逻辑推演**\n\n#### **第一阶段：宏观问题洞察与矛盾识别**\n\n1.  **起点：拥抱未来的技术范式。**\n    作者的思考始于一个宏观且确定的未来趋势：6G网络将实现全面自动化，其核心范式是零触网络。ZTN承诺了前所未有的效率、可扩展性和自适应性。\n\n2.  **核心矛盾的浮现：机遇与风险并存。**\n    在深入审视ZTN的架构特性（分布式、开放性、异构性）后，作者敏锐地识别出一个根本性矛盾：**ZTN为自动化管理而生，但其开放和复杂的特性恰恰极大地扩展了网络攻击面，带来了前所未有的安全挑战。** 传统的、基于静态规则和人工干预的安全机制，在这种动态、复杂的环境下已然失效。\n\n3.  **初步的解题方向：智能化。**\n    面对这一矛盾，作者将目光投向了当时最具潜力的颠覆性技术——大型语言模型（LLMs）。LLMs强大的自然语言理解、推理和自主决策能力，似乎为解决动态环境下的安全自动化问题提供了一把“金钥匙”。\n\n> **思考小结：** 作者的思考起点并非“如何用LLM做安全”，而是“如何解决6G ZTN的根本安全难题”。LLM是在这个宏大问题背景下，被识别为最具潜力的赋能技术。\n\n#### **第二阶段：问题聚焦与现有方案批判**\n\n1.  **从宏观到具体：锁定“安全自动化”。**\n    作者将“ZTN的安全难题”进一步聚焦到“安全自动化”这一具体能力上。他们认为，实现智能化的安全自动化是保障6G ZTN的关键。\n\n2.  **批判性审视：发现两大核心挑战。**\n    通过对现有安全编排、自动化与响应（SOAR）平台和相关研究的深入分析，作者发现它们普遍存在两个致命缺陷，这构成了本文要解决的核心问题：\n    *   **挑战一（生命周期割裂）：** 现有系统往往是“半自动”的。它们可能擅长策略生成，或擅长工具调用，但缺乏一个**端到端、全自动的闭环**，无法无缝地完成“策略生成 -> 编排执行 -> 效果验证 -> 反馈更新”的全流程。这导致系统依赖大量人工干预，无法适应真实世界的并行和对抗场景。\n    *   **挑战二（适应性僵化）：** 现有系统大多依赖监督学习，需要昂贵且难以维护的标注数据。更致命的是，静态数据集无法捕捉真实攻击的动态演化，导致模型对新型威胁和零日攻击的泛化能力极差。\n\n> **思考小结：** 作者的思考从“为什么需要安全自动化”深化为“为什么现有的安全自动化还不够”。通过精准定位两大核心挑战，为后续提出针对性解决方案奠定了坚实的基础。\n\n#### **第三阶段：形成核心假设与设计原则**\n\n1.  **提出核心假设：**\n    作者假设，要同时解决上述两大挑战，必须构建一个全新的系统。这个系统不能是现有技术的简单拼凑，而应是一个**原生集成了LLM、能够实现闭环反馈和自我进化的智能体**。\n\n2.  **确立六大设计原则（指导性假设）：**\n    为了将这个模糊的假设具体化，作者提出了六个高维度的设计原则，作为构建理想系统的“宪法”：\n    *   **可学习:** 能从持续的网络状态中学习。\n    *   **自适应:** 能动态响应威胁情报。\n    *   **实用:** 生成的策略必须能被真实基础设施执行。\n    *   **自动:** 覆盖全生命周期，无需人工干预。\n    *   **高效:** 支持并行执行，加速验证和反馈。\n    *   **可插拔:** 架构模块化，适应异构环境。\n\n> **思考小结：** 在这一阶段，作者完成了从“问题发现者”到“方案架构师”的转变。六大原则不仅是技术目标，更是他们用来评判和筛选最终实现方案的标尺。\n\n#### **第四阶段：方法论的双管齐下**\n\n基于上述原则，作者意识到单一的技术无法满足所有要求。因此，他们的思考路径分化为两个相辅相成的部分：一个**系统架构**和一个**核心算法**。\n\n1.  **架构创新：SecLoop——为“挑战一”而生。**\n    *   **思想来源：** 为了实现“全自动”和“闭环”，作者构想了一个完整的自动化框架。SecLoop就是这个思想的物化。\n    *   **逻辑演进：**\n        *   **输入：** 系统必须感知真实世界，因此以IDS告警为起点。\n        *   **大脑：** LLM作为核心决策代理，负责理解告警并生成策略。\n        *   **手脚：** 需要一个“安全编排中心（SOC）”来执行策略，调用真实的安全工具。\n        *   **试炼场：** 如何验证策略的有效性且不影响生产环境？作者创造性地提出了**并行BATTLE-FIELD**，一个基于IaC的、可复现的红蓝对抗模拟环境。这完美解决了“实用”和“高效”原则。\n        *   **闭环：** BATTLE-FIELD的执行结果（策略执行、攻击评估、服务状态）被反馈给一个“策略执行验证器”，形成闭环。\n    *   **结论：** SecLoop架构本身，就是对“挑战一（生命周期割裂）”的直接回答，它实现了从感知到决策、执行、反馈的端到端自动化。\n\n2.  **算法创新：SA-GRPO——为“挑战二”而生。**\n    *   **思想来源：** SecLoop提供了“身体”和“循环系统”，但还需要一个“学习神经系统”来驱动自我进化，以解决“挑战二（适应性僵化）”。\n    *   **逻辑演进：**\n        *   **为什么是强化学习（RL）？** 因为RL通过交互和奖励来学习，天然适合解决无标签、动态环境下的决策问题，完美契合“可学习”和“自适应”原则。\n        *   **为什么是GRPO（Group Relative Policy Optimization）？** 标准的RL算法（如PPO）可能效率不高。作者选择了GRPO，因为它通过**对比一组候选策略的优劣**来学习，比单样本学习更稳定、高效。这契合了“高效”原则。\n        *   **为什么是“Security-Aware”（安全感知）？** 通用的奖励函数无法衡量安全策略的复杂性。作者对GRPO进行了深度定制，设计了**四维奖励函数**：\n            *   **格式奖励：** 保证输出可解析。\n            *   **执行奖励：** 保证策略能在BATTLE-FIELD中跑起来。\n            *   **评估奖励：** 衡量防御效果，这是核心。\n            *   **惩罚奖励：** 防止模型生成过度防御或破坏性的操作。\n        *   **如何与SecLoop结合？** SA-GRPO利用SecLoop的**并行BATTLE-FIELD**能力，同时部署一组策略，收集它们的执行反馈，计算相对优势，然后更新LLM策略模型。\n    *   **结论：** SA-GRPO算法，就是对“挑战二（适应性僵化）”的直接回答。它是一个专门为SecLoop框架定制的、安全感知的强化学习引擎，驱动整个系统实现自我进化。\n\n---\n\n### **最终逻辑链总结**\n\n作者的思考过程是一个典型的**“自顶向下，逐层分解，然后综合创新”**的学术演进路径：\n\n1.  **宏观洞察：** 6G ZTN的未来愿景与其内在安全风险构成核心矛盾。\n2.  **问题聚焦：** 将矛盾分解为现有安全自动化方案的“生命周期割裂”和“适应性僵化”两大具体挑战。\n3.  **原则确立：** 提出六大设计原则作为构建理想解决方案的指导方针。\n4.  **方案综合：**\n    *   针对**“生命周期割裂”**，设计了**SecLoop**这一**系统架构**，构建了端到端的自动化闭环。\n    *   针对**“适应性僵化”**，设计了**SA-GRPO**这一**核心算法**，利用并行反馈和定制化奖励驱动系统自我进化。\n\n最终，SecLoop和SA-GRPO并非两个孤立的技术，而是一个**共生体**：SecLoop为SA-GRPO提供了数据和执行环境，SA-GRPO为SecLoop注入了学习和进化的灵魂。二者共同构成了作者对“如何在6G ZTN中实现真正的、智能的安全自动化”这一宏大问题的完整回答。", "summary_translation": "\n零接触网络（Zero-Touch Networks, ZTNs）代表了一种变革性范式，旨在实现全自动和智能化的网络管理，为第六代（6th-generation, 6G）网络的复杂性提供了所需的扩展性和适应性。然而，6G网络的分布式架构、高度开放性和深度异构性扩大了攻击面，并带来了前所未有的安全挑战。为应对这一问题，安全自动化旨在实现动态和复杂环境下的智能安全管理，成为保障6G ZTNs安全的关键能力。尽管前景广阔，但在6G ZTNs中实施安全自动化仍面临两大主要挑战：1）在真实世界、并行和对抗性条件下，实现从安全策略生成到验证和更新的全生命周期自动化；2）使安全策略能够适应不断演变的威胁和动态环境。为此，我们提出了SecLoop和SA-GRPO。SecLoop是首个完全自动化框架，将大语言模型（Large Language Models, LLMs）集成于安全策略生成、编排、响应和反馈的整个生命周期中，从而在动态网络环境中实现智能和自适应防御，有效应对了第一个挑战。此外，我们提出了SA-GRPO，一种新颖的安全感知群体相对策略优化算法，通过对比并行SecLoop执行中收集的群体反馈，迭代优化安全策略，从而解决了第二个挑战。我们在五个基准测试上进行了广泛的真实世界实验，涵盖11个MITRE ATT&CK流程和20余种攻击类型，结果证明了所提SecLoop和SA-GRPO的优越性。我们将向社区发布该平台，以推动安全自动化在下一代通信领域的发展。", "summary_generated_time": "2025-12-11 11:27:12", "summary_model": "z-ai/glm-4.6"}, {"index": "#90", "title": "Evolving Excellence: Automated Optimization of LLM-based Agents", "link": "/arxiv/2512.09108", "arxiv_id": "2512.09108", "authors": "Paul Brookes, Vardan Voskanyan, Rafail Giavrimis, Matthew Truscott, Mina Ilieva, Chrystalla Pavlou, Alexandru Staicu, Manal Adham, Will Evers- Hood, Jingzhi Gong, Kejia Zhang, Matvey Fedoseev, Vishal Sharma, Roman Bauer, Zheng Wang, Hema Nair, Wei Jie, Tianhua Xu, Aurora Constantin, Leslie Kanthan, Michail Basios", "summary": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies. We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications. We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.", "subjects": "Software Engineering, Artificial Intelligence", "date": "2025-12-09", "category": "cs.AI", "crawl_time": "2025-12-11T11:00:06.201714", "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献与你的研究目标高度契合。以下是根据筛选标准的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 **ARTEMIS** 的方法论和平台，其核心功能是 **自动优化和演化 LLM 智能体**。论文明确指出，LLM智能体因配置不佳（如提示词、工具描述、参数）而表现不佳，而ARTEMIS通过“演化优化平台”和“遗传算子”来自动发现和改进这些配置。这完全符合“构建、改进或演化 LLM智能体”的核心要求，特别是属于 **自我演化** 的范畴。它不是将智能体作为工具去解决某个领域问题，而是研究如何让智能体本身变得更好。 2.  **第二步：正面指标** - 论文包含了大量你的核心关注点： - **核心范式**: `LLM-based Agents`, `Self-Evolving`, `Evolutionary Algorithms` (通过 \"evolutionary optimization platform\" 和 \"genetic operators\" 体现)。 - **演化机制**: `Self-Improvement` (整个平台的目标), `Generational Evolution` (通过遗传算法实现), `Iterative Improvement` (优化过程的本质)。 - 这些关键词和概念在摘要中密集出现，表明论文的研究焦点与你的方向高度一致。 3.  **第三步：排除标准** - 论文的主要贡献不涉及 `Safety`, `Alignment`, `Interpretability` 等安全与对齐问题。 - 论文也未涉及 `Vision`, `MLLMs` 等多模态内容。 - 因此，该论文没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **自我演化的应用**: 论文虽然将ARTEMIS应用到了多个具体领域（如编程竞赛、代码优化、数学推理），但其 **核心贡献是提出了一种通用的、新颖的“自我演化”机制**。这完全符合你设定的“例外”规则：即使应用在特定领域，只要核心是提出新的自我演化机制，就应该保留。ARTEMIS本身就是这个机制。 **最终决策**: 综合以上分析，这篇论文的核心贡献是提出一个名为ARTEMIS的自动化演化平台，用于优化LLM智能体的配置。这直接命中了你研究课题中的 **“自我演化”** 方向。它不是简单的应用，也不是关于基础模型能力或基础设施的研究，而是专注于如何让智能体系统通过演化机制实现自我完善和性能提升。因此，这篇论文是高度相关且应该被保留的前沿研究。", "summary2": "\n本文旨在解决LLM agents因配置次优而性能不佳，且手动调优困难的问题。针对多种LLM agent系统，我们提出了一种名为Artemis的无代码进化优化平台，它利用语义感知的遗传算子对agent配置进行联合优化。在四个代表性agent系统（ALE, Mini-SWE, CrewAI, MathTales-Teacher）及其对应benchmark上，通过接受率、性能分数、成本和准确率等指标验证了其有效性。", "inspiration_trace": "\n好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出其核心方法“Artemis”的逻辑链，还原其从观察到方法论的完整思考过程。\n\n---\n\n### **作者产出《Evolving Excellence》的思考路径推演**\n\n#### **第一步：观察与问题定义——发现“配置困境”**\n\n1.  **宏观观察：** 作者首先观察到LLM智能体领域的一个普遍现象：尽管智能体架构（如ReAct, ToT）日益复杂，理论潜力巨大，但在实际部署中，其性能往往不稳定且未达到预期。\n2.  **深入归因：** 通过实践和文献调研，作者将性能瓶颈归因于**“配置”**问题，而非模型能力本身。一个智能体是一个多组件流水线，其性能由系统提示、工具描述、参数（如temperature）等多个异构因素共同决定。\n3.  **核心问题提炼：** 作者将此现象提炼为**“LLM智能体的配置困境”**。这个困境的核心特征是：\n    *   **高维异构性：** 配置空间混合了自然语言（提示）、离散选择（模型）和连续参数，极其复杂。\n    *   **组件间强耦合：** 各组件相互影响，孤立优化（如只调提示）效果有限。\n    *   **非可微与高成本：** 性能目标无法求导，且每次评估（运行benchmark）成本高昂，使得传统优化方法（如梯度下降、网格搜索）失效。\n    *   **手动调优的脆弱性：** 依赖专家直觉，耗时数周，且结果不可复现、难以泛化。\n\n> **思考节点：** “我们面临的不是一个简单的参数调优问题，而是一个复杂的、系统性的工程难题。现有工具无法有效解决这个‘配置困境’，这构成了研究的核心动机。”\n\n#### **第二步：审视现有方案的局限性——寻找研究空白**\n\n1.  **梳理现有工具：** 作者系统性地审视了四类现有方法：\n    *   **提示工程（如APE, PromptBreeder）：** 结论是它们“只见树木，不见森林”，只优化了提示这一孤立组件，忽略了与工具、参数的协同效应。\n    *   **工作流优化（如ADAS, AFlow）：** 结论是它们往往需要修改智能体内部代码（非黑盒），且对提示本身的语义内容关注不足。\n    *   **多智能体分析（如MAST）：** 这类工作揭示了智能体失败的普遍性，反向印证了配置优化的重要性，但并未提供解决方案。\n    *   **通用优化框架（如DSPy）：** 作者发现其效果在不同任务上波动很大，表明其通用性和鲁棒性仍有待提升。\n2.  **定位研究空白：** 通过对比分析（如表1所示），作者清晰地定位了现有方案的共同短板：\n    *   **缺乏联合优化：** 无法协同调整所有组件。\n    *   **缺乏通用性：** 与特定架构绑定，或需要代码侵入。\n    *   **缺乏语义感知：** 在处理自然语言时，可能生成无意义的变异。\n\n> **思考节点：** “市场需要一个‘瑞士军刀’式的解决方案：它必须是**通用**的（不挑架构）、**黑盒**的（无需改代码）、**联合**的（同时优化所有组件），并且足够**智能**，能理解它在修改的是自然语言，而不是随机字符串。”\n\n#### **第三步：构思核心方法论——进化算法与LLM的联姻**\n\n1.  **匹配问题与技术：** 面对“配置困境”的特征（高维、非可微、昂贵、多模态），作者自然地想到了**进化算法**。\n    *   **为什么是EA？** EA天然适合处理混合类型变量，仅需适应度函数（benchmark结果）即可驱动搜索，其种群特性有助于探索多个局部最优解，避免陷入次优配置。\n2.  **突破性构想——语义化EA：** 传统EA操作的是二进制串或数字，直接用于自然语言会产生大量无意义结果。此时，作者提出了核心创新点：**利用LLM本身来实现遗传算子**。\n    *   **语义突变：** 让一个LLM“阅读”一个提示，并生成一个语义相似但措辞不同的新版本（例如，从“考虑边界情况”突变为“列出所有边界情况：n=0, n=1, 最大值”）。\n    *   **语义交叉：** 让一个LLM分析两个成功的提示，并融合它们的优点，生成一个“子代”提示。\n3.  **方法论成型：** 这一构想将EA的强大搜索框架与LLM的深层语义理解能力结合，形成了**“语义感知的进化优化”**这一核心方法论。它解决了如何在高维、非可微、且包含自然语言的空间中进行有效搜索的根本问题。\n\n> **思考节点：** “进化算法提供了‘骨架’，而LLM提供了‘大脑’。我们不是在随机变异，而是在进行‘有指导的创造性探索’。这就是Artemis的‘灵魂’。”\n\n#### **第四步：构建实用系统——从理论到平台的工程化**\n\n1.  **降低使用门槛：** 作者意识到，一个复杂的EA算法对普通开发者不友好。为了让方法真正落地，必须解决易用性问题。\n2.  **设计自动化流程：**\n    *   **自动组件发现：** 用户不应手动指定哪些文件是提示。作者构思利用语义搜索，让系统自动从代码库中识别出所有可配置组件（提示、YAML配置等）。\n    *   **自然语言交互：** 用户只需用自然语言描述优化目标（如“最大化准确率，同时降低成本”），系统即可将其转化为可计算的适应度函数。\n    *   **黑盒操作：** 整个优化过程通过标准输入/输出和执行日志与智能体交互，无需触碰智能体内部实现，保证了通用性。\n3.  **优化成本效率：** 针对评估成本高昂的问题，作者设计了**分层评估策略**：先用轻量级的LLM“裁判”快速筛选掉大量劣质配置，只将最有希望的候选者送去执行昂贵的完整benchmark。\n4.  **平台化封装：** 将上述所有功能封装成一个名为**Artemis**的无代码平台，用户只需上传代码、指定目标，即可启动优化。\n\n> **思考节点：** “我们的目标不是发表一篇算法论文，而是提供一个能被广泛使用的工具。因此，自动化、易用性和成本控制是和算法创新同等重要的工程目标。”\n\n#### **第五步：验证与边界探索——证明价值并明确适用场景**\n\n1.  **设计多样化实验：** 为了证明Artemis的通用性，作者精心挑选了四个差异巨大的场景：竞争编程（ALE）、代码性能优化（Mini-SWE）、数学推理（CrewAI）和小模型应用（MathTales）。这覆盖了不同任务、不同模型（商业/开源）、不同优化目标（准确率/性能/成本）。\n2.  **解读实验结果：**\n    *   **成功案例（ALE, Mini-SWE, MathTales）：** 显著的性能提升验证了核心假设——对于初始配置“欠优化”的智能体，Artemis能发现非直观的有效配置。这证明了方法的有效性。\n    *   **特殊案例（CrewAI）：** 在准确率上提升不大，但在成本上大幅降低。这并非失败，而是揭示了Artemis的**多目标优化能力**，并引出一个重要洞察：**优化效果与初始配置的“优化潜力”密切相关**。\n3.  **提炼洞见与局限：** 基于结果，作者明确了Artemis的适用边界：它最适用于**有明确优化指标、初始配置粗糙、存在较大提升空间**的智能体。对于已经高度调优的系统，其作用可能更多体现在成本等其他维度的优化上。\n\n> **思考节点：** “实验不仅是为了证明‘我们是对的’，更是为了回答‘我们何时对，以及为何对’。通过分析成功与不那么成功的案例，我们可以为用户提供清晰的实践指南，并指明未来的研究方向。”\n\n---\n\n### **总结：完整的逻辑链条**\n\n作者的思考始于一个普遍的**行业痛点（配置困境）**，通过批判性分析**现有方案的不足**，找到了一个明确的**研究空白**。接着，他们创造性地将**进化算法与LLM的能力相结合**，提出了**语义进化优化**这一核心方法论。为了使其具有实用价值，他们围绕**自动化、易用性和成本控制**进行了精心的**系统工程设计**，最终构建了Artemis平台。最后，通过**全面的实验验证**，他们不仅证明了方法的有效性，更重要的是，深刻揭示了其**适用边界和成功的关键因素**，完成了从观察到理论、再到实践和洞见的完整闭环。", "summary_translation": "\n基于大语言模型构建的智能体AI系统在自动化复杂工作流（从软件开发到客户支持）方面展现出巨大潜力。然而，由于配置次优——例如调校不佳的提示、工具描述和参数（这些通常需要数周的手动精调）——LLM智能体的性能常常不尽人意。现有的优化方法要么过于复杂而难以通用，要么孤立地优化各个组件，从而忽略了它们之间的关键相互依赖关系。我们提出了ARTEMIS，一个无代码进化优化平台，它通过语义感知的遗传算子对智能体配置进行联合优化。用户仅需提供基准脚本和自然语言目标，ARTEMIS便能自动发现可配置组件，从执行日志中提取性能信号，并在无需修改架构的情况下进化配置。\n\n我们在四个代表性智能体系统上对ARTEMIS进行了评估：在AtCoder启发式竞赛的竞赛编程任务中，\\emph{ALE Agent}的通过率提升了\\textbf{13.6\\%}；在SWE-Perf的代码优化任务中，\\emph{Mini-SWE Agent}实现了具有统计学意义的\\textbf{10.1\\%}性能提升；在Math Odyssey的成本与数学推理任务中，\\emph{CrewAI Agent}将评估所需的令牌数量显著减少了\\textbf{36.9\\%}。此外，我们还在GSM8K小学数学问题上评估了由较小的开源模型（Qwen2.5-7B）驱动的\\emph{MathTales-Teacher Agent}，其准确率提升了\\textbf{22\\%}。这证明了ARTEMIS能够有效优化基于商业模型和本地模型的智能体。", "summary_generated_time": "2025-12-11 11:29:24", "summary_model": "z-ai/glm-4.6"}, {"index": "#111", "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning", "link": "/arxiv/2512.08980", "arxiv_id": "2512.08980", "authors": "Chengqi Dong, Chuhuai Yue, Hang He, Rongge Mao, Fenghe Tang, S Kevin Zhou, Zekun Xu, Xiaohan Wang, Jiajun Chai, Wei Lin, Guojun Yin", "summary": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.", "subjects": "Computer Vision and Pattern Recognition, Artificial Intelligence", "date": "2025-12-05", "category": "cs.AI", "crawl_time": "2025-12-11T11:00:06.209198", "filter_reason": "这篇论文符合研究范围，应被保留。 **判断过程和核心依据如下：** 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为IMAgent的新型视觉智能体，并详细描述了如何通过端到端强化学习来训练它。这完全符合“构建、改进LLM智能体的方法论或新框架”的保留标准。它不是简单地将现有智能体应用于某个领域，而是在创造一个新的智能体架构和训练范式。 2.  **第二步：正面指标** - 论文包含了大量核心关注点： - **核心范式**: `LLM-based Agents` (基于VLM的智能体), `Multi-Agent Systems` (用于数据生成)。 - **智能体能力**: `Tool Use` (核心主题，开发了专门的工具), `Self-Reflection` (设计了视觉反思工具), `Planning` (隐含在多步推理和工具使用中)。 - **多智能体**: 明确提到使用`Multi-Agent System`来生成训练数据。 - 这些正面指标强烈表明该论文与您的研究方向高度相关。 3.  **第三步：排除标准** - **安全与对齐**: 论文未涉及安全、对齐或可解释性等问题，此项通过。 - **多模态与视觉**: 这是本案例的关键点。虽然论文标题和内容都围绕“Vision”，但其研究核心**并非**提出新的视觉模型或视觉理解算法。相反，VLM在这里是作为智能体感知环境的**工具**或“大脑”。论文的核心贡献在于智能体的**架构设计**（如何让智能体处理多图）、**训练方法**（端到端RL）和**工具设计**（视觉反思工具）。这完全符合排除标准中的例外条款：“除非它们被用作智能体感知环境的工具，而不是研究的核心”。因此，不应因此排除。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文讨论的是智能体如何通过工具使用和反思来完成复杂的多图像任务，这属于“智能体如何进行规划或在复杂任务中进行多步推理”的范畴，应予以保留。它不是在改进VLM本身的基础视觉推理能力，而是在构建一个能利用这种能力的智能体框架。 5.  **第五步：最终决策** - **综合分析**：该论文的核心贡献是构建了一个新的智能体框架（IMAgent），并提出了创新的训练方法（纯RL）和工具设计（视觉反思）。它同时触及了您研究焦点的两个核心方向：**单智能体**（工具使用、自我反思）和**多智能体**（利用多智能体系统进行数据生成）。尽管涉及视觉，但视觉是作为智能体的感知输入，而非研究主体。因此，这篇论文是关于“LLM智能体及其演化”的典型前沿研究，完全符合您的筛选要求。", "summary2": "\n本文旨在解决现有开源视觉智能体难以处理复杂多图像任务的问题。针对多图像输入场景，我们提出了一种名为IMAgent的视觉智能体，它通过端到端强化学习进行训练，并配备了视觉反思与确认两种专用工具，结合动作-轨迹两级掩码策略以稳定训练。在自建的多图像数据集MIFG-QA及多个单图像基准上，通过准确率等指标验证了其有效性。", "inspiration_trace": "\n好的，以下是对论文《IMAgent: Training Multi-Image Vision Agents via End2End Reinforcement Learning》作者核心思路的逻辑推演，旨在还原其从观察到方法论的思考过程。\n\n---\n\n### **第一步：宏观观察与问题定位**\n\n**起点：** 作者观察到当前视觉智能体领域的一个前沿趋势——以OpenAI O3为代表的“用图像思考”范式。这种范式通过在文本推理链中穿插视觉操作，超越了传统VLM“看图”的局限，实现了更高阶的视觉推理。\n\n**核心矛盾：** 尽管O3展示了巨大潜力，但开源社区的复现努力存在一个关键短板：**绝大多数方法仅限于单张图像输入**。然而，现实世界的复杂任务（如对比分析、多步解读）本质上都是多图像的。这导致现有开源模型与真实应用场景之间存在巨大鸿沟。\n\n**初步问题：** 如何构建一个能够处理**复杂多图像任务**的视觉智能体，以真正复现O3的“思考”能力？\n\n---\n\n### **第二步：深入探究与核心假设**\n\n**深层分析：** 作者没有停留在“多图像支持”这一表面需求，而是进一步探究：为什么现有模型（即使是单图像模型）在长链推理中表现不佳？通过实验观察（如图1所示），他们发现了一个关键现象：**随着推理步骤的深入，VLM对图像内容的注意力会逐渐衰减，甚至在多图像场景下趋近于零。**\n\n**核心假设：** 模型并非“不想”看图，而是在生成文本的CoT过程中，其注意力机制被语言上下文主导，形成了一种**“被动遗忘”**。模型在初始阶段“看到”图像，之后就依赖记忆中的文本信息进行推理，无法主动地、迭代地重新审视视觉证据。这才是阻碍其实现深度“视觉思考”的根本原因。\n\n**问题升级：** 因此，核心任务不仅是让模型“处理”多图像，更是要设计一种机制，**迫使并引导模型在推理过程中能够主动地、动态地重新分配注意力到关键的视觉内容上。**\n\n---\n\n### **第三步：方法论构建与工具设计**\n\n**思路转化：** 如何将“主动重新分配注意力”这一抽象概念转化为可执行的模型行为？作者借鉴了人类的认知习惯：当我们思考复杂问题时，会**反复确认细节**和**回顾全局**。\n\n**工具化思想：** 基于此，作者提出了两种专门的视觉工具，将人类的认知模式模型化：\n1.  **视觉确认：** 对应“确认细节”。通过一个“放大裁剪”工具，让模型能够聚焦于图像的特定局部区域，强制其进行精细化的视觉信息提取。\n2.  **视觉反思：** 对应“回顾全局”。通过一个“图像回看重用”工具，让模型能够重新加载并审视整张图像（或切换到另一张图像），进行全局信息的再整合和注意力重定向。\n\n**逻辑闭环：** 这两个工具的组合，形成了一个**“思考-行动-迭代”**的动态推理闭环。模型不再是被动地一次性处理完所有视觉信息，而是可以像人一样，在需要时随时“拿起放大镜”或“退后一步看全貌”，从而解决了注意力衰减的核心问题。\n\n---\n\n### **第四步：解决训练挑战与策略创新**\n\n**新的挑战：** 有了好的工具设计，如何让模型学会使用它们？作者指出了现有RL训练智能体的两大痛点：\n1.  **数据依赖：** 传统方法需要大量昂贵的人工标注SFT数据来教会模型工具调用格式。\n2.  **训练不稳定：** 纯RL训练工具使用行为时，无效的工具调用和轨迹会干扰学习过程，导致训练崩溃。\n\n**针对性创新：**\n1.  **数据层面：** 既然现有数据无法激发复杂的工具使用，那就**自己创造**。作者设计了一个多智能体协作的数据生成流水线，构建了MIFG-QA数据集。这个数据集不仅包含多图像，更重要的是其问题设计旨在**“逼迫”**模型必须通过多次视觉交互才能找到答案，从而为RL训练提供了高质量的“土壤”。\n\n2.  **算法层面：** 为了摆脱对SFT的依赖并保证纯RL的稳定性，作者提出了**“动作-轨迹两级掩码”策略**。\n    *   **动作级掩码：** 隔离工具执行结果对模型参数更新的直接干扰，防止模型“学坏”。\n    *   **轨迹级掩码：** 直接过滤掉无效的推理轨迹（如超长、重复、无答案），避免这些“坏例子”污染奖励信号，保护模型进行长程探索的积极性。\n\n3.  **激励层面：** 在“零RL”范式下，模型天生倾向于不使用工具。为此，作者在奖励函数中加入了**“工具增益”**，明确奖励有效的工具使用行为，引导模型度过最初的探索阶段，最终学会高效地与工具协同。\n\n---\n\n### **总结：思想的演进脉络**\n\n作者的思考过程是一个典型的**“观察现象 -> 挖掘本质 -> 设计方案 -> 攻克难点”**的学术创新链：\n\n1.  **从“单图像”到“多图像”**：识别了领域内的一个明显局限，将研究目标锁定在更具挑战性和实用性的多图像任务上。\n2.  **从“任务形式”到“认知本质”**：没有停留在表面，而是深入分析了模型在长推理中表现不佳的内在原因——**注意力衰减**，将问题从“如何支持多输入”深化为“如何实现动态视觉思考”。\n3.  **从“抽象理念”到“具体工具”**：将“动态思考”这一理念，巧妙地转化为**“视觉确认”与“视觉反思”**两种可操作的专用工具，使方法论有了坚实的落脚点。\n4.  **从“理想方法”到“工程实现”**：正视了纯RL训练的残酷现实，通过**自建数据集**和**创新训练策略（两级掩码、工具增益）**，系统性地解决了数据、稳定性和激励三大难题，最终让整个方法论得以闭环和实现。\n\n最终，IMAgent的诞生，不是单一技术的突破，而是这一整套环环相扣、层层递进的逻辑思考的必然结果。", "summary_translation": "\n好的，请看以下翻译：\n\n近期基于 VLM (视觉语言模型) 的智能体旨在通过工具调用复现 OpenAI O3 “用图像思考”的构想，但大多数开源方法将输入限制为单张图像，因此在真实世界的多图像问答 (QA) 任务上表现不足。为解决此问题，我们提出了 IMAgent，这是一个通过端到端强化学习 (RL) 训练的开源视觉智能体，专为处理复杂的多图像任务而设计。我们利用一个多智能体系统，生成了具有挑战性且视觉信息丰富的多图像问答对，以充分激发基础 VLM 的工具调用潜力。经过人工验证，我们构建了 MIFG-QA 数据集，其中包含 1 万个用于训练和评估的样本。我们发现，当推理步骤加深时，VLM 可能会逐渐忽略视觉输入。为此，我们开发了两个专门的工具：视觉反思与确认，使模型能够在推理过程中主动地将其注意力重新分配给图像内容。得益于我们精心设计的行动-轨迹两级掩码策略，IMAgent 能够通过纯强化学习训练实现稳定的工具调用行为，且无需依赖成本高昂的监督微调 (SFT) 数据。大量实验表明，IMAgent 在现有的单图像基准测试中保持了强劲性能，同时在我们提出的多图像数据集上取得了显著提升。我们的分析也为研究社区提供了具有实践价值的见解。代码与数据即将发布。", "summary_generated_time": "2025-12-11 11:30:13", "summary_model": "z-ai/glm-4.6"}, {"index": "#126", "title": "AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution", "link": "/arxiv/2512.08951", "arxiv_id": "2512.08951", "authors": "Kamer Ali Yuksel, Hassan Sawaf", "summary": "Creative coding and real-time shader programming are at the forefront of interactive digital art, enabling artists, designers, and enthusiasts to produce mesmerizing, complex visual effects that respond to real-time stimuli such as sound or user interaction. However, despite the rich potential of tools like GLSL, the steep learning curve and requirement for programming fluency pose substantial barriers for newcomers and even experienced artists who may not have a technical background. In this paper, we present AI Co-Artist, a novel interactive system that harnesses the capabilities of large language models (LLMs), specifically GPT-4, to support the iterative evolution and refinement of GLSL shaders through a user-friendly, visually-driven interface. Drawing inspiration from the user-guided evolutionary principles pioneered by the Picbreeder platform, our system empowers users to evolve shader art using intuitive interactions, without needing to write or understand code. AI Co-Artist serves as both a creative companion and a technical assistant, allowing users to explore a vast generative design space of real-time visual art. Through comprehensive evaluations, including structured user studies and qualitative feedback, we demonstrate that AI Co-Artist significantly reduces the technical threshold for shader creation, enhances creative outcomes, and supports a wide range of users in producing professional-quality visual effects. Furthermore, we argue that this paradigm is broadly generalizable. By leveraging the dual strengths of LLMs-semantic understanding and program synthesis, our method can be applied to diverse creative domains, including website layout generation, architectural visualizations, product prototyping, and infographics.", "subjects": "Neural and Evolutionary Computing, Artificial Intelligence, Graphics", "date": "2025-11-27", "category": "cs.AI", "crawl_time": "2025-12-11T11:00:06.214018", "filter_reason": "这篇论文符合我的研究范围，应被保留。判断依据如下： 1.  **核心判断 (第一步):** 论文的核心贡献并非简单地将LLM应用于创意编码领域，而是提出了一种**新颖的“自我演化”框架**。标题和摘要中反复强调的“Evolution”（演化）和“iterative evolution and refinement”（迭代演化和精炼）是其核心。这直接命中了我的研究焦点之一“自我演化”。它不是在解决一个特定的着色器问题，而是在构建一个能够让用户引导智能体（LLM）不断迭代、完善其产出（着色器代码）的系统。因此，它不属于“非演化型应用”的排除范畴。 2.  **正面指标 (第二步):** 论文包含了多个核心关注点。 *   **自我演化:** 这是论文最核心的范式，其机制借鉴了交互式演化计算，并由LLM驱动。 *   **迭代改进:** 摘要明确提到“iterative evolution and refinement”，这与自我演化的目标一致。 *   **工具使用:** LLM在这里扮演了智能体的角色，其核心能力之一是“程序合成”，即生成和修改GLSL代码。这可以被视为智能体使用其内部能力作为工具来完成演化任务。 3.  **排除标准 (第三步):** 论文未触发任何排除标准。虽然其输出是视觉内容，但研究的核心并非视觉模型或多模态理解，而是**如何驱动一个演化过程**。视觉是演化结果的表现形式，而非研究的技术主体。论文也未涉及安全、对齐等被排除的主题。 4.  **特殊和模糊情况 (第四步):** 这篇论文是“自我演化的应用”这一特殊情况的完美范例。 *   **保留 (例外):** 论文的核心是提出一种新的“自我演化”机制——即一个由用户引导、LLM驱动的交互式演化框架。尽管它被应用在“GLSL Shader Animation”这个特定领域，但其方法论贡献是普适的（作者也明确指出“this paradigm is broadly generalizable”）。因此，根据筛选规则，这种提出新演化机制的论文，即使有特定应用场景，也应该被保留。 **总结:** 该论文的本质是构建一个**自我演化的LLM智能体框架**。它探索了如何将LLM的代码生成能力与交互式演化思想相结合，以实现一个能够根据用户反馈持续迭代和改进其产出的智能系统。这完全符合我关于“LLM智能体及其演化”中“自我演化”方向的研究目标。因此，最终判断为保留。", "summary2": "\n本文旨在降低GLSL着色器创作的技术门槛，使非专业用户也能创作复杂的实时视觉效果。针对用户通过视觉选择来引导GLSL着色器进化的场景，我们提出了一种名为AI Co-Artist的框架，该框架利用LLM（如GPT-4）作为语义进化算子，执行着色器代码的交叉和变异。在一项包含50名参与者的用户研究中，通过对比创作数量、首次成功时间、编译错误率和用户满意度等指标，验证了其有效性。", "inspiration_trace": "\n好的，以下是对论文《AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution》核心方法逻辑链的系统性推演，旨在还原作者的思考过程。\n\n---\n\n### **作者核心方法的逻辑演进链**\n\n#### **第一步：宏观观察与核心矛盾识别**\n\n*   **起点：** 作者观察到GLSL着色器在实时、交互式数字艺术领域拥有巨大潜力，能创造出令人惊叹的视觉效果。\n*   **核心矛盾：** 这种强大的能力与极高的技术门槛并存。GLSL编程复杂、抽象，需要深厚的编程知识，这使得大量有创意但非技术背景的艺术家、设计师被排斥在外。**核心问题由此诞生：如何打破技术壁垒，让非程序员也能进行高质量的着色器艺术创作？**\n\n#### **第二步：现有解决方案的审视与不足**\n\n*   **审视方向1：传统编程工具。** 结论是，它们无法解决根本问题，因为它们依然要求用户具备编程能力。\n*   **审视方向2：AI代码助手（如GitHub Copilot）。** 作者认识到这类工具的局限性：它们本质上是**“反应式”**的，主要用于补全或优化已有代码，无法主动进行**“探索式”**的创意生成。用户仍需主导编程逻辑，门槛依旧存在。\n*   **审视方向3：交互式进化计算（IEC）。** 作者从经典项目**Picbreeder**中获得关键启发。Picbreeder证明了“用户作为策展人，通过视觉选择来引导进化”这一模式的成功。它将创作过程从“编写”转变为“选择”，完美规避了编程需求。\n*   **初步假设形成：** **能否将Picbreeder的“用户策展+进化”模式，与LLM的“代码生成”能力结合起来？** 这样，用户只需负责审美判断，而AI负责技术实现。\n\n#### **第三步：关键融合点的创新性思考**\n\n*   **挑战：** 传统进化算法依赖“交叉”和“变异”操作。但在GLSL这种复杂的程序语言上，简单的、随机的代码块交叉或参数变异极易产生无效、崩溃或无意义的代码。这种**“语法层面”**的操作过于脆弱。\n*   **关键洞见：** LLM（如GPT-4）不仅仅是代码生成器，它拥有对代码**“语义”**的理解能力。它知道一段代码“做什么”，而不仅仅是“是什么”。这提供了一个革命性的可能性：**能否让LLM执行“语义层面”的进化操作？**\n*   **概念升华：**\n    *   **语义变异：** 不再是随机改动一个数字，而是向LLM下达指令：“在保持这个着色器核心结构的同时，创造性地增强其视觉效果。”LLM会基于其对“视觉增强”的理解，进行有意义的、连贯的修改（如改变颜色映射、引入新的动态模式）。\n    *   **语义交叉：** 不再是粗暴地交换代码片段，而是指令LLM：“将这两个着色器的视觉特征融合成一个和谐的新作品。”LLM会理解如何将一个着色器的空间结构与另一个的调色板相结合，生成真正意义上的“混血儿”。\n\n#### **第四步：方法论框架的最终构建**\n\n基于以上思考，作者构建了“AI Co-Artist”的完整方法论框架：\n\n1.  **定义用户角色：** 用户是**“创意策展人”**。其唯一任务是观察、比较，并选择符合其审美的视觉结果。交互被简化为点击选择。\n2.  **定义AI角色：** LLM是**“技术实现与创意引擎”**。它接收用户的选择（作为“父代”），并执行“语义交叉”和“语义变异”操作，生成新的、有效的“子代”着色器代码。\n3.  **构建交互闭环：**\n    *   **呈现：** 系统向用户展示一个由多个着色器组成的“种群”。\n    *   **选择：** 用户选择一个或多个心仪的着色器。\n    *   **进化：** 系统根据选择数量，自动调用LLM进行“变异”（单选）或“交叉+变异”（多选）。\n    *   **替换：** 新生成的着色器替换掉种群中未被选中的个体，形成新一代。\n    *   **循环：** 用户在新一代种群上继续选择，驱动创意不断演进。\n4.  **增强体验：** 为了让创作过程更具吸引力，作者引入了**音频响应**功能。这使得着色器不仅是静态图像，而是能与音乐同步的动态艺术，极大地丰富了用户的审美维度和创作乐趣。\n\n#### **第五步：价值的提炼与展望**\n\n*   **核心贡献总结：** 作者将此方法提炼为一种**“人机协同创作”**的新范式。其核心是**“人类审美引导 + AI语义进化”**。\n*   **范式泛化：** 作者进一步意识到，这个范式并非仅限于着色器。任何需要专业知识、且审美评价占主导地位的创意领域（如网页布局、建筑设计、产品原型），都可以套用此框架。这极大地提升了工作的理论价值和普适性。\n*   **未来探索：** 作者甚至提出了更前沿的设想：用多模态视觉-语言模型（VLM）替代人类策展人，实现**“全自动闭环美学进化”**，将这一范式推向极致。\n\n---\n\n**总结：** 作者的思考路径始于一个明确的**“技术民主化”**问题，通过批判性地审视现有工具，巧妙地将**交互式进化**的用户模型与**LLM**的代码生成能力相结合。其最核心的创新飞跃在于，**利用LLM的语义理解能力，将传统脆弱的、语法层面的进化操作，升级为鲁棒的、语义层面的创意融合与变异**，最终构建了一个让用户只需“凭感觉选择”，即可驱动复杂技术作品演进的协同创作系统。", "summary_translation": "\n好的，请看以下翻译：\n\n创意编码和实时着色器编程是交互式数字艺术领域的前沿技术，使艺术家、设计师及爱好者能够创作出可响应声音或用户交互等实时刺激的、令人着迷的复杂视觉效果。然而，尽管 GLSL (OpenGL Shading Language) 等工具潜力巨大，但其陡峭的学习曲线和对编程熟练度的要求，为新手乃至缺乏技术背景的资深艺术家设置了巨大的障碍。本文介绍了 AI Co-Artist，这是一种新颖的交互式系统。该系统利用大型语言模型的能力，特别是 GPT-4，通过一个用户友好且视觉驱动的界面，支持 GLSL 着色器的迭代式演进与优化。我们系统的设计灵感源自 Picbreeder 平台开创的用户引导式进化原则，它使用户能够通过直观的交互方式来进化着色器艺术，而无需编写或理解代码。AI Co-Artist 既是创意伙伴，也是技术助手，让用户得以探索实时视觉艺术广阔的生成式设计空间。通过包括结构化用户研究和定性反馈在内的全面评估，我们证明了 AI Co-Artist 显著降低了着色器创作技术门槛，提升了创作成果，并帮助不同背景的用户创作出专业级的视觉效果。此外，我们认为这一范式具有广泛的普适性。通过利用 LLMs 的双重优势——语义理解和程序合成，我们的方法可应用于多种创意领域，包括网站布局生成、建筑可视化、产品原型设计和信息图表制作。", "summary_generated_time": "2025-12-11 11:28:13", "summary_model": "z-ai/glm-4.6"}]}, {"name": "Computation and Language", "count": 4, "papers": [{"index": "#7", "title": "MOA: Multi-Objective Alignment for Role-Playing Agents", "link": "/arxiv/2512.09756", "arxiv_id": "2512.09756", "authors": "Chonghua Liao, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li", "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.", "subjects": "Computation and Language", "date": "2025-12-10", "category": "cs.CL", "crawl_time": "2025-12-11T11:00:05.203159", "filter_reason": "这篇论文符合您的研究范围，应该被保留。我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的核心贡献是提出一个名为MOA（Multi-Objective Alignment）的**强化学习框架**，其目标是**构建和改进**一种特定的LLM智能体——角色扮演智能体。它不是将现有智能体作为工具应用到某个领域，而是专注于如何让智能体本身变得更强大、更全面。因此，它通过了第一步的核心判断。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: 论文明确研究 `LLM-based Agents` (Role-Playing Agents)。 - **智能体能力**: 论文旨在提升智能体的多项核心能力，包括遵循多轮指令（涉及`Planning`）、展现领域知识、保持一致的语言风格。这些是衡量一个智能体是否优秀的关键指标。 - **演化机制**: 论文使用强化学习（RL）框架来优化智能体，这本质上是一种通过反馈进行迭代和`Self-Improvement`（自我完善）的机制。其目标是让智能体在多个维度上持续优化，这与“自我演化”的方向高度相关。 3.  **第三步：排除标准** - **安全与对齐**: 这是本论文最需要仔细辨析的一点。虽然标题和摘要中多次出现 \"Alignment\"（对齐），但这里的对齐**不是**指您筛选标准中需要排除的 `Safety`、`Security` 或 `Ethical Alignment`（安全、伦理对齐）。这里的 \"Multi-Objective Alignment\" 指的是将智能体的行为与**多个细粒度的任务目标（rubrics）**进行对齐，例如“角色知识”、“人格风格”、“多轮对话能力”等。这是一种**功能性对齐**或**任务对齐**，其目的是提升智能体的综合性能，而非解决安全问题。因此，它**不触发**排除标准。 - **多模态与视觉**: 论文完全聚焦于文本对话和语言风格，不涉及视觉或多模态内容，因此不触发排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 论文关注的是智能体在复杂多轮对话场景下的综合表现，这属于智能体层面的规划和推理能力，而非LLM基础的数学或逻辑推理。因此，符合保留条件。 - **自我演化的应用**: 论文的核心就是提出一种新的优化框架（可以视为一种演化机制），因此完全符合保留条件。 5.  **第五步：最终决策** - **核心贡献**: 论文的核心贡献是构建了一个新的框架（MOA），用于**改进和演化**LLM智能体（特别是角色扮演智能体），使其能够同时掌握多种复杂技能。 - **与研究目标的契合度**: 该研究直接对应您的核心目标“构建、改进或演化LLM智能体”。它属于**单智能体**研究范畴，因为它专注于提升单个智能体的能力；同时，其通过强化学习进行迭代优化的方法，也触及了**自我演化**的核心理念。 综上所述，尽管标题中的\"Alignment\"一词可能引起误解，但论文的实质内容是关于如何通过一种新颖的多目标优化框架来**提升LLM智能体的综合能力**，这与您的研究课题高度相关。因此，最终判断为保留。", "summary2": "\n本文旨在解决现有方法在训练角色扮演智能体时，无法同时优化知识、风格等多个相互冲突维度，且输出多样性低的问题。针对通用角色扮演任务，我们提出了一种名为MOA的强化学习框架，其核心是多目标优化策略（动态选择枢轴维度并过滤冲突样本）和多样化的rollout策略（结合思维增强与离策略引导）。我们在PersonaGym和RoleMRC基准上，通过多个细粒度评估指标（如EA, LH, KR, SC等）验证了其有效性，使8B模型性能媲美甚至超越GPT-4o等强大基线。", "inspiration_trace": "\n好的，作为一名学术思维分析专家，我将为你系统性地推演《MOA: Multi-Objective Alignment for Role-Playing Agents》这篇论文背后作者可能的思考过程。我们将从一个宏观问题出发，逐步聚焦，还原其从观察到假设，再到最终方法论的完整逻辑链。\n\n---\n\n### **第一步：宏观问题的确立——如何构建“真正会演戏”的智能体？**\n\n作者的研究起点，必然是对“角色扮演智能体”这一前沿领域的宏观审视。他们观察到，尽管RPA在客服、游戏、内容生成等领域展现出巨大潜力，但现有模型的表现与“真正会演戏”的理想状态相去甚远。\n\n一个优秀的演员需要具备什么？\n1.  **理解剧本**：遵循多轮指令，理解场景和任务。\n2.  **融入角色**：掌握角色的背景知识，言行符合其身份。\n3.  **展现演技**：保持稳定的语言风格、情感和口头禅。\n\n作者敏锐地意识到，这三个核心技能是**相互冲突**的。例如，一个知识渊博的教授角色，在回答问题时可能会因为追求知识的准确性（技能2）而失去其平易近人的语言风格（技能3）。因此，核心问题浮出水面：\n\n**核心问题：如何训练一个能够同时掌握多种冲突技能，从而实现高质量、高通用性角色扮演的智能体？**\n\n---\n\n### **第二步：诊断现有范式的根本缺陷**\n\n为了回答上述问题，作者首先对当时主流的训练范式进行了“诊断”，试图找到它们的“病根”。\n\n#### **1. 对SFT（监督微调）的诊断：**\n*   **观察**：SFT是训练RPA的主流方法，通过让模型模仿大量合成对话数据来学习。\n*   **发现问题**：\n    *   **“形似而神不似”**：SFT倾向于过拟合数据的表面特征（如特定句式、关键词），导致模型成为一个“复读机”，而非真正理解角色内核的“演员”。这导致性能天花板低。\n    *   **“创造力枯竭”**：过度模仿导致模型输出的多样性极低。一个缺乏多样性的模型，难以在后续优化中探索更好的表现，容易陷入局部最优。\n\n**结论**：SFT是一种“模仿式学习”，只能教会模型“怎么说”，但无法教会它在复杂场景下“如何演”，更无法处理技能间的冲突。\n\n#### **2. 对RL（强化学习）的初步尝试与诊断：**\n*   **观察**：受RL在推理任务（如数学、代码）上成功的启发，一些研究尝试将其用于角色扮演。\n*   **发现问题**：\n    *   **奖励过于粗糙**：早期方法（如RAIDEN-R1）使用关键词匹配作为奖励，这只能评估“知识”等单一维度，完全忽略了“风格”、“一致性”等更细腻的演技层面。\n    *   **奖励的内在冲突被忽略**：这是最关键的发现。作者意识到，角色扮演的奖励是多维度的（知识、风格、一致性等），且这些维度之间存在天然冲突。例如，一个长篇大论的回答可能在“知识”维度得分高，但在“风格”维度得分低。\n    *   **加权平均的陷阱**：标准的RL方法（如GRPO）通常将多个奖励加权求和成一个标量。作者通过一个精妙的例子（图1中的o2, o3, oG）指出，这种做法会“混淆视听”。一个在“风格”上很差但在“知识”上很好的样本，在优化“风格”时会被错误地当作正样本，引入噪声，使策略更新变得低效且不稳定。\n\n**结论**：直接将现有RL框架迁移到角色扮演任务上是行不通的，因为它没有正视并处理“多目标冲突”这一核心挑战。\n\n---\n\n### **第三步：形成核心假设——从“同时优化”到“分而治之”**\n\n在诊断了现有方法的根本缺陷后，作者的思想发生了关键转折。他们不再试图用一个单一的奖励信号去“同时”优化所有冲突的目标，而是提出了一个革命性的假设：\n\n**核心假设：我们不应将角色扮演视为一个单目标优化问题，而应将其视为一个多目标优化问题。与其粗暴地将冲突的目标加权平均，不如设计一种策略，动态地、有侧重地逐一攻克这些目标，同时智能地规避它们之间的冲突。**\n\n这个假设包含两个关键思想：\n1.  **动态聚焦**：在训练的每一步，不是所有目标都同等重要。应该优先学习当前“最容易提升”的维度。\n2.  **冲突规避**：在聚焦于某个维度进行优化时，必须主动“屏蔽”那些在该维度上表现不佳、但在其他维度上表现很好的“干扰样本”，以防止它们误导学习过程。\n\n---\n\n### **第四步：构建方法论——MOA框架的诞生**\n\n基于上述核心假设，作者开始设计具体的实现方案，即MOA（Multi-Objective Alignment）框架。这个框架由两大支柱构成，分别对应了SFT和标准RL的缺陷。\n\n#### **支柱一：多目标优化策略——解决“冲突”问题**\n\n这是MOA的核心创新，直接将“分而治之”的假设算法化。\n*   **如何实现“动态聚焦”？**\n    *   **思想来源**：课程学习。先易后难。\n    *   **具体设计**：作者设计了“枢轴维度选择”机制。系统会追踪每个奖励维度在历史训练中的表现曲线，通过线性回归预测其趋势。**当前实际表现远超其历史趋势的维度，就是“最有潜力”的维度**，即本轮优化的“枢轴维度”。这是一种数据驱动的、动态的决策过程。\n*   **如何实现“冲突规避”？**\n    *   **具体设计**：作者设计了“冲突样本消除”机制。在确定了枢轴维度后，算法会筛选出一批样本。这些样本不仅要满足“在枢轴维度上表现好”，还要满足“在所有维度的加权总分上表现好”。通过这种方式，那些“偏科”严重的干扰样本就被排除了，确保了用于策略更新的样本是“高质量且无冲突”的。\n\n#### **支柱二：多样化采样策略——解决“多样性”与“质量”问题**\n\n作者认识到，即使有了好的优化算法，如果输入的样本本身质量差、多样性低，效果也会大打折扣。这恰恰是SFT模型的通病。\n*   **如何打破SFT模型的“僵化”？**\n    *   **观察与假设**：作者通过实验发现，让强大的闭源模型（如Claude）在回答前先进行“思考”，能显著提升其在角色扮演任务上的表现（图3）。\n    *   **推论**：强制模型进行“思考”，可以引导它从模仿表面语言，转向理解角色的内在逻辑（情感、背景、动机），从而打破SFT带来的僵化模式，生成更高质量、更多样化的响应。\n    *   **具体设计**：提出了“思维增强采样”，通过一个精心设计的Prompt（图2），引导模型在生成最终回答前，先输出一段结构化的“内心戏”。\n*   **如何防止模型“钻空子”？**\n    *   **观察与假设**：基于“LLM-as-a-Judge”的奖励很容易被“破解”，比如模型发现写长答案就能在“知识”维度上得高分。\n    *   **推论**：如果训练样本中混入一些来自更强模型（如GPT-4o）的、无法被轻易破解的高质量样本，就能为模型提供一个更稳定、更可靠的学习目标，从而抑制其“投机取巧”的行为。\n    *   **具体设计**：提出了“离策略指导”，在采样时混合一部分来自强大闭源模型的输出，共同参与优势函数的计算。\n\n---\n\n### **总结：思想的演进脉络**\n\n1.  **起点**：构建一个能同时掌握多种冲突技能的“好演员”智能体。\n2.  **诊断**：发现SFT只会“模仿”导致僵化，标准RL因“奖励冲突”而失效。\n3.  **洞见**：角色扮演的本质是**多目标冲突优化**，而非单目标优化。\n4.  **假设**：应采用**“分而治之”**的策略，动态聚焦并规避冲突。\n5.  **方案**：\n    *   **对内**：设计**多目标优化算法**（枢轴维度选择 + 冲突样本消除）来管理目标冲突。\n    *   **对外**：设计**多样化采样策略**（思维增强 + 离策略指导）来提升样本质量和多样性。\n6.  **成果**：MOA框架，一个能够系统性地解决角色扮演核心挑战的全新RL训练范式。\n\n最终，作者通过严谨的实验验证了这一整套思考的有效性，使得一个8B模型在多项指标上媲美甚至超越了GPT-4o等强大基线，完美地闭环了从问题发现到方法创新的整个学术探索过程。", "summary_translation": "\n角色扮演智能体必须同时掌握多种存在冲突的技能：遵循多轮指令、展现领域知识以及保持一致的语言风格。现有研究要么依赖监督微调，该方法会过拟合表面线索并导致多样性低下；要么应用强化学习，但该方法无法在多个维度上进行学习以实现全面的RPA优化。本文提出了MOA (Multi-Objective Alignment，多目标对齐)，这是一个用于通用RPA的强化学习框架，能够实现多维度、细粒度的评分标准优化。MOA引入了一种新颖的多目标优化策略，该策略同时在多个细粒度评分标准上进行训练，以提升优化性能。此外，为解决模型输出的多样性与质量问题，我们还采用了带有离策略引导的思维增强推演。在PersonaGym和RoleMRC等具有挑战性的基准测试上进行的广泛实验表明，MOA使得一个8B参数的模型能够在多个维度上匹配甚至超越GPT-4o和Claude等强大的基线模型。这证明了MOA在构建能够同时满足角色知识、角色风格、多样场景以及复杂多轮对话等需求的RPA方面具有巨大潜力。", "summary_generated_time": "2025-12-11 11:26:06", "summary_model": "z-ai/glm-4.6"}, {"index": "#18", "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "link": "/arxiv/2512.09487", "arxiv_id": "2512.09487", "authors": "Yucan Guo, Miao Su, Saiping Guan, Zihao Sun, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng", "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval", "date": "2025-12-10", "category": "cs.CL", "crawl_time": "2025-12-11T11:00:05.206291", "filter_reason": "这篇论文的核心贡献是提出了一个名为 `RouteRAG` 的新框架，该框架使用强化学习（RL）来让LLM在生成答案的过程中，自主地、动态地决定何时进行推理、从文本或图数据库中检索什么内容、以及何时给出最终答案。 根据筛选标准，我的判断过程如下： 1.  **第一步：核心判断** - **保留**。这篇论文的本质不是简单地将RAG技术应用于某个领域，而是提出了一种**构建和改进LLM智能体**的新方法论。`RouteRAG`框架赋予了LLM一个统一的生成策略，使其能够根据当前状态自主决策（推理、检索、终止），这完全符合“构建LLM智能体”的定义。它不是非演化型应用，因为其核心是提出一个自适应的、可学习的框架，而非一个固定的应用方案。 2.  **第二步：正面指标** - 论文明确包含了多个核心关注点： - **智能体能力**: `Planning`（规划何时推理、检索和回答）、`Tool Use / Tool Augmentation`（将文本和图检索作为工具，并学习选择使用哪个工具）。 - **核心范式**: 其工作方式与 `ReAct` (Reasoning and Acting) 范式高度相似，在推理和行动（检索）之间交替进行，并通过RL进行端到端优化。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全、对齐、可解释性或视觉多模态问题，因此没有触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是“保留”的典型案例。它研究的不是提升LLM本身的基础推理能力，而是构建一个**智能体框架**来指导LLM在复杂任务中进行多步推理和规划。模型学习的是一个决策策略，这属于典型的Agentic AI研究范畴。 **最终决策**: 综合来看，`RouteRAG`的核心贡献在于构建了一个具有自主规划和工具使用能力的LLM智能体框架。它通过强化学习让智能体学会了如何高效地、自适应地利用外部知识（文本和图）来完成复杂任务。这完全符合我研究课题中“单智能体”方向下的“规划”和“工具使用”子方向。因此，这篇论文应该被保留。", "summary2": "\n本文旨在解决现有混合检索增强生成（RAG）系统依赖固定检索流程、无法在推理中自适应选择检索模式且图检索成本高昂的问题。针对需要多跳推理的复杂问答任务，我们提出了一种基于强化学习（RL）的RouteRAG框架，通过统一的生成策略学习在多轮推理中自适应地进行文本-图混合检索，并在五个QA基准上通过EM和F1指标验证了其有效性。", "inspiration_trace": "\n好的，作为一名学术思维分析专家，我将基于您提供的论文内容，系统性地推演作者提出 RouteRAG 这一核心方法的逻辑链，还原其从观察到最终方法论的思考过程。\n\n---\n\n### **第一步：宏观问题的确立——RAG的演进与瓶颈**\n\n作者的思考起点，必然是当前人工智能领域最核心的范式之一：检索增强生成（RAG）。\n\n1.  **观察现状：** RAG 已被证明是解决大型语言模型（LLMs）知识陈旧、产生幻觉问题的有效方案。它通过引入外部知识，极大地提升了模型的事实准确性。\n2.  **识别演进方向：** 作者敏锐地注意到 RAG 技术的两个主要演进趋势：\n    *   **从“单轮”到“多轮”：** 早期 RAG 是“一次检索，一次生成”的模式。但复杂问题需要多步推理，因此出现了多轮 RAG，允许模型在推理过程中多次、动态地检索信息。\n    *   **从“文本”到“图”：** 除了非结构化的文本，结构化的知识图谱能提供实体间的显式关系，对多跳推理至关重要。因此，基于图的 RAG 应运而生。\n3.  **发现核心矛盾：** 这两个演进方向之间存在一个尚未解决的张力。\n    *   **多轮文本 RAG** 虽然灵活，但通常依赖大型模型的内在规划能力，或通过简单的提示来引导，小型模型难以学会“何时检索、检索什么”。\n    *   **图 RAG** 虽然结构性强，但普遍存在两个问题：一是计算成本高昂；二是大多采用“一次性”检索，无法在推理过程中动态适应，导致不必要的开销。\n\n**核心问题浮现：** **如何设计一个 RAG 系统，既能像多轮 RAG 一样自适应地推理，又能像图 RAG 一样利用结构化知识，同时还能避免不必要的计算开销，尤其对于能力较弱的小型模型？**\n\n---\n\n### **第二步：具体问题的聚焦——现有方案的局限性**\n\n在确立了宏观问题后，作者会深入剖析现有方案的具体不足，从而找到创新的切入点。\n\n1.  **观察一：决策的“被动性”。** 现有的混合 RAG 系统（如 HybGRAG）通常遵循一个固定的、预先设计好的流程（例如，先检索文本，再检索图）。模型只是一个被动的执行者，缺乏根据当前推理状态动态决策的能力。\n2.  **观察二：效率的“牺牲”。** 图检索虽然强大，但成本高。现有系统没有机制去权衡“检索带来的收益”与“检索付出的成本”。对于一个简单问题，可能一次文本检索就够了，但系统仍可能执行昂贵的图检索，造成资源浪费。\n3.  **观察三：能力的“鸿沟”。** 多轮 RAG 的成功（如 Search-R1）依赖于强化学习（RL）来训练模型学会决策。但这些工作只关注文本检索。如何将这种“学会决策”的能力扩展到“文本+图”的混合场景，是一个空白。\n\n**关键假设形成：** **能否将整个“推理-检索”过程视为一个统一的序列决策问题，并利用强化学习（RL）来训练一个策略模型，让它学会自主地、高效地管理整个流程？**\n\n---\n\n### **第三步：核心思想的提出——用 RL 统一一切**\n\n基于上述假设，作者的核心方法论开始成型。这个方法论的核心是“统一”和“学习”。\n\n1.  **统一行动空间：** 为了让模型能够自主决策，必须给它一个清晰的“行动菜单”。作者设计了一个统一的生成策略，将推理、检索和作答都转化为模型可以生成的特殊 Token。\n    *   `</search>...</search>`：触发检索动作。\n    *   `[passage]`, `[graph]`：指定检索模式（文本、图或混合）。\n    *   `</answer>...</answer>`：生成最终答案。\n    *   这样，模型在每一步都可以决定：是继续内部思考，还是去检索，以及去哪里检索，或是直接作答。**这解决了“被动性”问题。**\n\n2.  **统一优化目标：** RL 的关键在于奖励函数的设计。作者的目标是同时优化“准确性”和“效率”。但直接优化两者是困难的，因为模型可能连准确性都还没学会。\n    *   **“分而治之”的灵感：** 作者提出了一个巧妙的**两阶段训练框架**。\n        *   **第一阶段：先求“准”。** 奖励函数只关注答案的正确性（`R_outcome`）。目标是让模型首先学会如何利用混合检索来正确回答问题，建立一个能力基线。\n        *   **第二阶段：再求“省”。** 在模型具备基本能力后，引入一个“效率奖励”（`R_efficiency`）。这个奖励只在答案正确时生效，奖励那些检索用时低于平均水平的轨迹，惩罚冗余检索。**这解决了“效率牺牲”问题。**\n\n3.  **统一学习主体：** 整个流程由同一个 LLM 策略模型驱动。它既是“思考者”，也是“决策者”，还是“作答者”。通过端到端的 RL 训练，模型能够学会在推理的不同阶段，如何最有效地调用外部知识。**这解决了“能力鸿沟”问题，让小模型也能学会复杂的决策。**\n\n---\n\n### **第四步：方法论的最终成型——RouteRAG**\n\n至此，RouteRAG 的完整框架已经清晰。\n\n*   **名称的由来：** “Route”一词精准地概括了其核心——模型学会了在“文本”和“图”这两条知识路径之间进行智能路由。\n*   **框架的闭环：**\n    1.  **输入：** 一个问题。\n    2.  **循环：** 策略模型根据当前上下文，生成下一个行动（思考、检索文本、检索图、作答）。\n    3.  **交互：** 如果是检索动作，则调用相应的检索器，并将结果返回给模型。\n    4.  **终止：** 模型生成 `<answer>` 标记，输出最终答案。\n*   **训练的保障：** 采用 GRPO 算法进行训练，因为它在稀疏奖励场景下更稳定，适合这种只有在最后才知道答案对错的 RAG 任务。\n\n**最终，作者的思考路径形成了一个完美的闭环：从观察 RAG 领域的宏观趋势与矛盾，到聚焦现有方案的具体缺陷，再到提出“用 RL 统一决策”的核心假设，最后通过设计统一的行动空间、分阶段的优化目标和端到端的训练框架，将这一假设落地为高效、自适应的 RouteRAG 方法。**", "summary_translation": "\n检索增强生成将非参数知识整合到大型语言模型中，这些知识通常来源于非结构化文本和结构化图。尽管近期研究已通过强化学习将基于文本的RAG推进至多轮推理阶段，但将这些进展扩展到混合检索领域仍面临着额外的挑战。现有的基于图或混合检索的系统通常依赖于固定或手工设计的检索管道，缺乏在推理过程中动态整合补充证据的能力。此外，尽管图证据提供了对多跳推理至关重要的关系结构，但其检索成本要高昂得多。为解决上述局限性，我们提出了\\model{}，这是一个基于强化学习的框架，能够让大型语言模型实现多轮、自适应的图-文本混合检索增强生成。\\model{}通过强化学习联合优化整个生成过程，使模型能够在一个统一的生成策略下，学习何时进行推理、从文本或图中检索何种内容、以及何时生成最终答案。为引导该学习过程，我们设计了一个两阶段的训练框架，该框架兼顾了任务结果与检索效率，从而使模型能够在利用混合证据的同时，避免不必要的检索开销。在五个问答基准测试上的实验结果表明，\\model{}显著优于现有的RAG基线模型，这突显了端到端强化学习在支持复杂推理任务实现自适应、高效检索方面的优势。", "summary_generated_time": "2025-12-11 11:24:46", "summary_model": "z-ai/glm-4.6"}, {"index": "#27", "title": "CORE: A Conceptual Reasoning Layer for Large Language Models", "link": "/arxiv/2512.09222", "arxiv_id": "2512.09222", "authors": "Vishwas Hegde, Vindhya Shigehalli", "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-12-10", "category": "cs.CL", "crawl_time": "2025-12-11T11:00:05.208787", "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接聚焦于**构建和改进LLM智能体**。 1.  **核心判断 (第一步):** 论文的本质是提出一种名为CORE的新框架/方法论，用于解决LLM在多轮交互中的核心缺陷。它不是将LLM作为工具应用于某个特定领域，也不是单纯提升LLM的基础推理能力，而是构建了一个位于LLM之上的“概念推理层”。这个层通过引入持久化的状态和结构化的算子，直接改进了智能体的架构。因此，根据第一步的核心判断标准，这篇论文应该**保留**。 2.  **正面指标 (第二步):** 论文与你的核心关注点高度契合： *   **智能体能力:** 论文的核心贡献之一是解决**记忆**问题。它提出的“持久的局部概念”是一种高级的、结构化的记忆机制，远超简单的对话历史回放，直接解决了“内部表示不会在轮次之间持续存在”这一智能体关键痛点。 *   **智能体能力:** 论文明确提出了“概念推理”，旨在解决多轮交互中的“不一致的推理模式”和“漂移”问题。这属于**规划/推理**的范畴，其提出的“认知算子”与ReAct等范式在思想上是一致的，都是为智能体的自主行动提供结构化的指导框架。 *   **核心范式:** 整篇论文都在讨论如何构建一个更稳定、更高效的`LLM-based Agent`系统。 3.  **排除标准 (第三步):** 论文的主要贡献不涉及安全、对齐、可解释性或多模态。它的焦点是智能体的性能和架构稳定性，因此没有触发任何排除标准。 4.  **特殊和模糊情况 (第四步):** *   **推理/规划:** 这篇论文是“保留”情况的完美范例。它不是提出一个新的数学数据集来微调模型，而是提出了一个**智能体框架**，该框架通过分离“概念推理”和“语言生成”来改进智能体在复杂、多步任务中的表现。这完全符合你对Agentic框架的研究兴趣。 **总结:** 这篇论文的核心贡献是提出了一种名为CORE的**智能体架构改进方案**。它通过引入一种新颖的**持久化记忆机制**（局部概念）和**结构化的推理流程**（认知算子），直接解决了LLM智能体在多轮交互中的记忆和推理稳定性问题。这完全属于你研究范围中的“单智能体”方向，特别是“记忆”和“规划”子方向。因此，这篇论文是高度相关且应该被筛选出来的前沿研究。", "summary2": "\n本文旨在解决LLM在多轮交互中因缺乏持久化语义状态而导致的推理漂移和提示词膨胀问题。针对多轮对话场景，我们提出了一种名为CORE的概念优先推理层，它通过一个持久化的局部概念状态和一组通用的认知算子来管理对话语义，无需修改模型权重。在一个初步原型上，我们通过累积提示词token数量验证了其有效性，实现了约42%的减少。", "inspiration_trace": "\n好的，以下是基于您提供的论文内容，对作者产出CORE方法的思考过程的系统性推演。\n\n---\n\n### 作者产出CORE方法的逻辑链推演\n\n#### 第一步：宏观观察——发现LLM多轮交互的“失忆症”问题\n\n作者首先观察到一个普遍现象：尽管大型语言模型（LLM）在单轮对话中表现出色，但在多轮、任务导向的交互中表现不佳。他们注意到，随着对话深入，系统似乎会“忘记”早期的意图和约束，导致推理漂移、前后不一致，并且为了维持上下文，需要不断将整个对话历史塞入提示，造成巨大的计算浪费。\n\n**核心洞察：问题的根源不在于模型本身的表达能力，而在于其交互范式。**\n\n#### 第二步：深入诊断——归因于“Token-First”架构的根本缺陷\n\n作者将上述现象归因于当前LLM系统的底层架构，即“Token-First”范式。在这个范式下：\n1.  **无状态性**：每次调用模型都是一个独立的、无状态的请求。模型在生成响应后，其内部的、富含语义的隐藏状态会立即丢弃。\n2.  **依赖重建**：由于没有持久化的内部状态，系统在每一轮都必须从不断增长的token历史中“重新推断”用户意图、任务约束和中间结论。\n3.  **效率与稳定性悖论**：这种“重建”过程不仅效率低下（重复计算），而且不稳定（历史信息可能被截断，导致推断偏差）。\n\n**核心诊断：LLM系统缺乏一个独立于token流、能够持久化承载“任务意义”的层。**\n\n#### 第三步：跨界类比——从人类认知中寻找灵感\n\n为了寻找解决方案，作者将目光投向了人类认知科学。他们引用皮亚杰的“图式”理论：人类并非逐字重读对话历史来理解情境，而是依赖于一个持续演化的、高层次的概念结构（图式）。通过“同化”和“顺应”，我们高效地更新理解，保持意义的连续性。\n\n**核心假设：如果能为LLM系统构建一个外部的、类似“图式”的持久化概念状态，或许能解决多轮交互的稳定性与效率问题。**\n\n#### 第四步：思想聚焦——将“概念状态”具象化为可操作的组件\n\n有了“外部概念状态”这一核心假设后，作者开始思考如何具体实现它。他们将其分解为两个相辅相成的核心问题：\n1.  **如何表示“推理的动作”？** 在对话中，模型进行的操作是多样的（比较、总结、规划等）。如果让模型自己从文本中猜测，就会导致不一致。因此，需要将这些“认知动作”显式化、标准化。\n2.  **如何表示“推理的内容”？** 需要一个结构化的数据容器，来存储任务的核心语义，如目标、约束、中间结果等，而不是原始的对话文本。\n\n**核心设计：将抽象的“概念层”具体化为“Global CORE”（操作符库）和“Local CORE”（局部概念状态）两大组件。**\n\n#### 第五步：方法论构建——形成CORE的完整架构\n\n基于上述分解，作者构建了CORE（Conceptual Reasoning Layer）的完整方法论：\n1.  **Global CORE（全局认知操作符库）**：定义一个有限的、通用的“认知动词”集合（如比较、总结、分解、评估等）。这构成了推理的“语法”，规范了每一轮的推理模式，避免了模式漂移。\n2.  **Local CORE（局部概念状态）**：设计一个轻量级、结构化的对象，用于存储单个任务的“语义快照”，包括任务摘要、约束、中间结果等。这构成了推理的“名词”，是持久化的意义载体。\n3.  **交互循环**：设计一个新的交互范式。每一轮，系统不再向模型发送冗长的历史，而是发送一个“概念通信包”，包含：**（当前操作符 + 局部概念状态 + 用户最新指令）**。模型输出后，系统再将其结果解析并更新回局部概念状态。\n\n**核心创新：通过分离“推理操作”与“推理内容”，并将它们从token流中剥离出来，实现了从“Token-First”到“Concept-First”的范式转变。**\n\n#### 第六步：确立边界——明确CORE的定位与价值\n\n最后，作者通过对比现有工作（如提示工程、思维链、记忆增强系统等），来精确定义CORE的独特贡献。他们强调：\n*   CORE不是更高级的提示工程，因为它引入了**持久化的语义状态**。\n*   CORE与思维链正交，因为它关注的是**跨轮次的连续性**，而非单轮内的推理步骤。\n*   CORE不存储外部知识，而是存储**任务本身的内部状态**。\n\n**最终结论：CORE是一个模型无关的、位于应用层的接口创新。它通过在系统层面分离概念与文本，为构建更稳定、高效、可解释的多轮LLM系统提供了一个全新的、可扩展的方向。**", "summary_translation": "\n大语言模型擅长处理单轮生成，但在多轮交互中，由于内部表示无法在对话轮次间持续存在，模型仍需从不断扩展的令牌历史中重建用户意图和任务状态。这种令牌优先范式会导致漂移、不一致的推理模式以及随着对话深入而不断增长的提示。我们提出了CORE，一种概念优先交互层，旨在提升多轮交互的稳定性，且无需修改模型权重。CORE将一个小型通用认知算子库与一个持久的Local Concept (局部概念) 相结合，后者是一个紧凑的语义状态，用于捕捉任务、约束、偏好和中间结果。在每次模型调用中，模型仅需接收该概念状态、用户的最新指令以及所选的算子，从而无需重放完整的对话历史。一个初步模拟CORE行为的原型显示，其累积提示令牌减少了约42%，但该数值反映了原型测试的特定条件，不应被视为真实世界中的性能估算。CORE提供了一种模型无关机制，将概念推理与语言生成分离开来，为构建更稳定的多轮系统指明了一个可扩展的方向。", "summary_generated_time": "2025-12-11 11:29:49", "summary_model": "z-ai/glm-4.6"}, {"index": "#36", "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "link": "/arxiv/2512.09897", "arxiv_id": "2512.09897", "authors": "Haoye Lu, Pavan Seshadri, Kaheer Suleman", "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-12-10", "category": "cs.CL", "crawl_time": "2025-12-11T11:00:05.211408", "filter_reason": "这篇论文符合筛选标准，应被保留。 **判断过程如下:** 1.  **第一步：核心判断** *   **论文本质**: 这篇论文的核心贡献是提出了一种名为SCOPE的新框架，用于构建和改进LLM智能体的规划能力。它并非将LLM或现有智能体框架简单地应用到一个新领域，而是针对现有LLM规划智能体（如ADaPT）在推理时需要反复调用LLM导致效率低下的问题，提出了一种创新的解决方案：在初始化时利用LLM生成子目标来预训练一个轻量级学生模型，从而在后续推理中摆脱对LLM的依赖。这本质上是对智能体架构和训练过程的改进。 *   **结论**: **保留**。论文的核心是关于“构建、改进LLM智能体”的方法论，完全符合第一步的保留标准。 2.  **第二步：正面指标** *   论文明确涉及了多个核心关注点： *   `Agentic AI` / `LLM-based Agents`: 整篇论文都在讨论如何构建一个基于LLM的规划智能体。 *   `Planning`: 这是论文最核心的关键词和贡献点，提出了一种新的“分层规划”方法。 *   `ReAct`: 虽然没有直接使用ReAct这个词，但其分层规划、目标分解的思想与ReAct等Agentic推理范式一脉相承。 *   **结论**: 论文在“单智能体”方向的“规划”能力上具有很强的正面指标。 3.  **第三步：排除标准** *   论文的主要贡献不在于安全、对齐、可解释性或水印。虽然它提到了其方法可能导致“可解释性降低”，但这只是效率提升带来的一个副作用，而非研究的主要贡献。 *   论文的研究环境是纯文本的，不涉及视觉或多模态内容。 *   **结论**: 未触发任何排除标准。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 该论文是典型的关于“智能体如何进行规划”的研究。它提出了一个具体的Agentic框架（SCOPE）来解决复杂任务中的多步推理问题，完全符合“保留”的条件，而非仅仅提升LLM本身的基础数学或逻辑能力。 *   **自我演化**: 论文不涉及自我演化。LLM作为“一次性教师”后便不再参与，学生模型也是预训练后固定下来的，不会根据经验进行自我完善。但这不影响其被保留，因为研究课题的三个方向是“或”的关系。 5.  **第五步：最终决策** *   综合以上分析，论文“SCOPE”的核心贡献在于提出了一种新颖的、更高效的LLM智能体规划框架。它直接解决了现有Agentic LLM在规划任务中的一个关键瓶颈（计算效率），属于对智能体本身的构建和改进。该研究精准地落在“单智能体”范畴下的“规划”子方向，与研究课题“LLM智能体及其演化”的核心目标高度契合。因此，应将其保留。", "summary2": "\n本文旨在解决基于LLM的层次化规划方法在文本环境中计算成本高昂且依赖持续查询的问题。针对文本环境中的长期规划任务，我们提出了一种SCOPE方法，其核心是利用LLM作为一次性教师，在初始化时从演示轨迹中生成子目标，并以此预训练一个轻量级的层次化学生模型。在TextCraft环境上，通过任务成功率和推理时间等指标验证了其有效性。", "inspiration_trace": "\n### 作者思考过程的逻辑链还原\n\n#### 1. **宏观问题：长期规划在文本环境中的根本挑战**\n   - **观察起点**：作者从强化学习（RL）的核心难题出发——在开放、文本化的环境（如TextCraft）中，代理需执行长时程任务（如合成物品），但面临三大障碍：动作空间无限开放、环境观察模糊、奖励信号稀疏（仅任务完成时反馈）。这导致传统RL方法（如分层强化学习）依赖人工设计状态抽象或奖励函数，难以泛化到非结构化场景。\n   - **关键痛点**：现有方法（如ADaPT）虽引入LLMs进行高级规划，但需在训练和推理中反复调用LLM（如GPT-3.5），造成计算成本高昂（推理耗时164.4秒）和部署瓶颈。同时，LLM参数固定，无法适应目标任务，限制了性能提升。\n\n#### 2. **现有方法的局限性与新假设的形成**\n   - **深入分析**：作者剖析了LLM依赖的根源——LLMs虽编码世界知识，但作为“在线教师”持续生成子目标，导致效率低下。例如，ADaPT在每一步都查询LLM以动态调整子目标，但LLM未与环境交互，子目标可能次优。\n   - **核心假设**：作者提出一个颠覆性想法——能否将LLM从“持续教师”转为“一次性教师”？即，仅在初始化时利用LLM的语义知识生成子目标，然后训练轻量级学生模型独立执行。假设依据：LLM生成的子目标虽不完美（因无环境交互），但可能提供足够结构化指导，结合RL微调可弥补次优性。\n\n#### 3. **从假设到方法论的演进：SCOPE的诞生**\n   - **关键转折点**：作者意识到，子目标生成可从“动态查询”转向“静态提取”。具体而言，直接从人类演示轨迹中一次性提取子目标序列，而非训练中反复调用LLM。这源于两个洞察：\n     - **效率优先**：移除训练中的LLM查询，大幅降低计算开销。\n     - **知识蒸馏**：LLM作为教师，其输出（子目标）可“蒸馏”到学生模型中，但仅需一次。\n   - **方法框架化**：基于此，作者设计SCOPE（Subgoal-COnditioned Pretraining for Efficient planning），核心逻辑链：\n     - **初始化阶段**：用LLM分析演示轨迹，生成子目标分解函数（如Python程序），将轨迹切分为子序列（e.g., 在TextCraft中，从“获取原木”到“合成木板”）。\n     - **预训练阶段**：用这些子目标训练轻量级学生模型（分层代理：经理提议子目标，员工执行动作），通过模仿学习初始化。\n     - **微调阶段**：引入世界模型（模拟环境），用RL（如交叉熵方法）优化代理，使子目标适应环境动态，补偿LLM的次优性。\n   - **创新聚焦**：作者刻意简化LLM角色——仅作为“一次性知识源”，而非持续参与者。这解决了计算成本和适应性问题，但牺牲了子目标的可解释性（e.g., LLM子目标不如人工设计的直观）。\n\n#### 4. **验证与迭代：从思想到实证**\n   - **假设验证**：作者在TextCraft环境中测试SCOPE，对比ADaPT。结果支持假设：SCOPE成功率（0.56）超越ADaPT（0.52），推理时间降至3.0秒（提升55倍）。这证明“一次性教师”可行，即使子目标次优，RL微调可提升性能。\n   - **迭代洞察**：通过消融实验（如替换为人工子目标），作者发现子目标质量（而非可解释性）是关键——LLM子目标虽模糊，但与环境因果对齐时，仍提供强引导。若破坏对齐（e.g., 随机映射物品名），性能骤降，确认了“因果 grounded”的重要性。\n   - **最终升华**：作者将SCOPE定位为“高效替代方案”——它不追求完美子目标，而是平衡效率与性能，为文本规划提供可扩展路径。\n\n### 逻辑链总结\n作者从**宏观问题**（文本规划的效率瓶颈）出发，通过**观察现有方法缺陷**（LLM依赖的高成本），形成**核心假设**（一次性LLM指导可行），进而**演进方法论**（静态子目标提取+分层RL微调），最终**实证验证**（TextCraft实验）。思想脉络聚焦于“效率优先”原则：将LLM从动态组件转为静态初始化工具，用轻量模型实现高效部署。这一过程体现了从问题抽象到假设驱动的创新闭环。", "summary_translation": "\n在复杂的基于文本的环境中，长期规划面临着重大挑战，这源于其开放式的动作空间、模糊的观察以及稀疏的反馈。近期研究表明，大型语言模型蕴含了丰富的世界语义知识，可用于有效指导智能体在具身和纯文本场景中进行高层推理与规划。然而，现有方法通常严重依赖于在训练和推理过程中对 LLMs 的反复查询，这不仅导致计算成本高昂，也使其难以高效部署。此外，这些方法通常采用一个未经修改的预训练 LLM，其参数在整个训练过程中保持不变，因此无法适应目标任务。\n\n为解决上述局限，我们提出了 SCOPE (Subgoal-COnditioned Pretraining for Efficient planning, 基于子目标条件的高效规划预训练)，这是一种 one-shot (一次性) 分层规划器。它仅在初始化阶段利用 LLM 生成的子目标来预训练一个 lightweight student model (轻量级学生模型)。与此前通过在训练过程中反复提示 LLM 以自适应生成子目标来 distill (蒸馏) 其知识的方法不同，我们的方法直接从示例 trajectories (轨迹) 中派生出子目标。这一设计无需反复查询 LLM，显著提升了效率，但其代价是降低了 explainability (可解释性)，并且可能产生 suboptimal (次优) 的子目标。\n\n尽管这些子目标是 suboptimal (次优) 的，但我们在 TextCraft 环境上的实验结果表明，LLM 生成的子目标仍可作为基于文本的规划任务中 hierarchical goal decomposition (分层目标分解) 的一个强有力的起点。与基于 LLM 的分层智能体 ADaPT (Prasad et al., 2024) 相比（其 success rate (成功率) 为 0.52），我们的方法成功率达到 0.56，并将 inference time (推理时间) 从 164.4 秒大幅缩短至 3.0 秒。", "summary_generated_time": "2025-12-11 11:26:32", "summary_model": "z-ai/glm-4.6"}]}, {"name": "Machine Learning", "count": 3, "papers": [{"index": "#18", "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning", "link": "/arxiv/2512.09706", "arxiv_id": "2512.09706", "authors": "Kaichen He, Zihao Wang, Muyao Li, Anji Liu, Yitao Liang", "summary": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA", "subjects": "Machine Learning", "date": "2025-12-10", "category": "cs.LG", "crawl_time": "2025-12-11T11:00:06.031128", "filter_reason": "这篇论文完全符合你的研究范围，其核心贡献直接聚焦于构建和改进LLM智能体。以下是我的详细判断过程： 1.  **第一步：核心判断** - **保留**。这篇论文的本质是提出一个名为 `CrossAgent` 的新颖智能体模型和相应的训练流程。它的核心贡献不是将现有智能体应用于某个领域，而是**解决智能体本身的一个根本性局限**：即现有智能体被限制在静态、预定义的行动空间中。论文通过构建一个能够自主选择不同粒度行动（如高级API调用或低级GUI操作）的统一模型，直接推动了LLM智能体构建技术的发展。这完全符合“构建、改进LLM智能体”的核心目标。 2.  **第二步：正面指标** - 论文包含了多个核心关注点： - **核心范式**: `Agentic AI` (摘要中明确提及)。 - **智能体能力**: - `Planning`: 论文强调智能体能够“autonomously selects the most effective interface for each step of a trajectory”（为轨迹的每一步自主选择最有效的接口），这本质上是一种动态规划和决策能力。同时，摘要提到了“long-horizon reasoning”（长期推理），这是规划能力的核心体现。 - `Tool Use / Tool Augmentation`: 论文的核心就是让智能体掌握“heterogeneous action spaces”（异构行动空间），例如API、GUI事件等，这正是高级工具使用能力的体现。 - **演化机制**: 论文通过强化学习（`Multi-Turn Group Relative Policy Optimization (GRPO) algorithm`）来训练智能体，使其能够“learn adaptive action switching”（学习自适应行动切换），这是一种通过环境反馈进行自我完善和迭代的学习机制，与“自我演化”的理念高度契合。 3.  **第三步：排除标准** - 论文的主要贡献不涉及安全与对齐（Safety, Alignment, Interpretability等），也不以多模态或视觉模型本身为核心研究对象。虽然实验在Minecraft环境中进行（该环境有视觉输入），但论文的焦点是智能体的**行动决策逻辑**，而非视觉感知模型。因此，它没有触犯任何排除标准。 4.  **第四步：处理特殊和模糊情况** - **推理/规划**: 这篇论文是关于智能体如何在复杂任务中进行多步推理和规划的典型案例。它不是在提升LLM的基础数学或逻辑能力，而是在构建一个能让LLM在动态环境中进行有效规划和行动的框架。因此，根据规则，应该**保留**。 - **自我演化的应用**: 这篇论文虽然应用在Minecraft这个特定环境中，但其核心贡献是提出了一种新的**自适应行动选择机制和训练方法**，这是一种通用的智能体能力增强方案。根据规则，这种提出新“自我演化”或“自我完善”机制的论文，即使应用在特定领域，也应该**保留**。 5.  **第五步：最终决策** - 综合以上分析，这篇论文的核心是构建一个名为 `CrossAgent` 的新型LLM智能体，它通过创新的训练方法，掌握了在异构行动空间中进行自主规划和工具使用的能力。这直接命中了你研究目标中的“单智能体”方向，并触及了“自我演化”的训练机制。它不是简单的应用，而是对智能体核心能力的根本性改进。因此，这篇论文是高度相关且应该被保留的前沿研究。", "summary2": "\n本文旨在解决现有智能体因受限于静态动作空间而适应性不足的局限性。针对开放世界Minecraft中需要跨层级交互的复杂场景，我们提出了CrossAgent模型。其核心是结合冷启动监督微调与多轮GRPO算法的训练流程，使模型能自主选择最优动作空间。在包含800余项任务的OpenHA基准上，通过任务完成率（FT）和平均成功率（ASR）等指标验证，CrossAgent实现了SOTA性能，显著优于固定动作空间的基线模型。", "inspiration_trace": "\n好的，以下是我对这篇论文作者思考过程的系统性推演，旨在还原其从宏观观察到具体方法论的逻辑演进。\n\n---\n\n### **第一步：宏观观察与问题识别**\n\n**起点：AI智能体发展的范式转变与核心瓶颈**\n\n作者首先观察到了一个宏观趋势：AI智能体正从围绕大模型“工程化”复杂工作流（如设计工具链、规划模块），转向通过后训练直接打造“原生”智能体模型。然而，他们敏锐地捕捉到了这一新范式下的一个核心瓶颈：**现有智能体被“锁定”在单一、静态的动作空间中**。\n\n*   **具象化问题**：有的智能体只会调用API（高级、抽象），有的只会操作GUI（中级、视觉），有的只会输出底层机器人指令（低级、精确）。它们就像只会用一种特定工具的工匠，面对复杂多变的世界时，显得僵硬和脆弱。\n*   **提炼核心矛盾**：这种“一招鲜”的设计与真实世界需求的**动态性**和**异构性**之间存在根本矛盾。一个任务的最佳交互粒度（是应该用高级指令快速完成，还是用底层操作精细控制？）并非一成不变，甚至会随着任务的进展而变化。\n\n**核心问题由此诞生：能否构建一个统一的智能体模型，使其不再受限于单一动作空间，而是能像人类一样，根据上下文自主选择最合适的交互“工具”或“粒度”？**\n\n---\n\n### **第二步：形成核心假设与洞察**\n\n**从“是什么”到“应该是什么”**\n\n基于上述问题，作者提出了一个颠覆性的核心假设：**动作空间的选择本身，应该是一个可学习的、动态的决策过程，而非一个预设的、静态的约束。**\n\n*   **关键洞察**：最优的动作空间不仅在**任务之间**不同，更在**任务内部的每一步**都可能不同。论文中给出了一个生动的例子：一个研究智能体在大部分时间里用高效的搜索API，但当遇到验证码时，必须切换到精细的GUI操作。这揭示了“动态切换”的必要性。\n*   **确立研究目标**：因此，目标不再是训练一个在某个动作空间里表现最好的专家，而是训练一个能够**自主权衡**——在效率与精度之间、在高级抽象与底层控制之间——做出最优选择的“战略家”。\n\n---\n\n### **第三步：分解挑战与设计解决方案**\n\n**如何让模型学会“自主选择”？**\n\n这个宏大的目标带来了三个环环相扣的挑战，作者的设计思路正是为了逐一攻克它们：\n\n**挑战一：冷启动问题——模型如何先“听懂”多种语言？**\n\n*   **问题**：一个模型如果从未见过“GUI语言”或“API语言”，它自然无法使用。直接将混合数据扔给它，可能导致模型混淆或产生偏见。\n*   **解决思路**：**先教词汇，再教作文。** 必须有一个基础阶段，让模型首先学会理解和生成所有异构动作空间的“语法”和“语义”。\n*   **方法论雏形（Stage 1）**：**混合空间监督微调**。构建一个包含各种动作空间轨迹的平衡数据集，通过SFT让模型掌握解码和生成不同动作的基础能力。此阶段的目标是“能力”而非“决策”。\n\n**挑战二：单步决策问题——模型如何学会“选词”？**\n\n*   **问题**：即使模型能生成所有类型的动作，它如何知道在特定情境下，哪一种“词”（动作空间）是最佳选择？仅仅展示混合数据是不够的，需要一个明确的优化信号。\n*   **解决思路**：**将动作选择视为一个独立的、可优化的决策问题。** 需要一个奖励机制来告诉模型，在某个状态下，选择哪个动作空间更“好”。\n*   **方法论雏形（Stage 2）**：**单轮强化学习**。将问题简化为单步决策：给定一个状态，预测一个动作。奖励函数被设计为**动作空间无关**的——只要模型生成的动作（无论以何种形式）解析后的“原始动作”与正确答案一致，就给予奖励。这迫使模型学习哪种动作空间在特定情境下更可靠、更有效，从而形成初步的“策略偏好”。\n\n**挑战三：长期规划问题——模型如何学会“写一篇好文章”？**\n\n*   **问题**：一个在单步上看起来最优的选择（如用精细的Raw Action），在长轨迹中可能导致效率低下（如任务超时）。模型需要具备全局视野，平衡即时收益与长期效率。\n*   **解决思路**：**将优化目标从单步正确性提升到整个任务的成功率和效率。**\n*   **方法论雏形（Stage 3）**：**多轮强化学习**。在完整的任务轨迹上进行训练，奖励信号是最终的**任务成败**。同时，引入一个**成本惩罚项**（如生成的token长度），鼓励模型在能成功完成任务的前提下，偏爱更简洁、高效的动作空间（如高级API）。这教会了模型进行长周期的权衡与规划。\n\n---\n\n### **第四步：整合与验证**\n\n**从“分步解决”到“统一框架”**\n\n将上述三个阶段的解决方案串联起来，就构成了CrossAgent的核心训练流程：**SFT（学词汇） → STRL（学选词） → MTRL（学作文）**。这个渐进式的课程设计，确保了模型能够平稳地从掌握基础能力，过渡到学习局部最优策略，最终进化为具备全局视野的战略决策者。\n\n**验证思路：**\n\n*   **环境选择**：选择Minecraft作为验证场，因为它完美融合了需要不同动作粒度的场景（探索、战斗、合成GUI操作）。\n*   **对比实验**：与被限制在单一动作空间的“专家”模型对比，以证明“动态切换”的优越性。\n*   **消融实验**：移除训练流程中的某个阶段（如STRL），以证明每个阶段的必要性。\n*   **案例分析**：通过可视化模型在不同任务阶段的动作空间分布，直观展示其自适应策略，证明其决策是“智能”而非“随机”的。\n\n### **总结：作者的思考脉络**\n\n作者的思考过程是一个典型的**“观察-洞察-假设-分解-验证”**的学术创新链条：\n\n1.  **始于观察**：发现现有智能体“工具单一”的普遍局限。\n2.  **升华为洞察**：认识到最优交互粒度的“动态性”和“上下文依赖性”是关键。\n3.  **提出核心假设**：动作空间选择应是可学习的。\n4.  **分解问题**：将“学会选择”这一复杂目标拆解为“听懂”、“会选”、“善规划”三个递进的子问题。\n5.  **设计解决方案**：针对性地提出三阶段训练流程，每个阶段精准解决一个子问题。\n6.  **系统验证**：通过精心设计的实验，全方位证明了其方法论的有效性和优越性。\n\n最终，这篇论文的贡献不仅是提出了一个名为CrossAgent的模型，更重要的是，它为构建真正通用、自适应的AI智能体提供了一套全新的、可复现的**思想框架和训练哲学**。", "summary_translation": "\n智能体AI的范式正在从工程化的复杂工作流转向训练后原生模型。然而，现有智能体通常局限于静态、预定义的 `action spaces (动作空间)`——例如仅使用 `APIs (应用程序编程接口)`、`GUI events (图形用户界面事件)` 或 `robotic commands (机器人指令)`。这种刚性限制了其在动态环境中的适应性，因为在这些环境中，最优的交互粒度会因上下文而异。为弥合这一差距，我们提出了 `CrossAgent`，这是一个统一的智能体模型，它能够掌握 `heterogeneous action spaces (异构动作空间)`，并为 `trajectory (轨迹)` 中的每一步自主选择最有效的接口。我们引入了一个全面的 `training pipeline (训练流程)`，该流程将 `cold-start supervised fine-tuning (冷启动监督微调)` 与 `Multi-Turn Group Relative Policy Optimization (GRPO)` 算法相结合。这种方法使智能体能够学习 `adaptive action switching (自适应动作切换)`——即在无需人工指定规则的情况下，平衡高层效率与底层精度。在开放世界 `Minecraft` 环境中超过800个任务上的大量实验表明，`CrossAgent` 实现了 `state-of-the-art (最先进的)` 性能。通过动态利用多样化 `action spaces (动作空间)` 的优势，我们的模型显著优于 `fixed-action baselines (固定动作基线模型)`，在 `long-horizon reasoning (长程推理)` 中展现出卓越的泛化能力和效率。所有代码和模型均可在 https://github.com/CraftJarvis/OpenHA 获取。", "summary_generated_time": "2025-12-11 11:26:49", "summary_model": "z-ai/glm-4.6"}, {"index": "#86", "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL", "link": "/arxiv/2512.09629", "arxiv_id": "2512.09629", "authors": "Emanuele La Malfa, Ping Zhu, Samuele Marro, Sara Bernardini, Michael Wooldridge", "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-12-10", "category": "cs.LG", "crawl_time": "2025-12-11T11:00:06.093530", "filter_reason": "这篇论文完全符合您的研究范围，其核心贡献与您的筛选标准高度契合。以下是我的详细判断过程： 1.  **第一步：核心判断——保留** - **论文本质**: 这篇论文的核心贡献是提出一个全新的、端到端的**规划框架**。这个框架并非简单地将LLM作为工具应用于某个领域，而是构建了一个由多个LLM驱动的智能体（一个`orchestrator`和多个`sub-modules (agents)`）协同工作的系统。这个系统的目的是解决复杂的规划问题，其方法论本身就是关于“构建LLM智能体”的。 - **排除项检查**: - **非演化型应用**: 论文不是将已有框架应用于生物或金融，其贡献在于框架本身，因此不属于此项排除。 - **非Agentic的推理**: 论文明确提出了一个`Agentic`框架，涉及多个智能体协同、迭代完善，这远超出了提升LLM基础推理能力的范畴。 - **基础设施**: 论文不涉及硬件或部署优化。 2.  **第二步：正面指标——高度相关** - **核心范式**: 论文标题和摘要中明确提到了 `Agentic LLMs`，其描述的系统本质上是一个 `Multi-Agent System` (MAS)。 - **智能体能力**: 论文的核心是 `Planning`。它通过多个智能体协作，实现了对规划问题的迭代完善，这体现了 `Self-Correction` 或 `Self-Refinement` 的能力。同时，它将外部 `PDDL` 规划引擎作为工具使用，完全符合 `Tool Use / Tool Augmentation` 的定义。 - **多智能体**: 论文明确提到了 `orchestrator` 和 `sub-modules (agents)`，描述了一个由多个智能体角色分工、协作完成任务的系统。 - **演化机制**: 摘要中的 \"iteratively refined by sub-modules (agents)\" 直接对应了 `Iterative Improvement` 这一演化机制。 3.  **第三步：排除标准——未触发** - 论文的主要贡献不涉及安全、对齐、可解释性或水印。 - 论文不涉及视觉或多模态内容，PDDL是符号化的规划语言，不属于排除范围。 4.  **第四步：处理特殊和模糊情况——符合保留规则** - **推理/规划**: 这篇论文是“保留”情况的完美范例。它不是在研究如何让LLM本身更会做数学题，而是在研究如何构建一个**智能体框架**来让LLM解决复杂的规划任务。这与 `ReAct`、`ToT` 等Agentic规划范式一脉相承，是其方法论的创新和扩展。 **最终决策**: 综合以上分析，这篇论文的核心贡献在于**构建了一个由多个LLM智能体协同工作的、具备工具使用和自我完善能力的规划框架**。这直接命中了您研究课题中的“单智能体”和“多智能体”方向，并触及了“自我演化”的迭代改进机制。因此，这篇论文是您需要筛选的前沿论文，应予以保留。", "summary2": "\n本文旨在实现从自然语言到可验证计划的端到端自动化。针对人类用自然语言书写的模糊规划需求，我们提出了一种由LLM驱动的编排器，它能动态生成多智能体工作流以迭代精化PDDL模型。我们在Google NaturalPlan、PlanBench、Blocksworld和Tower of Hanoi等基准上，通过准确率等指标验证了其有效性，相比基线LLM有显著性能提升。", "inspiration_trace": "\n好的，以下是我基于您提供的论文内容，对作者提出其核心方法的逻辑链进行的系统性推演，旨在还原其思考过程。\n\n---\n\n### **作者核心方法的逻辑演进推演**\n\n#### **第一步：观察到一个根本性的“能力错位”**\n\n作者的思考始于对人工智能领域两大范式的深刻洞察：\n\n1.  **经典AI规划（以PDDL为代表）**：其优势在于**形式化、可验证、能保证正确性和最优性**。但其致命弱点是**门槛高、不灵活**，需要专家手工编写领域模型，且无法处理自然语言中常见的模糊性、矛盾性和不完整性。\n2.  **大语言模型（LLMs）**：其优势在于**强大的自然语言理解和泛化能力**，能轻松处理模糊指令。但其劣势同样明显：在需要**长程、多步、逻辑严谨的规划任务上表现糟糕**，容易产生幻觉，无法提供可靠性保证。\n\n**核心矛盾**：我们如何构建一个既能理解人类模糊意图（LLM的优势），又能生成可信赖、可验证的行动计划（PDDL的优势）的系统？这是作者试图解决的宏观问题。\n\n#### **第二步：审视现有解决方案的局限性**\n\n作者接着审视了当时试图弥合这一鸿沟的主流方法，即“LLM作为前端，PDDL规划器作为后端”的混合模式（如LLM+P）。\n\n*   **初步认可**：作者承认这比纯LLM直接生成计划要好，因为它引入了符号推理的“正确性锚点”。\n*   **发现关键缺陷**：作者敏锐地指出，这些现有方案大多是**“固定的、预定义的工作流”**。这意味着，LLM一次性地将自然语言翻译成PDDL，然后交给规划器。这个过程是**静态的、一次性的**。\n\n**思考的转折点**：如果LLM第一次生成的PDDL模型有细微的语义错误、语法不兼容特定求解器、或者未能完全捕捉到人类指令中的隐含约束，这个静态管道就会失败。它缺乏一个**自我诊断和自我修复**的机制。这便是作者要突破的关键瓶颈。\n\n#### **第三步：提出核心假设——“规划即迭代优化过程”**\n\n基于对静态管道缺陷的认识，作者提出了一个核心假设：**将自然语言完美地翻译成可执行的PDDL模型，不应被视为一个单一的翻译任务，而应是一个动态的、迭代的优化过程。**\n\n这个思想类似于人类专家解决问题的方式：\n1.  **初步理解与草拟**：根据指令，先形成一个初步方案（第一版PDDL）。\n2.  **验证与诊断**：用工具（规划器、验证器）来测试这个方案，找出问题所在（语法错误、逻辑矛盾、不可解等）。\n3.  **针对性修复**：根据诊断出的具体问题，调用相应的专业知识进行修复。\n4.  **循环往复**：直到方案通过所有验证，满足所有要求。\n\n这个“草拟-验证-修复”的循环，就是作者方法论的核心思想。\n\n#### **第四步：引入“智能体”范式来结构化优化过程**\n\n如何将这个抽象的“迭代优化”过程工程化？作者借鉴了多智能体系统的思想，但进行了创新性应用：\n\n*   **任务分解**：将“修复PDDL”这个复杂任务，分解为一系列更小、更专业的子任务。例如：\n    *   修复语法错误。\n    *   检查时间逻辑的一致性。\n    *   确保约束条件被正确编码。\n    *   适配特定求解器的语法。\n*   **专家智能体**：为每个子任务设计一个专门的“智能体”。每个智能体都是一个LLM，被赋予了特定的角色、知识和工具（如读取验证器日志）。这使得系统具备了**专业化分工**的能力。\n\n#### **第五步：设计“编排器”——实现动态工作流的核心**\n\n有了专家智能体，谁来决定在哪个阶段调用哪个专家？如果这个调度逻辑是硬编码的，系统又会变得僵化。\n\n**最终的解决方案**：设计一个**“编排器”智能体**。这个编排器是整个系统的大脑，它不直接解决规划问题，而是：\n1.  **观察全局状态**：接收原始的自然语言指令、当前版本的PDDL、规划器的运行日志、验证器的错误报告等所有信息。\n2.  **进行战略决策**：基于这些信息，动态判断当前最紧迫的问题是什么（是语法问题？还是逻辑问题？）。\n3.  **动态调度**：从“专家智能体池”中选择最合适的一个来执行修复任务。\n\n这个“编排器+专家智能体”的设计，完美实现了作者最初设想的**“动态创建多智能体工作流”**，彻底摆脱了静态管道的束缚。\n\n#### **第六步：闭合“端到端”的最后一环**\n\n至此，系统已经可以从自然语言生成一个经过反复验证和优化的、正确的PDDL计划。但作者的目标是“端到端”，意味着最终输出要对人类友好。\n\n*   **最后的润色**：因此，在流程的末端，增加了一个**“自然语言翻译”智能体**。它的任务是将形式化的PDDL计划，忠实地、一步步地翻译回人类可读的自然语言描述。\n\n这确保了系统不仅**“做得对”**（通过PDDL保证），还**“说得清”**（通过自然语言翻译保证），完成了从人类意图到可解释行动的完整闭环。\n\n---\n\n### **总结：思想的演进脉络**\n\n作者的思考过程是一个典型的**“观察-批判-假设-构建”**的学术创新路径：\n\n1.  **始于观察**：发现LLM与经典规划器之间的“能力错位”。\n2.  **经由批判**：指出当时主流的“静态管道”方案缺乏鲁棒性和自修复能力。\n3.  **形成假设**：提出“规划应是一个迭代优化过程”的核心洞见。\n4.  **构建方案**：巧妙地引入“编排器+专家智能体”的动态多智能体架构，将迭代优化的思想工程化，并最终通过自然语言翻译实现了真正的“端到端”。\n\n这个逻辑链条清晰地展示了作者如何从一个宏观的领域痛点出发，逐步聚焦，最终设计出一个既具理论创新性（动态工作流）又有实践价值（端到端自动化）的解决方案。", "summary_translation": "\n本文提出了一种由验证器支持的端到端规划框架。一个编排器接收以自然语言书写的人类规范，并将其转换为一个 PDDL (Planning Domain Definition Language，规划领域定义语言) 模型；在该模型中，领域和问题由子模块进行迭代完善，以解决常见的规划需求（如时间约束和最优性），以及人类规范中可能存在的模糊性和矛盾性。随后，经过验证的领域和问题被传递给一个外部规划引擎以生成规划方案。编排器和智能体均由大语言模型驱动，且在流程的任何阶段都不需要人工干预。最后，一个模块将最终的规划方案翻译回自然语言，以提高人类可读性，同时保持每个步骤的正确性。我们在多个领域和任务上展示了我们框架的灵活性和有效性，包括 Google NaturalPlan benchmark 和 PlanBench，以及像 Blocksworld (积木世界) 和 Tower of Hanoi (汉诺塔) 这样的规划问题（在这些问题上，LLM 即使在处理小规模实例时也难以胜任）。我们的框架可以与任何 PDDL planning engine (规划引擎) 和 validator (验证器) 集成（我们已经测试了 Fast Downward、LPG、POPF、VAL 和 uVAL），这标志着在由 LLM 辅助的端到端规划方面迈出了重要一步。", "summary_generated_time": "2025-12-11 11:25:23", "summary_model": "z-ai/glm-4.6"}, {"index": "#97", "title": "Architectures for Building Agentic AI", "link": "/arxiv/2512.09458", "arxiv_id": "2512.09458", "authors": "Sławomir Nowaczyk", "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-12-10", "category": "cs.LG", "crawl_time": "2025-12-11T11:00:06.103898", "filter_reason": "这篇论文完全符合你的研究范围，是一篇高度相关的核心文献。我的判断过程如下： 1.  **第一步：核心判断——保留** 论文的核心贡献是提出一个用于构建Agentic AI的**架构框架**和**分类法**。它不是将智能体作为工具去解决某个特定领域的问题，而是直接研究智能体本身应该如何被设计和构建。这完全符合“核心贡献在于构建、改进或演化LLM智能体”的要求。 2.  **第二步：正面指标——高度匹配** 论文摘要中包含了你关注的所有核心范式和能力： *   **核心范式**: 明确提到了 `Agentic AI` 和 `Multi-Agent Systems`。 *   **单智能体能力**: 详细列出了智能体的关键组件，包括 `planner` (规划), `tool-router` (工具使用), `memory` (记忆), `verifiers` (自我修正), 以及 `self-improvement agents` (自我演化)。 *   **多智能体**: 明确将 `multi-agent systems` 作为其分类法中的一个重要类别。 *   **演化机制**: 直接提到了 `self-improvement agents`，这与你的“自我演化”方向完全一致。 3.  **第三步：排除标准——未触发** *   **安全与对齐**: 虽然摘要中提到了 `safety monitor` 和 `reliability`，但论文的**主旨**是探讨如何通过**架构设计**来实现可靠性，安全只是其中一个组件。论文的核心是“如何构建”，而不是“如何对齐”。因此，它不属于被排除的安全与对齐研究。 *   **多模态与视觉**: 论文未涉及相关内容。 4.  **第四步：处理特殊和模糊情况** *   **推理/规划**: 论文明确将 `planner` 作为智能体架构的核心组件之一，并讨论了其在闭环决策中的作用，这完全符合“保留”关于智能体规划框架的论文的标准。 **最终决策**: 这篇论文的标题和摘要清晰地表明，其核心贡献是**系统性地梳理和提出构建Agentic AI的架构原则和分类方法**。它直接覆盖了你研究的三个核心方向（单智能体、多智能体、自我演化），并深入探讨了规划、工具使用、记忆等关键能力。这并非一篇应用型或基础设施型论文，而是一篇关于Agentic AI“如何构建”的框架性、方法论论文，与你的研究目标高度契合。因此，应予以保留。", "summary2": "\n本文旨在提升 `agentic AI` 系统的可靠性。针对构建 `agentic AI` 的通用场景，我们提出了一种原则性架构框架，其核心在于组件化、规范化接口和显式控制回路。我们通过理论分析和一个诊断代理的运行示例，阐述了该框架如何通过设计选择（如 `simulate-before-actuate`）来保障系统可靠性，并提出了一个实用的代理分类法。", "inspiration_trace": "\n好的，以下是我对作者撰写《Architectures for Building Agentic AI》一文时，其核心思想演进过程的系统性推演。\n\n---\n\n### 作者核心思想的逻辑演进链\n\n#### 第一步：观察与问题识别\n\n*   **宏观观察：** 当前AI领域，尤其是生成式AI（如大语言模型），其研究焦点 overwhelmingly 集中于模型本身——如何让模型更大、数据更多、能力更强。人们普遍相信，更强的模型自然会带来更强的系统。\n*   **关键问题：** 然而，当这些强大的模型被部署到真实的、自主的“代理”系统中时，它们暴露出了根本性的脆弱性。模型会“幻觉”、行为不可预测、难以审计，在复杂任务中容易失控。一个看似强大的模型，在闭环决策系统中可能成为一个不可靠的“黑箱”。\n*   **初步结论：** 单纯优化模型，无法解决系统级的可靠性问题。可靠性并非模型的内在属性，而是系统涌现出的属性。我们问错了问题。\n\n#### 第二步：提出核心假设\n\n*   **核心假设：** **代理AI的可靠性，首先且最重要的是一个架构问题。**\n*   **思想转变：** 作者将视角从“模型中心论”转向“系统中心论”。他认为，模型只是系统中的“推理引擎”，而系统的整体可靠性——即能否在约束下持续、安全、可预测地达成目标——取决于这个引擎被如何“安装”、“连接”和“管控”。\n*   **类比：** 这就像一台高性能的发动机（模型），如果把它装在一个没有刹车、没有底盘、没有仪表盘的车壳里，它不仅无法提供可靠的驾驶体验，反而极其危险。可靠性来自于刹车系统、底盘结构、仪表盘等“架构”设计。\n\n#### 第三步：思想溯源与融合\n\n*   **回溯经典：** 作者意识到，AI代理并非全新概念。经典AI领域（如BDI、反应式/慎思式架构）已经对“目标”、“规划”、“承诺”等控制抽象有深入研究。这些经典理论提供了宝贵的“控制骨架”，确保了行为的可预测性和可解释性。\n*   **正视新生：** 同时，作者也承认生成式AI带来了颠覆性变化：它用端到端的神经网络替代了手写规则，极大地提升了灵活性和泛化能力，但也引入了前所未有的不确定性（如工具调用的随机性、推理步骤的不可靠性）。\n*   **融合之道：** 关键在于**融合**——用经典架构的“纪律性”来约束和引导生成式模型的“灵活性”。架构负责“治理”（Governance），模型负责“提案”（Proposal）。这正是文章标题“Architectures for Building Agentic AI”的深层含义。\n\n#### 第四步：构建分析框架\n\n*   **挑战：** 现代代理系统形态各异，单一的“最佳架构”并不存在。如何系统性地讨论这个问题？\n*   **解决方案：** 作者没有提出一个具体的架构，而是构建了一个**模式分类法**。他将纷繁复杂的代理系统归纳为五个核心模式：工具使用、记忆增强、规划与自我改进、多智能体、以及具身/网络代理。\n*   **分析维度：** 对于每一种模式，作者都从统一的维度进行分析：\n    1.  **它如何增强能力？** (e.g., 工具使用扩展了行动边界)\n    2.  **它引入了哪些独特的可靠性风险？** (e.g., 工具使用可能导致“幻觉工具调用”)\n    3.  **需要哪些架构上的“可靠性工具”来对冲这些风险？** (e.g., 用白名单和Schema验证来对冲幻觉风险)\n*   **框架价值：** 这个框架将讨论从“哪个架构最好”转变为“在特定模式下，如何通过架构设计来管理其固有的风险-收益权衡”。这使得分析既有普适性，又有针对性。\n\n#### 第五步：提炼最终方法论\n\n*   **从分析到设计：** 在对各种模式进行分析后，作者提炼出了一套跨越所有模式的、可操作的**设计原则**。这些原则是确保可靠性的“第一性原理”。\n*   **三大支柱：**\n    1.  **组件化：** 将系统分解为职责单一的模块（目标管理器、规划器、工具路由器、执行器、验证器等）。这能隔离故障，限制“爆炸半径”。\n    2.  **有纪律的接口：** 在组件之间建立强约束的“契约”。使用Schema验证、最小权限原则、幂等性设计等，将模型自由、不可控的文本输出，转化为可预测、可审计的结构化操作。\n    3.  **显式的控制与保障循环：** 在系统中嵌入“监督者”。通过验证器、模拟器、安全监控器、预算和终止条件等，形成持续的反馈回路，确保行为不偏离轨道，并能优雅降级。\n*   **最终结论：** 可靠的代理AI不是“炼”出来的，而是“建”出来的。它源于一种设计哲学：**模型负责提出方案，而架构负责裁决、执行和兜底**。通过这套方法论，我们可以将一个强大但不可靠的“天才模型”，转变为一个强大且可靠的“专业系统”。\n\n---\n\n**总结：** 作者的思考路径，是从一个普遍的行业现象（模型崇拜）出发，洞察到其根本缺陷（系统不可靠），进而提出一个颠覆性的核心假设（可靠性是架构属性）。然后，他通过融合经典理论与新生技术，构建了一个系统性的分析框架，最终提炼出一套普适的、可操作的设计原则，为解决“如何构建可靠的代理AI”这一核心问题提供了清晰的路线图。", "summary_translation": "\n\n本章认为，智能体与生成式AI的可靠性主要是一种架构属性。我们将智能体系统定义为在闭环中运行、以目标为导向、使用工具的决策者，并阐述了可靠性是如何通过有原则的组件化、规范化的接口以及明确的控制与保障循环而产生的。其中，有原则的组件化包括 goal manager (目标管理器)、planner (规划器)、tool-router (工具路由器)、executor (执行器)、memory (记忆模块)、verifiers (验证器)、safety monitor (安全监控器) 和 telemetry (遥测)；规范化的接口则指 schema-constrained (模式约束的)、validated (经过验证的) 和 least-privilege tool calls (最小权限工具调用)。在经典理论基础之上，我们提出了一套实用的分类法——tool-using agents (工具使用智能体)、memory-augmented agents (记忆增强智能体)、planning and self-improvement agents (规划与自我改进智能体)、multi-agent systems (多智能体系统) 以及 embodied or web agents (具身或网络智能体)——并分析了每种模式如何重塑 reliability envelope (可靠性边界) 与 failure modes (失效模式)。我们提炼出了一系列设计指导原则，涵盖 typed schemas (类型化模式)、idempotency (幂等性)、permissioning (权限管理)、transactional semantics (事务语义)、memory provenance and hygiene (记忆来源与卫生)、runtime governance (运行时治理，包括 budgets (预算) 和 termination conditions (终止条件)) 以及 simulate-before-actuate safeguards (先模拟后执行的安全保障)。", "summary_generated_time": "2025-12-11 11:26:28", "summary_model": "z-ai/glm-4.6"}]}, {"name": "Multiagent Systems", "count": 2, "papers": [{"index": "#2", "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection", "link": "/arxiv/2512.09396", "arxiv_id": "2512.09396", "authors": "Zishu Wei, Qixiang Ma, Xavier Hu, Yuhang Liu, Hui Zang, Yudong Zhao, Tao Wang, Shengyu Zhang, Fei Wu", "summary": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.", "subjects": "Multiagent Systems, Artificial Intelligence", "date": "2025-12-10", "category": "cs.MA", "crawl_time": "2025-12-11T11:00:05.068560", "filter_reason": "这篇论文完全符合您的研究范围，应当保留。我的判断过程如下： **第一步：核心判断——保留** 这篇论文的本质是提出一个名为GAIR的**新颖的LLM智能体框架**。它的核心贡献并非简单地将现有智能体应用于GUI自动化领域，而是**构建了一个具有独特架构和反思机制的智能体系统**来解决该领域的挑战。因此，它不属于“非演化型应用”的排除范畴，其核心是关于“构建、改进LLM智能体”的方法论。 **第二步：正面指标——高度匹配** 论文包含了多个您关注的核心正面指标： 1.  **核心范式**: 论文明确提出了一个 `LLM-based Agent` 框架。 2.  **智能体能力**: 论文的核心机制 `Information-Joint Reasoning` 涉及 `Planning` 和 `Reasoning`。更关键的是，它提出了 `Group Reflection` 机制，这直接对应了 `Self-Reflection` 和 `Self-Correction`。 3.  **多智能体**: GAIR的架构包含一个通用模型和多个GUI专用模型，由通用模型进行协调和决策，这体现了多智能体系统中协作与分工的思想。 4.  **演化机制**: `Group Reflection` 是一种明确的**自我演化**机制。当智能体（通用模型）判断信息不足时，它会进入反思状态，通过指导其他模型来获取更高质量的信息，从而迭代优化自身的决策能力。这完全符合“通过经验、反思进行自我完善和迭代”的定义。 **第三步：排除标准——未触发** 1.  **安全与对齐**: 论文未涉及安全、对齐、可解释性等主题。 2.  **多模态与视觉**: 虽然论文使用了MLLMs来处理GUI（视觉信息），但它们是作为智能体**感知环境的工具**。研究的核心是GAIR这个框架本身，而不是MLLMs的视觉能力。这符合您设定的“除非它们被用作智能体感知环境的工具，而不是研究的核心”的例外情况。 **第四步：处理特殊和模糊情况——符合保留规则** 1.  **推理/规划**: 论文的推理是嵌入在智能体框架中的多步决策过程，属于智能体的规划能力，而非提升LLM本身的基础推理能力。 2.  **自我演化的应用**: 这是本论文最关键的亮点。它完美地符合了“自我演化的应用”的保留规则。论文的核心贡献正是提出了一种**新的“自我演化”机制**，即使它被应用在GUI自动化这个特定领域，也应该被保留。 **第五步：最终决策** 综合以上分析，这篇论文的核心贡献是构建了一个名为GAIR的LLM智能体框架，其创新点在于引入了“Group Reflection”这一**自我反思与演化机制**，使智能体能够迭代优化其信息收集和决策过程。该研究直接命中了您“单智能体”和“自我演化”两个核心方向，并且其多模型协作的架构也与“多智能体”思想相关。因此，这篇论文是您研究课题“LLM智能体及其演化”的典型前沿范例，应予以保留。", "summary2": "\n本文旨在构建能处理多样化任务的高性能GUI自动化代理。针对广泛的GUI自动化场景，我们提出了一种GAIR框架，其核心是利用通用MLLM对多个GUI专用模型进行信息联合推理与群组反思。我们在UI-I2E-Bench和ScreenSpot基准上通过成功率指标验证了其有效性。", "inspiration_trace": "\n好的，作为一名学术思维分析专家，我将为您系统性地推演作者提出GAIR框架的完整逻辑链，还原其从宏观问题到具体方法的思考演进过程。\n\n---\n\n### **作者产出GAIR框架的逻辑推演**\n\n#### **阶段一：宏观问题识别——GUI自动化的“不可能三角”**\n\n作者的思考始于一个宏大且现实的观察：**GUI自动化任务的极端多样性**。\n\n1.  **问题广度**：任务场景横跨日常应用（网页浏览、移动购物）到专业领域（CAD设计、视频编辑）。这不仅仅是界面不同，其背后的逻辑、操作范式和领域知识也截然不同。\n2.  **能力要求**：完成这些任务需要AI具备多维度的异构能力，包括但不限于：精准的视觉感知（定位小图标）、复杂的语义理解（解读专业术语）、高级的推理规划（多步操作决策）以及精确的执行控制（生成坐标）。\n\n由此，作者识别出一个核心矛盾，即**GUI自动化的“不可能三角”**：**通用性、专业性、高效性**难以在单一模型中同时实现。\n*   追求**通用性**的模型（如通用MLLM），往往缺乏对GUI密集布局和专业领域的深度理解。\n*   追求**专业性**的模型（如针对特定任务微调的GUI-MLLM），又受限于其训练数据，无法泛化到其他领域。\n*   而试图通过无限扩大模型和数据来同时兼顾两者，又会牺牲**高效性**（部署成本、推理速度）。\n\n这个“不可能三角”是作者所有后续思考的起点和要解决的根本问题。\n\n#### **阶段二：现有方案的局限与核心矛盾的深化**\n\n在识别了宏观问题后，作者审视了当时主流的研究路径，并进一步深化了对矛盾的理解。\n\n1.  **观察“单模型”路径的瓶颈**：\n    *   **通用模型**：作者敏锐地指出，通用MLLM的训练数据（自然图像）与GUI截图（密集、信息元素小）存在“分布鸿沟”。这导致它们在GUI感知上存在先天不足，容易忽略关键信息。\n    *   **专用模型**：作者观察到，像AGUVIS、Ferret-UI这类模型在各自领域表现出色，但存在“知识壁垒”。一个精通网页购物的模型，面对CAD软件时几乎是无用的。这种“垂直领域”的局限性使其无法构建一个普适的自动化系统。\n\n2.  **提炼出三大具体挑战**：基于对单模型路径的分析，作者将“不可能三角”具象化为三个亟待解决的技术挑战：\n    *   **信息对齐与判别**：如果使用多个专用模型，它们输出的信息可能冲突、冗余或片面。如何整合这些信息，并判断哪个模型在当前情境下更可信？\n    *   **决策与执行的冲突**：专用模型擅长“做什么”（如定位元素），但未必擅长“为什么这么做”（如基于任务全局进行推理）。决策的合理性与操作的精确性之间存在脱节。\n    *   **错误的避免与纠正**：一次错误的决策可能导致任务失败。系统如何在执行操作前，主动发现并纠正潜在的推理错误？\n\n至此，作者已经清晰地勾勒出了问题的全貌：**单一模型无法破解“不可能三角”，而组合多个模型又会带来新的整合与协调难题。**\n\n#### **阶段三：核心假设的形成——从“构建更强模型”到“构建更智能的系统”**\n\n面对上述困境，作者的思考发生了关键的范式转变。他们不再执着于“如何训练一个无所不能的超级模型”，而是提出了一个革命性的假设：\n\n**假设：我们能否不追求模型的“全能”，而是构建一个由“专才”和“通才”协同工作的“智能体系统”？**\n\n这个假设是GAIR思想的基石。它蕴含了几个核心洞见：\n\n1.  **能力解耦**：将GUI自动化所需的能力进行拆分。让“专才”（GUI-specific MLLMs）负责感知和执行，让“通才”（General-purpose MLLM）负责推理和决策。\n2.  **优势互补**：利用“专才”的领域深度和“通才”的广度知识与推理能力，形成1+1>2的效果。\n3.  **动态交互**：系统不应是静态的、一次性的信息处理。各组件之间应该能够动态对话、反馈和迭代，以应对复杂和不确定的情况。\n\n这个假设将问题从“模型工程”转向了“系统架构设计”，为后续的方法论开辟了全新的道路。\n\n#### **阶段四：方法论的系统化构建——GAIR框架的诞生**\n\n基于上述核心假设，作者开始系统化地构建GAIR框架，将抽象的“协同系统”思想具体化为可操作的机制。\n\n1.  **机制一：信息联合推理**\n    *   **角色分工**：这是对“能力解耦”假设的直接实现。\n        *   **“专才”团队**：部署多个GUI-specific MLLMs。它们如同一个专家组，从不同角度（有的擅长定位，有的擅长文本理解）对同一GUI界面进行信息提取。\n        *   **“通才”指挥官**：引入一个General-purpose MLLM作为中央决策者。它不直接看图，而是阅读“专家组”提交的文本报告。\n    *   **工作流程**：指挥官利用其强大的语言理解和世界知识，对来自多个专家的、可能存在冲突的信息进行整合、判别和加权，最终形成一个全面、可靠的认知，并据此做出高层决策。这直接解决了“信息对齐与判别”和“决策与执行的冲突”两大挑战。\n\n2.  **机制二：群体反思**\n    *   **触发条件**：这是对“动态交互”假设的深化，专门用于解决“错误的避免与纠正”挑战。当“通才”指挥官认为现有信息不足以做出可靠决策时（即置信度低），该机制被触发。\n    *   **反思过程**：系统进入一个迭代优化的循环。\n        *   **下达指令**：指挥官不再直接决策，而是反过来向“专家组”下达更具针对性的指令和提示。例如：“模型A，你擅长识别按钮，请重点检查这个区域是否存在‘确认’按钮。”\n        *   **重新观察**：“专家组”根据新指令，进行第二轮、更聚焦的信息提取。\n        *   **再次决策**：指挥官基于新的、更高质量的信息，重新进行推理和决策。\n    *   **核心价值**：这个机制将系统从“被动响应”升级为“主动求证”，赋予了系统自我纠错和深化理解的能力，极大地提升了鲁棒性和任务成功率。\n\n至此，GAIR的完整逻辑链条形成：**从“不可能三角”的宏观困境出发，通过批判性分析现有方案的局限，提出“专才+通才”协同的系统级假设，并最终通过“信息联合推理”和“群体反思”两大核心机制，将这一假设系统化、工程化，构建出一个高效、鲁棒且可扩展的GUI自动化智能体框架。**", "summary_translation": "\n构建用于 GUI automation (图形用户界面自动化) 任务的 AI 系统已成为研究热点，其中 MLLMs (多模态大语言模型) 被用于处理用户需求并生成操作。然而，GUI automation 涵盖了从文档处理、在线购物到 CAD (计算机辅助设计)、视频编辑等多种多样的任务。不同任务间的多样性要求用于 GUI automation 的 MLLMs 具备 heterogeneous capabilities (异构能力) 并掌握 multidimensional expertise (多维专业知识)，这为构建此类模型带来了挑战。为应对这一挑战，我们提出了 GAIR (GUI Automation via Information-Joint Reasoning and Group Reflection，即基于信息联合推理与群体反思的图形用户界面自动化)。这是一个新颖的基于 MLLM 的 GUI automation agent framework (智能体框架)，旨在通过整合 heterogeneous models (异构模型) 的知识与能力，构建性能更优的 GUI automation 智能体系统。由于不同的 GUI-specific MLLMs (面向特定GUI任务的MLLMs) 在不同数据集上训练，因而各有所长，GAIR 引入了一个 general-purpose MLLM (通用MLLM) 来联合处理来自多个 GUI-specific MLLMs 的信息，从而进一步提升该智能体框架的性能。该 general-purpose MLLM 同时也作为 decision maker (决策者)，负责根据先前收集的信息执行合理的操作。当 general-purpose MLLM 判断信息不足以做出合理决策时，GAIR 会进入 group reflection status (群体反思状态)。在此状态下，general-purpose MLLM 会根据各 GUI-specific MLLM 的优势与劣势，为其提供不同的 instructions and hints (指令与提示)，引导它们收集更具价值和准确性的信息，以支持更深层次的推理与决策。我们通过在 GUI benchmarks (GUI基准测试) 上进行的大量实验，评估了 GAIR 的有效性与可靠性。", "summary_generated_time": "2025-12-11 11:25:13", "summary_model": "z-ai/glm-4.6"}, {"index": "#3", "title": "WOLF: Werewolf-based Observations for LLM Deception and Falsehoods", "link": "/arxiv/2512.09187", "arxiv_id": "2512.09187", "authors": "Mrinal Agarwal, Saad Rana, Theo Sundoro, Hermela Berhe, Spencer Kim, Vasu Sharma, Sean O'Brien, Kevin Zhu", "summary": "Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.", "subjects": "Multiagent Systems, Artificial Intelligence", "date": "2025-12-09", "category": "cs.MA", "crawl_time": "2025-12-11T11:00:05.068857", "filter_reason": "这篇论文完全符合你的研究范围，核心依据如下： 1.  **第一步核心判断 (保留):** 论文的核心贡献是构建了一个名为 **WOLF** 的 **多智能体社交演绎基准**。它不是一个简单的应用，而是提出了一个全新的、结构化的 **多智能体系统框架**。摘要中明确提到，WOLF是一个“programmable LangGraph state machine”，用于研究“adversarial multi-agent interaction”。这完全符合“构建、改进LLM智能体”或“构建多智能体系统”的核心要求。 2.  **第二步正面指标 (高度匹配):** 论文包含了大量你的核心关注点： *   **核心范式:** 论文的核心是 `Multi-Agent Systems (MAS)`，研究的是LLM智能体之间的交互。 *   **多智能体:** 论文深入探讨了智能体间的 `Communication` (辩论回合)、`Collaboration` (村民合作) 与 `Negotiation`/博弈 (狼人欺骗)。它还研究了 `Social Learning` 和 `Agent Society` 的动态，即“演化的信任动态”。 *   **智能体能力:** 智能体需要进行复杂的 `Planning` (如何欺骗、如何投票)、利用 `Memory` (记住历史言论和投票来建立怀疑度)、以及 `Self-Reflection` (自我评估言论的诚实度)。 3.  **第三步排除标准 (未触发):** *   论文虽然研究“欺骗”，但其主要贡献并非 `Safety` 或 `Alignment`。它的目标是**衡量和评估**智能体在对抗环境下的欺骗与反欺骗能力，这是一个关于**智能体能力边界**的研究，而不是如何让智能体更安全或更对齐。因此，它不属于安全与对齐的排除范畴。 *   论文不涉及多模态或视觉内容。 4.  **第四步特殊/模糊情况 (不适用):** 论文的核心就是多智能体框架本身，不涉及推理/规划或自我演化应用的模糊地带。 **总结:** 这篇论文的核心贡献是**构建了一个用于研究LLM智能体在复杂、对抗性社交环境中进行欺骗和推理的多智能体基准和框架**。它直接命中了你研究焦点中的“多智能体”方向，并深入探讨了智能体间的通信、博弈、记忆和规划等关键能力。因此，这是一篇与你研究课题高度相关的前沿论文，应该被保留。", "summary2": "\n本文旨在解决LLM欺骗评估的静态局限性，实现对其欺骗产生与检测能力的分离测量。针对多智能体社交推理场景，我们提出了一种基于Werewolf游戏的多智能体基准WOLF，它通过角色化agent、可编程状态机、结合自我与同伴评估的欺骗测量协议及纵向怀疑分数追踪，实现了对欺骗产生与检测的分离测量。在100次模拟游戏（7320条陈述）中，通过欺骗产生率、检测精确率（71-73%）、召回率（48-61%）及怀疑度纵向趋势等指标验证了其有效性。", "inspiration_trace": "\n好的，以下是对《WOLF: Werewolf-based Observations for LLM Deception and Falsehoods》一文作者核心思路的逻辑推演，旨在还原其从宏观问题到具体方法的思考过程。\n\n---\n\n### **第一步：宏观观察与问题识别**\n\n**起点：LLM应用范式的转变。**\n作者首先观察到一个根本性趋势：大型语言模型（LLMs）正从单轮问答的工具，演变为在多智能体系统中扮演自主角色的参与者（如助手、顾问、决策者）。这种转变带来了新的、更复杂的风险。\n\n**聚焦核心风险：欺骗。**\n在多智能体交互中，最关键的风险不再是简单的错误信息，而是**战略性欺骗**。作者注意到，LLMs不仅具备了强大的欺骗能力（如GPT-4欺骗人类绕过验证），更关键的是，它们在**欺骗他人**和**识破他人欺骗**这两种能力上存在显著的不对称性。LLMs是高明的“骗子”，却是迟钝的“侦探”。\n\n**形成核心假设：**\n基于此观察，作者提炼出一个核心的、可验证的假设：**“欺骗能力比检测能力在LLM中发展得更快”**。这个假设成为了整篇论文的驱动力，将一个普遍的担忧转化为了一个具体的科学问题。\n\n---\n\n### **第二步：审视现有工具并发现根本缺陷**\n\n**目标：如何验证上述假设？**\n作者需要找到一个评估方法，能够同时且分离地测量LLM的“欺骗生产”和“欺骗检测”能力。他们审视了现有的两类工具：\n\n1.  **静态欺骗数据集（如MASK, OpenDeception）：**\n    *   **优点：** 可量化，有标签。\n    *   **致命缺陷：** 它们将欺骗简化为静态的文本分类任务，完全忽略了欺骗在真实世界中的**交互性、对抗性和长期演化性**。这就像通过看照片来学拳击，无法捕捉动态博弈的本质。\n\n2.  **多智能体游戏框架（如Werewolf Arena, AvalonBench）：**\n    *   **优点：** 提供了动态、对抗的交互环境，欺骗会自然涌现。\n    *   **致命缺陷：** 它们是“黑盒”。评估指标停留在游戏层面（如输赢、被投票出局），无法回答核心问题：**哪句话是谎言？谎言是如何被构造的？它被识破了吗？信任是如何随时间变化的？** 它们无法分离“欺骗生产”和“欺骗检测”这两个变量。\n\n**结论：存在一个关键的研究空白。**\n现有工具要么是动态但不可观测，要么是可观测但非动态。作者需要一个**既能实现动态交互，又能进行细粒度、可分离测量的新基准**。\n\n---\n\n### **第三步：构思理想解决方案的设计原则**\n\n为了填补上述空白，作者开始构思理想评估工具应具备的特性：\n\n1.  **环境必须是动态且对抗性的：** 必须有一个能自然激发欺骗和检测动机的场景。社交推理游戏是完美选择，而**狼人杀**因其角色不对称（狼人需欺骗，村民需检测）和明确的阶段性（夜晚行动、白天讨论），成为最佳载体。\n\n2.  **测量必须是可分离的：** 如何同时测量“生产”和“检测”？作者的关键洞见在于，将分析单元从“游戏”下沉到**“每一次公开陈述”**。\n    *   **测量“生产”：** 让**说话者自己**评估其陈述的诚实性（自我评估）。\n    *   **测量“检测”：** 让**所有其他玩家**评估该陈述的欺骗性（同伴评估）。\n\n3.  **分析必须是纵向的：** 信任不是一成不变的。需要一个机制来追踪怀疑度的演变。作者借鉴了时间序列分析的思想，采用**指数平滑**来更新每个玩家对其他玩家的怀疑分数，从而捕捉信任的累积和消散过程。\n\n4.  **欺骗必须是分类的：** 简单的“真/假”标签过于粗糙。为了深入理解欺骗的机制，需要一个**标准化的分类法**（如 omission, distortion, fabrication, misdirection），以探究不同欺骗策略的成功率。\n\n5.  **实验必须是可控且可复现的：** 为了使其成为一个严谨的学术基准，必须消除随机性带来的干扰。这意味着需要**固定的角色分配、严格的流程控制（如基于状态机的LangGraph）和完整的日志记录**，确保每次实验都可被精确复现和分析。\n\n---\n\n### **第四步：整合并形式化为WOLF方法论**\n\n在上述设计原则的指导下，作者将所有想法整合成一个具体的、可执行的框架，即**WOLF (Werewolf-based Observations for LLM Deception and Falsehoods)**。\n\n*   **核心架构：** 一个基于LangGraph的、可编程的狼人杀状态机，确保了流程的严格控制和可复现性。\n*   **核心创新：** 为每一次公开陈述，都附加一个**“双重评估”**模块——说话者的自我诚实度报告和同伴的欺骗性评分。这直接实现了“欺骗生产”与“欺骗检测”的分离测量。\n*   **核心指标：** 基于双重评估数据，构建了一套完整的指标体系，包括欺骗生产率、检测准确率、校准度（Brier分数）和纵向怀疑度趋势，全方位回答了引言中提出的三个研究问题。\n*   **最终产出：** WOLF不再仅仅是一个游戏，而是一个**动态、可控、可观测的“显微镜”**，专门用于在对抗性多智能体交互中，剖析LLM的欺骗与反欺骗能力。\n\n---\n\n### **总结：思想的演进脉络**\n\n作者的思考路径是一个典型的**“从现象到本质，从批判到建构”**的学术创新过程：\n\n**宏观趋势（LLM多智能体化） → 核心问题（欺骗能力不对称） → 现有方案批判（静态 vs. 黑盒） → 理想方案设计（动态+可分离+纵向+分类+可控） → 具体方法实现（WOLF框架）。**\n\n这个逻辑链条清晰地展示了，WOLF的诞生并非偶然，而是作者在深刻洞察LLM发展带来的新挑战后，对现有研究工具进行系统性反思，并基于严谨的设计原则，一步步构建出的针对性解决方案。其核心思想演进，始终围绕着如何**在动态交互中，实现对“欺骗”这一复杂行为的精细化、可分离测量**。", "summary_translation": "\n欺骗是多智能体推理 (multi-agent reasoning) 领域的一项根本性挑战：有效的系统必须策略性地隐藏信息，同时侦测他人的误导行为。然而，大多数评估将欺骗简化为静态分类 (static classification)，忽视了现实欺骗动态所具有的交互性、对抗性和纵向性 (longitudinal nature)。大型语言模型 (Large language models, LLMs) 能够令人信服地实施欺骗，但在侦测同伴的欺骗行为方面能力仍然较弱。我们提出了 WOLF，一个基于狼人杀 (Werewolf) 游戏的多智能体社交推理 (social deduction) 基准，它能够对欺骗生成 (deception production) 与检测 (detection) 进行分离式测量。WOLF 将基于角色的智能体 (role-grounded agents)（村民、Villager、狼人、Werewolf、预言家、Seer、医生、Doctor）嵌入到一个可编程的 LangGraph 状态机 中，该状态机具有严格的昼夜循环、辩论回合 和多数投票 机制。每一条陈述都作为一个独立的分析单元 (analysis unit)，同时记录发言者的自我评估诚实度 (self-assessed honesty) 和其他智能体的同伴评估欺骗性 (peer-rated deceptiveness)。欺骗行为通过一套标准化的分类法 (taxonomy)（隐瞒、omission、歪曲、distortion、捏造、fabrication、误导、misdirection）进行分类，而怀疑分数 (suspicion scores) 则经过纵向平滑 (longitudinally smoothed) 处理，以捕捉即时判断和演化的信任动态 (evolving trust dynamics)。结构化日志 (Structured logs) 记录了所有的提示、输出 和状态转换，以确保实验的完全可复现性 (reproducibility)。在总计 7,320 条陈述和 100 次运行的实验中，狼人角色在 31% 的回合中作出了欺骗性陈述，而同伴检测的精确率 (precision) 达到了 71-73%，总体准确率 (overall accuracy) 约为 52%。在识别狼人角色时精确率更高，但也会对村民角色产生误报 (false positives)。随着游戏回合的推进，对狼人角色的怀疑度从约 52% 上升至 60% 以上，而对村民和医生角色的怀疑度则稳定在 44-46% 左右。这种分化趋势表明，延长的互动能够在提高对欺骗性角色召回率 (recall) 的同时，不会加剧对诚实角色的误判。WOLF 将欺骗评估的研究范畴从静态数据集 (static datasets) 中拓展开来，为在对抗性多智能体互动中衡量欺骗与反欺骗能力提供了一个动态且可控的测试平台。", "summary_generated_time": "2025-12-11 11:25:21", "summary_model": "z-ai/glm-4.6"}]}], "overview": "\n### 今日AI论文速览 (2025-12-10)\n\n#### 开篇导语\n今日AI研究浪潮的核心指向了更强大、更自主的智能体。研究者们不再满足于简单的工具调用，而是从三个维度推动边界：一是重构智能体的**内部架构**，引入分层规划与概念推理，以提升多轮交互的稳定性；二是革新**训练范式**，大规模采用强化学习与进化算法，让智能体在动态环境中自主学习最优策略；三是将智能体推向**真实世界的复杂任务**，从GUI自动化到网络安全，进行严格的实战检验与评估。\n\n---\n\n### 主题分类与论文速览\n\n#### 一、 智能体架构的进化：从单一执行到分层规划\n这个主题聚焦于如何设计更稳定、更高效的智能体内部结构，使其能够处理复杂、长期的任务，而不仅仅是单次指令的响应。\n\n*   论文提出了 **CORE**，一个概念优先的交互层，通过维护一个持久的**Local Concept**来捕捉任务状态，从而解决了多轮对话中的上下文漂移问题，显著减少了提示词长度。(2512.09222 [cs.CL])\n*   介绍了 **SCOPE**，一种高效的分层规划方法，它仅在初始化时利用LLM生成的子目标来预训练一个轻量级学生模型，从而在推理时大幅减少对LLM的依赖并提升速度。(2512.09897 [cs.CL])\n*   提出了一个端到端的规划框架，使用LLM智能体将自然语言规范转换为可验证的**PDDL**模型，并迭代优化，最终交由外部规划器求解，实现了从自然语言到可执行计划的自动化流程。(2512.09629 [cs.LG])\n*   这篇论文主张智能体的可靠性主要源于其**架构设计**，并提出了一套基于**组件化**、**规范化接口**和**显式控制回路**的实用分类与设计原则，为构建可靠的智能体系统提供了理论指导。(2512.09458 [cs.LG])\n\n#### 二、 训练即核心：用强化学习与进化算法锻造智能体\n一个显著的趋势是，研究者们正从依赖静态提示转向使用动态的学习算法来训练智能体，使其能够通过试错和优化掌握复杂技能。\n\n*   提出了 **MOA** 框架，通过**多目标强化学习**策略，让角色扮演智能体在知识、风格、多样性等多个维度上同时进行优化，使8B模型在多项指标上媲美GPT-4o。(2512.09756 [cs.CL])\n*   介绍了 **RouteRAG**，一个基于强化学习的框架，使LLM能够自适应地在文本和图之间进行**混合检索**，并学习何时检索、何时生成，在复杂问答任务中实现了高效且端到端的优化。(2512.09487 [cs.CL])\n*   提出了 **CrossAgent**，一个统一的智能体模型，通过**Multi-Turn GRPO**算法进行训练，使其能够掌握异构的动作空间（如API、GUI），并自主选择最优交互粒度，在Minecraft中实现了SOTA性能。(2512.09706 [cs.LG])\n*   为解决6G零接触网络的安全挑战，提出了 **SecLoop** 框架和 **SA-GRPO** 算法，通过并行执行和群体反馈来迭代优化安全策略，实现了完全自动化的安全生命周期管理。(2512.09485 [cs.AI])\n*   介绍了 **ARTEMIS**（优化平台），一个无代码进化优化平台，通过**语义感知的遗传算子**自动联合优化智能体的提示、工具描述等配置，在多个基准上实现了显著性能提升。(2512.09108 [cs.AI])\n*   提出了 **IMAgent**，一个通过**端到端强化学习**训练的开源视觉智能体，专门用于处理复杂的多图像任务，并设计了专门的视觉反思工具来确保模型在推理中不忽略图像信息。(2512.08980 [cs.AI])\n\n#### 三、 走向真实世界：专业智能体的应用与评估\n智能体的能力正在被推向极限，在高度专业化甚至高风险的领域进行测试和应用，其表现开始与人类专家进行直接比较。\n\n*   提出了 **GAIR** 框架，通过一个通用模型协调多个专用的GUI模型，并引入**群体反思**机制，让模型根据彼此的优缺点进行信息补充，以应对多样化的GUI自动化任务。(2512.09396 [cs.MA])\n*   发布了 **WOLF**，一个基于“狼人杀”游戏的多智能体社交推理基准，它能够分离评估**欺骗的产生与检测**能力，为研究对抗性多智能体交互中的动态欺骗行为提供了可控的测试平台。(2512.09187 [cs.MA])\n*   发布了首个在真实企业网络环境中对AI智能体与人类网络安全专家的全面评估，其中提出的**ARTEMIS**（渗透测试框架）在性能上超越了大部分人类专家，展现了AI在系统化漏洞挖掘方面的巨大潜力。(2512.09882 [cs.AI])\n*   提出了 **AI Co-Artist**，一个由LLM驱动的交互式框架，允许用户通过直观的界面迭代演化和优化**GLSL着色器**，极大地降低了创意编码的技术门槛，展示了LLM在艺术创作领域的应用潜力。(2512.08951 [cs.AI])\n\n---\n\n### 今日看点\n\n*   **趋势观察：强化学习与进化算法正成为训练高级智能体的“新范式”。** 从多目标对齐（MOA）、混合检索（RouteRAG）到跨层级动作（CrossAgent），大量研究不约而同地采用RL或进化算法来训练智能体。这标志着智能体开发正从“工程调优”向“端到端学习”转变，让模型自主发现最优策略，而非依赖人工设计的复杂规则。\n\n*   **颠覆性观点：从“令牌优先”到“概念优先”的架构转变。** CORE论文挑战了当前LLM多轮交互中不断拼接历史令牌的主流做法。它提出将“概念”而非“令牌”作为交互的核心单元，通过持久化的概念状态来维持上下文。这种架构上的根本性变革，若能成熟，有望解决长对话中的上下文漂移和计算成本激增等核心痛点。\n\n*   **潜力技术：AI智能体在关键领域的“实战能力”得到验证。** 网络安全渗透测试的对比研究尤其值得关注。AI智能体不仅在成本和效率上展现出优势，其发现漏洞的能力已能媲美甚至超越大部分人类专家。这预示着AI智能体正从实验室的“玩具”走向关键基础设施的“准员工”，其可靠性和专业能力正在被真实世界的严苛标准所检验。\n\n*   **跨界融合：“智能体社会”的雏形初现。** 无论是GAIR中多个专用模型的协同反思，WOLF中不同角色的对抗博弈，还是ARTEMIS中多智能体的并行漏洞挖掘，都指向一个趋势：解决复杂问题可能不再依赖单一强大的智能体，而是需要一个由不同角色、不同能力的智能体组成的“社会”。如何设计这些智能体间的通信、协作与竞争机制，将成为下一个研究热点。"}