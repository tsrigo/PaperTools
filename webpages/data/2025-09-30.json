{"date": "2025-09-30", "categories": [{"name": "Artificial Intelligence", "count": 17, "papers": [{"index": "#11", "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning", "link": "/arxiv/2509.25004", "arxiv_id": "2509.25004", "authors": "Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo", "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.180595", "filter_reason": "这篇论文的核心贡献是提出CLPO（课程学习与策略优化相结合）的新算法，旨在增强大语言模型的通用推理能力。论文通过创建动态教学反馈循环，利用模型自身性能进行实时难度评估，构建在线课程，从而提高LLM在数学和通用推理任务上的表现。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求。论文关注的是基础能力改进和新训练范式，而非将LLM作为工具应用于特定领域。同时，论文包含了多个正面指标，如大语言模型、推理能力、强化学习方法等，且不涉及任何排除标准中的领域。因此，该论文非常符合研究范围。", "summary2": "本文旨在解决LLM推理中RLVR方法因统一训练样本处理导致的学习效率低下问题。针对不同难度问题，我们提出了一种CLPO（Curriculum-guided Learning for Policy Optimization）方法，通过在线课程学习和自适应问题重构机制实现引导式自我进化，并在八个数学和通用推理基准测试上通过pass@1指标验证了其有效性，平均提升6.96%。", "inspiration_trace": "# CLPO方法逻辑链分析：从问题观察到方法创新\n\n## 一、宏观问题：LLM推理能力提升的瓶颈\n\n### 1.1 核心观察\n作者首先观察到大型语言模型(LLM)在复杂推理任务上仍面临显著挑战，尤其是数学问题解决、科学探究和代码生成等需要多步推理的领域。虽然传统监督微调(SFT)可以初步对齐模型能力，但其依赖静态数据集，成本高昂且难以覆盖所有推理路径。\n\n### 1.2 现有强化学习方法的局限\n作者进一步观察到，新兴的强化学习与可验证奖励(RLVR)方法虽然允许模型从自身生成中学习，但存在关键缺陷：\n- **统一训练策略**：现有方法(如GRPO)对所有训练样本一视同仁，忽略了问题难度相对于模型当前能力的巨大差异\n- **低效探索**：对模型已掌握问题的过度探索和对挑战性问题缺乏有效指导\n- **性能瓶颈**：导致策略多样性下降和策略熵崩溃，限制长期改进\n\n## 二、问题聚焦：探索效率与质量的平衡\n\n### 2.1 现有改进路径的分析\n作者梳理了两类现有改进方法及其局限：\n\n**路径一：优化算法改进**\n- 代表工作：DAPO、GFPO等\n- 思路：通过提高探索数量或维持更高熵来增强模型探索能力\n- 局限：探索质量比数量更关键，高熵并不总能保证高效探索\n\n**路径二：引入外部指导**\n- 代表工作：Critique-GRPO、LUFFY\n- 思路：利用外部批评模型或专家轨迹提供精确指导\n- 局限：依赖昂贵的外部资源，未解决内源性学习问题\n\n### 2.2 核心假设形成\n基于以上分析，作者提出核心假设：\n> \"能否在不依赖外部指导的情况下，实现高效、有针对性的内源性学习？\"\n\n这一假设引导作者思考如何将模型自身rollout信息从简单的奖励计算信号提升为更高级的学习指导机制。\n\n## 三、理论突破：从静态学习到动态课程\n\n### 3.1 关键洞察\n作者获得关键洞察：**问题难度评估是优化学习效率的核心**。不同难度的问题对模型学习的价值不同，需要区分对待：\n- 已掌握问题：继续探索收益低\n- 中等难度问题：处于学习边缘，适合泛化训练\n- 困难问题：当前无法解决，需要简化处理\n\n### 3.2 理论框架转变\n作者提出从GRPO的\"静态学习范式\"转向\"动态课程学习范式\"：\n- **GRPO范式**：模型被动接收静态分布D中的问题，仅优化响应空间\n- **CLPO范式**：模型主动构建与自身能力共同进化的动态课程，同时优化问题空间和响应空间\n\n## 四、方法构建：CLPO三大核心机制\n\n### 4.1 在线课程学习(Online Curriculum Learning)\n**问题**：如何实时评估问题难度？\n**解决方案**：\n- 利用模型自身rollout性能计算经验准确率\n- 基于预定义难度阈值(τ_hard, τ_med)动态划分问题难度\n- 构建实时难度评估机制，为后续处理提供精确信号\n\n### 4.2 自适应问题重构(Adaptive Problem Restructuring)\n**问题**：如何根据难度调整问题以促进学习？\n**解决方案**：\n- 模型作为自己的\"教师\"，通过特定提示重构问题\n- 对中等难度问题：应用多样化策略，生成语义等价但表述变化的问题\n- 对困难问题：应用简化策略，降低认知复杂度\n- 保持原问题答案不变，确保学习目标一致性\n\n### 4.3 难度感知策略优化(Difficulty-aware Policy Optimization)\n**问题**：如何根据问题难度调整优化策略？\n**解决方案**：\n- 引入动态KL正则化机制\n- 对困难问题：减小KL约束(λ_hard < λ_non-hard)，扩大探索范围\n- 对非困难问题：增大KL约束，确保稳定性\n- 实现探索与利用的动态平衡\n\n## 五、整合与验证：CLPO框架形成\n\n### 5.1 方法整合\n作者将三大机制整合为CLPO(Curriculum-guided Learning for Policy Optimization)框架，形成\"引导自我进化\"(Guided Self-Evolution)的闭环：\n1. 评估问题难度 → 2. 自适应重构问题 → 3. 难度感知优化 → 1. 重新评估难度...\n\n### 5.2 实验验证\n通过八个数学和通用推理基准测试验证CLPO有效性：\n- 平均pass@1性能提升6.96%\n- 在最具挑战性的AIME2024基准上实现SOTA\n- 消融研究确认各组件贡献\n- 测试时间缩放分析证明模型探索路径多样性提升\n\n## 六、逻辑演进总结\n\n作者的创新思路遵循了清晰的逻辑链条：\n1. **观察现象**：LLM推理能力受限，现有RLVR方法效率低下\n2. **分析局限**：统一训练策略忽略问题难度差异，探索质量不足\n3. **形成假设**：能否通过内源性机制实现高效、有针对性的学习\n4. **理论突破**：从静态学习转向动态课程学习范式\n5. **方法构建**：设计三大核心机制解决难度评估、问题重构和优化调整\n6. **整合验证**：形成CLPO框架并通过实验验证有效性\n\n这一思路体现了从宏观问题到具体解决方案的系统思考，将课程学习理念与强化学习优化深度融合，实现了LLM推理能力训练的范式创新。", "summary_translation": "最近，在线可验证奖励强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）已成为提升大语言模型（Large Language Models, LLMs）推理能力的关键范式。然而，现有方法通常统一对待所有训练样本，忽视了问题难度相对于模型当前能力的巨大差异。这种统一的训练策略导致对模型已掌握问题的低效探索，同时缺乏对最具挑战性问题的有效指导，限制了学习效率和上限性能。为解决此问题，我们提出CLPO（课程引导的策略优化学习，Curriculum-guided Learning for Policy Optimization），这是一种在策略优化过程中创建动态教学反馈循环的新型算法。CLPO的核心利用模型自身的推理（rollout）性能进行实时难度评估，从而构建在线课程（Online Curriculum）。该课程进而引导自适应问题重构（Adaptive Problem Restructuring）机制，使模型成为自己的老师：它多样化中等难度问题以促进泛化，并简化具有挑战性的问题使其更易达成。我们的方法将静态训练过程转变为与模型能力共同进化的动态过程。实验表明，CLPO在八个具有挑战性的数学和通用推理基准测试中实现了最先进的性能，相比其他方法平均pass@1提升了6.96%，展示了其更高效训练更强推理能力模型的潜力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#2", "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following", "link": "/arxiv/2509.25148", "arxiv_id": "2509.25148", "authors": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu", "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data synergy.We evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.170215", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式(UniAPL)来改进LLM的基础能力。论文针对的是LLM后训练对齐问题，提出统一对抗偏好学习框架，将监督微调(SFT)和强化学习(RL)两种模态统一在一个框架中，解决分布不匹配问题。这明显属于改进LLM基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文包含多个相关主题： - 核心概念：明确关注LLMs，使用Qwen3系列模型进行实验 - 训练方法：涉及强化学习(RL)作为偏好学习的一种模态，并将其与SFT统一 - 能力方向：虽然不直接针对推理能力，但关注指令遵循能力(instruction-following)，这是一种重要的通用能力，对模型的整体表现和推理能力有基础性影响 第三，从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。虽然提到了\"beneficial and safe\"，但这只是作为背景，论文核心是提出统一训练框架。 最后，论文提出的UniAPL框架通过动态对齐策略分布与专家分布，解决了传统SFT后跟RL的顺序流程中的分布不匹配问题，这种方法论创新有望提升LLM的通用能力，包括其推理和问题解决能力。论文的实验结果也表明，UniAPL能够显著提升模型性能，甚至使小模型达到大模型的效果。 因此，尽管论文没有直接针对推理能力进行研究，但它提出的训练范式创新属于提升LLM基础能力的研究范畴，符合筛选标准。", "summary2": "本文旨在解决大型语言模型后训练对齐过程中的分布不匹配问题。针对静态专家数据与动态策略之间的分布差异，我们提出了一种统一对抗偏好学习(UniAPL)框架，并在多个指令跟随基准测试上通过准确率等指标验证了其有效性。", "inspiration_trace": "# UniAPL方法逻辑链推演\n\n## 一、宏观问题：AI对齐的核心挑战\n\n作者从AI安全与对齐的根本问题出发：如何确保强大的大型语言模型(LLMs)行为既有益又安全。他们观察到，后训练对齐过程本质上是一个统一的**偏好学习(Preference Learning)**挑战，包含两种不同模式：\n- 从示范偏好学习（如监督微调SFT）\n- 从比较偏好学习（如强化学习RL）\n\n## 二、关键观察：现有方法的根本缺陷\n\n作者深入分析了当前行业标准的\"SFT-then-RL\"流水线，发现其存在**分布不匹配(distributional mismatch)**的根本缺陷，导致两个相互关联的问题：\n\n### 1. 离线学习问题（模仿的脆弱性）\n- SFT在固定的专家数据分布(P_expert)上进行离线学习\n- 随着策略自身生成分布(P_policy)的漂移，学到的知识变得脆弱且难以泛化\n\n### 2. 在线学习问题（无基础的探索）\n- 后续RL阶段探索超出静态专家数据以提高泛化能力\n- 但缺乏对专家演示中丰富真实知识的实时访问，导致探索效率低下且易发生灾难性遗忘\n\n这种根本性分离使两种数据源无法协同正则化，导致对齐过程低效且不稳定。\n\n## 三、理论假设：重新概念化对齐问题\n\n作者提出核心假设：对齐不应被视为简单的信息衰减问题，而应重新概念化为**约束优化问题**。最优对齐策略必须同时满足来自两个来源的约束：\n\nπ_aligned = arg max_πθ E[R_ψ(y|x)] subject to D(πθ || π*) ≤ ε\n\n其中，R_ψ是来自偏好信息的奖励函数，约束条件强制策略保持对专家策略的忠诚度。\n\n## 四、解决方案：构建对抗性桥梁\n\n基于上述假设，作者提出了**统一对抗偏好学习(UniAPL)**框架，通过引入对抗性目标来动态弥合策略分布与专家分布之间的差距。\n\n### 1. 对抗性梯度信号\n引入判别器D_φ，给定提示x、学生响应y_s和教师响应y_t，输出相似性得分。学生策略被训练以最大化此分数，产生对抗性梯度g_ADV，将学生策略拉向教师的语义流形。\n\n### 2. 对齐组件的梯度公式\n将每个对齐阶段视为其对整体策略更新的梯度贡献：\n\n- **对抗性监督微调(A-SFT)**：结合模仿梯度和对抗梯度\n  g_A-SFT = g_SFT + λ_adv g_ADV\n\n- **对抗性群体相对策略优化(A-GRPO)**：结合偏好寻求梯度和对抗梯度\n  g_A-GRPO = g_GRPO + λ_adv g_ADV\n\n### 3. 统一训练目标\n最终表达为单阶段统一训练目标，其梯度是四个基本学习信号的协同组合：\n\ng_Unified = α∇θ L_SFT（模仿信号）\n+ (1-α)∇θ L_GRPO（偏好信号）\n+ λ_adv(α∇θ L_ADV|SFT + (1-α)∇θ L_ADV|PREF)（全局分布正则化）\n\n## 五、优势与验证\n\n这种统一范式直接解决了顺序范式的核心挑战，具有三个显著优势：\n1. **固有防止无基础策略漂移**：专家分布成为动态正则器，不断锚定策略\n2. **协同数据利用增强泛化**：每个梯度更新同时平衡模仿和偏好优化\n3. **简化高效训练流程**：将复杂多阶段训练替换为单一连续训练运行\n\n作者通过在指令跟踪任务上的实验验证了方法的有效性，证明UniAPL不仅实现了更强的性能，还生成了更接近专家演示的输出。\n\n## 六、结论：统一对齐的新范式\n\nUniAPL通过将对齐重新概念化为单阶段约束优化问题，提供了一个原则性框架，有效解决了顺序SFT-then-RL对齐中的分布不匹配问题。这一方法不仅具有理论意义，还通过显著的经验增益和简化的训练流程得到了验证，为整合更多样化的监督信号和开发更强大可扩展的AI系统引导方法奠定了基础。", "summary_translation": "# 中文翻译\n\n塑造强大且有益安全的LLMs（大型语言模型）是AI对齐（AI alignment）的核心。我们认为后训练对齐（post-training alignment）本质上是一个统一的偏好学习（Preference Learning）问题，涉及两种模式：演示偏好（demonstrated preferences，例如监督微调Supervised Fine-Tuning, SFT）和比较偏好（comparative preferences，例如强化学习Reinforcement Learning, RL）。标准的顺序流程——先进行SFT再进行RL——由于关键的分布不匹配（distributional mismatch）而存在缺陷：SFT使用静态专家数据，但随着策略（policy）的演变，其生成分布会发生漂移，导致SFT知识变得脆弱。随后的RL在没有直接访问专家演示中丰富的真实知识（ground-truth knowledge）的情况下进行探索，导致效率低下且缺乏基础的更新。这种分离阻止了数据源之间的相互正则化（mutual regularization）。为解决此问题，我们将对齐重新定义为约束优化问题（constrained optimization problem），并提出统一对抗偏好学习（Unified Adversarial Preference Learning, UniAPL），这是一个新颖的框架，能够动态地将策略的分布与专家的分布对齐。UniAPL实现了单阶段统一训练目标（single-stage unified training objective），从SFT和偏好数据的混合批次中联合学习。在每个梯度步骤（gradient step）中，密集的专家演示直接为在线探索提供基础并进行正则化，本质上解决了分布不匹配问题并最大化数据协同效应（data synergy）。我们使用Qwen3-235B-Instruct-2507作为教师模型（teacher model），在指令跟随任务（instruction-following tasks）上评估UniAPL。我们的模型匹配或超过了强大的GRPO基线（GRPO baselines）：在Qwen3-0.6B上提升5.77%（匹配32B模型），在Qwen3-4B上提升3.75%，甚至超越了教师模型。对响应长度和对数概率分布（log-probability distributions）的分析证实，UniAPL的输出紧密模仿专家演示，同时实现了更强的性能和更好的行为对齐（behavioral alignment）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#24", "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning", "link": "/arxiv/2509.24765", "arxiv_id": "2509.24765", "authors": "Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song", "summary": "Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMs' logical performance.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.193071", "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的基础能力——具体来说是逻辑推理能力。论文提出了LogicAgent，这是一种基于符号学的多视角推理框架，旨在增强LLM处理抽象命题、模糊语境和冲突立场的能力，这属于提升LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于逻辑推理(logical reasoning)，这是通用推理能力的重要组成部分；新兴范式上提出了基于LLM的智能体(LLM-based agents)框架。 第三，论文不涉及任何排除标准中的领域：没有研究多模态与视觉问题，没有将LLM应用于特定领域（虽然使用了哲学概念构建数据集，但这只是为了评估逻辑推理能力，而非应用焦点），也没有主要关注模型可靠性的应用层面问题。 在特殊和模糊情况处理上，论文提出的LogicAgent是一种通用的智能体框架，旨在提升LLM的通用推理能力，而非应用于特定领域，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种新的方法论来增强LLM的逻辑推理能力，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大语言模型在处理抽象命题、模糊语境和冲突立场时的逻辑推理问题。针对语义复杂性和逻辑复杂性交织的场景，我们提出了一种基于符号学方格的多视角推理框架LogicAgent，并在RepublicQA等数据集上通过准确率等指标验证了其有效性。该方法通过语义结构化、逻辑推理和反思验证三个阶段，结合一阶逻辑和三值决策方案，实现了比强基线平均高6.25-7.05%的性能提升。", "inspiration_trace": "# 从模糊到判决：LogicAgent方法论的逻辑演进\n\n## 一、宏观问题：LLM逻辑推理的语义与逻辑双重复杂性\n\n作者观察到，尽管大语言模型(LLM)在逻辑推理方面取得了进展，但它们在处理真实世界推理场景时仍面临根本性挑战。这些场景往往涉及**抽象命题**、**模糊语境**和**冲突立场**，而现有方法未能充分考虑**语义复杂性**与**逻辑复杂性**之间的相互作用。\n\n语义复杂性表现为表达式允许多种解释或表面形式，而逻辑复杂性则要求在上下文前提和语义交互上进行推理。作者识别出关键问题：现有方法将这两者割裂处理，导致在面对哲学抽象、概念对立和深层逻辑关系时表现不佳。\n\n## 二、问题分解：现有方法的局限性分析\n\n作者系统分析了现有方法的不足：\n\n1. **线性推理路径的局限**：如Chain-of-Thought和Tree-of-Thought等方法虽改进了逐步推理，但主要遵循单一线性路径，难以捕捉语义对立和矛盾关系。\n\n2. **符号增强方法的不足**：Logic-LM等符号增强框架虽结合了LLM和一阶逻辑求解器，但缺乏对语义结构的系统处理，难以应对模糊语境。\n\n3. **基准数据集的简单性**：ProntoQA、ProofWriter等现有基准主要基于简单模板和语义直接的内容，缺乏抽象概念和系统性组织的对立关系，无法充分评估真实世界推理的复杂性。\n\n这一分析揭示了研究空白：需要一种能同时处理语义复杂性和逻辑复杂性的框架。\n\n## 三、理论启发：符号学方阵的逻辑迁移\n\n作者从Greimas的符号学方阵(Greimas' Semiotic Square)中获得关键启发。这一结构主义语义框架将二元对立扩展为四元素结构，包含：\n\n- **对立关系(Contraries)**：如S1与S2，两者不能同时为真，但可能同时为假\n- **矛盾关系(Contradictions)**：如S1与¬S1，两者不能同时为真或假\n- **语义蕴含关系**：如S1 ⇒ ¬S2和S2 ⇒ ¬S1\n\n作者洞察到，这一语义框架可以迁移到一阶逻辑中，为处理语义复杂性和逻辑复杂性提供统一的理论基础。这形成了方法论的核心假设：通过符号学方阵引导的多视角推理，可以系统性地处理语义模糊性和逻辑复杂性。\n\n## 四、核心假设：多视角推理与反思验证\n\n基于理论启发，作者形成了四个核心假设：\n\n1. **符号学-逻辑整合假设**：将Greimas符号学方阵整合到一阶逻辑中，可以同时处理语义复杂性和逻辑复杂性。\n\n2. **多视角推理假设**：通过同时考虑原命题、其矛盾命题和对立命题，可以提高推理的鲁棒性，特别是在模糊场景下。\n\n3. **三值决策假设**：引入{True, False, Uncertain}三值决策方案，可以更真实地处理边界情况，避免二元决策的局限性。\n\n4. **反思验证假设**：通过结构化的反思验证机制，可以检测和纠正推理过程中的不一致性，提高推理的准确性。\n\n这些假设共同构成了LogicAgent方法的理论基础。\n\n## 五、方法论设计：三阶段推理框架\n\n基于核心假设，作者设计了LogicAgent框架，包含三个相互连接的阶段：\n\n### 1. 语义结构化阶段\n- **符号学方阵构建**：为输入命题生成其矛盾命题(¬S1)、对立命题(S2)和对立命题的矛盾(¬S2)\n- **一阶逻辑转换**：为每个命题生成对应的FOL表示\n- **验证机制**：通过真值表评估、CFG语法验证和LLM语义验证确保命题的一致性\n\n### 2. 逻辑推理阶段\n- **转换器**：将自然语言前提转换为一阶逻辑，确保逻辑结构的准确性\n- **规划器**：构建推理蓝图，确定评估目标、选择相关前提和推理规则\n- **求解器**：执行符号推理，生成中间结论和最终分类\n\n### 3. 反思验证阶段\n- **直接解析**：当S1和¬S1产生互补结果时采用\n- **快速反思**：当任一命题结果为不确定时触发，分析推理内部一致性\n- **深度反思**：当S1和¬S1产生相同结果时，利用符号学方阵中的语义蕴含关系进行验证\n\n这一设计实现了从语义结构化到符号推理，再到反思验证的完整推理循环，模拟了人类在模糊情境中的多视角推理过程。\n\n## 六、评估基准：RepublicQA的设计\n\n为验证方法有效性，作者设计了RepublicQA基准数据集，基于柏拉图《理想国》的哲学概念构建：\n\n- **语义复杂性**：包含抽象哲学命题和多样化的语境前提\n- **逻辑复杂性**：系统组织的对立和矛盾关系，要求深层逻辑推理\n- **语言复杂性**：达到大学级阅读难度(FKGL=11.94)，词汇和结构多样性显著超过现有基准\n\nRepublicQA的设计反映了作者对真实世界推理挑战的理解：语义丰富、逻辑复杂、概念抽象。这为评估LogicAgent处理语义模糊性和概念复杂性的能力提供了理想平台。\n\n## 七、实验验证：从假设到证据\n\n通过系统实验，作者验证了核心假设：\n\n1. **多视角推理的有效性**：在RepublicQA上，LogicAgent超过强基线平均6.25%，证明符号学引导的多视角推理能有效处理语义复杂性。\n\n2. **泛化能力验证**：在四个主流逻辑推理基准上，LogicAgent实现平均7.05%的提升，表明方法不仅适用于哲学推理，还能泛化到多种逻辑推理任务。\n\n3. **组件贡献分析**：消融实验表明，移除符号学方阵导致性能下降最大(-8.16%)，验证了多视角推理的核心作用；移除反思验证阶段也导致显著下降(-4.36%)，证实了反思机制的重要性。\n\n4. **语义-符号整合优势**：对比实验显示，在语义丰富的RepublicQA上，同时使用自然语言和FOL表示效果最佳；而在其他基准上，FOL表示更为关键，证明了双模态表示的互补价值。\n\n## 八、结论：从模糊到判决的完整路径\n\nLogicAgent的演进体现了从问题识别到方法解决的完整逻辑链条：\n\n1. **问题识别**：LLM逻辑推理面临语义与逻辑双重复杂性挑战\n2. **理论启发**：符号学方阵为处理语义对立和逻辑关系提供了结构化框架\n3. **假设形成**：多视角推理、三值决策和反思验证可增强LLM的逻辑性能\n4. **方法设计**：三阶段推理框架实现了语义结构化、逻辑推理和反思验证的整合\n5. **验证评估**：通过专门设计的基准和系统实验，验证了方法的有效性和泛化能力\n\n这一演进路径展示了如何从观察到的实际问题出发，通过理论启发形成创新假设，进而设计系统解决方案，最终通过严格实验验证其有效性。LogicAgent不仅提高了LLM在逻辑推理任务上的性能，更重要的是，它为处理自然语言中的语义模糊性和逻辑复杂性提供了一种新的范式，弥合了符号推理与神经推理之间的鸿沟。", "summary_translation": "逻辑推理是大型语言模型（large language models, LLMs）的基本能力。然而，现有研究在很大程度上忽视了逻辑复杂性与语义复杂性之间的相互作用，导致现有方法难以处理涉及抽象命题、模糊语境和冲突立场等具有挑战性的场景，而这些场景正是人类推理的核心。针对这一空白，我们提出了LogicAgent，这是一个由符号方阵（semiotic square）引导的框架，旨在共同解决逻辑复杂性和语义复杂性问题。LogicAgent在一阶逻辑（first-order logic, FOL）中明确执行多视角演绎，同时通过存在性导入检查（existential import checks）来缓解空洞推理，该检查采用三值决策方案（True, False, Uncertain）以更真实地处理边界情况。此外，为了克服现有数据集的语义简单性和低逻辑复杂性，我们引入了RepublicQA，这是一个达到大学难度水平（FKGL = 11.94）的基准测试，其词汇和结构多样性显著高于之前的基准测试。RepublicQA基于哲学概念，具有抽象命题和系统组织的对立关系与矛盾关系，使其成为评估逻辑推理的语义最丰富的资源。实验表明，LogicAgent在RepublicQA上实现了最先进的性能，比强基线（strong baselines）平均提高了6.25%，并且能有效泛化到主流逻辑推理基准测试，包括ProntoQA、ProofWriter、FOLIO和ProverQA，额外实现了7.05%的平均提升。这些结果突显了我们基于符号学的多视角推理在提升大型语言模型逻辑性能方面的显著效果。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#22", "title": "Query Circuits: Explaining How Language Models Answer User Prompts", "link": "/arxiv/2509.24808", "arxiv_id": "2509.24808", "authors": "Tung-Yu Wu, Fazl Barez", "summary": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.191953", "filter_reason": "这篇论文的核心贡献是提出\"查询电路\"(query circuits)方法，用于解释语言模型如何将特定输入映射到输出，追踪模型内部的信息流。论文不仅提出了这一新方法，还开发了评估指标(NDF)和高效识别电路的采样方法。虽然论文不直接提出新的训练范式或直接增强模型推理能力的方法，但它通过提供模型决策的局部、输入级别解释，有助于深入理解LLM的内部推理机制。这种对模型推理过程的理解是提升模型通用推理能力的重要基础，符合\"增强模型内在可解释性，从而提升模型的通用可靠性和推理质量\"的标准。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或应用层面的模型可靠性问题。因此，这篇论文符合关于\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决语言模型决策解释问题，特别是解释模型如何回答特定用户查询。针对单个输入查询的解释需求，我们提出了一种query circuits方法，通过在模型内部直接追踪信息流来识别关键子网络，并在IOI、arithmetic、MMLU和ARC等benchmark上通过NDF指标验证了其有效性。实验表明，仅使用1.3%的模型连接即可恢复约60%的MMLU问题性能。", "inspiration_trace": "# 从宏观问题到创新方法：Query Circuits的逻辑演进\n\n## 1. 宏观问题：语言模型的可解释性需求\n\n论文从语言模型在高风险领域（医疗、自动驾驶）的应用需求出发，提出一个根本性问题：**如何解释语言模型为何对特定用户查询产生特定输出？**\n\n- 当医疗AI决定患者是否需要手术时，其推理过程必须可解释\n- 当自动驾驶系统选择错误控制动作时，其失败模式必须可解释\n- 现有方法主要解释模型的全局能力，而非特定输入-输出映射\n\n## 2. 现有方法的局限性分析\n\n作者系统评估了三类主流可解释性方法，揭示其核心局限：\n\n### 2.1 能力电路(Capability Circuits)\n- **功能**：识别实现特定技能（如间接对象识别）的子网络\n- **局限**：只能解释模型的全局算法能力，无法解释为何对特定输入产生特定输出\n- **例证**：能解释模型如何进行间接对象识别，但不能解释对特定句子的具体回答\n\n### 2.2 基于代理模型的方法（如稀疏自编码器）\n- **功能**：在代理模型中发现电路\n- **局限**：\n  - 代理模型可能无法忠实重构原始模型激活\n  - 可能未捕捉LLM的真实机制\n  - 训练成本高，可访问性受限\n\n### 2.3 其他方法\n- 视觉模型电路发现和LLM输入依赖特征分析\n- **共同局限**：不提供LLM在提示级别的电路解释\n\n## 3. 问题聚焦：查询电路发现\n\n基于上述局限，作者提出新研究任务：**查询电路发现(Query Circuit Discovery)**\n\n### 3.1 核心定义\n- **目标**：识别LLM内部驱动其对单个输入查询做出决策的特定电路\n- **与能力电路的本质区别**：\n  - 能力电路：全局解释（模型如何实现特定算法技能）\n  - 查询电路：局部解释（模型如何处理特定输入并产生输出）\n\n### 3.2 技术挑战识别\n\n作者通过实验发现三个关键挑战：\n\n#### 挑战1：评估指标不稳定性\n- **现象**：广泛使用的标准化忠实度分数(NFS)在通用数据集(如MMLU)上表现极不稳定\n- **证据**：NFS值常超过1或低于0，无法可靠指示电路何时开始捕获模型行为\n- **影响**：无法有效评估电路质量和监控发现进度\n\n#### 挑战2：能力电路方法在查询设置中性能下降\n- **现象**：直接应用能力电路方法发现查询电路效果差\n- **证据**：在IOI数据集上，查询电路平均恢复不到50%性能，而能力电路恢复约65%\n- **原因**：\n  - 特征归因受梯度噪声影响\n  - 间接效应计算忽略边缘间组合效应\n  - 单个查询中传输无关特征的边缘可能显示非零梯度，但组合时贡献很小\n\n#### 挑战3：复杂查询需要高边缘预算\n- **现象**：处理复杂查询需要大量边缘才能形成有效电路\n- **证据**：在MMLU天文学上，EAP-IG需约100k边缘(25.9%)才能超过随机基线\n- **问题**：是复杂查询确实需要更多边缘，还是现有方法无法识别有效电路？\n\n## 4. 解决方案：创新方法设计\n\n针对上述挑战，作者提出两个核心创新：\n\n### 4.1 新评估指标：标准化偏差忠实度(NDF)\n\n#### 设计动机\n- NFS在通用数据集上不稳定，需要更稳健的评估指标\n\n#### 核心定义\n```\nNDF(Cq) = 1 - min( [L(M(q)) - L(Cq(q))] / [L(M(q)) - L(M(q'))], 1 )\n```\n\n#### 关键特性\n- **对称性**：围绕原始模型性能对称惩罚上下偏差\n- **有界性**：严格限制在[0,1]区间\n- **解释性**：NDF=1表示电路与原模型性能相同；NDF=0表示性能偏差超过原始与损坏查询间的差距\n\n#### 优势\n- 在通用数据集上提供稳定可靠的评估\n- 能有效跟踪发现进度\n\n### 4.2 新发现方法：Best-of-N (BoN)采样\n\n#### 设计动机与关键观察\n- **观察**：基于原始查询发现的电路可能不忠实，但基于其释义发现的电路可以成功\n- **假设**：由于梯度噪声和忽略组合效应，单个查询的边缘评分只能捕获粗略模式，不足以一致选择形成忠实电路的边缘集\n- **类比**：发现忠实电路类似\"彩票假设\"——原始查询和其释义发现的电路是\"彩票\"，成功恢复模型性能的是\"中奖彩票\"\n\n#### 方法流程\n1. 生成原始查询q的p个释义{q1, ..., qp}\n2. 使用{q, q1, ..., qp}计算边缘重要性分数，形成边缘分数矩阵{S, S1, ..., Sp}\n3. 利用这些矩阵形成p+1个电路\n4. 测量忠实度分数，选择最优电路\n\n#### 效率优化变体\n为提高效率，作者提出两种变体：\n\n##### 4.2.1 插值BoN (iBoN)\n- **原理**：在两个先前发现的忠实电路间插值，无需额外LLM前向传播\n- **方法**：从较小电路开始，添加来自较大电路的高分边缘，直到达到目标边缘数\n\n##### 4.2.2 带约束自适应分数矩阵的BoN (BoN-CSM)\n- **原理**：利用所有先前发现的电路建立分数矩阵和层级矩阵\n- **方法**：按层级（优先来自较小电路的边缘）和分数排序边缘，选择前N个形成电路\n\n## 5. 实验验证与结果\n\n作者在多个基准测试上验证方法有效性：\n\n### 5.1 数据集\n- IOI（间接对象识别）\n- 算术加法和乘法\n- MMLU（多个类别）\n- ARC Challenge\n\n### 5.2 主要结果\n- **有效性**：BoN、iBoN和BoN-CSM一致优于基线方法\n- **稀疏性**：在MMLU上，仅使用5k边缘（1.3%）的电路可实现平均0.6的NDF\n- **对比**：普通EAP-IG需要约200k边缘（51.7%）才能达到相同水平\n- **意义**：证明即使在复杂查询中，LLM内的紧凑子网络仍可恢复大部分模型行为\n\n### 5.3 消融研究\n- **释义数量**：BoN性能随释义数量增加而单调提高，但收益递减\n- **运行效率**：BoN运行时间略长于EAP-IG（500步），但性能显著优于EAP-IG（即使1000步）\n\n## 6. 结论与贡献\n\n### 6.1 核心贡献\n1. **任务定义**：提出查询电路发现任务，区分于能力电路发现和基于代理模型的方法\n2. **挑战解决**：\n   - 提出NDF解决评估指标不稳定问题\n   - 提出BoN采样及其变体解决发现方法效果差问题\n3. **实证验证**：证明模型内的小电路可解释单个查询上的大部分行为\n\n### 6.2 研究意义\n- 为忠实、可扩展的提示级别LLM决策解释提供了实用路径\n- 推进了从输入依赖激活稀疏性到电路稀疏性的研究\n- 为高风险场景中LLM决策的可解释性提供了新思路\n\n这一逻辑演进展示了作者从宏观可解释性需求出发，通过批判性分析现有方法局限，聚焦到具体研究问题，系统识别技术挑战，并提出创新解决方案的完整思维过程。", "summary_translation": "解释语言模型为何产生特定输出需要局部的、输入级别的解释。现有方法揭示了全局能力电路（global capability circuits）（例如，间接对象识别），但未能解释模型为何以特定方式回答特定输入查询。我们引入了查询电路（query circuits），它直接追踪模型内部将特定输入映射到输出的信息流动。与基于代理的方法（surrogate-based approaches）（例如，稀疏自编码器）不同，查询电路是在模型内部识别的，从而产生更忠实且计算上可访问的解释。为使查询电路实用化，我们解决了两个挑战。首先，我们引入了标准化偏差忠实度（Normalized Deviation Faithfulness, NDF），这是一个稳健的指标，用于评估发现的电路在多大程度上能够恢复模型对特定输入的决策，并且广泛适用于我们设置之外的电路发现。其次，我们开发了基于采样的方法，以高效识别那些稀疏但忠实描述模型行为的电路。在多个基准测试（IOI、算术、MMLU和ARC）中，我们发现模型内存在极其稀疏的查询电路，可以恢复其在单个查询上的大部分性能。例如，一个仅覆盖模型连接1.3%的电路可以恢复约60%的MMLU问题性能。总体而言，查询电路为忠实、可扩展地解释语言模型如何处理单个输入提供了一个步骤。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#38", "title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs", "link": "/arxiv/2509.24377", "arxiv_id": "2509.24377", "authors": "Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu", "summary": "Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://github.com/reml-group/PRISM.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.210916", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础推理能力，特别是数学推理能力。论文提出了PRISM框架，将数学推理分为策略规划和定向执行两个阶段，这是一种新的训练/推理范式，旨在增强LLM的通用推理能力，而非将LLM应用于特定领域。 其次，论文包含多个正面指标：核心概念方面明确关注Large Language Models (LLMs)；能力方向专注于mathematical reasoning和planning；虽然未明确提及强化学习或自我进化，但提出了训练轻量级策略适配器的方法，属于训练方法创新。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。数学推理被视为LLM的基础能力之一，而非特定领域应用。 在特殊情况下，虽然论文提到\"tool-integrated reasoning\"作为现有方法之一，但其核心贡献是提出一种通用的策略路由框架，而非将工具应用在特定领域。PRISM框架是一种通用方法论，适用于各种数学推理问题，旨在增强LLM的通用问题解决能力。 综上所述，这篇论文的核心贡献是提出了一种新的框架来增强LLM的数学推理能力，通过动态选择最适合特定问题的推理策略来提高模型性能，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决LLMs在数学推理中单一策略无法适应不同问题需求及忽略效率-有效性权衡的问题。针对数学推理任务，我们提出了一种PRISM框架，将推理解耦为策略规划和执行两个阶段，并在五个数学推理基准上通过Pass@1和Pass@5准确率验证了其有效性。", "inspiration_trace": "# 从问题到解决方案：PRISM框架的逻辑演进\n\n## 一、宏观问题：提升LLMs的数学推理能力\n\n作者从提升大型语言模型(LLMs)在数学推理任务中的性能这一宏观问题出发。数学推理需要精确的逻辑推理、符号操作和多步问题解决，是一个长期存在的挑战。现有方法主要依赖固定的推理策略，如自然语言推理(NLR)、代码增强推理(CAR)、工具集成推理(TIR)或基于集成的推理(EBR)。\n\n## 二、关键观察：现有方法的局限性\n\n通过系统性实验分析，作者发现了两个核心挑战：\n\n### 挑战1：单一策略无法适应所有问题\n作者评估了四种推理策略在不同数学问题类型上的性能（如图1所示），发现没有单一策略在所有问题类别上都表现最佳。例如，某些策略在代数问题上表现出色，但在几何问题上表现较差。这表明僵化地坚持固定推理范式无法充分释放LLMs的潜力。\n\n### 挑战2：忽略了效率与效果之间的权衡\n作者进一步评估了不同策略的推理效率，发现没有单一策略在所有情况下都实现最佳效率。现有方法通常忽略计算成本、延迟和资源效率，导致大量计算开销并未带来相应的准确性提升。\n\n## 三、核心假设：问题感知的策略选择\n\n基于以上观察，作者形成了核心假设：\n**如果能够根据具体问题动态选择最适合的推理策略，而不是固定使用单一策略，那么可以在保持计算效率的同时提高数学推理的整体性能。**\n\n这一假设包含两个关键点：\n1. 不同问题适合不同的推理策略\n2. 动态选择策略可以平衡效果和效率\n\n## 四、解决方案设计：PRISM框架\n\n为验证这一假设，作者提出了PRISM（Planning and Routing through Instance-Specific Modeling）框架，将数学推理解耦为两个核心阶段：\n\n### 阶段1：策略规划\n- **目标**：为每个问题实例预测最适合的推理策略\n- **实现方法**：训练一个轻量级的策略适配器(Strategy Adapter)\n\n#### 数据构建\n作者设计了一个创新的多策略性能分析方法：\n1. 在标准基准问题上执行四种推理策略\n2. 使用多维度评分函数评估每个解决方案：\n   - 正确性：是否产生正确解决方案\n   - 过程质量：是否遵循数学上有效的推理步骤\n   - 计算效率：时间和资源消耗\n3. 将原始分数转换为软目标分布，形成约13,000个实例的MathStrat数据集\n\n#### 策略适配器训练\n- 训练策略适配器预测策略概率分布\n- 使用KL散度损失匹配目标分布\n- 添加辅助交叉熵损失强化顶级策略学习\n\n### 阶段2：目标执行\n- **目标**：根据策略适配器的预测，动态选择最适合的执行路径\n- **实现方法**：基于置信度的自适应路由策略\n\n作者设计了三种执行模式：\n1. **自信路由**：高置信度且偏好明显时，仅执行单一最佳策略\n2. **审议路由**：置信度高但策略排名接近时，执行前两个策略并通过多数投票聚合结果\n3. **探索路由**：预测器置信度不足时，执行所有可用策略并通过多数投票选择最终答案\n\n这种基于置信度的编排机制实现了战略灵活性和计算效率的平衡。\n\n## 五、验证与评估\n\n作者通过广泛实验验证了PRISM的有效性：\n\n1. **主要结果**：在五个数学推理基准上，PRISM一致优于单一策略和集成基线，在MATH基准上达到53.2%的准确率，比最佳单一策略基线提高3.1%。\n\n2. **消融研究**：逐步添加路由组件的结果表明，完整的自适应路由策略对实现最佳性能至关重要。\n\n3. **性能-效率权衡**：PRISM在提高准确率的同时实现了更好的计算效率。\n\n4. **可扩展性**：在不同规模的模型上评估表明，PRISM在不同模型容量下都能提供一致的改进。\n\n## 六、逻辑演进总结\n\n作者的思维链条展现了从宏观问题到具体解决方案的系统性演进：\n\n1. **观察现象**：现有数学推理方法使用固定策略，无法适应不同类型的问题\n2. **发现问题**：单一策略无法适应所有问题，且忽略了效率与效果之间的权衡\n3. **提出假设**：动态选择最适合的推理策略可以提高整体性能\n4. **设计解决方案**：提出PRISM框架，解耦策略规划和目标执行\n5. **实现方法**：构建多策略偏好数据集，训练策略适配器，设计自适应路由策略\n6. **验证假设**：通过广泛实验证明PRISM的有效性、优越性和可扩展性\n\n这一逻辑链条体现了科学研究中的系统性思维，从问题识别到解决方案设计的完整思考过程，为数学推理任务提供了一个创新且有效的方法论。", "summary_translation": "现有方法通常利用固定策略，如自然语言推理（natural language reasoning）、代码增强推理（code-augmented reasoning）、工具集成推理（tool-integrated reasoning）或基于集成的推理（ensemble-based reasoning），来指导大型语言模型（Large Language Models, LLMs）执行数学推理。我们的分析表明，单一策略无法适应特定问题的需求，因此忽略了有效性和效率之间的权衡。为解决这些问题，我们提出了通过实例特定建模进行规划和路由（Planning and Routing through Instance-Specific Modeling, PRISM），这是一个将数学推理分解为两个阶段的新框架：策略规划和定向执行。具体而言，我们首先整理了一个多策略偏好数据集，我们称之为MathStrat，捕获每个问题-策略对的正确性、过程质量和计算效率。然后，我们基于该数据集训练了一个轻量级的策略适配器（Strategy Adapter），以获得在上述四种推理策略上的置信度分布。在推理时，自适应路由策略（adaptive routing policy）基于预测器置信度动态调整推理方法。它指导模型对高置信度预测使用单策略执行，对竞争性场景使用双策略验证，对不确定情况使用全面的多策略探索。在五个数学推理基准测试上的大量实验表明，PRISM始终优于单个策略和集成基线，在不同基础模型上实现了0.9%到7.6%的改进。自适应路由方法对各种模型架构的数学推理任务显示出特别强大的优势。我们的代码已在https://github.com/reml-group/PRISM上发布。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#35", "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling", "link": "/arxiv/2509.24460", "arxiv_id": "2509.24460", "authors": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Likang Xiao, Yanwei Ren, Quan Chen, Xianglong Liu", "summary": "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.203969", "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力的。它提出了一种名为ContextPRM的新方法，将过程奖励模型(PRM)的学习目标从验证领域特定知识转变为建模领域无关的逻辑流，通过关注思维链(CoT)步骤之间的上下文连贯性来增强模型的通用推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究。 其次，从正面指标看，论文包含了多个相关主题：核心概念上明确讨论大语言模型(LLMs)；能力方向上聚焦于推理能力(reasoning)，特别是数学推理(mathematical reasoning)和逻辑推理；训练方法上涉及过程奖励模型(PRM)，这与强化学习相关；新兴范式上采用了思维链(CoT)方法。 第三，论文不主要聚焦于排除标准中的任何领域。虽然论文在法律、历史、哲学等非数学领域进行了测试，但这些是用来验证模型泛化能力的测试领域，而非论文的主要研究焦点。论文的核心是提出一种通用的推理方法，而非针对特定应用领域。 最后，论文不涉及需要特殊判断的模糊情况。它没有将LLM作为工具应用于特定领域，而是直接提升LLM本身的通用推理能力，使其能够在多个领域表现出色。 综上所述，这篇论文完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决Process Reward Models (PRMs)在非数学领域泛化能力有限的问题。针对多领域推理任务，我们提出了一种ContextPRM方法，通过专注于思维链步骤间的上下文连贯性而非领域特定知识来建模领域无关的逻辑流程。在MMLU-Pro的九个非数学领域上，通过加权多数投票评估，ContextPRM实现了比基线平均6.5%的准确率提升，显著优于先前最先进方法。", "inspiration_trace": "# ContextPRM核心方法的逻辑链推演\n\n## 1. 宏观问题：大语言模型(LLM)的推理能力提升\n\n**观察**：LLM在复杂推理任务上仍有局限性，需要提升其推理能力。  \n**聚焦**：测试时扩展(TTS)作为一种利用额外计算资源提高性能的策略，其效果严重依赖于指导推理过程的验证器质量。  \n**关键问题**：如何构建有效的验证器来提升LLM的推理能力？\n\n## 2. 问题聚焦：过程奖励模型(PRM)的领域局限性\n\n**观察**：PRM作为验证器的一种，在数学推理任务上表现出色，但在其他领域泛化能力有限。  \n**证据**：VersaPRM研究表明，数学PRM难以迁移到非数学任务。  \n**深入分析**：\n- 现有PRM主要关注领域特定知识的验证\n- 不同学科有异质性推理风格：从科学领域的形式化符号推导到人文学科的细微论证\n- 这种异质性使得基于领域特定知识的PRM难以跨领域泛化\n\n## 3. 核心假设：从领域特定知识到领域无关逻辑流\n\n**洞察**：不同领域虽然知识内容不同，但逻辑推理结构可能有共通之处。  \n**假设**：如果将学习目标从验证领域特定知识转变为建模领域无关的逻辑流，可能会提高PRM的泛化能力。  \n**理论依据**：逻辑流是跨领域通用的，而领域知识是特定的；通过关注思维链步骤之间的上下文连贯性，可以捕捉这种领域无关的逻辑流。\n\n## 4. 方法设计：ContextPRM的两大创新组件\n\n### 4.1 上下文感知训练方法\n\n**传统PRM局限**：将每个步骤视为独立正确性评估，随着上下文增长，难以诊断当前步骤失败的真正原因。  \n**创新设计**：\n- 构建复合上下文表示，明确考虑连续推理步骤之间的关系\n- 为每个步骤提供前一步骤作为直接上下文，通过特殊格式化函数标记\n- 将k步CoT转换为k个独立训练样本，每个样本评估步骤间的逻辑转换有效性\n- 损失函数从评估步骤正确性转变为评估步骤间连贯性\n\n### 4.2 基于上下文连贯性的CoT标注标准\n\n**传统标注局限**：主要识别事实错误，无法识别孤立正确但在推理链中逻辑有缺陷的步骤。  \n**创新标注标准**：\n- 三级分类：Good(正确、上下文适当、有贡献)、Okay(正确但冗余或进展最小)、Bad(包含错误、误解、逻辑谬误或误导)\n- 识别\"正确但无逻辑依据\"的步骤\n- 一旦发现第一个\"Bad\"步骤，所有后续步骤自动标记为不正确\n- 实验证明此标注标准能更精确地定位推理链中的初始错误(修改率达42.82%)\n\n## 5. 验证与结果：从假设到实证\n\n**实验设计**：在MMLU-Pro-CoT-Eval测试集上评估多领域性能，比较WMV和BoN采样方法。  \n**关键结果**：\n- ContextPRM在非数学领域实现6.5%的平均准确率提升，显著优于VersaPRM的2.2%\n- 消融研究证明上下文感知训练和上下文连贯性标注具有协同效应\n- 单领域训练研究显示，即使在单一领域(如法律、心理学)上训练，ContextPRM也表现出显著的多领域泛化能力\n- 逻辑密集型领域(如哲学、心理学)的训练比知识密集型领域(如历史、物理)产生更好的泛化效果\n\n## 6. 理论贡献与启示\n\n**核心洞见**：通过将学习目标从验证孤立正确性转变为建模领域无关的上下文连贯性，成功提高了PRM的跨领域泛化能力。  \n**方法论贡献**：建立了标注标准与训练方法的紧密耦合，确保训练目标与标注逻辑一致。  \n**更广泛启示**：关注推理的结构而非仅仅是其内容，是构建更通用、更强大的多领域测试时扩展过程奖励模型的有前景方向。", "summary_translation": "过程奖励模型（Process reward models, PRMs）已证明通过利用测试时扩展（test-time scaling, TTS）在增强大型语言模型（large language models, LLMs）的数学推理能力方面具有显著效果。然而，虽然大多数PRMs在数学领域表现出实质性提升，但特定领域训练数据的稀缺性和基于知识的学习模式限制了它们在面对其他领域时的泛化能力。为解决这一限制，我们将学习目标从验证特定领域知识转向建模领域无关的逻辑流。我们的方法以思维链（chain-of-thought, CoT）步骤之间的上下文连贯性为中心，通过一种新颖的数据注释和训练框架实现，增强了模型在不同领域的泛化能力。例如，我们得到的模型ContextPRM在MMLU-Pro的九个非数学领域（包括法律、历史和哲学）中，通过加权多数投票实现了比多数投票基线显著提高6.5%的平均准确率，大幅超越了VersaPRM的2.2%提升和其他专注于数学的PRMs的0.5%提升，展示了在数学和非数学领域的一致性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#39", "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision", "link": "/arxiv/2509.24351", "arxiv_id": "2509.24351", "authors": "Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu", "summary": "The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at https://github.com/reml-group/AMCS.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.211437", "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种自适应蒙特卡洛搜索(AMCS)框架，用于改进过程奖励模型(PRM)的训练数据质量，从而增强大语言模型的复杂数学推理能力。这明确属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文的核心贡献是将数据生成从固定的静态搜索转变为自适应的动态搜索，在节点值估计和路径扩展层面进行了创新，这是一种提升LLM通用推理能力的新方法。 从正面指标看，论文包含了多个相关主题：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上使用了过程奖励模型(PRM)，这与强化学习框架相关。 论文不符合任何排除标准：不涉及多模态与视觉内容，不专注于特定应用领域(数学推理被视为LLM的通用能力而非特定领域应用)，也不关注模型可靠性的应用层面问题。 论文也不涉及需要特殊处理的模糊情况，如智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文直接致力于提高大语言模型的通用推理能力(特别是数学推理)，提出了新的训练范式和方法，完全符合我的研究范围。", "summary2": "本文旨在解决Process Reward Model (PRM)训练中过程监督数据生成的效率低下和灵活性不足问题。针对数学推理任务，我们提出了一种Adaptive Monte Carlo Search (AMCS)框架，通过不确定性驱动的自适应采样和时间自适应策略的路径扩展，将数据生成从静态转变为动态过程。在MATH、AIME等四个数学推理基准上，AMCS训练的PRM实现了高达76.2%的准确率，显著优于基线方法，且在小参数模型监督下能超越大参数模型，展现出强大的泛化能力。", "inspiration_trace": "# 从静态到动态：自适应蒙特卡洛搜索的逻辑演进\n\n## 宏观问题：提升LLMs的数学推理能力\n\n大型语言模型(LLMs)在复杂数学推理任务中面临挑战，需要有效方法增强其推理能力。研究表明，过程奖励模型(PRM)通过评估推理过程中的每个步骤，能够显著提升LLMs的数学推理表现。然而，PRM的效果高度依赖于高质量的过程监督数据。\n\n## 问题聚焦：过程监督数据生成的瓶颈\n\n获取高质量过程监督数据面临两大挑战：\n1. 人工标注每个推理步骤需要大量专业知识和努力\n2. 现有自动化方法（如基于蒙特卡洛的方法）存在效率和灵活性不足\n\n## 关键观察：现有方法的局限性\n\n通过深入分析，作者发现现有方法在两个层面存在根本性问题：\n\n### 节点价值估计层面\n- **现象**：现有方法对所有推理节点采用固定预算采样策略（如统一分配16个样本）\n- **问题**：忽略了不同节点扩展的难度差异，导致资源分配不合理\n- **后果**：简单节点过度采样，复杂节点采样不足，整体效率低下\n\n### 路径扩展层面\n- **现象**：现有方法在搜索阶段采用固定的探索和利用策略\n- **问题**：无法根据搜索进展动态调整探索与利用的平衡\n- **后果**：难以准确定位错误推理步骤，数据生成过程缺乏灵活性\n\n## 假设形成：自适应是关键\n\n基于上述观察，作者提出核心假设：\n1. 如果能根据推理步骤的不确定性动态调整采样资源，将提高节点价值估计效率\n2. 如果能在路径扩展过程中自适应地平衡探索和利用，将提高数据生成灵活性\n\n## 方法设计：自适应蒙特卡洛搜索(AMCS)\n\n为验证假设，作者设计了AMCS框架，包含两个核心创新：\n\n### 1. 不确定性驱动的自适应采样\n- **初始采样与聚类**：将多样化的初始rollouts按生成置信度和解决方案复杂度特征分组\n- **迭代优化**：对不确定性高的集群分配更多采样资源，对已收敛集群减少采样\n- **终止条件**：设置节点置信度、总预算和集群收敛三重标准，确保计算效率\n\n### 2. 自适应路径扩展\n- **利用价值**：基于自适应MC估计的节点级成功概率\n- **探索奖励**：采用UCT风格奖励鼓励访问较少节点\n- **动态权衡**：随时间指数衰减调整探索权重，从初始广泛探索逐渐转向最有希望方向的利用\n\n## 实验验证：从数据到模型\n\n作者通过完整实验链验证AMCS有效性：\n1. **数据生成**：使用AMCS构建包含20万标注推理轨迹的MathSearch-200K数据集\n2. **模型训练**：基于数据集训练PRM模型Qwen2.5-Math-7B-PRM-AMCS\n3. **性能评估**：在四个数学推理基准测试中，AMCS显著优于现有方法\n4. **深入分析**：验证监督数据分布特性和自适应资源分配模式\n\n## 结论：动态自适应的价值\n\nAMCS通过将数据生成从固定的、静态的转变为自适应的、动态的搜索策略，解决了现有方法在效率和灵活性方面的瓶颈。实验证明，高质量的PRM不仅能提升LLMs的数学推理能力，甚至能使小模型超越弱监督的大模型，展示了过程监督质量相对于模型规模的重要性。这一研究为自动生成高质量过程监督数据提供了新范式。", "summary_translation": "过程数据的质量在训练过程奖励模型（Process Reward Model, PRM，过程奖励模型）中起着关键作用，该模型能够增强大语言模型的复杂数学推理能力。现有方法基于固定预算采样策略（fixed-budget sampling strategy，固定预算采样策略）来估计推理步骤的质量，并在自动数据生成过程中导航庞大的搜索空间以执行路径扩展，导致其效率低下且缺乏灵活性。为解决这些问题，我们提出了自适应蒙特卡洛搜索（Adaptive Monte Carlo Search, AMCS，自适应蒙特卡洛搜索）框架，该框架在节点值估计和路径扩展层面上将数据生成从固定的静态搜索转变为自适应的动态搜索。一方面，AMCS通过为不确定的推理步骤分配更多样本，同时为那些较易估计的步骤使用较少样本，从而自适应地优化估计。另一方面，它通过具有时间自适应策略的蒙特卡洛算法（Monte Carlo algorithm，蒙特卡洛算法）增强路径扩展，该策略从广泛探索开始，并逐渐转向利用最有前景的方向。借助AMCS，我们构建了一个包含约20万个过程监督示例的大规模数据集MathSearch-200K，用于训练PRMs。为验证我们方法的有效性，我们在四个数学推理基准测试上进行了广泛的实验。实验结果表明，Qwen2.5-Math-7B-PRM-AMCS在MATH500上与GLM-4-9B配合使用时达到了高达76.2%的准确率，优于所有基线PRMs。值得注意的是，由Qwen2.5-Math-7B-PRM-AMCS监督的7B模型超越了具有较弱监督的72B模型。此外，Qwen2.5-Math-7B-PRM-AMCS在分布外问题（out-of-distribution problems，分布外问题）上保持了一致的优势，展示了强大的泛化能力。我们的代码可在https://github.com/reml-group/AMCS获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#55", "title": "Humanline: Online Alignment as Perceptual Loss", "link": "/arxiv/2509.24207", "arxiv_id": "2509.24207", "authors": "Sijia Liu, Niklas Muennighoff, Kawin Ethayarajh", "summary": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.231022", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的训练方法，特别是对齐(alignment)技术。论文提出了一个新的理论框架，解释为什么在线对齐(如GRPO)比离线对齐(如DPO)表现更好，并基于此创建了\"humanline\"变体。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，而不是将LLM作为工具应用到特定领域。 其次，从正面指标来看，论文虽然未直接提及\"Large language models\"，但讨论的DPO、GRPO、PPO等技术都是LLM训练中的核心对齐方法。在训练方法方面，论文明确讨论了强化学习方法(RLHF, RL)，特别是PPO/GRPO等在线对齐方法，这是提高LLM通用能力的重要技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的理论框架和训练方法，通过模拟人类感知来改进模型的对齐过程，从而提高模型在可验证和不可验证任务上的性能。虽然论文没有直接讨论推理、规划等具体能力，但改进对齐方法本质上是提高模型整体性能和通用性的重要途径，这与提高LLM通用推理能力的目标是一致的。论文中提到的\"可验证和不可验证任务\"很可能包括需要推理能力的任务，因此这篇论文对研究大语言模型的通用推理能力具有重要参考价值。", "summary2": "本文旨在解释在线对齐方法（如GRPO）为何比离线方法（如DPO）表现更好。基于行为经济学的展望理论，我们提出了一种humanline设计模式，将人类感知偏差纳入对齐目标，通过humanline syncing和humanline clipping实现。在AlpacaEval2和MATH500上的实验表明，humanline变体使用离线数据时能达到与在线方法相当的性能，同时训练速度提升6倍，在数学推理任务中采样频率可降低64倍而不损失性能。", "inspiration_trace": "# 从观察到方法：Humanline核心方法的逻辑演进\n\n## 一、问题起源：在线对齐方法为何更优？\n\n**初始观察**：在线对齐方法（如GRPO）通常比离线对齐方法（如DPO）表现更好，但原因不明。\n\n**现有解释不足**：\n- 在线方法数据覆盖更好\n- 在线方法更强调生成而非判别\n- 在线方法搜索空间更简单\n\n作者认为这些解释未触及本质，提出核心问题：如果目标是最大化人类效用，那么在线/离线二分法本身可能是偶然的。\n\n## 二、跨学科视角：引入前景理论\n\n**理论迁移**：作者引入行为经济学中的前景理论（Prospect Theory），该理论解释了人类为何做出不最大化期望值的决策。\n\n**核心概念**：\n1. **价值函数**：映射结果到主观价值，表现为风险规避和损失规避\n2. **权重函数**：替代客观概率，反映人类对概率的主观感知（高估极端结果，低估典型结果）\n\n**创新扩展**：将前景理论应用于生成模型对齐，将输出的\"surprisal\"（log[πθ(y|x)/πref(y|x)]）视为结果。\n\n## 三、核心假设：人类感知概率分布的关键作用\n\n**关键洞见**：人类效用最大化不仅取决于价值函数，还取决于人类如何感知模型输出的概率分布。\n\n**理论支持**：作者证明在线在策略采样比离线离策略采样更好地近似人类感知的分布（图2）：\n- 从较差模型采样时，隐含容量函数比人类容量函数饱和更快\n- 从较好模型采样时，隐含容量函数饱和更慢\n\n## 四、重大发现：裁剪操作与感知偏差的联系\n\n**意外发现**：PPO/GRPO中的裁剪操作（最初仅为稳定训练）实际上恢复了人类感知概率的感知偏差。\n\n**理论证明**：作者证明了PPO/GRPO的裁剪组件是人类线采样（humanline sampling）在极限条件下的特殊情况。这一发现揭示了最先进的对齐方法之所以成功，是因为它们在某种程度上已经是感知损失。\n\n## 五、方法论创新：人类线变体（Humanline Variants）\n\n**设计思路**：既然成功源于模拟人类感知，那么可以显式地将感知偏差纳入任何对齐目标，而不必限制于在线在策略数据。\n\n**核心设计模式**：\n1. **人类线同步（Humanline Syncing）**：每k步将参考模型权重与策略模型同步\n2. **人类线裁剪（Humanline Clipping）**：在损失计算前不对称裁剪token-wise似然比到[εP, εR]范围\n\n**理论优势**：这种方法允许从任何来源（在线、离线、在策略、离策略）获取数据，并以反映人类感知的方式选择性地使用它。\n\n## 六、实验验证与意义\n\n**验证平台**：\n1. 不可验证奖励：指令跟随任务\n2. 可验证奖励：数学推理任务\n\n**关键结果**：\n- 离线数据+人类线变体匹配在线方法性能（图1）\n- 数学推理中，人类线GRPO允许数据采样频率降低64倍而不损失性能（图6）\n\n**核心意义**：在线/离线二分法对最大化人类效用是偶然的，关键在于数据是否反映人类感知的模型结果分布。人类线变体实现了更快、更便宜、更灵活的对齐，同时不牺牲性能。\n\n这一逻辑链条从观察现象出发，通过跨学科理论引入，形成核心假设，发现意外联系，最终提出创新方法论，并通过实验验证其有效性，展现了完整的科学思维过程。", "summary_translation": "在线对齐（Online alignment，例如GRPO）通常比离线对齐（Offline alignment，例如DPO）表现更佳——但原因是什么？借鉴行为经济学中的前景理论（Prospect theory），我们提出了一个以人为中心的解释。我们证明了在线策略采样（Online on-policy sampling）能更好地近似人类对模型可生成内容的感知分布，而PPO/GRPO风格的裁剪（clipping）——最初引入只是为了稳定训练——恢复了人类感知概率的一种感知偏差。从这个意义上说，PPO/GRPO已经起到了感知损失（Perceptual losses）的作用。\n\n我们的理论进一步表明，在线/离线二分法本身对于最大化人类效用是偶然的，因为我们可以通过以模仿人类感知的方式选择性训练任何数据来实现相同效果，而不局限于在线策略数据。这样做将使我们能够更快速、更经济、更灵活地进行后训练（Post-train），同时不牺牲性能。为此，我们提出了一种设计模式，将概率的感知扭曲（Perceptual distortions）明确纳入DPO/KTO/GRPO等目标中，创建了它们的\"humanline\"变体。令人惊讶的是，我们发现这些humanline变体，即使使用离线非策略数据（Offline off-policy data）训练，也能在可验证（Verifiable）和不可验证（Unverifiable）任务上匹配其在线对应方法的性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#56", "title": "Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback", "link": "/arxiv/2509.24159", "arxiv_id": "2509.24159", "authors": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu", "summary": "Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.231552", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是提出了一种新的训练范式\"鲁棒偏好优化\"(RPO)。论文关注的是如何更好地将模型与人类价值观对齐，这是提升模型通用能力的重要方面。对齐是LLM推理和行为的基础，更好的对齐可以提升模型在各种任务上的表现，包括推理任务。 其次，从正面指标分析： - 核心概念：论文明确关注大语言模型(LLMs)的对齐问题，提到了Mistral和Llama 3模型。 - 训练方法：论文明确讨论了RLHF（基于人类反馈的强化学习），并提出了改进方法，这符合强化学习优化的指标。 - 能力方向：虽然论文没有直接讨论推理、规划或问题解决，但对齐方法的改进可以间接提升模型在这些方面的表现。 第三，论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于任何特定应用领域 - 不是从应用层面讨论水印、安全或安全问题，而是从基础方法层面改进对齐算法 最后，论文提出的是一种通用的训练框架，可以应用于多种对齐算法（DPO, IPO, SimPO,和CPO），提升它们的鲁棒性和效果。这种基础性的方法改进有助于提升大语言模型的通用能力，符合研究目标。 因此，尽管论文没有直接讨论推理能力，但其提出的对齐方法改进可以间接提升模型的推理表现，且属于基础能力改进的范畴，故应保留。", "summary2": "本文旨在解决LLM对齐中人类偏好数据噪声问题。针对噪声和多元化的偏好数据场景，我们提出了一种基于EM算法的Latent Collective Preference Optimization (LCPO)框架，通过推断标签正确性并作为自适应权重校准训练损失，并在Mistral和Llama 3模型上通过AlpacaEval 2和Arena-Hard基准测试的win rate指标验证了其有效性，实现了高达7.0%的性能提升。", "inspiration_trace": "# LCPO核心方法逻辑链分析\n\n## 1. 宏观问题：LLM对齐中的根本挑战\n\n论文从大型语言模型(LLM)与人类价值观对齐的核心挑战出发：\n- **问题背景**：LLM对齐是开发安全可靠AI系统的关键前提\n- **主流方法**：基于人类反馈的强化学习(RLHF)及其简化变体(如DPO)\n- **关键缺陷**：现有方法都基于一个有问题的假设——人类偏好是同质的且数据无噪声\n\n## 2. 问题观察：现实与假设的差距\n\n作者敏锐观察到现实与理论假设之间的鸿沟：\n- **观察1**：人类偏好本质上是多元的(pluralistic)，不同标注者有不同观点\n- **观察2**：标注过程存在错误，导致数据中包含显著噪声\n- **影响量化**：研究表明20%-40%的偏好对可能是噪声，仅10%的噪声率增加就能导致模型胜率下降30%\n\n## 3. 问题分析：现有方法的局限性\n\n深入分析传统方法的根本缺陷：\n- **硬标签问题**：将人类反馈视为确定性的二元选择，无法表达不确定性\n- **平等权重问题**：每个标签被赋予同等置信度，无法区分可靠反馈和噪声\n- **脆弱性根源**：标准损失函数会导致模型过度拟合损坏的标签，性能随噪声增加而急剧下降\n\n## 4. 核心假设：重新框架化问题\n\n基于以上分析，作者提出两个创新性假设重新定义问题：\n\n**假设1：潜在集体偏好**\n- 存在一个不可观测的\"潜在集体偏好\"yw ≻∗ yl，代表理想化共识\n- 观察到的偏好yw ≻k yl只是这一真实值的噪声信号\n- 引入二元潜在变量zi表示标签是否正确，用参数ηk建模标注者可靠性\n\n**假设2：偏好的通用概率模型**\n- 任何偏好损失函数Lpref都可转换为概率分布\n- 偏好概率与指数化负损失成正比：p(yw ≻∗ yl|x, θ) = σ(Lpref(x, yl ≻ yw; θ) - Lpref(x, yw ≻ yl; θ))\n- 这建立了损失函数与概率模型的统一联系\n\n## 5. 方法推导：期望最大化(EM)算法\n\n基于上述假设，作者推导出LCPO算法：\n\n**E步：推断标签正确性**\n- 计算每个标签正确的后验概率wi作为\"软标签\"\n- wi = p(yw,i ≻∗ yl,i |xi, θ)ηki / [p(yw,i ≻∗ yl,i |xi, θ)ηki + p(yl,i ≻∗ yw,i |xi, θ)(1-ηki)]\n- 这相当于估计每个标注者的可靠性\n\n**M步：加权参数更新**\n- 使用wi作为权重更新策略参数θ，形成加权损失函数\n- 更新标注者可靠性ηk为其所有标签的平均置信度\n- 实现策略优化和可靠性估计的相互促进\n\n## 6. 实际实现：小批量训练优化\n\n为平衡效率与性能，作者提出实用实现方案：\n- 使用指数移动平均(EMA)进行在线可靠性更新\n- ηk ← (1-α)ηk + α·(∑wi)/Nk,B\n- 这使算法能够适应大规模训练场景\n\n## 7. 理论分析：收敛性保证\n\n作者提供严格理论证明：\n- **引理4.1**：在完美校准模型下，真实标注者可靠性是EM更新的不动点\n- **定理4.2**：EM迭代保证收敛到真实可靠性η⋆k\n- 这为LCPO提供了坚实的理论基础\n\n## 8. 实验验证：实证效果\n\n通过广泛实验验证LCPO的有效性：\n- **元框架验证**：LCPO一致增强四种主流对齐算法(DPO、IPO、SimPO、CPO)\n- **性能提升**：在AlpacaEval 2和Arena-Hard上实现最高7.0%的胜率提升\n- **理论验证**：受控实验证实LCPO能准确估计标注者可靠性\n\n## 逻辑链总结\n\n作者从LLM对齐的宏观挑战出发，通过观察现实与假设的差距，分析现有方法的根本缺陷，提出创新的核心假设，推导出基于EM算法的解决方案，并通过理论分析和实验验证其有效性，最终形成一个通用且强大的LLM对齐框架。这一完整逻辑链展示了从问题识别到方法创新的系统性思考过程。", "summary_translation": "标准的人类偏好对齐方法，如人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF），是将大型语言模型（Large Language Models, LLMs）与人类价值观对齐的基石技术。然而，这些方法都建立在一个关键但有缺陷的假设之上：人类偏好是同质的（代表单一、统一的偏好），且收集的数据是无噪声的（无错误的）。实际上，这两点都不成立，因为人类偏好是多元的，且标注者可能会犯错。这导致了记录数据与真实偏好之间的差异，可能会误导模型并降低其性能。\n\n为应对这一挑战，我们提出了鲁棒偏好优化（Robust Preference Optimization, RPO）。RPO采用期望最大化（Expectation-Maximization, EM）算法来推断每个标签正确性的后验概率，并利用该概率在训练损失中自适应地重新加权每个数据点，以减轻噪声影响。我们通过建立任意偏好损失与其相应概率模型之间的理论联系，进一步推广了这一方法。这种推广使得现有对齐算法能够系统地转化为其鲁棒对应版本，从而将RPO从一个特定算法提升为鲁棒偏好对齐的元框架。\n\n理论上，我们证明了在完美校准模型的条件下，RPO保证能够收敛到数据集的真实噪声水平。我们的实验证明了RPO作为元框架的有效性，它持续增强了四种最先进的对齐算法（DPO、IPO、SimPO和CPO）。当应用于Mistral和Llama 3模型时，RPO增强的方法在AlpacaEval 2和Arena-Hard上实现了显著的胜率提升，分别提高了7.0%和5.4%。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#45", "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge", "link": "/arxiv/2509.24276", "arxiv_id": "2509.24276", "authors": "Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan", "summary": "Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.", "subjects": "Artificial Intelligence", "date": "2025-09-29", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.214670", "filter_reason": "这篇论文的核心贡献是提出G-reasoner框架，通过集成图和语言基础模型来增强LLMs对图结构知识的推理能力。从本质上看，论文致力于解决LLMs在处理结构化知识方面的局限性，提出了一种新的方法（QuadGraph抽象和图基础模型）来增强LLMs的通用推理能力，而不是将LLM作为工具应用到特定领域。论文明确关注\"reasoning\"这一核心能力方向，并讨论了如何通过结合图结构知识来提升LLMs的推理性能。虽然涉及图结构知识，但这是一种通用的知识表示方法，不是针对特定应用领域的研究。论文提出的框架具有通用性，旨在提升LLMs的基础推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决大型语言模型在图结构知识上推理能力有限的问题。针对多样化的图结构知识场景，我们提出了一种G-reasoner框架，结合QuadGraph标准化表示和图基础模型进行联合推理，并在六个基准数据集上通过EM、F1和ACC等指标验证了其有效性。", "inspiration_trace": "# G-reasoner核心方法的逻辑推演\n\n## 1. 宏观问题：LLMs在知识密集型推理中的局限性\n\n**观察**：\n- 大型语言模型(LLMs)展现出色推理能力，但受限于静态和不完整的参数化知识\n- LLMs难以获取最新和领域特定知识，限制了在真实场景中的应用\n\n**核心问题**：\n如何使LLMs能够有效利用外部知识进行复杂推理，特别是在需要理解知识结构和关系的知识密集型任务中？\n\n## 2. 中观问题：RAG方法的局限性\n\n**观察**：\n- 检索增强生成(RAG)通过引入外部知识缓解了LLMs的知识限制\n- 现有RAG方法在知识密集型任务上表现不佳，因为：\n  * 相关信息分散，难以有效关联\n  * 对知识结构的建模能力弱\n\n**关键发现**：\n- 图是自然的知识结构建模方式，能有效捕捉知识间关系\n- 但LLMs本质上是非结构化的，无法有效处理图结构数据\n\n**核心问题**：\n如何弥合LLMs与图结构知识之间的鸿沟，使LLMs能够有效推理图结构知识？\n\n## 3. 具体问题：现有GraphRAG方法的局限性\n\n**观察**：\n- 现有图增强RAG(GraphRAG)方法主要关注两方面：\n  1. 图构建：设计特定图结构组织知识关系\n  2. 图增强推理：提升LLMs在图结构上的推理能力\n\n**关键局限**：\n- 依赖特定图结构，泛化性差\n- 基于图搜索的方法未充分利用基础模型推理能力\n- 基于代理的方法计算成本高、延迟大\n- GNN-based方法(如GFM-RAG)仍局限于特定图类型\n\n**核心问题**：\n如何开发统一方法，能适应各种图结构并有效推理，同时保证可扩展性和泛化性？\n\n## 4. 假设形成与解决思路\n\n**核心假设**：\n1. 若能将异构图结构统一为标准化表示，则可开发通用推理模型\n2. 若能同时建模图拓扑和文本语义，则可增强推理能力\n3. 若能实现高效训练推理机制，则可支持大规模图处理\n\n**解决思路**：\n- 设计统一图接口，标准化异构知识表示\n- 开发图基础模型，联合推理拓扑结构和语义信息\n- 优化训练推理效率，支持大规模应用\n\n## 5. 方法设计：G-reasoner框架\n\n### 5.1 统一图表示：QuadGraph\n\n**设计动机**：\n现有图结构(知识图、文档图、层次图)各具特点但互不兼容，阻碍了通用推理模型的开发\n\n**解决方案**：\n设计四层图结构统一表示：\n1. **属性层**：捕获节点共同属性\n2. **知识图层**：表示实体及关系三元组\n3. **文档层**：包含非结构化文本信息\n4. **社区层**：基于语义/结构相似性分组节点\n\n**创新点**：\n通过层次化设计，QuadGraph能统一现有各种图结构知识，为通用推理奠定基础\n\n### 5.2 图基础模型推理\n\n**设计动机**：\n现有方法要么忽略文本语义，要么无法处理多样化节点类型，推理能力有限\n\n**解决方案**：\n- 采用查询依赖GNN作为模型主干\n- 融合节点文本语义与图拓扑结构\n- 设计类型特定预测器，支持多样化节点预测\n\n**关键公式**：\n```\nh_l^v = Update(h_{l-1}^v, Agg({Msg(h_{l-1}^v, h_l^r, h_{l-1}^{v'})|(v,r,v')∈E}))\n```\n\n**创新点**：\n首次实现图拓扑与文本语义的联合推理，支持任意节点类型的预测\n\n### 5.3 语言基础模型增强\n\n**设计动机**：\n图推理结果需有效融入LLMs以提升下游任务性能\n\n**解决方案**：\n- GFM预测节点相关性得分\n- 选择各类型top-k相关节点\n- 构建提示模板，整合查询与图推理结果\n- LLM基于增强信息生成最终答案\n\n### 5.4 大规模训练与推理优化\n\n**设计动机**：\n图数据规模庞大，传统方法难以高效处理\n\n**解决方案**：\n- **混合精度训练**：提高吞吐量2.1倍，减少GPU内存17.5%\n- **分布式消息传递**：图分割+本地聚合+跨设备交换，支持大规模图处理\n\n**创新点**：\n首次在图基础模型中实现高效分布式训练，显著提升可扩展性\n\n## 6. 实验验证与效果评估\n\n**验证思路**：\n- 六个基准数据集全面评估\n- 对比非结构化与图增强方法\n- 测试跨图结构泛化能力\n- 消融研究关键组件贡献\n- 分析训练推理效率\n\n**关键结果**：\n- 在所有数据集上显著超越SOTA方法\n- 在不同图结构上展现强大泛化能力\n- 消融实验证实各组件必要性\n- 推理速度提升10倍以上，效率显著\n\n## 7. 逻辑链条总结\n\n从宏观问题到具体解决方案，G-reasoner的逻辑演进遵循以下路径：\n\n```\nLLMs知识限制 → RAG方法局限 → 图结构知识优势 → 现有GraphRAG不足 → 统一表示需求 → \nQuadGraph设计 → 图语义联合推理 → GFM模型开发 → 效率优化 → 实验验证\n```\n\n这一逻辑链条体现了作者从问题本质出发，通过系统性分析和创新设计，最终提出G-reasoner这一统一框架的完整思考过程，为图结构知识与语言模型的融合提供了新范式。", "summary_translation": "大语言模型（Large language models, LLMs）在复杂推理方面表现出色，但仍受到静态和不完整参数化知识（parametric knowledge）的限制。检索增强生成（Retrieval-augmented generation, RAG）通过引入外部知识来缓解这一问题，然而现有的RAG方法由于信息碎片化和知识结构建模能力弱，在知识密集型任务中表现不佳。图（Graphs）为知识内部关系建模提供了一种自然方式，但大语言模型本质上是非结构化的，无法有效处理图结构数据进行推理。近期的图增强检索生成（Graph-enhanced RAG, GraphRAG）试图通过构建定制化图并使大语言模型能够在其上进行推理来弥合这一差距。然而，这些方法通常依赖于临时图设计（ad-hoc graph designs）、启发式搜索（heuristic search）或昂贵的智能体管道（agent pipelines），这阻碍了其可扩展性和泛化能力。\n\n为应对这些挑战，我们提出了G-reasoner，这是一个统一框架，集成了图基础模型（graph foundation models）和语言基础模型（language foundation models），用于对多样化的图结构知识进行推理。我们方法的核心是QuadGraph，这是一种标准化的四层抽象（four-layer abstraction），能够将异构知识源（heterogeneous knowledge sources）统一为共同的图表示。在此基础上，我们引入了一个3400万参数的图基础模型（Graph foundation model, GFM），该模型共同捕捉图拓扑结构（graph topology）和文本语义（textual semantics），并与大语言模型集成以增强下游应用（downstream applications）中的推理能力。为确保可扩展性和效率，我们实现了混合精度训练（mixed-precision training）和分布式消息传递（distributed message-passing），以支持使用更多GPU来扩展图基础模型。在六个基准测试（benchmarks）上的广泛实验表明，G-reasoner始终优于最先进的基线方法（state-of-the-art baselines），显著增强大语言模型的推理能力，并实现了高效率和跨图泛化（cross-graph generalization）能力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#68", "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL", "link": "/arxiv/2509.23870", "arxiv_id": "2509.23870", "authors": "Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu", "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.", "subjects": "Artificial Intelligence", "date": "2025-09-28", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.243134", "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究智能体强化学习(Agentic RL)中的奖励校准问题，提出了一种改进训练方法来增强自主智能体的决策能力。论文核心不是将LLM作为工具应用到特定领域，而是研究如何改进LLM作为智能体的基础训练机制，特别是奖励机制和梯度耦合问题，这直接关系到提升LLM的通用推理能力，因此应该保留。 第二步：正面指标 论文包含多个正面指标： - 核心概念：提到\"autonomous agents\"，虽然未直接使用\"LLMs\"术语，但\"Agentic RL\"通常指基于LLM的智能体强化学习 - 能力方向：关注解决\"long-horizon, real-world tasks\"，这隐含了推理和规划能力 - 训练方法：明确涉及\"reinforcement learning (RL)\"，这是提升LLM能力的关键方法 - 新兴范式：讨论\"agentic RL\"，与\"llm-based agents\"直接相关 第三步：排除标准 论文不符合任何排除标准： - 未涉及多模态与视觉内容 - 未聚焦于医疗、化学、生物等特定应用领域 - 未讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理 论文讨论的是通用的智能体强化学习方法，而非将智能体应用在特定领域。它研究的是如何通过改进奖励机制和解决梯度耦合问题来提升智能体的决策质量，这属于通用推理能力的提升范畴。 核心贡献：论文揭示了智能体强化学习中奖励校准和梯度耦合的关键问题，并提出了一种训练actor分类好/坏动作的方法，以分离好/坏动作的嵌入并减轻梯度干扰，从而提升智能体的决策能力和推理质量。这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决智能体强化学习中GRPO算法的奖励校准问题。针对高度相似的训练样本导致梯度耦合问题，我们提出了一种生成式分类解耦（GCD）方法，训练actor同时作为分类器来区分好坏动作，并在ALFWorld和ScienceWorld数据集上通过成功率等指标验证了其有效性。", "inspiration_trace": "# 从问题到解决方案：GRPO在智能体强化学习中奖励校准的重新思考\n\n## 1. 宏观问题：智能体强化学习的长时程任务挑战\n\n**观察起点**：当前大型语言模型驱动的自主智能体在解决复杂长时程任务时表现不佳。监督微调泛化能力差，而现有强化学习方法主要关注单轮响应，难以处理需要多步交互的长时程任务。\n\n**主流观点**：基于结果的强化学习方法（如GRPO）在数学推理等领域表现良好，但在多轮交互智能体任务中表现不佳，常导致重复、无效动作等失败模式。\n\n**主流解释**：这种失败归因于\"奖励校准问题\"——在长轨迹中，有缺陷的中间动作仍可能导致成功结果，从而被错误地强化。因此，研究者们尝试通过引入步骤级奖励来提供更细粒度的反馈。\n\n## 2. 挑战主流：重新思考奖励校准问题\n\n**质疑与理论分析**：作者挑战了这一主流观点，通过数学证明（Lemma 3.1）表明基于结果的方法如GRPO原则上能够惩罚有害行为——对于有风险的动作（r > 0），其期望优势是负的，这意味着GRPO应该自然地阻止这类动作。\n\n**核心问题**：如果理论上GRPO应该能够惩罚有缺陷动作，为什么在训练后仍然出现并持续存在有缺陷行为（如回声陷阱或重复响应）？\n\n**深入分析**：考虑\"挤压效应\"（在DPO训练中，所选样本的概率实际上也会下降），作者证明这种效应并不阻碍GRPO的整体收敛：高奖励（好）动作的概率质量应该增加，而坏动作的可能性逐渐减少。\n\n## 3. 新假设：梯度耦合问题\n\n**观察**：作者发现根本原因不在于奖励信号，而在于智能体任务数据固有的高度相似性。在智能体任务中，连续轮次仅因一个新的观察而不同，且受限的动作空间导致相似的思维过程和输出。\n\n**数据分析**：通过实验证明ALFWorld中的样本间相似度明显高于GSM8K数学推理任务（图3a）。不同样本之间的高度相似性直接导致其梯度的相似性。\n\n**新假设**：梯度耦合——对一个样本执行梯度下降可能会不自觉地影响相似样本的可能性。实验验证显示，使用一个样本进行单步梯度下降会导致配对样本的概率发生明显变化（图3b）。\n\n## 4. 学习动态分析：安全区域与危险区域\n\n**建模分析**：将来自相似正样本的梯度耦合建模为对有缺陷动作的恒定正优势c，与动作的内在自我纠正优势A = qr(q - 1)竞争。\n\n**关键发现**：自我纠正机制形成两个不同的动态区域：\n- **安全区域（q < 0.5）**：任何不希望的有缺陷动作概率增加都会遇到更强的自我纠正惩罚，形成稳定的负反馈循环。\n- **危险区域（q > 0.5）**：随着概率增加，自我纠正惩罚减弱，动作极易受到外部梯度耦合的影响，导致概率失控增加。\n\n**冷启动重要性**：良好初始化的模型可以确保有缺陷动作以低概率开始，将它们牢牢置于\"安全区域\"。\n\n## 5. 长期动态与收敛条件\n\n**共存动作分析**：考虑好的动作S1（概率p1，期望优势A1 > 0）和相似的有缺陷动作S2（概率p2，期望优势A2 < 0）的共同进化。\n\n**关键发现**：如果|A1| ≫ |A2|，S1的强正优势可能会泄漏并对S2产生正推动，导致两个概率最初都上升。有缺陷动作的概率p2只有在好动作的概率p1变得足够高以产生克服梯度耦合的抑制效应后才会开始下降。\n\n**实验验证**：图5显示，随着训练进行，一些有缺陷动作的概率下降，但其他一些有缺陷动作的一致性可能会增加，只有当正动作的一致性相对较高时，有缺陷动作的一致性才会开始下降。\n\n## 6. 解决方案：生成式分类解耦（GCD）\n\n**核心洞察**：为了减轻梯度干扰，目标是削弱样本间的影响δ，确保好动作被强化而有缺陷动作被抑制。\n\n**现有方法不足**：如Deng et al. (2025a)尝试通过识别负样本中的特定标记并应用惩罚来限制它们对正样本的影响，但这种方法不适用于基于智能体的强化学习，因为在智能体任务中，相似性主要来自重叠的动作序列和详细推理中的共享结构。\n\n**新方法思路**：如果两个相似样本有不同的结果（一个导致成功，一个导致失败），关键是要区分它们的表示以避免有害的梯度干扰。\n\n**具体方案**：生成式分类解耦（GCD），即训练智能体同时充当分类器。\n- 引入一个辅助任务，让模型学习判断给定动作是好是坏。\n- 通过对结果标签进行监督，迫使模型学习区分性的表示。\n- 对于两个输入输出对(x1, y1)和(x2, y2)，其中y1成功而y2不成功，分类目标将它们的隐藏嵌入在表示空间中拉开。\n- 总体训练目标：L = LGRPO + LGCD，其中LGCD是应用于分类任务的GRPO风格损失。\n\n## 7. 辅助策略：基于提示的纠正\n\n**动机**：虽然基于评论家的训练减弱了梯度耦合，但不能完全消除它。当有缺陷动作的概率很高（\"危险区域\"）时，剩余的耦合最危险。\n\n**解决方案**：基于提示的纠正。\n- 在训练过程中收集模型自己生成的评论，突出显示其容易犯的特定错误。\n- 然后综合这些常见错误，并将它们作为明确的指令注入到后续任务的提示中。\n- 这作为一种强大的、有针对性的干预，将有缺陷动作的概率拖出\"危险区域\"并进入\"安全区域\"。\n- 一旦概率降低，自然的自我纠正机制可以有效地接管并继续在强化学习期间抑制有缺陷行为。\n\n## 8. 实验验证与结论\n\n**主要结果**：在ALFWorld和ScienceWorld任务上验证了所提方法的有效性。\n- 当使用冷启动时，不同方法在域内测试数据上的性能接近，但在域外数据上测试时，作者的方法大大提高了性能。\n- 消融研究证明通过训练模型作为生成式判断器来分类动作是否良好，可以有效分离好动作和坏动作的嵌入，从而帮助减少梯度耦合并提高性能。\n- 梯度耦合分析显示，作者的方法有效地分离了正负样本的嵌入，相同类别样本和不同类别样本之间的影响差距远大于原始GRPO。\n\n**最终贡献**：\n1. 诊断了基于结果的强化学习在智能体任务中的失败，并将其归因于样本相似性导致的梯度干扰而非奖励校准问题。\n2. 展示了训练过程如何进行，揭示了有缺陷动作概率可能增加而非减少的原因和时间，并展示了冷启动在智能体强化学习中的重要性。\n3. 提出了一种新的训练范式，其中智能体同时学习充当评论家，有效解耦有害梯度，增强模型的区分能力，并显著提高性能。", "summary_translation": "构建能够解决长期视野、现实世界任务的自主智能体（autonomous agents）已获得显著的研究关注。但基于结果的奖励可能导致奖励校准不当（reward miscalibration），这意味着它可能错误地向有缺陷的中间步骤分配正向奖励，这被视为训练过程中不良行为被强化的关键原因。然而，我们揭示基于结果的奖励确保了那些有缺陷的中间步骤具有预期的负优势（negative advantage），这意味着有缺陷的行为应在训练过程中受到惩罚。即使考虑到\"挤压效应\"（squeezing effect），良好行为的概率质量（probability mass）也应该增加，并且行为体（actor）应该逐渐摆脱有害行为。这表明有缺陷的行为应在训练过程中受到惩罚。\n\n我们进一步确定相似样本之间的梯度耦合（gradient coupling）是智能体强化学习（agentic RL）中的一个关键问题，输入提示（input prompt）极其相似且输出动作空间（output action space）有限，因此在训练过程中，来自表现良好样本的梯度可能由于相似的输入观察和输出动作而无意中强化次优或不正确的动作。我们表明，在梯度耦合存在的情况下，一些有缺陷的行为可能会被增强。为解决此问题，我们提出训练行为体（actor）来区分好坏动作，以分离好坏动作的嵌入（embedding）并减轻梯度干扰，大量实验证明了其有效性。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#78", "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance", "link": "/arxiv/2509.23730", "arxiv_id": "2509.23730", "authors": "Siyao Song, Cong Ma, Zhihao Cheng, Shiye Lei, Minghao Li, Ying Zeng, Huaixiao Tou, Kai Jia", "summary": "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.", "subjects": "Artificial Intelligence", "date": "2025-09-28", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.253817", "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是提出一种新的强化学习框架EAPO（Expert-Assisted Policy Optimization），旨在通过外部专家辅助增强LLM的推理能力，这明显属于改进LLM基础能力和通用推理能力的范畴，而非将LLM作为工具应用于特定领域。 其次，论文包含多个关键正面指标：明确涉及大型语言模型(LLMs)这一核心概念；专注于reasoning能力（特别是数学推理）；采用强化学习(RL)作为训练方法；并包含多轮交互与专家咨询的智能体协作元素。 第三，论文不涉及任何排除标准领域：没有多模态与视觉内容，不专注于特定应用领域（数学推理被视为通用推理能力的一部分），也不关注模型基础设施或应用层面的可靠性。 最后，在特殊情况下，论文中提到的与外部专家的多轮交互和咨询机制，是一种通用的智能体协作框架，目的是增强LLM的通用推理能力，而非应用于特定领域。 论文的核心贡献是提出了一种通过外部专家辅助来增强LLM推理能力的新训练范式，这种方法可以提升模型固有的推理能力，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型在强化学习优化中面临的探索效率低下和奖励稀疏问题。针对复杂推理任务，我们提出了一种EAPO（Expert-Assisted Policy Optimization）方法，将专家咨询设计为可学习动作，使策略模型能自适应决定何时咨询专家，并在数学推理基准（AIME 2024/2025、AIMO 2025）上通过Pass@32和Var指标验证了其有效性，平均比自我探索模型提高5个百分点。", "inspiration_trace": "# EAPO方法逻辑演进分析\n\n## 一、问题识别：稀疏奖励与低效探索\n\n作者首先观察到大型语言模型(LLMs)在复杂推理任务中面临的核心挑战：\n\n1. **稀疏奖励问题**：现有方法主要依赖基于结果的监督，只有在推理过程结束时才能获得奖励信号，导致探索效率低下。\n   > \"Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards.\"\n\n2. **独立探索的局限性**：传统强化学习方法要求策略模型独立探索广阔的搜索空间，缺乏有效指导，优化过程不稳定。\n   > \"existing reinforcement learning (RL) algorithms rely on policy model to explore and exploit on its own to produce long chains of thought, resulting in inefficient exploration and suboptimal optimization.\"\n\n3. **现有方法不足**：RLHF存在奖励黑客、过度优化问题；测试时扩展方法(如Tree-of-Thoughts)通信成本高且不提升模型本身能力。\n\n## 二、核心假设：专家指导的价值\n\n基于上述问题，作者提出三个关键假设：\n\n1. **专家知识可缓解稀疏奖励**：引入外部专家指导可提供中间步骤反馈，密集化奖励信号。\n   > \"By incorporating external expert assistance, EAPO increases the likelihood of producing partially correct intermediate states and ultimately correct answers, densifying the reward signal...\"\n\n2. **自适应咨询可行**：策略模型能学习何时需要专家帮助，而非盲目依赖或完全忽略。\n   > \"EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals...\"\n\n3. **知识可内化**：通过适当训练，模型能逐渐内化专家知识，最终独立表现优异。\n   > \"External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities.\"\n\n## 三、方法设计：EAPO框架构建\n\n基于假设，作者设计了EAPO框架，包含四个关键组件：\n\n1. **专家咨询作为可学习动作**：将\"咨询专家\"添加到动作空间，使模型能选择独立推理或寻求帮助。\n   > \"the agent's action space is augmented with a special consult experts action, which enables agent improve its reasoning strategy when external assistance is needed.\"\n\n2. **多专家并行推理**：允许模型并行咨询多个专家，提高信息覆盖面。\n   > \"To broaden information coverage within a single turn while keeping computation manageable, we instantiate up to K replicas of the expert model.\"\n\n3. **渐进式减少依赖**：通过课程学习设计，随训练进展逐渐降低专家咨询频率。\n   > \"With a curriculum-like optimization design, the agent is allowed to consult experts freely without in the early stages of training... As training progresses, the model is incentivized to consult less frequently...\"\n\n4. **咨询惩罚机制**：引入惩罚项，进一步鼓励模型减少对专家的依赖。\n   > \"To further strengthen the model's own capability, a penalty term on consultation is introduced.\"\n\n## 四、理论解释：EAPO有效性机制\n\n作者进一步提供了EAPO有效性的理论解释：\n\n1. **缓解稀疏奖励**：专家指导提供中间步骤反馈，增加部分正确状态产生概率。\n   > \"EAPO enables the policy model to consult expert for assistance at critical steps during training, supplying cues of problem-solving from the external experts.\"\n\n2. **信息增益**：专家协助作为外部信号增强上下文，降低决策不确定性。\n   > \"EAPO can be viewed as a lightweight mechanism of information injection: expert assistance acts as external signals that augment the historical context...\"\n\n3. **隐式课程学习**：专家指导引导模型更快进入高奖励区域，为后续学习奠定基础。\n   > \"Under these conditions, EAPO acts as an implicit curriculum, guiding the model more rapidly into regions with high reward...\"\n\n## 五、实验验证：性能与内化证据\n\n通过数学推理基准测试验证EAPO有效性：\n\n1. **性能提升**：在AIME和AIMO数据集上，EAPO平均比自我探索模型提高5个百分点。\n   > \"Experiments on mathematical reasoning benchmarks... show that EAPO consistently outperforms... with an average gain of 5 points over self-exploratory models.\"\n\n2. **稳定性增强**：EAPO训练模型在测试集上表现更低的方差，推理更稳定。\n   > \"training with assistance from experts, policy model achieves lower variance on test sets, indicating greater stability and consistency...\"\n\n3. **内化证据**：随训练进行，模型咨询专家频率逐渐降低，表明知识内化。\n   > \"The policy model calls experts frequently in early training to obtain solution cues, but the number of calls declines markedly as training proceeds.\"\n\n## 六、演进逻辑总结\n\nEAPO方法的逻辑演进可概括为：从观察到LLMs在复杂推理中面临稀疏奖励和低效探索问题，假设引入专家指导可缓解这些问题，设计将专家咨询作为可学习动作的EAPO框架，通过渐进式减少依赖实现知识内化，最终使模型在评估时能独立推理且性能优异。这一过程体现了从问题识别到假设验证，再到方法设计与理论解释的完整科研逻辑链。", "summary_translation": "大语言模型（Large language models, LLMs）最近在可验证奖励下通过强化学习（reinforcement learning, RL）优化后，在推理能力方面取得了显著进展。现有方法主要依赖基于结果的监督（outcome-based supervision）来增强大语言模型的内部推理，但往往导致探索效率低下和奖励稀疏的问题。为缓解这一问题，我们提出了专家辅助策略优化（Expert-Assisted Policy Optimization, EAPO），这是一种新颖的强化学习框架，通过在训练过程中融入与外部专家的多轮交互（multi-turn interactions）来增强探索能力。与以往策略孤立推理的方法不同，EAPO激励策略自适应地确定何时以及如何咨询专家，从而产生更丰富的奖励信号和更可靠的推理轨迹（reasoning trajectories）。外部协助最终将专家知识内化到策略模型（policy model）中，放大了模型固有的推理能力。在评估阶段，策略模型已经过充分优化，能够独立解决问题，生成改进的推理路径和更准确的解决方案。在数学推理基准测试（包括AIME 2024、AIME 2025和AIMO 2025）上的实验表明，EAPO一致性地优于专家辅助工作流（expert-assisted workflow）、专家蒸馏模型（expert-distilled models）和强化学习基线（RL baselines），相比自探索模型（self-exploratory models）平均提高了5分。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#85", "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs", "link": "/arxiv/2509.23619", "arxiv_id": "2509.23619", "authors": "Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, Qiang Xu", "summary": "The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.", "subjects": "Artificial Intelligence", "date": "2025-09-28", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.262876", "filter_reason": "这篇论文完全符合我的研究目标，核心原因如下： 首先，从本质上看，论文的核心贡献是提出一种名为\"Reasoning Scaffolding\"（推理支架）的新框架，用于提升语言模型的通用推理能力。论文明确指出当前主流方法（从文本理由中进行行为克隆）的局限性，即只教会小模型模仿表面模式而非思维的底层算法结构。作者提出的解决方案是将推理重构为结构化生成过程，通过多任务训练使模型内化连贯推理的计算模式。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，论文包含了多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理(reasoning)，特别是逻辑推理；虽然论文没有涉及强化学习等特定训练方法，但它提出了一种创新的多任务训练范式来提升推理能力。 第三，论文不符合任何排除标准：它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，论文提出的\"推理支架\"方法通过将思维过程抽象为可解释的语义信号，不仅提升了模型的推理能力，还间接增强了模型的可解释性和逻辑一致性，这与提升模型内在可靠性的目标是一致的。 总之，这篇论文直接针对大语言模型的通用推理能力提升，提出了一种新的训练范式，完全符合我的研究范围。", "summary2": "本文旨在解决从大型语言模型(LLMs)向小型语言模型(SLMs)蒸馏推理能力时存在的逻辑鲁棒性不足问题。针对文本理由的行为克隆这一传统方法，我们提出了一种Reasoning Scaffolding框架，将推理过程抽象为离散语义信号的结构化生成过程，并在GSM8K、StrategyQA等多个推理基准测试上通过Pass@1指标验证了其有效性。", "inspiration_trace": "# Reasoning Scaffolding方法论逻辑推演\n\n## 一、宏观问题发现：推理蒸馏的根本缺陷\n\n作者首先观察到当前大型语言模型(LLMs)向小型语言模型(SLMs)转移推理能力的主流方法存在根本性限制。现有方法主要是通过行为克隆从文本理由(Chain-of-Thought rationales)中提取推理能力，这本质上是一种表面模仿。\n\n**核心问题**：传统方法让小型模型学习\"写什么\"，而不是\"如何思考\"。这导致学生模型只是学会了表面文本模式，而非底层的算法思维结构，在面对新问题时表现出逻辑脆弱性。\n\n## 二、深入分析：现象背后的本质原因\n\n作者进一步剖析了这一现象的根本原因：\n\n1. **文本模仿的局限性**：行为克隆将推理视为文本模仿任务，强制小型模型进行一种机械记忆，而非真正的推理学习。\n\n2. **算法结构的缺失**：现有方法无法捕捉和转移教师模型思维过程中的算法结构，只是复制了表面的语言风格。\n\n3. **逻辑一致性的缺乏**：结果是学生模型在面对新问题时容易产生逻辑不一致或无意义的论证。\n\n这一分析揭示了问题的本质：我们需要的是思维结构的转移，而非文本的克隆。\n\n## 三、关键洞察：推理过程的结构化抽象\n\n通过观察教师模型的推理过程，作者获得了关键洞察：\n\n**核心发现**：教师模型的推理过程可以从冗长的文本中抽象为结构化的\"蓝图\"。这个蓝图由离散的、可解释的语义信号组成（如\"对比\"、\"补充\"、\"结论\"等），这些信号控制着连贯论证的流程。\n\n作者观察到在推理轨迹中存在特定的关键词（如\"wait\"、\"but\"、\"ok\"、\"in addition\"），这些词自然地标志着推理中的转换点，可以归类为7种语义信号。\n\n## 四、形成假设：结构化推理转移的可能性\n\n基于上述洞察，作者形成了核心假设：\n\n**核心假设**：如果我们能够将教师模型的推理过程抽象为结构化的语义信号，并让小型模型同时学习这些信号和相应的推理步骤，那么小型模型将能够内化推理的计算模式，而不仅仅是克隆文本。\n\n这一假设将问题从\"如何模仿文本\"转变为\"如何转移思维结构\"。\n\n## 五、解决方案设计：Reasoning Scaffolding框架\n\n基于核心假设，作者提出了\"Reasoning Scaffolding\"（推理支架）框架，该框架包含三个关键组件：\n\n### 1. 逻辑表示蒸馏（Logic Representation Distillation）\n\n**目标**：将教师模型的推理过程抽象为结构化的语义信号。\n\n**方法**：\n- 查询大型推理模型获取详细的推理轨迹\n- 将推理轨迹分割为单个步骤\n- 通过两阶段方法为每个步骤分配语义信号：\n  - 第一阶段：基于关键词匹配进行初始标记\n  - 第二阶段：使用强大的LLM进行语义验证和修正\n\n**创新点**：将冗长的推理文本抽象为7种离散、可解释的语义信号，形成推理的\"结构蓝图\"。\n\n### 2. 推理提议者和语义信号预测器的联合训练\n\n**目标**：让小型模型同时学习预测推理流程和生成具体内容。\n\n**方法**：\n- 设计双分支架构：一个分支用于生成下一步推理，另一个分支用于预测语义信号\n- 通过多任务目标训练小型模型：\n  - 任务1：预测下一个语义信号，预期推理流程\n  - 任务2：基于该信号生成相应的推理步骤\n- 总损失函数：L(t) = L(t)_token + L(t)_signal\n\n**创新点**：使用信号预测任务作为逻辑一致性的强大正则化器，迫使学生模型内化推理的计算模式。\n\n### 3. 语义信号引导的推理\n\n**目标**：在推理过程中利用学习到的支架指导模型思维。\n\n**方法**：\n- 在推理过程中，每一步推理都由预测的语义信号引导\n- 采用自适应策略，计算预测信号的置信度\n- 如果置信度超过阈值，使用该信号指导下一步推理\n- 如果低于阈值，终止推理轨迹并生成结论\n\n**创新点**：通过可解释的语义信号引导推理过程，使小型模型能够模仿教师模型的思维结构，而非简单复制文本。\n\n## 六、验证与优化：实证检验与改进\n\n作者通过实验验证了方法的有效性，并进行了一系列优化：\n\n### 实验验证\n- 在多个推理基准测试上评估方法（GSM8K、StrategyQA等）\n- 结果显示，与最先进的蒸馏方法相比，该方法在准确性和逻辑一致性方面都有显著提高\n- 特别是在小型模型上，提升更为明显，证明了方法的有效性和普适性\n\n### 优化策略\n1. **自适应信号预测**：基于置信度阈值决定是否使用预测信号，确保推理可靠性\n2. **推理轨迹优化**：通过保留关键结论步骤并修剪其他步骤，显著减少令牌使用量，同时保持推理性能\n\n## 七、反思与展望：局限性与未来方向\n\n作者也客观评估了方法的局限性，并提出了未来研究方向：\n\n### 局限性\n1. 主要提取高级话语标记，而非形式化的算法或逻辑操作\n2. 注释方法结合了启发式技术和基于LLM的验证，引入了对教师格式和外部模型的依赖\n\n### 未来方向\n1. 结合更细粒度的逻辑或算法信号，弥合话语级和形式推理之间的差距\n2. 开发更鲁棒、自监督的信号提取方法，减少对昂贵预言机的依赖\n\n## 总结：从问题到解决方案的逻辑演进\n\n作者的思想演进展现了一条清晰的逻辑链：从发现现有方法的表面模仿问题，到分析其逻辑脆弱性的本质原因，再到洞察推理过程的结构化特征，形成结构化推理转移的核心假设，最终设计出\"Reasoning Scaffolding\"框架，通过多任务训练让小型模型真正内化推理的计算模式。这一方法论不仅解决了当前推理蒸馏的根本缺陷，为创建真正具备推理能力的小型模型提供了新路径，也为语言模型的知识转移研究开辟了新方向。", "summary_translation": "从大型语言模型（Large Language Models, LLMs）中蒸馏推理的主流方法——即从文本理由中进行行为克隆（behavioral cloning）——存在根本性局限。这种方法教导小型语言模型（Small Language Models, SLMs）模仿表面模式，而非思想的底层算法结构，导致逻辑鲁棒性（logical robustness）严重不足。我们认为，蒸馏过程不应克隆文本，而应直接转移这种算法结构。我们提出了推理支架（Reasoning Scaffolding）框架，该框架将推理重新构建为一个结构化的生成过程。我们的方法首先将教师模型的思想过程抽象为一系列离散的、可解释的语义信号（例如，对比[Contrast]、添加[Addition]），这些信号充当支架。随后，学生模型通过多任务目标进行训练，既要（1）预测下一个语义信号，预期推理流程，又要（2）基于该信号生成相应的步骤。这种多任务方案作为一种强大的正则化器（regularizer），迫使学生模型内化连贯推理的计算模式。在一系列具有挑战性的推理基准测试（reasoning benchmarks）中，我们的方法在准确性和逻辑一致性方面均显著优于最先进的蒸馏技术（state-of-the-art distillation），为创建真正具有推理能力而不仅是流利模仿者的小型模型提供了路径。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#91", "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks", "link": "/arxiv/2509.23537", "arxiv_id": "2509.23537", "authors": "Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang", "summary": "We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.", "subjects": "Artificial Intelligence", "date": "2025-09-28", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.266255", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究多轮多智能体编排框架，通过多个LLM智能体之间的交互和投票机制来提高问题解决能力，这属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。其次，论文满足多个正面指标：核心概念明确涉及大型语言模型(LLMs)；能力方向上，虽然摘要未直接提及\"reasoning\"，但多智能体通过交互和投票达成共识的过程本质上是一种推理和问题解决，且在MuSR（多步推理）等基准上进行测试；新兴范式方面，论文明确研究\"multi-turn multi-agent orchestration\"，属于多智能体系统的新兴范式。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面研究。最后，论文提出的是通用的多智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用。因此，这篇论文的核心贡献——通过多智能体协作框架提升LLM的通用推理能力——完全符合研究目标。", "summary2": "本文旨在探索多智能体协作是否能超越最强单一LLM的性能。针对多个LLM在复杂推理任务上的表现，我们提出了一种多轮多智能体编排框架，让多个LLM智能体通过迭代提出答案或投票直到达成共识，并在GPQA-Diamond、IFEval和MuSR三个基准测试上通过准确率指标验证了其有效性。", "inspiration_trace": "# 从单一模型局限到多智能体编排：逻辑演进分析\n\n## 宏观问题：LLM能力的边界与突破\n\n论文的出发点源于对一个根本性问题的思考：**如何突破单一大型语言模型(LLM)的性能天花板？** 作者观察到，尽管LLM能力迅速提升，但没有任何一个模型能在所有任务上表现卓越，这引发了一个更深层的问题：是否存在一种方法能整合不同模型的优势，实现超越单一最强模型的性能？\n\n## 观察与聚焦：多智能体协作的潜力\n\n作者通过三个关键观察将问题聚焦：\n\n1. **模型互补性现象**：不同LLM在不同类型任务上各有所长，表明模型间存在潜在互补性。例如，Gemini 2.5 Pro在GPQA-Diamond上表现最佳(85.9%)，而GPT-5在IFEval上领先(87.4%)。\n\n2. **集体智慧的启示**：人类解决复杂问题时常通过协作、辩论达成更优决策，这一机制可能在AI系统中同样有效。\n\n3. **研究空白识别**：现有多智能体协作研究缺乏与强单一模型的直接比较，且未系统考察协调策略对共识形成的影响。\n\n由此，核心问题聚焦为：**多轮多智能体编排能否有效整合不同LLM的优势，超越最强单一模型的性能？**\n\n## 假设形成：理论基础构建\n\n基于聚焦的问题，作者提出三个核心假设：\n\n1. **互补性假设**：不同LLM的优势可通过适当协调机制结合，产生超越任何单一模型的集体性能。\n\n2. **共识形成假设**：多轮交互(答案提案与投票)能使智能体逐步修正错误，达成比初始判断更准确的共识。\n\n3. **协调机制影响假设**：协调策略(如身份透明度、投票可见性)将显著影响共识质量和形成过程。\n\n## 方法设计：多轮多智能体编排框架\n\n为验证假设，作者设计了一个结构化的三阶段编排框架：\n\n1. **智能体行动阶段**：智能体异步操作，可选择生成新答案或对现有答案投票。引入\"动态重启\"机制——当有新答案提出时，中断当前投票，使所有智能体重新评估，防止过早共识。\n\n2. **共识阶段**：所有智能体完成答案生成和投票后，获多数票的答案被选为共识。动态重启后仅统计重启后的投票，确保基于最新信息决策。\n\n3. **最终呈现阶段**：获胜智能体整合所有参与者的见解和推理，生成综合最终答案。\n\n这一设计的关键创新在于动态重启机制和结构化投票流程，旨在平衡效率与全面性。\n\n## 实验验证：系统测试与消融研究\n\n作者设计两组互补实验验证方法：\n\n1. **基准比较实验**：在GPQA-Diamond、IFEval和MuSR三个基准上，比较编排系统与四个单一LLM(Gemini 2.5 Pro、GPT-5、Grok 4、Claude Sonnet 4)的性能。\n\n2. **协调策略消融实验**：在GPQA-Diamond上，控制两个关键变量：\n   - 投票身份披露(匿名vs实名)\n   - 投票计数可见性(隐藏vs显示)\n\n通过测量自我投票率、首轮答案选择率和共识僵局率等指标，量化协调策略的影响。\n\n## 发现与意义：验证与启示\n\n实验结果证实了核心假设，并带来重要启示：\n\n1. **编排系统有效性**：编排系统在三个基准中的两个取得最高准确率，平均表现(81.2%)超越所有单一模型，证实了多智能体协作的价值。\n\n2. **协调策略的关键影响**：\n   - 身份披露增加自我投票(如GPT-5从81.0%升至88.4%)和共识僵局(从14.1%升至23.2%)\n   - 投票可见性引发羊群效应，首轮答案选择率从54.1%升至67.8%\n\n3. **改进空间识别**：分析显示，即使系统出错，参与智能体中常有正确答案(如GPQA-Diamond上64%的错误案例中至少有一个智能体正确)，表明协调机制仍有优化空间。\n\n## 逻辑演进总结\n\n从宏观问题到具体方法，作者的逻辑演进呈现清晰路径：从观察单一LLM的局限性→识别多智能体协作潜力→形成互补性、共识形成和协调机制影响假设→设计结构化编排框架→通过系统实验验证→揭示协调策略的关键作用→指出未来改进方向。这一研究不仅证明了多轮多智能体编排的有效性，更重要的是揭示了协调机制设计对系统性能的决定性影响，为未来多智能体系统研究提供了重要理论基础和方法指导。", "summary_translation": "我们研究了多轮多智能体编排(multi-turn multi-agent orchestration，多轮多智能体协同工作)，其中多个大型语言模型(large language model, LLM)智能体通过多轮交互，迭代提出答案或进行投票，直至达成共识。我们在GPQA-Diamond、IFEval和MuSR三个数据集上使用四种LLM模型（Gemini 2.5 Pro、GPT-5、Grok 4和Claude Sonnet 4）进行了两项实验：(i)将编排方法与单一LLM基线进行基准测试；以及(ii)在GPQA-Diamond上进行消融实验(ablations，消融研究)，改变智能体是否能看到答案作者以及是否能观察正在进行的投票。编排方法匹配或超过了最强的单一模型，并且始终优于其他模型。对最佳可实现编排性能的分析显示出进一步改进的潜力。消融实验表明，揭示作者身份会增加自我投票和平局情况，而显示正在进行的投票会放大羊群效应(herding，群体跟随行为)，这加快了收敛速度，但有时可能导致过早达成共识。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#104", "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning", "link": "/arxiv/2509.23285", "arxiv_id": "2509.23285", "authors": "Yifei Chen, Guanting Dong, Zhicheng Dou", "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.", "subjects": "Artificial Intelligence", "date": "2025-09-27", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.286358", "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是关于改进LLM的通用推理能力，具体聚焦于\"Tool-Integrated Reasoning (TIR)\"，即通过集成外部工具来增强大语言模型的内部推理能力。这明显属于提升LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标的主题：核心概念上明确研究LLMs；能力方向上专注于reasoning（推理）；训练方法上提出了\"Self-Evolved Preference Learning\"和\"Self-Evolved Direct Preference Optimization (DPO)\"，属于self-evolve（自我进化）的训练方法；新兴范式上关注tool use（工具使用），这些都是高度相关的主题。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 在特殊和模糊情况处理上，论文提出的是一种通用的工具使用框架（Tool-Light）来增强LLM的通用推理能力，而不是将工具应用在特定领域，这符合保留标准。论文通过提高推理效率和质量，间接有助于提升模型的通用推理能力。 论文的核心贡献是提出了Tool-Light框架，通过数据集构建和多阶段微调（包括SFT和Self-Evolved DPO）来鼓励LLM高效准确地执行工具集成推理，这直接服务于提升LLM通用推理能力的研究目标。因此，这篇论文完全符合我的研究范围。", "summary2": "本文旨在解决Tool-Integrated Reasoning (TIR)模型中工具使用不当和过度思考的问题。针对多工具调用场景，我们提出了一种基于信息熵分析的Tool-Light框架，包含熵引导采样策略和两阶段训练方法，并在10个推理任务数据集上通过Efficiency和Necessity指标验证了其有效性。", "inspiration_trace": "# Tool-Light方法逻辑推演：从问题观察到方法创新\n\n## 1. 宏观问题：工具集成推理的效率与准确性挑战\n\n作者从大型语言模型(LLMs)的核心局限性出发：尽管LLMs在多种推理任务上表现出色，但当面临复杂任务（如深度信息检索和精确计算）时，仅依靠内部推理能力往往不足。Tool-Integrated Reasoning (TIR)方法应运而生，允许模型在推理过程中调用外部工具以弥补内部知识或能力的不足。\n\n然而，作者观察到现有TIR模型存在三个关键问题：\n- **工具调用不合理**：模型表现出工具使用不足或过度使用的次优行为\n- **工具后过度思考**：当获得低质量工具调用结果时，模型容易出现过度思考甚至分析瘫痪\n- **推理过程不稳定**：缺乏平衡工具使用与内部推理的有效机制\n\n这些问题构成了作者研究的出发点：如何激励LLMs高效准确地执行TIR，同时稳定推理过程？\n\n## 2. 现有方法局限性分析\n\n作者系统审视了现有解决方案，发现其局限性：\n\n1. **强化学习方法**：主要针对单个工具优化，难以泛化到多工具调用场景\n2. **多工具推理优化**：通常只关注工具过度使用问题，忽视了工具使用不足以及工具调用结果对后续推理过程的影响\n3. **全面性不足**：现有工作未能全面解决\"不正确工具调用\"问题，包括过度使用、使用不足和工具后推理效率低下\n\n这些局限性表明，需要一种更全面的方法来优化TIR过程，不仅关注工具使用的数量，还要关注工具使用的质量和推理效率。\n\n## 3. 新视角引入：从信息熵角度探索TIR过程\n\n作者创新性地引入信息熵理论来分析TIR过程。受先前研究启发（推理链的高熵部分往往决定推理方向），作者进行了初步实验，分析TIR任务中的信息熵特征。\n\n关键发现：\n1. **工具调用后的熵变模式**：当模型接收到工具调用结果后，其输出信息熵先上升，然后波动，最后在下一个工具调用到来前急剧下降\n2. **工具调用数量与熵关系**：对于同一样本，低熵链往往涉及较少的工具调用，且随着推理进行，高熵链和低熵链之间的工具调用差异越来越明显\n\n这些发现揭示了工具调用与推理过程信息熵之间的内在联系，为优化TIR提供了新思路。\n\n## 4. 核心假设形成\n\n基于上述观察，作者形成了三个核心假设：\n\n1. **熵-效能关联假设**：工具调用的效率与推理过程中的信息熵变化密切相关\n2. **优化路径假设**：通过优化信息熵分布，可以减少不必要的工具调用，避免过度思考，同时确保在必要时进行工具调用\n3. **训练框架假设**：结合信息熵指导和偏好学习，可以设计出更有效的TIR训练框架\n\n这些假设构成了Tool-Light方法设计的理论基础。\n\n## 5. 方法设计：Tool-Light框架\n\n基于核心假设，作者设计了Tool-Light框架，从数据构建和算法两个角度优化模型的TIR能力。\n\n### 5.1 数据构建：熵引导采样策略\n\n作者设计了一种创新的熵引导采样方法：\n1. **主链生成**：首先生成一个主推理链\n2. **高熵分支**：从最高熵位置分支创建多个路径（基于高熵位置更可能产生多样性输出的观察）\n3. **正负对筛选**：应用严格标准选择高质量的正负对\n4. **混合采样**：将熵引导采样与直接采样相结合，确保数据多样性和平衡\n\n这种方法将计算复杂度从O(mn)降低到O(n log m)，显著提高了采样效率。\n\n### 5.2 算法设计：两阶段TIR训练范式\n\n作者提出了\"两阶段TIR训练\"流程：\n\n1. **监督微调(SFT)**：使用现有SFT数据训练基础模型，帮助模型快速获取完成TIR任务的能力\n\n2. **自进化DPO训练**：\n   - **预对齐DPO训练**：增强模型推理能力，同时减少冗余工具调用\n   - **自进化DPO对齐**：交替采样和训练过程，使模型学习必要的工具调用，同时根据模型当前熟练程度动态调整训练数据复杂性\n\n关键创新点在于自进化机制：模型通过自身生成的数据进行迭代改进，不断优化工具调用策略。\n\n## 6. 实验验证与结果分析\n\n作者在10个具有挑战性的推理任务上验证了Tool-Light的有效性，包括知识密集型和数学推理任务。\n\n关键结果：\n1. **性能提升**：Tool-Light在多个任务上达到最先进水平，平均性能显著优于基线方法\n2. **工具使用效率**：在效率和必要性指标上表现最佳，表明模型能够平衡工具使用与内部推理\n3. **熵分布优化**：Tool-Light的输出序列具有较低的熵分布，证实了熵引导策略的有效性\n4. **推理简化**：相比基线方法，Tool-Light产生了更短的输出序列，同时保持高准确性，有效减轻了过度思考现象\n\n## 7. 逻辑链条总结\n\n作者的研究思路形成了一个完整的逻辑链条：\n\n1. **问题识别**：从TIR的效率和准确性挑战出发，识别工具调用不合理和过度思考等关键问题\n2. **视角创新**：引入信息熵理论分析TIR过程，发现工具调用与熵变化的关联模式\n3. **假设形成**：基于熵观察形成核心假设，提出通过优化熵分布来改善TIR效能的思路\n4. **方法设计**：设计Tool-Light框架，包括创新的熵引导采样策略和两阶段自进化训练范式\n5. **实验验证**：通过多任务实验验证方法有效性，证实了熵-效能关联假设和优化路径假设\n\n这一研究不仅解决了TIR中的实际问题，还为理解和优化LLMs的工具使用行为提供了新的理论视角，体现了从问题观察到方法创新的完整科研思路。", "summary_translation": "工具集成推理（Tool-Integrated Reasoning, TIR）使大型语言模型（Large Language Models, LLMs）能够通过集成外部工具来提高其内部推理能力。然而，采用TIR的模型常常表现出次优行为，如工具使用不足或过度，以及在工具调用后过度思考。如何激励LLMs高效准确地执行TIR，同时稳定推理过程，仍然是一个悬而未决的问题。在本文中，我们首先从信息熵的角度探讨工具调用对模型推理的影响。我们的研究结果表明，工具调用结果导致后续推理的信息熵发生明显变化，推理链的整体熵值根据工具调用的次数而变化。基于这些见解，我们提出了Tool-Light，这是一个旨在鼓励LLMs高效准确地执行TIR的框架。我们的框架包括数据集构建和多阶段微调。在数据集构建方面，我们采用使用微调模型的连续自我进化采样，结合了原始采样（vanilla sampling）和熵引导采样（entropy-guided sampling）。此外，我们在采样过程中建立了选择正负对（positive-negative pairs）的严格标准。训练过程采用两阶段方法，包括监督微调（Supervised Fine-Tuning, SFT）和自我进化直接偏好优化（Self-Evolved Direct Preference Optimization, DPO）。在10个数据集上的实验结果证明了Tool-Light的有效性，显著提高了模型执行TIR任务的效率。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#112", "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers", "link": "/arxiv/2509.23178", "arxiv_id": "2509.23178", "authors": "Tian Qin, Yuhan Chen, Zhiwei Wang, Zhi-Qin John Xu", "summary": "Transformers are able to perform reasoning tasks, however the intrinsic mechanism remains widely open. In this paper we propose a set of information propagation rules based on Transformers and utilize symbolic reasoning tasks to theoretically analyze the limit reasoning steps. We show that the limit number of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with $L$ attention layers in a single-pass.", "subjects": "Artificial Intelligence", "date": "2025-09-27", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.295058", "filter_reason": "这篇论文的核心贡献是提出了一套基于Transformer的信息传播规则，并利用符号推理任务来理论上分析推理步骤的极限。论文研究了具有L个注意力层的模型在一次前向传播中能够执行的推理步骤数量限制，这直接关注大语言模型的基础推理能力，特别是多步推理能力的理论分析。根据筛选标准的第一步，这篇论文是关于改进LLM基础能力的研究，符合保留条件。论文明确涉及\"reasoning tasks\"和\"symbolic multi-step reasoning tasks\"，这与我们的研究目标高度相关。同时，论文不涉及任何需要排除的特定应用领域或多模态研究。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在分析Transformer在符号多步推理任务中的推理能力极限。针对符号多步推理任务，我们提出了一套基于Transformer的信息传播规则，并在合成数据集上通过推理准确率和模型维度等指标验证了其有效性。理论分析表明，具有L层注意力块的Transformer在单次前向传播中能够执行的推理步数极限在O(2^{L-1})和O(3^{L-1})之间，实验结果支持了这一理论预测。", "inspiration_trace": "# 从观察到理论：Transformer推理极限的分析逻辑链\n\n## 1. 宏观问题的提出\n\n**观察起点**：Transformer模型在逻辑推理任务中展现出惊人能力，甚至能在国际数学奥林匹克竞赛中达到与顶尖人类选手相当的水平。然而，常用的推理策略如Chain-of-Thought (CoT)等存在\"过度思考\"现象，消耗大量计算资源。\n\n**核心问题**：Transformer模型的内在单次推理能力极限是什么？具体而言，在不依赖迭代提示或外部脚手架的情况下，模型在单次前向传播中能执行多少步有效推理？\n\n## 2. 初步假设与研究方向\n\n**假设形成**：Transformer的推理能力可能源于其架构特性，特别是注意力机制和层叠结构中的信息传播方式。模型可能通过某种机制在单次前向传播中并行执行多步推理。\n\n**研究方向确定**：\n- 研究Transformer架构中的信息传播机制\n- 使用符号推理任务进行理论分析\n- 探索推理步数的理论极限\n\n## 3. 深入分析与机制识别\n\n**关键机制发现**：通过分析大量关于上下文学习和归纳头的研究，识别出三个核心机制：\n\n1. **缓冲机制**：Transformer通过线性变换将多条信息存储到不同子空间，使每个位置能同时保存多个令牌的信息。\n\n2. **相邻位置匹配**：模型利用位置编码在相邻令牌间建立连接，类似于人类依赖前一个词预测下一个词的行为模式。\n\n3. **相同令牌匹配**：归纳头内的基本机制，使包含相同信息的节点能够相互关注，赋予模型强大的分布外泛化能力。\n\n**关键观察**：传统观点认为每层只执行一步推理，但这远未达到Transformer的上限。相邻位置匹配和相同令牌匹配可在单层内多次发生，使浅层模型也能执行多步推理——作者将此现象称为\"并行推理\"。\n\n**问题聚焦**：给定只有相邻位置匹配和相同令牌匹配，具有L层注意力块的Transformer能执行的并行推理步数的上下界是什么？\n\n## 4. 理论框架构建\n\n**符号推理任务定义**：为进行理论分析，作者精确定义了推理对、推理链、推理序列等概念，建立了形式化的推理任务描述框架。\n\n**信息传播规则提取**：基于Transformer行为，提出五条核心规则：\n1. 初始设置规则：定义第0层节点的构造方式\n2. 掩码条件规则：注意力只能从前向节点传播到后向节点\n3. 相邻位置匹配规则：奇数位置节点的信息可传播到后续偶数位置节点\n4. 相同令牌匹配规则：具有相同值集的节点可以相互传播信息\n5. 残差连接规则：确保信息在层间传递时不丢失\n\n**理论分析**：通过数学归纳法，作者证明了在信息传播规则下，对于L层Transformer，推理步数的极限满足：\n- 下界：O(2^(L-1))\n- 上界：O(3^(L-1))\n\n这一结果源于两个关键架构特性：(i)令牌在每层内并行执行推理；(ii)每个嵌入可编码来自不同子线性空间的多个令牌信息。\n\n## 5. 实验验证与结果\n\n**实验设计**：作者构建了与理论分析一致的符号推理任务，训练不同层数和维度的Transformer模型，测试其多步推理能力。\n\n**关键发现**：\n- 3层Transformer能完美解决3步推理问题，但需要较大的隐藏维度\n- 测试准确度随隐藏维度dm单调增加，最终接近100%\n- 对于4步推理，模型达到46.1%准确度；5步推理时降至25.1%\n- 当推理步数超过理论上限(3^(L-1)-1)/2时，模型性能显著下降\n\n**因果验证**：通过干预实验，作者证实当模型正确执行推理时，其信息流动符合预设的推理规则；而当推理步数超过理论极限时，模型依赖记忆而非真正推理。\n\n## 6. 结论与意义\n\n**最终结论**：具有L层注意力块的Transformer在单次前向传播中能执行的最大推理步数Sp满足：\n2^(L-1) - 1 ≤ Sp ≤ (3^(L-1) - 1)/2\n\n**理论意义**：这一分析首次揭示了Transformer在单次前向传播中的推理能力极限，为理解大型语言模型的内在机制提供了理论基础，也为设计更高效的推理策略指明了方向。\n\n**实践启示**：结果解释了为什么复杂推理任务需要多层架构或迭代推理策略，同时也表明通过优化信息传播方式，有可能提升模型的推理效率。", "summary_translation": "Transformers (Transformer模型) 能够执行推理任务，但其内在机制(intrinsic mechanism)在很大程度上仍未明确。在本文中，我们提出了一套基于Transformers的信息传播规则(information propagation rules)，并利用符号推理任务(symbolic reasoning tasks)来理论分析极限推理步骤(limit reasoning steps)。我们表明，对于具有L个注意力层(attention layers)的模型，在单次传递(single-pass)中，极限推理步骤数介于$O(3^{L-1})$和$O(2^{L-1})$之间。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#120", "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models", "link": "/arxiv/2509.23108", "arxiv_id": "2509.23108", "authors": "Morgan McCarty, Jorge Morales", "summary": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.", "subjects": "Artificial Intelligence", "date": "2025-09-27", "category": "cs.AI", "crawl_time": "2025-10-06T21:53:08.304495", "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)如何通过命题推理(propositional reasoning)来完成传统上被认为需要视觉心理意象的任务。论文创建了认知心理学中的经典心理意象任务的新项目，测试了几个最先进的LLMs，发现它们的表现显著高于人类平均水平，特别是当模型分配更多的推理token时表现更强。这表明LLMs具有一种新兴的认知能力，能够通过纯文本推理完成依赖意象的任务。论文关注的是LLMs的基础推理能力和认知机制，而不是将LLM作为工具应用到特定领域，也不涉及多模态、特定应用领域或模型可靠性等排除标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，特别是关于逻辑推理和问题解决能力的研究。", "summary2": "本文旨在探索大型语言模型(LLMs)是否能完成传统上需要视觉心理意象(mental imagery)的任务。针对60个心理意象任务指令集(48个全新创建和12个来自经典研究)，我们提出了一种基于命题推理(propositional reasoning)的测试方法，并在多个最先进的LLMs与100名人类受试者上通过加权评分系统验证了其有效性。实验发现，最佳LLMs(GPT-5和o3系列)表现显著超过人类平均水平(高9.4%-12.2%)，且图像辅助处理反而降低性能，表明LLMs可能通过纯语言处理完成了依赖意象的任务。", "inspiration_trace": "# 从心理意象本质到LLMs认知能力：论文核心方法的逻辑推演\n\n## 1. 宏观问题：心理意象的本质表征\n\n论文的出发点源于认知心理学中一个持续数十年的根本性争论：**心理意象的本质是什么？**\n- **意象理论(Pictorial view)**：心理意象以类似图像的形式表征，类似于视觉体验\n- **命题理论(Propositional view)**：心理意象可通过命题描述（语言元素）捕捉，无需真正图像表征\n\n这一争论触及人类思维的基本机制，影响我们对认知过程的理解。\n\n## 2. 关键观察：理论与现实的不一致性\n\n作者注意到一个与主流意象理论不符的现象：\n- **无意象人群(Aphantasics)**：约1-4%人口报告没有有意识的心理意象\n- **矛盾发现**：这些人在传统上被认为需要心理意象的任务中表现与常人相当\n\n如果意象理论正确，无意象人群应无法完成此类任务，但事实并非如此，暗示可能有其他机制（如命题推理）支持任务完成。\n\n## 3. 研究机会：LLMs作为理想测试平台\n\n作者敏锐地识别到LLMs提供了一个独特的研究机会：\n- **LLMs的特性**：基于语言训练和处理，无已知视觉意象能力\n- **研究价值**：若LLMs能完成传统上需要视觉意象的任务，将支持命题理论\n- **创新视角**：将AI系统作为认知科学理论的测试平台，跨越学科界限\n\n## 4. 核心假设：命题推理支持的心理意象\n\n基于上述观察，作者提出具体假设：\n**LLMs可通过纯命题推理（语言处理）完成传统上被认为需要视觉心理意象的任务**\n\n若假设成立，将表明：\n1. 命题理论在某些情境下可行\n2. LLMs具有新兴认知能力\n3. 心理意象任务可能不严格依赖视觉表征\n\n## 5. 方法选择：经典任务的创新适应\n\n为验证假设，作者选择并改进了Finke等(1989)的经典心理意象任务：\n- **任务特点**：按指令对想象中的字母/形状进行转换，最终识别物体\n- **改进创新**：\n  * 创建48个全新指令集（避免训练数据污染）\n  * 增加认知负荷（允许每步最多两个字母）\n  * 不限制最终图像只有一种规范形式\n  * 更新语言表述，提高清晰度\n\n## 6. 实验设计：多维度比较验证\n\n作者设计了全面的实验框架：\n- **人类基线**：100名受试者建立性能基准\n- **LLM测试**：多种最先进模型（Claude、Gemini、OpenAI）\n- **图像辅助条件**：测试图像生成是否提升性能\n- **评估系统**：结合专家评分和众包评分处理答案主观性\n\n这一设计允许直接比较LLMs和人类在心理意象任务上的表现，验证核心假设。\n\n## 7. 扩展验证：推理与图像的对比实验\n\n为进一步验证，作者设计了对比实验：\n- **推理令牌测试**：不同推理级别对性能的影响\n- **图像辅助推理**：强制模型生成并修改图像，而非仅依靠想象\n- **上下文测试**：单上下文与多上下文比较，测试上下文学习效果\n\n这些实验设计提供了多角度验证假设的机会。\n\n## 8. 结果分析与理论解释\n\n实验结果支持了作者的核心假设：\n- **主要发现**：最佳LLMs（GPT-5和o3系列）显著超越人类表现（高出9.4%-12.2%）\n- **关键证据**：添加图像反而降低最佳模型性能，支持命题推理而非视觉处理\n- **推理效应**：推理令牌数量与性能正相关，表明认知处理的重要性\n\n作者提出**空间意象与物体意象的区分**作为解释框架，可能调和不同理论观点。\n\n## 9. 理论贡献与未来方向\n\n研究不仅验证了假设，还提出了更广泛的理论意义：\n- **认知科学**：重新点燃心理意象表征形式的辩论\n- **人工智能**：揭示LLMs新兴的认知能力\n- **跨学科价值**：为两个领域提供新研究范式和方法\n\n未来方向包括：研究无意象人群策略、探索更复杂任务、分析LLMs内部表征机制等。\n\n---\n\n这一逻辑链展示了作者如何从宏观的认知科学问题出发，通过观察理论与现实的矛盾，识别LLMs作为研究平台的新机会，提出具体假设，设计创新实验，最终通过实证数据验证假设并提出理论解释。整个过程体现了科学研究的典型逻辑：从问题到假设，再到实验验证和理论解释，同时展现了跨学科研究的创新价值。", "summary_translation": "本研究提供了一种基准测试人工系统中复杂认知行为的新方法。几乎普遍情况下，大型语言模型（Large Language Models, LLMs）在可能包含在其训练数据中且仅使用自然语言即可完成的任务上表现最佳，这限制了我们对它们新兴的复杂认知能力的理解。在这项工作中，我们创建了几十个来自认知心理学（cognitive psychology）的经典心理意象任务（mental imagery task）的新项目。传统上，认知心理学家认为这类任务只能通过视觉心理意象（visual mental imagery）解决（即仅靠语言是不够的）。大型语言模型是测试这一假设的理想选择。首先，我们通过向纯文本模型提供书面指令，并要求它们在执行上述任务中的转换后报告结果对象，从而测试了几个最先进的大型语言模型。然后，我们通过测试100名人类受试者完成完全相同的任务来创建基线。我们发现，最佳的大型语言模型的表现显著高于人类平均水平。最后，我们测试了设置为不同推理水平的推理模型（reasoning models），发现当模型分配更多推理令牌（reasoning tokens）时表现最强。这些结果提供了证据，表明最佳的大型语言模型可能具有完成依赖意象任务的能力，尽管其架构具有非图像性质。我们的研究不仅展示了大型语言模型在执行新任务时的一种新兴认知能力，还为该领域提供了一个新任务，为其他已经高度能力的模型留下了大量改进空间。最后，我们的发现重新点燃了关于人类视觉意象表征形式的争论，表明命题推理（propositional reasoning）（或至少是非意象推理）可能足以完成长期以来被认为依赖意象的任务。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Computation and Language", "count": 87, "papers": [{"index": "#1", "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents", "link": "/arxiv/2509.25189", "arxiv_id": "2509.25189", "authors": "Gongrui Zhang, Jialiang Zhu, Ruiqi Yang, Kai Qiu, Miaosen Zhang, Zhirong Wu, Qi Dai, Bei Liu, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Yuan Zhang, Xin Li, Zhaoyi Liu, Xin Geng, Baining Guo", "summary": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.384453", "filter_reason": "这篇论文的核心贡献是提出InfoAgent，一个通过创新的数据合成管道和协调的网络搜索工具驱动的深度研究智能体，旨在增强大语言模型的通用信息寻求和推理能力。论文符合我的研究目标，原因如下：1）论文本质上是关于改进LLM的基础能力，特别是通过工具使用和信息寻求来增强其推理能力；2）论文包含多个正面指标，如LLM、推理、规划、问题解决、强化学习、智能体、工具使用和深度研究等主题；3）论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面；4）论文提出的智能体/工具使用方法是通用的，旨在增强LLM的通用问题解决能力，而不是应用在特定领域。论文通过两阶段训练方法（监督微调和强化学习）显著提高了模型推理驱动的工具使用能力，这正是我所关注的提高LLM通用推理能力的研究方向。", "summary2": "本文旨在提高自主信息搜索代理的性能。针对复杂多步信息检索任务，我们提出了一种InfoAgent框架，结合创新的数据合成流程和自托管搜索工具，并在BrowseComp、BrowseComp-ZH和Xbench-DS等基准上通过准确率验证了其有效性。", "inspiration_trace": "# InfoAgent核心方法的逻辑演进分析\n\n## 一、宏观问题：深度研究智能体的能力瓶颈\n\n作者从互联网知识获取工具发展不均的宏观现象出发，观察到大型语言模型(LLM)通过工具交互扩展能力的趋势，特别是深度研究智能体(DRA)的潜力。DRA能自主规划、推理并执行多步骤信息获取行动，完成复杂研究任务，已成为下一代信息平台的定义特征。然而，作者发现现有DRA存在明显的能力瓶颈，无法充分发挥其潜力。\n\n## 二、问题识别：两大核心挑战\n\n通过分析现有研究，作者识别出实现高效DRA面临的两个关键挑战：\n\n1. **数据合成瓶颈**：现有开源DRA通常执行浅层搜索，主要因为它们在相对简单的数据上训练。这些数据缺乏足够的复杂性和多样性，无法迫使智能体进行长时程检索和连接推理。\n\n2. **交互环境限制**：许多智能体依赖商业搜索API或模拟环境，前者限制了透明度和可控性，后者在面对真实世界复杂问题时表现不佳。DRA的强化学习基础设施需要能处理高并发搜索调用并提供一致结果的工具。\n\n## 三、假设形成：突破瓶颈的可能路径\n\n基于上述问题，作者形成了两个核心假设：\n\n1. **数据复杂性假设**：如果能够构建更具挑战性的训练数据，系统性地增加问题难度，将迫使智能体执行长时程检索和连接推理，从而提高其能力上限。\n\n2. **环境透明度假设**：如果能够建立一个专用的、自托管的搜索基础设施，而非依赖商业搜索API，将提供对输出的细粒度控制和透明的实验环境，促进智能体能力的进一步发展。\n\n## 四、方法论形成：InfoAgent的核心创新\n\n基于这些假设，作者设计了InfoAgent的方法论，包含三个关键创新：\n\n### 1. 创新的数据合成流程\n\n作者设计了一个两阶段的流程来自动合成复杂的、多实体的搜索问题：\n\n- **实体树构建**：从维基百科实体集出发，构建实体树结构，每个节点包含实体名称、URL和提取的事实。通过命名实体识别识别子实体，递归扩展形成实体树。\n\n- **实体模糊化**：通过三阶段过程对事实进行模糊化处理：(1)实体替换（如\"Albert Einstein\"变为\"a famous physicist\"）；(2)数字/日期范围化（如\"1992\"变为\"early 1990s\"）；(3)语义重述。这使问题更难通过内部知识或直接搜索解决。\n\n- **子树采样与QA生成**：从实体森林中采样子树，为根节点生成关于其属性的问题，而非根节点本身，增加问题难度。使用高级模型(如o3)进行多轮测试，过滤出高难度且可解决的问题。\n\n### 2. 自托管搜索基础设施\n\n作者开发了专用的搜索和浏览工具，提供高质量、高吞吐量的信息访问：\n\n- **搜索功能**：不仅返回URL和快照，还通过爬虫获取完整网页内容，使用BM25、嵌入模型和重排序模型提取与查询最相关的文本块，最后用LLM生成简洁摘要。\n\n- **浏览功能**：给定URL，返回网页的语义文档，通过智能文本块选择过滤噪声内容，提供深度信息探索能力。\n\n这一设计解决了商业API的局限性，提供了透明、可控的实验环境。\n\n### 3. 两阶段训练策略\n\n作者采用两阶段方法对基础模型(Qwen3-14B)进行后训练：\n\n- **冷启动监督微调(SFT)**：在合成的长轨迹数据上进行微调，灌输长时程搜索行为，使模型学会规划、信息检索和回溯等复杂能力。\n\n- **强化学习(RL)**：采用GRPO(Generalized Relative Preference Optimization)进一步增强模型的推理驱动工具使用能力。选择适当难度的问题进行训练，提高效率和稳定性。\n\n## 五、验证与优化：实验驱动的迭代\n\n作者在多个深度研究基准上评估InfoAgent，包括BrowseComp、BrowseComp-ZH和Xbench-DS等。实验结果验证了方法论的有效性：\n\n1. InfoAgent(14B参数)在多个基准上超过了更大规模的开源模型(如WebSailor-72B和DeepDive-32B)。\n2. 消融实验证实了SFT冷启动的重要性、自托管工具相对于传统检索器的优势，以及长轨迹训练数据的价值。\n3. 工具调用分析显示，作者合成的数据集需要平均20.3次工具调用，显著高于现有工作(ASearcher: 5.4次，DeepDive: 9.5次)，证明了其更高的复杂性。\n\n## 六、逻辑演进总结\n\nInfoAgent的核心方法形成了一条清晰的逻辑链条：从深度研究智能体的能力瓶颈观察出发，识别数据合成和交互环境两大核心挑战，形成通过增加数据复杂度和提高环境透明度来突破瓶颈的假设，最终设计出创新的数据合成流程、自托管搜索基础设施和两阶段训练策略的完整方法论。这一过程体现了从问题识别到假设形成，再到方法论设计和实验验证的系统化创新思路，为自主信息获取智能体的发展提供了新的方向。", "summary_translation": "构建通过与外部工具交互来扩展能力的大语言模型（Large Language Model）代理代表了人工智能研究和应用的新前沿。在本文中，我们介绍了InfoAgent，这是一个由创新的数据合成（data synthesis）管道和协调的网络（orchestrated web）搜索工具驱动的深度研究（deep research）代理。为了构建具有挑战性且难以找到的查询，我们构建了实体树（entity trees）并应用带有实体模糊化（entity fuzzification）的子树采样（sub-tree sampling）来系统地增加问题难度。与先前严重依赖商业搜索工具的工作不同，我们开发了一个专用的自托管（self-hosted）搜索基础设施，增强了代理环境的透明度，并促进了代理能力的进一步发展。我们通过测量正确回答一个问题所需的平均工具调用（tool calls）次数来评估我们数据管道的有效性，并表明我们的代理在配备我们的工具时能产生更好的性能。我们的InfoAgent是从Qwen3-14B通过两阶段方法进行后训练（post-trained）的：冷启动（cold-start）监督微调以灌输长期视野（long-horizon）搜索行为，随后进行强化学习（reinforcement learning），显著提高了推理驱动（reasoning-driven）的工具使用。通过我们的方法，InfoAgent在BrowseComp上达到15.3%的准确率，在BrowseComp-ZH上达到29.2%，在Xbench-DS上达到40.4%，优于先前的开源深度研究代理，如WebSailor-72B和DeepDive-32B。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#14", "title": "An empirical study on the limitation of Transformers in program trace generation", "link": "/arxiv/2509.25073", "arxiv_id": "2509.25073", "authors": "Simeng Sun", "summary": "We study Transformers on the task \\emph{program trace generation} (PTG), where models produce step-by-step execution traces for synthetic programs. Unlike existing algorithmic problems, PTG externalizes reasoning through long traces where each step is trivial. We train small Transformers with diverse modifications, including alternative position encodings, softmax replacements, hybrid model, and short convolutions. While these models achieve strong in-distribution accuracy, they exhibit systematic failures when generalizing to various factors (e.g., program length, trace steps), though some designs significantly improve generalization.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.418559", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究课题。具体分析如下： 第一步：核心判断 这篇论文的本质是研究Transformer架构在程序跟踪生成(PTG)任务上的局限性及其改进方法。程序跟踪生成要求模型为合成程序生成逐步执行跟踪，这本质上是一种逻辑推理任务，需要模型理解程序逻辑并按照步骤执行推理。论文探讨了通过修改模型架构（如替代位置编码、softmax替换、混合模型和短卷积）来提高模型的泛化能力，这属于\"改进LLM的基础能力\"和\"增强其逻辑推理能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 论文涉及多个正面指标： - 能力方向：论文核心研究的是reasoning（推理），特别是logical reasoning（逻辑推理），因为程序跟踪生成本质上是一种逻辑推理过程。 - 虽然论文没有明确提到\"Large language models\"或\"LLMs\"，但研究的Transformer架构是LLM的基础，且程序跟踪生成是评估模型推理能力的重要任务。 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 虽然研究的是程序跟踪生成，但这更准确地说是一个评估模型推理能力的基准任务，而非针对特定领域（如医疗、化学等）的应用研究 - 没有涉及模型可靠性方面的水印、安全性等问题 第四步：特殊和模糊情况 论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。 综上所述，这篇论文通过研究Transformer在程序跟踪生成任务上的表现和改进方法，探索了如何增强模型的逻辑推理和泛化能力，符合提高大语言模型通用推理能力的研究目标。因此，我认为这篇论文应该被保留。", "summary2": "本文旨在研究Transformers在程序跟踪生成任务中的局限性。针对合成程序和输入数据，我们提出了多种Transformers架构修改（包括替代位置编码、softmax替换等），并在控制四个因素（程序长度、跟踪步骤、变量数量和输入大小）的数据集上通过whole-trace accuracy验证了其有效性。", "inspiration_trace": "# 从宏观问题到方法论：Transformers在程序跟踪生成中的局限性研究逻辑链\n\n## 1. 宏观问题：Transformers在程序执行任务中的能力边界\n\n作者研究的出发点是探索Transformers架构在程序执行和推理任务中的能力边界。作为现代大语言模型的基础架构，Transformers在多种任务上表现出色，但它们在需要精确规则遵循和长序列推理的任务中的能力仍不完全清楚，特别是在模拟计算机程序执行方面。\n\n## 2. 现有方法观察：算法任务的局限性\n\n作者观察到现有算法任务研究（如字符串反转、模加法、排列组合等）存在两个关键局限：\n- 这些任务要求模型在训练期间将固定算法内化到参数中\n- 任务复杂性与输入大小紧密耦合\n\n这种设计无法充分评估模型处理任意程序和长序列推理的能力，也无法区分模型是真正学会了算法还是仅仅记忆了输入-输出映射。\n\n## 3. 新任务设计：程序跟踪生成(PTG)\n\n为解决上述局限，作者提出程序跟踪生成(PTG)任务，其核心特点是：\n- 在上下文中明确提供算法，模型无需内化算法到参数\n- 输入和算法解耦，程序有独立于输入大小的复杂度\n\nPTG要求模型为给定程序和输入生成完整的逐步执行跟踪，将推理过程外部化为长而结构化的跟踪，迫使模型像计算机一样执行可靠顺序操作。\n\n## 4. 研究问题聚焦：从内部执行到外部跟踪\n\n基于PTG任务，作者将研究问题从：\n> \"Transformers能否在给定未见/更长输入的情况下内部执行固定程序？\"\n\n转变为：\n> \"Transformers能否在给定任意程序的情况下，在长序列上一致地执行简单操作？\"\n\n这一转变使研究更关注模型的上下文灵活性和执行任意程序的能力，而非仅处理固定程序和更大输入的能力。\n\n## 5. 控制变量设计：四个关键因素\n\n为精确评估程序执行能力，作者设计了四个关键因素进行控制：\n- **程序长度**：测试当\"指令内存\"增加时的模型行为\n- **跟踪步骤**：测试当\"数据内存\"增加时的模型行为\n- **变量数量**：测试处理更多变量的能力\n- **输入大小**：测试将算法应用于更大实体的能力\n\n这种设计使作者能系统评估Transformers在不同维度上的泛化能力。\n\n## 6. 假设形成：架构选择对泛化性能的影响\n\n基于对Transformers架构的理解，作者假设不同架构选择（如位置编码、注意力机制等）会对PTG任务的泛化性能产生显著影响，特别是某些修改可能更适合处理长序列和精确规则遵循。\n\n## 7. 实验设计：多样化架构修改\n\n为验证假设，作者比较了标准Transformers与多种修改版本：\n- 替代位置编码方法（NoPE, ALiBi, NaPE, Fox, PaTH）\n- Softmax替换（STB, α-entmax）\n- 短1D卷积（Canon）\n- 混合模型（SWAN）\n\n作者训练约154M参数的小型Transformers，在分布内数据上达到接近完美性能，然后评估分布外条件下的表现。\n\n## 8. 实验结果与分析\n\n实验结果显示：\n- 所有模型在分布内数据上实现了强准确性\n- 但在泛化到各种因素时表现出系统性失败\n- 架构选择对分布外性能有显著影响\n- NaPE（混合NoPE和ALiBi头部）平均优于其他修改\n\n特别是，作者发现可靠生成长跟踪仍具挑战性，模型在处理更复杂/更长的指令时表现不佳，处理更大实体大小的能力受位置编码严重影响。\n\n## 9. 结论与启示\n\n基于实验结果，作者得出结论：现有Transformers在逐步执行程序方面存在局限性，这对推理任务和精确指令遵循至关重要。某些架构选择（如NaPE）可显著改善泛化性能，为未来改进Transformers架构提供了方向。\n\n这一研究通过系统性的实验设计，揭示了Transformers在程序执行任务中的具体局限性，并为改进模型架构提供了实证基础。", "summary_translation": "我们研究了Transformers（变换器）在\\emph{program trace generation}（程序轨迹生成，PTG）任务上的表现，该任务要求模型为合成程序（synthetic programs）生成逐步的执行轨迹（execution traces）。与现有的算法问题（algorithmic problems）不同，PTG通过长轨迹将推理过程（reasoning）外化（externalizes），其中每个步骤都是微不足道的。我们训练了经过多样化修改的小型Transformers，包括替代位置编码（alternative position encodings）、softmax替代函数（softmax replacements）、混合模型（hybrid model）和短卷积（short convolutions）。尽管这些模型在分布内准确率（in-distribution accuracy）方面表现出色，但在向各种因素（例如程序长度、轨迹步骤）泛化（generalizing）时表现出系统性失效，不过某些设计显著改善了泛化能力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#13", "title": "Scaling Generalist Data-Analytic Agents", "link": "/arxiv/2509.25084", "arxiv_id": "2509.25084", "authors": "Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen", "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.", "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.418132", "filter_reason": "这篇论文的核心贡献是提出DataMind，一种可扩展的数据合成和智能体训练方法，用于构建通用的数据分析智能体。论文主要关注增强LLM在数据分析领域的通用推理能力，特别是处理多格式、大规模数据文件和长时程、多步推理的能力。根据筛选标准，该论文符合研究目标，原因如下： 1. 核心判断：论文的本质是改进LLM的基础能力和提出新的训练范式，特别是增强其多步推理能力。DataMind方法结合了SFT和RL的训练目标，旨在提升模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 2. 正面指标：论文包含了所有关键正面指标： - 核心概念：训练了DataMind-14B和DataMind-7B等LLM模型 - 能力方向：明确关注\"long-horizon, multi-step reasoning\"（长时程、多步推理） - 训练方法：使用了结合SFT和RL损失的动态调整训练目标 - 新兴范式：核心是构建\"generalist data-analytic agents\"（通用数据分析智能体） 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）。 4. 特殊情况处理：虽然论文关注数据分析，但它提出的是一种通用的智能体训练框架，而不是针对特定领域的应用。论文的核心是增强LLM的通用推理能力，这与研究目标高度一致。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，特别是在数据分析领域的多步推理能力，完全符合研究范围。", "summary2": "本文旨在构建开源通用数据分析智能体，解决现有模型在处理多样化格式、大规模数据文件和长周期多步推理方面的不足。针对数据分析任务场景，我们提出了一种DATA MIND可扩展数据合成和智能体训练方法，结合细粒度任务分类、知识增强轨迹采样、动态SFT-RL训练目标及稳定代码执行框架，并在多个数据分析基准上通过准确率指标验证了其有效性，DATA MIND-14B达到71.16%平均得分，超越了专有模型DeepSeek-V3.1和GPT-5。", "inspiration_trace": "# DATA MIND方法论逻辑链推演\n\n## 1. 宏观问题：开源数据分析智能体的局限性\n\n**观察**：当前数据分析智能体领域存在明显的不平衡。专有模型通过提示工程或多智能体框架在数据分析任务上表现良好，但开源模型在处理现实世界数据分析需求时存在显著差距。\n\n**核心问题**：开源模型难以应对多样化格式、大规模数据文件和长视野、多步推理，而这些正是实际数据分析场景的核心需求。\n\n## 2. 问题分解：三大关键挑战\n\n### 挑战一：数据资源不足\n**观察**：训练高质量数据分析智能体需要大规模、多样化的任务及解决方案轨迹，但现有公开基准仅提供有限的测试集，缺乏详细的逐步轨迹注释。\n\n**问题本质**：开源社区缺乏可用于训练的高质量数据分析任务轨迹数据集，无法支撑通用数据分析智能体的训练需求。\n\n### 挑战二：训练策略不当\n**观察**：现有智能体训练遵循\"SFT-then-RL\"范式，但在数据分析这一新场景中，如何稳定长视野训练以及平衡SFT与RL的权重分配尚不明确。\n\n**问题本质**：缺乏针对数据分析智能体特点的优化训练策略，导致训练不稳定或效果不佳。\n\n### 挑战三：不稳定的基于代码的多轮展开\n**观察**：数据分析涉及大量代码执行和文件I/O操作，在有限内存资源下，并行智能体展开和多轮代码生成容易导致环境崩溃。\n\n**问题本质**：缺乏内存友好且稳定的代码执行环境，阻碍了大规模训练和部署。\n\n## 3. 假设形成：解决方案的初步构想\n\n### 假设一：数据合成假设\n**假设**：如果能设计一个自动化流程，生成多样化、高质量的数据分析任务及解决方案轨迹，就能解决训练数据不足的问题。\n\n**推论**：通过细粒度任务分类和递归任务组合机制，可以合成覆盖广泛领域和难度梯度的数据分析任务。\n\n### 假设二：动态训练假设\n**假设**：如果能动态调整SFT与RL的权重比例，就能在训练初期充分利用专家知识，后期鼓励模型探索，从而稳定训练并提升性能。\n\n**推论**：SFT可以作为训练的稳定器，而RL则有助于模型发现新的推理模式，两者动态结合能达到最佳效果。\n\n### 假设三：稳定执行假设\n**假设**：如果能优化代码执行环境，实现异步交互和内存高效管理，就能解决多轮代码展开的稳定性问题。\n\n**推论**：通过分块代码维护和严格的环境隔离，可以在有限资源下实现大规模并行训练。\n\n## 4. 方法设计：DATA MIND的构建\n\n### 组件一：数据合成与查询生成\n**实现路径**：\n1. 构建细粒度任务分类法（18个类别），确保查询多样性\n2. 设计递归的从易到难任务组合机制，通过链接多个任务类型逐步提升难度\n3. 从互联网和开放社区收集多样化数据文件，并进行严格筛选\n\n**创新点**：通过任务分类和递归组合，系统性地增加了合成查询的多样性和难度，覆盖了现实世界数据分析的复杂需求。\n\n### 组件二：专家轨迹采样与过滤\n**实现路径**：\n1. 引入知识增强的轨迹采样策略，使用高级工作流引导模型生成轨迹\n2. 采用自洽性过滤，通过多个独立轨迹的一致性验证确保答案质量\n3. 应用基于规则的过滤（格式合规性、长度控制、语言完整性）\n\n**创新点**：自洽性过滤确保了轨迹质量，而反馈机制使模型能够从失败中学习，不断改进推理路径。\n\n### 组件三：动态训练目标\n**实现路径**：\n1. 结合SFT损失和RL损失，使用动态系数γ调度两者权重\n2. 设计无效轮次过滤机制，屏蔽包含无效步骤的轨迹\n3. 实现冷启动策略，为RL训练提供良好初始状态\n\n**创新点**：动态平衡SFT与RL，在训练初期侧重知识吸收，后期鼓励探索，解决了传统训练策略的稳定性问题。\n\n### 组件四：内存友好的多轮展开框架\n**实现路径**：\n1. 异步交互：解耦模型生成和代码执行，避免资源争用\n2. 分块代码维护：仅生成当前步骤所需代码，减少内存占用\n3. 安全控制：隔离执行环境，限制资源使用，确保稳定性\n\n**创新点**：通过异步交互和分块维护，在有限资源下实现了高效稳定的多轮代码执行。\n\n## 5. 实验验证：假设检验与洞察发现\n\n### 验证一：数据合成效果\n**结果**：成功构建DATA MIND-12K高质量轨迹集，覆盖多样化任务类别和数据格式。\n**结论**：数据合成假设得到验证，自动化生成的数据集质量足以支持高性能智能体训练。\n\n### 验证二：动态训练策略效果\n**结果**：DATA MIND-14B在多个基准上超越专有模型，平均得分71.16%。\n**结论**：动态训练假设得到验证，SFT与RL的动态平衡确实能提升训练稳定性和模型性能。\n\n### 验证三：稳定执行框架效果\n**结果**：成功实现大规模并行训练，环境崩溃率显著降低。\n**结论**：稳定执行假设得到验证，内存友好的多轮展开框架确实能解决代码执行稳定性问题。\n\n### 关键洞察\n1. **自洽性过滤比最佳轨迹选择更重要**：轨迹多样性比单一\"最佳\"路径更能提升模型推理能力。\n2. **SFT损失的双面性**：既是RL训练的稳定器，也可能是过度拟合的源头。\n3. **RL的局限性**：能缩小不同基础模型间的差距，但难以逆转其固有能力排序。\n\n## 6. 结论：从问题到解决方案的完整逻辑链\n\nDATA MIND的提出源于对开源数据分析智能体局限性的观察，通过分解为三大核心挑战（数据资源不足、训练策略不当、执行环境不稳定），形成三个关键假设（数据合成假设、动态训练假设、稳定执行假设），最终设计出四个相互支撑的解决方案组件（数据合成与查询生成、专家轨迹采样与过滤、动态训练目标、内存友好的多轮展开框架）。\n\n这一逻辑链展现了从宏观问题到具体解决方案的系统思考过程，不仅解决了开源数据分析智能体的关键问题，还为社区提供了可复现的训练框架和有价值的经验洞察。DATA MIND的成功验证了\"数据+算法+工程\"协同创新的强大力量，为构建更通用的AI智能体提供了重要参考。", "summary_translation": "数据分析代理（Data-analytic agents，指能够自动进行数据分析的智能系统）正成为自动化科学发现和创新人工智能（Innovating AI，指具有创新能力的人工智能）愿景的关键催化剂。然而，当前方法严重依赖于专有模型（proprietary models，指闭源的商业化模型）的提示工程（prompt engineering，指设计输入提示以优化模型输出的技术），而开源模型（open-source models）则难以应对现实世界分析所需的多格式、大规模数据文件以及长期（long-horizon，指需要长时间规划的任务）多步推理（multi-step reasoning）。\n\n本文介绍了DataMind，一种可扩展的数据合成（data synthesis）和代理训练方法（recipe），旨在构建通用型（generalist）数据分析代理。DataMind解决了构建开源数据分析代理的三个关键挑战，包括数据资源不足、训练策略不当以及基于代码的多轮交互（multi-turn rollout，指代理与环境的多次交互过程）不稳定。具体而言，DataMind采用了：1) 细粒度任务分类（fine-grained task taxonomy）和递归的由易到难任务组合机制，以增加合成查询的多样性和难度；2) 知识增强的轨迹采样（trajectory sampling）策略，随后进行基于模型和基于规则的过滤；3) 结合SFT（Supervised Fine-Tuning，监督微调）和RL（Reinforcement Learning，强化学习）损失的动态可调整训练目标；4) 内存节约且稳定的基于代码的多轮交互框架。\n\n基于DataMind，我们精心策划了DataMind-12K，这是一个涵盖多个领域、任务类别和数据文件格式的高质量轨迹集（trajectory set，指代理执行任务的过程记录）。在DataMind-12K上训练后，我们的DataMind-14B模型在多个数据分析基准测试上以71.16%的平均分达到了最先进水平（state-of-the-art），超越了最强的专有基线模型DeepSeek-V3.1和GPT-5。我们的DataMind-7B模型也以68.10%的得分在所有开源模型中表现最佳。我们还将探索性试验中获得的一些经验见解（empirical insights）纳入分析实验，旨在为社区提供关于代理训练（agentic training，指训练具有代理能力的模型）的可操作见解。我们将向社区发布DataMind-12K和DataMind-7B、14B模型，以支持未来的研究。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#23", "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes", "link": "/arxiv/2509.24945", "arxiv_id": "2509.24945", "authors": "Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra", "summary": "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.428393", "filter_reason": "这篇论文的核心贡献是探索并挑战了关于语言模型推理能力的两个假设：(1)推理能力只在足够大的模型中出现，(2)这种能力需要在大规模数据集上训练。论文通过精心策划和重采样数据集，证明了可以用更少的数据（约2T tokens）在小型语言模型（sub-billion-parameter）上实现强大的推理能力。这完全符合研究目标，因为论文的本质是改进LLM的基础推理能力，提出新的训练范式（数据策划和重采样），增强其通用推理能力。论文在多个推理基准上测试了模型性能，如AIME数学推理测试，证明其方法的有效性。论文没有将LLM作为工具应用于特定领域，也没有关注多模态、特定应用或模型可靠性等排除标准中的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在探索sub-billion参数语言模型的推理能力极限。针对大规模数据训练的假设，我们提出了一种benchmark-free、自我进化的数据优化方法，通过精心策划和重采样约2T tokens的高质量开源数据，开发了MobileLLM-R1系列模型。在GSM8K、HumanEval、MATH、AIME和LCBv6等多个推理benchmark上，通过accuracy和pass@1等指标验证了其有效性，MobileLLM-R1-950M在AIME上取得15.5分，远超同类开源模型，且仅用Qwen3训练数据的11.7%就达到相当性能。", "inspiration_trace": "# 从宏观问题到方法论：MobileLLM-R1的逻辑演进\n\n## 宏观问题：如何在资源受限设备上实现强大的语言模型推理能力？\n\n### 观察与背景分析\n- **现实需求**：大型语言模型(LLMs)已展示显著推理能力，但给资源受限设备带来压力；未来个人助手、智能家居和机器人将依赖设备上推理处理复杂任务。\n- **技术瓶颈**：长上下文推理加剧内存使用，KV缓存增长急剧增加内存占用。\n- **主流假设**：(1)推理能力只在足够大模型中出现；(2)这种能力需要在大规模数据集(>10T tokens)上训练。\n\n### 挑战主流假设\n- **假设1已被部分挑战**：Qwen3-0.6B和DeepSeek蒸馏变体等亚十亿参数推理模型表明小模型也能具备推理能力。\n- **假设2仍存疑**：推理能力是否真的需要极大规模数据集？这一问题尚未得到充分探索。\n\n### 核心研究问题聚焦\n在严格容量限制下，如何最有效地赋予小型推理模型强大能力并释放其隐藏潜力？更基础地，如何首先赋予预训练模型推理的潜在能力？\n\n### 关键观察与洞见\n1. **小模型的敏感性**：小型语言模型对数据质量极为敏感，噪声容易淹没其有限容量。\n2. **知识编码冲突**：模型缩小时，神经元必须编码更多重叠知识，增加干扰和冲突风险。\n3. **推理能力本质**：推理能力出现可理解为标记概率空间的系统性转变——从一般用途LLM到推理专门化模型的转变。\n\n### 形成假设\n1. **假设1**：通过精心策划的高质量数据，可以在小规模模型中培养强大推理能力。\n2. **假设2**：不需要极大规模数据集，较少的高质量数据(约2T tokens)足以实现强大推理能力。\n3. **假设3**：数据策划和混合策略对小模型推理能力培养至关重要。\n\n### 方法论形成\n\n#### 1. 训练课程设计\n借鉴教育心理学原理，设计完整训练课程，从一般知识到专业推理能力逐步引导模型：\n\n- **预训练阶段**：接触多样化语料库，建立语言和世界知识基础，同时获得基本数学和推理能力。\n- **中期训练阶段**：战略性地将数据分布转向推理丰富领域(数学、编码、结构化问题解决)。\n- **后训练阶段**：通过监督微调(SFT)使模型与人类偏好行为保持一致。\n\n#### 2. 数据策划与优化\n- **无基准自我进化数据优化**：利用跨领域影响定制数据混合，无需暴露基准数据。\n- **数据-模型共同进化策略**：适应中期训练过程中模型容量的快速变化。\n- **分层拒绝采样**：结合FineWeb-Edu分类器和Ask-LLM范式，构建代表性数据子集。\n- **影响力分析**：使用留一法(LOO)评估数据集贡献，利用影响力分数指导标记重新加权。\n\n#### 3. 影响力计算与应用\n- **自我影响力与交叉影响力**：计算训练样本对验证损失的影响，量化与目标能力的连接强度。\n- **数据集级别加权**：根据影响力分数为每个数据集分配采样权重，优先选择对多种能力有积极贡献的数据集。\n- **迭代压缩**：在中期训练中动态移除负影响力样本，调整数据采样比例，直到大多数样本影响力为零或负值。\n\n### 实验验证与发现\n- **模型规模**：在140M、360M、950M参数模型上验证方法。\n- **比较基准**：与完全开源模型(OLMo、SmolLM)和部分开源模型(Qwen、Gemma、LLaMA)比较。\n- **关键发现**：\n  - MobileLLM-R1-950M仅使用Qwen3的11.7%训练token，匹配或超过Qwen3-0.6B在多个推理基准的表现。\n  - 在MATH准确性上比Olmo 1.24B高5倍，比SmolLM2 1.7B高2倍。\n  - 证明了数据质量和策划比数据规模更重要。\n\n### 结论与启示\n提出了以数据为中心的框架，在有限参数和标记情况下最大化小型语言模型推理能力。通过数据质量、标记效率和原则性数据策划，成功在小模型中实现强大推理能力，挑战了推理能力需要极大数据集的主流假设，为未来研究提供了完全开源的训练方案。", "summary_translation": "大型语言模型(large language models, LLMs)从本能反应到思维链(chain-of-thought, CoT)推理的范式转变，助长了两个普遍假设：(1)推理能力仅在足够大的模型中才会出现，(2)这种能力需要在海量数据集上进行训练。尽管第一个假设已经被最近的十亿参数级以下的推理模型所挑战，如Qwen3-0.6B和DeepSeek蒸馏变体，但第二个假设在很大程度上仍未受到质疑。在这项工作中，我们重新审视了推理能力出现所需扩展到极大规模语料库(>10T tokens)的必要性。通过精心筛选和重新采样我们根据设计的指标确定为有益的开源数据集，我们证明了强大的推理能力可以用少得多的数据出现。具体而言，我们表明仅约2T tokens的高质量数据就足够了，并且在这约2T tokens重新采样的数据集上进行4.2T tokens的预训练，随后采用既定的后训练(post-training)程序，使得MobileLLM-R1的开发成为可能，这是一系列十亿参数级以下的推理模型，其性能显著优于在完全开源数据上训练的先前模型。例如，MobileLLM-R1-950M在AIME评分上达到15.5，而OLMo-2-1.48B仅为0.6，SmolLM-2-1.7B仅为0.3。值得注意的是，尽管与Qwen3专有的36T-token预训练语料库相比，MobileLLM-R1-950M仅使用了11.7%的tokens进行训练，但它在多个推理基准测试(benchmarks)中匹配或超越了Qwen3-0.6B。为了促进这一方向的进一步研究，我们已经发布了完整的训练方案、数据来源、数据混合比例和模型检查点(model checkpoints)，以及在整个研究过程中获得的关键见解。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#19", "title": "Circuit Distillation", "link": "/arxiv/2509.25002", "arxiv_id": "2509.25002", "authors": "Somin Wadhwa, Silvio Amir, Byron C. Wallace", "summary": "Model distillation typically focuses on behavioral mimicry, where a student model is trained to replicate a teacher's output while treating its internal computations as a black box. In this work we propose an alternative approach: Distilling the underlying computational mechanisms implemented by a teacher model. Specifically, we propose circuit distillation, which introduces an objective to align internal representations between analogous circuit components in teacher and student models. We propose a method to match ``functionally correspondent'' circuit components and introduce a loss reflecting similarities between the representations that these induce. We evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks using models from the Llama3 family. Our results demonstrate that circuit distillation outperforms standard distillation, successfully transferring algorithmic capabilities by adjusting only a small, targeted subset of student model parameters. This work establishes the feasibility of transferring mechanisms, which may in turn allow for efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.426307", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的模型蒸馏方法——\"电路蒸馏\"(Circuit Distillation)，其核心目标是转移模型内部的计算机制而非仅仅模仿输出行为。这属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，因为它关注如何通过理解和转移模型内部的计算机制来提升模型能力，特别是算法能力的转移。 其次，从正面指标分析，论文明确使用了Llama3模型家族进行评估，包含了\"Large language models, LLMs\"这一核心概念。同时，论文在实体跟踪(entity tracking)和心理理论(ToM)任务上进行了评估，这两个任务都与推理能力密切相关，尤其是心理理论(ToM)涉及高级认知推理能力，属于\"reasoning\"能力方向。 第三，论文不涉及任何排除标准中的领域。它没有关注多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。 最后，在特殊和模糊情况处理上，论文提到了\"interpretable and controllable internal student mechanisms\"，这涉及到模型的可解释性，属于通过增强模型内在可解释性来提升模型通用推理质量的情况，符合保留标准。 论文的核心贡献是提出了一种新的蒸馏方法，通过匹配和转移模型内部的\"电路\"组件，成功实现了算法能力的转移，这种方法有望通过可解释和可控的内部机制来高效蒸馏目标能力，这与提升大语言模型的通用推理能力直接相关。", "summary2": "本文旨在解决传统模型蒸馏仅关注行为模仿而忽视内部计算机制转移的问题。针对实体跟踪和心理理论任务，我们提出了一种电路蒸馏方法，通过匹配功能对应的电路组件并引入表示相似性损失函数来对齐教师和学生模型的内部计算机制，并在Llama3系列模型上通过任务准确率和电路性能指标验证了其有效性。", "inspiration_trace": "# Circuit Distillation 的逻辑演进：从宏观问题到方法论创新\n\n## 1. 宏观问题：模型蒸馏的根本局限\n\n模型蒸馏旨在将大型教师模型的能力转移到小型学生模型中，以提高效率。然而，传统蒸馏方法存在一个根本性瓶颈：学生模型只能从教师输出中学习，必须自行推断如何执行任务。这种\"行为模仿\"范式限制了知识转移的深度和效率。\n\n## 2. 关键观察：内部机制的缺失\n\n作者观察到传统蒸馏方法忽视了教师模型的内部计算机制。尽管这些方法能有效转移任务特定知识，但无法传递教师模型内部的算法实现。这引发了一个核心问题：如果能让学生学习教师模型的内部算法，而非仅仅模仿输出，是否能实现更有效的知识转移？\n\n## 3. 洞察与假设：机械可解释性的启示\n\n机械可解释性领域的最新进展揭示了Transformer模型内存在人类可理解的算法子图，称为\"电路\"(circuits)。这些电路是模型实现特定功能的可识别组件。基于此，作者提出了核心假设：\n\n**通过在组件级别强制功能对齐，不仅可以蒸馏教师的知识，还可以蒸馏其内部算法。**\n\n如果学生模型能够模仿教师模型中特定电路的计算机制，那么它可能会更有效地学习任务，并且可能只需要调整更少的参数。\n\n## 4. 技术挑战与解决路径\n\n要实现这一假设，必须解决两个关键技术挑战：\n\n### 挑战1：不同规模模型间的组件对应关系\n\n如何在不同大小的教师和学生模型之间建立功能上对应的组件关系？\n\n**解决方案：基于消融影响的映射策略**\n- 测量每个组件(注意力头)移除时对任务性能的影响\n- 将具有相似功能重要性的组件进行映射\n- 这种基于功能而非结构的方法适用于不同规模的模型\n\n具体实现：\n- 计算学生头hs的消融影响：ΔPs(hs) = Ps_base - Ps_abl(hs)\n- 计算教师头ht的消融影响：ΔPt(ht) = Ps_base - Pt_abl(ht)\n- 映射最小化|ΔPs(hs) - ΔPt(ht)|的头对\n\n### 挑战2：表示对齐的度量与优化\n\n如何设计目标函数来强制对应电路之间的表示对齐？\n\n**解决方案：中心核对齐(CKA)损失**\n- CKA对正交变换和各向同性缩放不变，适合比较不同架构的表示\n- 通过测量表示诱导的相似性模式来比较功能相似性\n- 将CKA转化为损失项：L_CKA(Ks, Kt) = 1 - CKA(Ks, Kt)\n\n## 5. 方法论形成：电路蒸馏框架\n\n结合上述解决方案，作者提出了\"电路蒸馏\"的完整框架：\n\n1. **识别目标电路**：利用已有的机械可解释性方法(如路径补丁)识别教师模型中与任务相关的电路组件\n\n2. **建立组件映射**：使用消融影响相似性将学生组件映射到教师组件\n\n3. **复合损失函数**：\n   L_total = L_task(y, ŷ_s) + λ Σ L_CKA(K_s^(c), K_t^(c))\n   \n   其中：\n   - L_task是传统任务损失\n   - L_CKA是CKA-based电路相似性损失\n   - λ平衡任务性能和机械对齐目标\n\n4. **参数高效训练**：仅调整映射到的电路组件参数，而非整个模型\n\n## 6. 实验验证与效果确认\n\n作者在两个认知任务上验证了电路蒸馏的有效性：\n\n1. **实体追踪任务**：测试模型追踪和更新实体信息的能力\n2. **心理理论(ToM)任务**：测试模型在部分可观察条件下对信念进行推理的能力\n\n实验结果表明：\n- 电路蒸馏优于标准蒸馏方法\n- 通过仅调整11-15%的注意力头参数，成功转移了算法能力\n- 电路蒸馏不仅提高了性能，还提供了效率增益和收敛速度提升\n\n## 7. 理论贡献与实践意义\n\n电路蒸馏的贡献在于：\n\n1. **概念创新**：将蒸馏焦点从输出行为转向内部机制\n2. **方法创新**：提供了组件功能映射和表示对齐的具体方法\n3. **效率提升**：实现了参数高效的知识转移\n4. **可解释性增强**：产生内部计算更可控的学生模型\n\n这一工作为构建更高效、更可解释的模型提供了新思路，证明了不仅知识可以蒸馏，产生知识的算法也可以蒸馏。", "summary_translation": "模型蒸馏（model distillation）通常专注于行为模仿（behavioral mimicry），即训练学生模型（student model）复制教师模型（teacher model）的输出，同时将其内部计算视为黑盒（black box）。在本研究中，我们提出了一种替代方法：蒸馏教师模型所实现的基础计算机制。具体而言，我们提出了电路蒸馏（circuit distillation），该方法引入了一个目标，用于对齐教师模型和学生模型中类似电路组件（circuit components）之间的内部表征。我们提出了一种匹配\"功能对应\"（functionally correspondent）电路组件的方法，并引入了一种损失函数（loss），反映这些组件所诱导的表征之间的相似性。我们使用Llama3系列的模型，在实体跟踪（entity tracking）和心理理论（theory of mind, ToM）任务上评估了电路蒸馏的效果。我们的结果表明，电路蒸馏优于标准蒸馏，通过仅调整学生模型参数中的一小部分目标子集，就能成功地转移算法能力。这项工作证明了转移机制的可行性，这反过来可能通过可解释和可控的学生模型内部机制，实现针对教师模型特定能力的高效蒸馏。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#26", "title": "Expanding Computation Spaces of LLMs at Inference Time", "link": "/arxiv/2509.24884", "arxiv_id": "2509.24884", "authors": "Yoonna Jang, Kisu Yang, Isabelle Augenstein", "summary": "Chain-of-thought (CoT) rationale enables language models to use additional task-related text for problem-solving, benefiting not only from detailed reasoning steps but also from the expanded computational space of longer inputs. Prior work has trained filler or special tokens to serve as additional computation spaces. In this study, we investigate whether language models can leverage artificially inserted sequences of filler tokens solely at inference. We first identify effective token types, numbers, and insertion locations, then examine at what stage of training models begin to exploit the expanded computation space, and finally analyze dynamics within these spaces via attention maps. Experiments on models ranging from 1.7B to 32B across open-domain QA and math tasks show that appropriate token types and counts vary, but placing filler tokens directly before the final 'Answer:' token is most effective. Smaller models benefit most, up to 12.372 percentage points in SmolLM2-1.7B-Instruct, indicating that these spaces act as additional computational capacity rather than redundant input. Attention maps reveal that expanded spaces often continue the original attention mechanism and sometimes focus on questions or answer options, suggesting meaningful computation for problem-solving.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.435162", "filter_reason": "这篇论文完全符合研究目标，其核心是改进大语言模型的基础推理能力。论文提出在推理时通过插入填充令牌来扩展LLM的计算空间，这是一种增强模型通用问题解决能力的新方法。从核心判断来看，论文本质上是研究如何通过扩展计算空间来提升LLM的推理能力，类似于思维链(CoT)的扩展，属于改进LLM基础能力的研究。从正面指标看，论文明确涉及LLMs核心概念，并在开放域问答和数学任务上验证了推理能力提升。论文不符合任何排除标准，它不关注多模态、特定应用领域或模型可靠性等排除领域。虽然论文没有涉及智能体、工具使用等新兴范式，但其核心贡献——通过扩展推理时的计算空间来增强LLM的通用推理能力——直接与研究目标\"提高大语言模型的通用推理能力\"高度一致。实验表明这种方法特别有利于提升小模型的推理能力，进一步证明其通用价值。", "summary2": "本文旨在探索语言模型在推理时如何利用额外计算空间提升性能。针对多种规模的LLMs，我们提出了一种在输入中插入填充标记（如空格、句号等）扩展计算空间的方法，并在MMLU、ARC、GSM8K和MATH-500数据集上通过准确率和精确匹配率验证了其有效性。实验表明，该方法特别使小型模型（如SmolLM2-1.7B）性能提升达12.372个百分点，且无需额外训练。", "inspiration_trace": "# 从思维链到计算空间扩展：LLM推理能力提升的逻辑推演\n\n## 一、宏观问题起点：思维链有效性的本质探究\n\n**观察现象**：思维链(CoT)提示显著提升了语言模型的推理性能，使模型能够逐步分解问题并显式化推理轨迹。\n\n**核心疑问**：CoT的有效性究竟来源于什么？是详细的推理步骤内容，还是仅仅是更长的输入提供了更大的计算空间？\n\n**前期研究基础**：已有工作通过训练模型使用特殊标记或填充字符作为额外计算空间，证实了扩展空间的价值。但这些方法都需要在预训练或微调阶段进行专门训练。\n\n## 二、关键假设形成：无需训练的计算空间扩展\n\n**假设提出**：当前的语言模型可能能够在无需专门训练的情况下，仅通过在推理时插入填充标记来利用扩展的计算空间。\n\n**假设依据**：\n1. 模型在训练过程中已经接触过各种常见标记（如句号、空格等）\n2. 初步研究表明，即使没有特定形式的训练，提供额外标记作为扩展计算空间也能增强模型性能\n\n**研究聚焦**：将宏观问题具体化为三个研究问题：\n- RQ1: 什么类型和数量的标记有效，插入在何处最有效？\n- RQ2: 模型在训练的哪个阶段开始利用扩展计算空间？\n- RQ3: 扩展的标记空间如何与原始输入交互并影响答案预测？\n\n## 三、方法论构建：系统探索计算空间扩展\n\n### 1. 填充标记选择\n**逻辑**：选择模型在训练语料中自然见过的标记，确保模型熟悉这些标记\n**实施**：选择六种字符作为填充标记：空格(' ')、换行符('\\n')、制表符('\\t')、句号('.')、<pad>标记、破折号('-')\n\n### 2. 实验设计\n**变量控制**：\n- **数量维度**：从16到8192个填充标记，探索\"多少是足够，多少是过多\"\n- **位置维度**：在\"Answer:\"标记前后插入，确定最佳位置\n- **模型维度**：选择1.7B到32B参数范围的模型，探索模型大小的影响\n- **任务维度**：选择开放域问答(MMLU、ARC)和数学任务(GSM8K、MATH-500)\n\n### 3. 分析方法\n**多层次验证**：\n- **性能评估**：测量准确率变化，量化效果\n- **训练阶段分析**：通过中间检查点追踪能力发展\n- **注意力机制分析**：通过注意力图揭示内部计算过程\n\n## 四、实验发现与理论完善\n\n### 1. 基础发现\n**现象确认**：模型确实可以利用填充标记作为扩展计算空间，无需明确训练\n**关键证据**：SmolLM2-1.7B-Instruct模型性能提升高达12.372个百分点\n\n### 2. 优化条件发现\n**标记类型与数量**：\n- 不同模型有不同最优标记类型（如SmolLM在句号上表现最佳）\n- 小模型能从更多标记中受益（最多256个），而大模型受益较少\n- 超过1024个标记会导致性能下降，出现\"lost-in-the-middle\"现象\n\n**插入位置**：\n- 将填充标记直接放在\"Answer:\"标记前效果最佳\n- 放在\"Answer:\"后会导致模型预测更多填充标记而非答案\n\n### 3. 机制解释\n**计算空间本质**：\n- 扩展空间不是冗余输入，而是提供额外计算容量\n- 小模型受益更多，表明这些空间补偿了其有限的参数容量\n\n**注意力模式分析**：\n- 扩展空间继续原始注意力机制\n- 填充标记有时直接关注问题或答案选项，参与有意义计算\n- 在中间层，填充标记参与解释问题和选项；在后期层，影响答案选择\n\n### 4. 能力发展轨迹\n**训练阶段分析**：\n- 模型在预训练后期逐渐获得利用扩展空间的能力\n- 指令微调阶段进一步增强了这种能力\n- 表明这是一种自然出现的能力，不需要专门训练\n\n## 五、理论贡献与实践意义\n\n### 1. 核心理论贡献\n**计算空间扩展理论**：证明了语言模型可以利用推理时插入的填充标记作为额外计算空间，这种空间不是简单冗余，而是进行有意义计算的容量。\n\n**能力发展理论**：模型在预训练过程中自然获得利用扩展计算空间的能力，无需专门训练。\n\n### 2. 实践意义\n**小模型增强策略**：为资源受限的小模型提供了一种简单有效的性能提升方法\n**推理优化思路**：开辟了通过扩展计算空间而非仅增加参数来提升模型性能的新途径\n**模型理解深化**：通过注意力分析，揭示了模型内部计算过程的更多细节\n\n这一研究从对思维链有效性的本质探究出发，通过假设无需训练即可扩展计算空间，系统设计实验验证，最终形成了完整的理论框架，为理解和提升语言模型推理能力提供了新视角。", "summary_translation": "思维链（Chain-of-thought, CoT）原理使语言模型能够使用额外的任务相关文本来解决问题，不仅受益于详细的推理步骤，还受益于更长输入所带来的扩展计算空间。先前的研究已经训练了填充符（filler）或特殊标记（tokens）作为额外的计算空间。在本研究中，我们探讨了语言模型是否能够仅在推理阶段利用人工插入的填充符序列。我们首先确定了有效的标记类型、数量和插入位置，然后检查模型在训练的哪个阶段开始利用扩展的计算空间，最后通过注意力图（attention maps）分析这些空间内的动态变化。在开放域问答（open-domain QA）和数学任务上，对从1.7B到32B规模模型的实验表明，适当的标记类型和数量各不相同，但将填充符直接放在最终的'Answer:'标记前最为有效。较小的模型获益最大，在SmolLM2-1.7B-Instruct模型中提升高达12.372个百分点，表明这些空间充当的是额外的计算能力，而非冗余输入。注意力图（attention maps）显示，扩展的空间通常会延续原始的注意力机制，有时会聚焦于问题或答案选项，这表明这些空间进行了有意义的问题解决计算。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#35", "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space", "link": "/arxiv/2509.24771", "arxiv_id": "2509.24771", "authors": "Guibin Zhang, Fanci Meng, Guancheng Wan, Zherui Li, Kun Wang, Zhenfei Yin, Lei Bai, Shuicheng Yan", "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the reasoning capabilities of Large Language Models (LLMs) during the inference phase without altering model parameters. However, existing TTS methods are largely independent, implying that LLMs have not yet evolved to progressively learn how to scale more effectively. With the objective of evolving LLMs to learn ``how to scale test-time computation,'' we propose LatentEvolve, a self-evolving latent TTS framework inspired by the complementary learning system (CLS) theory. Analogous to the human brain's dual system of a fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve comprises two evolutionary components: \\textit{daytime scaling}, which rapidly retrieves historical latent representations to better guide current LLM reasoning; and \\textit{nighttime scaling}, which integrates past latent optimizations in a manner akin to the human brain's consolidation of experiences during sleep. The alternation of daytime and nighttime processes facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive dynamics in a fully unsupervised manner. Extensive experiments across eight benchmarks and five model backbones demonstrate that our LatentEvolve surpasses state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and exhibits exceptional cross-domain and cross-backbone generalization.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.445233", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。从核心判断来看，论文的本质是提出LatentEvolve，一种自我进化的潜在测试时扩展(TTS)框架，旨在增强大语言模型的推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升LLM本身的通用推理能力。 从正面指标看，论文明确包含多个相关主题：核心概念方面直接研究Large Language Models (LLMs)；能力方向专注于reasoning能力的提升；训练方法涉及self-evolve（自我进化）这一创新范式。论文在八个基准测试上验证了其方法的有效性，表明其关注的是通用推理能力而非特定领域应用。 论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它提出了一种受人类互补学习系统理论启发的通用框架，通过\"daytime scaling\"和\"nighttime scaling\"两个组件的交替，实现了LLM推理能力的自我进化，这种创新方法直接服务于提升LLM的通用推理能力这一核心目标。", "summary2": "本文旨在解决LLMs在测试时计算扩展(TTS)中缺乏自我进化能力的问题。针对测试时推理任务，我们提出了一种LatentEvolve自我进化潜在空间框架，通过daytime scaling和nighttime scaling两个互补组件实现快速适应和慢速整合，并在8个基准测试和5个模型骨干上通过准确率等指标验证了其有效性，超越了现有SOTA方法最高达13.33%。", "inspiration_trace": "# LatentEvolve核心方法的逻辑链推演\n\n## 1. 宏观问题识别：测试时间缩放的自进化困境\n\n作者首先观察到大型语言模型(LLMs)的测试时间缩放(TTS)领域存在一个根本性问题：\n> \"现有TTS方法大多是相互独立的，意味着LLMs尚未进化到能够逐步学习如何更有效地进行缩放。\"\n\n这一观察揭示了核心问题：**当前TTS方法无法从解决问题的经验中学习并进化，每个查询的推理过程被视为独立事件，缺乏跨任务的知识积累与能力提升机制**。这种独立性限制了TTS范式通过与环境的持续交互而逐步进化的潜力。\n\n## 2. 现有方法分析：独立处理的局限性\n\n作者系统分析了现有TTS方法的局限性：\n\n- **并行缩放**：生成多个候选响应并聚合，但成功选择策略不传递到未来任务\n- **顺序缩放**：迭代改进解决方案，但成功反思策略局限于特定实例\n- **混合与内部化缩放**：虽然更复杂，但仍缺乏经验积累机制\n\n关键发现是：**无论具体形式如何，现有TTS方法都缺乏\"自进化\"能力**，无法将解决一个问题的经验转化为提升未来问题解决效率的知识。\n\n## 3. 观察与启发：互补学习系统的启示\n\n作者从认知科学领域获得关键启发：\n> \"我们提出了LatentEvolve，一个受互补学习系统(CLS)理论启发的自进化潜在TTS框架。\"\n\nCLS理论描述了人类大脑的双系统学习机制：\n- **海马体**：快速学习特定情景记忆\n- **新皮层**：慢速整合这些经验为一般知识，尤其在睡眠期间\n\n这一观察为解决TTS自进化问题提供了新思路：**可以设计一个模拟人类认知系统的双阶段学习框架，实现快速适应与慢速整合的互补**。\n\n## 4. 核心假设：双阶段进化机制\n\n基于CLS理论，作者形成核心假设：\n\n> \"LatentEvolve通过双阶段进化运行：白天缩放用于快速的、情景性的适应；夜间缩放用于慢速的程序性整合。\"\n\n这一假设可表述为：**通过模拟海马体-新皮层互补系统，设计一个交替进行快速实例适应(白天)和慢速知识整合(夜间)的框架，可以实现LLMs在测试时间的自进化能力**。\n\n这一假设的关键创新点在于：**将原本独立的TTS过程转变为相互关联、经验积累的进化过程**。\n\n## 5. 方法论构建：从假设到具体实现\n\n作者将核心假设转化为具体的方法论框架：\n\n### 5.1 白天缩放：快速情景适应\n\n模拟海马体的快速记忆功能，实现三个关键机制：\n\n1. **关联检索**：维护情景缓冲区M，存储高质量经验三元组(上下文嵌入、初始潜在序列、优化潜在序列)，对新查询检索相关经验\n\n2. **知情潜在初始化**：不仅利用最终优化状态，更捕获从初始到优化的\"动量\"(Δzj = z∗j - zbase,j)，通过加权动量转移构建更优初始化\n\n3. **自监督细化与归档**：采用自奖励策略迭代优化潜在序列，将高置信度经验存入缓冲区\n\n### 5.2 夜间缩放：慢速知识整合\n\n模拟新皮层在睡眠中的知识整合功能：\n\n1. **潜在编织者**：引入小型LLM作为知识整合模型，学习从上下文和初始状态预测优化潜在序列\n\n2. **经验回放整合**：定期使用缓冲区中的高置信度经验更新编织者参数，将分散经验整合为程序知识\n\n### 5.3 双阶段进化循环\n\n将白天和夜间缩放整合为循环进化过程：\n- 每个查询先通过潜在编织者获得改进初始化\n- 白天缩放进行快速适应并存储经验\n- 定期触发夜间缩放整合经验\n- 形成交替变换：(M, Wψ) → M' → (M', W'ψ)\n\n## 6. 验证与完善：实验驱动的迭代\n\n作者通过多维度实验验证和完善方法：\n\n1. **性能验证**：在八个基准测试和五个模型骨干上验证，LatentEvolve超越现有方法高达13.33%\n\n2. **泛化能力验证**：证明跨域和跨骨干泛化能力，如MATH上的训练可提升GPQA和JAMA性能\n\n3. **持续学习验证**：证明在新域上学习不会降低原域性能，甚至可提升\n\n4. **组件必要性验证**：通过消融研究证明白天和夜间组件的互补必要性\n\n5. **参数敏感性分析**：研究关键参数如潜在维度L'和进化周期T的影响，优化框架配置\n\n## 逻辑链总结\n\nLatentEvolve的核心逻辑链展现了从宏观问题到创新解决方案的完整思考路径：\n\n**问题观察** → **跨领域启发** → **核心假设** → **方法论构建** → **实验验证**\n\n这一路径从\"TTS方法缺乏自进化能力\"的宏观问题出发，通过人类认知系统的CLS理论获得启发，形成\"双阶段进化可实现TTS自进化\"的核心假设，进而构建白天缩放(快速适应)与夜间缩放(慢速整合)的具体方法论，最终通过广泛实验验证其有效性、泛化能力和持续学习潜力。\n\n这一创新不仅解决了TTS领域的自进化困境，更为LLMs实现类似人类认知的持续学习能力提供了新路径。", "summary_translation": "测试时缩放（Test-time Scaling, TTS）已被证明能够显著增强大型语言模型（Large Language Models, LLMs）在推理阶段的推理能力，而无需改变模型参数。然而，现有的TTS方法大多是独立的，这意味着LLMs尚未进化到能够逐步学习如何更有效地进行缩放。为了使LLMs进化到能够学习\"如何缩放测试时计算\"（how to scale test-time computation），我们提出了LatentEvolve，这是一个受互补学习系统（complementary learning system, CLS）理论启发的自我进化潜在TTS框架。类似于人脑中快速回忆的海马体（hippocampus）和缓慢巩固的新皮层（neocortex）的双系统，LatentEvolve包含两个进化组件：\\textit{daytime scaling}（白天缩放），它快速检索历史潜在表示以更好地指导当前LLM推理；以及\\textit{nighttime scaling}（夜间缩放），它以类似于人脑在睡眠期间巩固经验的方式整合过去的潜在优化。白天和夜间过程的交替促进了LLM TTS的快速和缓慢进化，以完全无监督的方式反映了人类认知动态。在八个基准测试（benchmarks）和五个模型骨干（model backbones）上的广泛实验表明，我们的LatentEvolve超越了最先进的TTS方法，如LatentSeek和TTRL，最高提升达$13.33\\%$，并表现出卓越的跨领域和跨骨干泛化能力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#34", "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models", "link": "/arxiv/2509.24781", "arxiv_id": "2509.24781", "authors": "Jun Rao, Yunjie Liao, Xuebo Liu, Zepeng Lin, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang", "summary": "Existing alignment methods for preference optimization of large language models (LLMs) aim to enhance model performance by utilizing pairs of positive and negative samples. However, due to the limited capacity of models in scoring or generating responses, the quality of positive and negative samples may become similar during training, which complicates optimization for preference learning. To address this issue, we introduce SeaPO, a Strategic Error Amplification method that leverages three error types commonly occurring in LLMs to introduce specific error patterns into the model Preference Optimization. This strategy ensures that negative samples are more erroneous than positive samples and preference-based training is employed to mitigate the occurrence of these errors, thereby enhancing model performance. Evaluations across five capability dimensions and different model scales (1.5B to 14B) demonstrate that the generated data significantly improved overall model performance, particularly in terms of truthfulness, with improvements of 5-10 percentage points observed. Further analysis reveals that task performance varies depending on the error types introduced. Injecting the most common error types improves performance in related tasks, while a mix of error types leads to a broader performance enhancement: most tasks show stable improvements, while a few tasks exhibit significant gains.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.444654", "filter_reason": "这篇论文的核心贡献是提出SeaPO（Strategic Error Amplification）方法，用于大语言模型的偏好优化。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从本质上看，这篇论文专注于改进LLM的基础能力，提出了一种新的训练范式来增强模型的偏好学习。它通过策略性地放大错误来确保负样本比正样本更有错误性，从而提高模型的整体性能和真实性，这属于改进LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念明确聚焦于大语言模型(LLMs)；训练方法涉及偏好优化，这与强化学习人类反馈(RLHF)相关；虽然未直接提及reasoning等术语，但提高模型真实性是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准：它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，虽然论文没有涉及智能体/工具使用，但它关注于通过新方法提高模型的真实性，这与减少幻觉相关，属于提升模型内在可靠性和推理质量的方法，符合研究目标。 综上所述，SeaPO论文致力于通过改进训练范式来提高LLM的通用推理能力，特别是真实性方面，因此符合研究范围。", "summary2": "本文旨在解决大型语言模型偏好优化中正负样本质量相似导致的优化困难问题。针对LLMs在评分或生成响应时的能力限制，我们提出了一种SeaPO策略性错误放大方法，通过引入三种常见错误类型（正确性、逻辑和幻觉）来构造负样本，并在多个能力维度和不同模型规模（1.5B到14B）上通过准确率等指标验证了其有效性，尤其在真实性任务上实现了5-10个百分点的性能提升。", "inspiration_trace": "# SeaPO方法逻辑链分析：从问题观察到方法论形成\n\n## 一、宏观问题：LLM偏好优化的样本质量困境\n\n在大型语言模型(LLMs)的偏好优化过程中，研究者面临一个根本性挑战：**如何确保正负样本之间存在足够的质量差异以实现有效学习**。现有方法（如DPO、KTO等）依赖于高质量的正负样本对，但实际应用中存在一个悖论——如果模型本身能力有限，它可能难以生成或识别真正高质量的正负样本，导致两者质量相近，使偏好优化效果大打折扣。\n\n## 二、问题观察：现有方法的局限性\n\n作者通过深入分析，识别出三个关键观察：\n\n### 1. 模型能力限制导致样本质量模糊\n- 评分模型可能误判正负样本，特别是在复杂任务中\n- 拒绝采样产生的\"负样本\"有时可能与\"正样本\"质量相当，甚至更好\n- 这种质量模糊性使偏好学习信号变得微弱，模型难以区分\n\n### 2. 传统负样本构建方法效率低下\n- 需要多次采样和强大的评分模型来识别正负样本\n- 计算开销大，且对模型能力要求高\n- 缺乏对错误类型的明确控制，导致负样本多样性不足\n\n### 3. 现有研究关注点不平衡\n- 大多数工作专注于生成更正确的答案，忽视了提高错误样本的质量\n- 缺乏系统性分析和利用LLMs常见错误类型的研究\n\n## 三、核心假设：战略性错误的作用机制\n\n基于上述观察，作者提出了一系列假设，形成了SeaPO的理论基础：\n\n### 1. 错误类型可定义性假设\nLLMs的错误并非随机分布，而是可以被系统性地分类为几种主要类型。作者将这些错误定义为\"战略性错误\"(Strategic Errors)：\n- **正确性错误**：与事实准确性或计算相关的问题（如错误事实、计算错误）\n- **逻辑错误**：推理或论证存在缺陷（如矛盾、不支持的结论）\n- **幻觉错误**：生成完全捏造或虚假的信息\n\n### 2. 错误放大假设\n通过在负样本中故意引入这些特定类型的错误，可以：\n- 确保负样本比正样本更容易出错\n- 创建明确的学习信号，增强偏好学习效果\n- 减少对复杂评分模型的依赖\n\n### 3. 错误减少假设\n基于这些故意引入错误的负样本进行偏好优化训练，可以：\n- 减少模型在实际应用中产生这些类型错误的概率\n- 提高模型在相关任务上的表现\n- 实现更全面的性能提升\n\n## 四、方法论构建：SeaPO的设计逻辑\n\n基于上述假设，作者设计了SeaPO方法，包含两个核心组件：\n\n### 1. 错误注入的负样本生成\n**设计逻辑**：如果传统方法难以获取高质量负样本，为什么不主动构建具有明确错误特征的负样本？\n\n**实现方式**：\n- 定义战略性错误类型及其特征（如表1所示）\n- 使用目标模型本身对原始正确答案进行后编辑，故意注入指定比例的错误\n- 形式化表示：aerror = Injector(x, y, e)，其中e为特定错误类型\n\n**创新点**：不再依赖采样和评分模型获取负样本，而是通过明确的错误注入过程生成负样本，降低了计算开销和对强大模型的依赖。\n\n### 2. 错误聚焦的偏好优化\n**设计逻辑**：既然我们定义了特定错误类型，优化目标应该直接针对减少这些错误的发生概率。\n\n**实现方式**：\n- 选择KTO作为基础优化算法（因为它不需要匹配的数据对）\n- 优化目标：减少负样本概率，从而最小化引入的战略错误的发生概率\n- 数学表达：LKTO(πθ, πref) = Ex,y∼D[λy - v(rθ(x,y))]，其中rθ(x,y)是输出的奖励\n\n**创新点**：将优化目标明确指向减少特定错误类型，而非一般的偏好对齐。\n\n## 五、实验验证：假设检验与效果评估\n\n作者通过一系列实验验证了SeaPO的有效性和假设的正确性：\n\n### 1. 主要结果验证\n- SeaPO在不同模型规模（1.5B到14B）和任务上持续优于基线方法\n- 在错误丰富的数据集（如MATH和TruthfulQA）上表现尤为突出\n- 例如，Qwen2.5-7B在MATH上提高10个百分点，在TruthfulQA上提高12个百分点\n\n### 2. 分析实验深化理解\n- **错误类型影响分析**：不同错误类型对不同任务有不同影响，验证了错误类型与任务关联的假设\n- **错误注入质量分析**：生成的负样本确实比原始负样本包含更多错误，验证了错误放大的假设\n- **错误严重程度分析**：中等错误程度的样本效果最佳，说明错误需要可识别但不过度\n- **优化目标分析**：SeaPO在不同学习算法中均表现良好，证明方法的鲁棒性\n\n## 六、逻辑演进总结\n\nSeaPO方法的逻辑演进展现了一个清晰的问题解决路径：\n\n1. **从宏观问题出发**：识别LLM偏好优化中正负样本质量相似的根本挑战\n2. **深入观察现象**：分析现有方法的局限性，特别是负样本构建的问题\n3. **形成核心假设**：提出战略性错误的概念及其在偏好优化中的潜在作用\n4. **设计创新方法**：构建包含错误注入和错误聚焦优化的SeaPO框架\n5. **系统验证假设**：通过广泛的实验验证方法的有效性和假设的正确性\n\n这一逻辑链条不仅解决了一个具体的技术问题，还为LLM的偏好优化提供了一个新的思路：通过主动控制和利用错误，而非被动接受错误，来提升模型性能。这种方法论的创新在于将\"错误\"从需要避免的对象转变为可以利用的工具，体现了问题解决思维的转变。", "summary_translation": "现有的针对大型语言模型(LLMs)偏好优化的对齐方法旨在通过利用正负样本对来提高模型性能。然而，由于模型在评分或生成响应方面的能力有限，正负样本的质量在训练过程中可能变得相似，这使偏好学习的优化变得复杂。为解决这一问题，我们提出了SeaPO，一种策略性错误放大(Strategic Error Amplification)方法，该方法利用大型语言模型中常见的三种错误类型，在模型偏好优化(Preference Optimization)中引入特定的错误模式。这一策略确保负样本比正样本包含更多错误，并采用基于偏好的训练来减轻这些错误的发生，从而提高模型性能。在五个能力维度和不同模型规模(1.5B至14B)上的评估表明，生成的数据显著提高了整体模型性能，特别是在真实性(truthfulness)方面，观察到5-10个百分点的提升。进一步分析显示，任务性能根据引入的错误类型而有所不同。注入最常见的错误类型可提高相关任务的性能，而混合错误类型则带来更广泛的性能提升：大多数任务表现出稳定的改进，而少数任务则显示出显著增益。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#38", "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents", "link": "/arxiv/2509.24704", "arxiv_id": "2509.24704", "authors": "Guibin Zhang, Muxin Fu, Shuicheng Yan", "summary": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \\textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \\textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.446844", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进LLM的基础能力，提出了一种新的记忆框架来增强LLM驱动智能体的推理能力。论文提出的MemGen框架通过\"记忆触发器\"和\"记忆编织器\"实现了记忆与推理的紧密交织，这属于增强LLM通用推理能力的方法论研究，特别是关于智能体框架和自我进化的研究方向。 其次，论文包含多个正面指标：明确涉及\"Large Language Model (LLM)-powered agents\"这一核心概念；关注推理能力的提升(\"enrich its reasoning\")；涉及自我进化(\"Self-Evolving Agents\")；并且研究LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容；没有专注于特定应用领域；也没有关注模型可靠性方面的应用问题。 在特殊和模糊情况处理上，论文提出的是一种通用的智能体记忆框架，而非针对特定领域的应用。实验在\"八个基准测试\"上进行，展示了其跨领域泛化能力，进一步证明其通用性。 论文的核心贡献是提出了一种动态生成记忆框架，使LLM驱动的智能体能够通过记忆与推理的交织循环实现自我进化，自发演化出类似人类的记忆能力，从而提升通用推理能力。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决LLM agent记忆机制无法实现推理与记忆流畅交织的问题。针对agent与环境交互过程中的经验学习场景，我们提出了一种MemGen动态生成式记忆框架，通过memory trigger和memory weaver实现记忆与推理的紧密交织，并在8个基准测试上通过任务完成率和准确率等指标验证了其有效性。实验表明MemGen超越了现有外部记忆系统，表现出强大跨领域泛化能力，且能自发演化出类似人类的记忆层级结构。", "inspiration_trace": "# MemGen核心方法的逻辑推演\n\n## 1. 宏观问题：智能体如何实现自我进化？\n\n论文从智能体自我进化的根本问题出发。智能体需要通过与环境的交互不断学习和改进，而记忆机制是这一过程的核心。正如人类大脑通过记忆不断 refine 自身能力，LLM驱动的智能体也需要类似的记忆机制来实现持续进化。\n\n> \"Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions.\"\n\n## 2. 现有范式的局限性分析\n\n作者系统分析了现有两种主要记忆范式及其根本缺陷：\n\n### 2.1 参数化记忆的困境\n- **工作原理**：通过直接更新模型参数内化经验\n- **核心局限**：灾难性遗忘(catastrophic forgetting)——新知识获取导致原有知识丢失\n- **本质问题**：记忆与推理能力耦合，无法独立进化\n\n### 2.2 检索式记忆的不足\n- **工作原理**：将经验外化到结构化数据库，任务执行时检索\n- **核心局限**：缺乏与推理的流畅交织，依赖刚性执行流程\n- **本质问题**：记忆是外部的、静态的，无法与认知过程动态互动\n\n## 3. 与人类认知的关键差距\n\n通过对比人类认知，作者识别出两个关键差距：\n\n1. **缺乏无缝交织**：人类大脑中，推理和记忆形成\"连续的思想流\"，而现有方法仅在任务开始时检索一次记忆\n2. **缺乏生成性重构**：现有方法通过嵌入相似性检索记忆，而非生成性地重构为新颖、连贯的见解\n\n这引出了核心研究问题：\n> \"How can we architect agent memory as a dynamic cognitive faculty, capable of fluid, reconstructive processes that interweave seamlessly with reasoning?\"\n\n## 4. 潜在记忆的探索与局限\n\n作者考察了潜在记忆(Latent Memory)作为替代方案，但发现现有方法仍有局限：\n\n1. **KV缓存方法**：主要解决长上下文问题，而非记忆重构\n2. **潜在令牌嵌入**：仍依赖侵入式参数更新，无法避免灾难性遗忘\n\n这些方法未能实现记忆与推理的动态交织和生成性重构。\n\n## 5. 核心假设：动态生成式记忆框架\n\n基于以上分析，作者提出核心假设：设计一个动态生成式记忆框架，使智能体具备类人认知能力。这一框架应该：\n\n1. **动态监控**：持续监控智能体的认知状态\n2. **按需生成**：在推理过程中的关键时刻动态调用记忆生成\n3. **无缝集成**：将记忆与推理紧密交织，形成递归对话\n\n## 6. 方法论设计：MemGen框架\n\n基于假设，作者设计了MemGen框架，包含两个协同组件：\n\n### 6.1 记忆触发器(Memory Trigger)\n- **功能**：作为元认知监控器，识别记忆调用的适当时机\n- **实现**：RL训练的轻量级LoRA适配器，监控推理器的隐藏状态\n- **关键创新**：仅在语义显著边界(如句子结束)激活，平衡记忆效用与计算效率\n- **训练目标**：确保关键记忆被调用，同时避免不必要的调用\n\n### 6.2 记忆编织器(Memory Weaver)\n- **功能**：以当前状态为刺激，生成机器原生的潜在记忆\n- **实现**：另一个LoRA适配器，接受隐藏状态作为\"钩子\"，生成潜在令牌序列\n- **关键创新**：不是简单检索，而是生成性地重构记忆\n- **训练目标**：仅更新编织器参数，保持核心推理器不变，避免灾难性遗忘\n\n## 7. 关键创新与突破\n\nMemGen的核心创新在于实现了三个关键突破：\n\n1. **动态记忆调用**：不同于传统方法的一次性检索，可在推理过程中任何关键时刻调用记忆\n2. **生成式记忆重构**：根据当前状态生成性重构记忆，而非静态检索\n3. **推理-记忆无缝交织**：形成递归对话，而非线性展开\n\n## 8. 实验验证与意外发现\n\n通过广泛实验，作者不仅验证了MemGen的性能优势，还发现了一个重要现象：\n\n**自发演化的类人记忆层次**：在没有明确监督的情况下，MemGen自发演化出三种类人记忆功能：\n- 规划记忆：支持高级任务规划\n- 程序记忆：捕获任务特定操作知识\n- 工作记忆：维护长上下文的一致性和理解\n\n这一发现表明，MemGen不仅解决了技术问题，还朝着更自然的机器认知形式迈出了重要一步。\n\n## 9. 结论：从技术方案到认知范式\n\nMemGen代表了一种从技术方案到认知范式的转变。通过将记忆视为动态认知能力而非静态存储，MemGen实现了记忆与推理的无缝交织，为自我进化智能体的发展提供了新路径。这一工作不仅解决了现有记忆机制的技术局限，还为构建更接近人类认知的智能系统开辟了新方向。", "summary_translation": "代理记忆塑造了大型语言模型（LLM）驱动的代理如何类似人脑那样通过环境交互逐步自我完善。现有范式仍然受限：参数化记忆（parametric memory）强制调整模型参数，而基于检索的记忆（retrieval-based memory）将经验外化到结构化数据库中，但两者都未能捕捉到人类认知背后推理和记忆的流动交织。为弥补这一差距，我们提出了MemGen，一种动态生成记忆框架，为代理配备了类似人类的认知能力。它由一个\\textit{记忆触发器}（memory trigger）和一个\\textit{记忆编织器}（memory weaver）组成，前者监控代理的推理状态以决定显式记忆调用，后者将代理的当前状态作为刺激，构建潜在令牌序列作为机器原生记忆来丰富其推理。通过这种方式，MemGen使代理能够在整个推理过程中回忆和增强潜在记忆，产生记忆与认知紧密交织的循环。在八个基准测试上的大量实验表明，MemGen比领先的外部记忆系统如ExpeL和AWM高出最多$38.22\\%$，超过GRPO最多$13.44\\%$，并展现出强大的跨领域泛化能力。更重要的是，我们发现没有显式监督的情况下，MemGen自发演化出不同的类似人类记忆能力，包括规划记忆（planning memory）、程序性记忆（procedural memory）和工作记忆（working memory），这表明机器认知正朝着更自然的形式涌现的轨迹。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#37", "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution", "link": "/arxiv/2509.24726", "arxiv_id": "2509.24726", "authors": "Shaobo Wang, Zhengbo Jiao, Zifan Zhang, Yilang Peng, Xu Ze, Boyu Yang, Wei Wang, Hu Wei, Linfeng Zhang", "summary": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets-typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum-requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4, and Claude-4.1-Opus.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.446354", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是提出Socratic-Zero框架，通过三个智能体（Teacher、Solver和Generator）的共同进化来生成高质量训练数据，从而提高大语言模型的推理能力。这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划和多步推理等通用能力\"的范畴，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究large language models (LLMs) - 能力方向：专注于reasoning，特别是math reasoning，在多个数学推理基准测试上评估 - 训练方法：涉及evolution和self-evolve概念，通过智能体共同进化实现自我改进 - 新兴范式：提出了llm-based agents和multi-agent systems框架 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于特定应用领域（虽然使用数学推理作为评估基准，但方法是通用的） - 不讨论模型可靠性的应用层面问题 第四步特殊和模糊情况：论文提出的是通用的智能体协作框架来增强LLM的通用推理能力，而非针对特定领域的应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过智能体共同进化来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型推理能力提升对大规模高质量人工标注数据的依赖问题。针对数学推理任务，我们提出了一种基于三个智能体（Teacher、Solver和Generator）共同进化的Socratic-Zero框架，并在七个数学推理基准测试上通过准确率等指标验证了其有效性。实验表明，该方法从仅100个种子问题开始，实现了平均+20.2百分点的性能提升，且生成的合成数据使学生模型超越了多个最先进商业模型。", "inspiration_trace": "# Socratic-Zero方法论逻辑推演\n\n## 1. 宏观问题：数据依赖困境\n\n**观察**：当前大型语言模型(LLMs)在推理任务上的突破严重依赖大规模、高质量数据集，而这些数据集通常需要人工标注，难以扩展。\n\n**核心问题**：如何在不依赖大规模人工标注数据的情况下，提升LLMs的数学推理能力？\n\n## 2. 问题细化：三大关键挑战\n\n**挑战1：静态范式限制**\n- 现有方法中，数据集在收集后被冻结，课程预先设计，模型在固定问题分布上训练\n- 无法适应训练过程中模型能力的变化\n- 无法利用反馈信号针对特定弱点\n\n**挑战2：现有合成数据方法缺陷**\n- 缺乏有效的质量控制机制\n- 重复使用低价值样本严重影响效果\n- 无法动态适应模型不断发展的能力\n\n**挑战3：知识传递低效**\n- 学生模型被动接受教师反馈而不评估可靠性\n- 当指导不理想时，学习质量下降\n- 无法根据学生能力动态调整内容\n\n## 3. 关键观察与启发\n\n**哲学启发**：苏格拉底式学习方法\n- 教师(苏格拉底)通过提问引导思考，而非直接给出答案\n- 实践者(亚里士多德)通过被引导沿推理路径学习\n- 学徒教师(柏拉图)通过观察内化教学方法\n\n**技术观察**：\n- 数据合成方法(如LLM2LLM)引入缺陷感知机制\n- 自主自玩范式(如Absolute Zero)探索连续任务生成\n- 偏好学习方法(如DPO)直接优化偏好，提高效率\n\n**核心洞察**：需要一个闭环系统，能够产生自我改进的课程，不需要预先存在的任务或标签。\n\n## 4. 核心假设形成\n\n**假设1：协同进化假设**\n如果设计一个系统，其中解题器、教师和生成器三个智能体协同进化，可以创造自我改进循环，从最少种子数据开始自主提升推理能力。\n\n**假设2：动态课程假设**\n如果课程能根据解题器表现动态调整难度，保持在解题器能力前沿(既不太简单也不太困难)，学习效率将最大化。\n\n**假设3：知识蒸馏假设**\n如果生成器能学习并内化教师的问题生成策略，就能以可扩展方式产生高质量课程，减少对昂贵教师模型的依赖。\n\n**假设4：偏好学习假设**\n如果解题器不仅从正确解决方案学习，还从成功和失败轨迹的偏好反馈中学习，可以更有效地改进推理能力。\n\n## 5. 方法论设计：Socratic-Zero框架\n\n### 5.1 三智能体架构\n\n**Solver(解题器)**：\n- 策略πθS，将问题q映射到解决方案轨迹y\n- 通过偏好学习改进推理能力\n\n**Teacher(教师)**：\n- 固定高容量LLM，提供两个确定性函数：\n  - 验证函数V(q, y) → {0, 1}：判断解决方案正确性\n  - 问题细化函数G(q, yfail) → (q', y'ref)：基于失败解决方案创建新问题\n\n**Generator(生成器)**：\n- 策略πθG，学习模仿教师的问题细化策略\n- 通过价值加权监督微调(WSFT)训练\n\n### 5.2 协同进化机制\n\n**课程动态扩展**：\n1. 收集解题器在当前课程Dt上的失败集合Ft\n2. 教师细化每个失败案例，创建新问题-解决方案对\n3. 扩充课程：Dt+1 = Dt ∪ Dnew\n\n**解题器训练**：\n- 生成多个解决方案尝试，分为\"获胜\"和\"失败\"集合\n- 使用直接偏好优化(DPO)更新参数，最大化首选解决方案可能性\n\n**生成器训练**：\n- 定义效用函数U(q'|πθS)，以目标成功率0.5为中心的高斯函数\n- 通过加权监督微调(WSFT)学习生成高效用问题\n\n### 5.3 动态课程分区\n\n根据解题器表现将问题分为三区：\n- **已掌握区域**：解题器始终能正确解决的问题\n- **学习区域**：解题器间歇性能解决的问题\n- **太难区域**：解题器始终无法解决的问题\n\n新问题主要从已掌握和学习区域生成，确保课程扩展保持在解题器的\"最近发展区\"内。\n\n## 6. 实验验证与结果\n\n**解题器效果**：\n- Socratic-Solver-8B在七个数学推理基准上平均准确率56.1%，比静态增强方法提高15.4个百分点\n- 跨架构泛化：在GLM4-9B和Qwen3-14B上也表现一致改进\n- 数学推理能力转移到更广泛认知能力，平均提高6.02个百分点\n\n**生成器效果**：\n- Socratic-Generator-32B实现95.6%有效率，显著优于基础模型(89.1%)\n- 产生的合成数据使学生模型达到37.72%准确率，优于包括Qwen3-235B-A22B(37.13%)等在内的最先进商业模型\n\n**消融研究**：\n- 初始监督微调(SFT)至关重要：无SFT版本仅达11.98%，SFT初始化模型达28.02%\n- 高斯奖励函数N(μ=0.5, σ=0.2)表现最佳，优于其他参数设置\n\n## 7. 结论与展望\n\n**核心贡献**：\n- 提出多智能体协同进化框架，将推理改进形式化为自适应课程学习问题\n- 实现从最小种子数据(仅100个问题)开始的自主推理提升\n- 在数学推理基准上取得最先进结果，展示强泛化能力\n\n**未来方向**：\n- 建立多智能体动态的理论基础，包括收敛分析\n- 将框架扩展到科学发现、现实世界决策等更广泛领域\n- 探索分层扩展，多个解题器-生成器对在不同难度水平运行\n\nSocratic-Zero成功将苏格拉底方法的哲学对话转化为协同进化的计算框架，通过三个智能体的动态交互，实现了从最少数据开始的自主推理能力提升，为解决数据依赖问题提供了新思路。", "summary_translation": "近期大语言模型（large language models, LLMs）在推理任务（reasoning tasks）上的突破严重依赖大规模、高质量的数据集——这些数据集通常需要人工标注，因此难以扩展。尽管数据合成（data synthesis）或蒸馏（distillation）提供了一种有前景的替代方案，但现有方法在数据质量不一致和无法动态适应模型不断发展的能力方面存在困难，导致训练信号次优。为解决这些局限性，我们提出了Socratic-Zero（苏格拉底-零），这是一个完全自主的框架，通过三个智能体的共同进化——教师（Teacher）、解决者（Solver）和生成者（Generator）——从最少的种子示例中生成高质量训练数据。解决者（Solver）通过学习成功和失败轨迹（trajectories）上的偏好反馈（preference feedback）不断完善其推理能力；教师（Teacher）根据解决者的弱点自适应地设计越来越具有挑战性的问题；而生成者（Generator）则蒸馏（distills）教师的问题设计策略，以实现可扩展、高保真的课程（curriculum）生成。这个闭环系统产生了一个自我改进的课程——无需预先存在的任务或标签。值得注意的是，仅从100个种子问题开始，我们的Socratic-Solver-8B在七个数学推理基准测试（benchmarks）（AMC23、AIME24-25、Olympiad、MATH-500、Minerva和GSM8K）上比先前的数据合成方法平均提高了20.2个百分点，并且在Qwen3和GLM4系列模型上都有一致的提升。更令人惊讶的是，来自Socratic-Generator-32B的合成数据使学生大语言模型在这些基准测试上能够实现比其他最先进（state-of-the-art, SOTA）商业大语言模型更优越的性能，包括Qwen3-235B-A22B、DeepSeek-V3.1-671B、GPT-5、Gemini-2.5-Pro、Grok-4和Claude-4.1-Opus。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#48", "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training", "link": "/arxiv/2509.24494", "arxiv_id": "2509.24494", "authors": "Hongcheng Wang, Yinuo Huang, Sukai Wang, Guanghui Ren, Hao Dong", "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.457520", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文本质上是关于改进LLM推理能力的训练方法。具体来说，它提出了GRPO-MA方法来优化GRPO算法，以更稳定和高效地训练大语言模型的链式思维(CoT)推理能力。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的标准。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于reasoning，特别是Chain-of-Thought推理，并在数学和代码任务上验证 - 训练方法：使用强化学习(GRPO)进行训练优化 第三步排除标准：虽然论文提到了VLMs和\"多样化的多模态任务\"，但多模态并非论文的主要焦点，而是作为评估方法的一种应用场景。论文核心是提出一种通用的训练方法改进，而非专注于多模态技术研究。论文也不涉及医疗、化学等特定应用领域，或水印、安全等模型可靠性问题。 第四步特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊内容。关于多模态方面，论文只是将多模态任务作为评估场景之一，而非研究核心。 综合判断：论文的核心贡献是提出GRPO-MA方法来改进强化学习训练过程，以增强大语言模型的链式思维推理能力。这是一种通用推理能力的提升方法，符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。因此，这篇论文应该被保留。", "summary2": "本文旨在解决GRPO算法在Chain-of-Thought训练中的梯度耦合、稀疏奖励和不稳定优势估计问题。针对LLMs和VLMs的推理训练，我们提出GRPO-MA方法，通过为每个思维生成多个答案优化训练，并在数学、代码和多模态任务上通过准确率和训练效率验证了其有效性。", "inspiration_trace": "# GRPO-MA核心方法的逻辑推演\n\n## 宏观问题：如何提高基于强化学习的链式思考(CoT)训练的稳定性和效率？\n\n### 第一步：观察现有方法(GRPO)的表现与局限\n\n作者首先观察到GRPO算法在训练CoT推理方面的成功应用，如DeepSeek-R1模型所示。然而，通过深入分析，作者识别出GRPO面临的三个关键挑战：\n\n1. **梯度耦合问题**：思考和答案的梯度在GRPO中是耦合的。这导致一个根本矛盾：推理过程可能有效但最终答案错误，或推理有缺陷却产生正确答案。这种不一致性会扭曲梯度方向，削弱训练效果。\n\n2. **稀疏奖励信号**：GRPO需为每个问题采样多个完整CoT-答案对，计算成本高。在有限采样下，难以捕获罕见成功结果，导致全零奖励组。由于GRPO通过组内归一化计算优势，全零奖励集会崩溃为统一零优势，消除信息梯度，阻碍优化。\n\n3. **不稳定优势估计**：从概率角度看，\"好\"思考应增加生成\"好\"答案的概率。评估思考质量的最稳健方法是评估其产生的多个答案的总体分布。GRPO基于单个答案估计思考优势，结合高温采样的随机性，导致优势估计方差增大。\n\n### 第二步：形成核心假设\n\n基于上述观察，作者提出三个关键假设：\n\n1. 若能减少思考与答案间的梯度耦合，可能提高训练效果。\n2. 若能增加奖励信号密度而不显著增加计算成本，可解决稀疏奖励问题。\n3. 若能通过多答案样本估计思考优势，可降低优势估计方差，提高训练稳定性。\n\n### 第三步：探索解决方案方向\n\n针对这些假设，作者探索了可能的解决路径：\n\n1. **解耦思考与答案评估**：分别评估思考过程和最终答案质量，而非将它们视为整体。这可通过为每个思考生成多个答案，基于这些答案的平均奖励评估思考质量。\n\n2. **增强奖励信号密度**：通过为每个思考生成多个答案，在不生成大量完整推理响应的情况下增加奖励信号密度，避免全零奖励导致的优势崩溃。\n\n3. **稳定优势估计**：基于多个答案的平均奖励估计思考优势，降低估计方差。理论上，随着每个思考的答案数量增加，思考优势的方差应减少。\n\n### 第四步：形成GRPO-MA方法论\n\n基于上述探索，作者形成了GRPO-MA（GRPO with Multi-Answer）方法论：\n\n1. **核心创新**：对于K个思考中的每一个，采样M个答案。思考的价值是其M个答案的平均奖励，用于推导其相对于其他思考的优势。同时，每个K×M答案也接收自己的优势。这两个优势分别用于更新思考和答案标记。\n\n2. **理论支持**：基于delta方法的理论分析表明，随着M增加，思考优势的方差单调递减至零。相比之下，增加K只能将方差减少到一个非零常数。\n\n3. **实现框架**：\n   - 给定提示p，生成K个思考{th₁, ..., thₖ}\n   - 对每个思考thᵢ，生成M个答案{ansᵢ,₁, ..., ansᵢ,ₘ}\n   - 计算思考价值V(thᵢ) = (1/M)∑ⱼRᵢ,ⱼ，并归一化得到思考优势A(thᵢ)\n   - 计算每个答案的优势A(ansᵢ,ⱼ)\n   - 结合两个层次的优势进行更新：基于思考优势更新思考，基于答案优势更新答案\n\n### 第五步：验证与评估\n\n作者通过多维度实验验证GRPO-MA的有效性：\n\n1. **实验范围**：在数学、代码和多种多模态任务（目标检测、可供性预测、轨迹预测等）以及基于模拟器的视觉操作任务上评估。\n\n2. **关键发现**：\n   - 与GRPO基线相比，GRPO-MA在训练时间略微增加的情况下产生明显性能提升\n   - 与生成K×M个响应的基线相比，GRPO-MA仅用约60%训练时间实现相似或更好性能\n   - 在稀疏奖励任务中，GRPO-MA显著优于标准GRPO\n   - 消融研究表明增加M通常提高模型性能，且思考优势估计的稳定性比奖励信号丰富性更关键\n\n3. **理论验证**：梯度分析证实GRPO-MA比GRPO减少了梯度尖峰，验证了理论预测。\n\n### 第六步：总结与展望\n\n作者总结GRPO-MA的核心贡献：\n\n1. 提出GRPO-MA算法，一种简单有效且通用的GRPO改进策略\n2. 提供理论分析，证明该方法可提高优势估计稳定性，产生更稳定梯度\n3. 在多个不同任务上验证GRPO-MA始终优于基线GRPO\n\n同时，作者也指出研究局限性，如计算约束、思考值独立性假设的简化，以及缺乏通用奖励模型等，为未来研究指明方向。\n\n这一逻辑推演展示了从观察到问题识别，从假设形成到解决方案探索，最终形成方法论并验证的完整研究思路，体现了严谨的学术思维过程。", "summary_translation": "最近的进展，如DeepSeek-R1，表明GRPO算法（一种强化学习(Reinforcement Learning, RL)方法）能够有效地训练大型语言模型(Large Language Models, LLMs)和视觉语言模型(Vision-Language Models, VLMs)中的思维链(Chain-of-Thought, CoT)推理。在本文中，我们分析了GRPO面临的三个挑战：思想与答案之间的梯度耦合(gradient coupling)、由有限并行采样导致的稀疏奖励信号(sparse reward signals)以及不稳定的优势估计(advantage estimation)。为缓解这些挑战，我们提出了GRPO-MA，这是一个简单但具有理论基础的方法，它利用每个思想过程的多答案生成(multi-answer generation)，从而实现更稳健和高效的优化。理论上，我们证明了思想优势(thought advantage)的方差随着每个思想生成的答案数量增加而减少。实证上，我们的梯度分析(gradient analysis)证实了这一效果，表明GRPO-MA相比GRPO减少了梯度尖峰(gradient spikes)。在数学、代码和多样化多模态任务(multimodal tasks)上的实验表明，GRPO-MA显著提高了性能和训练效率。我们的消融研究(ablation studies)进一步揭示，增加每个思想的答案数量持续提升模型性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#52", "title": "Alternatives To Next Token Prediction In Text Generation - A Survey", "link": "/arxiv/2509.24435", "arxiv_id": "2509.24435", "authors": "Charlie Wyatt, Aditya Joshi, Flora Salim", "summary": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.459466", "filter_reason": "这篇论文的核心是探讨替代传统下一个词元预测(NTP)范式的方法，以解决大语言模型在长期规划、错误累积和计算效率等方面的弱点。论文明确聚焦于改进LLM的基础能力，特别是通过\"Plan-then-Generate\"和\"Latent Reasoning\"等方法来增强模型的规划和推理能力，这与研究目标中\"提高大语言模型本身的通用推理能力\"高度一致。论文没有涉及任何特定应用领域（如医疗、化学等），也不是将LLM作为工具应用于特定问题，而是专注于改进LLM的核心生成机制和推理能力。作为一篇综述，它系统地分类了五种改进LLM推理能力的方法家族，包括多词元预测、先规划后生成、潜在推理等，这些都是直接针对提升LLM通用推理能力的研究方向。因此，这篇论文完全符合筛选标准，应该被纳入研究范围。", "summary2": "本文旨在解决Next Token Prediction (NTP)在文本生成中的局限性问题，包括长期规划能力差、错误累积和计算效率低等。针对现有LLMs的NTP范式，我们提出了一种系统性的分类框架，将替代方法分为五大类：Multi-Token Prediction、Plan-then-Generate、Latent Reasoning、Continuous Generation Approaches和Non-Transformer Architectures，并通过综合分析各类代表性研究成果，验证了这些方法在提升生成质量和效率方面的有效性。", "inspiration_trace": "# 从NTP局限到替代方案：逻辑推演链\n\n## 1. 宏观问题：NTP范式的根本性矛盾\n\n**观察起点**：下一个词预测(NTP)范式驱动了大型语言模型(LLMs)的空前成功，但同时也成为其最持久弱点的根源。尽管LLMs在流畅性方面表现出色，却经常表现出脆弱行为：幻觉事实、长输出中失去连贯性、在需要长期规划的任务中挣扎。\n\n**核心矛盾**：NTP在实现局部流畅性的同时，牺牲了全局连贯性和推理能力。即使模型规模、上下文长度和训练数据不断增长，这些局限性依然存在，表明问题不是偶然的，而是NTP框架本身的结构性问题。\n\n## 2. 深入观察：NTP的多维度局限性\n\n### 2.1 推理时失败：贪婪生成与错误累积\n- **现象**：NTP的自回归特性鼓励短视优化，优先考虑短期标记可能性而非长期连贯性\n- **问题**：早期生成中的微小错误会复合成后期序列中的显著不连贯或事实漂移\n- **证据**：即使在大海捞针检索任务中表现近乎完美，LLMs在生成长文档摘要时仍表现不佳\n\n### 2.2 粒度不匹配：标记与想法的鸿沟\n- **现象**：NTP在子词标记上操作，而人类推理在想法、句子和话语结构上展开\n- **问题**：子词标记既非语言上自然，也非语义上有意义，导致在简单字符级任务上失败\n- **证据**：LLMs难以回答\"strawberry中有多少个r\"等简单问题，因为无法预先规划整体响应\n\n### 2.3 计算效率低下：顺序生成的代价\n- **现象**：Transformer在每个层和时间步计算全序列注意力，导致O(n²)复杂度\n- **问题**：即使适度长度文档也需要数百或数千解码步骤，每步都涉及完整前向传递\n- **证据**：现代NLP架构的能源消耗和环境影响已成为关注焦点\n\n### 2.4 训练时失败：教师强制的误导\n- **现象**：模型使用教师强制训练，学习在给定真实前文的情况下预测下一个标记\n- **问题**：导致模型学习虚假捷径(\"Clever Hans\"效应)，利用输出前缀猜测合理延续而非理解任务\n- **证据**：在路径查找等前瞻任务中，训练准确率100%的模型在测试时完全失败\n\n### 2.5 关键发现：Transformer内部已存在高级表示\n- **现象**：尽管输出是顺序标记流，Transformer内部潜在状态表现出显著复杂性和前瞻性\n- **问题**：NTP限制可能不在于模型表示复杂想法的能力，而在于必须一次一个子词表达它们\n- **证据**：研究发现LLMs内部形成\"大于标记\"的抽象，特定神经元对应于多标记概念\n\n## 3. 形成核心假设：NTP的根本缺陷\n\n基于上述观察，形成核心假设：**NTP的根本问题在于其标记级、顺序生成的本质，而非Transformer架构本身**。\n\n这一假设基于以下推理链：\n1. NTP鼓励短视局部决策，而非全局规划\n2. 子词标记与人类自然思考单位(想法、句子)存在粒度不匹配\n3. 顺序生成导致计算效率低下，无法并行处理\n4. 教师强制训练使模型学习虚假捷径，而非真正理解\n5. Transformer内部已存在更高级别表示，但被约束为一次输出一个标记\n\n## 4. 探索解决方案：超越NTP的新范式\n\n基于核心假设，探索应满足以下目标的新范式：\n- 支持更全局的规划和推理\n- 在更合适的粒度上操作(想法、句子或概念级别)\n- 实现更高效的并行生成\n- 避免教师强制训练的陷阱\n- 充分利用Transformer内部已存在的高级表示\n\n## 5. 提出五类替代方法：从观察到系统化\n\n通过对现有文献的系统回顾，将替代NTP的方法归纳为五大家族：\n\n### 5.1 多标记预测(MTP)\n- **核心思想**：将预测目标从单个标记扩展到一块k个未来标记\n- **逻辑基础**：解决NTP的短视问题，使模型能够\"向前看\"多个步骤\n- **实现方式**：使用共享上下文表示(\"主干\")馈送多个并行输出头\n- **优势**：提高计算效率，鼓励短期规划，部分并行化\n- **局限**：通常只关注短期未来，缺乏真正的全局规划机制\n\n### 5.2 先计划后生成(PtG)\n- **核心思想**：引入高级规划阶段指导后续标记生成\n- **逻辑基础**：人类写作和思考通常先有全局规划再填充细节\n- **实现方式**：两阶段过程——先生成高级抽象计划，再基于计划生成输出序列\n- **优势**：引入全局连贯性，减轻下一个标记解码的短视性\n- **局限**：符号规划器缺乏可扩展性，潜在规划器引入可解释性挑战\n\n### 5.3 潜在推理(LR)\n- **核心思想**：用潜在状态上的自回归替换标记级生成\n- **逻辑基础**：解决子词标记与高级语义之间的粒度不匹配\n- **实现方式**：生成潜在向量序列再解码为文本，或在发出标记前细化潜在状态\n- **优势**：在句子、思想或概念级别操作，减少序列长度\n- **局限**：生成过程不透明，训练复杂，潜在对齐问题\n\n### 5.4 连续生成方法(CG)\n- **核心思想**：通过迭代转换同时初始化和细化整个输出\n- **逻辑基础**：人类创作过程常涉及反复修改和全局优化\n- **实现方式**：定义从噪声到连贯文本的轨迹，通过扩散、流匹配或能量模型实现\n- **优势**：全局规划和修订，减轻错误累积，双向细化\n- **局限**：与自回归模型相比质量差距，推理成本高，固定长度限制\n\n### 5.5 非Transformer架构(NTA)\n- **核心思想**：通过改变模型架构本身绕过NTP\n- **逻辑基础**：不同序列建模机制可能更适合语言本质\n- **实现方式**：使用状态空间模型、联合嵌入预测架构等替代架构\n- **优势**：线性时间复杂度，隐式潜在空间抽象\n- **局限**：难以适应生成任务，表示漂移，评估不匹配\n\n## 6. 形成统一方法论：超越NTP的框架\n\n最终形成统一的方法论框架，将替代NTP的方法组织为五大家族，每类方法包含：\n- 清晰的定义和形式化描述\n- 代表性论文和实现方式\n- 失败模式和局限性分析\n- 讨论和未来研究方向\n\n这一框架不仅帮助理解当前替代NTP的方法，还为研究人员提供了探索语言模型如何超越标记级生成的路线图，朝着能够更像人类一样规划、推理和交流的模型发展。\n\n## 结论：从问题到解决方案的逻辑演进\n\n整个逻辑演进过程展现了从观察到假设再到解决方案的科学研究思维：\n1. **问题识别**：发现NTP范式下的根本性矛盾\n2. **现象分析**：系统观察NTP在多个维度的局限性\n3. **假设形成**：提出NTP的根本问题在于标记级顺序生成的本质\n4. **方向探索**：确定新范式应满足的关键目标\n5. **方法分类**：系统化五类替代方法，每类针对NTP的不同局限\n6. **框架构建**：形成统一的方法论框架，指导未来研究\n\n这一逻辑链不仅解释了当前NTP替代方案的研究现状，也为语言模型的未来发展提供了清晰路径。", "summary_translation": "下一词元预测(Next Token Prediction, NTP)范式推动了大型语言模型(Large Language Models, LLMs)前所未有的成功，但同时也是其最持久弱点的根源，如长期规划能力差、错误累积和计算效率低下。鉴于探索NTP替代方案的兴趣日益增长，本综述描述了NTP替代方案的新兴生态系统。我们将这些方法分为五大类：(1) 多词元预测(Multi-Token Prediction)，目标是预测未来的一组词元而非单个词元；(2) 先规划后生成(Plan-then-Generate)，预先创建全局高级计划以指导词元级解码；(3) 潜在推理(Latent Reasoning)，将自回归过程本身转移到连续潜在空间；(4) 连续生成方法(Continuous Generation Approaches)，通过扩散、流匹配或基于能量的方法，用迭代并行细化替代顺序生成；(5) 非Transformer架构(Non-Transformer Architectures)，通过其固有的模型结构规避NTP。通过综合这些方法的见解，本综述提供了一个分类法，以指导针对词元级生成已知局限性模型的研究，从而为自然语言处理开发新的变革性模型。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#56", "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model", "link": "/arxiv/2509.24389", "arxiv_id": "2509.24389", "authors": "Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen", "summary": "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.467375", "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步：核心判断——这篇论文的本质是提出了一种新的模型架构LLaDA-MoE，结合了Mixture-of-Experts (MoE)架构和扩散模型技术。论文的核心贡献在于提高大语言模型的计算效率（保持7B参数容量但只激活1.4B参数），同时保持或提升其在多种通用任务上的性能。这属于改进LLM的基础能力和架构的研究，符合\"改进LLM的基础能力\"的研究目标。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确提到\"large language diffusion model\"，属于LLMs范畴 - 能力方向：特别提到了模型在\"数学推理\"(mathematical reasoning)方面的表现 - 新兴范式：提到了模型在\"智能体\"(agent)任务上的表现 第三步：排除标准——论文不主要聚焦于任何排除领域： - 虽然提到了\"Diffusion Language Model\"，但这是应用于语言模型的扩散技术，而非视觉或多模态领域的扩散模型 - 没有聚焦于任何特定应用领域（如医疗、化学等） - 没有主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况——论文提到模型在智能体任务上的表现，但主要是评估而非提出新的智能体框架；没有明确讨论幻觉、可解释性或安全性问题。 综合判断：这篇论文的核心是改进LLM的基础架构和效率，同时评估了其在数学推理等通用推理能力上的表现。虽然主要关注点是计算效率，但其目的是提升模型在多种通用任务上的性能，符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。因此，这篇论文应该被保留。", "summary2": "本文旨在解决扩散语言模型的计算效率问题。针对大规模语言模型训练和推理的高计算开销场景，我们提出了一种基于稀疏MoE架构的扩散语言模型LLaDA-MoE，并在多个基准测试上通过准确率、性能比较等指标验证了其有效性。该模型在推理时仅激活1.4B参数，却超越了之前的8B参数扩散语言模型，并实现了与Qwen2.5-3B-Instruct相当的性能。", "inspiration_trace": "# LLaDA-MoE核心方法的逻辑推演\n\n## 宏观问题：语言模型的效率与性能平衡\n\n大型语言模型(LLMs)快速发展并广泛应用于各类任务，但面临一个核心挑战：如何在保持强大性能的同时提高计算效率？当前主流的自回归(AR)模型虽然表现出色，但计算开销巨大，尤其是随着模型规模增长。这引出一个关键问题：是否存在一种既能保持高性能又能提高计算效率的语言模型范式？\n\n## 观察一：扩散语言模型的潜力\n\n作者首先观察到掩码扩散模型(MDMs)作为一种替代性建模范式正受到关注。MDMs通过迭代细化部分掩码序列来生成文本，与AR模型相比具有相当的扩展性和性能。最新研究表明，如LLaDA等8B参数的MDMs已能媲美LLaMA3 8B的性能，证明扩散语言建模是一个有前景的方向。\n\n**关键观察**：MDMs提供了与AR模型不同的技术路径，但现有MDMs主要依赖密集的Transformer架构，计算效率仍有提升空间。\n\n## 观察二：MoE架构在AR模型中的成功\n\n作者进一步观察到，稀疏专家混合(MoE)架构在AR模型中已得到广泛验证。MoE通过为每个token只激活一小部分参数，实现了与更大密集模型相当的性能，同时大幅降低了计算开销。这种\"稀疏激活\"机制为解决语言模型的效率问题提供了有效途径。\n\n**关键观察**：MoE架构在AR模型中已证明能有效平衡性能与效率，但在MDMs领域尚未探索。\n\n## 假设：MoE与MDMs的结合潜力\n\n基于以上观察，作者提出核心假设：将稀疏MoE架构整合到掩码扩散语言模型的训练目标中，可以在保持高效推理（激活少量参数）的同时，发挥MoE的优势，从而实现更高效的MDMs。\n\n这一假设建立在两个关键洞见之上：\n1. MDMs的迭代掩码预测机制与MoE的稀疏激活机制可能存在天然契合点\n2. MoE的参数效率优势可能使MDMs在更大规模上更具可行性\n\n## 方法论形成：LLaDA-MoE的设计\n\n为验证这一假设，作者设计了LLaDA-MoE模型，具体方法论形成过程如下：\n\n### 1. 架构设计决策\n\n**决策点**：如何在MDMs中有效整合MoE架构？\n- 采用稀疏MoE架构，总参数量7B，但推理时仅激活1.4B参数\n- 设置64个专家网络，每个token通过路由机制选择top-8专家\n- 保留MDMs的核心组件（双向注意力、掩码预测），同时用MoE替换密集前馈网络\n\n**创新点**：首次将MoE架构与扩散语言模型结合，实现\"稀疏激活的扩散建模\"。\n\n### 2. 训练策略优化\n\n**决策点**：如何确保MoE-MDMs的有效训练？\n- 设计多阶段训练流程：预训练(20T tokens)→退火(1T tokens)→SFT\n- 在预训练阶段2增加数学和代码数据的比例，提升模型在这些领域能力\n- 采用变量长度训练(1%步骤)减少训练-推理分布不匹配\n- 引入辅助损失函数(L_Z和L_LB)解决MoE训练中的专家负载不平衡问题\n\n**创新点**：针对MoE-MDMs的特性定制训练策略，确保稳定收敛和性能优化。\n\n### 3. 推理机制设计\n\n**决策点**：如何在保持MDMs特性的同时实现高效推理？\n- 采用标准MDMs的迭代去掩码过程\n- 引入半自回归采样策略，将序列分块并行处理\n- 实现低置信度重掩码机制，提高生成质量\n\n**创新点**：结合MoE的稀疏激活与MDMs的迭代生成，实现参数高效的推理。\n\n## 实验验证与结论\n\n通过在多个基准测试上的评估，作者验证了LLaDA-MoE的有效性：\n1. 仅激活1.4B参数的情况下，超越了之前的8B参数扩散语言模型\n2. 经过指令微调后，性能与Qwen2.5-3B-Instruct相当，尽管激活参数更少\n3. 证明了稀疏MoE架构是构建更高效MDMs的可行路径\n\n这一研究不仅验证了初始假设，还为扩散语言模型开辟了新的设计空间，展示了如何通过架构创新实现语言模型的效率与性能平衡。", "summary_translation": "我们介绍了LLaDA-MoE，这是一种采用Mixture-of-Experts (MoE，专家混合)架构的大型语言扩散模型(language diffusion model)，该模型从零开始在大约20T个token（标记）上进行了训练。LLaDA-MoE在保持70亿参数容量的同时，在推理(inference)过程中仅激活14亿参数，从而显著降低了计算开销(computational overhead)，实现了具有竞争力的性能。我们的实证评估(empirical evaluation)表明，LLaDA-MoE在具有更大参数的扩散语言模型中实现了最先进的(state-of-the-art)性能，在多个基准测试(benchmark)中超越了之前的扩散语言模型LLaDA、LLaDA 1.5和Dream。经过指令微调(instruct-tuned)的模型LLaDA-MoE-7B-A1B-Instruct在知识理解、代码生成、数学推理、智能体(agent)和对齐(alignment)任务上展现出与Qwen2.5-3B-Instruct相当的能力，尽管使用了更少的激活参数。我们的研究结果表明，将稀疏MoE架构(sparse MoE architecture)整合到掩码扩散语言模型(masked diffusion language model)的训练目标中，仍然能在少量激活参数的高效推理条件下发挥MoE的优势，并为扩散语言模型的进一步探索开辟了广阔空间。LLaDA-MoE模型已在Huggingface上发布。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#59", "title": "Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining", "link": "/arxiv/2509.24356", "arxiv_id": "2509.24356", "authors": "Matthew Theodore Roque, Dan John Velasco", "summary": "Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.468996", "filter_reason": "这篇论文的核心是研究在数据受限情况下如何通过课程学习和文本简化来优化语言模型的预训练过程，提高模型的表示质量和性能。根据筛选标准的第一步，这属于改进LLM基础能力的研究，提出了新的训练范式（课程学习结合文本简化），旨在增强模型的语言理解、常识推理等通用能力。论文评估了模型在\"常识推理\"等能力上的表现，虽然不是直接针对推理能力的研究，但其方法可以增强模型的通用推理能力。论文没有涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。", "summary2": "本文旨在解决数据受限预训练环境中的优化问题。针对有限预训练数据场景，我们提出了一种结合文本简化和课程学习的预训练方法，并在人类撰写段落与LLM简化版本组成的平行语料库上，通过微调性能和零样本任务准确率验证了其有效性。", "inspiration_trace": "# 从宏观问题到方法创新：文本简化与课程学习的逻辑推演\n\n## 1. 宏观问题观察：数据受限预训练的挑战\n\n**起点：现有研究的盲区**\n- 作者观察到，大多数语言模型预训练研究聚焦于大规模数据集场景\n- 现实中，预训练常面临数据受限环境，不可避免需要重复使用相同语料库\n- 在数据受限条件下，两个关键因素被忽视：(1)训练数据呈现顺序；(2)同一文本的简化版本使用\n\n**核心问题提出**：在数据有限的情况下，如何优化预训练效果，超越简单的数据重复？\n\n## 2. 问题聚焦：两个关键假设\n\n**假设1：文本简化优于重复暴露**\n- 推理：重复相同数据可能导致收益递减，而简化版本可能提供新的语言模式\n- 问题：在数据受限环境中，用简化文本替代重复暴露是否能提高表示质量？\n\n**假设2：文本复杂度排序影响学习效果**\n- 推理：人类学习遵循从简单到复杂的课程，机器学习可能类似\n- 问题：按文本复杂度排序（简单到复杂vs复杂到简单vs混合）是否会产生更好的表示？\n\n## 3. 方法构建：从理论到实验设计\n\n**理论基础构建**：\n- 借鉴课程学习理论(Bengio et al., 2009)：逐渐增加复杂度的训练顺序可能提升学习效率\n- 整合文本简化研究：简化文本保留语义内容但降低表面复杂度\n\n**实验设计**：\n- 数据基础：构建平行语料库，人工撰写段落(HW)与LLM简化版本(SIMP)对齐\n- 变量控制：保持架构、分词器、上下文长度、优化器等变量恒定，仅改变数据顺序\n- 四种数据调度策略：\n  1. BASELINE：两轮HW（模拟数据受限场景）\n  2. INTERLEAVED：HW和SIMP均匀混合\n  3. SIMP→HW：先简化后原始文本（标准课程）\n  4. HW→SIMP：先原始后简化文本（反课程）\n\n## 4. 验证框架：多维度评估\n\n**评估维度设计**：\n- 微调评估：测试表示质量在下游任务中的迁移能力\n- 零样本评估：测试模型在未见任务上的泛化能力\n- 样本效率分析：通过不同规模的微调数据集测试预训练效果\n\n**任务选择**：\n- 语言知识：BLiMP、BLiMP Supplement\n- 实体跟踪：Entity Tracking\n- 世界知识：MMLU、EWoK\n- 常识推理：ARC、HellaSwag、PIQA等\n\n## 5. 结果发现与理论修正\n\n**假设1验证**：\n- 发现：简化数据确实优于重复原始数据\n- 修正：效果与模型规模相关，124M模型有适度提升，256M模型提升更明显\n- 解释：更大模型能更好吸收释义变化，混合暴露产生一致改进\n\n**假设2验证**：\n- 发现：课程效果依赖于模型规模\n- 修正：较小模型(124M)受益于简单到复杂课程(SIMP→HW)，较大模型(256M)偏好平衡暴露(INTERLEAVED)\n- 解释：较小模型需要渐进式学习，而较大模型能同时处理两种复杂度\n\n## 6. 方法论贡献：数据受限预训练的新路径\n\n**核心贡献**：\n- 提供首个关于文本简化和课程调度在数据受限预训练中交互作用的对照研究\n- 证明在数据有限情况下，文本简化可作为重复暴露的有效补充\n- 揭示模型规模是选择最佳数据调度策略的关键因素\n\n**实践意义**：\n- 为数据受限环境提供实用优化方法，无需收集新数据\n- 提供针对不同模型规模的预训练策略指导\n- 开辟数据增强新方向：通过文本变换而非简单重复扩展数据效用\n\n这一逻辑链条从宏观问题出发，通过观察现有研究盲区，聚焦具体假设，设计严谨实验，验证并修正理论，最终形成针对数据受限预训练的创新方法论，体现了从问题到解决方案的系统性思考过程。", "summary_translation": "大多数关于语言模型预训练（language model pretraining）的研究都集中在大型数据集上，而在数据受限（data-constrained）环境中的优化问题仍存在未解之谜。在这些情况下，训练数据顺序（training data order）的影响以及包含同一文本的替代版本（alternative versions）的效果仍未得到充分探索。我们通过研究预训练中的课程学习（curriculum learning）来解决这一问题，重点关注文本复杂度排序（text-complexity ordering）和通过简化（simplification）进行数据增强（data augmentation）。我们提出以下问题：(1) 简化文本是否比重复使用原始数据更能提高表示质量（representation quality）？(2) 按文本复杂度排序数据是否能产生更好的表示？为回答这些问题，我们基于一组平行语料库（parallel corpora），其中人工撰写的段落与大型语言模型（LLM）简化的变体对齐，并测试了四种数据调度（data schedules）方式：重复暴露（repeated exposure）、从低到高复杂度（low-to-high complexity）、从高到低复杂度（high-to-low）和交错式（interleaved）。我们通过微调（fine-tuning）从样本效率（sample efficiency）角度分析模型的表示质量，以及其在语言知识（linguistic knowledge）、实体跟踪（entity tracking）、世界知识（world knowledge）和常识推理（commonsense reasoning）方面的零样本（zero-shot）性能。我们的研究结果表明，添加简化数据相比重复暴露基线（repeated-exposure baseline）提高了微调和零样本性能：较小模型受益于从低到高复杂度的排序，而较大模型在交错式排序下表现更好。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#58", "title": "Reinforcement Mid-Training", "link": "/arxiv/2509.24375", "arxiv_id": "2509.24375", "authors": "Yijun Tian, Shaoyu Chen, Zhichao Xu, Yawei Wang, Jinhe Bi, Peng Han, Wei Wang", "summary": "The development of state-of-the-art large language models is commonly understood as a two-stage process involving pre-training and post-training. We point out the need for an additional intermediate stage called reinforcement mid-training with potential for strong performance gains. In this paper, we formally define the problem and identify three key challenges: (1) inefficient training due to excessive reasoning steps, (2) disregard of the imbalanced token entropy distribution, and (3) underutilization of token information. To address these challenges, we propose RMT, a framework for efficient, adaptive, and unified reinforcement mid-training with various innovative components. In particular, we first introduce a dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking. Next, we design a curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens. Finally, we present a dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information. Extensive experiments demonstrate the superiority of RMT over state-of-the-art methods, achieving up to +64.91% performance improvement with only 21% of the reasoning length in language modeling. We also show that checkpoints obtained after reinforcement mid-training can benefit the subsequent post-training, yielding up to +18.76% improvement in the mathematical domain.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.468506", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"强化中训练\"(RMT)的新训练范式，用于提高大语言模型的推理能力和训练效率。从核心判断来看，论文本质上是改进LLM的基础能力，特别是通过动态token预算机制减少不必要的推理步骤，通过基于课程的自适应采样方法促进从易到难的学习轨迹，以及通过结合强化学习和下一个token预测的双重训练策略来充分利用token信息。这些方法都是直接提升LLM的通用推理能力，而不是将LLM作为工具应用到特定领域。论文在数学领域展示了改进，但这只是作为通用推理能力的一个示例，而不是论文的主要焦点。从正面指标来看，论文包含了LLM核心概念、推理能力（特别是数学推理）和强化学习方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合研究目标，即筛选出那些致力于提高大语言模型本身的通用推理能力的论文。", "summary2": "本文旨在 [解决大型语言模型训练过程中的效率问题]。针对 [预训练和后训练之间的中间阶段]，我们提出了一种 [强化中训练框架RMT，包含动态token预算和自适应采样策略]，并在 [语言建模任务] 上通过 [准确率和推理长度] 验证了其有效性。", "inspiration_trace": "# 从观察到创新：RMT框架的逻辑演进\n\n## 1. 宏观问题：LLM训练流程的优化空间\n\n作者首先观察到大型语言模型(LLM)的发展通常被简化为两阶段过程：预训练(pre-training)和后训练(post-training)。预训练赋予模型基础知识和语言能力，后训练则使模型与人类目标对齐。然而，这种二分法可能忽略了模型能力发展的连续性和中间阶段的优化空间。\n\n## 2. 关键发现：被忽视的中间阶段\n\n通过分析现有研究，作者发现了一个被忽视的中间阶段——中期训练(mid-training)。这一阶段利用预训练数据，但采用比一般预训练更有针对性的目标，系统性地增强模型在特定领域(如数学推理)的能力，为后续后训练做更好准备。与后训练不同，中期训练不依赖带标签或奖励信号的领域特定数据，而是利用无标签预训练数据。\n\n## 3. 现有方法的局限性分析\n\n作者深入分析了现有强化中期训练方法(如RPT)面临的三个关键挑战：\n\n### 挑战1：低效训练与过度思考\n- **观察**：现有方法生成过长的推理链，导致计算开销剧增\n- **问题**：如图1(a)所示，现有方法需要显著更多token进行推理，却不能保证性能提升\n\n### 挑战2：忽略不平衡的token熵分布\n- **观察**：token的熵值(不确定性和学习难度)各不相同，但现有方法不加区分地采样\n- **问题**：在训练早期，模型能力不足时过度暴露于高难度token，导致学习不稳定\n\n### 挑战3：token信息利用不足\n- **观察**：如图1(b)所示，大多数token表现出低熵，但现有方法只关注高熵token\n- **问题**：忽略大量低熵token导致信息损失和错失学习机会\n\n## 4. 核心假设形成\n\n基于上述观察，作者形成了核心假设：\n- 在预训练和后训练之间引入专门的中期训练阶段能显著提升LLM性能\n- 通过解决三个关键挑战，可设计出高效、自适应和统一的强化中期训练框架\n\n## 5. 解决方案设计：RMT框架\n\n为验证假设并解决三个挑战，作者设计了RMT框架，包含三个创新组件：\n\n### 组件1：动态token预算机制（解决挑战1）\n- **设计思路**：引入随训练进度衰减的token预算，约束推理长度\n- **实现**：Bt = max(Bmin, ⌊B0 · γ^(t/T)⌋)，配合三角形长度奖励函数\n- **预期效果**：使模型随能力成熟自适应减少推理开销，提高效率\n\n### 组件2：基于课程的自适应采样（解决挑战2）\n- **设计思路**：根据token熵值分类难度，动态调整采样概率\n- **实现**：将token分为简单、中等、困难三类，通过分段线性插值实现课程调度\n- **预期效果**：从易到难渐进学习，确保训练稳定性\n\n### 组件3：双重训练策略（解决挑战3）\n- **设计思路**：结合token选择性强化学习与token包容性下一token预测\n- **实现**：对高熵token应用强化学习，对低熵token应用下一token预测，统一损失函数\n- **预期效果**：兼顾关键token的针对性学习和所有token信息的充分利用\n\n## 6. 实验验证与结果\n\n作者通过多维度实验验证RMT框架：\n\n### 语言建模任务\n- **结果**：RMT-Q3实现+64.91%性能提升，推理长度仅为基线的21%\n\n### 持续后训练\n- **结果**：RMT检查点在后续后训练中实现+18.76%性能提升\n\n### 效率分析\n- **结果**：响应长度显著减少，生成时间大幅降低\n\n### 消融研究\n- **结果**：各组件均有贡献，移除任一组件都会降低性能\n\n## 7. 结论与意义\n\nRMT框架成功解决了强化中期训练中的关键挑战，实现了高效、自适应和统一的训练。这一工作不仅为LLM生命周期引入了有价值的新阶段，也为未来研究开辟了新方向。作者通过系统性的观察、假设形成、解决方案设计和实验验证，构建了一个完整的逻辑链条，展示了从问题发现到方法创新的科学思维过程。", "summary_translation": "# 中文翻译\n\n最先进的大型语言模型（state-of-the-art large language models）的发展通常被理解为一个包含预训练（pre-training）和后训练（post-training）的两阶段过程。我们指出需要增加一个称为强化中间训练（reinforcement mid-training）的额外中间阶段，该阶段具有强大的性能提升潜力。在本文中，我们正式定义了该问题并确定了三个关键挑战：(1) 由于推理步骤过多导致的训练效率低下，(2) 忽略了不平衡的令牌熵（token entropy）分布，以及 (3) 令牌（token）信息的利用不足。\n\n为应对这些挑战，我们提出了RMT，一个高效、自适应且统一的强化中间训练框架，包含多种创新组件。具体而言，我们首先引入了一种动态令牌预算（dynamic token budget）机制，该机制限制了不必要的推理步骤并减轻了模型过度思考（overthinking）问题。接下来，我们设计了一种基于课程的自适应采样（curriculum-based adaptive sampling）方法，该方法促进了从简单到复杂令牌的渐进式学习轨迹。最后，我们提出了一种双重训练（dual training）策略，将强化学习（reinforcement learning）与下一令牌预测（next-token prediction）相结合，确保对关键令牌进行有针对性的学习并充分利用所有令牌信息。\n\n大量实验证明了RMT相对于最先进方法的优越性，在语言建模中仅用21%的推理长度就实现了高达+64.91%的性能提升。我们还表明，在强化中间训练后获得的检查点（checkpoints）可以使后续的后训练受益，在数学领域带来高达+18.76%的改进。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#70", "title": "Prompt and Parameter Co-Optimization for Large Language Models", "link": "/arxiv/2509.24245", "arxiv_id": "2509.24245", "authors": "Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong", "summary": "Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.479979", "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文本质上是关于改进LLM基础能力的研究，提出了一种名为MetaTuner的新框架，联合优化提示和参数来提升LLM性能。这是一种新的训练范式，属于增强LLM通用能力的方法论研究，而非将LLM作为工具应用于特定领域。 从正面指标看，论文明确关注\"Large Language Models (LLMs)\"这一核心概念；虽然未直接提及reasoning等词，但提示优化和参数微调都是提高LLM通用推理能力的方法；同时，论文提出了一种创新的训练方法，符合训练方法这一正面指标。 从排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性等应排除的内容。 论文的核心贡献是提出了一种联合优化提示和参数的新框架，通过两个神经网络分别生成提示和参数，并允许它们共享底层编码层以实现知识共享。这种方法通过结合提示优化（显式自然语言）和微调（隐式参数更新）两种互补的方法，提升了LLM的通用能力，因此完全符合我筛选\"致力于提高大语言模型通用推理能力\"论文的研究目标。", "summary2": "本文旨在解决大语言模型中提示优化和微调被独立研究而忽略协同潜力的问题。针对提示优化（离散优化）和微调（连续优化）的互补特性，我们提出了一种MetaTuner框架，通过共享元编码器联合生成提示和参数，并设计监督正则化损失解决混合优化挑战。在MATH、GSM8K、HotpotQA和CosmosQA四个基准测试上，通过准确率等指标验证了其有效性，实验结果表明MetaTuner始终优于现有基线方法。", "inspiration_trace": "# MetaTuner方法论逻辑演进分析\n\n## 一、宏观问题：LLM性能优化的二元路径\n\n论文从大语言模型(LLMs)性能优化的宏观视角出发，观察到存在两条并行但分离的研究路径：\n- **提示优化(Prompt Optimization)**：通过显式自然语言引导模型行为\n- **参数微调(Fine-tuning)**：通过隐式参数更新调整模型内部表示\n\n这两种方法从互补角度提升LLM能力，但学术界通常孤立研究它们，忽视了潜在的协同效应。\n\n## 二、关键观察：各自局限性与相互依赖\n\n通过初步实验(图1)，作者发现两个关键现象：\n\n1. **性能差异**：微调方法在MATH和HotpotQA数据集上平均表现优于提示优化策略\n2. **敏感性依赖**：微调效果高度依赖于所选提示，次优提示会导致性能显著下降，甚至低于纯提示优化方法\n\n这揭示了核心问题：两种方法各自存在明显局限性——提示优化难以适应复杂任务数据模式，而微调则受限于手动设计的提示可能不是最优的。\n\n## 三、核心假设：协同优化的可能性\n\n基于观察，作者提出核心假设：\n> 提示和参数可视为两个互补维度，共同影响从输入到输出的预测过程。通过统一框架优化它们，可以找到最佳组合，相互弥补弱点。\n\n这一假设引出了统一优化目标：\n```\nmin θ,pi Σ L(Mθ(pi, xi), yi)\n```\n其中提示被视为\"特殊参数\"，与模型参数共同优化。\n\n## 四、面临挑战：混合优化难题\n\n实现上述假设面临三大挑战：\n\n1. **机制差异**：提示优化是外部优化(寻找合适输入上下文)，微调是内部优化(修改参数拟合数据分布)\n2. **空间异质**：提示优化是离散优化问题，微调在连续参数空间操作\n3. **梯度障碍**：离散-连续混合优化导致非可微梯度问题，难以端到端训练\n\n## 五、解决方案：MetaTuner框架设计\n\n为解决上述挑战，作者提出MetaTuner框架，其核心设计包含三个关键创新：\n\n### 1. 提示生成连续化\n- **思路**：用神经网络生成提示，将离散优化转化为连续任务\n- **实现**：从初始提示出发，利用LLM重写生成优化提示\n- **公式化**：`pi = Gφ(˜p, xi)`，将离散问题转化为连续参数优化\n\n### 2. 联合生成架构\n- **思路**：设计共享-私有参数结构，实现提示和参数的协同生成\n- **实现**：引入共享元编码器，连接提示解码器和参数解码器\n- **公式化**：`min φs,φp,φq Σ L(MF(φs,φq)(˜p,xi)(G(φs,φp)(˜p, xi), xi), yi)`\n- **优势**：共享参数实现知识共享和相互正则化，私有参数保持各自灵活性\n\n### 3. 监督正则化优化\n- **思路**：设计特殊损失函数解决离散-连续混合优化问题\n- **实现**：将目标分为两项——固定提示参数的任务损失和提示生成的监督正则化损失\n- **公式化**：\n  ```\n  min φs,φp,φq Σ L(MF(φs,φq)(˜p,xi)(G(φs,φ'p)(˜p, xi), xi), yi) \n               + Σ α·L(G(φs,φp)(˜p, xi), pi)\n  ```\n- **优势**：避免Gumbel-Softmax等近似方法带来的梯度偏差，直接在离散空间优化\n\n## 六、实验验证：从理论到实践\n\n作者通过系统实验验证MetaTuner的有效性：\n\n1. **整体性能**：在四个数据集上显著优于基线方法，证明联合优化的优势\n2. **消融研究**：验证各组件贡献，确认共享结构和监督正则化的必要性\n3. **参数共享分析**：探索不同共享比例对性能的影响，发现模型大小与最佳共享比例相关\n4. **监督正则化分析**：确认优于Gumbel-Softmax等替代方案，并研究关键超参数影响\n5. **泛化能力**：证明在未见数据集上的优越表现，验证方法的鲁棒性\n\n## 七、逻辑演进总结\n\nMetaTuner的提出体现了清晰的逻辑演进链：\n```\n宏观问题(二元路径分离) → 关键观察(各自局限) → 核心假设(协同可能) → \n面临挑战(混合优化) → 解决方案(三重创新) → 实验验证(系统评估)\n```\n\n这一演进过程展示了作者从现象观察到理论假设，再到方法创新和实验验证的完整科学思维路径，成功地将提示优化和微调两个分离的研究方向统一到一个框架中，实现了\"1+1>2\"的协同效果。", "summary_translation": "提示优化(prompt optimization)和微调(fine-tuning)是提升大型语言模型(Large Language Models, LLMs)性能的两种主要方法。它们从互补的角度增强LLMs的能力：前者通过显式的自然语言，后者通过隐式的参数更新。然而，先前的研究通常孤立地研究这两种方法，导致它们的协同潜力在很大程度上未被充分探索。为了弥合这一差距，在本文中，我们提出了MetaTuner，这是一种联合集成提示优化和微调用于LLMs训练的新型框架。具体而言，我们引入了两个神经网络分别生成提示(prompts)和参数(parameters)，同时允许它们共享一个公共的底层编码层(bottom encoding layer)以实现知识共享。在最终监督信号(supervised signals)的引导下，我们的框架被优化以发现提示和参数之间的最佳组合。考虑到提示学习(prompt learning)涉及离散优化，而微调在连续参数空间(continuous parameter space)中操作，我们设计了一种监督正则化损失(supervised regularization loss)来有效训练我们的框架。在多个不同基准测试(benchmarks)上的大量实验表明，我们的方法始终优于基线方法(baselines)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#80", "title": "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight", "link": "/arxiv/2509.24169", "arxiv_id": "2509.24169", "authors": "Haolin Yang, Hakaze Cho, Kaize Ding, Naoya Inoue", "summary": "Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of \"key heads\" most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.506873", "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)机制，并提出了一种新的方法——学习任务向量(LTVs)来增强这一能力。从本质上看，论文关注的是改进LLM的基础能力而非将其作为工具应用到特定领域，这符合第一步的核心判断标准。论文明确研究LLMs的内部工作机制，提出了直接训练任务向量的新范式，而不是从模型中提取，这可以被视为一种新的训练方法来增强LLM的能力。虽然论文没有直接针对推理或规划，但上下文学习是LLM的一项基础能力，与通用问题解决相关。论文深入分析了任务向量如何通过注意力头的OV电路引导预测，以及它们在Transformer中的传播方式，这种机制层面的理解可能对改进LLM的通用推理能力有重要启示。论文不符合任何排除标准，没有涉及多模态、特定应用领域或应用层面的模型可靠性。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究范围。", "summary2": "本文旨在解决大型语言模型中任务向量(TVs)提取方法复杂低效且缺乏机制解释的问题。针对多种分类和生成任务场景，我们提出了一种直接训练学习任务向量(LTVs)的方法，并在七个数据集上通过准确率等指标验证了其有效性。实验表明，LTVs不仅优于提取的TVs，还揭示了TVs主要通过注意力头OV电路影响预测，以及其在模型中传播的线性机制。", "inspiration_trace": "# 从观察到创新：任务向量学习方法的逻辑演进\n\n## 1. 宏观问题：大型语言模型如何实现上下文学习？\n\n**观察现象**：大型语言模型(LLMs)展现出一种非凡能力——仅通过输入中提供的演示示例就能学习并执行新任务，这一现象被称为上下文学习(ICL)。这与传统机器学习范式形成鲜明对比，因为模型没有经过显式训练就能适应新任务。\n\n**核心疑问**：ICL的内部机制是什么？LLMs如何从演示示例中提取信息并用于新查询的预测？\n\n## 2. 中观聚焦：任务向量在ICL中的角色\n\n**现有发现**：先前研究(Hendel et al., 2023)提出，LLMs利用演示的方式是将它们压缩成任务向量(TVs)——演示所示例任务的简洁向量表示。这些TVs可以被注入到零样本提示的隐藏状态中，以实现ICL级别的性能。\n\n**识别局限**：现有TV研究存在两个关键缺陷：\n1. **方法效率低下**：现有方法依赖于复杂的过滤或优化来从模型表示中构建TVs，过程繁琐且不透明\n2. **机制理解不足**：大多数研究仅表明注入TVs提高了性能，但未阐明LLMs如何利用TVs做出正确预测的核心机制\n\n## 3. 具体问题：如何改进TV获取并揭示其工作机制？\n\n**方法分析**：现有TV提取方法存在明显不足：\n- **Vanilla TV**：定义为ICL与零样本提示的隐藏状态差异，准确性低且对注入层选择高度敏感\n- **Function Vector (FV)**：从选定的注意力头输出中构建，依赖于头选择过程，忽略了头间相互关联\n\n**关键假设**：\n1. 如果直接训练TVs而非从模型表示中提取，可能会获得更有效的TVs，不受模型表示质量限制\n2. 通过系统分析TVs与模型组件的相互作用，可以揭示TVs有效性的工作机制\n\n## 4. 方法创新：学习任务向量(LTVs)\n\n**核心思路**：摒弃提取范式，转而直接训练TVs。具体方法是将向量θ添加到特定层的隐藏状态，并通过梯度下降优化它，最小化预测损失：-log p(yq|xq, θ, L, P)\n\n**突破优势**：\n- 消除了对ICL隐藏状态的依赖，不受表示质量限制\n- 注入位置和层更加灵活，不限于最后位置或单层\n- 可同时训练和注入多个TVs，提高性能上限\n\n## 5. 机制探索：TVs如何影响模型计算？\n\n### 低层次机制\n**假设**：TVs主要通过注意力头的OV电路影响预测。\n\n**验证方法**：\n1. 通过OV电路重建TV效果：计算TV与所有后续层注意力头OV电路的乘积和，将其作为重建TV注入\n2. 识别关键注意力头：计算各头显著性分数，消融关键头与随机头对比\n\n**发现**：注入的TVs主要通过注意力头的OV电路被利用，一小部分\"关键头\"最具决定性，这些头主要分布在注入层后和最后层，且较少受注意力下沉影响。\n\n### 高层次机制\n**假设**：尽管Transformer存在非线性，TV传播可能主要是线性的。\n\n**验证方法**：分析TV注入后隐藏状态的层动态，跟踪三个指标：\n1. Logit Lens准确率：中间层隐藏状态解码准确率\n2. Logit差异：正确与错误标签间的logit差距\n3. 任务对齐：隐藏状态与标签unembeddings的平均余弦相似度\n\n**发现**：TV传播确实主要是线性的，可分解为旋转和缩放两个操作：\n- 早期TVs：被旋转到与任务相关子空间对齐，提高相关标签的logits\n- 后期TVs：主要在幅度上被缩放，旋转效应减弱\n\n## 6. 统一理论：旋转-缩放动态\n\n**综合理解**：TVs的工作机制可统一为旋转-缩放动态：\n- 在低层次，TVs通过注意力头OV电路实现其效果\n- 在高层次，后续层对TVs进行线性变换，早期以旋转为主，后期以缩放为主\n- 这种动态为不同深度的TVs如何塑造最终预测提供了统一解释\n\n**理论意义**：LTVs不仅提供了获取有效TVs的实用方法，还为理解ICL的机制基础提供了原则性视角。\n\n这一逻辑链展示了从观察ICL现象，到关注TVs作用，发现现有方法局限，提出创新方法，最终深入揭示机制的完整思考过程，体现了科学研究从现象到本质的演进路径。", "summary_translation": "大型语言模型 (Large Language Models, LLMs) 能够通过上下文示例来执行新任务，这种现象被称为上下文学习 (in-context learning, ICL)。最近的研究表明，这些示例被压缩成任务向量 (task vectors, TVs)，这是LLMs用于预测的紧凑任务表示。然而，先前的研究通常使用繁琐且不透明的方法从模型输出或隐藏状态中提取TVs，并且很少阐明TVs影响计算的机制。在这项工作中，我们解决了这两个局限性。首先，我们提出直接训练学习型任务向量 (Learned Task Vectors, LTVs)，它们在准确性上超过提取的TVs，并表现出卓越的灵活性——能够在任意层、位置甚至与ICL提示一起有效工作。其次，通过系统分析，我们研究了TVs的机制作用，表明在低层次上，它们主要通过注意力头OV电路 (attention-head OV circuits) 来引导预测，其中一小部分\"关键头\" (key heads) 最具决定性。在更高层次上，我们发现尽管Transformer (Transformer) 存在非线性，TV的传播在很大程度上是线性的：早期的TV被旋转到与任务相关的子空间以提高相关标签的logits值 (logits)，而后期的TV主要在幅度上进行缩放。总而言之，LTVs不仅为获取有效的TVs提供了实用方法，还为了解ICL的机制基础提供了一个有原则的视角。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#76", "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "link": "/arxiv/2509.24193", "arxiv_id": "2509.24193", "authors": "Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang", "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.", "subjects": "Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.488866", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是改进LLM的基础推理能力。AceSearcher提出了一个合作式自我对弈框架，训练单个大语言模型在分解器和求解器两个角色间切换，这是一种新的训练范式，旨在增强LLM的复杂推理能力。论文结合了监督微调和强化微调，优化最终答案准确性，这些都是提升LLM通用推理能力的方法论研究。 其次，论文包含多个正面指标主题： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：专注于推理能力(reasoning)，特别是复杂推理任务和多步推理 - 训练方法：使用强化学习(reinforcement learning)和自我对弈(self-play)框架 - 新兴范式：提出了类似智能体的框架，让同一模型在不同角色间切换 第三，论文不主要聚焦于任何排除标准中的领域。虽然提到了在金融推理任务上的实验，但这只是作为评估模型性能的一个应用场景，而非论文核心焦点。论文的核心是提出一种通用的推理和搜索增强框架。 最后，在特殊和模糊情况处理上，论文提出的自我对弈框架可以视为一种通用的智能体框架，用于增强LLM的通用问题解决能力，而非应用在特定领域，因此符合保留标准。 综上所述，AceSearcher论文的核心贡献是提出了一种通过强化自我对弈来增强LLM推理和搜索能力的通用框架，完全符合\"提高大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在解决搜索增强型LLM在复杂推理任务中多跳检索效率低且推理能力有限的问题。针对复杂推理场景，我们提出了一种AceSearcher协作式自我博弈框架，训练单个LLM在分解器和求解器两个角色间切换，并通过两阶段微调（SFT和RFT）优化模型。在10个数据集上的三个推理密集型任务中，通过精确匹配等指标验证了其有效性，平均提升7.6%，且AceSearcher-32B以不到DeepSeek-V3 5%的参数达到相当性能。", "inspiration_trace": "# AceSearcher核心方法的逻辑推演\n\n## 1. 宏观问题：搜索增强LLMs的复杂推理困境\n\n作者从观察到一个核心问题出发：**现有搜索增强大型语言模型(LLMs)在复杂推理任务上表现不佳**。具体表现为：\n- 难以有效进行多跳检索(multi-hop retrieval)\n- 在需要整合多段信息进行推理的任务上能力有限\n- 现有方法大多局限于简单问题，无法处理真实世界中的复杂推理需求\n\n## 2. 现有方法的局限性分析\n\n作者系统分析了三类主流方法的缺陷：\n\n### 2.1 传统RAG方法的局限\n- **单步检索限制**：大多数RAG系统仅考虑单步检索，无法处理需要多跳信息收集的复杂问题\n- **简单问题偏向**：主要设计用于可通过单次检索解决的简单问题\n- **长尾知识处理不足**：在处理长尾或动态变化的知识时表现不佳\n\n### 2.2 多步搜索方法的局限\n- **依赖强大闭源模型**：迭代提示方法通常需要强大的闭源LLMs作为基础\n- **推理延迟增加**：树搜索算法虽能提高效果，但显著增加了推理时间\n- **推理能力要求高**：这些方法假设模型已具备强推理能力，但未专门针对此进行优化\n\n### 2.3 强化学习方法的局限\n- **内存密集型**：现有RL框架内存消耗大，不适合资源受限环境部署\n- **数据依赖性强**：过度依赖QA数据集进行监督，限制了在更广泛任务上的应用\n- **中间标注需求**：通常需要中间步骤的标注，获取成本高\n\n## 3. 核心假设与思路形成\n\n基于上述分析，作者提出了三个关键假设：\n\n### 3.1 人类问题解决策略假设\n**核心假设**：人类解决复杂问题的策略——将复杂任务分解为更简单的子问题——可以应用于LLMs。\n\n作者观察到人类在面对复杂问题时，会自然地将问题分解为更小、更易管理的子问题，然后逐一解决。这种策略被认为是解决复杂推理的有效方法。\n\n### 3.2 分解质量与答案质量关联假设\n**关键洞见**：更好的问题分解会导致更准确的答案。\n\n作者假设，如果能够有效地将复杂问题分解为合适的子问题，那么基于这些子问题检索到的相关信息将更加相关和全面，从而提高最终答案的准确性。\n\n### 3.3 单一模型多角色协同假设\n**创新思路**：单一LLM可以通过扮演不同角色来协同完成复杂任务。\n\n作者提出，不需要多个专门的模型，而是训练一个LLM在适当的时候扮演不同的角色——分解器和求解器——通过自我协作来完成复杂推理任务。\n\n## 4. AceSearcher框架设计\n\n基于上述假设，作者设计了AceSearcher框架：\n\n### 4.1 双角色定义\n- **分解器(Decomposer, ρ)**：将原始问题q分解为子问题序列z = (z₁, z₂, ..., zₙ)\n- **求解器(Solver, π)**：逐步生成中间答案w = (w₁, w₂, ..., wₙ)和最终答案a'\n\n### 4.2 联合学习目标\n作者定义了联合学习目标：\n```\npθ(a|q) = Σz pθ(z|q) Σw pθ(a|q,z,w) pθ(w|q,z)\n```\n这一目标反映了分解器和求解器的协同作用：分解器生成子问题，求解器基于子问题和检索到的上下文生成答案。\n\n### 4.3 两阶段训练框架\n作者设计了两阶段训练框架来解决不同层面的挑战：\n\n**第一阶段：监督微调(SFT)**\n- **目标**：建立基础能力\n- **数据策略**：混合三类数据\n  - 上下文丰富的QA数据（增强上下文利用能力）\n  - 问题分解数据（提升问题分解能力）\n  - 思维链数据（增强多步推理能力）\n- **训练方法**：标准下一个标记预测目标\n\n**第二阶段：强化微调(RFT)**\n- **目标**：在缺乏中间标注的情况下优化性能\n- **核心创新**：仅使用最终答案准确率作为奖励信号\n- **环境设计**：\n  - RAG环境：处理多跳QA和事实验证\n  - 上下文推理环境：处理表格、段落等复杂推理\n- **奖励设计**：结合精确匹配(EM)和格式正确性\n- **优化方法**：迭代偏好优化(Iterative DPO)\n\n## 5. 方法实现的关键创新\n\n### 5.1 无需中间标注的强化学习\n作者面临的核心挑战是如何在没有中间步骤标注的情况下训练模型。他们的解决方案是：\n\n1. **假设验证**：通过实验验证\"更好的分解导致更准确答案\"的假设\n2. **奖励设计**：仅基于最终答案的准确性设计奖励函数\n3. **联合优化**：同时优化分解器和求解器，使分解器产生有利于求解器生成正确答案的子问题\n\n### 5.2 偏好数据构建策略\n为解决稀疏奖励信号下的高方差问题，作者设计了创新的偏好数据构建方法：\n\n1. **多轨迹采样**：对每个问题生成多个候选分解和解决方案\n2. **基于期望奖励的排序**：根据对应解决方案的期望奖励对分解进行排序\n3. **多层次偏好对**：构建分解偏好对、子问题偏好对和最终答案偏好对\n4. **统一优化**：将三类偏好对合并，统一优化分解器和求解器\n\n### 5.3 实际效率优化\n作者还考虑了实际部署的效率问题：\n\n1. **推理策略优化**：对中间问题直接生成答案，仅对最终答案生成完整推理过程\n2. **上下文长度控制**：限制每个子问题的文档数量，防止输入过长\n3. **迭代优化平衡**：采用两轮迭代DPO，平衡性能与效率\n\n## 6. 验证与完善逻辑链\n\n作者通过大量实验验证了AceSearcher的有效性，完善了整个逻辑链：\n\n### 6.1 整体性能验证\n- 在10个数据集上的三个任务（多跳QA、多跳事实验证、文档级推理）上评估\n- 结果显示AceSearcher平均精确匹配提高7.6%，显著优于现有方法\n- 证明了方法在不同任务和领域的通用性\n\n### 6.2 参数效率验证\n- AceSearcher-32B使用不到DeepSeek-V3模型5%的参数就达到匹配性能\n- 小规模模型(1.5B和8B)常超过参数多达9倍的现有搜索增强LLMs\n- 证明了方法的高效性和可扩展性\n\n### 6.3 消融研究验证\n- 验证了SFT和RFT两个阶段的必要性\n- 验证了分解器(ρ)和求解器(π)两个组件的互补作用\n- 验证了SFT数据混合中搜索和推理数据的重要性\n- 比较了不同RL算法的效果，确认了所提方法的优势\n\n### 6.4 效率研究验证\n- 数据效率：仅用2K SFT示例就能匹配强基线，经过RFT后超越它们\n- 推理效率：虽然比标准RAG有更高延迟，但实现了显著性能提升\n- 证明了方法在资源受限环境中的实用性\n\n## 7. 逻辑链总结\n\nAceSearcher核心方法的逻辑推演展现了从宏观问题到具体解决方案的完整思考过程：\n\n1. **问题观察**：搜索增强LLMs在复杂推理任务上的局限性\n2. **局限分析**：系统分析现有方法在多跳检索和复杂推理上的不足\n3. **假设形成**：基于人类问题解决策略提出三个核心假设\n4. **框架设计**：提出双角色协作的AceSearcher框架和两阶段训练策略\n5. **方法实现**：解决无中间标注的强化学习挑战，设计创新的偏好数据构建策略\n6. **验证完善**：通过全面实验验证方法有效性、效率和各组件贡献\n\n这一逻辑链不仅解决了搜索增强LLMs在复杂推理任务上的核心挑战，还提供了一种高效、可扩展的解决方案，为未来相关研究提供了新的思路和方法论。", "summary_translation": "搜索增强型大语言模型（Search-augmented LLMs）通常在复杂推理任务上表现不佳，这是由于无效的多跳检索（multi-hop retrieval）和有限的推理能力所致。我们提出了AceSearcher，一种合作自我对弈框架（cooperative self-play framework），该框架训练单个大语言模型（LLM）在两个角色之间交替：一个是分解器（decomposer），负责分解复杂查询；另一个是求解器（solver），负责整合检索到的上下文以生成答案。AceSearcher将在搜索、推理和分解任务的多样化混合数据集上的监督微调（supervised fine-tuning）与针对最终答案准确性优化的强化微调（reinforcement fine-tuning）相结合，消除了对中间标注的需求。在10个数据集上的三个推理密集型任务（reasoning-intensive tasks）的广泛实验表明，AceSearcher优于最先进的基线模型（state-of-the-art baselines），实现了平均精确匹配（exact match）提升7.6%。值得注意的是，在文档级金融推理任务（document-level finance reasoning tasks）上，AceSearcher-32B仅使用不到DeepSeek-V3模型5%的参数，就达到了与之相当的性能。即使在较小规模（1.5B和8B参数）下，AceSearcher也常常超越参数量多达其9倍的现有搜索增强型大语言模型，凸显了其在处理复杂推理任务方面的卓越效率和有效性。我们的代码将发布于https://github.com/ritaranx/AceSearcher和https://huggingface.co/AceSearcher。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#75", "title": "Can Large Language Models Express Uncertainty Like Human?", "link": "/arxiv/2509.24202", "arxiv_id": "2509.24202", "authors": "Linwei Tao, Yi-Fan Yeh, Bo Kai, Minjing Dong, Tao Huang, Tom A. Lamb, Jialin Yu, Philip H. S. Torr, Chang Xu", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.488259", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究大语言模型如何像人类一样表达不确定性，通过语言置信度(Linguistic Confidence, LC)的方式提升模型表达可靠不确定性的能力。这不是将LLM作为工具应用到特定领域的研究，而是专注于提升LLM本身的一种基础能力——准确评估和表达自己的知识边界和置信度。这种能力对于模型的推理质量和可靠性至关重要，因此符合保留标准。 第二步：正面指标 论文明确涉及\"Large language models, LLMs\"这一核心概念。虽然未直接提及数学推理、逻辑推理或规划，但不确定性表达与推理质量密切相关，可靠的置信度估计能增强推理任务的准确性和可信度，因此与能力方向间接相关。 第三步：排除标准 论文不符合任何排除标准。它不涉及多模态与视觉研究，不是针对特定应用领域（如医疗、化学等）的研究，也不主要关注模型可靠性的应用层面（如水印、安全等）。 第四步：特殊和模糊情况 论文提出了一种新方法（语言置信度LC和相关的微调框架）来增强模型表达不确定性的能力，这可以视为提升模型内在可靠性和推理质量的方法。通过更准确地表达不确定性，模型可以减少过度自信导致的误导性回答，从而间接减少幻觉和提高推理质量，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种提升大语言模型表达不确定性能力的方法，这属于改进LLM基础能力的研究，与提高模型通用推理能力的研究目标一致，因此应该被保留。", "summary2": "本文旨在解决大语言模型如何像人类一样自然表达不确定性的问题。针对大语言模型在高风险场景中的不确定性表达需求，我们提出了一种基于语言置信度(Linguistic Confidence)的方法，通过模糊限制语表达不确定性，并在SimpleQA、PopQA和NQ-Open等QA基准上通过校准误差(ECE)和AUROC指标验证了其有效性。", "inspiration_trace": "# 从问题到方法：大型语言模型表达不确定性的逻辑演进\n\n## 一、宏观问题：LLMs如何可靠地表达不确定性？\n\n### 观察现实需求\n大型语言模型(LLMs)在教育、医疗、法律等高风险领域应用日益广泛，但存在一个根本矛盾：模型可能产生幻觉和事实错误，而人类用户往往过度依赖其输出。例如，有律师使用ChatGPT生成的虚假案例导致专业制裁。这引出一个核心问题：**如何让LLMs可靠地表达不确定性，以支持人类决策？**\n\n### 现有方法的局限性分析\n作者系统考察了三类主流不确定性表达方法，发现各自存在明显缺陷：\n\n1. **基于token概率的方法**（如perplexity）：\n   - 优势：简单、计算成本低\n   - 局限：需要访问模型内部logits，而商业LLM API通常不提供这些信息\n\n2. **多次采样的方法**（如语义熵、自洽性）：\n   - 优势：通常效果较好\n   - 局限：计算成本高，需要多次模型调用或辅助网络，实用性受限\n\n3. **口头化数值不确定性**（如直接给出0-100分数）：\n   - 优势：直接明了\n   - 局限：偏离自然交流方式，用户不会自然地以这种方式提问\n\n这些观察揭示了一个研究空白：**需要一种既高效又符合人类交流习惯的不确定性表达方法**。\n\n## 二、提出假设：语言置信度(LC)作为替代方案\n\n### 从人类交流中获取灵感\n作者观察到人类在表达不确定性时通常使用模糊语言（如\"probably\"、\"might\"、\"I am not entirely sure\"）。这引发了一个关键假设：**语言置信度(Linguistic Confidence, LC)——通过自然语言表达不确定性——可能是一个理想的替代方案**。\n\n### 假设的理论优势\nLC方法具有三个潜在优势：\n1. **无缝集成**：自然融入回应中，不需额外结构\n2. **计算高效**：几乎不需要额外计算开销\n3. **人类对齐**：模仿人类自然传达不确定性的方式\n\n### 现有研究的不足\n作者发现相关研究（如Yona等人2024年的工作）存在明显局限：\n- 仅在小数据集（仅18个不确定性短语）上验证\n- 未在标准不确定性估计指标（如校准和区分性）下评估\n- 使用LLM进行置信度映射，计算成本高（约$3/评估轮次）\n\n这些不足表明，**LC作为不确定性表达方法尚未得到充分探索**，需要更系统的研究。\n\n## 三、验证假设：构建数据集和评估方法\n\n### 关键挑战识别\n为验证LC的有效性，作者识别出两个核心挑战：\n1. **缺乏大规模、多样化的模糊表达数据集**\n2. **缺乏高效可靠的置信度映射方法**\n\n### 解决方案一：构建人类标注的数据集\n作者设计了一个五步流程构建高质量数据集：\n\n1. **收集不确定表达**：\n   - 从SimpleQA采样200个问题\n   - 使用4个LLMs生成不同置信度水平的表达（高、中、低、最低、完全不确定）\n   - 获得40,000个表达，从中采样10,000个\n\n2. **获取人类标注**：\n   - 通过Amazon Mechanical Turk平台\n   - 每个表达由5人标注0-100的置信度分数\n   - 包含5个专家预先标注的验证项用于质量控制\n\n3. **筛选可靠标注者**：\n   - 识别遵循指示的标注者（如对拒绝回答的表达给出0分）\n   - 使用这些可靠标注者的数据作为筛选基准\n\n4. **确定有效分数范围**：\n   - 对每个置信度水平，计算平均值±1标准差的范围\n   - 将此范围作为有效注释的上下限\n\n5. **构建最终数据集**：\n   - 保留至少有3个有效注释的表达\n   - 使用平均分数作为最终置信度标注\n\n### 解决方案二：开发轻量级置信度映射器\n为解决LLM-based映射方法的高成本问题，作者开发了轻量级模型：\n\n1. **模型设计**：\n   - 使用DistilRoBERTa作为编码器\n   - 后接带sigmoid激活的线性层\n   - 输出0-1之间的置信度分数\n\n2. **训练策略**：\n   - 在LLM生成的句子数据集上训练（标签：完全不确定=0，最低=0.25，低=0.5，中等=0.75，高=1）\n   - 在人类标注的表达上训练（之前因注释不足被丢弃的数据）\n\n3. **评估结果**：\n   - MSE显著低于LLM-based基线（50.68 vs 183.23-385.40）\n   - 计算成本接近零，延迟极低（1.32秒 vs 551-678秒）\n\n这一步验证了**可以高效可靠地将模糊语言映射为数值置信度**，为后续系统性研究奠定了基础。\n\n## 四、系统性研究：评估LLMs的语言置信度\n\n### 研究设计\n作者在多个QA基准（SimpleQA、NQ-Open、PopQA）上评估了多种现代LLMs的LC性能：\n\n1. **评估维度**：\n   - **校准(Calibration)**：预测置信度与实际正确性的对齐程度（使用ECE评估）\n   - **区分性(Discriminability)**：区分正确与错误答案的能力（使用AUROC评估）\n\n2. **对比方法**：\n   - 口头化数值置信度(VNC)\n   - 语义不确定性(SU)\n   - 自评估方法P(True)和困惑度（仅开源模型）\n\n3. **提示策略**：\n   - LC：普通QA提示\n   - LC+：明确指示模型在不确定时使用模糊语言\n\n### 关键发现\n实验结果揭示了几个重要现象：\n\n1. **普通LC表现不佳**：\n   - 在默认提示下，大多数模型的LC校准和区分性都很差\n   - AUROC仅略高于50%，几乎无法区分知道与不知道的内容\n\n2. **明确提示显著改善LC+性能**：\n   - LC+在校准和区分性上大幅提升\n   - 在某些模型（如Qwen3-235b）上达到最佳校准性能\n   - 与语义不确定性等强基线相当\n\n3. **推理过程的积极影响**：\n   - 增加推理预算不仅提高了数值置信度校准\n   - 也显著增强了语言置信度的表达\n\n这些发现表明：**LLMs确实有能力通过语言表达不确定性，但需要适当的提示或推理才能可靠地实现**。\n\n## 五、改进方法：微调框架以提高LC可靠性\n\n### 问题识别\n虽然LC+提示改善了性能，但它需要额外指令，偏离了自然交互。更理想的情况是模型在直接回答问题时就能自然表达不确定性。\n\n### 解决方案：微调框架\n作者设计了一个四步微调框架：\n\n1. **计算语义不确定性作为监督信号**：\n   - 使用基础模型为每个问题生成10个答案\n   - 计算语义不确定性作为不确定性代理\n   - 将其离散化为五个水平（完全不确定、最低、低、中等、高）\n\n2. **生成不确定句子**：\n   - 使用SOTA LLMs（如GPT-5）生成与指定置信度水平一致的表达\n   - 预构建不确定性句子数据库以简化实现\n\n3. **构建微调数据集**：\n   - 从SimpleQA采样200个问题\n   - 为每个问题检索40个LLM生成的回应\n   - 形成8,000个问题-答案对\n\n4. **微调基础模型**：\n   - 采用LoRA进行高效微调\n   - 保持大多数预训练权重冻结，降低计算成本\n\n### 实验结果\n在Qwen3-8B模型上的评估显示：\n\n1. **一致的性能提升**：\n   - 与基础模型相比，微调模型在校准和区分性上均有改进\n   - 在NQ-Open上超越了VNC和SU等强基线\n\n2. **更自然的表达**：\n   - 微调后的模型在回答时自然使用模糊语言\n   - 不需要额外提示就能表达不确定性\n\n这些结果表明，**通过适当微调，可以使LLMs在自然交互中可靠地表达语言置信度**。\n\n## 六、结论与展望：语言置信度的定位与未来\n\n### 核心贡献总结\n作者将语言置信度定位为LLMs不确定性估计的可扩展、高效且与人类对齐的方法，主要贡献包括：\n\n1. **首个大规模、多样化的模糊表达数据集**\n2. **轻量级置信度映射器**，实现高效可靠的LC评估\n3. **对现代LLMs的LC进行首次系统性研究**\n4. **微调框架**，提高LC的可靠性\n\n### 局限性与未来方向\n作者也坦诚研究的局限性：\n\n1. **主观性挑战**：模糊语言的感知因人而异，将平均分数作为真值不是最优解\n2. **默认性能不足**：LC并非固有可靠，需要明确提示或微调才能发挥作用\n3. **应用范围有限**：研究主要集中在QA任务，未来需扩展到推理和多模态场景\n\n### 逻辑链总结\n从宏观问题到具体方法，作者的思考路径清晰可见：\n\n**现实需求→现有方法局限→提出LC假设→构建评估工具→系统性验证→方法改进→定位与展望**\n\n这一完整逻辑链条展示了如何从观察问题到提出假设，再到验证假设并改进方法，最终形成一个系统的研究框架。作者不仅解决了LLMs不确定性表达的实际问题，还为这一领域开辟了新的研究方向，体现了学术研究的创新性和系统性。", "summary_translation": "大语言模型（LLMs）越来越多地被用于高风险场景，在这些场景中，过度自信的回应可能会误导用户。可靠的置信度估计（confidence estimation）已被证明能够增强信任度和任务准确性。然而，现有方法面临实际障碍：logits（模型输出层的原始分数）通常被隐藏，多重采样（multi-sampling）计算成本高，而口头表达的数值不确定性（verbalized numerical uncertainty）（例如给出0-100的分数）偏离了自然交流方式。我们重新审视语言置信度（linguistic confidence, LC），其中模型通过模糊限制语（hedging language）（例如\"probably\"、\"might\"）表达不确定性，提供了一种轻量级且以人为中心的替代方案。为推进这一方向，我们（1）发布了首个多样化、大规模的模糊限制语数据集，其中包含人工标注的置信度分数；并且（2）提出了一种轻量级映射器（mapper），能够以几乎零成本将模糊限制语转换为置信度分数。基于这些资源，我们（3）对现代LLMs和QA（问答）基准测试中的LC进行了首次系统性研究，结果表明，尽管大多数LLMs在表达可靠LC方面表现不佳，但精心设计的提示（prompting）能够实现具有竞争力的校准性（calibration）和区分度（discriminability）。最后，我们（4）引入了一种微调（fine-tuning）框架，进一步提高了LC的可靠性。总而言之，我们的工作将语言置信度定位为一种可扩展、高效且符合人类思维的（human-aligned）LLMs不确定性估计方法，并呼吁对这一有前景但尚未充分探索的方向进行更深入的研究。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#81", "title": "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis", "link": "/arxiv/2509.24164", "arxiv_id": "2509.24164", "authors": "Haolin Yang, Hakaze Cho, Naoya Inoue", "summary": "We investigate the mechanistic underpinnings of in-context learning (ICL) in large language models by reconciling two dominant perspectives: the component-level analysis of attention heads and the holistic decomposition of ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel framework based on Task Subspace Logit Attribution (TSLA) to identify attention heads specialized in TR and TL, and demonstrate their distinct yet complementary roles. Through correlation analysis, ablation studies, and input perturbations, we show that the identified TR and TL heads independently and effectively capture the TR and TL components of ICL. Using steering experiments with geometric analysis of hidden states, we reveal that TR heads promote task recognition by aligning hidden states with the task subspace, while TL heads rotate hidden states within the subspace toward the correct label to facilitate prediction. We further show how previous findings on ICL mechanisms, including induction heads and task vectors, can be reconciled with our attention-head-level analysis of the TR-TL decomposition. Our framework thus provides a unified and interpretable account of how large language models execute ICL across diverse tasks and settings.", "subjects": "Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.507405", "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究大语言模型(LLM)的上下文学习(ICL)机制，通过分析注意力头来理解任务识别(TR)和任务学习(TL)的内部工作机制。论文提出了TSLA框架来识别专门用于TR和TL的注意力头，并揭示了它们在ICL过程中的不同作用。这属于对LLM基础能力和内部工作机制的研究，特别是关注上下文学习这种通用推理能力的机制分析，而不是将LLM作为工具应用到特定领域。因此，根据第一步的标准，应该保留。 第二步：正面指标 - 核心概念：论文明确研究大语言模型(LLMs)的上下文学习机制，符合这一指标。 - 能力方向：上下文学习(ICL)本身就是一种通用推理能力的体现，论文研究的是ICL的机制，涉及到任务识别和任务学习，这与推理能力直接相关，符合这一指标。 - 训练方法：论文没有明确提到强化学习、进化或自我进化等训练方法，不符合这一指标。 - 新兴范式：论文没有直接讨论基于LLM的智能体、多智能体系统、工具使用或深度研究等新兴范式，不符合这一指标。 第三步：排除标准 论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 第四步：特殊和模糊情况 论文关注的是LLM内部工作机制的可解释性，通过分析注意力头来理解上下文学习的机制。这属于\"增强模型内在的可解释性\"的情况，从而提升我们对模型推理能力的理解。根据标准，应该保留。 最终决策： 这篇论文的核心贡献是提供了一个统一且可解释的框架，用于理解大语言模型如何执行上下文学习。这种研究有助于我们深入理解LLM的通用推理机制，从而可能指导未来改进LLM推理能力的方法开发。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。", "summary2": "本文旨在揭示大型语言模型中上下文学习(ICL)的机制基础。针对ICL中的任务识别(TR)和任务学习(TL)组件，我们提出了一种基于任务子空间Logit归因(TSLA)的新框架，用于识别专门负责TR和TL的注意力头。在多个数据集上通过相关性分析、消融研究和输入扰动验证了这些头的独立作用，并通过几何分析揭示了TR头对齐隐藏状态与任务子空间，而TL头在子空间内旋转状态朝向正确标签的机制。", "inspiration_trace": "# 推演作者核心方法的逻辑链\n\n## 1. 宏观问题识别：ICL机制理解的范式鸿沟\n\n作者首先观察到大型语言模型(LLMs)的上下文学习(ICL)能力存在两种主导研究范式：\n- **内省范式**：关注模型内部组件(如注意力头)的作用，能定位特定组件但缺乏功能解释\n- **整体范式**：将ICL分解为任务识别(TR)和任务学习(TL)两个功能组件，提供功能视图但无法追溯到具体组件\n\n**核心问题**：两种范式各自存在局限性，缺乏一个既能提供机制精确性又能提供功能清晰度的统一框架。\n\n## 2. 问题聚焦：连接功能组件与具体注意力头\n\n作者将问题聚焦于一个关键挑战：如何将ICL的功能分解(TR和TL)与具体的模型组件(注意力头)联系起来？\n\n具体需要回答：\n- 哪些注意力头专门负责TR和TL功能？\n- 这些头如何协同工作实现完整ICL？\n- 既有发现(如归纳头、任务向量)如何与TR-TL框架协调？\n\n## 3. 现有方法的局限性分析\n\n作者深入分析了识别TR/TL头的现有方法(Lieberum等, 2023)的缺陷：\n\n**TR头识别问题**：\n- 现有方法仅关注对特定演示标签(如positive/negative)的logit贡献\n- 但标签是任意超参数，改变标签(如改为favourable/unfavourable)不改变任务本质\n- 因此，现有方法无法捕捉超越表面标签的任务语义\n\n**TL头识别问题**：\n- 现有方法仅关注提升正确标签logit的头\n- 忽略了标签间的竞争：提升正确标签的头可能同时提升错误标签\n- 无法区分真正的任务映射头与普遍提升所有标签的头\n\n**核心洞察**：需要一种能(a)捕捉任务语义本质和(b)评估标签间相对贡献的新方法。\n\n## 4. 几何视角的核心假设\n\n基于LLMs将相关语义编码为子空间的特性，作者形成了关键假设：\n\n**TR头假设**：\n- TR头应使其输出与任务子空间(由演示标签的unembedding向量张成)对齐\n- 这种对齐应独立于具体标签选择，反映任务语义的本质\n\n**TL头假设**：\n- TL头应在任务子空间内增加正确与错误标签间的logit差异\n- 几何上表现为在子空间内将隐藏状态朝正确标签方向旋转\n\n这些假设将功能概念(TR/TL)转化为可测量的几何性质。\n\n## 5. 方法论创新：任务子空间Logit归因(TSLA)\n\n基于上述假设，作者提出了TSLA方法：\n\n**TR分数设计**：\n```\nTR_score = ∥Proj_{W_Y^U} a_{lN,k}∥²\n```\n- 测量头输出在任务子空间上的投影范数\n- 捕捉对任务相关语义的整体贡献，不依赖具体标签选择\n- 提供理论保证：高TR分数意味着头输出最佳匹配任务子空间\n\n**TL分数设计**：\n```\nTL_score = Ave_{y'∈Y/{y*}}(a_{l,N,k}^⊤(W_{y*}^U - W_{y'}^U)) / ∥Proj_{W_Y^U} a_{lN,k}∥²\n```\n- 分子测量头创造的标签间logit差异\n- 分母归一化，评估在任务子空间内的相对贡献\n- 几何上表示投影头输出与正确-错误标签差异的对齐程度\n\n## 6. 多层次验证策略\n\n作者设计了全面的验证策略，从多个角度检验方法：\n\n**头特化验证**：\n- 分析TR、TL头与归纳头(IH)的重叠和相关性\n- 研究不同层中这些头的分布模式\n\n**功能独立性验证**：\n- 引入新指标\"任务识别比\"(TR ratio)分离TR和TL贡献\n- 通过消融实验测试TR和TL头的独立作用\n- 在输入扰动(打乱文本、重标记)下验证独立性\n\n**几何机制验证**：\n- 通过转向实验检验TR/TL头作为任务向量的效果\n- 分析TR/TL输出对隐藏状态几何性质的影响\n- 层面分析验证TR/TL头对几何变化的驱动作用\n\n## 7. 理论整合与统一框架建立\n\n通过实验结果，作者整合了既有发现并建立统一框架：\n\n**关键发现**：\n- TR头与IH高度重叠，表明IH主要通过增强TR影响ICL\n- 零样本预测失败主要源于隐藏状态与任务子空间弱对齐\n- TR输出对齐任务子空间，TL输出在子空间内旋转朝正确标签\n\n**统一框架**：\n- TR头负责将隐藏状态与任务子空间对齐，实现任务识别\n- TL头在子空间内旋转隐藏状态朝向正确标签，实现任务学习\n- 这种机制共同解释了ICL如何在不同任务和设置中执行\n\n这一框架不仅解释了ICL的基本机制，还自然调和了归纳头、任务向量等既有发现，提供了组件级与功能级视角的统一理解。", "summary_translation": "我们通过协调两种主流观点来研究大语言模型中上下文学习(in-context learning, ICL)的机制基础：注意力头(attention heads)的组件级分析(component-level analysis)以及将ICL整体分解(holistic decomposition)为任务识别(Task Recognition, TR)和任务学习(Task Learning, TL)。我们提出了一个基于任务子空间Logit归因(Task Subspace Logit Attribution, TSLA)的新框架，用于识别专门负责TR和TL的注意力头，并展示了它们不同但互补的作用。通过相关性分析(correlation analysis)、消融研究(ablation studies)和输入扰动(input perturbations)，我们证明了所识别的TR和TL头能够独立且有效地捕捉ICL的TR和TL组件。通过结合隐藏状态(hidden states)几何分析(geometric analysis)的引导实验(steering experiments)，我们揭示了TR头通过将隐藏状态与任务子空间(task subspace)对齐来促进任务识别，而TL头则在子空间内将隐藏状态旋转至正确标签以促进预测。我们进一步展示了先前关于ICL机制的发现，包括归纳头(induction heads)和任务向量(task vectors)，如何与我们对TR-TL分解的注意力头级分析相协调。因此，我们的框架为大语言模型如何在各种任务和设置中执行ICL提供了一个统一且可解释的说明。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#83", "title": "Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE", "link": "/arxiv/2509.24130", "arxiv_id": "2509.24130", "authors": "Guancheng Wan, Lucheng Fu, Haoxin Liu, Yiqiao Jin, Hui Yi Leong, Eric Hanchen Jiang, Hejia Geng, Jinhe Bi, Yunpu Ma, Xiangru Tang, B. Aditya Prakash, Yizhou Sun, Wei Wang", "summary": "The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.508715", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是我的详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础能力，具体关注提示词工程的鲁棒性问题。论文提出的TARE框架通过减少提示词在语义相同但表达不同情况下的性能波动，增强了LLM的稳定性和可靠性。提示词工程是直接影响LLM推理能力和输出质量的关键因素，因此这属于改进LLM基础能力的范畴，应当保留。 第二步：正面指标 - 核心概念：论文明确聚焦于Large Language Models (LLMs)，符合这一指标。 - 能力方向：虽然论文没有直接讨论推理能力，但提示词的鲁棒性直接影响LLM在推理任务中的表现。稳定的提示词可以确保LLM在面对不同表述的相同问题时保持一致的推理能力。 - 训练方法：论文提到了\"evolutionary search\"作为现有方法之一，并提出了\"Textual Sharpness-Aware Evolving\"(TARE)框架，涉及进化方法，部分符合这一指标。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 不主要关注模型可靠性的应用层面（如水印、安全等） 第四步：特殊和模糊情况 论文关注提示词鲁棒性，通过减少提示词微小变化导致的性能波动，间接提高了模型输出的稳定性和可靠性。这可以视为提升模型内在可靠性的方法，有助于提高LLM的通用推理质量，因此应当保留。 综合判断：该论文提出的TARE框架通过优化提示词的鲁棒性，提高了LLM在面对语义相同但表达不同的问题时的稳定性，这直接关系到LLM的通用推理能力和问题解决能力，符合研究范围的核心目标。", "summary2": "本文旨在解决大型语言模型提示优化中的脆弱性问题，即提示在语义保留的释义下表现不稳定。针对提示景观的文本锐度问题，我们提出了一种TARE（Textual Sharpness-Aware Evolving）框架，通过内部对抗性搜索和外部鲁棒性选择来优化提示的鲁棒性，并在Big Bench Hard和GSM8K数据集上通过准确率指标验证了其有效性。", "inspiration_trace": "# 从提示词脆弱性到文本锐度感知优化：TARE方法的逻辑演进\n\n## 1. 宏观问题：LLMs的提示词脆弱性\n\n**观察现象**：大型语言模型(LLMs)的性能高度依赖于精心设计的提示词，但现有提示词优化方法存在一个根本缺陷——它们主要追求点状准确性(point-wise accuracy)，却忽略了一个关键问题：小的、语义保持的改写往往会导致性能大幅波动。\n\n**核心问题**：这种脆弱性严重限制了LLM系统在实际应用中的可靠性。一个在特定输入上表现优异的提示词，在面对轻微改写或重新表述时可能完全失效。\n\n## 2. 问题形式化：文本锐度概念的提出\n\n**关键洞察**：作者将这种脆弱性定义为\"提示词景观的文本锐度\"(textual sharpness of the prompt landscape)，即提示词性能在语义空间中的变化剧烈程度。\n\n**理论联系**：作者注意到深度学习领域已有研究表明，模型在损失景观中收敛到平坦最小值时具有更好的泛化性能。锐度感知最小化(SAM)等技术在连续参数空间中已取得成功，但将其原理应用于离散、语义丰富的文本提示词空间仍是一个未被探索的挑战。\n\n**研究问题聚焦**：\n1. 如何在离散、语义空间中正式定义和量化提示词的\"锐度邻域\"？\n2. 如何设计实用算法在离散景观中导航，找到既有效又对语义扰动稳健的提示词？\n\n## 3. 理论基础：语义邻域与文本锐度的形式化\n\n**语义邻域定义**：作者首先构建了提示词空间的语义不相似性度量，定义各向同性邻域：\n```\nB(p, ρ_text) := {p' ∈ P : d_text(p, p') ≤ ρ_text}\n```\n\n**各向异性扩展**：为捕捉提示词不同组件的异质性敏感性，引入各向异性度量和椭球邻域，允许对不同语义组件施加不同程度的扰动。\n\n**文本锐度量化**：基于语义邻域，将文本锐度定义为局部最坏情况风险：\n```\nLS(p, ρ_text) := max_{p'∈B(p,ρ_text)} LD(p')\n```\n\n**稳健优化目标**：将问题转化为最小化文本锐度感知风险：\n```\nmin_{p∈P} LS(p, ρ_text)\n```\n\n## 4. 方法设计：TARE框架的构建\n\n**设计原则**：基于黑盒、无导数优化，语义邻域保留任务意图，内部对抗搜索暴露弱点，外部稳健更新选择候选。\n\n**核心算法**：\n1. **内部对抗搜索**：在语义邻域内采样候选变体，评估性能并识别最坏情况。\n2. **外部稳健更新**：基于当前提示词和其最差邻居生成改进候选，选择在语义邻域内表现最稳健的提示词。\n\n**与SAM的类比**：TARE在离散文本空间中镜像了SAM的逻辑——内部搜索发现最坏扰动，外部更新向更平坦区域移动，共同优化任务性能和稳健性。\n\n## 5. 方法改进：ATARE的各向异性与自适应\n\n**局限性识别**：各向同性邻域平等对待所有提示词组件，但实际敏感性是异质的——核心约束、方法指导和风格元素具有不同的重要性。\n\n**ATARE创新**：\n1. **敏感性估计**：计算每个组件的对抗性增益，动态调整权重。\n2. **各向异性采样**：根据组件敏感性进行差异化扰动，对敏感组件精细探索，对稳健组件广泛探索。\n3. **自适应半径调度**：根据验证结果动态调整邻域大小，平衡探索与保真度。\n\n## 6. 优化器实现：LATO的景观感知设计\n\n**优化挑战**：外部稳健更新需要智能的优化器来生成改进候选。\n\n**LATO设计**：景观感知文本优化器通过分析当前提示词和其最差邻居的性能差异，直接感知局部景观几何形状，引导提示词向更平坦的语义盆地移动。\n\n**核心机制**：LATO不仅修正当前错误，还从邻居的失败模式中学习，预先解决最脆弱的方面，使提示词本质上对类似扰动更加稳健。\n\n## 7. 实验验证与扩展应用\n\n**多维度验证**：通过优越性、有效性、韧性和敏感性四个维度全面评估方法，证明TARE和ATARE在多个推理任务上显著优于现有基线。\n\n**扩展应用**：将框架原则扩展到解决方案优化领域，利用ATARE的各向异性特性处理逻辑推理链中不同组件的异质性敏感性，保持正确推理骨架不变的同时针对性改进逻辑缺陷。\n\n## 8. 逻辑演进总结\n\n从观察到方法论的完整逻辑链：\n1. **问题识别**：提示词优化的脆弱性现象\n2. **理论构建**：引入文本锐度概念，形式化语义邻域\n3. **方法设计**：TARE框架实现离散空间中的锐度感知优化\n4. **方法改进**：ATARE引入各向异性和自适应机制\n5. **实现优化**：LATO提供景观感知的优化能力\n6. **验证扩展**：实验证明有效性并拓展应用场景\n\n这一演进过程展示了如何将连续优化理论成功适配到离散语义空间，解决了提示词优化的稳健性问题，为LLMs的可靠应用提供了新思路。", "summary_translation": "大型语言模型（Large Language Models, LLMs）的性能取决于精心设计的提示（prompts）。然而，现有的提示优化方法，从启发式编辑（heuristic edits）和强化学习（reinforcement learning）到进化搜索（evolutionary search），主要针对点状准确率（point-wise accuracy）。这些方法很少强制执行释义不变性（paraphrase invariance）或搜索稳定性（searching stability），因此无法在实践中解决这种脆弱性问题。自动提示搜索仍然脆弱：微小的、保持语义的释义（semantically preserving paraphrases）常常导致性能大幅波动。我们将这种脆弱性识别为提示景观（prompt landscape）的文本锐度（textual sharpness）。\n\n在这项工作中，我们首次对提示在离散语义空间（discrete, semantic space）中的文本锐度进行了形式化处理，并提出了一个在语义邻域（semantic neighborhood）上的可操作鲁棒性标准（operational robustness criterion）；该设计是黑盒（black-box）或仅API（API-only）的，不需要梯度来更新模型参数。然后，我们介绍了TARE（Textual Sharpness-Aware Evolving，文本锐度感知进化），这是一个无导数框架（derivative-free framework），它在内部基于采样的对抗搜索（sampling-based adversarial search）和外部鲁棒选择（robust selection）之间交替进行，前者通过困难的释义（hard paraphrases）对提示施加压力，后者则倾向于选择那些邻域仍然保持强大的候选者。我们进一步提出了ATARE，它学习各向异性权重（anisotropic weights）来塑造语义邻域，并随时间调整其半径以平衡探索（exploration）和保真度（fidelity）。\n\n多样化的任务评估了我们的方法，其最小化文本锐度差距（textual sharpness gap）的设计使得提示在释义下保持准确率，优于仅追求准确率的提示搜索，同时保持了计算上的实用性。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#85", "title": "Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems", "link": "/arxiv/2509.24116", "arxiv_id": "2509.24116", "authors": "Minsoo Kim, Seung-won Hwang", "summary": "LLM-based agents have seen promising advances, yet they are still limited in \"hard-exploration\" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.509643", "filter_reason": "这篇论文的核心贡献是提出GLoW（双尺度世界模型），一种用于增强LLM智能体在\"hard-exploration\"任务中表现的新方法。根据筛选标准，这篇论文符合我的研究目标，原因如下： 首先，从本质上看，论文专注于提升LLM本身的基础能力，特别是探索和问题解决能力，而不是将LLM作为工具应用到特定领域。论文提出的双尺度世界模型和Multi-path Advantage Reflection机制，旨在增强LLM通过探索学习新知识的能力，这属于提升LLM通用推理能力的范畴。 其次，论文包含多个正面指标：核心概念上明确关注\"LLM-based agents\"；能力方向上涉及\"hard-exploration\"问题，这与问题解决能力直接相关；方法上提出了一种从试错中学习的机制，与强化学习有相似之处；并且明确关注LLM-based agents这一新兴范式。 第三，论文不符合任何排除标准。虽然使用了文本游戏作为评估基准，但这只是作为测试通用探索能力的平台，而不是专注于游戏领域本身。论文没有涉及多模态、特定应用领域或模型可靠性等排除标准中的内容。 最后，论文提出的是一种通用的智能体框架来增强LLM的探索能力，而不是将智能体应用在特定领域，这符合特殊情况的判断标准。 综上所述，这篇论文致力于提高LLM本身的通用推理能力（特别是探索和问题解决能力），符合我的研究范围。", "summary2": "本文旨在解决LLM agents在hard-exploration问题中的局限性。针对具有大状态-动作空间、欺骗性局部最优和稀疏奖励的文本游戏环境，我们提出了一种双尺度世界模型框架GLoW，并在Jericho benchmark上通过游戏得分和样本效率验证了其有效性。", "inspiration_trace": "# GLoW方法逻辑链分析：从宏观问题到双尺度世界模型\n\n## 一、宏观问题：LLM智能体在硬探索任务上的局限性\n\n**观察现象**：尽管LLM智能体在利用预训练知识的任务（如机器人规划、软件工程）中表现出色，但在需要通过探索学习新知识的\"硬探索\"任务中表现不佳。\n\n**问题定义**：硬探索问题具有三大特征：\n- 大规模状态-动作空间（如文本游戏中的组合爆炸）\n- 欺骗性局部最优（容易陷入无法逃脱的局部区域）\n- 稀疏奖励（成功前几乎没有反馈信号）\n\n**核心挑战**：LLM智能体面临两个关键瓶颈：\n1. **全局学习**：如何维护探索过程中有价值发现的长期知识\n2. **局部试错**：如何从稀疏环境反馈中快速细化探索策略\n\n## 二、现有方法分析与局限\n\n**观察**：当前LLM智能体方法（如ReAct、Reflexion）支持局部试错，但缺乏长期知识积累机制。\n\n**启发**：Go-Explore算法通过维护已发现状态存档并在\"选择有希望状态\"和\"从该状态继续探索\"之间交替，在硬探索问题上取得突破。\n\n**局限识别**：\n- 原始Go-Explore使用手工启发式方法进行状态选择，随机动作采样进行探索\n- 后续改进（如IGE）虽引入LLM推理，但选择标准仍不明确，探索能力有限\n\n## 三、核心洞察：双尺度学习的必要性\n\n**关键洞察**：选择和探索都需要从过去经验中学习，但在不同尺度上：\n1. **全局尺度**：需要维护完整轨迹上下文，理解如何达到高价值状态及进展停滞原因\n2. **局部尺度**：需要从同一状态的多条探索路径中比较，识别关键决策点的优势信号\n\n**理论支持**：\n- 全局层面受UCB价值分解启发，平衡利用与探索\n- 局部层面借鉴优势函数思想，减少稀疏奖励环境中的方差\n\n## 四、方法构建：全局世界模型\n\n**设计思路**：超越孤立状态存档，维护轨迹前沿以保留完整上下文\n\n**核心组件**：\n1. **价值排序轨迹前沿**：维护k个最高价值轨迹F = {τ₁, τ₂, ..., τₖ}，按最大累积奖励排序\n2. **LLM生成轨迹分析**：通过g_LLM分析前沿轨迹，提取关键状态及其双重价值：\n   - W_global = {(s₁, v₁, v'₁), (s₂, v₂, v'₂), ..., (sₖ, vₖ, v'ₖ)}\n   - vᵢ：已实现价值，v'ᵢ：未来价值潜力估计\n\n**状态选择机制**：通过align_LLM评估存档状态与W_global的匹配度，自然平衡利用（与已证明高奖励区域相似）与探索（优先接近已识别瓶颈的高潜力状态）。\n\n## 五、方法构建：局部世界模型\n\n**设计思路**：从Q值到优势的转换，通过多路径比较减少稀疏奖励环境中的方差\n\n**核心组件**：\n1. **多路径优势反思(MAR)**：从同一状态采样n条轨迹，比较不同结果，产生伪密集优势信号\n2. **语义优势表示**：不仅记录哪些动作有益，还解释为什么有效及适用条件\n   - W_local = {(s*₁, A_s*₁), ..., (s*_k, A_s*_k)}\n\n**探索策略**：π_explore(a|s_t, h_t) = Agent_LLM(s_t, h_t, W_local, T_s, F)\n结合局部优势学习、当前探索历史和全局成功策略指导探索\n\n## 六、实验验证与效果\n\n**实验设置**：在Jericho文本游戏基准测试上评估，该测试具有部分可观察性和组合状态-动作空间挑战\n\n**关键结果**：\n1. 在7/10游戏中达到LLM方法新SOTA\n2. 与需要100-800倍更多环境交互的RL方法相比，性能相当\n3. 显著优于直接可比的IGE方法（8/10游戏）\n\n**消融研究**：\n- 移除MAR导致性能显著下降，证明优势学习的有效性\n- 移除全局价值分析或轨迹前沿均降低性能，确认全局模型贡献\n- 组件间存在明显协同效应，整体性能源于互补设计\n\n## 七、逻辑演进总结\n\n从\"LLM智能体在硬探索任务中表现不佳\"的宏观问题出发，通过分析现有方法缺乏长期知识积累和有效局部学习的局限，形成了\"需要在不同尺度上进行结构化学习\"的核心洞察，进而设计了双尺度世界模型框架：全局模型维护轨迹前沿和潜在价值分析用于状态选择，局部模型通过多路径优势反思指导探索。整个逻辑链条从问题识别到方法设计再到验证，形成了一个完整、严谨的研究思路，最终实现了在保持样本效率的同时显著提升LLM智能体在硬探索任务中的性能。", "summary_translation": "基于LLM的智能体（LLM-based agents）已经取得了令人瞩目的进展，但它们在需要通过探索学习新知识的\"硬探索\"（hard-exploration）任务中仍然存在局限。我们提出了GLoW，一种利用双尺度世界模型（dual-scale world models）的新方法，该方法在全局尺度上维持高价值发现的轨迹前沿（trajectory frontier），同时通过多路径优势反思机制（Multi-path Advantage Reflection mechanism）从探索中的局部试错中学习，该机制推断基于优势的进展信号以指导探索。为了评估我们的框架在硬探索任务上的表现，我们应对了基于文本游戏的Jericho基准测试套件（Jericho benchmark suite），其中GLoW在基于LLM的方法中取得了最新的最先进性能（state-of-the-art performance）。与最先进的基于强化学习的方法（state-of-the-art RL-based methods）相比，我们的方法取得了相当的性能，同时所需的环境交互减少了100-800倍。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#86", "title": "Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics", "link": "/arxiv/2509.24102", "arxiv_id": "2509.24102", "authors": "Guangliang Liu, Xi Chen, Bocheng Chen, Xitong Zhang, Kristen Johnson", "summary": "Moral reasoning has emerged as a promising research direction for Large Language Models (LLMs), yet achieving generalization remains a central challenge. From a linguistic standpoint, this difficulty arises because LLMs are adept at capturing distributional semantics, which fundamentally differs from the morals which operate at the pragmatic level. This paper investigates how LLMs can achieve generalized moral reasoning despite their reliance on distributional semantics. We propose pragmatic inference methods grounded in moral foundations theory, which leverage contextual information at each step to bridge the pragmatic gap and guide LLMs in connecting moral foundations with moral reasoning objectives. Experimental results demonstrate that our approach significantly enhances LLMs' generalization in moral reasoning, providing a foundation for future research grounded in moral foundations theory.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.510140", "filter_reason": "这篇论文符合我的研究目标，因为它的核心是提升大语言模型(LLMs)在道德推理方面的通用能力。根据筛选标准分析：首先，论文本质上是改进LLM的基础推理能力，提出了一种基于道德基础理论的语用推理方法，这属于\"增强其逻辑、多步推理等通用能力\"的范畴。道德推理可以被视为通用推理能力的一个重要子集，涉及逻辑判断和原则应用。其次，论文包含正面指标中的核心概念\"Large language models\"和能力方向\"reasoning\"。第三，论文不符合排除标准，虽然涉及道德推理，但并未将其作为特定应用领域（如社会学）的研究，而是将其视为提升LLM通用推理能力的一种途径。论文提出的语用推理方法旨在弥合LLM依赖的分布语义与道德推理所需的语用层面之间的差距，这实质上是增强模型内在推理机制的研究，符合\"提高大语言模型本身的通用推理能力\"的核心目标。", "summary2": "本文旨在解决大型语言模型在道德推理任务中泛化能力不足的问题。针对LLMs依赖分布语义而道德推理本质上是语用层面的差异，我们提出了一种基于道德基础理论的语用推理方法，并在Moral Integration Corpus (MIC)基准上通过预测准确率验证了其有效性。", "inspiration_trace": "# 论文核心方法逻辑链分析：从语用差距到道德推理泛化\n\n## 一、宏观问题：LLMs道德推理的泛化困境\n\n**观察现象**：大型语言模型(LLMs)在道德推理任务中表现不佳，尤其是泛化能力有限。即使经过大量数据训练，模型在面对新情境时仍难以做出合理的道德判断。\n\n**问题定位**：这一挑战并非简单的数据不足或模型规模问题，而是源于LLMs的工作机制与道德推理本质之间的根本性不匹配。\n\n## 二、问题根源：分布语义与语用学的鸿沟\n\n**理论分析**：\n- LLMs基于**分布语义**(distributional semantics)工作，遵循Firth(1957)的分布假说——语言单位的含义由其出现的上下文决定\n- 道德推理本质上是**语用学**(pragmatics)层面的任务，涉及隐含意义、社会规范和行为后果等\"未言明\"的变量\n\n**核心发现**：存在\"语用差距\"(pragmatic gap)——即\"所说的\"和\"道德上隐含的\"之间的差距。正如论文所述：\"metaphorically speaking, the gap between what is said and what is morally implied\"。\n\n**理论支持**：引用Spencer-Oatey和Xing(2019)的研究，说明道德推理需要从语言使用、道德规范和行为后果的隐含意义中推导结论，而这正是LLMs所缺乏的能力。\n\n## 三、核心假设：通过语用推理弥合差距\n\n**假设形成**：如果能够将道德推理中通常未被言明的变量(如社会规范、元语用评价等)**文本化**(textualizing)，使LLMs能够学习这些变量，就可以弥合分布语义与语用学之间的差距。\n\n**理论框架选择**：采用**道德基础理论**(Moral Foundations Theory, MFT)作为框架，因为：\n1. MFT提供了一个原则性的方式来注释、分析和建模语言中的道德维度\n2. 六个道德基础(关怀、公平、自由、忠诚、权威、神圣)可以作为连接具体情境和抽象道德原则的桥梁\n3. 这些基础是\"common ground which underlie all interactions\"，为道德推理提供了统一的理论基础\n\n## 四、方法设计：三种语用推理框架\n\n基于上述假设，作者设计了三种逐步复杂的语用推理框架，形成从简单到复杂的逻辑演进：\n\n### 框架1：道德基础分类(MFC)\n- **输入**：经验法则(Rule of Thumb, RoT)和六个道德基础的定义\n- **推理步骤**：\n  1. 识别RoT中包含的判断和相关行动\n  2. 推断该行动的后果(行为后果)\n  3. 将行动和后果与相关道德基础联系起来，通过文本化它们的元语用链接\n- **目标**：从抽象规则中提取共同原则，建立RoT与MFs之间的联系\n\n### 框架2：道德判断\n- **输入**：提示-回复对和潜在的道德基础\n- **推理步骤**：\n  1. 解释道德基础的定义\n  2. 从回复中得出结论\n  3. 解释回复的结论如何维护或违反道德基础\n- **目标**：基于具体情境和道德基础做出道德判断，处理语境特定的解释\n\n### 框架3：联合道德基础分类和道德判断(MFC-Judgment)\n- **输入**：提示-回复对和六个道德基础的定义\n- **推理步骤**：\n  1. 基于提示，从回复中得出结论\n  2. 解释回复的结论与道德基础的关联\n  3. 做出道德判断，并解释结论如何遵守或违反道德基础\n- **目标**：自动推断道德基础并做出相应的道德判断，更接近现实场景\n\n**设计逻辑**：这三个框架形成了一个从抽象到具体、从简单到复杂的渐进过程，逐步逼近真实世界的道德推理场景。\n\n## 五、实验验证：方法有效性的实证检验\n\n**实验设计**：\n- 使用MIC(Moral Integration Corpus)数据集\n- 采用Llama3.2-1B和3B作为基础模型\n- 设计三种数据规模(5K、10K、23.5K样本)\n- 与基线方法(base和base+)进行比较\n\n**关键结果**：\n1. 在所有任务和数据规模上，提出的语用推理方法均显著优于基线方法\n2. 随着道德基础数量增加，基线方法性能显著下降，而提出的方法性能下降较小\n3. 干预实验表明，该方法确实促进了LLMs有效利用道德基础，而非仅依赖表面相关性\n4. 语言建模困惑度(perplexity)分析显示，提出的方法导致更低的困惑度，表明更好的语言建模能力\n\n## 六、深入分析：方法机制与局限性\n\n**道德基础依赖性分析**：通过替换模型自动选择的道德基础为真实标签，发现性能显著提升，证明模型确实依赖于道德基础进行推理。\n\n**个别道德基础分析**：发现模型在常见道德基础(如关怀)上表现更好，而在罕见道德基础(如神圣)上表现较差，表明数据不平衡是一个瓶颈。\n\n**方法泛化潜力**：讨论了将语用推理过程文本化的概念可能广泛适用于其他非逻辑推理任务，如理解隐喻、幽默和讽刺。\n\n## 七、结论与展望：从问题到解决方案的完整闭环\n\n**核心贡献**：通过语用推理方法弥合了分布语义与道德推理的语用学本质之间的差距，显著提高了LLMs在道德推理任务中的泛化能力。\n\n**理论意义**：证明了将道德推理中通常未被言明的变量文本化的有效性，为解决LLMs在语用学层面的局限性提供了新思路。\n\n**实践意义**：提出的三种语用推理框架为构建更具道德推理能力的AI系统提供了具体方法。\n\n**未来方向**：\n1. 探索不依赖道德基础的推理路径\n2. 解决数据不平衡问题\n3. 将方法扩展到多语言环境以考察文化差异\n\n这一逻辑链清晰地展示了作者从观察问题、分析根源、形成假设、设计方法到验证效果的完整思考过程，形成了一个从宏观问题到具体解决方案的严密逻辑演进。", "summary_translation": "道德推理已成为大型语言模型（LLMs）的一个有前景的研究方向，然而实现泛化仍然是一个核心挑战。从语言学的角度来看，这一困难的出现是因为LLMs擅长捕捉分布语义学（distributional semantics），而道德则是在语用（pragmatic）层面运作的，两者存在根本差异。本文探讨了LLMs如何在依赖分布语义学的情况下实现泛化的道德推理。我们提出了基于道德基础理论（moral foundations theory）的语用推理（pragmatic inference）方法，该方法在每一步都利用上下文信息来弥合语用差距，并引导LLMs将道德基础与道德推理目标联系起来。实验结果表明，我们的方法显著增强了LLMs在道德推理方面的泛化能力，为未来基于道德基础理论的研究奠定了基础。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#89", "title": "Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?", "link": "/arxiv/2509.24090", "arxiv_id": "2509.24090", "authors": "Matteo Boffa, Jiaxuan You", "summary": "Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.511685", "filter_reason": "这篇论文的核心贡献是研究大语言模型(LLMs)处理大量约束的能力，提出了\"大规模约束生成\"(LSCG)这一新问题，并开发了FoCusNet方法来提升LLMs在这一任务上的表现。从本质上看，论文关注的是LLMs的基础能力——约束解析和遵循能力，这属于逻辑推理和问题解决的通用能力范畴，而不是将LLMs作为工具应用到特定领域。论文评估了不同模型特性和技术（包括思维链）对性能的影响，这些都是提升LLM通用推理能力的研究方向。论文不涉及多模态、特定应用领域或模型基础设施等排除标准中的内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型(LLMs)处理大量约束的能力问题。针对大规模细粒度通用约束场景，我们提出了一种FoCusNet辅助模型，用于将原始大量约束解析为相关子集，并在Words Checker任务上通过准确率、精确率和召回率等指标验证了其有效性。实验表明，随着约束数量增加，传统方法性能显著下降，而FoCusNet实现了8-13%的准确率提升，证明了其在约束聚焦方面的优势。", "inspiration_trace": "# 从观察到解决方案：FoCusNet的逻辑演进\n\n## 1. 宏观问题发现：现实约束与当前研究的差距\n\n作者首先观察到现实世界中存在大量需要遵循众多约束的场景，如旅行指南、编程文档等。然而，现有研究主要聚焦于LLMs处理少量特定任务约束的能力，缺乏对大规模通用约束处理能力的探索。这形成了一个研究空白：**LLMs能否有效处理数百甚至数千个细粒度通用约束？**\n\n## 2. 问题框架定义：从具体到抽象\n\n基于上述观察，作者定义了\"大规模约束生成\"(LSCG)的新问题框架：\n- 核心特征：约束数量大(C ≥ 100)、细粒度但通用\n- 关键挑战：模型需从大量约束中识别与特定任务相关的部分\n- 典型场景：如\"如何在伊斯兰国家做一个好游客\"配合长篇旅行指南\n\n这一框架将抽象的现实需求转化为可研究的技术问题。\n\n## 3. 实验设计：Words Checker的创建\n\n为系统研究LSCG，作者设计了Words Checker作为具体测试实例：\n- 任务：判断句子是否包含禁用词列表中的任何词\n- 特点：简单明确，无需复杂推理，便于隔离约束数量的影响\n- 变量控制：创建不同规模的禁用词列表(100、500、1000)\n\n这一设计使作者能够精确测量约束数量增加对LLM性能的影响。\n\n## 4. 现有方法评估：发现性能瓶颈\n\n通过Words Checker实验，作者揭示了现有方法的显著局限：\n- 所有模型在约束数量增加时性能均大幅下降(~30%准确率损失)\n- 传统引导策略(如思维链、最佳N次)不仅无法缓解问题，反而导致过度思考和幻觉\n- 模型倾向于逐个处理约束，失去焦点，混淆推理过程与实际任务\n\n这些发现表明，简单扩展现有方法无法解决大规模约束问题。\n\n## 5. 核心假设形成：从人类认知获取灵感\n\n基于实验发现，作者提出关键假设：**通过预先筛选相关约束，可帮助LLM专注于重要信息，从而提高大规模约束下的性能**。这一假设受到两方面启发：\n- 人类认知策略：面对大量信息时先筛选再深入处理\n- 检索增强生成(RAG)的成功经验：通过检索相关信息辅助生成\n\n## 6. 方法设计：FoCusNet的架构创新\n\n为验证假设，作者设计了FoCusNet作为解决方案：\n- 定位：轻量级辅助模型，非基础模型修改\n- 功能：将原始约束集解析为相关子集，缩小LLM注意力范围\n- 训练：针对特定任务进行二元分类训练，识别约束相关性\n- 协作：与LLM形成互补关系，FoCusNet负责筛选，LLM负责最终判断\n\n这一设计在指令微调和简单测试引导之间提供了平衡点。\n\n## 7. 实验验证：假设证实与效果量化\n\n通过Words Checker实验，作者验证了FoCusNet的有效性：\n- FoCusNet本身达到90%的约束检测准确率\n- 成功缩小搜索空间(平均从1000词筛选至30个可疑词)\n- 与传统方法相比，实现8-13%的准确率提升\n- 即使在1000个约束的情况下，仍保持相对稳定的性能\n\n实验结果证实了作者的初始假设：约束预筛选确实能显著提升LLM在大规模约束场景下的表现。\n\n## 8. 贡献总结与未来展望\n\n最后，作者将研究定位为LLM约束处理领域的新探索方向：\n- 理论贡献：提出LSCG问题框架，扩展约束生成研究边界\n- 实践贡献：提供Words Checker测试基准和FoCusNet解决方案\n- 社区贡献：开源代码和数据集，促进领域发展\n\n同时，作者也指出了当前工作的局限性，为未来研究指明方向，如多模态约束处理、更通用架构设计等。\n\n这一完整逻辑链展现了从问题观察到方法创新再到实验验证的系统性研究过程，体现了作者在LLM约束处理领域的深入思考和创新能力。", "summary_translation": "近期研究探讨了大型语言模型 (Large Language Models, LLMs) 在少量特定任务要求的明确提示下的约束生成能力。相比之下，我们提出了大规模约束生成 (Large-Scale Constraint Generation, LSCG)，这是一个新问题，用于评估LLMs是否能够解析大量细粒度的通用约束列表。为了检验LLMs处理日益增多的约束的能力，我们创建了一个LSCG的实际实例，称为Words Checker。在Words Checker中，我们评估了模型特征（例如，大小、系列）和引导技术（例如，简单提示、思维链、N选最佳）对性能的影响。我们还提出了FoCusNet，一个小型专用模型，它将原始约束列表解析为更小的子集，帮助LLM专注于相关约束。实验表明，随着约束数量的增加，现有解决方案的性能显著下降，而FoCusNet则显示出8-13%的准确率提升。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#88", "title": "GEAR: A General Evaluation Framework for Abductive Reasoning", "link": "/arxiv/2509.24096", "arxiv_id": "2509.24096", "authors": "Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen", "summary": "Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.", "subjects": "Computation and Language, Artificial Intelligence, Machine Learning", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.511211", "filter_reason": "这篇论文的核心贡献是提出GEAR评估框架和基于动量的课程学习策略，专门用于评估和提升大语言模型的溯因推理能力。溯因推理(abductive reasoning)是一种重要的通用逻辑推理能力，涉及生成合理假设来解释观察到的现象，这直接符合研究目标中\"增强LLM逻辑推理能力\"的要求。 从筛选标准分析： 1. 核心判断：论文本质是改进LLM的基础推理能力，提出了新的评估框架和训练方法（基于动量的课程学习），而非将LLM作为工具应用于特定领域。 2. 正面指标：论文明确包含核心概念\"Large language models\"，能力方向聚焦于\"reasoning\"（特别是溯因推理这种逻辑推理），并提出了新的训练方法。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域或模型基础设施等排除内容。 4. 特殊情况：论文关注提升模型生成可靠假设的能力，这与减少幻觉、增强模型内在可靠性相关，属于提升通用推理质量的范畴。 论文通过GEAR框架评估和改进LLM的溯因推理能力，这种能力是通用推理能力的重要组成部分，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大语言模型溯因推理能力评估不足的问题。针对多种溯因推理任务，我们提出了一种GEAR评估框架，通过一致性、泛化性和多样性三个维度评估假设质量，并在9个LLMs和4个流行benchmark上通过自动化指标验证了其有效性。", "inspiration_trace": "# GEAR框架提出逻辑链分析\n\n## 1. 宏观问题：AI知识发现能力的评估困境\n\n论文从一个根本性问题出发：大型语言模型(LLMs)在指令遵循和演绎推理方面取得了显著进展，但一个核心问题仍未解决——**这些模型是否能够真正发现新知识，以及如何评估这种能力？** 这一宏观问题指向了AI研究中一个关键缺口：缺乏对模型创造性、发现性思维的系统评估方法。\n\n## 2. 问题聚焦：溯因推理的核心地位\n\n作者将问题聚焦到**溯因推理**(Abductive Reasoning)上，即生成合理解释观察结果的假设的能力。这种推理方式与科学发现、创新思维等高阶认知活动密切相关，却相比演绎推理受到的关注少得多。溯因推理成为评估AI知识发现能力的理想切入点，因为它直接体现了\"从观察到假设\"的核心认知过程。\n\n## 3. 现有方法批判：评估范式的不匹配\n\n作者深入分析了现有溯因推理评估方法的两大流派及其根本缺陷：\n\n### 基于黄金答案(Gold-answer)的评估\n- **理论缺陷**：采用Harman的\"最佳解释推理\"(IBE)定义，但\"最佳\"标准模糊不清，因情境而异\n- **实践缺陷**：依赖单一\"黄金\"假设，忽视了科学解释的多元性；约80%同样合理的假设被错误标记为不正确\n- **哲学缺陷**：违背了溯因推理的\"证据不确定性\"本质，即给定有限数据，多个假设可能同样合理\n\n### 基于人类的评估\n- **可扩展性问题**：昂贵、耗时、难以大规模应用\n- **主观性问题**：结果高度依赖评估者的专业背景和判断标准\n- **一致性问题**：不同评估者间的一致性低，结果难以复现\n\n这些批判揭示了现有评估范式与溯因推理本质之间的根本不匹配：**评估应该测试模型提出多种、新颖且合理的解释性假设的能力，而非测试与单一\"黄金\"解释的一致性。**\n\n## 4. 理论重构：回归溯因推理的本质\n\n为克服现有方法的局限性，作者回归Peirce的原始定义，将溯因推理重新概念化为\"从给定观察中生成假设的更一般任务\"。基于此，作者提出了评估科学假设的三个经典标准，构建了GEAR框架的理论基础：\n\n### 一致性(Consistency)\n- **核心思想**：假设必须与观察到的事实不矛盾\n- **形式化定义**：生成的假设f与观察集O一致，当且仅当∀(in_i, out_i) ∈ O, f(in_i) = out_i\n- **理论依据**：科学解释的基本要求，确保与现有证据兼容\n\n### 泛化性(Generalizability)\n- **核心思想**：好的假设应超越观察数据，对未见案例做出可测试预测\n- **形式化定义**：假设f1比f2更一般，如果其输入域D1大于D2（M(D1) > M(D2)）\n- **理论依据**：Popper的可证伪性理论，更好的假设承载更高的经验内容，做出更精确、风险更大的预测\n\n### 多样性(Diversity)\n- **核心思想**：假设集应提供多种不同视角，避免过早收敛到单一解释\n- **形式化定义**：通过γ-多样性（平均唯一预测数）和β-多样性（预测模式分散度）量化\n- **理论依据**：Chamberlin的\"多重工作假说\"和Platt的\"强推理\"，强调竞争性解释对科学进步的重要性\n\n## 5. 方法论构建：GEAR框架的设计\n\n基于上述理论基础，作者设计了GEAR(General Evaluation for Abductive Reasoning)框架，将抽象理论转化为可操作评估方法：\n\n### 技术实现路径\n1. **问题形式化**：将观察表示为输入-输出对集合O = {(in_i, out_i)}，假设表示为函数f\n2. **样本空间构建**：为每个任务定义适当的样本空间S，用于评估泛化性和多样性\n3. **度量方法设计**：\n   - 一致性：直接检查假设是否与所有观察匹配\n   - 泛化性：测量假设在样本空间S上的覆盖率\n   - 多样性：通过γ-多样性和β-多样性量化假设集的差异程度\n\n### 框架优势\n- **可扩展性**：完全自动化，无需人类黄金答案\n- **可靠性**：所有指标透明定义和计算，不依赖黑盒模型\n- **开放性**：分数仅在模型产生新的、合理的假设时提高，不会像静态基准那样饱和\n\n## 6. 实验验证：框架的有效性检验\n\n作者通过大规模实验验证GEAR框架的有效性：\n\n### 实验设计\n- **数据集**：四个流行的溯因推理基准(MINI-ARC, ACRE, LIST FUNCTIONS, ARC-2025)\n- **模型**：九个LLMs，包括API模型和开源模型\n- **规模**：1,500个问题，生成50,340个候选假设\n\n### 关键发现\n1. **一致性挑战**：即使是70B级别的模型，也只产生约20%一致的假设\n2. **观察数量影响**：增加初始观察数量会降低多样性，但对一致性影响有限\n3. **模型规模与能力**：模型大小与溯因多样性弱相关，表明数据和训练方法可能比原始规模更重要\n4. **黄金答案评估缺陷**：约80%同样合理的假设被现有基准错误标记为不正确\n\n这些发现不仅验证了GEAR框架的敏感性，还揭示了先前评估方法未能捕捉的重要洞见。\n\n## 7. 应用扩展：从评估到训练\n\n作者进一步将GEAR从评估框架扩展为训练信号，形成完整的解决方案：\n\n### 训练信号构建\n1. **偏好数据准备**：从生成的假设中构建偏好对，分三个阶段分配偏好：\n   - 指令遵循/格式合规性\n   - 一致性\n   - GEAR分数（泛化性和多样性）\n\n2. **基于动量的课程学习**：\n   - **核心思想**：先学习提高最快且最容易的内容，然后转向更困难的信号\n   - **实现方法**：使用指数加权移动平均(EWMA)跟踪每种偏好类型的损失，将最近的改进转化为采样权重\n   - **优势**：避免手动调整固定比例，让每个基础模型自然趋向其首选的混合比例\n\n### 训练效果\n- 提高了所有三个GEAR目标——一致性、泛化性和多样性\n- 这些增益转移到已建立的溯因推理基准测试中\n- 基于动量的课程学习策略始终优于固定比例基线\n\n## 8. 理论与实践的闭环：GEAR的完整价值\n\n作者通过GEAR框架实现了理论与实践的闭环：\n\n1. **理论贡献**：重新概念化了溯因推理评估，强调了多元假设的重要性\n2. **方法贡献**：提供了可操作、可扩展、可靠的评估框架\n3. **实践贡献**：将评估转化为训练信号，提高了模型的溯因推理能力\n4. **应用前景**：框架不仅限于编程领域，还可扩展到自然语言设置（需解决语义表示等挑战）\n\nGEAR框架不仅解决了\"如何评估溯因推理\"的问题，还通过\"如何利用评估改进模型\"形成了完整的解决方案，为AI系统的知识发现能力评估与提升提供了新范式。", "summary_translation": "自大型语言模型（large language models, LLMs，大型语言模型）出现以来，研究主要集中在指令遵循和演绎推理（deductive reasoning，演绎推理）上。一个核心问题仍然存在：这些模型能否发现新知识，以及我们如何评估这种能力？我们通过研究溯因推理（abductive reasoning，溯因推理）——即生成合理解释观察结果的假设的过程——来解决这个问题，并引入GEAR（General Evaluation for Abductive Reasoning，溯因推理通用评估），这是一个通用、全自动、透明且无标签的评估范式。GEAR通过三个指标对假设集进行评分：一致性（consistency，每个假设都能解释观察结果）、泛化性（generalizability，一致的假设能在未见输入上做出有意义的预测）和多样性（diversity，假设集涵盖不同的预测和模式）。以这种方式构建，GEAR具有可扩展性（无需人工标准答案）、可靠性（确定性评分与经典溯因一致）和开放性（只有当模型产生新的合理假设时分数才会提高，不像静态基准在准确率高时会饱和）。使用GEAR，我们对九个LLM在四个包含1,500个问题的溯因基准测试上进行了细粒度研究，生成了超过50,000个候选假设，并揭示了被标准答案或纯人工评估所掩盖的模型差异。我们进一步提出了一种基于动量的课程（momentum-based curriculum，基于动量的课程），通过学习速度调整GEAR衍生的训练数据：它从模型快速学习的内容开始，一旦模型在基础目标上变得自信，就转向更难的目标，如生成多样化的假设。在没有标准标签监督的情况下，这一策略改善了所有GEAR目标，并且这些改进转移到了已建立的溯因推理基准测试上。总体而言，GEAR提供了一个评估溯因的原则性框架，并提供无标签、可扩展的训练信号，帮助LLM产生更多样化和可靠的假设。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#97", "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "link": "/arxiv/2509.23982", "arxiv_id": "2509.23982", "authors": "Lucio La Cava, Andrea Tagarelli", "summary": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.", "subjects": "Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning, Neural and Evolutionary Computing", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.521223", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种名为PaLRS的新方法，用于改进大语言模型的基础能力。论文提出了一种训练-free的偏好对齐方法，通过利用LLM残差流中编码的偏好信号，提取轻量级转向向量来增强模型性能。这明显属于改进LLM本身的基础能力和提出新训练范式的研究，而不是将LLM作为工具应用到特定领域。 其次，论文满足多个正面指标： - 核心概念：明确关注大语言模型(LLMs) - 能力方向：特别强调了数学推理能力的提升，这正是通用推理能力的核心组成部分 - 训练方法：讨论了与RLHF等传统偏好对齐方法的对比，并提出了一种新的更高效的方法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文提到了数学推理和代码生成，但这是作为评估模型通用推理能力的基准，而不是针对特定领域的应用。 特别值得注意的是，论文明确指出PaLRS方法\"保留了基线的通用性能\"，并在\"数学推理和代码生成基准上取得了一致的改进\"，这直接表明其研究目标是提升LLM的通用推理能力，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、高效的偏好对齐方法来增强大语言模型的通用推理能力，完全符合研究目标。", "summary2": "本文旨在解决大型语言模型偏好对齐过程中需要大量数据和计算资源的问题。针对偏好对齐任务，我们提出了一种基于残差流的模型转向方法PALRS，并在多个开源LLMs上通过数学推理(GSM8K)和代码生成(HumanEval)基准测试验证了其有效性。PALRS仅需少量偏好对即可提取轻量级转向向量，在推理时应用，无需更新模型参数，实现了高效灵活的偏好对齐。", "inspiration_trace": "# 从问题到解决方案：PALRS方法的逻辑推演\n\n## 一、宏观问题：LLM偏好对齐的困境\n\n大型语言模型(LLMs)的偏好对齐是使其有用且符合人类偏好的关键步骤。然而，现有方法如RLHF和DPO面临三大核心挑战：\n- **数据依赖性**：需要大量精心策划的偏好数据集\n- **计算密集性**：需要对数十亿参数进行昂贵的优化\n- **缺乏灵活性**：一旦对齐，模型难以适应新的偏好设置\n\n这些限制使得偏好对齐成为资源密集且不够灵活的过程。\n\n## 二、关键观察：残差流中的可操控特征\n\n作者注意到一个新兴研究方向：LLMs的残差流激活编码了上下文丰富且可线性化的特征。这些特征可用于在不改变权重或不需要额外训练的情况下操控模型行为。已有研究表明残差干预能有效：\n- 缓解拒绝行为\n- 擦除概念\n- 诱导角色行为\n- 提高事实性\n\n这引出一个核心问题：能否利用残差流的特性来解决偏好对齐的挑战？\n\n## 三、核心假设：偏好方向的可提炼性\n\n作者提出假设：对于特定领域(如数学或编码)的问题，选择和拒绝响应的残差流激活之间的差异可以被提炼成引导方向，用于通过轻量级推理时间干预来诱导模型中的期望行为。\n\n为支持这一假设，论文提供了关键证据(图1)：\n1. **幅度证据**：选择和拒绝的激活在残差空间中相距较远(欧几里得距离大)，意味着可以定义有效的引导向量\n2. **方向一致性**：选择和拒绝激活之间的差异在方向上大多一致(余弦角度小)，表明可以提炼出可泛化的聚合引导向量\n\n这两个观察共同表明：残差流中确实编码了可用于偏好对齐的线性可访问信息。\n\n## 四、方法论构建：PALRS的形成\n\n基于上述观察和假设，作者构建了PALRS(Preference alignment of Large Language Models via Residual Steering)方法，其核心逻辑链如下：\n\n### 4.1 残差流激活的利用\n- 残差流激活是标记在模型中流动时累积的隐藏表示，编码了所有上下文信息\n- 这些激活是线性可解释的，因为子层输出被添加到残差流中\n- 残差流与模型的预测分布相关联，因为最终层将残差流投影到标记上\n\n### 4.2 候选偏好方向的提取\n采用差异均值方法计算选择和拒绝响应的残差流激活的平均值：\n```\nμ(+)i,ℓ = 1/|D| ∑ x_i,ℓ(t(+))\nμ(-)i,ℓ = 1/|D| ∑ x_i,ℓ(t(-))\n```\n候选偏好方向定义为两者之差：\n```\nr_i,ℓ = μ(+)i,ℓ - μ(-)i,ℓ\n```\n\n### 4.3 位置和层的选择策略\n- **位置选择**：关注指令后的标记位置，确保模型已处理给定文本并开始生成输出\n- **层选择**：专注于中后层(0.3L-0.9L)，因为早期层主要处理句法和结构特征，中期层出现语义和推理过程，后期层直接影响输出\n\n### 4.4 最优引导方向的选择\n选择与选择响应的平均残差流激活最严格对齐的向量：\n```\nr* = arg max_(i,ℓ)∈C |r_i,ℓ · μ(+)ℓ|\n```\n然后重新缩放r*使其范数与||μ(+)ℓ*||匹配，以便更好地控制激活添加步骤中的乘法因子效果。\n\n### 4.5 引导方向的应用\n通过激活添加将选定的偏好方向添加到模型生成的新响应的残差流激活中：\n```\nx'ℓ* := xℓ*(t) + αˆr*\n```\n其中α是控制引导效果强度的系数。\n\n## 五、方法验证：从理论到实践\n\n作者通过一系列实验验证了PALRS的有效性：\n1. **目标任务性能**：在数学推理(GSM8K)和代码生成(HumanEval)基准测试上，PALRS一致提升了模型性能\n2. **通用能力保持**：在其他领域的基准测试上，PALRS-aligned模型保持了基线的一般用途性能\n3. **与DPO的比较**：PALRS-aligned模型在性能上优于DPO-aligned模型，同时节省了大量时间\n4. **参数敏感性**：通过调整引导系数α，可以控制引导效果的强度，避免过度引导\n\n## 六、逻辑链总结\n\nPALRS方法的形成遵循了清晰的逻辑演进：\n1. **问题识别**：现有偏好对齐方法存在数据、计算和灵活性方面的限制\n2. **现象观察**：残差流激活编码了可线性化的特征，可用于操控模型行为\n3. **假设形成**：选择和拒绝响应的残差激活差异可提炼成有效的引导方向\n4. **方法构建**：设计了一套完整的残差引导框架，包括方向提取、选择和应用\n5. **实验验证**：在多个任务和模型上验证了方法的有效性和效率\n\n这一逻辑链从宏观问题出发，通过观察现象形成假设，进而构建系统方法论，最终通过实验验证，展现了一个完整的研究思路演进过程。PALRS的核心创新在于将残差流操控技术应用于偏好对齐领域，提供了一种无需训练、即插即用的对齐机制。", "summary_translation": "偏好对齐（Preference alignment）是使大型语言模型（Large Language Models, LLMs）变得有用并与（人类）偏好保持一致的关键步骤。现有方法，如人类反馈强化学习（Reinforcement Learning from Human Feedback）或直接偏好优化（Direct Preference Optimization），通常需要精心策划的数据和针对数十亿参数的昂贵优化，最终导致持久的特定任务模型。在这项工作中，我们介绍了通过残差引导（Residual Steering）实现大型语言模型偏好对齐（Preference alignment of Large Language Models via Residual Steering, PaLRS）的方法，这是一种无训练（training-free）的方法，利用了编码在大型语言模型残差流（residual streams）中的偏好信号。仅需少至一百个偏好对（preference pairs），PaLRS就能提取轻量级、即插即用（plug-and-play）的引导向量（steering vectors），可在推理时（inference time）应用，推动模型朝向期望的行为发展。我们在各种中小型开源大型语言模型上评估了PaLRS，结果表明经过PaLRS对齐的模型在数学推理和代码生成基准测试上取得了一致的性能提升，同时保持了基线的通用性能。此外，与经过直接偏好优化（Direct Preference Optimization, DPO）对齐的模型相比，它们表现更好且节省了大量时间。我们的研究结果表明，PaLRS为标准偏好优化流程提供了一种有效、更高效且更灵活的替代方案，它提供了一种无训练、即插即用的对齐机制，仅需最少的数据。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#102", "title": "Assessing Large Language Models in Updating Their Forecasts with New Information", "link": "/arxiv/2509.23936", "arxiv_id": "2509.23936", "authors": "Zhangdie Yuan, Zifeng Ding, Andreas Vlachos", "summary": "Prior work has largely treated future event prediction as a static task, failing to consider how forecasts and the confidence in them should evolve as new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework for evaluating whether large language models appropriately revise their predictions in response to new information. In particular, EVOLVECAST assesses whether LLMs adjust their forecasts when presented with information released after their training cutoff. We use human forecasters as a comparative reference to analyze prediction shifts and confidence calibration under updated contexts. While LLMs demonstrate some responsiveness to new information, their updates are often inconsistent or overly conservative. We further find that neither verbalized nor logits-based confidence estimates consistently outperform the other, and both remain far from the human reference standard. Across settings, models tend to express conservative bias, underscoring the need for more robust approaches to belief updating.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.529290", "filter_reason": "这篇论文的核心是研究大语言模型(LLMs)如何根据新信息更新预测的能力，属于对LLM通用推理能力的评估和研究。论文提出了EVOLVECAST框架来评估LLMs在面对新证据时如何调整预测和置信度，这直接涉及到模型的推理能力，特别是信念更新(belief updating)这一通用推理能力。从第一步核心判断来看，论文关注的是改进LLM的基础能力，而非将其作为工具应用到特定领域。论文满足正面指标中的核心概念(Large language models)和能力方向(reasoning)两个关键指标。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型基础设施等内容。虽然论文主要是评估性的而非提出新的改进方法，但它对LLM推理能力的深入理解和评估对于进一步改进LLM的通用推理能力具有重要意义，符合\"提高大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型如何根据新信息动态更新预测的问题。针对未来事件预测场景，我们提出了E VOLVE CAST框架，用于评估LLMs在面对训练截止后新信息时的预测更新能力。在Metaculus平台构建的1,613个问题-新闻对数据集上，通过MDA、MSE、SMSPE和Brier Score等指标验证了模型的方向一致性、幅度对齐和置信度校准。实验发现，尽管LLMs能对新信息做出响应，但更新常不一致或过于保守，且基于语言表达和logits的置信度估计均未达到人类预测者水平。", "inspiration_trace": "# 从静态到动态：EVOLVECAST框架的逻辑演进\n\n## 一、宏观问题：静态评估与现实需求的脱节\n\n作者首先观察到，现实世界中的决策本质上是动态的——决策者需要不断根据新信息调整对未来事件的预测。然而，当前大型语言模型(LLMs)的评估方法存在根本性局限：\n\n- **评估视角的静态性**：大多数评估关注模型对训练截止日期前已知信息的回忆和推理能力\n- **预测任务的动态性**：实际预测需要基于不完整证据不断调整信念，而非一次性判断\n- **能力盲区**：现有框架无法评估模型如何随时间演变更新其预测和置信度\n\n这引出了一个核心问题：**如何评估LLMs在动态环境中根据新信息更新预测的能力？**\n\n## 二、问题聚焦：预测中的信念更新机制\n\n从宏观问题出发，作者将焦点缩小到预测任务中的信念更新过程：\n\n1. **预测的本质特征**：\n   - 预测是时间性和动态的，不同于事实检索\n   - 需要基于不完整或不确定证据进行推理\n   - 要求在新信号出现时调整已有信念\n\n2. **现有研究的局限**：\n   - 即使考虑置信度的概率预测研究，也主要关注单次静态预测\n   - 缺乏对模型如何根据新信息调整信念的考察\n   - 置信度校准问题尚未在动态背景下得到充分研究\n\n3. **核心研究问题**：\n   - LLMs能否像人类预测者那样，根据新信息适当更新其预测和置信度？\n   - 这种更新在方向、幅度和校准方面与人类表现有何差异？\n\n## 三、关键假设：基于观察的理论构建\n\n作者通过文献观察和现实分析，形成了三个关键假设：\n\n1. **响应性假设**：LLMs可能能够根据新信息调整其预测，但这种调整可能与人类预测者的调整方式存在系统性差异。\n\n2. **校准假设**：无论是语言表达的置信度还是基于logits的置信度，都可能无法准确反映预测的不确定性，导致校准不良。\n\n3. **保守性假设**：LLMs在更新预测时可能表现出保守性，即对新信息的反应不足，更新幅度小于人类预测者。\n\n这些假设构成了后续实验验证的理论基础，也为评估框架的设计提供了方向。\n\n## 四、方法论构想：动态评估框架的初步设计\n\n基于上述假设，作者开始构思评估框架的核心要素：\n\n1. **动态场景模拟**：需要构建一个随时间演变的信息环境，模拟真实预测场景\n2. **变化测量机制**：需要设计方法来捕捉和量化模型预测和置信度的变化\n3. **人类参考标准**：需要将模型行为与人类预测者的行为进行比较，建立评估基准\n\n这一阶段的核心挑战是如何将抽象的\"信念更新\"概念转化为可操作、可测量的评估指标。\n\n## 五、EVOLVECAST框架：从构想到具体实现\n\n作者将初步构想发展为完整的EVOLVECAST框架，包含三个关键组成部分：\n\n### 1. 任务形式化\n- 将动态预测任务定义为：评估LLMs是否以与参考标准相当的方式根据新信息修改其预测和置信度\n- 形式化表示：对于预测问题q，在时间t出现新信息x_t，测量模型信念p_t = P(Yes|x_t)的变化Δp = p_t - p_0\n\n### 2. 多维度评估标准\n为全面评估信念更新质量，作者设计了三个互补的评估维度：\n\n- **方向一致性**：使用平均方向准确性(MDA)评估模型和参考预测是否在同一方向更新信念\n- **幅度一致性**：通过均方误差(MSE)和对称均方百分比误差(SMSPE)衡量更新幅度的匹配程度\n- **置信度校准**：使用Brier分数评估模型置信度与参考标准的一致性\n\n### 3. 数据集构建\n- 从Metaculus预测平台获取问题和人类预测数据\n- 通过监控评论流检测新信息出现的时间点\n- 使用Google Search API检索相关新闻文章，并通过语义相似度选择最相关的更新\n- 最终构建包含1,613个问题-新闻对的数据集\n\n## 六、实验验证：从理论到实证\n\n作者设计了系统的实验来验证假设并评估框架有效性：\n\n### 1. 模型选择\n- 评估三个开源推理模型及其基础对应模型\n- 覆盖不同架构(Qwen和LLaMA)和参数规模(1.5B/7B/8B)\n- 目的是分离推理风格后训练的效果\n\n### 2. 实验条件\n- **基础条件**：仅提供预测问题，测量初始预测\n- **更新条件**：提供预测问题+新闻更新，测量更新后预测\n- **累积条件**：提供完整序列的新闻更新，测试 richer 上下文的影响\n\n### 3. 置信度提取方法\n- **黑盒方法**：模型明确表达1-10尺度的置信度，归一化到[0,1]\n- **白盒方法**：从模型输出概率中计算平均token概率\n\n## 七、结果分析与理论完善\n\n实验结果证实了作者的初始假设，并揭示了更深层次的发现：\n\n1. **响应性验证**：LLMs确实能够根据新信息调整预测，但更新往往不一致或过于保守\n\n2. **校准问题**：两种置信度提取方法均未显示出明显优势，且都远未达到人类参考标准\n\n3. **保守性确认**：模型普遍表现出保守偏差，对新信息的反应幅度小于人类预测者\n\n4. **意外发现**：\n   - 累积新闻上下文并未改善性能，反而可能导致下降\n   - 直接方向性提示比基于概率的方法产生了更高的方向一致性\n   - 即使提供人类预测作为参考锚点，模型也无法有效利用\n\n这些发现不仅验证了初始假设，还揭示了LLMs在建模信念动态方面的基本困难，强调了需要超越静态预测准确性的评估框架。\n\n## 八、逻辑演进总结\n\n从宏观问题到具体方法，作者的思考过程展现了一个清晰的逻辑演进：\n\n**现实需求(动态决策) → 研究空白(静态评估) → 核心问题(信念更新能力) → 关键假设(响应性、校准、保守性) → 方法构想(动态评估框架) → 具体实现(EVOLVECAST) → 实验验证 → 理论完善**\n\n这一逻辑链不仅展示了作者如何从观察现实世界需求出发，识别研究空白，形成假设，设计评估框架，并通过实验验证假设的完整思考过程，也体现了学术研究从问题识别到方法构建的标准路径。最终，EVOLVECAST框架填补了评估LLMs动态预测能力的研究空白，为未来的研究提供了新的方向和方法论基础。", "summary_translation": "先前的研究大多将未来事件预测视为静态任务，未能考虑随着新证据出现，预测及其置信度应如何演变。为填补这一空白，我们提出了EVOLVECAST，一个用于评估大型语言模型（large language models）是否能够根据新信息适当修正其预测的框架。具体而言，EVOLVECAST评估当大型语言模型（LLMs）面对训练截止日期后发布的信息时，是否会调整其预测。我们以人类预测者作为比较参考，分析在更新情境下的预测变化和置信度校准（confidence calibration）。虽然大型语言模型（LLMs）对新信息表现出一定的响应性，但它们的更新往往不一致或过于保守。我们进一步发现，无论是语言化（verbalized）还是基于logits的置信度估计，都没有一种能持续优于另一种，且两者都远未达到人类参考标准。在各种设置中，模型往往表现出保守偏差（conservative bias），这凸显了对更稳健的信念更新（belief updating）方法的迫切需求。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#99", "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs", "link": "/arxiv/2509.23967", "arxiv_id": "2509.23967", "authors": "Ken Deng, Zizheng Zhan, Wen Xiang, Wenqiang Zhu, Tianhao Peng, Xinping Lei, Weihao Li, Jingxuan Xu, Kun Wu, Yifan Yao, Haoyang Huang, Huaixi Tang, Kepeng Lei, Zhiyi Lai, Songwei Yu, Zongxian Feng, Zuchen Gao, Weihao Xie, Chenchen Zhang, Yanan Wu, Yuanxing Zhang, Lecheng Huang, Yuqun Zhang, Jie Liu, Zhaoxiang Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu", "summary": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off). Specifically, HiPO combines a hybrid data pipelineproviding paired Think-on and Think-off responseswith a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning. Experiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy. Finally, we hope HiPO a can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.527735", "filter_reason": "根据筛选标准，这篇论文完全符合研究范围。首先，从核心判断来看，论文的本质是改进大语言模型的基础推理能力，提出了混合策略优化(HiPO)这一新框架，使LLM能够自适应地选择推理策略（何时进行详细推理，何时直接响应）。这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、多步推理等通用能力\"的标准。 其次，论文包含多个正面指标：明确关注Large Language Models (LLMs)这一核心概念；专注于reasoning能力（特别是在数学和编码基准测试中验证）；采用了reinforcement learning作为训练方法（混合强化学习奖励系统）。 第三，论文不符合任何排除标准：不涉及多模态与视觉内容；不聚焦于特定应用领域（虽然使用数学和编码基准测试，但这些是通用推理能力的评估，而非特定领域应用）；不关注模型可靠性层面的水印、安全等问题。 论文的核心贡献是提出了一种能动态控制推理过程的新方法，在保持或提高准确性的同时显著提高推理效率，这直接服务于提升LLM的通用推理能力这一研究目标。因此，这篇论文应该被保留。", "summary2": "本文旨在解决大型语言模型中过度思考问题，导致不必要的token使用和推理成本增加。针对复杂推理任务场景，我们提出了一种HiPO(Hybrid Policy Optimization)框架，使LLMs能够有选择地决定何时进行详细推理(Think-on)，何时直接响应(Think-off)。在数学和编码基准测试上通过准确率和token长度等指标验证了其有效性，显著减少了token长度同时保持或提高了准确性。", "inspiration_trace": "# HiPO方法逻辑链分析：从问题洞察到解决方案\n\n## 一、宏观问题：LLM推理的效率与准确性矛盾\n\n**起点观察**：大型语言模型(LLMs)在复杂任务上取得成功，关键在于思维链(CoT)推理——通过分步解析提升准确性。\n\n**问题识别**：CoT导致\"过度思考\"(overthinking)现象——即使是简单问题也生成冗长推理链，造成token使用量激增、延迟提高、效率下降，形成推理质量与计算成本的根本张力。\n\n## 二、现有方法分析与局限\n\n**方法分类**：作者系统梳理两类自适应推理控制方法：\n1. 基于训练的自适应推理：RL/SFT通过长度惩罚或简洁奖励鼓励简洁推理\n2. 外部控制：手工提示或动态指令限制推理\n\n**局限洞察**：现有方法存在三重缺陷：\n- 监督信号粗糙，无法精确区分问题复杂度\n- 单调激励阻碍模型对困难问题进行深入推理\n- 缺乏准确性、延迟与token效率间的原则性权衡机制\n\n## 三、核心假设形成\n\n**关键洞察**：需要一种动态平衡机制，使模型能自主决定何时\"深度思考\"(Think-on)、何时\"直接回答\"(Think-off)。\n\n**双假设构建**：\n1. **混合数据假设**：提供成对的Think-on/Think-off响应数据，能让模型学习问题复杂度与推理深度的映射关系\n2. **混合奖励假设**：设计平衡两种模式的奖励系统，可防止过度依赖冗长推理，同时确保模式选择与实际性能增益一致\n\n## 四、方法论设计：HiPO框架\n\n### 1. 混合数据构建管道\n\n**数据生成逻辑**：\n- 为每个查询在两种模式下各采样N个响应，验证正确性\n- 选择通过率更高的模式；若差异小于阈值δ，则倾向Think-off（鼓励简洁）\n- 保留获胜模式中最短的正确响应，强制1%查询随机分配模式以增加多样性\n- 生成模式选择解释，增强模型对决策理由的理解\n\n**格式设计**：使用特殊标记(<judge>、<think_on/off>、<answer>)清晰分隔判断、推理和答案，形成结构化训练信号\n\n### 2. 混合强化学习奖励系统\n\n**奖励机制设计**：\n- **基本奖励**：结合答案正确性(ACC)和格式正确性(FORMAT)\n- **偏差调整**：动态规范化模式贡献，防止过度拟合Think-on模式\n- **双优势函数**：\n  * 判断优势(Ajudge)：全局视角评估模式选择价值\n  * 答案优势(Aanswer)：模式内视角评估响应质量\n- **令牌级分配**：根据令牌所属段(判断/答案)分配相应优势值\n\n**训练范式**：两阶段策略——冷启动阶段建立基础能力，强化学习阶段精细调优模式选择\n\n## 五、验证与优化\n\n**实验验证**：在数学和编程多基准测试中证明HiPO能显著减少token长度(平均30%)，同时保持或提高准确性(平均提升6.2%)\n\n**消融研究**：系统验证各组件贡献，包括：\n- 最短响应选择策略的有效性\n- 全局/局部优势函数的必要性\n- 超参数(γ、N、ω)对性能的影响\n\n**动态分析**：追踪训练过程中Think-on/Think-off激活比例变化，证明模型确实学会根据任务难度自适应调整推理深度\n\n## 六、逻辑链条总结\n\n从\"过度思考\"问题观察→现有方法局限分析→动态平衡需求洞察→混合数据与混合奖励双假设→HiPO框架设计→实验验证优化，形成完整闭环。作者通过系统性思考，将LLM推理的效率-准确性矛盾转化为可计算的自适应控制问题，创新性地结合数据构建与强化学习，实现了对推理深度的原则性控制。", "summary_translation": "大型语言模型(LLMs)越来越依赖思维链(CoT, chain-of-thought)推理来提高在复杂任务上的准确性。然而，总是生成冗长的推理过程是低效的，导致过多的令牌(token)使用和更高的推理成本。本文介绍了混合策略优化(Hybrid Policy Optimization, 即HiPO)，这是一个自适应推理控制框架，使大型语言模型能够有选择地决定何时进行详细推理(Think-on)以及何时直接响应(Think-off)。具体而言，HiPO结合了一个混合数据管道（提供成对的Think-on和Think-off响应）和一个混合强化学习(reinforcement learning)奖励系统，该系统在平衡准确性和效率的同时，避免过度依赖详细推理。在数学和编码基准测试(benchmarks)中的实验表明，HiPO可以在保持或提高准确性的同时，显著减少令牌长度。最后，我们希望HiPO能成为一种高效自适应推理的原则性方法，推动面向推理的大型语言模型在现实世界资源敏感环境中的部署。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#103", "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step", "link": "/arxiv/2509.23924", "arxiv_id": "2509.23924", "authors": "Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao", "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.529793", "filter_reason": "这篇论文的核心贡献是提出了解决Masked Diffusion Language Models (MDLMs)训练和推理不一致性的新方法，包括EOSER和ASS解码调度器以及CJ-GRPO强化学习算法。从第一步核心判断来看，论文本质上是关于改进LLM的基础能力和提出新的训练范式的研究，特别是增强模型的推理能力，这符合保留标准。从第二步正面指标看，论文涉及大语言模型(MDLMs)、推理能力(明确在数学和规划基准测试上实验)以及强化学习方法(CJ-GRPO)，满足多个正面指标。从第三步排除标准看，论文虽然标题中提到\"Diffusion\"，但这是指语言模型架构而非视觉领域的扩散模型，且论文没有聚焦于特定应用领域或模型可靠性的应用层面问题。综合分析，这篇论文致力于提高大语言模型本身的通用推理能力，通过新的解码策略和强化学习算法来优化模型性能，完全符合\"大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在优化掩码扩散语言模型(MDLMs)的解码效率和强化学习训练。针对MDLMs在推理过程中存在的<EOS>陷阱和轨迹不一致问题，我们提出了EOSER、ASS解码调度器和CJ-GRPO算法，并在数学和规划推理任务上通过准确率等指标验证了其有效性。实验结果表明，所提方法能显著减少解码步骤（从L/2到log₂L），同时保持或提高模型性能，特别是在规划任务上表现突出。", "inspiration_trace": "# 从问题观察到方法构建：MDLMs优化逻辑链分析\n\n## 一、宏观问题：掩码扩散语言模型的未解之谜\n\n掩码扩散语言模型(MDLMs)作为自回归模型的潜在替代方案，展现出并行解码、灵活生成顺序和更少推理步骤的潜力。然而，一个根本性问题悬而未决：**直接将自回归模型中成熟的技术迁移到MDLMs是否是最优解？** 这一宏观问题引出了两个关键挑战。\n\n## 二、具体挑战：解码策略与强化学习的不一致性\n\n### 挑战1：解码策略的悖论\n作者观察到：在MDLMs训练中未使用的block-wise和semi-AR解码策略，在推理时却意外地优于全扩散式解码。这引发了一个核心问题：**为什么未经专门训练的解码策略反而表现更好？**\n\n### 挑战2：强化学习的轨迹不一致\n直接将AR模型设计的RL算法应用于MDLMs会导致训练-推理不一致，因为MDLM解码是非因果的(并行的)，这导致rollout轨迹和优化轨迹之间存在根本性不匹配。\n\n## 三、关键观察：揭示MDLMs的行为模式\n\n通过实验，作者发现了三个关键现象：\n\n1. **token置信度演化规律**：在去噪过程中，平均token置信度在初始步骤较低，但随着去噪进行急剧增加。\n   \n2. **<EOS>置信度异常**：<EOS>的平均置信度显著高于<non-EOS>，尤其在早期步骤。\n\n3. **解码策略性能差异**：semi-AR(block-wise)解码优于全扩散解码，因为它通过块约束避免了在早期步骤解码序列末尾的<EOS>。\n\n这些观察揭示了一个核心问题：**全扩散解码存在\"<EOS>陷阱\"**——在早期低置信度阶段，模型过早生成序列结束符，导致偏离正确的去噪轨迹。\n\n## 四、假设形成：从现象到理论解释\n\n基于观察，作者形成了三个核心假设：\n\n1. **<EOS>陷阱假设**：全扩散解码性能较差是因为在早期低置信度阶段，模型对<EOS>的置信度过高，导致过早生成序列结束符。\n\n2. **非均匀调度假设**：由于token置信度从低到急剧增加的变化趋势，均匀的步骤大小解码调度器可能是次优的。早期应谨慎解码(解码较少token)，后期应更积极(解码更多token)。\n\n3. **轨迹一致性假设**：由于MDLM的非因果性质，直接应用AR模型的RL算法会导致rollout轨迹和优化轨迹不一致，从而引入优化误差。\n\n## 五、方法论构建：三管齐下的解决方案\n\n### 1. EOS Early Rejection (EOSER)\n**目标**：解决全扩散解码中的\"<EOS>陷阱\"问题。\n\n**方法**：引入随去噪步骤s变化的衰减系数γ，早期抑制<EOS>的置信度，随去噪进行逐渐恢复其概率。\n\n```\nγ = γmin + (γmax - γmin) * s/(S-1)\n```\n\n**核心思想**：通过动态调整<EOS>的置信度，防止模型在早期过早结束序列，同时允许在适当时候生成序列结束符。\n\n### 2. Ascending Step-Size (ASS) Decoding Scheduler\n**目标**：利用token置信度变化趋势，减少解码步骤数量。\n\n**方法**：用2的幂次方调度替代固定步骤大小，早期解码较少token，后期解码更多token。\n\n```\nAscending Step-Size Scheduler(s) = {2^(s+1), if s = S-1; 2^s, otherwise}\n```\n\n**核心思想**：将解码步骤从L/2减少到log₂L，时间复杂度从O(L)降低到O(log₂L)，实现更高效的推理。\n\n### 3. Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\n**目标**：确保rollout和优化轨迹的一致性，减少优化误差。\n\n**方法**：记录每个去噪步骤的中间状态(包括token置信度和解码位置)，利用这些状态指导优化轨迹。\n\n```\npθ(x^s_g | x^{s-1}_g) = πθ(·| x^{s-1}_g) · pos^s_g\n```\n\n**核心思想**：通过保持rollout和优化轨迹的一致性，解决MDLM非因果性质导致的RL优化不一致问题。\n\n## 六、实验验证：从理论到实践\n\n作者在数学和规划任务上验证了方法的有效性：\n\n1. **EOSER**显著提高了全扩散解码的性能，有效缓解了\"<EOS>陷阱\"。\n2. **ASS调度器**在保持性能的同时大幅减少了解码步骤，实现了O(log₂L)的时间复杂度。\n3. **CJ-GRPO**与EOSER和ASS调度器结合，在更少解码步骤下实现了与更多步骤相当甚至更好的性能。\n\n特别值得注意的是，作者还发现了任务特性与解码策略的匹配关系：规划任务(如Countdown和Sudoku)更适合并行推理的全扩散解码，而数学任务(如GSM8K和MATH500)更适合顺序推理的semi-AR解码。这一发现为未来混合扩散式和自回归式推理提供了方向。\n\n## 七、逻辑链总结\n\n从宏观问题到具体方法论的完整逻辑链：\n\n**宏观问题** → **具体挑战** → **关键观察** → **核心假设** → **方法论构建** → **实验验证**\n\n这一逻辑链展示了作者如何从\"MDLMs技术迁移是否最优\"的宏观问题出发，通过观察\"<EOS>陷阱\"等现象，形成关于解码策略和强化学习一致性的假设，最终构建出EOSER、ASS和CJ-GRPO三管齐下的解决方案，并通过实验验证其有效性。这一系统性思考过程为MDLMs的优化提供了新的思路和方法。", "summary_translation": "掩码扩散语言模型（Masked diffusion language models, MDLMs）最近作为一种自回归（autoregressive, AR）语言模型的有前景的替代方案而出现，提供了并行解码（parallel decoding）、灵活生成顺序（flexible generation orders）以及更少推理步骤（inference steps）的潜力等特性。尽管有这些优势，但专门为MDLMs量身定制的解码策略和强化学习（reinforcement learning, RL）算法仍然研究不足。一种朴素的方法是将AR模型中成熟的技术直接转移到MDLMs上。然而，这引发了一个直接的问题：这种朴素的转移真的是最优的吗？例如，1）分块（Block-wise）和半自回归（semi-AR）解码策略在MDLMs的训练过程中并未被使用，那么为什么它们在推理过程中会优于完整的扩散式解码呢？2）将为AR模型设计的RL算法直接应用于MDLMs表现出训练-推理不一致性，因为MDLM解码是非因果（non-causal）的（并行的）。这导致了推演轨迹（rollout trajectory）和优化轨迹（optimization trajectory）之间的不一致。\n\n为了应对这些挑战，我们提出了EOS早期拒绝（EOS Early Rejection, EOSER）和递增步长（Ascending Step-Size, ASS）解码调度器，它们释放了MDLMs执行完整扩散式解码的潜力，在更少的解码步骤中实现了有竞争力的性能。此外，我们引入了一致性轨迹组相对策略优化（Consistency Trajectory Group Relative Policy Optimization, CJ-GRPO）来驯服MDLMs，它强调了推演轨迹和优化轨迹之间的一致性，并减少了由跳步优化（skip-step optimization）引起的优化错误。我们使用LLaDA-8B-Instruct在推理任务上进行了广泛的实验，如数学和规划基准测试。结果表明，所提出的EOSER和ASS机制，以及CJ-GRPO，在有效且高效地驯服MDLMs方面显示出巨大的潜力。\n\n代码：https://github.com/yjyddq/EOSER-ASS-RL。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#106", "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models", "link": "/arxiv/2509.23863", "arxiv_id": "2509.23863", "authors": "Ziyi Yang, Weizhou Shen, Ruijun Chen, Chenliang Li, Fanqi Wan, Ming Yan, Xiaojun Quan, Fei Huang", "summary": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles-questioner, responder, and verifier-within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder's output and the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model's evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.531494", "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出SPELL（一种多角色自我博弈强化学习框架）来提高大语言模型的长上下文推理能力。从第一步核心判断来看，论文本质上是关于改进LLM的基础推理能力（长上下文推理），提出了新的训练范式（自我博弈强化学习），属于增强LLM通用推理能力的研究。从第二步正面指标看，论文包含了所有关键主题：核心概念（LLMs）、能力方向（reasoning）、训练方法（reinforcement learning, self-evolve）和新兴范式（multi-agent systems）。论文不符合第三步的任何排除标准，因为它不涉及多模态、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的多角色框架是一种通用的智能体协作方法，用于增强LLM的通用问题解决能力，而非针对特定领域。总之，SPELL框架通过自我博弈和强化学习直接提升了LLM的通用推理能力，与研究目标高度一致。", "summary2": "本文旨在解决长上下文语言模型推理能力提升的问题。针对长文本处理困难及人工标注稀缺的场景，我们提出了一种SPELL多角色自我博弈强化学习框架，集成了提问者、响应者和验证者三个循环角色，并在六个长上下文基准测试上通过pass@k等指标验证了其有效性，实现了平均7.6点的性能提升。", "inspiration_trace": "# SPELL方法逻辑链推演\n\n## 宏观问题：长上下文推理能力的发展瓶颈\n\n作者从观察到的一个核心现象出发：**大语言模型(LLMs)的长上下文推理能力发展明显滞后于其他能力**。这一滞后不仅源于长文本处理的固有难度，更因为两个关键瓶颈：\n- 人工标注成本高昂且不可靠（在LongBench-V2上人类准确率仅25.1%）\n- 缺乏可编程验证的奖励信号（传统RLVR方法难以应用于长上下文场景）\n\n## 问题聚焦：外部监督的局限性\n\n作者进一步观察到，随着模型能力接近或超越人类水平，外部监督成为根本性瓶颈：\n- 现有长上下文对齐方法（LongAlign、LongPO等）均依赖某种形式的外部监督\n- 上下文长度增加时，可靠标注获取难度和成本呈指数级增长\n- 监督多样性随上下文长度增加而减少\n\n## 核心假设：自博弈强化学习的潜力\n\n基于上述观察，作者提出核心假设：**自博弈强化学习可以让模型通过生成和解决自己的任务来实现自我进化，无需人类标签**。但这一假设面临关键挑战：\n- 长上下文推理中，答案可能语义正确但表达方式差异很大\n- 简单字符串匹配或多数投票无法提供可靠奖励信号\n\n## 初步思路：多角色自我博弈框架\n\n作者提出初步解决方案：**单一模型扮演三个互补角色**：\n1. **提问者**：从原始文档生成问题及参考答案\n2. **回答者**：基于文档尝试解决这些问题\n3. **验证者**：评估回答者输出与参考答案的语义等价性\n\n这一设计形成闭环系统，使模型能够自主生成训练数据和奖励信号。\n\n## 关键创新：解决具体挑战\n\n作者进一步聚焦并解决了三个关键挑战：\n\n### 1. 稳定奖励信号生成\n**问题**：如何验证语义等价性？  \n**解决方案**：验证者通过在可验证任务上训练自一致性，使用多数投票聚合多个判断，减少方差。  \n**创新点**：结合规则检查和验证者共识，`rres_i = max(Rrule(yi, a), vver_i)`，避免语义正确但表达不同导致的误判。\n\n### 2. 自适应课程学习\n**问题**：如何确保问题难度与模型能力匹配？  \n**解决方案**：引入历史记忆机制，逐步增加文档长度和问题复杂度。  \n**创新点**：高斯形状奖励函数，以0.5成功概率为中心，`rque = exp(-(rres-μ)²/2σ²)`，确保问题既不太简单也不过于困难。\n\n### 3. 训练稳定性保障\n**问题**：如何平衡不同角色的贡献？  \n**解决方案**：角色特定动态采样策略，防止验证者样本主导更新。  \n**创新点**：统一策略优化，联合优化所有三个角色的GRPO目标，`JGRPO(θ) = Jque_GRPO(θ) + Jres_GRPO(θ) + Jver_GRPO(θ)`。\n\n## 最终方法：SPELL框架\n\n通过上述逻辑链，作者最终形成了SPELL（Self-Play Reinforcement Learning for Evolving Long-Context Language Models）框架，这是一个：\n- **自给自足的闭环系统**：单一模型通过扮演三个互补角色实现自我进化\n- **无需外部监督**：不依赖人工标注或可编程验证的奖励信号\n- **自适应课程学习**：根据模型能力动态调整任务难度\n- **训练稳定高效**：通过角色特定奖励设计和动态采样策略确保训练稳定性\n\n这一方法在12个不同架构和大小的模型上验证，一致提升了长上下文推理能力，甚至在某些情况下超越了依赖大量人工标注的指令微调模型，证明了自博弈强化学习在长上下文推理领域的巨大潜力。", "summary_translation": "大型语言模型（LLMs，大型语言模型）在长上下文推理（long-context reasoning，长上下文推理）方面的进展一直落后于其他最近的进展。这一差距不仅源于处理长文本的固有难度，还源于可靠的人工注释（human annotations，人工注释）和可编程验证的奖励信号（programmatically verifiable reward signals，可编程验证的奖励信号）的稀缺。在本文中，我们提出了SPELL，一个多角色自我博弈强化学习（multi-role self-play reinforcement learning，多角色自我博弈强化学习）框架，能够实现长上下文推理的可扩展、无标签优化（scalable, label-free optimization，可扩展、无标签优化）。SPELL在单个模型中集成了三个循环角色——提问者（questioner，提问者）、响应者（responder，响应者）和验证者（verifier，验证者），以实现持续的自我改进。提问者从原始文档生成问题并配以参考答案；响应者学习基于文档解决这些问题；验证者评估响应者输出与提问者参考答案之间的语义等价性（semantic equivalence，语义等价性），产生奖励信号以指导持续训练。为了稳定训练，我们引入了一个自动课程（automated curriculum，自动课程），逐渐增加文档长度，以及一个奖励函数（reward function，奖励函数），根据模型不断发展的能力调整问题难度。在六个长上下文基准测试（long-context benchmarks，长上下文基准测试）上的广泛实验表明，SPELL在各种LLMs上持续提高性能，并且优于在大规模注释数据上微调的同等规模模型。值得注意的是，SPELL在强大的推理模型Qwen3-30B-A3B-Thinking上实现了pass@8指标平均7.6分的提升，提高了其性能上限，并显示出扩展到更强大模型的潜力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#109", "title": "Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions", "link": "/arxiv/2509.23782", "arxiv_id": "2509.23782", "authors": "Yoonah Park, Haesung Pyun, Yohan Jo", "summary": "Large Language Models (LLMs) often fail on multiple-choice questions (MCQs) despite demonstrating correct knowledge in other contexts, such as free-form generation. To investigate the mechanism underlying this knowledge-prediction gap on MCQs and alleviate it, we conduct a probing analysis and find that residual streams in certain layers contain a subspace spanned by two important bases: a \\emph{knowledge basis} that encodes the probability of the ground-truth answer for a given MCQ and a \\emph{prediction basis} that encodes the probability of the answer choice predicted by the model. We observe that incorrect predictions arise from a misalignment of the model's hidden states along these two bases. Hence, we introduce \\textbf{KAPPA} (Knowledge-Aligned Prediction through Projection-based Adjustment), a parameter-free intervention that transforms the hidden states to align the prediction coordinate with the knowledge coordinate within this subspace. Experiments on binary-choice reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA substantially improves accuracy and consistently outperforms baselines. While optimal subspaces differ across tasks, subspaces generalize to some extent, as supported by cross-dataset experiments. Moreover, KAPPA extends its effectiveness to free-form questions beyond MCQs. Our work provides a new geometric understanding of the knowledge-prediction gap and offers a practical method for better aligning model behavior with its latent knowledge.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.538097", "filter_reason": "我按照筛选标准对这篇论文进行了全面分析，判断其符合研究目标。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLMs)在多选题场景中的\"知识-预测差距\"问题，并提出了解决方案。论文的核心贡献是发现LLM内部存在知识基和预测基两个重要维度，并提出了KAPPA方法来对齐这两个维度，从而提高模型的推理准确性。这明显属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确研究Large language models (LLMs)，并聚焦于reasoning和problem-solving能力，特别是解决多选题这类需要推理的任务。虽然论文没有涉及强化学习、智能体等新兴范式，但已足够表明其与研究目标相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不专注于医疗、化学等特定应用领域，也不讨论水印、安全等模型可靠性问题。 最后，论文提出的KAPPA方法是一种通用的干预方法，用于对齐模型的预测与其潜在知识，这可以被视为提升LLM通用推理能力的方法，而非针对特定领域的应用。论文还指出该方法的效果可以扩展到多选题以外的自由形式问题，进一步证明其通用性。 综上所述，这篇论文致力于提高大语言模型的通用推理能力，符合研究目标的核心要求。", "summary2": "本文旨在解决大语言模型在多项选择题上存在的知识-预测差距问题。针对LLMs在MCQs中无法正确表达其内部知识的现象，我们提出了一种基于投影调整的无参数干预方法KAPPA，通过在残差流中识别知识-预测子空间并对齐坐标来弥合差距。在Big-Bench-Hard和ARC-Challenge的二项选择重构数据集上，通过准确率指标验证了其有效性，显著提升了模型性能并展现出跨数据集泛化能力。", "inspiration_trace": "# 从观察到干预：KAPPA方法的逻辑演进\n\n## 1. 宏观问题：知识-预测差距的发现\n\n**起点现象**：大型语言模型(LLMs)在多选题(MCQ)评估中表现不佳，尽管它们在其他情境(如自由形式生成)中展示了正确的知识。这引发了一个核心问题：为什么模型拥有知识却无法在特定任务中正确表达？\n\n**关键观察**：\n- 模型可能在自由形式设置中生成正确答案，但在同一问题的MCQ格式中选择错误选项\n- 即使LLMs选择了错误答案，其内部表示中仍编码了正确答案，可通过线性探针检测到\n\n## 2. 假设形成：几何错位理论\n\n**核心假设**：知识-预测差距源于模型隐藏状态沿着两个重要基的几何错位：\n- 知识基(knowledge basis)：编码给定MCQ的正确答案概率\n- 预测基(prediction basis)：编码模型预测的答案选择概率\n\n**理论框架**：当模型的隐藏状态沿着这两个基对齐时，预测通常是正确的；而当它们错位时，预测往往是错误的。这表明问题不在于知识缺失，而在于知识表达与预测机制之间的不协调。\n\n## 3. 验证方法：探测分析与子空间识别\n\n**验证策略**：\n1. **探测分析**：在模型残差流中寻找是否存在由知识基和预测基张成的子空间\n2. **数据构建**：为每个输入提示存储真实标签和模型预测，创建两个数据集\n3. **线性探针训练**：训练两个独立的逻辑回归分类器，一个预测真实标签，一个预测模型输出\n4. **子空间构建**：使用训练好的探针权重作为两个基，构建知识-预测子空间\n\n**关键发现**：\n- 在某些层的残差流中确实存在由这两个功能不同的基张成的子空间\n- 知识探针的准确率通常超过模型自身的预测准确率，证实了模型内部编码了正确知识\n- 导致正确预测的隐藏状态激活通常沿着两个基很好地对齐，而错位通常导致错误预测\n\n## 4. 解决方案：KAPPA干预方法\n\n**方法设计**：基于上述发现，提出KAPPA（Knowledge-Aligned Prediction through Projection-based Adjustment）方法，这是一种基于测试时的干预，通过几何变换调整隐藏状态。\n\n**核心机制**：\n1. **目标**：通过最小化扰动，使预测坐标与知识坐标对齐\n2. **数学形式**：将问题表述为约束优化问题，推导出闭式解：\n   ```\n   h' = h + (ũ_knowledgeᵀh̃ - ũ_predictionᵀh̃)/||u_prediction||² * u_prediction\n   ```\n3. **几何解释**：该变换仅在预测方向上修改隐藏状态，保留所有正交分量不变\n\n**方法扩展**：引入更一般化的对齐约束，通过超参数w和β参数化目标映射，以适应更广泛的知识-预测关系：\n```\nũ_predictionᵀh̃' = w·ũ_knowledgeᵀh̃ + β·sign(ũ_knowledgeᵀh̃)\n```\n\n## 5. 实验验证与效果评估\n\n**主要发现**：\n1. **显著性能提升**：在BBH-Binary上，KAPPA将Llama-2-7B-Chat的准确率从50.9%提升至70.8%\n2. **跨数据集泛化**：在一个数据集上训练的探针可有效应用于其他数据集，表明子空间反映了基本的模型属性\n3. **超越MCQ**：KAPPA提高了自由形式生成的准确性，证明其不仅限于多选题场景\n4. **不影响一般能力**：在vicuna-eval上，KAPPA在知识和推理密集型任务上表现优异，同时保持生成连贯响应的能力\n\n## 6. 理论贡献与实践意义\n\n**理论贡献**：\n- 提供了知识-预测差距的几何理解框架\n- 揭示了LLMs内部表示的结构化特性\n- 证明了模型失败往往源于表达机制而非知识缺失\n\n**实践意义**：\n- 提供了一种无需参数更新的轻量级干预方法\n- 为提高模型忠实度提供了新思路\n- 为理解和改进LLMs的内部工作机制开辟了新途径\n\n这一完整的逻辑链条展示了作者如何从宏观问题出发，通过观察、假设形成、方法设计、实验验证，最终提出一个既有理论深度又有实用价值的解决方案，成功弥合了LLMs中的知识-预测差距。", "summary_translation": "大型语言模型（Large Language Models, LLMs）尽管在其他情境（如自由形式生成）中展现出正确知识，但在多项选择题（multiple-choice questions, MCQs）上常常表现不佳。为探究MCQs中这种知识-预测差距背后的机制并缓解该问题，我们进行了探测分析，发现特定层的残差流（residual streams）包含一个由两个重要基（bases）张成的子空间（subspace）：一个编码给定MCQ正确答案概率的\\emph{知识基}（knowledge basis）和一个编码模型预测答案选择概率的\\emph{预测基}（prediction basis）。我们观察到，错误预测源于模型隐藏状态（hidden states）在这两个基上的不对齐。因此，我们提出了\\textbf{KAPPA}（Knowledge-Aligned Prediction through Projection-based Adjustment，基于投影调整的知识对齐预测），这是一种无参数的干预方法，通过变换隐藏状态来使该子空间内的预测坐标与知识坐标对齐。在Big-Bench-Hard和ARC-Challenge的二选一重构任务上的实验表明，KAPPA显著提高了准确性，并持续优于基线方法。虽然最优子空间因任务而异，但子空间在一定程度上具有泛化性，跨数据集实验支持了这一点。此外，KAPPA的有效性不仅限于MCQs，还扩展到了自由形式问题。我们的工作为知识-预测差距提供了新的几何理解，并提供了一种实用方法来更好地使模型行为与其潜在知识（latent knowledge）对齐。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#111", "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality", "link": "/arxiv/2509.23765", "arxiv_id": "2509.23765", "authors": "Junliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, Haifeng Wang", "summary": "Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model's internal knowledge boundaries, exacerbating the so-called \"hallucination tax\". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model's expressed knowledge and the base model's parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model's internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.", "subjects": "Computation and Language, Artificial Intelligence, Machine Learning", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.539253", "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是减少幻觉和提高事实性，这直接关联到LLM的通用推理能力。论文提出了\"Knowledge-Level Consistency Reinforcement Learning Framework (KLCF)\"这一新的训练范式，通过强化学习方法优化模型的事实表达，属于提升LLM核心能力的研究。 从正面指标看，论文明确涉及大语言模型(LLMs)这一核心概念，使用了强化学习(RL)作为训练方法，虽然未直接提及\"reasoning\"一词，但减少幻觉和提高事实性是推理能力的重要组成部分，特别是在长文本生成中保持事实一致性需要强大的推理能力。 论文不符合排除标准，它没有聚焦于多模态与视觉、特定应用领域或应用层面的模型可靠性研究。相反，它针对的是LLM的通用问题——幻觉和事实性缺陷。 在特殊和模糊情况处理上，论文提出了减少幻觉的新方法，通过知识一致性强化学习来提升模型的内在可靠性和推理质量，这正是提升LLM通用推理能力的关键方向。 综上所述，这篇论文的核心贡献是提出了一种新的强化学习框架来提高LLM的事实性和减少幻觉，这直接关联到提升LLM的通用推理能力，因此符合我的研究目标。", "summary2": "本文旨在解决大型语言模型在长文本生成中的幻觉和事实性缺陷问题。针对长文本生成场景，我们提出了一种知识级一致性强化学习框架(KLCF)，通过双事实对齐机制(Dual-Fact Alignment)共同优化事实召回和精确度，并在FActScore、LongWiki等多个长文本基准测试上通过FActScore、Recall@K、Precision等指标验证了其有效性。", "inspiration_trace": "# KLCF方法逻辑推演：从问题观察到解决方案\n\n## 一、宏观问题识别：长文本生成中的幻觉困境\n\n**核心观察**：\n1. 大型语言模型(LLMs)在长文本生成中存在严重的幻觉问题，即生成与既定事实相冲突的内容\n2. 幻觉在长文本中尤为危险，因为早期错误会通过\"滚雪球效应\"级联放大，逐渐破坏整个回应的可信度\n3. 现有RLHF框架主要依赖偏好奖励，但忽略了模型的内部知识边界，导致模型可能捏造超出其知识范围的事实\n\n**关键问题**：如何在提高长文本生成的事实性的同时，避免模型过度保守而牺牲信息覆盖率？\n\n## 二、深入分析：知识表达与参数化知识的错位\n\n**核心洞见**：\n1. LLMs的参数化知识(Parametric Knowledge)来源于预训练，而对齐模型学习如何表达这些知识\n2. 长文本事实性问题的本质是\"表达知识\"(Expressed Knowledge)与\"参数化知识\"之间的不一致\n3. 现有方法主要关注事实精确度(precision)，而忽略了事实召回(recall)，导致模型倾向于保守回应\n\n**形成假设**：\n1. 如果能够增加对齐模型的\"表达知识\"与基础模型的\"参数化知识\"之间的一致性，可以减少幻觉\n2. 需要同时优化两个目标：最大化从参数化知识中的事实召回，同时最小化超出知识边界的内容生成\n3. 有可能仅通过模型内部知识提供奖励信号来实现这种对齐，无需依赖外部检索\n\n## 三、方法论设计：双事实对齐机制\n\n基于上述分析，作者设计了知识层面一致性强化学习框架(KLCF)，核心是双事实对齐机制：\n\n### 1. 离线数据准备阶段\n- **知识边界探测**：从基础模型采样，探测其知识边界\n- **结构化提取**：将非结构化文本转换为可验证的原子声明集\n- **事实验证**：通过本地知识索引验证声明，获得可靠性标签\n- **资源构建**：准备事实清单和真实性奖励模型训练数据\n\n### 2. 双奖励机制设计\n**基于清单的一致性奖励(Checklist Reward)**：\n- 目的：提高事实覆盖率和召回率\n- 机制：监督生成与预构建清单的一致性\n- 计算：结合事实召回奖励和事实精确度奖励的加权调和平衡\n\n**基于置信度的真实性奖励(Truthfulness Reward)**：\n- 目的：防止生成超出知识边界的错误陈述\n- 机制：使用自我评估模块进行细粒度的真实性评估\n- 计算：评估生成声明为真的概率，取平均值作为奖励\n\n### 3. 辅助奖励与优化\n- **辅助奖励**：一般奖励、格式奖励和长度惩罚，确保整体生成质量\n- **策略优化**：使用组相对策略优化(GRPO)算法，实现高效训练\n\n## 四、关键创新与优势\n\n**核心创新**：\n1. 首次提出\"知识层面一致性\"作为长文本事实性对齐的核心目标\n2. 设计双事实对齐机制，同时优化事实召回和精确度\n3. 完全不需要外部知识的轻量级奖励系统，实现高效可扩展的在线RL训练\n\n**优势体现**：\n1. **效率**：比传统方法快3-4倍，无需外部检索\n2. **平衡**：同时优化召回和精确度，避免幻觉-保守主义权衡\n3. **可扩展**：适用于不同模型规模，从7B到32B均表现一致\n\n## 五、验证与启示\n\n**实验验证**：\n- 消融研究证实双奖励机制的必要性：单独使用任一奖励都会导致召回与精确度的失衡\n- 扩展性研究显示方法在不同模型规模上一致有效\n- 效率分析证明方法显著降低计算开销\n\n**理论启示**：\n1. 知识表达一致性是解决长文本幻觉问题的关键方向\n2. 内部知识边界可以作为有效的奖励信号源\n3. 双目标优化机制能够平衡事实性与全面性的矛盾\n\n这一逻辑链条从宏观问题出发，通过深入分析形成核心假设，最终设计出创新的双事实对齐机制，为解决长文本生成中的幻觉问题提供了全新思路。", "summary_translation": "幻觉（hallucination）和事实性缺陷（factuality deficits）仍然是大型语言模型（large language models, LLMs）在长文本生成（long-form generation）中可靠性的关键障碍。现有的人类反馈强化学习（reinforcement learning from human feedback, RLHF）框架主要依赖偏好奖励（preference rewards），然而它们常常忽视模型的内部知识边界（internal knowledge boundaries），加剧了所谓的\"幻觉税\"（hallucination tax）。为应对这一挑战，我们提出了知识级一致性强化学习框架（Knowledge-Level Consistency Reinforcement Learning Framework, KLCF），这是一个新颖的框架，专注于策略模型（policy model）的表达知识与基础模型（base model）的参数知识（parametric knowledge）之间的一致性，并引入了双事实对齐（Dual-Fact Alignment）机制来共同优化事实召回率（factual recall）和精确度（precision）。具体而言，KLCF利用预训练知识边界（pretrained knowledge boundaries）构建事实检查清单（fact checklist），指导在线强化学习（online reinforcement learning）以提高事实覆盖率和召回率；同时，它基于基础模型的内部知识训练一个自我评估模块（self-assessment module），以增强生成过程中的事实精确度。与依赖外部检索（external retrieval）或繁重验证（heavy verification）的先前方法不同，我们的奖励设计完全无需外部知识（external-knowledge-free）且轻量级（lightweight），使KLCF高效且易于扩展到大规模训练。实验结果表明，KLCF在多个长文本基准测试（long-form benchmarks）上显著提高了事实性指标（factuality metrics），并有效缓解了模型幻觉（model hallucinations）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#119", "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs", "link": "/arxiv/2509.23657", "arxiv_id": "2509.23657", "authors": "Shulin Huang, Yiran Ding, Junshu Pan, Yue Zhang", "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs) attracts widespread attention. While reinforcement learning (RL) has shown superior performance for improving complex reasoning, its impact on cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains unexplored. We present the first systematic investigation into cross-lingual reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation model, we conduct experiments on diverse multilingual reasoning benchmarks, including math reasoning, commonsense reasoning, and scientific reasoning. Our investigation yields two significant findings: (1) Tuning with RL not only achieves higher accuracy but also demonstrates substantially stronger cross-lingual generalization capabilities compared to SFT. (2) RL training on non-English data yields better overall performance and generalization than training on English data, which is not observed with SFT. Furthermore, through comprehensive mechanistic analyses, we explore the underlying factors of RL's superiority and generalization across languages. Our results provide compelling evidence that RL enables the model with more robust reasoning strategies, offering crucial guidance for more equitable and effective multilingual reasoning.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.548440", "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是研究如何通过强化学习(RL)来提高大语言模型(LLM)的推理能力，特别是跨语言推理能力，这直接属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力。 其次，论文包含多个正面指标：核心概念上明确研究Large language models (LLMs)；能力方向上聚焦于reasoning，特别是math reasoning、commonsense reasoning和scientific reasoning；训练方法上使用reinforcement learning作为主要技术手段。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 论文的核心贡献在于系统研究了强化学习相比监督微调在提升LLM跨语言推理能力方面的优势，并发现RL训练能使模型获得更强大的推理策略。这直接服务于提升大语言模型通用推理能力的研究目标，因此应该被保留。", "summary2": "本文旨在探索强化学习(RL)如何改善大语言模型的跨语言推理能力。针对多语言推理场景，我们提出了一种比较RL与监督微调(SFT)在跨语言泛化能力差异的方法，并在MGSM、MMath500、MMLU-ProX-Lite等多语言推理基准上通过准确率和泛化分数验证了其有效性。研究发现RL不仅优于SFT，且使用非英语数据训练效果更佳，为多语言推理提供了新思路。", "inspiration_trace": "# 强化学习提升LLM跨语言推理能力的逻辑链分析\n\n## 一、宏观问题：多语言推理的挑战与不平衡\n\n论文从多语言在人类社会和AI发展中的重要性出发，指出全球7000多种语言各具特色，而现有LLM在英语与其他语言间存在显著性能差距。作者观察到，尽管LLM在推理能力上取得进展，但跨语言推理仍面临重大挑战：模型不仅需要理解不同语言的语义内容，还需在多样化语言环境中执行逻辑推理和问题解决。\n\n## 二、现有方法的局限性\n\n作者系统梳理了现有研究路径的不足：\n\n1. **提示策略的局限**：当前多语言推理研究主要依赖\"先翻译后解决\"等提示策略，这些方法本质上仍依赖英语作为中介，而非提升模型内在的跨语言泛化能力。\n\n2. **监督微调(SFT)的缺陷**：SFT通过模仿专家思维链示例增强推理能力，但其学习范式基于模仿和记忆给定轨迹，导致模型对训练数据中的语言和模式产生过拟合，限制了跨语言迁移能力。\n\n## 三、关键观察与假设形成\n\n作者通过文献分析发现强化学习(RL)在推理任务中的独特优势：\n\n1. **RL在推理任务中的成功**：研究表明RL在数学和逻辑推理等复杂任务上表现优越，通过奖励引导机制使模型获得更稳健、可泛化的推理策略。\n\n2. **RL的跨任务泛化能力**：RL不仅提高模型性能，还展现出更强的跨任务泛化能力，这引发作者思考：RL是否也具备跨语言泛化的优势？\n\n基于这些观察，作者提出核心假设：**RL可能通过学习不依赖特定语言的推理策略，实现比SFT更强的跨语言泛化能力**。\n\n## 四、实验验证与意外发现\n\n为验证假设，作者设计了系统性实验，比较RL和SFT在多语言推理基准上的性能：\n\n### 发现1：RL的跨语言泛化优势\n实验结果表明，RL不仅准确性高于SFT，还展现出显著更强的跨语言泛化能力。RL训练使模型能更有效地将一种语言中学到的推理能力迁移到另一种语言。\n\n### 发现2：非英语数据的意外优势\n与传统认知相反，使用非英语数据(如中文和德语)进行RL训练比使用英语数据产生更好的整体性能和泛化能力。这一现象在SFT中未观察到，挑战了\"以英语为中心\"的训练范式。\n\n## 五、深入分析：探索RL优势的潜在机制\n\n为解释上述发现，作者从三个维度进行深入分析：\n\n### 1. 语言不一致性分析\n- **观察**：RL训练的模型在推理过程中不严格使用训练语言，而是采用多语言或混合语言进行推理。\n- **验证**：通过强制语言一致性的实验(添加语言控制提示和一致性奖励)，发现约束语言使用会显著降低RL模型的跨语言性能。\n- **结论**：语言灵活性使模型能够利用预训练中建立的更强大的多语言推理模块，而非局限于特定语言模式。\n\n### 2. 采样机制分析\n- **方法**：引入拒绝采样微调(RFT)作为SFT和RL之间的中间基线，比较不同采样策略的效果。\n- **发现**：从Base→SFT→RFT→RL，性能逐步提升，表明模型对解决路径的探索程度与推理能力正相关。\n- **解释**：RL的在线策略优化过程通过探索多样化解决路径，使模型学习更稳健、可泛化的推理策略，超越简单模仿学习。\n\n### 3. 语义特征转移分析\n- **方法**：分析模型训练前后语义表示的变化，使用PCA可视化最终层隐藏状态的转移模式。\n- **发现**：RL(特别是使用非英语数据)训练导致的语义表示转移最小，更接近预训练模型。\n- **解释**：预训练建立了对跨语言迁移至关重要的多语言推理结构，RL通过最小化对这些结构的干扰，保持了更强的泛化能力。\n\n## 六、方法论形成：RL跨语言推理优势的整合框架\n\n基于以上分析，作者形成了关于RL在跨语言推理中优势的整合方法论：\n\n1. **语言灵活性机制**：RL允许模型在推理过程中自由切换语言，这种灵活性使模型能够激活和利用预训练中建立的多语言推理模块，而非局限于特定语言模式。\n\n2. **探索-优化平衡机制**：RL通过采样多样化解决路径并进行在线策略优化，使模型能够发现更稳健、可泛化的推理策略，而非简单模仿给定解决方案。\n\n3. **语义空间稳定性机制**：RL(特别是使用非英语数据)能够更好地保持预训练中建立的多语言推理结构，最小化语义空间的偏移，从而实现更好的跨语言泛化。\n\n## 七、结论与意义\n\n作者通过系统研究揭示：RL相比SFT不仅提高了LLM的推理准确性，还显著增强了跨语言泛化能力；反直觉地，使用非英语数据进行RL训练比英语数据更有效。这些发现挑战了以英语为中心的训练范式，为构建更公平、有效的多语言推理系统提供了新思路。\n\n这一研究逻辑链条从宏观问题出发，通过观察现有方法局限性，形成创新假设，通过实验验证获得意外发现，进而深入分析潜在机制，最终形成系统方法论，展现了完整的科学探究过程。", "summary_translation": "提升大语言模型(Large Language Models, LLMs)的复杂推理能力引起了广泛关注。尽管强化学习(Reinforcement Learning, RL)在提升复杂推理方面表现出优越性能，但与监督微调(Supervised Fine-Tuning, SFT)相比，其对跨语言泛化的影响尚未被探索。我们首次对RL和SFT的跨语言推理泛化进行了系统性研究。我们以Qwen2.5-3B-Base为基础模型，在多种多样的多语言推理基准上进行了实验，包括数学推理、常识推理和科学推理。我们的研究得出了两个重要发现：(1) 与SFT相比，使用RL进行调优不仅实现了更高的准确率，还表现出明显更强的跨语言泛化能力。(2) 在非英语数据上进行RL训练比在英语数据上训练能获得更好的整体性能和泛化效果，而这一现象在SFT中并未观察到。此外，通过全面的机制分析，我们探讨了RL在不同语言中表现优越和泛化的潜在因素。我们的结果提供了有力证据，证明RL能够使模型具备更稳健的推理策略，为更公平和有效的多语言推理提供了重要指导。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#120", "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models", "link": "/arxiv/2509.23653", "arxiv_id": "2509.23653", "authors": "Zemin Huang, Yuhang Wang, Zhiyang Chen, Guo-Jun Qi", "summary": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect tokens: once a token is generated, it typically remains fixed. The key challenge is to identify potential errors in the inputs. In this paper, we propose \\emph{\\underline{Rem}asking-\\underline{e}nabled \\underline{Di}ffusion Language Model (RemeDi}, a mask-based DLM that introduces \\emph{remasking} as another fundamental mechanism, enabling more flexible text refinement in diffusion-based text generation. To achieve this, RemeDi jointly predicts token distributions and per-token confidence scores at each step. The confidence scores determine which tokens to be unmasked after the current step, allowing the model to identify tokens with low quality and remask them. These remasked tokens can be resampled with richer context in subsequent steps. We design a remask-aware pipeline to train this ability, including supervised fine-tuning which teaches the model to detect and remask incorrect tokens in addition to predict mask tokens, and reinforcement learning which optimizes full generation trajectories toward higher rewards. Experiments show that RemeDi achieves the state-of-the-art results among open-source DLMs on multiple datasets.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.548889", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"RemeDi\"的扩散语言模型，通过引入\"重新掩码\"(remasking)机制，使模型能够识别低质量token并进行重新采样，从而增强模型的自我修正和推理能力。论文设计了一个包含监督微调和强化学习的训练管道来优化这种能力。这种方法本质上是提升大语言模型的基础能力和通用推理能力，特别是通过自反思机制来改进输出质量，符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的要求。论文使用了强化学习作为训练方法，这是正面指标之一。虽然论文没有明确讨论数学或逻辑推理，但其提出的自我修正机制可以提升模型的问题解决能力。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。相反，论文提出的方法可以看作是提升模型内在可靠性和推理质量的新方法，这符合保留标准。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决基于掩码的扩散语言模型无法修订已生成不正确token的问题。针对文本生成场景，我们提出了一种RemeDi模型，引入自我反思的重新掩码机制，使模型能够识别低质量token并重新掩码以便后续重采样。在GSM8K、MATH、HumanEval等多个benchmark上通过准确率等指标验证了其有效性，取得了开源DLMs中最先进的性能。", "inspiration_trace": "# 从问题到解决方案：RemeDi的逻辑演进\n\n## 1. 宏观问题：扩散语言模型的固有局限\n\n**观察现象**：扩散语言模型(DLMs)作为自回归模型的替代方案，具有并行生成和灵活顺序的优势，特别是基于掩码的DLMs已成为主流变体。\n\n**核心问题**：现有DLMs存在一个根本性局限——一旦token被生成(unmasked)，就会保持固定不变，无法在后续步骤中修正。这导致早期生成的错误token会一直保留到生成结束，即使随着更多上下文的出现，这些错误变得明显。\n\n**关键挑战**：如何让模型能够识别已生成但可能错误的token，并提供一种机制来修正这些错误？\n\n## 2. 现有解决方案的不足\n\n**文献调研发现**：作者分析了两类现有解决方案：\n1. **推理时修正方法**：如predictor-corrector采样器，在推理时随机重新掩码部分token。这些方法缺乏识别哪些token实际错误的机制，效率低下且难以优化。\n2. **修改扩散过程方法**：如结合掩码扩散与均匀扩散或编辑扩散。这些方法要么缺乏确保掩码token数量单调减少的机制，要么没有提供原则性的错误检测方法。\n\n**洞察**：现有方法都没有解决核心问题——如何让模型自我反思并识别需要修正的token。\n\n## 3. 核心假设：自我反思的重新掩码\n\n**关键洞察**：如果模型能够评估每个已生成token的质量，并有选择地重新掩码低质量token，就能在后续步骤中用更丰富的上下文重新采样，从而提高生成质量。\n\n**直观例子**：模型最初生成\"left for the pies\"，但后来发现语义不匹配，应改为\"used for the pies\"。传统DLM无法修正这种错误，但如果模型能识别\"left\"是低质量token并重新掩码它，就能在后续步骤中生成正确的\"used\"。\n\n**核心假设**：通过联合预测token分布和每个token的置信度分数，模型可以自我反思并决定哪些token需要被重新掩码和重新生成。\n\n## 4. 方法设计：RemeDi架构\n\n**架构创新**：基于上述假设，作者设计了双流transformer架构：\n1. **Token预测流(TPS)**：负责预测掩码token的概率分布。\n2. **Unmasking策略流(UPS)**：负责输出每个token的置信度分数，决定哪些token应被unmask或重新掩码。\n\n**工作机制**：在每个扩散步骤中，模型首先通过UPS预测每个位置的置信度分数，然后基于这些分数决定哪些token保持unmask、哪些被重新掩码。与现有DLM不同，RemeDi允许已经生成的token被重新掩码，实现真正的自我修正能力。\n\n**关键保证**：设计噪声调度确保掩码token数量单调减少，维持扩散模型的基本特性。\n\n## 5. 训练策略：两阶段学习流程\n\n**挑战**：如何训练模型具备自我反思的重新掩码能力？\n\n**解决方案**：作者设计了两个互补的训练阶段：\n\n### 阶段1：Remask SFT（监督微调）\n**目标**：教会模型检测和重新掩码不正确的token。\n**方法**：\n- 构建包含两类噪声的训练样本：掩码token和随机替换的不正确token。\n- 设计标签策略：干净token标记为保持unmask，不正确token标记为重新掩码，掩码token标记为基于预测概率的软标签。\n- 结合扩散损失和置信度预测损失进行联合训练。\n\n### 阶段2：Remask RL（强化学习）\n**目标**：优化整个生成轨迹，提高最终输出质量。\n**方法**：\n- 将生成过程建模为马尔可夫决策过程，每个步骤包含unmasking策略和token预测策略。\n- 使用GRPO算法优化生成轨迹，奖励基于任务类型设计（可验证正确性或奖励模型评估）。\n- 通过强化学习微调整体生成策略，提高模型在复杂任务上的表现。\n\n## 6. 实验验证与效果分析\n\n**验证方法**：作者在多个基准数据集上评估RemeDi，包括数学问题、代码生成和一般任务。\n\n**关键发现**：\n1. **性能提升**：RemeDi在所有评估任务上实现了最先进的性能，显著超过现有DLMs。\n2. **重新掩码行为分析**：\n   - 代码生成中重新掩码最频繁，其次是数学推理，一般任务中最少。\n   - 随着问题难度增加，重新掩码频率也增加，表明迭代细化对复杂问题尤为重要。\n3. **消融研究**：\n   - Remask SFT优于传统SFT，特别是在数学和代码任务上。\n   - Remask RL比现有RL方法收敛更快且性能更好。\n\n**可视化验证**：通过具体例子展示了重新掩码如何实现多种修正操作，包括纠正计算错误、优化文本质量、合并/拆分token、插入和删除token等。\n\n## 7. 逻辑演进总结\n\n从观察到解决方案，作者展现了清晰的逻辑链条：\n\n1. **问题识别**：发现DLMs无法修正已生成token的根本局限。\n2. **现有方法分析**：识别现有解决方案的不足，缺乏原则性的错误检测机制。\n3. **核心洞察**：提出自我反思的重新掩码概念，允许模型评估并修正低质量token。\n4. **架构设计**：创建双流transformer架构，分离token预测和unmasking决策。\n5. **训练策略**：设计两阶段学习流程，先教会模型检测错误，再优化整体生成轨迹。\n6. **实验验证**：通过严格实验证明方法有效性，并深入分析机制行为。\n\n这一逻辑演进展示了作者如何从一个观察到的局限性出发，通过系统分析和创新思考，最终提出一个有效的解决方案，为扩散语言模型的发展开辟了新方向。", "summary_translation": "基于掩码的扩散语言模型(Mask-based Diffusion Language Models, DLMs)难以修正不正确的标记(token)：一旦生成了标记，它通常会保持固定。关键挑战在于识别输入中的潜在错误。在本文中，我们提出了\\emph{\\underline{重}掩码-\\underline{启}用的\\underline{扩}散语言模型(RemeDi)}，这是一种基于掩码的DLM，它引入了\\emph{重掩码}(remasking)作为另一种基本机制，能够在基于扩散的文本生成中实现更灵活的文本精炼。为实现这一目标，RemeDi在每个步骤中联合预测标记(token)分布和每个标记的置信度得分。置信度得分决定了在当前步骤后哪些标记将被取消掩码，使模型能够识别低质量的标记并对其进行重新掩码。这些被重新掩码的标记可以在后续步骤中利用更丰富的上下文进行重新采样。我们设计了一个感知重掩码的管道来训练这种能力，包括监督微调(supervised fine-tuning)，它教导模型除了预测掩码标记外，还能检测并重新掩码不正确的标记；以及强化学习(reinforcement learning)，它优化完整生成轨迹以获得更高的奖励。实验表明，在多个数据集上，RemeDi在开源DLMs中实现了最先进(state-of-the-art)的结果。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#121", "title": "Fast Thinking for Large Language Models", "link": "/arxiv/2509.23633", "arxiv_id": "2509.23633", "authors": "Haoyu Zheng, Zhuonan Wang, Yuqian Yuan, Tianwei Lin, Wenqiao Zhang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang, Hongyang He", "summary": "Reasoning-oriented Large Language Models (LLMs) often rely on generating explicit tokens step by step, and their effectiveness typically hinges on large-scale supervised fine-tuning or reinforcement learning. While Chain-of-Thought (CoT) techniques substantially enhance performance on complex reasoning tasks, they remain inefficient, requiring long reasoning traces that increase latency and token usage. In this work, we introduce Latent Codebooks for Fast Thinking, a framework that uses concise CoT sketches only during training to learn a codebook of discrete strategy priors. At inference, the model conditions on a handful of continuous thinking vectors distilled from the codebook in a single pass, enabling strategy-level guidance without producing explicit reasoning tokens. To complement this design, we propose GainRouter, a lightweight routing mechanism that adaptively switches between fast codebook guided inference and slow explicit reasoning, thereby suppressing overthinking and reducing unnecessary token generation. Experiments across multiple reasoning benchmarks show that our approach achieves competitive or superior accuracy while substantially lowering inference cost, offering a practical path toward efficient and controllable reasoning in large language models.", "subjects": "Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.549441", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进LLM的基础推理能力，提出了\"Latent Codebooks for Fast Thinking\"框架，旨在解决传统思维链(CoT)技术效率低下的问题。这明显属于改进LLM通用推理能力的研究，而非将LLM应用于特定领域。 其次，论文符合多个正面指标：核心概念上明确关注\"Reasoning-oriented Large Language Models (LLMs)\"；能力方向上专注于推理(reasoning)，并在多个推理基准上进行了实验验证；虽然未直接使用强化学习等方法，但提到了与这些方法的对比。 第三，论文不符合任何排除标准：没有涉及多模态与视觉内容，没有将LLM应用于特定领域，也没有主要关注模型可靠性的应用层面问题。 论文的核心贡献是提出了一种新的训练和推理范式，通过在训练阶段使用简洁的CoT草图学习离散策略先验的码本，在推理阶段使用少量连续思维向量进行策略级引导，从而提高推理效率。这种方法直接增强了LLM的通用推理能力，完全符合研究目标。", "summary2": "本文旨在解决大型语言模型中Chain-of-Thought推理效率低下的问题。针对推理任务场景，我们提出了一种Latent Codebooks for Fast Thinking框架，通过学习离散策略先验码本并在推理时使用少量连续思考向量引导单次推理，并在数学推理和编程基准测试上通过准确性和生成token数量验证了其有效性。", "inspiration_trace": "# 从问题到解决方案：LC-FT方法的逻辑演进\n\n## 1. 宏观问题：LLM推理的效率困境\n\n作者首先观察到大型语言模型(LLMs)在复杂推理任务中面临的核心矛盾：\n- **推理效率与质量的权衡**：Chain-of-Thought (CoT)技术显著提升了复杂推理性能，但需要生成冗长的推理轨迹，导致高延迟和大量token消耗\n- **过度思考问题**：在简单实例上，扩展推理可能引入不必要步骤，甚至增加错误率\n- **训练成本障碍**：强大的多步推理通常依赖过程监督或强化学习优化，增加了大规模部署的复杂性\n\n这一核心问题构成了研究的出发点：**如何在保持推理质量的同时，大幅提升LLM的推理效率？**\n\n## 2. 核心洞察：人类快速思维的启发\n\n作者从人类认知过程中获得关键灵感：\n- **人类问题解决模式**：许多看似复杂的问题可通过快速回忆和应用先验解决方案模式来解决，无需详尽推导\n- **\"识别-行动\"机制**：人类专家能快速\"识别情况\"并提前承诺高概率解决方案\n\n这引导作者提出核心假设：**LLM能否内化一套紧凑的、可重用的推理策略先验，并在推理时仅获取少量针对性指导，从而实现快速思考？**\n\n## 3. 方法假设：从显式到隐式的推理转变\n\n基于上述洞察，作者形成三个关键假设：\n1. **码本假设**：可以通过学习离散策略先验的码本，将冗长CoT压缩为紧凑表示\n2. **隐式推理假设**：推理时仅需少量连续思考向量指导解码，无需生成显式推理token\n3. **自适应路由假设**：可设计轻量级机制，在快速码本引导和慢速显式推理间智能切换\n\n这些假设指向一个统一目标：**将显式的、token密集的\"慢思考\"转换为隐式的、向量驱动的\"快思考\"**\n\n## 4. 方法设计：LC-FT框架的构建\n\n### 4.1 潜在码本快速思考(LC-FT)\n\n作者设计了一个两阶段学习框架：\n\n**阶段1：简洁CoT数据构建与码本学习**\n- 创建仅用于训练信号的简洁教师推理数据集\n- 设计可学习码本C ∈ R^(M×H)，存储M个原型推理策略\n- 通过注意力机制从码本提取K个思考令牌向量T\n\n**阶段2：思考令牌注入与模型微调**\n- 在Transformer第L层注入思考令牌，在此之前掩码，之后可见\n- 使用LoRA适配器从注入层开始进行参数高效微调\n- 通过对齐损失和标准监督损失优化模型\n\n**创新点**：将冗长文本CoT替换为紧凑抽象策略先验，推理时不增加输出长度\n\n### 4.2 GainRouter：智能路由机制\n\n为解决\"何时快思考、何时慢思考\"的问题，作者设计：\n\n**特征提取与决策机制**\n- 提取问题向量和思考令牌向量，通过注意力聚合证据\n- 结合问题特征和码本不确定性信号计算自适应阈值\n- 默认使用快速模式，仅在预测失败风险高时切换到CoT推理\n\n**创新点**：抑制过度思考，减少不必要token生成，提供可控的准确率-成本权衡\n\n## 5. 实验验证：效率与质量的平衡\n\n作者在数学推理(AIME、OlympiadBench)和编程任务(MBPP、HumanEval)上验证方法：\n\n**关键发现**：\n- LC-FT在保持竞争力的准确率同时，显著降低推理成本\n- GainRouter匹配Qwen3-Thinking准确率，但减少33-65%的token使用\n- 码本激活模式呈现稀疏性和任务特异性，证实学习到可重用策略原型\n- 消融实验验证码本层、精炼器和LoRA适配的各自贡献\n\n**核心结论**：LC-FT实现了\"快速思考优先\"的推理范式，在保持高质量推理的同时大幅提升效率\n\n## 6. 逻辑演进总结\n\n作者的方法论形成了一条清晰的逻辑链：\n\n**问题识别** → CoT推理效率低下、过度思考、训练成本高  \n**核心洞察** → 人类快速思维启发：通过先验模式快速解决问题  \n**假设形成** → 可学习离散策略码本，用少量连续向量指导解码  \n**方法设计** → LC-FT框架(码本学习、思考令牌注入) + GainRouter(自适应路由)  \n**实验验证** → 在数学推理和编程任务上验证效率与有效性  \n**最终贡献** → 为LLM提供高效可控推理的实用路径\n\n这一逻辑演进展示了作者如何从实际问题出发，通过跨领域类比形成创新假设，设计出系统解决方案，并通过严谨实验验证其有效性，最终为LLM推理效率问题提供了一个兼具理论创新和实用价值的解决路径。", "summary_translation": "面向推理的大型语言模型 (Reasoning-oriented Large Language Models, LLMs) 通常依赖于逐步生成显式标记，其有效性通常取决于大规模监督微调 (supervised fine-tuning) 或强化学习 (reinforcement learning)。尽管思维链 (Chain-of-Thought, CoT) 技术显著提升了复杂推理任务的性能，但它们仍然效率低下，需要长的推理轨迹，这增加了延迟和标记使用量。在这项工作中，我们介绍了快速思考的潜在码本 (Latent Codebooks for Fast Thinking)，这是一个仅在训练期间使用简洁的思维链草图来学习离散策略先验码本的框架。在推理时，模型基于从码本中蒸馏出的少量连续思考向量（单次通过），实现策略级指导，而无需生成显式推理标记。为了补充这一设计，我们提出了增益路由器 (GainRouter)，一种轻量级路由机制，它能在快速码本引导推理和慢速显式推理之间自适应切换，从而抑制过度思考并减少不必要的标记生成。跨多个推理基准的实验表明，我们的方法在大幅降低推理成本的同时，实现了具有竞争力或更优的准确性，为大型语言模型中的高效且可控推理提供了一条实用路径。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#125", "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales", "link": "/arxiv/2509.23574", "arxiv_id": "2509.23574", "authors": "Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang", "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election \\textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.551472", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是关于改进语言模型的基础推理能力，它聚焦于思维链(CoT)蒸馏技术，提出了一种新的训练范式MoRSD（模型导向的推理选择蒸馏），旨在通过选择高质量的推理过程来增强小型语言模型的推理能力。这直接符合我们对提高LLM通用推理能力的研究目标。 其次，论文包含多个正面指标：核心概念涉及语言模型(SLMs和大型教师模型)，能力方向明确关注推理(reasoning)特别是多步推理能力，这些都是我们研究范围的核心要素。 第三，论文不涉及任何排除标准领域，它没有关注多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 论文的核心贡献是提出了一种高效的CoT蒸馏方法，通过控制推理过程的准确性、多样性和难度，使用更少的高质量推理就能显著提升小型模型的推理能力。这种方法论研究直接针对提升LLM的通用推理能力，而非将LLM作为工具应用于特定领域，因此完全符合我们的研究目标。", "summary2": "本文旨在 [解决CoT蒸馏中忽视推理质量而过度关注数量的问题]。针对 [小型语言模型需要从大型教师模型获取推理能力的场景]，我们提出了一种 [MoRSD方法，通过自引导推理选择器选择高质量推理过程]，并在 [七个数据集覆盖三个不同任务] 上通过 [准确率] 验证了其有效性。", "inspiration_trace": "# 从问题到方法：MoRSD的逻辑演进链\n\n## 一、宏观问题：推理能力迁移的效率挑战\n\n**出发点**：大型语言模型(LLMs)通过思维链(CoT)展现出强大的推理能力，但其巨大规模限制了实际部署。小型语言模型(SLMs)计算效率高，却缺乏复杂推理能力。因此，核心问题是：**如何高效地将LLMs的推理能力迁移到SLMs上？**\n\n## 二、问题聚焦：CoT蒸馏的质量困境\n\n作者观察到现有CoT蒸馏方法存在三大局限：\n\n1. **数量导向偏见**：现有方法主要关注增加推理链数量(从1增加到8个)，而忽视质量差异\n2. **噪声传递风险**：不加选择地使用所有生成的推理链，可能将错误或噪声信息传递给学生模型\n3. **模型特异性缺失**：大多数方法忽略学生模型的特定能力和限制，采用\"一刀切\"的蒸馏策略\n\n这引导作者聚焦到更具体的问题：**如何选择一小部分高质量的、针对特定学生模型的推理链，以实现高效有效的蒸馏？**\n\n## 三、核心假设：质量优于数量的理论基石\n\n基于上述观察，作者形成四个关键假设：\n\n1. **质量假设**：一小部分精心挑选的高质量推理链可能比整个数据集更能增强学生模型的推理能力\n2. **特异性假设**：推理链的质量评估应考虑学生模型的特定能力，不同学生模型可能需要不同的推理链\n3. **多维质量假设**：推理链质量是多维概念，包括准确性、多样性和难度\n4. **难度假设**：推理链对学生模型的难度是衡量其对蒸馏贡献的重要指标\n\n## 四、方法设计：MoRSD的四阶段框架\n\n基于这些假设，作者设计Model-Oriented Rationale Selection Distillation (MoRSD)方法，包含四个逻辑相连的阶段：\n\n### 1. 推理链生成：获取原始数据\n- 提示教师模型生成多个推理链\n- 使用固定模板确保格式一致性\n- 构建完整数据集D<sub>full</sub>\n\n### 2. 自我评估：量化推理链价值\n- **创新点**：提出推理链难度(RD)指标\n- **核心思想**：RD衡量学生在给定推理链下生成正确答案的能力\n- **计算方法**：RD = PPL(答案|推理链,问题) / PPL(答案|问题)\n- **逻辑**：RD值越低，表明该推理链对学生理解问题更有帮助\n\n### 3. 推理链选择：三维质量过滤\n作者设计了一个三级过滤流程，体现了多维质量假设：\n\n- **准确性选择**：过滤数据集，确保达到预设准确率阈值δ\n- **多样性选择**：使用N-gram和Jaccard相似度消除冗余推理链\n- **难度选择**：选择RD值最低的k个推理链，体现模型特异性假设\n\n### 4. 蒸馏：高效知识迁移\n- 使用筛选后的高质量推理链数据集D<sub>selected</sub>微调学生模型\n- 目标函数：最小化学生模型在生成正确答案和推理链上的损失\n\n## 五、实验验证：假设检验与效果评估\n\n作者通过多角度实验验证了方法的有效性：\n\n### 1. 整体性能验证\n- 在7个数据集上平均提高4.6%准确率\n- 使用更少推理链实现更好性能，验证了质量假设\n\n### 2. 各维度影响分析\n- **准确性影响**：性能随数据集准确率提高而提高，但超过阈值后收益平稳\n- **多样性影响**：性能随推理链多样性增加而提高\n- **难度影响**：RD值越低的推理链，蒸馏效果越好\n\n### 3. 消融研究\n移除任何选择组件(准确性、多样性或难度选择)都会导致性能显著下降，验证了多维质量假设和各组件的必要性。\n\n### 4. 推理链质量分析\n使用ChatGPT作为裁判评估推理链质量，结果显示低RD推理链质量显著高于高RD推理链，验证了难度假设和RD指标的有效性。\n\n## 六、结论与贡献：从理论到实践\n\n通过上述逻辑链条，作者得出结论：MoRSD通过自主选择高质量的推理链，实现了用更少的数据获得更好的性能，为高效CoT蒸馏提供了新思路。\n\n主要贡献体现为：\n1. 提出了一种简单有效的方法，证明\"少即是多\"在CoT蒸馏中的适用性\n2. 提出了模型特定的RD指标，使学生模型能根据自身需求定制数据\n3. 通过广泛实验验证了方法的有效性和各组件的重要性\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过观察、假设形成、方法设计、实验验证到结论的系统性思考过程，体现了科研工作的严谨性和创新性。", "summary_translation": "思维链(Chain-of-thought, CoT)蒸馏旨在通过从更大的教师模型(teacher models)转移多步推理能力，来增强小型语言模型(small language models, SLMs)的推理能力。然而，现有工作低估了基本原理(rationale)质量，主要关注数据数量，这可能会将噪声或错误信息传递给学生模型(student model)。为解决上述问题，我们提出了面向模型的基本原理选择蒸馏(Model-Oriented Rationale Selection Distillation, MoRSD)，该方法能够识别并选择高质量的基本原理进行蒸馏，以进一步提高性能。我们进一步提出了基本原理难度(Rationale Difficulty, RD)指标，用于衡量学生模型在给定基本原理下生成正确答案的能力。与基线相比，我们通过控制基本原理的准确性、多样性和难度，在三个任务的七个数据集上实现了4.6%的平均提升，同时使用了更少的基本原理。我们的结果表明，一小部分高质量的基本原理比整个数据集更能增强学生模型的推理能力。我们的方法有望成为高效CoT蒸馏的一种可能解决方案。我们的代码将在https://github.com/Leon221220/MoRSD上发布。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#122", "title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank", "link": "/arxiv/2509.23595", "arxiv_id": "2509.23595", "authors": "Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong", "summary": "Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.549991", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Timber\"的无训练方法，通过微调权重差异来部分地将Instruct模型恢复到Base模型的状态，以增强Instruct模型的探索能力(exploration capability)同时保持其利用能力(exploitation)。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从本质上看，这篇论文是关于改进LLM基础能力的研究，属于后训练优化范畴，而不是将LLM作为工具应用到特定领域。论文提出的Timber方法直接针对LLM的内在能力进行改进，符合\"改进LLM的基础能力\"这一核心判断标准。 其次，论文明确涉及大语言模型（Llama和Qwen系列）这一核心概念，并通过增强模型的探索能力来提升其整体性能。虽然论文没有直接提及reasoning、planning或problem-solving等具体能力方向，但探索能力的提升往往与模型的推理能力和问题解决能力密切相关，特别是在处理复杂任务时。 第三，论文不涉及任何排除标准中提到的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，虽然论文没有涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，但其提出的无训练方法为改进LLM的通用能力提供了新的视角，这与研究目标\"提高大语言模型（LLM）本身的通用推理能力\"是一致的。论文中提到的Pass@k性能提升也表明该方法可能对模型的推理能力有积极影响。 综上所述，这篇论文符合研究范围，应该被保留。", "summary2": "本文旨在解决大型语言模型后训练过程中探索能力受限的问题。针对Instruct模型与Base模型之间的权重差异，我们提出了一种基于有效秩(effective rank)的无训练精炼方法Timber，通过部分恢复Instruct模型至Base状态来增强探索能力。在Llama和Qwen系列模型上，通过Pass@k等指标验证了Timber能显著提升模型性能，特别是在多采样场景下。", "inspiration_trace": "# Timber方法逻辑链分析：从观察到方法论\n\n## 1. 宏观问题：大型语言模型训练的局限性\n\n作者从大型语言模型(LLMs)训练的基本流程出发，识别出一个核心问题：LLMs通常采用两阶段训练流程（预训练→后训练），但后训练阶段存在根本性局限。这一阶段旨在将Base模型转化为具有指令遵循能力的Instruct模型，但作者质疑这一过程的深度和效果。\n\n## 2. 观察现象：后训练的\"表面性\"\n\n通过深入研究，作者发现后训练过程是\"表面的\"(superficial)——它仅利用预训练期间Base模型已获得的知识模式，而不引入新的表征能力。为提供定量证据，作者引入有效秩(effective rank, eRank)作为分析工具：\n\n- **eRank定义**：衡量权重矩阵奇异值分布的均匀性，反映矩阵的有效维度\n- **关键发现**：Base和Instruct模型对应层的eRank几乎完全相同（图1）\n- **结论**：后训练对模型的有效维度影响微乎其微，从权重层面证实了其表面性\n\n## 3. 问题聚焦：利用与探索的权衡\n\n基于上述观察，作者进一步识别出一个关键权衡问题：\n\n- **利用能力(Exploitation)增强**：Instruct模型在Pass@1指标上表现优异\n- **探索能力(Exploration)受限**：但在Pass@k（k较大时）指标上表现不如Base模型\n- **根本原因**：表面性后训练过度优化高奖励路径，抑制了采样空间的多样性\n\n这一权衡成为作者要解决的核心问题：如何在不损害利用能力的前提下增强探索能力？\n\n## 4. 解决思路：部分恢复Base状态\n\n作者提出一个创新思路：利用Base模型增强Instruct模型。这一思路基于以下洞察：\n\n- Base模型包含几乎所有知识，Instruct模型仅激发部分高奖励思维模式\n- 后训练的表面性表明，Base和Instruct模型在权重层面高度相似\n- 模型合并相关研究表明，无需训练的权重整合策略是可行的\n\n因此，解决方案是将Instruct模型部分恢复到其Base状态，以重新获得部分探索能力。\n\n## 5. 方法形成：基于eRank的权重增量细化\n\n为实现上述思路，作者设计了Timber方法。关键挑战是如何精确地\"部分恢复\"：\n\n- **初步尝试**：简单线性缩放权重增量效果不佳，因后训练修改具有脆弱性\n- **关键洞察**：eRank可作为自适应阈值，有效分离奇异值谱的主要成分（图3）\n- **方法设计**：\n  1. 计算权重增量：W∆ = WI - WB\n  2. SVD分解：SVD(W∆) → UΣV^T\n  3. 基于eRank设置阈值：K := ⌈eRank(W∆)⌉\n  4. 细化奇异值：\n     - Timber-L：移除尾部奇异值（置零）\n     - Timber：衰减尾部奇异值（缩放因子λ）\n  5. 构建细化权重：W+I = WB + U refine(Σ)V^T\n\n## 6. 实验验证：方法的有效性与鲁棒性\n\n作者通过广泛实验验证Timber方法：\n\n- **测试范围**：Llama和Qwen系列，模型规模从0.6B到30B\n- **评估指标**：多个主流基准测试，特别关注Pass@k指标\n- **关键结果**：\n  - Timber一致地优于原始Instruct模型（表2）\n  - Pass@k性能显著提升，证明探索能力增强（图5）\n  - 对衰减因子λ具有鲁棒性（图4）\n\n这一完整的逻辑链从宏观问题出发，通过定量观察识别本质问题，聚焦核心权衡，提出创新解决思路，形成具体方法，并通过实验验证有效性，展现了清晰的科学研究思维过程。", "summary_translation": "后训练（Post-training），即将预训练的基础模型（Base model）转化为相应的指令模型（Instruct model）的过程，被广泛认为是表面的。在这项工作中，我们首先通过提供权重层面的新颖定量证据来强化这一假设，即有效秩（effective rank, eRank）的变化微乎其微。然而，这种表面性也存在一个关键的权衡，即以限制探索能力（exploration）为代价来提高利用能力（exploitation）。为解决这一问题，我们提出了Timber，一种简单而有效的免训练方法，能够在保持利用能力的同时增强指令模型的探索能力。其关键洞见是通过微妙而有针对性的权重增量（weight deltas）细化，将指令模型部分恢复至配对的基础模型。在Llama和Qwen系列上的广泛实验表明，Timber持续改进了原始指令模型，尤其在Pass@k性能方面表现突出。我们的发现为权重层面的后训练阶段提供了新的见解，并提出了无需训练即可优化指令模型的实用策略。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#130", "title": "The Impact of Role Design in In-Context Learning for Large Language Models", "link": "/arxiv/2509.23501", "arxiv_id": "2509.23501", "authors": "Hamidreza Rouzegar, Masoud Makrehchi", "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.559043", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是研究提示工程中的角色设计如何影响大语言模型在上下文学习(ICL)中的表现。论文不是将LLM作为工具应用到特定领域，而是研究如何通过改进提示设计来提升LLM的基础能力。论文特别评估了角色设计对模型在数学推理等任务上的影响，这直接关联到提升LLM的通用推理能力。因此，根据第一步的判断标准，应该保留。 第二步：正面指标 论文包含以下正面指标： - 核心概念: 明确研究Large Language Models (LLMs)，包括GPT-3.5、GPT-4o、Llama2等模型 - 能力方向: 特别提到了math reasoning（数学推理），这正是通用推理能力的核心组成部分 虽然论文没有涉及reinforcement learning、llm-based agents等新兴范式，但它包含了足够的核心正面指标。 第三步：排除标准 论文不聚焦于任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），虽然评估了多种任务，但这些是通用任务而非特定领域应用 - 不讨论模型可靠性方面的watermarking、safety等问题 第四步：特殊和模糊情况 论文研究的是通过角色设计这种提示工程方法来增强LLM的通用能力，特别是推理能力。这与\"智能体/工具使用\"的情况类似，都是提出一种通用方法来增强LLM的问题解决能力，因此应该保留。 综合判断：这篇论文的核心贡献是研究角色设计如何提升LLM在多种任务（包括数学推理）上的表现，这直接关联到提升LLM的通用推理能力，符合研究目标。因此，最终判断为True。", "summary2": "本文旨在研究角色设计在大型语言模型上下文学习中的影响。针对多种NLP任务和数学推理问题，我们提出了五种不同的角色配置提示设计，并在五个数据集上通过F1分数和结构准确性验证了其有效性。实验表明，包含系统、用户和助手角色的FewSUA配置在大多数NLP任务中表现最佳，而数学推理任务则受益于允许解释的提示设计。", "inspiration_trace": "# 角色设计在大型语言模型上下文学习中的影响：逻辑链推演\n\n## 1. 宏观问题：提示工程的盲区\n\n**起点观察**：大型语言模型(LLMs)通过上下文学习(ICL)展现出强大能力，无需额外微调即可适应多样化任务。然而，现有研究主要集中在提示内容选择和结构优化上，忽视了一个关键维度——角色设计。\n\n**问题识别**：作者发现，尽管提示工程已被广泛研究，但提示中角色分配(系统、用户、助手)对模型行为的影响机制尚未被系统探索。这构成了研究领域的明显空白。\n\n## 2. 中观聚焦：角色设计的潜在价值\n\n**假设形成**：基于初步观察，作者提出核心假设——明确的角色区分可能会显著影响LLMs在ICL中的表现，且这种影响可能因任务类型和模型架构而异。\n\n**理论支撑**：作者引用相关研究表明，角色扮演提示能增强LLMs的推理能力，但这些研究主要局限于零样本设置，缺乏对不同学习范式的系统比较。\n\n## 3. 微观实验：系统性验证\n\n**实验设计**：为验证假设，作者构建了严谨的实验框架：\n- **模型选择**：采用GPT-3.5、GPT-4o、Llama2-7b和Llama2-13b四种代表性LLMs\n- **任务覆盖**：选择情感分析、文本分类、问答和数学推理等多样化任务\n- **提示配置**：设计五种基本角色设计变体，从简单到复杂逐步增加角色区分度\n\n**核心方法演进**：\n1. **基础角色设计**：从ZeroU(无角色区分)到FewSUA(完整系统-用户-助手角色)\n2. **复杂任务适配**：针对数学推理任务，设计Reasoning-First等高级提示策略\n3. **评估维度创新**：引入\"结构准确性\"指标，评估模型对预期提示结构的遵循程度\n\n## 4. 发现与验证：角色设计的差异化效应\n\n**关键发现**：\n- **简单NLP任务**：FewSUA配置表现最佳，表明明确角色区分和示例对提高模型性能至关重要\n- **复杂推理任务**：允许解释的配置(如Reasoning-First)优于严格结构遵循，表明在复杂任务中，鼓励模型表达思维过程更有效\n- **模型差异**：较小模型(如Llama2)在结构遵循方面表现较差，需要更明确的角色设计\n\n**理论修正**：基于实验结果，作者修正了初始假设，形成了更精细的方法论——角色设计效果存在任务和模型依赖性，不存在\"一刀切\"的最优策略。\n\n## 5. 方法论形成：角色设计的指导原则\n\n**核心贡献**：作者提出了关于角色设计在ICL中的系统方法论：\n- **任务适配原则**：简单NLP任务受益于明确角色区分(FewSUA)，复杂推理任务需要更精细设计(Reasoning-First)\n- **模型考量原则**：较小模型需要更明确的角色设计来遵循结构要求\n- **结构-性能权衡原则**：在复杂推理任务中，结构准确性与任务性能可能存在权衡\n\n**实践意义**：为研究人员和从业者提供了针对不同任务和模型优化提示设计的具体指导，填补了提示工程中角色设计研究的空白。\n\n这一逻辑链从宏观问题出发，通过观察、假设形成、实验验证，最终形成了系统性的方法论，展现了严谨的学术思维过程。", "summary_translation": "上下文学习(In-context learning, ICL)使大型语言模型(Large Language Models, LLMs)能够基于提示(prompts)生成预测，而无需额外的微调(fine-tuning)。尽管提示工程(prompt engineering)已被广泛研究，但提示中角色设计(role design)的影响仍未得到充分探索。本研究使用来自OpenAI的GPT-3.5和GPT-4o以及来自Meta的Llama2-7b和Llama2-13b，考察了角色配置(role configurations)在零样本(zero-shot)和少样本(few-shot)学习场景中的影响。我们评估了模型在多个数据集上的表现，重点关注情感分析(sentiment analysis)、文本分类(text classification)、问答(question answering)和数学推理(math reasoning)等任务。我们的研究结果表明，基于角色的提示结构(role-based prompt structuring)有潜力提升大型语言模型(LLM)的性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#132", "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models", "link": "/arxiv/2509.23441", "arxiv_id": "2509.23441", "authors": "Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li", "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit harmful behaviors. Current alignment strategies typically embed safety into model weights, making these controls implicit, static, and difficult to modify. This paper introduces Cognition-of-Thought (CooT), a novel decoding-time framework that equips LLMs with an explicit cognitive self-monitoring loop. CooT couples a standard text Generator with a cognitive Perceiver that continuously monitors the unfolding sequence. The Perceiver uses a structured, precedence-based hierarchy of principles (e.g., safety over obedience) to detect potential misalignments as they arise. When violations are flagged, CooT intervenes by rolling back the generation to the point of error and regenerating under injected guidance that combines universal social priors with context-specific warnings. CooT thus transforms alignment from a fixed property into an explicit, dynamic, and auditable process active during inference, allowing for flexible policy updates without retraining the model. Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.560080", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出Cognition-of-Thought (CooT)框架，这是一种新的解码时推理框架，旨在增强LLM的认知自我监控能力。论文的核心贡献不是将LLM作为工具应用到特定领域，而是改进LLM本身的基础推理能力，特别是社会对齐推理能力。这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 其次，从正面指标分析，论文明确关注大语言模型(LLMs)这一核心概念，并直接针对推理能力(reasoning)，特别是社会对齐推理(social-aligned reasoning)。虽然不是传统意义上的训练方法，但CooT框架代表了一种新兴的推理范式，通过自我监控循环增强模型的推理过程。 第三，论文不符合排除标准。它不涉及多模态与视觉内容，不专注于特定应用领域（如医疗、化学等），虽然涉及安全性，但是从提升模型内在推理质量的角度出发，而非单纯的应用层面研究。 在处理特殊和模糊情况时，虽然论文涉及安全性，但它是通过提出一种新的推理框架(CooT)来增强模型的社会对齐推理能力，从而提升模型的通用推理质量，这符合\"提出新方法来增强模型内在可靠性，从而提升模型的通用推理质量\"的保留标准。 综上所述，这篇论文的核心贡献是提出一种新的推理框架来增强LLM的通用推理能力，特别是社会对齐推理能力，完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决大型语言模型在推理过程中可能产生有害行为的问题，同时使alignment控制更加显式、动态和可审计。针对LLMs在生成过程中可能出现的潜在风险，我们提出了一种Cognition-of-Thought (CooT)框架，通过Generator和Perceiver的耦合架构实现实时监控和干预，并在AIR-Bench 2024和SocialEval等基准测试上通过安全合规率和社会推理性能指标验证了其有效性。", "inspiration_trace": "# 从问题到方案：CooT方法的逻辑演进\n\n## 宏观问题：大型语言模型的安全与对齐困境\n\n### 观察阶段：当前对齐策略的根本局限\n作者首先观察到大型语言模型(LLMs)的核心矛盾：它们在复杂推理任务上表现出色，却仍可能产生有害行为。深入分析主流对齐策略(如RLHF、DPO等)后发现一个共同问题：**对齐被嵌入为模型权重的隐式属性**，导致三大局限：\n- 隐式性：规范优先级被\"烘焙\"到参数中，推理时不可见\n- 静态性：部署后难以修改或更新\n- 不灵活性：无法适应不同上下文或政策需求\n\n这引出一个关键问题：能否将对齐从静态属性转变为动态过程？\n\n### 借鉴阶段：人类认知的启发\n作者转向人类认知过程寻找解决方案，发现：\n1. 心理学研究表明可靠推理建立在**实时自我监控与调节**能力上(Flavell, 1979)\n2. 道德心理学强调这种调节由**优先级层次结构**引导，如避免伤害优先于服从(Kohlberg, 1963)\n3. 人类日常交流中自然交织语义表达与认知对齐，如说话者意识到可能冒犯他人时会重新表述\n\n这形成核心假设：**如果为LLMs构建显式认知自我监控循环，将对齐转变为动态过程，可实现更有效的安全与社会对齐**。\n\n### 构建阶段：CooT框架的设计\n基于这一假设，作者设计了Cognition-of-Thought(CooT)框架，包含两个核心组件：\n\n#### 1. Generator-Perceiver耦合架构\n- **Generator**：负责语义生成，类似标准语言模型\n- **Perceiver**：作为\"内部批评者\"，持续监控生成序列并评估是否符合规范\n\n这种设计模拟人类认知中的\"内心独白\"机制，实现生成与认知的并行运作。\n\n#### 2. 认知状态系统\n为使Perceiver有效监控，作者构建结构化状态系统：\n- 借鉴阿西莫夫机器人三定律，定义优先级层次：**安全 > 利他主义 > 自我主义**\n- 设计状态向量：y_t = (y_t^(S), y_t^(A), y_t^(E)) ∈ {-1, 0, 1}^3\n- 定义可行状态空间，确保状态向量符合优先级约束\n\n这种设计使Perceiver能检测原则违反及优先级冲突，如当模型为服从指令而可能造成伤害时。\n\n#### 3. 干预机制\n当检测到不对齐时，触发双重干预：\n- **因果回滚**：通过注意力分析识别不安全轨迹起源点，回滚到错误开始前的位置\n- **思想干预**：注入双重指导\n  * 通用社会指导：基于行为、情感和社交技能清单(BESSI)\n  * 上下文相关指导：针对特定风险生成纠正措施\n\n### 验证阶段：实验证明\n作者通过多维度实验验证CooT有效性：\n1. **安全对齐评估**：在AIR-Bench 2024上，CooT平均合规率达0.80，比基础模型提高13%\n2. **社会智能评估**：在SocialEval上，CooT提高亲社会行为9.02%，同时减少反社会行为\n3. **消融研究**：证明每个组件(回滚、指导注入、优先级感知状态)都至关重要\n\n### 贡献与意义\nCooT的核心创新在于将对齐从静态、隐式的模型权重属性转变为动态、可审计、可编辑的推理过程。这种方法不仅提高了安全性和社会推理能力，还使对齐决策变得透明可解释，且无需重新训练即可灵活更新政策，为LLMs的安全对齐提供了全新范式。\n\n这一演进过程展示了从问题观察到灵感借鉴，从假设形成到框架设计，再到实验验证的完整科学思维路径，体现了作者对LLMs对齐问题的深刻洞察与创新解决方案。", "summary_translation": "大语言模型（Large language models, LLMs）擅长复杂推理，但仍可能表现出有害行为。当前的对齐策略（alignment strategies）通常将安全性嵌入模型权重中，使这些控制变得隐式、静态且难以修改。本文提出了思维认知（Cognition-of-Thought, CooT），一种新颖的解码时（decoding-time）框架，为大语言模型配备了明确的认知自我监控循环。CooT将一个标准文本生成器（Generator）与一个持续监控展开序列的认知感知器（Perceiver）相结合。感知器（Perceiver）使用一种结构化的、基于优先级的原则层次结构（例如，安全优先于服从）来检测出现的潜在不对齐（misalignments）情况。当检测到违规时，CooT通过将生成回滚到错误点，并在注入的指导下重新生成内容来进行干预，该指导结合了普遍的社会先验（social priors）和特定情境的警告。因此，CooT将对齐从一种固定属性转变为在推理（inference）过程中活跃的显式、动态且可审计的过程，允许在不重新训练模型的情况下灵活更新策略。在多个基准测试（benchmarks）和模型系列上的广泛实验证实，CooT一致性地提高了安全性和社会推理性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#140", "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization", "link": "/arxiv/2509.23371", "arxiv_id": "2509.23371", "authors": "Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng", "summary": "Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.", "subjects": "Computation and Language, Artificial Intelligence, Machine Learning", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.569353", "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，提出了一种新的训练范式MetaAPO（Meta-Weighted Adaptive Preference Optimization），用于优化大语言模型与人类价值观的对齐过程。这属于改进LLM基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，从正面指标看，论文明确包含\"Large language models, LLMs\"这一核心概念，并且讨论的\"preference optimization\"与强化学习方法（如RLHF）密切相关，是提升LLM性能的重要训练范式。虽然论文没有直接提及推理能力，但通过改进模型对齐过程，可能间接提升模型的整体推理和问题解决能力。 第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，论文提出的是一种通用的优化框架，旨在解决模型训练中的数据分布不匹配问题，这属于基础模型研究的范畴，与提高LLM通用能力的目标一致。因此，这篇论文应该被保留。", "summary2": "本文旨在解决大型语言模型对齐中数据生成与偏好优化间的分布不匹配问题。针对预收集离线偏好数据与模型策略间的分布差距，我们提出了一种MetaAPO框架，通过轻量级元学习器动态耦合数据生成与模型训练。在AlpacaEval 2、Arena-Hard和MT-Bench上，通过胜率(WR)、长度控制胜率(LC)和平均多轮得分(Score)等指标验证了其有效性，MetaAPO性能优于现有方法，同时减少42%在线标注成本。", "inspiration_trace": "# 从宏观问题到创新方法：MetaAPO的逻辑演进\n\n## 1. 宏观问题：大型语言模型与人类价值观的对齐挑战\n\n### 观察现象：LLM对齐中的关键矛盾\n- **对齐需求**：大型语言模型(LLMs)需要与人类价值观和意图对齐，确保其输出有帮助、诚实且无害\n- **现有方法局限**：强化学习从人类反馈(RLHF)虽有效但计算成本高且训练不稳定；离线偏好优化方法(如DPO、SimPO)简化了流程却面临分布不匹配问题\n- **核心矛盾**：静态的离线偏好数据与动态演化的模型策略之间存在分布差距，导致对齐效果受限\n\n## 2. 问题聚焦：分布不匹配的本质与现有解决方案的不足\n\n### 深入观察：离线与在线数据的权衡困境\n- **离线数据特点**：人类标注的离线数据质量高、多样性丰富，但与当前模型策略存在分布不匹配\n- **在线数据特点**：模型生成的在线数据更符合当前策略分布，但往往缺乏多样性和质量，可能引入噪声\n- **现有方法局限**：依赖静态启发式或解耦的在线采样策略，无法适应模型的动态学习状态\n\n### 关键假设：需要动态耦合机制\n- **假设1**：数据采样与偏好优化过程应紧密耦合，而非独立处理\n- **假设2**：需要一种自适应机制，能根据模型当前状态动态平衡离线与在线数据的贡献\n- **假设3**：元学习方法可能提供解决方案，通过学习\"对齐差距估计\"来指导数据采样和优化\n\n## 3. 方法论形成：MetaAPO框架的构建\n\n### 核心思想1：元加权自适应在线采样\n- **问题**：如何确定哪些离线样本需要在线增强？\n- **解决方案**：\n  1. 使用偏好评分函数评估离线样本与当前模型的对齐程度\n  2. 引入轻量级元学习器将偏好分数映射为权重w∈[0,1]\n  3. 低权重样本(对齐较差)有更高概率被选中进行在线生成\n  4. 通过奖励模型评估生成响应，构建在线偏好对\n\n### 核心思想2：元加权偏好优化\n- **问题**：如何有效结合离线与在线数据进行优化？\n- **解决方案**：\n  1. 设计元加权损失函数：L(θ) = w·ℓ_offline + (1-w)·ℓ_online\n  2. 元权重w动态平衡离线与在线数据的贡献\n  3. 高权重强化从可靠离线数据学习，低权重增加对在线反馈的依赖\n\n### 核心思想3：元学习器作为对齐差距估计器\n- **问题**：如何确保权重分配的合理性？\n- **解决方案**：\n  1. 元学习器与策略模型交替训练\n  2. 最小化元损失：L_meta(ϕ) = -E[h_ϕ(ℓ_off)·ℓ_off + (1-h_ϕ(ℓ_off))·ℓ_on]\n  3. 梯度分析表明：当在线样本优于离线样本时，降低离线权重；反之则提高离线权重\n  4. 提供理论保证：随着元缓冲区增大，学习到的元学习器风险收敛到最优函数风险\n\n## 4. 实验验证与效果确认\n\n### 实验设计\n- **评估基准**：AlpacaEval 2、Arena-Hard和MT-Bench\n- **基础模型**：Llama-3.1-8B和Qwen2.5-7B\n- **对比方法**：离线(DPO、SimPO)、在线(Online DPO、PPO)和混合(ADPO、SELM)方法\n\n### 关键发现\n- **性能优势**：MetaAPO在所有基准测试上显著优于基线方法\n- **效率提升**：减少42%的在线注释成本，整体训练时间减少50%以上\n- **动态行为**：训练过程展现\"探索-整合\"行为，有效识别并解决对齐差距\n- **组件重要性**：消融实验证实元学习器、自适应采样和加权机制都是成功的关键因素\n\n## 5. 逻辑演进总结\n\n从宏观的LLM对齐挑战出发，作者观察到分布不匹配是核心问题，特别是离线数据与动态模型策略之间的差距。通过深入分析离线与在线数据的权衡困境，提出需要一种动态耦合机制的关键假设。基于这些假设，构建了MetaAPO框架，通过元学习器实现数据采样与模型优化的紧密耦合，动态平衡离线与在线数据的贡献。最终，通过理论分析和实验验证，确认了该方法的有效性和效率。\n\n这一逻辑演进体现了从问题观察到假设形成，再到方法论构建和验证的完整科研思维过程，成功解决了LLM对齐中的关键挑战。", "summary_translation": "偏好优化（Preference optimization）对于使大型语言模型（LLMs）与人类价值观和意图保持一致至关重要。此过程中的一个重大挑战是预先收集的离线偏好数据与不断发展的模型策略之间的分布不匹配（distribution mismatch）。现有方法尝试使用静态启发式方法或解耦的在线采样策略来缩小这一差距，但它们往往无法适应模型的动态学习状态。为了弥合这一差距，我们提出了元加权自适应偏好优化（Meta-Weighted Adaptive Preference Optimization, MetaAPO），这是一个将数据生成与模型训练动态耦合的新框架。MetaAPO采用了一个轻量级元学习器（meta-learner）作为\"对齐差距估计器\"（alignment gap estimator），以评估在线策略采样（on-policy sampling）相对于离线数据的潜在效益。这指导了有针对性的在线生成，并为优化目标分配样本级别的元权重（meta-weights），动态平衡在线和离线数据的质量和分布。在AlpacaEval 2、Arena-Hard和MT-Bench上的实验表明，MetaAPO在各种设置下始终优于现有的偏好优化方法，同时将在线注释成本降低了42%。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#145", "title": "Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces", "link": "/arxiv/2509.23291", "arxiv_id": "2509.23291", "authors": "Joseph Marvin Imperial, Harish Tayyar Madabushi", "summary": "Policy compliance assessment is a fundamental task of evaluating whether an input case strictly complies with a set of human-defined rules, more generally known as policies. In practice, human experts follow a systematic, step-by-step process to identify violations with respect to specific stipulations outlined in the policy. However, such documentation of gold-standard, expert-level reasoning processes is costly to acquire. In this paper, we introduce Policy Reasoning Traces (PRT), a form of specialized generated reasoning chains that serve as a reasoning bridge to improve an LLM's policy compliance assessment capabilities. Our empirical evaluations demonstrate that the use of PRTs for both inference-time and training-time scenarios significantly enhances the performance of open-weight and commercial models, setting a new state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also highlight how PRTs can improve an LLM's ability to accurately cite policy clauses, as well as influence compliance decisions through their high utilization from the raw chains of thought.", "subjects": "Computation and Language, Machine Learning", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.571926", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围，理由如下： 第一步核心判断：论文的核心是提出\"Policy Reasoning Traces (PRT)\"，一种专门的生成推理链形式，作为推理桥梁来提高LLM的政策合规评估能力。这本质上是一种改进LLM推理能力的新方法，类似于思维链(CoT)的变体，专门针对政策合规任务。论文关注的是提升LLM的基础推理能力，而非将LLM作为工具应用到特定领域，因此符合保留标准。 第二步正面指标：论文明确包含多个正面指标： - 核心概念：研究的是语言模型(LLMs)的政策合规评估能力 - 能力方向：直接涉及推理(reasoning)能力，特别是政策合规评估中的逻辑推理(logical reasoning)和问题解决(problem-solving) - 论文提出的PRT方法可以视为一种增强模型推理能力的新范式 第三步排除标准：虽然论文提到了HIPAA和GDPR政策，可能看似涉及特定应用领域，但论文的核心不是解决医疗或数据保护领域的具体问题，而是提高LLM在政策合规评估这一通用推理任务上的能力。政策合规评估本身是一种通用推理任务，类似于逻辑推理，因此不应被排除。 第四步特殊和模糊情况：论文提出的PRT方法能提高LLM\"准确引用政策条款\"的能力，这与增强模型内在的可解释性和推理质量相关，符合保留标准。 综合判断：这篇论文的核心贡献是提出一种新的推理方法(PRT)来增强LLM的通用推理能力，特别是在政策合规评估任务上。虽然论文使用HIPAA和GDPR作为评估案例，但其研究目标是提升LLM的通用推理能力，而非专注于特定应用领域。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在提高大型语言模型在政策合规性评估任务中的表现。针对复杂的政策解读场景，我们提出了一种Policy Reasoning Traces (PRT)方法，作为连接政策特定约束与合规判断的推理桥梁。在HIPAA、GDPR和ModelSpec三个多领域政策数据集上，通过准确率指标验证了PRT在推理时和训练时的有效性，显著提升了开放权重和商业模型的性能，并为HIPAA和GDPR设立了新的最先进水平。", "inspiration_trace": "# 政策推理追踪(PRT)方法的逻辑演进分析\n\n## 1. 宏观问题：政策合规性评估的自动化挑战\n\n**观察起点**：在医疗、法律、安全等高风险领域，经常需要评估特定案例是否符合既定政策（如法规、规则）。例如，法院需审查医疗交易是否符合数据隐私法规(GDPR)，企业需确保操作符合行业规范等。\n\n**核心问题**：如何使大型语言模型(LLM)能像人类专家一样，系统化地评估案例是否符合复杂政策，并给出准确判断？\n\n## 2. 深入分析：人类专家的推理过程\n\n**关键观察**：人类专家进行政策合规评估时，遵循系统化、逐步的推理过程：\n- 识别案例中的关键要素\n- 将其与政策中的具体条款对应\n- 分析是否符合/违反规定\n- 基于分析得出结论\n\n**核心障碍**：获取这种专家级推理过程的详细记录成本高昂且耗时，难以大规模获取作为训练数据。\n\n## 3. 灵感来源：法律领域的先例推理\n\n**领域洞察**：在法律领域，法院经常参考先例或先前案例的推理过程，识别适用的法律条款，然后作出裁决。这种基于案例的推理方式为解决复杂政策合规问题提供了思路。\n\n**核心假设**：如果能为LLM提供类似专家推理过程的示例，作为\"推理桥梁\"，可能显著提升其政策合规评估能力。\n\n## 4. 创新突破：利用LLM生成伪专家推理\n\n**关键思考**：既然真实专家推理过程难以获取，是否可以利用现有的高级推理LLM来生成\"伪专家\"推理过程？\n\n**方法构想**：使用前沿推理模型(如DeepSeek-R1)，基于已有的案例-裁决对，生成详细的推理过程，模拟专家的思考路径。\n\n## 5. 方法形成：政策推理追踪(PRT)\n\n**概念定义**：PRT是一种专门的生成推理链，作为连接政策具体约束与合规判断的\"推理桥梁\"。\n\n**生成方式**：\n```\nPRT_i = M_E(c_i, P, v_i)\n```\n其中M_E是专家推理模型，c_i是案例，P是政策，v_i是黄金标准裁决。\n\n**应用方式**：\n- **推理时**：作为上下文学习中的少样本示例\n- **训练时**：作为监督微调的编译数据集\n\n## 6. 验证设计：多维度评估框架\n\n**验证假设**：如果PRT有效，应能：\n1. 提高多种模型在多种政策上的评估准确性\n2. 增强模型准确引用政策条款的能力\n3. 展示跨政策泛化能力\n4. 被模型实际用作推理过程的一部分\n\n**实验设计**：\n- **政策选择**：HIPAA(医疗)、GDPR(数据隐私)、ModelSpec(模型安全)\n- **模型测试**：覆盖开源和商业模型\n- **评估方法**：标准提示、少样本学习、自我反馈、添加PRT等对比\n\n## 7. 实证发现：效果验证与优化\n\n**核心发现**：\n1. PRT显著提升模型性能，HIPAA上提升50-100%\n2. 在GDPR上建立新的最先进水平\n3. 提高政策条款引用准确性(+2-13.7%)\n4. 展示跨政策泛化能力\n5. 模型确实将PRT用作推理桥梁(利用率>80%)\n\n**方法优化**：比较通用专家模型(DeepSeek-R1)与领域专家模型(SAUL LM-54B)生成的PRT，发现通用模型通常表现更好。\n\n## 8. 反思与局限：方法的边界认知\n\n**局限认识**：\n1. PRT不是完美监督，可能包含不一致性和幻觉\n2. 对已针对特定政策优化的模型(如OpenAI模型在ModelSpec上)收益有限\n3. 增加上下文长度，需要更多推理预算\n\n**未来方向**：探索使用偏好调优优化更高质量的PRT，但可能需要领域专家的广泛注释工作。\n\n## 逻辑演进总结\n\n从\"政策合规评估自动化\"这一宏观问题出发，通过分析人类专家推理过程的关键作用，识别出缺乏专家推理数据的核心障碍，进而创新性地提出利用LLM自身生成伪专家推理的解决方案，最终形成PRT方法并通过多维度实验验证其有效性。整个逻辑链条体现了从问题观察到假设形成、方法设计、实验验证的完整科研思维过程。", "summary_translation": "政策合规评估（Policy compliance assessment）是一项基础任务，用于评估一个输入案例是否严格遵守一套人类定义的规则，这些规则通常被称为政策（policies）。在实践中，人类专家遵循一个系统的、循序渐进的过程，以识别与政策中概述的具体规定相关的违规行为。然而，获取这种黄金标准（gold-standard）的专家级推理过程文档成本高昂。在本文中，我们介绍了政策推理轨迹（Policy Reasoning Traces, PRT），这是一种专门的生成推理链形式，作为推理桥梁来提高大型语言模型（LLM）的政策合规评估能力。我们的实证评估表明，在推理时间（inference-time）和训练时间（training-time）场景中使用PRT显著增强了开源权重（open-weight）和商业模型的性能，为HIPAA（健康保险可携性和责任法案）和GDPR（通用数据保护条例）政策设定了新的最先进水平（state-of-the-art）。除了准确性提升外，我们还强调了PRT如何提高大型语言模型准确引用政策条款的能力，以及通过高度利用原始思维链（raw chains of thought）来影响合规决策。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#144", "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning", "link": "/arxiv/2509.23330", "arxiv_id": "2509.23330", "authors": "Peng Yu, Zeyuan Zhao, Shao Zhang, Luoyi Fu, Xinbing Wang, Ying Wen", "summary": "Large language models (LLMs) have achieved significant advancements in reasoning capabilities through reinforcement learning (RL) via environmental exploration. As the intrinsic properties of the environment determine the abilities that LLMs can learn, the environment plays a important role in the RL finetuning process. An ideal LLM reasoning environment should possess three core characteristics: scalability, generalizable reasoning, and verifiability. However, existing mathematical and coding environments are difficult to scale due to heavy reliance on expert annotation, while the skills learned in game-based environments are too specialized to generalize. To bridge this gap, we introduce the \\textbf{S}tructured \\textbf{I}n-context \\textbf{E}nvironment (SIE) framework. SIE achieves scalability by automatically constructing reasoning environments from large-scale structured data, where the rich compositional patterns naturally support generalizable reasoning. Moreover, the explicit schemas and reasoning chains in structured data provide a foundation for rule-based verifiability. Experimental results show that SIE framework not only achieves substantial improvements in in-domain structured reasoning, but also enables the learned compositional reasoning skills to generalize effectively to out-of-domain mathematical and logical reasoning tasks. We further explored learning in information-limited partial SIEs and found that LLMs can infer the missing information through exploring the environment, leading to robust reasoning improvements and generalization performance.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.571411", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。以下是我的详细判断过程： 第一步：核心判断——论文的核心是关于改进LLM的推理能力。论文提出了SIE框架，这是一种新的训练/微调范式，通过强化学习和环境设计来增强LLM的推理能力。论文明确关注的是提升模型的逻辑推理和数学推理等通用能力，而不是将LLM作为工具应用于特定领域。 第二步：正面指标——论文包含多个关键正面指标： - 核心概念：明确提及\"Large language models (LLMs)\" - 能力方向：多次强调\"reasoning capabilities\"、\"generalizable reasoning\"、\"mathematical and logical reasoning\" - 训练方法：核心方法为\"reinforcement learning (RL)\"，通过环境探索进行微调 - 新兴范式：虽然未直接提及智能体，但\"environmental exploration\"概念与智能体交互环境的思想一致 第三步：排除标准——论文不涉及任何排除标准中的领域： - 未涉及多模态与视觉内容 - 未专注于医疗、化学、生物等特定应用领域 - 未主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况处理——论文中的环境探索概念是为了提升通用推理能力，而非应用于特定领域，符合保留条件。 核心贡献分析：论文提出的SIE框架通过结构化上下文环境和强化学习来提升LLM的推理能力，使模型能够学习可泛化的组合推理技能，并有效应用于数学和逻辑推理任务。这一贡献直接针对提升LLM的通用推理能力，完全符合研究目标。论文不仅关注了推理能力的提升，还通过实验验证了这些能力的泛化性，这正是\"通用推理能力\"研究的核心。", "summary2": "本文旨在解决LLM推理环境中可扩展性、可泛化性和可验证性的挑战。针对结构化数据场景，我们提出了一种结构化上下文环境(SIE)框架，通过从大规模知识图谱自动构建可扩展推理环境，并在WebQSP、CWQ和GrailQA等数据集上通过pass@1指标验证了其有效性。", "inspiration_trace": "# 从问题到方法：SIE框架的逻辑演进\n\n## 宏观问题：如何有效提升LLM的推理能力\n\n作者首先观察到，通过强化学习(RL)进行环境探索已成为提升大语言模型(LLM)推理能力的主要范式。然而，现有研究主要聚焦于算法优化，忽视了训练环境的关键作用。这引发了一个核心问题：**环境的内在特性如何决定LLM能够学习哪些能力？**\n\n## 理想环境的特征定义\n\n通过分析现有环境的局限性，作者提出了理想LLM推理环境应具备的三个核心特征：\n\n1. **可扩展性**：能以自动化、低成本方式从大规模数据源构建高质量训练环境\n2. **可泛化推理**：学到的推理策略能转移到其他通用推理领域\n3. **可验证性**：具备清晰、客观的规则来验证答案正确性\n\n## 现有环境的局限性分析\n\n作者识别出两类主要环境及其缺陷：\n\n- **内部化规则环境**（如数学）：依赖昂贵专家标注，难以扩展\n- **外部化规则环境**（如游戏）：技能过于专业化，泛化能力差\n\n这引出了一个关键问题：**如何构建同时满足可扩展性、可泛化性和可验证性的推理环境？**\n\n## 结构化数据的潜力发现\n\n作者观察到结构化数据（如知识图谱）具有独特优势：\n\n1. **丰富资源**：存在大量现实世界结构化数据，支持自动化环境构建\n2. **知识浓缩**：结构化数据是人类经验和领域知识的高度浓缩，推理模式泛化潜力强\n3. **显式约束**：内置模式和约束支持基于规则的验证\n\n这形成了一个核心假设：**从大规模结构化数据自动构建推理环境，可能平衡可扩展性和可泛化性**。\n\n## SIE框架的设计与实现\n\n基于上述假设，作者提出了结构化上下文环境(SIE)框架，其核心创新在于：\n\n1. **环境表示**：将环境动态编码为结构化上下文，作为LLM提示中的软约束\n2. **自动化构建管道**：\n   - 种子子图检索（双向检索提高效率）\n   - 支持子图提取（最短路径算法）\n   - 干扰子图过滤（两阶段语义过滤）\n   - 部分SIE构建（控制信息比例模拟不同难度）\n3. **RL训练机制**：使用GRPO算法，结合答案奖励和格式奖励指导模型学习组合推理策略\n\n## 实验验证与发现\n\n作者通过系统实验验证了SIE框架的有效性：\n\n1. **结构化推理提升**：在WebQSP、CWQ等任务上，RL w/ SIE比无上下文RL平均提升50%以上\n2. **训练效率优势**：相比传统监督微调(SFT w/ SRD)，SIE中的RL训练效率高出40%以上\n3. **跨域泛化能力**：学到的推理策略成功泛化到数学(GSM8K、MATH500)和逻辑(KK-easy、KK-hard)任务\n4. **信息受限鲁棒性**：即使在信息不完整的部分SIE中，模型仍能通过环境探索推断缺失信息\n\n## 逻辑演进总结\n\n从\"如何提升LLM推理能力\"的宏观问题出发，作者通过识别理想环境的特征、分析现有方法的局限、发现结构化数据的潜力，逐步形成了SIE框架的核心思想。这一框架成功解决了环境构建的可扩展性、推理能力的可泛化性和结果验证的可靠性三大挑战，为提升LLM推理能力提供了新范式。整个逻辑链条体现了从问题观察到假设形成，再到方法设计和实验验证的完整科研思维过程。", "summary_translation": "大语言模型（Large Language Models, LLMs）通过环境探索的强化学习（Reinforcement Learning, RL）在推理能力方面取得了显著进展。由于环境的内在特性决定了LLMs可以学习的能力，环境在RL微调过程中扮演着重要角色。一个理想的LLM推理环境应具备三个核心特征：可扩展性（scalability）、可泛化推理（generalizable reasoning）和可验证性（verifiability）。然而，现有的数学和编码环境由于严重依赖专家标注而难以扩展，而在基于游戏的环境中学习的技能过于专业化而难以泛化。为了弥补这一差距，我们提出了结构化上下文环境（Structured In-context Environment, SIE）框架。SIE通过从大规模结构化数据中自动构建推理环境来实现可扩展性，其中丰富的组合模式自然支持可泛化推理。此外，结构化数据中的显式模式和推理链为基于规则的可验证性提供了基础。实验结果表明，SIE框架不仅在域内结构化推理方面实现了显著改进，还使学习到的组合推理技能能够有效泛化到域外数学和逻辑推理任务。我们进一步探索了在信息受限的部分SIE中的学习，发现LLMs可以通过探索环境来推断缺失信息，从而带来稳健的推理改进和泛化性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#150", "title": "PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness", "link": "/arxiv/2509.23206", "arxiv_id": "2509.23206", "authors": "Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, pengxin, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen", "summary": "Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.672588", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：论文的核心是关于改进LLM在多轮对话中调用函数的能力，提出了PARL-MT框架，通过引入进度感知(progress awareness)来增强模型的规划和多步推理能力。这属于提高LLM本身的通用推理能力的研究，特别是规划和多步推理方面，因此应该保留。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确讨论了大型语言模型(LLMs) - 能力方向：强调了\"progress awareness\"（进度感知），涉及总结过去交互和规划未来行动的能力，直接对应planning和problem-solving能力 - 训练方法：提出了\"Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm\"，使用了强化学习方法 - 新兴范式：讨论了\"function calling\"（函数调用），属于tool use的范畴 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 虽然提到\"travel planning or multi-stage data analysis\"等应用场景，但只是作为例子，论文本身是提出通用框架而非针对特定领域 - 不主要聚焦于模型可靠性方面的研究 第四步特殊和模糊情况：论文讨论的\"function calling\"属于工具使用范畴，且PARL-MT是一种通用框架，用于增强LLM在多轮对话中调用函数的能力，属于通用的工具使用方法，而非针对特定领域的应用，因此应该保留。 综合判断：这篇论文的核心贡献是提出了一种增强LLM通用推理能力（特别是规划和多步推理）的新方法，通过引入进度感知和强化学习来改进多轮函数调用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型在多轮对话中缺乏进度感知能力的问题。针对多轮函数调用场景，我们提出了一种PARL-MT框架，通过Progress Awareness Generation管道和Progress Awareness-Guided Reinforcement Learning算法，将进度感知整合到模型训练中。在BFCL-V3和τ-Bench两个公共基准上，通过Executable Function Accuracy等指标验证了该方法显著优于现有策略，有效提升了模型在多轮对话中的函数调用性能。", "inspiration_trace": "# PARL-MT方法论逻辑推演分析\n\n## 一、宏观问题：多轮对话中的函数调用挑战\n\n**起点**：现实世界应用（如旅行规划、企业工作流）通常需要多轮对话完成，而非孤立交互。在这些场景中，LLMs不仅需要在每一步准确调用函数，还必须具备\"进度感知\"能力——即总结历史交互并规划未来行动，以确保任务连贯执行。\n\n**核心矛盾**：现有方法无法有效处理多轮对话中的长期依赖关系，导致重复调用函数、省略参数等问题，成为性能瓶颈。\n\n## 二、问题观察与现有方法分析\n\n### 1. 现有方法局限性观察\n\n**单轮训练方法的缺陷**：\n- 将多轮对话降级为孤立单轮样本，忽视任务级规划\n- 训练中排除未来对话信息，阻碍模型规划能力发展\n- 过度关注单轮准确性，忽略对未来有用的历史信息\n\n**端到端强化学习的缺陷**：\n- 随对话增长，输入冗余加剧，决策效率低下\n- 缺乏进度感知的显式整合，难以对齐局部行动与全局任务完成\n\n### 2. 关键洞察：进度感知的核心作用\n\n作者观察到，成功执行多轮任务需要两种关键能力：\n- **历史总结**：准确总结交互历史，减少冗余，辅助决策\n- **未来规划**：系统规划未来执行步骤，确保任务连贯性\n\n**核心假设**：显式地将进度感知纳入LLMs训练，可以显著提升多轮函数调用性能。\n\n## 三、方法论形成：从假设到设计\n\n基于上述洞察，作者提出PARL-MT框架，包含两个核心组件：\n\n### 1. 进度感知生成管道(PAG)\n\n**设计逻辑**：若要模型具备进度感知能力，首先需要高质量的数据集。\n\n**实现路径**：\n- **轨迹分割**：将多轮对话在函数调用处分割，保留关键上下文\n- **感知生成**：使用LLM生成包含历史总结、未来计划和基本原理的紧凑感知文档\n- **模型感知验证**：确保生成的感知包含足够信息重建原始函数调用\n- **多样性保持增强**：通过语义变换增强鲁棒性，避免过拟合\n- **模型预热**：使用增强数据集进行初步训练，建立基础进度感知能力\n\n**关键创新**：首次构建了将对话总结与任务规划相结合的显式进度感知数据集。\n\n### 2. 进度感知引导强化学习(PAG-RL)\n\n**设计逻辑**：将进度感知整合到强化学习中，解决上下文冗余和局部-全局对齐问题。\n\n**实现路径**：\n- **感知引导滚动**：每步先生成进度感知，再基于感知生成动作，减少冗余\n- **复合奖励设计**：结合结构有效性、模式正确性、任务成功和执行效率\n- **优化程序**：采用GRPO算法，将归一化优势均匀分配给轨迹中所有标记\n\n**关键创新**：首次将进度感知显式整合到多轮强化学习训练中，提供更高效的决策指导。\n\n## 四、验证与效果评估\n\n**实验设计**：在BFCL-V3和τ-Bench两个多轮函数调用基准测试上验证，使用多种骨干LLMs。\n\n**核心发现**：\n1. PARL-MT在所有基准测试和模型上一致优于现有方法\n2. 消融研究证实PAG和PAG-RL都对性能提升有贡献\n3. 进度感知能力评估显示模型在总结和规划方面显著提升\n4. 实际案例证明PARL-MT在准确性和效率上均优于基线\n\n## 五、逻辑链总结\n\n从宏观问题到最终方法，作者形成了清晰的逻辑链条：\n\n1. **问题识别**：现实多轮对话需要进度感知能力，现有方法无法满足\n2. **洞察发现**：进度感知（历史总结+未来规划）是解决多轮函数调用瓶颈的关键\n3. **假设形成**：显式整合进度感知可显著提升多轮函数调用性能\n4. **方法设计**：\n   - PAG：构建高质量进度感知数据集并预热模型\n   - PAG-RL：将进度感知整合到强化学习中，减少冗余并提供高效决策\n5. **实验验证**：在多个基准测试上验证方法有效性，证明假设成立\n\n这一逻辑链条从实际问题出发，通过观察、分析、假设形成、方法设计和实验验证，形成了一个完整的研究思路，核心创新在于识别并形式化了\"进度感知\"概念，并设计了专门方法将其整合到LLMs训练中。", "summary_translation": "大型语言模型(LLMs, Large Language Models)在单轮函数调用方面已经取得了令人瞩目的成功，然而现实世界的应用（如旅行规划或多阶段数据分析）通常在多轮对话中展开。在这些场景中，LLMs不仅需要在每一步发出准确的函数调用，还必须保持进度意识(progress awareness)，即总结过去交互并规划未来行动的能力，以确保连贯的、长周期的任务执行。然而，现有方法要么将多轮训练简化为孤立的单轮样本（忽视了任务级规划），要么采用端到端强化学习(RL, Reinforcement Learning)，这种方法存在冗余问题且缺乏对进度意识的明确整合。为克服这些局限性，我们提出了PARL-MT框架，该框架明确地将进度意识整合到用于多轮函数调用的LLMs训练中。PARL-MT结合了(i)进度意识生成(PAG, Progress Awareness Generation)流程，该流程自动构建将对话摘要与未来任务规划相结合的数据集；以及(ii)进度意识引导的强化学习(PAG-RL, Progress Awareness-Guided Reinforcement Learning)算法，该算法将进度意识整合到RL训练中，以减少上下文冗余并提高局部行动与全局任务完成之间的一致性。在两个公共基准上的实证结果表明，PARL-MT显著优于现有方法，凸显了进度意识在实现稳健高效的多轮函数调用方面的有效性。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#156", "title": "Pretraining LLM with Latent Thoughts in Continuous Space", "link": "/arxiv/2509.23184", "arxiv_id": "2509.23184", "authors": "Boyi Zeng, He Li, Shixiang Song, Yixuan Wang, Ziwei He, Xinbing Wang, Zhouhan Lin", "summary": "The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts. Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, ours-1.4B (Pythia Arch), pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.681456", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出一种新的预训练方法论\"Pretraining Language Models with Latent Thoughts\"，该方法通过在预训练阶段引入中间\"潜在思维\"来增强语言模型的推理能力。这明显属于改进LLM基础能力和提出新训练范式的研究，直接受到思维链(CoT)的启发，旨在提升模型的通用推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标主题。核心概念方面明确聚焦于大语言模型(LLMs)；能力方向方面，论文直接关联推理能力，特别是通过形成\"类似于CoT的链\"来增强模型推理；训练方法方面，提出了一种新的预训练范式，虽然不是强化学习或进化方法，但同样是改进模型训练的创新方法。 第三步排除标准：论文完全不涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)等排除领域，纯粹关注提升LLM本身的通用能力。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 核心贡献是提出了一种在连续空间中使用潜在思维预训练LLM的新方法，通过增加计算步骤来增强模型的推理能力，这直接符合提高大语言模型通用推理能力的研究目标。实验证明该方法能显著提升模型性能，且增加潜在思维数量可持续改善模型表现，进一步证实了其对推理能力的增强作用。", "summary2": "本文旨在解决传统语言模型扩展方法面临收益递减的问题。针对语言模型预训练，我们提出了一种在连续空间中使用潜在思想的预训练方法，让模型在预测每个token前先生成中间潜在思想，并在Pile数据集和多个下游任务上通过perplexity和准确率等指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到创新方法：预训练连续空间潜在思想的逻辑推演\n\n## 1. 宏观问题：语言模型能力提升的新路径\n\n论文始于对语言模型发展现状的观察：传统的提升方法——扩大参数规模和数据规模——正面临收益递减的困境。具体表现为：\n- 数据稀缺性（data scarcity）限制了高质量数据的获取\n- 扩展定律（scaling laws）逐渐饱和，增加参数带来的性能提升越来越小\n- 训练开销（training overheads）变得 prohibitive，成本急剧上升\n\n这引发了一个根本性问题：除了传统的\"向上扩展\"（scaling up）外，是否存在其他途径可以有效提升语言模型的能力？\n\n## 2. 关键观察：Chain-of-Thought的启示\n\n作者将目光投向了Chain-of-Thought (CoT)方法的显著成功。CoT通过在测试时扩展生成步骤（生成更长的推理链）来提高性能，这一现象启发了一个关键思考：\n\n**如果测试时的计算步骤扩展能够提升性能，那么在预训练期间是否可以通过类似的计算步骤扩展来提高每个token的生成质量？**\n\n这一观察将问题从\"如何扩大模型\"转向\"如何深化每个生成步骤的计算\"，标志着思维的重要转变。\n\n## 3. 问题聚焦：预训练阶段的计算步骤扩展\n\n基于CoT的启发，作者将问题聚焦为：如何在预训练阶段实现计算步骤的扩展，以提高每个token的生成质量？\n\n这个问题进一步分解为三个子问题：\n- 如何在预训练阶段引入额外的计算步骤？\n- 如何确保这种扩展在计算上是高效的？\n- 如何避免对专门数据集的依赖？\n\n## 4. 现有方法的局限性分析\n\n作者系统分析了现有相关方法的局限性：\n\n**垂直扩展（Vertical Scaling）方法**：如PonderLM，通过重复使用参数来加深网络。但这种方法导致训练不稳定性，且在可比推理预算下往往无法优于标准密集模型。\n\n**离散token空间方法**：如插入\"暂停\"token或规划token。这些方法受限于离散词汇空间，且通常需要专门的数据或复杂训练方案。\n\n**CoT及其变体**：如Coconut，在CoT数据上微调语言模型，使用\"连续思想链\"模拟推理步骤。但这种方法通常仅在问题提出后应用，且需要专门的CoT数据。\n\n这些分析揭示了研究空白：缺乏一种在预训练阶段、在连续空间中、无需专门数据的计算步骤扩展方法。\n\n## 5. 核心假设：连续空间中的潜在思想\n\n基于以上分析，作者提出了核心假设：**如果允许模型在生成每个实际token之前，先生成一个中间的潜在思想（在连续的隐藏状态空间中），然后将这个潜在思想作为输入来预测实际的后续token，那么模型将能够在无约束的连续空间中细化其预测，从而提高性能。**\n\n这一假设的关键创新点在于：\n- 使用连续的隐藏状态空间而非离散的token空间\n- 在生成每个token之前引入额外的计算步骤\n- 通过潜在思想实现预测的细化\n- 在预训练阶段学习这种能力，而非仅在测试时或微调阶段\n\n## 6. 方法设计：潜在思想的生成与利用\n\n基于核心假设，作者设计了具体的方法框架：\n\n**推理过程**：对于每个要生成的token，模型首先计算其对应的最后一个隐藏状态（即潜在思想），然后将这个隐藏状态作为下一个token生成步骤的输入嵌入，模拟递归思考过程。\n\n**训练过程**：由于推理是顺序的，纯自回归训练计算成本过高。为解决此问题，作者采用Jacobi迭代来近似真实的自回归隐藏状态，实现并行训练：\n1. 初始隐藏状态估计（迭代0）\n2. 通过Jacobi迭代并行更新状态\n3. 损失计算\n\n**位置嵌入**：隐藏状态被反馈到模型作为输入时，继承与其对应的原始token嵌入相同的位置编码。\n\n这一设计巧妙地解决了三个子问题：通过潜在思想引入额外计算步骤；通过Jacobi迭代确保计算效率；通过标准预训练目标避免对专门数据的依赖。\n\n## 7. 实验验证：从假设到证据\n\n作者通过一系列精心设计的实验验证了方法的有效性：\n\n**大规模预训练**：在300B-token的Pile数据集上预训练，证明方法在参数效率和数据效率方面均表现出优越扩展特性。\n\n**下游任务评估**：在一般基准测试和指令跟随任务上评估，证明预训练模型不仅优于类似大小模型，甚至超过参数数量翻倍的模型。\n\n**基线方法比较**：与Looped Transformer、Pause Token、Pondering LLM等方法比较，证明该方法在所有语言建模基准上实现最低困惑度，在下游任务上获得最高平均准确率。\n\n**现成模型应用**：在LLaMA-3-3B上进行持续预训练，验证方法对现有大规模基础模型的有效性。\n\n**消融研究**：研究关键组件影响，包括Jacobi迭代次数、位置嵌入策略和潜在思想数量，证明增加潜在思想数量（形成类似CoT的链）可持续提高性能。\n\n## 8. 结论与意义：新的扩展维度\n\n作者得出结论：使用潜在思想预训练的语言模型在相同推理成本下，始终优于参数数量翻倍的对应模型，以及之前的相关方法，即使这些方法使用双倍推理预算。\n\n这项工作的核心贡献在于引入了一个新的语言模型能力扩展维度——**水平扩展（horizontal scaling）**，与传统的垂直扩展（加深网络）形成对比。通过在每个token生成前引入连续空间中的潜在思想，模型能够在预训练阶段学习\"思考\"能力，为提升语言模型性能提供了全新思路。\n\n这一逻辑链条展示了从观察现象、分析问题、提出假设、设计方法到验证效果的完整科学研究过程，体现了作者敏锐的观察力、系统的分析能力和创新的思维方法。", "summary_translation": "思维链(Chain-of-Thought, CoT)的显著成功——通过在测试时扩展生成步骤来提升性能——启发我们提出一个问题：我们能否在预训练期间利用类似的计算步骤扩展来改进每个单独令牌的生成？为解决这一问题，我们提出了一种新颖的预训练方法：使用潜在思维预训练语言模型(Pretraining Language Models with Latent Thoughts)。我们的方法预训练一个语言模型(LM，语言模型)首先生成一个中间潜在思维——即当前位置的最后一个隐藏状态——然后将其用作输入来预测实际的后续令牌。这一额外的计算步骤使语言模型能够在无约束的连续空间(unconstrained continuous space)中优化其预测。我们的实验表明，在相同的推理成本(identical inference cost)下，每个令牌生成一个额外潜在思维的语言模型优于参数数量翻倍的标准模型。例如，我们的模型ours-1.4B (Pythia Arch，Pythia架构)，在来自Pile数据集的3000亿个令牌上预训练，在语言建模和一系列通用下游任务上显著超过了使用相同数据训练的原始Pythia-2.8B模型。此外，在每个实际令牌之前增加生成的潜在思维数量——形成类似于CoT的链——持续地提升了模型的性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#153", "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs", "link": "/arxiv/2509.23196", "arxiv_id": "2509.23196", "authors": "Haonan Wang, Weida Liang, Zihang Fu, Nie Zheng, Yifan Zhang, Yao Tong, Tongyao Zhu, Hao Jiang, Chuang Li, Jiaying Wu, Kenji Kawaguchi", "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME'25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight-refine-solve framework.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.674503", "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，论文的本质是改进LLM的推理能力，而非将其应用于特定领域。论文提出了Insight-to-Solve (I2S)方法，这是一种测试时程序，将上下文示例转化为明确的、可重用的见解，并推导出针对特定问题的推理轨迹，直接针对提升LLM的通用推理能力。 从正面指标来看，论文明确涉及\"reasoning LLMs (RLMs)\"这一核心概念，并聚焦于reasoning能力（特别是在AIME和GPQA等数学和逻辑推理基准测试上）。同时，论文提到了\"verifier-based reinforcement learning\"和\"self-refined\"等与强化学习和自我进化相关的训练方法。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。虽然论文处理了模型推理过程中的问题（语义误导和策略转移失败），但这是从提升模型内在推理质量的角度出发，而非对这些现象的社会学研究或应用层面讨论。 综上所述，这篇论文的核心贡献是提出了一种新的方法来增强LLM的通用推理能力，完全符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决推理大模型(RLMs)在使用上下文推理示例时表现下降的问题。针对高质量推理示例反而降低模型准确性的现象，我们提出I2S(Insight-to-Solve)方法，将示例转化为可重用见解并推导目标推理轨迹，在AIME'25、GPQA等benchmark上通过准确率指标验证了其有效性，GPT-4.1在AIME'25上提升14.0%。", "inspiration_trace": "# 从危害到帮助：推理示例转化为推理大模型资产的逻辑演进\n\n## 一、宏观问题：推理大模型的上下文学习悖论\n\n**起点观察**：现代推理大模型(RLMs)展现出反直觉现象——使用少样本思维链(CoT)提示时，其性能反而不如直接回答。这与传统大语言模型的行为形成鲜明对比，传统LLM通常从CoT示例中获益。\n\n**现象普遍性**：这一现象不仅存在于开源模型中，也被DeepSeek和OpenAI等组织观察到。DeepSeek特别指出RLMs对提示的敏感性，标准少样本CoT提示可能损害模型性能；OpenAI的推理模型手册也明确建议避免使用CoT提示。\n\n## 二、初始假设与验证：示例质量并非根本原因\n\n**初始假设**：传统CoT示例是为早期、较弱的模型设计的，现在可能落后于RLMs自身生成的推理轨迹，从而限制而非增强其能力。\n\n**验证实验**：使用DeepSeek-R1生成的高质量推理轨迹作为示例，这些问题与目标问题紧密匹配。在多个模型(Qwen系列和DeepSeek-R1蒸馏模型)和基准测试(AIME'25和GPQA)上进行评估。\n\n**意外发现**：即使使用高质量、紧密匹配的演示，准确率相对于直接推理仍然持续下降，且随着添加更多示例，下降更加明显。这推翻了初始假设，表明问题不仅在于示例质量。\n\n## 三、深入分析：揭示失败机制\n\n为了理解为什么对人类有帮助的演示对RLMs有害，作者进行了深入研究，识别出两个关键失败机制：\n\n### 1. 语义误导(Semantic Misguidance)\n- **表现形式**：当演示和目标问题共享高词汇重叠时，模型将它们视为等价物，直接复制中间步骤甚至最终结论。\n- **根本原因**：模型过度关注表面相似性，忽略问题的深层结构差异。演示充当\"语义诱饵\"，强调表面线索而掩盖问题结构。\n- **实例佐证**：在数学问题中，共享的\"digits 1-8\"短语导致模型错误地将演示中的\"分成两组\"和\"总和约18\"的策略应用到需要不同结构的目标问题上。\n\n### 2. 策略转移失败(Strategy Transfer Failure)\n- **表现形式**：即使演示编码了有用的结构见解，模型也无法适当地提取并应用这些策略到目标问题。\n- **根本原因**：模型倾向于错误地提取(跳过关键步骤)和错误地应用(使用不相关策略)，将复杂约束简化为简单规则。\n- **实例佐证**：在座位排列问题中，模型未能将演示中\"列举相邻对\"的方法转移到目标问题上，而是错误地将三元组约束简化为成对约束。\n\n## 四、初步尝试：两步推理\n\n**设计思路**：基于分析，作者尝试将答案生成与推理分离——首先使用演示引出推理轨迹，然后丢弃演示，仅使用问题和引出的轨迹生成最终答案。\n\n**实验结果**：这种方法确实带来了一致的改进，但仍未达到直接推理设置的性能。然而，这些结果是有希望的，并推动作者研究如何提高推理轨迹的质量。\n\n## 五、方法设计：从洞察到解决方案(I2S)\n\n基于前面的发现，作者提出了I2S (Insight-to-Solve)方法，这是一个测试时间推理流程，将推理示例转化为抽象指导，然后应用于目标问题。\n\n### 设计原则\n1. **提取结构见解并应用于目标**：专注于提取嵌入在示例中的高级推理策略，而不是原始CoT内容，鼓励模型内化可转移的解决方案模式。\n2. **补充而非覆盖内部推理**：现代RLMs已经拥有强大的潜在推理能力；演示应充当提示，而不是抑制这种能力的刚性模板。\n\n### 结构化推理转移流程\n1. **比较生成**：模型生成演示问题和目标问题之间的结构化比较，突出相似性和差异，帮助识别哪些方面是相关的。\n2. **分析推导**：基于比较和演示推理轨迹，模型生成分析，过滤掉无关细节，保留可转移策略。\n3. **推理生成**：模型通过将分析应用于目标问题来生成自己的推理轨迹，防止逐字重用演示步骤。\n\n## 六、方法扩展：迭代自我完善(I2S+)\n\n对于特别困难的问题，作者引入了I2S+，在基础流程上集成了迭代自我完善循环，使模型能够逐步改进其推理。每次完善迭代包括三个阶段：\n\n1. **建议**：模型生成一组候选修改或扩展建议，如替代中间步骤、完整性检查或错误纠正。\n2. **审查**：模型通过生成评估每个建议质量的检查语句来审查剩余建议。\n3. **完善**：模型采用最佳评级的建议，并使用它来修改当前的推理轨迹，提高一致性和正确性。\n\n## 七、实验验证：有效性证明\n\n作者在多个基准测试上验证了I2S和I2S+的有效性：\n\n- **封闭式基准测试**：在AIME'25和GPQA上，I2S+相对于直接推理带来显著提升(如R1-Distill-Qwen-7B在AIME'25上提升+9.33%)。\n- **开放式推理任务**：在通用推理任务上，I2S和I2S+也优于直接推理和其他基线。\n- **闭源模型**：即使对于GPT模型，该方法也有帮助——在AIME'25上，GPT-4.1提高+14.0%，o1-mini在AIME上提高+2.7%，在GPQA上提高+1.7%。\n\n## 八、效率分析：计算成本与收益\n\n作者研究了额外测试时间计算如何有效地转化为准确性：\n\n- 与并行测试时间扩展基线(如majority@N)相比，I2S在适度预算下更有效地将计算转化为收益。\n- 在相同数量的问题条件调用下，I2S在AIME'25和GPQA上始终优于majority@3。\n- I2S+与使用相当数量问题条件调用的majority@32相比，在GPQA上表现更好或相当。\n\n## 九、结论：从危害到帮助的范式转变\n\n论文得出结论，推理演示并非本质上对RLMs有害；当正确使用时，它们可以改善而不是降低性能。通过洞察-完善-解决框架，可以有效地利用上下文演示，将它们从潜在的危害转变为有价值的资产。\n\n这一研究不仅解释了RLMs中观察到的反直觉现象，还提供了一个实用的测试时间扩展方法，在各种模型和任务上始终优于现有方法，为推理大模型的有效利用开辟了新途径。", "summary_translation": "近期的推理型大型语言模型(RLMs, reasoning LLMs)，特别是那些通过基于验证器的强化学习(verifier-based reinforcement learning)训练的模型，在使用少样本思维链(few-shot CoT, chain-of-thought)时往往表现不如直接回答(direct answering)。我们使用DeepSeek-R1的高质量推理轨迹(reasoning traces)作为示例(demonstrations)重新审视这一悖论，发现即使示例是最优的，增加更多范例(exemplars)也会持续降低准确性。详细分析揭示了这种下降背后的两个机制：(i) 语义误导(semantic misguidance)，即高文本相似性导致模型将目标视为与范例相同，并逐字复制中间步骤；以及(ii) 策略迁移失败(strategy transfer failure)，即模型难以提取有用的推理策略并将其应用于目标问题。\n\n基于这些发现，我们提出了洞察到解决(Insight-to-Solve, I2S)，这是一种顺序测试时程序(test-time procedure)，将示例转化为明确、可重用的洞察(reusable insights)，并推导出针对目标的推理轨迹；此外，推理过程可以进行自我优化(self-refined)以提高连贯性和正确性(I2S+)。在多样化基准测试(benchmarks)上的广泛实验表明，I2S和I2S+在开源和闭源模型(open- and closed-source models)上均持续优于直接回答和测试时扩展(test-time scaling)基线方法(baselines)。即使对于GPT模型，我们的方法也有帮助：在AIME'25上，GPT-4.1提升了+14.0%，o1-mini在AIME上提高了+2.7%，在GPQA上提高了+1.7%，这表明上下文示例(in-context demonstrations)可以通过洞察-优化-解决框架(insight-refine-solve framework)得到有效利用。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#158", "title": "Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models", "link": "/arxiv/2509.23146", "arxiv_id": "2509.23146", "authors": "Zichao Yu, Ming Li, Wenyi Zhang, Weiguo Gao", "summary": "Tree search has recently emerged as a powerful framework for aligning generative models with task-specific rewards at test time. Applying tree search to Masked Diffusion Language Models, however, introduces two key challenges: (i) parallel unmasking yields highly correlated branches, limiting exploration, and (ii) reward evaluation via sampled completions produces high-variance estimates, making pruning unstable. We propose TReASURe, a tree-search test-time alignment method that addresses these issues. It introduces (i) UnmaskBranch, a branching strategy based on first-hitting unmasking that diversifies both token content and reveal order with a single model call per parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic resubstitution to score partially masked sequences with low-variance proxy completions. Theoretically, we quantify branching efficiency gains in NFEs (number of function evaluations), show that the scoring rule approximates the true reward with error bounded by predictive uncertainty, and prove improvements with larger tree widths. Empirically, TReASURe achieves state-of-the-art results on perplexity, linguistic acceptability, and control of sentiment and toxicity, outperforming prior methods under matched compute budgets, with especially strong gains in low-NFE regimes.", "subjects": "Computation and Language, Machine Learning", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.682497", "filter_reason": "我按照筛选标准对这篇论文进行了详细分析： 第一步：核心判断 这篇论文的本质是提出一种名为TReASURe的树搜索测试时间对齐方法，用于改进掩码扩散语言模型的生成能力。论文的核心贡献是解决树搜索应用于掩码扩散语言模型时的两个关键挑战：(1)并行去掩码导致分支高度相关，限制探索；(2)通过采样完成进行奖励评估产生高方差估计，使修剪不稳定。论文提出的UnmaskBranch分支策略和ResubstituteScore修剪规则，旨在增强语言模型的生成质量和控制能力，这属于改进LLM基础能力和推理能力的范畴，而非将LLM作为工具应用到特定领域。 第二步：正面指标 - 核心概念：论文明确研究\"Masked Diffusion Language Models\"，属于LLM范畴。 - 能力方向：树搜索是一种规划方法，与problem-solving相关，论文通过改进树搜索来增强模型的推理和规划能力。 - 训练方法：论文提到\"reward-aligned\"（奖励对齐），这与强化学习中的奖励概念相关。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉领域，虽然提到\"Diffusion\"，但指的是语言模型中的扩散模型，而非视觉扩散模型。 - 不聚焦于特定应用领域，而是关注通用语言模型的改进。 - 虽然提到\"control of sentiment and toxicity\"，但这只是作为评估指标，不是论文主要焦点。 第四步：特殊和模糊情况 论文没有明确讨论智能体/工具使用，也没有专门针对幻觉、可解释性或安全性的改进，因此不适用于这些特殊情况的判断。 最终决策：这篇论文符合研究范围，因为它致力于通过改进树搜索方法来增强大语言模型本身的通用推理能力和生成质量，而非将LLM作为工具应用到特定领域。论文提出的TReASURe方法是一种通用的测试时间对齐技术，能够提升语言模型的基础能力，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决在Masked Diffusion Language Models中应用树搜索进行测试时对齐的两个关键挑战：并行去掩码产生高度相关分支和采样奖励评估的高方差问题。针对MDLMs的离散特性，我们提出了一种名为TR EASUR E的树搜索方法，引入了UNMASK BRANCH分支策略和RESUBSTITUTE SCORE剪枝规则。在困惑度、语言可接受性、情感和毒性控制等多个可控文本生成任务上，通过奖励函数和NFE指标验证了其有效性，在匹配计算预算下实现了最先进的性能。", "inspiration_trace": "# 从观察到创新：TReASURe方法的逻辑演进\n\n## 1. 宏观问题：生成模型的测试时对齐\n\n生成模型（如自回归和扩散模型）在预训练后往往需要根据特定任务目标进行对齐。传统方法通过微调实现，但这需要大量计算资源和训练数据。测试时对齐(TTA)提供了一种替代方案，允许在推理时引导模型输出朝向任务特定目标，无需重新训练。\n\n## 2. 中观聚焦：MDLMs中的测试时对齐挑战\n\n掩码扩散语言模型(MDLMs)作为自回归模型的新兴替代方案，从全掩码token开始，通过离散去噪步骤逐步揭示token。MDLMs具有灵活的采样调度和条件模式，特别适合可控生成任务。\n\n然而，将树搜索方法（在连续扩散模型中表现出色）应用于MDLMs时，作者发现两个关键挑战：\n\n**挑战1：并行去掩码导致高度相关分支**\n- MDLMs中所有掩码位置同时更新，导致紧密耦合的token分布\n- 重复采样产生高度相似候选，探索空间受限\n- 本地重采样很少改变下一个提交token，轨迹多样性不足\n\n**挑战2：奖励评估高方差导致修剪不稳定**\n- MDLMs输出分类分布，估计价值函数需采样完整序列\n- 小扰动可导致奖励大变化，使修剪决策不可靠\n\n## 3. 微观观察：问题具体表现\n\n通过实验，作者验证了这些挑战：\n\n**观察1：分支效率低下**\n- 实验显示，即使使用更宽的beam，大多数节点只折叠到一两个有效分支\n- 表明并行采样产生大量冗余分支，浪费计算资源\n\n**观察2：奖励估计波动大**\n- 去噪步骤中奖励值波动显著，中间步骤尤其不稳定\n- 高方差使得基于这些估计的修剪不可靠\n\n## 4. 核心假设：解决问题的关键点\n\n基于观察，作者形成两个关键假设：\n\n**假设1：仅在提交事件时分支可提高多样性**\n- 只在实际发生去掩码事件时扩展搜索，避免浪费更新\n- 同时实现去掩码顺序和token内容多样化\n- 每个父节点只需一次模型调用，保持效率\n\n**假设2：确定性重新填充可降低方差**\n- 重用分支时获得的概率，通过确定性填充构建临时完成\n- 单一代理完成用奖励评分一次，无需额外模型调用\n- 提供低方差、确定性评分，使修剪更稳定\n\n## 5. 方法论构建：TReASURe设计\n\n基于假设，作者开发了TReASURe方法，包含两个核心组件：\n\n**组件1：UNMASK BRANCH**\n- 使用首次命中采样(FHS)直接跳到下一个去掩码时间\n- 评估模型一次，通过均匀选择掩码索引和枚举top-b(n)个token生成子节点\n- 在去掩码顺序和token内容上引入多样性，每个父节点只需一次模型调用\n\n**组件2：RESUBSTITUTE SCORE**\n- 重用分支概率，通过确定性填充掩码位置构建临时完成\n- 已提交token保持固定，掩码位置填充当前头部预测\n- 单一代理完成用奖励评分一次，修剪成本与父节点数量成线性比例\n\n## 6. 理论验证：方法有效性保障\n\n作者提供三个理论支持：\n\n**理论1：UNMASK BRANCH效率优势**\n- 证明与朴素并行采样相比，显著减少模型评估次数\n- 朴素方法需b(n)/(1-exp(-nh))次评估，而UNMASK BRANCH只需一次\n\n**理论2：RESUBSTITUTE SCORE准确性**\n- 证明在Hamming-Lipschitz奖励假设下，重新填充奖励与真实期望奖励差距由模型预测不确定性限定\n- 当模型对顶部预测高置信度时，重新填充奖励接近期望奖励\n\n**理论3：树宽度增加的奖励单调性**\n- 证明增加树宽度m(n)总是改善最终奖励\n- 为使用更宽搜索提供理论保证\n\n## 7. 实验验证：方法优越性证明\n\n通过全面实验验证：\n\n**实验1：与基线方法比较**\n- TReASURe在所有任务(CoLA、Toxicity、Sentiment和Gen. PPL)上实现最先进性能\n- 在低NFE制度下已超过基线在更高NFE下的结果\n- 随NFE增加性能稳步提高，符合理论预测\n\n**实验2：奖励轨迹可视化**\n- 去噪过程中奖励稳步上升，表明TReASURe引导中间状态朝向更理想区域\n\n**实验3：组件消融研究**\n- RESUBSTITUTE SCORE优于匹配NFE下的前一步评分\n- 以远少于真实后验评分的评估次数实现相当性能\n- 增加beam宽度不保证更好奖励，而增加树宽度则单调改善性能\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过观察和假设形成，最终构建出创新方法论的系统性思考过程。", "summary_translation": "树搜索(Tree search)最近已成为一种强大的框架，用于在测试时间(test time)将生成模型(generative models)与任务特定奖励(task-specific rewards)对齐。然而，将树搜索应用于掩码扩散语言模型(Masked Diffusion Language Models)会带来两个关键挑战：(i) 并行解掩(parallel unmasking)产生高度相关的分支(correlated branches)，限制了探索(exploration)；(ii) 通过采样完成(sampled completions)进行奖励评估(reward evaluation)会产生高方差估计(high-variance estimates)，使剪枝(pruning)不稳定。我们提出了TReASURe，一种解决这些问题的树搜索测试时间对齐(tree-search test-time alignment)方法。它引入了(i) UnmaskBranch，一种基于首次命中解掩(first-hitting unmasking)的分支策略，通过每个父节点(parent node)的单次模型调用来多样化标记内容(token content)和揭示顺序(reveal order)；以及(ii) ResubstituteScore，一种使用确定性重新替换(deterministic resubstitution)来评估部分掩码序列(partially masked sequences)的剪枝规则(pruning rule)，采用低方差代理完成(low-variance proxy completions)。理论上，我们量化了在NFEs (number of function evaluations，函数评估次数)方面的分支效率提升，表明评分规则(scoring rule)以预测不确定性(predictive uncertainty)为界的误差逼近真实奖励，并证明了更大树宽度(tree widths)带来的改进。实证上(Empirically)，TReASURe在困惑度(perplexity)、语言可接受性(linguistic acceptability)以及情感(sentiment)和毒性(toxicity)控制方面取得了最先进的(state-of-the-art)结果，在匹配的计算预算(compute budgets)下优于先前方法，在低NFE机制(low-NFE regimes)下尤其表现出强劲的提升。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#157", "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs", "link": "/arxiv/2509.23166", "arxiv_id": "2509.23166", "authors": "Chenxing Wei, Hong Wang, Ying He, Fei Yu, Yao Shu", "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.681960", "filter_reason": "这篇论文完全符合我的研究目标，因为它专注于提高大语言模型本身的通用推理能力。具体分析如下： 第一步：核心判断——论文的本质是改进LLM的基础能力。论文提出了测试时策略自适应(T2PAM)和ROSA算法，使LLM能够在多轮交互中根据用户反馈实时调整自己的策略，实现对话中的自我修正。这明显是关于提升LLM基础推理和适应能力的研究，而非将LLM作为工具应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确关注Large Language Models (LLMs) - 能力方向: 涉及reasoning和problem-solving，特别是复杂任务的多轮交互推理能力 - 训练方法: 利用用户反馈作为奖励信号来优化模型策略，与强化学习思想一致 - 新兴范式: 关注LLM在多轮交互中的表现，与智能体交互和自我进化相关 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不专注于医疗、化学、生物等特定应用领域 - 不主要讨论水印、安全性等应用层面的可靠性问题 第四步：特殊和模糊情况——论文提出的是一种通用的自适应机制，用于增强LLM在多轮交互中的推理和适应能力，而不是针对特定领域的应用，因此应该保留。 论文的核心贡献是提出了一种新的范式(T2PAM)和算法(ROSA)，使LLM能够在测试时根据用户反馈进行策略自适应，从而提升其在复杂多轮交互中的推理能力和自我修正能力。这直接关系到提升LLM的通用推理能力，完全符合我的研究目标。", "summary2": "本文旨在解决LLMs在多轮交互中性能下降的问题。针对多轮对话场景，我们提出了一种测试时策略适应(T2PAM)范式及ROSA算法，在多个推理任务数据集上通过准确率和纠错提升率验证了其有效性。", "inspiration_trace": "# 从问题观察到方法创新：ROSA算法的逻辑演进\n\n## 1. 宏观问题识别：多轮交互中的性能退化\n\n作者首先观察到一个核心问题：**大型语言模型(LLMs)在多轮对话中表现不佳**。具体表现为：\n- LLMs主要在静态、单轮数据上训练，但实际应用中多轮交互是完成复杂任务的基本范式\n- 随着对话轮次增加，模型性能逐渐下降，难以适应实时用户反馈\n- 现有模型在多轮场景中表现出能力减弱和不稳定性\n\n这一观察揭示了**训练范式与实际应用场景之间的根本不匹配**。\n\n## 2. 现有方法分析：识别局限性\n\n作者系统分析了现有方法的不足，为创新奠定基础：\n\n**传统训练方法**：\n- 监督微调(SFT)和基于人类反馈的强化学习(RLHF)主要依赖单轮数据\n- 多轮训练策略成本高昂，受限于长上下文序列训练和数据收集难度\n\n**推理时方法**：\n- 提示工程：作为上下文学习，难以在少数轮次内实现有效偏好对齐\n- 检索增强生成(RAG)：增加显著推理开销，性能依赖外部数据库质量\n- 模型编辑：结构上不适合编码细粒度用户偏好\n- 现有测试时方法：主要针对单轮任务，计算成本高\n\n这一分析揭示了**现有方法在效率、适应性和计算成本之间的权衡困境**。\n\n## 3. 核心洞察：范式转换的必要性\n\n基于上述分析，作者形成关键洞察：**需要将模型对齐从静态、离线训练阶段转移到动态、在线推理过程**。\n\n这一洞察引出核心假设：如果能够设计一种方法，在推理时利用对话中的用户反馈作为奖励信号，实时更新模型参数，使模型能够动态适应用户偏好，那么就可以提高多轮交互的效果。\n\n## 4. 新范式提出：T2PAM\n\n基于核心洞察，作者提出了**测试时多轮交互策略适应(T2PAM)**新范式，其核心思想是：\n- 将用户反馈视为主动信号，引导实时参数更新\n- 在对话过程中实现自我纠正，使策略逐步演进并与用户偏好对齐\n- 保持计算轻量级，不引起用户感知的延迟或内存开销\n\nT2PAM范式形式化为三个步骤：\n1. **生成**：模型根据对话上下文生成响应\n2. **反馈**：用户提供反馈，映射为标量奖励信号\n3. **适应**：基于反馈在线更新模型参数\n\n## 5. 实现挑战与解决方案：ROSA算法\n\n要实现T2PAM范式，作者面临两个主要挑战：\n\n**挑战1：如何高效利用反馈更新策略？**\n- 传统RLHF方法依赖迭代梯度优化，计算成本高\n- 需要避免昂贵的迭代优化，实现轻量级更新\n\n**解决方案**：作者提出**最优参考一步适应(ROSA)**算法，核心创新是：\n- 利用RLHF目标函数，但直接求解其闭式最优解\n- 将用户反馈转化为最优策略的解析估计\n- 在单一步骤中将模型参数引导向这个最优策略\n\n**挑战2：如何保证方法有效性？**\n- 需要理论保证随着交互增加，模型策略会收敛到用户偏好\n\n**解决方案**：作者提供了严格的理论分析，证明：\n- 每次反馈都能保证减少与用户策略的误差（单调错误减少）\n- 随着交互次数增加，模型策略会累积收敛到用户偏好\n- 统一误差界限平衡了学习速度与稳定性\n\n## 6. ROSA算法设计细节\n\nROSA算法的具体设计遵循三个关键步骤：\n\n**步骤1：基于RLHF目标的轮次适应**\n- 建立最大化期望奖励同时惩罚与前一策略偏离的优化目标\n- 引入KL散度正则化确保稳定受控的更新\n\n**步骤2：从理论最优到实用一步更新**\n- 利用RLHF目标的闭式解直接确定最优策略\n- 针对实践中仅观察到单个响应反馈的情况，构建实用目标\n- 通过指数重加权调整错误响应的概率，指导模型减少生成错误\n\n**步骤3：通过线性化优化进行高效参数更新**\n- 使用一阶泰勒展开近似策略函数\n- 采用共轭梯度算法高效求解参数更新，避免显式构建海森矩阵\n- 实现计算和内存高效的在线适应\n\n## 7. 实验验证与效果评估\n\n作者通过广泛实验验证ROSA的有效性：\n\n**有效性验证**：\n- 在数学推理、一般推理、代码生成和多语言推理四个领域评估\n- 结果显示ROSA在所有基准数据集和模型上一致优于基线方法\n- 特别是在\"纠正提升\"指标上表现突出，证明其强大的自我纠正能力\n\n**效率验证**：\n- 更新过程可异步执行，对用户基本不可感知\n- GPU内存开销仅适度增加，适合现有硬件部署\n- 时间-准确率曲线显示ROSA在更短时间内达到更高准确率\n\n## 8. 逻辑演进总结\n\n作者提出核心方法的完整逻辑链可概括为：\n\n**问题观察** → **方法分析** → **洞察形成** → **范式提出** → **算法设计** → **理论保证** → **实验验证**\n\n这一演进过程体现了从实际问题出发，通过系统分析现有方法局限性，形成创新性洞察，提出新范式，设计具体算法，提供理论保证，最终通过实验验证的完整科学研究路径。ROSA算法的核心创新在于将模型适应从训练阶段转移到推理阶段，实现了高效、轻量级的在线策略适应，为解决LLMs在多轮交互中的性能退化问题提供了新思路。", "summary_translation": "大型语言模型（Large Language Models, LLMs）采用多轮交互（multi-turn interaction）作为完成复杂任务的基本范式。然而，在扩展交互（extended interactions）中，它们的性能通常会下降，因为它们通常在静态、单轮数据（single-turn data）上进行训练，这阻碍了它们适应实时用户反馈的能力。为了解决这一限制，我们首先提出了一种新范式：多轮交互的测试时策略适应（Test-Time Policy Adaptation for Multi-Turn Interactions, T2PAM），它利用正在进行的交互中的用户反馈作为奖励信号，来估计与用户偏好一致的潜在最优策略（latent optimal policy），然后更新一小部分参数以引导模型朝向这一策略，最终实现高效的对话内自我修正（in-conversation self-correction）。接着，我们介绍了最优参考的一步适应（Optimum-Referenced One-Step Adaptation, ROSA），这是一个实现T2PAM的轻量级算法。ROSA在单个、高效的更新步骤中引导模型参数朝向理论最优策略，避免了昂贵的迭代基于梯度的优化（gradient-based optimization），并最小化了计算开销（computational overhead）。我们提供了严格的理论分析（theoretical analysis），保证随着交互次数的增加，ROSA的策略会收敛（converges）到用户的偏好。在具有挑战性的基准测试（benchmark）上的大量实验表明，ROSA在任务有效性和效率方面都取得了显著改进。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#155", "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts", "link": "/arxiv/2509.23188", "arxiv_id": "2509.23188", "authors": "Guancheng Wan, Leixin Sun, Longxu Dou, Zitong Shi, Fang Wu, Eric Hanchen Jiang, Wenke Huang, Guibin Zhang, Hejia Geng, Xiangru Tang, Zhenfei Yin, Yizhou Sun, Wei Wang", "summary": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.680906", "filter_reason": "这篇论文的核心贡献是提出了一种全栈框架（Diagnose, Localize, Align）来解决LLM多智能体系统在指令冲突下的可靠性问题。从本质上看，论文专注于改进LLM多智能体系统的基础能力，特别是协作推理和指令遵从性，这完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。 论文包含了多个正面指标：核心概念上明确关注LLM驱动的多智能体系统；能力方向上涉及协作推理；新兴范式上专注于多智能体系统和工具使用。论文提出的CRAS评分、注意力漂移分析和SAIL对齐方法都是为了增强模型对指令层级结构的理解，从而提高其在复杂任务中的推理能力。 虽然论文在实验中使用了MedQA作为评估基准，但这只是验证方法有效性的手段，而不是论文的主要焦点。论文的核心是提出一种通用的框架，可以应用于各种多智能体系统，而不是针对特定领域的问题。因此，这篇论文符合研究范围，应该被保留。", "summary2": "本文旨在解决LLM多智能体系统在指令冲突下的可靠性问题。针对系统-用户和对等-对等指令冲突场景，我们提出了一种三阶段全栈框架（Diagnose-Localize-Align），包括上下文化角色adherence评分（CRAS）、注意力漂移分析和指令层外科手术对齐（SAIL）。在多个基准测试和MAS框架上，通过准确率和CRAS指标验证了其有效性，显著提高了指令层次遵从性（如AutoGen在MedQA上提升5.60%），无需全模型微调。", "inspiration_trace": "# 从问题观察到解决方案：LLM多智能体系统指令冲突处理框架的逻辑演进\n\n## 1. 宏观问题观察：LLM多智能体系统的可靠性瓶颈\n\n### 现象观察\n作者首先观察到LLM驱动的多智能体系统(MAS)虽然在协作推理、工具使用和角色专业化协调方面取得了快速进展，但在可靠性关键场景的部署中存在一个系统性故障模式：**指令冲突下的层级合规性问题**。具体表现为：\n\n- 每个智能体同时受到高级别系统指令（身份、约束）和低级别用户/对等请求的约束\n- 当系统指令与用户指令或对等指令之间出现冲突时，智能体会错误地优先级处理，偏离角色、违反约束\n- 现有宏观指标（如任务成功率、pass@k）掩盖了这些微观故障，无法提供干预指导\n\n### 核心问题提出\n基于以上观察，作者提出第一个关键问题：**如何量化智能体在交互中是否忠实地遵循其角色和约束？**\n\n## 2. 诊断阶段：从宏观到微观的评估\n\n### 思考路径\n作者认识到现有宏观指标无法捕捉微观层面的角色遵循失败，需要一个能够上下文感知、细粒度评估角色遵循的方法。\n\n### 解决方案：CRAS评分系统\n作者提出**上下文角色遵循评分(CRAS)**，这是一个查询级别的、基于规则的诊断工具，将角色遵循分解为四个互补维度：\n\n1. **目标对齐(GA)**：行动和中间步骤是否持续推进任务目标\n2. **角色一致性(RC)**：语言、推理风格和方法选择是否与角色一致\n3. **知识边界遵循(KBA)**：主张是否保持在预定知识范围内\n4. **约束合规性(CC)**：是否违反明确约束条件\n\n### 实现机制\n- 为每个查询上下文编程实例化评估规则\n- 使用独立评估器将轨迹映射到各维度分数\n- 聚合分数获得CRAS总分\n\n### 预期效果\n- 从宏观成功评估提升到上下文感知的遵循评估\n- 提供稳定、可复现的信号用于有针对性修复\n- 为后续定位和对齐阶段奠定基础\n\n## 3. 定位阶段：深入模型内部寻找问题根源\n\n### 现象观察\n通过CRAS驱动的诊断，作者发现在冲突情况下，角色遵循在系统和用户指令碰撞时确实会下降，尽管一般能力保持完整。这表明存在一个**局部仲裁机制**，而非全局弱点。\n\n### 核心问题提出\n这引发第二个关键问题：**指令仲裁在模型的哪个部分发生？**\n\n### 思考路径\n作者假设如果存在局部仲裁机制，应该可以通过分析模型在冲突和非冲突输入下的行为差异来定位。注意力机制可能是这种仲裁的关键，因为它决定了模型如何权衡不同信息。\n\n### 解决方案：注意力漂移分析\n作者提出通过对比冲突和非冲突输入下的注意力行为来定位冲突敏感区域：\n\n1. 构建程序化冲突数据集，包含仅在指令兼容性上不同的匹配输入对\n2. 沿三个互补轴量化每个头的变化：\n   - 幅度(∆mag)：注意力强度的变化\n   - 方向(∆dir)：注意力模式的方向性重新定位\n   - 分布(∆dist)：注意力分布的重塑\n3. 结合指标获得头级别漂移分数：S(l,h) = λmag∆mag(l,h) + λdir∆dir(l,h) + λdist∆dist(l,h)\n4. 选择前k%的头作为焦点头，收集它们的层形成冲突敏感层集\n\n### 关键发现\n冲突敏感模块表现出明显的行为变化，且**集中在中间层**（如Qwen2.5-7B的19-23层，LLaMA3.1-8B的18-22层）。这与只有部分注意力头在功能上关键的证据相呼应。\n\n## 4. 对齐阶段：针对性的解决方案\n\n### 现象观察\n通过诊断和定位，作者已确定了角色遵循失败的量化方法(CRAS)和指令仲裁发生的模型位置（中间层）。\n\n### 核心问题提出\n这引出第三个关键问题：**是否可以仅通过焦点对齐加强指令层级合规性，而不影响一般能力？**\n\n### 思考路径\n作者推测既然已定位仲裁机制，应该可以只针对这些特定区域优化，而不需调整整个模型。这种\"外科手术式\"方法可最大限度减少对模型一般能力的干扰。\n\n### 解决方案：SAIL外科手术对齐\n作者提出**指令层的外科手术对齐(SAIL)**，具体包括：\n\n1. **偏好数据构建**：使用CRAS为每个冲突上下文构建偏好对\n2. **相对注意力贡献**：定义每个token的焦点头注意力贡献比例作为权重\n3. **SAIL损失**：设计token加权的偏好优化损失，根据焦点注意力贡献加权token级学习\n4. **焦点层适配器**：仅在焦点层的注意力投影上安装LoRA适配器，冻结非焦点参数\n\n### 预期效果\n- 集中学习信号在仲裁发生的确切位置和时间\n- 提高指令层级合规性，同时保持一般能力\n\n## 5. 整体框架：Diagnose-Localize-Align\n\n### 整合思考\n作者将三个阶段整合为全栈框架：\n- **诊断(CRAS)**：提供细粒度评估信号，识别问题\n- **定位(注意力漂移分析)**：揭示问题在模型内部的位置\n- **对齐(SAIL)**：提供针对性解决方案，仅针对问题区域优化\n\n### 框架优势\n- **全栈性**：从评估到定位到修复的完整流程\n- **高效性**：不需要全模型微调，仅更新焦点层\n- **精确性**：通过CRAS的细粒度评估和注意力漂移的精确定位，确保干预针对性\n- **可解释性**：每个阶段都提供可解释信号和洞察\n\n## 6. 实验验证\n\n### 实验设计\n作者在四个数据集（MMLU、SciBench、GPQA、MedQA）和四个多智能体系统（Dylan、MacNet、AutoGen、SelfConsistency）上评估，使用两个指令调整模型作为基础架构：LLaMA3.1-8B-Instruct和Qwen2.5-7B-Instruct。\n\n### 主要结果\n- SAIL在多数情况下提高性能，尤其在复杂推理基准上\n- 例如，在AutoGen框架下，MedQA准确率提高5.60%\n- 消融研究验证了token级奖励机制和冲突驱动的层选择的有效性\n\n### 鲁棒性和敏感性分析\n- 方法在不同训练阶段表现出稳定性能提升\n- 对学习率和LoRA秩的敏感性分析表明，适当超参数选择对方法有效性至关重要\n\n## 逻辑链总结\n\n作者从观察到解决方案的逻辑演进可概括为：\n\n1. **问题识别**：观察到LLM多智能体系统在指令冲突下的可靠性问题，现有宏观指标无法捕捉微观角色遵循失败\n\n2. **诊断需求**：需要量化智能体在交互中是否忠实遵循角色和约束的方法\n\n3. **诊断解决方案**：提出CRAS，多维度、上下文感知的角色遵循评分系统\n\n4. **深入分析**：通过CRAS发现角色遵循在指令冲突时下降，表明存在局部仲裁机制\n\n5. **定位需求**：确定模型中指令仲裁发生的精确位置\n\n6. **定位解决方案**：通过注意力漂移分析，发现冲突敏感模块集中在中间层\n\n7. **对齐需求**：是否可通过焦点对齐加强指令层级合规性，而不影响一般能力？\n\n8. **对齐解决方案**：提出SAIL，外科手术式对齐方法，仅在焦点层安装LoRA适配器\n\n9. **整体框架**：整合诊断、定位和对齐三个阶段，形成全栈框架\n\n10. **实验验证**：在多个基准和MAS框架上验证方法有效性，证明其能提高指令层级合规性而不影响一般能力\n\n这一逻辑链展示了作者从观察到假设，再到解决方案的系统性思考过程，体现了科学研究的严谨性和创新性，为LLM多智能体系统在指令冲突下的可靠性问题提供了一个全面而精确的解决框架。", "summary_translation": "由大型语言模型(Large Language Model, LLM)驱动的多智能体系统(multi-agent systems, MAS)已迅速推进了复杂任务中的协作推理、工具使用和角色专业化协调。然而，对可靠性要求高的部署仍然受到一种系统性故障模式的阻碍：在指令冲突(instruction conflicts，系统-用户、对等-对等)下的层级合规性(hierarchical compliance)，即智能体在存在竞争性需求时错误地优先处理系统级规则。此外，广泛使用的宏观层面指标(macro-level metrics，如pass@k)掩盖了这些微观层面的违规行为，并为修复提供了很少可行的指导。在这项工作中，我们提出了一个全栈的三阶段框架：(1) 诊断(Diagnose) - 上下文化角色遵守度得分(Contextualized Role Adherence Score, CRAS)，一种针对每个查询的、上下文感知的评分指标，将角色遵守度分解为四个可测量的维度；(2) 定位(Localize) - 注意力漂移(attention drift)分析，揭示指令冲突是通过主要集中在中层的注意力头(attention heads)解决的；(3) 对齐(Align) - 指令层的外科手术式对齐(Surgical Alignment of Instruction Layers, SAIL)，该方法仅在定位的焦点层上安装LoRA，并优化一个基于token加权的DPO风格偏好目标(DPO-style preference objective)，该目标根据token的焦点注意力贡献给予奖励。在标准基准测试和MAS框架中，我们的外科手术式方法提高了指令层级合规性(instruction hierarchy compliance)(例如，在MedQA上使用AutoGen提高了5.60%)，而无需进行全模型微调(full-model finetuning)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#159", "title": "Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning", "link": "/arxiv/2509.23140", "arxiv_id": "2509.23140", "authors": "Song Jin, Juntian Zhang, Yong Liu, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan", "summary": "Recent advancements have endowed Large Language Models (LLMs) with impressive general reasoning capabilities, yet they often struggle with personalization reasoning - the crucial ability to analyze user history, infer unique preferences, and generate tailored responses. To address this limitation, we introduce TagPR, a novel training framework that significantly enhances an LLM's intrinsic capacity for personalization reasoning through a tagging the thought approach. Our method first develops a data-driven pipeline to automatically generate and semantically label reasoning chains, creating a structured dataset that fosters interpretable reasoning. We then propose a synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on this tagged data to establish foundational reasoning patterns, followed by a multi-stage reinforcement learning (RL) process. This RL phase is guided by a unique composite reward signal, which integrates tag-based constraints and a novel Personalization Reward Model with User Embeddings (PRMU) to achieve fine-grained alignment with user-specific logic. Extensive experiments on the public LaMP benchmark and a self-constructed dataset demonstrate that our approach achieves state-of-the-art results, delivering an average improvement of 32.65% over the base model across all tasks. Our work validates that structured, interpretable reasoning is a highly effective pathway to unlocking genuine personalization capabilities in LLMs.", "subjects": "Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.683076", "filter_reason": "这篇论文的核心贡献是提出了一种名为TagPR的新训练框架，用于增强大语言模型(LLM)的个性化推理能力。从筛选标准来看，该论文符合我的研究目标，理由如下： 首先，在核心判断层面，论文的本质是改进LLM的基础推理能力，而非将其作为工具应用到特定领域。论文提出了\"标记思维\"的新方法，结合监督微调和多阶段强化学习过程来增强模型的推理能力，这符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理等通用能力\"的保留标准。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)这一核心概念；聚焦于推理能力(reasoning)，特别是个性化推理；采用了强化学习(RL)作为关键训练方法；通过创建结构化数据集促进可解释推理，这与提升模型内在能力的方向一致。 在排除标准方面，论文没有涉及多模态与视觉、特定应用领域(如医疗、化学等)或模型可靠性的应用层面问题。虽然论文关注\"个性化推理\"，但这应被视为通用推理能力的一个特定方面，而非针对特定领域的应用研究。 在特殊和模糊情况处理上，论文提出的\"标记思维\"方法增强了模型的可解释性，这属于\"增强模型内在的可解释性\"的情况，应当保留。这种方法通过提升推理的可解释性来提高模型的推理质量，而非仅进行社会学研究或应用层面的讨论。 综上所述，该论文致力于通过新方法提升LLM的通用推理能力，符合我的研究范围。", "summary2": "本文旨在解决大型语言模型在个性化推理方面的不足。针对用户历史分析和偏好推断任务，我们提出了一种TagPR框架，通过\"标记思维\"方法和多阶段强化学习训练策略，并在LaMP基准和自构建数据集上通过准确率、F1分数、ROUGE等指标验证了其有效性，平均性能提升32.65%。", "inspiration_trace": "# 从问题到解决方案：TagPR方法的逻辑演进\n\n## 1. 宏观问题：LLMs的个性化推理困境\n\n**观察现象**：大型语言模型(LLMs)在数学、编程等通用推理任务上表现出色，但在个性化推理方面存在明显不足。即使最强大的推理型LLMs也难以有效分析用户历史、推断独特偏好并生成定制化响应。\n\n**问题本质**：通用推理模型倾向于优先考虑内部通用逻辑，而非用户特定上下文，导致响应过于通用或与用户偏好相矛盾。现有方法如R2P和RPM主要作为外部支架，未从根本上增强模型内在的个性化推理能力。\n\n## 2. 核心洞察：个性化推理的结构化本质\n\n**关键观察**：个性化推理不是单一直觉行为，而是结构化的多步骤过程——分析用户历史、识别重复模式、将模式应用到新情境。\n\n**假设形成**：标准LLMs的不透明、自由形式推理不适合这种程序性任务。如果能够将推理过程外部化为离散、可解释的步骤，每个步骤标记语义标签，就能释放模型的个性化潜力。\n\n## 3. 方法论构思：\"标记思想\"框架\n\n**核心思想**：通过\"标记思想\"(Tagging the Thought)将复杂个性化任务转化为明确、可管理的过程，使用语义标签作为认知路标。\n\n**框架设计**：TagPR框架包含两个关键组成部分：\n1. 数据驱动的管道：自动生成和语义标记推理链，创建结构化数据集\n2. 协同训练策略：监督微调(SFT)建立基础推理模式，多阶段强化学习(RL)进行精细调整\n\n## 4. 具体实现：从数据构建到训练策略\n\n**数据构建管道**：\n- 原始推理链生成：使用强大推理模型生成多样化候选\n- 两阶段过滤：准确性过滤和LLM质量评估确保数据完整性\n- 两阶段标记：探索性标记生成广泛标签，限制性标记确保一致性\n\n**训练策略**：\n- 基础SFT：在标记数据集上微调，建立结构化推理基础\n- 引导RL阶段：使用复合奖励信号（标签约束+个性化奖励模型PRMU）\n- 探索RL阶段：简化奖励信号，鼓励模型自由探索策略空间\n\n**个性化奖励模型(PRMU)**：引入可学习用户嵌入捕获个人偏好，提供细粒度奖励信号，实现与用户特定逻辑的精细对齐。\n\n## 5. 实验验证与效果评估\n\n**全面实验**：在LaMP基准和自构建数据集上评估，与多种基线方法比较。\n\n**关键结果**：\n- TagPR在所有任务上取得最先进结果，平均提升32.65%\n- 8B参数模型超越更大的专有模型（如GPT-4o、Gemini-2.5-Pro）\n- 消融研究验证各组件的必要性和协同效应\n\n**泛化能力**：在跨语言新基准上展示卓越零样本泛化性能，证明学习到可转移的个性化技能。\n\n## 6. 结论与启示\n\n**核心结论**：训练LLMs生成结构化、可解释的推理是释放真正个性化能力的有效途径。TagPR通过\"标记思想\"方法显著增强了模型内在个性化推理能力。\n\n**研究意义**：为构建更复杂、更符合用户需求的智能系统铺平道路，验证了结构化推理在个性化任务中的价值，为未来研究提供新方向。", "summary_translation": "最近的技术进步使大型语言模型（Large Language Models, LLMs）具备了令人印象深刻的通用推理能力，然而它们在个性化推理（personalization reasoning）方面常常表现不佳——个性化推理是分析用户历史、推断独特偏好并生成定制化回应的关键能力。为解决这一局限性，我们提出了TagPR，一种新颖的训练框架，通过标记思维（tagging the thought）方法显著增强了大型语言模型的个性化推理内在能力。我们的方法首先开发了一个数据驱动的流程，自动生成并语义标记推理链（reasoning chains），创建一个促进可解释推理的结构化数据集。随后，我们提出了一种协同训练策略，首先在标记数据上进行监督微调（Supervised Fine-Tuning, SFT）以建立基础推理模式，然后进行多阶段强化学习（reinforcement learning, RL）过程。该强化学习阶段由独特的复合奖励信号（composite reward signal）引导，该信号整合了基于标签的约束和一种新颖的用户嵌入个性化奖励模型（Personalization Reward Model with User Embeddings, PRMU），以实现与用户特定逻辑的细粒度对齐。在公开的LaMP基准测试和自构建数据集上的大量实验表明，我们的方法取得了最先进（state-of-the-art）的成果，在所有任务上相比基础模型平均提升了32.65%。我们的工作验证了结构化、可解释的推理是释放大型语言模型真正个性化能力的一条高效途径。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#167", "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents", "link": "/arxiv/2509.23040", "arxiv_id": "2509.23040", "authors": "Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang", "summary": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.692643", "filter_reason": "这篇论文的核心贡献是提出ReMemR1（一种带有回调增强记忆的记忆增强LLM智能体）和多级奖励强化学习(RLMLR)训练方法，旨在解决LLM在长上下文推理中的挑战。从第一步核心判断来看，论文本质上是改进LLM的基础推理能力，特别是长上下文推理能力，而不是将LLM作为工具应用到特定领域。论文提出的记忆机制允许非线性推理和重新审视早期证据，这直接增强了LLM的通用推理能力。从第二步正面指标看，论文包含多个相关主题：大型语言模型(LLMs)、推理能力(reasoning)、强化学习(RL)和LLM智能体(llm-based agents)。从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。在第四步特殊情况处理中，论文提出的是一种通用的智能体框架来增强LLM的推理能力，而非应用于特定领域。综合分析，这篇论文明确符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型在长上下文问答中面临的证据分散和信息丢失问题。针对数百万token的长文档场景，我们提出了一种ReMemR1方法，通过回调增强内存机制实现历史记忆检索和非线性推理，并在HotpotQA和2WikiMultiHopQA数据集上通过准确性指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到创新方法：ReMemR1的逻辑演进\n\n## 宏观问题：长上下文推理的瓶颈\n\n大型语言模型(LLMs)在处理长文档推理任务时面临根本性挑战。在现实场景中（如法律文档分析或科学文献综述），回答单个查询的关键证据可能分散在数百万个token中，这使得模型难以有效跟踪长距离依赖关系并将分散信息综合成连贯答案。\n\n## 问题分解：现有方法的局限性\n\n作者首先审视了主流解决方案——\"边读边记\"(memorize while reading)范式，并识别出三个关键局限：\n\n### 1. 不可逆的仅向前处理\n**观察**：在多跳推理任务中，不同跳的证据可能需要同时使用。例如，在寻找第一跳证据时可能遇到第二跳证据，但由于问题尚未解决，代理无法识别其重要性。\n**问题**：随着记忆更新，这种未被识别的关键证据可能被永久遗忘，限制了代理整合分散信息的能力。\n\n### 2. 记忆覆盖导致的信息渐进丢失\n**观察**：固定长度记忆缓冲区需要不断压缩信息，早期关键细节（如\"Dr Aris Thorne曾在芝加哥做博士后\"）在多次更新后不可避免地丢失。\n**问题**：这种记忆退化使模型难以维持完整上下文，阻碍了合成分散在文档远端证据的能力。\n\n### 3. 稀疏且延迟的监督信号\n**观察**：现有方法通常仅使用最终答案正确性作为单一奖励信号。\n**问题**：这种稀疏奖励对长序列中间记忆更新提供有限指导，导致优化效率低下和次优的记忆管理策略。\n\n## 假设形成：突破性思考\n\n基于上述观察，作者提出三个核心假设：\n\n1. **非线性推理假设**：如果允许代理在需要时回溯和访问历史记忆，可以打破线性处理限制，实现更灵活的推理路径。\n\n2. **智能记忆管理假设**：通过设计更智能的记忆检索机制，可以减少关键信息的丢失，维持长期推理所需的关键事实。\n\n3. **密集监督假设**：如果在训练过程中提供更细粒度的中间步骤监督，可以更有效地优化记忆管理策略，加速学习过程。\n\n## 方法论构建：ReMemR1的诞生\n\n### 1. 突破状态表示限制\n**核心创新**：将传统MDP中的状态st = mt扩展为st = (mt, qt)，其中qt是回调查询。\n**实现机制**：代理不仅更新记忆mt，还生成回调查询qt+1来检索历史记忆{mi}i≤t，使状态转换变为：\n```\nst+1 = (mt+1, qt+1) = πθ(Q, ct, mt, E({mi}i≤t, qt))\n```\n**解决的问题**：使代理能够构建非线性推理路径，选择性重新整合早期证据，打破不可逆向前处理的限制。\n\n### 2. 设计检索增强的记忆机制\n**核心创新**：引入检索函数E，从历史记忆中选择与回调查询最相关的内容。\n**实现机制**：使用基于词重叠的启发式检索方法，E(X, b) = argmaxx∈X recall(b, x)，其中recall(a, b)表示a中出现在b中的词的比例。\n**解决的问题**：使代理能够在需要时检索和重新整合可能被覆盖的关键信息，减轻信息渐进丢失问题。\n\n### 3. 多级奖励强化学习框架\n**核心创新**：提出强化学习与多级奖励(RLMLR)框架，结合轨迹级结果奖励和密集的步骤级状态奖励。\n**实现机制**：\n- **轨迹级结果奖励**：基于最终答案正确性，Rout(g) = maxy∈Y I(ŷ(g) = y)\n- **步骤级状态奖励**：包括三部分\n  - 记忆更新信息增益：rmemory,t(g) = maxy∈Y recall(mt(g), y) - maxy∈Y recall(mt-1(g), y)\n  - 回调检索奖励：rcallback,t(g) = maxy∈Y recall(y, E({mi(g)}i≤t, qt(g)) ∪ mt(g) ∪ ct) - maxy∈Y recall(y, mt(g) ∪ ct)\n  - 格式奖励：确保正确使用<callback>和<memory>标签\n- **综合优势计算**：Ât(g) = αÂout(g) + (1-α)Âstate,t(g)\n\n**解决的问题**：提供密集的细粒度监督，缓解传统仅结果RL的稀疏性问题，有效塑造中间行为。\n\n## 实验验证与确认\n\n通过系统性实验，作者验证了ReMemR1的有效性：\n\n1. **性能验证**：在HotpotQA和2WikiMultiHopQA数据集上，ReMemR1在所有模型规模和上下文长度上均优于现有方法，特别是在超长上下文(6400文档)情况下仍保持稳定性能。\n\n2. **消融研究**：\n   - RLMLR的有效性：α=0.8时性能最佳，证明结合结果奖励和步骤级奖励的必要性\n   - RL驱动vs规则回调：RL驱动的回调明显优于使用问题本身作为固定查询的规则方法\n\n3. **远距离证据挑战**：在特意设计的远距离证据设置下，ReMemR1显著优于基线，证实了其非线性文档利用能力。\n\n## 结论：从问题到解决方案的完整逻辑链\n\n作者从长上下文推理的宏观挑战出发，通过系统分析现有方法的局限性，形成了三个核心假设，并逐步构建出ReMemR1这一创新解决方案。这一方法通过引入历史增强的状态表示、智能记忆检索机制和多级奖励强化学习框架，有效解决了不可逆向前处理、信息渐进丢失和稀疏监督信号三大关键挑战，为长上下文LLM代理提供了新的发展方向。整个逻辑演进体现了从问题观察到假设形成，再到方法论构建和实验验证的完整科学研究过程。", "summary_translation": "大型语言模型（Large language models）在长上下文问答（long-context question answering）中面临挑战，因为查询的关键证据可能分散在数百万个标记（tokens）中。现有研究为大型语言模型配备了记忆语料库（memory corpus），该语料库在单次文档扫描过程中动态更新，这种方法也被称为\"边读边记\"（\"memorize while reading\"）方法。虽然这种方法能够高效扩展，但它存在不可逆的单向处理、通过覆盖导致的信息丢失以及稀疏的强化学习（reinforcement learning）信号等问题。\n\n为应对这些挑战，我们提出了ReMemR1，一种具有回调增强记忆（callback-enhanced memory）的记忆增强代理（memory-augmented agent），它允许从整个记忆历史中进行选择性检索，并支持非线性推理和重新审视早期证据。为进一步加强训练，我们提出了多级奖励强化学习（Reinforcement Learning with Multi-Level Rewards, RLMLR），该方法将最终答案奖励与密集的步骤级信号相结合，以指导有效使用记忆。总体而言，这些贡献减轻了信息退化，改进了监督，并支持多跳记忆利用（multi-hop memory utilizing）。在长文档问答（long-document QA）上的实验表明，与现有的基于记忆的方法相比，我们的方法取得了显著改进，这验证了ReMemR1作为长上下文推理代理（long-context reasoning agents）的有效解决方案。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#164", "title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents", "link": "/arxiv/2509.23071", "arxiv_id": "2509.23071", "authors": "Muzhi Li, Jinhu Qi, Yihong Wu, Minghao Zhao, Liheng Ma, Yifan Li, Xinyu Wang, Yingxue Zhang, Ho-fung Leung, Irwin King", "summary": "Retrieval-augmented generation agents development is hindered by the lack of process-level supervision to effectively guide agentic capabilities like task decomposition, retriever invocation, and stepwise decision-making. While reinforcement learning offers a potential solution, it suffers from sparse rewards and the limited reasoning capabilities of large language models (LLMs). Meanwhile, existing data synthesis methods only produce chain-of-thought rationales and fail to model environmental interactions. In this paper, we propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent development. EviPath comprises: (i) Abductive Subtask Planning, which decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; (ii) Faithful Sub-question Answering, which uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question; and (iii) Conversational Fine-Tuning, which formats the complete agent-environment interaction trajectory into a dialogue format suitable for Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data. Extensive experiments on widely-used question-answering benchmarks show that an 8B parameter model trained with EviPath-synthesized data significantly and consistently outperforms state-of-the-art baselines with a double-digit absolute EM gain of 14.7% in open-domain question answering.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.690954", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。 首先，从核心判断来看，该论文的本质是提出一种新的训练范式(EviPath)来增强大语言模型的推理能力和工具使用能力。论文关注的是如何通过合成数据和特定的训练方法来提升LLM的通用推理能力，包括任务分解、检索器调用和逐步决策等智能体能力。这明显是关于改进LLM基础能力和通用推理能力的研究，而不是将LLM作为工具应用到特定领域。 其次，论文包含多个正面指标主题： - 核心概念：明确讨论large language models (LLMs) - 能力方向：聚焦于溯因推理(Abductive Reasoning)、推理路径合成、任务分解和逐步决策等推理和规划能力 - 新兴范式：研究检索增强生成智能体(RAG agents)和工具使用能力(tool-use capabilities) 第三，论文不涉及任何排除标准中的领域： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等），而是提出通用方法 - 不讨论模型可靠性方面的水印、安全等问题 最后，关于智能体/工具使用的特殊处理，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而不是将智能体应用在特定领域，因此符合保留标准。 综上所述，该论文的核心贡献是提出一种新的证据锚定推理路径合成范式，用于提升大语言模型的通用推理能力和工具使用能力，完全符合研究目标。", "summary2": "本文旨在解决RAG智能体开发中缺乏过程级监督的问题。针对多跳问答任务，我们提出了一种基于溯因推理的EviPath框架，通过证据锚定的推理路径合成来训练智能体。在HotpotQA、MuSiQue和2WikiMultihopQA等基准测试上，通过EM和F1指标验证了其有效性，8B参数模型实现了14.7%的绝对EM增益，显著优于现有基线方法。", "inspiration_trace": "# 从问题到解决方案：EviPath方法的逻辑演进\n\n## 1. 宏观问题识别：RAG智能体的过程监督缺失\n\n**观察现象**：检索增强生成(RAG)智能体在解决复杂多跳问题时表现不佳，尽管它们具备自主收集外部知识的能力。\n\n**核心问题**：现有数据集只提供最终答案和支持事实，缺乏**过程级监督信号**，无法有效训练智能体的关键能力：\n- 任务分解\n- 检索器调用\n- 逐步决策制定\n\n这一根本性限制导致RAG智能体在实际应用中无法提供可靠性能。\n\n## 2. 现有方法分析：两种主流范式的局限性\n\n### 2.1 强化学习方法的问题\n\n**假设**：通过结果奖励优化决策过程可以训练RAG智能体。\n\n**发现局限**：\n- 奖励信号稀疏且延迟，难以将功劳分配给单个决策\n- RAG智能体包含检索器等不可微分组件，端到端梯度反向传播不可行\n- 严重依赖模型内在推理能力，缺乏先验知识时无法发现正确动作\n\n### 2.2 数据合成方法的问题\n\n**假设**：合成思维链(CoT)理由可以提供足够的训练信号。\n\n**发现局限**：\n- 生成的路径通常是事后解释，而非真实的问题解决过程\n- 无法模拟与外部环境的交互，缺乏智能体核心能力训练\n\n## 3. 新视角提出：溯因推理的引入\n\n**关键洞察**：将推理路径合成重新定义为**溯因推理问题**——从观察到的结果(答案)反向推导出最可能的推理过程。\n\n**理论依据**：\n- 溯因推理适合从结果推断原因\n- 可以利用最终答案和支持证据作为约束，反向工程出最优推理轨迹\n- 这种方法能生成结构化的、交互式的、面向目标的推理路径\n\n**创新点**：首次将RAG智能体的推理路径合成表述为溯因推理问题，为生成过程监督信号提供了新思路。\n\n## 4. 方法设计：EviPath框架的构建\n\n基于溯因推理视角，设计了与RAG智能体Planner-Executor架构一致的三阶段框架：\n\n### 4.1 溯因子任务规划(ASP)\n\n**核心思想**：从答案反向推导任务分解和执行路径。\n\n**实现机制**：\n- 分析最终答案和证据间依赖关系，反向工程推理图\n- 将推理图线性化为子问题序列，创建\"黄金\"计划\n- 模拟智能体迭代执行过程，生成思想和检索查询\n\n### 4.2 忠实子问题回答(FSA)\n\n**核心思想**：构建模拟环境确保推理忠实性。\n\n**实现机制**：\n- 利用完整支持事实作为稳定知识库，避免实际检索错误\n- 三步过程：推导中间答案→识别黄金证据→生成基于证据的推理思想\n- 在包含噪声的上下文中训练，增强模型的抗干扰能力\n\n### 4.3 对话微调(CFT)\n\n**核心思想**：将完整轨迹转换为适合监督学习的格式。\n\n**实现机制**：\n- 将推理路径格式化为用户-助手对话形式\n- 同时训练规划者(高级规划)和执行者(子问题回答)能力\n- 通过单一优化目标联合学习所有组件\n\n## 5. 验证与意义：从理论到实践\n\n**实验验证**：\n- 在三个多跳问答数据集上，8B参数模型显著超越所有基线\n- 实现14.7%的绝对EM增益，证明方法有效性\n- 消融研究确认各组件的必要性\n\n**核心贡献**：\n- 建立了数据驱动的RAG智能体开发新范式\n- 解决了数据稀缺和依赖复杂强化学习的双重挑战\n- 证明高质量过程监督信号比模型规模和学习算法更关键\n\n## 6. 逻辑演进总结\n\n从识别RAG智能体缺乏过程监督的根本问题出发，通过分析现有方法的局限性，引入溯因推理的新视角，最终设计出EviPath框架，实现了从证据到轨迹的完整推理路径合成。这一演进过程体现了从问题观察到理论创新，再到方法设计和实验验证的完整科研逻辑，为RAG智能体开发提供了新的有效路径。", "summary_translation": "检索增强生成（Retrieval-augmented generation）智能体开发受到缺乏过程级监督（process-level supervision）的阻碍，这种监督能够有效指导任务分解（task decomposition）、检索器调用（retriever invocation）和逐步决策（stepwise decision-making）等智能体能力。虽然强化学习（reinforcement learning）提供了一个潜在的解决方案，但它面临稀疏奖励（sparse rewards）和大型语言模型（large language models, LLMs）推理能力有限的问题。与此同时，现有的数据合成（data synthesis）方法仅产生思维链（chain-of-thought）推理过程，而无法对环境交互（environmental interactions）进行建模。\n\n在本文中，我们提出了EviPath，一种用于RAG智能体开发的证据锚定推理路径合成（evidence-anchored reasoning path synthesis）范式。EviPath包括：（i）溯因式子任务规划（Abductive Subtask Planning），它将问题分解为子问题，并根据它们之间的依赖关系迭代规划最优解决方案路径；（ii）忠实子问题回答（Faithful Sub-question Answering），它使用支持证据构建代理环境（proxy environment）以生成每个子问题的推理思路和答案；以及（iii）对话式微调（Conversational Fine-Tuning），它将完整的智能体-环境交互轨迹（agent-environment interaction trajectory）格式化为适合监督微调（Supervised Fine-Tuning）的对话格式。EviPath使大型语言模型（LLMs）能够直接从合成数据中学习复杂推理和工具使用（tool-use）能力。\n\n在广泛使用的问答基准（question-answering benchmarks）上进行的大量实验表明，使用EviPath合成数据训练的8B参数模型在开放域问答（open-domain question answering）任务上显著且一致地优于最先进的基线方法（state-of-the-art baselines），实现了14.7%的双位数绝对EM（Exact Match，精确匹配）增益。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#165", "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks", "link": "/arxiv/2509.23067", "arxiv_id": "2509.23067", "authors": "Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo", "summary": "The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.691533", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步核心判断：这篇论文的本质是提出一种名为\"语义投票\"(Semantic Voting)的新方法，用于大语言模型在不可验证的开放性任务上的自我改进。这明显属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用于特定领域。论文关注的是LLM的通用能力提升，特别是通过自我改进机制来增强模型性能，符合保留标准。 第二步正面指标：论文明确包含核心概念\"Large language models, LLMs\"；虽然未直接强调推理能力，但开放性任务通常需要推理能力，且论文提出的语义投票方法可以提升LLM在这些任务上的表现；论文关注自我改进(self-improvement)，这与自我进化(self-evolve)概念相关，属于训练方法的范畴。 第三步排除标准：论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）等排除领域，完全避开了这些排除标准。 第四步特殊和模糊情况：论文提到自我评估机制的\"过度自信问题\"和\"内在偏差\"，这与幻觉问题相关。论文提出的方法旨在减轻这些问题，从而提升模型的通用可靠性和推理质量，符合保留标准。 最终决策：论文的核心贡献是提出了一种通用的自我改进方法，通过语义投票机制提升LLM在开放性任务上的表现，减轻了传统自我评估方法的计算负担和内在偏差。这直接关系到提升LLM的通用推理能力，特别是在处理需要复杂推理的开放性任务时。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决大型语言模型在不可验证开放任务上自我改进的计算开销大和过度自信问题。针对翻译和摘要等开放性任务，我们提出了一种语义投票方法，通过轻量级句子嵌入模型实现软匹配，替代传统的自我评估机制。在WMT24++翻译数据集和CNN/Dailymail、PubMed摘要数据集上，通过BLEU、n-MQM、Rouge-L和BLEURT等指标验证了其有效性，实现了计算效率显著提升(仅为自我评估方法的0.1%-5%)的同时保持或提高了模型性能。", "inspiration_trace": "# 从问题到解决方案：语义投票的逻辑演进\n\n## 宏观问题：监督数据的高昂成本\n\n**起点：** 获取高质量监督数据的成本不断攀升，专家注释的推理轨迹、细粒度人类反馈等方法需要大量人力投入，且泛化性有限。\n\n**观察：** 这促使研究人员探索LLM自我改进的可能性，通过无监督或自监督方式生成伪标签，减少对外部昂贵监督的依赖。\n\n## 现有方法的局限与启示\n\n### 多数投票的有效性与局限性\n\n**观察：** 多数投票在可验证任务(如算术问题、选择题)上表现优异，通过选择最频繁出现的答案作为伪标签。\n  \n**局限：** 多数投票依赖精确匹配(hard matching)，无法适用于翻译、摘要等不可验证的开放性任务，这些任务存在多个语义等效但表达不同的答案。\n\n### 自我评估方法的缺陷\n\n**观察：** 为解决不可验证任务的挑战，研究者开发了自我评估方法，主要包括：\n- Self-judging：利用\"LLM-as-a-Judge\"范式，模型评估自身生成的回答\n- Entropy Minimization：通过Shannon熵等指标估计输出置信度，优先选择低熵(高置信度)输出\n\n**局限：** \n- 计算开销大：需要额外推理步骤\n- 引入偏差：自我偏好偏差导致过度自信问题\n- 不一致性：在不同模型和任务上表现不稳定\n\n## 核心洞察与假设\n\n**关键观察：** 尽管自我评估方法试图构建精细的伪反馈，但研究表明LLM即使从噪声标签中也能有效学习，这引发了对精细伪反馈必要性的质疑。\n\n**核心假设：** \n1. 语义共识可替代精确匹配作为质量指标\n2. 在多个生成的回答中，与其他回答语义更相似的回答质量更高\n3. 轻量级句子嵌入模型足以捕捉语义相似性，无需昂贵的自我评估\n\n## 方法创新：语义投票\n\n### 从硬匹配到软匹配\n\n**创新点：** 放宽多数投票的精确匹配要求，转向基于语义相似性的软匹配(soft matching)。\n\n**实现：** \n```\nS_sv(a_j^i|A_i) = 1/|A_i| * Σ f_sim(a_j^i, a_k^i)\n```\n其中f_sim使用句子嵌入的余弦相似度计算语义相似性。\n\n### 解决噪声与异常值问题\n\n**观察：** 随机生成的候选回答可能包含语义偏离的异常值，影响投票结果。\n\n**解决方案：** 在语义投票前引入聚类过滤步骤，仅保留最大聚类内的回答：\n```\nC_max^i = arg max_k |C_k^i|\nS_sv(a_j^i|C_max^i) = 1/|C_max^i| * Σ f_sim(a_j^i, a_k^i)\n```\n\n### 构建训练信号\n\n**实现：** 选择语义投票得分最高和最低的回答构建偏好对，使用DPO(直接偏好优化)训练模型。\n\n**选择理由：** DPO对噪声标签具有鲁棒性，适合处理不可避免的伪信号噪声。\n\n## 实验验证与优化\n\n### 多维度验证\n\n**任务选择：** 翻译(WMT24++)和摘要(CNN/Dailymail, PubMed)\n**评估指标：** 结合词汇指标(BLEU, ROUGE-L)和语义指标(n-MQM, BLEURT)\n**基线对比：** 与Self-judging和Entropy Minimization方法全面比较\n\n### 关键实验设计\n\n1. **反转实验：** 验证语义投票信号的有效性，排除\"虚假奖励\"可能性\n2. **扩展性实验：** 测试与其他训练范式(GRPO)的兼容性\n3. **消融实验：** 验证聚类和语义投票各组件的必要性\n4. **超参数研究：** 探索生成参数和聚类参数的影响\n\n## 结论与贡献\n\n**核心贡献：** 提出一种高效、无需自我评估的LLM自我改进方法，在不可验证的开放性任务上实现与自我评估方法相当或更好的性能，同时将计算开销降低至原来的0.1%-5%。\n\n**方法论意义：** 证明语义共识是开放性任务中有效的质量指标，为LLM自我改进提供了简单、有效且可扩展的新范式。", "summary_translation": "获取监督数据（supervised data）的成本不断上升，推动了对大型语言模型（large language models, LLMs）自我改进（self-improvement）的浓厚兴趣。像多数投票（majority voting）这样简单的无监督信号（unsupervised signals）已被证明在生成可验证任务（verifiable tasks）的伪标签（pseudo-labels）方面有效，而它们对于不可验证任务（unverifiable tasks）（如翻译）的适用性则受到响应开放性特征的限制。因此，自我评估机制（self-evaluation mechanisms）（如自我判断（self-judging）和熵最小化（entropy minimization））主要用于推导伪标签。然而，依赖大型语言模型（LLMs）的自我评估通常会导致高计算开销，并由于内在偏差（intrinsic biases）而引入过度自信（overconfidence）问题。为应对这些挑战，我们提出了一种针对不可验证任务的新型无自我评估（self-evaluation-free）方法，专为轻量级但有效的自我改进而设计。受可验证任务中常用的多数投票启发，我们提出了语义投票（semantic voting）作为一种新机制，该机制将硬匹配（hard matching）（即精确匹配（exact matching））的原则放宽为软匹配（soft matching）（即语义相似性（semantic similarity））。软匹配通过利用轻量级句子嵌入模型（sentence embedding model）来量化语义相似性，从而减轻了自我评估的过度计算负担和与内在偏差相关的限制。综合实验表明，我们的方法在计算效率方面取得了显著提升，并在多种模型架构和任务上整体表现优于自我评估方法。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#177", "title": "HEART: Emotionally-driven test-time scaling of Language Models", "link": "/arxiv/2509.22876", "arxiv_id": "2509.22876", "authors": "Gabriela Pinto, Palash Goyal, Yiwen Song, Souradip Chakraborty, Zifeng Wang, Tomas Pfister, Hamid Palangi", "summary": "Test-time scaling has shown considerable success in improving the performance of language models on complex reasoning tasks without requiring fine-tuning. However, current strategies such as self-reflection primarily focus on logical or structural refinement. They do not leverage the guiding potential of affective feedback. Inspired by psychological research showing that emotions can modulate cognitive performance, we introduce HEART--a novel framework that uses emotionally-driven prompts for iterative self-correction. HEART provides feedback on a model's incorrect response using a curated set of concise, emotionally charged phrases based on the six universal emotions categorized by Dr. Paul Ekman. By systematically varying the emotional tone of the feedback across iterations, our method guides the model to escape flawed reasoning paths and explore more promising alternatives. We evaluate our framework on challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam, and SimpleQA. Our results reveal a significant new phenomenon: when guided by an oracle verifier, this affective iteration protocol unlocks significantly deeper reasoning, leading to consistent and substantial increases in accuracy over state-of-the-art baselines with the same verifier. However, we also identify a critical bottleneck for practical deployment. In a verifier-free setting, it struggles to harness these gains consistently, highlighting as a key challenge for future work. Our findings suggest that the next frontier in machine reasoning may lie not just in refining logic, but also in understanding and leveraging the `HEART' of the models.", "subjects": "Computation and Language, Machine Learning", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.703105", "filter_reason": "这篇论文的核心贡献是提出了一种名为HEART的新框架，通过情感驱动的提示进行迭代自我修正，以提高大语言模型在复杂推理任务上的表现。该方法基于心理学研究，使用情感化的反馈来引导模型摆脱有缺陷的推理路径，探索更有希望的替代方案。论文在多个具有挑战性的推理基准上评估了该方法，结果表明当由验证器引导时，这种情感迭代协议能解锁更深层次的推理，显著提高准确性。这完全符合研究目标中\"提高大语言模型本身的通用推理能力\"的要求，因为论文关注的是改进LLM的基础推理能力，提出了一种新的测试时扩展范式，而不是将LLM作为工具应用到特定领域。论文的主题与正面指标中的\"大语言模型\"和\"推理能力\"高度吻合，同时不涉及任何排除标准中的领域。", "summary2": "本文旨在解决语言模型测试时扩展中缺乏情感反馈的问题。针对复杂推理任务，我们提出了HEART框架，利用情感提示进行迭代自我修正，并在多个推理基准上通过准确性验证了其有效性。", "inspiration_trace": "# HEART方法逻辑链推演：从问题洞察到方法论创新\n\n## 1. 宏观问题：语言模型推理能力的局限性\n\n作者从大型语言模型(LLMs)的一个根本性问题出发：尽管LLMs展示了显著能力，但在复杂推理任务中实现可靠、准确的推理仍是一个核心挑战。这一观察构成了研究的起点，引导作者思考如何提升模型的推理能力。\n\n## 2. 现有方法的双向观察与局限分析\n\n作者系统考察了提升LLMs推理能力的两类主流方法，并识别出它们的互补性局限：\n\n### 2.1 结构化推理方法\n- **代表技术**：Chain-of-Thought (CoT)及其变体\n- **核心机制**：通过外部化推理过程，在模型输出上施加逻辑框架\n- **关键优势**：程序上稳健，提供清晰的逻辑路径\n- **根本局限**：作者敏锐地将其描述为\"affectively sterile\"（情感上贫乏），缺乏驱动高质量人类推理的动机背景\n\n### 2.2 情感提示方法\n- **代表技术**：EmotionPrompt等\n- **核心机制**：通过点燃模型的\"认知状态\"和引导其焦点来提升性能\n- **关键优势**：动机上强大，能激发模型潜能\n- **根本局限**：结构上不精确，作为\"一次性\"全局刺激，缺乏引导多步自我修正过程所需的有针对性指导\n\n## 3. 关键洞察：情感与认知的整合视角\n\n作者从认知科学研究中获得关键启示：情感不是认知的障碍，而是不可或缺的组成部分，塑造注意力、动机和问题解决。这一洞察成为连接两类方法的桥梁，启发了一个核心问题：如何将情感反馈整合到迭代自我修正过程中？\n\n理论基础包括：\n- Paul Ekman的六种基本情绪理论（快乐、悲伤、惊讶、愤怒、恐惧、厌恶）\n- 对立过程理论(opponent-process theory)：情感可以触发纠正性认知状态\n\n## 4. 核心假设的形成\n\n基于以上观察和洞察，作者形成了核心假设：**有针对性的情感反馈可以触发纠正性认知状态**，帮助模型放弃有缺陷的推理路径并尝试新的方法。例如，表达对答案\"失望\"的提示可以激励模型放弃其有缺陷的推理，类似于人类如何利用不满来重新开始解决问题的努力。\n\n这一假设将情感从简单的性能增强器转变为推理过程的引导机制，标志着研究思路的重要转变。\n\n## 5. 方法设计：HEART框架的构建\n\n基于核心假设，作者设计了HEART框架，包含两个主要组件：\n\n### 5.1 情感提示构建（Affective Cue Prompts）\n- **理论基础**：基于Ekman的六种基本情绪\n- **设计方法**：精心设计30个情感提示，每种情绪5个不同表达\n- **质量控制**：先由强大LLM生成，再由人类研究人员手动优化，确保情感表达的多样性和自然性\n\n### 5.2 情感迭代协议（HEART Protocol）\n- **迭代机制**：从标准CoT响应开始，如果初始响应不正确，启动一系列修正尝试\n- **动态情感序列**：受对立过程理论启发，在迭代中交替使用积极和消极情感组\n- **候选生成**：在每个迭代中，使用当前激活的情感组中的每个AC-Prompt作为反馈生成新的候选答案\n- **候选解析**：设计两种场景\n  - S1（Oracle Selection）：理想化场景，假设访问全知验证器\n  - S2（Generative Synthesis）：现实场景，使用生成式集成器合成新答案\n\n## 6. 实验验证与关键发现\n\n作者通过精心设计的实验验证了HEART的有效性，并获得了重要发现：\n\n### 6.1 Oracle-guided场景下的显著效果\n- HEART一致且显著地优于所有基线方法\n- 证明动态情感提示在引导模型生成正确解决方案方面非常有效\n- 例如，在Humanity's Last Exam上，Deepseek-R1与HEART达到84.16%的准确率，显著高于基线\n\n### 6.2 Verifier-free场景下的关键瓶颈\n- HEART在无验证器场景下表现不如Oracle场景\n- 揭示了实际瓶颈：模型在合成过程中选择正确推理路径的能力有限\n- 这一发现指明了未来研究方向：改进自主选择机制\n\n### 6.3 动态情感序列的关键作用\n- 通过消融研究，作者发现动态情感序列是性能提升的主要驱动因素\n- 交替使用积极和消极提示提供了更强大的动机循环，防止模型陷入单一思维模式\n\n## 7. 理论贡献与实践意义\n\n### 7.1 理论贡献\n- 首次将情感反馈整合到迭代自我修正过程中\n- 提供了动态、迭代的情感提示可以显著改善推理和自我修正的第一个有力证据\n- 确定了选择机制作为关键瓶颈，为未来研究指明方向\n\n### 7.2 实践意义\n- 为更自然、协作的人机系统铺平道路\n- 可能在个性化教育、创意协作写作等领域解锁新能力\n- 建议未来研究方向应超越纯逻辑，走向更全面、与人类对齐的模型\n\n## 8. 伦理考量与未来方向\n\n作者也考虑了伦理问题，承认使用苛刻语言的含义，并强调这些提示严格作为诊断工具，目的是理解模型机制，而不是认可或规范有害的交互模式。\n\n未来工作方向包括：\n- 增强核心算法：用自适应选择模型替换预定义的情感序列\n- 扩展应用范围：将框架扩展到多模态LLMs，探索更广泛的评估领域\n\n## 总结\n\nHEART方法的逻辑演进展现了从宏观问题（LLMs推理能力限制）到具体方法论的完整思考过程。作者通过观察现有方法的互补性局限，从认知科学中获取灵感，形成核心假设，设计并验证了创新框架。这一过程不仅解决了技术问题，还开辟了将情感与认知整合的新研究方向，体现了跨学科思维在AI研究中的价值。", "summary_translation": "测试时扩展(Test-time scaling)在无需微调的情况下，显著提高了语言模型在复杂推理任务上的性能，已取得相当大的成功。然而，当前策略如自我反思(self-reflection)主要集中于逻辑或结构上的优化，并未利用情感反馈(affective feedback)的指导潜力。受心理学研究启发——研究表明情绪能够调节认知表现——我们提出了HEART（一种利用情感驱动提示进行迭代自我修正的新型框架）。HEART使用一套精心策划的简洁情感短语，基于Paul Ekman博士分类的六种普遍情绪，为模型的错误响应提供反馈。通过在迭代过程中系统性地改变反馈的情感基调，我们的方法引导模型摆脱有缺陷的推理路径，探索更有前景的替代方案。\n\n我们在具有挑战性的推理基准测试上评估了我们的框架，包括OlympiadBench、Humanity's Last Exam和SimpleQA。我们的结果揭示了一个重要的新现象：当由oracle verifier（神谕验证器）引导时，这种情感迭代协议能够解锁显著更深入的推理能力，导致在使用相同验证器的情况下，准确度相对于最先进的基线方法有了持续且大幅的提升。然而，我们也识别出了实际部署中的一个关键瓶颈。在无验证器(verifier-free)的设置中，该方法难以稳定地利用这些增益，这突显了未来工作的一个关键挑战。我们的研究结果表明，机器推理的下一个前沿可能不仅在于完善逻辑，还在于理解和利用模型的\"HEART\"（情感核心）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#172", "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?", "link": "/arxiv/2509.22947", "arxiv_id": "2509.22947", "authors": "Mohammed Sabry, Anya Belz", "summary": "Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure.", "subjects": "Computation and Language, Artificial Intelligence, Machine Learning", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.695204", "filter_reason": "这篇论文符合我的研究目标，因为它专注于提升大语言模型的基础通用推理能力，而非将LLM作为工具应用于特定领域。 从第一步核心判断来看，论文的本质是研究如何通过预训练数据的设计来增强大语言模型的上下文学习(ICL)能力。ICL是大语言模型的一种核心通用推理能力，允许模型通过少量示例学习新任务。论文提出了Bi-Induct方法，通过在预训练流中注入目标合成数据来研究归纳电路的发展，这属于改进LLM基础能力的研究范畴，而非将LLM应用于特定领域。 从第二步正面指标看，论文明确包含核心概念\"Large language models\"，并研究\"reasoning\"能力方向中的上下文学习(ICL)，这是大语言模型进行多步推理和问题解决的基础能力。 从第三步排除标准看，论文不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，因此不符合任何排除标准。 论文的核心贡献在于探索了预训练数据设计对LLM通用推理能力(ICL)的影响，挑战了早期归纳电路激活直接改善ICL的假设，并提出了机制感知的预训练诊断方法。这些研究直接关注如何提升大语言模型本身的通用推理能力，符合我的研究目标。", "summary2": "本文旨在解决在相同计算预算下，自然文本预训练与加入目标合成示例哪种更有利于上下文学习(ICL)的问题。针对Transformer模型的归纳头(induction heads)激活，我们提出了一种Bi-Induct方法，在预训练流中注入前向或后向复制片段，并在0.13B到1B参数模型上通过ICL基准性能、头级别遥测和语言建模困惑度验证了其有效性。实验发现，尽管Bi-Induct加速了小规模模型归纳头的出现，但并未一致提升ICL性能，而自然文本训练产生了更具功能必要性的归纳电路。", "inspiration_trace": "# 从宏观问题到方法创新：Bi-Induct的逻辑演进\n\n## 1. 宏观问题的提出\n\n**核心问题**：在有限计算预算下，如何最有效地提升大型语言模型的上下文学习(ICL)能力？\n\n这一问题源于实际约束与理论挑战的交汇点：ICL能力对Transformer模型至关重要，但其发展通常需要海量计算资源和训练数据。在现实场景中，计算预算往往是固定的，因此如何在相同计算条件下最大化ICL性能成为一个关键研究问题。\n\n## 2. 关键观察与背景分析\n\n### 观察一：ICL的神经机制基础\n- 研究发现Transformer模型会发展出特定的\"感应头\"(induction heads)结构\n- 这些感应头形成两头部 motif，实现重复模式匹配：当token A在上下文中重现时，模型会增加预测之前A之后出现的token的概率\n- 这种机制被证实与ICL能力密切相关，特别是在模式匹配任务上\n\n### 观察二：感应头发展的延迟性\n- 感应头通常在训练后期才出现，需经过数十亿token训练\n- 这一延迟表现为训练损失的第一个平台期后的相变过程\n- 这种延迟意味着计算资源的低效利用——模型在前期训练中并未发展关键能力\n\n### 观察三：现有方法的局限性\n- 已有尝试通过修改目标函数(如多token预测)或使用特定任务数据来缩短ICL平台期\n- 但这些研究要么专注于纯合成任务训练，要么依赖架构/目标级别的干预\n- 缺乏在自然语言预训练环境中，通过数据级别干预的系统研究\n\n## 3. 核心假设的形成\n\n基于以上观察，作者提出核心假设：\n\n**假设**：在相同计算预算(iso-FLOPs)下，通过在预训练数据中有针对性地注入合成示例来明确锻炼感应电路，相比仅使用自然文本预训练，能更有效地加速感应头出现并增强ICL性能。\n\n这一假设基于以下推理链：\n1. 如果感应头是ICL的关键机制，那么直接训练模型执行感应任务应加速这些结构形成\n2. 数据级别干预比架构/目标级别干预更容易大规模部署\n3. 精心设计的合成示例可在最小化自然语言建模能力负面影响的同时，提供明确的感应信号\n\n## 4. 方法设计：Bi-Induct的诞生\n\n### 设计原则\n- **针对性**：合成示例应直接锻炼感应机制的核心功能\n- **轻量级**：仅替换小部分自然token，保持自然分布主导\n- **可扩展**：方法应易于在不同模型规模上实现和比较\n- **可控性**：引入明确的对照组以分离变量效应\n\n### 具体实现\n\n#### 4.1 合成片段构造\n设计三种类型的定向复制片段：\n- **前向复制(Induction)**：序列后跟相同序列(如\"ABCDE ABCDE\")\n- **后向复制(Anti-induction)**：序列后跟其反转(如\"ABCDE EDCBA\")\n- **平衡混合(Balanced)**：随机选择前向或后向复制\n\n这种设计直接针对感应电路的核心功能，同时提供控制条件(后向复制作为对照)。\n\n#### 4.2 课程安排策略\n- **线性退火注入**：注入概率随时间从初始值线性衰减至零\n- **前期加载**：在训练早期集中注入复制提示，对应感应头通常出现的阶段\n- **保持主导**：确保自然文本在整体训练数据中保持主导地位\n\n#### 4.3 实验验证框架\n设计多维度评估体系：\n1. **行为评估**：标准ICL基准和函数式探针的少样本性能\n2. **机制评估**：头部级别遥测技术，测量感应和反感应复制分数\n3. **质量保障**：保留数据集上的困惑度评估\n4. **消融研究**：通过消融顶部感应头验证其对ICL的贡献\n\n## 5. 预期与发现的对比\n\n### 预期结果\n1. Bi-Induct将加速感应头出现，特别是在较小规模模型中\n2. 这种加速将转化为更好的ICL性能\n3. 合成数据注入不会显著损害整体语言建模质量\n\n### 实际发现\n1. **部分验证**：Bi-Induct确实加速了感应头出现，在0.13B和0.5B模型中更明显\n2. **关键差异**：早期激活并不一致转化为更好的少样本泛化能力\n3. **规模效应**：在1B模型上，纯自然文本基线在函数式ICL探针上表现最佳\n4. **机制差异**：自然数据发展出\"承载负载\"的感应头，而Bi-Induct产生更分布式、冗余的感应活动\n5. **鲁棒性**：压力测试(标签置换、不同shot数、HITS@1 vs HITS@3)保持了这些趋势\n\n## 6. 理论演进与最终洞见\n\n### 理论调整\n基于实验发现，作者对初始理论进行了关键修正：\n\n**初始观点**：加速感应头出现是提升ICL的关键\n**修正观点**：感应头变得\"功能上必要\"比\"早期出现\"更重要\n\n### 核心洞见\n1. **存在性与必要性的区别**：感应头的存在不等于它们对模型功能至关重要\n2. **电路质量优于数量**：自然训练发展出更集中、承载负载的感应电路，而合成数据诱导出更分散、冗余的电路\n3. **规模依赖性**：较小模型从合成数据中受益更多，而较大模型能更有效地从自然文本中提取关键模式\n4. **路径转移**：随着模型规模增大，预测质量越来越多地通过FFN/残差路径路由，减少对局部高分数感应头的依赖\n\n## 7. 方法论贡献与启示\n\n### 方法论贡献\nBi-Induct作为一种机制感知的数据重写方法，提供了：\n- 一种轻量级、可扩展的预训练干预策略\n- 将机制解释学见解转化为实际训练方法的范例\n- 系统评估合成数据对特定电路发展影响的框架\n\n### 研究启示\n1. **超越加速**：未来工作应关注如何使诱导的电路变得功能上必要，而不仅仅是早期出现\n2. **机制感知诊断**：需要发展更精细的工具来评估电路的功能必要性，而非仅测量其存在\n3. **数据设计原则**：预训练数据混合应 fostering 承载负载的结构，而非仅促进存在性结构\n\n这一逻辑演进展示了从宏观问题到具体方法，再到理论修正的完整科研思维过程，体现了假设驱动研究与实证发现的有机结合。", "summary_translation": "在预训练过程中显式训练归纳电路(induction circuit，归纳推理电路)是否能改善上下文学习(in-context learning, ICL)，或者在计算量保持不变(iso-FLOPs，等浮点运算数)的情况下，自然文本是否足够？为了测试目标合成数据能否加速归纳头(induction heads，归纳推理头)的出现并增强ICL，我们引入了Bi-Induct，这是一种轻量级课程，将前向复制(Induction，归纳)、后向复制(Anti，反归纳)或平衡混合注入预训练流。我们在iso-FLOPs条件下训练了从0.13B到1B参数的模型，评估了(i)少样本ICL基准，(ii)头部级别遥测(head-level telemetry)，和(iii)保留语言建模困惑度(held-out language modeling perplexity)。我们的发现挑战了早期归纳电路激活直接改善ICL的假设。虽然Bi-Induct在小规模上加速了归纳头的出现，但这并不总能产生更强的泛化能力。在标准LM基准上，Bi-Induct与仅自然文本训练表现相当；在函数式ICL探测(function-style ICL probes)上，1B的仅自然文本模型表现最佳。压力测试(如标签置换、HITS@1 vs. HITS@3、1 vs. 10 shots)保持了这些趋势。遥测显示较大的仅自然文本模型在没有显式归纳模式的情况下发展出更广泛、更早的归纳头。反归纳数据未能引发有意义的激活。合成数据带来的困惑度惩罚随规模增大而缩小，表明更大的模型能够以最小代价吸收非自然模式。关键的是，消融(ablating)顶部2%的归纳头比随机消融更能降低ICL性能，特别是对于仅自然文本模型，表明存在更集中、承重的电路。Bi-Induct变体表现出更多冗余的归纳活动，意味着不同的电路利用方式。总体而言，诱导激活并不足够：ICL的增益取决于这些电路变得功能上必要。这些结果强调了机制感知的预训练诊断和能够培养承重结构而不仅仅是存在结构的数据混合。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#176", "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "link": "/arxiv/2509.22887", "arxiv_id": "2509.22887", "authors": "EunJeong Hwang, Yuwei Yin, Giuseppe Carenini, Peter West, Vered Shwartz", "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.", "subjects": "Computation and Language", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.702547", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是提升LLM的通用推理能力，特别是社交推理能力。论文提出ToMAgent (ToMA)，通过将心智理论(ToM)与对话前瞻配对来训练模型，这是一种新的训练范式，旨在增强LLM的基础推理能力，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确聚焦于\"LLM-based social agents\" - 能力方向：涉及推理能力，特别是社交推理和目标导向的推理行为 - 新兴范式：涉及LLM智能体框架 第三步排除标准：论文不符合任何排除标准。虽然涉及社交智能，但这是在提升LLM的通用推理能力，而非将LLM应用到特定社会学研究领域。社交智能是人类智能的核心组成部分，提升LLM的社交推理能力属于提升其通用推理能力的范畴。 第四步特殊情况处理：论文提出的ToMAgent是一种通用的智能体框架，专注于提升LLM的社交推理能力，符合\"提出通用的智能体协作框架来增强LLM的通用问题解决能力\"的情况。 综合分析，论文的核心贡献是通过引入心智理论和新的训练范式来提升LLM的社交推理能力，这是一种通用推理能力的提升，符合研究目标。", "summary2": "本文旨在解决LLM社交代理缺乏心智理论(ToM)的问题。针对社交对话场景，我们提出了一种ToMAgent (TOMA)方法，通过将ToM与对话前瞻配对来训练，产生对实现对话目标最有用的心理状态。在Sotopia交互式社交评估基准上，通过目标达成、关系维护和知识获取等指标验证了其有效性，相比基线模型实现了高达18.9%的性能提升。", "inspiration_trace": "# 推演ToMAgent方法的逻辑链\n\n## 1. 宏观问题：社交智能的缺失\n\n**观察起点**：人类社交互动的成功不仅取决于表达自己的意图，还取决于理解对话伙伴的心理状态。这种理解他人心理状态的能力被称为\"心智理论\"(Theory of Mind, ToM)，是人类社交智能的核心组成部分。\n\n**问题识别**：当前的聊天机器人和基于LLM的社交代理通常没有整合ToM能力，这限制了它们在复杂社交场景中的表现。尽管这些系统被部署在理解用户至关重要的环境中（如工作面试、客户服务），但它们往往缺乏真正理解用户心理状态的能力。\n\n## 2. 现状分析：LLM的ToM能力与局限\n\n**研究发现**：LLM是否已经具备ToM能力存在争议。虽然一些研究显示LLM在ToM基准测试上表现良好，但其他研究表明这种能力不一致且表面化。\n\n**现有方法评估**：改进LLM ToM能力的方法包括思维链提示、神经符号方法、贝叶斯逆向规划和推理时假设生成等。然而，这些方法通常在静态和人工的ToM基准上评估，要求模型作为观察者而非动态环境中的参与者回答问题，因此无法充分反映ToM在真实社交互动中的价值。\n\n**研究缺口**：现有研究忽视了明确心理状态建模在社交互动中的作用，而专注于训练模型生成导致成功对话的话语。\n\n## 3. 核心假设：ToM能提升社交代理性能\n\n**假设提出**：明确使用ToM的LLM在对话中会表现得更好，更有效地实现目标。\n\n**初步验证**：通过简单提示模型在对话轮次之间生成心理状态，已经能显著提高目标实现。这表明心理状态建模对社交互动有积极影响。\n\n**深入假设**：通过将ToM与对话前瞻相结合，可以产生对实现对话目标最有用的心理状态。这种方法可以让模型学习哪些心理状态对成功社交互动最有价值。\n\n## 4. 方法设计：ToMAgent框架\n\n**核心思路**：创建一个训练框架，使代理能够生成关于他人心理状态的假设，模拟这些假设导致的对话结果，并选择最有利于实现目标的心理状态-话语对进行训练。\n\n**关键组件设计**：\n\n1. **心理状态假设生成**：针对每个对话上下文，生成K个关于他人心理状态的假设，涵盖信念、欲望、意图、情感和知识等至少三个维度。\n\n2. **话语生成**：基于每个心理状态假设，生成J个可能的回应话语。\n\n3. **对话前瞻模拟**：对每个心理状态-话语对，模拟最多四轮后续对话，评估对话结果。\n\n4. **效用评估**：计算每个模拟对话的目标达成得分，保留平均得分≥9的配对（或得分最高的配对）。\n\n5. **模型微调**：使用筛选出的高质量心理状态-话语对训练模型，使其能够联合学习心理状态预测和话语生成。\n\n**数学表达**：训练目标为最小化联合损失：\nLCE(ϕ) = -log Pϕ(m*|H) - log Pϕ(u*|H, m*)\n\n## 5. 实验验证：ToM的实证效果\n\n**验证环境**：使用Sotopia交互式社交评估基准，包含合作、谈判、说服和冲突等多种目标导向社交场景。\n\n**评估指标**：目标实现程度(Goal)、关系维护(Relationship)和知识获取(Knowledge)。\n\n**对比设置**：\n- 基础模型(Base)：未经微调的原始语言模型\n- 基础+心理状态(Base+MS)：先生成心理状态再生成话语的两步提示方法\n- 仅微调话语(FT+Uttr)：仅微调话语生成，消融心理状态监督\n- 仅微调心理状态(FT+MS)：仅微调心理状态生成，消融话语生成监督\n- ToMAgent(FT+MS+Uttr)：联合微调心理状态和话语生成\n\n**实验结果**：ToMAgent在所有指标上均优于基线模型，表现出更具战略性、目标导向的推理行为，同时能更好地维护与伙伴的关系。特别是在长对话中，ToMAgent表现出更强的适应能力，随着对话轮次增加，目标完成度不断提高。\n\n## 6. 深入分析：ToM的作用机制\n\n**策略分析**：ToMAgent采用更多战略性策略（如妥协、适应、提供解决方案），而基础模型更依赖人际策略（如建立融洽关系）和直接目标追求方法（如持续请求、直接请求）。\n\n**心理状态分析**：ToMAgent生成更多关于意图的假设，较少依赖情感，同时生成更多一阶信念（关于他人心理状态的推理），表明其更好地推断他人心理状态。\n\n**场景适应性**：ToMAgent在各种社交场景类型（合作、谈判、说服、冲突）中均优于基础模型，尤其在冲突场景中优势更明显，表明ToM在复杂社交互动中的价值。\n\n## 结论：从问题到方法的逻辑演进\n\n从识别社交智能中ToM的缺失，到分析现有方法的局限，提出ToM能提升社交代理性能的核心假设，最终设计出通过对话前瞻训练框架整合ToM的ToMAgent方法，并通过实证研究验证了其有效性。这一逻辑演进展示了如何从宏观问题出发，通过观察、假设和验证，形成一套有效的方法论，为构建具有社交智能的LLM代理提供了新思路。", "summary_translation": "心理理论(Theory of Mind, ToM)——即对他人心理状态的理解——是人类社交智能的一个关键方面，然而，聊天机器人和基于大语言模型(LLM)的社交智能体通常并未整合这一能力。在这项工作中，我们证明了明确使用心理理论(ToM)的大语言模型(LLM)在对话方面表现更佳，能够更有效地实现目标。在证明仅仅提示模型在对话轮次之间生成心理状态已经能带来显著益处之后，我们进一步介绍了ToMAgent (ToMA)，一种专注于心理理论(ToM)的对话智能体。ToMA通过将心理理论(ToM)与对话前瞻(dialogue lookahead)配对进行训练，以生成对实现对话目标最有用的心理状态。在Sotopia互动社交评估基准上的实验表明，我们的方法相较于一系列基线方法具有有效性。综合分析显示，ToMA表现出更具战略性、目标导向的推理行为，这使得它能够进行长期视野(long-horizon)的适应，同时与对话伙伴保持更好的关系。我们的研究结果为整合心理理论(ToM)以构建具有社交智能的大语言模型(LLM)智能体迈出了一步。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#180", "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing", "link": "/arxiv/2509.22854", "arxiv_id": "2509.22854", "authors": "Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang", "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.", "subjects": "Computation and Language", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.704641", "filter_reason": "根据筛选标准，这篇论文符合研究范围。首先，从核心判断来看，论文的本质是改进LLM的基础能力，特别是\"in-context learning\"(上下文学习)能力。论文提出的\"In-Context Routing (ICR)\"方法通过在注意力对数级别内化可泛化的ICL模式，增强模型的通用推理能力，这属于改进LLM本身基础能力的研究，而非将LLM作为工具应用于特定领域。 其次，论文包含正面指标中的核心概念\"Large language models, LLMs\"，且研究的in-context learning与推理能力密切相关，因为上下文学习本质上涉及模型如何从给定信息中学习和推理。虽然论文没有直接提到reasoning、planning等关键词，但改进ICL能力实际上是在提升模型的通用推理能力。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性问题。虽然论文在多个领域的真实世界数据集上进行了评估，但其目的是测试方法的泛化能力，而非解决特定领域问题。 最后，论文的核心贡献是提出了一种新的训练/推理范式来增强LLM的通用上下文学习能力，这直接符合\"提高大语言模型本身的通用推理能力\"的研究目标。ICR方法通过改进注意力机制，使模型能够更好地从上下文中学习和推理，从而提升了LLM的通用推理能力。", "summary2": "本文旨在解决隐式上下文学习(ICL)泛化性不足的问题。针对大型语言模型(LLMs)的上下文学习场景，我们提出了一种基于注意力路由的上下文路由(ICR)方法，通过在注意力logits层面提取和利用可泛化的ICL模式。在12个跨领域数据集和多个LLMs上，通过准确率等指标验证了ICR的有效性，结果显示ICR在域内和域外任务上均优于现有隐式ICL方法，且无需特定任务的检索或训练。", "inspiration_trace": "# 从问题到方法：ICR的逻辑演进分析\n\n## 1. 宏观问题：零样本上下文学习的困境\n\n大型语言模型(LLMs)的上下文学习(ICL)能力允许模型通过少量示例适应新任务，无需参数更新。然而，传统ICL面临两个根本性挑战：\n- **效率瓶颈**：插入上下文示例显著增加序列长度和推理成本\n- **稳定性问题**：性能对示例顺序和格式微小变化高度敏感\n\n这引出了核心问题：**如何在零样本设置下获得ICL的能力，同时克服传统方法的局限性？**\n\n## 2. 现有方法观察：残差流干预的局限\n\n作者观察到现有隐式ICL方法主要采用\"向量注入\"范式，即将ICL示例转化为密集向量并注入模型残差流。然而，这种方法存在根本性缺陷：\n\n- **泛化能力有限**：向量通常从特定任务示例构建，难以跨任务迁移\n- **信息容量受限**：固定大小向量只能编码有限信息，添加新知识需重新构建\n- **机制理解不足**：缺乏模型无关、输入无关的理论基础\n- **模拟而非内化**：促使模型\"模拟\"ICL行为而非真正\"内化\"ICL机制\n\n## 3. 关键发现：多任务ICL中的潜在模式\n\n通过多任务ICL实验(图1)，作者发现了一个重要现象：来自不同任务的ICDs有时能提升性能，有时却会降低性能。这表明：\n- 不同任务的ICDs可能嵌入了**潜在的跨任务ICL模式**\n- 显式提示引入的噪声可能**掩盖**这种潜在模式\n- 现有方法无法有效提取和利用这种**可泛化的结构模式**\n\n## 4. 核心假设：注意力空间中的ICL本质\n\n基于上述观察，作者提出了一个关键假设：**更具泛化性的ICL模式存在于注意力空间而非残差流中**。具体而言：\n\n- 注意力机制是transformer中学习ICL的核心组件\n- ICL的本质在于查询如何通过注意力路径被路由到相关信息\n- 注意力logits是查询-键交互的直接体现，是挖掘ICL模式的自然入口点\n\n这一假设引导作者从\"后验残差干预\"转向\"注意力空间调制\"。\n\n## 5. 方法推导：从假设到ICR\n\n### 5.1 提取跨任务ICL模式\n\n作者首先设计了一种提取可泛化ICL模式的方法：\n1. 在多个领域执行显式ICL，收集最后一个token的Q和K投影\n2. 构建跨领域的ICL基础，通过PCA提取\"主ICL方向\"(PIDs)\n3. 理论分析表明，这些PIDs捕获了跨领域共享的ICL结构，而领域特定变化在聚合中趋于平均\n\n### 5.2 设计注意力路由机制\n\n基于提取的PIDs，作者提出了\"注意力路由\"范式：\n- 使用路由向量α为PIDs分配权重，形成低秩调制\n- 在零样本推理中，将注意力动态偏向提取的PIDs\n- 这种方法在注意力logits级别直接控制信息流，而非后验干预\n\n### 5.3 构建查询条件路由器\n\n为使方法适应不同输入，作者设计了可学习的路由器：\n- 使用冻结文本编码器处理查询，生成输入表示\n- 通过两个MLP分支生成路由矩阵α(x)和门控矩阵γ(x)\n- α(x)根据查询语义自适应调制PIDs，γ(x)调节各注意力头贡献\n- 实现了\"一次训练，重复使用\"的框架\n\n### 5.4 多目标训练策略\n\n为确保训练效果，作者设计了综合训练目标：\n- 监督交叉熵：提供语义监督\n- 置信度对齐：确保路由预测不降低模型置信度\n- 稀疏路由：鼓励稀疏调制，特别是对后层施加更大稀疏性惩罚\n\n## 6. 逻辑链条总结\n\n从宏观问题到最终方法，作者的逻辑演进可概括为：\n\n**问题**：零样本ICL的效率与稳定性挑战  \n↓  \n**观察**：现有向量注入方法泛化能力有限  \n↓  \n**发现**：多任务ICL中存在潜在跨任务模式  \n↓  \n**假设**：ICL本质存在于注意力空间而非残差流  \n↓  \n**方法1**：通过PCA提取跨任务PIDs  \n↓  \n**方法2**：设计注意力路由机制调制注意力logits  \n↓  \n**方法3**：构建查询条件路由器实现自适应调制  \n↓  \n**方法4**：设计多目标训练策略确保效果  \n↓  \n**最终方法**：In-Context Routing (ICR)\n\n这一逻辑演进展示了作者从问题本质出发，通过深入观察和理论分析，提出创新假设，并逐步推导出一种在注意力空间内化ICL模式的新方法，实现了高效、稳定且可泛化的零样本上下文学习。", "summary_translation": "# 中文翻译\n\n隐式上下文学习(Implicit in-context learning, ICL)作为一种新兴的有前景的范式，在大型语言模型(Large Language Models, LLMs)的表示空间中模拟ICL行为，旨在以零样本成本实现少样本性能。然而，现有方法主要依赖于将偏移向量注入残差流中，而这些偏移向量通常是从标记的演示或特定任务对齐中构建的。此类设计未能充分利用ICL底层的结构机制，并且泛化能力有限。为解决这一问题，我们提出了上下文路由(In-Context Routing, ICR)，这是一种新颖的隐式ICL方法，在注意力对数(attention logits)层面内化了可泛化的ICL模式。该方法提取ICL过程中出现的可重用结构方向，并采用可学习的输入条件路由器(input-conditioned router)相应地调节注意力对数，从而实现一次训练、多次重用的框架。我们在涵盖多个领域和多种大型语言模型的12个真实世界数据集上对ICR进行了评估。结果表明，ICR始终优于需要特定任务检索或训练的先前隐式ICL方法，同时在对现有方法而言具有挑战性的域外任务上展现出强大的泛化能力。这些发现使ICR有望推动ICL实用价值的边界扩展。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#187", "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "link": "/arxiv/2509.22750", "arxiv_id": "2509.22750", "authors": "Jeonghyun Park, Ingeol Baek, Seunghyun Yoon, Haeun Jang, Aparna Garimella, Akriti Jain, Nedim Lipka, Hwanhee Lee", "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is inseparable from the reasoning process itself. This ambiguity creates a distinct challenge, where multiple reasoning paths emerge from a single question, each requiring independent resolution. Since each sub-question is ambiguous, the model must resolve ambiguity at every step. Thus, answering a single question requires handling multiple layers of ambiguity throughout the reasoning chain. We find that current Large Language Models (LLMs) struggle in this setting, typically exploring wrong reasoning paths and producing incomplete answers. To facilitate research on multi-hop ambiguity, we introduce MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE), a benchmark designed to analyze and evaluate this challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM verification pipeline. Our experiments reveal that even state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and significant challenge. To establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning and InstructiON (CLARION), a multi-agent framework that significantly outperforms existing approaches on MIRAGE, paving the way for more adaptive and robust reasoning systems.", "subjects": "Computation and Language, Artificial Intelligence", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.713349", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：论文的核心是关于改进LLM在多跳推理(multi-hop reasoning)中处理模糊性(ambiguity)的能力。它提出了MIRAGE基准测试和CLARION多智能体框架，旨在增强LLM的逻辑推理能力，特别是处理复杂、模糊的多步推理问题。这属于改进LLM基础推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确讨论Large Language Models (LLMs)在推理任务中的表现 - 能力方向：专注于reasoning，特别是multi-hop reasoning（多跳推理），属于逻辑推理范畴 - 新兴范式：提出了CLARION多智能体框架(multi-agent framework)，属于llm-based agents和multi-agent systems的研究 第三步排除标准：论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的CLARION框架是一种通用的多智能体协作方法，用于增强LLM处理模糊推理的能力，属于提升通用推理能力的范畴，应予以保留。 最终决策：这篇论文的核心贡献是提出了一种新的评估基准和多智能体框架来提升LLM在模糊多跳推理任务中的表现，这直接符合\"提高大语言模型本身通用推理能力\"的研究目标。论文关注的是LLM的基础推理能力提升，而非特定领域应用，因此应被纳入研究范围。", "summary2": "本文旨在解决多跳问答中的歧义性问题。针对现实世界中多跳问答存在的多重推理路径歧义，我们提出了MIRAGE基准和CLARION多智能体框架，并在包含1,142个高质量多跳歧义问题的MIRAGE基准上通过STR-EM、Disambig-F1和LLM-as-a-Judge等指标验证了其有效性。", "inspiration_trace": "# 从问题观察到方法论形成：CLARION框架的逻辑演进\n\n## 一、宏观问题发现：多跳问答中的歧义挑战\n\n**观察现象**：现实世界中的多跳问答(Multi-hop QA)存在一个根本性挑战——问题本身包含与推理过程不可分割的歧义性。这种歧义导致从单个问题中产生多个推理路径，每条路径都需要独立解决。\n\n**实证支持**：\n- 实验数据表明，在真实用户查询中，48.4%的问题是歧义的，17.7%涉及多跳推理，13.3%同时具有这两种特性\n- 现有LLMs在处理这类问题时表现不佳，通常探索错误的推理路径并产生不完整答案\n\n**核心问题**：当前模型倾向于只解决单个子问题的歧义，过早地修剪掉其他推理路径，导致答案不完整且无法满足用户的完整信息需求。\n\n## 二、问题分类与分析：多跳歧义的本质\n\n作者将多跳歧义系统性地分为三类，每类具有不同的特征和处理需求：\n\n### 1. 多跳句法歧义\n- **定义**：问题允许多个格式正确的句法解析，导致不同的推理链\n- **典型触发**：介词短语附着、代词共指、协调范围(\"and/or\")、量词范围\n- **处理需求**：需要模型进行句法解析(Resolve)\n\n### 2. 多跳一般歧义\n- **定义**：查询范围过于狭窄，导致早期承诺和修剪其他有效推理路径\n- **典型触发**：过度指定约束、过于精细的实体/区域粒度、限制性修饰词\n- **处理需求**：需要模型进行范围调整和意图重聚焦(Generalize)\n\n### 3. 多跳语义歧义\n- **定义**：相同词语映射到多种含义或实体，产生不同的多跳推理路径\n- **典型触发**：歧义名称、首字母缩略词、品牌-普通名词重叠\n- **处理需求**：需要模型进行意义/实体选择(Interpret)\n\n## 三、假设形成：解决多跳歧义的关键思路\n\n基于问题分析，作者形成三个核心假设：\n\n1. **独特挑战假设**：解决与多步推理过程深度集成的歧义是一个独特的、很大程度上未解决的挑战\n\n2. **系统性处理假设**：有效解决多跳歧义问答需要系统性地检测、消除歧义并回答问题，而非简单应用现有方法\n\n3. **两阶段处理假设**：将任务分为规划和执行两个阶段可能更有效——先分析问题并消除歧义，再执行推理计划\n\n## 四、方法论设计：CLARION框架的提出\n\n基于上述假设，作者设计了CLARION(CLarifying Ambiguity with a Reasoning and InstructiON)框架，采用两阶段多智能体方法：\n\n### 4.1 规划智能体(Planning Agent)\n在任何检索或回答之前分析输入问题，执行三个顺序操作：\n\n1. **歧义检测**：确定问题是否包含歧义，无歧义则直接传递给执行智能体\n\n2. **歧义类型分类**：将歧义问题分为句法、一般或语义三类\n\n3. **问题澄清**：基于检测到的类型，将原始问题重写为消除歧义的变体，同时保留信息需求\n\n### 4.2 执行智能体(Acting Agent)\n通过ReAct风格提示方案执行推理计划，展开为思想→行动→观察循环：\n\n1. **搜索**：当需要额外证据时检索外部文档\n\n2. **规划**：如果当前计划不足，重新调用检测、类型分类或澄清\n\n3. **回答**：一旦收集到足够证据，合成最终输出\n\n**设计特点**：\n- 所有行动采用JSON格式确保可靠解析和自动执行\n- 限制最多五次迭代防止无限循环\n- 达到限制时强制执行Answer行动\n\n## 五、验证与评估：方法论的有效性证明\n\n作者在MIRAGE基准测试上验证CLARION框架，与三个强基线比较：\n\n1. **无检索**：仅LLM推理，无外部上下文\n2. **朴素RAG**：标准检索-然后-阅读流水线\n3. **DIVA**：三阶段多样化-验证-适应RAG框架\n\n**实验结果**：\n- CLARION在所有评估指标上显著优于基线方法\n- 消融研究证实每个组件的贡献，其中问题澄清组件最为关键\n- 结果支持了作者的假设：识别歧义并明确重写问题对实现稳健的多跳歧义问答至关重要\n\n## 六、逻辑演进总结\n\nCLARION框架的提出体现了清晰的科研思路演进：\n\n1. **从现象到问题**：观察到多跳问答中的歧义挑战，识别出当前方法的不足\n\n2. **从问题到分类**：系统分析多跳歧义的本质，将其分为三类并明确每类的处理需求\n\n3. **从分析到假设**：基于问题本质形成三个核心假设，指明解决方向\n\n4. **从假设到方法**：设计两阶段多智能体框架，系统化处理多跳歧义问题\n\n5. **从方法到验证**：通过严格实验验证方法有效性，证明假设的正确性\n\n这一逻辑链条从宏观问题出发，逐步聚焦到具体解决方案，展现了从观察、假设到形成最终方法论的完整思考过程，为解决多跳歧义问答这一复杂挑战提供了创新且有效的思路。", "summary_translation": "现实世界中的多跳问答（Multi-hop Question Answering, QA）通常涉及与推理过程本身密不可分的模糊性。这种模糊性带来了一种独特的挑战，即从单个问题中会产生多条推理路径，每条路径都需要独立解决。由于每个子问题都是模糊的，模型必须在每一步都解决模糊性。因此，回答单个问题需要在整个推理链中处理多层次的模糊性。我们发现当前的大型语言模型（Large Language Models, LLMs）在这种设置下表现不佳，通常会探索错误的推理路径并产生不完整的答案。\n\n为促进多跳模糊性的研究，我们提出了MIRAGE（MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions，用于幻觉问题的多跳推理与模糊性评估），这是一个旨在分析和评估模糊性解释与多跳推理这一具有挑战性交叉领域的基准测试。MIRAGE包含1,142个高质量的多跳模糊问题示例，这些示例根据句法、一般和语义模糊性的分类法进行分类，并通过严格的多LLM验证流程精心筛选。我们的实验表明，即使是最新模型在MIRAGE上也表现不佳，这证实了解决模糊性与多步推理相结合是一个独特且重大的挑战。\n\n为建立一个稳健的基线，我们提出了CLARION（CLarifying Ambiguity with a Reasoning and InstructiON，通过推理和指令澄清模糊性），这是一个多智能体框架，在MIRAGE上显著优于现有方法，为开发更具适应性和稳健性的推理系统铺平了道路。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#183", "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "link": "/arxiv/2509.22824", "arxiv_id": "2509.22824", "authors": "Chi Ruan, Dongfu Jiang, Yubo Wang, Wenhu Chen", "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.", "subjects": "Computation and Language", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.711258", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"Critique Reinforcement Learning (CRL)\"的新训练范式，旨在通过让模型学会评论和反思来增强其推理能力。论文本质上是改进LLM的基础能力，提出新的训练范式来增强其逻辑推理能力，完全符合第一步的\"保留\"标准。从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)，特别是在逻辑推理任务上表现出色，并提出了基于强化学习(RL)的新训练方法。论文强调其方法不仅提高了代码生成能力，还增强了通用推理能力，这种能力可以迁移到广泛的任务中，这正是研究目标所关注的\"通用推理能力\"。论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面。虽然论文提到了代码生成，但这是作为增强通用推理能力的手段而非特定领域应用。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在增强代码生成模型的推理和批判能力，解决标准强化学习缺乏明确批判或反思机制的问题。针对代码生成和一般推理任务，我们提出了一种Critique Reinforcement Learning (CRL)方法，通过奖励模型对预测批判的正确性进行反馈，并开发了CRITIQUE-CODER模型。在LiveCodeBench、EvalPlus等代码基准测试及BBEH逻辑推理基准上，通过准确率、Pass@1等指标验证了其有效性，CRITIQUE-CODER-8B在LiveCodeBench上达到60.8%，超越了DeepCoder-14B和GPT-o1等模型。", "inspiration_trace": "# 从问题到方法论：Critique Reinforcement Learning的逻辑演进\n\n## 1. 宏观问题：如何提升大型语言模型的推理能力？\n\n在当代AI研究中，提升大型语言模型(LLM)的推理能力是一个核心挑战。特别是在代码生成、数学问题求解和逻辑推理等复杂任务中，模型需要展现出超越简单模式匹配的深度思考能力。研究者们已经探索了多种路径，但如何系统性地增强模型的批判性思维和自我反思能力，仍是一个开放性问题。\n\n## 2. 现有方法及其局限性分析\n\n### 2.1 强化学习与思维链结合的现状\n\n论文首先观察到，强化学习(RL)与思维链(CoT)的结合已成为提升模型推理能力的主流范式。这种方法使模型能够迭代优化中间推理步骤，在代码生成等领域取得了显著进展。然而，作者敏锐地指出这一范式的根本局限：\n\n> \"虽然有效，但它主要关注生成响应，缺乏明确促进批判或反思的机制。\"\n\n标准RLVR(带可验证奖励的强化学习)虽然能提高问题解决能力，但难以激发模型对现有解决方案的内部批判或反思行为。模型被训练为直接生成正确答案，却很少被要求评估和判断已有解决方案的质量。\n\n### 2.2 批判学习的新兴研究\n\n与此同时，作者注意到另一条研究路线正在兴起：Critique-Fine-Tuning (CFT)和Critique-Guided-Distillation (CGD)等方法表明，明确教导LLM如何进行批判可以有效释放其推理潜力。这些方法通过让模型学习评判和反思，展示了提升推理能力的新途径。\n\n## 3. 关键洞察与假设形成\n\n### 3.1 核心洞察\n\n通过分析现有研究，作者形成了两个关键洞察：\n\n1. **互补性洞察**：RL和批判学习具有天然互补性——RL专注于提高问题解决能力，而批判学习则培养批判性思维和推理能力。\n\n2. **机制缺失洞察**：标准RL框架中缺乏对批判和反思的明确激励机制，这限制了模型推理能力的进一步提升。\n\n### 3.2 核心假设\n\n基于这些洞察，作者提出了一个大胆而创新的假设：\n\n> \"将批判机制整合到RL范式中，通过奖励模型对解决方案的准确判断，可以同时优化问题解决技能和批判能力，并且这些能力可能转移到广泛的任务中。\"\n\n这一假设暗示，如果能够让模型在生成解决方案的同时，也学习如何评判解决方案的正确性，可能会产生更强大的推理能力。\n\n## 4. 方法论设计：Critique Reinforcement Learning (CRL)\n\n### 4.1 核心思想\n\n为了验证假设，作者设计了Critique Reinforcement Learning (CRL)，其核心思想简洁而有力：\n\n- 模型被提示为给定的(问题,解决方案)对生成批判\n- 奖励仅基于生成的批判的最终判断标签c ∈ {True, False}是否与真实判断c*一致\n- 这与标准RLVR形成鲜明对比，后者只激励模型为给定问题生成正确解决方案\n\nCRL的创新在于它将批判学习融入RL框架，通过明确的奖励信号引导模型发展批判性思维能力。\n\n### 4.2 实现方案：CRITIQUE-CODER\n\n基于CRL范式，作者进一步开发了CRITIQUE-CODER模型，采用混合训练策略：\n\n- 用20%的CRL数据替代标准RL数据\n- CRL数据帮助模型发展批判性思维和推理能力\n- RL数据专注于提高问题解决性能\n- 两种数据类型结合，使模型能够整合两种范式的优势\n\n这种设计反映了作者的深入思考：CRL不应完全替代RL，而应作为其补充，因为CRL训练主要关注评判问题-解决方案对，而不生成实际解决方案。\n\n## 5. 实验验证与结果分析\n\n### 5.1 数据集构建与训练策略\n\n作者精心设计了数据集构建过程：\n\n- 从rStar-Coder的人类种子RL数据集构建CRL数据集\n- 通过过滤测试用例提高效率和一致性\n- 生成候选解决方案并根据测试用例通过率确定其判断\n- 采用两阶段训练策略，逐步增加上下文长度以充分利用长推理链的潜力\n\n### 5.2 评估结果\n\n实验结果有力地支持了作者的假设：\n\n1. **性能提升**：CRITIQUE-CODER在所有评估基准上均优于仅使用RL的模型\n   - CRITIQUE-CODER-8B在LiveCodeBench (v5)上达到60%以上，优于DeepCoder-14B和GPT-o1等推理模型\n   - 在Qwen3-4B上，LiveCodeBench得分从54.2提升到59.0(+4.8)\n\n2. **能力迁移**：CRL不仅提高了代码生成能力，还增强了通用推理能力\n   - 在BIG-Bench Extra Hard逻辑推理基准上，CRITIQUE-CODER超越了基线和RL训练模型\n   - 平均比基线模型提高+6.1分，证明批判能力可以转移到其他领域\n\n3. **最佳比例**：消融研究确认20%的CRL数据混合比例是最佳实践\n   - 完全用CRL替代RL会导致性能下降，证实了CRL作为RL补充而非替代的价值\n\n## 6. 结论与意义\n\n通过这一系列严谨的推理和实验，作者得出结论：CRL作为一种新颖的强化学习训练框架，成功地将批判学习整合到RL范式中，解决了标准RL框架中缺乏批判和反思激励的问题。\n\n这一工作的意义不仅在于提出了一种新的训练方法，更在于它揭示了批判性思维与问题解决能力之间的深层联系，为未来提升LLM推理能力的研究指明了新方向。正如作者所言：\n\n> \"CRL通过赋予模型更强的批判和推理能力来增强标准RL——这些能力不仅在编码任务中表现出来，还能有效转移到更广泛的推理领域。\"\n\n这一完整的逻辑链条——从宏观问题出发，分析现有方法局限性，形成创新假设，设计具体方法，并通过实验验证——展示了学术研究的严谨思维过程，也为如何系统性地解决复杂AI问题提供了范例。", "summary_translation": "强化学习(Reinforcement Learning, RL)已成为一种流行的训练范式，特别是与推理模型(reasoning models)配对时。虽然有效，但它主要关注生成响应，缺乏明确促进批评(critique)或反思(reflection)的机制。几项最近的研究，如批评微调(Critique-Fine-Tuning, CFT)和批评引导蒸馏(Critique-Guided-Distillation, CGD)，已经展示了明确教授大语言模型(Large Language Models, LLMs)如何批评的好处。受这些研究启发，我们提出了批评强化学习(Critique Reinforcement Learning, CRL)，在该方法中，模型被要求为给定的(问题，解决方案)对生成批评。奖励仅取决于生成批评的最终判断标签$c \\in \\{\\texttt{True}, \\texttt{False}\\}$是否与真实判断$c^*$一致。基于这一点，我们引入了\\textsc{Critique-Coder}，它通过用CRL数据替代20%的标准RL数据，在RL和CRL的混合数据上进行训练。我们对多个模型(\\textsc{Critique-Coder})进行微调，并在不同基准测试上评估它们，以展示它们相对于仅使用RL的模型的优势。我们表明，\\textsc{Critique-Coder}在所有评估的基准测试上始终优于仅使用RL的基线模型。值得注意的是，我们的\\textsc{Critique-Coder-8B}在LiveCodeBench (v5)上可以达到超过60%的准确率，优于其他推理模型，如DeepCoder-14B和GPT-o1。除了代码生成，\\textsc{Critique-Coder}还展示了增强的一般推理能力，这在其在BBEH数据集的逻辑推理任务上的更好表现中得到证明。这表明在代码数据集上应用CRL增强了通用推理和批评能力，这些能力可以在广泛范围内转移到各种任务中。因此，我们相信CRL作为标准RL对LLM推理的极好补充。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#195", "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression", "link": "/arxiv/2509.25176", "arxiv_id": "2509.25176", "authors": "Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang", "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.925497", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的核心是提出SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression），一种针对大型推理模型(LRMs)的强化学习方法。该方法通过在训练过程中交替压缩和扩展推理预算，动态调整最大展开长度，从而提高模型的推理效率和准确性。这明显是关于改进LLM基础推理能力的研究，属于强化学习优化的方法论研究，符合保留标准。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确研究大型推理模型(LRMs)，即具有推理能力的大语言模型 - 能力方向：专注于推理能力(reasoning)，特别是在数学推理任务(AIME24)上评估 - 训练方法：核心是强化学习(Reinforcement Learning)方法，通过迭代训练提升模型性能 第三步排除标准：论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（虽然使用数学竞赛作为评估基准，但目的是提升通用推理能力） - 不关注模型基础设施、部署优化或硬件加速 第四步特殊和模糊情况：论文不涉及需要特殊判断的智能体/工具使用或幻觉/可解释性/安全等问题。 综上所述，这篇论文的核心贡献是提出一种新的强化学习训练范式来增强大语言模型的推理能力和效率，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型推理模型(LRMs)在强化学习训练中产生冗余思考模式导致的效率与性能权衡问题。针对数学推理任务，我们提出了一种名为SIRI的迭代强化学习方法，通过交替压缩和扩展推理预算来动态调整输出长度。在DeepSeek-R1-Distill-Qwen模型上，通过AIME24、AIME25等数据集的Pass@1准确率和token使用量评估，验证了SIRI在提高推理准确率的同时显著减少冗余token的有效性。", "inspiration_trace": "# SIRI方法逻辑推演：从问题到解决方案\n\n## 宏观问题：大型推理模型的效率与性能困境\n\n大型推理模型(LRMs)通过强化学习提升推理能力时，展现出显著的性能提升，但同时也产生了一个严重问题：模型输出中包含大量冗余推理token，导致训练和推理时间大幅增加。这引发了一个核心挑战：**如何在保持或提高推理准确性的同时，显著减少冗余token，提升推理效率？**\n\n## 现象观察：性能与效率的权衡困境\n\n作者通过观察现有方法发现：\n\n1. **重复思维模式**：LRMs在强化学习后表现出明显的重复思考、不必要的回溯和非推理填充内容\n2. **现有方法的局限性**：先前尝试减少冗余的方法（如引入长度惩罚、长度截断、混合推理策略等）无一例外地导致模型性能下降或停滞\n3. **关键发现**：DeepScaleR的8K训练阶段显示，尽管响应长度急剧下降，模型性能却有所提升，表明模型能将关键推理步骤压缩到更短上下文中，为后续探索释放能力\n\n## 假设形成：压缩-扩展周期的潜力\n\n基于上述观察，作者提出核心假设：**通过周期性地交替压缩和扩展推理预算，可以同时实现性能提升和效率优化**。具体假设包括：\n\n1. **压缩阶段**：强制模型在有限上下文中做出精确有价值的决策，减少冗余token，提高推理密度\n2. **扩展阶段**：放松长度限制，为模型提供探索和规划的空间，特别是在长视野设置中\n3. **周期性迭代**：每个压缩-扩展周期后，模型性能应提升而输出长度减少，逐步逼近性能-效率权衡的帕累托前沿\n\n## 方法设计：SIRI框架的构建\n\n基于假设，作者设计了SIRI(Scaling Iterative Reinforcement Learning with Interleaved Compression)框架，包含三个关键组件：\n\n### 1. 奖励塑造机制\n采用长度限制奖励函数：\n```\nR(y) = {1, 如果可以从clip(y, L)中提取正确答案; 0, 其他情况}\n```\n这种方法在组内响应多样化时特别有效，能引导模型保留正确且密集的推理模式，同时修剪低效或错误的模式。\n\n### 2. 长度调度器设计\n设计控制压缩和探索行为的调度器，需满足：\n- 压缩阶段防止性能下降\n- 扩展阶段鼓励探索\n- 确保模型在扩展阶段结束前生成长度达到平稳\n\n提出三种调度器变体：\n- **阶梯调度器**：直接在压缩阶段将最大生成长度从Lmax降至Lmin，扩展阶段再恢复\n- **余弦调度器**：使用余弦函数平滑地调整长度，使压缩和恢复过程更平缓\n- **阶梯-余弦调度器**：结合前两者优点，确保过程平滑性的同时，在Lmax处充分探索，在Lmin处充分利用\n\n### 3. 迭代训练框架\n通过多个压缩-扩展周期的迭代，逐步提升模型性能和效率，形成螺旋上升的优化轨迹。\n\n## 验证与优化：实验驱动的改进\n\n通过实验验证假设并优化方法：\n\n1. **有效性验证**：SIRI在减少token使用的同时提高性能。例如，在DeepSeek-R1-Distill-Qwen-1.5B上，SIRI-low在三次迭代后AIME24性能提高43.2%，token使用减少46.9%\n\n2. **调度器优化**：\n   - 发现周期较长的调度器(640周期)表现最佳，压缩阶段更平滑，扩展阶段更持久\n   - 余弦调度器减轻压缩过程中的性能损失，阶梯调度器最大化扩展过程中的性能提升\n\n3. **行为机制分析**：\n   - 压缩-扩展方案主要影响模型的回溯和验证行为\n   - \"wait\"等代表回溯验证的标记频率在压缩和扩展过程中显著变化\n   - 熵值在压缩阶段降低，扩展阶段增加，但保持在稳定范围内而非崩溃\n\n## 结论：突破性能-效率权衡的新范式\n\nSIRI方法通过周期性地在训练过程中交替压缩和扩展token预算，成功地突破了大型推理模型中性能与效率的权衡困境。这一发现揭示了通过动态调整输出截断长度来平衡探索和效率的潜力，为大型推理模型的训练提供了新的研究方向。", "summary_translation": "我们介绍SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression，交错压缩的迭代强化学习扩展），这是一种简单而有效的大型推理模型（Large Reasoning Models, LRMs）强化学习方法，能够实现更高效和准确的推理。现有研究已经观察到LRMs中存在重复性思维模式，而减少这些模式的尝试往往以性能为代价。在本文中，我们表明这种权衡可以通过一种训练机制来克服，该机制通过在训练过程中动态调整最大推演长度（rollout length），迭代地在压缩和扩展推理预算之间交替。压缩阶段减少了推演长度，迫使模型在有限的上下文中做出精确且有价值的决策，从而有效减少冗余标记（tokens）并提高推理密度。扩展阶段则放宽长度限制，为模型在长时程（long-horizon）设置中进行探索和规划提供空间。值得注意的是，我们发现经过每个压缩-扩展周期后，模型的性能即使在其输出长度减少的情况下也有所提高，稳步推动其更接近性能-效率权衡中的帕累托前沿（Pareto frontier）。在DeepSeek-R1-Distill-Qwen-1.5B上进行训练，SIRI-low在三次迭代后将AIME24上的性能提高了43.2%，同时减少了46.9%的标记使用量，而SIRI-high相比所有其他方法达到了最高准确率（图1）。我们的研究结果揭示了在训练过程中周期性振荡LRM的输出截断长度的潜力，以动态平衡推理中的探索和效率，从而收敛到两者之间的最优\"甜点\"（sweet spot）。我们的模型已公开发布。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#199", "title": "The Era of Real-World Human Interaction: RL from User Conversations", "link": "/arxiv/2509.25137", "arxiv_id": "2509.25137", "authors": "Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston", "summary": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.", "subjects": "Artificial Intelligence, Computation and Language, Machine Learning", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.927030", "filter_reason": "这篇论文的核心贡献是提出了一种新的强化学习范式\"Reinforcement Learning from Human Interaction (RLHI)\"，该方法使大语言模型能够直接从真实用户对话中学习，而非依赖预标注的专家反馈。这完全符合我的研究目标，因为它专注于改进LLM的基础能力，提出了新的训练范式来增强模型性能。 具体分析如下： 1. 核心判断：论文本质上是关于改进LLM基础能力的研究，提出了新的强化学习方法(RLHI)来优化模型的对齐能力和指令跟踪能力。论文明确提到这些方法还能\"enhances performance on reasoning benchmarks\"，表明其对推理能力的提升作用。 2. 正面指标：论文符合多个正面指标，包括： - 核心概念：针对对话模型(属于LLM范畴) - 能力方向：明确提及对推理基准性能的提升 - 训练方法：核心是提出新的强化学习方法(RLHI)，与RLHF相关但更先进 3. 排除标准：论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性(应用层面)。 4. 特殊情况：论文虽然涉及用户交互，但不是将LLM作为工具应用于特定领域，而是提出通用方法来增强模型的基础能力，因此应该保留。 综上所述，这篇论文通过提出新的强化学习范式来改进LLM的通用能力(包括推理能力)，完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决当前对话模型依赖预注释专家反馈而非真实用户交互的问题。针对真实用户对话场景，我们提出了一种Reinforcement Learning from Human Interaction (RLHI)方法，包括User-Guided Rewrites和User-Based Rewards两种互补技术，并在WildChat数据集上通过个性化、指令遵循和推理能力指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到方法论：RLHI的逻辑演进链\n\n## 宏观问题：如何实现语言模型的持续改进与多面对齐？\n\n### 观察一：当前对齐方法的局限性\n- **现状**：现有模型主要依赖静态的专家标注数据进行对齐（预标注问题、固定演示、排名或评分）\n- **问题**：\n  1. 反映标注者非自然场景下的观点，而非真实用户的真实多元长期目标\n  2. 捕获静态、无上下文的判断，而非动态、情境化的需求\n  3. 受标注预算限制，而非随实际使用和用户多样性扩展\n\n### 观察二：人类学习方式的启示\n- **核心洞察**：人类通过与环境和他人的持续互动学习，接收反馈并调整行为\n- **映射到AI**：语言模型的丰富监督来源已存在于真实世界中——模型与真实用户的自然互动\n- **关键发现**：这种互动揭示长期历史中的隐藏用户偏好和动态上下文需求\n\n## 假设形成：真实人类互动可作为有效监督信号\n\n### 假设核心\n真实人类互动能提供可扩展、有效的个性化对齐监督，并具有三个关键特性：\n\n1. **上下文基础**：\n   - 出现在任务或对话流程中\n   - 直接与用户情境需求和模型先前输出相关\n   - 被用户个性化知识（历史、偏好）塑造\n\n2. **演化分布**：\n   - 反映随时间变化的目标和环境\n   - 提供与人类需求真实分布时间相关的监督\n\n3. **多样监督信号**：\n   - 包含显式高带宽信号（修正、澄清）和隐式线索（不参与、沮丧）\n   - 可能包含风格、情感基调甚至对抗性输入\n\n## 假设验证：真实人类互动数据分析\n\n### 数据分析（WildChat-1M数据集）\n1. **用户反馈模式**：\n   - 26.51%用户消息为\"带反馈的重新尝试\"\n   - 对话后期阶段，此类反馈占比高达83.15%\n   - 这些反馈虽短（平均272字符），但语义密集\n\n2. **上下文多样性**：\n   - 真实用户互动上下文多样性(0.865)显著高于HH-RLHF(0.751)和HelpSteer2(0.848)\n   - 表明真实用户互动反映更广泛的真实日常需求\n\n3. **用户偏好多样性**：\n   - 用户personas呈现明显差异（如专业知识水平、信息密度、语调、结构偏好）\n   - 存在独特偏好子集（如反复请求类比或角色扮演）\n   - 用户需求可能跨领域变化或随时间演变\n\n## 方法论构建：RLHI框架\n\n### 核心挑战\n如何从有机互动中提取有效信号，将长期用户偏好与轮级响应关联？\n\n### 解决方案：双路径方法\n\n#### 路径一：RLHI with User-Guided Rewrites\n**逻辑**：利用用户自然语言反馈直接改进不满意输出\n- **步骤**：\n  1. 识别\"带反馈的重新尝试\"\n  2. 基于用户反馈修改原始响应\n  3. 形成偏好对（修改版 vs 原始版）\n  4. 整合用户persona（从长期历史提取）\n  5. 应用基于persona的条件DPO训练\n\n**关键创新**：将用户自然语言反馈转化为具体改进信号，而非简化为二元标签\n\n#### 路径二：RLHI with User-Based Rewards\n**逻辑**：针对无明确反馈的请求，利用用户长期偏好指导响应选择\n- **步骤**：\n  1. 为每个请求生成多个候选响应\n  2. 使用条件于用户persona的奖励模型评分\n  3. 选择最高分和最低分响应形成偏好对\n  4. 应用基于persona的条件偏好优化\n\n**关键创新**：将长期用户偏好转化为响应选择的指导信号\n\n### 统一框架：Persona-Conditioned Preference Optimization\n两种方法通过基于persona的条件偏好优化，将长期用户personas与轮级偏好联系起来，实现个性化对齐。\n\n## 实验验证与发现\n\n### 关键实验结果\n1. **个性化与指令遵循**：两种RLHI变体均显著优于基线\n2. **标准基准**：User-Based Rewards在AlpacaEval 2.0达到77.9%长度控制胜率\n3. **推理能力**：User-Guided Rewrites将四个推理基准平均准确率从26.5提升至31.8\n\n### 关键发现\n1. 用户引导重写优于从头生成（利用上下文反馈）\n2. 基于用户的奖励能捕捉长期偏好，实现更强对齐\n3. 强化学习优于监督微调（利用偏好信号）\n4. 人类互动数据嘈杂，需要质量过滤\n5. 用户多样性对RLHI效果至关重要\n\n## 结论：从静态标注到动态互动的范式转变\n\n论文最终论证了有机人类互动为个性化对齐提供了可扩展、有效的监督，标志着从静态专家标注向动态真实用户互动的范式转变，为构建能随时间改进的个性化助手指明了方向。", "summary_translation": "我们主张，为了实现持续模型改进和多方面对齐（multifaceted alignment），未来的模型必须从自然人类交互中学习。当前的对话模型（conversational models）是使用预先标注的、专家生成的人类反馈（expert-generated human feedback）进行对齐的。在这项工作中，我们介绍了Reinforcement Learning from Human Interaction (RLHI，人类交互强化学习)，这是一种直接从真实环境中的用户对话（in-the-wild user conversations）中学习的范式。我们开发了两种互补方法：(1) RLHI with User-Guided Rewrites（用户引导重写的人类交互强化学习），该方法基于用户的自然语言后续回应修改不满意的模型输出；(2) RLHI with User-Based Rewards（基于用户奖励的人类交互强化学习），该方法通过一个基于用户长期交互历史（称为persona，角色）的奖励模型（reward model）进行学习。这些方法共同通过persona-conditioned preference optimization（角色条件偏好优化）将长期用户角色与轮次级偏好（turn-level preferences）联系起来。在基于WildChat数据集的对话训练中，两种RLHI变体在个性化（personalization）和指令遵循（instruction-following）方面均优于强基线，且类似的反馈增强了在推理基准测试（reasoning benchmarks）上的性能。这些结果表明，有机人类交互（organic human interaction）为个性化对齐（personalized alignment）提供了可扩展且有效的监督（effective supervision）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#202", "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones", "link": "/arxiv/2509.25123", "arxiv_id": "2509.25123", "authors": "Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng", "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.928189", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是研究LLM如何通过强化学习(RL)组合已有技能来获取新技能，这直接涉及到改进LLM的基础推理能力和提出新的训练范式。论文探讨了LLM的函数组合推理能力，即从已知的f(x)和g(x)推理出未见过的f(g(x))，这属于逻辑推理和多步推理的核心研究。 其次，论文包含多个正面指标：核心概念明确是LLMs；能力方向聚焦于推理能力，特别是逻辑推理和问题解决；训练方法采用强化学习(RL)；虽然未直接讨论智能体或工具使用，但研究的技能组合机制与这些新兴范式的底层能力相关。 第三，论文不符合任何排除标准，它没有涉及多模态与视觉、特定应用领域或模型可靠性的应用层面问题，而是专注于通用推理机制的研究。 最后，论文的核心贡献是揭示了LLM通过RL能够真正获取新技能（而非仅仅激活现有技能），并通过组合已有技能实现更复杂的推理能力，这种能力还能泛化和迁移到不同任务。这些发现对于提升LLM的通用推理能力具有重要意义，完全符合研究目标。", "summary2": "本文旨在解决强化学习(RL)是否真正教会大型语言模型(LLMs)新技能的争议问题。针对LLMs在RL训练中技能获取的机制，我们提出了一种基于组合性技能的合成框架，通过字符串转换任务定义原子技能和组合技能，并在Llama-3.1-8B-Instruct模型上通过准确率和泛化能力验证了其有效性。实验证明RL能使模型通过组合已有技能获得新技能，且这些技能能泛化到更复杂问题和跨任务场景。", "inspiration_trace": "# 从宏观问题到核心方法：论文逻辑链推演\n\n## 宏观问题：RL是否真正教会LLM新技能？\n\n论文始于一个根本性问题：强化学习(RL)在大型语言模型(LLM)后训练中是真正教授新技能，还是仅仅激活了模型已有的能力？这一问题触及了当前对RL在LLM中作用的激烈争论核心。\n\n## 矛盾观察：实证成功与理论质疑的对立\n\n作者首先识别了两个看似矛盾的观察：\n1. **实证成功**：RL即使在没有先前监督微调的情况下也能取得显著成果，甚至能直接基于基础模型进行训练。\n2. **理论质疑**：有观点认为RL的贡献仅限于重新加权现有推理策略，而非真正学习新能力。\n\n这种矛盾表明，我们需要更深入地理解RL在LLM中的实际作用机制。\n\n## 理论启发：从人类认知技能获取中寻找答案\n\n作者转向认知科学理论，特别是Anderson(1982)的研究，该研究表明人类通过组合和内化现有技能来获得新认知技能。这一理论启发作者提出一个核心假设：LLM可能也通过类似机制，即组合现有技能来获得真正的新技能。\n\n## 假设形成：RL组合性假说\n\n基于上述理论启发，作者提出了\"RL组合性假说\"：\n> 一旦模型通过下一个词元预测(NTP)训练获得了任务所需的原子不可分解技能，通过适当激励的RL可以教会模型通过组合原子技能来学习新技能。\n\n这一假说将研究焦点从\"RL是否教授新技能\"转向\"RL如何通过组合现有技能教授新技能\"。\n\n## 方法设计：构建受控实验框架\n\n为验证假说，作者设计了一个受控合成框架，解决现有研究中的关键问题：\n\n### 1. 解决数据污染问题\n- 设计字符串转换预测任务，使用无意义函数标识符(如func_16)\n- 在RL期间隐藏函数定义，确保任务只能通过原子技能获取训练解决\n\n### 2. 精确定义技能层次\n- **原子技能**：单一、不可分解的转换，如func_16(x)\n- **组合技能**：原子技能的嵌套组合，如func_15(func_16(x))\n- **难度控制**：通过嵌套深度控制任务难度，Level n涉及n个函数组合\n\n### 3. 两阶段训练协议\n- **阶段1**：通过RFT学习原子技能，模型观察函数定义\n- **阶段2**：通过RL或RFT学习组合技能，模型仅观察函数名称和组合\n\n## 实验验证：系统性测试假说\n\n作者通过一系列针对性实验验证假说的各个方面：\n\n### 实验1：RL教授新组合技能\n- **方法**：比较三种RL配置(仅原子技能、仅Level-2组合、混合)\n- **发现**：包含组合任务的RL使模型能泛化到更复杂问题，而仅原子技能的RL则不能\n- **结论**：RL在组合数据上教授了可泛化的新技能\n\n### 实验2：组合数据是关键激励\n- **方法**：比较RL与RFT在相同组合数据上的效果\n- **发现**：RL显著优于RFT，后者几乎无法泛化\n- **结论**：RL本身是学习可泛化组合技能的重要因素，而非仅有组合数据\n\n### 实验3：组合技能的跨任务迁移\n- **方法**：测试字符串任务上学习的组合技能能否迁移到Countdown任务\n- **发现**：在字符串任务上接受RL训练的模型在Countdown任务上表现显著更好\n- **结论**：RL学习的组合技能可作为元技能，增强模型在不同领域使用原子技能的能力\n\n### 实验4：突破\"重排序幻觉\"\n- **方法**：分析不同难度级别上的pass@k性能\n- **发现**：在简单问题上，RL与基础模型差距随k增加而缩小；但在困难问题上，差距反而扩大\n- **结论**：先前认为RL仅重排序的结论源于评估和训练模型已表现良好的任务，RL确实能学习新技能\n\n### 实验5：RL转变推理行为\n- **方法**：分析不同模型在组合问题上的失败模式\n- **发现**：RL训练的模型完全消除了\"忽略组合\"错误，主要失败模式变为原子预测错误\n- **结论**：RL从根本上改变了模型的行为，使其能正确理解和处理组合结构\n\n## 最终方法论：技能获取视角的LLM开发\n\n基于以上发现，作者提出了一个更广泛的LLM开发方法论：\n1. **基础模型阶段**：构建具有必要基本技能的基础模型\n2. **后训练阶段**：通过RL和适当激励获取更高级技能，这些技能能更好地泛化到复杂和跨域问题\n\n这一方法论强调了基础模型开发与后训练策略之间的协调，从技能获取的角度重新思考LLM的发展路径。\n\n## 总结：从问题到方法的逻辑链条\n\n论文展现了清晰的逻辑演进：从宏观问题(RL是否教授新技能)→矛盾观察→理论启发→假说形成→方法设计→实验验证→最终方法论。通过精心设计的受控实验，作者不仅回答了初始问题，还揭示了RL在LLM中作用的更深层次机制，为LLM的后训练提供了新的理论指导。", "summary_translation": "强化学习(RL)是教授大语言模型(LLMs)真正的新技能，还是仅仅激活其已有的能力？这个问题是关于强化学习在大语言模型后训练中作用的持续争论的核心。一方面，即使没有先前的监督微调(supervised finetuning)，强化学习也能取得强有力的实证结果；另一方面，批评者认为强化学习除了重新加权现有的推理策略外，贡献甚微。本研究提供了具体证据，表明大语言模型可以通过组合现有技能在强化学习过程中获得真正的新技能，这反映了人类获取新认知技能的核心机制之一。为了减轻数据污染(data contamination)和其他混杂因素(confounding factors)的影响，并允许对任务复杂性进行精确控制，我们为研究开发了一个合成框架(synthetic framework)。具体而言，我们将技能定义为给定x时推断字符串转换函数(string transformation function) f(x)输出的能力。当大语言模型在强化学习之前已经学习了f和g时，我们的实验表明，强化学习使其能够学习它们未见过的组合h(x)=g(f(x))。此外，这种组合能力(compositional ability)可以泛化到更困难的问题，例如在强化学习训练期间未见过的超过2个函数的组合。令人惊讶的是，我们的实验表明，在源任务(source task)上获得的组合技能能够迁移到不同的目标任务(target task)。这种迁移甚至在没有对目标任务进行组合训练的情况下也能发生，只需要对目标任务的原子技能(atomic skills)有先验知识。我们的定性分析(qualitative analysis)表明，强化学习从根本上改变了模型的推理行为。相比之下，使用相同数据的下一个词元预测训练(next-token training)并未产生这些发现。我们的系统性实验为大语言模型学习提供了新的见解，表明首先构建具有基本技能的基础模型，然后使用强化学习来激励复杂问题的高级、可泛化技能的价值。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#200", "title": "Rethinking Entropy Regularization in Large Reasoning Models", "link": "/arxiv/2509.25133", "arxiv_id": "2509.25133", "authors": "Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, Jing Shao", "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise in enhancing the reasoning abilities of large reasoning models (LRMs). However, it suffers from a critical issue: entropy collapse and premature convergence. Naive entropy regularization, a common approach for encouraging exploration in the traditional RL literature, fails to address this problem in the context of LRM. Our analysis reveals that this failure stems from the vast action space and long trajectories in LRMs, which easily trigger a global entropy explosion as the model indiscriminately explores all possible actions and states. To address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method that confines exploration to a meaningful subset of actions and states. SIREN achieves this through a two-step entropy masking mechanism, consisting of a top-p mask and a peak-entropy mask. In addition, regularization is transformed into a self-anchored form to stabilize training. Across five mathematical benchmarks, SIREN attains superior average performance over previous entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes greater response diversity and maintains entropy at an appropriate level, which helps to preserve the validation pass@k throughout training. This effectively mitigates the premature convergence problem common in RLVR for LRM.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.927368", "filter_reason": "这篇论文完全符合我的研究目标和筛选标准。从核心判断来看，论文的本质是关于改进大型推理模型(LRMs)的基础能力，提出了一种新的强化学习训练范式(SIREN)来增强模型的推理能力。论文明确关注解决熵崩溃和过早收敛问题，这些都是提升模型推理能力的关键技术挑战。 在正面指标方面，论文包含了多个相关主题：核心概念上提到了\"Large reasoning models (LRMs)\"；能力方向上明确关注\"reasoning abilities\"，特别是在数学推理方面；训练方法上讨论了\"Reinforcement learning with verifiable rewards (RLVR)\"，这与强化学习方法论直接相关。 从排除标准看，论文没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。虽然论文在数学基准测试上评估了方法，但数学推理被视为通用推理能力的一部分，而非特定应用领域。论文的核心是改进推理模型的一般训练方法，而不是解决特定领域的问题。 论文的核心贡献是提出SIREN方法，通过选择性熵正则化来解决大型推理模型在强化学习训练中遇到的熵崩溃和过早收敛问题，从而提升模型的通用推理能力。这完全符合我的研究目标，即筛选致力于提高大语言模型本身通用推理能力的论文。", "summary2": "本文旨在解决大型推理模型(LRMs)中强化学习与可验证奖励(RLVR)的熵崩溃和过早收敛问题。针对LRMs巨大的动作空间和长轨迹场景，我们提出了一种选择性熵正则化方法SIREN，通过top-p掩码和峰值熵掩码将探索限制在有意义的子集内，并采用自锚定正则化稳定训练。在五个数学基准测试上，SIREN实现了显著优于基线方法的性能，如AIME24/25上+6.6 maj@k的提升，有效促进了响应多样性并维持了适当熵水平。", "inspiration_trace": "# 从问题到解决方案：SIREN方法的逻辑演进\n\n## 1. 宏观问题：大型推理模型中的探索困境\n\n作者首先关注到大型推理模型(LRMs)在强化学习与可验证奖励(RLVR)训练中面临的核心问题：**熵崩溃和过早收敛**。这一现象表现为策略在训练早期就变得过度确定性，导致模型输出趋同，最终降低训练效率和整体性能。这是一个影响LRMs推理能力的根本性问题。\n\n## 2. 传统解决方案及其局限性\n\n面对探索不足的问题，作者转向传统强化学习中常用的解决方案——**熵正则化**。这种方法通过增加策略的随机性来鼓励探索，在传统RL环境中已被证明有效。\n\n然而，作者通过实验发现，在LRM环境中，朴素熵正则化表现出强烈的**超参数敏感性**：小的熵系数几乎不产生效果，而大的熵系数则迅速触发\"熵爆炸\"。这表明传统方法在LRM环境中存在根本性的不适应，无法有效解决探索问题。\n\n## 3. 问题根源分析：从现象到本质\n\n作者深入探究为什么传统熵正则化在LRM中失效，提出了一个关键假设：**LRM的巨大动作空间和长轨迹**可能是导致问题的根本原因。\n\n为验证这一假设，作者进行了对比实验，比较了原始模型和经历熵爆炸的模型：\n\n**关键发现**：\n1. **动作空间层面**：在熵爆炸模型中，概率分布在整个词汇表上几乎均匀，而在原始模型中，概率质量集中在小的、语义有意义的子集上。\n2. **轨迹层面**：在熵爆炸模型中，几乎所有令牌位置的熵都保持均匀高，而在原始模型中，只有一小部分令牌表现出高熵。\n\n**结论**：LRM的巨大动作空间使得通过将概率质量扩散到许多低效用令牌来增加熵变得容易。自回归生成过程将这种不确定性向前传播，导致熵随序列长度累积并最终爆炸。\n\n## 4. 概念提炼：构建解决方案的理论基础\n\n基于上述分析，作者提炼出两个关键概念，为解决方案奠定基础：\n\n### 4.1 策略核(Policy Nucleus)\n- **定义**：词汇表中包含最重要语义令牌的子集，这些令牌的生成是合理的，不太可能导致不连贯响应。\n- **洞见**：探索应该集中在策略核内，而不是在整个词汇表上均匀分布。这为解决巨大动作空间问题提供了方向。\n\n### 4.2 关键令牌(Critical Tokens)\n- **观察**：在长轨迹中，只有一小部分令牌在探索中起着不成比例的重要作用，通常对应于决定推理方向的句子开头或逻辑连接词。\n- **特征**：这些关键令牌往往表现出相对较高的熵。\n- **洞见**：令牌级熵可用于识别关键令牌，允许有选择地应用正则化。这为解决长轨迹问题提供了思路。\n\n## 5. 解决方案设计：SIREN方法的提出\n\n基于上述概念和洞见，作者提出了**SIREN(Selective Entropy Regularization)**方法，核心思想是将探索限制在更有效的动作和状态子集中。\n\n### 5.1 Top-p掩码：解决动作空间问题\n- **机制**：将探索限制在策略核内，即累积概率达到阈值p的最小令牌子集。\n- **实现**：重新定义概率分布，仅在该子集内计算熵，避免在无意义令牌上分散概率质量。\n\n### 5.2 Peak-entropy掩码：解决轨迹问题\n- **机制**：选择熵位于轨迹内τ-分位数以上的令牌作为关键令牌。\n- **实现**：仅在这些选定的关键令牌上聚合熵，避免在长轨迹中所有位置均匀增加熵。\n\n### 5.3 自锚定正则化：稳定训练\n- **动机**：朴素熵正则化（最大化熵）在LRM环境中容易陷入两个极端：过高导致全局爆炸，过低无法缓解过早收敛。\n- **创新**：将正则化定义为聚合熵与熵锚之间的均方误差(MSE)，并将熵锚初始化为模型初始状态的熵值。\n- **优势**：这种自锚定机制使模型保持适当的探索水平，既不导致爆炸也不陷入过早收敛。\n\n## 6. 实验验证：从理论到实践\n\n作者通过广泛的实验验证了SIREN的有效性：\n\n### 6.1 性能提升\n- 在五个数学基准测试上，SIREN优于之前的熵相关RLVR方法。\n- 在AIME24/25上，使用Qwen2.5-Math-7B实现了+6.6 maj@k的显著改进。\n- 在不同规模的模型上（7B、1.5B）和不同架构的模型上（Qwen、LLaMA）均表现出一致的改进。\n\n### 6.2 机制分析\n- **探索促进**：SIREN实现了更高的pass@k和困惑度(PPL)，表明它有效促进了更广泛的探索。\n- **训练动态**：SIREN在训练早期保持高熵水平，然后逐渐收敛，确保持续探索同时保持性能。\n- **消融研究**：移除任何单个组件都会导致性能下降，证实了三个组件的互补性和必要性。\n\n## 7. 逻辑演进总结\n\n作者提出SIREN方法的逻辑链展现了从宏观问题到具体解决方案的系统性思考过程：\n\n1. **问题识别**：LRMs中的熵崩溃和过早收敛\n2. **方案评估**：传统熵正则化在LRM中的失效\n3. **根源分析**：巨大动作空间和长轨迹导致熵爆炸\n4. **概念提炼**：策略核和关键令牌的概念构建\n5. **方案设计**：SIREN的三组件选择性熵正则化\n6. **实验验证**：性能提升和机制分析\n\n这一逻辑链条体现了作者从观察现象、分析本质、提炼概念到设计解决方案的完整科研思维过程，为解决大型推理模型中的探索问题提供了新的思路和方法。", "summary_translation": "可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)在提升大推理模型(Large Reasoning Models, LRM)的推理能力方面展现出巨大潜力。然而，该方法面临一个关键问题：熵崩溃(entropy collapse)和过早收敛(premature convergence)。朴素熵正则化(naive entropy regularization)是传统强化学习文献中鼓励探索的常用方法，但在大推理模型(LRM)的背景下无法解决这一问题。我们的分析表明，这种失败源于大推理模型(LRM)中巨大的动作空间(action space)和长轨迹(long trajectories)，当模型不加选择地探索所有可能的动作和状态时，容易引发全局熵爆炸(global entropy explosion)。\n\n为解决此问题，我们提出了SIREN(SelectIve entRopy rEgulatioN，选择性熵正则化)，该方法将探索限制在有意义的动作和状态子集上。SIREN通过两步熵掩码机制(entropy masking mechanism)实现这一目标，该机制包括top-p掩码(top-p mask)和峰值熵掩码(peak-entropy mask)。此外，正则化被转换为自锚定形式(self-anchored form)以稳定训练过程。在五个数学基准测试(mathematical benchmarks)中，SIREN相比以往与熵相关的RLVR方法取得了更优的平均性能，以在Qwen2.5-Math-7B模型上AIME24/25测试中maj@k指标提升6.6%为例。进一步分析证实，SIREN促进了更大的响应多样性(response diversity)，并将熵维持在适当水平，这有助于在整个训练过程中保持验证pass@k指标。这有效缓解了大推理模型(LRM)中RLVR常见的过早收敛问题。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#198", "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "link": "/arxiv/2509.25140", "arxiv_id": "2509.25140", "authors": "Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister", "summary": "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.926657", "filter_reason": "根据筛选标准，我判断这篇论文符合研究目标，理由如下： 第一步核心判断：这篇论文的本质是提出ReasoningBank，一种新的记忆框架，用于增强大语言模型代理的推理能力。论文核心关注如何让代理从自身经验中提炼可泛化的推理策略，并通过记忆机制实现自我进化，这明显属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的范畴，因此应该保留。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确关注\"large language model agents\" - 能力方向：聚焦于\"reasoning strategies\"和\"generalizable reasoning\" - 训练方法：涉及\"self-evolving\"概念 - 新兴范式：研究\"llm-based agents\"及其记忆机制 第三步排除标准：论文没有主要聚焦于任何排除领域。虽然提到了\"web browsing and software engineering benchmarks\"，但这些仅是评估方法的基准测试，而非论文主要焦点。 第四步特殊情况处理：论文提出的是通用的智能体记忆框架，用于增强LLM的通用推理能力和自我进化能力，而不是将智能体应用于特定领域，因此符合保留条件。 综合来看，这篇论文的核心贡献是提出了一种通过记忆机制来增强大语言模型代理通用推理能力的方法，使代理能够从经验中学习并自我进化，这与研究目标\"提高大语言模型的通用推理能力\"高度契合。", "summary2": "本文旨在解决大型语言模型代理无法从积累的交互历史中学习的问题。针对连续任务流场景，我们提出了一种ReasoningBank记忆框架，从代理自判断的成功和失败经验中提炼可泛化推理策略，并引入记忆感知测试时扩展(MaTTS)来加速学习过程。在WebArena、Mind2Web和SWE-Bench-Verified基准上，通过成功率和交互步骤数验证了其有效性，相比现有方法在有效性上提升达34.2%，效率上减少16.0%交互步骤。", "inspiration_trace": "# ReasoningBank核心方法的逻辑推演\n\n## 宏观问题：代理系统的经验学习困境\n\n作者首先观察到，随着大型语言模型(LLM)代理在持久现实世界角色中广泛应用，它们面临一个根本性问题：**代理无法从累积的交互历史中学习**。这导致它们不断重复过去的错误，丢弃有价值的见解，缺乏随时间推移而自我进化的能力。这一观察构成了研究的出发点。\n\n## 问题具体化：现有记忆机制的局限\n\n作者将宏观问题具体化为两个关键挑战：\n\n1. **内容局限性**：现有代理记忆方法主要存储原始轨迹或仅成功的工作流程，缺乏提炼更高层次、可转移的推理模式的能力。\n   \n2. **经验利用不均衡**：现有方法过度强调成功经验，忽视了失败经验中蕴含的宝贵教训，而这些失败教训恰恰是避免未来错误的关键。\n\n这一具体化揭示了问题的本质：不是代理没有记忆，而是记忆的质量和利用方式存在根本缺陷。\n\n## 核心假设：推理记忆的价值\n\n基于以上观察，作者形成了三个核心假设：\n\n1. **假设一**：如果能够从成功和失败经验中提炼出可泛化的推理策略，而不仅仅是存储原始轨迹，代理将能更有效地解决新任务。\n   \n2. **假设二**：如果建立一个闭环记忆系统，使代理能够检索相关记忆指导行动，并将新经验整合回记忆系统，代理将能持续进化。\n   \n3. **假设三**：如果通过增加计算资源生成多样化经验，将为合成更高质量记忆提供丰富的对比信号，形成记忆与扩展的协同效应。\n\n## 方法论发展：从理论到实践\n\n### ReasoningBank的设计\n\n基于假设一和二，作者设计了ReasoningBank记忆框架：\n\n1. **结构化记忆模式**：每个记忆项包含标题、描述和内容三部分，将原始经验提炼为可转移的推理单元，既保留高级策略，又去除低级执行细节。\n\n2. **闭环记忆流程**：\n   - **记忆检索**：代理根据当前查询检索相关记忆项\n   - **记忆构建**：任务完成后，从新经验中提取记忆项\n   - **记忆整合**：将新记忆项整合到ReasoningBank中\n\n3. **成功与失败经验的平衡利用**：通过LLM-as-a-judge判断经验成败，从成功中提取有效策略，从失败中提取预防性教训，形成全面的记忆库。\n\n### MaTTS的引入\n\n基于假设三，作者进一步提出记忆感知的测试时扩展(MaTTS)：\n\n1. **并行扩展**：为同一查询生成多个轨迹，通过对比不同轨迹识别一致的成功模式，过滤掉虚假解决方案。\n   \n2. **顺序扩展**：在单轨迹内迭代改进推理，将中间推理过程也作为记忆信号，丰富记忆内容。\n\n3. **记忆与扩展的协同**：高质量记忆引导扩展探索向更有希望的方向，而多样化经验又锻造更强记忆，形成正反馈循环。\n\n## 验证与优化：实验支持\n\n作者通过在Web浏览和软件工程基准测试上的广泛实验验证了方法的有效性：\n\n1. **ReasoningBank的优越性**：在有效性(相对提高34.2%)和效率(交互步骤减少16.0%)方面均优于现有方法。\n   \n2. **MaTTS的放大效应**：进一步提高了性能，特别是在ReasoningBank基础上效果最佳。\n   \n3. **协同效应验证**：实验证明记忆与测试时扩展之间存在强大的协同作用，验证了假设三。\n\n## 总结：从问题到解决方案的逻辑演进\n\n作者从观察到的代理无法从过去经验中学习的宏观问题出发，通过具体化现有方法的局限，形成关于推理记忆价值的假设，进而发展出ReasoningBank和MaTTS这一创新方法，最终通过实验验证了其有效性。这一逻辑链条体现了从问题观察到假设形成，再到方法论发展和实验验证的完整科研思路，为构建自适应和终身学习的代理系统提供了新途径。", "summary_translation": "随着大型语言模型代理（large language model agents）在持续现实世界角色（persistent real-world roles）中的应用日益广泛，它们自然会遇到连续的任务流（continuous streams of tasks）。然而，一个关键限制是它们无法从累积的交互历史中学习，导致它们被迫丢弃有价值的见解并重复过去的错误。我们提出了ReasoningBank，一种新颖的记忆框架（memory framework），它从代理自我判断的成功和失败经验（self-judged successful and failed experiences）中提炼出可泛化推理策略（generalizable reasoning strategies）。在测试时间（test time），代理从ReasoningBank检索相关记忆以指导其交互，然后将新的学习成果整合回去，使其能够随时间推移变得更加能干。\n\n基于这个强大的经验学习器，我们进一步引入了记忆感知的测试时间扩展（memory-aware test-time scaling, MaTTS），它通过扩展代理的交互经验（interaction experience）来加速和多样化这一学习过程。通过为每个任务分配更多计算资源，代理生成丰富多样的经验，这些经验为合成更高质量的记忆提供了丰富的对比信号（contrastive signals）。更好的记忆反过来指导更有效的扩展，在记忆和测试时间扩展之间建立了强大的协同作用。\n\n在网页浏览和软件工程基准测试（web browsing and software engineering benchmarks）中，ReasoningBank持续优于存储原始轨迹（raw trajectories）或仅存储成功任务例程（successful task routines）的现有记忆机制，同时提高了有效性和效率；MaTTS进一步放大了这些收益。这些发现将记忆驱动的经验扩展（memory-driven experience scaling）确立为一个新的扩展维度，使代理能够自我进化，并自然产生涌现行为（emergent behaviors）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#207", "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training", "link": "/arxiv/2509.24923", "arxiv_id": "2509.24923", "authors": "Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra", "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.934900", "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断 这篇论文的本质是研究如何通过监督微调(SFT)和强化学习(RL)两种训练范式来提升大型语言模型(LLM)在序列决策任务中的探索能力。论文聚焦于多臂老虎机这一经典决策问题，提出了新的奖励设计方法来优化LLM的训练过程，并分析了不同训练方法对LLM行为的影响。这明显属于\"改进LLM的基础能力、提出新的训练范式\"的范畴，特别是增强了LLM在决策制定和问题解决方面的通用能力，符合保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：研究sequential decision-making（序列决策），这涉及到planning和problem-solving能力，是推理能力的重要体现 - 训练方法：重点研究reinforcement learning (RL)作为训练LLM的方法 - 新兴范式：提到LLMs成为autonomous agents（自主智能体）的潜力，与llm-based agents相关 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 研究的是通用的序列决策问题（多臂老虎机任务），而非特定应用领域 - 没有主要聚焦于水印、安全性等模型可靠性问题 第四步：特殊和模糊情况 论文涉及通用智能体框架研究，讨论如何通过训练提高LLM作为自主智能体的决策能力，而不是将智能体应用于特定领域，因此应该保留。 综合判断：这篇论文的核心贡献在于研究如何通过强化学习等训练方法提升LLM在决策制定中的通用能力，探索-利用平衡是推理和决策的关键组成部分。论文提出的奖励设计方法和训练策略分析直接服务于提升LLM的通用推理能力，因此完全符合我的研究目标。", "summary2": "本文旨在解决大型语言模型在多臂赌博机任务中探索-利用权衡的问题。针对多臂赌博机任务，我们提出了一种通过监督微调和强化学习训练LLM的方法，设计了战略奖励和算法奖励两种新颖奖励信号，并在多种Gaussian和Bernoulli赌博机环境中通过累积遗憾、时间平均奖励和最优臂选择频率验证了其有效性。", "inspiration_trace": "# 从观察到方法：LLM探索能力研究的逻辑演进\n\n## 1. 宏观问题：LLM在序列决策中的探索困境\n\n**观察起点**：大型语言模型(LLM)展现出作为自主智能体的潜力，但在序列决策任务中存在系统性缺陷——它们往往探索不足，倾向于短视的贪婪行为，过度利用已知奖励而牺牲探索。\n\n**具体表现**：在多臂赌博机(MAB)这一经典探索-利用权衡测试平台上，LLM即使经过精心提示，仍表现出次优的探索偏差，这成为其成为真正自主智能体的主要障碍。\n\n## 2. 现有方法与知识缺口\n\n**已有尝试**：研究者提出了两种主要训练范式来改善LLM的探索能力：\n- 监督微调(SFT)：通过专家轨迹训练，模仿最优探索算法(如UCB)\n- 强化学习(RL)：直接从环境奖励中学习有效策略\n\n**关键缺口**：作者识别出两个核心问题：\n1. **机制不明**：SFT和RL如何塑造LLM的探索策略尚不清楚\n2. **泛化未知**：这些策略如何泛化到更长视野和分布外环境缺乏研究\n\n## 3. 研究假设的形成\n\n基于上述缺口，作者提出了三层假设：\n\n**假设1(性能)**：SFT和RL训练能显著提升LLM在MAB任务上的探索能力，达到接近理论最优算法(如UCB、Thompson Sampling)的性能。\n\n**假设2(泛化)**：训练后的策略能够泛化到比训练环境更长的时间跨度和不同的奖励分布。\n\n**假设3(行为)**：仅关注平均遗憾会掩盖重要行为特征，训练可能导致策略偏向利用而牺牲长期鲁棒性。\n\n## 4. 方法设计：从比较到创新\n\n### 4.1 训练范式比较框架\n\n作者构建了一个系统性的比较框架，同时研究SFT和RL两种范式：\n\n**SFT方法**：使用UCB生成的专家轨迹进行监督学习，模型学习显式计算UCB值并做出决策。\n\n**RL方法**：采用PPO算法，将LLM决策过程建模为分层MDP，在token级别优化策略。\n\n### 4.2 奖励信号的创新设计\n\n为解决RL中的信用分配问题，作者设计了三种递进的奖励信号：\n\n**原始奖励(RL-OG)**：直接使用MAB环境的随机奖励\n- **局限**：奖励内在随机性导致信用分配困难和学习效率低下\n\n**策略奖励(RL-STR)**：基于即时遗憾设计\n- **创新点**：$\\tilde{r}_t = \\frac{\\mu_{A_t} - \\min_i \\mu_i}{\\mu^* - \\min_i \\mu_i} \\in [0, 1]$\n- **优势**：直接优化动作效用，简化信用分配，减少方差\n\n**算法奖励(RL-ALG)**：基于专家算法的二元信号\n- **创新点**：$r_t = 1$ 如果代理动作匹配神谕决策，否则为0\n- **优势**：绕过基于回报的信用分配，使代理自由发现内部算法\n\n### 4.3 评估方法的拓展\n\n超越传统的累积遗憾指标，作者引入了更丰富的行为诊断工具：\n\n- **贪婪频率**：选择贪婪动作的相对频率\n- **后缀失败率**：代理在时间t后不再选择最优臂的频率\n- **最佳臂频率**：选择最优臂的比例\n\n## 5. 实验验证与核心发现\n\n### 5.1 性能与泛化验证\n\n**发现1**：SFT和RL均能提升基础模型性能，RL-ALG和SFT表现最佳，甚至超过UCB教师策略。\n\n**发现2**：训练后的模型展现出强大的泛化能力，能够适应6倍长的视野和跨分布环境，RL策略在跨分布转移上比SFT更可靠。\n\n### 5.2 行为分析的意外发现\n\n**发现3**：训练后的模型表现出更强的贪婪倾向，虽然提高了平均回报，但牺牲了长期鲁棒性。\n\n**发现4**：模仿UCB学习的代理通过采用更贪婪的变体来超越其\"教师\"，揭示了训练过程中的\"利用偏差\"。\n\n**发现5**：RL-ALG发现了UCB的近似变体(如$Q_t(a) + C \\times \\sqrt{\\frac{\\log(N_t(a)+1)}{N_t(a)}}$)，混合了模仿和机会主义利用。\n\n## 6. 核心洞见：利用偏差的涌现机制\n\n作者最终揭示了\"利用偏差\"的涌现机制：\n\n**根本原因**：训练数据中探索信号稀疏而利用信号频繁，加上复杂的信用分配问题，导致训练过程偏向于优化短期奖励。\n\n**表现形式**：学习到的策略虽然平均性能优秀且泛化能力良好，但更容易出现早期灾难性失败，过早放弃探索。\n\n**理论意义**：这一发现挑战了仅通过平均遗憾评估策略充分性的传统观点，强调了在LLM训练中平衡探索与利用的重要性。\n\n## 7. 方法论贡献与启示\n\n作者的研究不仅提供了比较SFT和RL训练范式的系统框架，还提出了创新的奖励设计方法，特别是算法奖励(RL-ALG)为解决LLM中的信用分配问题提供了新思路。\n\n最终，这项研究倡导超越平均遗憾的定制奖励设计和评估方法，以促进稳健的探索行为，为未来LLM在序列决策任务中的训练提供了重要指导。", "summary_translation": "尽管大型语言模型（Large Language Models, LLMs）有望成为自主代理（autonomous agents），但它们在顺序决策（sequential decision-making）中通常进行次优探索（suboptimally explore）。近期研究试图通过监督微调（supervised fine-tuning, SFT）或强化学习（reinforcement learning, RL）来增强这一能力，在经典的多臂赌博机（multi-armed bandit）任务上改善了遗憾（regret）。然而，这些学习方法如何塑造探索策略及其泛化能力仍不明确。我们通过两种方式研究这两种范式：一是使用专家轨迹（expert trajectories）对LLMs进行SFT训练，二是采用一系列定制的奖励信号（tailored reward signals）进行RL训练，包括用于减少方差的战略性、基于遗憾的奖励（regret-shaped reward），以及能够实现神谕模仿（oracle imitation）的算法奖励（algorithmic reward）。由此产生的代理（agents）性能优于预训练模型，并与上置信界（Upper Confidence Bound, UCB）和汤普森采样（Thompson Sampling）相当，且能够稳健地泛化到6倍更长的时间范围（horizons）以及不同的赌博机家族（bandit families）。行为分析（behavioral analysis）表明，性能提升通常源于更复杂但更贪婪的利用（exploitation）：与预训练模型相比，RL/SFT代理更容易发生早期灾难性失败（early catastrophic failure），过早放弃探索。此外，被训练来模仿UCB的代理通过采用更具剥削性的变体（exploitative variants）学会了超越其\"教师\"（teacher）。我们的研究结果阐明了每种训练范式在何种情况下更为可取，并倡导超越平均遗憾（average regret）的定制奖励设计和评估，以促进稳健的探索行为。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#212", "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity", "link": "/arxiv/2509.24836", "arxiv_id": "2509.24836", "authors": "Zhen Bi, Zhenlin Hu, Jinnan Yang, Mingyang Chen, Cheng Deng, Yida Xue, Zeyu Yang, Qing Shen, Zhenfang Liu, Kang Zhao, Ningyu Zhang, Jungang Lou", "summary": "Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data.Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.", "subjects": "Artificial Intelligence, Computation and Language, Machine Learning", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.936874", "filter_reason": "这篇论文完全符合我的研究目标。从核心判断来看，论文的本质是改进LLM的基础逻辑推理能力，而非将其作为工具应用于特定领域。论文的核心贡献是提出了\"数据推理强度\"(DRI)这一新指标和相应的\"重新认知优化策略\"，通过优化训练数据的逻辑推理复杂性来提升LLM的逻辑推理能力，而不是简单增加数据量或改变数据格式。从正面指标看，论文明确研究大型语言模型(LLMs)，特别关注逻辑推理能力，并在强化学习框架下验证了方法。论文不涉及任何排除标准中的多模态与视觉、特定应用领域或模型可靠性的应用层面研究。这是一篇典型的致力于提高大语言模型本身通用推理能力的研究，通过新的训练范式增强模型的逻辑推理能力，与我的研究目标高度一致。", "summary2": "本文旨在解决大语言模型逻辑推理能力受限于训练数据利用不足的问题。针对训练数据内部推理复杂性被忽视的现状，我们提出了一种数据推理强度(DRI)量化指标及再认知优化策略，并在Reclor、LogicBench、LogiQA和LogiQA2.0等逻辑推理基准上通过准确率等指标验证了其有效性。", "inspiration_trace": "# 从观察到方法论：数据推理强度(DRI)的提出逻辑链\n\n## 宏观问题：LLMs逻辑推理能力的边界\n\n论文的出发点是一个宏观问题：如何突破大型语言模型(LLMs)的逻辑推理能力边界。作者观察到，尽管LLMs在逻辑推理任务上取得了显著进展，但如何从数据角度优化模型认知能力仍是一个核心挑战。\n\n## 观察一：现有方法的根本局限\n\n作者首先发现，当前数据优化方法存在本质瓶颈：\n- 现有方法仅停留在数据表达形式的表面转换\n- 忽视了对数据内部推理复杂性的深入探索\n- 面临两大挑战：缺乏对数据逻辑推理复杂性的高效量化，以及从模型推理边界角度分析数据的盲目性\n\n## 观察二：数据复杂性与模型性能的关系\n\n进一步研究表明：\n- 数据复杂性只有在结构合理、接近模型经验且处于其推理边界附近时，才能有效刺激LLMs的推理能力\n- 训练数据的质量结构对推理能力起决定性作用，精确数据选择优于简单数据增量\n- 当前LLMs远未充分利用可用推理数据的潜力，存在显著性能提升空间\n\n## 核心假设：双边界约束理论\n\n基于上述观察，作者提出核心假设：\n- LLM的逻辑推理性能受两个边界共同约束：训练数据的潜力和模型本身的认知潜力\n- 当前LLMs既未达到数据潜力的上限，也未触及自身认知能力的边界\n\n## 理论框架：推理效率模型\n\n受Roofline模型启发，作者构建了理论框架：\n- 将LLM逻辑推理性能视为数据驱动的推理潜力与模型内在认知成本的效率比率\n- 定义有效推理能力η(M, D) = E(D)/C(M)\n- 由于模型认知成本C(M)难以改变，提高性能的最实际路径是通过精心设计的数据来增强数据推理潜力E(D)\n\n## 方法一：数据推理强度(DRI)的提出\n\n为量化E(D)，作者提出了数据推理强度(DRI)指标：\n1. **分解阶段**：将每个样本的推理轨迹分解为逻辑元素（常量、谓词和逻辑表达式）\n2. **组合推理阶段**：基于分解的逻辑元素进行结构推理，生成推理步骤序列\n3. **量化计算**：\n   - 逻辑强度：从结构复杂性、谓词和常量数量三个维度计算\n   - 推理强度：结合前提条件强度和逐步演绎强度\n   - 归一化处理：应用对数压缩和sigmoid归一化，将结果映射到[0,1]区间\n\n## 方法二：重新认知优化策略\n\n基于DRI指标，作者提出了两阶段优化策略：\n\n1. **模型认知重塑阶段**：\n   - 根据DRI分数重新排序训练数据\n   - 让模型首先探索全谱系的DRI示例，建立广泛推理模式\n   - 受Sweller认知负荷理论启发，最小化外在认知负荷\n\n2. **认知推理增强阶段**：\n   - 根据归一化DRI分数引导模型注意力分配\n   - 使模型更关注高DRI样本，获得更大学习回报\n   - 基于资源理性分析，在有限资源下最大化推理收益\n\n## 实验验证与发现\n\n通过系统实验，作者验证了：\n- DRI能有效分离样本的潜在推理潜力，且与模型错误率呈正相关\n- 低DRI数据可安全修剪，高DRI数据是性能提升的催化剂\n- 重新认知优化策略在不同数据集和DRI区间上均显著降低错误率\n- 该方法在监督学习和强化学习框架下均有效\n\n## 结论：从数据复杂性到推理边界突破\n\n论文最终形成完整逻辑链：通过量化数据推理复杂性(DRI)，发现当前LLMs远未达到数据潜力和自身认知能力的边界，进而提出重新认知优化策略，系统性地增强训练数据的逻辑推理强度，从而推动LLMs接近其逻辑推理性能的理论边界。这一工作为理解和激活LLMs训练数据的未开发推理潜力提供了新视角。", "summary_translation": "大型语言模型（large language models, LLMs，大型语言模型）的最新进展凸显了训练数据结构和质量在塑造推理行为中的重要性。然而，大多数现有方法专注于转换数据格式，而忽视了训练样本的内部推理复杂性（internal reasoning complexity，内部推理复杂性），导致数据的推理潜力（reasoning potential，推理潜力）未被充分探索和利用。在本研究中，我们提出LLM逻辑推理性能（logical reasoning performance，逻辑推理性能）受到训练数据潜力和模型认知能力（cognitive capacity，认知能力）的共同约束。为了使这种关系可测量，我们引入了数据推理强度（Data Reasoning Intensity, DRI，数据推理强度），这是一种通过分解和聚合样本的逻辑结构来量化其潜在逻辑推理复杂性的新指标。这使我们能够分析当前LLMs如何有效利用逻辑推理信号（logical reasoning signals，逻辑推理信号），并识别相对于数据潜力的性能差距。基于这一见解，我们提出了一种再认知优化策略（re-cognizing optimization strategy，再认知优化策略），系统性地增强训练数据的逻辑推理强度。我们的方法不是增加数据量，而是重新优化现有样本，使其更好地与LLM的逻辑推理边界（logical reasoning boundary，逻辑推理边界）对齐。大量实验表明，我们的方法在性能和泛化能力方面显著优于以数据为中心的策略（data-centric strategies，以数据为中心的策略）。我们进一步在强化学习框架（reinforcement learning framework，强化学习框架）下验证了我们的方法。我们的结果表明，优先考虑数据中的推理复杂性（reasoning complexity，推理复杂性）而非纯粹规模或表面形式，对于实现LLMs的全部认知潜力（cognitive potential，认知潜力）至关重要。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#214", "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries", "link": "/arxiv/2509.24711", "arxiv_id": "2509.24711", "authors": "Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, Han Qiu", "summary": "Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.937643", "filter_reason": "这篇论文完全符合我的研究目标，核心贡献是研究大型推理模型(LRMs)对其能力边界的自我认知能力，并提出优化策略来提升模型的推理效率和可靠性。 首先，从核心判断角度看，论文本质上是关于改进LLM的基础推理能力的。论文研究的是模型如何认知自身能力边界，并提出两种优化策略（推理表达式监控和隐藏状态监控）来避免无效推理，这直接提升了模型的通用推理能力，而不是将LLM作为工具应用到特定领域。 其次，从正面指标看，论文明确包含核心概念\"Large Reasoning Models (LRMs)\"，并聚焦于\"reasoning\"能力方向，特别是数学推理等复杂推理任务。虽然论文没有明确提到强化学习等训练方法或智能体等新兴范式，但其研究内容与提升LLM通用推理能力直接相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域，也不关注模型可靠性的应用层面（如水印、安全等）。 最后，在特殊情况下，论文关注模型对自身能力边界的认知，这可以视为一种提高模型内在可靠性和推理质量的方法，与减少幻觉和提高可解释性有一定关联，应该保留。 综上所述，这篇论文的核心贡献是通过研究模型对自身能力边界的自我认知，提出方法来优化推理过程，提高效率和可靠性，完全符合\"提高大语言模型通用推理能力\"的研究目标。", "summary2": "本文旨在研究大型推理模型(LRMs)是否具有对其能力边界的自我意识。针对LRMs在处理超出能力边界问题时进行无效推理的现象，我们提出了推理表达监测和隐藏状态监测两种边界感知策略，并在多个数学推理benchmark上通过准确率、硬拒绝率、token使用量和溢出率等指标验证了其有效性。实验表明这些策略使LRMs能够避免无效推理，将token使用量减少62.7%-93.6%，同时保持准确性。", "inspiration_trace": "# 大型推理模型能力边界自我意识的逻辑演进\n\n## 一、宏观问题：非生产性推理的困境\n\n作者从观察到一个普遍现象出发：大型推理模型(LRMs)在面对超出其能力范围的问题时，会陷入非生产性推理，持续生成内容直到上下文限制，不仅产生错误答案，还浪费大量计算资源。这一现象揭示了当前LRMs回答范式的根本缺陷——模型无法识别问题与其自身能力边界的关系，导致对不可解问题徒劳求解。\n\n## 二、核心假设：能力边界的自我意识\n\n基于上述问题，作者提出核心研究问题：**LRMs是否具有对其能力边界的自我意识？** 即模型是否\"知道\"自己不能解决某些问题。这一假设源于作者观察到的初步现象：LRMs在推理过程中表现出的人类化语气(如\"我不确定\"、\"可能是错的\")往往与最终答案的正确性高度一致。\n\n## 三、初步验证：表达信念与答案正确性的关联\n\n作者首先通过量化分析验证假设：\n1. **构建指标**：定义Can%和Cannot%两个指标，分别衡量模型表达信心时答案正确的概率和表达不确定性时答案错误的概率\n2. **实验发现**：当LRMs表达更多不确定性时，最终答案几乎总是错误的(Cannot% = 100%)\n3. **结论**：LRMs确实表现出对自身能力边界的认知，它们\"知道\"自己不能解决什么问题\n\n## 四、黑盒视角：推理表达轨迹分析\n\n假设得到初步验证后，作者从黑盒角度深入探究能力边界如何通过外部可观察的推理表达体现：\n\n1. **表达轨迹构建**：\n   - 识别自信表达(如\"我认为这是正确的\")和不确定表达(如\"我不确定\")\n   - 将推理过程分阶段，计算各阶段表达密度，形成表达密度轨迹\n\n2. **关键发现**：\n   - 可解问题：自信轨迹呈加速增长趋势(凹形)\n   - 不可解问题：不确定轨迹主导并趋于收敛(凸形)\n\n3. **量化指标设计**：\n   - 置信度差异(ConfDiff)：测量不确定与自信轨迹差距的累积符号\n   - 置信度曲率(ConfCurv)：计算表达密度轨迹的二阶导数\n\n4. **验证结果**：这些指标能在推理早期(如2%阶段)高精度区分可解与不可解问题\n\n## 五、白盒视角：隐藏状态中的边界编码\n\n进一步，作者探究能力边界信号是否在推理开始前就已存在于模型内部：\n\n1. **隐藏状态提取**：聚焦最后一个输入token的隐藏状态，它封装了整个问题的语义信息\n\n2. **实验设计**：\n   - 扩展数据集，包含简单(GSM8K)和极难(HLE)问题，放大能力边界差异\n   - 应用线性分类器(LDA、逻辑回归)测试可解与不可解问题的可分性\n\n3. **关键发现**：\n   - 可解与不可解问题在隐藏空间中几乎线性可分(准确率近100%)\n   - 接近能力边界的可解问题需要更长推理时间(1.5-2倍token使用)\n\n4. **结论**：能力边界信息在推理开始前就已编码在隐藏状态中\n\n## 六、方法设计：边界感知的推理策略\n\n基于黑盒和白盒分析的发现，作者设计两种测试时监测策略：\n\n### 1. 推理表达监测(Monitor_express)\n- **适用场景**：黑盒环境\n- **实施流程**：\n  1. 实时构建自信与不确定表达轨迹\n  2. 使用ConfDiff/ConfCurv指标判断问题是否超出能力边界\n  3. 对超出边界的问题提前终止推理，重新提示模型输出简洁方法\n\n### 2. 隐藏状态监测(Monitor_hidden)\n- **适用场景**：白盒环境\n- **实施流程**：\n  1. 提取最后一个输入token的隐藏状态\n  2. 使用预训练线性分类器预测问题是否可解\n  3. 对不可解问题约束输出以自我意识前缀开始，避免完整推理\n\n## 七、实验验证与效果评估\n\n作者通过系统实验验证所提策略的有效性：\n\n1. **评估指标**：\n   - 准确率(ACC)：衡量可解问题的解决质量\n   - 硬性放弃(HA)：衡量识别不可解问题的能力\n   - Token使用量：反映推理效率\n   - 溢出率：识别不完整响应的比例\n\n2. **关键结果**：\n   - 两种策略均能保持可解问题的准确性(ACC几乎不变)\n   - 显著提高对不可解问题的识别能力(HA接近100%)\n   - 大幅减少token使用(降低62.7%-93.6%)和溢出率(降低高达100%)\n   - 不同模型对策略响应不同：有些擅长指令跟随，有些更容易受输出引导\n\n## 八、逻辑演进总结\n\n作者从观察到方法论的完整逻辑链展现了清晰的科学思维过程：\n1. **问题识别**：发现LRMs非生产性推理的普遍现象\n2. **假设形成**：提出LRMs可能具有能力边界自我意识\n3. **初步验证**：通过表达信念与答案正确性的关联验证假设\n4. **黑盒分析**：从推理表达轨迹揭示能力边界信号\n5. **白盒分析**：从隐藏状态发现更早的边界编码\n6. **方法设计**：基于双重视角设计边界感知推理策略\n7. **实验验证**：证明策略在保持准确性的同时显著提高效率和可靠性\n\n这一研究不仅揭示了LRMs的内在工作机制，还为解决实际应用中的效率问题提供了有效方法，体现了从现象观察到理论创新再到实用解决方案的完整科研路径。", "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在数学等复杂推理任务上表现出令人印象深刻的性能，但它们也表现出一些暴露其局限性的不当行为。特别是，当面对难题时，LRMs经常进行无效推理直到达到上下文限制（context limit），产生错误答案的同时浪费大量计算资源。这一现象反映了一个根本问题：当前的回答范式（answering paradigms）忽略了问题与LRMs能力边界（capability boundaries）之间的关系。在本文中，我们研究了LRMs是否具备对自身能力边界的自我意识。我们首先观察到，LRMs可能通过表达的推理置信度（reasoning confidence）来了解自己无法解决的问题。对于黑盒模型（black-box models），我们发现推理表达揭示了边界信号，可解决问题呈现加速增长的置信度轨迹（confidence trajectory），而不可解决问题则呈现收敛的不确定性轨迹（uncertainty trajectory）。对于白盒模型（white-box models），我们表明最后一个输入标记（input token）的隐藏状态（hidden states）编码了边界信息，可解决问题和不可解决问题甚至在推理开始前就是线性可分的。基于这些发现，我们提出了两种简单而有效的优化策略：推理表达监控（reasoning expression monitoring）和隐藏状态监控（hidden states monitoring）。实验证明，这些具有边界意识的策略使LRMs能够在不牺牲准确性的情况下避免无效推理，通过减少62.7%至93.6%的标记（token）使用量，显著提高了可靠性和效率。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#211", "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval", "link": "/arxiv/2509.24869", "arxiv_id": "2509.24869", "authors": "Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian", "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.", "subjects": "Information Retrieval, Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.936242", "filter_reason": "这篇论文的核心贡献是提出Retro*方法，通过基于标准的相关性评分机制和定制的强化学习算法来优化LLM的推理能力。根据筛选标准，我判断这篇论文符合研究目标，原因如下： 首先，从本质上看，这篇论文的核心是改进LLM的基础推理能力，而非将LLM作为工具应用到特定领域。论文提出了一种新的推理方法（基于标准的相关性评分机制）和强化学习算法来增强LLM的推理能力，特别是在处理需要细粒度推理的文档检索任务时。 其次，论文包含多个正面指标：明确关注LLMs的reasoning能力，采用了reinforcement learning作为训练方法，并提到了LLM agents和RAG等新兴范式。 第三，论文不主要聚焦于排除标准中的领域。虽然应用场景是文档检索，但这不是一个特定领域应用（如医疗、化学等），而是一个通用任务。论文的焦点是提高LLM的推理能力，而不是文档检索本身。 最后，虽然论文涉及文档检索这一应用场景，但其提出的方法（基于标准的相关性评分机制、测试时扩展的多个推理轨迹结合、定制的强化学习算法）都是通用的，旨在提高LLM的推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。 因此，这篇论文符合研究范围。", "summary2": "本文旨在解决需要密集推理的文档检索问题，特别是当文档与任务的连接是间接或隐含的情况。针对这种推理密集型检索场景，我们提出了一种Retro*方法，它采用基于rubric的相关性评分机制，并通过分数整合支持测试时扩展。我们在BRIGHT benchmark上通过nDCG@10指标验证了其有效性，结果表明Retro*优于现有文档检索方法，取得了最先进的性能。", "inspiration_trace": "# Retro*方法逻辑演进分析\n\n## 1. 宏观问题识别：推理密集型文档检索的挑战\n\n作者从当前信息检索领域的一个核心挑战出发：随着LLM智能体和RAG系统的普及，需要检索那些与任务有间接或隐含关联的文档，而非仅直接匹配的文档。这种需求在多个领域普遍存在：\n- 软件工程：需要找到与目标问题共享相似设计模式的程序\n- 数学：需要检索基于相同底层定理的证明，即使表达方式不同\n- 科学研究：需要找到能提供理论支持的文献\n\n这种检索任务需要细粒度的推理能力，而现有IR技术主要设计用于捕获直接语义关系，难以处理这种复杂推理需求。\n\n## 2. 现有方法局限性分析\n\n作者深入剖析了当前推理增强型IR方法的三个关键瓶颈：\n\n1. **缺乏相关性测量功能**：现有方法主要提供相对排序，无法提供下游任务所需的相关性绝对水平测量。\n\n2. **测试时扩展性不灵活**：现有方法专注于生成单一推理路径，忽略了探索和整合多个推理路径的潜力。\n\n3. **并行能力有限**：逐列表或逐集合的方法必须顺序处理整个候选集，在处理大量文档时效率低下。\n\n## 3. 核心假设形成\n\n基于问题分析和局限性研究，作者形成了四个关键假设：\n\n1. **假设1**：明确定义相关性评分标准可以引导LLM进行更细粒度推理，产生更准确、可解释的相关性评分。\n\n2. **假设2**：生成和整合多个推理轨迹可以提高相关性估计的可靠性和稳定性。\n\n3. **假设3**：点对点评估方法可以支持大规模并行处理，提高推理效率。\n\n4. **假设4**：专门设计的强化学习算法可以同时优化模型准确评分和有效区分文档的能力。\n\n## 4. 方法设计：Retro*的核心创新\n\n基于上述假设，作者设计了Retro*方法，包含两个核心组件：\n\n### 4.1 基于标准的相关性评分机制\n\n作者引入细粒度的相关性标准集，包含两部分：\n- **相关性定义**：声明检索任务的具体意图\n- **评分标准**：定义0-100分数范围及五个区间的明确解释（高度相关、相关、中度相关、轻微相关、不相关）\n\n这种设计使模型能够直接测量相关性，而非仅提供相对排序。\n\n### 4.2 通过分数整合实现测试时扩展\n\n为提高检索准确性，Retro*为每个查询-文档对采样K次，产生一组推理轨迹，并通过加权平均整合分数：\n```\n¯s ← Σ(wi * si) / Σwi\n```\n\n## 5. 训练策略：两阶段优化\n\n为充分发挥设计潜力，作者开发了两阶段训练策略：\n\n### 5.1 监督微调(SFT)阶段\n- **数据来源**：利用强大教师模型生成推理轨迹\n- **数据过滤**：通过分数整合筛选高质量轨迹，选择最接近整合分数的轨迹\n\n### 5.2 强化学习(RL)阶段\n引入复合奖励设计，同时优化两个功能：\n- **文档内奖励**：提高单个文档评分准确性，奖励接近整合分数的轨迹\n- **文档间奖励**：提高文档排序性能，奖励能正确区分相关与不相关文档的轨迹\n\n复合奖励公式：\n```\nR(y, s) = α · Rintra(y, s) + (1-α) · Rinter(y, s)\n```\n\n## 6. 实验验证与效果确认\n\n在BRIGHT基准测试上的实验验证了Retro*的有效性：\n\n1. **性能优势**：Retro*显著优于现有方法，实现了最先进的性能\n2. **测试时扩展效果**：多轨迹整合进一步提高了性能，7B模型+16次采样甚至超过了标准32B模型\n3. **并行效率**：点对点设计使其在处理大量候选文档时比其他方法更高效\n4. **相关性测量可靠性**：基于标准的评分机制提供了更清晰、可解释的相关性测量\n\n## 7. 逻辑演进总结\n\nRetro*方法的逻辑演进展示了从宏观问题到创新解决方案的完整思维链：\n\n**问题识别** → **局限性分析** → **假设形成** → **方法设计** → **训练策略** → **实验验证**\n\n这一演进过程体现了作者如何通过深入分析现有方法的不足，形成创新假设，设计出既能提供可解释相关性测量，又支持测试时扩展和高效并行的解决方案，最终通过实验验证其有效性，为推理密集型文档检索任务提供了新的方法论。", "summary_translation": "随着大型语言模型代理（LLM agents）和检索增强生成（RAG）的日益普及，检索对解决任务至关重要的文档变得越来越重要，即使这些文档与任务的连接是间接或隐含的。解决这一问题需要细粒度推理来准确评估任务与每个候选文档之间的相关性。然而，这种能力对现有信息检索（IR）技术构成了重大挑战。尽管推理增强的IR领域最近取得了进展，现有方法在适用性、可扩展性和效率方面仍面临显著挑战。在这项工作中，我们提出了Retro*，一种针对推理密集型文档检索的新方法。我们的方法引入了基于标准（rubric-based）的相关性评分机制，使模型能够根据明确定义的标准推理任务与文档之间的关系，从而产生细粒度、可解释的相关性分数。Retro*还通过分数整合（score integration）结合多个推理轨迹（reasoning trajectories）来支持测试时扩展（test-time scaling），从而产生更可靠的相关性估计。为了优化Retro*的推理能力，我们引入了一种针对其相关性评分机制定制的新型强化学习（reinforcement learning）算法，该算法采用两个复合奖励来充分利用每个训练样本的轨迹。我们的实验表明，Retro*以显著优势优于现有文档检索方法，在BRIGHT基准（BRIGHT benchmark）上实现了最先进的性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#221", "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention", "link": "/arxiv/2509.24393", "arxiv_id": "2509.24393", "authors": "Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu", "summary": "Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.940128", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围，理由如下： 第一步核心判断：这篇论文的本质是改进大型推理模型(LRMs)的安全推理能力。论文提出了一种新的训练范式\"干预偏好优化\"(IPO)，专门针对思维链(CoT)推理过程中可能存在的有害内容进行改进。这属于改进LLM基础能力的范畴，特别是增强其推理过程的安全性和可靠性，而不是将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题： - 核心概念：明确提到\"Large Reasoning Models (LRMs)\"，这是专注于推理能力的大语言模型 - 能力方向：核心关注\"reasoning\"，特别是\"chain-of-thought (CoT) reasoning\"，这正是通用推理能力的关键表现形式 - 训练方法：涉及\"process supervision\"和\"preference learning\"，这与强化学习方法相关 第三步排除标准：论文不涉及任何应排除的领域： - 不涉及多模态与视觉内容 - 不聚焦于任何特定应用领域（如医疗、化学等） - 虽然涉及安全性，但是从推理过程本身的角度，而非应用层面的水印或安全技术 第四步特殊情况处理：论文属于提出新方法来增强模型内在安全性的情况。作者提出的IPO方法通过改进推理过程的安全性来提升模型的通用可靠性和推理质量，这符合\"如果论文提出一种新方法来增强模型内在的安全性，从而提升模型的通用可靠性和推理质量，应该保留\"的标准。 核心贡献：论文提出了干预偏好优化(IPO)方法，通过将合规步骤替换为安全触发器，并构建偏好学习对来强制执行安全推理，从而提升大型推理模型在推理过程中的安全性。这种方法是通用的，旨在提高LLM的内在推理能力，特别是推理过程的安全性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型推理模型(LRMs)中的推理安全问题。针对LRMs在思维链推理中可能包含有害内容的问题，我们提出了一种Intervened Preference Optimization (IPO)方法，通过将符合性步骤替换为安全触发器并进行偏好学习，在越狱和对抗性安全基准上通过有害性比率等指标验证了其有效性，实现了推理和响应安全性的显著提升。", "inspiration_trace": "# 大型推理模型安全推理方法的逻辑演进\n\n## 一、宏观问题：推理过程中的安全隐患\n\n**出发点**：大型推理模型(LRMs)在解决复杂问题上取得了显著进展，但其链式思维(CoT)推理过程中常含有害内容，即使最终回答看似安全。这一问题在现有方法中被忽视，主要因为：\n- 现有安全对齐方法主要关注最终回答的安全性\n- 恶意用户可能访问并利用不安全的推理过程，带来潜在风险\n- 在医疗、金融、法律等关键领域部署时，这种隐患尤为危险\n\n## 二、关键观察：推理安全与回答安全的脱节\n\n**实证发现**：作者评估了现有安全对齐的LRMs(如RealSafe和STAR)，发现：\n1. 即使在简单恶意提示(JailbreakBench)上，最终回答通常安全，但推理过程仍显示不安全模式\n2. 在更具对抗性的基准测试(StrongReject和WildJailbreak)上，这种差距更为明显\n3. 安全推理往往导致安全回答，但安全回答不一定来自安全推理\n\n**核心洞察**：推理级别的安全性被现有方法忽视，但却是更可靠的安全路径。\n\n## 三、初步尝试与局限：强化学习的不足\n\n**假设验证**：作者尝试使用强化学习(GRPO)直接奖励安全推理，但发现：\n1. 虽然强调推理安全性有所改善，但绝对安全分数仍然有限\n2. 约50%的有害提示几乎无法生成安全推理轨迹，导致训练信号不足\n3. 低滚动多样性限制了强化学习的有效性\n\n**结论**：需要一种更有效的方法来增加安全推理的多样性并提供强训练信号。\n\n## 四、深入分析：推理过程中的安全动态\n\n**系统性研究**：作者分析了推理过程中安全性的演变模式，发现了三个关键特性：\n\n1. **安全触发步骤(Safety Triggers)**：\n   - 安全推理轨迹中存在关键步骤，之后安全延续概率接近100%\n   - 这些步骤通常包含明确承认风险、重构任务或调用安全指南的内容\n   - 超过90%的安全轨迹包含此类转折点\n\n2. **服从线索(Compliance Cues)**：\n   - 不安全推理轨迹中的特定步骤，表示倾向于遵守用户恶意请求\n   - 这些线索与不安全延续强烈相关(皮尔逊相关系数0.85)\n   - 一旦出现，模型越来越倾向于解决问题导向的思维链\n\n3. **干预效果(Corrective Interventions)**：\n   - 用安全触发步骤替代服从线索可显著降低延续的有害性\n   - 即使是最小干预也能有效将推理引导至安全方向\n   - 这种干预具有累积效应，可重复应用直到获得安全延续\n\n## 五、方法论形成：干预偏好优化(IPO)\n\n**核心思想**：基于上述发现，作者提出了干预偏好优化(IPO)，通过在关键安全步骤进行显式监督来巩固安全推理。\n\n**方法设计**：\n1. **干预生成**：\n   - 识别推理轨迹中的第一个服从线索\n   - 用从安全触发池中采样的安全触发步骤替代该线索\n   - 生成安全的推理延续\n\n2. **偏好构建**：\n   - 将原始轨迹和干预后的轨迹形成偏好对\n   - 这些对共享相同前缀但在干预步骤处出现分歧\n\n3. **偏好学习**：\n   - 使用直接偏好优化(DPO)在这些对上训练模型\n   - 鼓励模型偏好被纠正的安全延续而非原始不安全服从\n\n**技术优势**：\n- 通过主动干预增加滚动多样性，克服强化学习的局限性\n- 在关键步骤提供局部奖励信号，比稀疏最终奖励更高效\n- 最小化干预和分布内采样，限制安全对齐中的分布偏移\n\n## 六、验证与效果：安全与性能的双赢\n\n**实验结果**：\n1. **安全性提升**：\n   - 在多个安全基准测试上，推理有害率显著降低(相对减少超过30%)\n   - 最终回答的安全性也达到或超过现有基线\n   - 整体平均安全性表现最佳\n\n2. **能力保持**：\n   - 在数学、编程和科学推理等多项任务上保持甚至提高了核心推理能力\n   - 在安全性与实用性之间取得了良好平衡\n\n3. **效率优势**：\n   - 相比GRPO，计算成本显著降低(生成需求减少约65%)\n   - 训练时间大幅缩短(约40分钟 vs 2小时以上)\n\n## 逻辑演进总结\n\n从\"推理过程存在安全隐患\"的宏观问题出发，作者通过观察发现\"推理安全与回答安全脱节\"的现象，提出\"应优先考虑推理级别安全性\"的假设。在验证过程中，发现强化学习方法因低滚动多样性而效果有限，进而深入分析推理过程中的安全动态，识别出安全触发步骤、服从线索和干预效果三个关键特性。基于这些发现，最终形成了干预偏好优化(IPO)方法，通过在关键安全步骤进行显式监督，有效解决了大型推理模型的安全推理问题，同时保持了模型的推理能力。这一逻辑链条展示了从问题观察到方法形成的完整思维过程。", "summary_translation": "尽管大型推理模型 (Large Reasoning Models, LRMs) 在解决复杂问题方面取得了进展，但其思维链 (chain-of-thought, CoT) 推理经常包含有害内容，即使最终回答看起来安全，这些有害内容仍然可能存在。我们表明，在现有方法中这个问题仍然存在，这些方法忽视了安全推理的独特重要性，从而破坏了它们的可信度，并在不安全推理被恶意用户获取和利用的情况下，在应用中构成潜在风险。因此，在本文中，我们将注意力转向推理本身的安全性对齐，并探索过程监督 (process supervision) 作为解决方案。然而，由于低展开多样性 (rollout diversity) 和有限的训练信号，简单奖励安全推理被证明是不充分的。为了应对这一挑战，我们首先深入研究安全推理的特性，并发现了几个关键见解：1) 安全推理通常由几个关键的安全触发步骤 (safety triggers) 巩固；2) 顺从提示 (compliance cues) 与不安全延续强烈相关；3) 纠正干预 (corrective interventions) 能可靠地将不安全轨迹引导向更安全的轨迹。受这些见解的启发，我们提出了干预偏好优化 (Intervened Preference Optimization, IPO)，这是一种通过对齐方法，通过用安全触发器替代顺从步骤，并构建具有强信号的偏好学习 (preference learning) 对来强制执行安全推理。在越狱 (jailbreak) 和对抗性安全基准 (adversarial safety benchmarks) 上的实验表明，IPO显著提高了推理和回答的整体安全性，优于基于监督式微调 (SFT) 和强化学习 (RL) 的基线方法，有害性相对减少超过30%，同时在不同推理任务中保持优异性能。这些结果突显了推理显式对齐的重要性，并为构建更安全的大型推理模型提供了实用路径。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#230", "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit", "link": "/arxiv/2509.24248", "arxiv_id": "2509.24248", "authors": "Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen", "summary": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.", "subjects": "Artificial Intelligence, Computation and Language, Machine Learning", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.969370", "filter_reason": "这篇论文符合我的研究目标，应该被保留。根据筛选标准，我的判断过程如下： 第一步核心判断：论文的本质是关于改进大型推理模型(LRMs)的推理效率，解决\"过度思考\"问题。论文提出的SpecExit框架通过轻量级草稿模型预测未来token和提前退出信号，减少不必要的计算，同时保持准确性。这属于改进LLM基础能力和推理效率的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个正面指标。核心概念方面，它明确关注\"large reasoning models (LRMs)\"，这是LLMs的一个子集；能力方向方面，论文直接针对\"reasoning tasks\"和推理效率问题进行研究。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用推理模型的效率优化。 第四步特殊和模糊情况：论文不涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等问题。 论文的核心贡献是提出一种新的框架来加速大型推理模型的推理过程，减少计算开销同时保持准确性，这直接提升了大语言模型的通用推理能力，符合我的研究目标。", "summary2": "", "inspiration_trace": "`标记内的推理内容，识别有效推理轨迹(保留产生正确答案的最小推理段)\n- **信号标注**:\n  - 置信度: 预测步骤上logit概率的几何平均\n  - 剩余推理长度: 从初始``仍产生正确答案的token数\n  - 进度: 从0到1的归一化值，表示CoT的相对进展\n- **多任务学习**: 联合优化token分类和信号回归，使用动态加权平衡不同任务\n\n**3. 信号引导推理**\n- **停止条件**: 引入\"步骤分割token\"(如段落分隔符`\\n\\n`)作为自然分段点，当信号超过阈值时触发早期退出\n- **信号平滑**: 使用指数加权移动平均(EWMA)平滑信号，增强鲁棒性\n- **与推测解码集成**: 在推测解码流程中，目标模型不仅验证token，还提取隐藏状态生成早期退出信号\n\n#### 第六步：验证假设并优化方法\n\n作者通过实验验证SpecExit的有效性，并进行了一系列优化：\n\n**1. 主要结果**\n- 在多个基准测试(GSM8K、MATH500、AIME等)上，SpecExit显著减少推理轨迹(最多66%)\n- 实现高达2.5倍的端到端延迟加速\n- 保持与基线相当或更高的准确性\n\n**2. 消融研究**\n- **信号类型**: 比较单一信号与组合信号的效果，发现组合信号能平衡准确性和效率\n- **信号平滑**: 比较不同平滑策略，EWMA提供最稳定的平衡\n- **步骤分割token**: 比较段落分隔符与话语标记，发现段落分隔符更具通用性\n\n**3. 实际部署**\n- 在PyTorch和vLLM中实现SpecExit，证明其易于部署和实际应用价值\n\n#### 第七步：形成理论贡献和实践意义\n\n基于实验结果，作者总结了SpecExit的理论贡献和实践意义：\n\n**理论贡献**:\n- 证明隐藏状态包含可用于早期退出的丰富推理信号\n- 提出一种不修改目标模型、无探测开销的早期退出机制\n- 展示多任务学习在联合优化token预测和信号提取中的有效性\n\n**实践意义**:\n- 为LRMs的实际部署提供高效解决方案\n- 开源代码促进社区采用和进一步研究\n- 为利用隐藏状态提高模型效率提供新思路\n\n### 完整逻辑链总结\n\n从宏观问题到具体解决方案，作者的逻辑链可总结为：\n\n1. **问题识别**: LRMs的过度思考导致冗余输出和高延迟\n2. **现有方法分析**: 推理时方法有探测开销，基于训练的方法改变输出分布，推测解码不解决过度思考\n3. **关键观察**: 隐藏状态编码推理充分性信号，可替代昂贵探测\n4. **核心假设**: 扩展推测解码框架，利用隐藏状态同时预测token和早期退出信号\n5. **方法设计**: SpecExit框架，包括扩展MTP层、信号提取训练和信号引导推理\n6. **验证优化**: 实验证明有效性，消融研究优化关键组件\n7. **贡献总结**: 理论上证明隐藏状态的价值，实践上提供高效部署方案\n\n这一逻辑链体现了从问题观察到假设形成，再到方法设计和验证的完整科研过程，最终提出了一个既创新又实用的解决方案。</think># SpecExit方法逻辑链推演：从问题观察到解决方案\n\n## 第一步：识别核心问题 - 大型推理模型的\"过度思考\"现象\n\n**观察现象**：大型推理模型(LRMs)如OpenAI-o1、DeepSeek-R1等在复杂推理任务上表现优异，但存在明显的\"过度思考\"(overthinking)问题。这些模型产生不必要冗长的思维链(CoT)输出，导致两个关键瓶颈：\n- **计算资源浪费**：生成过多冗余token\n- **端到端延迟过高**：限制了实际应用场景的部署\n\n**问题根源**：LRMs遵循\"测试时间缩放定律\"，即更长CoT通常提升性能，但过度依赖扩展推理导致冗余。作者发现这种冗余不是必要的，存在优化空间。\n\n## 第二步：分析现有解决方案的局限性\n\n**推理时早期退出方法**：\n- **原理**：通过监控模型信号(如中间答案、输出logits)在检测到足够证据时终止解码\n- **关键局限**：引入\"探测开销\"(detection overhead)，实际延迟收益有限；通用性差，效果高度依赖任务和模型(如图1a显示DEER方法在某些情况下甚至增加延迟)\n\n**基于训练的方法**：\n- **原理**：通过强化学习或监督微调改变模型行为\n- **关键局限**：可能改变模型输出分布；需要大量再训练成本；在未见任务上泛化能力差\n\n**推测解码(Speculative Decoding)**：\n- **优势**：通过轻量级草稿模型并行验证候选token，减少每token延迟，不改变目标模型输出\n- **局限**：单独使用不能解决过度思考问题，模型仍生成完整CoT\n\n**核心矛盾**：现有方法无法同时满足三个条件——不改变目标模型输出、无额外探测开销、有效解决过度思考。\n\n## 第三步：关键观察 - 隐藏状态中的推理信号\n\n**实验发现**：作者通过初步实验发现，模型的隐藏状态(hidden states)编码了关于推理过程的丰富信息。如图1b所示，使用简单MLP训练隐藏状态预测推理进度，结果显示：\n- 复杂任务初期，预测信号较暗(需继续推理)\n- 随着推理充分，信号逐渐变浅(可终止)\n\n**洞察**：隐藏状态提供任务复杂性和推理充分性的细粒度指标，可作为高效早期退出的自然信号源，替代昂贵的探测机制。\n\n## 第四步：形成核心假设\n\n基于隐藏状态的观察，作者提出核心假设：\n> **可以扩展推测解码框架，利用草稿模型的隐藏状态同时预测未来token和早期退出信号，在不引入探测开销的情况下实现高效早期退出。**\n\n假设的关键要素：\n1. **不修改目标模型**：保持原始输出分布\n2. **无额外探测开销**：通过扩展草稿模型实现\n3. **利用隐藏状态的丰富信息**：提取已编码的推理充分性信号\n\n## 第五步：设计SpecExit框架\n\n### 1. 整体架构设计\n**核心创新**：扩展多token预测(MTP)层，在标准token预测基础上增加三个辅助预测头：\n- **置信度(confidence)**：反映生成可靠性，定义为预测步骤logit概率的几何平均\n- **推理进度(progress)**：0-1归一化值，表示CoT相对进展\n- **剩余推理长度(remaining reasoning length)**：估计完成推理还需的token数\n\n**技术实现**：\n```\nW h = [Wtok h, Wconf h, Wprog h, Wrem h]\n```\n其中Wtok h产生标准token预测，其他三个矩阵分别预测三种信号。\n\n### 2. 信号提取训练\n**数据构建**：\n- 获取模型完整响应，提取``标记内的推理内容\n- 识别有效推理轨迹：保留产生正确答案的最小推理段，后续视为冗余\n\n**信号标注与回归**：\n- **多任务学习目标**：L = Lcls + λcLconf + λpLprog + λrLrem\n- **损失函数设计**：\n  - 置信度和进度：MSE损失\n  - 剩余推理长度：MSLE损失(处理大范围值)\n- **动态加权**：基于梯度幅度平衡不同任务贡献，防止大梯度任务主导学习\n\n### 3. 信号引导推理\n**停止条件设计**：\n- **步骤分割token**：引入自然分段点(如段落分隔符`\\n\\n`)作为语义连贯边界\n- **信号阈值**：当平滑信号超过预设阈值时触发早期退出\n- **信号平滑**：使用指数加权移动平均(EWMA)增强鲁棒性，避免噪声干扰\n\n**与推测解码集成**：\n- 草稿模型提出候选token和信号\n- 目标模型并行验证token并生成最终信号\n- 当信号超过阈值且遇到步骤分割token时，插入`</think>`终止推理\n\n## 第六步：验证假设并优化方法\n\n### 1. 主要实验结果\n在多个基准测试(GSM8K、MATH500、AIME等)上验证：\n- **效率提升**：减少推理轨迹66%，实现2.5倍端到端延迟加速\n- **准确性保持**：与基线相当或更高，证明早期退出不损害推理质量\n- **通用性**：在数学、科学、编码、逻辑等多领域有效\n\n### 2. 消融研究优化\n**信号类型比较**：\n- 单一信号(仅置信度/进度/剩余长度)各有局限\n- 组合信号互补优势，平衡准确性和效率(图5)\n\n**信号平滑策略**：\n- EWMA提供最稳定的平衡，相比无平滑、动量预测等方法更优(表2)\n\n**步骤分割token选择**：\n- 段落分隔符比话语标记更具通用性，不依赖特定数据或模型模式(图6)\n\n## 第七步：形成理论贡献与实践意义\n\n### 理论贡献\n1. **隐藏状态价值证明**：首次系统证明隐藏状态包含可用于早期退出的推理充分性信号\n2. **新范式提出**：创建不修改目标模型、无探测开销的早期退出机制\n3. **多任务学习应用**：展示联合优化token预测和信号提取的有效方法\n\n### 实践意义\n1. **部署友好**：在PyTorch和vLLM中实现，易于集成到现有推理系统\n2. **显著效率提升**：为LRMs实际部署提供可行解决方案，降低延迟和计算成本\n3. **开源促进**：公开代码促进社区采用和进一步研究\n\n## 逻辑链总结\n\n从问题观察到解决方案的完整逻辑链：\n\n```\n问题识别 → 现有方法分析 → 关键观察 → 核心假设 → 方法设计 → 验证优化 → 贡献总结\n```\n\n这一链条体现了从宏观现象到微观机制，再到具体解决方案的系统性思考过程。作者通过敏锐观察隐藏状态的信号价值，创造性地将其与推测解码结合，解决了LRMs过度思考的关键瓶颈，同时保持了模型输出的完整性和准确性。", "summary_translation": "尽管在推理任务上表现强劲，大型推理模型（large reasoning models, LRMs）常常遭受过度思考（overthinking）的问题，产生不必要的长输出并导致高端到端延迟（end-to-end latency），这对其在现实世界中的部署构成了重大限制。为解决过度思考问题，早期退出机制（early-exit mechanisms）被提出，用于在典型完成前终止推理过程，表明这种方法可以在最小化对准确性影响的情况下有效缩短生成长度。然而，这些机制对探测机制（probing mechanisms）的依赖引入了检测开销（detection overhead），限制了它们在端到端延迟方面的收益，并损害了它们在不同问题上的泛化能力。受投机解码（speculative decoding）中隐藏状态（hidden states）使用的启发，我们提出了SpecExit，这是一个新颖的框架，能够直接从轻量级草稿模型（lightweight draft model）中预测未来令牌（tokens）和早期退出信号，而无需探测开销。我们的方法提供了显著改进，与投机解码基线相比，平均生成长度减少了66%，端到端延迟实现了2.5倍的加速，同时没有损害准确性。我们的方法利用隐藏状态中的固有信号来提供有效的早期退出信号，这表明隐藏状态在高效推理中有更广泛的应用前景。我们的代码可在https://github.com/Tencent/AngelSlim获取。", "summary_generated_time": "2025-10-06 22:10:21", "summary_model": "z-ai/glm-4.5"}, {"index": "#233", "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends", "link": "/arxiv/2509.24203", "arxiv_id": "2509.24203", "authors": "Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding", "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.970418", "filter_reason": "这篇论文的核心贡献是对大语言模型强化学习算法的理论分析和改进，特别是关于REINFORCE及其变种的离线策略特性。论文提出了将REINFORCE适应离线设置的两个通用原则：正则化策略更新和主动塑造数据分布，这些改进有助于更好地应用强化学习来训练LLM。强化学习是提升LLM通用推理能力（包括数学推理、逻辑推理等）的关键技术，论文的研究目标（如源代码示例使用的GSM8K数学推理数据集所示）与提升LLM的推理能力直接相关。论文不涉及特定应用领域或模型基础设施优化，而是专注于改进LLM的基础训练方法，因此符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在揭示group-relative REINFORCE算法的off-policy本质并澄清GRPO及其相关算法的一些误解。针对LLM强化学习中的off-policy训练场景，我们提出了一种基于KL正则化替代目标的新解释框架，并在GSM8k、MATH、ToolACE等多个数据集上通过准确率、训练奖励等指标验证了其有效性。", "inspiration_trace": "# 从观察到方法论：Group-Relative REINFORCE的Off-Policy本质\n\n## 1. 宏观问题：LLM强化学习的实际挑战\n\n作者从大型语言模型(LLM)强化学习的实际应用困境出发，观察到三个核心问题：\n- **实际约束**：rollout生成与模型训练速度不匹配、数据来源多样、奖励反馈延迟\n- **基础设施复杂性**：LLM-RL系统复杂，使on-policy训练不切实际\n- **算法局限性**：主流算法如PPO和GRPO本质上是on-policy方法，对off-policyness容忍有限\n\n这引发了一个根本性问题：**能否在LLM强化学习中实现更高效、更灵活的off-policy学习？**\n\n## 2. 观察与聚焦：REINFORCE的矛盾现象\n\n作者注意到一个有趣现象：尽管REINFORCE及其变体(如GRPO)理论上被视为on-policy算法，但实践中常被用于off-policy场景。这促使作者聚焦于一个核心问题：\n\n**Group-Relative REINFORCE是否具有天然的off-policy解释，而不仅是on-policy算法的扩展？**\n\n## 3. 假设形成：第一性原理的重新思考\n\n作者提出一个大胆假设：\n> Group-Relative REINFORCE可以被重新解释为本质上是一个off-policy算法，而不需要对训练数据分布做特定假设。\n\n为验证这一假设，作者决定从第一性原理出发，重新推导Group-Relative REINFORCE，避开标准策略梯度理论中的on-policy假设。\n\n## 4. 理论推导：三步揭示off-policy本质\n\n### 步骤1：定义KL正则化代理目标\n作者引入KL正则化的代理目标：\n```\nmax_θ J(θ; π_θt) := E_x[E_y[r(x,y)] - τ·D_KL(π_θ(·|x) || π_θt(·|x))]\n```\n并证明其最优策略满足一致性条件：对于任意响应对y₁,y₂，有\n```\nr₁ - τ·[log π(y₁|x) - log π_θt(y₁|x)] = r₂ - τ·[log π(y₂|x) - log π_θt(y₂|x)]\n```\n\n### 步骤2：构建有限样本代理损失\n基于一致性条件，作者定义代理损失：\n```\nL̃(θ; x, π_θt) := 1/K² Σ_{i<j} (a_i - a_j)²/(1+τ)²\n```\n其中a_i := r_i - τ[log π_θ(y_i|x) - log π_θt(y_i|x)]\n\n### 步骤3：单步梯度推导的关键洞察\n在θ=θ_t处进行梯度计算时，作者发现一个关键事实：log π_θ(y_i|x) - log π_θt(y_i|x)的值为零，这大大简化了梯度计算。最终推导出：\n```\ng(θ; x, {y_i,r_i}) = 2τ/(1+τ)² · 1/K Σ_i (r_i - r̄) ∇_θ log π_θ(y_i|x)\n```\n这正是Group-Relative REINFORCE的更新规则，且推导过程未对数据分布做任何on-policy假设。\n\n## 5. 深入分析：识别缺陷与提出原则\n\n基于新解释，作者分析了vanilla REINFORCE的缺陷，并提出两个增强原则：\n\n### 原则1：正则化策略更新\n确保在次优数据分布下，优化轨迹保持有界和稳定。\n\n### 原则2：主动塑造数据分布\n通过加权训练样本引导策略更新方向，而非简单按原样使用。\n\n## 6. 方法论形成：统一解释现有算法\n\n作者将新框架应用于重新解释多种现有算法：\n\n### GRPO的重新理解\n- **发现**：GRPO的off-policy有效性主要源于裁剪(作为正则化)而非重要性采样\n- **启示**：可扩大裁剪范围加速训练而不牺牲稳定性\n\n### OPMD和AsymRE的统一视角\n- **OPMD**：可视为REINFORCE损失加上均方正则化损失\n- **AsymRE**：可视为REINFORCE损失加上KL散度正则化\n\n### 数据加权策略的理论依据\n- **RED-Drop**：丢弃低奖励样本的策略在off-policy框架下自然合理\n- **RED-Weight**：高奖励样本上加权可视为一种有效的数据分布塑造\n\n## 7. 实验验证：从理论到实践\n\n作者通过广泛实验验证了理论洞察：\n- REC系列实验证明裁剪比重要性采样更关键\n- 扩大裁剪范围可加速训练且保持稳定\n- RED方法在各种off-policy设置下表现优异\n- OPMD和AsymRE作为正则化REINFORCE的解释得到验证\n\n## 8. 核心贡献：新视角与新机会\n\n作者最终贡献了一个全新的理解框架：\n1. 揭示Group-Relative REINFORCE的天然off-policy本质\n2. 提出增强off-policy REINFORCE的两个通用原则\n3. 统一解释多种现有算法，澄清其工作机制\n4. 为LLM的off-policy RL开辟了原则性算法设计的新途径\n\n这一工作不仅解决了初始观察到的实际问题，更通过理论创新为整个领域提供了新的思考方向和设计原则。", "summary_translation": "针对大型语言模型(large language models, LLMs)的离线强化学习(off-policy reinforcement learning, RL)正日益受到关注，这主要是由实际应用中的现实约束、LLM-RL基础设施的复杂性以及对强化学习方法论进一步创新的需求所推动的。尽管经典的REINFORCE及其现代变体如组相对策略优化(Group Relative Policy Optimization, GRPO)通常被视为对离策略性(off-policyness)容忍度有限的在线策略(on-policy)算法，但我们在本工作中提出了一个不假设特定训练数据分布的组相对REINFORCE的基本原理推导，表明它具有天然的离线策略解释。这一视角产生了将REINFORCE适应离线策略环境的两个通用原则：正则化策略更新和主动塑造数据分布。我们的分析揭示了关于GRPO中重要性采样(importance sampling)和裁剪(clipping)作用的一些误解，将两种最新算法——在线策略镜像下降(Online Policy Mirror Descent, OPMD)和非对称REINFORCE(Asymmetric REINFORCE, AsymRE)统一并重新解释为REINFORCE损失的正则化形式，并为看似启发式的数据加权策略提供了理论依据。我们的研究结果带来了可操作的见解，这些见解通过广泛的实证研究得到了验证，并为LLMs离线强化学习中有原则的算法设计开辟了新的机会。本工作的源代码可在https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#231", "title": "Learning to Ponder: Adaptive Reasoning in Latent Space", "link": "/arxiv/2509.24238", "arxiv_id": "2509.24238", "authors": "Yixin He, Lumingyuan Tang", "summary": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.969667", "filter_reason": "这篇论文的核心贡献是提出FR-Ponder框架，一种通过潜在空间中的自适应推理来分配计算资源的方法。从本质上看，论文直接关注如何提高大语言模型本身的推理能力，特别是通过让模型根据输入复杂性调整推理深度，而不是像现有方法那样对所有输入应用统一的推理深度。论文使用了强化学习方法（GRPO）来平衡性能和计算成本，并在数学推理任务（GSM8K和MATH500）上验证了其有效性。这篇论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的推理机制，完全符合\"大语言模型通用推理能力\"的研究范围。论文提出的自适应推理框架可以被视为一种增强LLM通用推理能力的新范式，属于改进LLM基础推理能力的研究。论文不涉及任何排除标准中的领域，如多模态、特定应用领域或模型可靠性的应用层面问题。", "summary2": "本文旨在解决大型语言模型在推理任务中计算资源分配不灵活的问题。针对不同复杂度的推理任务，我们提出了一种FR-Ponder框架，通过潜在空间中的引导向量实现实例自适应推理计算分配，并在GSM8K、MATH500和GPQA数据集上通过准确率、平均令牌数和FLOPs验证了其有效性。该方法使用轻量级控制器（<1M参数）决定推理深度，在保持或提高准确率的同时实现了30-50%的计算资源减少。", "inspiration_trace": "# FR-Ponder方法逻辑链推演：从固定计算到自适应推理\n\n## 1. 宏观问题：LLM的固定计算分配低效\n\n**观察现象**：大型语言模型(LLM)在处理不同复杂度的任务时，采用相同的计算资源分配策略。无论是简单事实查询还是复杂数学推理，模型对每个token都消耗相同的计算量。\n\n**问题本质**：这种刚性计算分配导致系统性低效：\n- 简单问题：过度计算，浪费资源\n- 复杂问题：计算不足，推理质量受限\n\n**影响范围**：随着模型规模扩大至数千亿参数，这种计算-准确性不匹配问题日益严重，限制了LLM的实用性和可持续性。\n\n## 2. 现有解决方案的局限性分析\n\n作者系统评估了三类主流方法，发现各自存在根本缺陷：\n\n### 2.1 多遍方法（如思维链CoT、自洽性）\n- **机制**：通过采样多个推理轨迹实现自适应\n- **局限**：推理成本与采样次数成正比，计算开销呈倍数增长\n- **核心矛盾**：自适应性与计算效率不可兼得\n\n### 2.2 架构修改方法（如早期退出、层跳过）\n- **机制**：修改模型结构实现动态计算分配\n- **局限**：需要重新训练基础模型，降低部署灵活性，可能损害原始能力\n- **核心矛盾**：自适应性与模型完整性不可兼得\n\n### 2.3 推测解码方法\n- **机制**：通过草稿-验证范式加速推理\n- **局限**：需维护多个模型，仅提供粗粒度自适应\n- **核心矛盾**：效率与细粒度控制不可兼得\n\n### 2.4 分数推理框架（最接近的前期工作）\n- **机制**：从对比提示中提取\"推理向量\"控制推理深度\n- **局限**：需手动调整缩放因子，缺乏单次推理内的动态适应\n- **核心矛盾**：方向控制与时间控制未解耦\n\n## 3. 核心洞察：自适应计算的正交分解\n\n**关键突破**：作者认识到自适应计算可分解为两个正交维度：\n1. **思考什么**（内容方向）：推理的方向和路径\n2. **思考多久**（时间控制）：推理的深度和持续时间\n\n**核心假设**：若能将这两个维度解耦，则可实现：\n- 保持基础模型能力不变（无需修改权重）\n- 实现细粒度、实例自适应的计算分配\n- 单次推理内动态调整推理深度\n\n**理论依据**：基于线性表示假说，不同推理模式（如逐步思考vs直接回答）对应表示空间中的不同方向，可通过向量操作进行干预。\n\n## 4. 方法设计：FR-Ponder框架\n\n基于上述洞察，作者设计了FR-Ponder框架，包含四个关键组件：\n\n### 4.1 转向向量提取\n- **思路**：提取编码\"更深层次推理\"的表示方向\n- **实现**：通过对比提示（\"逐步思考\"vs\"直接回答\"）计算隐藏状态差异\n  ```\n  h_steer = E[x∼D][z_deliberative(x) - z_direct(x)]\n  ```\n- **优势**：一次性提取，固定使用，无需修改模型权重\n\n### 4.2 自适应思考机制\n- **状态演化**：通过加性转向引导表示变化\n  ```\n  z_{k+1} = z_k + α(k)·h_steer\n  ```\n  其中α(k) = α₀·e^(-βk)实现指数衰减，确保稳定性\n- **控制器架构**：轻量级神经网络（<1M参数）\n  - 输入：当前隐藏状态\n  - 输出：继续思考的概率\n  - 决策：若ϕ(z_k) ≤ τ则停止，否则应用思考步骤\n\n### 4.3 训练方法\n- **问题建模**：将自适应计算建模为马尔可夫决策过程(MDP)\n- **奖励设计**：多组件奖励函数平衡五个目标：\n  ```\n  R = w_acc·Accuracy - w_flops·FLOPs + w_comp·Completeness \n      + w_qual·Quality - w_rep·Repetition\n  ```\n- **优化算法**：采用组相对策略优化(GRPO)\n  - 通过组内比较实现方差缩减\n  - 无需单独的价值网络\n  - 适合轻量级控制器设计\n\n### 4.4 课程学习框架\n- **三阶段渐进**：\n  1. 教师示范阶段：完全由教师策略控制\n  2. 混合训练阶段：逐渐从教师转向学生\n  3. 自主学习阶段：完全由学生控制器决策\n- **质量门控**：过滤低质量轨迹，确保训练稳定性\n\n## 5. 理论基础与保证\n\n作者为FR-Ponder提供了坚实的理论支撑：\n\n### 5.1 转向向量一致性\n**定理**：在变换器表示空间的温和条件下，转向向量估计器以O(N^(-1/2))速率收敛到真实转向方向。\n\n### 5.2 通用逼近能力\n**定理**：对于紧致子集上的任何连续延续值函数，存在控制器网络可以任意精度逼近。\n\n### 5.3 收敛保证\n**定理**：GRPO实现O(1/√T)的收敛率，并具有1/G的方差缩减因子。\n\n### 5.4 计算开销界限\n**定理**：FR-Ponder的开销为O(K·d)，相对开销为O(K/n)≈O(1/√n)，对大型模型影响微乎其微。\n\n## 6. 实验验证与效果\n\n作者在多个数据集和模型上验证了FR-Ponder的有效性：\n\n### 6.1 实验设置\n- **数据集**：GSM8K（小学数学）、MATH500（中学数学）、GPQA（研究生科学问题）\n- **模型**：LLaMA-3系列（8B、70B）和Qwen-2.5系列（0.5B、3B、7B）\n- **评估指标**：准确性、平均token数、平均FLOPs\n\n### 6.2 核心结果\n- **准确性**：在保持或提高准确性的同时，显著降低计算开销\n- **效率**：实现30-50%的token减少，FLOPs大幅降低\n- **适应性**：计算分配与问题难度高度相关，简单问题快速解决，复杂问题深入推理\n- **泛化性**：在不同模型规模和架构上均表现良好，尤其对小模型提升明显\n\n## 7. 方法优势总结\n\nFR-Ponder的核心创新在于将推理过程转化为元认知决策过程：\n\n1. **解耦设计**：将\"思考什么\"与\"思考多久\"分离，实现更灵活的控制\n2. **轻量级适配**：仅训练<1M参数的控制器，保持基础模型冻结\n3. **单次推理**：无需多次前向传递，实时调整计算深度\n4. **多目标平衡**：通过精心设计的奖励函数，同时优化准确性和效率\n5. **理论保证**：提供收敛性、稳定性和效率的理论保证\n\n## 8. 逻辑链总结\n\n从宏观问题到具体方法，作者形成了清晰的逻辑链：\n\n```\n固定计算低效 → 现有方法局限 → 正交分解洞察 → FR-Ponder设计 → 理论保证 → 实验验证\n```\n\n这一逻辑链体现了从问题识别、分析、假设到方法设计、验证的完整科学思维过程，最终实现了LLM推理从固定计算到自适应推理的范式转变，为大型语言模型的高效推理提供了新的思路。", "summary_translation": "测试时计算(test-time compute)已成为增强大语言模型(LLM)推理的关键范式，然而现有方法如Best-of-N和多数投票(majority voting)在所有输入上应用统一的推理深度，在简单查询上浪费计算资源，同时可能对复杂问题思考不足。我们提出了FR-Ponder，一种单图(single-graph)、无需骨干训练(backbone-training-free)的框架，通过潜在转向(latent steering)分配实例自适应(instance-adaptive)推理计算。一个参数量少于100万的控制器观察隐藏状态(hidden states)，并决定停止或通过向冻结表示(frozen representations)添加预计算的转向向量(pre-computed steering vector)来应用小步思考(ponder step)。我们的方法从大语言模型中提取与更深推理输出和直接输入输出(direct IO)相关联的潜在转向向量(latent steering vector)，并通过可调节的缩放因子(tunable scaling factor)重新应用它，使模型能够根据每个输入的复杂性调整其推理深度。为了平衡性能和计算成本，我们采用组相对策略优化(Group Relative Policy Optimization, GRPO)作为奖励信号(reward signal)来自适应调节推理深度，在实现任务准确性的同时减轻过度推理(overreasoning)。通过课程学习(curriculum learning)和精心的奖励工程(reward engineering)，FR-Ponder学习到与问题难度相关的校准计算分配(calibrated compute allocation)。在GSM8K和MATH500数据集上，FR-Ponder改进了计算-准确性前沿(compute-accuracy frontier)，以更低的浮点运算次数(FLOPs)提供更好的匹配准确性，并且在不修改骨干权重(backbone weights)的情况下，与早期退出(early-exit)基线相比表现更优。分析结果可视化了解释性转向方向(interpretable steering directions)，并表明学习的计算分配与问题难度相关。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#234", "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models", "link": "/arxiv/2509.24156", "arxiv_id": "2509.24156", "authors": "Yuhui Wang, Changjiang Li, Guangke Chen, Jiacheng Liang, Ting Wang", "summary": "Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively \"hacking\" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.970742", "filter_reason": "这篇论文完全符合我的研究目标。首先，从核心判断来看，论文本质上是研究大语言模型推理机制的内在工作原理，特别是分析思维链(CoT)推理和记忆检索两种竞争机制如何影响模型的推理过程。这不是将LLM应用于特定领域，而是专注于提升LLM本身的通用推理能力。论文提出的FARL框架是一种新的训练范式，通过结合记忆遗忘和强化学习来抑制检索捷径，从而增强模型的推理主导行为和通用推理能力。其次，论文包含多个正面指标：核心概念涉及大型推理模型(LRMs)，能力方向聚焦于推理能力(特别是CoT推理)，训练方法包含强化学习。论文不涉及任何排除标准中的领域，如多模态视觉、特定应用领域或模型可靠性的应用层面问题。因此，这篇论文明确致力于提高大语言模型的通用推理能力，完全符合我的研究范围。", "summary2": "本文旨在 [解决大型推理模型答案生成机制不一致问题]。针对 [推理与检索机制竞争场景]，我们提出了一种 [FARL框架，结合记忆遗忘与强化学习抑制检索捷径]，并在 [多QA数据集和不同LRMs] 上通过 [扰动成功率和准确率] 验证了有效性。", "inspiration_trace": "# 大型推理模型中推理与检索机制的逻辑分析：从观察到FARL方法的提出\n\n## 1. 宏观问题：推理模型的不一致性现象\n\n**观察起点**：大型推理模型(LRMs)如GPT o-series、Gemini 2.5和DeepSeek-R1等通过思维链(CoT)推理展现出强大的问题解决能力，但研究发现这些模型的最终答案经常与它们自己的推理痕迹相矛盾。\n\n**核心矛盾**：为什么这些模型在展示详细推理过程的同时，最终答案却与推理过程不一致？这种不一致性揭示了模型内部工作机制的什么问题？\n\n## 2. 形成核心假设：双机制竞争模型\n\n**假设提出**：作者假设这种不一致性源于两种生成答案的竞争机制：\n- **CoT推理机制**：通过逐步逻辑推理得出答案\n- **记忆检索机制**：直接从内部记忆中检索答案\n\n**理论依据**：\n- LRMs建立在现有基础模型之上，通过蒸馏或强化学习激发推理能力，导致多种能力共存\n- 内部知识作为竞争因素，可能影响显式推理过程\n- 最终答案可能是两种机制竞争的结果，而非单一过程的产物\n\n## 3. 研究问题与实验验证\n\n### 研究问题聚焦：\n1. **RQ1**：LRMs是否同时使用推理和检索来推导答案？\n2. **RQ2**：什么因素影响一种能力相对于另一种能力的优势？\n3. **RQ3**：我们如何控制这些能力的相对强度？\n\n### 实验设计：\n- **推理扰动**：在CoTs中注入误导性线索，观察是否改变最终答案\n- **检索扰动**：通过微调\"毒害\"模型记忆，观察是否影响答案生成\n- **组合扰动**：同时应用两种扰动，观察\"拉锯战\"现象\n\n### 关键发现：\n- **RQ1验证**：两种机制确实同时运作，当它们指向同一答案时效应增强，指向不同答案时出现竞争\n- **RQ2分析**：\n  * 问题领域：数学/逻辑领域更依赖推理，其他领域更依赖检索\n  * 训练方法：蒸馏模型更依赖检索，RL训练模型更依赖推理\n  * 模型规模：更大模型更倾向于推理主导\n  * 神经机制：中间层(12-16层)是推理与检索竞争的关键控制点\n\n## 4. 问题识别：当前微调范式的局限性\n\n**关键发现**：模型可以利用检索机制作为\"捷径\"，有效\"黑客攻击\"奖励信号，破坏真正的推理发展。\n\n**具体问题**：在RL训练过程中，特别是蒸馏模型倾向于：\n1. 检索正确答案而不考虑CoT相关性\n2. 生成捏造的CoT来证明记忆答案的合理性(\"事后解释\")\n3. 这种行为膨胀了批量平均奖励，不公平地惩罚通过真正推理获得正确答案的样本\n\n## 5. 解决方案：FARL框架的提出\n\n**核心洞察**：\n- RL训练比蒸馏更能促进真正的推理能力\n- 数学问题自然激发更稳健的推理\n- 需要阻止检索捷径，净化奖励信号\n\n**FARL设计原理**：通过强迫模型\"忘记\"特定的记忆答案，阻止检索捷径，迫使推理机制占主导地位，并使其在RL期间得到改进。\n\n**方法实现**：\n- 修改标准RL流程，在GRPO迭代后引入遗忘步骤\n- 采用负偏好优化(NPO)作为遗忘方法\n- 迭代过程：GRPO优化 → 遗忘抑制 → GRPO优化 → ...\n\n## 6. 实验验证与效果评估\n\n**评估指标**：\n- 扰动实验中的R-PSR和T-PSR（衡量推理与检索主导性）\n- 准确率(ACC)和响应的平均标记长度(MTL)\n- 推理图属性（循环、直径和小世界指数）\n\n**实验结果**：\n- RL优于SFT：RL模型表现出更强的推理主导性，训练域内准确率提升19.8%（SFT仅8.6%）\n- FARL优于标准RL：\n  * R-PSR和T-PSR分别降低47.8%和38.5%，表明更强的推理主导行为\n  * 域内准确率提升22.8%，域外准确率提升5.8%\n  * 推理图质量显著提升，小世界指数提高84.0%\n\n## 7. 逻辑链总结\n\n从观察到FARL方法的完整逻辑链：\n\n**不一致现象** → **双机制假设** → **实验验证** → **影响因素分析** → **发现奖励信号问题** → **提出FARL解决方案** → **实验验证效果**\n\n这一研究不仅揭示了LRMs答案生成的内在机制，还提供了控制这些机制相对强度的有效方法，为更有效地激发模型推理能力开辟了新方向。FARL通过结合遗忘与强化学习，成功抑制了检索捷径，促进了真正的推理发展，提高了模型的泛化能力。", "summary_translation": "大型推理模型(LRMs，Large reasoning models)通过思维链(CoT，Chain-of-Thought)推理在解决复杂问题方面展现出前所未有的能力。然而，最近的研究表明，这些模型的最终答案经常与它们自己的推理过程相矛盾。我们假设这种不一致性源于两种竞争的答案生成机制：思维链(CoT)推理和记忆检索(memory retrieval)。为验证这一假设，我们进行了对照实验，在推理过程中引入误导性线索，在检索过程中提供损坏答案，以此挑战大型推理模型。我们在多个模型和数据集上的结果证实，这两种机制同时运作，其相对优势受多种因素影响：问题领域、模型规模和微调方法(fine-tuning approaches)(例如，强化学习(reinforcement learning)与知识蒸馏(distillation))。这些发现揭示了当前推理微调范式(reasoning fine-tuning paradigms)的一个关键局限：模型可以利用检索机制作为捷径，有效地\"破解\"奖励信号，从而阻碍真正的推理能力发展。为应对这一挑战，我们提出了FARL，一种将记忆遗忘(memory unlearning)与强化学习(reinforcement learning)相结合的新型微调框架。通过在微调过程中谨慎抑制检索捷径，FARL促进了推理主导行为，并增强了可泛化的推理能力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#239", "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models", "link": "/arxiv/2509.23962", "arxiv_id": "2509.23962", "authors": "Guanxu Chen, Yafu Li, Yuxian Jiang, Chen Qian, Qihan Ren, Jingyi Yang, Yu Cheng, Dongrui Liu, Jing Shao", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.981810", "filter_reason": "这篇论文完全符合我的研究范围。根据筛选标准，我进行了如下分析： 第一步：核心判断——论文的本质是关于改进LLM的基础推理能力。论文提出了\"Conditional advANtage estimatiON (CANON)\"方法，用于增强大型语言模型在强化学习中的推理能力，特别是在数学推理和复杂逻辑任务上的表现。这属于\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的范畴，符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确研究Large language models (LLMs) - 能力方向：专注于reasoning，特别是mathematical reasoning和high-complexity logic tasks - 训练方法：研究Reinforcement Learning with Verifiable Rewards (RLVR)，属于强化学习范畴 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有专注于特定应用领域（虽然提到数学和逻辑任务，但这些是通用推理能力的测试场景，而非特定领域应用） - 没有关注模型可靠性方面的水印、安全等问题 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的情况，如智能体/工具使用或幻觉/可解释性/安全等。 综上所述，这篇论文的核心贡献是通过改进强化学习中的优势估计方法来提升LLM的通用推理能力，完全符合我筛选\"致力于提高大语言模型本身的通用推理能力\"论文的研究目标。", "summary2": "本文旨在解决大型推理模型强化学习中如何有效利用训练指标先验而不预设方向偏好的问题。针对具有可验证奖励的强化学习场景，我们提出了一种条件优势估计方法(CANON)，通过将采样响应按指标值分组并计算组间与组内优势来放大目标指标影响。在三个LLMs上通过六个数学推理基准和三个高复杂度逻辑任务验证，基于熵的CANON在数学任务上实现1.9点准确率提升，基于响应长度的CANON在性能-效率权衡上建立了新的Pareto前沿。", "inspiration_trace": "# 从观察到创新：CANON方法的逻辑演进\n\n## 宏观问题：如何有效利用训练指标提升大模型推理能力？\n\n在大型推理模型(LRMs)的强化学习中，研究者观察到一些训练指标(如熵、响应长度)与模型的不同推理行为相关联。这些指标可能包含有价值的先验知识，可以指导训练过程并提高模型性能。然而，如何有效利用这些指标而不引入人为偏见，成为一个核心挑战。\n\n## 现有方法的局限：方向性偏见的陷阱\n\n作者观察到当前方法存在明显局限：\n\n1. **手工设计的方向性偏好**：现有方法通过奖励塑形或优势塑形整合人类先验，但这些方法通常假设特定指标是\"越高越好\"或\"越低越好\"。\n\n2. **超参数敏感性**：没有仔细的超参数调整，这些方向性先验可能过度偏向并导致失败。\n\n3. **指标效应的复杂性**：例如，熵指标在不同场景下有不同影响——高熵响应倾向于探索性，适合复杂问题；低熵响应表现出确定性，在模型能力范围内的问题上更准确。\n\n## 关键洞察：不预设偏好的可能性\n\n作者提出一个核心假设：**是否可以在不预设指标方向偏好的情况下，放大特定指标变化的影响？**\n\n这一假设基于一个重要观察：模型输出中可能存在可以利用的内在倾向，这些倾向可以自然地促进有益行为的学习，如增强探索或提高推理效率。\n\n## 方法论的演进：从分组到条件优势估计\n\n### 第一步：条件重组\n\n作者首先提出将采样响应基于给定指标的值重新分组为两组(高值组和低值组)。这一步骤的目的是显式引入比较目标，为后续的优势估计创造条件。\n\n### 第二步：双重优势估计\n\n基于分组，作者设计了两种互补的优势估计机制：\n\n1. **组间优势(Inter-group advantage)**：通过跨组比较确定哪种指标趋势(高值或低值)导致更高准确性。\n   \n   ```\n   如果响应属于高值组，则与低值组的平均奖励比较\n   如果响应属于低值组，则与高值组的平均奖励比较\n   ```\n\n2. **组内优势(Intra-group advantage)**：在同一组内识别更好的响应，特别优先考虑来自平均奖励较低的组中的正确响应。\n\n   ```\n   响应与其所属组的平均奖励比较\n   ```\n\n### 第三步：统一框架与理论支持\n\n作者将两种优势估计结合为统一形式：\n```\nCANON = μ * inter-group advantage + (1-μ) * intra-group advantage\n```\n\n通过理论分析，作者证明了当两组大小相等时，组间优势放大了分组指标对优势计算的影响。更重要的是，他们发现现有的DR.GRPO方法实际上是CANON在μ=0.5时的特例，这表明CANON提供了一个更通用的框架。\n\n### 第四步：加权条件控制\n\n为了进一步精细控制指标趋势，作者引入了加权机制，允许对特定组给予不同权重。例如，通过略微降低较长响应的权重，CANON可以实现高效的推理，而不会显著降低性能。\n\n## 实验验证与效果确认\n\n作者通过广泛的实验验证了CANON的有效性：\n\n1. **基于熵的CANON**：在数学任务上实现了1.9点的准确率提升，表明组间优势能有效识别有利于数学推理的熵趋势。\n\n2. **基于长度的CANON**：显著提高了推理效率，在性能-成本权衡中建立了更优的帕累托前沿。\n\n3. **动态调度策略**：通过在不同训练阶段调整μ值，CANON-Dynamic在数学推理和复杂逻辑推理任务上都取得了优异表现。\n\n4. **高效推理**：通过调整长度权重α，CANON-Eff在低令牌预算场景下实现了2.63倍的性能提升，并在相同性能水平下减少了45.5%的令牌消耗。\n\n## 逻辑链条总结\n\n从宏观问题出发，作者通过以下逻辑链条发展出CANON方法：\n\n1. **观察问题**：现有方法依赖手工设计的方向性偏好，可能导致过度偏向\n2. **提出假设**：是否可以在不预设偏好的情况下放大指标影响\n3. **核心创新**：通过条件重组和双重优势估计机制，自然识别模型输出中的有益倾向\n4. **理论支持**：证明方法的有效性，并展示现有方法是其特例\n5. **实验验证**：在多个任务和模型上验证方法的有效性和灵活性\n\n这一逻辑演进过程展示了作者如何从实际问题出发，通过深入观察和理论思考，最终提出一个既创新又实用的解决方案，为大语言模型的强化学习提供了新的思路。", "summary_translation": "# 可验证奖励强化学习(RLVR)在大语言模型(LLMs)条件优势估计(CANON)方法\n\n可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)在大语言模型(large language models, LLMs)领域已取得显著进展，有效提升了LLMs在具有明确正确性标准任务上的推理能力，如数学推理任务。在强化学习过程中，多个训练指标，如熵(entropy)或响应长度(response length)，已被观察到与不同推理行为相关联。先前方法通过奖励(reward)或优势整形(advantage shaping)整合此类先验知识，这通常依赖于手工设计的惩罚和偏好（例如，越高越好或越低越好）。然而，若未经仔细的超参数调优(hyperparameter tuning)，这些方向性先验可能过度偏向并导致失败。为此，我们引入条件优势估计(Conditional advANtage estimatiON, CANON)，在不预设目标指标方向的情况下，增强其影响力。具体而言，CANON基于目标指标(target metric)的高低值将采样响应重新分为两组，通过组间比较衡量哪种指标趋势有助于更好性能，并在同一组内识别更优响应。总之，基于熵(entropy)的CANON在数学推理和高复杂度逻辑任务上，于三种LLMs中均一致性地优于先前方法。当应用于响应长度(response length)时，CANON进一步提高了令牌效率(token efficiency)，在性能-成本权衡中产生了更有利的帕累托前沿(Pareto frontier)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#240", "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "link": "/arxiv/2509.23946", "arxiv_id": "2509.23946", "authors": "Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb", "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution.This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language, Machine Learning", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.982553", "filter_reason": "这篇论文完全符合研究目标，其核心贡献是提出Explore-Execute Chain（E²C）框架，一种新的结构化推理范式，专门用于改进大语言模型的通用推理能力。从本质上看，论文针对思维链(CoT)的局限性，将推理过程分解为探索和执行两个阶段，通过监督微调和强化学习相结合的训练方法，提升了模型的推理效率、路径探索能力和可解释性。这明显属于改进LLM基础能力和通用推理能力的范畴。 论文包含多个关键正面指标：明确以大语言模型(LLMs)为研究对象；专注于推理(reasoning)、规划(planning)等核心能力方向；采用强化学习(RL)作为训练方法之一。虽然论文提到了在医学基准上的测试结果，但这仅是为了证明其跨领域适应能力，而非将LLM应用于特定领域，因此不触犯排除标准。此外，论文通过分离规划与执行来增强模型的可解释性，这也符合保留标准。 综上所述，该论文直接致力于提升LLM的通用推理能力，提出了新的训练范式和推理框架，与研究目标高度一致。", "summary2": "本文旨在解决传统Chain-of-Thought方法计算效率低、推理路径探索有限和可解释性差的问题。针对大型语言模型的推理任务，我们提出了Explore-Execute Chain (E2C)框架，将推理解耦为探索和执行两个阶段，并在AIME'2024、MATH500等数学推理基准和MedQA、MedMCQA等医学推理基准上通过准确率和计算效率验证了其有效性。", "inspiration_trace": "# Explore-Execute Chain (E²C) 方法论逻辑链分析\n\n## 一、宏观问题：LLM推理效率与质量的根本矛盾\n\n### 初始观察\n大型语言模型(LLM)在复杂推理任务中展现出显著能力，主要通过Chain-of-Thought (CoT)及其变体实现。然而，作者观察到这些方法存在一个根本性矛盾：\n\n**现有方法的混合架构问题**：\n- CoT及其变体采用单一、自回归的生成过程\n- 这种架构本质上将两个认知功能混合在一起：高层次战略规划与低层次逐步执行\n- 这种混合导致计算资源分配不合理、推理路径探索受限、可解释性降低\n\n### 具体表现\n1. **计算效率低下**：模型在创造性规划和常规计算上花费同等计算资源\n2. **探索多样性受限**：贪婪生成过程限制了初始策略多样性，次优早期选择可能破坏整个推理路径\n3. **可解释性不足**：规划与执行混合，难以理解模型的决策过程\n\n## 二、核心假设：分离探索与执行的认知优势\n\n### 假设一：认知分离假设\n基于对人类推理过程的观察，作者提出核心假设：\n- **人类推理二元性**：人类解决复杂问题时通常先进行高层次战略规划，再进行详细执行\n- **功能分离优势**：将这两个认知功能明确分离可以提高推理效率和质量\n\n### 假设二：信息分布假设\n- **探索阶段应高度信息丰富**：包含解决问题的关键战略信息\n- **执行阶段应高度确定性**：严格遵循选定计划，专注于精确计算\n\n### 假设三：训练优化假设\n- **专门训练必要性**：仅通过提示无法实现这种范式转变，需要专门设计的训练方法\n- **因果依赖重要性**：必须建立从探索到执行的明确因果依赖关系\n\n## 三、方法论形成：从概念到实现\n\n### 1. 框架设计\n基于上述假设，作者提出Explore-Execute Chain (E²C)框架：\n\n**形式化定义**：\n```\np(e|c) → p'(π,e|c) = p'(π|c) · p'(e|π,c)\n```\n其中：\n- `p'(π|c)`：探索阶段，应具有\"信息丰富性\"(Informative Property)\n- `p'(e|π,c)`：执行阶段，应具有\"确定性\"(Deterministic Property)\n\n**关键特性**：\n- 探索阶段：生成简洁、高层次的战略计划\n- 执行阶段：以计划为指导，进行详细、确定的计算\n\n### 2. 训练方法设计\n为使模型掌握这种新范式，作者设计了两阶段训练方法：\n\n**阶段一：监督微调(SFT)**\n- **挑战**：简单方法（先执行后总结）会破坏因果结构\n- **解决方案**：设计因果数据生成算法(Algorithm 2)\n  1. 先生成完整解决方案\n  2. 将其提炼为探索步骤\n  3. 提示模型生成严格遵循探索的新执行\n- **目的**：实现推理范式的转变，满足信息丰富性要求\n\n**阶段二：强化学习(RL)**\n- **创新点**：为探索阶段标记分配更高系数λ(λ>1)\n- **目的**：\n  - 利用探索的信息丰富性加速收敛\n  - 通过强化学习的熵减少效应增强执行确定性\n- **两阶段RL训练**：\n  1. 第一阶段：高温(τ1)和多轮采样(k1)，鼓励广泛探索\n  2. 第二阶段：低温(τ2)和少轮采样(k2)，加强确定性执行\n\n### 3. 推理效率优化\n基于E²C框架的分离特性，作者设计了高效的推理策略：\n\n**测试时扩展策略**：\n- **传统方法问题**：生成多个完整的、昂贵的推理链\n- **E²C解决方案**：\n  1. 采样大量廉价的探索计划\n  2. 通过语义聚类或LLM选择最有希望的计划\n  3. 仅执行选定的计划\n\n**两种实现方式**：\n1. **聚类加权投票**：将探索计划聚类，仅执行每个聚类的中心计划\n2. **LLM聚合**：使用外部LLM将多个探索计划合成为一个综合计划\n\n### 4. 领域适应方法\n利用E²C的模块化特性，作者设计了高效的领域适应方法：\n\n**Exploration-Focused SFT (EF-SFT)**：\n- **洞察**：执行组件具有可迁移性，主要需要适应的是探索组件\n- **方法**：仅针对特定领域示例的探索部分进行微调\n- **优势**：显著减少数据和计算需求（仅需标准SFT的3.5%令牌）\n\n## 四、验证与应用\n\n### 验证思路\n作者通过三个关键实验验证E²C框架的有效性：\n\n1. **数学推理实验**：验证基本推理能力和框架优势\n2. **医疗推理实验**：验证跨领域泛化和数据高效适应能力\n3. **测试时扩展实验**：验证性能-成本权衡优势\n\n### 关键结果\n1. **性能提升**：在AIME'24基准上，E²C比GRPO基线提高1.5-1.9%\n2. **计算效率**：达到58.1%准确率仅需可比方法(如Forest-of-Thought)不到10%的解码令牌\n3. **领域适应效率**：EF-SFT使用仅3.5%的训练令牌，比标准SFT提高高达14.5%的准确率\n\n## 五、逻辑链总结\n\n从宏观问题到最终方法论的完整逻辑链条：\n\n1. **问题识别**：现有CoT方法混合了高层规划与低层执行，导致效率低、探索受限、可解释性差\n\n2. **观察与假设**：\n   - 人类推理通常分离规划与执行\n   - 分离这两个阶段可能提高效率和质量\n   - 探索阶段应信息丰富，执行阶段应高度确定性\n\n3. **框架设计**：\n   - 提出E²C框架，明确分离探索和执行阶段\n   - 形式化定义两个阶段及其特性要求\n\n4. **方法实现**：\n   - 设计两阶段训练方法(SFT+RL)和专门数据生成算法\n   - 开发高效测试时扩展策略和领域适应方法\n\n5. **验证与应用**：\n   - 在多个基准上验证方法的有效性和效率\n   - 证明E²C在性能-成本权衡上的优越性\n\n这一逻辑链条展示了作者如何从对现有方法局限性的深刻理解出发，基于认知科学洞察提出创新假设，最终设计出既高效又可解释的推理框架，为LLM推理提供了新的范式。", "summary_translation": "思维链（Chain-of-Thought, CoT）及其变体显著提升了大型语言模型（Large Language Models, LLMs）的推理能力，然而其单一且自回归的架构本质上将高层战略规划与底层逐步执行混为一谈，导致计算效率低下、推理路径探索有限以及可解释性降低。为解决这些问题，我们提出了探索-执行链（Explore-Execute Chain, $E^2C$），这是一种结构化推理框架，将推理解耦为两个不同阶段：一个探索阶段，随机生成简洁的高层计划；随后是一个执行阶段，确定性执行所选计划。我们的方法采用两阶段训练方法论，结合监督微调（Supervised Fine-Tuning, SFT）——通过一种新颖的数据生成算法增强，该算法强制执行严格的计划遵循——以及后续的强化学习（Reinforcement Learning, RL）阶段，该阶段利用探索的信息性并强化执行的确定性。这种分解实现了一种高效的测试时扩展策略：在AIME'2024上，$E^2C$测试时扩展（Test Time Scaling）达到58.1%的准确率，仅使用可比方法（如思维森林，Forest-of-Thought）所需解码令牌的<10%，大幅减少了自一致性开销。对于跨领域适应，我们的探索聚焦监督微调（Exploration-Focused SFT, EF-SFT）仅使用标准SFT所用令牌的3.5%进行微调，却在医学基准测试上比标准SFT高出高达14.5%的准确率，通过将规划与执行分离，实现了最先进的性能、强大的泛化能力和更高的可解释性。该项目的代码和预训练模型可在以下网址获取：https://github.com/yks23/Explore-Execute-Chain.git", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#244", "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR", "link": "/arxiv/2509.23808", "arxiv_id": "2509.23808", "authors": "Fanding Huang, Guanbo Huang, Xiao Fan, Yi He, Xiao Liang, Xiao Chen, Qinting Jiang, Faisal Nadeem Khan, Jingyan Jiang, Zhi Wang", "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.985287", "filter_reason": "这篇论文完全符合我的研究目标，核心在于提高大语言模型的通用推理能力。首先，论文的本质是改进LLM的基础推理能力，提出了名为VERL（Velocity-Exploiting Rank-Learning）的新方法，通过在隐藏状态空间中解耦探索和利用来增强LLM的推理能力，而非将LLM作为工具应用到特定领域。其次，论文包含多个正面指标：核心概念上明确关注LLMs；能力方向上专注于reasoning和problem-solving；训练方法上采用强化学习框架。第三，论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性的应用层面研究。论文的核心贡献是通过创新的隐藏状态分析方法，提出了一种能够同时增强探索和利用能力的新训练范式，直接提升了LLM的通用推理能力，在多个推理基准上取得了显著改进，最高达到21.4%的准确率提升。这完全符合我寻找的\"致力于提高大语言模型本身的通用推理能力\"的研究论文。", "summary2": "本文旨在解决大型语言模型在可验证奖励强化学习(RLVR)中的探索-利用权衡问题。针对传统token-level分析导致的固有权衡限制，我们提出了一种基于隐藏状态表示的Velocity-Exploiting Rank-Learning (VERL)方法，通过有效秩(ER)量化探索，并提出其导数ERV和ERA来捕捉利用动态。在多个LLM和推理基准上的实验表明，VERL能够同时增强探索和利用能力，在Gaokao 2024数据集上实现了高达21.4%的绝对准确率提升。", "inspiration_trace": "# 从探索-利用权衡到隐藏状态解耦：VERL方法的逻辑演进\n\n## 1. 宏观问题：RLVR中的探索-利用权衡是固有约束吗？\n\n在可验证奖励的强化学习(RLVR)领域，主流观点将进展解释为探索(exploration)与利用(exploitation)之间的权衡结果。这种观点几乎完全基于token级别分析：探索被视为高熵token分布，利用被视为高置信度、低熵分布。这导致了一个普遍假设——探索和利用之间存在固有权衡，因为模型输出分布不能同时是均匀和尖锐的。\n\n然而，作者观察到这种观点存在一个根本性问题：**这种权衡是推理过程的内在属性，还是仅仅是测量层面的假象？**\n\n## 2. 问题聚焦：token级别测量的局限性\n\n深入分析token级别度量的缺陷，作者发现三个关键局限：\n\n- **探索度量的两难困境**：将探索等同于token级别熵面临内在矛盾——过高熵产生不连贯噪声，过低熵又扼杀了探索本身。\n- **利用度量的脆弱性**：通过手工设计启发式奖励定义利用，导致模型追逐表面代理，泛化性差。\n- **研究思维的局限性**：现有工作陷入\"平衡\"权衡的循环，而非质疑权衡本身存在。\n\n这些观察引发了一个核心假设：**探索-利用权衡可能不是推理的固有属性，而是token级别测量的假象**。\n\n## 3. 假设形成：隐藏状态空间中的解耦可能性\n\n基于对token级别测量局限性的认识，作者提出了一个大胆假设：**在更细粒度的隐藏状态空间中，探索和利用可能是解耦的**。为验证这一假设，需要：\n\n1. 将分析从token级别转移到语义丰富的隐藏状态空间\n2. 设计新度量工具，在隐藏状态层面量化探索和利用\n3. 实证检验两者在隐藏状态级别的相关性\n\n## 4. 方法设计：新度量工具的构建\n\n为在隐藏状态空间中量化探索和利用，作者引入了三级度量体系：\n\n### 4.1 零阶度量：有效秩(ER)量化探索\n- **定义**：ER = exp(-∑pⱼlog(pⱼ))，其中pⱼ是归一化奇异值\n- **物理意义**：衡量隐藏状态表示的语义多样性，高ER表示模型利用丰富多样的内部特征集，是探索行为的直接标志\n- **理论保证**：1 ≤ erank(Z) ≤ rank(Z) ≤ min{T,D}，提供比传统秩更连续、更细致的探索度量\n\n### 4.2 一阶度量：有效秩速度(ERV)量化利用\n- **定义**：ERV = Δ⁽¹⁾ER，衡量ER相对于历史平均的变化率\n- **物理意义**：捕捉信息获取效率，大ERV表示模型正以超过历史趋势的速度丰富表示，标志深入且富有成效的推理路线\n\n### 4.3 二阶度量：有效秩加速度(ERA)预测趋势\n- **定义**：ERA = Δ⁽²⁾ER，衡量ERV的变化率\n- **物理意义**：表明推理过程是在加速还是饱和，作为预测元控制器具有理论稳定性\n- **关键性质**：ERA独立于矩阵规模(Δ⁽²⁾ER = O(1))，提供稳定信号\n\n## 5. 实证验证：探索与利用的解耦证据\n\n通过新度量工具，作者获得了关键发现：\n\n### 5.1 响应级别分析\n- 正确和不正确推理路径在ER和ERV上表现不同：不正确路径显示更高ER和ERV，表明过度探索和过度信息获取可能偏离正确推理\n- 关键发现：正确推理轨迹在ERA上 consistently 表现更高值，表明信息获取的加速度是区分正确与错误推理的关键指标\n\n### 5.2 数据集级别分析\n- 策略优化与数据集级别多样性扩展呈强正相关：随着模型改进，数据集级别的ER及其一阶、二阶差异一致增加\n- ER揭示超越传统秩限制的优化：后期训练中传统秩平台化，表明模型已固定在有限数量的线性独立推理\"方向\"，而同时上升的ER表明模型在提高现有解决方案空间的质量\n\n### 5.3 解耦的核心证据\n- 在隐藏状态级别，探索(ER)和利用(ERV)显示接近零的相关性\n- 这提供了强有力的证据表明：**权衡不是RLVR推理的固有属性，而是有偏见的token级别测量的假象**\n- 两个能力不是对抗性的，而是可以解耦并同时增强的\n\n## 6. 方法创新：VERL - 超越权衡的协同增强框架\n\n基于探索和利用在隐藏状态级别解耦的核心洞察，作者提出了Velocity-Exploiting Rank-Learning (VERL)方法，通过直接塑造RL优势函数实现协同增强：\n\n### 6.1 稳定表示偏差指标\n- 计算当前轨迹的ER、ERV和ERA值\n- 通过指数移动平均(EMA)归一化这些值，计算相对偏差dₖ\n- 偏差dₖ量化当前轨迹表示结构与策略近期平均行为的偏离程度\n\n### 6.2 ERA驱动的动态权重机制\n- 定义两个正交单位向量：探索导向向量w_explore = [1,0]和利用导向向量w_exploit = [0,1]\n- 通过二阶度量偏差d₂动态插值权重：w_dyn = β·w_explore + (1-β)·w_exploit，其中β = sigmoid(d₂)\n- **核心机制**：当ERA高(d₂≫0)时，表示未来过度自信风险，增加β favor 探索；当ERA低(d₂≤0)时，表示推理饱和，减少β强调利用\n\n### 6.3 优势函数重塑\n- 定义辅助优势Φ = w_dyn,0·tanh(d₀) + w_dyn,1·tanh(d₁)\n- 重塑优势：Â = A⁽⁰⁾ + min[max(0,Φᵢ), |A⁽⁰⁾|/κ]，其中κ是稳定训练的裁剪因子\n- **协同效应**：前瞻性地放大探索奖励防止过度自信，同时加强利用收益巩固推理路径，同时增强两种能力\n\n## 7. 实验验证：VERL的有效性与泛化性\n\n### 7.1 性能提升\n- 在多个LLM和推理基准上显示一致增益，Gaokao 2024数据集上高达21.4%绝对准确率提升\n- 同时提高了Pass@1(利用能力)和Pass@k(探索能力)，特别是在挑战性基准上\n\n### 7.2 消融实验\n- **步长分析**：不同步长值下VERL性能提升稳健，表明底层信号对采样频率不过度敏感\n- **优势裁剪分析**：所有测试κ值下VERL一致增强性能，证明其稳定性\n- **信号组成分析**：仅使用探索相关项导致模型过早瓶颈，仅使用利用相关项导致快速平台化，结合两项实现最佳性能\n\n## 8. 理论贡献：重新定义探索与利用的关系\n\nVERL方法的核心贡献是重新定义了探索和利用在RLVR中的关系：\n\n1. **挑战传统观点**：探索-利用权衡是token级别分析的假象，而非推理的固有属性\n2. **揭示解耦现象**：在隐藏状态表示中，探索和利用可以解耦并同时增强\n3. **提供量化工具**：ER、ERV和ERA提供了量化隐藏状态动态的新框架\n4. **实现协同增强**：VERL是首个实现协同探索-利用增强原则的操作方法\n\n这一逻辑演进展现了从观察现象、质疑假设、设计新度量、实证验证到方法创新的完整科学研究链条，为RLVR领域提供了新的理论基础和实践方法。", "summary_translation": "在可验证奖励强化学习(Reinforcement Learning for Verifiable Rewards, RLVR)领域，一个普遍观点通过探索-利用权衡(exploration-exploitation trade-off)的视角来解释近期进展，这一观点主要受到token级别指标(token-level metrics)的影响。我们重新审视这一视角，提出这种感知到的权衡可能并非根本约束，而是测量层面的产物。为探究此问题，我们将分析转移到语义丰富的隐藏状态空间(hidden-state space)，采用有效秩(Effective Rank, ER)来量化探索，并提出其新型的一阶和二阶导数，即有效秩速度(Effective Rank Velocity, ERV)和有效秩加速度(Effective Rank Acceleration, ERA)，以捕捉利用动态。我们的分析揭示，在隐藏状态层面，探索和利用可以实现解耦(第4节)。这一发现揭示了同时增强这两种能力的机会。这一见解促成了我们的方法——速度利用秩学习(Velocity-Exploiting Rank-Learning, VERL)，这是首个通过直接塑造RL优势函数(RL advantage function)来实现协同探索-利用增强原则的方法。关键创新在于利用理论稳定的ERA作为预测性元控制器(meta-controller)，创建协同的双通道激励结构(dual-channel incentive structure)。VERL不强制权衡，而是前瞻性地放大探索奖励以预防过度自信，同时强化利用收益以巩固推理。在多种大型语言模型(LLMs)和推理基准上的实验显示了一致的性能提升，包括在具有挑战性的2024年高考数据集(Gaokao 2024 dataset)上准确率绝对提升高达21.4%。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#247", "title": "Anchored Supervised Fine-Tuning", "link": "/arxiv/2509.23753", "arxiv_id": "2509.23753", "authors": "He Zhu, Junyou Su, Peng Lai, Ren Ma, Wenjia Zhang, Linyi Yang, Guanhua Chen", "summary": "Post-training of large language models involves a fundamental trade-off between supervised fine-tuning (SFT), which efficiently mimics demonstrations but tends to memorize, and reinforcement learning (RL), which achieves better generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged as a promising middle ground, reweighting SFT objectives with token probabilities and achieving improvements in certain reasoning domains, though it exhibits instability in other tasks. We provide a analysis of DFT through the reward-weighted regression (RWR) framework, revealing that it corresponds to a specific auxiliary distribution choice that yields provably tighter RL bounds than standard SFT. However, our analysis also uncovers a critical limitation: this construction lacks distributional anchoring, leading to progressive drift that undermines training stability. To address this, we propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's reweighting with lightweight KL regularization to preserve tightness while ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation, achieving substantial improvements with minimal computational overhead. Our RWR framework provides a systematic lens for understanding post-training methods and demonstrates that principled theoretical analysis leads to both stronger guarantees and practical gains.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.992425", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是提出一种新的训练范式（Anchored Supervised Fine-Tuning, ASFT）来改进大语言模型的后训练过程。论文关注的是LLM的基础能力改进，特别是解决监督微调(SFT)和强化学习(RL)之间的权衡问题，旨在提高模型的推理能力和训练稳定性。论文明确提到在数学推理(mathematical reasoning)方面的改进，这直接符合\"改进LLM的通用推理能力\"的研究目标。 其次，论文包含多个正面指标：核心概念上讨论大语言模型；能力方向上专注于推理能力，特别是数学推理；训练方法上探讨了强化学习与监督微调的结合，提出了新的训练方法。 关于排除标准，虽然论文提到了在\"医学知识基础\"上的测试，但这只是作为评估模型性能的一个领域，而不是论文的主要焦点。论文的核心是提出一种通用的训练方法，而不是专注于医学或其他特定应用领域。论文也不涉及多模态与视觉、模型可靠性等排除领域。 在特殊和模糊情况处理方面，论文提出的ASFT方法通过KL正则化来确保训练稳定性，这间接有助于提高模型的推理质量和可靠性，符合提升模型通用推理能力的目标。 综上所述，这篇论文的核心贡献是提出一种新的通用训练方法来增强大语言模型的推理能力和稳定性，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型后训练中监督微调(SFT)与强化学习(RL)之间的效率与泛化权衡问题。针对SFT容易记忆而RL计算成本高的场景，我们提出了一种锚定监督微调(ASFT)方法，通过在动态微调(DFT)基础上添加轻量级KL正则化来防止分布漂移。在数学推理、医学知识和代码生成任务上，通过准确率和KL散度等指标验证了ASFT的优越性能和稳定性。", "inspiration_trace": "# 从权衡困境到锚定解决方案：ASFT方法的逻辑演进\n\n## 1. 宏观问题：大语言模型后训练的根本权衡\n\n作者首先识别了大语言模型(LLMs)后训练领域的核心困境：监督微调(SFT)与强化学习(RL)之间存在根本性权衡。\n\n**SFT的优势与局限**：\n- 高效：数据与计算效率高，能快速获取期望行为\n- 局限：倾向于记忆表面模式，缺乏鲁棒泛化能力\n\n**RL的优势与局限**：\n- 高效：通过结果驱动更新和探索，发现可转移行为，泛化性强\n- 局限：计算成本高、训练不稳定、实现复杂\n\n这一权衡激发了研究需求：如何保留SFT的效率，同时获得RL的泛化优势？\n\n## 2. 中观观察：现有折中方法的局限\n\n作者聚焦于动态微调(DFT)这一新兴折中方案，通过初步实验发现关键问题：\n\n**DFT的创新点**：\n- 识别标准SFT中的病态奖励结构（当模型概率接近零时导致无界方差）\n- 通过基于概率的重新加权解决此问题，在数学推理任务中取得显著改进\n\n**DFT的局限性**：\n- 领域特异性：在推理密集型领域表现出色，但在知识密集型任务中不稳定\n- 缺乏理论依据：设计选择缺乏理论基础\n- 训练不稳定性：某些任务中表现出明显的性能波动\n\n这些观察引出核心问题：DFT为何在某些领域有效而在其他领域失效？其不稳定的根源是什么？\n\n## 3. 理论框架：奖励加权回归(RWR)视角\n\n为深入理解DFT，作者采用奖励加权回归(RWR)框架进行分析，建立SFT和RL之间的理论连接：\n\n**SFT作为RL下界的理论解释**：\n- 在稀疏奖励设置下，SFT可视为优化RL目标的下界\n- 随着训练进行，当πθ偏离πref时，这一下界变得愈发松散\n\n**辅助分布的一般化框架**：\n- 通过辅助分布q(τ)可构建更紧密的界限\n- 辅助分布选择决定了界限紧密性与优化稳定性\n- 形成了\"有效性vs稳定性\"的基本权衡\n\n这一框架为理解DFT并开发新方法提供了系统性视角。\n\n## 4. DFT理论分析：优势与根本缺陷\n\n在RWR框架下，作者对DFT进行了深入分析，揭示其双重特性：\n\n**关键发现1：DFT对应特定辅助分布选择**\n- DFT目标等价于选择特定辅助分布构造\n- 这一构造直接恢复DFT序列级目标\n\n**关键发现2：DFT实现比SFT更紧密的界限**\n- 证明DFT产生的RL目标下界比标准SFT更紧密\n- 当策略对示范样本分配非均匀概率时，界限严格更紧密\n- 解释了DFT在策略分布表现出足够方差领域中的优越性能\n\n**关键发现3：DFT存在分布漂移**\n- 策略分布在训练中逐渐偏离参考分布\n- 辅助分布q越来越集中在高pθ(τ)轨迹上，形成反馈循环\n- 这种分布漂移破坏RWR框架基本假设，威胁下界保证有效性\n\n这一分析揭示了DFT领域特定有效性的理论解释，以及其不稳定性的根本原因。\n\n## 5. 核心假设：锚定机制解决分布漂移\n\n基于对DFT的理论分析，作者形成核心假设：通过添加分布锚定机制可解决DFT的分布漂移问题，同时保持其紧密性优势。\n\n**分布漂移的根源**：\n- DFT的辅助分布构造缺乏锚定机制\n- 基础不等式u ≥ 1 + log u仅在u=1时取等号\n- 在DFT中，当πθ(τ)=qθ(τ)时界限紧密，但随着训练进行，pθ(τ)变得非均匀，导致不等式松散\n\n**假设形成**：\n- 添加适当锚定机制可控制分布漂移\n- 保持界限紧密性，提高训练稳定性和性能\n- 关键在于找到平衡点：既允许足够探索获得紧密界限，又防止过度漂移导致不稳定\n\n## 6. 方法提出：锚定监督微调(ASFT)\n\n基于上述分析，作者提出锚定监督微调(ASFT)，通过轻量级KL正则化解决DFT的分布漂移问题：\n\n**方法设计**：\n- 在DFT目标函数基础上添加KL散度正则化项\n- 将策略约束在参考检查点的信任区域内\n- 数学表达式：LASFT(θ) = LDFT(θ) + λEs[DKL(πθ(·|s)∥πbase(·|s))]\n\n**理论保证**：\n- KL项不改变下界结构，保留DFT紧密性优势\n- 锚定机制提供明确方差控制，防止破坏性指数增长\n- 在参考策略周围创建信任区域，允许探索更紧密界限同时保持稳定性\n\n**实际实现**：\n- 在token级别实现，使用标准化每位置分配序列级权重\n- 确保与理论框架数学等效性，同时实现高效计算\n- 相比标准SFT仅需最小计算开销，仅添加简单KL惩罚\n\n## 7. 实验验证：多领域性能评估\n\n作者在多个领域对ASFT进行全面验证，证明其优越性：\n\n**数学推理**：\n- 10万训练样本下，ASFT平均比DFT提高4.85分(18.6%)，比基础模型提高17.89分(142%)\n- 在挑战性基准如AMC23上表现尤为突出(36.72% vs DFT的27.19%)\n\n**医学知识**：\n- 1万样本任务中，ASFT比SFT提高8.28分(24.8%)，比基础模型提高10.65分(33.9%)\n- 仅需完整RL方法训练成本的3%\n\n**代码生成**：\n- 在代码生成任务上实现最高平均分数，显著提高HumanEval和HumanEval+性能\n\n**训练稳定性**：\n- ASFT保持稳定KL散度，同时在领域内和领域外评估中实现优越性能\n- 解决了DFT在知识密集型任务中的严重分布漂移问题\n\n## 8. 扩展分析：方法鲁棒性与应用潜力\n\n作者进一步分析了ASFT的鲁棒性和应用潜力：\n\n**模型规模扩展**：\n- 在不同规模模型(LLaMA-2-7B/70B, Qwen2.5-7B/32B/72B)上，ASFT始终优于基线\n- 改进随模型规模扩大保持稳健，在低资源设置下实现有效稳定适应\n\n**与RL方法比较**：\n- ASFT优于所有基于SFT方法，显著缩小与先进RL方法(如DAPO)的差距\n- 性能排序SFT < ASFT < DAPO实证支持RWR框架理论预测\n\n**作为RL初始化的增强**：\n- ASFT为后续RL微调提供优越初始化点\n- ASFT+DAPO比SFT+DAPO获得一致增益(+3.86分)\n- 表明KL锚定不仅提高直接监督性能，还为RL优化创建更稳定基础\n\n**计算效率优化**：\n- 提出ASFT-LoRA解决内存限制问题\n- 利用低秩适应数学特性实现内存高效实现\n- 在资源受限环境中提供实用折中方案\n\n## 结论：从理论到实践的系统演进\n\n作者提出ASFT的逻辑链展现了从问题识别到方法解决的完整科学思维过程：从宏观权衡困境出发，通过中观观察发现DFT的局限性，构建理论框架深入分析，揭示根本缺陷并提出核心假设，最终设计出ASFT方法并通过多领域实验验证其有效性。这一演进过程体现了理论分析与实践创新的紧密结合，为解决大语言模型后训练的核心挑战提供了新思路。", "summary_translation": "大型语言模型的后训练涉及监督微调(supervised fine-tuning, SFT)和强化学习(reinforcement learning, RL)之间的基本权衡。前者能高效地模仿示例但容易导致记忆，后者能实现更好的泛化但计算成本更高。动态微调(Dynamic Fine-Tuning, DFT)最近作为一种有前景的中间路线出现，它通过标记概率(token probabilities)对SFT目标进行重新加权，并在某些推理领域取得了改进，尽管它在其他任务中表现出不稳定性。我们通过奖励加权回归(reward-weighted regression, RWR)框架对DFT进行了分析，揭示它对应于一种特定的辅助分布(auxiliary distribution)选择，这种选择产生的RL界限可证明比标准SFT更紧。然而，我们的分析也揭示了一个关键限制：这种构造缺乏分布锚定(distributional anchoring)，导致渐进漂移(progressive drift)，从而破坏训练稳定性。为解决这一问题，我们提出了锚定监督微调(Anchored Supervised Fine-Tuning, ASFT)，它通过轻量级KL正则化(KL regularization)增强DFT的重新加权机制，以保持紧致性同时确保稳定性。实验上，ASFT在数学推理、医学知识基础(medical knowledge grounding)和代码生成等任务上一致地优于SFT和DFT，以最小的计算开销实现了显著改进。我们的RWR框架为理解后训练方法提供了系统视角，并展示了基于原则的理论分析如何带来更强的保证和实际收益。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#215", "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment", "link": "/arxiv/2509.24610", "arxiv_id": "2509.24610", "authors": "Liang Lin, Zhihao Xu, Junhao Dong, Jian Zhao, Yuchen Yuan, Guibin Zhang, Miao Yu, Yiming Zhang, Zhengtao Yao, Huahui Yi, Dongrui Liu, Xinfeng Li, Kun Wang", "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing multiple human preferences: improvements in one dimension frequently come at the expense of others, creating unavoidable trade-offs between competing objectives like helpfulness and harmlessness. While prior work mainly focuses on constraint-based optimization algorithms and data selection strategies to mitigate conflicts, these approaches overlook the fundamental issue of resolving conflicts directly at the parameter level. In this paper, we present OrthAlign, an innovative approach that pioneers a new paradigm by leveraging orthogonal subspace decomposition to fundamentally resolve gradient-level conflicts in multi-objective preference alignment. OrthAlign strategically decomposes parameter update spaces into orthogonal subspaces, ensuring that optimization toward different preferences occurs in mathematically non-interfering directions. Building upon this, we provide theoretical guarantees demonstrating that when parameter increments satisfy both orthogonal subspace constraints and spectral norm bounds, the resulting updates exhibit linear Lipschitz growth rather than exponential instability, ensuring stable convergence across all preference dimensions. Extensive experiments show that: I. OrthAlign achieves maximum single-preference improvements ranging from 34.61% to 50.89% after multiple-objective alignment across helpful, harmless, and truthful dimensions. II. With an average overall reward improvement of 13.96%.", "subjects": "Machine Learning, Computation and Language", "date": "2025-09-29", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.938049", "filter_reason": "根据筛选标准，这篇论文符合我的研究目标。首先，从核心判断来看，该论文的本质是提出一种名为OrthAlign的新方法，通过正交子空间分解技术解决大语言模型在多目标对齐中的参数级冲突问题。这明显属于改进LLM基础能力和提出新训练范式的研究范畴，而非将LLM作为工具应用到特定领域。 其次，论文明确涉及LLM核心概念，并关注模型的基础能力改进。虽然论文没有直接讨论推理能力，但多目标对齐（如同时优化有用性、无害性和真实性）是提升LLM整体能力的基础，间接支持更好的推理和问题解决能力。 第三，论文不涉及任何排除标准中的领域。它既不关注多模态与视觉，也不针对特定应用领域（如医疗、化学等），同时虽然涉及安全性（无害性），但这是作为多目标对齐的一个维度进行研究，而非专门研究模型可靠性的应用层面技术。 最后，在特殊和模糊情况处理上，论文虽然涉及安全性，但提出了基础性的技术解决方案（正交子空间分解），而非进行社会学研究或应用层面的讨论。这种方法通过解决参数更新中的冲突，确保模型在多个维度上都能稳定提升，从而增强模型的通用能力。 因此，这篇论文致力于提高LLM的基础能力，提出新的训练范式，与\"大语言模型通用推理能力\"的研究目标一致。", "summary2": "本文旨在解决大型语言模型在多目标偏好对齐中的冲突问题。针对多个竞争的人类偏好（如有用性、无害性和真实性），我们提出了一种OrthAlign方法，利用正交子空间分解确保不同偏好的参数更新在数学上不相互干扰，并在Llama-3-SFT和Mistral-SFT等多个模型上通过无害率、有用胜率和TruthfulQA MC2等指标验证了其有效性。", "inspiration_trace": "# OrthAlign方法逻辑链分析\n\n## 1. 宏观问题：多目标对齐的根本困境\n\n作者从大语言模型(LLM)对齐领域的核心挑战出发：当模型需要同时满足多种人类偏好时(如有用性、无害性和诚实性)，在一个维度上的改进往往导致其他维度性能下降，形成不可避免的权衡取舍。这种困境限制了LLM在实际应用中的表现，因为现实场景通常需要模型同时满足多个有时相互冲突的目标。\n\n## 2. 问题观察：现有方法的局限性\n\n作者系统分析了三类主流方法及其局限：\n\n- **约束优化方法**：通过添加约束项缓解冲突，但仅是\"妥协\"而非\"解决\"\n- **数据选择策略**：依赖人工筛选数据，引入系统偏见且劳动密集\n- **模型合并方法**：合并多个专门模型，但牺牲了单个目标的性能\n\n关键观察：这些方法都未触及问题的本质——**参数层面的梯度干扰**。作者通过数学分析发现，不同目标的参数更新梯度不是正交的，而是相互干扰的，量化表达为：\n```\n|⟨∇θL(Di), ∇θL(Dj)⟩| / (||∇θL(Di)||² · ||∇θL(Dj)||²) ≠ 0, for i ≠ j\n```\n\n## 3. 理论洞察：正交子空间分解的潜力\n\n基于上述观察，作者提出核心假设：**如果将不同目标的参数更新限制在相互正交的子空间中，可从根本上消除目标间干扰**。这一假设建立在两个理论基础：\n\n1. **奇异值分解(SVD)理论**：参数矩阵可分解为不同奇异值对应的正交子空间，与小特征值相关的空间与当前偏好信息近似正交\n2. **稳定性理论**：当参数增量同时满足正交子空间约束和谱范数界限时，更新呈现线性Lipschitz增长而非指数不稳定\n\n数学上，这意味着通过正交投影矩阵P⊥约束新偏好的梯度更新，使增量矩阵ΔW被限制在互不干扰的正交子空间中。\n\n## 4. 方法设计：OrthAlign框架\n\n基于理论洞察，作者设计了OrthAlign方法，包含三个关键组件：\n\n### 4.1 正交化偏好更新与稳定性控制\n- 将参数更新空间分解为正交子空间\n- 已对齐偏好的更新发生在主奇异向量空间\n- 新偏好更新限制在与主子空间正交的补空间中\n- 提供理论保证：满足约束时更新呈现线性增长而非指数不稳定\n\n### 4.2 自适应子空间秩选择\n- 关键洞察：低秩约束下影响微小的方向，在奇异值更新后可能变得显著\n- 动态秩选择规则：重新缩放最后k个奇异值为前r个的平均值，选择最大k使奖励偏移保持在容忍度τ内\n- 数学表达：`k = max{k: R(UΣ̂(k)Vᵀ; Xsafe) - R(W; Xsafe) ≤ τ}`\n\n### 4.3 子空间约束的多偏好对齐\n- 基于选定左奇异向量形成投影矩阵：`P = ÛÛᵀ`\n- 通过投影操作约束新偏好的参数更新：`ΔW_new = P · ∇W L_new(W)`\n- 确保参数更新严格限制在与先前偏好正交的子空间内\n\n## 5. 实验验证：效果与泛化性\n\n作者通过全面实验验证OrthAlign的有效性：\n\n- **多目标对齐性能**：在两目标场景超越最佳基线8.77%，三目标场景平均提升13.96%\n- **分布保持分析**：t-SNE可视化显示OrthAlign保持分布一致性，基线方法则出现明显偏移\n- **作为性能增强器**：可提升现有方法的无害性25.06%和有用性4.86%\n- **自适应秩选择影响**：展示不同秩设置下的目标权衡模式，验证动态选择的必要性\n\n## 6. 逻辑链条总结\n\nOrthAlign的完整逻辑链条体现了从问题到解决方案的系统思考：\n1. 识别LLM多目标对齐中的根本困境\n2. 观察到现有方法未解决参数层面的梯度干扰\n3. 提出正交子空间分解可消除干扰的理论假设\n4. 设计包含正交化更新、自适应秩选择和子空间约束的方法框架\n5. 通过实验验证方法的有效性、稳定性和泛化性\n\n这一创新方法通过在参数级别实现非干扰更新，为多目标对齐提供了全新范式，从根本上解决了目标间的权衡取舍问题。", "summary_translation": "大语言模型（Large language model, LLM）对齐（alignment）在处理多种人类偏好时面临一个关键困境：某一维度的改进往往以其他维度为代价，在有用性（helpfulness）和无害性（harmlessness）等竞争目标之间产生不可避免的权衡取舍。尽管先前的工作主要集中在基于约束的优化算法（constraint-based optimization algorithms）和数据选择策略（data selection strategies）来缓解冲突，但这些方法忽视了在参数层面直接解决冲突的根本问题。在本文中，我们提出了OrthAlign，这是一种创新方法，通过利用正交子空间分解（orthogonal subspace decomposition）开创了新范式，从根本上解决了多目标偏好对齐中的梯度级冲突（gradient-level conflicts）。OrthAlign策略性地将参数更新空间（parameter update spaces）分解为正交子空间（orthogonal subspaces），确保针对不同偏好的优化在数学上不相互干扰的方向上进行。在此基础上，我们提供了理论保证，证明当参数增量（parameter increments）同时满足正交子空间约束（orthogonal subspace constraints）和谱范数界限（spectral norm bounds）时，所产生的更新表现出线性Lipschitz增长（linear Lipschitz growth）而非指数不稳定性（exponential instability），确保在所有偏好维度上实现稳定收敛。大量实验表明：I. 在有用性、无害性和真实性维度上进行多目标对齐后，OrthAlign实现了34.61%至50.89%的最大单偏好改进。II. 平均总体奖励改进达到13.96%。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#252", "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models", "link": "/arxiv/2509.23676", "arxiv_id": "2509.23676", "authors": "Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang", "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-28", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:04.995120", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是深入研究大型推理模型(LRMs)中的推理机制，探讨推理轨迹如何影响最终答案生成，这直接关系到LLM的基础推理能力提升。论文通过三个阶段（实证评估、注意力分析和机制干预）来理解推理过程在模型内部的功能作用，属于对LLM内在推理能力的机制性研究，而非将LLM作为工具应用到特定领域。 其次，论文明确符合多个正面指标：它研究的是大型推理模型(LRMs)，直接属于LLMs范畴；核心聚焦于reasoning能力，探索推理过程与答案生成的关系；虽然未明确提及特定训练方法或新兴范式，但研究的是DeepSeek R1蒸馏模型，这类模型通常涉及先进的训练技术。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 最后，论文的特殊情况处理方面，它不是关于智能体/工具使用的研究，也不是主要关注幻觉/可解释性/安全问题，而是专注于推理过程的机制性分析，这恰恰是提升LLM通用推理能力的核心研究方向。 因此，这篇论文的核心贡献是深入理解LLMs如何利用推理标记生成答案，揭示中间推理在塑造模型输出中的功能作用，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在 [解决大型推理模型中推理过程与答案生成之间的关系问题]。针对 [DeepSeek R1蒸馏模型]，我们提出了一种 [三阶段分析方法（实证评估、注意力分析和机制干预）]，并在 [MATH-500和WildBench数据集] 上通过 [答案准确率、注意力权重和归一化logit差异] 验证了其有效性。", "inspiration_trace": "# 从宏观问题到方法论：论文核心方法的逻辑演进\n\n## 一、宏观问题的确立\n\n论文始于一个根本性问题：**大型推理模型(LRMs)生成的显式推理痕迹在多大程度上影响最终答案的生成？**\n\n作者观察到，DeepSeek R1等新型推理模型会生成两个明确分离的部分：推理段(Reasoning segment)和答案段(Answer segment)。这引发了一个核心疑问：这些推理痕迹是真正用于生成答案的关键组成部分，还是仅仅作为事后的合理化解释？这一问题触及模型推理的本质，关系到模型的可解释性、可信度和可控性。\n\n## 二、问题聚焦与研究空白\n\n作者通过文献分析发现现有研究的局限性：\n\n1. **领域局限**：大多数研究局限于数学和代码领域，缺乏对开放领域的广泛验证\n2. **方法局限**：现有研究主要关注推理长度与答案质量的关联性，而非推理与答案间的功能依赖关系\n3. **机制不明**：推理痕迹如何被模型内部利用的信息流机制尚不清楚\n\n基于这些空白，作者将宏观问题聚焦为三个具体研究问题：\n- 推理痕迹是否提高答案质量？\n- 答案如何关注推理过程？\n- 推理的小变化能否改变答案？\n\n## 三、研究假设的形成\n\n针对上述问题，作者提出三个核心假设：\n\n1. **功能假设**：推理痕迹不仅提高答案质量，且这种改进在多个领域普遍存在，尤其在蒸馏模型中更为显著\n2. **注意力假设**：答案标记会显著关注推理标记，且存在特定的\"推理聚焦头\"(RFHs)跟踪推理轨迹\n3. **因果假设**：推理和答案之间存在功能依赖关系，对推理痕迹的微小干预会直接影响最终答案\n\n## 四、方法论设计：三阶段递进研究框架\n\n为验证上述假设，作者设计了一个由表及里、层层递进的三阶段研究方法：\n\n### 阶段一：实证评估（黑盒视角）\n**目的**：从外部验证推理痕迹对答案质量的影响\n**方法**：比较模型在有推理痕迹和无推理痕迹情况下的性能\n**创新点**：将评估扩展到数学(MATH-500)和开放领域(WildBench)，超越前人仅限于数学和代码的研究\n**逻辑**：如果推理痕迹确实被用于生成答案，包含推理痕迹应显著提高答案质量\n\n### 阶段二：注意力分析（灰盒视角）\n**目的**：探究模型内部信息流机制，特别是答案如何关注推理过程\n**方法**：分析答案标记对不同提示段的注意力模式，从段级到层级再到头级\n**创新点**：发现并分析特定的\"推理聚焦头\"(RFHs)，这些头跟踪推理过程，包括自我反思步骤\n**逻辑**：注意力机制控制transformer模型中的信息流，若推理痕迹被用于生成答案，答案标记应显著关注推理标记\n\n### 阶段三：机制干预（白盒视角）\n**目的**：验证推理和答案之间的因果关系\n**方法**：使用激活修补(activation patching)技术，在受控环境中扰动推理痕迹\n**创新点**：设计新的\"上下文对象比较\"任务，创建对齐的干净和损坏提示，实现精确干预\n**逻辑**：强注意力本身不保证功能依赖，需通过干预验证推理痕迹对答案生成的因果影响\n\n## 五、方法论的内在逻辑演进\n\n这一三阶段方法体现了从现象到机制、从相关到因果的逻辑演进：\n\n1. **由表及里**：从外部行为观察(实证评估)到内部机制分析(注意力分析)再到因果验证(机制干预)\n\n2. **由相关到因果**：首先建立推理与答案的相关性(实证评估)，然后探究信息流动路径(注意力分析)，最后验证因果关系(机制干预)\n\n3. **多角度验证**：每个阶段从不同角度验证核心假设，形成证据链，增强结论可靠性\n\n4. **工具创新**：在分析过程中开发新工具(RFHs)用于解释和调试模型行为，扩展方法的应用价值\n\n## 六、总结\n\n论文作者从\"推理痕迹如何影响答案生成\"这一宏观问题出发，通过识别现有研究空白，形成三个核心假设，最终设计出一个由表及里、层层递进的三阶段研究框架。这一方法论不仅系统回答了核心问题，还提供了理解、解释和调试推理模型的新工具，体现了\"观察-假设-验证-创新\"的完整科学研究逻辑链。", "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在生成最终答案的同时会产生显式推理轨迹，然而这些轨迹对答案生成的影响程度仍不明确。在本研究中，我们对三个蒸馏版的DeepSeek R1模型中推理与答案生成之间的相互作用进行了三阶段调查。首先，通过实证评估，我们证明包含显式推理能持续提升不同领域的答案质量。其次，注意力分析表明，答案标记（answer tokens）在很大程度上关注推理标记（reasoning tokens），其中某些中层推理聚焦头（Reasoning-Focus Heads, RFHs）密切跟踪推理轨迹，包括自反思线索。第三，我们使用激活修补（activation patching）应用机制干预，以评估答案标记对推理激活的依赖性。我们的结果显示，对关键推理标记的扰动能够可靠地改变最终答案，这证实了从推理到答案存在方向性和功能性的信息流动。这些发现加深了我们对LRMs如何利用推理标记生成答案的理解，突显了中间推理在塑造模型输出中的功能性作用。我们的数据和代码在此URL公开可用：\\href{https://aka.ms/R2A-code}{https://aka.ms/R2A-code}。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#266", "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning", "link": "/arxiv/2509.23292", "arxiv_id": "2509.23292", "authors": "Ningning Xu, Yuxuan Jiang, Shubhashis Roy Dipta", "summary": "Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.033473", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是改进大语言模型的基础能力，特别是工具使用能力。论文提出了\"Pattern-Aware Tool-Integrated Reasoning\"这一新框架，专注于提升LLM\"如何使用工具\"而不仅仅是\"何时使用工具\"，这是一种新的训练范式，旨在增强模型的推理能力，特别是在数学推理方面。这完全符合第一步中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的保留标准。 其次，从正面指标来看，论文包含了多个相关主题： - 核心概念：明确提到了\"large reasoning models (LRMs)\"，与LLMs直接相关 - 能力方向：重点关注\"reasoning\"，特别是\"math reasoning\" - 新兴范式：深入探讨\"tool use\"，这是LLM新兴范式的重要组成部分 第三，论文不符合任何排除标准。它不涉及多模态与视觉内容，不聚焦于特定应用领域（虽然测试于数学数据集，但方法是通用的），也不主要讨论模型可靠性等应用层面问题。 最后，从特殊和模糊情况处理来看，论文提出的是一种通用的工具使用方法，旨在增强LLM的通用问题解决能力，而不是将工具应用在特定领域。这完全符合\"提出一种通用的智能体协作框架或工具使用方法来增强LLM的通用问题解决能力\"的保留标准。 综上所述，这篇论文的核心贡献是提出了一种新的、模式感知的工具集成推理框架，通过改进LLM的工具使用能力来提升其通用推理能力，特别是在数学推理方面取得了显著进步。这完全符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决工具集成推理中工具使用模式选择不当导致失败的问题。针对数学推理场景，我们提出了一种模式感知的两阶段框架，首先构建calculator和algorithmic两种模式的代码能力，然后通过DPO对齐模式选择偏好，并在MATH500和AIME24数据集上通过Code@1和Pass@1指标验证了其有效性。", "inspiration_trace": "# 从问题观察到方法形成：模式感知工具集成推理的逻辑演进\n\n## 1. 宏观问题与初始观察\n\n**宏观问题**：如何提高大型推理模型(LRMs)在复杂问题上的表现？\n\n**初始观察**：\n- 工具集成推理(TIR)已成为提升LRMs能力的关键方法\n- 现有研究聚焦于\"何时调用工具\"(when)，而忽视了\"如何使用工具\"(how)\n- 作者观察到两种工具使用模式：计算器模式(直接计算)和算法模式(问题编码为程序)\n\n## 2. 问题识别与关键发现\n\n**核心问题**：模型常将工具使用策略与问题需求不匹配，以机械方式应用代码，导致工具失效。\n\n**关键案例**：\n- 计算1000! ÷ (800! × 2!)：计算器模式导致溢出，算法模式(先代数化简)成功\n- 寻找π中第一个10位素数：算法模式因上下文限制失败，计算器模式(扫描式)成功\n\n**关键发现**：失败源于模式选择不当，而非推理能力不足。正确的模式选择能将失败转为成功。\n\n## 3. 假设形成\n\n**核心假设**：若能训练模型根据问题特性选择合适的工具使用模式，将显著提高TIR效果。\n\n**假设依据**：\n- 不同数学问题适合不同工具策略\n- 模型需同时掌握两种模式能力并能智能选择\n- 现有方法缺乏对工具使用模式的显式建模\n\n## 4. 方法设计：两阶段学习框架\n\n**第一阶段：代码能力获取**\n- 目标：使模型掌握两种工具使用模式的能力\n- 方法：对每个问题生成两种风格解决方案(计算器风格和算法风格)\n- 训练：使用监督学习，最小化负对数似然\n- 期望：模型能稳定生成两种模式的可执行代码\n\n**第二阶段：模式偏好对齐**\n- 目标：使模型能根据问题特性选择合适模式\n- 方法：使用直接偏好优化(DPO)学习模式选择偏好\n- 训练：利用教师模型标记的偏好数据\n- 期望：模型能智能选择最适合问题的工具使用模式\n\n## 5. 实验验证与结果\n\n**数据构建**：\n- 使用Gemini-2.5-flash-lite作为教师模型\n- 对OpenR1-Math-220k数据集生成双模式解决方案\n- 评估集包括MATH500、AIME24等挑战性数学基准\n\n**关键结果**：\n- 基础模型几乎无代码使用能力\n- SFT后模型开始广泛使用Python代码\n- 添加DPO后，代码使用率和准确率显著提升\n- 例如，AIME24准确率从26.7%提高到50.0%\n\n## 6. 结论与贡献\n\n**主要结论**：TIR中的失败常源于模式选择不匹配，而非推理能力不足。模式感知方法能显著提高工具使用效果。\n\n**核心贡献**：\n1. 识别并形式化了TIR中的模式不匹配问题\n2. 提出两阶段框架：先构建代码能力，再对齐模式选择\n3. 实证证明该方法显著提高代码可靠性和任务性能\n\n这一逻辑演进从宏观问题出发，通过具体案例观察，形成核心假设，设计针对性解决方案，并通过实验验证其有效性，最终形成了一个完整的模式感知工具集成推理方法。", "summary_translation": "工具集成推理（Tool-integrated reasoning, TIR）已成为提升大型推理模型（Large Reasoning Models, LRMs）解决复杂问题能力的关键方法。先前的研究主要关注何时调用工具，而忽视了工具的应用方式。我们识别出两种常见模式：一种是使用代码进行直接计算的\"计算器模式\"（calculator pattern），另一种是将问题编码为程序的\"算法模式\"（algorithmic pattern）。即使推理过程是合理的，模式选择不当也常常导致失败。我们提出了一个两阶段框架，首先从两种模式中构建代码能力，然后将模式选择与教师偏好对齐。在多个具有挑战性的数学数据集上，我们的模式感知方法显著提升了代码使用率和准确性，例如将MATH500数据集上的Code@1从64.0%提高到70.5%，将AIME24数据集上的Code@1从26.7%提高到50.0%。这些成果突显了模式感知方法在工具集成推理中的有效性。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#263", "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking", "link": "/arxiv/2509.23392", "arxiv_id": "2509.23392", "authors": "Jinyi Han, Ying Huang, Ying Liao, Zishang Jiang, Xikun Lu, Haiquan Zhao, Xinyi Wang, Guanghao Zhou, Sihang Jiang, Jiaqing Liang, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao", "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.006015", "filter_reason": "这篇论文完全符合研究目标。论文的核心贡献是提出一种名为\"Just-Enough Thinking\"（JET）的新训练方法，用于优化大型推理模型(LRMs)的推理过程。论文发现这些模型在推理早期已经积累了足够信息，后续步骤往往是冗余的，因此训练模型主动终止不必要的推理步骤。这直接符合研究目标中\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、数学、规划、多步推理等通用能力\"的要求。论文不是将LLM作为工具应用到特定领域，而是专注于提升模型本身的通用推理能力，特别是推理效率。它符合多个正面指标，包括核心概念（大型推理模型）、能力方向（推理）和训练方法（强化学习）。同时，论文不符合任何排除标准，没有涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的内容。因此，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决大型推理模型（LRMs）在复杂任务中过度思考导致的计算效率低下问题。针对LRMs在推理过程中产生冗余步骤的场景，我们提出了一种Just-Enough Thinking (JET)方法，通过轨迹截断和质量控制的长度奖励训练模型主动终止不必要推理，并在多个数学推理基准测试上通过准确率和输出长度指标验证了其有效性。", "inspiration_trace": "# 大型推理模型的\"过度思考\"问题与解决方案：JET方法的逻辑演进\n\n## 1. 宏观问题：大型推理模型的计算效率挑战\n\n大型推理模型(LRMs)在复杂推理任务上展现出卓越性能，但其System-2式深度推理过程伴随着巨大的计算成本。这些模型往往执行超出必要数量的推理步骤，消耗过多计算资源才能得出正确答案，这种现象被称为\"过度思考\"(overthinking)。这与人类认知的高效性和适应性形成鲜明对比，因此开发在保持高准确性的同时提高计算效率的方法成为LRMs面临的核心挑战。\n\n## 2. 现有解决方案及其局限性\n\n为解决推理效率问题，强化学习(RL)已成为有前景的范式，通过额外奖励信号引导模型生成既正确又简洁的答案。现有方法主要分为两类：\n\n- **自适应思维模式选择方法**：通过监督微调为模型配备多种推理模式(思考/不思考)，然后用RL选择最适合每个问题的模式。\n- **基于长度的优化方法**：引入显式长度奖励，鼓励更简洁的推理。\n\n然而，这些方法面临根本性局限：LRMs自然倾向于冗长输出，很少生成简短推理轨迹，导致训练数据偏差；而人工压缩答案会引入模型自然生成分布与人工缩短样本间的显著分布不匹配，破坏梯度更新，损害学习效果。\n\n## 3. 关键观察：证据积累模型的启示\n\n作者从认知科学中的证据积累模型(Evidence Accumulation Models)获得启发。该模型描述人类决策是一个动态过程，其中信息被整合直到达到阈值，之后进一步的证据仅用于支持决策。基于此，作者提出假设：LRM推理过程类似，推理轨迹的早期部分已包含足够信息确定最终答案，继续生成只会导致冗余推理。\n\n为验证这一假设，作者进行了初步实验：在MATH500数据集上评估限制模型仅使用推理序列早期部分对答案正确性的影响。实验结果令人惊讶：\n- 限制模型仅使用推理的前75%保留了超过90%的原始正确解决方案\n- 仅使用前一半仍在约四分之三的问题上产生正确答案\n- 即使只有四分之一的推理，近一半的原始正确解决方案仍然有效\n\n这表明关键问题解决信息在早期积累，后续步骤对正确性贡献很小。\n\n## 4. 核心洞察：推理过程中的信息积累特性\n\n基于实验结果，作者形成核心洞察：LRMs在推理过程早期阶段已积累大部分关键问题解决信息，允许它们在显著减少推理token数量的同时保持高准确性。这一发现为解决\"过度思考\"问题提供了新思路——如果模型能够在积累足够信息后主动停止推理，就能在保持准确性的同时大幅提高计算效率。\n\n## 5. 方法论设计：Just-Enough Thinking (JET)\n\n基于上述洞察，作者提出Just-Enough Thinking (JET)方法，训练模型主动终止不必要的推理。JET通过两个关键组件实现：\n\n### 5.1 两阶段Rollout构建\n- **阶段1：完整推理** - 标准rollout阶段，模型生成完整推理轨迹，捕获不受约束的推理行为。\n- **阶段2：轨迹截断** - 通过截断完整轨迹构建更短推理路径，在每个截断点添加\"停止思考\"提示，鼓励模型立即提供结论。\n\n为有效识别截断位置，作者提出渐进式提前停止(PES)策略，为每个完整推理轨迹生成一系列截断变体，增加包含最优截断点的机会，同时保持与模型生成分布的一致性。\n\n### 5.2 奖励和目标设计\n- **基础奖励**：结合格式奖励和正确性奖励，确保答案格式正确且准确。\n- **长度奖励**：设计基于准确性的长度奖励，遵循三个原则：\n  1. 正确性优先：只有正确响应才有资格获得长度奖励\n  2. 简洁性偏好：在正确响应中，较短推理路径获得更高奖励\n  3. 按问题标准化：奖励相对于每个问题的最短和最长正确响应进行测量\n\n这种奖励设计确保模型在生成正确答案的同时，倾向于更简洁的推理路径。\n\n## 6. 实验验证与结果\n\n作者在多个基准测试上验证JET的有效性，结果表明：\n\n- JET实现了显著的输出长度减少而不影响准确性，例如DeepSeek-Distill-Qwen-1.5B在Olympiad基准上准确率提高4.6%，同时输出长度减少46.3%\n- 在具有挑战性的数学推理任务上表现出色，展现出任务感知能力，能根据任务难度动态调整推理深度\n- 在不同模型规模上保持稳定性能\n- 在常识推理任务上表现出强大的泛化能力\n\n## 7. 逻辑演进总结\n\n作者从大型推理模型的计算效率挑战出发，分析现有方法的局限性，从认知科学获得启发形成假设，通过实验验证推理过程中的信息积累特性，最终设计出JET方法。这一方法通过两阶段rollout构建和质量控制的长度奖励，成功解决了LRMs在强化学习过程中很少生成短推理路径的问题，实现了在保持准确性的同时显著提高推理效率的目标。整个逻辑链条展示了从问题观察到解决方案的完整思考过程，体现了跨领域思维和实证验证相结合的研究方法。", "summary_translation": "大型推理模型（Large Reasoning Models, LRMs）在具有挑战性的任务上取得了令人瞩目的性能，然而它们的深度推理（deep reasoning）通常会产生大量的计算成本（computational costs）。为了实现高效推理（efficient reasoning），现有的强化学习（reinforcement learning）方法在推演（rollout）阶段仍然难以构建简短的推理路径（reasoning path），限制了有效学习。受证据积累模型（Evidence Accumulation Models）的启发，我们发现LRMs在推理早期已经积累了足够的信息，使得进一步的推理步骤（reasoning steps）变得冗余（redundant）。基于这一见解，我们提出了适度思考（Just-Enough Thinking, JET）方法，该方法训练模型主动终止（proactively terminate）不必要的推理（unnecessary reasoning）。JET在推演阶段执行轨迹截断（trajectory truncation），使模型接触到简短且分布一致（distributionally consistent）的推理路径。此外，它使用质量控制的长度奖励（quality-controlled length reward）来更好地鼓励简洁推理（concise reasoning）同时保持正确性（maintaining correctness）。大量实验（Extensive experiments）表明，JET在不牺牲准确性（sacrificing accuracy）的情况下显著提高了推理效率（reasoning efficiency）。特别是，DeepSeek-Distill-Qwen-1.5B模型在奥林匹克基准测试（Olympiad benchmark）上实现了4.6%的准确性提升（accuracy gain），同时将输出长度（output length）减少了46.3%。我们的代码可在GitHub上获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#269", "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning", "link": "/arxiv/2509.23129", "arxiv_id": "2509.23129", "authors": "Haotian Liu, Shuo Wang, Hongteng Xu", "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.035408", "filter_reason": "这篇论文的核心贡献是提出了一种名为C$^2$GSPG的置信度校准组序列策略梯度方法，旨在通过强化学习增强大语言模型的自我意识推理能力。论文直接针对LLM的通用推理能力进行改进，特别是解决现有RL方法中的过度自信问题，提高模型在逻辑和数学推理任务中的表现。它符合筛选标准中的核心判断，即论文本质是关于改进LLM的基础能力和提出新的训练范式。论文包含多个正面指标，如关注LLMs的核心概念、reasoning能力方向以及reinforcement learning训练方法。同时，论文不符合任何排除标准，不涉及多模态、特定应用领域或模型可靠性的应用层面研究。虽然论文讨论了过度自信问题（与幻觉相关），但它是从提升推理能力的角度出发，符合\"提出新方法来减少幻觉，从而提升模型的通用可靠性和推理质量\"的保留标准。因此，这篇论文完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决强化学习推理模型中的过度自信问题，实现自我感知推理。针对逻辑和数学推理任务，我们提出了一种置信度校准的组序列策略梯度方法（C²GSPG），通过GSPG框架消除token级别偏差，并应用交叉熵正则化器校准模型置信度与序列奖励的一致性。在K&K逻辑推理数据集和多个数学推理benchmark上，通过准确率、Brier分数和期望校准误差（ECE）等指标验证了该方法在推理性能和置信度校准方面的优越性。", "inspiration_trace": "# C²GSPG方法提出的逻辑链分析\n\n## 一、宏观问题：推理模型的过度自信困境\n\n作者从强化学习(RL)在推理模型中的应用中发现一个根本性问题：尽管GRPO及其变体能提升大语言模型(LLM)的推理能力，但这些方法普遍存在**过度自信(overconfidence)问题**——模型无法意识到自己的错误。具体表现为模型生成序列的概率与相应奖励之间存在严重不匹配：高概率序列往往导致低奖励，使模型缺乏自我认知能力。\n\n## 二、问题深入：现有方法的局限性分析\n\n作者深入探究了现有解决方案的不足，发现两个关键缺陷：\n\n1. **策略优化与置信度校准的冲突**：现有方法通过调整奖励函数或降低高置信度低质量样本的重要性来缓解过度自信，但这些方法往往牺牲推理准确性或无法充分校准置信度。\n\n2. **token-level偏差问题**：GRPO等方法在token级别计算策略梯度，导致低概率token在训练中过度主导，影响模型稳定性。\n\n这些观察引出一个核心问题：**如何在不损害策略优化的前提下实现有效的置信度校准？**\n\n## 三、关键洞察：序列级概率与奖励的一致性\n\n作者提出一个基本假设：**模型生成序列的概率应反映其对问题的真实置信度**。基于此，作者获得两个关键洞察：\n\n1. **序列级概率的优越性**：相比token-level概率或口头表达的置信度，归一化的序列级概率能更直接、更普遍地衡量模型的内部确定性。\n\n2. **二元奖励场景下的协同性**：在二元奖励场景中，策略优化和置信度校准的目标天然一致——两者都希望模型对正确答案更自信，对错误答案更不自信。\n\n## 四、理论假设：梯度方向一致性\n\n基于上述洞察，作者提出一个可验证的理论假设：**在二元奖励场景下，策略优化方向和置信度校准方向具有相同的梯度符号**。这一假设若成立，意味着两个目标可以协同工作而非相互冲突。\n\n作者通过数学推导证明了这个假设：对于正确响应(ri=1)，策略优化方向(ri-m)和正则化方向(ri-cθ,i)均为非负；对于错误响应(ri=0)，两者均为非正。这种一致性为设计统一的目标函数提供了理论基础。\n\n## 五、方法构建：C²GSPG的设计\n\n基于理论假设，作者构建了C²GSPG方法，包含两个核心组件：\n\n### 1. Group Sequence Policy Gradient (GSPG)框架\n- **消除token-level偏差**：在序列级别而非token级别计算策略梯度，确保所有token被均匀对待。\n- **置信度定义**：使用归一化序列级概率作为模型置信度：cθ,i = exp(1/|oi|∑logπθ,i,t)\n- **置信度调制优势函数**：设计˜Ai = (ri-m)/(1-cθold,i)，使低置信度序列在梯度更新中获得适当权重。\n\n### 2. 置信度校准正则化器\n- **二元交叉熵正则化**：添加β[ri log cθ,i + (1-ri) log(1-cθ,i)]项，直接对齐置信度与奖励。\n- **理论保证**：在二元奖励场景下，正则化器与策略优化共享相同梯度方向，形成协同效应。\n\n### 3. 非二元奖励场景的扩展\n- **非线性奖励归一化**：使用sigmoid函数将任意奖励映射到[0,1]范围，保持奖励顺序的同时放大正负反馈差距。\n- **自适应正则化裁剪**：当梯度方向冲突时(Ii(β)=0)，禁用正则化项，避免干扰有效的策略信号。\n\n## 六、验证与完善：实验驱动的迭代\n\n作者通过逻辑和数学推理任务验证方法，并根据实验结果进一步完善：\n\n1. **二元奖励场景验证**：在数学推理任务中，C²GSPG显著提高了准确性和置信度校准，证实了理论假设的正确性。\n\n2. **非二元奖励场景优化**：在逻辑推理任务中，通过消融实验验证了非线性归一化和自适应裁剪的必要性，解决了梯度冲突问题。\n\n3. **超参数调优**：发现正则化强度β控制准确性与校准之间的权衡，为实际应用提供了调节手段。\n\n## 七、总结：从问题到解决方案的完整逻辑链\n\n作者从\"推理模型过度自信\"这一宏观问题出发，通过分析现有方法的局限性，提出\"序列级概率与奖励一致性\"的核心洞察，建立\"梯度方向一致性\"的理论假设，最终设计出C²GSPG方法。整个逻辑链条体现了从观察→假设→理论→方法→验证的完整科研思维过程，成功解决了在不损害推理性能的前提下实现有效置信度校准的关键挑战。", "summary_translation": "强化学习（Reinforcement Learning, RL）方法，以群体相对策略优化（Group Relative Policy Optimization, GRPO）及其变体为代表，在开发推理模型中发挥着核心作用。然而，这些方法常常面临一个关键的自负问题（overconfidence issue），这阻碍了它们实现具有自我意识的推理模型。在本研究中，我们提出了一种简单而有效的置信度校准群体序列策略梯度方法（confidence-calibration group sequence policy gradient method），称为C$^2$GSPG，该方法在抑制自负的同时，同步提升了推理性能。\n\n在原理上，我们提出了一个用于学习推理模型的群体序列策略梯度框架（Group Sequence Policy Gradient, GSPG），该框架消除了GRPO及其变体中常见的token级别偏差（token-level bias）。在此框架中，我们使用归一化的序列级别概率（normalized sequence-level probability）来定义每个推理问题的模型置信度，然后应用交叉熵正则化器（cross-entropy regularizer）将模型置信度校准到序列的奖励。我们证明，对于二元奖励（binary rewards），置信度校准正则化器和GSPG是协同的，因为它们的目标总是共享相同的梯度方向。对于非二元奖励（non-binary rewards），我们应用非线性奖励归一化（nonlinear reward normalization）和自适应正则化器裁剪（adaptive regularizer clipping），以减轻两个目标之间的潜在冲突。\n\n将C$^2$GSPG应用于逻辑和数学推理任务中对大型语言模型进行后训练，我们展示了它在推理准确性和置信度校准两方面均优于最先进方法（state-of-the-art methods）的性能。C$^2$GSPG的代码可在https://github.com/HaotianLiu123/CCGSPG获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#271", "title": "Multiplayer Nash Preference Optimization", "link": "/arxiv/2509.23102", "arxiv_id": "2509.23102", "authors": "Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi", "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.036757", "filter_reason": "根据筛选标准，这篇论文符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是关于改进LLM的基础能力，具体来说是通过提出Multiplayer Nash Preference Optimization (MNPO)这一新的训练范式来增强LLM与人类偏好的对齐能力。论文不是将LLM作为工具应用到特定领域，而是直接改进LLM本身的基础能力，这符合第一步的保留标准。 其次，从正面指标分析，论文明确涉及Large language models (LLMs)这一核心概念，并且提出了基于强化学习(RLHF)的新训练方法。虽然论文没有直接提及reasoning、planning等能力方向，但alignment能力的提升对于LLM理解复杂指令、执行多步任务等推理相关能力有重要支撑作用。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性应用层面的研究。 最后，论文提出的MNPO框架通过将偏好学习从双人扩展到多人设置，解决了现有方法在处理非传递性和异质性偏好时的局限性，这种基础方法论的改进有助于提升LLM的通用能力，间接支持其推理能力的增强。 综上所述，这篇论文的核心贡献是提出了一种改进LLM基础能力的新训练范式，符合\"提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决传统RLHF方法难以捕捉现实世界中偏好的非传递性和异质性问题，以及现有NLHF方法仅限于两玩家设置的局限性。针对复杂人类偏好对齐场景，我们提出了多人Nash偏好优化(MNPO)框架，将偏好优化建模为n玩家游戏，每个策略在正则化到参考模型的同时与对手群体竞争。在AlpacaEval 2.0、Arena-Hard和MT-Bench等指令遵循基准上，通过胜率和长度控制胜率等指标验证了MNPO的有效性，实验表明其一致优于现有NLHF基线，特别是在异质性注释条件下实现了更优的对齐质量。", "inspiration_trace": "# 多人纳什偏好优化(MNPO)的逻辑推演\n\n## 1. 宏观问题：LLM对齐中的偏好建模挑战\n\n**观察**：人类偏好本质上是非传递性和异质性的，而传统RLHF方法基于Bradley-Terry模型，假设偏好是传递性的，可用标量奖励函数表示。实证研究表明，这种假设在真实场景中往往不成立。\n\n**核心问题**：如何在LLM对齐过程中捕捉和建模人类偏好的非传递性和异质性？现有方法在处理复杂偏好结构时存在根本性局限。\n\n## 2. 中间问题：双人博弈框架的局限性\n\n**观察**：近期研究将偏好优化重新定义为在一般偏好神谕定义的博弈中寻找纳什均衡，产生了纳什学习从人类反馈中学习(NLHF)范式。这种方法通过INPO、ONPO、EGPO等算法实现了更好的理论保证和实证稳定性。\n\n**关键局限**：现有NLHF方法仍局限于双人设置（单个策略对单个对手），而现实世界的偏好对齐涉及多样化注释者、异质性评估标准或混合历史模型，更适合建模为多人博弈。\n\n**核心假设**：将NLHF扩展到多人环境可以更好地捕捉真实偏好结构的复杂性，减少单一对手偏见，提供更全面的偏好覆盖。\n\n## 3. 具体问题：多人博弈 formulation 的挑战\n\n**关键挑战**：\n1. 如何制定目标，在多个策略间平衡公平性和竞争力？\n2. 如何设计算法收敛到多人纳什均衡？\n3. 如何在扩展到更大LLMs时确保可计算性？\n\n**理论需求**：需要在多人环境中定义纳什均衡，确保其存在性和唯一性，并设计实用算法来近似均衡，同时保持计算效率。\n\n## 4. 解决方案：多人纳什偏好优化(MNPO)\n\n### 4.1 核心思想\n\n将LLM对齐构建为n人博弈，每个策略同时与所有其他策略竞争，同时正则化到参考策略，创建竞争均衡，平衡对群体的性能与对可信基线的adherence。\n\n### 4.2 理论构建\n\n**多人博弈 formulation**：\n- 考虑n个策略{π₁, π₂, ..., πₙ}，每个策略寻求最大化其相对于所有其他策略的平均偏好概率\n- 目标函数：J(πᵢ, {πⱼ}ⱼ≠ᵢ) = Eₓ∼D[Eᵧᵢ∼πᵢ,{ᵧⱼ|ᵧⱼ∼πⱼ}ⱼ≠ᵢ[P(yᵢ ≻ {yⱼ}ⱼ≠ᵢ | x)] - τ KL(πᵢ(·|x)‖πᵣₑf(·|x))]\n\n**纳什均衡和对偶间隙**：\n- 定义多人纳什均衡：策略π*，其中没有玩家可以通过单方面偏离来改善其目标\n- 扩展对偶间隙概念到多人设置：DualGap(π) := E_{πⱼ∈Oπ}[max_{πⱼ} min_{Oπ\\πⱼ} J(πⱼ, π∪Oπ\\πⱼ)] - min_{Oπ} J(π, Oπ)\n- 证明均衡条件满足当且仅当DualGap(π*) = 0\n\n### 4.3 算法设计\n\n**迭代框架**：\n- 基于乘法权重更新设计：πᵢ^{(t+1)}(y|x) ∝ (∏_{j≠i} πⱼ^{(t)}(y|x))^{1/(n-1)} exp(η/(n-1) ∑_{j≠i} P(y ≻ πⱼ^{(t)} | x))\n- 引入时间依赖的MNPO(TD-MNPO)，对手集使用历史策略的加权组合自适应演化\n- 设计损失函数L'ₜ(π) = E_{y,y'∼πₜ, yʷ,yˡ∼λP(y,y')}[(hₜ(π, yʷ, yˡ) - 1/(2η))²]，避免计算难以处理的归一化因子\n\n**理论保证**：\n- 证明MNPO继承了双人方法的均衡保证\n- 证明MNPO统一了许多现有偏好优化方法作为特例\n- 证明算法收敛到多人纳什均衡\n\n## 5. 实证验证\n\n**实验设计**：在指令跟随和推理基准测试上全面评估，特别关注异质性注释条件和混合策略评估场景。\n\n**关键结果**：\n- MNPO在指令跟随基准测试上一致性优于现有NLHF基线\n- 在异质性注释条件下实现更优越的对齐质量\n- 在混合策略评估场景中展现更强鲁棒性\n\n## 6. 结论与意义\n\n**核心贡献**：\n- 理论框架：建立MNPO的自然均衡表征，包括明确定义的纳什策略和对偶间隙\n- 算法创新：引入时间依赖的MNPO，对手集自适应演化\n- 实证验证：证明MNPO在多样化场景下的优越性能\n\n**深远意义**：通过将NLHF进展与对齐LLMs到多样化、非传递性人类偏好的实际需求相结合，MNPO为下一代对齐技术建立了可扩展的基础，更好地捕捉了真实世界偏好结构的复杂性。", "summary_translation": "人类反馈强化学习（Reinforcement learning from human feedback, RLHF）已成为将大型语言模型（large language models, LLMs）与人类偏好对齐的标准范式。然而，基于Bradley-Terry假设的奖励方法难以捕捉真实世界偏好的非传递性（non-transitive）和异质性（heterogeneous）。为解决这一问题，最近的研究将alignment重新框架化为双人纳什博弈（Nash game），从而产生了人类反馈纳什学习（Nash learning from human feedback, NLHF）。尽管这一视角启发了如INPO、ONPO和EGPO等具有强大理论和实证保证的算法，但它们本质上仍局限于双人交互（two-player interactions），存在单一对手偏见（single-opponent bias），无法捕捉真实偏好结构的全部复杂性。在这项工作中，我们提出了多人纳什偏好优化（Multiplayer Nash Preference Optimization, MNPO），这是一个将NLHF推广到多人制度（multiplayer regime）的新框架。它将alignment表述为n人博弈（n-player game），其中每个策略（policy）在与对手群体竞争的同时被正则化（regularized）朝向参考模型（reference model）。我们的框架在多人环境中建立了定义明确的纳什均衡（Nash equilibria），并将对偶间隙（duality gap）概念扩展到量化近似质量（approximation quality）。我们证明MNPO继承了双人方法的均衡保证（equilibrium guarantees），同时实现了更丰富的竞争动态（competitive dynamics）和更多样化的偏好结构（preference structures）覆盖。通过全面的实证评估，我们表明MNPO在指令跟踪基准（instruction-following benchmarks）上始终优于现有的NLHF基线，在异质性注释条件（heterogeneous annotator conditions）和混合策略评估场景（mixed-policy evaluation scenarios）下实现了卓越的对齐质量（alignment quality）。这些结果共同确立了MNPO作为将LLM与复杂、非传递性人类偏好对齐的原则性和可扩展框架。代码可在https://github.com/smiles724/MNPO获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#267", "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding", "link": "/arxiv/2509.23234", "arxiv_id": "2509.23234", "authors": "Runyan Tan, Shuang Wu, Phillip Howard", "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.034075", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"$p$-less sampling\"的无超参数解码方法，用于改进大语言模型的输出质量。根据筛选标准，我判断它符合研究范围，原因如下： 首先，从核心判断来看，这篇论文的本质是改进LLM的基础解码能力，而不是将LLM作为工具应用到特定领域。论文提出了一种新的采样/解码策略，这是一种改进LLM基础能力的方法论研究，符合第一步的保留标准。 其次，论文包含多个正面指标：它明确关注Large Language Models (LLMs)这一核心概念；在能力方向上，论文特别在数学推理(math reasoning)和逻辑推理(logical reasoning)任务上验证了方法的有效性，这些正是通用推理能力的重要组成部分。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究，而是专注于通用的解码方法改进。 最后，论文提出的方法通过改进解码策略来提高LLM在推理任务上的表现，这与研究目标\"提高大语言模型本身的通用推理能力\"直接相关。虽然论文也提到了创意写作任务，但其核心贡献在于提升LLM在数学和逻辑推理等通用能力上的表现，因此符合研究范围。", "summary2": "本文旨在解决大型语言模型(LLM)解码过程中采样策略对超参数敏感的问题。针对不同生成任务和温度配置下需要调整超参数的场景，我们提出了一种基于信息论的p-less采样方法，该方法无需超参数，能在每个解码步骤动态设置截断阈值。我们在数学、逻辑推理和创意写作任务上通过准确率、生成效率和多样性等指标验证了其有效性，实验表明p-less采样在高温下仍能保持高质量输出，且推理效率更高。", "inspiration_trace": "# p-less Sampling方法逻辑演进分析\n\n## 1. 宏观问题观察：现有采样方法的局限性\n\n作者首先观察到大型语言模型(LLM)的输出质量高度依赖于采样策略的选择。现有的截断式采样方法（如top-p、top-k、ϵ-sampling、mirostat、min-p等）存在两个核心问题：\n\n- **超参数敏感性**：这些方法的性能依赖于超参数选择，而最佳超参数值因任务和温度配置而异，需要大量调优工作。\n- **高温退化现象**：当温度增加时，这些方法往往导致文本质量显著下降，出现退化现象，无法在高温下保持输出质量。\n\n## 2. 问题深入分析：高温退化的根本原因\n\n作者进一步分析了为什么现有方法在高温下表现不佳：\n\n- **温度的本质**：温度参数用于调整模型输出的概率分布，高温会使概率分布更加平坦，增加随机性。\n- **静态截断机制**：现有方法的截断阈值通常是静态的或仅基于局部信息（如最高概率token的概率），无法适应分布的全局变化。\n- **缺乏分布感知**：大多数方法没有充分利用整个概率分布的信息来确定截断阈值，导致在高温下无法有效过滤低质量token。\n\n## 3. 理论探索：寻找信息理论基础\n\n作者转向信息理论，寻找能够动态适应概率分布变化的理论基础：\n\n- **概率分布的全局特性**：需要一种能够捕捉整个概率分布特性的度量，而非仅依赖局部信息。\n- **熵的概念**：熵是衡量概率分布不确定性的指标，但直接使用熵可能不够直观。\n- **Rényi熵家族**：作者探索了Rényi熵，特别是二阶Rényi熵（碰撞熵），它与概率平方和有关，能够反映分布的集中程度。\n\n## 4. 关键洞察：正确随机猜测的概率\n\n作者提出了一个核心问题：\"给定一个token概率分布，我们应该从哪个可靠子集中采样？\"\n\n- **随机猜测的正确概率**：在给定概率分布下，随机选择一个token恰好是\"正确\"token的概率是多少？\n- **数学表达**：这个概率可以表示为所有token概率的平方和：L[P] = Σ P(v)^2\n- **作为阈值**：可以将这个概率作为截断阈值，只保留概率不低于此阈值的token，确保采样质量。\n\n## 5. 方法形成：p-less sampling\n\n基于上述洞察，作者形成了p-less sampling方法：\n\n1. **计算阈值**：在每个解码步骤，计算L[P] = Σ P(v)^2作为阈值\n2. **构建采样集**：保留概率不低于L[P]的token：Vp-less = {v ∈ V : Pθ(v|x₁:t-₁) ≥ L[Pθ]}\n3. **归一化采样**：从保留的token中根据归一化后的概率进行采样\n\n## 6. 理论验证：连接到信息理论\n\n作者将p-less sampling与信息理论建立联系，增强方法的理论基础：\n\n- **Rényi熵的关系**：L[P] = exp(-H₂(P))，其中H₂(P)是二阶Rényi熵，表明p-less阈值与分布的碰撞熵直接相关\n- **与Shannon熵的关系**：L[P]与Shannon熵呈负相关，随熵增加而减少\n- **统计解释**：L[P]是概率质量函数二阶矩的无偏估计量乘以词汇表大小\n\n## 7. 变体扩展：p-less-norm sampling\n\n作者进一步提出了p-less-norm采样，通过减去归一化的\"不正确随机猜测\"概率来放宽阈值：\n\n- **更宽松的阈值**：¯L[P] = L[P] - (1/(|V|-1)) × Σ_{u≠v} P(u)P(v)\n- **适用场景**：在需要更多多样性的场景中表现更好，如创意写作任务\n\n## 8. 优势分析：为什么p-less更优？\n\n作者分析了p-less相比现有方法的优势：\n\n- **分布感知**：动态适应整个token概率分布，而非仅使用局部信息\n- **有效阈值**：数学上保证非空候选集，避免其他方法的边界情况\n- **温度适应性**：随温度动态调整，在高温下保持稳定性能\n- **无超参数**：完全消除超参数调优需求\n- **计算效率**：不需要排序，时间复杂度从O(N log N)降低到O(N)\n\n## 9. 实验验证：多场景测试\n\n作者通过全面实验验证了p-less的有效性：\n\n- **数学和逻辑推理任务**：在多个数据集上测试，p-less在不同温度下表现稳定，AUC值普遍高于其他方法\n- **创意写作任务**：在高温下保持文本质量，优于其他方法，人类评估也偏好p-less生成的文本\n- **效率分析**：采样速度更快（比min-p快22%），生成长度更短但不牺牲准确性\n- **多样性分析**：在保持准确性的同时提供合理的多样性，实现帕累托最优\n\n## 总结\n\n作者从观察现有采样方法的超参数敏感性和高温退化问题出发，通过深入分析问题根源，转向信息理论寻找解决方案，最终提出基于\"正确随机猜测概率\"的p-less sampling方法。该方法不仅具有坚实的理论基础，还通过实验验证了其在多种任务和场景下的优越性能，实现了从问题观察到理论探索，再到方法形成和验证的完整逻辑演进。", "summary_translation": "从大型语言模型（Large Language Models, LLMs）获得高质量输出通常依赖于基于采样的解码策略选择，以在每个生成步骤中概率性地选择下一个token（令牌）。虽然已经提出了多种此类采样方法，但它们的性能可能对超参数（hyperparameters）的选择敏感，而超参数可能需要根据生成任务和温度（temperature）配置进行不同设置。在这项工作中，我们介绍了$p$-less采样：一种基于信息理论（information-theoretic）的采样方法，它基于整个token（令牌）概率分布在每个解码步骤动态设置截断阈值（truncation threshold）。与现有方法不同，$p$-less采样没有超参数，并且在温度（temperature）增加时持续产生高质量输出。我们提供了关于$p$-less采样的理论视角以奠定我们提出方法的基础，并进行实验以在一系列数学、逻辑推理和创意写作任务上实证验证其有效性。我们的结果展示了$p$-less采样如何持续优于现有采样方法，同时在更高温度（temperature）值下表现出更少的文本质量下降。我们进一步展示了$p$-less如何通过较低的平均token（令牌）采样时间和更短的生成长度实现比替代方法更高的推理时间（inference-time）效率，而不牺牲准确性。最后，我们通过定性示例、案例研究和多样性评估提供分析，以突出$p$-less的优势。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#272", "title": "Causally-Enhanced Reinforcement Policy Optimization", "link": "/arxiv/2509.23095", "arxiv_id": "2509.23095", "authors": "Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang", "summary": "Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-27", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.037540", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究课题。首先，从核心判断来看，论文的本质是提出一种名为\"Causally-Enhanced Policy Optimization (CE-PO)\"的新训练框架，用于改进大语言模型的因果推理能力，减少模型使用捷径策略和产生虚假推理的问题。这是一种直接提升LLM基础推理能力的方法，特别是针对逻辑和因果推理能力的增强，属于改进LLM通用推理能力的核心研究。 其次，论文包含多个正面指标：明确关注大语言模型(LLMs)作为核心概念；专注于reasoning能力，特别是causal reasoning（因果推理）这一重要的逻辑推理方向；提出了一种强化学习方法(CE-PO)，是对PPO/GRPO等现有强化学习训练范式的改进。 第三，论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 最后，虽然论文涉及减少\"不忠实的思维链\"(unfaithful chain-of-thought)，但它不是从应用层面讨论幻觉问题，而是通过增强因果一致性来提升模型内在的推理质量和可靠性，这符合提升模型通用推理能力的研究目标。 论文的核心贡献是提出了一种通用的强化学习训练框架，通过增强因果一致性来提升LLM的推理能力，这直接符合研究课题的核心目标——提高大语言模型的通用推理能力。", "summary2": "本文旨在解决大型语言模型在强化学习中通过捷径策略获得表面正确答案但推理不忠实的问题。针对强化训练中的语言模型，我们提出了一种基于Jacobian敏感性和反事实强化的因果增强策略优化方法(CE-PO)，并在4个推理基准测试上通过准确率、鲁棒性等指标验证了其有效性。", "inspiration_trace": "# 从观察到解决方案：CE-PO方法的逻辑演进\n\n## 1. 宏观问题：LLMs强化学习中的推理不忠实\n\n**观察**：大型语言模型(LLMs)在强化目标训练时，往往通过捷径策略获得表面正确的答案，将正确输出与虚假或不忠实的推理配对，并且在小的因果扰动下性能显著下降。\n\n**核心矛盾**：模型优化了\"产生什么答案\"，但几乎没有约束\"如何获得答案\"，导致推理过程与最终答案之间的因果断裂。\n\n## 2. 问题聚焦：奖励信号与推理过程的不匹配\n\n**具体观察**：\n- 模型学会\"游戏化\"奖励信号（利用长度、格式或词汇重叠）\n- 正确答案与不忠实或虚假的推理轨迹被配对\n- 现有强化目标缺乏奖励因果连贯性的内部信号\n\n**关键发现**：当模型面临Z→X→Y的生成路径（提示→推理→答案）时，缺乏对这一路径因果连贯性的约束，导致模型可能生成正确但推理不忠实的输出。\n\n## 3. 根本假设：因果连贯性是稳健推理的关键\n\n**核心假设**：稳健推理不仅需要准确的输出，还需要生成路径上的因果连贯性，即：\n- 提示(Z)真正告知推理(X)\n- 提示和推理共同真正告知最终答案(Y)\n- 对提示或推理的小而相关的因果编辑会在下游引起相应的适当变化\n\n**理论依据**：如果模型的内部影响遵循Z→X→Y的顺序，思维链(CoT)轨迹更有可能\"因正确理由而正确\"，并在分布变化时更好地泛化。\n\n## 4. 技术挑战：如何实现因果连贯性约束\n\n**三大挑战**：\n1. **可测量性**：现有忠实性工具（显著性、原始输入-梯度属性）脆弱，无法作为训练信号\n2. **稳健性**：朴素梯度信号易被非语义因素（词频、位置、格式）夸大，导致捷径学习\n3. **目标平衡**：稳健性方法常牺牲准确性，而仅优化任务奖励会保留捷径利用\n\n## 5. 解决方案：CE-PO框架的构建\n\n### 5.1 雅可比信号：因果效应的微分代理\n\n**思路**：使用雅可比矩阵作为局部、直接的因果效应代理，量化上游嵌入的微小扰动如何改变下游对数似然。\n\n**实现**：\n- 计算Z→X、X→Y和Z→Y的雅可比敏感性\n- 对Z→Y，通过投影移除X的介导效应，获得直接因果效应\n\n### 5.2 反事实强化：抑制虚假敏感性\n\n**观察**：朴素梯度信号容易被非语义因素夸大，导致奖励黑客攻击（如模型通过增加输出长度获得更高奖励）。\n\n**解决方案**：\n- 构建反事实（打乱令牌顺序），保留表面统计量但破坏语义对齐\n- 计算反事实雅可比，识别虚假子空间\n- 通过正交投影从基础信号中移除这些方向，得到去混杂的因果对齐信号\n\n### 5.3 奖励融合：准确性与连贯性的平衡\n\n**挑战**：如何在准确性和因果连贯性之间取得平衡？\n\n**解决方案**：\n- 将雅可比响应归一化为稳定的标量连贯性分数\n- 使用Minkowski（幂均值）组合器融合连贯性分数与任务准确性\n- 通过调整组合器参数，实现准确性与连贯性之间的可调权衡\n\n### 5.4 统一奖励：无缝集成到现有框架\n\n**实现**：将融合后的奖励与PPO/GRPO集成，无需架构更改，形成统一的奖励信号用于策略优化。\n\n## 6. 实验验证：CE-PO的有效性\n\n**主要结果**：\n- 在4个数据集上，CE-PO比基线平均提高5.49%准确率（高达9.58%）\n- 提高对相关性-因果性翻转和轻微反事实编辑的稳健性\n- 有效减少奖励黑客攻击和不忠实的思维链\n\n**关键发现**：\n- 移除任何雅可比项都会导致奖励黑客攻击\n- Minkowski组合器参数p提供了准确性与连贯性之间的平滑权衡机制\n- CE-PO选择性增强推理或因果相关词汇，而非均匀提升所有词汇\n\n## 7. 理论保证：CE-PO的稳健性基础\n\n**核心贡献**：提供正交化TRPO下界证明，表明CE-PO通过正交化优势函数，有效减少对虚假方向的敏感性，提供更稳健的策略优化。\n\n**理论意义**：如果策略变化完全位于虚假子空间，则对性能没有贡献，从而从根本上防止奖励黑客攻击。\n\n## 结论：从问题到解决方案的逻辑演进\n\nCE-PO方法的逻辑演进展现了从观察到假设再到方法论的完整思考过程：从LLMs强化学习中的推理不忠实这一宏观问题出发，聚焦到奖励信号与推理过程不匹配的具体问题，提出因果连贯性是稳健推理的关键假设，针对可测量性、稳健性和目标平衡三大技术挑战，设计出包含雅可比信号、反事实强化、奖励融合和统一优化的CE-PO框架，最终通过实验验证和理论保证确认了其有效性。这一逻辑链不仅解决了具体问题，还为构建\"因正确理由而正确\"的AI系统提供了新思路。", "summary_translation": "使用强化目标训练的大型语言模型（Large language models, LLMs）常常通过捷径策略获得表面上正确的答案，将正确输出与虚假或不忠实的推理配对，并在小的因果扰动下性能下降。我们提出了因果增强策略优化（Causally-Enhanced Policy Optimization, CE-PO），这是一个即插即用的奖励塑形框架，通过从提示（prompt, Z）到推理（rationale, X）再到答案（answer, Y）的生成路径上的因果连贯性的可微代理来增强策略优化。CE-PO使用基于雅可比（Jacobian）的敏感性估计模型内部影响，通过反事实强化这些信号以抑制干扰线索，并使用闵可夫斯基（Minkowski, 幂平均）组合器将得到的连贯性得分与任务准确性反馈融合，暴露出准确性和连贯性之间的单一可调权衡。统一奖励无需架构更改即可与PPO/GRPO集成。在推理基准和因果压力测试中，CE-PO减少了奖励黑客（reward hacking）和不忠实的思维链（chain-of-thought），同时提高了对相关性-因果性翻转（correlation-causation flips）和轻度反事实编辑（light counterfactual edits）的鲁棒性，同时保持接近的准确性。在4个数据集上的实验结果表明，CE-PO比基线平均提高了5.49%的准确性（高达9.58%），同时提高了对相关性-因果性翻转和轻度反事实编辑的鲁棒性。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#284", "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective", "link": "/arxiv/2509.22613", "arxiv_id": "2509.22613", "authors": "Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen", "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.", "subjects": "Artificial Intelligence, Machine Learning, Machine Learning", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.048563", "filter_reason": "这篇论文完全符合我的研究目标，核心贡献在于从理论角度分析强化学习(RL)如何提升大语言模型(LLM)的规划能力。根据筛选标准，我判断如下： 第一步核心判断：论文本质上是研究如何通过强化学习方法改进LLM的规划能力，这属于提升LLM通用推理能力的范畴。规划(planning)是通用推理能力的重要组成部分，论文关注的是LLM基础能力的提升，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个关键正面指标： - 核心概念：明确研究Large Language Models (LLMs) - 能力方向：聚焦于planning能力，这是推理能力的关键组成部分 - 训练方法：研究reinforcement learning (RL)对LLM的影响，包括策略梯度和Q学习 - 新兴范式：规划能力是llm-based agents和tool use的基础能力 第三步排除标准：论文不涉及任何排除标准中的领域，没有研究多模态与视觉、特定应用领域或模型可靠性的应用层面问题。 第四步特殊和模糊情况：论文不存在模糊情况，它明确研究通用规划能力的理论提升，而非特定领域应用。 综合来看，这篇论文通过理论分析探讨了强化学习对LLM规划能力的影响，揭示了不同RL方法的优缺点，属于提升LLM通用推理能力的基础研究，完全符合我的研究目标。", "summary2": "本文旨在 [解决强化学习在语言模型规划中的理论基础问题]。针对 [语言模型规划任务]，我们提出了一种 [基于图抽象的理论分析框架]，并在 [Blocksworld规划基准和Erdős-Rényi图] 上通过 [准确性和输出多样性指标] 验证了其有效性。", "inspiration_trace": "## 面临的挑战\n作者面临的核心挑战是：强化学习(RL)在增强语言模型规划能力方面取得成功，但其理论基础尚不明确；监督微调(SFT)在规划任务中存在根本局限，倾向于记忆而非泛化；现有RL方法如策略梯度存在多样性崩溃问题。\n\n## 关键洞察\n作者通过图论抽象将规划问题建模为路径寻找，发现SFT本质上是记忆训练数据中的共现关系，无法利用传递性信息；RL优于SFT主要在于其迭代数据生成过程促进了探索；策略梯度存在多样性崩溃问题，而Q-learning具有保持多样性和支持离线学习的双重优势。\n\n## 解决方案演进\n作者首先建立理论框架分析SFT局限，然后揭示策略梯度与SFT的联系及多样性崩溃问题，转而分析Q-learning发现过程奖励可避免奖励黑客，最终证明Q-learning在保持多样性和离线学习方面的优势，并通过实验验证理论发现。\n\n## 创新点总结\n该研究首次从理论角度系统分析RL在语言模型规划中的作用机制，揭示SFT记忆和RL泛化的理论基础，发现并证明策略梯度多样性崩溃现象，提出Q-learning在语言模型规划中的优势，为改进语言模型规划中的RL方法提供理论基础。", "summary_translation": "最近的强化学习(reinforcement learning, RL)方法显著提升了大型语言模型(Large Language Models, LLMs)的规划能力，然而其有效性的理论基础仍然难以捉摸。在这项工作中，我们通过一个可处理的基于图的抽象来研究强化学习的优势和局限性，重点关注策略梯度(policy gradient, PG)和Q学习(Q-learning)方法。我们的理论分析表明，监督微调(supervised fine-tuning, SFT)可能会引入基于共现的伪解(spurious solutions)，而强化学习主要通过探索(exploration)实现正确规划，强调了探索在实现更好泛化(generalization)中的作用。然而，我们也表明策略梯度存在多样性崩溃(diversity collapse)问题，即在训练过程中输出多样性减少，且在达到完美准确度后仍然存在。相比之下，Q学习提供了两个关键优势：离策略学习(off-policy learning)和收敛(convergence)时的多样性保持(diversity preservation)。我们进一步证明，在Q学习中需要精心设计奖励(reward design)以防止奖励黑客(reward hacking)问题。最后，我们将我们的框架应用于真实世界规划基准(benchmark)Blocksworld，确认了这些行为在实际中确实存在。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#277", "title": "Not only a helper, but also a teacher: Interactive LLM Cascade", "link": "/arxiv/2509.22984", "arxiv_id": "2509.22984", "authors": "Yu Wu, Shuo Wu, Ye Tao, Yansong Li, Anand D. Sarwate", "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger models often having better performance but higher cost: choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost. To improve the cascading efficiency, we propose Inter-Cascade, an online and interactive LLM Cascade that extends the role of strong model from a backup helper to a long-term teacher. In our system, when a strong model resolves a difficult query, it also distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning. Empirically, compared with standard LLM Cascade baselines across multiple benchmarks, the Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction). Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs, and provides a general, scalable framework applicable to both open-source and API-based LLMs.", "subjects": "Artificial Intelligence, Computation and Language", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.040318", "filter_reason": "这篇论文符合我的研究目标，应该被保留。以下是我的详细判断过程： 第一步：核心判断——这篇论文的本质是提出一种名为\"Inter-Cascade\"的在线交互式LLM级联系统，其核心创新在于让强模型不仅作为解决困难问题的助手，还作为\"教师\"角色，将解决方案提炼为通用、可重用的问题解决策略，从而提升弱模型的能力。这明显属于改进LLM基础能力和提出新的训练/交互范式的研究，目的是增强LLM的问题解决能力，而不是将LLM应用到特定领域。 第二步：正面指标——论文包含多个相关主题： - 核心概念：明确聚焦于Large Language Models (LLMs) - 能力方向：关注problem-solving strategies（问题解决策略），直接涉及LLM的推理能力 - 新兴范式：描述了多个LLM之间的交互和知识传递，类似于multi-agent系统的概念 第三步：排除标准——论文不涉及任何排除领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 没有讨论模型可靠性方面的应用问题（如水印、安全等） 第四步：特殊和模糊情况处理—— 论文提出的Inter-Cascade框架是一种通用的多LLM协作方法，让强模型\"教\"弱模型通用问题解决策略，这可以视为一种通用的智能体协作框架，旨在提升LLM的通用问题解决能力，而非针对特定领域的应用。 最终决策：论文的核心贡献是提出了一种让LLM之间进行知识传递和策略提炼的方法，从而提升整体推理能力，这完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。论文关注的是如何增强LLM的基础推理和问题解决能力，而非将其作为工具应用到特定领域，因此应该被保留。", "summary2": "本文旨在解决LLM Cascade系统中重复查询导致的高成本问题。针对相似或重复查询场景，我们提出了一种Inter-Cascade框架，使强模型不仅作为助手还作为教师，将解决方案提炼为可重用策略提升弱模型性能，并在多个推理和事实性benchmark上通过Pipeline Accuracy、Strong LLM Call Rate和成本降低等指标验证了其有效性。", "inspiration_trace": "# 从问题到方案：Inter-Cascade的逻辑演进\n\n## 1. 宏观问题：性能与成本的权衡\n\n作者首先观察到大型语言模型(LLMs)应用中的一个基本矛盾：**模型性能与使用成本之间的权衡**。更大的模型通常表现更好，但计算成本、延迟或API费用也更高。这一矛盾促使作者思考：如何在保证性能的同时降低成本？\n\n## 2. 现有方案的局限性分析\n\n作者深入研究了现有的LLM Cascade方法，该方法通过让弱模型处理简单查询，将困难查询转移到强模型来平衡性能和成本。然而，作者发现这种方法存在三个关键局限：\n\n- **非适应性**：延迟决策是离线训练的，一旦部署就固定不变，无法根据实际查询流调整\n- **重复查询浪费**：当面临相似或重复查询时，系统会反复调用昂贵的强模型，造成资源浪费\n- **单向知识流**：弱模型只能卸载任务，无法从强模型的能力中学习和提升\n\n这些局限导致系统效率低下，特别是在有大量相似查询的场景中。\n\n## 3. 关键洞察与假设形成\n\n基于对局限性的分析，作者提出了一个关键洞察：**可以利用输入相似性，让强模型帮助弱模型适应并处理相似查询**。这一洞察基于以下假设：\n\n- **相似查询具有相似解决模式**：语义上相似的查询可能需要类似的解决策略\n- **强模型的知识可以提炼**：强模型解决查询的知识可以被提炼为通用策略，而不仅仅是具体答案\n- **弱模型可以通过上下文学习**：弱模型可以通过在输入中添加策略来提高性能，而不需要参数更新\n\n## 4. 概念框架：角色转变\n\n基于上述假设，作者形成了Inter-Cascade的核心概念框架，其核心思想是**角色转变**：将强模型从单纯的\"备份助手\"扩展为\"长期教师\"。具体表现为：\n\n- 强模型不仅回答困难查询，还提炼可重用的问题解决策略\n- 这些策略被存储并用于增强弱模型的后续查询处理能力\n- 弱模型通过使用这些策略实现\"在职\"学习，动态提高性能\n\n## 5. 方法设计：交互式知识传递\n\n作者将概念框架具体化为可实现的系统设计，包含四个关键组件：\n\n1. **策略生成器**：在强模型中添加策略生成函数，将查询映射为包含查询、答案和通用解决思路的策略\n2. **策略存储库**：构建查询-策略对的集合，动态更新，形成\"知识库\"\n3. **策略匹配函数**：通过相似性排序检索top-k相关策略，实现精准知识传递\n4. **增强输入机制**：将查询与检索到的策略拼接形成增强输入，提高弱模型的处理能力\n\n## 6. 理论验证：置信度校准改进\n\n为了验证方法的有效性，作者提供了理论分析，证明：\n\n- 添加策略后，弱模型的置信度分数能更好地近似其正确概率\n- 在相同置信度阈值下，使用策略可以降低风险容忍度，提高系统准确性\n- 这种改进不是偶然的，而是有理论保证的系统性能提升\n\n## 7. 实验验证与优化\n\n作者通过多维度实验验证了方法的有效性：\n\n- **性能提升**：在多个推理和事实类数据集上，弱模型准确率提升高达33.06个百分点\n- **成本降低**：强模型调用减少高达48.05%，API成本节省高达49.63%\n- **机制验证**：对比随机策略和基于相似性检索的策略，证明相似性检索的关键作用\n- **置信度分析**：验证策略不仅提高准确性，还改善弱模型的置信度校准\n\n## 8. 系统完善与扩展\n\n最后，作者完善了系统并考虑了实际应用和扩展性：\n\n- **鲁棒性设计**：通过置信度阈值确保系统在策略库包含错误策略时仍能保持鲁棒性\n- **多模型扩展**：将方法扩展到N-LLM级联系统，提高通用性\n- **实际应用考虑**：讨论策略库维护、定期修剪和离线微调的可能性\n- **未来方向**：提出改进策略生成、相似性评估和防止不匹配的未来工作方向\n\n通过这一完整的逻辑链条，作者从宏观问题出发，经过观察、假设、概念形成、方法设计、理论验证和实验优化，最终提出了Inter-Cascade这一创新方法，实现了LLM Cascade从静态非适应系统到动态交互式学习系统的转变。", "summary_translation": "大型语言模型（Large Language Models, LLMs）在能力上差异很大，较大的模型通常具有更好的性能但成本也更高：选择LLM模型通常涉及在性能和成本之间进行权衡。LLM Cascade（级联）是一种将困难查询从弱/便宜模型推迟到强/昂贵模型的范式。这种方法是非自适应的：推迟决策是离线训练的。当面对相似或重复的查询时，LLM Cascade可能会反复咨询昂贵的模型，从而产生更高的成本。为了提高级联效率，我们提出了Inter-Cascade（交互级联），一种在线且交互式的LLM Cascade，它将强模型的角色从备用助手扩展为长期教师。在我们的系统中，当强模型解决一个困难查询时，它还会将其解决方案提炼为通用的、可重用的问题解决策略，从而在后续查询中提升弱模型的性能。将策略添加到查询中使弱模型能够随时间动态提高其性能，避免了计算和时间密集型的微调（fine-tuning）。从实证角度看，与多个基准测试（benchmarks）中的标准LLM Cascade基线相比，Inter-Cascade显著提高了弱模型的准确性（高达33.06个绝对百分点）和整个系统的准确性（高达5.53个绝对百分点），同时减少了对强模型的调用（相对减少高达48.05%）并节省了相应的费用（相对节省高达49.63%）。Inter-Cascade展示了LLM之间有效的上下文知识转移（in-context knowledge transfer），并提供了一个通用的、可扩展的框架，适用于开源和基于API的LLM。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#280", "title": "Adaptive Margin RLHF via Preference over Preferences", "link": "/arxiv/2509.22851", "arxiv_id": "2509.22851", "authors": "Yaswanth Chittepu, Prasann Singhal, Greg Durrett, Scott Niekum", "summary": "Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.", "subjects": "Machine Learning, Artificial Intelligence, Computation and Language", "date": "2025-09-26", "category": "cs.CL", "crawl_time": "2025-10-06T20:55:05.041902", "filter_reason": "根据筛选标准，这篇论文符合研究目标。首先，从核心判断来看，论文的本质是改进大语言模型的基础训练方法，具体聚焦于强化学习人类反馈(RLHF)和直接偏好优化(DPO)技术。论文提出了一种名为DPO-PoP的新方法，通过利用\"偏好之上的偏好\"(preference over preferences)来推断自适应边界，从而提高模型的泛化能力和对齐质量。这明显属于改进LLM基础能力的范畴，符合研究目标。 从正面指标看，论文涉及核心概念LLMs（虽然未直接提及，但RLHF和DPO是LLM训练的关键技术）和训练方法中的强化学习(RLHF)，这些都是提高LLM通用推理能力的重要方向。 论文不涉及任何排除标准中的领域，如多模态与视觉、特定应用领域或模型可靠性的应用层面问题。论文关注的是通用训练方法的改进，而非特定领域应用。 虽然论文没有直接讨论推理、规划或问题解决能力，但通过改进RLHF和DPO这一基础训练范式，论文间接提升了模型的泛化能力和对齐质量，这有助于提高LLM的通用推理能力。因此，这篇论文符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决RLHF中偏好强度差异建模问题。针对人类偏好数据，我们提出了一种基于偏好之上的偏好(PoP)的自适应边际DPO方法(DPO-PoP)，并在UltraFeedback数据集上通过分类准确率、生成胜率等指标验证了其有效性。该方法利用偏好间比较信息推断自适应边际，无需精确评分，提高了模型判别和生成性能。", "inspiration_trace": "# 从观察到方法：DPO-PoP的逻辑演进分析\n\n## 宏观问题：如何提升RLHF的泛化性与鲁棒性\n\n作者从机器学习中的基本原理出发：边界（margin）在分类任务中是提高泛化和鲁棒性的关键因素。在RLHF领域，这一原理同样适用——如何有效利用边界信息来提升奖励模型的学习效果，成为作者关注的宏观问题。\n\n## 观察一：现有RLHF边界处理的局限性\n\n作者系统分析了现有RLHF方法的边界处理策略，发现三类主要方法：\n1. **无边界方法**：将所有偏好视为同等重要，忽略了偏好强度的差异\n2. **固定边界方法**：为所有偏好分配相同的边界值，无法适应不同偏好的特性\n3. **简单函数边界**：基于偏好评分的简单函数作为边界，但这些评分往往存在噪声\n\n这些方法的共同缺陷是无法准确捕捉不同偏好的相对强度差异，导致模型泛化能力受限。\n\n## 观察二：准确偏好评分的获取困境\n\n作者进一步发现，即使采用自适应边界，现有方法通常假设能获得准确的偏好评分，但这在实践中存在严重问题：\n- 人类标注者在使用Likert量表等评分方案时，即使对偏好顺序达成一致，也常因量表解释不同而给出不一致分数\n- 这种不一致性直接影响了从评分推导出的边界质量\n- 研究表明，比较性标注（如Best-to-Worst scaling）比评分尺度标注产生更可靠结果\n\n## 核心假设：偏好之上的偏好能更好建模偏好强度\n\n基于以上观察，作者提出创新性假设：利用\"偏好之上的偏好\"(Preference over Preferences)可以更有效地建模偏好强度。这一假设基于以下推理：\n1. 比较性标注（判断哪个偏好更强）比为单个响应分配精确数值分数更容易且更可靠\n2. 偏好之上的偏好允许推断连续实值边界，而非离散评分选项\n3. 这种方法能避免评分校准问题，同时保留偏好强度的相对信息\n\n## 方法设计：DPO-PoP的构建\n\n基于核心假设，作者设计了DPO-PoP方法，其逻辑演进如下：\n\n### 1. 数据收集创新\n- 传统方法：收集单个响应的评分或简单偏好对\n- 创新方法：收集\"偏好之上的偏好\"数据，即标注者判断两个偏好中哪一个反映更强的区分度\n\n### 2. 边界推断机制\n- 核心洞见：若(A ≻ B) ≻ (C ≻ D)，则r(A)-r(B) > r(C)-r(D)\n- 边界设定：将较弱偏好的奖励差异作为较强偏好的边界下限\n- 数学表达：通过stop-gradient操作确保边界在优化中保持固定\n\n### 3. 优化稳定性处理\n- 问题发现：原始公式因无边界值导致梯度不稳定\n- 解决方案：\n  a. 边界裁剪：将边界值限制在[0, Mmax]区间内\n  b. 目标策略：使用缓慢更新的目标策略计算边界，防止快速波动\n\n### 4. 最终目标函数\n```\nmin_θ E_DPoP [-log σ(β(log πθ(y+^s|x_s)/πref(y+^s|x_s) - log πθ(y-^s|x_s)/πref(y-^s|x_s)) - sg[clip[0,Mmax](β(log π̂θ(y+^w|x_w)/πref(y+^w|x_w) - log π̂θ(y-^w|x_w)/πref(y-^w|x_w)))])]\n```\n\n## 实验验证与发现\n\n作者通过实验验证了方法的有效性，并发现重要现象：\n\n### 1. 判别与生成性能的权衡\n- DPO-PoP-iter：在弱偏好上表现更好，提高了测试分类准确率\n- DPO-PoP-random：在强偏好上表现更好，提高了生成质量\n- 关键发现：过度关注弱偏好会损害生成性能，反之亦然\n\n### 2. 采样策略的差异化设计\n基于上述发现，作者提出两种采样策略：\n- **迭代采样**：每个偏好与k个较弱偏好比较，有利于判别性能\n- **随机采样**：随机选择偏好对比较，有利于生成性能\n\n## 总结：从问题到解决方案的完整逻辑链\n\n作者的思考过程展现了清晰的逻辑演进：\n1. **问题识别**：RLHF中边界处理不足导致泛化性受限\n2. **观察分析**：现有方法存在边界处理局限和评分获取困难\n3. **假设形成**：偏好之上的偏好能更好建模偏好强度\n4. **方法设计**：构建DPO-PoP框架，整合PoP监督到DPO中\n5. **优化改进**：解决训练稳定性问题\n6. **实验验证**：证实方法有效性并发现性能权衡\n7. **实用优化**：提出差异化采样策略适应不同需求\n\n这一完整逻辑链条展示了作者从宏观问题出发，通过系统观察形成创新假设，进而设计出有效解决方案的思考过程，体现了严谨的学术思维和方法论创新。", "summary_translation": "基于边界（margin）的优化是提高分类任务中泛化能力和鲁棒性的基础。在人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）中从偏好学习奖励模型的背景下，现有方法通常依赖于无边界、固定边界，或仅是偏好评分的简单函数的边界。然而，这些公式往往未能考虑不同偏好的强度差异，例如某些偏好与响应之间更大的边界相关联，或者它们依赖于从评分中得出的噪声边界信息。我们认为，对偏好强度进行建模可以带来更好的泛化能力和更真实的对齐。\n\n此外，许多使用自适应边界（adaptive margins）的现有方法假设能够获得准确的偏好分数，而人类很难可靠地提供这些分数。我们提出了一种利用\"偏好之上的偏好\"（preferences over preferences）的方法，即标注表明两个偏好中哪一个反映了更强的区分度。我们使用这种序数信号（ordinal signal）为每个数据点推断自适应边界。我们引入了直接偏好优化（Direct Preference Optimization, DPO）的扩展版本DPO-PoP，它整合了来自偏好之上偏好监督的自适应边界，从而提高了判别性和生成性性能。\n\n在UltraFeedback数据集上的实证结果表明，我们的方法优于原始DPO、使用固定边界的DPO以及使用真实边界的DPO。此外，我们展示了判别性（discriminative）和生成性（generative）性能之间存在权衡：提高测试分类准确度，特别是通过牺牲强偏好的正确标注来正确标注弱偏好，可能导致生成质量的下降。为了应对这种权衡，我们提出了两种收集偏好之上偏好标签的采样策略：一种有利于判别性性能，另一种有利于生成性性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Machine Learning", "count": 26, "papers": [{"index": "#19", "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts", "link": "/arxiv/2509.25020", "arxiv_id": "2509.25020", "authors": "Jiayu Liu, Zhenya Huang, Anya Sims, Enhong Chen, Yee Whye Teh, Ning Miao", "summary": "The current paradigm for reasoning in large language models (LLMs) involves models \"thinking out loud\" via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to \"think while speaking,\" which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional \"thoughts\". Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs.", "subjects": "Machine Learning", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.219194", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：论文的本质是提出MARCOS，一种新的推理范式来改进大语言模型的基础推理能力。论文针对当前思维链(CoT)推理方法的局限性（速度慢、计算成本高、推理受限于离散token空间），提出将推理建模为连续、高维\"思想\"的隐马尔可夫链。这明显属于改进LLM基础能力和通用推理能力的研究，而非将LLM作为工具应用到特定领域。 第二步正面指标：论文包含多个相关主题。核心概念方面，明确针对大语言模型(LLMs)；能力方向方面，直接针对reasoning能力，并在数学推理基准GSM8K上进行了验证；训练方法方面，提出了两阶段变分训练方案，并提到其方法为强化学习提供了新机会。 第三步排除标准：论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性（应用层面）的研究。 第四步特殊和模糊情况：论文提出的是一种通用的推理改进方法，不是针对特定领域的应用，因此不需要特殊处理。 核心贡献在于提出了一种全新的推理范式，将LLM的推理过程从离散token生成转变为连续高维思想空间的马尔可夫链，显著提升了推理效率和质量。这直接服务于提升LLM的通用推理能力，完全符合研究目标。", "summary2": "本文旨在 [解决当前大型语言模型中思维链推理存在的速度慢、计算成本高、信息瓶颈和推理短视等问题]。针对 [数学推理任务]，我们提出了一种 [将推理建模为连续高维思想的隐马尔可夫链的新范式MARCOS]，并在 [GSM8K、SVAMP和MultiArith三个基准测试] 上通过 [准确率和推理速度] 验证了其有效性。", "inspiration_trace": "# MARCOS方法逻辑链分析\n\n## 1. 宏观问题：LLM推理的效率与质量困境\n\n作者从大语言模型(LLM)推理的核心挑战出发：传统思维链(CoT)方法虽然有效，但存在三个根本性局限：\n- **计算效率低**：需自回归生成大量CoT标记，推理缓慢且计算成本高\n- **信息瓶颈**：离散标记空间限制了推理步骤间的信息传递带宽\n- **思考-说话耦合**：模型被迫\"边思考边说话\"，导致短视推理\n\n## 2. 现有方法观察与不足分析\n\n作者系统考察了现有连续推理方法，发现两类方法均存在局限：\n\n**时间轴方法**（如Coconut、CoLaR）：\n- 本质上是标记压缩技术，未摆脱标记级推理限制\n- 采用静态转换函数，无法建模多样化解空间\n\n**深度轴方法**（如循环Transformer）：\n- 缺乏中间推理状态的监督\n- 仅是参数效率化的transformer深度增加方法\n\n关键洞察：现有方法未能从根本上解决CoT的局限性，需要全新范式。\n\n## 3. 关键假设：思考与说话的分离\n\n从神经科学发现（语言主要是交流工具而非思维工具）获得灵感，作者提出核心假设：\n- **思考与说话应解耦**：模型应能完全思考后再表达，而非同步进行\n- **连续空间推理**：在连续高维空间中进行推理可避免离散化造成的信息损失\n- **随机性建模**：推理过程存在多模态解空间，需显式建模随机性而非静态映射\n\n## 4. 方法论构建：MARCOS架构设计\n\n基于上述假设，作者构建了MARCOS（Markov Chain of Continuous Thoughts）框架，将推理建模为条件隐马尔可夫模型(cHMM)：\n\n### 4.1 三阶段架构设计\n\n**理解阶段**：\n- 将输入问题转换为特征向量序列\n- 为后续思考提供条件信息\n\n**思考阶段**（核心创新）：\n- 设计两组神经元：深度神经元（促进深层推理）和浅层神经元（易于表达）\n- 引入随机变量Rk控制推理随机性，实现多模态推理路径探索\n- 使用双向transformer作为思考器，迭代更新思维神经元\n\n**说话阶段**：\n- 将浅层神经元翻译为自然语言\n- 可选中间步骤，支持并行解码提高效率\n\n### 4.2 两阶段变分训练方案\n\n针对随机映射带来的训练挑战，作者提出创新训练策略：\n\n**第一阶段**：\n- 最小化重构损失和稀疏性损失\n- 固定随机性预测器g\n\n**第二阶段**：\n- 训练随机性预测器g最小化KL散度\n- 固定其他参数确保训练稳定\n\n这种设计受VAE的ELBO启发，有效处理了随机映射的训练难题。\n\n## 5. 实验验证与结果分析\n\n作者在三个数学推理基准（GSM8K、SVAMP、MultiArith）上验证MARCOS：\n\n**主要发现**：\n- 显著优于现有连续推理方法（GSM8K上提高8.66%）\n- 首次达到并超越基于标记的CoT性能（GSM8K上提高4.7%）\n- 实现高达15.7倍的推理加速\n\n**消融实验**：\n- 深度神经元、随机变量和稀疏性损失均为关键组件\n- 移除稀疏性损失导致模型崩溃，验证了其重要性\n\n**随机性分析**：\n- 随机变量Rk的不同维度控制不同推理特性（如输出长度、格式）\n- 实现了步骤级而非标记级的随机性控制\n\n## 6. 方法意义与未来方向\n\n作者总结了MARCOS的创新意义：\n- 首次实现思考与说话的明确分离\n- 提供了步骤级随机性控制的新范式\n- 为非自回归解码和强化学习提供了新方向\n\n未来工作包括大规模预训练、专用强化学习算法开发以及更高效解码策略探索。\n\n---\n\n## 逻辑链总结\n\n作者从LLM推理的效率与质量困境出发，通过系统分析现有方法的不足，结合神经科学洞察提出思考与说话分离的核心假设，进而构建了MARCOS这一全新推理范式。该方法通过条件隐马尔可夫模型建模推理过程，创新性地设计了三阶段架构和两阶段训练方案，最终在实验中验证了其优越性。这一逻辑链条展示了从问题识别、假设形成、方法设计到实验验证的完整研究思路，体现了严谨的学术思维和创新的方法论构建过程。", "summary_translation": "当前大型语言模型（large language models, LLMs）中的推理范式涉及模型通过一系列令牌（token）\"大声思考\"，这种方法被称为思维链（chain-of-thought, CoT）。尽管这种方法有效，但它存在几个显著的缺点。首先，推理需要自回归（autoregressive）地生成通常数千个CoT令牌，这个过程缓慢且计算成本高昂。其次，它将推理限制在令牌的离散空间中，在推理步骤之间造成信息瓶颈。第三，它从根本上将推理与令牌生成纠缠在一起，迫使LLMs\"边说边想\"，这可能导致短视的推理。\n\n鉴于这些局限性，我们重新构想LLMs中的推理方式，并提出一个新范式：MARCOS。在我们的方法中，我们不是自回归地生成令牌，而是将推理建模为一个连续的、高维度的\"思想\"的隐马尔可夫链（hidden Markov chain）。每个推理步骤涉及内部思想的转换，其中显式推理步骤（可能由数百个令牌组成）作为可观察变量，这些变量是窥视隐含思想的窗口。由于这种潜在过程与标准监督学习不兼容，我们进一步提出了一种两阶段变分训练方案（two-phase variational training scheme）。\n\n我们在三个基准测试上的实验表明，MARCOS优于现有的连续推理方法，并且首次实现了与基于令牌的CoT相当的性能，甚至在GSM8K上超越了CoT 4.7%，同时推理速度提高了高达15.7倍。除此之外，MARCOS还提供了额外的优势，例如在随机性控制上实现了步骤级别而非令牌级别的控制，为LLMs中的强化学习（reinforcement learning）和推理开辟了重要机会。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#23", "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards", "link": "/arxiv/2509.24981", "arxiv_id": "2509.24981", "authors": "Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan", "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.226397", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是关于改进LLM的基础推理能力，提出了一种新的强化学习训练范式ROVER（Random Policy Valuation for Diverse Reasoning），专门针对提升LLM的数学推理能力。论文的核心贡献是发现并证明了一个重要结果：最优动作可以从固定均匀随机策略的Q函数中恢复，从而简化了传统的强化学习训练过程，提高了推理质量和多样性。 其次，从正面指标来看，论文明确包含多个相关主题：核心概念是\"Large language models (LLMs)\"，能力方向聚焦于\"reasoning\"特别是\"math reasoning\"，训练方法涉及\"reinforcement learning (RL)\"，具体是\"RL with Verifiable Rewards (RLVR)\"范式。 从排除标准来看，论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性的应用层面问题。虽然论文聚焦于数学推理，但这被视为LLM通用推理能力的一个重要方面，而非特定领域应用。 综上所述，这篇论文的核心贡献是提出了一种新的、简化的强化学习方法来提升LLM的通用推理能力，完全符合研究目标。", "summary2": "本文旨在解决LLM推理中强化学习训练不稳定和多样性崩溃的问题。针对数学推理任务中的可验证奖励场景，我们提出了一种基于随机策略评估的ROVER方法，通过评估固定均匀随机策略的Q函数并从softmax采样动作，绕过了传统策略迭代循环。在多个数学推理基准上通过pass@1、pass@256和多样性指标验证了其有效性，实现了+8.2的质量提升和+17.6%的多样性改善。", "inspiration_trace": "# 从复杂到简约：ROVER方法的逻辑演进\n\n## 宏观问题：强化学习在LLM推理中的困境\n\n当前，强化学习与可验证奖励(RLVR)已成为提升大语言模型(LLM)推理能力的主流范式。然而，现有方法主要依赖PPO和GRPO等策略优化框架，这些方法遵循广义策略迭代(GPI)循环，交替评估当前策略价值和改进策略。尽管有效，但它们普遍面临两大核心问题：\n\n1. **训练不稳定性**：策略持续演化导致评估目标非平稳，引发训练波动\n2. **多样性崩溃**：奖励最大化导向的迭代优化导致探索空间缩小，熵崩溃\n\n这些问题迫使研究者引入各种复杂启发式技巧（如裁剪、KL正则化、数据选择等），增加了实现复杂度和调优难度。\n\n## 关键观察：LLM推理任务的特殊结构\n\n作者敏锐地观察到，标准LLM数学推理任务可形式化为一种特殊的有限水平马尔可夫决策过程(MDP)，具有三个关键特性：\n\n1. **确定性状态转移**：每个动作导致确定性的新分支\n2. **树状结构动态**：部分序列只有一个父状态，形成树状而非图状结构\n3. **二元终端奖励**：只在终端状态给出正确/错误的二元反馈\n\n这与PPO等通用强化学习算法设计的复杂控制环境（如电子游戏、机器人控制）形成鲜明对比，后者通常具有随机转移、复杂奖励结构和图状状态空间（可能有循环）。这一观察引出了核心问题：\n\n> 我们是否正在对结构上更简单（尽管规模更大）的问题应用了不必要的复杂工具？\n\n## 理论假设：随机策略价值的惊人潜力\n\n基于上述观察，作者提出一个反直觉的假设：在这种特殊MDP结构下，最优操作可能通过简单地评估固定均匀随机策略并基于其Q值贪婪选择动作来获得。\n\n这一假设挑战了强化学习领域的传统认知——均匀策略通常被认为过于简单，无法为控制提供有意义的指导。然而，作者推测，在树状结构、确定性转移和二元终端奖励的MDP中，均匀策略的Q值可能包含足够的信息来指导最优决策。\n\n## 理论证明：简化最优控制的数学基础\n\n作者通过严格的理论分析证明了一个令人惊讶的结果：\n\n**定理**：在具有确定性转移、树状状态空间和二元终端奖励的有限水平MDP中，相对于均匀随机策略πu的Q函数的贪婪策略πgreedy(s) = argmax_a Q^πu(s,a)是最优的。\n\n这一发现具有深远意义：\n1. **简化了最优控制问题**：尽管LLM数学推理任务表面复杂，但其底层决策结构比通常认为的更简单\n2. **绕过了GPI循环**：只需评估固定均匀随机策略，无需迭代评估更新的策略\n3. **消除了启发式技巧**：不需要当前方法中许多复杂的启发式技巧\n\n## 方法设计：从理论到实践的ROVER\n\n基于理论证明，作者设计了ROVER（Random Policy Valuation for Diverse Reasoning）方法，将理论原则转化为实用且可扩展的算法。ROVER的核心设计包括：\n\n### 1. Q函数参数化\n直接利用LLM的内在参数表示Q函数，通过相对Q函数公式：\n```\nQ(s_t, a_t) = ρ[log π_θ(a_t|s_t) - log π_θ^old(a_t|s_t)]\n```\n其中π_θ^old是采样数据的行为策略，作为稳定锚点，减少波动。\n\n### 2. 低方差奖励\n采用组奖励中心化技术，对每个提示采样n个响应，减去经验平均奖励：\n```\n˜r(x, y^i) = r(x, y^i) - (1/n)∑r(x, y^i)\n```\n并将此中心化奖励广播到生成中的每个标记，提高信用分配效率。\n\n### 3. 多样性保持\n虽然理论证明贪婪选择已足够最优，但作者认识到多样性对推理任务的关键性。因此，ROVER使用softmax对均匀策略Q值进行采样：\n```\nπ_s(a|s) = exp(Q^πu(s,a)/ρ) / ∑_{a'} exp(Q^πu(s,a')/ρ)\n```\n其中ρ是温度参数，平衡探索与利用。作者进一步证明这种方法保持了相对于最优策略的性能保证。\n\n## 实验验证：简约方法的卓越表现\n\n作者在多个数学推理基准上验证了ROVER的有效性，包括Countdown任务（多个有效答案）和数学竞赛任务（单一明确答案）。实验结果表明：\n\n1. **质量提升**：在AIME24、AIME25和HMMT25任务上，ROVER的pass@1比最强基线提高+8.2\n2. **多样性增强**：pass@256提高+16.8，多样性指标提高+17.6%\n3. **泛化能力**：在OOD任务GPQA-diamond上也表现出色\n\n值得注意的是，ROVER发现了基础模型和标准RL方法（如GRPO）训练的模型所缺乏的新推理策略，证明其推动推理边界的潜力。\n\n## 逻辑演进总结\n\n从宏观问题到最终方法论的完整逻辑链可概括为：\n\n1. **问题识别**：当前LLM推理的强化学习方法存在训练不稳定和多样性崩溃问题\n2. **结构观察**：LLM数学推理任务具有特殊的MDP结构，比通用控制环境更简单\n3. **理论假设**：在这种特殊结构下，均匀随机策略的Q值可能包含足够信息指导最优决策\n4. **理论证明**：证明了在树状MDP中，均匀随机策略的Q函数的贪婪策略是最优的\n5. **方法设计**：基于理论证明，设计了ROVER方法，通过softmax采样均匀策略Q值平衡质量和多样性\n6. **实验验证**：在多个基准上验证了ROVER的优越性能，证实了理论假设的有效性\n\n这一逻辑演进展示了作者如何从实际问题出发，通过观察、假设、理论分析和实验验证，最终提出一个简单而有效的解决方案，完美体现了\"简约是终极的精致\"这一理念。", "summary_translation": "可验证奖励强化学习（RL with Verifiable Rewards, RLVR）已成为提升大型语言模型（large language models, LLMs）推理能力的一种有前景的范式。当前方法主要依赖于PPO和GRPO等策略优化框架（policy optimization frameworks），这些框架遵循广义策略迭代（generalized policy iteration），即在评估当前策略价值和基于评估改进策略之间交替进行。尽管这些方法有效，但它们常常面临训练不稳定性（training instability）和多样性崩溃（diversity collapse）的问题，需要复杂的启发式技巧（heuristic tricks）和精细的调整。\n\n我们观察到，数学推理中的标准RLVR可以被形式化为一种专门的有限时域马尔可夫决策过程（finite-horizon Markov Decision Process），具有确定性状态转移（deterministic state transitions）、树结构动态（tree-structured dynamics）和二元终端奖励（binary terminal rewards）。尽管规模庞大，但其底层结构比流行强化学习算法（RL algorithms）（如PPO）所针对的通用控制设置（general-purpose control settings）更为简单，这表明现有方法中的几种复杂技术可能被简化甚至省略。\n\n基于这一见解，我们证明了一个令人惊讶的结果：最优动作（optimal action）可以从固定均匀随机策略（uniformly random policy）的Q函数（Q-function）中恢复，从而绕过广义策略迭代循环（generalized policy iteration loop）及其相关的启发式方法。我们提出了多样推理的随机策略评估（Random Policy Valuation for Diverse Reasoning, ROVER），将这一原则转化为用于LLM数学推理的实用且可扩展的算法（scalable algorithm）。这是一种极简主义（minimalist）但高度有效的强化学习方法，它通过对这些均匀策略Q值（uniform-policy Q-values）应用softmax函数（softmax）来采样动作。ROVER在整个训练过程中保持了多样性，允许持续探索多条有效路径。\n\n在多个基础模型（base models）和标准数学推理基准（math reasoning benchmarks）上，尽管与强大而复杂的现有方法相比进行了根本性的简化，ROVER在质量（quality）（pass@1提高\\textbf{+8.2}，pass@256提高\\textbf{+16.8}）和多样性（diversity）（提高\\textbf{+17.6\\%}）两方面均表现出优越的性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#54", "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF", "link": "/arxiv/2509.24713", "arxiv_id": "2509.24713", "authors": "Jing Liu", "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment. We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. Our theoretical framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies. This approach provides both theoretical insights into reward model failures and practical interventions for improving longtail robustness.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.257289", "filter_reason": "这篇论文的核心是关于改进RLHF（Reinforcement Learning from Human Feedback）中的奖励模型，特别是提高其在长尾分布上的鲁棒性。RLHF是提升大语言模型对齐能力和通用推理能力的关键训练范式，属于LLM基础能力的改进范畴。论文提出了Circuit-Aware Reward Training (CART)方法，通过机制可解释性框架识别处理罕见事件的专门神经回路，从而改进训练策略。这种方法论研究有助于提升LLM的通用推理能力和鲁棒性，而不是将LLM作为工具应用于特定领域。论文虽然没有直接讨论数学或逻辑推理，但通过改进RLHF这一核心训练方法，可以间接提升LLM的通用推理能力。论文不符合任何排除标准，且在训练方法方面有很强的正面指标，因此符合研究目标。", "summary2": "本文旨在解决RLHF中奖励模型在长尾分布上的系统性失败问题。针对罕见偏好场景，我们提出了一种Circuit-Aware Reward Training (CART)框架，通过识别和处理专门处理长尾事件的神经回路来提高模型鲁棒性，并通过理论分析和回路评估指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到CART方法：逻辑推演链\n\n## 1. 宏观问题：RLHF中的长尾失败\n\n作者从强化学习中基于人类反馈的强化学习(RLHF)这一核心技术出发，识别出一个关键问题：**奖励模型在长尾分布上存在系统性失败**。这种失败导致奖励黑客(reward hacking)和模型对齐失败(misalignment)，使得语言模型在处理罕见或代表性不足的输入时产生不可预测的行为。\n\n## 2. 观察与发现：现有方法的局限性\n\n作者系统分析了现有解决方案的不足：\n\n- **黑盒处理**：当前方法(如KL正则化、奖励裁剪、集成方法)将奖励黑客视为黑盒优化问题，仅处理症状而不理解根本机制\n- **粗糙干预**：基于正则化的方法操作粗糙，未明确考虑罕见事件输入的内部表示\n- **缺乏洞察**：悲观奖励建模、对比和集成方法不揭示模型内部哪些组件驱动行为\n- **启发式依赖**：奖励塑形方法依赖启发式，需要大量领域知识，限制了可扩展性\n\n同时，作者注意到语言模型研究中的关键发现：**语言模型通过分布式专业化处理罕见标记**，即空间分布的子网络专门为罕见输入激活。\n\n## 3. 核心假设：电路专业化理论\n\n基于以上观察，作者提出核心假设：**奖励模型也发展出功能上不同的神经电路来处理长尾场景**。具体而言：\n\n- 奖励模型为不同类型的偏好场景发展专门的神经电路\n- 处理罕见偏好场景的电路(长尾电路)接收有限的训练信号，容易产生过度自信和不一致的预测\n- 这种脆弱性被策略利用，导致奖励黑客\n\n这一假设将奖励黑客重新定义为**机制性失败**，而非仅仅是优化问题。\n\n## 4. 理论框架：从电路到泛化边界\n\n作者构建了形式化理论框架，连接电路专业化与长尾性能：\n\n### 4.1 电路分解\n将奖励模型计算概念上分解为电路激活：\n```\nRθ(x) = Σ(wi · ai(x)) + ε(x)\n```\n其中ai(x)表示电路Ci的激活，wi是其输出权重，ε(x)是残差计算。\n\n### 4.2 长尾专业化定义\n定义电路Ci为τ-specialized for the tail：\n```\nEx∼ptail [|ai(x)|] - Ex∼phead [|ai(x)|] > τ\n```\n这捕捉了某些电路对罕见输入有优先响应的现象。\n\n### 4.3 泛化误差边界\n推导出长尾泛化误差边界：\n```\nLtail(θ) ≤ Lhead(θ) + Cp|Ctail|log(1/δ)/√Ntail + β·Div(Ctail, Chead)\n```\n\n这一边界揭示了两个关键见解：\n- 长尾误差随专业化电路数量增加而增加\n- 长尾和头部电路行为间的更大差异加剧泛化失败\n\n## 5. 方法论设计：CART框架\n\n基于理论框架，作者提出Circuit-Aware Reward Training (CART)，包含三个阶段：\n\n### 5.1 电路发现\n识别专门处理长尾偏好的电路：\n- **激活模式分析**：计算常见和罕见场景间的差异激活模式\n- **电路一致性分析**：使用互信息识别为长尾输入一起激活的神经元组\n- **因果验证**：通过激活修补验证电路功能\n\n### 5.2 脆弱性评估\n评估长尾电路被利用的脆弱性：\n- **预测一致性**：测量电路对语义相似输入产生一致输出的能力\n- **对抗敏感性**：评估电路激活小扰动对奖励的影响\n- **覆盖分析**：评估长尾分布的多少部分激活每个电路\n- **复合脆弱性分数**：整合上述指标为单一脆弱性分数\n\n### 5.3 针对性干预\n基于脆弱性评估应用精确干预：\n- **电路引导的数据增强**：生成专门激活脆弱电路的示例\n- **电路正则化**：引入方差惩罚项稳定脆弱电路\n- **渐进式电路强化**：通过课程学习逐渐强调长尾场景\n- **电路感知集成**：组合具有互补专业化的多个模型\n\n## 6. 完整逻辑链总结\n\n从宏观问题到CART方法的完整逻辑链条：\n\n**宏观问题** → **观察发现** → **核心假设** → **理论框架** → **方法论设计** → **CART方法**\n\n1. 从RLHF中的长尾失败问题出发\n2. 观察到现有方法将奖励黑客视为黑盒问题，同时注意到语言模型中的分布式专业化现象\n3. 提出奖励模型也发展专门电路处理长尾场景的核心假设\n4. 建立形式化理论框架，连接电路专业化与泛化边界\n5. 设计系统性方法论，包括电路发现、脆弱性评估和针对性干预\n6. 最终形成CART框架，整合所有组件到综合训练目标中\n\n这一逻辑链条体现了作者如何从问题识别出发，通过跨领域观察形成创新假设，建立理论支撑，最终设计出既有理论深度又有实用价值的方法论，为解决RLHF中的长尾鲁棒性问题提供了新思路。", "summary_translation": "强化学习从人类反馈（Reinforcement Learning from Human Feedback, RLHF）的奖励模型在长尾分布（longtail distributions）上表现出系统性故障，导致奖励黑客（reward hacking）和不对齐（misalignment）。我们提出了一个机制可解释性框架（mechanistic interpretability framework），用于识别奖励模型中负责稀有事件处理（rare-event processing）的专门神经电路（neural circuits）。借鉴最近显示语言模型中稀有标记（rare tokens）的分布式专业化（distributed specialization）的研究进展\\citep{liu2025no, liu2025emergent}，我们假设奖励模型也为长尾场景（longtail scenarios）发展出功能上不同的电路（functionally distinct circuits）。我们的理论框架在电路专业化（circuit specialization）、奖励泛化边界（reward generalization bounds）和长尾性能（longtail performance）之间建立了正式联系。我们引入了**电路感知奖励训练（Circuit-Aware Reward Training, CART）**，它使用电路分析（circuit analysis）来指导数据增强（data augmentation）、正则化（regularization）和集成策略（ensemble strategies）。这种方法既提供了对奖励模型故障的理论见解（theoretical insights），也提供了改善长尾鲁棒性（longtail robustness）的实际干预措施（practical interventions）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#58", "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory", "link": "/arxiv/2509.24653", "arxiv_id": "2509.24653", "authors": "Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu", "summary": "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.259395", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，该论文的本质是改进大语言模型的推理能力，特别是解决组合推理问题（如\"两跳推理的诅咒\"），这直接针对LLM的基础能力提升。论文提出的\"Identity Bridge\"机制通过在零跳身份任务上监督模型，成功使模型能够执行分布外的两跳推理，这明显属于增强LLM逻辑推理能力的创新方法。 其次，论文包含多个正面指标：核心概念上明确关注大语言模型(LLMs)；能力方向上专注于推理能力(reasoning)，特别是组合推理和两跳推理等逻辑推理形式。虽然论文未涉及强化学习或智能体等新兴范式，但其核心贡献已足够符合研究目标。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉内容，不针对特定应用领域（如医疗、化学等），也不关注模型基础设施或部署优化。 最后，论文没有涉及需要特殊判断的模糊情况，它纯粹聚焦于通过改进模型内部机制（潜在几何结构的重塑）来提升通用推理能力，这正是研究目标所寻求的论文类型。论文的理论分析和实验验证都围绕如何增强LLM的隐式推理能力展开，完全符合\"提高大语言模型本身的通用推理能力\"的核心目标。", "summary2": "本文旨在解决大型语言模型在组合推理任务中的\"两跳推理诅咒\"问题。针对合成和真实数据集，我们提出了Identity Bridge机制，通过在桥接实体上添加零跳身份任务来重塑模型潜在空间，并在不同复杂度的数据集上通过OOD两跳推理准确率验证了其有效性。理论分析表明该方法通过隐式核范数正则化促进跨任务内存共享，而小初始化或权重衰减可进一步增强高复杂度任务中的表示对齐和泛化能力。", "inspiration_trace": "# Identity Bridge方法逻辑推演\n\n## 1. 宏观问题：大型语言模型的组合推理困境\n\n大型语言模型(LLMs)在众多任务上取得了显著进展，但在组合推理任务上却存在根本性缺陷。具体表现为\"两跳推理诅咒\"：模型能够学习两个单独的单跳事实(\"A到B\"和\"B到C\")，却无法将它们组合成正确的结论(\"A到C\")，除非在训练数据中明确提供了这种组合或使用了显式的思维链推理。这一现象揭示了当前训练数据和方法的局限性，即使是最先进的系统也无法有效解决。\n\n## 2. 关键观察：组合性差距的具体表现\n\n作者通过实验观察到几个关键现象：\n- 模型在单跳任务上表现良好，但在两跳推理任务上完全失败\n- 即使模型在分布内(ID)的两跳任务上表现良好，也无法泛化到分布外(OOD)的两跳任务\n- 单独训练单跳任务和部分两跳任务，无法帮助模型实现OOD两跳推理\n- 这种组合性差距不会随着模型规模的增加而减小\n\n这些观察表明，问题不在于模型容量不足，而在于模型无法在潜在空间中有效组合已学习的单跳信息。\n\n## 3. 核心假设：潜在空间对齐不足\n\n基于观察，作者提出核心假设：模型无法进行组合推理的根本原因在于潜在空间中对齐不足。具体来说：\n- 第一跳(Subject → Bridge)和第二跳(Bridge → Object)的表示在潜在空间中没有正确对齐\n- 当处理两跳查询时，第二跳无法可靠地\"锁定\"第一跳产生的特征\n- 如果能够通过某种方式对齐这两跳的表示，模型可能能够实现有效的组合推理\n\n## 4. 理论分析：隐式正则化的作用\n\n为验证假设，作者采用简化的Emb-MLP模型进行理论分析：\n- 将问题转化为带约束的优化问题，目标是最大化多类边际\n- 证明基于梯度的训练会诱导隐式核范数正则化，倾向于低秩、跨任务共享结构的解决方案\n- 关键发现：在有身份监督的情况下，这种正则化促进了跨任务内存共享，并将第一跳Subject→Bridge表示与第二跳Bridge→Object映射对齐\n- 通过严格数学证明，身份监督确实能够在OOD组合上产生正边际\n\n## 5. 方法提出：Identity Bridge机制\n\n基于理论分析，作者提出了Identity Bridge方法：\n- 核心思想：通过在桥接实体上添加零跳身份任务，即(e2) → e2，鼓励模型实现身份变换f(e2) = e2\n- 这种监督虽然本身很简单，但能够重塑潜在空间的几何结构\n- 使第二跳能够可靠地利用第一跳产生的特征，从而实现组合推理\n- 对于复杂任务，使用小初始化或权重衰减可以增强正则化效果，进一步提高潜在空间对齐\n\n## 6. 实验验证：从简单到复杂\n\n作者通过一系列精心设计的实验验证了Identity Bridge的有效性：\n- 在简单数据集上，Identity Bridge使模型能够成功执行OOD两跳推理\n- 随着复杂度增加，标准模型的性能下降，表明梯度下降的隐式正则化本身不足以在高复杂度下实现强OOD组合\n- 使用小初始化的模型性能下降更平缓，表明更强的正则化效果进一步约束了潜在几何结构，更好地保留了组合所需的桥接-对象耦合\n- 隐藏状态分析显示，随着训练进行，单跳数据与目标桥接实体的表示逐渐对齐，测试准确度随之提高\n\n## 7. 扩展应用：大规模模型的隐式推理\n\n最后，作者将研究扩展到真实的大规模预训练模型：\n- 即使没有显式的两跳监督，微调信号显示模型在提示与桥接相关的线索时增加了正确答案的概率\n- 这表明预训练过程已经隐式地建立了某种形式的身份桥接\n- 这一发现为增强大型模型的隐式推理能力提供了重要启示：通过设计适当的训练目标和正则化策略，可以进一步强化这种隐式推理能力\n\n## 逻辑链条总结\n\n整个研究形成了一条清晰的逻辑链条：从大型语言模型的组合推理困境出发，通过观察发现潜在空间对齐不足是关键问题，提出假设并通过理论分析验证了隐式正则化的作用，进而提出Identity Bridge方法，通过实验验证其有效性，并最终扩展到大规模模型的应用。这一研究不仅解决了特定的技术问题，还为我们理解语言模型的推理机制提供了新的视角。", "summary_translation": "尽管取得了显著进展，大型语言模型（large language models）在组合推理（compositional reasoning）任务上经常失败，这一现象以\"两跳推理诅咒\"（curse of two-hop reasoning）为例证。本文介绍了身份桥接（Identity Bridge），一种简单而强大的机制，通过在零跳身份任务（zero-hop identity task）上监督模型来解决这种组合性差距（compositionality gap）。我们通过实证证明，这种添加使模型能够成功执行分布外两跳推理（out-of-distribution two-hop reasoning），而这是它们原本完全无法完成的任务。为解释这一现象，我们使用简化的嵌入多层感知机（Emb-MLP）模型进行了理论分析，证明身份监督（identity supervision）重塑了模型的潜在几何结构（latent geometry）。我们表明，这种对齐是由优化过程中的隐式核范数正则化（implicit nuclear-norm regularization）引起的，它倾向于选择跨任务共享结构的低秩解（low-rank solutions）。对于复杂任务，我们使用小初始化（small initialization）或权重衰减（weight decay）来增强正则化效果，这增强了潜在空间对齐效应并减缓了泛化衰减（generalization decay）。最后，我们将研究扩展到大规模模型，观察到它们仍然通过潜在记忆（latent memory）实现两跳推理，这为增强其隐式推理能力（implicit reasoning abilities）提供了重要启示。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#67", "title": "Short window attention enables long-term memorization", "link": "/arxiv/2509.24552", "arxiv_id": "2509.24552", "authors": "Loïc Cabannes, Maximilian Beck, Gergely Szilvasy, Matthijs Douze, Maria Lomeli, Jade Copet, Pierre-Emmanuel Mazaré, Gabriel Synnaeve, Hervé Jégou", "summary": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers. A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval. The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.269323", "filter_reason": "这篇论文的核心贡献是提出了一种名为SWAX的混合架构，结合了滑动窗口注意力和xLSTM线性RNN层，并通过随机改变滑动窗口大小来训练模型。论文的本质是改进LLM的基础架构和训练方法，特别是优化模型的长期记忆和上下文处理能力，这些能力是通用推理能力的基础组件。根据第一步的判断标准，这篇论文属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。在正面指标方面，论文涉及大语言模型的核心概念（注意力机制和RNN层），并提出了一种新的训练方法。论文不符合任何排除标准，没有涉及多模态、特定应用领域或模型可靠性等内容。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。", "summary2": "本文旨在 [解决混合架构中窗口长度选择和softmax注意力与线性RNN层之间相互作用的问题]。针对 [长上下文建模任务]，我们提出了一种 [结合滑动窗口注意力和xLSTM线性RNN层的混合架构SWAX，并采用随机窗口大小训练策略]，并在 [RULER benchmark和多个短上下文推理任务] 上通过 [准确率和困惑度] 验证了其有效性。", "inspiration_trace": "# 从宏观问题到核心方法：SWAX的逻辑演进\n\n## 宏观问题：如何高效处理长序列建模？\n\n在神经网络架构设计中，长序列处理一直面临根本性挑战：全局注意力机制（如Transformer）虽然精确但计算成本随序列长度线性增长；而线性RNN虽然计算恒定但精确度不足。这一矛盾促使研究者探索如何兼顾效率与性能。\n\n## 聚焦问题：混合架构如何优化？\n\n观察到近期研究表明滑动窗口注意力与线性RNN的混合架构优于单独使用任一组件，作者聚焦于一个关键但未充分研究的问题：**滑动窗口长度如何影响混合架构的性能，以及两种组件如何协同工作？**\n\n### 观察与初始假设\n\n**观察1**：现有研究（如De et al., 2024）主要关注验证困惑度，忽略了窗口大小对长上下文性能的影响。\n\n**假设1**：滑动窗口大小会影响模型组件的专门化程度——较大窗口可能导致模型过度依赖softmax注意力，而较小窗口可能迫使线性RNN更好地处理长期依赖。\n\n### 实验验证与反直觉发现\n\n**实验设计**：作者训练了不同窗口大小（128-2048）的SWAX模型（滑动窗口注意力+xLSTM），并在短上下文推理和长上下文检索任务上测试。\n\n**反直觉发现**：与直觉相反，**更大的滑动窗口实际上损害了长上下文性能**。在131k序列长度上，128窗口的SWAX模型准确率约30%，而2048窗口的模型准确率接近0%。\n\n### 机制解释：组件专门化与训练动态\n\n**关键洞察**：这种现象源于训练动态。当窗口较大时，大多数依赖关系落在窗口内，模型倾向于依赖更精确的softmax注意力而非线性RNN。然而，当测试超出窗口长度的序列时，模型无法泛化，因为它从未学会依赖线性RNN进行长上下文建模。\n\n相反，**短窗口迫使线性RNN层承担更多长期依赖建模的责任**，从而在长序列上表现更好。\n\n### 新问题与解决方案\n\n**问题2**：短窗口虽有利于长上下文性能，但在短上下文任务上表现不佳，因为许多提示无法适应小窗口。\n\n**假设2**：随机改变窗口大小可能同时解决两个问题——保持短窗口对长上下文的好处，同时获得大窗口对短上下文的优势。\n\n**实验验证**：作者在训练过程中随机选择窗口大小（128或2048，概率各0.5），并在最后10%训练中退火到固定窗口2048。\n\n**最终发现**：随机窗口训练的SWAX模型在短上下文任务上与固定2048窗口模型相当或更好，同时在长上下文任务上与固定128窗口模型相当或更好。\n\n## 核心方法论：SWAX与随机窗口训练\n\n基于以上逻辑演进，作者提出了最终方法论：\n\n1. **架构设计**：SWAX——交替堆叠滑动窗口注意力(SWA)和xLSTM线性RNN层的混合架构\n2. **训练策略**：随机窗口大小训练——在训练过程中随机切换窗口大小，迫使模型同时利用两种机制的优势\n\n这一方法论解决了传统架构在处理长序列时的效率与性能权衡，通过组件专门化和随机训练策略，实现了在短上下文和长上下文任务上的优异性能。", "summary_translation": "近期研究表明，将滑动窗口softmax注意力层(sliding window softmax attention layers)与线性循环神经网络(RNN)层(linear recurrent neural network layers)相结合的混合架构，性能优于单独使用这两种架构中的任何一种。然而，窗口长度的影响以及softmax注意力与线性RNN层之间的相互作用仍未得到充分研究。在本研究中，我们提出了SWAX，一种由滑动窗口注意力(sliding-window attention)和xLSTM线性RNN层(xLSTM linear RNN layers)组成的混合架构。关于SWAX的一个反直觉发现是，更大的滑动窗口并不能提高长上下文性能。事实上，短窗口注意力通过减少对softmax注意力机制在长上下文检索中的依赖，鼓励模型更好地训练xLSTM的长期记忆能力。小滑动窗口的问题在于它们对短上下文任务有害，而这些问题原本可以通过适度更大的滑动窗口所提供的信息来解决。因此，我们通过随机改变滑动窗口大小来训练SWAX，迫使模型同时利用更长的上下文窗口和xLSTM记忆。使用随机窗口大小训练的SWAX在短上下文和长上下文问题上都显著优于常规窗口注意力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#71", "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models", "link": "/arxiv/2509.24510", "arxiv_id": "2509.24510", "authors": "Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur", "summary": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.276777", "filter_reason": "这篇论文的核心贡献是提出并解释了基础模型（包括大语言模型）中的测试时训练（TTT）机制。作者通过理论分析和实验验证，阐明了TTT如何通过\"先泛化后专业化\"的方式提高模型性能，使模型能够将容量集中在与测试任务相关的概念上。这本质上是一种改进LLM基础能力、增强其通用问题解决能力的研究。虽然论文在ImageNet上进行了视觉领域的实验，但它同时也涵盖了语言任务，并且其理论框架是通用的，不局限于特定领域。TTT可以被视为一种增强模型推理和问题解决能力的新范式，因为它允许模型在测试时根据特定任务进行自适应调整，这与提高LLM通用推理能力的研究目标高度一致。论文没有主要聚焦于需要排除的领域，如特定应用领域或模型可靠性的应用层面问题。因此，这篇论文符合研究范围，应该被保留。", "summary2": "本文旨在解释测试时训练(TTT)在基础模型中有效的原理，即使在分布内数据上也能提升性能。针对大型基础模型的全局欠参数化特性，我们提出了一种\"泛化后专业化\"(specialization after generalization)机制，表明TTT通过重新分配模型容量来专注于与测试任务相关的概念。我们在ImageNet、MNIST和Pile数据集上通过分类准确率和bits per byte等指标验证了其有效性，并证明TTT在欠参数化模型中提升最大。", "inspiration_trace": "# 从宏观问题到方法论：测试时训练(TTT)的逻辑演进\n\n## 1. 宏观问题：TTT为何在分布内数据上依然有效？\n\n传统观点认为测试时训练(TTT)主要在分布外(OOD)适应或使用特权数据时有效。但随着基础模型规模扩大，大多数测试数据实际是分布内的(ID)，即模型在预训练中已见过类似数据。这引发核心问题：**即使测试数据是分布内的，且仅使用已见数据，TTT为何仍能提升预测性能？**\n\n## 2. 初步观察与假设\n\n### 关键观察：\n- 基础模型虽规模庞大，但仍是\"全局欠参数化\"的（模型规模扩大时性能持续提升）\n- 由于欠参数化，即使测试数据是分布内的，模型也无法同时在整个数据分布上近似真实情况\n\n### 核心假设：\nTTT提供了一种\"泛化后特化\"(specialization after generalization)机制——通过暂时\"遗忘\"不相关预训练知识，释放模型容量以更高分辨率学习与当前测试任务相关的概念。\n\n## 3. 理论框架构建：线性表示假设(LRH)\n\n为深入理解这一机制，作者引入线性表示假设：\n- 模型将高级概念表示为潜在空间中的方向\n- 概念空间通常是稀疏的（任何输入只激活少数概念）\n- 形式化：存在s-稀疏概念空间Φ: X → Rd¹，通过学习特征映射Ψ: X → Rd²近似（d² ≪ d¹）\n- 真实函数在概念空间中是线性的：f(x) = ⟨Φ(x), w*⟩\n\n**核心洞察**：现实世界概念数量远超模型维度，导致这些概念在模型激活中叠加(superimposed)。\n\n## 4. 关键观察与验证\n\n基于LRH框架，作者提出并通过实验验证了三个关键观察：\n\n### 观察O1：特征空间保留概念空间几何结构\n- **方法**：在ImageNet上训练稀疏自编码器(SAE)\n- **发现**：SAE映射到概念空间保留了局部几何结构，不同空间选择的邻域在概念空间中的余弦相似度分布几乎相同\n\n### 观察O2：局部邻域由少量概念支持\n- **方法**：在掩码概念向量上训练TTT分类器\n- **发现**：学习的掩码高度稀疏（平均激活约40个概念），远小于邻域中活跃概念总数（约180个）\n- **意义**：一小部分自适应选择的概念足以捕获局部区域相关信息\n\n### 观察O3：TTT隐式寻找稀疏解\n- **方法**：比较在密集重构和稀疏概念上训练的TTT模型\n- **发现**：两者准确性几乎相同，89%情况下预测一致，表明功能等效\n- **意义**：特征空间中的TTT隐式偏向于概念空间中的稀疏解\n\n## 5. 理论分析：何时与为何特化有效\n\n### 何时特化有效？\n通过图像分类和语言建模的缩放研究：\n- TTT在模型欠参数化时显著提升性能（测试损失未饱和阶段）\n- 随着模型规模增大，性能差距缩小（更大模型允许更好全局解缠概念）\n\n### 为何特化有效？\n理论分析表明：\n- 即使模型全局欠参数化（d² ~ log d¹），TTT仍能在测试时泛化\n- 概念在欠参数化特征空间中叠加时，全局训练无法解缠所有概念含义\n- TTT实现局部稀疏恢复，达到O(σ²s log(d₁/s)/k)的最优速率\n\n## 6. 最终方法论：泛化后特化框架\n\n基于以上分析，作者形成了完整的方法论：\n\n1. **基础模型全局欠参数化**：模型无法同时在整个数据分布上近似真实情况\n\n2. **概念叠加现象**：由于概念数量远超模型维度，概念在模型激活中叠加\n\n3. **TTT的特化机制**：通过暂时\"遗忘\"不相关知识，释放容量学习当前任务相关概念\n\n4. **局部稀疏恢复**：TTT在概念空间中执行局部稀疏恢复，有效\"重新加权\"相关概念\n\n5. **适用条件**：机制在欠参数化状态下最有效，随模型增大和概念解缠改善，效果减弱\n\n## 7. 实证验证与扩展\n\n作者通过多种实验验证了这一方法论：\n- MNIST、ImageNet和语言建模任务的缩放研究确认特化在欠参数化状态下最有效\n- 混合专家(MoE)模型作为TTT替代方案，增加专家数量可提高容量和准确性\n- 分析TTT与非参数方法区别，解释TTT在高维情况下更有效的原因\n\n这一完整逻辑链条从宏观问题出发，通过观察、假设、理论构建和实证验证，最终形成了\"泛化后特化\"的核心方法论，深入解释了TTT在基础模型中的工作机制。", "summary_translation": "最近的实证研究探索了在测试时继续针对特定任务训练模型的想法，即测试时训练(test-time training, TTT)，发现它能带来显著的性能提升。然而，对于TTT为何有效以及何时有效，人们的理解仍然有限。早期的解释主要集中在以下观察上：当TTT应用于分布外适应(out-of-distribution adaptation)或与特权数据(privileged data)一起使用时可能有所帮助。然而，基础模型(foundation models)规模的不断扩大，以及大多数测试数据是分布内(in-distribution)的事实，对这些解释提出了质疑。我们反而认为，基础模型仍然是全局欠参数化的(globally underparameterized)，而TTT提供了一种泛化后专业化(specialization after generalization)的机制，将模型能力集中在与测试任务相关的概念上。具体而言，在线性表示假设(linear representation hypothesis)下，我们提出了一个模型，其中TTT实现了比全局训练(global training)更小的分布内测试误差。我们通过在ImageNet上训练稀疏自编码器(sparse autoencoder)，实证验证了模型的关键假设，表明语义相关的数据点仅由少量共享概念解释。最后，我们在图像和语言任务上进行了扩展研究(scaling studies)，确认了我们模型的实际意义，并确定了专业化最有效的区域(regimes)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#84", "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "link": "/arxiv/2509.24372", "arxiv_id": "2509.24372", "authors": "Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen", "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.", "subjects": "Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.294152", "filter_reason": "这篇论文的核心贡献是提出了一种新的LLM微调方法——进化策略(ES)，并证明了它在多个方面优于现有的强化学习微调方法。根据筛选标准，这篇论文应该被保留，原因如下： 首先，从核心判断来看，论文的本质是改进LLM的基础能力和提出新的训练范式，而非将LLM作为工具应用到特定领域。论文专注于LLM的微调技术，这直接关系到提升模型的基础能力。 其次，论文符合多个正面指标：它明确关注Large language models (LLMs)这一核心概念；讨论的训练方法涉及evolution strategies(ES)，这与筛选标准中的\"evolution, self-evolve\"相关；虽然论文没有直接提及reasoning等能力，但改进微调方法通常会提升这些通用能力。 第三，论文不涉及任何排除标准中的领域：没有讨论多模态与视觉问题，没有聚焦于特定应用领域，也没有主要关注模型可靠性层面的水印、安全性等问题。 最后，论文提出的是一种通用的微调方法，可以提升LLM在各种任务上的表现，包括推理能力，这与\"提高大语言模型本身的通用推理能力\"的研究目标一致。论文表明ES方法在样本效率、长期奖励容忍度、对不同基础LLM的鲁棒性等方面优于RL方法，这些都是提升LLM通用能力的重要方面。 因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决大型语言模型微调中强化学习方法的局限性问题。针对数十亿参数的LLM微调任务，我们提出了一种可扩展的进化策略(ES)方法，直接在完整参数空间进行搜索优化，并在Countdown推理任务和简洁性微调任务上通过准确率、样本效率、鲁棒性和稳定性等指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到创新方法：ES在大规模LLM微调中的逻辑演进\n\n## 1. 宏观问题：如何有效微调大规模语言模型？\n\n微调预训练的大规模语言模型(LLMs)以适应下游任务和改进与用户偏好的一致性，已成为AI部署管道中的关键步骤。当前，强化学习(RL)是这种微调的主导选择，但这一领域仍存在重要挑战，需要探索更有效的方法。\n\n## 2. 观察一：现有RL微调方法的局限性\n\n作者首先系统分析了RL在LLM微调中的几个关键问题：\n\n- **样本效率低与高方差**：处理长时程奖励时（LLM微调中常见），RL方法样本效率低，梯度估计方差高。\n- **信用分配困难**：在token级别进行适当信用分配很困难，甚至可能没有帮助。\n- **基础模型敏感性**：RL技术对基础LLM选择敏感，导致不同模型上微调性能不一致。\n- **奖励hacking倾向**：RL固有地倾向于hack奖励函数，导致不良行为。\n- **运行不稳定性**：相同参数设置下，RL微调在多次运行中经常不稳定，显著增加微调成本。\n\n这些局限性表明，需要探索替代方法来解决RL在LLM微调中的固有缺陷。\n\n## 3. 观察二：进化策略(ES)的潜在优势\n\n作者观察到，进化策略(ES)作为一类基于群体的零阶优化算法，在传统控制和游戏问题中相比RL有几个独特优势：\n\n- 高度并行化能力\n- 很好地容忍长时程奖励\n- 广泛的探索能力\n- 更少的计算需求（无需反向传播）\n- 对设置参数的鲁棒性\n\n这些特性使ES成为理论上适合LLM微调的候选方法，但存在一个关键障碍。\n\n## 4. 观察三：ES在LLM时代的应用困境\n\n尽管ES有上述优势，但在LLM时代，ES受到的关注远少于RL。标准ES直接在原始参数空间中搜索和优化，过去实现的维度不超过几百万。普遍认为，对于数十亿参数的LLMs，在参数空间中探索比在动作空间中探索更加困难和样本效率低下。这一认知障碍导致ES在大规模LLM微调中未被充分考虑。\n\n## 5. 核心假设：ES可以扩展到大规模LLM微调\n\n基于以上观察，作者提出了一个挑战传统观念的核心假设：**ES可以扩展到数十亿参数的搜索空间，通过直接在LLM的全参数空间中搜索进行微调任务，并且可能比RL方法表现更好**。\n\n这一假设挑战了\"ES不适合高维优化问题\"的传统认知，开启了将ES应用于大规模LLM微调的可能性。\n\n## 6. 方法设计：实现ES的大规模扩展\n\n为验证假设，作者设计了一个基于算法简化的ES变体的内存高效实现，支持GPU内部和跨GPU并行化。关键创新包括：\n\n1. **随机种子噪声检索**：只存储随机种子而非完整噪声，大幅减少GPU内存使用。\n2. **并行评估**：通过为每个进程分配单独随机种子，实现扰动模型的完全并行评估。\n3. **层级原位扰动和恢复**：逐层原位扰动模型参数，减少峰值GPU内存使用。\n4. **奖励归一化**：使用z-score归一化奖励，使奖励规模在不同迭代和任务间保持一致。\n5. **贪心解码**：使用确定性解码确保性能差异仅来自参数空间探索。\n6. **参数更新分解**：以分解方式执行模型参数聚合更新，进一步减少内存需求。\n7. **学习率消化**：简化标准更新方程，降低计算复杂度。\n\n这些设计使ES能够首次在数十亿参数的LLM全参数空间中进行有效搜索。\n\n## 7. 实验验证：ES与RL的比较\n\n作者在标准推理基准任务(Countdown)和简洁性微调任务中验证了假设：\n\n- **性能优势**：ES在所有测试模型上都优于PPO和GRPO，平均比基线模型提高36.4%，而PPO和GRPO分别仅提高17.9%和21.4%。\n- **样本效率**：即使在数十亿参数空间中搜索，ES比RL更样本高效，通常只需不到20%的训练样本评估就能达到相同性能。\n- **小模型适用性**：ES能够改善较小基础模型(如Qwen2.5-0.5B)的性能，而RL在这些模型上几乎无效。\n- **奖励hacking抵抗**：ES没有表现出奖励hacking行为，而GRPO需要KL散度惩罚来避免此问题。\n- **运行稳定性**：ES在多次运行中表现一致，标准差远低于GRPO。\n\n这些结果强有力地支持了核心假设，表明ES在大规模LLM微调中确实优于RL方法。\n\n## 8. 理论解释：ES成功的关键机制\n\n为解释这些令人惊讶的结果，作者提出了几个关键机制：\n\n- **参数空间探索的稳定性**：在参数空间中采样噪声确保整个动作轨迹只依赖于单个采样，导致rollouts中方差显著降低，而RL中的动作空间探索在每个步骤注入噪声，导致高方差。\n- **解决方案分布优化**：ES内在优化解决方案分布，而RL优化单个解决方案，使ES更难hack奖励，且产生的解决方案更鲁棒。\n- **奖励景观平滑**：ES通过显式高斯卷积直接将噪声注入参数空间，有效平滑锯齿状奖励景观，而RL的噪声注入方式不一定保证参数空间中奖励景观的平滑性。\n\n这些机制解释了为什么ES在大规模LLM微调中能够克服传统认知中的限制，实现优异性能。\n\n## 9. 最终方法论：ES作为LLM微调的新范式\n\n基于以上逻辑演进，作者最终确立了ES作为LLM微调的新范式，超越了当前RL技术提供的范围。这一方法论不仅解决了RL在LLM微调中的多个关键问题，还在参数空间探索、仅结果微调以及大规模分布式后训练方面开辟了新的机会，为未来LLM微调研究提供了新方向。\n\n这一完整逻辑链从宏观问题出发，通过系统观察现有方法的局限性，识别替代方法的潜力，提出挑战传统观念的假设，设计创新方法进行验证，最终形成了一套全新的LLM微调方法论。", "summary_translation": "对预训练大型语言模型（large language models, LLMs）进行下游任务（down-stream tasks）微调是AI部署流程中的关键步骤。强化学习（reinforcement learning, RL）可以说是最突出的微调方法，促成了许多最先进（state-of-the-art）大型语言模型的诞生。相比之下，进化策略（evolution strategies, ES）虽然在参数量仅几百万的模型上曾表现出与强化学习相当的性能，但由于人们对其在更大模型上可扩展性（scalability）的悲观看法而被忽视。在这项工作中，我们报告了首次成功将进化策略（ES）扩展用于微调大型语言模型全部参数的尝试，展示了令人惊讶的事实：进化策略能够高效搜索数十亿参数，并在多个方面优于现有的强化学习微调方法，包括样本效率（sample efficiency）、对长期奖励（long-horizon rewards）的容忍度、对不同基础大型语言模型的鲁棒性（robustness）、较少的奖励破解（reward hacking）倾向，以及跨运行更稳定的性能。因此，它为解锁大型语言模型微调的新方向奠定了基础，超越了当前强化学习技术所能提供的范围。源代码提供于：https://github.com/VsonicV/es-fine-tuning-paper。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#110", "title": "The Impossibility of Inverse Permutation Learning in Transformer Models", "link": "/arxiv/2509.24125", "arxiv_id": "2509.24125", "authors": "Rohan Alur, Chris Hays, Manish Raghavan, Devavrat Shah", "summary": "In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens\" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.384377", "filter_reason": "根据筛选标准，这篇论文符合研究范围，应该被保留。我的判断过程如下： 第一步：核心判断——这篇论文的本质是研究decoder-only transformers在逆置换学习任务上的表达能力限制。论文探讨了大语言模型架构与推理能力之间的关系，并提出添加\"scratch tokens\"可能是一种增强模型推理能力的方法。这本质上是对LLM基础能力的研究，特别是关于其推理能力限制的探索，而不是将LLM作为工具应用到特定领域。论文还明确探讨了这一发现对思维链(CoT)提示如何启用推理的启示，这与提高LLM通用推理能力直接相关。 第二步：正面指标——论文包含多个相关主题： - 核心概念：论文研究的是decoder-only transformers，这是大语言模型的主流架构 - 能力方向：论文明确提到逆置换学习任务模拟了各种推理任务的自然鲁棒性属性，包括长上下文检索、多选QA和上下文学习 - 论文还探讨了思维链(CoT)提示如何启用推理，这是大语言模型推理的重要范式 第三步：排除标准——论文不涉及任何需要排除的领域： - 不涉及多模态与视觉领域 - 不聚焦于任何特定应用领域 - 不涉及模型可靠性的应用层面问题（如水印、安全等） 第四步：特殊和模糊情况——论文没有涉及需要特殊处理的智能体/工具使用或幻觉/可解释性/安全等模糊情况。 综合来看，这篇论文的核心贡献是研究大语言模型在推理任务上的表达能力限制，探讨模型架构与推理能力之间的关系，并提出可能增强模型推理能力的方法。这与\"提高大语言模型的通用推理能力\"的研究目标高度相关，因此应该被保留。", "summary2": "本文旨在研究decoder-only transformers中逆排列学习的不可能性问题。针对排列矩阵和被排列序列作为输入的场景，我们提出了一种通过去除causal attention mask或添加\"scratch tokens\"使逆排列学习成为可能的方法，并在简化的transformer模型上通过均方误差(MSE)指标验证了其有效性。", "inspiration_trace": "# 逆排列学习在Transformer模型中的不可能性：逻辑链分析\n\n## 一、宏观问题：语言模型的通用推理能力之谜\n\n**起点观察**：现代大型语言模型(LLMs)展现出令人惊讶的通用推理能力，能够处理多样化的任务（如上下文学习、多项选择问答、长上下文推理等）。这引发了一个根本性问题：**为什么单一模型架构能够胜任这些看似需要不同归纳偏置的任务？**\n\n## 二、聚焦关键挑战：排列不变性缺失\n\n**深入观察**：作者注意到一个关键现象——尽管LLMs在语言建模中需要位置敏感性，但在许多推理任务中却需要排列不变性：\n- 上下文学习应对示例顺序不变\n- 多项选择问答应对选项顺序不变\n- 长上下文推理应对事实顺序不变\n- 演绎推理应对谓词顺序不变\n\n**核心矛盾**：现代LLMs在这些需要排列不变性的任务上表现不佳，这种对顺序的敏感性破坏了模型可靠性。\n\n## 三、问题形式化：逆排列学习任务\n\n**关键洞察**：逆排列学习是实现排列不变性的自然方式。作者将问题形式化为：给定一个排列和该排列应用于序列后的结果，模型需要恢复原始的\"规范\"序列。\n\n**任务简化**：这个形式化比实际应用场景更简单（因为明确提供了排列描述），但作者推测即使是这个简化任务，仅解码器Transformer也无法解决。\n\n## 四、核心假设：因果注意力掩码的根本限制\n\n**大胆假设**：作者提出核心假设——**仅解码器Transformer中的因果注意力掩码是逆排列学习不可能的根本原因**。\n\n**直觉推理**：\n1. 任何非平凡排列都需要将至少一个元素从序列后部移到前部\n2. 因果注意力掩码阻止了从后部标记到前部标记的信息流动\n3. 因此，仅解码器Transformer无法实现必要的排列逆操作\n\n## 五、理论验证：不可能性证明\n\n**严格证明**：作者通过数学理论验证了假设，证明对于任何非平凡排列，任意深度的仅解码器Transformer都无法输出逆排列后的序列。\n\n**证明核心**：利用因果注意力掩码导致的残差流中每一行对后续行的不变性，证明了信息无法从后面的位置传递到前面的位置，从而阻止了逆排列的实现。\n\n## 六、解决方案探索：突破限制的两种路径\n\n### 路径一：移除因果注意力掩码\n**构造验证**：作者证明移除因果掩码以允许双向注意力机制，可以解决逆排列学习问题。\n**意义揭示**：这表明编码器-解码器架构可能没有仅解码器架构的相同限制。\n\n### 路径二：输入填充策略\n**意外发现**：更令人惊讶的是，简单地用\"scratch tokens\"填充输入也能使逆排列学习成为可能。\n**机制解释**：这些填充标记为模型提供了\"scratch space\"，可用于在前向传播期间执行必要的计算。\n\n## 七、推广意义：对LLMs推理机制的启示\n\n**深层思考**：作者将发现推广到更广泛的LLMs推理机制，提出一个大胆猜想：\n- 输入填充机制可能解释了\"思维链\"提示、\"scratchpad\"提示或\"reasoning tokens\"的成功\n- 即使这些中间标记不编码有意义的语义信息，它们也能通过提供额外计算空间来增强推理能力\n\n**理论贡献**：这为理解Transformer模型中的推理提供了一个新的、尚未探索的机制视角。\n\n## 逻辑链总结\n\n从LLMs通用推理能力的宏观问题出发，作者逐步聚焦到排列不变性这一具体挑战，通过形式化为逆排列学习任务，揭示了仅解码器Transformer的根本限制（因果注意力掩码），并通过理论证明验证了这一限制。随后，作者探索了两种突破限制的路径，特别是发现输入填充这一简单策略的有效性，最终将这一发现推广到对LLMs推理机制的新理解，形成了一个从观察、假设到理论验证再到推广应用的完整逻辑链条。", "summary_translation": "在本技术说明中，我们研究了仅解码器transformer（decoder-only transformers）中的逆排列学习（inverse permutation learning）问题。给定一个排列（permutation）以及应用了该排列的字符串，模型的任务是生成原始的（\"规范的\"（canonical））字符串。我们认为，该任务模拟了多种推理任务中的一种自然鲁棒性特性（robustness property），包括长上下文检索（long-context retrieval）、多项选择问答（multiple choice QA）和上下文学习（in-context learning）。\n\n我们的主要贡献是一个不可能性结果（impossibility result）：我们证明了任意深度（arbitrary depth）的仅解码器transformer（decoder-only transformer）无法学习此任务。这一结果涉及仅解码器transformer模型的表达能力（expressive capacity），与训练动态（training dynamics）或样本复杂度（sample complexity）无关。我们提供了两种替代构造（alternative constructions），在这些构造下逆排列学习（inverse permutation learning）是可行的。\n\n其中第一种构造强调了因果注意力掩码（causal attention mask）的基本作用，并揭示了编码器-解码器transformer（encoder-decoder transformers）与更流行的仅解码器架构（decoder-only architecture）之间的表达能力（expressivity）差距。后一种结果更为出人意料：我们表明，仅用\"草稿标记\"（scratch tokens）填充输入（padding）就能产生一种使逆排列学习（inverse permutation learning）成为可能的构造。\n\n我们推测（conjecture），这可能表明一种替代机制，通过这种机制，思维链提示（chain-of-thought prompting）或更一般地说，中间\"思考\"标记（intermediate \"thinking\" tokens）可以在大型语言模型中实现推理，即使这些标记不编码有意义的语义信息（semantic information）（例如，中间计算（intermediate computations）的结果）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#128", "title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?", "link": "/arxiv/2509.24005", "arxiv_id": "2509.24005", "authors": "Chenruo Liu, Yijun Dong, Qi Lei", "summary": "We initiate a unified theoretical and algorithmic study of a key problem in weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained student with pseudolabels from a weaker teacher on a downstream task with spurious correlations, does W2S happen, and how to improve it upon failures? We consider two sources of spurious correlations caused by group imbalance: (i) a weak teacher fine-tuned on group-imbalanced labeled data with a minority group of fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set pseudolabeled by the teacher with a minority group of fraction $\\eta_u$. Theoretically, a precise characterization of W2S gain at the proportional asymptotic limit shows that W2S always happens with sufficient pseudolabels when $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S gain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is corroborated by extensive experiments on various spurious correlation benchmarks and teacher-student pairs. To boost W2S performance upon failures, we further propose a simple, effective algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning. Our algorithm is group-label-free and achieves consistent, substantial improvements over vanilla W2S fine-tuning.", "subjects": "Machine Learning, Machine Learning", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.439677", "filter_reason": "这篇论文的核心是研究弱到强(W2S)泛化问题，特别是在存在虚假相关性的情况下如何提高模型性能。根据筛选标准，我判断它符合研究范围，原因如下： 1. 核心判断：论文的本质是关于改进模型训练范式和泛化能力的理论研究。它探讨了如何通过弱教师模型的伪标签来微调强预训练学生模型，并提出了一种算法来提高W2S性能。这属于改进LLM基础能力的研究，而非将LLM作为工具应用到特定领域。 2. 正面指标：虽然论文没有直接使用\"Large language models, LLMs\"术语，但提到的\"strong pre-trained student\"通常指的就是LLM。论文关注泛化能力(generalization)，这与推理能力密切相关，属于LLM基础能力的重要组成部分。 3. 排除标准：论文不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的内容，因此不符合排除标准。 4. 特殊情况处理：论文不涉及智能体/工具使用或幻觉/可解释性/安全等特殊或模糊情况，而是专注于模型训练和泛化能力的理论分析与算法改进。 综上所述，这篇论文致力于提高LLM的泛化能力，属于改进LLM基础能力的研究，符合\"提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在研究在存在虚假相关性的情况下，弱到强（W2S）泛化是否会发生以及如何改进其失败情况。针对由群体不平衡引起的虚假相关性问题，我们提出了一种理论框架，揭示了标记数据和无标签数据中少数群体比例差异对W2S泛化的影响，并提出了一种基于学生模型置信度的增强型W2S算法。在多个虚假相关性基准测试（Waterbirds、BFFHQ、ImageNet-9和BG-COCO）上通过平均准确率和最差群体准确率验证了其有效性。", "inspiration_trace": "# 从问题观察到方法论创新：弱到强泛化在虚假相关性下的逻辑推演\n\n## 1. 宏观问题：超对齐挑战下的弱到强泛化\n\n论文研究的核心宏观问题是：**在存在虚假相关性的情况下，弱到强(W2S)泛化是否会发生？** 这一问题源于对超对齐(superalignment)的探索——即超人类智能的模型能否从较弱的人类监督中学习。W2S泛化提供了积极答案：强大的预训练学生模型通过弱教师生成的伪标签进行微调，通常可以超越其教师表现。然而，现实场景中数据往往存在系统性偏见，如人口统计或获取因素相关的虚假相关性，这为W2S泛化带来了新的挑战。\n\n## 2. 问题观察与界定：虚假相关性的双重来源\n\n作者通过细致观察，识别出W2S场景中虚假相关性的两个关键来源：\n\n1. **弱教师自身的不平衡**：弱教师在不平衡标记数据（少数群体比例为ηℓ）上进行微调，导致其学习到虚假相关性\n2. **无标签数据的不平衡**：用于伪标记的无标签集本身存在不平衡（少数群体比例为ηu）\n\n这引出了核心研究问题：当ηu ≠ ηℓ时，W2S泛化是否会发生？若失败，如何改进？作者将问题精确界定为：研究标记数据和无标签数据中群体比例不匹配对W2S泛化的影响机制。\n\n## 3. 理论假设：建立分析框架\n\n为系统分析这一问题，作者构建了理论框架，提出三个关键假设：\n\n1. **特征解耦假设**：下游任务数据可分解为核心特征z(x)（决定标签y）和群体特征ξ(x)（决定群体标签g），其中核心特征在群体间不变，而群体特征引入虚假相关性\n\n2. **弱-强模型表示差异假设**：\n   - 弱教师表示φT严重纠缠核心特征和群体特征\n   - 强学生表示φS部分解纠缠这些特征，带来对虚假相关性的鲁棒性\n\n3. **比例渐近极限假设**：考虑高维特征维度dz、标记样本数n和无标签样本数N同时趋于无穷的极限情况，便于理论分析\n\n基于这些假设，作者推导出核心理论预测：**W2S增益与(ηu - ηℓ)²成反比**，即标记数据和无标签数据中少数群体比例的不匹配程度越大，W2S泛化越可能失败。\n\n## 4. 实验验证：理论与现实的桥梁\n\n为验证理论预测，作者设计了双重实验策略：\n\n1. **合成实验**：使用高斯数据验证理论预测，结果显示：\n   - 当ηu ≈ ηℓ时，W2S增益最大\n   - 教师与学生表示相似度越低，W2S增益越大\n   - 当ηu ≠ ηℓ时，W2S增益显著下降\n\n2. **真实世界评估**：在多个带有虚假相关性的基准数据集（Waterbirds、BFFHQ、ImageNet-9和BG-COCO）上验证，结果一致表明：\n   - 当ηℓ = 0.5（平衡标记数据）时，增加ηu提高W2S性能\n   - 当ηℓ = ηo（不平衡标记数据）时，W2S增益在ηu = ηo时最大，随ηu增加而减少\n   - W2S增益随|ηu - ηℓ|增加而单调下降\n\n这些实验结果与理论预测高度一致，证实了标记数据和无标签数据中群体比例不匹配是W2S泛化失败的关键因素。\n\n## 5. 方法论创新：Enhanced-W2S\n\n基于理论和实验发现，作者提出了Enhanced-W2S方法，专门解决ηu ≠ ηℓ时的W2S泛化失败问题。该方法包含两个创新组件：\n\n1. **基于置信度的样本选择**：选择学生模型预测置信度最高（熵最低）的样本子集进行再训练。这一操作隐式地平衡了数据分布，因为高置信度样本主要来自多数群体，有效降低了实际训练中的ηu。\n\n2. **广义交叉熵(GCE)损失**：对选定样本应用GCE损失而非标准交叉熵损失，减轻弱教师伪标签噪声的负面影响。GCE损失对高置信度但错误的伪标签惩罚较轻，提高了对噪声的鲁棒性。\n\n该方法的关键优势是**无需群体注释**，在标记数据或无标签数据高度群体不平衡的情况下显著提高了W2S增益。实验证明，Enhanced-W2S在多个数据集和模型对上始终优于原始W2S方法，平均提升达4-11%。\n\n## 6. 逻辑链条总结\n\n整个研究的逻辑演进形成了一个完整的闭环：\n\n**宏观问题**（超对齐下的W2S泛化）→ **问题观察**（虚假相关性的双重来源）→ **理论假设**（特征解耦与表示差异）→ **理论发现**（W2S增益与(ηu-ηℓ)²成反比）→ **实验验证**（合成与真实数据集一致支持理论）→ **方法创新**（基于置信度的样本选择与GCE损失）→ **效果验证**（显著提升W2S性能）\n\n这一研究不仅回答了\"在虚假相关性下W2S是否会发生\"的问题，更提供了\"如何改进失败情况\"的实用解决方案，为弱监督学习在现实场景中的应用提供了重要理论基础和方法支持。", "summary_translation": "我们对弱到强(weak-to-strong, W2S)泛化中的一个关键问题进行了统一的理论和算法研究：当使用较弱教师(weaker teacher)生成的伪标签(pseudolabels)在具有虚假相关性(spurious correlations)的下游任务上微调强预训练学生(strong pre-trained student)模型时，W2S是否会发生，以及如何在失败时改进它？我们考虑了由群体不平衡(group imbalance)引起的两种虚假相关性来源：(i)在群体不平衡的标记数据(labeled data)上微调的弱教师，其中少数群体(minority group)比例为$\\eta_\\ell$；(ii)由教师标记的群体不平衡未标记集(unlabeled set)，其中少数群体比例为$\\eta_u$。理论上，在比例渐近极限(proportional asymptotic limit)下对W2S增益(W2S gain)的精确表征表明，当$\\eta_u = \\eta_\\ell$时，只要有足够的伪标签，W2S总会发生；但当$\\eta_u \\ne \\eta_\\ell$时，W2S可能会失败，且W2S增益随着$(\\eta_u - \\eta_\\ell)^2$的增加而减小。我们的理论在各种虚假相关性基准(spurious correlation benchmarks)和教师-学生对(teacher-student pairs)上的大量实验中得到了证实。为了在W2S失败时提升其性能，我们进一步提出了一种简单有效的算法补救措施(algorithmic remedy)，即在W2S微调后，在强学生模型的高置信度数据子集(high-confidence data subset)上重新训练该模型。我们的算法是无群体标签的(group-label-free)，并且相对于原始的W2S微调(vanilla W2S fine-tuning)，实现了一致且显著的改进。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#148", "title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know", "link": "/arxiv/2509.23830", "arxiv_id": "2509.23830", "authors": "Albus Yizhuo Li", "summary": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive yet efficient Large Language Models (LLMs). However, the standard deterministic routing mechanism presents a significant limitation: its inherent brittleness is a key contributor to model miscalibration and overconfidence, resulting in systems that often do not know what they don't know. This thesis confronts this challenge by proposing a structured \\textbf{Bayesian MoE routing framework}. Instead of forcing a single, deterministic expert selection, our approach models a probability distribution over the routing decision itself. We systematically investigate three families of methods that introduce this principled uncertainty at different stages of the routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space}, and the final \\textbf{selection-space}. Through a series of controlled experiments on a 3-billion parameter MoE model, we demonstrate that this framework significantly improves routing stability, in-distribution calibration, and out-of-distribution (OoD) detection. The results show that by targeting this core architectural component, we can create a more reliable internal uncertainty signal. This work provides a practical and computationally tractable pathway towards building more robust and self-aware LLMs, taking a crucial step towards making them know what they don't know.", "subjects": "Machine Learning, Statistics Theory, Machine Learning", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.461388", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断 这篇论文的本质是改进LLM的基础架构组件，具体是针对Mixture-of-Experts (MoE)架构中的路由机制进行创新。论文提出了贝叶斯MoE路由框架，通过在路由决策上建模概率分布，而不是强制确定性选择，来解决模型校准不良和过度自信问题。这明显属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域。因此，根据第一步判断标准，应该保留。 第二步：正面指标 论文明确包含以下正面指标： - 核心概念：论文直接关注Large Language Models (LLMs)，特别是基于MoE架构的LLMs。 - 能力方向：虽然论文没有直接讨论推理或规划，但它研究的模型校准和不确定性建模是高质量推理的基础。一个能准确评估自身不确定性的模型更有可能在推理任务中表现出色。 第三步：排除标准 论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对任何特定应用领域（如医疗、化学、生物等） - 虽然关注模型可靠性，但这是从架构层面（MoE路由机制）来提高模型校准能力，而非应用层面的水印、安全或安全性问题。 第四步：特殊和模糊情况 论文关注模型校准和不确定性建模，这与减少幻觉和提高推理质量密切相关。论文提出的贝叶斯MoE路由框架是一种新的架构改进方法，旨在增强模型对自身不确定性的认知，从而减少过度自信和错误判断。这属于提出新方法来增强模型内在可靠性，从而提升通用推理质量的情况，应该保留。 最终决策 综合分析，这篇论文的核心贡献是通过改进MoE架构的路由机制，增强LLM的校准能力和不确定性建模，使其\"知道它们不知道什么\"。这种改进直接提升了模型的基础能力，为更高质量的推理提供了基础，完全符合\"提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型(LLMs)中的过度自信和校准问题，特别是Mixture-of-Experts(MoE)架构中确定性路由机制的脆弱性。针对MoE路由决策的不确定性建模问题，我们提出了一种结构化的贝叶斯MoE路由框架，在权重空间、logit空间和选择空间三个不同阶段引入原则性不确定性，并在3B参数的MoE模型上通过路由稳定性、分布内校准和分布外检测等指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到贝叶斯MoE路由：逻辑推导链\n\n## 1. 宏观问题：LLMs的\"不知道自己不知道\"困境\n\n大型语言模型(LLMs)面临一个根本性挑战：它们经常产生高度自信但错误的输出，这种现象被称为\"过度自信\"和\"幻觉\"。在高风险领域如医学和法律，这种\"不知道自己不知道\"的状态构成了重大障碍，阻碍了LLMs的可靠部署。\n\n**核心问题**：模型无法量化自身的预测不确定性，导致输出与实际知识状态不匹配。\n\n## 2. 问题聚焦：MoE架构中的路由机制\n\n作者将问题进一步聚焦到Mixture-of-Experts (MoE)架构的关键组件——路由机制上。MoE通过路由网络激活模型参数的一小部分，实现了大规模但高效的LLMs。然而，作者识别出一个关键脆弱点：\n\n**关键观察**：在MoE中，路由器的决策不是微调，而是决定哪些专门子网络被激活。错误的路由选择意味着将错误的专家应用于token，导致输出缺陷。在具有数十个堆叠MoE层的现代LLMs中，单个路由错误会引发级联故障。\n\n**假设形成**：确定性路由机制是MoE模型校准不良和过度自信的主要贡献者。\n\n## 3. 实验验证：路由机制的脆弱性与随机性潜力\n\n为验证假设，作者设计了两个关键实验：\n\n### 实验1：确定性路由的脆弱性\n- **方法**：在MoE层输入添加微小扰动，测量专家选择变化\n- **结果**：即使极小噪声(γ≥0.005)也导致稳定性显著下降\n- **关键发现**：不稳定性集中在特定层组(早期、中期和最终层)\n\n### 实验2：随机路由的潜力\n- **方法**：用温度控制的随机采样替换确定性Top-K选择\n- **结果**：在中后层引入随机性显著降低ECE(校准误差)，同时保持准确度\n- **洞见**：随机性作为正则化形式，迫使模型减少过度自信\n\n## 4. 方法论形成：从随机性到原则性贝叶斯框架\n\n基于实验结果，作者形成了核心方法论：\n\n**逻辑推导**：如果确定性路由是脆弱的，而简单随机性有益，那么原则性的、数据驱动的不确定性方法应该更优。\n\n这引导作者提出结构化贝叶斯路由框架，在路由管道的不同阶段引入原则性不确定性：\n\n### 4.1 专家质心空间(权重空间)方法\n将路由器的权重矩阵视为随机变量，通过贝叶斯多项逻辑回归建模：\n- **MC Dropout Router (MCDR)**：推理时保持dropout，近似权重后验\n- **SWAG Router (SWAGR)**：用SGD轨迹的权重矩近似高斯后验\n- **Deep Ensembles Router (DER)**：多个独立训练路由器作为经验后验样本\n\n### 4.2 专家Logit空间(潜在空间)方法\n直接在logit向量上建模概率分布，使用变分推断：\n- **Mean-Field Variational Router (MFVR)**：对角协方差高斯分布\n- **Full-Covariance Variational Router (FCVR)**：完全协方差高斯分布，捕获专家间相关性\n\n### 4.3 专家选择空间(决策空间)方法\n学习输入相关温度控制随机性：\n- **Variational Temperature Sampling Router (VTSR)**：神经网络预测温度，动态控制路由随机性\n\n## 5. 方法评估与验证\n\n通过系统实验验证三个核心假设：\n\n### 稳定性假设\n贝叶斯方法在输入扰动下表现出更高稳定性，FCVR达到0.897的Jaccard相似度，远超确定性基线的0.650。\n\n### 校准假设\n所有贝叶斯方法在不损害准确性的情况下显著改善校准，FCVR将ECE降低94%以上。\n\n### OoD检测假设\n贝叶斯路由器的不确定性信号显著提升OoD检测性能，logit方差等内部信号优于传统熵信号。\n\n## 6. 实用性考量\n\n作者分析了计算和内存开销，证明这些方法在实际应用中可行：\n- MCDR几乎无额外开销\n- FCVR提供最佳性能，中等计算成本\n- VTSR提供独特低延迟方案\n\n## 逻辑链条总结\n\n从\"LLMs不知道自己不知道\"的宏观问题出发，通过聚焦MoE路由机制，实验验证确定性路由的脆弱性和随机性的潜力，最终形成结构化贝叶斯路由框架。这一框架通过在权重空间、logit空间和选择空间引入原则性不确定性，实现了更稳定、更好校准、更具自我意识的LLMs，朝着\"让LLMs知道它们不知道什么\"的目标迈出了关键一步。", "summary_translation": "混合专家（Mixture-of-Experts, MoE）架构使得创建大规模且高效的大型语言模型（Large Language Models, LLMs）成为可能。然而，标准的确定性路由机制（deterministic routing mechanism）存在一个显著限制：其固有的脆性是模型校准不当（miscalibration）和过度自信（overconfidence）的关键因素，导致系统往往不知道自己不知道什么。本论文通过提出一个结构化的**贝叶斯MoE路由框架**（Bayesian MoE routing framework）来应对这一挑战。我们的方法不是强制进行单一的确定性专家选择，而是对路由决策本身进行概率分布建模。我们系统地研究了三类方法，这些方法在路由管道的不同阶段引入这种原则性不确定性：在**权重空间**（weight-space）、**logit空间**（logit-space）和最终的**选择空间**（selection-space）中。通过对一个30亿参数的MoE模型进行一系列对照实验，我们证明了该框架显著提高了路由稳定性、分布内校准（in-distribution calibration）和分布外（out-of-distribution, OoD）检测能力。结果表明，通过针对这一核心架构组件，我们能够创建更可靠的内部不确定性信号。这项工作为构建更强大和更具自我意识的LLMs提供了一条实用且计算上可行的途径，朝着使它们知道自己不知道什么的目标迈出了关键一步。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#157", "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement", "link": "/arxiv/2509.23799", "arxiv_id": "2509.23799", "authors": "Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu", "summary": "Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.473954", "filter_reason": "根据筛选标准，我判断这篇论文符合研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是改进LLM的基础能力。论文提出了SAE-RSV方法，通过稀疏自编码器来优化steering向量，这是一种不修改模型参数但能增强LLM控制能力的新技术。这属于改进LLM基础能力的范畴，而非将LLM作为工具应用到特定领域，因此应保留。 第二步：正面指标——论文明确符合\"Large language models, LLMs\"这一核心概念。虽然论文没有直接提及reasoning、planning等能力方向，也没有涉及强化学习等训练方法或智能体等新兴范式，但其提出的steering向量优化技术可以视为提升模型通用能力的一种方法。 第三步：排除标准——论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域（如医疗、化学等）或模型可靠性（应用层面）的研究。 第四步：特殊和模糊情况——论文不属于智能体/工具使用的特殊范畴。虽然其方法可能间接影响模型的可解释性和控制能力，但这不是论文的主要焦点。 综合判断：这篇论文的核心贡献是提出了一种通过稀疏自编码器优化steering向量的方法，以增强LLM的控制能力。这属于改进LLM基础能力的研究，虽然没有直接针对推理能力，但提供了一种可能间接提升模型在各种任务上表现的新技术，因此符合\"大语言模型通用推理能力\"的研究范围。", "summary2": "本文旨在解决LLM引导方法在小数据集场景下效果不佳的问题。针对有限训练数据导致的引导向量噪声问题，我们提出了一种基于稀疏自编码器的引导向量精炼方法（SAE-RSV），并在Llama-3-8B-Instruct模型上通过五个行为概念数据集以成功率和熵为指标验证了其有效性。", "inspiration_trace": "# 从宏观问题到创新方法：SAE-RSV的逻辑演进\n\n## 宏观问题：LLM行为控制的挑战\n\n大型语言模型(LLM)在各种任务中表现出色，但其可控性仍是一个核心挑战。如何在不修改模型参数的情况下有效控制LLM行为，特别是在资源受限场景中，是一个值得探索的研究方向。\n\n## 观察一：转向方法的数据依赖性\n\n作者首先观察到，转向方法(如CAA)通过修改内部表示来引导模型行为，是一种有前景的控制方向。然而，这些方法通常依赖大规模数据集来提取有用的行为信息，这限制了它们在实际场景中的应用，因为许多情况下只有少量训练样本可用。\n\n**核心问题浮现**：如何在小数据集条件下构建有效的转向向量？\n\n## 观察二：小数据集导致的转向向量缺陷\n\n进一步观察发现，从小数据集学习的转向向量存在两个关键问题：\n1. **噪声问题**：转向向量常包含大量与任务无关的噪声特征，因为LLM的隐藏激活可能捕获虚假相关性\n2. **缺失问题**：小数据集无法覆盖目标行为的所有场景，导致转向向量缺失部分任务相关特征\n\n**问题细化**：如何同时解决转向向量的噪声和缺失问题？\n\n## 观察三：现有方法的局限性\n\n作者注意到，现有的基于稀疏自编码器(SAE)的去噪方法主要依赖激活统计信息选择特征。然而，当训练样本较少时，这种统计方法不可靠，因为稳健的特征激活估计需要足够数据。此外，这些方法常选择表面相关的特征(如标点符号或停用词)，而非真正与任务相关的特征。\n\n**假设形成**：基于特征的语义而非统计特性来识别和精炼转向向量，可能更有效。\n\n## 方法论构建：从假设到实践\n\n### 假设一：语义驱动的特征去噪\n作者假设：如果能利用特征的语义信息(由SAE提供)来判断其与目标任务的相关性，就能更可靠地识别和去除噪声特征。\n\n**实现方案**：\n1. 构建种子特征集，测量正负样本间的激活差异\n2. 使用LLM判断每个特征是否与目标任务语义相关\n3. 将特征分为噪声特征和任务相关特征\n4. 构建噪声向量并从原始转向向量中减去\n\n### 假设二：语义驱动的特征增强\n作者进一步假设：除了去噪外，还应通过语义相似性恢复小数据集中缺失的任务相关特征。\n\n**实现方案**：\n1. 计算每个未识别特征的语义有用性分数，基于其与已识别相关特征的相似性和与噪声特征的差异性\n2. 选择得分最高的特征作为有用特征\n3. 构建有用向量并添加到原始转向向量中\n\n## 最终方法：SAE-RSV的整合\n\n作者将上述两个组件整合为统一框架SAE-RSV，其数学表达为：\n\n```\nv'_steer = α1 · v_steer - α2 · v_noise + α3 · v_useful\n```\n\n这一公式体现了三个关键操作的平衡：\n- 保留原始转向向量的核心方向(α1 · v_steer)\n- 去除噪声特征的干扰(-α2 · v_noise)\n- 增强缺失的任务相关信息(+α3 · v_useful)\n\n## 实验验证与发现\n\n通过在五个数据集上的实验，作者验证了SAE-RSV的有效性：\n1. SAE-RSV显著优于所有基线方法，包括监督微调\n2. 去噪和增强模块各自贡献了性能提升(平均分别提升3.6%和7.2%)\n3. 在小数据集条件下，原始转向向量中超过93.6%的特征是噪声，而仅捕获了42.2%的任务相关特征\n\n这些发现证实了作者的初始假设：基于语义的特征去噪和增强能有效解决小数据集条件下的转向向量质量问题。\n\n## 总结：从问题到解决方案的逻辑链\n\n作者的思考过程展现了清晰的逻辑演进：\n\n1. **宏观问题**：LLM行为控制的数据效率挑战\n2. **观察聚焦**：转向方法的数据依赖性和小数据集缺陷\n3. **假设形成**：语义驱动的特征选择可克服统计方法局限性\n4. **方法论构建**：分别设计去噪和增强模块\n5. **方法整合**：形成统一的SAE-RSV框架\n6. **实验验证**：证明方法的有效性和优越性\n\n这一逻辑链不仅解决了特定技术问题，还为资源受限场景下的LLM控制提供了新思路，体现了从观察到假设再到实践的科学研究范式。", "summary_translation": "Steering（引导）已成为一种在不修改模型参数的情况下控制大型语言模型（large language models, LLMs）的有前景方法。然而，大多数现有的steering（引导）方法依赖大规模数据集来学习清晰的行为信息，这限制了它们在许多真实场景中的适用性。从小数据集提取的steering vectors（引导向量）通常包含任务无关的噪声特征，这降低了它们的有效性。为了精炼从有限数据中学习的steering vectors（引导向量），我们提出了通过稀疏自编码器优化引导向量（Refinement of Steering Vector via Sparse Autoencoder, SAE-RSV）的方法，该方法利用SAE（sparse autoencoder，稀疏自编码器）对steering vectors（引导向量）进行语义去噪和增强。在我们的框架中，我们首先根据SAE（稀疏自编码器）提供的语义移除任务无关特征，然后通过与已识别相关特征的语义相似性，丰富小数据集中缺失的任务相关特征。大量实验表明，所提出的SAE-RSV方法显著优于所有基线方法，包括监督微调（supervised fine-tuning）。我们的研究结果表明，通过SAE（稀疏自编码器）精炼原始steering vector（引导向量），可以从有限的训练数据构建有效的引导向量。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#177", "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation", "link": "/arxiv/2509.23667", "arxiv_id": "2509.23667", "authors": "Sungmin Cha, Kyunghyun Cho", "summary": "For efficiency, preference alignment is often performed on compact, knowledge-distilled (KD) models. We argue this common practice introduces a significant limitation by overlooking a key property of the alignment's reference model: its distributional recall. We show that the standard KD -> Align workflow diminishes the model's capacity to align rare yet desirable behaviors, even under strong preference signals. We instead demonstrate that reversing the pipeline (i.e., Align -> KD) is essential: alignment must first be performed on a high-recall reference before distillation. Our contributions are threefold. First, we provide a minimal working explanation of how the reference model constrains preference alignment objectives at a fundamental level. Second, we validate this theory in a controllable Mixture-of-Gaussians experiment, where low-recall anchoring consistently results in suboptimal model performance. Finally, we demonstrate that the same phenomenon holds in LLM alignment with the SmolLM2 family: models aligned after KD fail to effectively align target behaviors, resulting in substantially lower reward and target precision. In contrast, our proposed Align -> KD pipeline robustly aligns these behaviors, yielding models with superior target-oriented metrics and lower variance. Together, these results establish reference-model recall as a first-order design choice in alignment, offering a clear principle: alignment must precede distillation.", "subjects": "Machine Learning", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.481769", "filter_reason": "这篇论文的核心是关于改进大语言模型的训练范式，特别是对齐(alignment)和知识蒸馏(knowledge distillation)的顺序问题。论文提出先对齐再蒸馏(Align -> KD)的流程，以保持模型的对齐效果，特别是对于罕见但理想的行为。这属于改进LLM基础能力和训练范式的研究，符合我的研究目标中\"提出新的训练范式\"的范畴。 具体分析： 1. 第一步核心判断：论文本质是改进LLM的基础训练流程，属于模型基础能力的改进，应该保留。 2. 第二步正面指标：论文明确涉及大语言模型(LLMs)这一核心概念，并在SmolLM2系列模型上验证了其方法。 3. 第三步排除标准：论文不涉及多模态、特定应用领域或模型可靠性的应用层面等排除领域。 4. 第四步特殊和模糊情况：论文虽未直接讨论推理能力，但对齐过程本身是提升模型整体性能和可靠性的关键环节，良好的对齐可以间接提高模型在各种任务上的表现，包括推理任务。 论文的核心贡献是提出了一种改进的模型训练流程，通过优化对齐和蒸馏的顺序来提高模型的对齐质量和整体性能。虽然不是直接针对推理能力的研究，但这种基础训练范式的改进可能会对模型的通用能力产生积极影响，包括推理能力。因此，这篇论文符合我的研究范围。", "summary2": "本文旨在解决大型语言模型对齐与知识蒸馏的顺序问题。针对当前先蒸馏后对齐(KD → Align)的常见做法，我们提出了先对齐后蒸馏(Align → KD)的流程，强调必须首先在高召回率参考模型上执行对齐。在Mixture-of-Gaussians实验和SmolLM2语言模型系列上，通过目标精度、召回率和奖励等指标验证了其有效性，证明KD → Align流程存在结构性低召回陷阱，而Align → KD能产生更稳定、性能更优的紧凑模型。", "inspiration_trace": "# 从宏观问题到核心方法：逻辑链推演\n\n## 1. 宏观问题起点：效率与对齐的平衡\n\n**核心挑战**：如何高效地对大型语言模型(LLM)进行人类偏好对齐，同时保持模型的计算效率？\n\n这是一个实际部署中的关键问题——我们既需要模型能够准确对齐人类偏好，又希望模型足够小以便高效部署。\n\n## 2. 观察到的现象：常见实践及其隐含假设\n\n**观察到的事实**：当前领域普遍采用 **KD → Align** 工作流程：\n- 预训练大型模型\n- 监督微调(SFT)\n- 知识蒸馏(KD)得到紧凑模型\n- 在紧凑模型上进行偏好对齐\n\n**隐含假设**：这种做法认为对齐是一个可以在模型压缩后独立进行的步骤，主要出于实用主义考虑：\n1. 降低对齐过程的计算成本\n2. 目标是获得紧凑的最终模型\n3. 紧凑模型通常更容易获取和使用\n\n## 3. 发现的关键问题：召回率缺失\n\n**异常发现**：知识蒸馏虽然保留了高频、高概率的模式，但会**系统性修剪稀有模式**，降低模型的分布召回率(distributional recall)。\n\n**核心洞察**：当使用这种低召回率的模型作为参考模型(π_ref)进行对齐时，即使面对强烈的偏好信号，对齐过程也无法有效恢复这些稀有但理想的行为。\n\n## 4. 提出假设：参考模型召回率的关键作用\n\n**核心假设**：参考模型的召回率是偏好对齐成功的决定性因素，而非次要考虑。\n\n**具体假设**：理想行为必须位于参考模型的支持范围内(即π_ref(y*|x) > 0)，否则会导致结构性失败，作者称之为\"**低召回率陷阱**\"。\n\n## 5. 理论分析：低召回率陷阱的机制\n\n作者从理论上解析了低召回率陷阱如何在对齐算法中体现：\n\n### 在RLHF/PPO中：\n- 策略更新由KL惩罚塑造的奖励信号驱动：r'(y|x) = R(y|x) - β log(π_θ(y|x)/π_ref(y|x))\n- 对于理想响应y*，如果π_ref(y*|x) ≈ 0，KL惩罚项会爆炸，即使奖励模型分配高奖励\n- 塑造后的奖励变得无限负，淹没任何正向信号，形成**学习陷阱**\n\n### 在DPO中：\n- DPO损失包含模型对数比和参考对数比：z = β[log π_θ(y_w|x)/π_θ(y_l|x) + log π_ref(y_l|x)/π_ref(y_w|x)]\n- 如果π_ref(y_w|x) ≈ ε << 1而π_ref(y_l|x)适中，参考偏移会使梯度消失\n- 涉及缺失/稀有模式的样本对获得可忽略的更新，导致**梯度饥饿**\n\n## 6. 提出解决方案：流程反转\n\n基于理论分析，作者提出了**Align → KD**的替代流程：\n1. 首先在高召回率的参考模型上进行对齐\n2. 然后将对齐后的模型蒸馏成紧凑模型\n\n**核心洞见**：这种流程反转将参考模型的召回率从\"实现细节\"提升为\"一阶设计选择\"。\n\n## 7. 实验验证：从理论到实证\n\n作者通过两个阶段的实验验证了假设：\n\n### 第一阶段：混合高斯实验(MoG)\n- 创建可控环境，直接操纵召回率\n- 结果一致显示：Align → KD流程在目标精度、平均奖励和稳定性上显著优于KD → Align\n- 即使增加训练预算或调整压缩强度，也无法解决KD → Align的根本缺陷\n\n### 第二阶段：LLM验证(SmolLM2家族)\n- 在真实语言模型上验证相同现象\n- 结果再次确认：Align → KD流程产生更高奖励、更强目标精度和更稳定训练动态的模型\n- 即使中间的高召回率对齐模型(p'_AK)已经超过最终的低召回率对齐模型(p''_KA)的性能\n\n## 8. 形成最终方法论原则\n\n基于理论和实验验证，作者形成了具有普遍指导意义的方法论原则：\n\n**\"对齐必须在蒸馏之前进行\"(Alignment Must Precede Distillation)**\n\n这一原则确立了参考模型召回率作为对齐过程中的**一阶设计参数**，强调了流程设计直接决定了偏好对齐的可靠性和效率，对研究和部署中对齐语言模型具有重要指导意义。\n\n---\n\n**逻辑链总结**：从实际部署中的效率与对齐平衡问题出发，通过观察常见实践中的隐含假设，发现知识蒸馏导致的召回率缺失问题，提出参考模型召回率的关键作用假设，通过理论分析揭示低召回率陷阱的机制，提出流程反转的解决方案，并通过实验验证形成最终的方法论原则。整个过程体现了从现象到本质、从理论到实践的完整科学推理链条。", "summary_translation": "为提高效率，偏好对齐（preference alignment）通常在紧凑的知识蒸馏（knowledge-distilled, KD）模型上执行。我们认为这种常见做法忽视了对齐参考模型的一个关键特性——其分布召回率（distributional recall），从而引入了重大限制。我们表明，标准的KD→对齐工作流程会降低模型对齐罕见但理想行为的能力，即使在强偏好信号下也是如此。相反，我们证明逆转这一流程（即对齐→KD）是必要的：对齐必须在蒸馏之前首先在高召回率参考模型上执行。\n\n我们的贡献有三方面。首先，我们提供了一个简明的工作解释，说明参考模型如何在基础层面上约束偏好对齐目标。其次，我们在一个可控的高斯混合（Mixture-of-Gaussians）实验中验证了这一理论，实验中低召回率锚定（low-recall anchoring）始终导致模型性能次优。最后，我们证明在SmolLM2系列的大语言模型（LLM）对齐中存在相同现象：在KD后进行对齐的模型无法有效对齐目标行为，导致奖励和目标精度显著降低。相比之下，我们提出的对齐→KD流程能够稳健地对齐这些行为，产生的模型具有更优的目标导向指标和更低的方差。\n\n总之，这些结果确立了参考模型召回率（reference-model recall）作为对齐中的一阶设计选择，提供了一个明确原则：对齐必须先于蒸馏。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#212", "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought", "link": "/arxiv/2509.23365", "arxiv_id": "2509.23365", "authors": "Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian", "summary": "Previous work shows that the chain of continuous thought (continuous CoT) improves the reasoning capability of large language models (LLMs) by enabling implicit parallel thinking, and a subsequent work provided theoretical insight by showing that a two-layer transformer equipped with continuous CoT can efficiently solve directed graph reachability by maintaining a superposition of multiple reasoning traces in the continuous thought. However, it remains unclear how the superposition mechanism is naturally learned from gradient-based training methods. To fill this gap, we theoretically analyze the training dynamics of a simplified two-layer transformer on the directed graph reachability problem to unveil how the superposition mechanism emerges during training in two training stages -- (i) a thought-generation stage that autoregressively expands the continuous thought, and (ii) a prediction stage that converts the thought into the final answer. Our analysis reveals that during training using continuous thought, the index-matching logit, an important quantity which reflects the strength of the model's local search ability, will first increase and then remain bounded under mild assumptions. The bounded index-matching logit effectively balances exploration and exploitation during the reasoning process: the model will exploit local problem structures to identify plausible search traces, and assign comparable weights to multiple such traces to explore when it is uncertain about which solution is correct, which results in superposition. Our experimental results tracking the growth of logits further validate our theory.", "subjects": "Machine Learning", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.503678", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文本质上是研究连续思维链(continuous CoT)如何提高大语言模型的推理能力，属于改进LLM基础能力的研究。论文通过理论分析揭示了超位置机制在训练过程中的形成，这直接关联到LLM的通用推理能力提升。其次，从正面指标看，论文明确涉及大语言模型(LLMs)核心概念，聚焦于推理能力(reasoning)和问题解决(problem-solving)，并研究思维链这一新兴推理范式的训练动态。第三，论文不符合任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。论文使用图可达性问题仅作为测试平台，而非特定领域应用。总体而言，该论文通过理论分析揭示了连续思维链如何增强LLM的推理能力，特别是通过超位置机制实现隐式并行思维，这直接贡献于提升LLM的通用推理能力，完全符合研究目标。", "summary2": "本文旨在揭示连续思维链(continuous CoT)中叠加机制(superposition)的训练动态。针对有向图可达性问题，我们提出了一种理论分析方法，研究两层transformer如何在思维生成和预测两个阶段中自然学习叠加机制。通过分析指数匹配logit的动态变化，我们证明了其在训练过程中保持有界，从而平衡探索与利用，使模型能同时维护多个合理推理轨迹。在ProsQA数据集上的实验验证了理论预测，模型最终达到96.2%的准确率，且注意力logit的增长模式与理论分析一致。", "inspiration_trace": "# 从观察到理论：连续思维链中叠加机制的训练动态分析\n\n## 一、宏观问题：连续思维链的推理优势与机制之谜\n\n**观察现象**：连续思维链(Continuous CoT)作为一种新兴的推理增强方法，在大型语言模型中展现出显著的推理能力提升。与传统的离散思维链不同，连续思维链在连续潜空间中保持推理轨迹，无需投影回离散token空间。\n\n**核心优势**：已有研究表明，连续思维链的一个重要理论优势是能够实现\"叠加\"(superposition)推理——当模型面临多个可能的推理路径且不确定哪一条正确时，它可以并行保持所有合理路径，而非被迫选择单一路径。\n\n**关键问题**：虽然叠加机制的理论优势已被证明，但一个根本性问题尚未解决：这种叠加机制是如何通过标准的基于梯度的训练方法自然学习的？训练动态如何导致这种复杂推理能力的涌现？\n\n## 二、问题聚焦：训练动态中的叠加机制形成\n\n**缩小研究范围**：为深入理解叠加机制的学习过程，作者将研究聚焦于一个具体的理论问题：基于梯度的训练方法是否自然导致叠加结构的形成？能否从理论上证明这一点？\n\n**选择分析对象**：作者选择有向图可达性问题作为研究载体，这是一个能够有效捕捉推理本质的抽象任务。具体任务是判断从给定起始节点到目标节点是否存在路径。\n\n**模型简化**：为便于理论分析，作者使用一个简化的两层transformer架构，重点关注连续思维链的两个关键训练阶段：\n1. 思维生成阶段：模型自回归地扩展连续思维\n2. 预测阶段：模型将生成的思维转换为最终答案\n\n**核心假设**：作者提出关键假设——在连续思维链训练过程中，索引匹配logit(反映模型局部搜索能力的重要量)将保持有界，这种有界性能够平衡探索与利用，从而自然导致叠加机制的出现。\n\n## 三、理论分析：从训练动态到叠加机制\n\n### 1. 思维生成阶段的分析\n\n**关键量定义**：作者定义索引匹配logit为反映模型局部搜索能力的核心量，通过参数μv来量化。\n\n**损失函数对比**：作者设计了两种损失函数进行对比分析：\n- COCONUT-BFS：明确鼓励模型预测任何在Nc+1集合内的节点\n- COCONUT：仅使用单一演示路径进行监督，更贴近实际训练场景\n\n**核心发现**：通过梯度流分析，作者证明了以下关键定理：\n- 在COCONUT-BFS下，μv至少以对数速率增长，导致注意力logit无界\n- 在COCONUT下，如果演示路径的节点度数不是最大值，则μv收敛到有限值μ*，所有注意力logit保持有界\n\n**机制解释**：有界的索引匹配logit实现了探索与利用的平衡：\n- 利用：模型利用图的局部结构识别合理的搜索轨迹\n- 探索：当不确定哪条路径正确时，为多条合理路径分配可比较的权重\n- 这种平衡自然导致了叠加机制的出现\n\n### 2. 预测阶段的分析\n\n**信息整合机制**：作者分析了模型如何利用生成的连续思维在候选节点中做出正确预测。预测依赖于两个关键信号：\n- 残差传递(residual carryover)：将最后思维中探索的节点信息带入答案token\n- 候选提升(candidate lift)：提高两个候选节点的logit\n\n**理论保证**：作者证明，适当的参数相对增长率确保可达候选节点具有最大logit，从而模型能够做出正确预测。\n\n## 四、实验验证：理论预测的实证支持\n\n**实验设计**：作者使用两层transformer在有向图可达性问题上进行训练，跟踪关键指标的变化：\n- 在思维生成阶段，跟踪前沿边与非前沿边之间的注意力logit差异作为μv的代理\n- 在预测阶段，跟踪残差传递和候选提升的代理指标\n\n**实验结果**：\n1. **思维生成阶段**：在COCONUT下，注意力logit差异首先增加然后稳定在有界值，与理论预测一致；而在COCONUT-BFS下，logit差异持续增长，无收敛趋势。\n2. **预测阶段**：残差传递和候选提升的代理指标在训练进入预测阶段后迅速增加并稳定，表明模型学会了正确利用连续思维进行预测。\n3. **长度泛化**：一旦在早期阶段建立叠加机制，模型能够快速重用该机制来进一步扩展搜索边界，即使没有明确训练更长的思维链。\n\n## 五、方法论形成：有界logit平衡探索与利用\n\n基于理论分析和实验验证，作者形成了关于连续思维链中叠加机制形成的方法论：\n\n**核心洞见**：连续思维链训练过程中，索引匹配logit的有界性是叠加机制出现的关键。这种有界性平衡了探索与利用，使模型能够并行保持多个合理的推理轨迹。\n\n**机制解释**：\n1. **有界logit的作用**：有界的索引匹配logit防止模型过度自信地基于局部特征(如节点入度)选择单一路径，避免过早丢弃正确路径。\n2. **探索-利用平衡**：适当的logit值使模型既能利用图的局部结构识别合理路径，又能在不确定时为多条路径分配可比较的权重进行探索。\n3. **叠加的自然涌现**：这种平衡自然导致了叠加机制的出现，使模型能够在连续空间中并行维护多个可能的推理路径。\n\n**理论与实践意义**：这一方法论不仅解释了连续思维链中叠加机制的形成过程，还为理解和改进大型语言模型的推理能力提供了新的理论视角，有助于更高效、可靠地扩展这一有前景的范式。", "summary_translation": "先前的研究表明，连续思维链(chain of continuous thought, continuous CoT)通过实现隐式并行思维提升了大语言模型(large language models, LLMs)的推理能力，后续研究则通过理论分析表明，配备连续思维链的两层transformer能够通过在连续思维中维持多个推理轨迹的叠加态(superposition)来有效解决有向图可达性(directed graph reachability)问题。然而，叠加机制(superposition mechanism)如何通过基于梯度(gradient-based)的训练方法自然习得，目前尚不清楚。为填补这一空白，我们从理论上分析了一个简化的两层transformer在有向图可达性问题上的训练动态(training dynamics)，以揭示叠加机制在两个训练阶段中是如何形成的——(i)自回归地扩展连续思维的思维生成阶段(thought-generation stage)，以及(ii)将思维转化为最终答案的预测阶段(prediction stage)。我们的分析揭示，在使用连续思维的训练过程中，index-matching logit（一个反映模型局部搜索能力(local search ability)强度的重要量度）在温和假设(mild assumptions)下会先增加然后保持有界(bounded)。有界的index-matching logit在推理过程中有效平衡了探索(exploration)和利用(exploitation)：模型会利用局部问题结构来识别合理的搜索轨迹，并在不确定哪个解决方案正确时为多个此类轨迹分配可比较的权重以进行探索，从而形成叠加态(superposition)。我们跟踪logit增长的实验结果进一步验证了我们的理论。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#225", "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning", "link": "/arxiv/2509.23252", "arxiv_id": "2509.23252", "authors": "Raviteja Anantha, Soheil Hor, Teodor Nicola Antoniu, Layne C. Price", "summary": "We present NanoFlux, a novel adversarial framework for generating targeted training data to improve LLM reasoning, where adversarially-generated datasets containing fewer than 200 examples outperform conventional fine-tuning approaches. The framework employs a competitive dynamic between models alternating as Attacker and Defender, supervised by a tool-augmented Judge, synthesizing multi-step questions with explanatory annotations that target specific reasoning capabilities. Fine-tuning a 4B-parameter model on NanoFlux-generated data yields performance gains across diverse domains compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning (GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical reasoning (MultiMedQA), while reducing computational requirements by 3-14x. Ablation studies reveal a non-monotonic relationship between dataset characteristics and model performance, uncovering domain-specific optimal points for question complexity and reasoning quality. NanoFlux automates training data generation through embedding-based novelty filtering, tool-augmented evaluation, and multi-hop reasoning, suggesting that future model improvements may lie in the intelligent synthesis of small, precisely targeted training datasets.", "subjects": "Machine Learning", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.515634", "filter_reason": "根据筛选标准，我对这篇论文进行了详细分析： 第一步核心判断：这篇论文的核心是提出NanoFlux，一个对抗性框架用于生成有针对性的训练数据，以提高LLM的推理能力。论文本质上是关于改进LLM的基础推理能力，提出了一种新的训练范式（对抗性双LLM框架），增强了模型的多步推理能力。这符合\"改进LLM基础能力\"和\"提出新训练范式\"的保留标准。 第二步正面指标：论文包含多个正面指标： - 核心概念：明确针对LLMs - 能力方向：专注于reasoning，特别是mathematical reasoning, scientific reasoning, medical reasoning和multi-step reasoning - 新兴范式：使用了tool-augmented Judge和tool-augmented evaluation，属于工具使用范畴 第三步排除标准：论文虽然提到了在医疗推理(MultiMedQA)上的应用，但这只是作为评估方法效果的基准之一，而非论文的核心焦点。论文的核心是通用的推理能力提升框架，不主要聚焦于特定应用领域，因此不违反排除标准。 第四步特殊和模糊情况：论文中的工具使用(tool-augmented Judge)是为了增强LLM的通用推理能力，属于通用智能体/工具使用方法，而非特定领域应用，因此应该保留。 综上所述，这篇论文的核心贡献是提出了一种通过对抗性框架生成高质量训练数据来增强LLM通用推理能力的新方法，完全符合\"致力于提高大语言模型本身的通用推理能力\"的研究目标。", "summary2": "本文旨在解决大型语言模型面临的\"基准耗尽\"问题，即高质量训练数据有限而模型仍存在推理缺陷的挑战。针对多领域推理任务，我们提出了一种NanoFlux对抗性框架，通过Attacker和Defender双LLM动态交互，由工具增强的Judge监督生成针对性训练数据。在GSMHard、GenomeBench和MultiMedQA三个基准测试上，通过仅200个合成示例微调4B参数模型，实现了性能提升(+5.9%、+3.6%、+16.6%)并降低计算需求3-14倍。", "inspiration_trace": "# NanoFlux方法论逻辑链推演\n\n## 一、宏观问题识别：数据瓶颈与推理困境\n\n作者首先识别了一个基础性问题：随着LLM性能不断提升，我们面临\"基准耗尽\"现象——高质量训练数据有限，而模型仍表现出关键推理失败。这引发了一个根本性问题：**如何在现有数据约束之外推进AI推理能力？**\n\n## 二、观察与问题聚焦\n\n### 2.1 现有方法局限性观察\n作者观察到：\n- 合成数据生成是潜在解决方案，但 naive 方法产生低信息样本\n- 有效数据合成通常依赖人类专业知识，与自动化目标相悖\n- 简单扩展数据量无法解决模型推理的根本缺陷\n\n### 2.2 相关工作启发\n从LIMO、R-Zero、Crescent等研究中，作者提炼出关键洞见：\n- **数据质量优于数量**：少量精选示例可解锁强大推理能力\n- **失败案例价值**：针对模型弱点的训练比无差别扩展更有效\n- **自我改进潜力**：模型可生成自己的训练数据并从中学习\n\n## 三、核心假设形成\n\n基于以上观察，作者提出三个核心假设：\n1. **小型高效假设**：极少量(<200)高质量对抗性样本可超越大规模数据集训练效果\n2. **自动化生成假设**：可通过完全自动化框架生成此类数据，无需人工筛选\n3. **领域通用假设**：这种框架应跨多个推理领域通用，无需领域特定工程\n\n## 四、方法论设计：从假设到实现\n\n### 4.1 对抗性架构设计\n作者设计了三角色对抗框架：\n- **攻击者(Attacker)**：通过\"概念缝合\"生成边界问题\n- **防御者(Defender)**：尝试解决攻击者生成的问题\n- **裁判(Judge)**：工具增强评估，确保问题质量和答案准确性\n\n### 4.2 关键创新机制\n为解决核心问题，作者引入三个关键机制：\n1. **目标对抗生成**：在模型能力边界合成问题，创造高信息训练信号\n2. **自动化质量保证**：裁判模型使用代码执行和网络搜索验证答案\n3. **嵌入新颖性过滤**：确保问题多样性和解决方案创新性\n\n### 4.3 领域适应性设计\n作者针对不同推理领域设计特定适配：\n- 数学推理：Python代码验证\n- 科学推理：XML结构化答案格式\n- 医学推理：领域特定模型和结构化响应\n\n## 五、实验验证与洞察发现\n\n### 5.1 实验设计\n作者在三个领域验证NanoFlux：\n- GSMHard（数学推理）\n- GenomeBench（科学推理）\n- MultiMedQA（医学推理）\n\n### 5.2 关键发现\n实验结果揭示了三个重要洞察：\n1. **效率与性能突破**：200个NanoFlux样本超越全数据集微调，计算需求减少3-14倍\n2. **非单调关系**：数据集特征与模型性能间存在非单调关系，揭示训练数据优化的复杂张力\n3. **领域特定最佳点**：不同领域在问题复杂性和推理质量上有不同最优配置\n\n## 六、方法论形成与理论贡献\n\n### 6.1 从数据数量到数据质量的范式转变\n基于实验结果，作者提出核心方法论转变：\n- **传统范式**：扩展数据量提升模型性能\n- **NanoFlux范式**：优化数据质量，精确针对模型弱点\n\n### 6.2 理论贡献\n作者形成了三个理论贡献：\n1. **对抗性数据生成理论**：模型间的对抗动态可自动识别并针对推理弱点\n2. **最小高效数据集理论**：存在极小数据集可触发显著性能提升\n3. **领域自适应优化理论**：不同推理领域需要不同的数据复杂性平衡点\n\n## 七、逻辑链总结\n\n从**数据瓶颈**这一宏观问题出发，作者通过观察现有方法局限性，从相关工作中获得启发，形成了**小型高效对抗性数据集**的核心假设。基于此假设，设计了**三角色对抗框架**，并通过实验验证了其有效性。最终，作者形成了**从数据数量扩展转向数据质量优化**的方法论，为解决LLM推理能力提升提供了新思路。\n\n这一逻辑链展示了作者如何从宏观问题出发，逐步聚焦，通过观察、假设、实验验证和洞察分析，最终形成NanoFlux这一创新方法论的完整思考过程。", "summary_translation": "我们提出了NanoFlux，一种新颖的对抗性框架（adversarial framework），用于生成有针对性的训练数据以提高LLM（大语言模型）推理能力，其中包含不到200个例子的对抗生成数据集表现优于传统的微调（fine-tuning）方法。该框架采用模型之间作为攻击者（Attacker）和防御者（Defender）交替进行的竞争动态，由工具增强的评判者（tool-augmented Judge）监督，合成带有解释性注释的多步骤问题，针对特定的推理能力。在NanoFlux生成的数据上对40亿参数模型进行微调，与完整基准（full-benchmark）微调相比，在多个领域实现了性能提升：数学推理（GSMHard）上提升5.9%，科学推理（GenomeBench）上提升3.6%，医疗推理（MultiMedQA）上提升16.6%，同时将计算需求降低了3-14倍。消融研究（Ablation studies）揭示了数据集特征与模型性能之间的非单调关系（non-monotonic relationship），发现了问题复杂性和推理质量的领域特定最优点。NanoFlux通过基于嵌入的新颖性过滤（embedding-based novelty filtering）、工具增强评估（tool-augmented evaluation）和多跳推理（multi-hop reasoning）实现训练数据生成的自动化，表明未来的模型改进可能在于小型、精确目标的训练数据集的智能合成。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#232", "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning", "link": "/arxiv/2509.23209", "arxiv_id": "2509.23209", "authors": "Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen", "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at https://github.com/Bluixe/towards_monotonic_improvement .", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.524323", "filter_reason": "这篇论文的核心贡献是提出了一种名为\"上下文价值知情ICRL\"(CV-ICRL)的新方法，用于解决上下文强化学习中的\"上下文歧义\"问题，从而实现智能体性能的单调改进。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，这篇论文研究的是改进序列模型的基础能力，特别是上下文强化学习(ICRL)能力，这是一种让模型能够从过去经验中学习并适应新任务的通用能力。论文提出的CV-ICRL方法是一种新的训练范式，通过引入\"上下文价值\"概念来增强模型的推理和决策能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、推理等通用能力\"的要求。 其次，从正面指标看，论文涉及了强化学习(RL)这一训练方法，以及问题解决(problem-solving)这一能力方向。虽然论文没有明确提到\"大语言模型\"这一术语，但其研究的\"large sequence models\"和上下文学习能力与大语言模型的核心特性高度相关。 第三，论文不符合任何排除标准。它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。 最后，在特殊和模糊情况处理上，论文研究的是通用的智能体学习框架，旨在增强智能体的推理和决策能力，而不是将智能体应用于特定领域，这符合保留条件。 综上所述，尽管论文没有明确指出其研究对象是大语言模型，但其研究的上下文强化学习能力是大语言模型的核心能力之一，且论文提出的方法旨在增强模型的通用推理能力，因此符合\"致力于提高大语言模型（LLM）本身的『通用推理能力』\"的研究目标。", "summary2": "本文旨在解决In-Context Reinforcement Learning (ICRL)中的性能退化问题，即测试时无法保持训练数据中的单调改进特性。针对ICRL中的Contextual Ambiguity现象，我们提出了一种Context Value Informed ICRL (CV-ICRL)方法，通过引入Context Value作为显式信号来指导策略决策，并在Dark Room和Minigrid环境上通过Average Episode Return和Degradation Frequency等指标验证了其有效性。", "inspiration_trace": "# 从问题观察到方法论形成：CV-ICRL的逻辑演进\n\n## 宏观问题：ICRL中的性能退化现象\n\n**起点问题**：为什么在上下文强化学习(ICRL)中，尽管训练数据展示了单调性能改进，但测试时模型无法保持这种改进特性？\n\n## 关键观察：训练与测试的差距\n\n通过实验分析，作者发现了一个核心矛盾：\n- 现有ICRL方法（如Algorithm Distillation）在训练时使用单调改进的策略数据\n- 但在测试阶段，这些模型经常出现性能退化，无法维持训练数据中表现出的单调改进（如图2所示）\n\n这一观察揭示了ICRL领域的一个基础性挑战：训练与测试之间的性能不一致性。\n\n## 问题分析：上下文歧义(Contextual Ambiguity)\n\n作者深入探究了性能退化的根本原因，提出了\"上下文歧义\"概念：\n\n1. **核心机制**：测试时的单次动作采样违反了需要充分采样以平均随机性的隐含假设\n2. **恶性循环**：单个随机的不良动作生成误导性上下文，使其看起来像是来自次优策略的历史，导致模型错误识别自身技能水平\n3. **注意力机制影响**：通过Transformer注意力热图分析，发现模型更关注近期信息，加剧了上下文歧义的影响（如图7所示）\n\n这一分析将问题从表面现象深入到了机制层面，为解决方案提供了理论基础。\n\n## 假设形成：引入明确的质量信号\n\n基于对问题的深入理解，作者形成了核心假设：\n\n> 如果能够为智能体提供一个明确的、无歧义的上下文质量标签，智能体就可以避免从噪声历史交互中进行危险推断，从而打破性能退化的循环。\n\n这一假设引导作者思考如何量化上下文的质量，并确保其具有单调性。\n\n## 理论构建：上下文价值(Context Value)\n\n作者引入了\"上下文价值\"(VC)的理论概念：\n\n1. **定义**：VC = J(π*_C)，表示给定上下文C中信息，策略理论上可实现的最优性能\n2. **关键性质**：VC具有单调性，当上下文扩展时，VC' ≥ VC（性质1）\n3. **理论保证**：引入VC可以收紧学习策略与上下文最优策略之间的性能界限（定理1）\n\n这一理论构建为解决上下文歧义提供了数学基础，将假设转化为可操作的理论框架。\n\n## 方法设计：CV-ICRL算法\n\n基于理论构建，作者设计了Context Value Informed ICRL (CV-ICRL)方法：\n\n1. **核心思想**：将Context Value作为明确信号整合到ICRL过程中\n2. **实践实现**：提出了两种Context Value估计方法：\n   - **CV-ICRL-ϕ(C)**：通过上下文C估计VC，作为Transformer模型的辅助输出头\n   - **CV-ICRL-ϕ(t)**：通过时间步t估计VC，与单调递增变量关联\n\n这一设计将理论转化为可实现的算法，解决了实际应用中的挑战。\n\n## 实验验证：效果评估\n\n作者通过全面的实验验证了方法的有效性：\n\n1. **主要结果**：在Dark Room和Minigrid环境中，CV-ICRL显著降低了性能退化频率，提高了整体性能（表1，图3-4）\n2. **消融实验**：验证了Context Value在上下文中的有效性以及ϕ(t)函数的必要性（图6）\n3. **泛化能力**：首次证明AD-like ICRL方法在Minigrid不同任务类型间的强泛化能力（图5）\n\n实验结果不仅验证了方法的有效性，还揭示了ICRL领域的新可能性。\n\n## 逻辑链条总结\n\n从宏观问题到具体方法，作者的思考路径形成了清晰的逻辑链条：\n\n```\n问题观察(性能退化) → 问题分析(上下文歧义) → 假设形成(明确质量信号) → \n理论构建(上下文价值) → 方法设计(CV-ICRL) → 实验验证(效果评估)\n```\n\n这一逻辑演进不仅解决了ICRL中的具体问题，还为上下文学习领域提供了新的理论视角和方法论框架。通过将抽象问题转化为可操作的概念和算法，作者成功地在理论与实践之间架起了桥梁。", "summary_translation": "# 情境强化学习（In-Context Reinforcement Learning, ICRL）的中文翻译\n\n情境强化学习（In-Context Reinforcement Learning, ICRL）已成为一种有前景的范式，用于开发能够通过将过去经验作为情境来快速适应新任务的智能体，而无需更新其参数。最近的方法通过在线强化学习（online RL）中的单调策略改进数据来训练大型序列模型，旨在实现持续改进的测试时性能。然而，我们的实验分析揭示了一个关键缺陷：这些模型在测试期间无法像训练数据那样显示出持续改进。\n\n理论上，我们将这种现象识别为情境模糊性（Contextual Ambiguity），即模型自身的随机动作可能产生一种交互历史，这种历史会误导性地类似于训练数据中的次优策略，从而引发不良动作选择的恶性循环。为解决情境模糊性，我们在训练阶段引入情境价值（Context Value），并提出情境价值知情情境强化学习（Context Value Informed ICRL, CV-ICRL）。CV-ICRL使用情境价值作为显式信号，表示在给定当前情境下策略理论上可达到的理想性能。随着情境的扩展，情境价值可以包含更多任务相关信息，因此理想性能应该是非递减的。我们证明情境价值收紧了相对于理想单调改进策略的性能差距的下界。我们进一步提出了两种在训练和测试时估计情境价值的方法。\n\n在Dark Room和Minigrid测试平台上进行的实验表明，CV-ICRL有效减轻了性能下降，并提高了在各种任务和环境中的整体ICRL能力。本文的源代码和数据可在https://github.com/Bluixe/towards_monotonic_improvement获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#235", "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse", "link": "/arxiv/2509.23183", "arxiv_id": "2509.23183", "authors": "Guohao Chen, Shuaicheng Niu, Deyu Chen, Jiahao Yang, Zitian Zhang, Mingkui Tan, Pengcheng Wu, Zhiqi Shen", "summary": "Test-time entropy minimization helps adapt a model to novel environments and incentivize its reasoning capability, unleashing the model's potential during inference by allowing it to evolve and improve in real-time using its own predictions, achieving promising performance. However, pure entropy minimization can favor non-generalizable shortcuts, such as inflating the logit norm and driving all predictions to a dominant class to reduce entropy, risking collapsed solutions (e.g., constant one-hot outputs) that trivially minimize the objective without meaningful learning. In this paper, we introduce ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time entropy minimization. ZeroSiam prevents collapse through asymmetric divergence alignment, which is efficiently achieved by a learnable predictor and a stop-gradient operator before the classifier. We provide empirical and theoretical evidence that ZeroSiam not only prevents collapse solutions, but also absorbs and regularizes biased learning signals, enhancing performance even when no collapse occurs. Despite its simplicity, extensive results show that ZeroSiam performs more stably over prior methods using negligible overhead, demonstrating efficacy on both vision adaptation and large language model reasoning tasks across challenging test scenarios and diverse models, including tiny models that are particularly collapse-prone.", "subjects": "Machine Learning, Networking and Internet Architecture", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.525922", "filter_reason": "这篇论文的核心贡献是提出ZeroSiam，一种用于测试时熵优化的非对称Siamese架构。根据筛选标准，我判断该论文符合研究目标，原因如下： 首先，从本质上看，论文聚焦于测试时熵优化技术，这是一种在推理阶段增强模型能力的方法。摘要明确指出该方法旨在\"释放模型在推理过程中的潜力，允许它实时进化和改进\"，这直接关联到提高LLM的通用推理能力，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确提到\"large language model reasoning tasks\"；能力方向上关注\"reasoning capability\"；训练方法上涉及\"evolve and improve in real-time\"的自我进化概念。 第三，虽然论文提到视觉适应任务，但这不是其主要焦点，而是作为方法的一个应用场景。论文的核心是提出一种通用的测试时优化方法，同时应用于视觉适应和LLM推理任务，因此不应因涉及视觉而被排除。 最后，论文提出的防止崩溃的方法有助于提高模型的推理质量和可靠性，这与提升LLM通用推理能力的目标一致。论文关注的是模型在推理时的能力提升，而非特定应用或基础设施优化。 综上所述，该论文致力于通过测试时熵优化技术提高大语言模型的通用推理能力，符合研究目标。", "summary2": "本文旨在解决测试时熵最小化中的模型崩溃问题。针对测试时适应和大型语言模型推理任务，我们提出了一种名为ZeroSiam的高效非对称孪生网络架构，通过可学习预测器和停止梯度操作实现对齐，防止模型崩溃。在ImageNet-C和多个数学推理基准上通过准确率等指标验证了其有效性，实现了更稳定的测试时熵优化。", "inspiration_trace": "# 从宏观问题到创新方法：ZeroSiam的逻辑演进\n\n## 1. 宏观问题：如何让模型在测试时适应新环境？\n\n**观察**：在实际应用中，测试数据往往与训练数据分布不同（分布偏移）。模型需要能够适应这些新环境以提高性能，但测试时通常没有真实标签可用。\n\n**聚焦**：如何设计一种无监督的测试时适应机制，让模型能够利用测试数据本身进行自我调整？\n\n## 2. 熵最小化：有前景的适应策略\n\n**假设**：熵作为预测不确定性的度量，最小化预测熵可以鼓励模型对测试数据做出更自信的预测，从而适应新环境。\n\n**初步验证**：前期工作（如Tent）确实证明了测试时熵最小化的有效性——模型仅通过最小化自身预测熵就能适应分布偏移。\n\n## 3. 发现关键问题：熵最小化导致的崩溃现象\n\n**深入观察**：纯粹的熵最小化会导致模型崩溃（collapse），表现为：\n- 模型增大logit范数来减少熵\n- 将所有预测推向一个主导类别\n- 最终退化为恒定的one-hot输出\n\n**问题本质**：熵最小化自然地驱动模型增加最大预测logit，而不考虑它是否与真实标签一致。在测试数据嘈杂或有分布偏移的情况下，这种机制会导致模型找到\"捷径解\"——虽然最小化了熵，但没有实现有意义的学习。\n\n## 4. 现有方法的局限性分析\n\n**评估当前解决方案**：现有方法（如SAR、EATA、DeYO）主要依赖启发式阈值来过滤不可靠的梯度。\n\n**发现局限**：\n- 阈值难以定义且在不同领域间难以泛化\n- 只能部分过滤噪声梯度，无法从根本上解决问题\n- 模型仍然面临崩溃风险，特别是在长期或更具挑战性的测试场景中\n\n**核心挑战**：需要一种更根本的解决方案，从架构设计上防止崩溃，而不仅仅是缓解症状。\n\n## 5. 跨领域灵感：自监督学习中的非对称设计\n\n**关键洞察**：自监督学习（SSL）中的负样本无关方法（如BYOL、SimSiam）通过非对称结构防止两个分支崩溃到相同的常数。\n\n**原理分析**：这些方法使用一个非对称设计（一个分支有预测器，另一个分支有stop-gradient），确保退化常数输出会产生非零对齐损失，从而防止两个分支崩溃到相同的常数。\n\n**创新假设**：这种非对称机制可能也适用于熵优化，防止不需要的平凡解，稳定自训练过程。\n\n## 6. 核心挑战：将非对称设计应用于熵最小化\n\n**识别障碍**：直接将SSL中的非对称设计应用到测试时熵最小化面临三大挑战：\n1. 测试时熵最小化通常只有一个预测分支，优化的是熵而不是相似性\n2. 传统孪生设计需要额外的骨干网络传递，影响测试时学习的效率\n3. 测试时学习需要轻量级设计，适合在线部署\n\n**突破思路**：如何在单次前向传播中嵌入非对称性，而不需要额外的骨干网络传递或数据增强？\n\n## 7. 解决方案：ZeroSiam的高效非对称孪生架构\n\n**核心创新**：在分类器之前插入一个预测器和stop-gradient操作符，将一个预测解耦为两个基于相同特征的非对称输出：\n- 在线分支（online branch）：通过可学习的预测器，用于优化熵\n- 目标分支（target branch）：原始logits，使用stop-gradient进行对齐\n\n**目标函数**：结合在线分支的熵最小化和与目标分支的对齐正则化：\n```\nL = H(po) + α D(po ∥ sg[pr])\n```\n其中H(p)是预测熵，D(·∥·)是散度（如ℓ2或KL），sg[·]表示stop-gradient。\n\n**工作原理**：\n1. 预测器初始化为身份映射，确保热启动\n2. 在线分支最小化熵以学习判别特征\n3. 目标分支通过stop-gradient进行非对称预测器-目标对齐，防止崩溃的常数解\n4. 预测器快速偏离身份映射，创建防止崩溃所需的非对称性\n\n## 8. 理论与实证验证\n\n**理论分析**：证明预测器h作为过滤机制，抑制对应于过度放大的logits的梯度更新方向，系统收敛到稳定平衡：存在hmin > 0使得H(po) > hmin且po → pr。\n\n**实证观察**：\n1. **预测器作为有偏学习信号的有效吸收器**：预测器参数θh在面对更高标签不平衡比率的数据流时更快速、更显著地偏离身份映射，主动吸收有偏学习信号。\n2. **非对称性抑制学习非泛化崩溃模式**：ZeroSiam抑制logit L2范数的快速增长和主导模式的出现，而Tent则受这些问题困扰。\n\n**关键发现**：ZeroSiam不仅能防止崩溃，还能吸收和调节有偏的学习信号，即使在没有崩溃的情况下也能提高性能。\n\n## 9. 效率优势与应用验证\n\n**效率优势**：ZeroSiam只需要一个单次前向传播，不需要数据增强、额外的骨干网络传递或教师模型，计算开销与原始Tent方法相当。\n\n**广泛验证**：在视觉适应（ImageNet-C）和大语言模型推理任务（Math-500、CollegeMath等）上的实验证明，ZeroSiam在各种挑战性测试场景和多样化模型（包括特别容易崩溃的小模型）上表现更稳定。\n\n**鲁棒性测试**：即使在极端情况下（如仅在错误标签上适应、纯噪声数据适应），ZeroSiam仍能保持稳定性能，而现有方法往往会崩溃。\n\n## 10. 逻辑演进总结\n\n从\"如何让模型在测试时适应新环境\"这一宏观问题出发，通过聚焦熵最小化策略，发现崩溃现象这一关键挑战，分析现有方法的局限性，从自监督学习中汲取非对称设计的灵感，克服将非对称设计应用于熵最小化的技术障碍，最终提出ZeroSiam这一高效非对称孪生架构，并通过理论与实证验证其有效性。整个过程体现了从观察、假设到形成最终方法论的清晰思考路径，解决了测试时熵最小化的根本性崩溃问题。", "summary_translation": "测试时熵最小化（Test-time entropy minimization）有助于使模型适应新环境并激励其推理能力，通过允许模型使用自身预测进行实时演化和改进，释放了模型在推理过程中的潜力，取得了有前景的性能。然而，纯熵最小化可能倾向于不可泛化的捷径，例如膨胀logit范数（logit norm）并将所有预测驱动到主导类别以减少熵，这可能导致坍塌解（collapsed solutions）（例如，恒定的one-hot输出），这些解在没有有意义学习的情况下平凡地最小化了目标。在本文中，我们提出了ZeroSiam，一种专为测试时熵最小化定制的高效非对称孪生网络架构（asymmetric Siamese architecture）。ZeroSiam通过非对称散度对齐（asymmetric divergence alignment）防止坍塌，这种对齐通过分类器前的可学习预测器（learnable predictor）和停止梯度操作符（stop-gradient operator）高效实现。我们提供了经验和理论证据，证明ZeroSiam不仅能防止坍塌解，还能吸收和正则化有偏学习信号，即使在未发生坍塌的情况下也能提高性能。尽管ZeroSiam设计简单，但大量结果表明，相比先前方法，它在使用可忽略开销的情况下表现更稳定，在视觉适应（vision adaptation）和大型语言模型（large language model）推理任务上都展现了有效性，这些任务涵盖了具有挑战性的测试场景和多样化模型，包括特别容易坍塌的小型模型。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#241", "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers", "link": "/arxiv/2509.23152", "arxiv_id": "2509.23152", "authors": "Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang", "summary": "Test-time scaling via solution sampling and aggregation has become a key paradigm for improving the reasoning performance of Large Language Models (LLMs). While reward model selection is commonly employed in this approach, it often fails to identify minority-yet-correct answers, which limits its effectiveness beyond that of simple majority voting. We argue that this limitation stems from a lack of informative critique signals during verifier training. To bridge this gap, we introduce Mirror-Critique, a framework that trains a verifier with informative critiques. Our key insight is to leverage the rich critique signal by contrasting model-generated solutions with ground-truth solutions. We deploy a small instruction-tuned model to synthesize high-quality critique data with rejection sampling that teaches the verifier not only what is wrong, but also why. The synthetic data is used to cold-start the LLMs in the RLVR process to further improve the verification ability. The resulting Mirror-Verifier is deployed to evaluate candidate solutions by generating multiple critiques per solution, aggregating them into a verify score used for weighted voting or selective abstention. The experimental results show that our Mirror-Verifier significantly outperforms majority voting in terms of solution accuracy and also improves the solver's honesty to recognize and abstain from answering beyond its capability boundaries.", "subjects": "Machine Learning", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.534189", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围，具体分析如下： 第一步：核心判断 这篇论文的本质是提出Mirror-Critique框架，通过训练带有信息性批判的验证器来提高大语言模型的推理性能。论文的核心贡献是改进LLM的基础推理能力，提出新的训练范式（使用RLVR过程），并增强模型的多步推理和自我评估能力。这完全符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑推理能力\"的保留标准。 第二步：正面指标 论文包含多个正面指标： - 核心概念：明确讨论Large Language Models (LLMs) - 能力方向：专注于reasoning能力的提升，特别是通过验证器增强模型的逻辑推理 - 训练方法：使用了强化学习(RLVR过程)来训练验证器 - 论文虽未明确提及新兴范式，但其验证机制可视为一种增强LLM通用能力的工具使用方法 第三步：排除标准 论文不涉及任何排除标准中的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域（如医疗、化学等） - 虽然提到\"honesty\"，但这是指模型识别自身能力边界的内在可靠性，而非应用层面的水印、安全等问题 第四步：特殊和模糊情况处理 论文提出的验证器可以视为一种通用工具，用于增强LLM的推理能力，而非针对特定领域的应用。论文关注的是提高模型的推理准确性和诚实性，这与减少幻觉和提高模型内在可靠性相关，属于提升模型通用推理能力的范畴。 综上所述，这篇论文明确致力于提高大语言模型本身的通用推理能力，通过创新的验证器训练方法和批判信号利用，增强了模型的逻辑推理和问题解决能力，完全符合研究课题的目标。", "summary2": "本文旨在解决测试时扩展中验证器无法识别少数但正确答案的问题。针对LLMs生成的解决方案，我们提出了一种Mirror-Critique框架，通过对比模型生成与真实解决方案合成高质量批判数据训练验证器，并在多个数学推理benchmark上通过准确率和诚实度分数验证了其有效性。", "inspiration_trace": "# Mirror-Critique方法论逻辑推演\n\n## 一、宏观问题观察\n\n**起点现象**：测试时扩展(test-time scaling)通过解决方案采样和聚合已成为提高LLMs推理性能的关键范式。然而，现有的奖励模型选择方法往往无法识别少数但正确的答案，限制了其有效性，无法超越简单多数投票的性能。\n\n**核心问题**：为什么现有验证方法无法有效识别少数但正确的解决方案？这限制了测试时扩展的潜力。\n\n## 二、问题深入分析\n\n**根本原因识别**：作者指出这一限制源于验证器训练过程中缺乏信息性的批判信号(critique signals)。传统验证器训练通常仅依赖二元标签(正确/错误)，无法提供关于解决方案为何成功或失败的充分解释。\n\n**现有方法局限**：\n1. 二元标签无法解释错误原因\n2. 高质量批判数据需从封闭源模型获取，成本高昂\n3. RLVR(可验证奖励的强化学习)在训练验证器方面的潜力尚未充分探索\n\n## 三、核心假设形成\n\n**关键洞察**：如果能够提供丰富的批判信号，教导验证器不仅判断正确性，还能理解背后的推理差距，那么验证器的性能将显著提高。\n\n**核心假设**：通过对比模型生成的解决方案与真实解决方案，可以合成高质量的批判数据，这些数据能够教导验证器理解错误原因，而不仅仅是标记错误。\n\n## 四、方法论设计\n\n### 1. 数据合成策略\n\n**创新思路**：设计低成本、自包含的批判数据合成管道，不依赖封闭源模型。\n\n**具体实现**：\n- 使用RLVR训练基础求解器，记录解决方案轨迹\n- 过滤冗余轨迹，保留有价值数据\n- 设计特定模板指导小型指令模型生成批判\n- 应用拒绝采样(rejection sampling)确保批判质量\n\n### 2. 验证器训练方法\n\n**冷启动问题解决**：认识到基础模型缺乏批判能力，直接进行强化学习效果不佳，因此先使用合成数据进行监督微调(SFT)作为冷启动。\n\n**数据平衡处理**：发现合成数据存在类别不平衡(负样本过多)，导致奖励 hacking问题，因此采用平衡采样策略，特别关注少数正确样本。\n\n**RLVR优化**：使用平衡数据集进行强化学习，进一步优化验证器能力。\n\n### 3. 测试时应用机制\n\n**多批判生成**：为每个候选解决方案生成多个独立批判，提高评估可靠性。\n\n**验证分数计算**：将多个批判聚合为验证分数，反映解决方案的可信度。\n\n**双重决策机制**：\n- 加权投票：基于验证分数选择最终答案，提高准确性\n- 选择性弃权：当所选答案的平均验证分数低于阈值时弃权，增强诚实性\n\n## 五、实验验证与效果确认\n\n**评估指标设计**：\n- 准确性：衡量模型生成正确答案的能力\n- 诚实性分数：新提出的指标，同时考虑正确性和提供错误信息的危害\n\n**实验结果验证**：\n- Mirror-Verifier在多个数学推理基准测试上显著优于基线方法\n- 能够有效识别少数但正确的解决方案\n- 在超出能力范围时能够准确弃权，提高系统诚实性\n\n## 六、贡献总结\n\n**方法论贡献**：提出了Mirror-Critique框架，通过合成丰富的批判数据训练验证器，解决了传统验证器训练中信息不足的问题。\n\n**性能提升**：在测试时扩展中实现显著准确性提升，同时提高模型诚实性，使其能够识别自身能力边界并适当弃权。\n\n**技术价值**：提供了一种自包含、低成本的验证器训练方法，不依赖封闭源模型，具有广泛适用性。\n\n这一逻辑链条展示了作者从观察测试时扩展的局限性，到分析问题根源，形成核心假设，设计创新解决方案，并通过实验验证其有效性的完整思考过程。", "summary_translation": "通过解决方案采样和聚合实现的测试时扩展（Test-time scaling）已成为提高大型语言模型（Large Language Models, LLMs）推理性能的关键范式。虽然奖励模型选择（reward model selection）在此方法中通常被采用，但它往往无法识别少数但正确的答案，这限制了其效果，使其仅优于简单多数投票（simple majority voting）。我们认为，这一限制源于验证器（verifier）训练过程中缺乏信息丰富的批判信号（informative critique signals）。为了弥合这一差距，我们引入了Mirror-Critique（镜像批判），这是一个通过信息丰富的批判来训练验证器的框架。我们的关键见解是通过对比模型生成的解决方案和真实解决方案（ground-truth solutions）来利用丰富的批判信号。我们部署了一个经过指令微调的小型模型（instruction-tuned model），通过拒绝采样（rejection sampling）合成高质量的批判数据，这些数据不仅教导验证器什么是错误的，还教导它为什么错误。这些合成数据被用于在RLVR过程中冷启动（cold-start）大型语言模型，以进一步提高验证能力。由此产生的Mirror-Verifier（镜像验证器）被用于评估候选解决方案，通过为每个解决方案生成多个批判，并将它们聚合成一个验证分数（verify score），该分数用于加权投票（weighted voting）或选择性弃权（selective abstention）。实验结果表明，我们的Mirror-Verifier在解决方案准确性方面显著优于多数投票，并且提高了解决器（solver）的诚实性，使其能够识别并避免回答超出其能力范围的问题。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#291", "title": "On the Capacity of Self-Attention", "link": "/arxiv/2509.22840", "arxiv_id": "2509.22840", "authors": "Micah Adler", "summary": "While self-attention is known to learn relations among tokens, we lack a formal understanding of its capacity: how many distinct relations can a single layer reliably recover for a given budget? To formalize this, we introduce Relational Graph Recognition (RGR), where the key-query channel represents a graph on $m$ items with $m'$ directed edges, and, given a context of items, must recover the neighbors of each item. We measure resources by the total key dimension $D_K = h\\,d_k$. Within this framework, we analytically derive a capacity scaling law and validate it empirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary (information-theoretic lower bound) and sufficient (explicit construction) in a broad class of graphs to recover $m'$ relations. This scaling law directly leads to a new, capacity-based rationale for multi-head attention that applies even when each item only attends to a single target. When embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a permutation, a single head suffices. However, compression ($m > d_{\\text{model}}$) forces relations into overlapping subspaces, creating interference that a single large head cannot disentangle. Our analysis shows that allocating a fixed $D_K$ across many small heads mitigates this interference, increasing the number of recoverable relations. Controlled single-layer experiments mirror the theory, revealing a sharp performance threshold that matches the predicted capacity scaling and confirms the benefit of distributing $D_K$ across multiple heads. Altogether, these results provide a concrete scaling law for self-attention capacity and a principled design rule for allocating key-query budget across heads.", "subjects": "Machine Learning", "date": "2025-09-26", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.585419", "filter_reason": "这篇论文的核心是研究自注意力机制（大语言模型的核心组件）的基础能力和理论限制，提出了关系图识别(RGR)框架，并推导出自注意力机制的能力扩展定律。虽然论文没有直接讨论LLM的推理能力，但它深入研究了LLM的核心组件（自注意力机制）的能力限制，这对于理解和提高LLM的通用推理能力具有重要意义。论文不属于任何排除标准，它不涉及多模态与视觉、特定应用领域或模型可靠性的应用层面研究。相反，它关注的是模型的基础能力和理论理解，这符合\"改进LLM的基础能力\"的研究目标。通过更好地理解自注意力机制的能力限制，我们可以设计更有效的模型架构，从而提高LLM的通用推理能力。论文还提供了多头注意力的新理论基础，解释了为什么分配固定维度到多个小头比单个大头更有优势，这对于优化LLM架构以提高其推理能力具有直接指导意义。", "summary2": "本文旨在 [解决自注意力机制的容量问题，即对于给定预算，单层自注意力能够可靠学习多少不同关系]。针对 [关系图识别场景]，我们提出了一种 [关系图识别框架和容量缩放定律]，并在 [理想化的单层自注意力模型] 上通过 [准确率指标] 验证了其有效性。", "inspiration_trace": "# 自注意力容量分析的逻辑演进\n\n## 1. 宏观问题：自注意力的能力边界\n\n**观察**：自注意力机制已成为现代深度学习的核心组件，在NLP、CV等领域取得革命性成功。然而，我们缺乏对其基本能力的形式化理解——特别是，一个固定大小的自注意力机制到底能处理多少关系？\n\n**核心问题**：对于一个给定的自注意力机制，单层能够表示并可靠恢复多少目标关系？这被称为该层的\"容量\"(capacity)。\n\n## 2. 问题聚焦：形式化\"关系容量\"\n\n**观察**：直接探测大型训练模型中的容量是困难的，因为现代transformer在共享子空间中叠加了多种关系，且注意力权重不一定与因果重要性一致。\n\n**假设**：如果能创建一个可控框架，使我们能够明确控制关系的结构和数量，同时保持自注意力的计算约束和对称性，就可以对容量进行原则性分析。\n\n**方法**：提出\"关系图识别\"(Relational Graph Recognition, RGR)框架\n- 将自注意力建模为图恢复问题：在m个项目上恢复m'条有向边\n- 键-查询通道表示图结构，给定项目上下文，必须恢复每个项目的邻居\n- 资源测量通过总键维度DK = h dk（头数×每头键维度）\n\n## 3. 理论分析：容量的上下界\n\n### 信息论下界\n\n**假设**：自注意力机制恢复关系的能力受信息论限制，容量应随关系数量m'和项目数量m增长，但随嵌入维度dmodel增加而减少。\n\n**推导**：证明恢复m'条边的图需要\n```\nDK = Ω((m'/dmodel) log(m²/m'))\n```\n这表明键-查询矩阵需要表达足够的信息来描述底层关系图。\n\n### 构造性上界\n\n**假设**：可以通过显式构造注意力权重来解决RGR问题，达到接近信息论下界的性能。\n\n**构造**：提出四种构造算法，从简单到复杂：\n1. 置换图与one-hot嵌入（热身）\n2. 压缩嵌入下的置换图\n3. 更一般的嵌入\n4. 一般图\n\n**结果**：实现\n```\nDK = O((m'+Δ) log m'/dmodel)\n```\n其中Δ是图的最大度数。在温和条件下(Δ/davg ≤ m/dmodel)，这闭合了上下界之间的差距。\n\n## 4. 多头注意力的新解释\n\n**观察**：传统观点认为多头注意力允许一个源概念关注多个不同目标。但实验发现，即使每个源只关注单个目标，多头注意力也有优势。\n\n**假设**：当使用压缩嵌入(dmodel ≪ m)时，许多关系必须存储在重叠的子空间中；将自注意力预算分配到多个小头可以减少干扰，增加可清晰分离的关系数量。\n\n**分析**：\n- 无压缩情况下(dmodel = m)，单头足够\n- 压缩设置下，单头产生太多噪声\n- 多头通过减少块大小B来局部化解嵌入噪声，使每个头只在较少坐标上聚合泄漏\n\n**核心机制**：噪声项N3(B) ≍ B/dmodel √(dk log m)，因此多头通过减小B来控制噪声。\n\n## 5. 实验验证\n\n**假设**：理论预测的容量缩放定律应在实验中得到验证，且多头优势应在置换图中明显。\n\n**实验设计**：\n- 在理想化自注意力设置中实验，反映理论模型\n- 任务：置换图（每个节点一个出/入边）\n- 节点嵌入：高斯单位范数向量，冻结使DK成为唯一容量旋钮\n\n**关键结果**：\n1. **尖锐性能阈值**：F1在狭窄DK窗口中急剧转变\n2. **容量缩放定律**：D*K ≈ 1.19 · m log m/dmodel (R² = 0.944)\n3. **多头优势**：最优头数h* ≈ 1.65 · m/dmodel - 6.64 (R² = 0.824)\n4. **上下文长度不变性**：容量阈值在ℓ ∈ {16, 32}范围内基本稳定\n\n## 6. 限制与未来方向\n\n**识别限制**：\n- 构造性上界仅在温和度偏斜假设下紧密\n- 充分性结果依赖于嵌入的几何属性\n- 实验仅限于置换图，未测试更大出度图\n\n**未来方向**：\n- 量化学习模型中的嵌入如何满足理论假设\n- 扩展实验到更一般的图类\n- 研究更长上下文长度的影响\n- 探索多层数字注意力中的容量累积\n\n## 总结\n\n本文从\"自注意力机制的能力边界是什么\"这一宏观问题出发，通过RGR框架将问题形式化为图恢复任务，建立了自注意力容量的缩放定律，并揭示了多头注意力即使在单目标情况下也能通过减少干扰提高容量的新机制。理论分析与实验验证紧密结合，形成了一套完整的自注意力容量理论，为理解和优化自注意力机制提供了新的理论基础。", "summary_translation": "尽管自注意力机制(self-attention)被公认为能够学习标记(tokens)之间的关系，但我们对其容量(capacity)缺乏正式理解：在给定预算(budget)的情况下，单个层(layer)能够可靠地恢复多少不同的关系？为形式化这一问题，我们引入了关系图识别(Relational Graph Recognition, RGR)框架，其中键-查询通道(key-query channel)表示一个包含$m$个项(items)和$m'$条有向边(directed edges)的图，并且在给定项(items)上下文(context)的情况下，必须恢复每个项的邻居。我们通过总键维度(total key dimension) $D_K = h\\,d_k$来衡量资源(resources)。在此框架内，我们解析推导(analytically derive)了一个容量扩展定律(capacity scaling law)并对其进行了实证验证(validate empirically)。我们证明，在广泛的图类别(broad class of graphs)中，$D_K = \\Theta(m' \\log m' / d_{\\text{model}})$对于恢复$m'$个关系既是必要的(信息论下界，information-theoretic lower bound)，也是充分的(显式构造，explicit construction)。该扩展定律(scaling law)直接导出了多头注意力(multi-head attention)的一种新的、基于容量的基本原理(capacity-based rationale)，即使当每个项(item)仅关注(attends to)单个目标(target)时也适用。当嵌入(embeddings)未压缩(uncompressed)($m = d_{\\text{model}}$)且图是排列(permutation)时，单头(single head)就足够了。然而，压缩(compression)($m > d_{\\text{model}}$)迫使关系进入重叠子空间(overlapping subspaces)，产生干扰(interference)，而单个大头(single large head)无法分离(disentangle)这种干扰。我们的分析表明，将固定的$D_K$分配到多个小头(small heads)可以减轻(mitigates)这种干扰，从而增加可恢复关系(recoverable relations)的数量。受控的单层实验(Controlled single-layer experiments)印证了理论，揭示了一个明显的性能阈值(sharp performance threshold)，该阈值与预测的容量扩展(predicted capacity scaling)相匹配，并证实了将$D_K$分配到多个头(distributing $D_K$ across multiple heads)的好处。总体而言，这些结果为自注意力容量(self-attention capacity)提供了一个具体的扩展定律(concrete scaling law)，并为在头(heads)之间分配键-查询预算(key-query budget)提供了有原则的设计规则(principled design rule)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#283", "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective", "link": "/arxiv/2509.22921", "arxiv_id": "2509.22921", "authors": "Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar", "summary": "We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-26", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.576309", "filter_reason": "这篇论文符合我的研究范围，核心理由如下： 首先，从本质上看，论文的核心是关于改进LLM的基础能力，提出了一种新的训练/优化范式。论文将大语言模型蒸馏问题构建为约束强化学习问题，这是一种新的方法论研究，而非将LLM作为工具应用于特定领域。 其次，论文符合多个正面指标： 1. 核心概念：明确关注大语言模型(LLM)的蒸馏问题 2. 能力方向：论文在数学推理任务上进行了实验，并证明其方法能提高模型的推理能力 3. 训练方法：使用了强化学习方法来优化蒸馏过程，属于新的训练范式探索 第三，论文不符合任何排除标准。虽然论文在数学推理任务上进行了实验，但数学推理被视为通用能力而非特定应用领域。论文的核心是提出一种通用的蒸馏方法，而不是专注于某个特定领域的应用。 论文的核心贡献是提出了一种原则性的优化框架，通过约束强化学习来改进LLM蒸馏过程，从而提高模型的推理能力。这种方法旨在解决如何更有效地从教师模型中提取知识，特别是关注推理能力的保持和提升，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。", "summary2": "本文旨在解决大语言模型蒸馏中任务特定奖励与教师模型保真度平衡的问题。针对数学推理任务，我们提出了一种基于约束强化学习的蒸馏方法，将蒸馏转化为在KL散度约束下最大化任务奖励的优化问题，并在GSM8K和MATH数据集上通过最终答案正确性、推理质量和约束满足率等指标验证了其有效性。", "inspiration_trace": "# 大语言模型蒸馏方法的逻辑演进：从约束马尔可夫决策过程视角\n\n## 一、宏观问题：大语言模型蒸馏的困境\n\n大语言模型(LLM)在各类任务中表现出色，但其庞大的规模和计算需求使其难以在资源受限环境中部署。知识蒸馏作为解决方案，旨在将大型教师模型的知识转移到小型学生模型中。然而，作者观察到传统蒸馏方法面临一个根本性挑战：**如何在保持教师模型推理能力的同时，优化学生模型在特定任务上的表现？**\n\n## 二、观察与问题分析\n\n### 1. 传统蒸馏方法的局限性\n作者首先观察到现有蒸馏方法主要关注最小化学生与教师模型间的KL散度，但这存在两个关键问题：\n- **次优学习问题**：仅关注KL散度会迫使学生模仿超出其能力的复杂推理路径，而非发现更简单但同样有效的推理策略\n- **奖励信号缺失**：忽略任务特定奖励信号，无法有效指导学生在复杂推理任务中的学习\n\n### 2. 现有混合方法的不足\n作者进一步分析了结合奖励信号与KL散度的现有方法（如引入超参数λ平衡两者），发现：\n- **超参数敏感性问题**：最佳λ难以预测，需要针对特定任务进行大量重新训练\n- **训练不稳定**：不同训练阶段可能需要不同的λ值，导致计算成本高昂且不稳定\n\n## 三、新视角：约束强化学习的引入\n\n### 1. 核心洞察\n作者提出一个关键视角转变：**将LLM蒸馏视为约束强化学习问题**，而非简单的正则化优化问题。具体表述为：\n- **目标**：最大化任务特定奖励\n- **约束**：学生与教师模型间的散度不超过预定义阈值d\n\n### 2. 相对优势\n这种表述相比传统方法具有显著优势：\n- **直观性**：阈值d直接以KL尺度表示，比调整λ更简单直观\n- **目标简化**：当学生足够接近教师时，目标自然简化为纯奖励最大化\n\n## 四、方法学挑战与突破\n\n### 1. 现有约束RL方法的局限性\n作者发现直接应用现有约束RL方法（如状态增强方法Saute）到蒸馏中存在问题：\n- **部署依赖**：需要在测试时访问教师模型计算每步约束，违背蒸馏初衷\n- **计算开销**：维护增强状态变量带来额外计算负担\n\n### 2. 关键假设与创新\n基于LLM的特性，作者提出一个关键假设：**在LLM蒸馏中，状态包含完整历史信息，因此增强状态是冗余的**。基于此假设，作者提出创新方法：\n- **去除状态增强**：利用完整历史信息重构剩余预算，消除对教师模型的测试时依赖\n- **修改奖励函数**：设计新的奖励函数，在满足约束时提供任务奖励，违反约束时施加差异化惩罚\n\n### 3. 理论保证\n作者通过严格理论分析证明：\n- **最优等价性**：无状态增强MDP与增强MDP的最优值函数等价\n- **约束满足**：当惩罚参数n→∞时，最优策略几乎必然满足约束\n- **Bellman最优性**：在标准假设下，Bellman方程成立\n\n## 五、实验验证与效果评估\n\n### 1. 实验设计\n作者在数学推理任务上进行系统实验，对比多种基线方法：\n- 纯奖励优化(GRPO)\n- 纯KL最小化(GKD, Mini-LLM)\n- 混合方法(GKD-GRPO)\n\n### 2. 关键发现\n实验结果证实了作者方法的有效性：\n- **平衡性能**：在推理质量和约束满足率方面优于基线，同时保持有竞争力的任务性能\n- **推理质量**：约束学生接近教师分布有助于保留和转移教师的推理能力\n- **奖励价值**：任务特定奖励对学生学习有效推理策略至关重要\n\n## 六、结论与意义\n\n作者的工作通过将LLM蒸馏重新表述为约束强化学习问题，成功解决了传统方法的核心挑战：\n1. **理论贡献**：提供了原则性框架，消除了对临时奖励加权的依赖\n2. **实践价值**：实现了无需教师访问的部署，同时保持约束满足的理论保证\n3. **应用前景**：为创建更小、可靠且专业的模型提供了新途径，这些模型可在教师模型的定义信任区域内可靠运行\n\n这一工作不仅解决了LLM蒸馏中的具体问题，也为约束优化在大型模型中的应用提供了新思路，展示了跨领域理论迁移的创新价值。", "summary_translation": "我们提出了一种新颖的大型语言模型（LLM，Large Language Model）蒸馏方法，将其表述为一个约束强化学习（constrained reinforcement learning）问题。尽管近期工作已开始探索将特定任务奖励（task-specific rewards）整合到蒸馏过程中，但现有方法通常依赖于临时奖励权重（ad-hoc reward weighting）。我们提出了一个原则性优化框架，该框架在最大化特定任务奖励的同时，约束与教师模型（teacher model）的差异（divergence）保持在指定阈值以下。我们的方法将约束状态增强强化学习（constrained state augmented reinforcement learning）适应于蒸馏设置，引入了一种修改后的奖励函数（reward function），该函数保持了约束满足（constraint satisfaction）的理论保证，而不需要在部署期间进行状态增强（state augmentation）或访问教师模型，且没有对偶拉格朗日方法（dual Lagrangian methods）的计算开销。通过对数学推理任务（mathematical reasoning tasks）的广泛实验，我们证明了与软拉格朗日松弛基线（soft Lagrangian relaxation baselines）相比，我们的方法实现了更好的约束满足率和更好的推理能力，同时保持了有竞争力的任务性能。我们的框架为资源受限环境（resource-constrained settings）下的奖励感知蒸馏（reward-aware distillation）提供了一个有理论依据且实际高效的解决方案。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#293", "title": "In-Context Learning can Perform Continual Learning Like Humans", "link": "/arxiv/2509.22764", "arxiv_id": "2509.22764", "authors": "Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding", "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing \"sweet spot\" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.", "subjects": "Machine Learning, Artificial Intelligence", "date": "2025-09-26", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.586388", "filter_reason": "这篇论文的核心是研究大型语言模型(LLMs)的上下文学习(ICL)能力，并将其扩展为上下文持续学习(ICCL)，提出了一种新的学习范式。从第一步核心判断来看，这属于改进LLM基础能力和提出新训练范式的研究，符合保留标准。持续学习能力是通用推理能力的重要组成部分，涉及到知识积累、记忆保留和跨任务学习等能力，这些都是通用推理的基础要素。论文没有将LLM作为工具应用到特定领域，而是关注LLM本身的能力提升机制。从第二步正面指标看，论文明确包含LLMs核心概念，虽然未直接提及reasoning、planning等术语，但持续学习本身就是一种通用问题解决能力。从第三步排除标准看，论文未涉及多模态、特定应用领域或模型可靠性的应用层面研究。因此，这篇论文符合\"大语言模型通用推理能力\"的研究范围，应该被保留。", "summary2": "本文旨在 [解决大型语言模型在持续学习中的知识积累和记忆保留问题]。针对 [多任务顺序到达的场景]，我们提出了一种 [上下文持续学习(ICCL)方法，通过任务调度和提示重排实现无参数更新的持续学习]，并在 [Markov-Chain基准测试] 上通过 [标准化性能指标和人类保留相似性指标(HRS-MD)] 验证了其有效性。", "inspiration_trace": "# 作者提出ICCL方法的逻辑链推演\n\n## 宏观问题：LLMs的持续学习能力\n\n作者从大语言模型(LLMs)的一个核心特性出发：**上下文学习(ICL)**能力，即模型无需参数更新就能通过提示中的示例适应新任务。然而，作者观察到现有研究主要关注ICL的单任务适应能力，忽视了其在多任务序列场景中的长期保留和知识积累潜力。\n\n> **核心问题**：ICL能否实现跨任务的持续学习，像人类一样长期保留并积累知识？\n\n## 关键观察：从人类记忆研究中获得启发\n\n作者从两个关键观察中获得了思路：\n\n1. **人类记忆的分布式练习效应**：认知科学研究表明，分布式练习(DP)比集中练习(MP)产生更好的长期记忆保留，存在一个\"间隔甜点\"。\n\n2. **传统持续学习的局限性**：基于梯度的持续学习方法(GBCL)面临灾难性遗忘(CF)和稳定性-可塑性困境，需要参数更新或显式记忆缓冲区。\n\n> **核心洞察**：ICL的\"无参数更新\"特性可能使其成为解决传统持续学习困境的新范式。\n\n## 假设形成：ICCL的认知可行性\n\n基于上述观察，作者提出了核心假设：\n\n1. **基础假设**：ICL可以通过任务调度和提示重排扩展为持续学习(ICCL)，使持续学习能力自然涌现。\n\n2. **具体假设**：\n   - ICCL会像人类一样从分布式练习中受益\n   - 可能存在一个\"间隔甜点\"使保留效果最大化\n   - 不同LLM架构的保留动态可能与人类记忆有不同程度的相似性\n\n## 方法论构建：从形式化到实验设计\n\n作者构建了一个完整的方法论体系：\n\n### 1. 形式化ICCL框架\n- 定义任务表示和历史经验序列\n- 引入**任务标识符**概念，用于区分不同任务但无需语义描述\n- 提出保留评估方法，量化模型对目标任务的记忆程度\n\n### 2. 设计调度策略\n- **单次练习(SP)**：目标任务只呈现一次\n- **集中练习(MP)**：目标任务连续重复呈现\n- **分布式练习(DP)**：交替呈现目标任务和其他任务\n\n### 3. 建立人类相似性度量\n- 基于ACT-R认知模型，提出**HRS-MD度量**，量化模型保留动态与人类记忆的相似性\n- 使用马氏距离比较模型拟合参数与人类参考分布的差异\n\n### 4. 设计实验验证\n- 使用**离散马尔可夫链**作为基准任务，减少预训练知识干扰\n- 评估不同类型LLMs(Transformer-based和线性注意力模型)和GBCL基线的保留性能\n- 分析不同调度策略和任务标识符对保留性能的影响\n\n## 验证与发现：实验结果支持假设\n\n通过实验，作者验证了其假设并获得了重要发现：\n\n1. **ICCL的有效性**：\n   - ICCL在保留性能上显著优于GBCL基线\n   - DP对ICCL的益处类似于对人类记忆的益处，确实存在\"间隔甜点\"\n\n2. **任务标识符的作用**：\n   - 任务标识符显著提高了ICCL的保留性能，特别是在复杂任务中\n\n3. **模型差异**：\n   - 线性注意力模型(MAMBA和RWKV)表现出最接近人类的保留模式\n   - 尽管这些模型的绝对保留性能落后于基于Transformer的LLMs\n\n## 结论与意义：建立认知合理的持续学习新范式\n\n作者得出结论：ICCL既在认知上合理，又在实践上有效，提供了一种仅推理的持续学习范式，解决了传统持续学习方法中的稳定性-可塑性困境。\n\n这一研究不仅为LLMs的上下文学习提供了新的应用方向，还建立了机器学习与人类认知科学之间的桥梁，为设计更符合人类认知模式的AI系统提供了新思路。", "summary_translation": "大型语言模型（Large language models, LLMs）可以通过上下文学习（in-context learning, ICL）适应新任务，而无需参数更新，这使它们成为快速适应的强大学习引擎。虽然大量研究已将ICL作为少样本学习器（few-shot learner）进行考察，但当多任务顺序到达时，它是否能实现长期保留（long-term retention）和跨任务知识积累（cross-task knowledge accumulation）仍未得到充分探索。受人类记忆研究的启发，我们研究了多任务环境中ICL的保留特性，并将其扩展到上下文持续学习（in-context continual learning, ICCL），其中持续学习能力通过任务调度（task scheduling）和提示重排（prompt rearrangement）而出现。在马尔可夫链基准测试（Markov-Chain benchmarks）上的实验表明，对于特定的大型语言模型，ICCL以类似于人类的方式从分布式练习（distributed practice, DP）中获益，持续揭示了保留的间隔\"最佳点\"（spacing \"sweet spot\"）。除了保留性能外，我们还提出了一种人类保留相似性度量（human-retention similarity metric），用于量化持续学习（continual-learning, CL）方法与人类保留动态（retention dynamics）的接近程度。使用该度量，我们表明线性注意力模型（linear-attention models）如MAMBA和RWKV表现出特别类似人类的保留模式，尽管它们的保留性能落后于基于Transformer的大型语言模型（Transformer-based LLMs）。总体而言，我们的结果确立了ICCL既在认知上合理（cognitively plausible）又在实践上有效，提供了一种仅推理的CL范式（inference-only CL paradigm），减轻了灾难性遗忘（catastrophic forgetting），并解决了传统CL方法中的稳定性-可塑性困境（stability-plasticity dilemma）。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#308", "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning", "link": "/arxiv/2509.25052", "arxiv_id": "2509.25052", "authors": "Sai Wang, Yu Wu, Zhongwen Xu", "summary": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T20:55:07.606343", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。首先，从核心判断来看，论文的本质是提出一种名为CEL的新型智能体架构，其核心贡献在于利用大语言模型进行推理和规划来学习，而非简单将LLM作为工具应用到特定领域。论文明确提出了一个新的学习范式，通过让LLM进行显式推理和规划来增强其通用问题解决能力，这直接符合\"改进LLM的基础能力、提出新的训练范式、增强其逻辑、规划和多步推理等通用能力\"的保留标准。 其次，论文满足多个正面指标：它明确以大语言模型(LLM)为核心概念；直接聚焦于推理(reasoning)和规划(planning)能力；提出了一种自我学习和进化的训练方法；并且属于基于LLM的智能体(llm-based agents)这一新兴范式。 第三，论文不符合任何排除标准：它不涉及多模态与视觉内容；虽然评估在游戏环境中进行，但这些只是作为测试平台，论文核心是提出通用架构而非特定应用领域；也没有聚焦于模型可靠性的应用层面问题。 最后，在特殊和模糊情况处理上，论文提出的是通用的智能体架构，而非针对特定领域的应用；同时，它强调通过显式推理构建透明和改进的世界模型，增强了模型的可解释性，从而提升通用推理质量。 综上所述，这篇论文的核心贡献是提出一种通过推理和规划来增强LLM通用能力的新方法，完全符合研究目标。", "summary2": "本文旨在[解决传统深度强化学习方法依赖大量经验且决策过程不透明的问题]。针对[网格世界游戏环境]，我们提出了一种[基于LLM的显式推理和规划的智能体架构CEL]，并在[Minesweeper, Frozen Lake和Sokoban三种网格世界任务]上通过[成功率]验证了其有效性。", "inspiration_trace": "# 从问题到方法：推演\"Cogito, Ergo Ludo\"的思考逻辑链\n\n## 一、宏观问题：如何创建能真正理解环境的智能体？\n\n人工智能领域长期追求的目标是创建能够掌握复杂环境的智能体。然而，作者观察到现有方法存在根本性局限：\n\n- **深度强化学习(DRL)**：虽然取得了显著成就，但依赖海量经验，将知识隐式编码在神经网络权重中，决策过程不透明\n- **基于LLM的智能体**：缺乏持续学习和适应的结构化机制，无法通过经验改进内部模型\n- **学习世界模型**：在不可解释的潜在状态上运行，将\"理解\"隐藏在黑盒中\n\n这引出了一个核心问题：**如何创建一个既能有效行动，又能以可解释方式理解环境的智能体？**\n\n## 二、核心假设：显式推理是通往高效可解释学习的关键\n\n基于对现有方法局限的分析，作者提出了一个核心假设：\n\n**通过利用大型语言模型进行显式推理和规划，可以创建一个能够构建环境显式、人类可读模型的智能体，从而实现更高效、更可解释的学习。**\n\n这一假设挑战了主流的\"黑盒\"学习范式，提出智能体应该通过\"思考\"来学习，而非仅仅通过反复试错。\n\n## 三、方法设计：构建\"Cogito, Ergo Ludo\"架构\n\n基于这一假设，作者设计了CEL架构，其核心创新在于：\n\n### 1. 基于语言的知识表示\n- 将所有信息（状态、动作、奖励、环境动态、策略）表示为自然语言字符串\n- 创建人类可读的\"世界模型\"和\"策略手册\"，而非隐式神经网络参数\n\n### 2. 双阶段学习循环\n- **阶段一：回合内决策** - 利用当前知识进行前瞻搜索和规划\n- **阶段二：回合后反思** - 分析完整轨迹，并行执行两个学习过程：\n  * **规则归纳**：提炼和更新对环境规则的理解\n  * **策略总结**：将成功和失败的行为模式提炼为可操作的策略手册\n\n### 3. 从零开始的学习机制\n- 初始状态为\"白板\"(tabula rasa)，除动作集外无任何先验知识\n- 通过交互-反思循环，自主构建环境模型和策略\n\n## 四、理论支撑：三大认知组件的协同\n\n作者设计了三个核心认知组件，它们共同构成了CEL的理论基础：\n\n### 1. 基于语言的世界模型(LWM)\n- 预测环境动态，输出为自然语言描述\n- 为显式规划提供基础，使智能体能够模拟和评估可能的未来结果\n\n### 2. 基于语言的价值函数(LVF)\n- 提供对状态长期成功潜力的定性、语言评估\n- 为决策提供关键启发式方法，评估当前状态的长期潜力\n\n### 3. 策略与战术手册(Π)\n- 将战略知识外显为可解释的文本手册\n- 实现快速上下文学习，显著加速战略适应\n\n## 五、实验验证：从理论到实践\n\n作者在三个具有挑战性的网格世界环境中验证了CEL：\n\n### 实验设计\n- **环境选择**：扫雷(逻辑推理)、冻结湖(导航)、推箱子(复杂规划)\n- **挑战设置**：稀疏奖励、未知规则\n- **评估指标**：成功率、学习效率、可解释性\n\n### 关键发现\n1. **有效性**：CEL在所有环境中均展示出显著的学习能力，成功率远超零样本基线\n2. **效率**：特别是在冻结湖任务中，仅用10个回合就达到97%的成功率\n3. **可解释性**：生成的规则书和策略手册全面且准确，展示了人类可读的\"理解\"\n4. **泛化能力**：在未见过的布局和新环境中表现出强大的泛化能力\n\n## 六、消融研究：确认关键组件的必要性\n\n通过系统的消融研究，作者确认了CEL架构中各组件的关键作用：\n\n- **静态规则vs动态更新**：仅进行一次规则归纳的智能体表现迅速下降，确认迭代规则更新对持续学习至关重要\n- **无规则vs完整模型**：没有规则归纳机制的智能体表现停滞，确认环境模型构建是实现能力的基础\n\n这些结果有力地支持了作者的核心假设：显式推理和规划是实现高效可解释学习的关键。\n\n## 七、更广泛意义：迈向透明智能的新范式\n\nCEL不仅是一个具体的智能体架构，更代表了一种新的智能体设计范式：\n\n1. **从隐式到显式**：将隐式神经网络知识转变为显式、人类可读的规则和策略\n2. **从黑盒到透明**：创建不仅有效行动，而且构建对世界透明理解的智能体\n3. **从经验驱动到推理驱动**：通过\"思考\"学习，而非仅通过反复试错\n\n这一范式为构建更通用、更可解释、更值得信赖的智能体提供了新路径，可能对人工智能的未来发展产生深远影响。\n\n## 总结：从观察到创新的逻辑链\n\n作者的思想路径形成了一个清晰的逻辑链：\n\n**观察局限(现有方法不透明、低效) → 核心问题(如何创建可理解的智能体) → 核心假设(显式推理是关键) → 方法设计(CEL架构) → 实验验证(多样化环境测试) → 结果确认(消融研究) → 更广泛意义(新智能体范式)**\n\n这一逻辑链展现了作者从宏观问题出发，通过系统观察、假设形成、方法设计和实验验证，最终提出创新性解决方案的完整思考过程，体现了严谨的学术思维和创新的方法论。", "summary_translation": "追求能够学习掌握复杂环境的人工智能代理（artificial agents）已经取得了显著成功，然而主流的深度强化学习（deep reinforcement learning）方法通常依赖于大量经验，将其知识不透明地编码在神经网络（neural network）权重中。我们提出了一种不同的范式，即代理通过推理（reasoning）和规划（planning）来学习。我们介绍了Cogito, ergo ludo (CEL)，一种新颖的代理架构，它利用大型语言模型（Large Language Model, LLM）来构建对环境机制和自身策略的明确、基于语言的理解。从没有任何先验知识（除动作集外）的白板状态（tabula rasa state）开始，CEL在交互和反思的循环中运行。在每个回合（episode）后，代理分析其完整轨迹（trajectory）以执行两个并行的学习过程：规则归纳（Rule Induction），在此过程中它完善对环境动力学的显式模型；以及策略与战术手册总结（Strategy and Playbook Summarization），在此过程中它将经验提炼为可操作的策略手册。我们在多样化的网格世界（grid-world）任务（即扫雷（Minesweeper）、冰湖（Frozen Lake）和推箱子（Sokoban））上评估了CEL，并表明CEL代理通过自主发现游戏规则和从稀疏奖励（sparse rewards）中开发有效策略（policies），成功地学会了掌握这些游戏。消融研究（Ablation studies）证实，迭代过程对于持续学习至关重要。我们的工作展示了一条通向更通用和可解释代理的路径，这些代理不仅能够有效行动，还能通过对原始经验的明确推理（explicit reasoning）来构建透明且不断改进的世界模型。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#368", "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models", "link": "/arxiv/2509.24261", "arxiv_id": "2509.24261", "authors": "Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, Lin Yan", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-09-29", "category": "cs.LG", "crawl_time": "2025-10-06T21:53:08.487753", "filter_reason": "根据筛选标准，这篇论文完全符合我的研究目标。首先，从核心判断来看，该论文的本质是改进大语言模型的基础推理能力，而非将其作为工具应用于特定领域。论文提出了一种新的风险敏感强化学习框架(RS-GRPO)，专门解决LLM在探索过程中的困境，这属于增强LLM通用推理能力的研究。 其次，论文包含多个正面指标：核心概念上明确聚焦于大语言模型(LLMs)；能力方向上专注于数学推理(math reasoning)；训练方法上提出了强化学习(RL)的新范式。这些都是提高LLM通用推理能力的关键要素。 第三，论文不符合任何排除标准。虽然论文使用了数学推理作为测试基准，但数学推理被视为通用推理能力的重要组成部分，而非特定应用领域。论文的核心贡献是提出一种通用的强化学习方法，而非解决特定领域问题。 最后，论文的核心贡献是通过改进强化学习算法来增强LLM的推理策略多样性，提高模型在推理任务上的表现，这与\"提高大语言模型本身的通用推理能力\"的研究目标高度一致。因此，这篇论文应该被保留。", "summary2": "本文旨在解决大型语言模型在强化学习微调中面临的探索困境问题。针对预训练LLMs初始策略分布尖锐导致的解决方案多样性不足问题，我们提出了一种风险敏感强化学习框架（Risk-Sensitive RL），并在6个数学推理基准测试上通过pass@k和pass@1指标验证了其有效性。", "inspiration_trace": "# 风险敏感强化学习解决LLM探索困境的逻辑推演\n\n## 1. 宏观问题：LLM强化学习中的探索困境\n\n**观察现象**：\n- 基于可验证奖励的强化学习(RLVR)能有效提升LLM在复杂推理任务上的表现\n- 但现有方法存在一个关键矛盾：提高了单解准确率(pass@1)，却抑制了解决方案多样性，导致多解性能(pass@k)停滞或下降\n- 这表明现有方法只是在强化预训练偏差，而非发现真正新颖的推理策略\n\n**问题定义**：作者将这一现象定义为\"探索困境\"(exploration dilemma)——标准RL算法无法从预训练LLM的尖锐初始策略分布中逃脱，被限制在局部最优解中。\n\n## 2. 问题根源：初始策略分布与RL算法的不匹配\n\n**深入分析**：\n- 传统RL设置(如游戏)通常从随机初始化策略开始，需要广泛探索\n- 而LLM微调则从高度专业化、已经围绕某些解决方案尖锐分布的策略开始\n- 如果这些初始峰值不在产生最优奖励的区域，标准RL优化器很难逃脱预训练模型偏差的\"引力\"\n\n**核心假设**：探索困境源于LLM优化景观与标准RL算法动态之间的根本不匹配。初始策略的尖锐分布导致RL算法陷入局部最优，无法发现更多样化和强大的推理路径。\n\n## 3. 解决思路：风险敏感强化学习的理论启发\n\n**理论借鉴**：\n- 风险敏感RL旨在建模和管理决策中的风险，超越基于期望的标准目标\n- 分布RL的出现促进了更细致的方法，包括风险寻求行为，已被证明可以促进探索\n- 风险寻求优化在游戏等领域显示出促进探索的潜力\n\n**关键假设**：风险寻求优化对于逃脱预训练模型的尖锐初始策略分布至关重要，能够实现更广泛的探索，从而发现新的推理策略。\n\n## 4. 方法设计：风险敏感目标函数\n\n**设计原则**：\n1. 应该重视所有高奖励结果，而非仅仅最可能的那个\n2. 应提供灵活机制，在优化平均奖励和最大奖励之间进行插值\n\n**具体实现**：\n- 采用基于指数效用的风险敏感目标：\n  J_RS(π_θ) = E_x~D[1/β log E_y~π_θ(·|x)[e^{βr(y)}]]\n- 超参数β控制风险敏感度：\n  - β→0：恢复标准期望奖励（风险中性）\n  - β→+∞：接近最大奖励（风险寻求，鼓励探索）\n  - β→-∞：接近最小奖励（风险规避，促进稳健性）\n\n## 5. 算法实现：风险敏感策略梯度\n\n**理论推导**：\n- 推导风险敏感目标的策略梯度形式\n- 提出风险敏感优势函数：\n  A_β^π(y) = 1/β [e^{βr(y)}/E_{y'~π(·|x)}[e^{βr(y')]} - 1]\n\n**算法特点**：\n- 实现为RS-GRPO算法，可作为标准GRPO中优势计算的直接替代\n- 只需修改优势计算部分，保持策略梯度结构不变，实现简单\n\n## 6. 理论支持：风险敏感方法的优势\n\n**关键发现**：\n- 标准策略梯度可能会降低最优动作的概率（当存在次优但优于平均的动作时）\n- 风险敏感策略梯度保证对最优动作的改进（当β足够大时）\n- β过大会导致收敛速度下降，提供实践指导：β应足够大以增强探索，但不宜过大影响收敛\n\n**实证验证**：\n- 设计多臂赌博机问题模拟探索困境\n- 结果显示标准风险中性策略被局部最优困住，而足够风险敏感的策略成功逃脱并达到全局最优\n\n## 7. 实验验证：全面评估方法效果\n\n**实验设置**：\n- 在六个数学推理基准和五个不同LLM上测试\n- 与标准GRPO和其他pass@k优化方法比较\n\n**主要结果**：\n- RS-GRPO一致且显著地提高了pass@k性能\n- 同时保持或增强了pass@1准确率\n- 实现了比现有方法更有效的平衡\n- 分析表明RS-GRPO发现了更多独特的解决方案，特别提高了困难问题的解决率\n\n**参数选择**：β=2提供了探索与利用的良好平衡，在pass@k和pass@1之间取得最优权衡。\n\n## 结论\n\n作者从观察到的LLM强化学习探索困境出发，通过分析问题根源（初始策略分布与RL算法的不匹配），借鉴风险敏感RL理论，提出风险寻求优化能帮助模型逃脱局部最优的核心假设，进而设计出风险敏感目标函数和RS-GRPO算法，并通过理论和实证验证了方法的有效性，形成了一个完整的逻辑链条。这一方法不仅解决了探索困境，还在保持pass@1性能的同时显著提升了pass@k表现，为LLM的强化学习微调提供了新思路。", "summary_translation": "可验证奖励强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)已被证明在增强大型语言模型(Large Language Models, LLMs)处理复杂推理任务方面是有效的。然而，现有方法面临一个探索困境(exploration dilemma)：预训练大型语言模型的尖峰初始策略(sharply peaked initial policies)将标准强化学习算法限制在狭窄的解决方案集合中，提高了单次通过率(pass@1)但抑制了解决方案的多样性和多次通过率(pass@k)。因此，RLVR往往只是提炼现有能力，而非发现新的推理策略。为克服这一问题，我们引入了一个风险敏感强化学习(Risk-Sensitive Reinforcement Learning)框架。我们的方法采用了一种在平均奖励和最大奖励之间进行插值的风险寻求目标(risk-seeking objective)，从而产生了一种新算法——风险敏感GRPO(Risk-Sensitive GRPO, RS-GRPO)，该算法通过放大对挑战性提示(challenging prompts)的学习来驱动更深入的探索。值得注意的是，RS-GRPO实现简单，仅需少量代码修改。在六个数学推理基准(mathematical reasoning benchmarks)和五种不同的大型语言模型上，RS-GRPO在保持或提高pass@1准确率的同时，一致地提升了pass@k性能。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#387", "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs", "link": "/arxiv/2509.24107", "arxiv_id": "2509.24107", "authors": "Shreyas Singh, Kunal Singh, Pradeep Moturi", "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T21:53:08.507628", "filter_reason": "这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。根据我的分析： 首先，从核心判断来看，论文本质上是提出了一种工具集成的推理方法(Fathom-DeepResearch)，通过增强小型语言模型(SLMs)的长期信息检索和综合能力来提高其通用推理能力。这属于改进LLM基础能力和提出新训练范式的研究，而非将LLM作为工具应用到特定领域。 其次，论文包含了所有重要的正面指标： - 核心概念：明确涉及SLMs(基于Qwen3-4B) - 能力方向：专注于\"tool-integrated reasoning\"，并在多样化推理任务(HLE、AIME-25等)上展示性能 - 训练方法：提出了RAPO强化学习方法和多智能体自我博弈训练策略 - 新兴范式：提出了智能体系统、工具使用和深度研究框架 第三，论文不主要聚焦于任何排除标准中的领域。虽然评估中提到了MedQA，但这只是作为系统泛化能力的测试基准，而非论文主要焦点。 特别地，在智能体/工具使用方面，论文提出的是一种通用的智能体协作框架来增强LLM的通用问题解决能力，而非针对特定领域的应用，完全符合保留标准。 综上所述，该论文的核心贡献是提出了一种新的训练范式和智能体系统，通过工具集成推理和强化学习方法来提升语言模型的通用推理能力，与研究目标高度一致。", "summary2": "本文旨在解决小型语言模型(SLMs)在长周期信息检索与合成方面的局限性。针对复杂、开放式的信息获取任务，我们提出了Fathom-DeepResearch智能体系统，该系统由Fathom-Search-4B和Fathom-Synthesizer-4B两个专门模型组成，并在DeepSearch基准测试(SimpleQA、FRAMES、WebWalker等)和DeepResearch-Bench上通过准确率、RACE和FACT等指标验证了其有效性。", "inspiration_trace": "# Fathom-DeepResearch方法论逻辑推演\n\n## 宏观问题：如何使小型语言模型(SLMs)具备长视界信息检索与综合能力？\n\n### 问题观察与分解\n\n作者首先观察到当前AI系统在复杂信息检索与综合任务中存在显著差距，特别是开源与闭源系统之间的性能鸿沟。通过深入分析，识别出四个核心挑战：\n\n#### 挑战1：多轮工具交互的训练不稳定性\n**观察**：GRPO等强化学习方法在单轮推理任务中有效，但在多轮工具交互环境中表现不稳定。工具响应导致策略模型分布偏移，引发解码不稳定和错误级联。\n\n**假设**：如果能稳定多轮强化学习过程，就能实现可靠的长视界工具使用。\n\n**解决方案路径**：开发RAPO（Reward Aware Policy Optimization），通过三个关键机制解决不稳定性：\n1. 数据集修剪：移除已解决的简单问题，形成自然课程学习\n2. 优势缩放：重新缩放令牌级优势，保持有效梯度幅度\n3. 重放缓冲区：在失败时注入高质量历史轨迹，恢复学习信号\n\n#### 挑战2：奖励黑客与低效工具调用\n**观察**：仅依赖最终正确性的稀疏奖励导致工具使用激增但性能下降，模型陷入重复调用模式。\n\n**假设**：设计能区分工具调用认知价值的细粒度奖励函数，可引导高效探索行为。\n\n**解决方案路径**：构建可控制的步骤级奖励系统，将工具调用分类为：\n- 搜索行为：唯一搜索 vs 冗余搜索\n- 查询行为：探索 vs 验证 vs 冗余查询\n\n基于分类计算奖励，惩罚冗余调用，同时奖励有价值的探索行为，实现工具使用的精确引导。\n\n#### 挑战3：高质量训练数据稀缺\n**观察**：现有数据集可通过参数知识或简单查询解决，无法反映真实网络检索的复杂性和不确定性。\n\n**假设**：创建必须依赖实时网络搜索、来源多样化且可验证的数据集，能提升模型检索能力。\n\n**解决方案路径**：开发DUET QA数据集，通过多智能体自我博弈管道：\n1. 两个搜索增强模型生成QA对\n2. 非搜索模型混淆问题以消除表面线索\n3. 交叉验证确保问题必须通过搜索解决且答案正确\n\n#### 挑战4：开放式查询的综合能力不足\n**观察**：现有方法主要针对封闭式查询优化，缺乏处理开放式探索性查询的综合能力。\n\n**假设**：专门的规划与综合模型能将多轮搜索轨迹转化为结构化、引用密集的报告。\n\n**解决方案路径**：设计Fathom-Synthesizer-4B，采用\"先计划后写作\"协议：\n1. 问题分解为子目标\n2. 定义报告结构\n3. 证据映射到各部分\n4. 生成引用严格的综合报告\n\n### 方法整合与系统构建\n\n作者将上述解决方案整合为Fathom-DeepResearch系统，包含两个专门模型：\n1. **Fathom-Search-4B**：专注于长视界信息检索与推理\n2. **Fathom-Synthesizer-4B**：专注于信息综合与报告生成\n\n这种分工设计使每个模型能专注于特定能力，同时协同工作完成端到端的研究任务。\n\n### 实验验证与迭代优化\n\n通过在多个基准测试上验证系统性能：\n- DeepSearch基准（SimpleQA、FRAMES等）\n- 一般推理基准（HLE、AIME-25等）\n- DeepResearch-Bench\n\n实验结果证实了方法的有效性，特别是在长视界工具使用（>20次调用）和开放式查询处理方面取得突破。\n\n### 逻辑链条总结\n\n从宏观问题出发，作者通过观察现有方法的局限性，提出针对性假设，开发创新解决方案，最终整合为完整系统。整个思考过程体现了：\n\n**问题识别→分解→假设形成→解决方案开发→系统整合→实验验证**\n\n的逻辑链条，每个环节都紧密衔接，形成了从理论到实践的完整方法论创新路径。", "summary_translation": "工具集成推理（Tool-integrated reasoning）已成为实现代理应用程序（agentic applications）的关键焦点。在这些应用中，DeepResearch Agents（深度研究代理）因其处理复杂、开放式信息检索任务的卓越性能而获得广泛关注。我们介绍了Fathom-DeepResearch，一个由两个专业模型组成的代理系统。第一个是Fathom-Search-4B，一个基于Qwen3-4B训练的DeepSearch（深度搜索）模型，通过实时网络搜索和定向网页查询进行基于证据的调查优化。其训练结合了三项创新：(i) DUETQA（双元问答），一个通过多代理自我博弈生成的5K样本数据集，强制执行严格的网络搜索依赖和异构源锚定；(ii) RAPO（奖励感知策略优化），GRPO的零开销扩展，通过课程修剪、奖励感知优势缩放和每个提示的重放缓冲区来稳定多轮可验证奖励强化学习；(iii) 可引导的步骤级奖励，根据认知行为和边际效用对每个工具调用进行分类，实现对搜索轨迹广度、深度和视野的明确控制。这些改进使得在必要时能够可靠地将工具调用扩展到20次以上。第二个是Fathom-Synthesizer-4B，同样基于Qwen3-4B训练，将多轮DeepSearch轨迹转换为结构化、引用密集的DeepResearch Reports（深度研究报告）以进行全面综合。在DeepSearch基准测试（SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue）和DeepResearch-Bench上的评估表明，该系统在开放权重类别中实现了最先进的性能，同时展现了对多样化推理任务（包括HLE, AIME-25, GPQA-Diamond和MedQA）的强大泛化能力。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#413", "title": "How LLMs Learn to Reason: A Complex Network Perspective", "link": "/arxiv/2509.23629", "arxiv_id": "2509.23629", "authors": "Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen", "summary": "Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.", "subjects": "Artificial Intelligence, Disordered Systems and Neural Networks, Statistical Mechanics, Machine Learning, Physics and Society", "date": "2025-09-28", "category": "cs.LG", "crawl_time": "2025-10-06T21:53:08.537905", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，这篇论文的本质是研究大语言模型(LLM)如何学习推理能力，并提出了一种新的训练方法(Annealed-RLVR)来增强模型的推理能力。论文不是将LLM作为工具应用于特定领域，而是直接关注提升LLM本身的基础推理能力，这符合\"保留\"标准。 其次，论文包含多个正面指标： - 核心概念：明确研究大语言模型(LLMs) - 能力方向：直接关注\"reasoning\"(推理)能力，这是论文的核心主题 - 训练方法：研究了\"Reinforcement Learning from Verifiable Rewards (RLVR)\"这一强化学习方法，并基于此提出了改进算法 第三，论文不符合任何排除标准。它没有涉及多模态与视觉、特定应用领域或模型可靠性(应用层面)的内容。 最后，论文不涉及特殊或模糊情况。它明确关注LLM的通用推理能力提升，而不是将智能体/工具应用于特定领域，也不是主要研究幻觉/可解释性/安全等问题。 论文的核心贡献是提出了一种复杂网络视角来理解LLM的推理过程，并基于此设计了新的训练算法来增强模型的推理能力。这直接符合研究目标中\"致力于提高大语言模型本身的通用推理能力\"的要求。", "summary2": "本文旨在解决LLMs在RLVR训练过程中出现的两阶段学习曲线、V型响应长度轨迹和灾难性遗忘等困惑行为。针对LLMs的推理过程，我们提出了一种基于稀疏概念网络的理论，该网络平均度接近2，并开发了Annealed-RLVR算法，在最大挫折点引入SFT\"加热\"步骤。在1.5B参数模型上通过best@k准确率等指标验证了该方法在分布内和分布外基准测试上均优于标准RLVR。", "inspiration_trace": "# 从现象到理论：LLM推理能力学习的复杂网络视角逻辑链\n\n## 一、宏观问题的提出：RLVR训练中的谜团\n\n作者首先观察到大型语言模型(LLMs)通过强化学习从可验证奖励(RLVR)训练时，普遍存在三个令人困惑的宏观现象：\n1. **两阶段学习曲线**：快速初始增益后进入长期平台期\n2. **V型响应长度轨迹**：正确解决方案长度先缩短后稳步增长\n3. **灾难性遗忘脆弱性**：与监督微调(SFT)交错时性能急剧下降\n\n这些现象在不同架构和基准测试中广泛存在，但缺乏统一解释。\n\n## 二、现有方法的局限与新思路的引入\n\n### 现有方法的障碍\n- 虽有研究将推理过程建模为\"推理图\"，但直接从高维潜空间构建完整图结构在计算上不可行\n- 微观分析的困难阻碍了对观察动态的结构起源的直接研究\n\n### 新视角：复杂系统与重整化群理论\n作者借鉴复杂系统科学和重整化群(RG)理论的核心逻辑：\n- 在复杂系统中，独立于微观细节的行为通常是涌现属性，由系统的大规模组织支配\n- 与其陷入难以处理的局部细节，不如直接假设系统大规模结构的核心原则来解释宏观行为\n\n## 三、核心假设的提出：稀疏概念网络理论\n\n基于上述思路，作者将焦点从完整推理图转向其粗粒度主干，提出核心假设：\n\n**假设1：概念网络（粗粒度推理图）是一个稀疏网络，其有效平均度被固定在⟨k⟩ ≈ 2**\n\n这一假设的关键含义：\n- 平均度为2意味着主要是树状结构，有利于泛化但本质上脆弱\n- 这种拓扑结构仍能支持罕见的大规模循环（如自我纠正），而不显著改变全局平均度\n\n## 四、假设验证与现象解释\n\n### 验证方法\n作者结合两种方法验证假设：\n1. **理论研究**：使用概念网络模型(CoNet)作为\"计算显微镜\"\n2. **数值实验**：在1.5B参数LLM上进行实验\n\n### 现象解释\n通过验证，作者揭示了观察到的谜题的结构叙事：\n\n1. **两阶段学习曲线与V型轨迹**：\n   - **第一阶段**：模型进行局部优化，快速发现短而高效的解决方案，形成断开的\"技能岛\"\n   - **第二阶段**：从局部发现转向全局整合，将这些\"岛\"编织成单一、扩展的概念网络\n   - 稀疏网络（平均度≈2）导致连接两个远距离概念需要遍历长中间路径，解释了V型曲线的上升斜率\n\n2. **灾难性遗忘的脆弱性**：\n   - 稀疏网络的低冗余性导致关键\"桥梁\"连接的脆弱性\n   - SFT会覆盖关键分支节点的策略，手术式切断大量下游子图的单连接点\n   - 损伤虽然影响深远，但在拓扑上是局部的，解释了为什么恢复速度很快\n\n## 五、微观机制的深入分析\n\n作者发现稀疏拓扑结构强加了RLVR训练的微观动态机制：\n\n### 1. 挫折诱导的遗忘：整合的代价\n- 在缓慢学习阶段开始时，系统进入最大挫折状态\n- 在⟨k⟩≈2约束下，技能岛为有限连接竞争，导致一些技能被竞争性抑制\n- 矛盾的是，这种状态最大化了探索能力：模型找到多样化、正确的分布外解决方案的能力达到峰值\n\n### 2. 相变式学习：增长的引擎\n- 在网络扩展前沿，新技能通过类似相变的过程被高效\"bolted on\"\n- 这种现象是\"临界学习\"(Learning at Criticality)机制在多任务环境中的扩展\n- 稀疏网络前沿的拓扑隔离使新技能能够通过干净的相变高效添加，不受系统范围竞争干扰\n\n## 六、理论应用：Annealed-RLVR算法的提出\n\n基于稀疏网络理论，作者识别出最大挫折状态既是危险瓶颈，也是探索潜力的峰值时刻，据此提出Annealed-RLVR算法：\n\n### 算法核心\n1. **时机选择**：在最大挫折状态（由宏观信号识别）进行干预\n2. **\"加热\"阶段**：应用短暂的SFT干预，打破策略对次优路线的过早承诺\n   - 仅对准确率低(<0.1)但存在正确解决方案的问题进行SFT\n3. **\"冷却\"阶段**：恢复标准RLVR，允许现在更灵活的策略重新稳定在更强大、更集成的最终状态\n\n### 算法原理\n- 利用稀疏网络理论识别的最佳干预时机\n- SFT作为\"加热\"冲击，打破连接，允许系统逃离可能的次优状态\n- 随后的RLVR作为受控\"冷却\"，引导系统重新形成连接，理想情况下稳定在更鲁棒的配置\n\n## 七、实验验证与效果\n\n作者在CoNet和1.5B参数LLM上验证了Annealed-RLVR的有效性：\n\n1. **性能提升**：在分布内和分布外(Minerva)基准测试上均优于标准RLVR\n2. **机制验证**：算法系统地减少了未解决问题数量，提高了已掌握问题的比例\n3. **探索-利用平衡**：SFT干预后，best@1准确率暂时下降，而best@16准确率上升，表明成功地将贪婪利用换取增强探索\n\n## 八、理论意义与未来展望\n\n### 核心贡献\n- 将RLVR从黑盒优化重新概念化为可预测的结构自组织过程\n- 为工程化未来AI系统的涌现推理能力提供了新的物理直觉\n- 揭示了稀疏网络结构如何统一解释RLVR训练中的多种现象\n\n### 未来方向\n- 长期目标是实证映射完整、微观的推理图\n- 这将产生一个强大的新分析对象：AI内部世界的详细地图\n- 为真正可解释的LLMs和未来人工智能的稳健安全性迈出关键一步\n\n## 总结\n\n作者从RLVR训练中的宏观谜团出发，借鉴复杂系统理论和重整化群思想，提出稀疏概念网络理论，通过实验验证解释了观察现象，深入分析了微观机制，并基于理论提出了实用的Annealed-RLVR算法，最终将RLVR重新概念化为结构自组织过程，为理解和工程化AI推理能力提供了新视角。这一完整的逻辑链条展现了从现象观察到理论形成再到方法应用的科学思考过程。", "summary_translation": "使用可验证奖励强化学习(Reinforcement Learning from Verifiable Rewards, RLVR)训练大语言模型表现出一系列独特且令人困惑的行为，这些行为尚未被充分理解，包括两阶段学习曲线(two-stage learning curve)、V形响应长度轨迹(V-shaped response-length trajectories)以及对灾难性遗忘(catastrophic forgetting)的显著脆弱性。在这项工作中，我们提出这些看似不同的现象可以用一个统一理论(unifying theory)来解释：模型的推理过程(reasoning process)映射到一个语义复杂网络(semantic complex network)的自组织(self-organization)，该网络的拓扑结构(topology)保持持续稀疏(persistently sparse)，平均度(average degree)固定在接近二。这种拓扑结构为遗忘和学习提供了一个基本机制：它首先将系统驱动到一个最大挫折状态(maximally frustrated state)，在此状态下形成\"技能岛\"(skill islands)，发生慢速学习(slow-learning)并诱导遗忘；然后系统进入一个急剧增长阶段(sharp growth phase)，新技能被\"添加\"(\"bolted on\")，这一过程由网络前沿(web's frontier)的相变类学习(phase-transition-like learning)驱动。基于这一理论，我们提出了\\textit{退火RLVR}(Annealed-RLVR)，这是一种有原则的算法(principled algorithm)，在最大挫折点引入基于SFT(SFT-based)的\"加热\"(\"heating\")步骤，以解决竞争瓶颈(competitive bottleneck)并增强模型的推理能力(reasoning capability)。在15亿参数模型(1.5B-parameter model)上的实验表明，该方法在分布内(in-distribution)和分布外(out-of-distribution)基准测试(benchmarks)上均优于标准RLVR。通过将RLVR从黑盒优化(black-box optimization)重塑为结构自组织(structural self-organization)的可预测过程(predictable process)，我们的工作为构建未来AI系统的涌现推理能力(emergent reasoning capabilities)提供了新的物理直觉(physical intuition)。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#449", "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction", "link": "/arxiv/2509.23186", "arxiv_id": "2509.23186", "authors": "Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen", "summary": "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.", "subjects": "Artificial Intelligence, Machine Learning", "date": "2025-09-27", "category": "cs.LG", "crawl_time": "2025-10-06T21:53:08.578833", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 第一步核心判断：这篇论文的本质是改进LLM的基础能力，特别是规划能力。论文研究多令牌预测(MTP)范式如何增强语言模型的传递关系学习能力，这直接关系到复杂规划这一通用推理能力的核心。作者提出了新的训练策略(Next-Token Injection和基于Transformer的传递层)来提升模型的路径规划能力，这属于改进LLM基础能力和提出新训练范式的研究。 第二步正面指标：论文包含了多个正面指标： - 核心概念：明确研究\"Large Language Models (LLMs)\" - 能力方向：专注于\"planning capability\"和\"complex planning\"，这是推理能力的重要组成部分 - 研究内容涉及\"transitive relation learning\"，这也是逻辑推理的关键方面 第三步排除标准：论文不涉及任何需要排除的领域： - 没有涉及多模态与视觉内容 - 没有针对特定应用领域(如医疗、化学等)，Blocksworld规划基准是通用规划能力的测试平台 - 没有专注于模型可靠性方面的应用层面问题 第四步特殊和模糊情况：论文不涉及需要特殊判断的情况，它明确聚焦于提升LLM的通用规划能力，而非特定领域的应用。 论文的核心贡献是通过理论分析和实验验证，揭示了多令牌预测范式如何帮助语言模型学习传递关系，并提出有效策略来增强模型的路径规划能力，这直接服务于提升LLM的通用推理能力，与研究目标高度一致。", "summary2": "本文旨在增强语言模型在复杂规划任务中的传递关系学习能力。针对图路径规划任务，我们提出了一种多令牌预测（MTP）范式及两种增强策略（Next-Token Injection和基于Transformer的传输层），并在合成图和Blocksworld规划基准上通过路径预测准确率验证了其有效性。", "inspiration_trace": "# 从问题洞察到方法创新：多令牌预测增强语言模型规划能力的逻辑推演\n\n## 一、宏观问题识别：语言模型的规划瓶颈\n\n作者从大型语言模型(LLMs)的一个根本性局限出发：尽管LLMs在多种任务上表现出色，但它们在**学习传递关系**方面存在显著困难。传递关系是复杂规划的基石，例如在路径规划中，如果知道A能到B且B能到C，理应能推断出A能到C。然而，标准自回归Transformer模型在这种传递推理上表现不佳，这构成了当前模型在结构化规划中的基本瓶颈。\n\n## 二、问题具体化：路径规划中的传递关系缺陷\n\n作者将宏观问题具体化为**图路径规划任务**中的传递关系学习：\n- 将规划问题抽象为有向图上的路径查找，节点表示状态，边表示可执行动作\n- 定义了四类测试路径，按传递推理难度递增：\n  - degree-0：训练中直接观察到的路径\n  - degree-1：需一步推理的路径\n  - degree-2：需两步推理的路径\n  - degree-3：需三步推理的路径\n\n**关键观察**：标准Transformer在degree-0/1任务上准确率超90%，但在degree-2任务上骤降至约60%，表明模型无法有效组合多个路径段来推断新的可达性关系。\n\n## 三、探索解决方案：多令牌预测范式的潜力\n\n作者转向探索**多令牌预测(MTP)**范式作为潜在解决方案，基于以下洞察：\n- MTP在单个训练步骤中预测多个未来令牌，提供更丰富的监督信号\n- MTP已显示出建模长期依赖和结构关系的潜力，被Meta和DeepSeek等公司采用\n- 然而，MTP在规划任务中的底层机制尚未被充分理解\n\n**核心假设**：MTP可能通过其多步骤监督机制帮助模型学习传递关系，特别是传递层可能逐渐学习多步邻接信息，使骨干模型能够捕捉未直接观察到的传递可达性关系。\n\n## 四、理论分析：揭示MTP的学习机制\n\n为验证假设，作者进行了严谨的理论分析：\n\n### 1. 简化模型构建\n构建简化Transformer模型（单层、单头、线性FFN、固定注意力矩阵），使分析可追踪。\n\n### 2. 2-令牌预测的数学推导\n通过数学推导证明：\n- **定理1**：传递矩阵W^T通过第2步预测误差更新，当模型低估节点k′的可达概率时，增加从中间节点d到k′的权重\n- **定理2**：第2步预测损失通过传递矩阵的梯度传播影响骨干参数W^M和W^V，使模型能学习传递可达性\n\n### 3. 关键发现\n- 传递层W^T逐渐学习多步邻接信息\n- 当W^T捕捉真实邻接关系时，2-令牌预测使骨干模型能学习传递可达性\n- MTP能实现比下一令牌预测更高阶的可达性学习，尽管可能引入少量虚假邻接噪声\n\n## 五、识别局限性并提出改进策略\n\n基于理论分析，作者识别出现有MTP架构的两个主要局限性：\n\n### 1. 传递层输入噪声问题\n传递层性能受骨干输出预测下一节点能力限制，预测偏差引入噪声。\n\n**解决方案：Next-Token Injection (NTI)**\n- 将真实下一令牌的嵌入向量注入骨干隐藏状态\n- 提供直接监督，类似ResNet捷径连接\n- 使梯度能绕过不稳定骨干状态，直接优化传递层\n\n### 2. 线性映射的表达能力限制\n线性传递层无法建模维度间依赖关系，限制多跳关系建模能力。\n\n**解决方案：Transformer-based传递层**\n- 用自注意力机制替换线性映射\n- 允许隐藏状态各维度交互整合信息\n- 显著增强传递层表达能力，更精确建模多跳关系\n\n## 六、实验验证：从理论到实践\n\n作者通过两类实验验证理论分析和改进策略：\n\n### 1. 合成图实验\n- 在随机DAG上评估不同degree路径的预测准确率\n- 结果显示：MTP模型在degree-2/3路径上显著优于基线；NTI和Transformer传递层进一步提升性能\n\n### 2. Blocksworld规划实验\n- 在经典规划基准上测试实际规划能力\n- 结果一致：改进的MTP方法在不同训练数据规模下均表现最佳\n\n### 3. 权重分析\n- 可视化传递层权重矩阵，确认其学习真实邻接关系\n- 分析骨干网络权重，验证传递可达性学习效果\n\n## 七、形成完整方法论\n\n基于理论分析和实验验证，作者形成了系统化的方法论：\n\n### 核心机制\nMTP通过多步骤监督使传递层学习多步邻接信息，进而使骨干模型能够捕捉未观察到的传递可达性关系，克服了标准自回归训练的传递性瓶颈。\n\n### 增强策略\n1. **NTI**：通过注入真实下一令牌信息减少噪声，提供更稳定监督\n2. **Transformer传递层**：通过自注意力建模维度间依赖，增强多跳关系建模能力\n\n### 应用框架\n该方法不仅适用于路径规划，还可扩展到其他需要传递关系推理的结构化规划任务，为构建具有结构感知能力的通用规划模型提供新思路。\n\n## 八、未来展望\n\n作者提出几个有前景的研究方向：\n1. 将方法应用于实际世界连续任务\n2. 扩展NTI至无监督设置\n3. 结合显式规划模块如思维链和回溯搜索\n4. 在强化学习范式中整合这些架构改进\n\n这一完整的逻辑链条展示了作者从宏观问题识别，到具体化问题，探索解决方案，理论分析验证，识别局限性并提出改进，最终形成系统化方法论的严谨思考过程。", "summary_translation": "大型语言模型（Large Language Models, LLMs）在多样化任务中取得了令人瞩目的性能，但在学习传递关系（transitive relations）方面仍然存在困难，而传递关系是复杂规划（complex planning）的基石。为解决这一问题，我们研究了多Token预测（Multi-Token Prediction, MTP）范式及其对传递关系学习的影响。我们使用由共享输出头（shared output head）和传递层（transfer layer）组成的Transformer架构，对MTP范式进行了理论分析。我们的分析表明，传递层逐渐学习多步邻接信息（multi-step adjacency information），进而使主干模型（backbone model）能够捕捉训练数据中未直接存在的未观测传递可达性关系（unobserved transitive reachability relations），尽管在邻接估计中存在一些不可避免的噪声。基于这一基础，我们提出了两种策略来增强传递层和整体学习质量：下一Token注入（Next-Token Injection, NTI）和基于Transformer的传递层。我们在合成图（synthetic graphs）和Blocksworld规划基准（Blocksworld planning benchmark）上的实验验证了我们的理论发现，并表明这些改进显著增强了模型的路径规划（path-planning）能力。这些发现深化了我们对具有MTP的Transformer如何在复杂规划任务中学习的理解，并提供了克服传递性瓶颈（transitivity bottleneck）的实用策略，为构建结构感知（structurally aware）和通用规划（general-purpose planning）模型铺平了道路。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#483", "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning", "link": "/arxiv/2509.22819", "arxiv_id": "2509.22819", "authors": "Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, Ke Ye", "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.", "subjects": "Artificial Intelligence, Formal Languages and Automata Theory, Machine Learning", "date": "2025-09-26", "category": "cs.LG", "crawl_time": "2025-10-06T21:53:08.618986", "filter_reason": "根据筛选标准，这篇论文完全符合\"大语言模型通用推理能力\"的研究范围。 首先，从核心判断来看，论文的本质是提出Hilbert这一智能体框架，通过结合非形式推理和形式验证来增强LLM的数学推理能力。这明显属于改进LLM基础能力的研究，特别是针对数学推理这一通用推理能力的提升，而非将LLM作为工具应用于特定领域。 其次，论文包含多个正面指标：核心概念上明确涉及Large language models (LLMs)；能力方向专注于reasoning，特别是math reasoning；新兴范式方面提出了llm-based agents框架，通过多个组件协作解决问题。 第三，论文不涉及任何排除标准中的领域：没有涉及多模态与视觉内容；虽然专注于数学证明，但数学推理被视为基础通用推理能力而非特定领域应用；虽然涉及形式验证，但这是作为提升推理能力的手段，而非专注于水印、安全等应用层面的可靠性问题。 最后，在特殊和模糊情况处理上，Hilbert是一个通用的智能体框架，其方法论（递归分解、结合非形式推理和形式验证）可以泛化到其他推理任务，而非仅限于特定领域应用。 论文的核心贡献是提出了一种新的智能体协作框架，通过结合非形式推理和形式验证，显著提高了LLM在数学推理方面的能力，这正是研究目标所关注的\"提高大语言模型本身的通用推理能力\"。", "summary2": "本文旨在弥合非形式推理LLMs与形式化定理证明系统之间的性能差距。针对数学定理证明问题，我们提出了一种HILBERT代理框架，通过递归分解和协调非形式推理LLM、专门证明LLM、形式化验证器和语义定理检索器，并在miniF2F和PutnamBench基准上通过问题解决率验证了其有效性，分别达到99.2%和70.0%，显著优于现有方法。", "inspiration_trace": "# HILBERT方法逻辑链推演\n\n## 1. 宏观问题：数学证明自动化的两难困境\n\n数学证明的自动化是人工智能领域的长期挑战。一方面，大型语言模型(LLMs)展现出强大的数学推理能力，能解决高难度竞赛问题；另一方面，其解决方案常包含无法自动验证的错误，缺乏可靠性。这引发了一个核心问题：如何同时获得LLMs的推理能力和形式化系统的验证保证？\n\n## 2. 现象观察：两种范式的性能鸿沟\n\n作者观察到两个关键现象：\n\n**现象一：通用LLMs的\"幻觉\"问题**\n- 通用LLMs如GPT-5、Gemini 2.5 Pro在AIME、Putnam等竞赛中表现优异\n- 但它们经常产生\"幻觉\"——听起来自信但实际错误的推理\n- 即使答案正确，底层推理常包含逻辑谬误、无根据假设和计算错误\n- 人工验证耗时、困难且易出错\n\n**现象二：专业证明LLMs的能力局限**\n- 形式化定理证明系统(如Lean 4)提供完全准确的自动验证\n- 专门训练的证明LLMs在生成形式化证明方面取得进展\n- 但存在显著性能差距：推理LLMs能非形式化解决约83%的PutnamBench问题，而最佳公开证明LLMs仅能形式化解决13%\n\n## 3. 深入分析：互补优势与现有方法的不足\n\n作者进一步分析发现：\n\n**分析一：两种方法的优势互补**\n- 通用LLMs擅长非形式化推理、问题分解和生成证明草图\n- 专业证明LLMs擅长生成语法正确的形式化证明\n- 两者能力互补：一个强于高层推理，一个强于精确实现\n\n**分析二：现有混合方法的局限性**\n- 早期方法(DSP、LEGO-Prover)使用通用LLMs生成草图，ATPs填充形式化部分，但受ATP能力限制\n- DSP+等方法采用浅层、单层分解，无法处理复杂子目标\n- 代理框架(COPRA、Prover-Agent)使用验证反馈迭代构建证明，但性能仍显著落后\n\n## 4. 核心假设：递归分解是关键\n\n基于以上分析，作者提出核心假设：**如果能够结合两种方法的优势，并实现递归分解能力，可能会显著提升形式化定理证明的性能**。\n\n关键洞察在于：现有方法失败的根本原因是缺乏递归分解能力——它们只能将原始问题分解一次，无法进一步分解仍然难以解决的子目标。\n\n## 5. 方法设计：HILBERT框架的诞生\n\n基于这一假设，作者设计了HILBERT框架，其核心创新包括：\n\n**创新一：多组件协调架构**\n- Reasoner：通用LLM，负责非形式化推理和问题分解\n- Prover：专业证明LLM，负责生成形式化证明\n- Verifier：形式化验证器，提供准确反馈\n- Retriever：语义定理检索器，提供相关知识支持\n\n**创新二：递归分解算法**\n- 当直接证明失败时，将问题分解为子目标\n- 对每个子目标采用两阶段策略：先尝试Prover，失败后使用Reasoner\n- 若仍失败，递归分解子目标，直到可解决或达到最大深度\n\n**创新三：验证反馈循环**\n- 利用Verifier反馈指导Proof修正\n- Reasoner解释编译错误，建议改进方案\n- 形成闭环优化系统\n\n## 6. 实验验证：从假设到证实\n\n为验证假设，作者在两个基准测试上进行了全面实验：\n\n**验证一：MiniF2F测试**\n- HILBERT达到99.2%通过率，比最佳公开方法高6.6个百分点\n- 即使使用较弱Prover，仍保持98.4%的高性能\n- 证明Reasoner的选择比Prover强度更关键\n\n**验证二：PutnamBench测试**\n- 解决70.0%问题(462/660)，比专有SeedProver高近20个百分点\n- 比最佳公开基线提高422%\n- 接近推理LLMs的非形式化解决率(82%)\n\n## 7. 消融研究：验证核心机制\n\n作者进一步验证了两个关键机制的有效性：\n\n**机制一：递归分解**\n- 性能随递归深度单调提升\n- 完整系统在D=3时达到98.7%，接近最优\n- 无浅层求解的变体需要更大深度才能达到类似性能\n\n**机制二：检索增强**\n- 提高通过率(98.4% vs 97.1%)\n- 显著减少计算资源(Reasoner调用从426降至420)\n- 通过提供相关定理简化证明，防止因定理名称错误导致的失败\n\n## 8. 结论与展望：缩小形式化与非形式化推理的鸿沟\n\nHILBERT成功缩小了非形式化推理与形式化证明生成之间的性能差距。作者展望未来可利用此框架训练更强的模型，形成\"生成证明→训练模型→解决更复杂问题\"的良性循环，持续推动形式化推理能力的发展。\n\n这一逻辑链展现了作者从宏观问题出发，通过观察现象、深入分析、形成假设、设计方法到实验验证的完整思考过程，体现了科学研究中的系统性创新思维。", "summary_translation": "大型语言模型（Large Language Models, LLMs）展示了令人印象深刻的数学推理能力，但它们的解决方案经常包含无法自动验证的错误。形式化定理证明系统（Formal theorem proving systems）如Lean 4提供了完全准确的自动验证，这推动了最近构建专门的证明器大型语言模型（prover LLMs）的努力，这些模型能够生成形式化语言中的可验证证明。然而，一个显著的差距仍然存在：当前的证明器大型语言模型解决的问题数量远少于在自然语言中运行的通用大型语言模型。我们介绍了Hilbert，一个通过结合非形式化推理和形式化验证的互补优势来弥合这一差距的智能体框架（agentic framework）。我们的系统协调四个组件：一个擅长数学推理的非形式化大型语言模型（informal LLM），一个针对Lean 4策略（tactics）优化的专用证明器大型语言模型，一个形式化验证器（formal verifier），和一个语义定理检索器（semantic theorem retriever）。面对证明器无法解决的问题，Hilbert采用递归分解（recursive decomposition）将问题分解为子目标，然后使用证明器或推理器大型语言模型来解决这些子目标。它利用验证器反馈在必要时修正不正确的证明。实验结果表明，Hilbert在关键基准测试上明显优于现有方法，在miniF2F上达到99.2%的准确率，比最佳公开方法高出6.6个百分点。Hilbert在PutnamBench上取得了已知的最佳结果。它解决了660个问题中的462个（70.0%），超过了SeedProver等专有方法（50.4%），比最佳公开基线方法提高了422%。因此，Hilbert有效地缩小了非形式化推理和形式化证明生成之间的差距。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}]}, {"name": "Multiagent Systems", "count": 2, "papers": [{"index": "#10", "title": "AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems", "link": "/arxiv/2509.24826", "arxiv_id": "2509.24826", "authors": "Hannah Kim, Kushan Mitra, Chen Shen, Dan Zhang, Estevam Hruschka", "summary": "Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.", "subjects": "Human-Computer Interaction, Multiagent Systems", "date": "2025-09-29", "category": "cs.MA", "crawl_time": "2025-10-06T20:55:00.689959", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。以下是详细分析： 第一步：核心判断——这篇论文的本质是提出AIPOM系统，用于增强多智能体系统中LLM的规划能力。这属于\"智能体协作框架\"的范畴，论文核心是改进LLM在规划方面的通用能力，而非将LLM作为工具应用到特定领域。因此符合保留标准。 第二步：正面指标——论文包含多个相关主题： - 核心概念: 明确提到\"Large language models (LLMs)\" - 能力方向: 明确提到\"planning\"，这是通用推理能力的重要组成部分 - 新兴范式: 明确提到\"multi-agent systems\"，这是当前LLM研究的重要方向 第三步：排除标准——论文不符合任何排除标准： - 不涉及多模态与视觉内容 - 不针对特定应用领域（如医疗、化学等） - 虽然提到透明度和可控性，但这是作为增强LLM规划能力的手段，而非主要研究模型可靠性的应用层面问题 第四步：特殊和模糊情况处理—— 论文涉及智能体/工具使用，但AIPOM是一个通用的多智能体系统框架，旨在增强LLM的通用规划能力和用户控制，而非针对特定领域的应用。因此符合保留条件。 综合判断：这篇论文的核心贡献是提出一个通用框架来增强LLM在多智能体系统中的规划能力，通过人机交互提高透明度和可控性，这直接关系到提升LLM的通用推理能力，特别是规划和问题解决方面，因此符合研究目标。", "summary2": "本文旨在解决基于LLM的多智能体系统规划中缺乏透明度和可控性的问题。针对多智能体工作流规划场景，我们提出了一种AIPOM系统，结合会话和图界面实现人机在环规划，并在GSM8K和Multi-Step Arithmetic数据集上通过执行准确性、同构子图匹配和图编辑距离等指标验证了其有效性。", "inspiration_trace": "# AIPOM核心方法逻辑推演\n\n## 宏观问题：多智能体系统规划中的透明性与可控性缺失\n\n### 问题观察\n1. **趋势识别**：大型语言模型(LLMs)正被越来越多地用于编排多智能体系统(OMAS)中的规划任务，这些系统通过多个专业化智能体协作解决复杂任务。\n   \n2. **现状分析**：尽管LLMs在规划方面展现出强大能力（复杂推理、跨领域泛化、世界知识利用），但在实际应用中存在显著问题，特别是在领域特定或高风险场景中。\n\n### 问题聚焦\n3. **核心挑战识别**：\n   - **准确性问题**：LLMs在专业领域可能生成不准确、不完整或与专家知识不符的输出。\n   - **透明度问题**：用户只能看到系统最终输出，无法了解底层计划结构或中间执行过程。\n   - **可控性问题**：传统聊天界面提供有限控制能力，难以进行细粒度的计划检查、优化和调试。\n\n4. **需求推导**：这些局限性表明人类监督应成为规划阶段的中心，需要支持用户积极参与和指导规划过程的交互界面。\n\n### 假设形成\n5. **核心假设**：通过增强透明度和可控性的人机交互界面，可以显著提高用户对多智能体工作流的理解、信任和控制能力。\n\n6. **设计原则确立**：\n   - **透明性原则**：系统应使计划结构和执行过程对用户完全可见。\n   - **可控性原则**：用户应能直接干预和修改计划的各个组成部分。\n   - **协作性原则**：应结合人类专业知识和LLM推理能力，实现混合主动规划。\n\n### 方法论构建\n7. **界面设计决策**：\n   - **双面板架构**：结合对话面板（自然语言交互）和计划面板（图形化表示）。\n   - **计划表示**：使用有向无环图(DAG)明确表示任务节点和依赖关系，包括智能体分配和数据流。\n   - **直接操作**：用户可通过直接操作图形元素修改计划结构、智能体分配和数据流。\n\n8. **交互模式设计**：\n   - **自然语言反馈**：适合高级指导，如塑造整体结构或意图。\n   - **直接操作**：适合精确或局部调整，保留大部分现有计划。\n   - **LLM辅助修复**：用户进行部分编辑后调用LLM完成、验证或修复计划。\n\n9. **系统架构实现**：\n   - **规划模块**：将用户请求转换为逻辑计划，考虑智能体能力和输入/输出要求。\n   - **对话模块**：解释用户话语并提取意图。\n   - **执行协调器**：管理跨智能体的子任务调度。\n   - **控制器**：协调各模块间通信，将用户输入转换为系统级操作。\n\n### 验证与评估\n10. **实验设计**：\n    - **定量实验**：评估计划优化性能，比较不同反馈格式（详细自然语言、模糊自然语言、直接操作+LLM修复）的效果。\n    - **试点研究**：比较不同计划表示格式（文本vs图形）和反馈模式（文本vs图形编辑）。\n\n11. **评估指标**：\n    - **功能正确性**：通过执行准确率衡量。\n    - **结构正确性**：通过图同构(ISO)和图编辑距离(GED)衡量。\n\n### 结论与展望\n12. **结论确认**：AIPOM通过结合对话和图形界面，通过灵活的人机交互增强了透明度和可控性，实验证明其在交互式计划优化中的有效性。\n\n13. **未来方向**：\n    - 应用于高风险领域（如医疗保健和金融）。\n    - 扩展用户交互，支持更复杂的操作（冻结、合并、拆分任务等）。\n    - 提高LLM在验证计划方面的辅助能力。\n    - 进行实际部署和更广泛的用户研究。", "summary_translation": "大型语言模型（Large language models, LLMs）正越来越多地被用于编排的多智能体系统（orchestrated multi-agent systems）中的规划。然而，现有的基于LLM的方法（LLM-based approaches）往往达不到人类期望，并且关键的是，缺乏让用户检查、理解和控制其行为的有效机制。这些局限性要求提高透明度、可控性和人类监督。为解决这一问题，我们引入了AIPOM，这是一个通过对话式和基于图形的界面（conversational and graph-based interfaces）支持人在环路规划（human-in-the-loop planning）的系统。AIPOM使用户能够透明地检查、细化和协作指导LLM生成的计划，显著增强了用户对多智能体工作流程（multi-agent workflows）的控制和信任。我们的代码和演示视频可在https://github.com/megagonlabs/aipom获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}, {"index": "#2", "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems", "link": "/arxiv/2509.24323", "arxiv_id": "2509.24323", "authors": "Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo", "summary": "The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.", "subjects": "Multiagent Systems, Computation and Language", "date": "2025-09-29", "category": "cs.MA", "crawl_time": "2025-10-06T20:55:00.687824", "filter_reason": "根据筛选标准，我判断这篇论文符合\"大语言模型通用推理能力\"的研究范围。具体分析如下： 第一步核心判断：这篇论文的本质是提出一种名为MAS$^2$的新型多智能体系统范式，该系统基于递归自我生成原则，能够自主构建定制化的多智能体系统。论文核心是改进LLM的基础能力，提出新的训练范式（Collaborative Tree Optimization），增强其自我进化、自适应配置和纠错能力，这些都是提升LLM通用推理能力的重要方面。论文不是将LLM作为工具应用于特定领域，而是专注于提升LLM系统本身的能力，因此应被保留。 第二步正面指标：论文包含多个正面指标主题： - 核心概念：明确提到\"Large Language Model (LLM)-powered multi-agent systems\" - 能力方向：涉及problem-solving，在复杂场景如\"deep research and code generation\"中表现优异 - 训练方法：提到系统的\"self-evolution\"轨迹，并提出\"Collaborative Tree Optimization\"来训练元智能体 - 新兴范式：完全围绕\"llm-based agents\"和\"multi-agent systems\"展开，并涉及\"deep research\" 第三步排除标准：论文不涉及任何需要排除的领域： - 不涉及多模态与视觉相关内容 - 不专注于特定应用领域（如医疗、化学等），而是在通用场景测试 - 不关注模型可靠性方面的水印、安全等问题 第四步特殊情况处理：论文提出的智能体框架是通用的，旨在增强LLM的通用问题解决能力，而不是应用于特定领域，符合保留标准。 综上所述，这篇论文的核心贡献是提出了一种自我生成、自我配置和自我纠错的多智能体系统框架，通过递归自我生成和动态组合来增强LLM的通用推理能力和自我进化能力，完全符合研究目标中关于\"智能体协作框架\"和\"自我进化\"等提升LLM通用推理能力的方法论研究。", "summary2": "本文旨在解决当前自动多智能体系统遵循静态\"生成一次即部署\"范式，导致系统在面对动态真实世界环境时脆弱且无法适应的问题。针对复杂任务场景，我们提出了一种基于递归自生成的MAS²范式，通过\"生成器-实现器-修正器\"三智能体框架动态构建和自适应修正任务特定的多智能体系统，并在七个基准测试上通过性能、成本和泛化能力指标验证了其有效性。", "inspiration_trace": "# MAS²方法逻辑链分析：从观察到创新\n\n## 1. 宏观问题：多智能体系统的演进与刚性瓶颈\n\n**观察起点**：多智能体系统(MAS)已从手动配置演进到完全自动化，形成了三个发展阶段：\n- 手动配置阶段（如AutoGen、MetaGPT）：完全依赖人工设计提示、角色和通信协议\n- 部分自动化阶段（如GPTSwarm）：自动设计通信拓扑，但其他组件仍需人工\n- 完全自动化阶段（如ADAS、MaAS）：端到端合成配置并在线适应\n\n**核心问题**：尽管自动化程度不断提高，现有系统普遍遵循\"生成一次就部署\"(generate-once-and-deploy)的刚性范式，无法适应现实世界的动态性和不确定性。\n\n## 2. 问题聚焦：现有方法的根本局限\n\n**深入分析**：作者将现有自动MAS方法分为两类，并识别其局限性：\n\n**外部模块方法**（如基于GNN、进化算法、搜索算法的系统）：\n- 局限于预定义的原子操作符搜索空间（如CoT、Reflexion）\n- 缺乏架构创新能力，无法突破设计空间的限制\n\n**智能体驱动方法**（如FlowReasoner、MAS-GPT）：\n- 虽然实现了任务级适应性，但仍遵循\"生成一次就部署\"范式\n- 系统一旦部署便保持不变，无法根据执行情况动态调整\n\n**根本矛盾**：现实世界环境充满不确定性（网络故障、工具崩溃、文件丢失），而现有系统缺乏应对这些扰动的自适应能力。\n\n## 3. 核心假设：递归自生成范式的提出\n\n**假设形成**：要突破现有方法的局限性，需要一种全新的范式，使多智能体系统能够：\n- 自主生成其他多智能体系统（自生成）\n- 根据任务需求动态配置系统（自配置）\n- 在运行时监控并修复系统（自修复）\n\n**核心思想**：提出MAS²范式，基于递归自生成原理——一个多智能体系统自主构建其他多智能体系统，实现真正的自适应能力。\n\n## 4. 方法设计：三智能体元架构\n\n**架构设计**：为实现递归自生成范式，设计了一个元多智能体系统(meta MAS)，包含三个专门化的元智能体：\n\n**生成器(Generator)**：\n- 功能：作为系统架构师，根据任务查询设计高级多智能体工作流模板\n- 输出：包含智能体角色、通信协议和工具集的抽象模板\n\n**实现者(Implementor)**：\n- 功能：将抽象模板转化为完全可执行系统\n- 输出：为每个角色分配具体LLM骨干，形成实例化的MAS\n\n**修复器(Rectifier)**：\n- 功能：实时监控系统执行状态，在检测到故障或资源超限时进行修正\n- 输出：对当前系统配置的修改，从局部调整到全局架构变更\n\n## 5. 训练方法：协作树优化(CTO)\n\n**训练挑战**：如何有效训练这三个专门化的元智能体，使其各司其职又协同工作？\n\n**解决方案**：提出协作树优化(CTO)框架，包含三个关键步骤：\n\n**轨迹收集**：\n- 三个智能体协作扩展决策树，表示多样化的MAS配置和执行路径\n- 生成器扩展K个候选模板，实现者为每个模板扩展N个实例化，修复器根据需要进一步分支\n\n**路径信用传播**：\n- 为每条轨迹（从根到叶节点的路径）分配成本敏感的奖励函数\n- 通过蒙特卡洛估计将终端结果归因于上游决策，为每个节点提供价值信号\n\n**专门化训练**：\n- 将价值注释树转化为包含偏好强度的偏好数据\n- 使用价值缩放损失函数专门化每个智能体的策略，优先学习高置信度决策\n\n## 6. 验证与评估：实验设计\n\n**评估框架**：在七个基准测试上评估MAS²，涵盖四个领域：\n- 多跳搜索（HotpotQA、Bamboogle、NQ）\n- 深度研究（BrowseComp+）\n- 代码生成（HumanEval、MBPP）\n- 数学推理（MATH）\n\n**评估维度**：\n- 性能优势：与现有SOTA方法的比较\n- 成本效益：在性能-成本权衡中的帕累托前沿位置\n- 跨骨干泛化：利用未见过的LLM的能力\n\n## 7. 逻辑链条总结\n\n从观察到创新的完整逻辑链条：\n\n**观察** → 多智能体系统演进过程中存在\"生成一次就部署\"的刚性范式\n  ↓\n**问题识别** → 这种刚性导致系统无法适应现实世界的动态性和不确定性\n  ↓\n**假设提出** → 递归自生成范式能够实现真正的自适应能力\n  ↓\n**方法设计** → 三智能体元架构（生成器-实现者-修复器）实现自生成、自配置、自修复\n  ↓\n**训练方法** → 协作树优化(CTO)框架通过路径信用传播和专门化训练实现元智能体协同\n  ↓\n**验证评估** → 多维度实验证明MAS²在性能、成本效益和泛化能力上的优势\n\n这一逻辑链条从宏观问题出发，逐步聚焦到具体方法设计，体现了作者系统性解决多智能体系统自适应挑战的创新思路。", "summary_translation": "过去两年见证了由大型语言模型（Large Language Model, LLM）驱动的多智能体系统（multi-agent systems, MAS）的迅猛崛起，这些系统利用集体智能并展现出向自我演化的显著轨迹。这一范式已迅速从需要定制配置提示词、工具、角色和通信协议的手动工程系统，发展为能够实现自动编排的框架。然而，主流的自动多智能体系统，无论是通过外部模块还是单个LLM智能体生成，大多遵循刚性的\"一次性生成并部署\"（generate-once-and-deploy）范式，使得生成的系统脆弱且难以应对现实世界环境的动态性和不确定性。\n\n为超越这一局限，我们提出了MAS$^2$，这是一种基于递归自生成原则的范式：一个能够为不同问题自主构建定制多智能体系统的多智能体系统。在技术上，我们设计了一个\"生成器-实现器-修正器\"（generator-implementer-rectifier）三智能体团队，能够根据实时任务需求动态组合并自适应修正目标智能体系统。我们提出了协同树优化（Collaborative Tree Optimization）方法来训练和专业化这些元智能体。\n\n在七个基准测试上的广泛评估表明，在深度研究和代码生成等复杂场景中，MAS$^2$相比最先进的MAS实现了高达19.6%的性能提升。此外，MAS$^2$展现出卓越的跨骨干网络泛化能力，有效利用未见过的LLM实现了高达15.1%的改进。关键的是，这些提升是在不产生过多token成本的情况下实现的，因为MAS$^2$始终位于成本-性能权衡的帕累托前沿（Pareto frontier）。源代码可在https://github.com/yeyeyeah2/MAS2获取。", "summary_generated_time": "2025-10-06 22:10:15", "summary_model": "z-ai/glm-4.5"}]}], "overview": "### 今日AI论文速览 (2025-09-30)\n\n今日AI研究呈现出多方向并进的繁荣景象，其中多智能体系统的自进化架构、推理模型的训练优化、新型记忆机制设计以及对齐方法的创新成为最突出的研究方向。这些研究不仅探索了模型能力边界的拓展，还关注了效率提升和认知机制模拟，展现了从架构创新到训练策略的全方位突破。\n\n## 一、多智能体系统的自进化与交互\n\n多智能体系统正朝着更加自主、自适应和可交互的方向发展，研究者们探索了如何让智能体系统实现自我生成、自我配置和自我修正。\n\n* **MAS²**提出了一种基于递归自生成的多智能体系统范式，通过\"**生成器-实现器-修正器**\"三智能体团队动态组合和自适应修正目标智能体系统。在七个基准测试中，MAS²比最先进的多智能体系统性能提升高达**19.6%**，并展现出卓越的跨主干泛化能力。(2509.24323 [cs.MA])\n\n* **AIPOM**引入了一种支持人机在环规划的系统，通过对话和基于图形的界面使用户能够透明地检查、完善和协同指导LLM生成的计划。AIPOM显著增强了用户对多智能体工作流程的控制和信任，解决了现有LLM方法缺乏有效机制让用户检查、理解和控制其行为的问题。(2509.24826 [cs.MA])\n\n* **Diagnose, Localize, Align**提出了一个三层框架来解决多智能体系统在指令冲突下的层次合规性问题：诊断（**CRAS评分指标**）、定位（注意力漂移分析）和对齐（**SAIL方法**）。该方法在标准基准和多智能体框架上提高了指令层次合规性，无需全模型微调。(2509.23188 [cs.CL])\n\n## 二、推理模型的训练与优化\n\n推理能力作为大语言模型的核心竞争力，今日研究在训练方法、计算效率和推理架构上均有重要突破，特别是在小模型推理能力提升方面成果显著。\n\n* **MobileLLM-R1**挑战了推理能力需要在大规模数据集上训练的假设，证明仅需约**2T token**的高质量数据就足以开发出强大的推理模型。MobileLLM-R1-950M在AIME上得分15.5，远超OLMo-2-1.48B的0.6分和SmolLM-2-1.7B的0.3分，尽管训练token数量仅为Qwen3的11.7%。(2509.24945 [cs.CL])\n\n* **SIRI**提出了一种简单而有效的RL方法，通过在训练期间动态调整最大滚动长度来迭代交替**压缩和扩展推理预算**。在DeepSeek-R1-Distill-Qwen-1.5B上训练后，SIRI-low在AIME24上性能提升43.2%，同时token使用减少46.9%。(2509.25176 [cs.CL])\n\n* **MARCOS**重新构想LLM中的推理，将其建模为连续高维\"思想\"的**隐马尔可夫链**，而不是自回归生成token序列。在三个基准测试中，MARCOS优于现有的连续推理方法，首次实现了与基于token的CoT相当的性能，在GSM8K上甚至超过4.7%，推理速度提高15.7倍。(2509.25020 [cs.LG])\n\n* **CLPO**提出了一种动态教学反馈循环算法，利用模型自身的滚动性能进行实时难度评估，构建**在线课程**。CLPO在八个具有挑战性的数学和一般推理基准上实现了最先进的性能，平均pass@1比其他方法提高6.96%。(2509.25004 [cs.AI])\n\n## 三、记忆架构与认知机制\n\n如何让AI智能体拥有更接近人类的记忆和认知能力是今日研究的热点，多种新型记忆架构被提出，旨在实现更自然的信息处理和自我进化能力。\n\n* **MemGen**提出了一种动态生成记忆框架，配备类人认知能力，包括**记忆触发器**和**记忆编织器**，使智能体能够回忆和增强潜在记忆。在八个基准测试中，MemGen超过了领先的外部记忆系统如ExpeL和AWM高达38.22%，超过了GRPO高达13.44%，并展现出强大的跨领域泛化能力。(2509.24704 [cs.CL])\n\n* **ReasoningBank**提出了一种记忆框架，从智能体自判断的成功和失败经验中提炼可泛化的**推理策略**。在网页浏览和软件工程基准测试中，ReasoningBank始终优于存储原始轨迹或仅成功任务例程的现有记忆机制。(2509.25140 [cs.CL])\n\n* **Identity Bridge**通过监督模型执行零跳身份任务解决了组合性差距，使模型能够成功执行分布外两跳推理。理论分析表明，身份监督通过优化过程中隐含的**核范数正则化**重塑了模型的潜在几何，诱导有利于低秩解决方案的潜在空间对齐。(2509.24653 [cs.LG])\n\n## 四、对齐与偏好学习的新方法\n\n模型对齐技术持续创新，研究者们探索了更高效、更鲁棒的偏好学习方法，以解决传统对齐方法中的分布不匹配和噪声敏感问题。\n\n* **UniAPL**重新将对齐定义为约束优化问题，提出**统一对抗偏好学习**框架，动态地将策略的分布与专家的分布对齐。在Qwen3-0.6B上比强GRPO基线提高5.77%，在Qwen3-4B上提高3.75%，甚至在某些任务上超越了教师模型。(2509.25148 [cs.AI])\n\n* **Robust Preference Optimization**提出了一种鲁棒偏好优化方法，使用**期望最大化算法**推断每个标签正确性的后验概率，自适应地重新权衡训练损失中的每个数据点。实验证明RPO作为元框架的一致有效性，持续增强了四种最先进对齐算法（DPO、IPO、SimPO和CPO）。(2509.24159 [cs.AI])\n\n* **Why Alignment Must Precede Distillation**论证了标准KD->Align工作流程削弱了模型对齐罕见但理想行为的能力，即使在强偏好信号下也是如此。理论和实验证明，反转流水线（即**Align->KD**）是必不可少的：对齐必须首先在高召回参考模型上执行，然后再进行蒸馏。(2509.23667 [cs.LG])\n\n## 五、工具增强智能体与信息检索\n\n工具使用能力作为智能体的核心竞争力，今日研究在工具集成推理、信息检索和深度研究方面取得了显著进展，特别是在提升小模型工具使用效率方面。\n\n* **InfoAgent**引入了一种由创新数据合成管道和编排的网络搜索工具驱动的**深度研究智能体**。InfoAgent在BrowseComp上达到15.3%准确率，在BrowseComp-ZH上达到29.2%，在Xbench-DS上达到40.4%，优于先前的开源深度研究智能体。(2509.25189 [cs.CL])\n\n* **Fathom-DeepResearch**引入了一个由两个专门模型组成的智能体系统：**Fathom-Search-4B**（通过实时网络搜索和针对性网页查询进行基于证据的调查）和**Fathom-Synthesizer-4B**（将多轮DeepSearch轨迹转换为结构化的引用密集型DeepResearch报告）。该系统在DeepSearch基准测试和DeepResearch-Bench上实现了开放权重类别中最先进的性能。(2509.24107 [cs.LG])\n\n* **Learning to Use Tools, Not Just When**识别了工具集成推理中的两种常见模式：**计算器模式**（使用代码进行直接计算）和**算法模式**（将问题编码为程序）。在具有挑战性的数学数据集上，这种模式感知方法显著提高了代码使用率和准确性，例如将MATH500上的Code@1从64.0%提高到70.5%。(2509.23292 [cs.CL])\n\n## 六、模型架构与训练创新\n\n基础模型架构和训练方法持续创新，研究者们探索了替代传统自回归生成的新方法，以及更高效的模型训练和优化技术。\n\n* **Alternatives To Next Token Prediction In Text Generation - A Survey**调查了替代下一个token预测(NTP)的新兴生态系统，将方法分为五个主要家族：**多Token预测**、**计划后生成**、**潜在推理**、**连续生成方法**和**非Transformer架构**。该调查为开发解决token级生成已知局限性、为自然语言处理开发新变革性模型的模型提供了分类法。(2509.24435 [cs.CL])\n\n* **LLaDA-MoE**引入了一种具有**混合专家(MoE)架构**的大语言扩散模型，从零开始在约20T token上训练。LLaDA-MoE在推理时仅激活1.4B参数，实现了显著减少的计算开销下的竞争性能，在多个基准测试上超越了之前的扩散语言模型。(2509.24389 [cs.CL])\n\n* **Evolution Strategies at Scale**报告了首次成功将**进化策略(ES)**扩展用于微调LLM的全部参数，证明ES可以在数十亿参数上有效搜索，并在多个方面优于现有的RL微调方法。ES在样本效率、对长时程奖励的容忍度、对不同基础LLM的鲁棒性、对奖励黑客的抵抗能力以及运行间性能稳定性方面表现出优势。(2509.24372 [cs.LG])\n\n### 今日看点\n\n1. **小模型推理能力的重大突破**：MobileLLM-R1和SIRI等研究证明，通过精心设计的高质量数据和创新的训练策略，亚十亿参数的小模型也能实现强大的推理能力，挑战了\"推理能力仅在大模型中涌现\"的传统认知，为资源受限场景下的AI应用开辟了新路径。\n\n2. **多智能体系统的自进化趋势**：MAS²和MemGen等研究展示了多智能体系统从静态配置向动态自演进的转变，这些系统能够根据任务需求自主生成、配置和修正自身架构，展现出接近组织的自适应能力，为构建更复杂、更灵活的AI系统提供了新范式。\n\n3. **推理架构的多元化探索**：从MARCOS的连续思想链到SIRI的压缩-扩展交替训练，研究者们正在突破传统自回归token生成的限制，探索更高效、更接近人类认知过程的推理架构，这些创新不仅提高了推理效率，还为理解AI推理机制提供了新视角。\n\n4. **对齐技术的效率与鲁棒性并重**：UniAPL和Robust Preference Optimization等研究在对齐方法上实现了重要突破，既解决了传统方法中的分布不匹配问题，又提高了对噪声数据的鲁棒性，同时降低了对大规模人工标注的依赖，为构建更可靠、更高效的AI对齐系统提供了新思路。"}